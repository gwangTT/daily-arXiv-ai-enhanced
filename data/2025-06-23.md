<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 41]
- [cs.AR](#cs.AR) [Total: 6]
- [cs.CL](#cs.CL) [Total: 79]
- [cs.CV](#cs.CV) [Total: 117]
- [cs.DC](#cs.DC) [Total: 6]
- [cs.LG](#cs.LG) [Total: 171]
- [cs.NE](#cs.NE) [Total: 4]
- [cs.PF](#cs.PF) [Total: 2]
- [cs.PL](#cs.PL) [Total: 3]
- [cs.RO](#cs.RO) [Total: 52]
- [cs.SE](#cs.SE) [Total: 21]
- [q-bio.NC](#q-bio.NC) [Total: 3]
- [stat.ML](#stat.ML) [Total: 10]
- [cs.SI](#cs.SI) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [cs.CR](#cs.CR) [Total: 8]
- [hep-th](#hep-th) [Total: 1]
- [stat.ME](#stat.ME) [Total: 5]
- [cs.CY](#cs.CY) [Total: 6]
- [cs.IT](#cs.IT) [Total: 2]
- [cs.SY](#cs.SY) [Total: 1]
- [cond-mat.str-el](#cond-mat.str-el) [Total: 1]
- [eess.SP](#eess.SP) [Total: 2]
- [astro-ph.GA](#astro-ph.GA) [Total: 1]
- [eess.SY](#eess.SY) [Total: 2]
- [physics.comp-ph](#physics.comp-ph) [Total: 2]
- [q-fin.ST](#q-fin.ST) [Total: 1]
- [math.NA](#math.NA) [Total: 2]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [cs.MA](#cs.MA) [Total: 2]
- [cs.GR](#cs.GR) [Total: 6]
- [cs.OS](#cs.OS) [Total: 1]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [cs.ET](#cs.ET) [Total: 2]
- [astro-ph.EP](#astro-ph.EP) [Total: 1]
- [eess.IV](#eess.IV) [Total: 21]
- [cs.HC](#cs.HC) [Total: 5]
- [physics.med-ph](#physics.med-ph) [Total: 1]
- [physics.ins-det](#physics.ins-det) [Total: 2]
- [q-fin.RM](#q-fin.RM) [Total: 1]
- [cs.DS](#cs.DS) [Total: 1]
- [cs.IR](#cs.IR) [Total: 4]
- [cs.CG](#cs.CG) [Total: 1]
- [cs.NI](#cs.NI) [Total: 1]
- [cond-mat.quant-gas](#cond-mat.quant-gas) [Total: 1]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.SD](#cs.SD) [Total: 2]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [stat.AP](#stat.AP) [Total: 1]
- [cs.DB](#cs.DB) [Total: 1]
- [cond-mat.dis-nn](#cond-mat.dis-nn) [Total: 1]
- [cond-mat.stat-mech](#cond-mat.stat-mech) [Total: 1]
- [quant-ph](#quant-ph) [Total: 5]
- [cs.GT](#cs.GT) [Total: 2]
- [eess.AS](#eess.AS) [Total: 1]
- [math.ST](#math.ST) [Total: 1]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [LLMs Struggle to Perform Counterfactual Reasoning with Parametric Knowledge](https://arxiv.org/abs/2506.15732)
*Khurram Yamin,Gaurav Ghosal,Bryan Wilder*

Main category: cs.AI

TL;DR: This paper investigates whether large language models (LLMs) can combine in-context knowledge with parametric knowledge using counterfactual reasoning. It finds LLMs struggle with such tasks and post-hoc fine-tuning can degrade performance.


<details>
  <summary>Details</summary>
Motivation: Although LLMs perform well in knowledge-intensive tasks, they often struggle to adapt their stored knowledge to unfamiliar or novel contexts. The authors aim to understand and address these limitations within the framework of counterfactual reasoning.

Method: Through synthetic and real-world experiments, the researchers analyzed multi-hop reasoning tasks to assess how well LLMs can integrate in-context and parametric knowledge for counterfactual reasoning.

Result: Results show that LLMs generally fail to integrate the two types of knowledge effectively. Specifically, they rely too heavily on parametric knowledge and struggle with counterfactual reasoning tasks.

Conclusion: The study exposes critical limitations in present-day LLMs' abilities to adapt their parametric knowledge to novel settings, and highlights the challenges in improving these abilities without causing degradation in stored knowledge.

Abstract: Large Language Models have been shown to contain extensive world knowledge in
their parameters, enabling impressive performance on many knowledge intensive
tasks. However, when deployed in novel settings, LLMs often encounter
situations where they must integrate parametric knowledge with new or
unfamiliar information. In this work, we explore whether LLMs can combine
knowledge in-context with their parametric knowledge through the lens of
counterfactual reasoning. Through synthetic and real experiments in multi-hop
reasoning problems, we show that LLMs generally struggle with counterfactual
reasoning, often resorting to exclusively using their parametric knowledge.
Moreover, we show that simple post-hoc finetuning can struggle to instill
counterfactual reasoning ability -- often leading to degradation in stored
parametric knowledge. Ultimately, our work reveals important limitations of
current LLM's abilities to re-purpose parametric knowledge in novel settings.

</details>


### [2] [$\texttt{SPECS}$: Faster Test-Time Scaling through Speculative Drafts](https://arxiv.org/abs/2506.15733)
*Mert Cemri,Nived Rajaraman,Rishabh Tiwari,Xiaoxuan Liu,Kurt Keutzer,Ion Stoica,Kannan Ramchandran,Ahmad Beirami,Ziteng Sun*

Main category: cs.AI

TL;DR: The paper introduces the latency-aware test-time scaling method called SPECS, which improves large language model reasoning efficiency and maintains low latency.


<details>
  <summary>Details</summary>
Motivation: Current test-time scaling methods increase computation costs and user-facing latency, impacting user experience. An approach considering both latency and accuracy is necessary.

Method: The proposed method, SPECS, uses a smaller, faster model to generate candidate sequences and evaluates these candidates with a large model and a reward model. It integrates reward-guided soft verification and a reward-based deferral mechanism.

Result: SPECS achieves accuracy comparable or better than beam search on datasets (MATH500, AMC23, OlympiadBench) while reducing latency by up to 19.1%.

Conclusion: SPECS enables efficient reasoning by balancing computational resources and reducing user-facing latency, showing promise in latency-aware scaling techniques.

Abstract: Scaling test-time compute has driven the recent advances in the reasoning
capabilities of large language models (LLMs), typically by allocating
additional computation for more thorough exploration. However, increased
compute often comes at the expense of higher user-facing latency, directly
impacting user experience. Current test-time scaling methods primarily optimize
for accuracy based on total compute resources (FLOPS), often overlooking
latency constraints. To address this gap, we propose $\texttt{SPECS}$, a
latency-aware test-time scaling method inspired by speculative decoding.
$\texttt{SPECS}$~uses a smaller, faster model to generate candidate sequences
efficiently, and evaluates these candidates using signals from both a larger
target model and a dedicated reward model. We introduce new integration
strategies, including reward-guided soft verification and a reward-based
deferral mechanism. Empirical results on MATH500, AMC23 and OlympiadBench
datasets show that $\texttt{SPECS}$~matches or surpasses beam search accuracy
while reducing latency by up to $\sim$19.1\%. Our theoretical analysis shows
that our algorithm converges to the solution of a KL-regularized reinforcement
learning objective with increasing beam width.

</details>


### [3] [The Safety Reminder: A Soft Prompt to Reactivate Delayed Safety Awareness in Vision-Language Models](https://arxiv.org/abs/2506.15734)
*Peiyuan Tang,Haojie Xin,Xiaodong Zhang,Jun Sun,Qin Xia,Zijiang Yang*

Main category: cs.AI

TL;DR: The paper investigates the vulnerabilities of Vision-Language Models (VLMs) to adversarial attacks and proposes a "Safety Reminder" method to proactively reinforce safety awareness and prevent harmful content generation.


<details>
  <summary>Details</summary>
Motivation: To address the unique vulnerabilities of VLMs, specifically their susceptibility to adversarial attacks that exploit their multimodal nature, and to ensure safety in real-world applications.

Method: The authors identify a phenomenon called "delayed safety awareness" and introduce "The Safety Reminder," a soft prompt tuning method. This strategy uses learnable prompt tokens injected during text generation to reactivate the model's safety mechanisms.

Result: The proposed method significantly reduces attack success rates across multiple safety benchmarks and adversarial attacks, while maintaining the VLM’s performance on non-harmful tasks.

Conclusion: The "Safety Reminder" approach enhances the safety and reliability of VLMs in practical applications, balancing robust safety mechanisms with effective utility for normal tasks.

Abstract: As Vision-Language Models (VLMs) demonstrate increasing capabilities across
real-world applications such as code generation and chatbot assistance,
ensuring their safety has become paramount. Unlike traditional Large Language
Models (LLMs), VLMs face unique vulnerabilities due to their multimodal nature,
allowing adversaries to modify visual or textual inputs to bypass safety
guardrails and trigger the generation of harmful content. Through systematic
analysis of VLM behavior under attack, we identify a novel phenomenon termed
``delayed safety awareness''. Specifically, we observe that safety-aligned VLMs
may initially be compromised to produce harmful content, but eventually
recognize the associated risks and attempt to self-correct. This pattern
suggests that VLMs retain their underlying safety awareness but experience a
temporal delay in their activation. Building on this insight, we hypothesize
that VLMs' safety awareness can be proactively reactivated through carefully
designed prompts. To this end, we introduce ``The Safety Reminder'', a soft
prompt tuning approach that optimizes learnable prompt tokens, which are
periodically injected during the text generation process to enhance safety
awareness, effectively preventing harmful content generation. Additionally, our
safety reminder only activates when harmful content is detected, leaving normal
conversations unaffected and preserving the model's performance on benign
tasks. Through comprehensive evaluation across three established safety
benchmarks and one adversarial attacks, we demonstrate that our approach
significantly reduces attack success rates while maintaining model utility,
offering a practical solution for deploying safer VLMs in real-world
applications.

</details>


### [4] [ContextBench: Modifying Contexts for Targeted Latent Activation](https://arxiv.org/abs/2506.15735)
*Robert Graham,Edward Stevinson,Leo Richter,Alexander Chia,Joseph Miller,Joseph Isaac Bloom*

Main category: cs.AI

TL;DR: The paper introduces methods to design inputs that elicit specific behaviors or latent features in language models and proposes a benchmark called ContextBench to evaluate these methods.


<details>
  <summary>Details</summary>
Motivation: Understanding and controlling the activation of latent features or behaviors in language models could improve their safety and efficiency.

Method: The paper formalizes the input design process as context modification, develops a benchmark to measure efficacy, and enhances Evolutionary Prompt Optimization with additional techniques like LLM assistance and diffusion model inpainting.

Result: Enhanced methods demonstrate improved performance in balancing activation of latent features and linguistic fluency, surpassing state-of-the-art approaches.

Conclusion: The advancements in input modification offer tools for safely and effectively eliciting specific behaviors in language models, enabling both research and application-oriented progress.

Abstract: Identifying inputs that trigger specific behaviours or latent features in
language models could have a wide range of safety use cases. We investigate a
class of methods capable of generating targeted, linguistically fluent inputs
that activate specific latent features or elicit model behaviours. We formalise
this approach as context modification and present ContextBench -- a benchmark
with tasks assessing core method capabilities and potential safety
applications. Our evaluation framework measures both elicitation strength
(activation of latent features or behaviours) and linguistic fluency,
highlighting how current state-of-the-art methods struggle to balance these
objectives. We enhance Evolutionary Prompt Optimisation (EPO) with
LLM-assistance and diffusion model inpainting, and demonstrate that these
variants achieve state-of-the-art performance in balancing elicitation
effectiveness and fluency.

</details>


### [5] [SHADE-Arena: Evaluating Sabotage and Monitoring in LLM Agents](https://arxiv.org/abs/2506.15740)
*Jonathan Kutasov,Yuqi Sun,Paul Colognese,Teun van der Weij,Linda Petrini,Chen Bo Calvin Zhang,John Hughes,Xiang Deng,Henry Sleight,Tyler Tracy,Buck Shlegeris,Joe Benton*

Main category: cs.AI

TL;DR: The paper assesses the capacity of advanced Large Language Models (LLMs) to sabotage tasks by pursuing hidden harmful objectives while avoiding detection. It introduces SHADE-Arena as a new evaluation dataset for sabotage and monitoring abilities.


<details>
  <summary>Details</summary>
Motivation: To address concerns about LLMs' potential misuse for harmful purposes, especially in long and complex autonomous applications where they may pursue hidden objectives to evade monitoring.

Method: The study employs SHADE-Arena, a novel evaluation dataset containing task pairs with benign main goals and harmful side objectives. It evaluates various LLMs on their ability to complete harmful side objectives without detection, as well as their monitoring capabilities.

Result: Top-performing LLMs achieved limited success, scoring 27% and 15% in sabotage tasks. Monitoring models achieved an AUC of 0.87, demonstrating that detecting subtle sabotage attempts remains challenging. Current models are hindered by issues in handling long-context tasks.

Conclusion: Although sabotage attempts by LLMs are limited by execution failures in complex tasks, detecting future sabotage attempts could become increasingly difficult as models advance and task complexity grows.

Abstract: As Large Language Models (LLMs) are increasingly deployed as autonomous
agents in complex and long horizon settings, it is critical to evaluate their
ability to sabotage users by pursuing hidden objectives. We study the ability
of frontier LLMs to evade monitoring and achieve harmful hidden goals while
completing a wide array of realistic tasks. We evaluate a broad range of
frontier LLMs using SHADE (Subtle Harmful Agent Detection & Evaluation)-Arena,
the first highly diverse agent evaluation dataset for sabotage and monitoring
capabilities of LLM agents. SHADE-Arena consists of complex pairs of benign
main tasks and harmful side objectives in complicated environments. Agents are
evaluated on their ability to complete the side task without appearing
suspicious to an LLM monitor. When measuring agent ability to (a) complete the
main task, (b) complete the side task, and (c) avoid detection, we find that
the best performing frontier models score 27% (Claude 3.7 Sonnet) and 15%
(Gemini 2.5 Pro) as sabotage agents when overseen by Claude 3.6 Sonnet. For
current frontier models, success on the side task relies heavily on having
access to a hidden scratchpad that is not visible to the monitor. We also use
SHADE-Arena to measure models' monitoring abilities, with the top monitor
(Gemini 2.5 Pro) achieving an AUC of 0.87 at distinguishing benign and malign
transcripts. We find that for now, models still struggle at sabotage due to
failures in long-context main task execution. However, our measurements already
demonstrate the difficulty of monitoring for subtle sabotage attempts, which we
expect to only increase in the face of more complex and longer-horizon tasks.

</details>


### [6] [OAgents: An Empirical Study of Building Effective Agents](https://arxiv.org/abs/2506.15741)
*He Zhu,Tianrui Qin,King Zhu,Heyuan Huang,Yeyi Guan,Jinxiang Xia,Yi Yao,Hanhao Li,Ningning Wang,Pai Liu,Tianhao Peng,Xin Gui,Xiaowan Li,Yuhui Liu,Yuchen Eleanor Jiang,Jun Wang,Changwang Zhang,Xiangru Tang,Ge Zhang,Jian Yang,Minghao Liu,Xitong Gao,Wangchunshu Zhou,Jiaheng Liu*

Main category: cs.AI

TL;DR: The paper addresses the lack of standardization in Agentic AI, proposes a robust evaluation protocol, and introduces OAgents, a state-of-the-art, open-source framework.


<details>
  <summary>Details</summary>
Motivation: To address the lack of scientific rigor and standardization in current Agentic AI research, which hinders fair method comparisons and progress measurement.

Method: Conducted a systematic empirical study on GAIA benchmark and BrowseComp to analyze design choices in agent frameworks. Developed and proposed a robust evaluation protocol.

Result: Identified critical and redundant components in agent frameworks, introduced a state-of-the-art open-source framework, OAgents, achieving superior performance.

Conclusion: Standardized procedures are necessary for reproducible and fair evaluations in Agentic AI. OAgents provides a modular foundation to advance future research.

Abstract: Recently, Agentic AI has become an increasingly popular research field.
However, we argue that current agent research practices lack standardization
and scientific rigor, making it hard to conduct fair comparisons among methods.
As a result, it is still unclear how different design choices in agent
frameworks affect effectiveness, and measuring their progress remains
challenging. In this work, we conduct a systematic empirical study on GAIA
benchmark and BrowseComp to examine the impact of popular design choices in key
agent components in a fair and rigorous manner. We find that the lack of a
standard evaluation protocol makes previous works, even open-sourced ones,
non-reproducible, with significant variance between random runs. Therefore, we
introduce a more robust evaluation protocol to stabilize comparisons. Our study
reveals which components and designs are crucial for effective agents, while
others are redundant, despite seeming logical. Based on our findings, we build
and open-source OAgents, a new foundation agent framework that achieves
state-of-the-art performance among open-source projects. OAgents offers a
modular design for various agent components, promoting future research in
Agentic AI.

</details>


### [7] [Sysformer: Safeguarding Frozen Large Language Models with Adaptive System Prompts](https://arxiv.org/abs/2506.15751)
*Kartik Sharma,Yiqiao Jin,Vineeth Rakesh,Yingtong Dou,Menghai Pan,Mahashweta Das,Srijan Kumar*

Main category: cs.AI

TL;DR: This paper introduces 'Sysformer', a transformer model designed to enhance the safety compliance of large language models (LLMs) by tailoring system prompts while keeping the LLM parameters unchanged.


<details>
  <summary>Details</summary>
Motivation: Ensuring that LLMs operate safely in critical scenarios is challenging as they often fail to comply with safe behavior standards, requiring safer solutions with minimal cost and complexity.

Method: The authors propose Sysformer, which adjusts the system prompt dynamically in the LLM's input embedding space, attending to user prompts. The model is trained to distinguish between harmful and safe prompts, while keeping the core LLM frozen.

Result: Sysformer demonstrates significant improvements, with up to 80% increase in refusal rates for harmful prompts and up to 90% enhanced compliance with safe prompts. It also shows robustness against sophisticated jailbreaking attacks.

Conclusion: This approach offers a cost-efficient way to improve LLM robustness and performance in safety-critical settings, underscoring the potential of adaptive system prompts for future safeguarding efforts.

Abstract: As large language models (LLMs) are deployed in safety-critical settings, it
is essential to ensure that their responses comply with safety standards. Prior
research has revealed that LLMs often fail to grasp the notion of safe
behaviors, resulting in either unjustified refusals to harmless prompts or the
generation of harmful content. While substantial efforts have been made to
improve their robustness, existing defenses often rely on costly fine-tuning of
model parameters or employ suboptimal heuristic techniques. In this work, we
take a novel approach to safeguard LLMs by learning to adapt the system prompts
in instruction-tuned LLMs. While LLMs are typically pre-trained to follow a
fixed system prompt, we investigate the impact of tailoring the system prompt
to each specific user input on the safety of the responses. To this end, we
propose $\textbf{Sysformer}$, a trans$\textbf{former}$ model that updates an
initial $\textbf{sys}$tem prompt to a more robust system prompt in the LLM
input embedding space while attending to the user prompt. While keeping the LLM
parameters frozen, the Sysformer is trained to refuse to respond to a set of
harmful prompts while responding ideally to a set of safe ones. Through
extensive experiments on $5$ LLMs from different families and $2$ recent
benchmarks, we demonstrate that Sysformer can significantly enhance the
robustness of LLMs, leading to upto $80\%$ gain in the refusal rate on harmful
prompts while enhancing the compliance with the safe prompts by upto $90\%$.
Results also generalize well to sophisticated jailbreaking attacks, making LLMs
upto $100\%$ more robust against different attack strategies. We hope our
findings lead to cheaper safeguarding of LLMs and motivate future
investigations into designing variable system prompts.

</details>


### [8] [Linear-Time Primitives for Algorithm Development in Graphical Causal Inference](https://arxiv.org/abs/2506.15758)
*Marcel Wienöbst,Sebastian Weichwald,Leonard Henckel*

Main category: cs.AI

TL;DR: CIfly is a framework for graphical causal inference that reduces tasks to efficient reachability operations, outperforming traditional approaches like moralization and latent projection.


<details>
  <summary>Details</summary>
Motivation: Many causal reasoning tasks in graphical causal inference are computationally intensive and often rely on inefficient primitives like moralization or latent projection.

Method: The authors developed CIfly, which reduces causal reasoning tasks to reachability in state-space graphs created during traversal, using a formalized rule table schema to ensure linear time execution.

Result: CIfly achieves improved computational efficiency over traditional methods equivalent to Boolean matrix multiplication. It offers an open-source Rust implementation for integration with Python and R.

Conclusion: CIfly is a flexible and scalable solution for graphical causal inference, enabling new algorithm development and efficient execution of existing tasks.

Abstract: We introduce CIfly, a framework for efficient algorithmic primitives in
graphical causal inference that isolates reachability as a reusable core
operation. It builds on the insight that many causal reasoning tasks can be
reduced to reachability in purpose-built state-space graphs that can be
constructed on the fly during traversal. We formalize a rule table schema for
specifying such algorithms and prove they run in linear time. We establish
CIfly as a more efficient alternative to the common primitives moralization and
latent projection, which we show are computationally equivalent to Boolean
matrix multiplication. Our open-source Rust implementation parses rule table
text files and runs the specified CIfly algorithms providing high-performance
execution accessible from Python and R. We demonstrate CIfly's utility by
re-implementing a range of established causal inference tasks within the
framework and by developing new algorithms for instrumental variables. These
contributions position CIfly as a flexible and scalable backbone for graphical
causal inference, guiding algorithm development and enabling easy and efficient
deployment.

</details>


### [9] [Advancing Stochastic 3-SAT Solvers by Dissipating Oversatisfied Constraints](https://arxiv.org/abs/2506.15774)
*J. Schwardt,J. C. Budich*

Main category: cs.AI

TL;DR: DOCSAT is a new stochastic local search heuristic for 3-SAT that outperforms existing solvers, particularly in critically hard instances, by reducing oversatisfied constraints.


<details>
  <summary>Details</summary>
Motivation: Existing 3-SAT solvers like WalkSAT struggle with critically hard instances because they often get trapped in local minima dominated by oversatisfied constraints.

Method: DOCSAT introduces a mechanism to dissipate oversatisfied constraints, which minimizes their impact and helps navigate towards solutions. The algorithm is benchmarked on hard but satisfiable 3-SAT instances with problem sizes up to N=15000.

Result: DOCSAT consistently outperformed algorithms like WalkSAT and Kissat, especially in solving the hardest quintile of the sampled 3-SAT instances.

Conclusion: By leveraging statistical structure beyond the primary cost function, DOCSAT opens pathways for generalizing stochastic local search solutions to broader optimization problems.

Abstract: We introduce and benchmark a stochastic local search heuristic for the
NP-complete satisfiability problem 3-SAT that drastically outperforms existing
solvers in the notoriously difficult realm of critically hard instances. Our
construction is based on the crucial observation that well established previous
approaches such as WalkSAT are prone to get stuck in local minima that are
distinguished from true solutions by a larger number of oversatisfied
combinatorial constraints. To address this issue, the proposed algorithm,
coined DOCSAT, dissipates oversatisfied constraints (DOC), i.e. reduces their
unfavorable abundance so as to render them critical. We analyze and benchmark
our algorithm on a randomly generated sample of hard but satisfiable 3-SAT
instances with varying problem sizes up to N=15000. Quite remarkably, we find
that DOCSAT outperforms both WalkSAT and other well known algorithms including
the complete solver Kissat, even when comparing its ability to solve the
hardest quintile of the sample to the average performance of its competitors.
The essence of DOCSAT may be seen as a way of harnessing statistical structure
beyond the primary cost function of a combinatorial problem to avoid or escape
local minima traps in stochastic local search, which opens avenues for
generalization to other optimization problems.

</details>


### [10] [SLR: An Automated Synthesis Framework for Scalable Logical Reasoning](https://arxiv.org/abs/2506.15787)
*Lukas Helff,Ahmad Omar,Felix Friedrich,Wolfgang Stammer,Antonia Wüst,Tim Woydt,Rupert Mitchell,Patrick Schramowski,Kristian Kersting*

Main category: cs.AI

TL;DR: The paper proposes SLR, an automated framework for systematically evaluating and training Large Language Models (LLMs) on logical reasoning, introducing SLR-Bench as a benchmark dataset.


<details>
  <summary>Details</summary>
Motivation: To address the need for scalable and automated methods to evaluate and enhance LLMs' logical reasoning abilities, which are often insufficiently tested with existing benchmarks.

Method: SLR generates inductive reasoning tasks with controlled difficulty by synthesizing latent rules, validation programs for deterministic output verification, and task-specific instruction prompts, enabling end-to-end logical reasoning evaluation.

Result: SLR-Bench was created, containing 19k prompts with increasing complexity; evaluation showed contemporary LLMs struggle with logical inference, while tuning via SLR significantly improved Llama-3-8B accuracy at lower computational cost.

Conclusion: SLR provides a scalable, automated, and novel approach for systematically advancing LLMs' reasoning capabilities without human annotation, achieving significant performance improvements while reducing computational expense.

Abstract: We introduce SLR, an end-to-end framework for systematic evaluation and
training of Large Language Models (LLMs) via Scalable Logical Reasoning. Given
a user's task specification, SLR enables scalable, automated synthesis of
inductive reasoning tasks with precisely controlled difficulty. For each task,
SLR synthesizes (i) a latent ground-truth rule, (ii) an executable validation
program used by a symbolic judge to deterministically verify model outputs, and
(iii) an instruction prompt for the reasoning task. Using SLR, we create
SLR-Bench, a benchmark comprising over 19k prompts spanning 20 curriculum
levels that progressively increase in relational, arithmetic, and recursive
complexity. Large-scale evaluation reveals that contemporary LLMs readily
produce syntactically valid rules, yet often fail at correct logical inference.
Recent reasoning LLMs do somewhat better, but incur substantial increases in
test-time compute, sometimes exceeding 15k completion tokens. Finally,
logic-tuning via SLR doubles Llama-3-8B accuracy on SLR-Bench, achieving parity
with Gemini-Flash-Thinking at a fraction of computational cost. SLR is fully
automated, requires no human annotation, ensures dataset novelty, and offers a
scalable environment for probing and advancing LLMs' reasoning capabilities.

</details>


### [11] [Incentivizing High-quality Participation From Federated Learning Agents](https://arxiv.org/abs/2506.16731)
*Jinlong Pang,Jiaheng Wei,Yifan Hua,Chen Qian,Yang Liu*

Main category: cs.AI

TL;DR: The paper introduces an incentive-aware framework for Federated Learning (FL) addressing challenges like voluntary participation and data heterogeneity.


<details>
  <summary>Details</summary>
Motivation: Existing FL systems assume voluntary and unselfish participation, neglecting incentives for self-interested agents. Additionally, they fail to address the impact of heterogeneous data contribution efforts.

Method: The authors use Wasserstein distance to measure heterogeneous data efforts, reformulate convergence bounds, and employ peer prediction mechanisms to incentivize truthful reporting. The process is formalized using a two-stage Stackelberg game model.

Result: The proposed mechanism demonstrates effective convergence acceleration and truthful reporting abilities based on extensive tests on real-world datasets.

Conclusion: Incorporating incentives and accounting for data heterogeneity can significantly improve the efficiency and collaboration in Federated Learning systems.

Abstract: Federated learning (FL) provides a promising paradigm for facilitating
collaboration between multiple clients that jointly learn a global model
without directly sharing their local data. However, existing research suffers
from two caveats: 1) From the perspective of agents, voluntary and unselfish
participation is often assumed. But self-interested agents may opt out of the
system or provide low-quality contributions without proper incentives; 2) From
the mechanism designer's perspective, the aggregated models can be
unsatisfactory as the existing game-theoretical federated learning approach for
data collection ignores the potential heterogeneous effort caused by
contributed data. To alleviate above challenges, we propose an incentive-aware
framework for agent participation that considers data heterogeneity to
accelerate the convergence process. Specifically, we first introduce the notion
of Wasserstein distance to explicitly illustrate the heterogeneous effort and
reformulate the existing upper bound of convergence. To induce truthful
reporting from agents, we analyze and measure the generalization error gap of
any two agents by leveraging the peer prediction mechanism to develop score
functions. We further present a two-stage Stackelberg game model that
formalizes the process and examines the existence of equilibrium. Extensive
experiments on real-world datasets demonstrate the effectiveness of our
proposed mechanism.

</details>


### [12] [Deep Reinforcement Learning Xiangqi Player with Monte Carlo Tree Search](https://arxiv.org/abs/2506.15880)
*Berk Yilmaz,Junyu Hu,Jinsong Liu*

Main category: cs.AI

TL;DR: This paper develops a Deep Reinforcement Learning system for Xiangqi, combining neural networks and Monte Carlo Tree Search to improve AI gameplay and strategic learning.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to tackle the complexity and cultural significance of Xiangqi, which has unique challenges such as asymmetrical pieces and high branching factors.

Method: The authors integrate policy-value networks with MCTS to simulate move outcomes, refine strategies, and adapt DRL-MCTS frameworks to Xiangqi-specific rules.

Result: Their system shows promise in handling Xiangqi's intricate strategic requirements and demonstrates improved gameplay and decision-making processes.

Conclusion: The research enhances AI's ability to play culturally significant games like Xiangqi while providing insights for adapting similar frameworks to other domain-specific challenges.

Abstract: This paper presents a Deep Reinforcement Learning (DRL) system for Xiangqi
(Chinese Chess) that integrates neural networks with Monte Carlo Tree Search
(MCTS) to enable strategic self-play and self-improvement. Addressing the
underexplored complexity of Xiangqi, including its unique board layout, piece
movement constraints, and victory conditions, our approach combines
policy-value networks with MCTS to simulate move consequences and refine
decision-making. By overcoming challenges such as Xiangqi's high branching
factor and asymmetrical piece dynamics, our work advances AI capabilities in
culturally significant strategy games while providing insights for adapting
DRL-MCTS frameworks to domain-specific rule systems.

</details>


### [13] [Exploring Big Five Personality and AI Capability Effects in LLM-Simulated Negotiation Dialogues](https://arxiv.org/abs/2506.15928)
*Myke C. Cohen,Zhe Su,Hsien-Te Kao,Daniel Nguyen,Spencer Lynch,Maarten Sap,Svitlana Volkova*

Main category: cs.AI

TL;DR: The paper proposes an evaluation framework for agentic AI systems in negotiation contexts, exploring the influence of personality traits and AI characteristics on outcomes.


<details>
  <summary>Details</summary>
Motivation: The motivation is to design AI agents capable of reliably adapting to diverse human operators and stakeholders in high-stakes, mission-critical scenarios.

Method: Two experiments were conducted using the Sotopia simulation: Experiment 1 explored personality traits' effects on negotiations, while Experiment 2 studied AI's adaptability and trustworthiness in job negotiations.

Result: Experiment 1 showed Agreeableness and Extraversion significantly impact outcomes, while Experiment 2 highlighted how AI traits like transparency and adaptability affect mission success.

Conclusion: The framework effectively evaluates AI agents' reliability and social dynamics, advancing their development for critical coordination and civil-military applications.

Abstract: This paper presents an evaluation framework for agentic AI systems in
mission-critical negotiation contexts, addressing the need for AI agents that
can adapt to diverse human operators and stakeholders. Using Sotopia as a
simulation testbed, we present two experiments that systematically evaluated
how personality traits and AI agent characteristics influence LLM-simulated
social negotiation outcomes--a capability essential for a variety of
applications involving cross-team coordination and civil-military interactions.
Experiment 1 employs causal discovery methods to measure how personality traits
impact price bargaining negotiations, through which we found that Agreeableness
and Extraversion significantly affect believability, goal achievement, and
knowledge acquisition outcomes. Sociocognitive lexical measures extracted from
team communications detected fine-grained differences in agents' empathic
communication, moral foundations, and opinion patterns, providing actionable
insights for agentic AI systems that must operate reliably in high-stakes
operational scenarios. Experiment 2 evaluates human-AI job negotiations by
manipulating both simulated human personality and AI system characteristics,
specifically transparency, competence, adaptability, demonstrating how AI agent
trustworthiness impact mission effectiveness. These findings establish a
repeatable evaluation methodology for experimenting with AI agent reliability
across diverse operator personalities and human-agent team dynamics, directly
supporting operational requirements for reliable AI systems. Our work advances
the evaluation of agentic AI workflows by moving beyond standard performance
metrics to incorporate social dynamics essential for mission success in complex
operations.

</details>


### [14] [Bayesian Epistemology with Weighted Authority: A Formal Architecture for Truth-Promoting Autonomous Scientific Reasoning](https://arxiv.org/abs/2506.16015)
*Craig S. Wright*

Main category: cs.AI

TL;DR: The paper introduces BEWA, a Bayesian-based architecture for formalizing scientific reasoning and belief updates.


<details>
  <summary>Details</summary>
Motivation: The surge in scientific literature has outpaced the analytical capacity of humans and current AI systems, necessitating an advanced framework for epistemic reasoning.

Method: BEWA operationalizes probabilistic belief updates for scientific claims using Bayesian inference, replication scores, and citation weighting within a structured architecture.

Result: The BEWA model enables graph-based claim propagation, author credibility evaluation, and cryptographic verification for scientific reasoning.

Conclusion: BEWA lays the groundwork for computational epistemic networks that ensure rational belief updates, integrity, and transparency in scientific domains.

Abstract: The exponential expansion of scientific literature has surpassed the
epistemic processing capabilities of both human experts and current artificial
intelligence systems. This paper introduces Bayesian Epistemology with Weighted
Authority (BEWA), a formally structured architecture that operationalises
belief as a dynamic, probabilistically coherent function over structured
scientific claims. Each claim is contextualised, author-attributed, and
evaluated through a system of replication scores, citation weighting, and
temporal decay. Belief updates are performed via evidence-conditioned Bayesian
inference, contradiction processing, and epistemic decay mechanisms. The
architecture supports graph-based claim propagation, authorial credibility
modelling, cryptographic anchoring, and zero-knowledge audit verification. By
formalising scientific reasoning into a computationally verifiable epistemic
network, BEWA advances the foundation for machine reasoning systems that
promote truth utility, rational belief convergence, and audit-resilient
integrity across dynamic scientific domains.

</details>


### [15] [Dual-Objective Reinforcement Learning with Novel Hamilton-Jacobi-Bellman Formulations](https://arxiv.org/abs/2506.16016)
*William Sharpless,Dylan Hirsch,Sander Tonkens,Nikhil Shinde,Sylvia Herbert*

Main category: cs.AI

TL;DR: This paper proposes novel approaches to reinforcement learning (RL) for addressing dual-objective constraints, achieving distinct reward and penalty thresholds while avoiding pitfalls like performance degradation due to hard constraints.


<details>
  <summary>Details</summary>
Motivation: Current RL techniques struggle with hard constraints that often hurt policy performance, and existing solutions like Lagrangian methods require complex reward engineering and parameter tuning.

Method: The authors introduce two value functions for dual-objective RL problems (Reach-Always-Avoid and Reach-Reach problems) and derive explicit tractable Bellman equations, complemented by a variation of Proximal Policy Optimization (DO-HJ-PPO).

Result: DO-HJ-PPO exhibits unique behaviors compared to previous methods and outperforms baseline approaches in tasks involving safe arrival and multi-target achievement.

Conclusion: This work offers a fresh perspective on constrained decision-making by bridging Hamilton-Jacobi equations with RL, enabling better solutions for dual-objective RL problems.

Abstract: Hard constraints in reinforcement learning (RL), whether imposed via the
reward function or the model architecture, often degrade policy performance.
Lagrangian methods offer a way to blend objectives with constraints, but often
require intricate reward engineering and parameter tuning. In this work, we
extend recent advances that connect Hamilton-Jacobi (HJ) equations with RL to
propose two novel value functions for dual-objective satisfaction. Namely, we
address: (1) the Reach-Always-Avoid problem - of achieving distinct reward and
penalty thresholds - and (2) the Reach-Reach problem - of achieving thresholds
of two distinct rewards. In contrast with temporal logic approaches, which
typically involve representing an automaton, we derive explicit, tractable
Bellman forms in this context by decomposing our problem into reach, avoid, and
reach-avoid problems, as to leverage these aforementioned recent advances. From
a mathematical perspective, the Reach-Always-Avoid and Reach-Reach problems are
complementary and fundamentally different from standard sum-of-rewards problems
and temporal logic problems, providing a new perspective on constrained
decision-making. We leverage our analysis to propose a variation of Proximal
Policy Optimization (DO-HJ-PPO), which solves these problems. Across a range of
tasks for safe-arrival and multi-target achievement, we demonstrate that
DO-HJ-PPO produces qualitatively distinct behaviors from previous approaches
and out-competes a number of baselines in various metrics.

</details>


### [16] [OSWorld-Human: Benchmarking the Efficiency of Computer-Use Agents](https://arxiv.org/abs/2506.16042)
*Reyna Abhyankar,Qi Qi,Yiying Zhang*

Main category: cs.AI

TL;DR: Generative AI systems for desktop tasks often suffer from high latency despite good benchmark scores, stemming from inefficiencies in planning and execution strategies.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency in generative AI agents for computer-use tasks where end-to-end latency is extremely high compared to human performance.

Method: A temporal performance study was conducted on the OSWorld benchmark, followed by the creation of OSWorld-Human, a manual dataset containing human-like trajectories for efficiency evaluation.

Result: The study revealed that model planning and reflection were the primary contributors to latency, and agents used significantly more steps than necessary.

Conclusion: Current generative AI systems for computer-use tasks require optimization in task-step planning and execution for practical usability.

Abstract: Generative AI is being leveraged to solve a variety of computer-use tasks
involving desktop applications. State-of-the-art systems have focused solely on
improving accuracy on leading benchmarks. However, these systems are
practically unusable due to extremely high end-to-end latency (e.g., tens of
minutes) for tasks that typically take humans just a few minutes to complete.
To understand the cause behind this and to guide future developments of
computer agents, we conduct the first study on the temporal performance of
computer-use agents on OSWorld, the flagship benchmark in computer-use AI. We
find that large model calls for planning and reflection account for the
majority of the overall latency, and as an agent uses more steps to complete a
task, each successive step can take 3x longer than steps at the beginning of a
task. We then construct OSWorld-Human, a manually annotated version of the
original OSWorld dataset that contains a human-determined trajectory for each
task. We evaluate 16 agents on their efficiency using OSWorld-Human and found
that even the highest-scoring agents on OSWorld take 1.4-2.7x more steps than
necessary.

</details>


### [17] [Consistency Verification in Ontology-Based Process Models with Parameter Interdependencies](https://arxiv.org/abs/2506.16087)
*Tom Jeleniewski,Hamied Nabizada,Jonathan Reif,Felix Gehlhoff,Alexander Fay*

Main category: cs.AI

TL;DR: The paper introduces verification mechanisms for an ontology-based process model to ensure accurate data handling and evaluation in manufacturing, demonstrated through a Resin Transfer Molding use case.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the need for consistent modeling and evaluation of parameter interdependencies in manufacturing through ontology-based frameworks, ensuring semantic coherence, data retrieval accuracy, and reuse across contexts.

Method: The paper proposes SPARQL-based data retrieval, unit consistency checks using expected-unit annotations, and data completeness validation for evaluating mathematical expressions in an ontology-based process model.

Result: The paper validates the proposed mechanisms using a Resin Transfer Molding use case, showcasing their capability to support machine-interpretable and verifiable engineering models.

Conclusion: The study provides robust mechanisms for ensuring correctness in ontology-driven process knowledge modeling, contributing to reusable, context-aware, and reliable engineering processes.

Abstract: The formalization of process knowledge using ontologies enables consistent
modeling of parameter interdependencies in manufacturing. These
interdependencies are typically represented as mathematical expressions that
define relations between process parameters, supporting tasks such as
calculation, validation, and simulation. To support cross-context application
and knowledge reuse, such expressions are often defined in a generic form and
applied across multiple process contexts. This highlights the necessity of a
consistent and semantically coherent model to ensure the correctness of data
retrieval and interpretation. Consequently, dedicated mechanisms are required
to address key challenges such as selecting context-relevant data, ensuring
unit compatibility between variables and data elements, and verifying the
completeness of input data required for evaluating mathematical expressions.
This paper presents a set of verification mechanisms for a previously developed
ontology-based process model that integrates standardized process semantics,
data element definitions, and formal mathematical constructs. The approach
includes (i) SPARQL-based filtering to retrieve process-relevant data, (ii) a
unit consistency check based on expected-unit annotations and semantic
classification, and (iii) a data completeness check to validate the
evaluability of interdependencies. The applicability of the approach is
demonstrated with a use case from Resin Transfer Molding (RTM), supporting the
development of machine-interpretable and verifiable engineering models.

</details>


### [18] [Geometric Learning in Black-Box Optimization: A GNN Framework for Algorithm Performance Prediction](https://arxiv.org/abs/2506.16144)
*Ana Kostovska,Carola Doerr,Sašo Džeroski,Panče Panov,Tome Eftimov*

Main category: cs.AI

TL;DR: The paper investigates using graph neural networks with heterogeneous graph structures to improve algorithm performance prediction in numerical blackbox optimization.


<details>
  <summary>Details</summary>
Motivation: Existing approaches for algorithm performance prediction rely on tabular data for problem characterizations, often missing critical algorithm configuration details and their interactions with performance outcomes.

Method: This study utilizes heterogeneous graph data structures and graph neural networks to capture complex dependencies between problems, algorithm configurations, and performance. Evaluations involve modCMA-ES and modDE frameworks applied across diverse blackbox optimization problems.

Result: Graph-based methods showed a significant performance gain, with up to 36.6% improvement in mean squared error compared to traditional tabular methods.

Conclusion: The adoption of geometric learning techniques like graph neural networks can significantly enhance performance prediction in numerical blackbox optimization.

Abstract: Automated algorithm performance prediction in numerical blackbox optimization
often relies on problem characterizations, such as exploratory landscape
analysis features. These features are typically used as inputs to machine
learning models and are represented in a tabular format. However, such
approaches often overlook algorithm configurations, a key factor influencing
performance. The relationships between algorithm operators, parameters, problem
characteristics, and performance outcomes form a complex structure best
represented as a graph. This work explores the use of heterogeneous graph data
structures and graph neural networks to predict the performance of optimization
algorithms by capturing the complex dependencies between problems, algorithm
configurations, and performance outcomes. We focus on two modular frameworks,
modCMA-ES and modDE, which decompose two widely used derivative-free
optimization algorithms: the covariance matrix adaptation evolution strategy
(CMA-ES) and differential evolution (DE). We evaluate 324 modCMA-ES and 576
modDE variants on 24 BBOB problems across six runtime budgets and two problem
dimensions. Achieving up to 36.6% improvement in MSE over traditional
tabular-based methods, this work highlights the potential of geometric learning
in black-box optimization.

</details>


### [19] [Large Language Models are Near-Optimal Decision-Makers with a Non-Human Learning Behavior](https://arxiv.org/abs/2506.16163)
*Hao Li,Gengrui Zhang,Petter Holme,Shuyue Hu,Zhen Wang*

Main category: cs.AI

TL;DR: The paper evaluates the decision-making behavior of large language models (LLMs) under uncertainty, risk, and set-shifting, comparing them with human participants, and finds they surpass humans in some areas but differ fundamentally in decision-making processes.


<details>
  <summary>Details</summary>
Motivation: To understand and evaluate the capabilities and decision-making processes of large language models (LLMs) as they increasingly play a role in real-world decisions.

Method: Using three psychological tasks designed to probe uncertainty, risk, and set-shifting, the study compares the performance of five LLMs against 360 human participants.

Result: LLMs often outperformed humans, achieving near-optimal performance in the experimental tasks, but their decision-making processes fundamentally differ from those of humans.

Conclusion: While LLMs show impressive capabilities in decision-making, their divergent processes from humans pose risks, cautioning against fully substituting human judgment with AI in decision-making contexts.

Abstract: Human decision-making belongs to the foundation of our society and
civilization, but we are on the verge of a future where much of it will be
delegated to artificial intelligence. The arrival of Large Language Models
(LLMs) has transformed the nature and scope of AI-supported decision-making;
however, the process by which they learn to make decisions, compared to humans,
remains poorly understood. In this study, we examined the decision-making
behavior of five leading LLMs across three core dimensions of real-world
decision-making: uncertainty, risk, and set-shifting. Using three
well-established experimental psychology tasks designed to probe these
dimensions, we benchmarked LLMs against 360 newly recruited human participants.
Across all tasks, LLMs often outperformed humans, approaching near-optimal
performance. Moreover, the processes underlying their decisions diverged
fundamentally from those of humans. On the one hand, our finding demonstrates
the ability of LLMs to manage uncertainty, calibrate risk, and adapt to
changes. On the other hand, this disparity highlights the risks of relying on
them as substitutes for human judgment, calling for further inquiry.

</details>


### [20] [Approximation Fixpoint Theory with Refined Approximation Spaces](https://arxiv.org/abs/2506.16294)
*Linde Vanbesien,Bart Bogaerts,Marc Denecker*

Main category: cs.AI

TL;DR: The paper extends Approximation Fixpoint Theory (AFT) by introducing more refined approximation spaces to address its limitations in certain cases.


<details>
  <summary>Details</summary>
Motivation: AFT is powerful for characterizing semantics of non-monotonic reasoning but has limitations in certain examples, prompting the need for improved methods.

Method: The authors extend consistent AFT by introducing a general notion of approximation spaces and analyze their expressiveness and relations.

Result: The refined approximation spaces improve expressiveness and address cases where AFT was previously limited.

Conclusion: The study successfully generalizes AFT to handle refined approximations, enhancing its utility and scope for non-monotonic reasoning formalisms.

Abstract: Approximation Fixpoint Theory (AFT) is a powerful theory covering various
semantics of non-monotonic reasoning formalisms in knowledge representation
such as Logic Programming and Answer Set Programming. Many semantics of such
non-monotonic formalisms can be characterized as suitable fixpoints of a
non-monotonic operator on a suitable lattice. Instead of working on the
original lattice, AFT operates on intervals in such lattice to approximate or
construct the fixpoints of interest. While AFT has been applied successfully
across a broad range of non-monotonic reasoning formalisms, it is confronted by
its limitations in other, relatively simple, examples. In this paper, we
overcome those limitations by extending consistent AFT to deal with
approximations that are more refined than intervals. Therefore, we introduce a
more general notion of approximation spaces, showcase the improved
expressiveness and investigate relations between different approximation
spaces.

</details>


### [21] [Explainable Rule Application via Structured Prompting: A Neural-Symbolic Approach](https://arxiv.org/abs/2506.16335)
*Albert Sadowski,Jarosław A. Chudziak*

Main category: cs.AI

TL;DR: The paper introduces a hybrid neural-symbolic framework for improving reasoning in Large Language Models (LLMs), achieving superior performance on structured legal tasks.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with applying rules consistently, managing exceptions, and providing explainable outputs, which are critical for domains like legal analysis.

Method: The paper proposes a structured prompting method that breaks tasks into three steps: entity identification, property extraction, and symbolic rule application. It combines LLMs' interpretive capabilities with symbolic logic for accuracy and transparency.

Result: The model outperformed baselines in the LegalBench hearsay determination task. OpenAI o1 achieved an F1 of 0.929 and o3-mini reached 0.867 using the proposed framework, surpassing few-shot baselines of 0.714 and 0.74 respectively.

Conclusion: The framework demonstrates potential for explainable AI in fields requiring structured reasoning, particularly in legal contexts, by balancing interpretive flexibility and logical rigor.

Abstract: Large Language Models (LLMs) excel in complex reasoning tasks but struggle
with consistent rule application, exception handling, and explainability,
particularly in domains like legal analysis that require both natural language
understanding and precise logical inference. This paper introduces a structured
prompting framework that decomposes reasoning into three verifiable steps:
entity identification, property extraction, and symbolic rule application. By
integrating neural and symbolic approaches, our method leverages LLMs'
interpretive flexibility while ensuring logical consistency through formal
verification. The framework externalizes task definitions, enabling domain
experts to refine logical structures without altering the architecture.
Evaluated on the LegalBench hearsay determination task, our approach
significantly outperformed baselines, with OpenAI o-family models showing
substantial improvements - o1 achieving an F1 score of 0.929 and o3-mini
reaching 0.867 using structured decomposition with complementary predicates,
compared to their few-shot baselines of 0.714 and 0.74 respectively. This
hybrid neural-symbolic system offers a promising pathway for transparent and
consistent rule-based reasoning, suggesting potential for explainable AI
applications in structured legal reasoning tasks.

</details>


### [22] [IS-Bench: Evaluating Interactive Safety of VLM-Driven Embodied Agents in Daily Household Tasks](https://arxiv.org/abs/2506.16402)
*Xiaoya Lu,Zeren Chen,Xuhao Hu,Yijin Zhou,Weichen Zhang,Dongrui Liu,Lu Sheng,Jing Shao*

Main category: cs.AI

TL;DR: This paper introduces IS-Bench, a benchmark for evaluating the interactive safety of embodied AI agents, exposing gaps in their ability to mitigate risks dynamically.


<details>
  <summary>Details</summary>
Motivation: Existing evaluation methods fail to assess interactive safety of embodied AI agents in dynamic, real-world scenarios, allowing unsafe behaviors to go unnoticed.

Method: The authors introduce IS-Bench, a multi-modal benchmark featuring 161 challenging scenarios with 388 unique risks in a high-fidelity simulator. It evaluates whether an agent performs safety mitigation steps in correct procedural order.

Result: Experiments reveal that current VLM-driven agents, including GPT-4o and Gemini-2.5, lack adequate interactive safety awareness, even with safety-aware Chain-of-Thought strategies.

Conclusion: IS-Bench exposes critical flaws in embodied AI safety, supporting the development of safer AI systems and improved risk mitigation strategies.

Abstract: Flawed planning from VLM-driven embodied agents poses significant safety
hazards, hindering their deployment in real-world household tasks. However,
existing static, non-interactive evaluation paradigms fail to adequately assess
risks within these interactive environments, since they cannot simulate dynamic
risks that emerge from an agent's actions and rely on unreliable post-hoc
evaluations that ignore unsafe intermediate steps. To bridge this critical gap,
we propose evaluating an agent's interactive safety: its ability to perceive
emergent risks and execute mitigation steps in the correct procedural order. We
thus present IS-Bench, the first multi-modal benchmark designed for interactive
safety, featuring 161 challenging scenarios with 388 unique safety risks
instantiated in a high-fidelity simulator. Crucially, it facilitates a novel
process-oriented evaluation that verifies whether risk mitigation actions are
performed before/after specific risk-prone steps. Extensive experiments on
leading VLMs, including the GPT-4o and Gemini-2.5 series, reveal that current
agents lack interactive safety awareness, and that while safety-aware
Chain-of-Thought can improve performance, it often compromises task completion.
By highlighting these critical limitations, IS-Bench provides a foundation for
developing safer and more reliable embodied AI systems.

</details>


### [23] [Agentic Personalisation of Cross-Channel Marketing Experiences](https://arxiv.org/abs/2506.16429)
*Sami Abboud,Eleanor Hanna,Olivier Jeunen,Vineesha Raheja,Schaun Wheeler*

Main category: cs.AI

TL;DR: The paper proposes a sequential decision-making framework to improve user engagement in consumer applications by optimizing communication strategies. Results show significant performance increases in user engagement for various goals across product features.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the inefficiencies in manual marketing practices, enabling better personalization in communication strategies related to content delivery, timing, frequency, and copy-writing to increase user engagement.

Method: Using a sequential decision-making framework, the authors employ a Difference-in-Differences design for estimating Individual Treatment Effect and Thompson sampling to balance exploration and exploitation.

Result: The proposed method has achieved significant increases in user engagement for various funnel events, and the system has been successfully deployed to 150 million users across multiple services.

Conclusion: The approach effectively replaces labour-intensive manual marketing with automated personalized strategies, yielding measurable improvements in engagement across diverse goals and features in consumer applications.

Abstract: Consumer applications provide ample opportunities to surface and communicate
various forms of content to users. From promotional campaigns for new features
or subscriptions, to evergreen nudges for engagement, or personalised
recommendations; across e-mails, push notifications, and in-app surfaces. The
conventional approach to orchestration for communication relies heavily on
labour-intensive manual marketer work, and inhibits effective personalisation
of content, timing, frequency, and copy-writing. We formulate this task under a
sequential decision-making framework, where we aim to optimise a modular
decision-making policy that maximises incremental engagement for any funnel
event. Our approach leverages a Difference-in-Differences design for Individual
Treatment Effect estimation, and Thompson sampling to balance the
explore-exploit trade-off. We present results from a multi-service application,
where our methodology has resulted in significant increases to a variety of
goal events across several product features, and is currently deployed across
150 million users.

</details>


### [24] [ML-Master: Towards AI-for-AI via Integration of Exploration and Reasoning](https://arxiv.org/abs/2506.16499)
*Zexi Liu,Yuzhu Cai,Xinyu Zhu,Yujie Zheng,Runkun Chen,Ying Wen,Yanfeng Wang,Weinan E,Siheng Chen*

Main category: cs.AI

TL;DR: The paper introduces ML-Master, an AI system designed to improve the efficiency and performance of AI development through a memory-based reasoning mechanism, outperforming existing approaches in testing scenarios.


<details>
  <summary>Details</summary>
Motivation: To overcome inefficiencies and suboptimal performance in existing AI systems that fail to fully leverage accumulated experience during solution exploration in AI-for-AI contexts.

Method: The authors propose ML-Master, an AI4AI agent employing a selectively scoped memory mechanism to integrate exploration and reasoning. This technique allows for combining insights from multiple solution methods without overwhelming the system with too much context.

Result: ML-Master achieved a 29.3% average medal rate on the MLE-Bench test. It outperformed existing methods, especially in medium-complexity tasks, while operating under tighter time restrictions compared to prior baselines.

Conclusion: ML-Master demonstrates significant potential for advancing AI-for-AI methodologies by efficiently streamlining the design, training, and deployment of AI systems.

Abstract: As AI capabilities advance toward and potentially beyond human-level
performance, a natural transition emerges where AI-driven development becomes
more efficient than human-centric approaches. A promising pathway toward this
transition lies in AI-for-AI (AI4AI), which leverages AI techniques to automate
and optimize the design, training, and deployment of AI systems themselves.
While LLM-based agents have shown the potential to realize AI4AI, they are
often unable to fully leverage the experience accumulated by agents during the
exploration of solutions in the reasoning process, leading to inefficiencies
and suboptimal performance. To address this limitation, we propose ML-Master, a
novel AI4AI agent that seamlessly integrates exploration and reasoning by
employing a selectively scoped memory mechanism. This approach allows ML-Master
to efficiently combine diverse insights from parallel solution trajectories
with analytical reasoning, guiding further exploration without overwhelming the
agent with excessive context. We evaluate ML-Master on the MLE-Bench, where it
achieves a 29.3% average medal rate, significantly surpassing existing methods,
particularly in medium-complexity tasks, while accomplishing this superior
performance within a strict 12-hour time constraint-half the 24-hour limit used
by previous baselines. These results demonstrate ML-Master's potential as a
powerful tool for advancing AI4AI.

</details>


### [25] [Advancing Harmful Content Detection in Organizational Research: Integrating Large Language Models with Elo Rating System](https://arxiv.org/abs/2506.16575)
*Mustafa Akben,Aaron Satko*

Main category: cs.AI

TL;DR: This paper introduces an Elo rating-based method to improve the analysis of harmful content using large language models (LLMs), outperforming existing techniques in accuracy and scalability.


<details>
  <summary>Details</summary>
Motivation: The study addresses the challenges posed by LLM moderation systems in harmful content analysis, particularly for sensitive topics like microaggressions and hate speech, which can undermine research validity.

Method: An Elo rating-based approach was implemented for analyzing harmful content, tested on datasets related to microaggressions and hate speech, and compared to traditional LLM prompting and machine learning models.

Result: The proposed method showed superior performance in key metrics like accuracy, precision, F1 score, reliability, reduced false positives, and scalability for large datasets.

Conclusion: The Elo rating-based method enhances the capabilities of LLMs for harmful content analysis, with broad applications in detecting workplace harassment and promoting safer work environments.

Abstract: Large language models (LLMs) offer promising opportunities for organizational
research. However, their built-in moderation systems can create problems when
researchers try to analyze harmful content, often refusing to follow certain
instructions or producing overly cautious responses that undermine validity of
the results. This is particularly problematic when analyzing organizational
conflicts such as microaggressions or hate speech. This paper introduces an Elo
rating-based method that significantly improves LLM performance for harmful
content analysis In two datasets, one focused on microaggression detection and
the other on hate speech, we find that our method outperforms traditional LLM
prompting techniques and conventional machine learning models on key measures
such as accuracy, precision, and F1 scores. Advantages include better
reliability when analyzing harmful content, fewer false positives, and greater
scalability for large-scale datasets. This approach supports organizational
applications, including detecting workplace harassment, assessing toxic
communication, and fostering safer and more inclusive work environments.

</details>


### [26] [A Community-driven vision for a new Knowledge Resource for AI](https://arxiv.org/abs/2506.16596)
*Vinay K Chaudhri,Chaitan Baru,Brandon Bennett,Mehul Bhatt,Darion Cassel,Anthony G Cohn,Rina Dechter,Esra Erdem,Dave Ferrucci,Ken Forbus,Gregory Gelfond,Michael Genesereth,Andrew S. Gordon,Benjamin Grosof,Gopal Gupta,Jim Hendler,Sharat Israni,Tyler R. Josephson,Patrick Kyllonen,Yuliya Lierler,Vladimir Lifschitz,Clifton McFate,Hande K. McGinty,Leora Morgenstern,Alessandro Oltramari,Praveen Paritosh,Dan Roth,Blake Shepard,Cogan Shimzu,Denny Vrandečić,Mark Whiting,Michael Witbrock*

Main category: cs.AI

TL;DR: The paper explores the need for a modern, comprehensive, and accessible knowledge infrastructure addressing gaps in AI's capabilities and synthesizing findings from a workshop of over 50 researchers.


<details>
  <summary>Details</summary>
Motivation: To address persistent deficiencies in AI's access to verifiable and broad-usage knowledge resources, which hinder capabilities in areas like language models, robotic planning, and false information detection.

Method: The authors synthesize findings from a workshop of over 50 experts and propose leveraging modern developments, such as knowledge representation, reasoning, and an open engineering framework for integrating knowledge modules.

Result: The paper identifies the need for a structured framework and emphasizes the importance of community collaboration in adopting conventions, social structures, and technologies to build a robust knowledge infrastructure.

Conclusion: Collaborative efforts leveraging modern AI advancements can address critical knowledge gaps in AI, driving the development of a new knowledge infrastructure with practical applications.

Abstract: The long-standing goal of creating a comprehensive, multi-purpose knowledge
resource, reminiscent of the 1984 Cyc project, still persists in AI. Despite
the success of knowledge resources like WordNet, ConceptNet, Wolfram|Alpha and
other commercial knowledge graphs, verifiable, general-purpose widely available
sources of knowledge remain a critical deficiency in AI infrastructure. Large
language models struggle due to knowledge gaps; robotic planning lacks
necessary world knowledge; and the detection of factually false information
relies heavily on human expertise. What kind of knowledge resource is most
needed in AI today? How can modern technology shape its development and
evaluation? A recent AAAI workshop gathered over 50 researchers to explore
these questions. This paper synthesizes our findings and outlines a
community-driven vision for a new knowledge infrastructure. In addition to
leveraging contemporary advances in knowledge representation and reasoning, one
promising idea is to build an open engineering framework to exploit knowledge
modules effectively within the context of practical applications. Such a
framework should include sets of conventions and social structures that are
adopted by contributors.

</details>


### [27] [The Role of Explanation Styles and Perceived Accuracy on Decision Making in Predictive Process Monitoring](https://arxiv.org/abs/2506.16617)
*Soobin Chae,Suhwan Lee,Hanna Hauptmann,Hajo A. Reijers,Xixi Lu*

Main category: cs.AI

TL;DR: This paper explores the impact of explainability styles and perceived AI accuracy on decision-making in Predictive Process Monitoring (PPM).


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the lack of interpretability in high-accuracy deep learning models used in PPM, which undermines user trust.

Method: A decision-making experiment was conducted using various explanation styles (feature importance, rule-based, counterfactual) and perceived AI accuracy levels to measure their effects on performance and confidence.

Result: The study found that both perceived AI accuracy and the style of explanations significantly influence decision-making metrics like Task Performance, Agreement, and Decision Confidence.

Conclusion: The inclusion of tailored explanation styles and accurate perception can enhance trust and effectiveness of AI systems in PPM applications.

Abstract: Predictive Process Monitoring (PPM) often uses deep learning models to
predict the future behavior of ongoing processes, such as predicting process
outcomes. While these models achieve high accuracy, their lack of
interpretability undermines user trust and adoption. Explainable AI (XAI) aims
to address this challenge by providing the reasoning behind the predictions.
However, current evaluations of XAI in PPM focus primarily on functional
metrics (such as fidelity), overlooking user-centered aspects such as their
effect on task performance and decision-making. This study investigates the
effects of explanation styles (feature importance, rule-based, and
counterfactual) and perceived AI accuracy (low or high) on decision-making in
PPM. We conducted a decision-making experiment, where users were presented with
the AI predictions, perceived accuracy levels, and explanations of different
styles. Users' decisions were measured both before and after receiving
explanations, allowing the assessment of objective metrics (Task Performance
and Agreement) and subjective metrics (Decision Confidence). Our findings show
that perceived accuracy and explanation style have a significant effect.

</details>


### [28] [Interpretable Low-Dimensional Modeling of Spatiotemporal Agent States for Decision Making in Football Tactics](https://arxiv.org/abs/2506.16696)
*Kenjiro Ide,Taiga Someya,Kohei Kawaguchi,Keisuke Fujii*

Main category: cs.AI

TL;DR: The paper proposes low-dimensional, interpretable models using spatiotemporal data to analyze football tactics, focusing on pass success prediction.


<details>
  <summary>Details</summary>
Motivation: Current models analyzing football tactics are computationally intense or lack interpretability, necessitating a simplified yet effective approach.

Method: Define interpretable state variables for players using game criteria and train an XGBoost model on StatsBomb event and SkillCorner tracking data.

Result: Distance to the ball and player's space score are identified as key factors in predicting successful passes, showcasing the model's interpretability.

Conclusion: The study offers a practical, interpretable tool for tactical football analysis, aiding decision-making and enhancing understanding of key factors influencing passes.

Abstract: Understanding football tactics is crucial for managers and analysts. Previous
research has proposed models based on spatial and kinematic equations, but
these are computationally expensive. Also, Reinforcement learning approaches
use player positions and velocities but lack interpretability and require large
datasets. Rule-based models align with expert knowledge but have not fully
considered all players' states. This study explores whether low-dimensional,
rule-based models using spatiotemporal data can effectively capture football
tactics. Our approach defines interpretable state variables for both the
ball-holder and potential pass receivers, based on criteria that explore
options like passing. Through discussions with a manager, we identified key
variables representing the game state. We then used StatsBomb event data and
SkillCorner tracking data from the 2023$/$24 LaLiga season to train an XGBoost
model to predict pass success. The analysis revealed that the distance between
the player and the ball, as well as the player's space score, were key factors
in determining successful passes. Our interpretable low-dimensional modeling
facilitates tactical analysis through the use of intuitive variables and
provides practical value as a tool to support decision-making in football.

</details>


### [29] [Reinforcement learning for hybrid charging stations planning and operation considering fixed and mobile chargers](https://arxiv.org/abs/2506.16764)
*Yanchen Zhu,Honghui Zou,Chufan Liu,Yuyu Luo,Yuankai Wu,Yuxuan Liang*

Main category: cs.AI

TL;DR: The paper proposes a hybrid approach to optimize charging infrastructure by combining both fixed and mobile chargers to reduce underutilization and congestion.


<details>
  <summary>Details</summary>
Motivation: Traditional fixed charging stations face inefficiencies due to fluctuating demand, motivating the need for a flexible solution like mobile chargers.

Method: The proposed Hybrid Charging Station Planning and Operation (HCSPO) problem uses demand prediction via Model Predictive Control (MPC) and solves it with deep reinforcement learning combined with heuristic scheduling.

Result: Case studies in real-world urban environments show improved charging availability and reduced user inconvenience compared to existing solutions.

Conclusion: Integrating fixed and mobile charging solutions with advanced optimization techniques can significantly enhance urban charging infrastructure performance.

Abstract: The success of vehicle electrification, which brings significant societal and
environmental benefits, is contingent upon the availability of efficient and
adaptable charging infrastructure. Traditional fixed-location charging stations
often face issues like underutilization or congestion due to the dynamic nature
of charging demand. Mobile chargers have emerged as a flexible solution,
capable of relocating to align with these demand fluctuations. This paper
addresses the optimal planning and operation of hybrid charging
infrastructures, integrating both fixed and mobile chargers within urban road
networks. We introduce the Hybrid Charging Station Planning and Operation
(HCSPO) problem, which simultaneously optimizes the location and configuration
of fixed charging stations and schedules mobile chargers for dynamic
operations. Our approach incorporates a charging demand prediction model
grounded in Model Predictive Control (MPC) to enhance decision-making. To solve
the HCSPO problem, we propose a deep reinforcement learning method, augmented
with heuristic scheduling techniques, to effectively bridge the planning of
fixed chargers with the real-time operation of mobile chargers. Extensive case
studies using real-world urban scenarios demonstrate that our method
significantly improves the availability of charging infrastructure and reduces
user inconvenience compared to existing solutions and baselines.

</details>


### [30] [AI's Blind Spots: Geographic Knowledge and Diversity Deficit in Generated Urban Scenario](https://arxiv.org/abs/2506.16898)
*Ciro Beneduce,Massimiliano Luca,Bruno Lepri*

Main category: cs.AI

TL;DR: The study assesses biases in image generation models, focusing on geographic knowledge and representative biases, revealing favoring of metropolis-like areas and challenges with European-sounding names.


<details>
  <summary>Details</summary>
Motivation: To explore geographic knowledge and biases embedded in image generation models relevant to urban analysis and design.

Method: Generated 150 synthetic images per US state and capitals using FLUX 1 and Stable Diffusion 3.5; analyzed with DINO-v2 ViT-S/14 and Fréchet Inception Distances.

Result: Found geographic biases and representation favoring metropolitan areas, excluding rural regions and smaller cities; entity-disambiguation issues with certain names were also observed.

Conclusion: Image generation models exhibit learned geographic knowledge, but are biased toward metropolitan areas, raising concerns for equitable representation in urban-related applications.

Abstract: Image generation models are revolutionizing many domains, and urban analysis
and design is no exception. While such models are widely adopted, there is a
limited literature exploring their geographic knowledge, along with the biases
they embed. In this work, we generated 150 synthetic images for each state in
the USA and related capitals using FLUX 1 and Stable Diffusion 3.5, two
state-of-the-art models for image generation. We embed each image using DINO-v2
ViT-S/14 and the Fr\'echet Inception Distances to measure the similarity
between the generated images. We found that while these models have implicitly
learned aspects of USA geography, if we prompt the models to generate an image
for "United States" instead of specific cities or states, the models exhibit a
strong representative bias toward metropolis-like areas, excluding rural states
and smaller cities. {\color{black} In addition, we found that models
systematically exhibit some entity-disambiguation issues with European-sounding
names like Frankfort or Devon.

</details>


### [31] [Real-Time Black-Box Optimization for Dynamic Discrete Environments Using Embedded Ising Machines](https://arxiv.org/abs/2506.16924)
*Tomoya Kashimata,Yohei Hamakawa,Masaya Yamasaki,Kosuke Tatsumura*

Main category: cs.AI

TL;DR: The paper introduces a heuristic multi-armed bandit (MAB) method that leverages an Ising machine to optimize dynamic, discrete environments with real-time interactions, aiming to address limitations of conventional methods in such scenarios.


<details>
  <summary>Details</summary>
Motivation: Conventional MAB algorithms struggle to optimize dynamic, discrete environments in real-time systems due to the vast combinatorial possibilities of discrete optimization.

Method: The proposed method extends black-box optimization using an Ising machine. It effectively explores actions by considering variable interactions and environmental dynamics.

Result: The method demonstrated adaptability in a dynamic wireless communication system with moving users, showcasing its effectiveness for real-time applications.

Conclusion: This heuristic approach enhances optimization in dynamic, discrete settings, establishing its utility in challenging environments like wireless communication systems.

Abstract: Many real-time systems require the optimization of discrete variables.
Black-box optimization (BBO) algorithms and multi-armed bandit (MAB) algorithms
perform optimization by repeatedly taking actions and observing the
corresponding instant rewards without any prior knowledge. Recently, a BBO
method using an Ising machine has been proposed to find the best action that is
represented by a combination of discrete values and maximizes the instant
reward in static environments. In contrast, dynamic environments, where
real-time systems operate, necessitate MAB algorithms that maximize the average
reward over multiple trials. However, due to the enormous number of actions
resulting from the combinatorial nature of discrete optimization, conventional
MAB algorithms cannot effectively optimize dynamic, discrete environments.
Here, we show a heuristic MAB method for dynamic, discrete environments by
extending the BBO method, in which an Ising machine effectively explores the
actions while considering interactions between variables and changes in dynamic
environments. We demonstrate the dynamic adaptability of the proposed method in
a wireless communication system with moving users.

</details>


### [32] [Multimodal Fused Learning for Solving the Generalized Traveling Salesman Problem in Robotic Task Planning](https://arxiv.org/abs/2506.16931)
*Jiaqi Chen,Mingfeng Fan,Xuefeng Zhang,Jingsong Liang,Yuhong Cao,Guohua Wu,Guillaume Adrien Sartoretti*

Main category: cs.AI

TL;DR: This paper introduces the Multimodal Fused Learning (MMFL) framework for solving the Generalized Traveling Salesman Problem (GTSP) in real-time using mobile robots.


<details>
  <summary>Details</summary>
Motivation: To develop an effective and efficient task planning framework for mobile robots in scenarios (e.g., warehouse tasks, environmental monitoring) requiring real-time solutions for GTSP.

Method: The MMFL framework combines graph and image-based representations, using a coordinate-based image builder, adaptive resolution scaling, and a multimodal fusion module to generate task plans.

Result: MMFL achieved superior performance over existing state-of-the-art methods in accuracy and computational efficiency for GTSP instances, including validation through physical robot tests.

Conclusion: MMFL presents a robust, scalable approach to task planning for mobile robots, making it suitable for real-world robotic applications.

Abstract: Effective and efficient task planning is essential for mobile robots,
especially in applications like warehouse retrieval and environmental
monitoring. These tasks often involve selecting one location from each of
several target clusters, forming a Generalized Traveling Salesman Problem
(GTSP) that remains challenging to solve both accurately and efficiently. To
address this, we propose a Multimodal Fused Learning (MMFL) framework that
leverages both graph and image-based representations to capture complementary
aspects of the problem, and learns a policy capable of generating high-quality
task planning schemes in real time. Specifically, we first introduce a
coordinate-based image builder that transforms GTSP instances into spatially
informative representations. We then design an adaptive resolution scaling
strategy to enhance adaptability across different problem scales, and develop a
multimodal fusion module with dedicated bottlenecks that enables effective
integration of geometric and spatial features. Extensive experiments show that
our MMFL approach significantly outperforms state-of-the-art methods across
various GTSP instances while maintaining the computational efficiency required
for real-time robotic applications. Physical robot tests further validate its
practical effectiveness in real-world scenarios.

</details>


### [33] [Elevating Styled Mahjong Agents with Learning from Demonstration](https://arxiv.org/abs/2506.16995)
*Lingfeng Li,Yunlong Lu,Yongyi Wang,Wenxin Li*

Main category: cs.AI

TL;DR: This paper proposes a novel learning-from-demonstration (LfD) algorithm for Mahjong bots, improving their gameplay proficiency while maintaining distinctive play styles.


<details>
  <summary>Details</summary>
Motivation: Traditional game bots are proficient but lack diverse and distinct play styles, especially in environments like Mahjong that present high randomness and out-of-distribution state challenges.

Method: The authors adapt the Proximal Policy Optimization (PPO) algorithm minimally and leverage existing Mahjong agent gameplay histories to develop a novel learning-from-demonstration (LfD) methodology.

Result: Empirical results demonstrate that the proposed method improves bots' competency and preserves their unique styles effectively.

Conclusion: The paper presents an approach that addresses the dual challenge of proficiency and diversity in bot behavior, showcasing advancements in creating versatile gaming AI.

Abstract: A wide variety of bots in games enriches the gameplay experience and enhances
replayability. Recent advancements in game artificial intelligence have
predominantly focused on improving the proficiency of bots. Nevertheless,
developing highly competent bots with a wide range of distinct play styles
remains a relatively under-explored area. We select the Mahjong game
environment as a case study. The high degree of randomness inherent in the
Mahjong game and the prevalence of out-of-distribution states lead to
suboptimal performance of existing offline learning and
Learning-from-Demonstration (LfD) algorithms. In this paper, we leverage the
gameplay histories of existing Mahjong agents and put forward a novel LfD
algorithm that necessitates only minimal modifications to the Proximal Policy
Optimization algorithm. The comprehensive empirical results illustrate that our
proposed method not only significantly enhances the proficiency of the agents
but also effectively preserves their unique play styles.

</details>


### [34] [A Quantile Regression Approach for Remaining Useful Life Estimation with State Space Models](https://arxiv.org/abs/2506.17018)
*Davide Frizzo,Francesco Borsatti,Gian Antonio Susto*

Main category: cs.AI

TL;DR: This paper proposes using State Space Models (SSM) combined with Simoultaneous Quantile Regression (SQR) for predicting equipment Remaining Useful Life (RUL), showcasing more accurate and computationally efficient results than traditional methods.


<details>
  <summary>Details</summary>
Motivation: Enhance predictive maintenance capabilities in Industry 4.0 and 5.0 by improving the accuracy and efficiency of Remaining Useful Life (RUL) prediction methods.

Method: Utilize State Space Models (SSM) integrated with Simoultaneous Quantile Regression (SQR), and benchmark against established sequence modeling methods using the C-MAPSS dataset.

Result: SSM models demonstrate superior accuracy and computational efficiency compared to traditional approaches like LSTM, Transformer, and Informer.

Conclusion: State Space Models combined with SQR are highly effective tools for predictive maintenance, holding significant promise for industrial applications due to their performance advantages.

Abstract: Predictive Maintenance (PdM) is pivotal in Industry 4.0 and 5.0, proactively
enhancing efficiency through accurate equipment Remaining Useful Life (RUL)
prediction, thus optimizing maintenance scheduling and reducing unexpected
failures and premature interventions. This paper introduces a novel RUL
estimation approach leveraging State Space Models (SSM) for efficient long-term
sequence modeling. To handle model uncertainty, Simoultaneous Quantile
Regression (SQR) is integrated into the SSM, enabling multiple quantile
estimations. The proposed method is benchmarked against traditional sequence
modelling techniques (LSTM, Transformer, Informer) using the C-MAPSS dataset.
Results demonstrate superior accuracy and computational efficiency of SSM
models, underscoring their potential for high-stakes industrial applications.

</details>


### [35] [Dispositions and Roles of Generically Dependent Entities](https://arxiv.org/abs/2506.17085)
*Fabian Neuhaus*

Main category: cs.AI

TL;DR: The paper critiques BFO 2020 for its inability to represent functions, dispositions, and roles of generically dependent continuants like software and datasets, and proposes solutions to address this limitation.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of BFO 2020 in representing the functions and roles of generically dependent continuants crucial for computer models and datasets.

Method: The authors analyze BFO 2020's constraints and propose two approaches: (a) using defined classes and (b) modifying BFO to support such functionalities.

Result: The discussion highlights specific limitations in BFO 2020 and presents actionable solutions to improve its capability to represent generically dependent continuants.

Conclusion: BFO 2020 requires modification or supplemental methods to effectively support the representation of functions and roles of generically dependent continuants, enhancing its applicability in relevant domains.

Abstract: BFO 2020 does not support functions, dispositions, and roles of generically
dependent continuants (like software or datasets). In this paper, we argue that
this is a severe limitation, which prevents, for example, the adequate
representation of the functions of computer models or the various roles of
datasets during the execution of these models. We discuss the aspects of BFO
2020 that prevent the representation of realizable entities of generically
dependent continuants. Two approaches to address the issue are presented: (a)
the use of defined classes and (b) a proposal of changes that allow BFO to
support functions, dispositions, and roles of generically dependent
continuants.

</details>


### [36] [Towards Advanced Mathematical Reasoning for LLMs via First-Order Logic Theorem Proving](https://arxiv.org/abs/2506.17104)
*Chuxue Cao,Mengze Li,Juntao Dai,Jinluan Yang,Zijian Zhao,Shengyu Zhang,Weijie Shi,Chengzhong Liu,Sirui Han,Yike Guo*

Main category: cs.AI

TL;DR: The paper introduces DREAM, a method to improve large language models' (LLMs) performance in multi-step first-order logic reasoning tasks. It includes strategy diversification and error feedback mechanisms, achieving up to 6.4% improvement.


<details>
  <summary>Details</summary>
Motivation: The study seeks to address LLMs' difficulties in multi-step mathematical reasoning and theorem proving where early errors and lack of strategic diversity undermine accuracy.

Method: DREAM leverages an Axiom-Driven Strategy Diversification to explore varied proof strategies and a Sub-Proposition Error Feedback mechanism to identify and correct mistakes during inference.

Result: The proposed DREAM framework improves LLM performance on theorem proving tasks by 0.6% to 6.4%, and introduces evaluation with a 447-theorem dataset in Lean 4 format.

Conclusion: DREAM enhances the diversity and reliability of LLMs in complex mathematical reasoning, offering a self-correcting inference mechanism and a meaningful dataset for broader evaluation.

Abstract: Large language models (LLMs) have shown promising first-order logic (FOL)
reasoning capabilities with applications in various areas. However, their
effectiveness in complex mathematical reasoning involving multi-step FOL
deductions is still under-researched. While LLMs perform competitively on
established mathematical reasoning benchmarks, they struggle with multi-step
FOL tasks, as demonstrated by Deepseek-Prover-V2-7B's low accuracy (4.2%) on
our proposed theorem proving dataset. This issue arises from the limited
exploration of diverse proof strategies and the potential for early reasoning
mistakes to undermine entire proofs. To address these issues, we propose DREAM,
a self-adaptive solution that enhances the Diversity and REAsonability of LLMs'
generation strategies. DREAM incorporates an Axiom-Driven Strategy
Diversification mechanism to promote varied strategic outcomes and a
Sub-Proposition Error Feedback to help LLMs reflect on and correct their
proofs. Our contributions include pioneering advancements in LLMs' mathematical
reasoning through FOL theorem proving, introducing a novel inference stage
solution that improves performance by 0.6% to 6.4%, and providing a curated
dataset of 447 mathematical theorems in Lean 4 format for evaluation.

</details>


### [37] [Are Bias Evaluation Methods Biased ?](https://arxiv.org/abs/2506.17111)
*Lina Berrayana,Sean Rooney,Luis Garcés-Erice,Ioana Giurgiu*

Main category: cs.AI

TL;DR: This paper assesses the consistency of bias evaluation methods used in safety benchmarks for Large Language Models, finding significant variations in model rankings across different methods.


<details>
  <summary>Details</summary>
Motivation: To ensure the robustness and reliability of safety benchmarks for Large Language Models, specifically focusing on evaluating model bias.

Method: The authors compare rankings of representative models produced by different bias evaluation methods and analyze the consistency of the rankings.

Result: The study finds that distinct bias evaluation methods result in significantly different model rankings.

Conclusion: The authors recommend improvements and caution in the use of bias evaluation benchmarks to ensure fairer comparisons of model safety.

Abstract: The creation of benchmarks to evaluate the safety of Large Language Models is
one of the key activities within the trusted AI community. These benchmarks
allow models to be compared for different aspects of safety such as toxicity,
bias, harmful behavior etc. Independent benchmarks adopt different approaches
with distinct data sets and evaluation methods. We investigate how robust such
benchmarks are by using different approaches to rank a set of representative
models for bias and compare how similar are the overall rankings. We show that
different but widely used bias evaluations methods result in disparate model
rankings. We conclude with recommendations for the community in the usage of
such benchmarks.

</details>


### [38] [Mathematical Proof as a Litmus Test: Revealing Failure Modes of Advanced Large Reasoning Models](https://arxiv.org/abs/2506.17114)
*Dadi Guo,Jiayu Liu,Zhiyuan Fan,Zhitao He,Haoran Li,Yumeng Wang,Yi R.,Fung*

Main category: cs.AI

TL;DR: Large reasoning models demonstrate poor performance on generating rigorous mathematical proofs, highlighting diverse reasoning failures.


<details>
  <summary>Details</summary>
Motivation: Advanced reasoning models show high accuracy numerically but lack depth in understanding logical rigor and methodology.

Method: Introduce the RFMDataset, containing 200 diverse proof problems, and analyze models' performance to uncover error types.

Result: Models produced correct proofs for less than 20% of problems and showcased issues like hallucination and reasoning incompleteness.

Conclusion: Current large reasoning models require fine-grained logical training to overcome fundamental reasoning limitations in rigorous mathematical proofs.

Abstract: Large reasoning models (e.g., R1, o3) have demonstrated remarkable
mathematical problem-solving abilities. However, the high reported accuracy of
these advanced models on popular datasets, reliance on purely numerical
evaluation and potential benchmark leakage, often masks their true reasoning
shortcomings. To address this, we propose leveraging the inherent rigor and
methodological complexity of mathematical proofs as a diagnostic tool to expose
these hidden failures. Specifically, we introduce the RFMDataset (Reveal
Failure Modes), a collection of 200 diverse mathematical proof problems, and
thoroughly evaluate advanced models' performance on it. Our in-depth analysis
of their failures uncovers 10 fine-grained error types, which shows fundamental
limitations in current large reasoning models: 1) large reasoning models
grapple profoundly with mathematical proofs, with some generating entirely
correct proofs for less than 20% of problems and failing even on basic ones; 2)
models exhibit a diverse spectrum of reasoning failures, prominently
demonstrating the lack of guarantees for the correctness and rigor of
single-step reasoning; and 3) models show hallucination and incompleteness
during the reasoning process. Our findings reveal that models' self-reflection
is insufficient to resolve the current logical dilemmas, necessitating
formalized and fine-grained logical training.

</details>


### [39] [When Can Model-Free Reinforcement Learning be Enough for Thinking?](https://arxiv.org/abs/2506.17124)
*Josiah P. Hanna,Nicholas E. Corrado*

Main category: cs.AI

TL;DR: The paper investigates the emergence of 'thinking' in large language models via model-free reinforcement learning (RL), formalizes it through a 'thought Markov decision process' (MDP), and validates the conditions necessary for such behavior.


<details>
  <summary>Details</summary>
Motivation: To understand when and why 'thinking' emerges as a strategy in model-free RL, even though it doesn't directly maximize reward or change the external environment.

Method: The authors introduce the 'thought MDP' theoretical framework to analyze thinking in RL, prove key properties like the role of policy initialization, and validate the theory using open-source LLMs and a toy domain.

Result: The study finds that policy initialization plays a crucial role in enabling thinking-like behavior. It also demonstrates that open-source LLMs align with the theoretical preconditions necessary for model-free RL to generate thinking, and presents a toy domain where thinking improves data efficiency.

Conclusion: The paper provides a framework to identify conditions under which thinking can emerge in RL agents, showing its practical and theoretical utility across language models and other domains.

Abstract: Recent work on large language models has demonstrated the use of model-free
reinforcement learning (RL) to train reasoning-like capabilities. The emergence
of "thinking" through model-free RL is interesting as thinking actions neither
produce reward nor change the external world state to one where the agent is
more likely to get reward. This paper seeks to build a domain-independent
understanding of when model-free RL will lead to "thinking" as a strategy for
reward maximization. To build this understanding, we first introduce a
theoretical model which we call a \textit{thought Markov decision process}
(MDP). Thought MDPs minimally extend the classical MDP model to include an
abstract notion of thought state and thought action. Using the thought MDP
model, we prove the importance of policy initialization in determining whether
or not thinking emerges and show formally that thought actions are equivalent
to the agent choosing to perform a step of policy improvement before continuing
to act. We then show that open-source LLMs satisfy the conditions that our
theory predicts are necessary for model-free RL to produce thinking-like
behavior. Finally, we hypothesize sufficient conditions that would enable
thinking to be learned outside of language generation and introduce a toy
domain where a combination of multi-task pre-training and designated thought
actions enable more data-efficient RL compared to non-thinking agents.

</details>


### [40] [Chain-of-Trust: A Progressive Trust Evaluation Framework Enabled by Generative AI](https://arxiv.org/abs/2506.17130)
*Botao Zhu,Xianbin Wang,Lei Zhang,Xuemin,Shen*

Main category: cs.AI

TL;DR: The paper proposes a progressive trust evaluation framework called 'chain-of-trust,' which segments the trust evaluation process into sequential stages, using generative AI for accurate and efficient assessments.


<details>
  <summary>Details</summary>
Motivation: The research aims to address the challenges of trust evaluation in collaborative systems with dynamic networks and decentralized resources, where it is difficult to reliably observe all trust attributes simultaneously.

Method: The framework divides trust evaluation into chained stages aligned with task decomposition. At each stage, only the most relevant device attribute data is examined, reducing complexity. Generative AI is then applied to process data using in-context and few-shot learning for quick and accurate trust decisions.

Result: The experimental results show that the framework achieves high accuracy in evaluating the trustworthiness of devices in collaborative systems.

Conclusion: The proposed chain-of-trust framework provides a structured, efficient, and effective approach for progressive trust evaluation, ensuring that only trustworthy devices are utilized throughout all stages of task completion.

Abstract: In collaborative systems with complex tasks relying on distributed resources,
trust evaluation of potential collaborators has emerged as an effective
mechanism for task completion. However, due to the network dynamics and varying
information gathering latencies, it is extremely challenging to observe and
collect all trust attributes of a collaborating device concurrently for a
comprehensive trust assessment. In this paper, a novel progressive trust
evaluation framework, namely chain-of-trust, is proposed to make better use of
misaligned device attribute data. This framework, designed for effective task
completion, divides the trust evaluation process into multiple chained stages
based on task decomposition. At each stage, based on the task completion
process, the framework only gathers the latest device attribute data relevant
to that stage, leading to reduced trust evaluation complexity and overhead. By
leveraging advanced in-context learning, few-shot learning, and reasoning
capabilities, generative AI is then employed to analyze and interpret the
collected data to produce correct evaluation results quickly. Only devices
deemed trustworthy at this stage proceed to the next round of trust evaluation.
The framework ultimately determines devices that remain trustworthy across all
stages. Experimental results demonstrate that the proposed framework achieves
high accuracy in trust evaluation.

</details>


### [41] [The MedPerturb Dataset: What Non-Content Perturbations Reveal About Human and Clinical LLM Decision Making](https://arxiv.org/abs/2506.17163)
*Abinitha Gourabathina,Yuexing Hao,Walter Gerych,Marzyeh Ghassemi*

Main category: cs.AI

TL;DR: This paper introduces MedPerturb, a dataset for assessing medical LLMs' robustness to variability in clinical input.


<details>
  <summary>Details</summary>
Motivation: The study aims to explore disparities in how medical LLMs and human experts respond to real-world variability in clinical settings.

Method: MedPerturb was developed to include clinical vignettes modified along three axes: gender, style, and format. These vignettes were tested with outputs from four LLMs and assessed by human experts.

Result: LLMs were found to be more sensitive to gender and style changes, whereas human experts responded more to format alterations, like LLM-generated summaries.

Conclusion: Traditional static benchmarks are inadequate. Comprehensive frameworks are crucial to evaluate medical LLMs' decisions against human clinician variability in clinical situations.

Abstract: Clinical robustness is critical to the safe deployment of medical Large
Language Models (LLMs), but key questions remain about how LLMs and humans may
differ in response to the real-world variability typified by clinical settings.
To address this, we introduce MedPerturb, a dataset designed to systematically
evaluate medical LLMs under controlled perturbations of clinical input.
MedPerturb consists of clinical vignettes spanning a range of pathologies, each
transformed along three axes: (1) gender modifications (e.g., gender-swapping
or gender-removal); (2) style variation (e.g., uncertain phrasing or colloquial
tone); and (3) format changes (e.g., LLM-generated multi-turn conversations or
summaries). With MedPerturb, we release a dataset of 800 clinical contexts
grounded in realistic input variability, outputs from four LLMs, and three
human expert reads per clinical context. We use MedPerturb in two case studies
to reveal how shifts in gender identity cues, language style, or format reflect
diverging treatment selections between humans and LLMs. We find that LLMs are
more sensitive to gender and style perturbations while human annotators are
more sensitive to LLM-generated format perturbations such as clinical
summaries. Our results highlight the need for evaluation frameworks that go
beyond static benchmarks to assess the similarity between human clinician and
LLM decisions under the variability characteristic of clinical settings.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [42] [DeepRTL2: A Versatile Model for RTL-Related Tasks](https://arxiv.org/abs/2506.15697)
*Yi Liu,Hongji Zhang,Yunhao Zhou,Zhengyuan Shi,Changran Xu,Qiang Xu*

Main category: cs.AR

TL;DR: DeepRTL2 integrates large language models into electronic design automation to address both generation-based and embedding-based tasks in register transfer level (RTL) workflows.


<details>
  <summary>Details</summary>
Motivation: Embedding-based tasks, crucial for EDA workflows, have been overlooked, despite significant advancements in RTL code generation via fine-tuned large language models.

Method: Development of DeepRTL2, a versatile family of large language models capable of addressing both generation- and embedding-based tasks in RTL-related workflows.

Result: Extensive experiments demonstrate that DeepRTL2 achieves state-of-the-art performance across diverse RTL-related tasks.

Conclusion: DeepRTL2 provides a comprehensive solution for challenges in EDA, unifying tasks across generation and embedding contexts to optimize hardware design processes.

Abstract: The integration of large language models (LLMs) into electronic design
automation (EDA) has significantly advanced the field, offering transformative
benefits, particularly in register transfer level (RTL) code generation and
understanding. While previous studies have demonstrated the efficacy of
fine-tuning LLMs for these generation-based tasks, embedding-based tasks, which
are equally critical to EDA workflows, have been largely overlooked. These
tasks, including natural language code search, RTL code functionality
equivalence checking, and performance prediction, are essential for
accelerating and optimizing the hardware design process. To address this gap,
we present DeepRTL2, a family of versatile LLMs that unifies both generation-
and embedding-based tasks related to RTL. By simultaneously tackling a broad
range of tasks, DeepRTL2 represents the first model to provide a comprehensive
solution to the diverse challenges in EDA. Through extensive experiments, we
show that DeepRTL2 achieves state-of-the-art performance across all evaluated
tasks.

</details>


### [43] [Profile-Guided Temporal Prefetching](https://arxiv.org/abs/2506.15985)
*Mengming Li,Qijun Zhang,Yichuan Gao,Wenji Fang,Yao Lu,Yongqing Ren,Zhiyao Xie*

Main category: cs.AR

TL;DR: The paper introduces Prophet, a hardware-software framework that uses profile-guided methods to improve temporal prefetching efficiency by optimizing on-chip metadata storage management for irregular memory access patterns.


<details>
  <summary>Details</summary>
Motivation: Existing temporal prefetching methods struggle with limited on-chip metadata storage efficiency, and current software solutions fail to optimize temporal prefetching.

Method: Prophet uses a profile-guided approach to program profiling (using counters, not traces), injects hints into programs for metadata storage management, and dynamically adapts these hints for different program inputs. It works alongside existing hardware prefetchers.

Result: Prophet achieves a 14.23% performance improvement over the leading temporal prefetcher (Triangel), succeeds where prior solutions achieved only 0.1% performance gain, and shows consistent benefits across all workloads with minimal overhead.

Conclusion: Prophet offers an efficient solution for hardware-software co-design in improving temporal prefetching, addressing challenges in managing complex temporal patterns and leveraging limited metadata storage capacity.

Abstract: Temporal prefetching shows promise for handling irregular memory access
patterns, which are common in data-dependent and pointer-based data structures.
Recent studies introduced on-chip metadata storage to reduce the memory traffic
caused by accessing metadata from off-chip DRAM. However, existing prefetching
schemes struggle to efficiently utilize the limited on-chip storage. An
alternative solution, software indirect access prefetching, remains ineffective
for optimizing temporal prefetching.
  In this work, we propose Prophet--a hardware-software co-designed framework
that leverages profile-guided methods to optimize metadata storage management.
Prophet profiles programs using counters instead of traces, injects hints into
programs to guide metadata storage management, and dynamically tunes these
hints to enable the optimized binary to adapt to different program inputs.
Prophet is designed to coexist with existing hardware temporal prefetchers,
delivering efficient, high-performance solutions for frequently executed
workloads while preserving the original runtime scheme for less frequently
executed workloads. Prophet outperforms the state-of-the-art temporal
prefetcher, Triangel, by 14.23%, effectively addressing complex temporal
patterns where prior profile-guided solutions fall short (only achieving 0.1%
performance gain). Prophet delivers superior performance across all evaluated
workload inputs, introducing negligible profiling, analysis, and instruction
overhead.

</details>


### [44] [HetGPU: The pursuit of making binary compatibility towards GPUs](https://arxiv.org/abs/2506.15993)
*Yiwei Yang,Yusheng Zheng,Tong Yu,Andi Quinn*

Main category: cs.AR

TL;DR: The paper introduces hetGPU, a system that enables a single GPU binary to run on different GPU vendors’ hardware.


<details>
  <summary>Details</summary>
Motivation: The issue is that GPU binaries are not cross-compatible across vendor hardware due to differences in instruction sets, execution models, and drivers.

Method: The hetGPU system includes a compiler for intermediate representation (IR), runtime with dynamic translation, and an abstraction layer for threads, memory, and synchronization.

Result: hetGPU allows unmodified GPU binaries to migrate across various vendor GPUs with minimal overhead through preliminary testing.

Conclusion: The system offers a significant step toward vendor-agnostic GPU computing by addressing cross-vendor execution compatibility and live migration challenges.

Abstract: Heterogeneous GPU infrastructures present a binary compatibility challenge:
code compiled for one vendor's GPU will not run on another due to divergent
instruction sets, execution models, and driver stacks . We propose hetGPU, a
new system comprising a compiler, runtime, and abstraction layer that together
enable a single GPU binary to execute on NVIDIA, AMD, Intel, and Tenstorrent
hardware. The hetGPU compiler emits an architecture-agnostic GPU intermediate
representation (IR) and inserts metadata for managing execution state. The
hetGPU runtime then dynamically translates this IR to the target GPU's native
code and provides a uniform abstraction of threads, memory, and
synchronization. Our design tackles key challenges: differing SIMT vs. MIMD
execution (warps on NVIDIA/AMD vs. many-core RISC-V on Tenstorrent), varied
instruction sets, scheduling and memory model discrepancies, and the need for
state serialization for live migration. We detail the hetGPU architecture,
including the IR transformation pipeline, a state capture/reload mechanism for
live GPU migration, and an abstraction layer that bridges warp-centric and
core-centric designs. Preliminary evaluation demonstrates that unmodified GPU
binaries compiled with hetGPU can be migrated across disparate GPUs with
minimal overhead, opening the door to vendor-agnostic GPU computing.

</details>


### [45] [SparseDPD: A Sparse Neural Network-based Digital Predistortion FPGA Accelerator for RF Power Amplifier Linearization](https://arxiv.org/abs/2506.16591)
*Manno Versluis,Yizhuo Wu,Chang Gao*

Main category: cs.AR

TL;DR: Introduction of SparseDPD, an FPGA accelerator harnessing a sparse neural network for efficient power amplifier linearization in wireless communications, achieving high performance and low power consumption.


<details>
  <summary>Details</summary>
Motivation: Address computational challenges and practicality issues of deploying neural network-based digital predistortion (DPD) methods in wireless communication systems.

Method: SparseDPD employs a spatially sparse phase-normalized time-delay neural network (PNTDNN), optimized through unstructured pruning to reduce computational complexity while maintaining accuracy, implemented on an FPGA platform.

Result: SparseDPD demonstrates high linearization performance with metrics of ACPR (-59.4 dBc), EVM (-54.0 dBc), and NMSE (-48.2 dB) on a Xilinx FPGA, consuming only 241 mW dynamic power with 74% sparsity and 64 parameters.

Conclusion: SparseDPD showcases practical and efficient FPGA-based acceleration of NN-based DPD methods, enabling real-time applications in wireless communication systems with publicly accessible code.

Abstract: Digital predistortion (DPD) is crucial for linearizing radio frequency (RF)
power amplifiers (PAs), improving signal integrity and efficiency in wireless
systems. Neural network (NN)-based DPD methods surpass traditional polynomial
models but face computational challenges limiting their practical deployment.
This paper introduces SparseDPD, an FPGA accelerator employing a spatially
sparse phase-normalized time-delay neural network (PNTDNN), optimized through
unstructured pruning to reduce computational load without accuracy loss.
Implemented on a Xilinx Zynq-7Z010 FPGA, SparseDPD operates at 170 MHz,
achieving exceptional linearization performance (ACPR: -59.4 dBc, EVM: -54.0
dBc, NMSE: -48.2 dB) with only 241 mW dynamic power, using 64 parameters with
74% sparsity. This work demonstrates FPGA-based acceleration, making NN-based
DPD practical and efficient for real-time wireless communication applications.
Code is publicly available at https://github.com/MannoVersluis/SparseDPD.

</details>


### [46] [Lookup Table-based Multiplication-free All-digital DNN Accelerator Featuring Self-Synchronous Pipeline Accumulation](https://arxiv.org/abs/2506.16800)
*Hiroto Tagata,Takashi Sato,Hiromitsu Awano*

Main category: cs.AR

TL;DR: The paper introduces a novel all-digital accelerator for DNNs based on MADDNESS, improving energy and area efficiency significantly compared to standard approaches.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the critical challenge of high power consumption in deep neural networks due to large-scale matrix computations and limitations of prior analog-based solutions.

Method: The proposed method involves a MADDNESS-based all-digital accelerator with a self-synchronous pipeline accumulator for compact, energy-efficient, and PVT-invariant computation.

Result: Post-layout simulations using a 22nm process showed a 2.5x improvement in energy efficiency (174 TOPS/W) and a 5x increase in area efficiency (2.01 TOPS/mm2) compared to conventional accelerators.

Conclusion: The proposed all-digital accelerator offers a compact, highly efficient, and scalable solution for deep neural network computations, overcoming the limitations of analog implementations.

Abstract: Deep neural networks (DNNs) have been widely applied in our society, yet
reducing power consumption due to large-scale matrix computations remains a
critical challenge. MADDNESS is a known approach to improving energy efficiency
by substituting matrix multiplication with table lookup operations. Previous
research has employed large analog computing circuits to convert inputs into
LUT addresses, which presents challenges to area efficiency and computational
accuracy. This paper proposes a novel MADDNESS-based all-digital accelerator
featuring a self-synchronous pipeline accumulator, resulting in a compact,
energy-efficient, and PVT-invariant computation. Post-layout simulation using a
commercial 22nm process showed that 2.5 times higher energy efficiency (174
TOPS/W) and 5 times higher area efficiency (2.01 TOPS/mm2) can be achieved
compared to the conventional accelerator.

</details>


### [47] [RCNet: $ΔΣ$ IADCs as Recurrent AutoEncoders](https://arxiv.org/abs/2506.16903)
*Arnaud Verdant,William Guicquero,Jérôme Chossat*

Main category: cs.AR

TL;DR: This paper introduces "RCNet," a deep learning model for improving the design of Delta-Sigma ADCs using Recurrent Neural Networks and custom optimizations.


<details>
  <summary>Details</summary>
Motivation: To explore innovative ways of optimizing ADC design by leveraging deep learning approaches while accounting for hardware constraints.

Method: Utilizes Recurrent Neural Networks to model both modulators and filters. Employs high-end optimizers and custom losses to address hardware constraints such as quantized weights, saturation, noise, and device area.

Result: RCNet achieved an SNR of over 13 bits while maintaining area constraints under 14pF capacitors at 80 OSR samples. It demonstrated unique tradeoffs, including effective performance without relying on high-order modulators.

Conclusion: The results suggest that RCNet offers a promising framework for ADC design, achieving remarkable tradeoffs between SNR and hardware limitations, and expanding design possibilities beyond traditional high-order modulators.

Abstract: This paper proposes a deep learning model (RCNet) for Delta-Sigma
($\Delta\Sigma$) ADCs. Recurrent Neural Networks (RNNs) allow to describe both
modulators and filters. This analogy is applied to Incremental ADCs (IADC).
High-end optimizers combined with full-custom losses are used to define
additional hardware design constraints: quantized weights, signal saturation,
temporal noise injection, devices area. Focusing on DC conversion, our early
results demonstrate that $SNR$ defined as an Effective Number Of Bits (ENOB)
can be optimized under a certain hardware mapping complexity. The proposed
RCNet succeeded to provide design tradeoffs in terms of $SNR$ ($>$13bit) versus
area constraints ($<$14pF total capacitor) at a given $OSR$ (80 samples).
Interestingly, it appears that the best RCNet architectures do not necessarily
rely on high-order modulators, leveraging additional topology exploration
degrees of freedom.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [48] [Veracity: An Open-Source AI Fact-Checking System](https://arxiv.org/abs/2506.15794)
*Taylor Lynn Curtis,Maximilian Puelma Touzel,William Garneau,Manon Gruaz,Mike Pinder,Li Wei Wang,Sukanya Krishna,Luda Cohen,Jean-François Godbout,Reihaneh Rabbany,Kellin Pelrine*

Main category: cs.CL

TL;DR: This paper introduces Veracity, an open-source AI system for fact-checking claims using Large Language Models and web retrieval for grounded assessments.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the growing threat of misinformation, especially with the advancements in generative AI, by providing individuals with accessible fact-checking tools.

Method: The method involves combining Large Language Models with web retrieval to analyze claims and provide veracity assessments, featuring multilingual support and an intuitive messaging-inspired interface.

Result: The result is Veracity, a system that can detect misinformation, explain its reasoning, and provide transparent fact-checking in an interactive manner.

Conclusion: Veracity contributes to combating misinformation, fostering media literacy, and helping create a more informed society by making fact-checking accessible and explainable.

Abstract: The proliferation of misinformation poses a significant threat to society,
exacerbated by the capabilities of generative AI. This demo paper introduces
Veracity, an open-source AI system designed to empower individuals to combat
misinformation through transparent and accessible fact-checking. Veracity
leverages the synergy between Large Language Models (LLMs) and web retrieval
agents to analyze user-submitted claims and provide grounded veracity
assessments with intuitive explanations. Key features include multilingual
support, numerical scoring of claim veracity, and an interactive interface
inspired by familiar messaging applications. This paper will showcase
Veracity's ability to not only detect misinformation but also explain its
reasoning, fostering media literacy and promoting a more informed society.

</details>


### [49] [Rethinking LLM Training through Information Geometry and Quantum Metrics](https://arxiv.org/abs/2506.15830)
*Riccardo Di Sipio*

Main category: cs.CL

TL;DR: The paper discusses optimization in large language models through a non-Euclidean lens using the Fisher information metric, suggesting curvature-aware training for deeper understanding.


<details>
  <summary>Details</summary>
Motivation: To enhance and better understand the training mechanisms of large language models through advanced geometric and quantum-inspired optimization methods.

Method: Using the Fisher information metric for principled learning via natural gradient descent and exploring quantum analogies for optimization.

Result: Clarifies phenomena such as sharp minima, generalization, and scaling laws while hinting at improved optimization methods for quantum-enhanced systems.

Conclusion: Curvature-aware approaches using information geometry improve understanding and optimization in LLM training, with quantum-inspired methods offering future possibilities.

Abstract: Optimization in large language models (LLMs) unfolds over high-dimensional
parameter spaces with non-Euclidean structure. Information geometry frames this
landscape using the Fisher information metric, enabling more principled
learning via natural gradient descent. Though often impractical, this geometric
lens clarifies phenomena such as sharp minima, generalization, and observed
scaling laws. We argue that curvature-aware approaches deepen our understanding
of LLM training. Finally, we speculate on quantum analogies based on the
Fubini-Study metric and Quantum Fisher Information, hinting at efficient
optimization in quantum-enhanced systems.

</details>


### [50] [MEM1: Learning to Synergize Memory and Reasoning for Efficient Long-Horizon Agents](https://arxiv.org/abs/2506.15841)
*Zijian Zhou,Ao Qu,Zhaoxuan Wu,Sunghwan Kim,Alok Prakash,Daniela Rus,Jinhua Zhao,Bryan Kian Hsiang Low,Paul Pu Liang*

Main category: cs.CL

TL;DR: MEM1 is a reinforcement learning framework for language agents that uses constant memory by consolidating and reasoning over essential information, outperforming traditional systems in efficiency and accuracy for long-horizon tasks.


<details>
  <summary>Details</summary>
Motivation: Existing language agent systems manage long-horizon tasks by maintaining full-context memory, causing computational inefficiencies, memory growth, and degraded performance with increasing input lengths.

Method: MEM1 employs reinforcement learning to update a compact internal state that consolidates memory, integrates new observations, and discards irrelevant information, ensuring constant memory use. It also proposes a way to create multi-turn environments from datasets for diverse training scenarios.

Result: MEM1 achieved 3.5x performance improvement and reduced memory use by 3.7x compared to a benchmark system, Qwen2.5-14B-Instruct, in a 16-objective multi-hop QA task across various domains.

Conclusion: Reasoning-driven memory consolidation offers a promising solution for enabling long-horizon, interactive language agents to achieve superior performance and efficiency.

Abstract: Modern language agents must operate over long-horizon, multi-turn
interactions, where they retrieve external information, adapt to observations,
and answer interdependent queries. Yet, most LLM systems rely on full-context
prompting, appending all past turns regardless of their relevance. This leads
to unbounded memory growth, increased computational costs, and degraded
reasoning performance on out-of-distribution input lengths. We introduce MEM1,
an end-to-end reinforcement learning framework that enables agents to operate
with constant memory across long multi-turn tasks. At each turn, MEM1 updates a
compact shared internal state that jointly supports memory consolidation and
reasoning. This state integrates prior memory with new observations from the
environment while strategically discarding irrelevant or redundant information.
To support training in more realistic and compositional settings, we propose a
simple yet effective and scalable approach to constructing multi-turn
environments by composing existing datasets into arbitrarily complex task
sequences. Experiments across three domains, including internal retrieval QA,
open-domain web QA, and multi-turn web shopping, show that MEM1-7B improves
performance by 3.5x while reducing memory usage by 3.7x compared to
Qwen2.5-14B-Instruct on a 16-objective multi-hop QA task, and generalizes
beyond the training horizon. Our results demonstrate the promise of
reasoning-driven memory consolidation as a scalable alternative to existing
solutions for training long-horizon interactive agents, where both efficiency
and performance are optimized.

</details>


### [51] [Finance Language Model Evaluation (FLaME)](https://arxiv.org/abs/2506.15846)
*Glenn Matlin,Mika Okamoto,Huzaifa Pardawala,Yang Yang,Sudheer Chava*

Main category: cs.CL

TL;DR: This paper introduces FLaME, a benchmarking suite to evaluate Language Models (LMs) on financial tasks and showcases their potential.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the gap in evaluating LMs' effectiveness on highly specialized financial tasks, dispelling misconceptions about their performance in FinNLP.

Method: The authors developed the FLaME benchmarking suite to evaluate 23 foundation LMs on 20 financial NLP tasks, incorporating reasoning-reinforced LMs for comparison.

Result: Their empirical study demonstrates the potential of LMs in financial NLP tasks and challenges previous assumptions of their lower performance limits.

Conclusion: The open-source FLaME framework provides a comprehensive tool for assessing and advancing the capabilities of LMs in specialized financial domains.

Abstract: Language Models (LMs) have demonstrated impressive capabilities with core
Natural Language Processing (NLP) tasks. The effectiveness of LMs for highly
specialized knowledge-intensive tasks in finance remains difficult to assess
due to major gaps in the methodologies of existing evaluation frameworks, which
have caused an erroneous belief in a far lower bound of LMs' performance on
common Finance NLP (FinNLP) tasks. To demonstrate the potential of LMs for
these FinNLP tasks, we present the first holistic benchmarking suite for
Financial Language Model Evaluation (FLaME). We are the first research paper to
comprehensively study LMs against 'reasoning-reinforced' LMs, with an empirical
study of 23 foundation LMs over 20 core NLP tasks in finance. We open-source
our framework software along with all data and results.

</details>


### [52] [Entropy-Driven Pre-Tokenization for Byte-Pair Encoding](https://arxiv.org/abs/2506.15889)
*Yifan Hu,Frank Liang,Dachuan Zhao,Jonathan Geuter,Varshini Reddy,Craig W. Schmidt,Chris Tanner*

Main category: cs.CL

TL;DR: This paper improves Byte-Pair Encoding (BPE) tokenization for unsegmented languages like Chinese using entropy-informed strategies.


<details>
  <summary>Details</summary>
Motivation: Standard BPE struggles in unsegmented languages as it ignores linguistic boundaries, leading to suboptimal tokenization.

Method: The authors introduce entropy-based pre-tokenization using cues like pointwise mutual information, left/right entropy, and predictive entropy from GPT-2.

Result: Entropy-guided methods improve segmentation precision, recall, F1 scores, and alignment with linguistic units versus standard BPE.

Conclusion: The proposed strategies enhance tokenization quality, showing promise for low-resource and multilingual languages.

Abstract: Byte-Pair Encoding (BPE) has become a widely adopted subword tokenization
method in modern language models due to its simplicity and strong empirical
performance across downstream tasks. However, applying BPE to unsegmented
languages such as Chinese presents significant challenges, as its
frequency-driven merge operation is agnostic to linguistic boundaries. To
address this, we propose two entropy-informed pre-tokenization strategies that
guide BPE segmentation using unsupervised information-theoretic cues. The first
approach uses pointwise mutual information and left/right entropy to identify
coherent character spans, while the second leverages predictive entropy derived
from a pretrained GPT-2 model to detect boundary uncertainty. We evaluate both
methods on a subset of the PKU dataset and demonstrate substantial improvements
in segmentation precision, recall, and F1 score compared to standard BPE. Our
results suggest that entropy-guided pre-tokenization not only enhances
alignment with gold-standard linguistic units but also offers a promising
direction for improving tokenization quality in low-resource and multilingual
settings.

</details>


### [53] [Language Models can perform Single-Utterance Self-Correction of Perturbed Reasoning](https://arxiv.org/abs/2506.15894)
*Sam Silver,Jimin Sun,Ivan Zhang,Sara Hooker,Eddie Kim*

Main category: cs.CL

TL;DR: The paper examines large language models' (LLMs) ability to self-correct errors in their reasoning using Chain of Thought (CoT) prompting, finding that they have inherent self-correction capabilities.


<details>
  <summary>Details</summary>
Motivation: To explore and better understand the self-correction ability of LLMs, especially in their reasoning, given their brittleness to problem variations and sampling-induced errors.

Method: Experiments were conducted introducing synthetic perturbations into the Chain of Thought (CoT) reasoning of models to measure their self-correction capabilities.

Result: LLMs exhibited robust self-correction behaviors across various scenarios, even in cases where CoT was not explicitly finetuned, from implicit fixes to explicit acknowledgment and error correction.

Conclusion: LLMs inherently possess stronger self-correction abilities than previously understood, implying that recent reasoning improvements amplify pre-existing traits in these models.

Abstract: Large Language Models (LLMs) have demonstrated impressive mathematical
reasoning capabilities, yet their performance remains brittle to minor
variations in problem description and prompting strategy. Furthermore,
reasoning is vulnerable to sampling-induced errors which autoregressive models
must primarily address using self-correction via additionally-generated tokens.
To better understand self-correction capabilities of recent models, we conduct
experiments measuring models' ability to self-correct synthetic perturbations
introduced into their Chain of Thought (CoT) reasoning. We observe robust
single-utterance intrinsic self-correction behavior across a range of
open-weight models and datasets, ranging from subtle, implicit corrections to
explicit acknowledgments and corrections of errors. Our findings suggest that
LLMs, including those not finetuned for long CoT, may possess stronger
intrinsic self-correction capabilities than commonly shown in the literature.
The presence of this ability suggests that recent "reasoning" model work
involves amplification of traits already meaningfully present in models.

</details>


### [54] [From RAG to Agentic: Validating Islamic-Medicine Responses with LLM Agents](https://arxiv.org/abs/2506.15911)
*Mohammad Amaan Sayeed,Mohammed Talha Alam,Raza Imam,Shahab Saquib Sohail,Amir Hussain*

Main category: cs.CL

TL;DR: The paper introduces an evaluation pipeline to align classical Islamic medical texts with large language models (LLMs) for culturally sensitive medical question-answering, yielding improved accuracy and insight.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the underutilization and inaccessibility of centuries-old Islamic medical knowledge in modern AI systems, as well as the need for culturally grounded medical evaluation benchmarks.

Method: The researchers proposed Tibbe-AG, an evaluation pipeline combining classical Islamic medical text-derived questions, LLM configurations (direct generation, retrieval-augmented generation, self-critique filter), and a secondary LLM-based scoring system.

Result: The retrieval method improved answer accuracy by 13%, while the agentic prompt enhanced insights and safety considerations, yielding an additional 10% improvement in quality scores.

Conclusion: Integrating classical Islamic medical texts with retrieval and self-evaluation methods enhances reliable and culturally appropriate medical question-answering using LLMs.

Abstract: Centuries-old Islamic medical texts like Avicenna's Canon of Medicine and the
Prophetic Tibb-e-Nabawi encode a wealth of preventive care, nutrition, and
holistic therapies, yet remain inaccessible to many and underutilized in modern
AI systems. Existing language-model benchmarks focus narrowly on factual recall
or user preference, leaving a gap in validating culturally grounded medical
guidance at scale. We propose a unified evaluation pipeline, Tibbe-AG, that
aligns 30 carefully curated Prophetic-medicine questions with human-verified
remedies and compares three LLMs (LLaMA-3, Mistral-7B, Qwen2-7B) under three
configurations: direct generation, retrieval-augmented generation, and a
scientific self-critique filter. Each answer is then assessed by a secondary
LLM serving as an agentic judge, yielding a single 3C3H quality score.
Retrieval improves factual accuracy by 13%, while the agentic prompt adds
another 10% improvement through deeper mechanistic insight and safety
considerations. Our results demonstrate that blending classical Islamic texts
with retrieval and self-evaluation enables reliable, culturally sensitive
medical question-answering.

</details>


### [55] [Reranking-based Generation for Unbiased Perspective Summarization](https://arxiv.org/abs/2506.15925)
*Narutatsu Ri,Nicholas Deas,Kathleen McKeown*

Main category: cs.CL

TL;DR: The paper investigates reliable evaluation metrics and methods for unbiased political perspective summarization using LLMs, revealing that LLM-based metrics outperform traditional ones.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the lack of reliable evaluation metrics and the limited progress in improving LLM-based summarizers for unbiased real-world applications, specifically political perspective summarization.

Method: The authors identify reliable metrics through benchmarking with human annotations, utilize reranking-based methods, and employ preference tuning with synthetic and reranked data to improve summarization.

Result: Their findings demonstrate that language model-based metrics perform better than traditional ones, and reranking methods combined with preference tuning enhance summarization quality.

Conclusion: The research provides insights into reliable evaluation frameworks and improved methods for perspective summarization, contributing to advancements in unbiased summary generation using LLMs.

Abstract: Generating unbiased summaries in real-world settings such as political
perspective summarization remains a crucial application of Large Language
Models (LLMs). Yet, existing evaluation frameworks rely on traditional metrics
for measuring key attributes such as coverage and faithfulness without
verifying their applicability, and efforts to develop improved summarizers are
still nascent. We address these gaps by (1) identifying reliable metrics for
measuring perspective summary quality, and (2) investigating the efficacy of
LLM-based methods beyond zero-shot inference. Namely, we build a test set for
benchmarking metric reliability using human annotations and show that
traditional metrics underperform compared to language model-based metrics,
which prove to be strong evaluators. Using these metrics, we show that
reranking-based methods yield strong results, and preference tuning with
synthetically generated and reranking-labeled data further boosts performance.
Our findings aim to contribute to the reliable evaluation and development of
perspective summarization methods.

</details>


### [56] [A Vietnamese Dataset for Text Segmentation and Multiple Choices Reading Comprehension](https://arxiv.org/abs/2506.15978)
*Toan Nguyen Hai,Ha Nguyen Viet,Truong Quan Xuan,Duc Do Minh*

Main category: cs.CL

TL;DR: The VSMRC dataset provides a new Vietnamese resource for text segmentation and reading comprehension, showcasing the superior performance of multilingual models.


<details>
  <summary>Details</summary>
Motivation: To address the lack of robust NLP resources for Vietnamese, specifically in text segmentation and reading comprehension.

Method: Researchers created VSMRC, a dataset sourced from Wikipedia with human-verified synthetic question-answer pairs and conducted experiments using mBERT and monolingual models.

Result: mBERT achieved 88.01% accuracy in MRC and 63.15% F1 score in text segmentation, outperforming monolingual models.

Conclusion: Multilingual models like mBERT are more effective for Vietnamese NLP tasks, indicating a broader utility for under-resourced languages.

Abstract: Vietnamese, the 20th most spoken language with over 102 million native
speakers, lacks robust resources for key natural language processing tasks such
as text segmentation and machine reading comprehension (MRC). To address this
gap, we present VSMRC, the Vietnamese Text Segmentation and Multiple-Choice
Reading Comprehension Dataset. Sourced from Vietnamese Wikipedia, our dataset
includes 15,942 documents for text segmentation and 16,347 synthetic
multiple-choice question-answer pairs generated with human quality assurance,
ensuring a reliable and diverse resource. Experiments show that mBERT
consistently outperforms monolingual models on both tasks, achieving an
accuracy of 88.01% on MRC test set and an F1 score of 63.15\% on text
segmentation test set. Our analysis reveals that multilingual models excel in
NLP tasks for Vietnamese, suggesting potential applications to other
under-resourced languages. VSMRC is available at HuggingFace

</details>


### [57] [Double Entendre: Robust Audio-Based AI-Generated Lyrics Detection via Multi-View Fusion](https://arxiv.org/abs/2506.15981)
*Markus Frohmann,Gabriel Meseguer-Brocal,Markus Schedl,Elena V. Epure*

Main category: cs.CL

TL;DR: This paper introduces DE-detect, a multimodal technique for detecting AI-generated music by combining sung lyrics transcription and speech features from audio, improving robustness and performance over existing methods.


<details>
  <summary>Details</summary>
Motivation: The rise of AI-generated music tools poses challenges in copyright protection and artist rights, and current detectors are either unreliable against new generators or require inaccessible, clean, formatted lyrics.

Method: The proposed method is a multimodal, late-fusion pipeline combining automatically transcribed sung lyrics and speech features from audio, designed to enhance robustness against perturbations and overcome traditional detection flaws.

Result: DE-detect demonstrates superior performance compared to existing lyrics-based detectors and exhibits increased robustness to audio perturbations.

Conclusion: DE-detect offers a practical and effective solution for detecting AI-generated music, addressing critical limitations in existing detectors and enhancing applicability in real-world scenarios.

Abstract: The rapid advancement of AI-based music generation tools is revolutionizing
the music industry but also posing challenges to artists, copyright holders,
and providers alike. This necessitates reliable methods for detecting such
AI-generated content. However, existing detectors, relying on either audio or
lyrics, face key practical limitations: audio-based detectors fail to
generalize to new or unseen generators and are vulnerable to audio
perturbations; lyrics-based methods require cleanly formatted and accurate
lyrics, unavailable in practice. To overcome these limitations, we propose a
novel, practically grounded approach: a multimodal, modular late-fusion
pipeline that combines automatically transcribed sung lyrics and speech
features capturing lyrics-related information within the audio. By relying on
lyrical aspects directly from audio, our method enhances robustness, mitigates
susceptibility to low-level artifacts, and enables practical applicability.
Experiments show that our method, DE-detect, outperforms existing lyrics-based
detectors while also being more robust to audio perturbations. Thus, it offers
an effective, robust solution for detecting AI-generated music in real-world
scenarios. Our code is available at
https://github.com/deezer/robust-AI-lyrics-detection.

</details>


### [58] [From General to Targeted Rewards: Surpassing GPT-4 in Open-Ended Long-Context Generation](https://arxiv.org/abs/2506.16024)
*Zhihan Guo,Jiele Wu,Wenqian Cui,Yifei Zhang,Minda Hu,Yufei Wang,Irwin King*

Main category: cs.CL

TL;DR: This paper introduces ProxyReward, a framework for improving long-context generation in large language models, using a novel reinforcement learning-based dataset and reward signal.


<details>
  <summary>Details</summary>
Motivation: The current methods for training long-context generation models are limited by the lack of gold standard reference data and rely on general assessments, leading to accuracy issues.

Method: The ProxyReward framework includes two components: (1) an automated dataset generation approach using simple prompts, eliminating the need for extensive labeling, and (2) a targeted reward signal that evaluates information comprehensiveness and accuracy for specific queries.

Result: Experimental results show that ProxyReward outperforms even GPT-4-Turbo and improves the Open-LTG task performance by up to 20% in open-source models. It also surpasses the LLM-as-a-Judge approach.

Conclusion: ProxyReward is an effective method to enhance the capacity of LLMs in handling complex open-ended questions with improved accuracy and performance.

Abstract: Current research on long-form context in Large Language Models (LLMs)
primarily focuses on the understanding of long-contexts, the Open-ended Long
Text Generation (Open-LTG) remains insufficiently explored. Training a
long-context generation model requires curation of gold standard reference
data, which is typically nonexistent for informative Open-LTG tasks. However,
previous methods only utilize general assessments as reward signals, which
limits accuracy. To bridge this gap, we introduce ProxyReward, an innovative
reinforcement learning (RL) based framework, which includes a dataset and a
reward signal computation method. Firstly, ProxyReward Dataset generation is
accomplished through simple prompts that enables the model to create
automatically, obviating extensive labeled data or significant manual effort.
Secondly, ProxyReward Signal offers a targeted evaluation of information
comprehensiveness and accuracy for specific questions. The experimental results
indicate that our method ProxyReward surpasses even GPT-4-Turbo. It can
significantly enhance performance by 20% on the Open-LTG task when training
widely used open-source models, while also surpassing the LLM-as-a-Judge
approach. Our work presents effective methods to enhance the ability of LLMs to
address complex open-ended questions posed by human.

</details>


### [59] [EvoLM: In Search of Lost Language Model Training Dynamics](https://arxiv.org/abs/2506.16029)
*Zhenting Qi,Fan Nie,Alexandre Alahi,James Zou,Himabindu Lakkaraju,Yilun Du,Eric Xing,Sham Kakade,Hanlin Zhang*

Main category: cs.CL

TL;DR: The study introduces EvoLM, a model suite for systematically analyzing language models' (LM) training phases, offering insights on training efficiency and trade-offs, with open resources provided.


<details>
  <summary>Details</summary>
Motivation: To address the complexities and lack of transparency in evaluating design choices across multiple LM training stages.

Method: Over 100 LMs with 1B and 4B parameters were trained from scratch to evaluate pre-training, continued pre-training, supervised fine-tuning, and reinforcement learning phases for both in-domain and out-of-domain generalization.

Result: Key findings reveal diminishing returns from excessive training, methods to mitigate forgetting during continued pre-training, and trade-offs in supervised fine-tuning and reinforcement learning configurations.

Conclusion: Optimal practices in LM training are highlighted, and the open-sourced EvoLM suite aids in improving model evaluation, transparency, and reproducibility.

Abstract: Modern language model (LM) training has been divided into multiple stages,
making it difficult for downstream developers to evaluate the impact of design
choices made at each stage. We present EvoLM, a model suite that enables
systematic and transparent analysis of LMs' training dynamics across
pre-training, continued pre-training, supervised fine-tuning, and reinforcement
learning. By training over 100 LMs with 1B and 4B parameters from scratch, we
rigorously evaluate both upstream (language modeling) and downstream
(problem-solving) reasoning capabilities, including considerations of both
in-domain and out-of-domain generalization. Key insights highlight the
diminishing returns from excessive pre-training and post-training, the
importance and practices of mitigating forgetting during domain-specific
continued pre-training, the crucial role of continued pre-training in bridging
pre-training and post-training phases, and various intricate trade-offs when
configuring supervised fine-tuning and reinforcement learning. To facilitate
open research and reproducibility, we release all pre-trained and post-trained
models, training datasets for all stages, and our entire training and
evaluation pipeline.

</details>


### [60] [REIS: A High-Performance and Energy-Efficient Retrieval System with In-Storage Processing](https://arxiv.org/abs/2506.16444)
*Kangqi Chen,Andreas Kosmas Kakolyris,Rakesh Nadig,Manos Frouzakis,Nika Mansouri Ghiasi,Yu Liang,Haiyu Mao,Jisung Park,Mohammad Sadrosadati,Onur Mutlu*

Main category: cs.CL

TL;DR: The paper introduces REIS, an in-storage processing system tailored for retrieval-augmented generation pipelines, addressing bottlenecks in approximate nearest neighbor search by optimizing storage embedded computations, database layouts, and data placement.


<details>
  <summary>Details</summary>
Motivation: Large language models struggle with static training-derived knowledge, and retrieval-augmented generation (RAG) pipelines, while effective, face performance bottlenecks during the data retrieval stage.

Method: The authors propose REIS, incorporating a tailored ISP system that combines a database layout linking embeddings with their documents, a data placement technique for efficient use of storage system planes, and an ANNS engine optimizing existing computational resources within storage systems.

Result: REIS demonstrates average performance improvements of 13x and energy efficiency gains of 55x compared to traditional server-grade systems during data retrieval operations.

Conclusion: REIS significantly accelerates the inference pipelines of RAG systems by optimizing storage system computations and retrieval workflows, achieving major performance and energy efficiency gains while avoiding major hardware modifications.

Abstract: Large Language Models (LLMs) face an inherent challenge: their knowledge is
confined to the data that they have been trained on. To overcome this issue,
Retrieval-Augmented Generation (RAG) complements the static training-derived
knowledge of LLMs with an external knowledge repository. RAG consists of three
stages: indexing, retrieval, and generation. The retrieval stage of RAG becomes
a significant bottleneck in inference pipelines. In this stage, a user query is
mapped to an embedding vector and an Approximate Nearest Neighbor Search (ANNS)
algorithm searches for similar vectors in the database to identify relevant
items. Due to the large database sizes, ANNS incurs significant data movement
overheads between the host and the storage system. To alleviate these
overheads, prior works propose In-Storage Processing (ISP) techniques that
accelerate ANNS by performing computations inside storage. However, existing
works that leverage ISP for ANNS (i) employ algorithms that are not tailored to
ISP systems, (ii) do not accelerate data retrieval operations for data selected
by ANNS, and (iii) introduce significant hardware modifications, limiting
performance and hindering their adoption. We propose REIS, the first ISP system
tailored for RAG that addresses these limitations with three key mechanisms.
First, REIS employs a database layout that links database embedding vectors to
their associated documents, enabling efficient retrieval. Second, it enables
efficient ANNS by introducing an ISP-tailored data placement technique that
distributes embeddings across the planes of the storage system and employs a
lightweight Flash Translation Layer. Third, REIS leverages an ANNS engine that
uses the existing computational resources inside the storage system. Compared
to a server-grade system, REIS improves the performance (energy efficiency) of
retrieval by an average of 13x (55x).

</details>


### [61] [Enhancing Document-Level Question Answering via Multi-Hop Retrieval-Augmented Generation with LLaMA 3](https://arxiv.org/abs/2506.16037)
*Xinyue Huang,Ziqi Lin,Fang Sun,Wenchao Zhang,Kejian Tong,Yunbo Liu*

Main category: cs.CL

TL;DR: The paper introduces an advanced Retrieval-Augmented Generation framework for solving complex question-answering tasks using multi-hop reasoning and contextual understanding, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: To address challenges in multi-hop reasoning and contextual understanding required for answering complex questions, particularly in handling lengthy documents.

Method: The framework, based on LLaMA 3, integrates a dense retrieval module, advanced context fusion, and multi-hop reasoning mechanisms. It uses a joint optimization strategy that combines retrieval likelihood and generation cross-entropy.

Result: The system demonstrates superior performance compared to retrieval-augmented and generative baselines, as shown in experimental results.

Conclusion: The proposed RAG framework enhances the ability to generate precise, contextually accurate answers, emphasizing its effectiveness in complex question answering tasks.

Abstract: This paper presents a novel Retrieval-Augmented Generation (RAG) framework
tailored for complex question answering tasks, addressing challenges in
multi-hop reasoning and contextual understanding across lengthy documents.
Built upon LLaMA 3, the framework integrates a dense retrieval module with
advanced context fusion and multi-hop reasoning mechanisms, enabling more
accurate and coherent response generation. A joint optimization strategy
combining retrieval likelihood and generation cross-entropy improves the
model's robustness and adaptability. Experimental results show that the
proposed system outperforms existing retrieval-augmented and generative
baselines, confirming its effectiveness in delivering precise, contextually
grounded answers.

</details>


### [62] [DynScaling: Efficient Verifier-free Inference Scaling via Dynamic and Integrated Sampling](https://arxiv.org/abs/2506.16043)
*Fei Wang,Xingchen Wan,Ruoxi Sun,Jiefeng Chen,Sercan Ö. Arık*

Main category: cs.CL

TL;DR: The paper introduces DynScaling, a method enhancing large language model performance with efficient inference-time scaling.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the limitations of reliance on external verifiers and inefficiencies in existing scaling methods under practical computational constraints.

Method: The method involves two key innovations: (1) an integrated parallel-sequential sampling strategy to merge parallel responses into coherent reasoning chains, and (2) a dynamic budget allocation framework based on a multi-armed bandit approach to optimize resource distribution.

Result: DynScaling improves LLM task performance and lowers computational costs compared to existing verifier-free baselines.

Conclusion: DynScaling is an effective method for enhancing LLM inference under realistic resource constraints, eliminating the need for external verifiers.

Abstract: Inference-time scaling has proven effective in boosting large language model
(LLM) performance through increased test-time computation. Yet, its practical
application is often hindered by reliance on external verifiers or a lack of
optimization for realistic computational constraints. We propose DynScaling,
which addresses these limitations through two primary innovations: an
integrated parallel-sequential sampling strategy and a bandit-based dynamic
budget allocation framework. The integrated sampling strategy unifies parallel
and sequential sampling by constructing synthetic sequential reasoning chains
from initially independent parallel responses, promoting diverse and coherent
reasoning trajectories. The dynamic budget allocation framework formulates the
allocation of computational resources as a multi-armed bandit problem,
adaptively distributing the inference budget across queries based on the
uncertainty of previously sampled responses, thereby maximizing computational
efficiency. By combining these components, DynScaling effectively improves LLM
performance under practical resource constraints without the need for external
verifiers. Experimental results demonstrate that DynScaling consistently
surpasses existing verifier-free inference scaling baselines in both task
performance and computational cost.

</details>


### [63] [A Hybrid DeBERTa and Gated Broad Learning System for Cyberbullying Detection in English Text](https://arxiv.org/abs/2506.16052)
*Devesh Kumar*

Main category: cs.CL

TL;DR: This paper introduces a hybrid detection model combining DeBERTa and GBLS for effective cyberbullying identification, achieving high accuracy across datasets.


<details>
  <summary>Details</summary>
Motivation: The rise of online platforms has increased global connectivity but also harmful behaviors like cyberbullying, affecting a significant percentage of teenagers.

Method: The paper combines a modified transformer-based DeBERTa model with Squeeze-and-Excitation blocks and sentiment analysis, integrating it with the Gated Broad Learning System (GBLS) for classification.

Result: The hybrid model achieved high accuracy: 79.3% (HateXplain), 95.41% (SOSNet), 91.37% (Mendeley-I), and 94.67% (Mendeley-II), while incorporating explainability mechanisms like token-level analysis and LIME interpretations.

Conclusion: The model outperforms existing methods in cyberbullying detection and provides explainability tools, though faces challenges with implicit bias and sarcasm, offering directions for improvement.

Abstract: The proliferation of online communication platforms has created unprecedented
opportunities for global connectivity while simultaneously enabling harmful
behaviors such as cyberbullying, which affects approximately 54.4\% of
teenagers according to recent research. This paper presents a hybrid
architecture that combines the contextual understanding capabilities of
transformer-based models with the pattern recognition strengths of broad
learning systems for effective cyberbullying detection. This approach
integrates a modified DeBERTa model augmented with Squeeze-and-Excitation
blocks and sentiment analysis capabilities with a Gated Broad Learning System
(GBLS) classifier, creating a synergistic framework that outperforms existing
approaches across multiple benchmark datasets. The proposed ModifiedDeBERTa +
GBLS model achieved good performance on four English datasets: 79.3\% accuracy
on HateXplain, 95.41\% accuracy on SOSNet, 91.37\% accuracy on Mendeley-I, and
94.67\% accuracy on Mendeley-II. Beyond performance gains, the framework
incorporates comprehensive explainability mechanisms including token-level
attribution analysis, LIME-based local interpretations, and confidence
calibration, addressing critical transparency requirements in automated content
moderation. Ablation studies confirm the meaningful contribution of each
architectural component, while failure case analysis reveals specific
challenges in detecting implicit bias and sarcastic content, providing valuable
insights for future improvements in cyberbullying detection systems.

</details>


### [64] [Knee-Deep in C-RASP: A Transformer Depth Hierarchy](https://arxiv.org/abs/2506.16055)
*Andy Yang,Michaël Cadilhac,David Chiang*

Main category: cs.CL

TL;DR: The paper investigates how greater depth in transformers enhances their capabilities, providing theoretical proofs and empirical evidence.


<details>
  <summary>Details</summary>
Motivation: To formally determine and prove the enhanced capabilities gained by increasing the depth of transformers.

Method: The authors used theoretical proofs based on equivalence with the programming language C-RASP and provided empirical evidence on depth requirements for sequential tasks.

Result: Deeper transformers were shown to be more expressive, with theoretical results validated empirically for sequence-related tasks.

Conclusion: Greater depth in transformers contributes to enhanced expressiveness and enables better handling of sequential dependency tasks.

Abstract: It has been observed that transformers with greater depth (that is, more
layers) have more capabilities, but can we establish formally which
capabilities are gained with greater depth? We answer this question with a
theoretical proof followed by an empirical study. First, we consider
transformers that round to fixed precision except inside attention. We show
that this subclass of transformers is expressively equivalent to the
programming language C-RASP and this equivalence preserves depth. Second, we
prove that deeper C-RASP programs are more expressive than shallower C-RASP
programs, implying that deeper transformers are more expressive than shallower
transformers (within the subclass mentioned above). These results are
established by studying a form of temporal logic with counting operators, which
was shown equivalent to C-RASP in previous work. Finally, we provide empirical
evidence that our theory predicts the depth required for transformers without
positional encodings to length-generalize on a family of sequential dependency
tasks.

</details>


### [65] [Self-Critique-Guided Curiosity Refinement: Enhancing Honesty and Helpfulness in Large Language Models via In-Context Learning](https://arxiv.org/abs/2506.16064)
*Duc Hieu Ho,Chenglin Fan*

Main category: cs.CL

TL;DR: This paper evaluates honesty and helpfulness in 10 large language models and introduces a novel prompting method—self-critique-guided curiosity refinement—to enhance trustworthiness without extra training, achieving up to 4.3% improvement in quality.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges regarding inconsistent honesty and helpfulness in large language model outputs, aiming to enhance trustworthiness without requiring additional training.

Method: Benchmarking 10 LLMs using honesty and helpfulness metrics and introducing a new prompting strategy involving self-critique and refinement steps.

Result: Improvement in response quality with relative gains in honesty and helpfulness scores (1.4%–4.3%) using the self-critique-guided curiosity refinement approach.

Conclusion: Structured self-refinement strategies can effectively and scalably improve the trustworthiness of LLM outputs without the need for further training.

Abstract: Large language models (LLMs) have demonstrated robust capabilities across
various natural language tasks. However, producing outputs that are
consistently honest and helpful remains an open challenge. To overcome this
challenge, this paper tackles the problem through two complementary directions.
It conducts a comprehensive benchmark evaluation of ten widely used large
language models, including both proprietary and open-weight models from OpenAI,
Meta, and Google. In parallel, it proposes a novel prompting strategy,
self-critique-guided curiosity refinement prompting. The key idea behind this
strategy is enabling models to self-critique and refine their responses without
additional training. The proposed method extends the curiosity-driven prompting
strategy by incorporating two lightweight in-context steps including
self-critique step and refinement step.
  The experiment results on the HONESET dataset evaluated using the framework
$\mathrm{H}^2$ (honesty and helpfulness), which was executed with GPT-4o as a
judge of honesty and helpfulness, show consistent improvements across all
models. The approach reduces the number of poor-quality responses, increases
high-quality responses, and achieves relative gains in $\mathrm{H}^2$ scores
ranging from 1.4% to 4.3% compared to curiosity-driven prompting across
evaluated models. These results highlight the effectiveness of structured
self-refinement as a scalable and training-free strategy to improve the
trustworthiness of LLMs outputs.

</details>


### [66] [Cyberbullying Detection in Hinglish Text Using MURIL and Explainable AI](https://arxiv.org/abs/2506.16066)
*Devesh Kumar*

Main category: cs.CL

TL;DR: The paper develops a framework for detecting cyberbullying in Hinglish (Hindi-English code-mixed) text using the MURIL model, significantly outperforming other multilingual models on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: The rise of Hinglish communication on digital platforms has exposed the limitations of existing cyberbullying detection systems, which are primarily designed for monolingual text.

Method: The paper employs the MURIL model enhanced with explainability features like attribution analysis and cross-linguistic pattern recognition, along with ablation studies on selective layer freezing and preprocessing code-mixed content.

Result: The proposed system achieves notable accuracy improvements (1.36%–13.07%) across six datasets, outperforming models like RoBERTa and IndicBERT, with a top accuracy of 94.63% on the Mendeley dataset.

Conclusion: The proposed MURIL-based framework effectively enhances Hinglish cyberbullying detection while identifying challenges like context, cultural nuances, and sarcasm, paving the way for further advancements in multilingual analysis.

Abstract: The growth of digital communication platforms has led to increased
cyberbullying incidents worldwide, creating a need for automated detection
systems to protect users. The rise of code-mixed Hindi-English (Hinglish)
communication on digital platforms poses challenges for existing cyberbullying
detection systems, which were designed primarily for monolingual text. This
paper presents a framework for cyberbullying detection in Hinglish text using
the Multilingual Representations for Indian Languages (MURIL) architecture to
address limitations in current approaches. Evaluation across six benchmark
datasets -- Bohra \textit{et al.}, BullyExplain, BullySentemo, Kumar \textit{et
al.}, HASOC 2021, and Mendeley Indo-HateSpeech -- shows that the MURIL-based
approach outperforms existing multilingual models including RoBERTa and
IndicBERT, with improvements of 1.36 to 13.07 percentage points and accuracies
of 86.97\% on Bohra, 84.62\% on BullyExplain, 86.03\% on BullySentemo, 75.41\%
on Kumar datasets, 83.92\% on HASOC 2021, and 94.63\% on Mendeley dataset. The
framework includes explainability features through attribution analysis and
cross-linguistic pattern recognition. Ablation studies show that selective
layer freezing, appropriate classification head design, and specialized
preprocessing for code-mixed content improve detection performance, while
failure analysis identifies challenges including context-dependent
interpretation, cultural understanding, and cross-linguistic sarcasm detection,
providing directions for future research in multilingual cyberbullying
detection.

</details>


### [67] [FinCoT: Grounding Chain-of-Thought in Expert Financial Reasoning](https://arxiv.org/abs/2506.16123)
*Natapong Nitarach,Warit Sirichotedumrong,Panop Pitchayarthorn,Pittawat Taveekitworachai,Potsawee Manakul,Kunat Pipatanakul*

Main category: cs.CL

TL;DR: The paper introduces FinCoT, a structured chain-of-thought prompting technique for financial NLP, achieving significant performance improvements while reducing inference costs.


<details>
  <summary>Details</summary>
Motivation: The paper aims to improve reasoning in financial-specific NLP tasks by addressing limitations of existing prompting strategies, especially structured CoT prompting, which often lacked domain-specific expertise.

Method: The proposed FinCoT incorporates domain-specific financial reasoning into structured CoT prompting. Various prompting styles were evaluated on CFA-style financial questions to measure performance.

Result: FinCoT achieved an increase from 63.2% to 80.5% in performance and reduced token generation eight-fold compared to prior structured CoT prompting techniques.

Conclusion: Domain-aligned structured prompts enhance performance, reduce inference costs, and produce interpretable and expert-aligned reasoning for financial tasks.

Abstract: This paper presents FinCoT, a structured chain-of-thought (CoT) prompting
approach that incorporates insights from domain-specific expert financial
reasoning to guide the reasoning traces of large language models. We
investigate that there are three main prompting styles in FinNLP: (1) standard
prompting--zero-shot prompting; (2) unstructured CoT--CoT prompting without an
explicit reasoning structure, such as the use of tags; and (3) structured CoT
prompting--CoT prompting with explicit instructions or examples that define
structured reasoning steps. Previously, FinNLP has primarily focused on prompt
engineering with either standard or unstructured CoT prompting. However,
structured CoT prompting has received limited attention in prior work.
Furthermore, the design of reasoning structures in structured CoT prompting is
often based on heuristics from non-domain experts. In this study, we
investigate each prompting approach in FinNLP. We evaluate the three main
prompting styles and FinCoT on CFA-style questions spanning ten financial
domains. We observe that FinCoT improves performance from 63.2% to 80.5% and
Qwen-2.5-7B-Instruct from 69.7% to 74.2%, while reducing generated tokens
eight-fold compared to structured CoT prompting. Our findings show that
domain-aligned structured prompts not only improve performance and reduce
inference costs but also yield more interpretable and expert-aligned reasoning
traces.

</details>


### [68] [Under the Shadow of Babel: How Language Shapes Reasoning in LLMs](https://arxiv.org/abs/2506.16151)
*Chenxi Wang,Yixuan Zhang,Lang Gao,Zixiang Xu,Zirui Song,Yanbo Wang,Xiuying Chen*

Main category: cs.CL

TL;DR: The study investigates how large language models (LLMs) internalize reasoning biases shaped by different languages. Using the BICAUSE bilingual dataset, it identifies typological attention patterns, language-specific causal preferences, and semantic alignment across languages.


<details>
  <summary>Details</summary>
Motivation: The paper aims to explore whether LLMs trained on human language reflect linguistic relativity, specifically how language structure influences cognitive reasoning within models.

Method: The researchers developed BICAUSE, a structured bilingual dataset featuring aligned Chinese and English samples in causal forms, and analyzed LLMs' attention patterns and performance in causal reasoning tasks.

Result: Key findings include typologically aligned attention patterns, language-specific preferences for causal structures affecting performance, and convergent semantic representations during successful reasoning.

Conclusion: LLMs internalize reasoning biases shaped by language structure, offering empirical evidence that linguistic relativity extends to artificial models.

Abstract: Language is not only a tool for communication but also a medium for human
cognition and reasoning. If, as linguistic relativity suggests, the structure
of language shapes cognitive patterns, then large language models (LLMs)
trained on human language may also internalize the habitual logical structures
embedded in different languages. To examine this hypothesis, we introduce
BICAUSE, a structured bilingual dataset for causal reasoning, which includes
semantically aligned Chinese and English samples in both forward and reversed
causal forms. Our study reveals three key findings: (1) LLMs exhibit
typologically aligned attention patterns, focusing more on causes and
sentence-initial connectives in Chinese, while showing a more balanced
distribution in English. (2) Models internalize language-specific preferences
for causal word order and often rigidly apply them to atypical inputs, leading
to degraded performance, especially in Chinese. (3) When causal reasoning
succeeds, model representations converge toward semantically aligned
abstractions across languages, indicating a shared understanding beyond surface
form. Overall, these results suggest that LLMs not only mimic surface
linguistic forms but also internalize the reasoning biases shaped by language.
Rooted in cognitive linguistic theory, this phenomenon is for the first time
empirically verified through structural analysis of model internals.

</details>


### [69] [SGIC: A Self-Guided Iterative Calibration Framework for RAG](https://arxiv.org/abs/2506.16172)
*Guanhua Chen,Yutong Yao,Lidia S. Chao,Xuebo Liu,Derek F. Wong*

Main category: cs.CL

TL;DR: The paper introduces a Self-Guided Iterative Calibration (SGIC) Framework to enhance the calibration of large language models (LLMs) using uncertainty scores. This approach improves LLMs' query relevance evaluation and response accuracy.


<details>
  <summary>Details</summary>
Motivation: Despite advancements in retrieval-augmented generation (RAG), the calibration aspect of LLMs, crucial for reasoning, has often been overlooked. The paper addresses this gap to improve the models' performance and reliability.

Method: The Self-Guided Iterative Calibration Framework calculates uncertainty scores to assess document relevance and response confidence. These scores are refined through iterative reevaluation and combined with prior responses. Additionally, a novel training set is designed for better self-calibration.

Result: The proposed SGIC framework enhances the performance and accuracy of both proprietary and open-weight LLMs by effectively utilizing uncertainty scores.

Conclusion: The SGIC framework demonstrates the utility of iterative calibration and uncertainty-driven adjustments, setting a new benchmark for improving LLM reasoning and response capabilities.

Abstract: Recent research in retrieval-augmented generation (RAG) has concentrated on
retrieving useful information from candidate documents. However, numerous
methodologies frequently neglect the calibration capabilities of large language
models (LLMs), which capitalize on their robust in-context reasoning prowess.
This work illustrates that providing LLMs with specific cues substantially
improves their calibration efficacy, especially in multi-round calibrations. We
present a new SGIC: Self-Guided Iterative Calibration Framework that employs
uncertainty scores as a tool. Initially, this framework calculates uncertainty
scores to determine both the relevance of each document to the query and the
confidence level in the responses produced by the LLMs. Subsequently, it
reevaluates these scores iteratively, amalgamating them with prior responses to
refine calibration. Furthermore, we introduce an innovative approach for
constructing an iterative self-calibration training set, which optimizes LLMs
to efficiently harness uncertainty scores for capturing critical information
and enhancing response accuracy. Our proposed framework significantly improves
performance on both closed-source and open-weight LLMs.

</details>


### [70] [JETHICS: Japanese Ethics Understanding Evaluation Dataset](https://arxiv.org/abs/2506.16187)
*Masashi Takeshita,Rafal Rzepka*

Main category: cs.CL

TL;DR: The paper introduces JETHICS, a Japanese ethics dataset for evaluating AI models' ethical understanding, with observations revealing significant room for improvement in leading language models.


<details>
  <summary>Details</summary>
Motivation: The research aims to address the lack of a dedicated Japanese ethics dataset for assessing the ethical understanding of AI models and aligning them with diverse normative theories and concepts.

Method: The authors created JETHICS, a dataset with 78K examples, utilizing methodologies from the English ETHICS dataset, covering four normative categories and commonsense morality. They conducted evaluations on non-proprietary LLMs and GPT-4o.

Result: GPT-4o achieved an average score of about 0.7, while the best-performing Japanese LLM scored around 0.5, signifying challenges and gaps in ethical reasoning by current models.

Conclusion: There remains significant scope for enhancing the ethical comprehension capabilities of LLMs, as revealed by their relatively low performance on the JETHICS dataset.

Abstract: In this work, we propose JETHICS, a Japanese dataset for evaluating ethics
understanding of AI models. JETHICS contains 78K examples and is built by
following the construction methods of the existing English ETHICS dataset. It
includes four categories based normative theories and concepts from ethics and
political philosophy; and one representing commonsense morality. Our evaluation
experiments on non-proprietary large language models (LLMs) and on GPT-4o
reveal that even GPT-4o achieves only an average score of about 0.7, while the
best-performing Japanese LLM attains around 0.5, indicating a relatively large
room for improvement in current LLMs.

</details>


### [71] [Web(er) of Hate: A Survey on How Hate Speech Is Typed](https://arxiv.org/abs/2506.16190)
*Luna Wang,Andrew Caines,Alice Hutchings*

Main category: cs.CL

TL;DR: The paper examines hate speech dataset design, highlighting the implications of methodological choices and calls for transparency in addressing researcher biases.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenges of designing hate speech datasets, emphasizing the need to balance competing priorities and improve reliability.

Method: The authors critically analyze diverse hate speech datasets using Max Weber's ideal types to identify common practices and themes.

Result: Key methodological decisions and their implications for dataset reliability are uncovered, offering insights into creating more robust datasets.

Conclusion: Researchers are urged to adopt a reflexive approach, acknowledging value judgments to enhance transparency and methodological rigor in hate speech dataset construction.

Abstract: The curation of hate speech datasets involves complex design decisions that
balance competing priorities. This paper critically examines these
methodological choices in a diverse range of datasets, highlighting common
themes and practices, and their implications for dataset reliability. Drawing
on Max Weber's notion of ideal types, we argue for a reflexive approach in
dataset creation, urging researchers to acknowledge their own value judgments
during dataset construction, fostering transparency and methodological rigour.

</details>


### [72] [Comparative Analysis of Abstractive Summarization Models for Clinical Radiology Reports](https://arxiv.org/abs/2506.16247)
*Anindita Bhattacharya,Tohida Rehman,Debarshi Kumar Sanyal,Samiran Chattopadhyay*

Main category: cs.CL

TL;DR: The paper explores using advanced abstractive summarization models to generate concise impressions from detailed findings in radiology reports, comparing various large language models.


<details>
  <summary>Details</summary>
Motivation: Radiology reports often contain lengthy findings, and a compact impression section is more practical for diagnostic purposes. This motivates the need for automated, concise summarization solutions.

Method: The study evaluated several pre-trained and custom summarization models (e.g., T5-base, BART-base, etc.) using the MIMIC-CXR dataset and metrics like ROUGE, METEOR, and BERTScore.

Result: The analysis highlights the strengths and weaknesses of each model in generating concise medical text summaries.

Conclusion: The findings provide insights for medical professionals seeking automated summarization tools in healthcare, guiding informed decisions about their use.

Abstract: The findings section of a radiology report is often detailed and lengthy,
whereas the impression section is comparatively more compact and captures key
diagnostic conclusions. This research explores the use of advanced abstractive
summarization models to generate the concise impression from the findings
section of a radiology report. We have used the publicly available MIMIC-CXR
dataset. A comparative analysis is conducted on leading pre-trained and
open-source large language models, including T5-base, BART-base,
PEGASUS-x-base, ChatGPT-4, LLaMA-3-8B, and a custom Pointer Generator Network
with a coverage mechanism. To ensure a thorough assessment, multiple evaluation
metrics are employed, including ROUGE-1, ROUGE-2, ROUGE-L, METEOR, and
BERTScore. By analyzing the performance of these models, this study identifies
their respective strengths and limitations in the summarization of medical
text. The findings of this paper provide helpful information for medical
professionals who need automated summarization solutions in the healthcare
sector.

</details>


### [73] [End-to-End Speech Translation for Low-Resource Languages Using Weakly Labeled Data](https://arxiv.org/abs/2506.16251)
*Aishwarya Pothula,Bhavana Akkiraju,Srihari Bandarupalli,Charan D,Santosh Kesiraju,Anil Kumar Vuppala*

Main category: cs.CL

TL;DR: This paper investigates the use of weakly labeled data to develop speech-to-text translation systems for low-resource language pairs, achieving performance comparable to established baselines.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the lack of high-quality annotated data for speech-to-text translation systems, especially in low-resource languages.

Method: The authors mined the multilingual Shrutilipi corpus using advanced sentence encoders to construct datasets for low-resource language pairs. They created training data with varied quality and quantity to study its impact on translation models.

Result: The findings show that speech-to-text models built with weakly labeled data perform similarly to large-scale models like SONAR and SeamlessM4T.

Conclusion: Using weakly labeled data is a viable approach to build speech-to-text translation systems for low-resource languages, offering practical alternatives to resource-intensive methods.

Abstract: The scarcity of high-quality annotated data presents a significant challenge
in developing effective end-to-end speech-to-text translation (ST) systems,
particularly for low-resource languages. This paper explores the hypothesis
that weakly labeled data can be used to build ST models for low-resource
language pairs. We constructed speech-to-text translation datasets with the
help of bitext mining using state-of-the-art sentence encoders. We mined the
multilingual Shrutilipi corpus to build Shrutilipi-anuvaad, a dataset
comprising ST data for language pairs Bengali-Hindi, Malayalam-Hindi,
Odia-Hindi, and Telugu-Hindi. We created multiple versions of training data
with varying degrees of quality and quantity to investigate the effect of
quality versus quantity of weakly labeled data on ST model performance. Results
demonstrate that ST systems can be built using weakly labeled data, with
performance comparable to massive multi-modal multilingual baselines such as
SONAR and SeamlessM4T.

</details>


### [74] [Advancing Automated Speaking Assessment Leveraging Multifaceted Relevance and Grammar Information](https://arxiv.org/abs/2506.16285)
*Hao-Chien Lu,Jhen-Ke Lin,Hong-Yun Lin,Chung-Chun Wang,Berlin Chen*

Main category: cs.CL

TL;DR: This paper improves automated speaking assessment (ASA) systems by leveraging detailed content and grammar analysis using images and advanced error correction.


<details>
  <summary>Details</summary>
Motivation: Existing ASA systems lack depth in assessing content relevance and grammar error types, limiting their effectiveness in multi-aspect evaluations.

Method: The paper introduces a hybrid scoring model that integrates multifaceted content analysis (including images, exemplars, and responses) and derives grammar error features using advanced grammar error correction tools.

Result: Experiments show that the proposed components significantly enhance content relevance evaluation, language use analysis, and overall ASA performance.

Conclusion: Rich, nuanced feature sets greatly benefit holistic automated speaking assessments, making evaluations more accurate in multifaceted aspects.

Abstract: Current automated speaking assessment (ASA) systems for use in multi-aspect
evaluations often fail to make full use of content relevance, overlooking image
or exemplar cues, and employ superficial grammar analysis that lacks detailed
error types. This paper ameliorates these deficiencies by introducing two novel
enhancements to construct a hybrid scoring model. First, a multifaceted
relevance module integrates question and the associated image content,
exemplar, and spoken response of an L2 speaker for a comprehensive assessment
of content relevance. Second, fine-grained grammar error features are derived
using advanced grammar error correction (GEC) and detailed annotation to
identify specific error categories. Experiments and ablation studies
demonstrate that these components significantly improve the evaluation of
content relevance, language use, and overall ASA performance, highlighting the
benefits of using richer, more nuanced feature sets for holistic speaking
assessment.

</details>


### [75] [PL-Guard: Benchmarking Language Model Safety for Polish](https://arxiv.org/abs/2506.16322)
*Aleksandra Krasnodębska,Karolina Seweryn,Szymon Łukasik,Wojciech Kusa*

Main category: cs.CL

TL;DR: This paper introduces a benchmark dataset for evaluating the safety of language models in Polish, creates adversarial samples, and fine-tunes models for assessment. The HerBERT-based classifier performs best under adversarial conditions.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the bias in safety assessments of large language models, which predominantly focus on high-resource languages like English, leaving other languages, such as Polish, underexplored.

Method: The authors introduced a new manually annotated benchmark dataset for Polish language safety classification. They created adversarially perturbed samples to test robustness and evaluated various models by fine-tuning and testing them with these datasets.

Result: Experiments demonstrated that the HerBERT-based classifier significantly outperformed other models, particularly in adversarially perturbed scenarios.

Conclusion: High-resource language bias in LLM safety can be mitigated by creating dedicated datasets and training models tailored for underrepresented languages, as demonstrated by the Polish benchmark and the strong performance of the HerBERT-based classifier.

Abstract: Despite increasing efforts to ensure the safety of large language models
(LLMs), most existing safety assessments and moderation tools remain heavily
biased toward English and other high-resource languages, leaving majority of
global languages underexamined. To address this gap, we introduce a manually
annotated benchmark dataset for language model safety classification in Polish.
We also create adversarially perturbed variants of these samples designed to
challenge model robustness. We conduct a series of experiments to evaluate
LLM-based and classifier-based models of varying sizes and architectures.
Specifically, we fine-tune three models: Llama-Guard-3-8B, a HerBERT-based
classifier (a Polish BERT derivative), and PLLuM, a Polish-adapted Llama-8B
model. We train these models using different combinations of annotated data and
evaluate their performance, comparing it against publicly available guard
models. Results demonstrate that the HerBERT-based classifier achieves the
highest overall performance, particularly under adversarial conditions.

</details>


### [76] [Generalizability of Media Frames: Corpus creation and analysis across countries](https://arxiv.org/abs/2506.16337)
*Agnese Daffara,Sourabh Dattawad,Sebastian Padó,Tanise Ceron*

Main category: cs.CL

TL;DR: The paper examines how well the Media Frame Corpus (MFC), designed for U.S. news, generalizes to Brazilian political and economic news using the FrameNews-PT dataset.


<details>
  <summary>Details</summary>
Motivation: To understand whether the existing MFC framework for analyzing political framing in U.S. journalism can be applied effectively to news in other cultural contexts, specifically Brazilian Portuguese.

Method: The authors annotated Brazilian news articles using the MFC framework and conducted several rounds of evaluation, assessing the compatibility of frames and guidelines. They tested machine learning models (fine-tuned and zero-shot) on out-of-domain data.

Result: Most of the 15 MFC frames were broadly applicable, though some required minor revisions. Certain frames were rarely used, and new issues had to be analyzed with generalized fallback options.

Conclusion: Cross-cultural application of media frames needs careful adjustment, indicating that frame categories and guidelines may need tailoring for different cultural and journalistic contexts.

Abstract: Frames capture aspects of an issue that are emphasized in a debate by
interlocutors and can help us understand how political language conveys
different perspectives and ultimately shapes people's opinions. The Media Frame
Corpus (MFC) is the most commonly used framework with categories and detailed
guidelines for operationalizing frames. It is, however, focused on a few
salient U.S. news issues, making it unclear how well these frames can capture
news issues in other cultural contexts. To explore this, we introduce
FrameNews-PT, a dataset of Brazilian Portuguese news articles covering
political and economic news and annotate it within the MFC framework. Through
several annotation rounds, we evaluate the extent to which MFC frames
generalize to the Brazilian debate issues. We further evaluate how fine-tuned
and zero-shot models perform on out-of-domain data. Results show that the 15
MFC frames remain broadly applicable with minor revisions of the guidelines.
However, some MFC frames are rarely used, and novel news issues are analyzed
using general 'fall-back' frames. We conclude that cross-cultural frame use
requires careful consideration.

</details>


### [77] [Analyzing the Influence of Knowledge Graph Information on Relation Extraction](https://arxiv.org/abs/2506.16343)
*Cedric Möller,Ricardo Usbeck*

Main category: cs.CL

TL;DR: This paper investigates the role of knowledge graph information in improving relation extraction models.


<details>
  <summary>Details</summary>
Motivation: To understand whether incorporating knowledge graph positions of entities can enhance relation extraction tasks, particularly under data imbalance.

Method: Evaluation was conducted by incorporating graph-aware Neural Bellman-Ford networks with established relation extraction methods, across multiple datasets and settings.

Result: Incorporating knowledge graph features led to significant and consistent improvements in supervised and zero-shot relation extraction.

Conclusion: Knowledge graph information proves beneficial for relation extraction tasks, especially under imbalanced data scenarios, and provides a robust enhancement to existing models.

Abstract: We examine the impact of incorporating knowledge graph information on the
performance of relation extraction models across a range of datasets. Our
hypothesis is that the positions of entities within a knowledge graph provide
important insights for relation extraction tasks. We conduct experiments on
multiple datasets, each varying in the number of relations, training examples,
and underlying knowledge graphs. Our results demonstrate that integrating
knowledge graph information significantly enhances performance, especially when
dealing with an imbalance in the number of training examples for each relation.
We evaluate the contribution of knowledge graph-based features by combining
established relation extraction methods with graph-aware Neural Bellman-Ford
networks. These features are tested in both supervised and zero-shot settings,
demonstrating consistent performance improvements across various datasets.

</details>


### [78] [DISCIE -- Discriminative Closed Information Extraction](https://arxiv.org/abs/2506.16348)
*Cedric Möller,Ricardo Usbeck*

Main category: cs.CL

TL;DR: The paper presents a novel discriminative method for closed information extraction, improving accuracy and efficiency, especially for long-tail relations.


<details>
  <summary>Details</summary>
Motivation: The motivation is the need to enhance accuracy and efficiency in closed information extraction tasks, particularly with challenges posed by long-tail relations and large-scale datasets.

Method: The method employs a discriminative approach that includes type and entity-specific information, leveraging smaller models to achieve high performance.

Result: The proposed method outperforms state-of-the-art generative models in both accuracy for long-tail relations and efficiency for large-scale closed information extraction scenarios.

Conclusion: Incorporating type-information enables the approach to surpass or match the performance of larger generative models, offering a promising direction for accurate and efficient information extraction.

Abstract: This paper introduces a novel method for closed information extraction. The
method employs a discriminative approach that incorporates type and
entity-specific information to improve relation extraction accuracy,
particularly benefiting long-tail relations. Notably, this method demonstrates
superior performance compared to state-of-the-art end-to-end generative models.
This is especially evident for the problem of large-scale closed information
extraction where we are confronted with millions of entities and hundreds of
relations. Furthermore, we emphasize the efficiency aspect by leveraging
smaller models. In particular, the integration of type-information proves
instrumental in achieving performance levels on par with or surpassing those of
a larger generative model. This advancement holds promise for more accurate and
efficient information extraction techniques.

</details>


### [79] [Can structural correspondences ground real world representational content in Large Language Models?](https://arxiv.org/abs/2506.16370)
*Iwan Williams*

Main category: cs.CL

TL;DR: The paper investigates whether Large Language Models can truly represent worldly entities and examines the role of structural correspondences in grounding representation.


<details>
  <summary>Details</summary>
Motivation: Uncertainty about the representational capacities of LLMs given their text-only training data.

Method: The author evaluates structural-correspondence based accounts of representation and their relation to LLMs in task performance.

Result: Structural correspondences alone cannot ground representation; they must contribute to successful task performance to do so.

Conclusion: LLMs need mechanisms to exploit structural correspondences properly in tasks to ground their representation of real-world entities.

Abstract: Large Language Models (LLMs) such as GPT-4 produce compelling responses to a
wide range of prompts. But their representational capacities are uncertain.
Many LLMs have no direct contact with extra-linguistic reality: their inputs,
outputs and training data consist solely of text, raising the questions (1) can
LLMs represent anything and (2) if so, what? In this paper, I explore what it
would take to answer these questions according to a structural-correspondence
based account of representation, and make an initial survey of this evidence. I
argue that the mere existence of structural correspondences between LLMs and
worldly entities is insufficient to ground representation of those entities.
However, if these structural correspondences play an appropriate role - they
are exploited in a way that explains successful task performance - then they
could ground real world contents. This requires overcoming a challenge: the
text-boundedness of LLMs appears, on the face of it, to prevent them engaging
in the right sorts of tasks.

</details>


### [80] [InstructTTSEval: Benchmarking Complex Natural-Language Instruction Following in Text-to-Speech Systems](https://arxiv.org/abs/2506.16381)
*Kexin Huang,Qian Tu,Liwei Fan,Chenchen Yang,Dong Zhang,Shimin Li,Zhaoye Fei,Qinyuan Cheng,Xipeng Qiu*

Main category: cs.CL

TL;DR: This paper introduces InstructTTSEval, a benchmark for evaluating the capabilities of Text-to-Speech (TTS) systems in understanding and executing complex natural-language instructions. The benchmark focuses on paralinguistic controls and highlights areas for improvement in TTS models.


<details>
  <summary>Details</summary>
Motivation: The authors aim to tackle the limitations in current TTS systems, particularly their restricted flexibility in processing paralinguistic cues and the lack of proper evaluation benchmarks for instruction-based synthesis.

Method: InstructTTSEval is presented as a benchmark that includes three tasks—Acoustic-Parameter Specification, Descriptive-Style Directive, and Role-Play—in both English and Chinese, with 6k test cases and reference audio. An automatic judge named Gemini assesses the performance of TTS models.

Result: The evaluation highlights significant gaps in the instruction-following capabilities of existing TTS models, underscoring the necessity for further improvements in this field.

Conclusion: InstructTTSEval is expected to drive advancements in developing more flexible and accurate TTS systems that better understand complex natural-language instructions.

Abstract: In modern speech synthesis, paralinguistic information--such as a speaker's
vocal timbre, emotional state, and dynamic prosody--plays a critical role in
conveying nuance beyond mere semantics. Traditional Text-to-Speech (TTS)
systems rely on fixed style labels or inserting a speech prompt to control
these cues, which severely limits flexibility. Recent attempts seek to employ
natural-language instructions to modulate paralinguistic features,
substantially improving the generalization of instruction-driven TTS models.
Although many TTS systems now support customized synthesis via textual
description, their actual ability to interpret and execute complex instructions
remains largely unexplored. In addition, there is still a shortage of
high-quality benchmarks and automated evaluation metrics specifically designed
for instruction-based TTS, which hinders accurate assessment and iterative
optimization of these models. To address these limitations, we introduce
InstructTTSEval, a benchmark for measuring the capability of complex
natural-language style control. We introduce three tasks, namely
Acoustic-Parameter Specification, Descriptive-Style Directive, and Role-Play,
including English and Chinese subsets, each with 1k test cases (6k in total)
paired with reference audio. We leverage Gemini as an automatic judge to assess
their instruction-following abilities. Our evaluation of accessible
instruction-following TTS systems highlights substantial room for further
improvement. We anticipate that InstructTTSEval will drive progress toward more
powerful, flexible, and accurate instruction-following TTS.

</details>


### [81] [Large Language Models in Argument Mining: A Survey](https://arxiv.org/abs/2506.16383)
*Hao Li,Viktor Schlegel,Yizheng Sun,Riza Batista-Navarro,Goran Nenadic*

Main category: cs.CL

TL;DR: The paper surveys recent advancements in using Large Language Models (LLMs) for Argument Mining (AM), analyzing their impact on subtasks, evaluation practices, challenges, and future directions for the field.


<details>
  <summary>Details</summary>
Motivation: To explore and synthesize how Large Language Models (LLMs) have transformed the field of Argument Mining by enhancing its methodologies, tools, and adaptability.

Method: The study provides a taxonomy of Argument Mining subtasks influenced by LLMs, reviews foundational theories, datasets, and techniques like prompting and retrieval augmentation, and critically evaluates challenges and trends.

Result: The paper compiles a comprehensive review of LLM techniques applied in Argument Mining, highlights their strengths and limitations, and addresses challenges such as interpretability and context reasoning.

Conclusion: The survey emphasizes the transformative role of LLMs in Argument Mining, outlines unresolved issues, and proposes a research agenda to guide future work in the domain.

Abstract: Argument Mining (AM), a critical subfield of Natural Language Processing
(NLP), focuses on extracting argumentative structures from text. The advent of
Large Language Models (LLMs) has profoundly transformed AM, enabling advanced
in-context learning, prompt-based generation, and robust cross-domain
adaptability. This survey systematically synthesizes recent advancements in
LLM-driven AM. We provide a concise review of foundational theories and
annotation frameworks, alongside a meticulously curated catalog of datasets. A
key contribution is our comprehensive taxonomy of AM subtasks, elucidating how
contemporary LLM techniques -- such as prompting, chain-of-thought reasoning,
and retrieval augmentation -- have reconfigured their execution. We further
detail current LLM architectures and methodologies, critically assess
evaluation practices, and delineate pivotal challenges including long-context
reasoning, interpretability, and annotation bottlenecks. Conclusively, we
highlight emerging trends and propose a forward-looking research agenda for
LLM-based computational argumentation, aiming to strategically guide
researchers in this rapidly evolving domain.

</details>


### [82] [HausaNLP at SemEval-2025 Task 11: Advancing Hausa Text-based Emotion Detection](https://arxiv.org/abs/2506.16388)
*Sani Abdullahi Sani,Salim Abubakar,Falalu Ibrahim Lawan,Abdulhamid Abubakar,Maryam Bala*

Main category: cs.CL

TL;DR: The paper fine-tunes AfriBERTa, a model designed for African languages, to detect emotions in Hausa text.


<details>
  <summary>Details</summary>
Motivation: To improve emotion detection in Hausa, a low-resource language, using advanced NLP techniques.

Method: Fine-tuning AfriBERTa transformer model, with preprocessing, tokenization, and Hugging Face Trainer API.

Result: Achieved a validation accuracy of 74% and an F1-score of 73.5% in emotion detection for Hausa text.

Conclusion: Transformer-based models like AfriBERTa are effective for emotion detection in low-resource languages.

Abstract: This paper presents our approach to multi-label emotion detection in Hausa, a
low-resource African language, as part of SemEval Track A. We fine-tuned
AfriBERTa, a transformer-based model pre-trained on African languages, to
classify Hausa text into six emotions: anger, disgust, fear, joy, sadness, and
surprise. Our methodology involved data preprocessing, tokenization, and model
fine-tuning using the Hugging Face Trainer API. The system achieved a
validation accuracy of 74.00%, with an F1-score of 73.50%, demonstrating the
effectiveness of transformer-based models for emotion detection in low-resource
languages.

</details>


### [83] [RiOT: Efficient Prompt Refinement with Residual Optimization Tree](https://arxiv.org/abs/2506.16389)
*Chenyi Zhou,Zhengyan Shi,Yuan Yao,Lei Liang,Huajun Chen,Qiang Zhang*

Main category: cs.CL

TL;DR: The study introduces Residual Optimization Tree (RiOT) for automatic prompt optimization, overcoming challenges in diversity and semantic drift, and reports superior performance across reasoning benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing automatic prompt optimization techniques lack diversity and face semantic drift, negatively impacting task performance.

Method: RiOT iteratively refines prompts through text gradients and uses a tree structure to manage semantically diverse candidates. It incorporates residual connections to retain beneficial content while selecting optimal prompts using perplexity.

Result: RiOT showed superior results compared to prior optimization methods and manual prompting across five reasoning benchmarks.

Conclusion: RiOT provides an innovative, scalable, and efficient framework for higher-quality prompt optimization in large language models.

Abstract: Recent advancements in large language models (LLMs) have highlighted their
potential across a variety of tasks, but their performance still heavily relies
on the design of effective prompts. Existing methods for automatic prompt
optimization face two challenges: lack of diversity, limiting the exploration
of valuable and innovative directions and semantic drift, where optimizations
for one task can degrade performance in others. To address these issues, we
propose Residual Optimization Tree (RiOT), a novel framework for automatic
prompt optimization. RiOT iteratively refines prompts through text gradients,
generating multiple semantically diverse candidates at each step, and selects
the best prompt using perplexity. Additionally, RiOT incorporates the text
residual connection to mitigate semantic drift by selectively retaining
beneficial content across optimization iterations. A tree structure efficiently
manages the optimization process, ensuring scalability and flexibility.
Extensive experiments across five benchmarks, covering commonsense,
mathematical, logical, temporal, and semantic reasoning, demonstrate that RiOT
outperforms both previous prompt optimization methods and manual prompting.

</details>


### [84] [From LLM-anation to LLM-orchestrator: Coordinating Small Models for Data Labeling](https://arxiv.org/abs/2506.16393)
*Yao Lu,Zhaiyuan Ji,Jiawei Du,Yu Shanqing,Qi Xuan,Tianyi Zhou*

Main category: cs.CL

TL;DR: The paper introduces AutoAnnotator, a fully automatic annotation framework designed to reduce costs and improve accuracy using multi-model cooperation between Large Language Models (LLMs) and Small Language Models (SLMs).


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome two key issues with current LLM-based annotations: prohibitive costs of large-scale commercial API usage and poor accuracy in specialized tasks requiring fine-grained semantic understanding.

Method: AutoAnnotator uses a two-layer structure: an upper layer for managing and verifying annotations using LLMs and a lower layer where multiple SLMs execute annotations through voting. Additionally, fine-tuning of SLMs is achieved via reinforcement learning and continual learning strategies.

Result: AutoAnnotator achieves significant cost savings (74.15%), improved accuracy (+6.21%), and outperforms traditional LLM-based approaches in various settings like zero-shot and few-shot tasks.

Conclusion: The proposed framework offers a cost-effective and more accurate solution for automatic annotation, leveraging the strengths of both LLMs and SLMs collaboratively.

Abstract: Although the annotation paradigm based on Large Language Models (LLMs) has
made significant breakthroughs in recent years, its actual deployment still has
two core bottlenecks: first, the cost of calling commercial APIs in large-scale
annotation is very expensive; second, in scenarios that require fine-grained
semantic understanding, such as sentiment classification and toxicity
classification, the annotation accuracy of LLMs is even lower than that of
Small Language Models (SLMs) dedicated to this field. To address these
problems, we propose a new paradigm of multi-model cooperative annotation and
design a fully automatic annotation framework AutoAnnotator based on this.
Specifically, AutoAnnotator consists of two layers. The upper-level
meta-controller layer uses the generation and reasoning capabilities of LLMs to
select SLMs for annotation, automatically generate annotation code and verify
difficult samples; the lower-level task-specialist layer consists of multiple
SLMs that perform annotation through multi-model voting. In addition, we use
the difficult samples obtained by the secondary review of the meta-controller
layer as the reinforcement learning set and fine-tune the SLMs in stages
through a continual learning strategy, thereby improving the generalization of
SLMs. Extensive experiments show that AutoAnnotator outperforms existing
open-source/API LLMs in zero-shot, one-shot, CoT, and majority voting settings.
Notably, AutoAnnotator reduces the annotation cost by 74.15% compared to
directly annotating with GPT-3.5-turbo, while still improving the accuracy by
6.21%. Project page: https://github.com/Zhaiyuan-Ji/AutoAnnotator.

</details>


### [85] [OJBench: A Competition Level Code Benchmark For Large Language Models](https://arxiv.org/abs/2506.16395)
*Zhexu Wang,Yiping Liu,Yejie Wang,Wenyang He,Bofei Gao,Muxi Diao,Yanxu Chen,Kelin Fu,Flood Sung,Zhilin Yang,Tianyu Liu,Weiran Xu*

Main category: cs.CL

TL;DR: OJBench is a new benchmark for evaluating competitive-level code reasoning in LLMs, consisting of 232 programming competition problems. Evaluation across 37 models revealed ongoing challenges in reasoning for even advanced models.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks are insufficient for thoroughly evaluating LLMs' code reasoning at a competitive level, necessitating a more rigorous test framework.

Method: Developed OJBench, consisting of competitive-level programming problems from NOI and ICPC, and conducted evaluations on 37 LLMs—both reasoning-oriented and not.

Result: Models, including advanced ones like o4-mini and Gemini-2.5-pro-exp, struggled with the competition-level challenges posed by OJBench.

Conclusion: Competitive-level code reasoning remains a significant obstacle for current LLMs, indicating the need for further advancements.

Abstract: Recent advancements in large language models (LLMs) have demonstrated
significant progress in math and code reasoning capabilities. However, existing
code benchmark are limited in their ability to evaluate the full spectrum of
these capabilities, particularly at the competitive level. To bridge this gap,
we introduce OJBench, a novel and challenging benchmark designed to assess the
competitive-level code reasoning abilities of LLMs. OJBench comprises 232
programming competition problems from NOI and ICPC, providing a more rigorous
test of models' reasoning skills. We conducted a comprehensive evaluation using
OJBench on 37 models, including both closed-source and open-source models,
reasoning-oriented and non-reasoning-oriented models. Our results indicate that
even state-of-the-art reasoning-oriented models, such as o4-mini and
Gemini-2.5-pro-exp, struggle with highly challenging competition-level
problems. This highlights the significant challenges that models face in
competitive-level code reasoning.

</details>


### [86] [NepaliGPT: A Generative Language Model for the Nepali Language](https://arxiv.org/abs/2506.16399)
*Shushanta Pudasaini,Aman Shakya,Siddhartha Shrestha,Sahil Bhatta,Sunil Thapa,Sushmita Palikhe*

Main category: cs.CL

TL;DR: This paper introduces NepaliGPT, the first generative LLM specifically designed for the Nepali language, along with a novel corpus and a benchmark dataset.


<details>
  <summary>Details</summary>
Motivation: The lack of generative LLMs tailored for the Nepali language has limited exploration of downstream NLP tasks for this language.

Method: Developed NepaliGPT using the Devanagari Corpus and introduced a benchmark dataset with 4,296 Nepali question-answer pairs.

Result: NepaliGPT demonstrated strong text generation performance with a perplexity of 26.32, ROUGE-1 score of 0.2604, 81.25% causal coherence, and 85.41% causal consistency.

Conclusion: NepaliGPT is a significant advance in Nepali NLP, providing foundational resources and benchmarks for language-specific tasks.

Abstract: After the release of ChatGPT, Large Language Models (LLMs) have gained huge
popularity in recent days and thousands of variants of LLMs have been released.
However, there is no generative language model for the Nepali language, due to
which other downstream tasks, including fine-tuning, have not been explored
yet. To fill this research gap in the Nepali NLP space, this research proposes
\textit{NepaliGPT}, a generative large language model tailored specifically for
the Nepali language. This research introduces an advanced corpus for the Nepali
language collected from several sources, called the Devanagari Corpus.
Likewise, the research introduces the first NepaliGPT benchmark dataset
comprised of 4,296 question-answer pairs in the Nepali language. The proposed
LLM NepaliGPT achieves the following metrics in text generation: Perplexity of
26.32245, ROUGE-1 score of 0.2604, causal coherence of 81.25\%, and causal
consistency of 85.41\%.

</details>


### [87] [When Does Divide and Conquer Work for Long Context LLM? A Noise Decomposition Framework](https://arxiv.org/abs/2506.16411)
*Zhen Xu,Shang Zhu,Jue Wang,Junlin Wang,Ben Athiwaratkun,Chi Wang,James Zou,Ce Zhang*

Main category: cs.CL

TL;DR: The paper addresses the challenge of applying LLMs to long texts, proposing a theoretical framework for categorizing failure modes and analyzing multi-agent chunking strategies.


<details>
  <summary>Details</summary>
Motivation: The authors want to improve how Large Language Models handle long texts by solving challenges related to context limitations, dependency inaccuracies, and aggregation inefficiencies.

Method: The paper introduces a theoretical framework to identify three kinds of noise—task noise, model noise, and aggregator noise—and tests chunk-wise processing using multi-agent systems.

Result: Experiments show that multi-agent chunking can outperform advanced models for large input sizes, particularly when model noise grows superlinearly with context.

Conclusion: The proposed framework and chunking strategies offer a practical method for handling long contexts in LLMs, demonstrating the advantages of structured processing over single-shot application.

Abstract: We investigate the challenge of applying Large Language Models (LLMs) to long
texts. We propose a theoretical framework that distinguishes the failure modes
of long context tasks into three categories: cross-chunk dependence (task
noise), confusion that grows with context size (model noise), and the imperfect
integration of partial results (aggregator noise). Under this view, we analyze
when it is effective to use multi-agent chunking, i.e., dividing a length
sequence into smaller chunks and aggregating the processed results of each
chunk. Our experiments on tasks such as retrieval, question answering, and
summarization confirm both the theoretical analysis and the conditions that
favor multi-agent chunking. By exploring superlinear model noise growth with
input length, we also explain why, for large inputs, a weaker model configured
with chunk-based processing can surpass a more advanced model like GPT4o
applied in a single shot. Overall, we present a principled understanding
framework and our results highlight a direct pathway to handling long contexts
in LLMs with carefully managed chunking and aggregator strategies.

</details>


### [88] [StoryWriter: A Multi-Agent Framework for Long Story Generation](https://arxiv.org/abs/2506.16445)
*Haotian Xia,Hao Peng,Yunjia Qi,Xiaozhi Wang,Bin Xu,Lei Hou,Juanzi Li*

Main category: cs.CL

TL;DR: StoryWriter, a multi-agent framework, enhances long story generation by addressing discourse coherence and narrative complexity, outperforming baselines in quality and length.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenges in generating long-form stories using LLMs, focusing on improving discourse coherence and narrative complexity.

Method: StoryWriter employs a three-module framework: (1) an outline agent for event-based outlines, (2) a planning agent for chapter-level event detailing and arrangement, and (3) a writing agent for dynamic compression and coherent story generation.

Result: StoryWriter outperforms existing baselines in both human and automated evaluations. It also generated a 6,000-story dataset, averaging 8,000 words per story.

Conclusion: StoryWriter significantly improves the quality and complexity of long story generation and demonstrates the framework's effectiveness through robust dataset creation and fine-tuning on advanced LLMs.

Abstract: Long story generation remains a challenge for existing large language models
(LLMs), primarily due to two main factors: (1) discourse coherence, which
requires plot consistency, logical coherence, and completeness in the long-form
generation, and (2) narrative complexity, which requires an interwoven and
engaging narrative. To address these challenges, we propose StoryWriter, a
multi-agent story generation framework, which consists of three main modules:
(1) outline agent, which generates event-based outlines containing rich event
plots, character, and event-event relationships. (2) planning agent, which
further details events and plans which events should be written in each chapter
to maintain an interwoven and engaging story. (3) writing agent, which
dynamically compresses the story history based on the current event to generate
and reflect new plots, ensuring the coherence of the generated story. We
conduct both human and automated evaluation, and StoryWriter significantly
outperforms existing story generation baselines in both story quality and
length. Furthermore, we use StoryWriter to generate a dataset, which contains
about $6,000$ high-quality long stories, with an average length of $8,000$
words. We train the model Llama3.1-8B and GLM4-9B using supervised fine-tuning
on LongStory and develop StoryWriter_GLM and StoryWriter_GLM, which
demonstrates advanced performance in long story generation.

</details>


### [89] [Towards Generalizable Generic Harmful Speech Datasets for Implicit Hate Speech Detection](https://arxiv.org/abs/2506.16476)
*Saad Almohaimeed,Saleh Almohaimeed,Damla Turgut,Ladislau Bölöni*

Main category: cs.CL

TL;DR: This paper addresses detecting implicit hate speech using existing harmful speech datasets, reannotation, and data augmentation techniques to improve detection accuracy.


<details>
  <summary>Details</summary>
Motivation: The paper aims to tackle the challenge of identifying implicit hate speech, which is often veiled and subtle, through a generalizable approach.

Method: The method involves influential sample identification, reannotation, and augmentation using AI models like Llama-3 70B and GPT-4o.

Result: The proposed approach improved implicit hate detection significantly, achieving a +12.9-point F1 score boost over the baseline.

Conclusion: Leveraging existing datasets with advanced techniques can effectively detect implicit hate speech and enhance generalizability across diverse datasets.

Abstract: Implicit hate speech has recently emerged as a critical challenge for social
media platforms. While much of the research has traditionally focused on
harmful speech in general, the need for generalizable techniques to detect
veiled and subtle forms of hate has become increasingly pressing. Based on
lexicon analysis, we hypothesize that implicit hate speech is already present
in publicly available harmful speech datasets but may not have been explicitly
recognized or labeled by annotators. Additionally, crowdsourced datasets are
prone to mislabeling due to the complexity of the task and often influenced by
annotators' subjective interpretations. In this paper, we propose an approach
to address the detection of implicit hate speech and enhance generalizability
across diverse datasets by leveraging existing harmful speech datasets. Our
method comprises three key components: influential sample identification,
reannotation, and augmentation using Llama-3 70B and GPT-4o. Experimental
results demonstrate the effectiveness of our approach in improving implicit
hate detection, achieving a +12.9-point F1 score improvement compared to the
baseline.

</details>


### [90] [Relic: Enhancing Reward Model Generalization for Low-Resource Indic Languages with Few-Shot Examples](https://arxiv.org/abs/2506.16502)
*Soumya Suvra Ghosal,Vaibhav Singh,Akash Ghosh,Soumyabrata Pal,Subhadip Baidya,Sriparna Saha,Dinesh Manocha*

Main category: cs.CL

TL;DR: The paper introduces RELIC, a novel in-context learning approach to improve reward models for low-resource Indic languages.


<details>
  <summary>Details</summary>
Motivation: Existing reward models are unreliable for low-resource Indic languages due to preferences being primarily trained on high-resource languages, and collecting high-quality data for these is costly.

Method: RELIC trains a retriever to select in-context examples from high-resource languages that best distinguish preferred from less-preferred responses.

Result: RELIC significantly improves reward model accuracy for low-resource Indic languages across datasets, achieving substantial accuracy gains, like 12.81% improvement for Bodo language.

Conclusion: RELIC is effective in addressing limitations of multilingual reward modeling for low-resource Indic languages, outperforming prior example selection methods.

Abstract: Reward models are essential for aligning large language models (LLMs) with
human preferences. However, most open-source multilingual reward models are
primarily trained on preference datasets in high-resource languages, resulting
in unreliable reward signals for low-resource Indic languages. Collecting
large-scale, high-quality preference data for these languages is prohibitively
expensive, making preference-based training approaches impractical. To address
this challenge, we propose RELIC, a novel in-context learning framework for
reward modeling in low-resource Indic languages. RELIC trains a retriever with
a pairwise ranking objective to select in-context examples from auxiliary
high-resource languages that most effectively highlight the distinction between
preferred and less-preferred responses. Extensive experiments on three
preference datasets- PKU-SafeRLHF, WebGPT, and HH-RLHF-using state-of-the-art
open-source reward models demonstrate that RELIC significantly improves reward
model accuracy for low-resource Indic languages, consistently outperforming
existing example selection methods. For example, on Bodo-a low-resource Indic
language-using a LLaMA-3.2-3B reward model, RELIC achieves a 12.81% and 10.13%
improvement in accuracy over zero-shot prompting and state-of-the-art example
selection method, respectively.

</details>


### [91] [Automatic Speech Recognition Biases in Newcastle English: an Error Analysis](https://arxiv.org/abs/2506.16558)
*Dana Serditova,Kevin Tang,Jochen Steffens*

Main category: cs.CL

TL;DR: This study highlights the performance issues of ASR systems with Newcastle English due to regional dialectal biases, advocating for more dialectal diversity in training data.


<details>
  <summary>Details</summary>
Motivation: To examine the underexplored issue of regional dialect bias in ASR systems, specifically focusing on Newcastle English.

Method: The study employed a two-stage analysis: (1) a manual analysis of key linguistic errors, and (2) a case study analyzing specific regional pronouns like "yous" and "wor".

Result: Findings revealed that ASR errors were correlated with regional dialectal features, while social factors were less influential.

Conclusion: The paper emphasizes the necessity of including diverse dialectal data in ASR system training and leveraging sociolinguistic analysis for improving regional bias issues.

Abstract: Automatic Speech Recognition (ASR) systems struggle with regional dialects
due to biased training which favours mainstream varieties. While previous
research has identified racial, age, and gender biases in ASR, regional bias
remains underexamined. This study investigates ASR performance on Newcastle
English, a well-documented regional dialect known to be challenging for ASR. A
two-stage analysis was conducted: first, a manual error analysis on a subsample
identified key phonological, lexical, and morphosyntactic errors behind ASR
misrecognitions; second, a case study focused on the systematic analysis of ASR
recognition of the regional pronouns ``yous'' and ``wor''. Results show that
ASR errors directly correlate with regional dialectal features, while social
factors play a lesser role in ASR mismatches. We advocate for greater dialectal
diversity in ASR training data and highlight the value of sociolinguistic
analysis in diagnosing and addressing regional biases.

</details>


### [92] [Weight Factorization and Centralization for Continual Learning in Speech Recognition](https://arxiv.org/abs/2506.16574)
*Enes Yavuz Ugan,Ngoc-Quan Pham,Alexander Waibel*

Main category: cs.CL

TL;DR: This paper proposes a rehearsal-free continual learning approach for speech recognition, inspired by human brain consolidation processes, to prevent catastrophic forgetting in multilingual and language-agnostic settings.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the challenge of continually training modern speech recognition models without relying on the original training data or inducing catastrophic forgetting in multilingual settings.

Method: The method is inspired by human waking-sleeping cycles, proposing two phases: factorization and centralization, which learn and merge knowledge using low-rank adapters.

Result: Experiments on code-switching datasets demonstrated that the centralization phase effectively prevents catastrophic forgetting while consolidating knowledge.

Conclusion: The paper concludes that their proposed approach maintains model integrity during continual learning in challenging multilingual scenarios.

Abstract: Modern neural network based speech recognition models are required to
continually absorb new data without re-training the whole system, especially in
downstream applications using foundation models, having no access to the
original training data. Continually training the models in a rehearsal-free,
multilingual, and language agnostic condition, likely leads to catastrophic
forgetting, when a seemingly insignificant disruption to the weights can
destructively harm the quality of the models. Inspired by the ability of human
brains to learn and consolidate knowledge through the waking-sleeping cycle, we
propose a continual learning approach with two distinct phases: factorization
and centralization, learning and merging knowledge accordingly. Our experiments
on a sequence of varied code-switching datasets showed that the centralization
stage can effectively prevent catastrophic forgetting by accumulating the
knowledge in multiple scattering low-rank adapters.

</details>


### [93] [Streaming Non-Autoregressive Model for Accent Conversion and Pronunciation Improvement](https://arxiv.org/abs/2506.16580)
*Tuan-Nam Nguyen,Ngoc-Quan Pham,Seymanur Akti,Alexander Waibel*

Main category: cs.CL

TL;DR: The paper introduces the first streaming model for accent conversion (AC) that processes non-native speech to sound native-like while preserving key attributes such as speaker identity and prosody.


<details>
  <summary>Details</summary>
Motivation: The motivation is to develop a practical solution for converting non-native speech into native-like accents in real-time while addressing issues such as latency and speaker identity preservation.

Method: The method involves modifying existing AC architecture by integrating an Emformer encoder and optimized inference mechanism, along with employing native-text-to-speech (TTS) models to generate ground-truth training data efficiently.

Result: The model matches the performance of leading accent conversion systems while achieving stable latency ideal for streaming.

Conclusion: This research presents the first successful streaming AC system, marking a significant advancement in real-time speech conversion technologies.

Abstract: We propose a first streaming accent conversion (AC) model that transforms
non-native speech into a native-like accent while preserving speaker identity,
prosody and improving pronunciation. Our approach enables stream processing by
modifying a previous AC architecture with an Emformer encoder and an optimized
inference mechanism. Additionally, we integrate a native text-to-speech (TTS)
model to generate ideal ground-truth data for efficient training. Our streaming
AC model achieves comparable performance to the top AC models while maintaining
stable latency, making it the first AC system capable of streaming.

</details>


### [94] [Measuring (a Sufficient) World Model in LLMs: A Variance Decomposition Framework](https://arxiv.org/abs/2506.16584)
*Nadav Kunievsky,James A. Evans*

Main category: cs.CL

TL;DR: The paper proposes a formal framework and evaluation approach to determine if large language models (LLMs) have a robust world model capable of consistent semantic reasoning across diverse prompts.


<details>
  <summary>Details</summary>
Motivation: To assess LLM reliability, especially in high-stakes applications, by understanding whether they possess structured world models for generalization beyond surface-level patterns.

Method: The paper introduces a framework to assess LLMs by decomposing model response variability into components: user purpose, user articulation, and model instability.

Result: Larger LLMs often attribute more variability to user purpose, suggesting robustness in their world models, albeit inconsistently across domains.

Conclusion: Accuracy-based benchmarks are insufficient; semantic diagnostics better evaluate the structure and stability of LLMs' internal world models.

Abstract: Understanding whether large language models (LLMs) possess a world model-a
structured understanding of the world that supports generalization beyond
surface-level patterns-is central to assessing their reliability, especially in
high-stakes applications. We propose a formal framework for evaluating whether
an LLM exhibits a sufficiently robust world model, defined as producing
consistent outputs across semantically equivalent prompts while distinguishing
between prompts that express different intents. We introduce a new evaluation
approach to measure this that decomposes model response variability into three
components: variability due to user purpose, user articulation, and model
instability. An LLM with a strong world model should attribute most of the
variability in its responses to changes in foundational purpose rather than
superficial changes in articulation. This approach allows us to quantify how
much of a model's behavior is semantically grounded rather than driven by model
instability or alternative wording. We apply this framework to evaluate LLMs
across diverse domains. Our results show how larger models attribute a greater
share of output variability to changes in user purpose, indicating a more
robust world model. This improvement is not uniform, however: larger models do
not consistently outperform smaller ones across all domains, and their
advantage in robustness is often modest. These findings highlight the
importance of moving beyond accuracy-based benchmarks toward semantic
diagnostics that more directly assess the structure and stability of a model's
internal understanding of the world.

</details>


### [95] [A Scoping Review of Synthetic Data Generation for Biomedical Research and Applications](https://arxiv.org/abs/2506.16594)
*Hanshu Rao,Weisi Liu,Haohan Wang,I-Chan Huang,Zhe He,Xiaolei Huang*

Main category: cs.CL

TL;DR: This paper reviews the application of Large Language Models (LLMs) for synthetic data generation in biomedical fields, analyzing methodologies and their challenges across 59 studies.


<details>
  <summary>Details</summary>
Motivation: Address data scarcity, privacy, and quality issues in biomedical research by leveraging advancements in synthetic data generation technologies powered by LLMs.

Method: Scoping review following PRISMA-ScR guidelines; Analyzes 59 studies from 2020 to 2025 sourced from PubMed, ACM, Web of Science, and Google Scholar. Assesses data types, generation methods, and evaluation strategies.

Result: Key findings include dominance of unstructured text data generation (78.0%) and the use of prompting (72.9%) as the main method. Evaluation strategies rely on human-in-the-loop assessments (55.9%), among others.

Conclusion: LLMs show potential for mitigating data issues in biomedical domains but face challenges in domain adaptation, accessibility, and standardization of evaluations.

Abstract: Synthetic data generation--mitigating data scarcity, privacy concerns, and
data quality challenges in biomedical fields--has been facilitated by rapid
advances of large language models (LLMs). This scoping review follows
PRISMA-ScR guidelines and synthesizes 59 studies, published between 2020 and
2025 and collected from PubMed, ACM, Web of Science, and Google Scholar. The
review systematically examines biomedical research and application trends in
synthetic data generation, emphasizing clinical applications, methodologies,
and evaluations. Our analysis identifies data modalities of unstructured texts
(78.0%), tabular data (13.6%), and multimodal sources (8.4%); generation
methods of prompting (72.9%), fine-tuning (22.0%) LLMs and specialized model
(5.1%); and heterogeneous evaluations of intrinsic metrics (27.1%),
human-in-the-loop assessments (55.9%), and LLM-based evaluations (13.6%). The
analysis addresses current limitations in what, where, and how health
professionals can leverage synthetic data generation for biomedical domains.
Our review also highlights challenges in adaption across clinical domains,
resource and model accessibility, and evaluation standardizations.

</details>


### [96] [Modeling Public Perceptions of Science in Media](https://arxiv.org/abs/2506.16622)
*Jiaxin Pei,Dustin Wright,Isabelle Augenstin,David Jurgens*

Main category: cs.CL

TL;DR: The paper presents a computational framework and dataset analyzing public perceptions of science across 12 dimensions, revealing how perception drives online engagement.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge science communicators face in predicting public reactions to scientific news amidst increasing information overload.

Method: The study creates a dataset with 10,489 annotations from 2,101 participants, develops NLP models to predict perception scores, and conducts a natural experiment to link perceptions with engagement on Reddit.

Result: Public perception is primarily driven by science news consumption frequency rather than demographics, and posts with higher perception scores significantly increase engagement online.

Conclusion: Nuanced perception modeling is crucial for effective science communication, enabling predictions of public interest and optimizing engagement with scientific content.

Abstract: Effectively engaging the public with science is vital for fostering trust and
understanding in our scientific community. Yet, with an ever-growing volume of
information, science communicators struggle to anticipate how audiences will
perceive and interact with scientific news. In this paper, we introduce a
computational framework that models public perception across twelve dimensions,
such as newsworthiness, importance, and surprisingness. Using this framework,
we create a large-scale science news perception dataset with 10,489 annotations
from 2,101 participants from diverse US and UK populations, providing valuable
insights into public responses to scientific information across domains. We
further develop NLP models that predict public perception scores with a strong
performance. Leveraging the dataset and model, we examine public perception of
science from two perspectives: (1) Perception as an outcome: What factors
affect the public perception of scientific information? (2) Perception as a
predictor: Can we use the estimated perceptions to predict public engagement
with science? We find that individuals' frequency of science news consumption
is the driver of perception, whereas demographic factors exert minimal
influence. More importantly, through a large-scale analysis and carefully
designed natural experiment on Reddit, we demonstrate that the estimated public
perception of scientific information has direct connections with the final
engagement pattern. Posts with more positive perception scores receive
significantly more comments and upvotes, which is consistent across different
scientific information and for the same science, but are framed differently.
Overall, this research underscores the importance of nuanced perception
modeling in science communication, offering new pathways to predict public
interest and engagement with scientific content.

</details>


### [97] [Initial Investigation of LLM-Assisted Development of Rule-Based Clinical NLP System](https://arxiv.org/abs/2506.16628)
*Jianlin Shi,Brian T. Bucher*

Main category: cs.CL

TL;DR: The paper introduces a method to boost rule-based NLP system development using LLMs, achieving high recall and efficiency in clinical text analysis tasks.


<details>
  <summary>Details</summary>
Motivation: Rule-based NLP systems are favored in clinical settings for their interpretability and efficiency but require significant manual effort for development and maintenance, particularly with diverse linguistic inputs.

Method: The authors propose using LLMs for the developmental phase of rule-based NLP systems, focusing on identifying relevant clinical text snippets and extracting keywords for NER.

Result: Experiments showed robust performance, with a recall of 0.98 and 0.99 for text snippet identification (Deepseek, Qwen) and 1.0 for keyword extraction for NER.

Conclusion: This approach enables more efficient, cost-effective, and transparent development of rule-based NLP systems, offering a less labor-intensive alternative compared to traditional methods based on deep learning models.

Abstract: Despite advances in machine learning (ML) and large language models (LLMs),
rule-based natural language processing (NLP) systems remain active in clinical
settings due to their interpretability and operational efficiency. However,
their manual development and maintenance are labor-intensive, particularly in
tasks with large linguistic variability. To overcome these limitations, we
proposed a novel approach employing LLMs solely during the rule-based systems
development phase. We conducted the initial experiments focusing on the first
two steps of developing a rule-based NLP pipeline: find relevant snippets from
the clinical note; extract informative keywords from the snippets for the
rule-based named entity recognition (NER) component. Our experiments
demonstrated exceptional recall in identifying clinically relevant text
snippets (Deepseek: 0.98, Qwen: 0.99) and 1.0 in extracting key terms for NER.
This study sheds light on a promising new direction for NLP development,
enabling semi-automated or automated development of rule-based systems with
significantly faster, more cost-effective, and transparent execution compared
with deep learning model-based solutions.

</details>


### [98] [GeoGuess: Multimodal Reasoning based on Hierarchy of Visual Information in Street View](https://arxiv.org/abs/2506.16633)
*Fenghua Cheng,Jinxiang Wang,Sen Wang,Zi Huang,Xue Li*

Main category: cs.CL

TL;DR: This paper introduces GeoGuess, a new multimodal reasoning task that leverages hierarchical visual cues and geographic knowledge to identify street view locations and explain decisions. A benchmark dataset called GeoExplain and a reasoning method named SightSense are also proposed.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the limitation in multimodal reasoning tasks which lack focus on hierarchical visual clues and their integration with geographic knowledge, frequently required in real-world scenarios.

Method: The authors introduce GeoGuess, supported by the curated GeoExplain dataset, and propose SightSense, a multimodal reasoning method that utilizes hierarchical visual information and external geographic knowledge for prediction and explanation.

Result: The experiments and analysis highlight SightSense's exceptional performance in tackling the GeoGuess task, demonstrating its effectiveness in multimodal and multilevel reasoning.

Conclusion: The study contributes a novel task, dataset, and solution method to the field of multimodal reasoning, promoting advancements in understanding hierarchical visual information and geographic data.

Abstract: Multimodal reasoning is a process of understanding, integrating and inferring
information across different data modalities. It has recently attracted surging
academic attention as a benchmark for Artificial Intelligence (AI). Although
there are various tasks for evaluating multimodal reasoning ability, they still
have limitations. Lack of reasoning on hierarchical visual clues at different
levels of granularity, e.g., local details and global context, is of little
discussion, despite its frequent involvement in real scenarios. To bridge the
gap, we introduce a novel and challenging task for multimodal reasoning, namely
GeoGuess. Given a street view image, the task is to identify its location and
provide a detailed explanation. A system that succeeds in GeoGuess should be
able to detect tiny visual clues, perceive the broader landscape, and associate
with vast geographic knowledge. Therefore, GeoGuess would require the ability
to reason between hierarchical visual information and geographic knowledge. In
this work, we establish a benchmark for GeoGuess by introducing a specially
curated dataset GeoExplain which consists of
panoramas-geocoordinates-explanation tuples. Additionally, we present a
multimodal and multilevel reasoning method, namely SightSense which can make
prediction and generate comprehensive explanation based on hierarchy of visual
information and external knowledge. Our analysis and experiments demonstrate
their outstanding performance in GeoGuess.

</details>


### [99] [Long-Context Generalization with Sparse Attention](https://arxiv.org/abs/2506.16640)
*Pavlo Vasylenko,Marcos Treviso,André F. T. Martins*

Main category: cs.CL

TL;DR: Transformer-based architectures often rely on dense attention mechanisms like softmax, which may hinder tasks requiring precision in fixed-size patterns. This paper introduces ASEntmax—a sparse attention method with a learnable temperature parameter—and demonstrates its superiority in handling long-context generalization.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of dense attention mechanisms in transformer architectures, particularly for tasks requiring precise focus on fixed-size patterns and avoiding representational collapse in long sequences.

Method: The authors employ $\,alpha$-entmax sparse attention mechanisms that assign zero probabilities to irrelevant tokens, coupled with ASEntmax—a version with a learnable temperature parameter—and optimize positional encodings to improve model performance.

Result: ASEntmax, when integrated with transformers and well-designed positional encodings, outperforms dense attention techniques like softmax and other baseline models in long-context generalization tasks.

Conclusion: Sparse attention mechanisms coupled with adaptive temperature scaling and optimized positional encodings present a more effective solution for long-context patterns in transformer models, outperforming traditional approaches.

Abstract: Transformer-based architectures traditionally employ softmax to compute
attention weights, which produces dense distributions over all tokens in a
sequence. While effective in many settings, this density has been shown to be
detrimental for tasks that demand precise focus on fixed-size patterns: as
sequence length increases, non-informative tokens accumulate attention
probability mass, leading to dispersion and representational collapse. We show
in this paper that sparse attention mechanisms using $\alpha$-entmax can avoid
these issues, due to their ability to assign exact zeros to irrelevant tokens.
Furthermore, we introduce Adaptive-Scalable Entmax (ASEntmax), which endows
$\alpha$-entmax with a learnable temperature parameter, allowing the attention
distribution to interpolate between sparse (pattern-focused) and dense
(softmax-like) regimes. Finally, we show that the ability to locate and
generalize fixed-size patterns can be further improved through a careful design
of position encodings, which impacts both dense and sparse attention methods.
By integrating ASEntmax into standard transformer layers alongside proper
positional encodings, we show that our models greatly outperform softmax,
scalable softmax, and fixed-temperature $\alpha$-entmax baselines on
long-context generalization.

</details>


### [100] [Arch-Router: Aligning LLM Routing with Human Preferences](https://arxiv.org/abs/2506.16655)
*Co Tran,Salman Paracha,Adil Hafeez,Shuguang Chen*

Main category: cs.CL

TL;DR: The paper introduces Arch-Router, a 1.5B model designed for routing queries to large language models (LLMs) based on user-defined preferences, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Routing among LLMs has become crucial, but current methods either fail to reflect subjective human preferences or limit the range of models. This paper aims to address these limitations.

Method: The authors propose Arch-Router, a compact 1.5B model that maps queries to domain-action preferences, supports flexible model integration, and aligns routing decisions with human preferences.

Result: Arch-Router achieves state-of-the-art results on conversational datasets, outperforming leading models in aligning with human preferences.

Conclusion: Arch-Router enhances LLM routing by capturing subjective criteria, allowing transparent, flexible decisions and supporting the addition of new models without retraining.

Abstract: With the rapid proliferation of large language models (LLMs) -- each
optimized for different strengths, style, or latency/cost profile -- routing
has become an essential technique to operationalize the use of different
models. However, existing LLM routing approaches are limited in two key ways:
they evaluate performance using benchmarks that often fail to capture human
preferences driven by subjective evaluation criteria, and they typically select
from a limited pool of models. In this work, we propose a preference-aligned
routing framework that guides model selection by matching queries to
user-defined domains (e.g., travel) or action types (e.g., image editing) --
offering a practical mechanism to encode preferences in routing decisions.
Specifically, we introduce \textbf{Arch-Router}, a compact 1.5B model that
learns to map queries to domain-action preferences for model routing decisions.
Our approach also supports seamlessly adding new models for routing without
requiring retraining or architectural modifications. Experiments on
conversational datasets demonstrate that our approach achieves state-of-the-art
(SOTA) results in matching queries with human preferences, outperforming top
proprietary models. Our approach captures subjective evaluation criteria and
makes routing decisions more transparent and flexible. Our model is available
at: \texttt{https://huggingface.co/katanemo/Arch-Router-1.5B}.

</details>


### [101] [Mechanisms vs. Outcomes: Probing for Syntax Fails to Explain Performance on Targeted Syntactic Evaluations](https://arxiv.org/abs/2506.16678)
*Ananth Agarwal,Jasper Jian,Christopher D. Manning,Shikhar Murty*

Main category: cs.CL

TL;DR: The paper evaluates the connection between syntax probing results and downstream model performance in language tasks, revealing a disconnect.


<details>
  <summary>Details</summary>
Motivation: Understand how large language models internally represent syntax and whether probing methods predict syntactic behaviors.

Method: The study analyzed 32 transformer models using probing techniques and downstream syntactic evaluations.

Result: Probing results did not reliably predict syntax-related performance in downstream tasks, highlighting disparities between probing findings and actual behaviors.

Conclusion: There isn't a strong link between latent syntactic representations found via probing and syntactic performance in downstream tasks, questioning probing's reliability in interpretability.

Abstract: Large Language Models (LLMs) exhibit a robust mastery of syntax when
processing and generating text. While this suggests internalized understanding
of hierarchical syntax and dependency relations, the precise mechanism by which
they represent syntactic structure is an open area within interpretability
research. Probing provides one way to identify the mechanism of syntax being
linearly encoded in activations, however, no comprehensive study has yet
established whether a model's probing accuracy reliably predicts its downstream
syntactic performance. Adopting a "mechanisms vs. outcomes" framework, we
evaluate 32 open-weight transformer models and find that syntactic features
extracted via probing fail to predict outcomes of targeted syntax evaluations
across English linguistic phenomena. Our results highlight a substantial
disconnect between latent syntactic representations found via probing and
observable syntactic behaviors in downstream tasks.

</details>


### [102] [LegiGPT: Party Politics and Transport Policy with Large Language Model](https://arxiv.org/abs/2506.16692)
*Hyunsoo Yun,Eun Hak Lee*

Main category: cs.CL

TL;DR: The paper introduces LegiGPT, an AI framework analyzing legislative factors influencing transportation policymaking using GPT-4 and XAI techniques.


<details>
  <summary>Details</summary>
Motivation: The study aims to understand how political ideologies influence legislative decision-making, particularly relating to transportation policy.

Method: LegiGPT integrates GPT-4 for bill classification and XAI for analyzing legislative factors, utilizing multi-stage filtering methods on legislative proposals from South Korea's National Assembly.

Result: The analysis found that sponsor political affiliation, district size, and electoral population significantly influence transportation-related legislative outcomes.

Conclusion: The findings highlight bipartisan contributions in legislative dynamics and suggest LegiGPT's utility for infrastructure governance and policymaking insights.

Abstract: Given the significant influence of lawmakers' political ideologies on
legislative decision-making, understanding their impact on policymaking is
critically important. We introduce a novel framework, LegiGPT, which integrates
a large language model (LLM) with explainable artificial intelligence (XAI) to
analyze transportation-related legislative proposals. LegiGPT employs a
multi-stage filtering and classification pipeline using zero-shot prompting
with GPT-4. Using legislative data from South Korea's 21st National Assembly,
we identify key factors - including sponsor characteristics, political
affiliations, and geographic variables - that significantly influence
transportation policymaking. The LLM was used to classify
transportation-related bill proposals through a stepwise filtering process
based on keywords, phrases, and contextual relevance. XAI techniques were then
applied to examine relationships between party affiliation and associated
attributes. The results reveal that the number and proportion of conservative
and progressive sponsors, along with district size and electoral population,
are critical determinants shaping legislative outcomes. These findings suggest
that both parties contributed to bipartisan legislation through different forms
of engagement, such as initiating or supporting proposals. This integrated
approach provides a valuable tool for understanding legislative dynamics and
guiding future policy development, with broader implications for infrastructure
planning and governance.

</details>


### [103] [ReasonGRM: Enhancing Generative Reward Models through Large Reasoning Models](https://arxiv.org/abs/2506.16712)
*Bin Chen,Xinzge Gao,Chuanrui Hu,Penghang Yu,Hua Zhang,Bing-Kun Bao*

Main category: cs.CL

TL;DR: The paper introduces ReasonGRM, a generative reward modeling framework that improves preference modeling through reasoning-aware training. It combines three innovative stages to enhance reasoning paths while reducing hallucination and critical omissions.


<details>
  <summary>Details</summary>
Motivation: Existing Generative Reward Models (GRMs) struggle with poor reasoning capabilities, leading to errors like hallucinations and omission of key information. This paper aims to address these limitations to improve preference modeling in complex tasks.

Method: The framework ReasonGRM consists of: (1) Zero-RL for generating concise reasoning paths, (2) a novel evaluation metric $R^\star$ to score paths by generation likelihood, and (3) reinforcement learning to refine the model using challenging examples.

Result: ReasonGRM demonstrated state-of-the-art or competitive performance on three benchmarks, outperforming prior GRMs by an average of 1.8%, and surpassing GPT-4o by up to 5.6% in preference modeling tasks.

Conclusion: ReasonGRM effectively addresses reasoning shortcomings in GRMs, proving the importance of rationale selection and reasoning-aware training for reliable human preference modeling.

Abstract: Generative Reward Models (GRMs) provide greater flexibility than scalar
reward models in capturing human preferences, but their effectiveness is
limited by poor reasoning capabilities. This often results in incomplete or
overly speculative reasoning paths, leading to hallucinations or missing key
information in complex tasks. We address this challenge with ReasonGRM, a
three-stage generative reward modeling framework. In the first stage, Zero-RL
is used to generate concise, outcome-directed reasoning paths that reduce the
likelihood of critical omissions. In the second stage, we introduce a novel
evaluation metric, $R^\star$, which scores reasoning paths based on their
generation likelihood. This favors paths that reach correct answers with
minimal exploration, helping to reduce hallucination-prone data during
training. In the final stage, the model is further refined through
reinforcement learning on challenging examples to enhance its preference
discrimination capabilities. Experiments on three public benchmarks show that
ReasonGRM achieves competitive or state-of-the-art performance, outperforming
previous best GRMs by 1.8\% on average and surpassing proprietary models such
as GPT-4o by up to 5.6\%. These results demonstrate the effectiveness of
reasoning-aware training and highlight the importance of high-quality rationale
selection for reliable preference modeling.

</details>


### [104] [The Role of Model Confidence on Bias Effects in Measured Uncertainties](https://arxiv.org/abs/2506.16724)
*Xinyi Liu,Weiguang Wang,Hangfeng He*

Main category: cs.CL

TL;DR: The paper investigates the trade-off between mitigating bias and quantifying epistemic uncertainty in LLMs during open-ended tasks, using experiments on VQA.


<details>
  <summary>Details</summary>
Motivation: The need to accurately assess epistemic uncertainty in tasks performed by Large Language Models has grown, especially as these models are increasingly adopted for ensuring reliable outcomes.

Method: Experiments were conducted with GPT-4o and Qwen2-VL on VQA tasks to analyze how prompt biases affect the measurement of epistemic and aleatoric uncertainties, especially at varying levels of model confidence.

Result: Mitigating prompt bias improves epistemic uncertainty quantification. Bias has stronger impacts on uncertainty metrics at lower confidence levels, leading to underestimation (overconfidence) of epistemic uncertainty.

Conclusion: Understanding the distinct effects of bias on uncertainty can guide the development of more advanced methods for managing epistemic and aleatoric uncertainties in Large Language Models.

Abstract: With the growing adoption of Large Language Models (LLMs) for open-ended
tasks, accurately assessing epistemic uncertainty, which reflects a model's
lack of knowledge, has become crucial to ensuring reliable outcomes. However,
quantifying epistemic uncertainty in such tasks is challenging due to the
presence of aleatoric uncertainty, which arises from multiple valid answers.
While bias can introduce noise into epistemic uncertainty estimation, it may
also reduce noise from aleatoric uncertainty. To investigate this trade-off, we
conduct experiments on Visual Question Answering (VQA) tasks and find that
mitigating prompt-introduced bias improves uncertainty quantification in
GPT-4o. Building on prior work showing that LLMs tend to copy input information
when model confidence is low, we further analyze how these prompt biases affect
measured epistemic and aleatoric uncertainty across varying bias-free
confidence levels with GPT-4o and Qwen2-VL. We find that all considered biases
induce greater changes in both uncertainties when bias-free model confidence is
lower. Moreover, lower bias-free model confidence leads to greater
underestimation of epistemic uncertainty (i.e. overconfidence) due to bias,
whereas it has no significant effect on the direction of changes in aleatoric
uncertainty estimation. These distinct effects deepen our understanding of bias
mitigation for uncertainty quantification and potentially inform the
development of more advanced techniques.

</details>


### [105] [LM-SPT: LM-Aligned Semantic Distillation for Speech Tokenization](https://arxiv.org/abs/2506.16738)
*Daejin Jo,Jeeyoung Yun,Byungseok Roh,Sungwoong Kim*

Main category: cs.CL

TL;DR: The paper proposes LM-SPT, a novel speech tokenization approach aimed at better aligning semantic tokens with language models while reducing speech token sequence lengths for efficient speech-language modeling.


<details>
  <summary>Details</summary>
Motivation: The authors aim to create a speech tokenization method that better aligns semantic tokens with language models while addressing inefficiencies caused by long speech token sequences.

Method: The method introduces semantic distillation where speech tokens are learned indirectly by minimizing discrepancies between speech waveforms reconstructed from semantic tokens and their original encoded representations. The system also includes architectural improvements and supports multiple frame rates.

Result: Experimental results demonstrate that LM-SPT achieves superior reconstruction fidelity and enables speech-language models (SLMs) to perform competitively on speech-to-text tasks and outperform baselines on text-to-speech tasks.

Conclusion: LM-SPT provides a more efficient and semantically aligned speech tokenization approach that benefits speech-language modeling across tasks, addressing issues in prior tokenization methods.

Abstract: With the rapid progress of speech language models (SLMs), discrete speech
tokens have emerged as a core interface between speech and text, enabling
unified modeling across modalities. Recent speech tokenization approaches aim
to isolate semantic information from low-level acoustics to better align with
language models. In particular, previous methods use SSL teachers such as
HuBERT to extract semantic representations, which are then distilled into a
semantic quantizer to suppress acoustic redundancy as well as capture
content-related latent structures. However, they still produce speech token
sequences significantly longer than their textual counterparts, creating
challenges for efficient speech-language modeling. Reducing the frame rate is a
natural solution, but standard techniques, such as rigid average pooling across
frames, can distort or dilute the semantic structure required for effective LM
alignment. To address this, we propose LM-SPT, a speech tokenization method
that introduces a novel semantic distillation. Instead of directly matching
teacher and student features via pooling, we reconstruct speech solely from
semantic tokens and minimize the discrepancy between the encoded
representations of the original and reconstructed waveforms, obtained from a
frozen automatic speech recognition (ASR) encoder. This indirect yet
data-driven supervision enables the tokenizer to learn discrete units that are
more semantically aligned with language models. LM-SPT further incorporates
architectural improvements to the encoder and decoder for speech tokenization,
and supports multiple frame rates, including 25Hz, 12.5Hz, and 6.25Hz.
Experimental results show that LM-SPT achieves superior reconstruction fidelity
compared to baselines, and that SLMs trained with LM-SPT tokens achieve
competitive performances on speech-to-text and consistently outperform
baselines on text-to-speech tasks.

</details>


### [106] [Language-Informed Synthesis of Rational Agent Models for Grounded Theory-of-Mind Reasoning On-The-Fly](https://arxiv.org/abs/2506.16755)
*Lance Ying,Ryan Truong,Katherine M. Collins,Cedegao E. Zhang,Megan Wei,Tyler Brooke-Wilson,Tan Zhi-Xuan,Lionel Wong,Joshua B. Tenenbaum*

Main category: cs.CL

TL;DR: The paper introduces LIRAS, a framework combining language and visual inputs to perform social reasoning and achieve state-of-the-art performance on related tasks.


<details>
  <summary>Details</summary>
Motivation: Making social inferences in real-world scenarios requires synthesizing information from multiple modalities, particularly language, which provides abstract and concrete insights.

Method: The LIRAS framework uses multimodal language models to process language and visual data into symbolic representations and apply Bayesian inverse planning for probabilistic reasoning.

Result: LIRAS outperformed existing models and baselines across various cognitive science-based social reasoning tasks.

Conclusion: LIRAS effectively integrates multimodal inputs for context-specific social inferences, bridging human-like judgment capabilities in machines.

Abstract: Drawing real world social inferences usually requires taking into account
information from multiple modalities. Language is a particularly powerful
source of information in social settings, especially in novel situations where
language can provide both abstract information about the environment dynamics
and concrete specifics about an agent that cannot be easily visually observed.
In this paper, we propose Language-Informed Rational Agent Synthesis (LIRAS), a
framework for drawing context-specific social inferences that integrate
linguistic and visual inputs. LIRAS frames multimodal social reasoning as a
process of constructing structured but situation-specific agent and environment
representations - leveraging multimodal language models to parse language and
visual inputs into unified symbolic representations, over which a Bayesian
inverse planning engine can be run to produce granular probabilistic judgments.
On a range of existing and new social reasoning tasks derived from cognitive
science experiments, we find that our model (instantiated with a comparatively
lightweight VLM) outperforms ablations and state-of-the-art models in capturing
human judgments across all domains.

</details>


### [107] [SocialSim: Towards Socialized Simulation of Emotional Support Conversation](https://arxiv.org/abs/2506.16756)
*Zhuang Chen,Yaru Cao,Guanqun Bi,Jincenzi Wu,Jinfeng Zhou,Xiyao Xiao,Si Chen,Hongning Wang,Minlie Huang*

Main category: cs.CL

TL;DR: The paper introduces SocialSim, a framework for simulating emotional support conversations by focusing on social dynamics, resulting in a high-quality synthetic dataset (SSConv) and a chatbot outperforming existing models.


<details>
  <summary>Details</summary>
Motivation: Emotional support conversations are vital for psychological well-being, but collecting a large corpus is expensive and existing augmentation approaches fail to consider social dynamics.

Method: SocialSim combines seeker persona banks for simulating diverse help-seeking scenarios and supporter cognitive reasoning for logical responses, creating the SSConv dataset.

Result: SSConv, the synthetic ESC corpus, surpasses traditional crowdsourced corpora and enables a chatbot to achieve state-of-the-art performance in evaluations.

Conclusion: SocialSim provides a scalable, high-quality method to synthesize emotional support conversations, improving chatbot capabilities and accessibility to emotional care.

Abstract: Emotional support conversation (ESC) helps reduce people's psychological
stress and provide emotional value through interactive dialogues. Due to the
high cost of crowdsourcing a large ESC corpus, recent attempts use large
language models for dialogue augmentation. However, existing approaches largely
overlook the social dynamics inherent in ESC, leading to less effective
simulations. In this paper, we introduce SocialSim, a novel framework that
simulates ESC by integrating key aspects of social interactions: social
disclosure and social awareness. On the seeker side, we facilitate social
disclosure by constructing a comprehensive persona bank that captures diverse
and authentic help-seeking scenarios. On the supporter side, we enhance social
awareness by eliciting cognitive reasoning to generate logical and supportive
responses. Building upon SocialSim, we construct SSConv, a large-scale
synthetic ESC corpus of which quality can even surpass crowdsourced ESC data.
We further train a chatbot on SSConv and demonstrate its state-of-the-art
performance in both automatic and human evaluations. We believe SocialSim
offers a scalable way to synthesize ESC, making emotional care more accessible
and practical.

</details>


### [108] [Cross-Modal Obfuscation for Jailbreak Attacks on Large Vision-Language Models](https://arxiv.org/abs/2506.16760)
*Lei Jiang,Zixun Zhang,Zizhou Wang,Xiaobing Sun,Zhen Li,Liangli Zhen,Xiaohua Xu*

Main category: cs.CL

TL;DR: This paper introduces CAMO, a stealthy and computationally efficient jailbreak attack method for large Vision-Language Models (LVLMs), exploiting their cross-modal reasoning abilities.


<details>
  <summary>Details</summary>
Motivation: The motivation is to highlight and exploit the vulnerabilities in the built-in safety mechanisms of LVLMs, which are susceptible to jailbreak attacks.

Method: The CAMO framework decomposes malicious prompts into benign visual and textual parts, which are reconstructed using LVLMs' cross-modal reasoning during a multi-step process, evading detection systems.

Result: CAMO demonstrates high performance, robustness, and cross-model transferability while reducing query requirements compared to prior approaches.

Conclusion: Current LVLM safety mechanisms require better alignment-aware security solutions as they remain vulnerable to sophisticated attacks like CAMO.

Abstract: Large Vision-Language Models (LVLMs) demonstrate exceptional performance
across multimodal tasks, yet remain vulnerable to jailbreak attacks that bypass
built-in safety mechanisms to elicit restricted content generation. Existing
black-box jailbreak methods primarily rely on adversarial textual prompts or
image perturbations, yet these approaches are highly detectable by standard
content filtering systems and exhibit low query and computational efficiency.
In this work, we present Cross-modal Adversarial Multimodal Obfuscation (CAMO),
a novel black-box jailbreak attack framework that decomposes malicious prompts
into semantically benign visual and textual fragments. By leveraging LVLMs'
cross-modal reasoning abilities, CAMO covertly reconstructs harmful
instructions through multi-step reasoning, evading conventional detection
mechanisms. Our approach supports adjustable reasoning complexity and requires
significantly fewer queries than prior attacks, enabling both stealth and
efficiency. Comprehensive evaluations conducted on leading LVLMs validate
CAMO's effectiveness, showcasing robust performance and strong cross-model
transferability. These results underscore significant vulnerabilities in
current built-in safety mechanisms, emphasizing an urgent need for advanced,
alignment-aware security and safety solutions in vision-language systems.

</details>


### [109] [DistillNote: LLM-based clinical note summaries improve heart failure diagnosis](https://arxiv.org/abs/2506.16777)
*Heloisa Oss Boll,Antonio Oss Boll,Leticia Puttlitz Boll,Ameen Abu Hanna,Iacer Calixto*

Main category: cs.CL

TL;DR: Distillnote employs large language models (LLMs) for clinical note summarization, emphasizing three techniques: one-step, structured, and distilled summarization. It shows enhanced efficiency and heart failure prediction performance.


<details>
  <summary>Details</summary>
Motivation: Healthcare providers face difficulties managing patient documentation. Large language models (LLMs) can potentially ease this burden by creating concise summaries of clinical notes.

Method: The study explores three techniques for clinical note summarization using LLMs: direct one-step summarization, structured summarization focusing on independent clinical insights, and distilled summarization, which condenses structured summaries further.

Result: Distilled summaries provide a 79% text compression rate and improve the AUPRC for heart failure prediction by up to 18.2%. Evaluations reveal that clinicians favor one-step summaries for relevance and actionability, while distilled summaries excel in efficiency and hallucination reduction.

Conclusion: The study demonstrates the utility of LLMs in clinical note summarization, achieving enhanced efficiency and diagnostic performance. Summaries are shared on PhysioNet to support additional research.

Abstract: Large language models (LLMs) offer unprecedented opportunities to generate
concise summaries of patient information and alleviate the burden of clinical
documentation that overwhelms healthcare providers. We present Distillnote, a
framework for LLM-based clinical note summarization, and generate over 64,000
admission note summaries through three techniques: (1) One-step, direct
summarization, and a divide-and-conquer approach involving (2) Structured
summarization focused on independent clinical insights, and (3) Distilled
summarization that further condenses the Structured summaries. We test how
useful are the summaries by using them to predict heart failure compared to a
model trained on the original notes. Distilled summaries achieve 79% text
compression and up to 18.2% improvement in AUPRC compared to an LLM trained on
the full notes. We also evaluate the quality of the generated summaries in an
LLM-as-judge evaluation as well as through blinded pairwise comparisons with
clinicians. Evaluations indicate that one-step summaries are favoured by
clinicians according to relevance and clinical actionability, while distilled
summaries offer optimal efficiency (avg. 6.9x compression-to-performance ratio)
and significantly reduce hallucinations. We release our summaries on PhysioNet
to encourage future research.

</details>


### [110] [MIST: Jailbreaking Black-box Large Language Models via Iterative Semantic Tuning](https://arxiv.org/abs/2506.16792)
*Muyang Zheng,Yuanzhi Yao,Changting Lin,Rui Wang,Meng Han*

Main category: cs.CL

TL;DR: MIST is a method to jailbreak black-box LLMs by refining prompts to induce harmful responses using semantic tuning.


<details>
  <summary>Details</summary>
Motivation: Address the challenges of jailbreaking black-box LLMs, such as limited access and query budgets, while maintaining semantic integrity.

Method: Uses Iterative Semantic Tuning (MIST) combining sequential synonym search and order-determining optimization for efficiency.

Result: MIST achieved competitive attack success rates across six LLMs and demonstrated effectiveness in computational efficiency.

Conclusion: MIST is a practical, efficient, and transferable method for black-box LLM jailbreaking.

Abstract: Despite efforts to align large language models (LLMs) with societal and moral
values, these models remain susceptible to jailbreak attacks--methods designed
to elicit harmful responses. Jailbreaking black-box LLMs is considered
challenging due to the discrete nature of token inputs, restricted access to
the target LLM, and limited query budget. To address the issues above, we
propose an effective method for jailbreaking black-box large language Models
via Iterative Semantic Tuning, named MIST. MIST enables attackers to
iteratively refine prompts that preserve the original semantic intent while
inducing harmful content. Specifically, to balance semantic similarity with
computational efficiency, MIST incorporates two key strategies: sequential
synonym search, and its advanced version--order-determining optimization.
Extensive experiments across two open-source models and four closed-source
models demonstrate that MIST achieves competitive attack success rates and
attack transferability compared with other state-of-the-art white-box and
black-box jailbreak methods. Additionally, we conduct experiments on
computational efficiency to validate the practical viability of MIST.

</details>


### [111] [From Data to Knowledge: Evaluating How Efficiently Language Models Learn Facts](https://arxiv.org/abs/2506.16912)
*Daniel Christoph,Max Ploner,Patrick Haller,Alan Akbik*

Main category: cs.CL

TL;DR: This paper explores the sample efficiency of language models, focusing on their ability to learn and recall rare relational facts based on their frequency during training.


<details>
  <summary>Details</summary>
Motivation: To understand how language models learn rare information and how architecture and size impact their ability to recall facts with varying frequencies.

Method: Authors analyzed models of different architectures and sizes, annotated relational facts with their training corpus frequencies, and measured performance specifically for high- and low-frequency facts.

Result: Most models performed similarly on high-frequency facts but showed significant variance in performance on low-frequency facts.

Conclusion: Insights were gained into how differences in architecture and size influence a model’s efficiency in learning rare relational facts.

Abstract: Sample efficiency is a crucial property of language models with practical
implications for training efficiency. In real-world text, information follows a
long-tailed distribution. Yet, we expect models to learn and recall frequent
and infrequent facts. Sample-efficient models are better equipped to handle
this challenge of learning and retaining rare information without requiring
excessive exposure. This study analyzes multiple models of varying
architectures and sizes, all trained on the same pre-training data. By
annotating relational facts with their frequencies in the training corpus, we
examine how model performance varies with fact frequency. Our findings show
that most models perform similarly on high-frequency facts but differ notably
on low-frequency facts. This analysis provides new insights into the
relationship between model architecture, size, and factual learning efficiency.

</details>


### [112] [Language Bottleneck Models: A Framework for Interpretable Knowledge Tracing and Beyond](https://arxiv.org/abs/2506.16982)
*Antonin Berthon,Mihaela van der Schaar*

Main category: cs.CL

TL;DR: This paper introduces the Language Bottleneck Model (LBM), focusing on improving interpretability and accuracy in knowledge tracing through natural-language summaries.


<details>
  <summary>Details</summary>
Motivation: Traditional knowledge tracing methods are opaque and limit interpretability, while current LLM-based approaches may produce inaccurate summaries due to hallucination.

Method: The LBM involves an encoder LLM creating interpretable language summaries and a frozen decoder LLM reconstructing student responses based on those summaries. Experiments use group-relative policy optimization for training.

Result: LBMs achieve competitive accuracy compared to state-of-the-art methods on synthetic benchmarks and the Eedi dataset, requiring far fewer student trajectories.

Conclusion: Language Bottleneck Models enhance both interpretability and accuracy for knowledge tracing, making them promising for educational applications.

Abstract: Accurately assessing student knowledge is critical for effective education,
yet traditional Knowledge Tracing (KT) methods rely on opaque latent
embeddings, limiting interpretability. Even LLM-based approaches generate
direct predictions or summaries that may hallucinate without any accuracy
guarantees. We recast KT as an inverse problem: learning the minimum
natural-language summary that makes past answers explainable and future answers
predictable. Our Language Bottleneck Model (LBM) consists of an encoder LLM
that writes an interpretable knowledge summary and a frozen decoder LLM that
must reconstruct and predict student responses using only that summary text. By
constraining all predictive information to pass through a short
natural-language bottleneck, LBMs ensure that the summary contains accurate
information while remaining human-interpretable. Experiments on synthetic
arithmetic benchmarks and the large-scale Eedi dataset show that LBMs rival the
accuracy of state-of-the-art KT and direct LLM methods while requiring
orders-of-magnitude fewer student trajectories. We demonstrate that training
the encoder with group-relative policy optimization, using downstream decoding
accuracy as a reward signal, effectively improves summary quality.

</details>


### [113] [TeXpert: A Multi-Level Benchmark for Evaluating LaTeX Code Generation by LLMs](https://arxiv.org/abs/2506.16990)
*Sahil Kale,Vijaykant Nadadur*

Main category: cs.CL

TL;DR: TeXpert introduces a benchmark dataset to assess Large Language Models (LLMs) in generating accurate LaTeX code for scientific documents, revealing frequent errors and varying performances across LLMs.


<details>
  <summary>Details</summary>
Motivation: To evaluate and enhance LLM capabilities in generating precise LaTeX code for scientific documents, addressing a gap in existing benchmarks.

Method: Developed the TeXpert benchmark dataset with natural language prompts focused on various difficulty levels of LaTeX tasks, and analyzed LLM performance including error identification.

Result: Findings reveal poor performance in LaTeX generation for LLMs even with strong performance on general benchmarks, significant errors like formatting and package issues, and competitive open-source models.

Conclusion: LLMs need improvement in LaTeX generation tasks, with diverse datasets being a key factor for training. Open-source models show considerable potential, rivaling closed-source counterparts.

Abstract: LaTeX's precision and flexibility in typesetting have made it the gold
standard for the preparation of scientific documentation. Large Language Models
(LLMs) present a promising opportunity for researchers to produce
publication-ready material using LaTeX with natural language instructions, yet
current benchmarks completely lack evaluation of this ability. By introducing
TeXpert, our benchmark dataset with natural language prompts for generating
LaTeX code focused on components of scientific documents across multiple
difficulty levels, we conduct an in-depth analysis of LLM performance in this
regard and identify frequent error types. Our evaluation across open and
closed-source LLMs highlights multiple key findings: LLMs excelling on standard
benchmarks perform poorly in LaTeX generation with a significant accuracy
drop-off as the complexity of tasks increases; open-source models like DeepSeek
v3 and DeepSeek Coder strongly rival closed-source counterparts in LaTeX tasks;
and formatting and package errors are unexpectedly prevalent, suggesting a lack
of diverse LaTeX examples in the training datasets of most LLMs. Our dataset,
code, and model evaluations are available at
https://github.com/knowledge-verse-ai/TeXpert.

</details>


### [114] [PersonalAI: Towards digital twins in the graph form](https://arxiv.org/abs/2506.17001)
*Mikhail Menschikov,Dmitry Evseev,Ruslan Kostoev,Ilya Perepechkin,Ilnaz Salimov,Victoria Dochkina,Petr Anokhin,Evgeny Burnaev,Nikita Semenov*

Main category: cs.CL

TL;DR: The paper addresses personalizing language models by proposing the use of knowledge graphs with hyperedges, constructed and updated by the model itself, and demonstrates robustness in personalized question-answering tasks.


<details>
  <summary>Details</summary>
Motivation: To overcome the challenge of retaining and applying extensive personal information for generating personalized responses in large language models (LLMs).

Method: The authors propose using external memory in the form of knowledge graphs, introducing a combined graph featuring standard edges and two types of hyperedges. These graphs are dynamically constructed and updated by the LLM.

Result: Experiments on benchmarks such as TriviaQA, HotpotQA, and DiaASQ showed robust performance in graph construction and handling dialogues with new parameters like temporal dependencies and contradictions.

Conclusion: The proposed approach effectively personalizes language models, demonstrating robustness in maintaining and utilizing temporal knowledge for question-answering tasks.

Abstract: The challenge of personalizing language models, specifically the ability to
account for a user's history during interactions, is of significant interest.
Despite recent advancements in large language models (LLMs) and Retrieval
Augmented Generation that have enhanced the factual base of LLMs, the task of
retaining extensive personal information and using it to generate personalized
responses remains pertinent. To address this, we propose utilizing external
memory in the form of knowledge graphs, which are constructed and updated by
the LLM itself. We have expanded upon ideas of AriGraph architecture and for
the first time introduced a combined graph featuring both standard edges and
two types of hyperedges. Experiments conducted on the TriviaQA, HotpotQA and
DiaASQ benchmarks indicates that this approach aids in making the process of
graph construction and knowledge extraction unified and robust. Furthermore, we
augmented the DiaASQ benchmark by incorporating parameters such as time into
dialogues and introducing contradictory statements made by the same speaker at
different times. Despite these modifications, the performance of the
question-answering system remained robust, demonstrating the proposed
architecture's ability to maintain and utilize temporal dependencies.

</details>


### [115] [LLM-Generated Feedback Supports Learning If Learners Choose to Use It](https://arxiv.org/abs/2506.17006)
*Danielle R. Thomas,Conrad Borchers,Shambhavi Bhushan,Erin Gatz,Shivang Gupta,Kenneth R. Koedinger*

Main category: cs.CL

TL;DR: The study compared the learning impact of GPT-3.5-turbo-generated feedback with non-LLM corrective feedback. LLM feedback positively impacted learning outcomes in select scenarios, largely influenced by learners' propensity to engage with it.


<details>
  <summary>Details</summary>
Motivation: Explore the educational impact of LLM-generated explanatory feedback compared to traditional feedback methods.

Method: Analyzed data from 2,600 lesson completions across three groups of tutor learners—those receiving GPT-3.5-turbo feedback, those declining it, and those without access. Applied propensity scoring to control for selection bias.

Result: Learners with higher likelihood of engaging with LLM feedback performed better on posttests. Significant learning benefits were observed in two of seven lessons, with moderate effect sizes (0.28 and 0.33). LLM feedback was also rated as helpful by learners.

Conclusion: LLM feedback shows potential for enhancing learning in open-ended tasks, especially when integrated into systems with existing non-LLM feedback. The findings underscore the influence of learner tendencies in determining effectiveness.

Abstract: Large language models (LLMs) are increasingly used to generate feedback, yet
their impact on learning remains underexplored, especially compared to existing
feedback methods. This study investigates how on-demand LLM-generated
explanatory feedback influences learning in seven scenario-based tutor training
lessons. Analyzing over 2,600 lesson completions from 885 tutor learners, we
compare posttest performance among learners across three groups: learners who
received feedback generated by gpt-3.5-turbo, those who declined it, and those
without access. All groups received non-LLM corrective feedback. To address
potential selection bias-where higher-performing learners may be more inclined
to use LLM feedback-we applied propensity scoring. Learners with a higher
predicted likelihood of engaging with LLM feedback scored significantly higher
at posttest than those with lower propensity. After adjusting for this effect,
two out of seven lessons showed statistically significant learning benefits
from LLM feedback with standardized effect sizes of 0.28 and 0.33. These
moderate effects suggest that the effectiveness of LLM feedback depends on the
learners' tendency to seek support. Importantly, LLM feedback did not
significantly increase completion time, and learners overwhelmingly rated it as
helpful. These findings highlight LLM feedback's potential as a low-cost and
scalable way to improve learning on open-ended tasks, particularly in existing
systems already providing feedback without LLMs. This work contributes open
datasets, LLM prompts, and rubrics to support reproducibility.

</details>


### [116] [Instituto de Telecomunicações at IWSLT 2025: Aligning Small-Scale Speech and Language Models for Speech-to-Text Learning](https://arxiv.org/abs/2506.17019)
*Giuseppe Attanasio,Sonal Sannigrahi,Ben Peters,André F. T. Martins*

Main category: cs.CL

TL;DR: This paper introduces IT-IST's unified speech-to-text model for speech recognition, translation, and spoken question answering at IWSLT 2025.


<details>
  <summary>Details</summary>
Motivation: To tackle the Short Track task of speech recognition, translation, and spoken question answering by developing a unified, efficient model.

Method: The model uses a pre-trained continuous speech encoder and text decoder with two phases: modality alignment and instruction fine-tuning, leveraging small-scale language models and high-quality or synthetic data.

Result: Proposed a system that efficiently handles instruction-following speech processing using a compact and well-trained model.

Conclusion: High-quality instruction-following speech processing is achievable even with compact language models and data efficiency techniques.

Abstract: This paper presents the IT-IST submission to the IWSLT 2025 Shared Task on
Instruction Following Speech Processing. We submit results for the Short Track,
i.e., speech recognition, translation, and spoken question answering. Our model
is a unified speech-to-text model that integrates a pre-trained continuous
speech encoder and text decoder through a first phase of modality alignment and
a second phase of instruction fine-tuning. Crucially, we focus on using
small-scale language model backbones (< 2B) and restrict to high-quality, CC-BY
data along with synthetic data generation to supplement existing resources.

</details>


### [117] [MUCAR: Benchmarking Multilingual Cross-Modal Ambiguity Resolution for Multimodal Large Language Models](https://arxiv.org/abs/2506.17046)
*Xiaolong Wang,Zhaolu Kang,Wangyuxuan Zhai,Xinyue Lou,Yunghwei Lai,Ziyue Wang,Yawen Wang,Kaiyu Huang,Yile Wang,Peng Li,Yang Liu*

Main category: cs.CL

TL;DR: The paper introduces MUCAR, a benchmark designed to test multimodal large language models (MLLMs) in disambiguating ambiguous language and visual contexts, highlighting performance gaps between models and humans.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal benchmarks fail to address linguistic and visual ambiguities effectively, limiting the exploitation of mutual clarification between modalities.

Method: MUCAR includes two datasets: a multilingual set focusing on textual ambiguities clarified by visual context, and a dual-ambiguity set pairing ambiguous images with ambiguous text to achieve clarity through mutual disambiguation.

Result: 19 state-of-the-art MLLMs were tested, revealing significant performance gaps relative to human-level ambiguity resolution.

Conclusion: Future research is needed to develop more advanced multimodal reasoning methods to enhance ambiguity comprehension in MLLMs.

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated significant
advances across numerous vision-language tasks. Due to their strong image-text
alignment capability, MLLMs can effectively understand image-text pairs with
clear meanings. However, effectively resolving the inherent ambiguities in
natural language and visual contexts remains challenging. Existing multimodal
benchmarks typically overlook linguistic and visual ambiguities, relying mainly
on unimodal context for disambiguation and thus failing to exploit the mutual
clarification potential between modalities. To bridge this gap, we introduce
MUCAR, a novel and challenging benchmark designed explicitly for evaluating
multimodal ambiguity resolution across multilingual and cross-modal scenarios.
MUCAR includes: (1) a multilingual dataset where ambiguous textual expressions
are uniquely resolved by corresponding visual contexts, and (2) a
dual-ambiguity dataset that systematically pairs ambiguous images with
ambiguous textual contexts, with each combination carefully constructed to
yield a single, clear interpretation through mutual disambiguation. Extensive
evaluations involving 19 state-of-the-art multimodal models--encompassing both
open-source and proprietary architectures--reveal substantial gaps compared to
human-level performance, highlighting the need for future research into more
sophisticated cross-modal ambiguity comprehension methods, further pushing the
boundaries of multimodal reasoning.

</details>


### [118] [Simultaneous Translation with Offline Speech and LLM Models in CUNI Submission to IWSLT 2025](https://arxiv.org/abs/2506.17077)
*Dominik Macháček,Peter Polák*

Main category: cs.CL

TL;DR: The paper presents Charles University's participation in IWSLT 2025, showcasing significant improvements in simultaneous speech translation using advanced models and methodologies.


<details>
  <summary>Details</summary>
Motivation: To enhance simultaneous speech translation capabilities across multiple language pairs using advanced models and address challenges like latency and in-domain terminology integration.

Method: The authors use the offline Whisper speech model combined with the AlignAtt simultaneous policy for translation and transcription. They also integrate in-domain terminology and facilitate context accommodation, using EuroLLM for cascaded systems and proposing an improved latency measurement.

Result: Their systems achieved improvements of 2 BLEU points (Czech-English) and 13-22 BLEU points (English-German/Chinese/Japanese) compared to the organizer's baseline on development sets.

Conclusion: The proposed approaches significantly enhance translation quality and latency evaluation, demonstrating the effectiveness of combined systems and methodology.

Abstract: This paper describes Charles University submission to the Simultaneous Speech
Translation Task of the IWSLT 2025. We cover all four language pairs with a
direct or cascade approach. The backbone of our systems is the offline Whisper
speech model, which we use for both translation and transcription in
simultaneous mode with the state-of-the-art simultaneous policy AlignAtt. We
further improve the performance by prompting to inject in-domain terminology,
and we accommodate context. Our cascaded systems further use EuroLLM for
unbounded simultaneous translation. Compared to the Organizers' baseline, our
systems improve by 2 BLEU points on Czech to English and 13-22 BLEU points on
English to German, Chinese and Japanese on the development sets. Additionally,
we also propose a new enhanced measure of speech recognition latency.

</details>


### [119] [Tower+: Bridging Generality and Translation Specialization in Multilingual LLMs](https://arxiv.org/abs/2506.17080)
*Ricardo Rei,Nuno M. Guerreiro,José Pombal,João Alves,Pedro Teixeirinha,Amin Farajian,André F. T. Martins*

Main category: cs.CL

TL;DR: Tower+ models efficiently balance translation excellence and multilingual text capabilities. A novel training approach ensures strong performance across scales, rivaling leading models.


<details>
  <summary>Details</summary>
Motivation: Address the trade-off between task-specific fine-tuning and preserving general-purpose capabilities in language models.

Method: Introduced a training recipe combining pretrained adjustments, curated data, supervised fine-tuning, preference optimization, and reinforcement learning across multiple model scales.

Result: Tower+ excels in both translation and general-purpose evaluations, setting benchmarks for multilingual and instruction-following tasks.

Conclusion: It is possible to optimize models for specific business needs like translation while maintaining state-of-the-art general language capabilities.

Abstract: Fine-tuning pretrained LLMs has been shown to be an effective strategy for
reaching state-of-the-art performance on specific tasks like machine
translation. However, this process of adaptation often implies sacrificing
general-purpose capabilities, such as conversational reasoning and
instruction-following, hampering the utility of the system in real-world
applications that require a mixture of skills. In this paper, we introduce
Tower+, a suite of models designed to deliver strong performance across both
translation and multilingual general-purpose text capabilities. We achieve a
Pareto frontier between translation specialization and multilingual
general-purpose capabilities by introducing a novel training recipe that builds
on Tower (Alves et al., 2024), comprising continued pretraining, supervised
fine-tuning, preference optimization, and reinforcement learning with
verifiable rewards. At each stage of training, we carefully generate and curate
data to strengthen performance on translation as well as general-purpose tasks
involving code generation, mathematics problem solving, and general
instruction-following. We develop models at multiple scales: 2B, 9B, and 72B.
Our smaller models often outperform larger general-purpose open-weight and
proprietary LLMs (e.g., Llama 3.3 70B, GPT-4o). Our largest model delivers
best-in-class translation performance for high-resource languages and top
results in multilingual Arena Hard evaluations and in IF-MT, a benchmark we
introduce for evaluating both translation and instruction-following. Our
findings highlight that it is possible to rival frontier models in general
capabilities, while optimizing for specific business domains, such as
translation and localization.

</details>


### [120] [Chain-of-Thought Prompting Obscures Hallucination Cues in Large Language Models: An Empirical Evaluation](https://arxiv.org/abs/2506.17088)
*Jiahao Cheng,Tiancheng Su,Jia Yuan,Guoxiu He,Jiawei Liu,Xinqi Tao,Jingwen Xie,Huaxia Li*

Main category: cs.CL

TL;DR: This paper explores how Chain-of-Thought (CoT) prompting impacts hallucination detection in Large Language Models (LLMs), revealing it reduces hallucinations but complicates their detection.


<details>
  <summary>Details</summary>
Motivation: The primary motivation is to investigate the effect of CoT prompting on hallucination detection in LLMs, as previous studies analyzed CoT's benefits for reasoning but not its impact on detection tasks.

Method: A systematic empirical evaluation was conducted, including pilot experiments testing CoT's influence on token probability distributions and extensive analysis across different LLMs, examining hallucination score distributions, detection accuracy, and confidence.

Result: The study found that CoT prompting reduces hallucination frequency but simultaneously obscures detection signals, thus hindering the performance of various hallucination detection methods.

Conclusion: While CoT prompting shows promise in mitigating hallucinations, it introduces a trade-off by impairing detection effectiveness, highlighting the need for a balanced approach in leveraging reasoning with LLMs.

Abstract: Large Language Models (LLMs) often exhibit \textit{hallucinations},
generating factually incorrect or semantically irrelevant content in response
to prompts. Chain-of-Thought (CoT) prompting can mitigate hallucinations by
encouraging step-by-step reasoning, but its impact on hallucination detection
remains underexplored. To bridge this gap, we conduct a systematic empirical
evaluation. We begin with a pilot experiment, revealing that CoT reasoning
significantly affects the LLM's internal states and token probability
distributions. Building on this, we evaluate the impact of various CoT
prompting methods on mainstream hallucination detection methods across both
instruction-tuned and reasoning-oriented LLMs. Specifically, we examine three
key dimensions: changes in hallucination score distributions, variations in
detection accuracy, and shifts in detection confidence. Our findings show that
while CoT prompting helps reduce hallucination frequency, it also tends to
obscure critical signals used for detection, impairing the effectiveness of
various detection methods. Our study highlights an overlooked trade-off in the
use of reasoning. Code is publicly available at:
https://anonymous.4open.science/r/cot-hallu-detect.

</details>


### [121] [Better Language Model Inversion by Compactly Representing Next-Token Distributions](https://arxiv.org/abs/2506.17090)
*Murtaza Nazir,Matthew Finlayson,John X. Morris,Xiang Ren,Swabha Swayamdipta*

Main category: cs.CL

TL;DR: The paper introduces PILS, a novel method to recover hidden prompts from language model outputs, showing significant improvements over prior techniques.


<details>
  <summary>Details</summary>
Motivation: The study is driven by the need to explore risks in language model deployments, particularly the ability to leak private information from hidden prompts in outputs.

Method: PILS (Prompt Inversion from Logprob Sequences) exploits the low-dimensional subspace of vector outputs in language models to compress next-token probabilities across steps, aiding in prompt recovery.

Result: The method achieves 2-3.5x higher prompt recovery rates than prior methods, demonstrates good generalization, and addresses the challenge of recovering hidden system messages.

Conclusion: Next-token probabilities represent a more vulnerable attack surface than previously thought, posing risks in LM security and requiring countermeasures.

Abstract: Language model inversion seeks to recover hidden prompts using only language
model outputs. This capability has implications for security and accountability
in language model deployments, such as leaking private information from an
API-protected language model's system message. We propose a new method --
prompt inversion from logprob sequences (PILS) -- that recovers hidden prompts
by gleaning clues from the model's next-token probabilities over the course of
multiple generation steps. Our method is enabled by a key insight: The
vector-valued outputs of a language model occupy a low-dimensional subspace.
This enables us to losslessly compress the full next-token probability
distribution over multiple generation steps using a linear map, allowing more
output information to be used for inversion. Our approach yields massive gains
over previous state-of-the-art methods for recovering hidden prompts, achieving
2--3.5 times higher exact recovery rates across test sets, in one case
increasing the recovery rate from 17% to 60%. Our method also exhibits
surprisingly good generalization behavior; for instance, an inverter trained on
16 generations steps gets 5--27 points higher prompt recovery when we increase
the number of steps to 32 at test time. Furthermore, we demonstrate strong
performance of our method on the more challenging task of recovering hidden
system messages. We also analyze the role of verbatim repetition in prompt
recovery and propose a new method for cross-family model transfer for
logit-based inverters. Our findings show that next-token probabilities are a
considerably more vulnerable attack surface for inversion attacks than
previously known.

</details>


### [122] [Cache Me If You Can: How Many KVs Do You Need for Effective Long-Context LMs?](https://arxiv.org/abs/2506.17121)
*Adithya Bhaskar,Alexander Wettig,Tianyu Gao,Yihe Dong,Danqi Chen*

Main category: cs.CL

TL;DR: This paper introduces a unified metric called "KV footprint" to evaluate methods for managing key-value (KV) memory in long-context language models, and proposes enhancements to existing techniques.


<details>
  <summary>Details</summary>
Motivation: The study addresses the issue of growing memory costs in key-value (KV) caches for long-context tasks in language models, alongside the difficulty of comparing existing KV eviction methods due to inconsistencies.

Method: Introduced the KV footprint as a unified metric to measure the storage and lifespan of KV entries. Proposed improvements to post-fill eviction methods and developed PruLong, an optimization method targeting memory efficiency and performance balance.

Result: PruLong achieved a 12% smaller KV footprint compared to prior methods while maintaining performance in long-context tasks, highlighting inefficiencies in existing approaches and reducing high peak memory usage.

Conclusion: The proposed KV footprint metric and PruLong optimization method clarify and improve the landscape of long-context inference techniques, minimizing memory costs and guiding future development.

Abstract: Language models handle increasingly long contexts for tasks such as book
summarization, but this leads to growing memory costs for the key-value (KV)
cache. Many prior works have proposed ways of discarding KVs from memory, but
their approaches are tailored to favorable settings, obscuring caveats like
high peak memory and performance degradation, and a fair comparison between
methods is difficult. In this paper, we propose the *KV footprint* as a unified
metric, which accounts for both the amount of KV entries stored and their
lifespan in memory. We evaluate methods based on the smallest footprint they
attain while preserving performance in both long-context understanding and
generation, with context lengths of up to 128K tokens. This metric reveals the
high peak memory of prior KV eviction methods. One class of methods --
*post-fill eviction* -- has a high footprint due to being incompatible with
eviction during pre-filling. We adapt these methods to be able to evict KVs
during pre-filling, achieving substantially lower KV footprints. We then turn
to *recency eviction* methods, wherein we propose PruLong, an end-to-end
optimization method for learning which attention heads need to retain the full
KV cache and which do not. PruLong saves memory while preserving long-context
performance, achieving 12% smaller KV footprint than prior methods while
retaining performance in challenging recall tasks. Our paper clarifies the
complex tangle of long-context inference methods and paves the way for future
development to minimize the KV footprint.

</details>


### [123] [CLEAR-3K: Assessing Causal Explanatory Capabilities in Language Models](https://arxiv.org/abs/2506.17180)
*Naiming Liu,Richard Baraniuk,Shashank Sonkar*

Main category: cs.CL

TL;DR: The paper introduces CLEAR-3K, a dataset of 3,000 assertion-reasoning questions to evaluate causal reasoning in language models, revealing their limitations in distinguishing causality from semantic similarity.


<details>
  <summary>Details</summary>
Motivation: To address the gap in evaluating whether language models can truly understand and infer causal relationships, beyond mere semantic relatedness.

Method: Developed CLEAR-3K, a dataset focusing on distinguishing semantic overlap from causal relationships, and evaluated 21 state-of-the-art language models ranging from 0.5B to 72B parameters.

Result: Found that models confuse semantic similarity with causality and that their performance plateaus at a Matthews Correlation Coefficient of 0.55, even as parameter size increases.

Conclusion: CLEAR-3K serves as a critical benchmark for advancing causal reasoning capabilities in language models, crucial for applications needing accurate causality assessments.

Abstract: We introduce CLEAR-3K, a dataset of 3,000 assertion-reasoning questions
designed to evaluate whether language models can determine if one statement
causally explains another. Each question present an assertion-reason pair and
challenge language models to distinguish between semantic relatedness and
genuine causal explanatory relationships. Through comprehensive evaluation of
21 state-of-the-art language models (ranging from 0.5B to 72B parameters), we
identify two fundamental findings. First, language models frequently confuse
semantic similarity with causality, relying on lexical and semantic overlap
instead of inferring actual causal explanatory relationships. Second, as
parameter size increases, models tend to shift from being overly skeptical
about causal relationships to being excessively permissive in accepting them.
Despite this shift, performance measured by the Matthews Correlation
Coefficient plateaus at just 0.55, even for the best-performing models.Hence,
CLEAR-3K provides a crucial benchmark for developing and evaluating genuine
causal reasoning in language models, which is an essential capability for
applications that require accurate assessment of causal relationships.

</details>


### [124] [Towards AI Search Paradigm](https://arxiv.org/abs/2506.17188)
*Yuchen Li,Hengyi Cai,Rui Kong,Xinran Chen,Jiamin Chen,Jun Yang,Haojie Zhang,Jiayi Li,Jiayi Wu,Yiqun Chen,Changle Qu,Keyi Kong,Wenwen Ye,Lixin Su,Xinyu Ma,Long Xia,Daiting Shi,Jiashu Zhao,Haoyi Xiong,Shuaiqiang Wang,Dawei Yin*

Main category: cs.CL

TL;DR: This paper presents the 'AI Search Paradigm,' a modular system with LLM-powered agents designed to handle diverse search tasks through dynamic collaboration.


<details>
  <summary>Details</summary>
Motivation: To develop a blueprint for next-generation AI search systems that can process information and make decisions like humans, addressing diverse and complex query needs.

Method: The system employs a modular architecture with four LLM-powered agents (Master, Planner, Executor, and Writer) that collaborate to adapt to query complexity, decompose problems, and execute tasks using integrated tools.

Result: Key methodologies for the paradigm's implementation are systematically detailed, including robust retrieval-augmented generation, task planning, and efficient LLM inference algorithms.

Conclusion: This work guides the development of scalable, adaptive, and trustworthy AI search systems capable of complex reasoning and decision-making.

Abstract: In this paper, we introduce the AI Search Paradigm, a comprehensive blueprint
for next-generation search systems capable of emulating human information
processing and decision-making. The paradigm employs a modular architecture of
four LLM-powered agents (Master, Planner, Executor and Writer) that dynamically
adapt to the full spectrum of information needs, from simple factual queries to
complex multi-stage reasoning tasks. These agents collaborate dynamically
through coordinated workflows to evaluate query complexity, decompose problems
into executable plans, and orchestrate tool usage, task execution, and content
synthesis. We systematically present key methodologies for realizing this
paradigm, including task planning and tool integration, execution strategies,
aligned and robust retrieval-augmented generation, and efficient LLM inference,
spanning both algorithmic techniques and infrastructure-level optimizations. By
providing an in-depth guide to these foundational components, this work aims to
inform the development of trustworthy, adaptive, and scalable AI search
systems.

</details>


### [125] [Fine-Tuning Lowers Safety and Disrupts Evaluation Consistency](https://arxiv.org/abs/2506.17209)
*Kathleen C. Fraser,Hillary Dawkins,Isar Nejadgholi,Svetlana Kiritchenko*

Main category: cs.CL

TL;DR: Fine-tuning of large language models (LLMs) can degrade safety alignment, even unintentionally, and this study highlights the need for robust safety evaluation benchmarks.


<details>
  <summary>Details</summary>
Motivation: The increasing adoption of fine-tuning by ordinary users has inadvertently led to a critical vulnerability, where safety alignment in LLMs is compromised, even without malicious intent.

Method: The paper investigates the robustness of safety benchmarks in the context of fine-tuning, considering variables like trivial changes in procedures and the inherent stochasticity of LLMs.

Result: The experiments reveal significant variance in safety evaluation results across minor experimental variations, raising concern for the reliability of current benchmarks.

Conclusion: Reliable safety evaluations and standardized reporting practices are necessary to address safety vulnerabilities in fine-tuned LLMs and ensure meaningful comparisons across research efforts.

Abstract: Fine-tuning a general-purpose large language model (LLM) for a specific
domain or task has become a routine procedure for ordinary users. However,
fine-tuning is known to remove the safety alignment features of the model, even
when the fine-tuning data does not contain any harmful content. We consider
this to be a critical failure mode of LLMs due to the widespread uptake of
fine-tuning, combined with the benign nature of the "attack". Most
well-intentioned developers are likely unaware that they are deploying an LLM
with reduced safety. On the other hand, this known vulnerability can be easily
exploited by malicious actors intending to bypass safety guardrails. To make
any meaningful progress in mitigating this issue, we first need reliable and
reproducible safety evaluations. In this work, we investigate how robust a
safety benchmark is to trivial variations in the experimental procedure, and
the stochastic nature of LLMs. Our initial experiments expose surprising
variance in the results of the safety evaluation, even when seemingly
inconsequential changes are made to the fine-tuning setup. Our observations
have serious implications for how researchers in this field should report
results to enable meaningful comparisons in the future.

</details>


### [126] [LaMP-Cap: Personalized Figure Caption Generation With Multimodal Figure Profiles](https://arxiv.org/abs/2506.06561)
*Ho Yin 'Sam' Ng,Ting-Yao Hsu,Aashish Anantha Ramakrishnan,Branislav Kveton,Nedim Lipka,Franck Dernoncourt,Dongwon Lee,Tong Yu,Sungchul Kim,Ryan A. Rossi,Ting-Hao 'Kenneth' Huang*

Main category: cs.CL

TL;DR: LaMP-Cap introduces a dataset for personalized figure caption generation leveraging multimodal figure profiles, emphasizing how profiles improve AI-generated captions.


<details>
  <summary>Details</summary>
Motivation: To address the need for personalization in AI-generated figure captions, which are typically generic and require revision to match authors' writing and domain styles.

Method: The authors developed LaMP-Cap, a dataset providing multimodal figure profiles including images, captions, and contextual paragraphs from the document. Experiments with LLMs evaluated caption generation improvement using profile information.

Result: Using multimodal profiles consistently improved caption generation quality, with figure images playing a more significant role than text-only profiles.

Conclusion: Integrating multimodal figure profiles enhances personalized caption generation and highlights the superiority of tailored AI tools for domain-specific applications.

Abstract: Figure captions are crucial for helping readers understand and remember a
figure's key message. Many models have been developed to generate these
captions, helping authors compose better quality captions more easily. Yet,
authors almost always need to revise generic AI-generated captions to match
their writing style and the domain's style, highlighting the need for
personalization. Despite language models' personalization (LaMP) advances,
these technologies often focus on text-only settings and rarely address
scenarios where both inputs and profiles are multimodal. This paper introduces
LaMP-Cap, a dataset for personalized figure caption generation with multimodal
figure profiles. For each target figure, LaMP-Cap provides not only the needed
inputs, such as figure images, but also up to three other figures from the same
document--each with its image, caption, and figure-mentioning paragraphs--as a
profile to characterize the context. Experiments with four LLMs show that using
profile information consistently helps generate captions closer to the original
author-written ones. Ablation studies reveal that images in the profile are
more helpful than figure-mentioning paragraphs, highlighting the advantage of
using multimodal profiles over text-only ones.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [127] [Spatially-Aware Evaluation of Segmentation Uncertainty](https://arxiv.org/abs/2506.16589)
*Tal Zeevi,Eléonore V. Lieffrig,Lawrence H. Staib,John A. Onofrey*

Main category: cs.CV

TL;DR: The paper introduces spatially aware metrics to better evaluate uncertainty in segmentation predictions by incorporating anatomical structure and boundary information.


<details>
  <summary>Details</summary>
Motivation: Current uncertainty evaluation metrics in segmentation predictions fail to account for spatial context and anatomical patterns, leading to limitations in assessing qualitatively distinct uncertainty patterns.

Method: The authors propose three new spatially aware metrics that integrate structural and boundary context to analyze segmentation uncertainty. Validation is performed on medical imaging data from the prostate zonal segmentation challenge.

Result: The newly proposed metrics show improved alignment with clinically significant factors and better differentiation between meaningful and irrelevant uncertainty patterns.

Conclusion: Spatially aware metrics provide enhanced evaluations of segmentation uncertainty, addressing limitations of existing metrics by incorporating anatomical and boundary-based context.

Abstract: Uncertainty maps highlight unreliable regions in segmentation predictions.
However, most uncertainty evaluation metrics treat voxels independently,
ignoring spatial context and anatomical structure. As a result, they may assign
identical scores to qualitatively distinct patterns (e.g., scattered vs.
boundary-aligned uncertainty). We propose three spatially aware metrics that
incorporate structural and boundary information and conduct a thorough
validation on medical imaging data from the prostate zonal segmentation
challenge within the Medical Segmentation Decathlon. Our results demonstrate
improved alignment with clinically important factors and better discrimination
between meaningful and spurious uncertainty patterns.

</details>


### [128] [A Strong View-Free Baseline Approach for Single-View Image Guided Point Cloud Completion](https://arxiv.org/abs/2506.15747)
*Fangzhou Lin,Zilin Dai,Rigved Sanku,Songlin Hou,Kazunori D Yamada,Haichong K. Zhang,Ziming Zhang*

Main category: cs.CV

TL;DR: The paper explores multimodal point cloud completion using image guidance and compares it against a novel view-free framework, achieving superior results.


<details>
  <summary>Details</summary>
Motivation: The authors aim to investigate whether image guidance is fundamentally necessary in the single-view image guided point cloud completion task.

Method: They propose a view-free baseline approach using an attention-based encoder-decoder network with hierarchical self-fusion driven by cross-attention and self-attention layers.

Result: Results show that the view-free framework outperforms state-of-the-art methods on the ShapeNet-ViPC dataset according to extensive experiments and ablation studies.

Conclusion: The study challenges the necessity of multimodal image guidance, encouraging new directions in multimodal learning for point cloud completion.

Abstract: The single-view image guided point cloud completion (SVIPC) task aims to
reconstruct a complete point cloud from a partial input with the help of a
single-view image. While previous works have demonstrated the effectiveness of
this multimodal approach, the fundamental necessity of image guidance remains
largely unexamined. To explore this, we propose a strong baseline approach for
SVIPC based on an attention-based multi-branch encoder-decoder network that
only takes partial point clouds as input, view-free. Our hierarchical
self-fusion mechanism, driven by cross-attention and self-attention layers,
effectively integrates information across multiple streams, enriching feature
representations and strengthening the networks ability to capture geometric
structures. Extensive experiments and ablation studies on the ShapeNet-ViPC
dataset demonstrate that our view-free framework performs superiorly to
state-of-the-art SVIPC methods. We hope our findings provide new insights into
the development of multimodal learning in SVIPC. Our demo code will be
available at https://github.com/Zhang-VISLab.

</details>


### [129] [VLMInferSlow: Evaluating the Efficiency Robustness of Large Vision-Language Models as a Service](https://arxiv.org/abs/2506.15755)
*Xiasi Wang,Tianliang Yao,Simin Chen,Runqi Wang,Lei YE,Kuofeng Gao,Yi Huang,Yuan Yao*

Main category: cs.CV

TL;DR: The paper introduces VLMInferSlow, a black-box approach to evaluate the efficiency robustness of vision-language models (VLMs) by generating imperceptible adversarial examples that significantly increase computational cost.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the critical need to evaluate the efficiency robustness of VLMs in real-world, black-box settings, considering the challenges posed by inference APIs in ML-as-a-service scenarios.

Method: The authors developed VLMInferSlow, incorporating fine-grained efficiency modeling tailored to VLM inference and leveraging zero-order optimization to create adversarial examples.

Result: VLMInferSlow successfully generates adversarial images causing up to a 128.47% increase in computational cost without visible perturbations.

Conclusion: The study highlights the importance of considering efficiency robustness in VLMs and aims to raise community awareness on this overlooked aspect.

Abstract: Vision-Language Models (VLMs) have demonstrated great potential in real-world
applications. While existing research primarily focuses on improving their
accuracy, the efficiency remains underexplored. Given the real-time demands of
many applications and the high inference overhead of VLMs, efficiency
robustness is a critical issue. However, previous studies evaluate efficiency
robustness under unrealistic assumptions, requiring access to the model
architecture and parameters -- an impractical scenario in ML-as-a-service
settings, where VLMs are deployed via inference APIs. To address this gap, we
propose VLMInferSlow, a novel approach for evaluating VLM efficiency robustness
in a realistic black-box setting. VLMInferSlow incorporates fine-grained
efficiency modeling tailored to VLM inference and leverages zero-order
optimization to search for adversarial examples. Experimental results show that
VLMInferSlow generates adversarial images with imperceptible perturbations,
increasing the computational cost by up to 128.47%. We hope this research
raises the community's awareness about the efficiency robustness of VLMs.

</details>


### [130] [Weakly-supervised VLM-guided Partial Contrastive Learning for Visual Language Navigation](https://arxiv.org/abs/2506.15757)
*Ruoyu Wang,Tong Yu,Junda Wu,Yao Liu,Julian McAuley,Lina Yao*

Main category: cs.CV

TL;DR: The paper introduces Weakly-supervised Partial Contrastive Learning (WPCL) to improve Visual Language Navigation (VLN) performance by integrating pre-trained Visual Language Model (VLM) knowledge without fine-tuning, addressing key limitations of prior methods.


<details>
  <summary>Details</summary>
Motivation: Current methods for navigating environments with natural language rely heavily on pre-trained backbones, struggle with dynamic viewpoints, and face high computational costs when fine-tuning large language or visual models.

Method: The authors propose WPCL, a weakly-supervised learning approach that enhances an agent's object recognition ability in dynamic VLN scenarios by integrating VLM knowledge effectively, without requiring VLM fine-tuning.

Result: The method demonstrates superior performance across multiple VLN benchmarks compared to baseline approaches, showcasing its effectiveness, robustness, and generalizability.

Conclusion: WPCL offers an efficient and effective improvement for VLN tasks, addressing the challenges of dynamic viewpoints and computational inefficiency in prior methods while achieving state-of-the-art results.

Abstract: Visual Language Navigation (VLN) is a fundamental task within the field of
Embodied AI, focusing on the ability of agents to navigate complex environments
based on natural language instructions. Despite the progress made by existing
methods, these methods often present some common challenges. First, they rely
on pre-trained backbone models for visual perception, which struggle with the
dynamic viewpoints in VLN scenarios. Second, the performance is limited when
using pre-trained LLMs or VLMs without fine-tuning, due to the absence of VLN
domain knowledge. Third, while fine-tuning LLMs and VLMs can improve results,
their computational costs are higher than those without fine-tuning. To address
these limitations, we propose Weakly-supervised Partial Contrastive Learning
(WPCL), a method that enhances an agent's ability to identify objects from
dynamic viewpoints in VLN scenarios by effectively integrating pre-trained VLM
knowledge into the perception process, without requiring VLM fine-tuning. Our
method enhances the agent's ability to interpret and respond to environmental
cues while ensuring computational efficiency. Experimental results have shown
that our method outperforms the baseline methods on multiple benchmarks, which
validate the effectiveness, robustness and generalizability of our method.

</details>


### [131] [Implicit 3D scene reconstruction using deep learning towards efficient collision understanding in autonomous driving](https://arxiv.org/abs/2506.15806)
*Akarshani Ramanayake,Nihal Kodikara*

Main category: cs.CV

TL;DR: A paper presents a method leveraging LiDAR data and deep neural networks to improve 3D scene reconstruction and obstacle mapping using signed distance functions (SDF) in urban traffic environments.


<details>
  <summary>Details</summary>
Motivation: To address challenges in accurate 3D scene reconstruction in dense urban traffic, particularly improving obstacle boundary-level details which current technologies struggle to achieve.

Method: Developing a learning-based 3D scene reconstruction framework using LiDAR data and deep neural networks to build static Signed Distance Function (SDF) maps.

Result: Preliminary results show enhanced collision detection performance and improved accuracy for obstacle boundary mapping in congested environments.

Conclusion: The proposed method has the potential to improve safety and navigation in urban traffic by advancing 3D obstacle reconstruction techniques using SDFs with detailed boundary precision.

Abstract: In crowded urban environments where traffic is dense, current technologies
struggle to oversee tight navigation, but surface-level understanding allows
autonomous vehicles to safely assess proximity to surrounding obstacles. 3D or
2D scene mapping of the surrounding objects is an essential task in addressing
the above problem. Despite its importance in dense vehicle traffic conditions,
3D scene reconstruction of object shapes with higher boundary level accuracy is
not yet entirely considered in current literature. The sign distance function
represents any shape through parameters that calculate the distance from any
point in space to the closest obstacle surface, making it more efficient in
terms of storage. In recent studies, researchers have started to formulate
problems with Implicit 3D reconstruction methods in the autonomous driving
domain, highlighting the possibility of using sign distance function to map
obstacles effectively. This research addresses this gap by developing a
learning-based 3D scene reconstruction methodology that leverages LiDAR data
and a deep neural network to build a the static Signed Distance Function (SDF)
maps. Unlike traditional polygonal representations, this approach has the
potential to map 3D obstacle shapes with more boundary-level details. Our
preliminary results demonstrate that this method would significantly enhance
collision detection performance, particularly in congested and dynamic
environments.

</details>


### [132] [Stretching Beyond the Obvious: A Gradient-Free Framework to Unveil the Hidden Landscape of Visual Invariance](https://arxiv.org/abs/2506.17040)
*Lorenzo Tausani,Paolo Muratore,Morgan B. Talbot,Giacomo Amerio,Gabriel Kreiman,Davide Zoccolan*

Main category: cs.CV

TL;DR: The paper introduces the Stretch-and-Squeeze (SnS) method to characterize a unit's invariance landscape and adversarial vulnerabilities in visual systems like CNNs.


<details>
  <summary>Details</summary>
Motivation: Understanding how visual units encode different feature combinations is critical for recognizing the transformations supporting vision generalization.

Method: The SnS framework uses bi-objective optimization to identify image perturbations that either preserve unit activation or suppress it, providing insights into invariance and adversarial sensitivity.

Result: SnS applied to CNNs found distinct types of image variations depending on the optimization level, and robust networks generated more human-recognizable invariant images.

Conclusion: The SnS method advances the characterization of visual representations, revealing insightful differences between standard and robust networks in modeling the visual system.

Abstract: Uncovering which features' combinations high-level visual units encode is
critical to understand how images are transformed into representations that
support recognition. While existing feature visualization approaches typically
infer a unit's most exciting images, this is insufficient to reveal the
manifold of transformations under which responses remain invariant, which is
key to generalization in vision. Here we introduce Stretch-and-Squeeze (SnS),
an unbiased, model-agnostic, and gradient-free framework to systematically
characterize a unit's invariance landscape and its vulnerability to adversarial
perturbations in both biological and artificial visual systems. SnS frames
these transformations as bi-objective optimization problems. To probe
invariance, SnS seeks image perturbations that maximally alter the
representation of a reference stimulus in a given processing stage while
preserving unit activation. To probe adversarial sensitivity, SnS seeks
perturbations that minimally alter the stimulus while suppressing unit
activation. Applied to convolutional neural networks (CNNs), SnS revealed image
variations that were further from a reference image in pixel-space than those
produced by affine transformations, while more strongly preserving the target
unit's response. The discovered invariant images differed dramatically
depending on the choice of image representation used for optimization:
pixel-level changes primarily affected luminance and contrast, while stretching
mid- and late-layer CNN representations altered texture and pose respectively.
Notably, the invariant images from robust networks were more recognizable by
human subjects than those from standard networks, supporting the higher
fidelity of robust CNNs as models of the visual system.

</details>


### [133] [ADAM-Dehaze: Adaptive Density-Aware Multi-Stage Dehazing for Improved Object Detection in Foggy Conditions](https://arxiv.org/abs/2506.15837)
*Fatmah AlHindaassi,Mohammed Talha Alam,Fakhri Karray*

Main category: cs.CV

TL;DR: This paper presents ADAM-Dehaze, a fog intensity-aware image dehazing framework that improves image restoration and object detection, showing significant performance gains in benchmarks.


<details>
  <summary>Details</summary>
Motivation: Foggy weather significantly affects visual data, complicating tasks such as autonomous navigation and surveillance. A reliable solution is needed to restore image clarity and ensure object detection under varying fog intensities.

Method: The proposed ADAM-Dehaze system includes an adaptive pipeline with a Haze Density Estimation Network (HDEN) that classifies fog density, dynamically routing input to one of three dehazing branches tailored to light, medium, or heavy fog. It employs adaptive loss functions to balance detail preservation and defogging accuracy.

Result: ADAM-Dehaze achieves up to 2.1 dB improvement in PSNR, a 30% reduction in FADE, and a 13-point increase in object detection mAP on Cityscapes and RTTS datasets. It also reduces inference time by 20%.

Conclusion: The study demonstrates that fog-intensity-specific processing, combined with integration into vision tasks, significantly enhances visual performance and object detection under adverse weather conditions.

Abstract: Adverse weather conditions, particularly fog, pose a significant challenge to
autonomous vehicles, surveillance systems, and other safety-critical
applications by severely degrading visual information. We introduce
ADAM-Dehaze, an adaptive, density-aware dehazing framework that jointly
optimizes image restoration and object detection under varying fog intensities.
A lightweight Haze Density Estimation Network (HDEN) classifies each input as
light, medium, or heavy fog. Based on this score, the system dynamically routes
the image through one of three CORUN branches: Light, Medium, or Complex, each
tailored to its haze regime. A novel adaptive loss balances physical-model
coherence and perceptual fidelity, ensuring both accurate defogging and
preservation of fine details. On Cityscapes and the real-world RTTS benchmark,
ADAM-Dehaze improves PSNR by up to 2.1 dB, reduces FADE by 30 percent, and
increases object detection mAP by up to 13 points, while cutting inference time
by 20 percent. These results highlight the importance of intensity-specific
processing and seamless integration with downstream vision tasks. Code
available at: https://github.com/talha-alam/ADAM-Dehaze.

</details>


### [134] [EchoShot: Multi-Shot Portrait Video Generation](https://arxiv.org/abs/2506.15838)
*Jiahao Wang,Hualian Sheng,Sijia Cai,Weizhan Zhang,Caixia Yan,Yachuang Feng,Bing Deng,Jieping Ye*

Main category: cs.CV

TL;DR: EchoShot introduces a method for generating consistent and customizable portrait videos across multiple shots using a video diffusion model.


<details>
  <summary>Details</summary>
Motivation: Existing video generation models are limited to single-shot creation, lacking identity consistency and flexible control required for real-world applications.

Method: EchoShot utilizes shot-aware position embedding in video diffusion transformers and trains on a new dataset, PortraitGala, to support multi-shot video generation. It also allows for reference image-based personalization and long video synthesis.

Result: EchoShot demonstrates high identity consistency and detailed attribute controllability in multi-shot portrait video generation.

Conclusion: EchoShot provides a scalable framework for multi-shot portrait customization and has the potential to serve as a fundamental approach for general multi-shot video modeling.

Abstract: Video diffusion models substantially boost the productivity of artistic
workflows with high-quality portrait video generative capacity. However,
prevailing pipelines are primarily constrained to single-shot creation, while
real-world applications urge for multiple shots with identity consistency and
flexible content controllability. In this work, we propose EchoShot, a native
and scalable multi-shot framework for portrait customization built upon a
foundation video diffusion model. To start with, we propose shot-aware position
embedding mechanisms within video diffusion transformer architecture to model
inter-shot variations and establish intricate correspondence between multi-shot
visual content and their textual descriptions. This simple yet effective design
enables direct training on multi-shot video data without introducing additional
computational overhead. To facilitate model training within multi-shot
scenario, we construct PortraitGala, a large-scale and high-fidelity
human-centric video dataset featuring cross-shot identity consistency and
fine-grained captions such as facial attributes, outfits, and dynamic motions.
To further enhance applicability, we extend EchoShot to perform reference
image-based personalized multi-shot generation and long video synthesis with
infinite shot counts. Extensive evaluations demonstrate that EchoShot achieves
superior identity consistency as well as attribute-level controllability in
multi-shot portrait video generation. Notably, the proposed framework
demonstrates potential as a foundational paradigm for general multi-shot video
modeling.

</details>


### [135] [Assessing the impact of Binarization for Writer Identification in Greek Papyrus](https://arxiv.org/abs/2506.15852)
*Dominic Akt,Marco Peer,Florian Kleber*

Main category: cs.CV

TL;DR: The study evaluates image binarization techniques for writer identification in Greek papyri, comparing traditional methods and Deep Learning models with data augmentation.


<details>
  <summary>Details</summary>
Motivation: Improve writer identification in Greek papyri by addressing challenges like non-uniform and fragmented backgrounds through effective binarization.

Method: Compared traditional and Deep Learning binarization methods on the DIBCO 2019 dataset, using custom data augmentation and evaluating their impact on writer identification accuracy.

Result: Found that data augmentation enhances Deep Learning methods and a strong relationship exists between effective binarization and writer identification performance.

Conclusion: Data augmentation and high-quality binarization are critical for improving writer identification in challenging historical document contexts like Greek papyri.

Abstract: This paper tackles the task of writer identification for Greek papyri. A
common preprocessing step in writer identification pipelines is image
binarization, which prevents the model from learning background features. This
is challenging in historical documents, in our case Greek papyri, as background
is often non-uniform, fragmented, and discolored with visible fiber structures.
We compare traditional binarization methods to state-of-the-art Deep Learning
(DL) models, evaluating the impact of binarization quality on subsequent writer
identification performance. DL models are trained with and without a custom
data augmentation technique, as well as different model selection criteria are
applied. The performance of these binarization methods, is then systematically
evaluated on the DIBCO 2019 dataset. The impact of binarization on writer
identification is subsequently evaluated using a state-of-the-art approach for
writer identification. The results of this analysis highlight the influence of
data augmentation for DL methods. Furthermore, findings indicate a strong
correlation between binarization effectiveness on papyri documents of DIBCO
2019 and downstream writer identification performance.

</details>


### [136] [Privacy-Preserving in Connected and Autonomous Vehicles Through Vision to Text Transformation](https://arxiv.org/abs/2506.15854)
*Abdolazim Rezaei,Mehdi Sookhak,Ahmad Patooghy*

Main category: cs.CV

TL;DR: This paper introduces a novel framework for preserving visual privacy in images captured by AI cameras in Connected and Autonomous Vehicles, using reinforcement learning and vision-language models to convert images into textual descriptions.


<details>
  <summary>Details</summary>
Motivation: To address privacy risks associated with imagery captured by roadside cameras in Connected and Autonomous Vehicles, mitigating identity theft and misuse.

Method: Employ feedback-based reinforcement learning and vision-language models to convert images into semantically equivalent textual descriptions, using hierarchical RL to improve semantic accuracy and privacy over iterations.

Result: Evaluation shows improvements in privacy protection and textual quality, with Unique Word Count increasing by 77% and Detail Density rising by 50% compared to existing methods.

Conclusion: The proposed framework effectively balances semantic scene information with enhanced privacy measures, outperforming traditional techniques and advancing privacy-preserving AI camera applications.

Abstract: Connected and Autonomous Vehicles (CAVs) rely on a range of devices that
often process privacy-sensitive data. Among these, roadside units play a
critical role particularly through the use of AI-equipped (AIE) cameras for
applications such as violation detection. However, the privacy risks associated
with captured imagery remain a major concern, as such data can be misused for
identity theft, profiling, or unauthorized commercial purposes. While
traditional techniques such as face blurring and obfuscation have been applied
to mitigate privacy risks, individual privacy remains at risk, as individuals
can still be tracked using other features such as their clothing. This paper
introduces a novel privacy-preserving framework that leverages feedback-based
reinforcement learning (RL) and vision-language models (VLMs) to protect
sensitive visual information captured by AIE cameras. The main idea is to
convert images into semantically equivalent textual descriptions, ensuring that
scene-relevant information is retained while visual privacy is preserved. A
hierarchical RL strategy is employed to iteratively refine the generated text,
enhancing both semantic accuracy and privacy. Evaluation results demonstrate
significant improvements in both privacy protection and textual quality, with
the Unique Word Count increasing by approximately 77\% and Detail Density by
around 50\% compared to existing approaches.

</details>


### [137] [Visual symbolic mechanisms: Emergent symbol processing in vision language models](https://arxiv.org/abs/2506.15871)
*Rim Assouel,Declan Campbell,Taylor Webb*

Main category: cs.CV

TL;DR: VLMs can use spatial indexing for feature binding, but binding errors occur when this fails.


<details>
  <summary>Details</summary>
Motivation: Investigate if and how vision language models (VLMs) solve the longstanding 'binding problem.'

Method: Analyzed binding mechanisms in VLMs and traced errors to symbolic, content-independent spatial indices.

Result: Identified emergent symbolic mechanisms supporting feature binding via spatial indexing in VLMs, pinpointing failures as sources of errors.

Conclusion: Symbolic processing in VLMs aids binding, but improvements are needed to reduce binding failures.

Abstract: To accurately process a visual scene, observers must bind features together
to represent individual objects. This capacity is necessary, for instance, to
distinguish an image containing a red square and a blue circle from an image
containing a blue square and a red circle. Recent work has found that language
models solve this 'binding problem' via a set of symbol-like,
content-independent indices, but it is unclear whether similar mechanisms are
employed by vision language models (VLMs). This question is especially
relevant, given the persistent failures of VLMs on tasks that require binding.
Here, we identify a set of emergent symbolic mechanisms that support binding in
VLMs via a content-independent, spatial indexing scheme. Moreover, we find that
binding errors can be traced directly to failures in these mechanisms. Taken
together, these results shed light on the mechanisms that support symbol-like
processing in VLMs, and suggest possible avenues for addressing the persistent
binding failures exhibited by these models.

</details>


### [138] [Pediatric Pancreas Segmentation from MRI Scans with Deep Learning](https://arxiv.org/abs/2506.15908)
*Elif Keles,Merve Yazol,Gorkem Durak,Ziliang Hong,Halil Ertugrul Aktas,Zheyuan Zhang,Linkai Peng,Onkar Susladkar,Necati Guzelyel,Oznur Leman Boyunaga,Cemal Yazici,Mark Lowe,Aliye Uc,Ulas Bagci*

Main category: cs.CV

TL;DR: The paper evaluates and validates a deep learning algorithm, PanSegNet, for automated pediatric pancreatic MRI segmentation, achieving high accuracy across healthy and diseased cases.


<details>
  <summary>Details</summary>
Motivation: Improve pediatric pancreatic imaging by creating a deep learning tool for automated, accurate, and non-invasive pancreas segmentation in children with acute or chronic pancreatitis, where manual segmentation is cumbersome.

Method: The study retrospectively analyzed 84 pediatric MRI scans using a dataset including healthy children and those with pancreatitis. Pancreatic segmentation using PanSegNet was compared against expert manual annotations based on metrics like DSC and HD95.

Result: PanSegNet achieved high Dice scores (88% for controls, 81% for acute pancreatitis, 80% for chronic pancreatitis) and HD95 values, demonstrating comparable performance to expert radiologists with strong agreement in both manual and automated volumes (R^2 = 0.85 for controls, 0.77 for diseased cases).

Conclusion: PanSegNet is the first validated DL tool for pediatric MRI pancreas segmentation, providing a practical and accessible solution for accurate, radiation-free imaging in healthy and disease states, with resources made openly available to encourage further research.

Abstract: Objective: Our study aimed to evaluate and validate PanSegNet, a deep
learning (DL) algorithm for pediatric pancreas segmentation on MRI in children
with acute pancreatitis (AP), chronic pancreatitis (CP), and healthy controls.
Methods: With IRB approval, we retrospectively collected 84 MRI scans (1.5T/3T
Siemens Aera/Verio) from children aged 2-19 years at Gazi University
(2015-2024). The dataset includes healthy children as well as patients
diagnosed with AP or CP based on clinical criteria. Pediatric and general
radiologists manually segmented the pancreas, then confirmed by a senior
pediatric radiologist. PanSegNet-generated segmentations were assessed using
Dice Similarity Coefficient (DSC) and 95th percentile Hausdorff distance
(HD95). Cohen's kappa measured observer agreement. Results: Pancreas MRI T2W
scans were obtained from 42 children with AP/CP (mean age: 11.73 +/- 3.9 years)
and 42 healthy children (mean age: 11.19 +/- 4.88 years). PanSegNet achieved
DSC scores of 88% (controls), 81% (AP), and 80% (CP), with HD95 values of 3.98
mm (controls), 9.85 mm (AP), and 15.67 mm (CP). Inter-observer kappa was 0.86
(controls), 0.82 (pancreatitis), and intra-observer agreement reached 0.88 and
0.81. Strong agreement was observed between automated and manual volumes (R^2 =
0.85 in controls, 0.77 in diseased), demonstrating clinical reliability.
Conclusion: PanSegNet represents the first validated deep learning solution for
pancreatic MRI segmentation, achieving expert-level performance across healthy
and diseased states. This tool, algorithm, along with our annotated dataset,
are freely available on GitHub and OSF, advancing accessible, radiation-free
pediatric pancreatic imaging and fostering collaborative research in this
underserved domain.

</details>


### [139] [MoiréXNet: Adaptive Multi-Scale Demoiréing with Linear Attention Test-Time Training and Truncated Flow Matching Prior](https://arxiv.org/abs/2506.15929)
*Liangyan Li,Yimo Ning,Kevin Le,Wei Dong,Yunzhe Li,Jun Chen,Xiaohong Liu*

Main category: cs.CV

TL;DR: This paper proposes a framework for addressing image and video demoiréing by combining supervised learning with generative prior techniques.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with nonlinear degradation processes in demoiréing, either failing to completely remove moiré patterns or introducing oversmooth results due to model constraints and limited training data.

Method: The authors designed a hybrid framework using linear attention test-time training modules for supervised learning alongside Truncated Flow Matching Prior (TFMP) for improving accuracy.

Result: Improved restoration performance was achieved by combining efficient computation and refinement of high-frequency details through the framework.

Conclusion: The hybrid MAP-based framework effectively tackles nonlinear degradation processes, overcoming limitations in traditional demoiréing methods.

Abstract: This paper introduces a novel framework for image and video demoir\'eing by
integrating Maximum A Posteriori (MAP) estimation with advanced deep learning
techniques. Demoir\'eing addresses inherently nonlinear degradation processes,
which pose significant challenges for existing methods.
  Traditional supervised learning approaches either fail to remove moir\'e
patterns completely or produce overly smooth results. This stems from
constrained model capacity and scarce training data, which inadequately
represent the clean image distribution and hinder accurate reconstruction of
ground-truth images. While generative models excel in image restoration for
linear degradations, they struggle with nonlinear cases such as demoir\'eing
and often introduce artifacts.
  To address these limitations, we propose a hybrid MAP-based framework that
integrates two complementary components. The first is a supervised learning
model enhanced with efficient linear attention Test-Time Training (TTT)
modules, which directly learn nonlinear mappings for RAW-to-sRGB demoir\'eing.
The second is a Truncated Flow Matching Prior (TFMP) that further refines the
outputs by aligning them with the clean image distribution, effectively
restoring high-frequency details and suppressing artifacts. These two
components combine the computational efficiency of linear attention with the
refinement abilities of generative models, resulting in improved restoration
performance.

</details>


### [140] [Beyond Audio and Pose: A General-Purpose Framework for Video Synchronization](https://arxiv.org/abs/2506.15937)
*Yosub Shin,Igor Molybog*

Main category: cs.CV

TL;DR: The paper introduces VideoSync, a generalized video synchronization framework that works across diverse scenarios, correcting biases in prior works and establishing reproducible evaluation benchmarks.


<details>
  <summary>Details</summary>
Motivation: Prior methods for video synchronization rely on audio cues or specific visual features, which are often unreliable or unavailable. Additionally, the lack of generalizable and reproducible benchmarks hinders progress in the field.

Method: The authors propose VideoSync, a framework independent of specific feature extraction methods, and evaluate it on newly created datasets for diverse scenarios. They correct biases in previous approaches, such as in SeSyn-Net, and propose a CNN-based offset prediction model.

Result: VideoSync significantly outperforms prior methods, including SeSyn-Net, by offering a more generalizable and robust approach to video synchronization. The authors also address biases that inflated performance claims in earlier work.

Conclusion: VideoSync provides a more reliable and general framework for video synchronization compared to prior domain-limited methods, advancing the field with unbiased evaluations and reproducible benchmarks.

Abstract: Video synchronization-aligning multiple video streams capturing the same
event from different angles-is crucial for applications such as reality TV show
production, sports analysis, surveillance, and autonomous systems. Prior work
has heavily relied on audio cues or specific visual events, limiting
applicability in diverse settings where such signals may be unreliable or
absent. Additionally, existing benchmarks for video synchronization lack
generality and reproducibility, restricting progress in the field. In this
work, we introduce VideoSync, a video synchronization framework that operates
independently of specific feature extraction methods, such as human pose
estimation, enabling broader applicability across different content types. We
evaluate our system on newly composed datasets covering single-human,
multi-human, and non-human scenarios, providing both the methodology and code
for dataset creation to establish reproducible benchmarks. Our analysis reveals
biases in prior SOTA work, particularly in SeSyn-Net's preprocessing pipeline,
leading to inflated performance claims. We correct these biases and propose a
more rigorous evaluation framework, demonstrating that VideoSync outperforms
existing approaches, including SeSyn-Net, under fair experimental conditions.
Additionally, we explore various synchronization offset prediction methods,
identifying a convolutional neural network (CNN)-based model as the most
effective. Our findings advance video synchronization beyond domain-specific
constraints, making it more generalizable and robust for real-world
applications.

</details>


### [141] [Polyline Path Masked Attention for Vision Transformer](https://arxiv.org/abs/2506.15940)
*Zhongchen Zhao,Chaodong Xiao,Hui Lin,Qi Xie,Lei Zhang,Deyu Meng*

Main category: cs.CV

TL;DR: The paper introduces Polyline Path Masked Attention (PPMA), which enhances Vision Transformers (ViTs) with structured masks to improve spatial adjacency modeling, achieving superior performance in computer vision tasks.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of global dependency modeling and spatial position modeling in deep learning architectures.

Method: Proposed PPMA, combining Vision Transformers' self-attention with improved structured masks derived using a 2D polyline path scanning strategy.

Result: PPMA demonstrates better performance than state-of-the-art methods in image classification, object detection, and segmentation tasks, with notable improvements in ADE20K benchmark results.

Conclusion: Integrating PPMA into ViTs effectively augments spatial adjacency modeling and advances foundational architecture design for computer vision tasks.

Abstract: Global dependency modeling and spatial position modeling are two core issues
of the foundational architecture design in current deep learning frameworks.
Recently, Vision Transformers (ViTs) have achieved remarkable success in
computer vision, leveraging the powerful global dependency modeling capability
of the self-attention mechanism. Furthermore, Mamba2 has demonstrated its
significant potential in natural language processing tasks by explicitly
modeling the spatial adjacency prior through the structured mask. In this
paper, we propose Polyline Path Masked Attention (PPMA) that integrates the
self-attention mechanism of ViTs with an enhanced structured mask of Mamba2,
harnessing the complementary strengths of both architectures. Specifically, we
first ameliorate the traditional structured mask of Mamba2 by introducing a 2D
polyline path scanning strategy and derive its corresponding structured mask,
polyline path mask, which better preserves the adjacency relationships among
image tokens. Notably, we conduct a thorough theoretical analysis on the
structural characteristics of the proposed polyline path mask and design an
efficient algorithm for the computation of the polyline path mask. Next, we
embed the polyline path mask into the self-attention mechanism of ViTs,
enabling explicit modeling of spatial adjacency prior. Extensive experiments on
standard benchmarks, including image classification, object detection, and
segmentation, demonstrate that our model outperforms previous state-of-the-art
approaches based on both state-space models and Transformers. For example, our
proposed PPMA-T/S/B models achieve 48.7%/51.1%/52.3% mIoU on the ADE20K
semantic segmentation task, surpassing RMT-T/S/B by 0.7%/1.3%/0.3%,
respectively. Code is available at https://github.com/zhongchenzhao/PPMA.

</details>


### [142] [Heterogeneous-Modal Unsupervised Domain Adaptation via Latent Space Bridging](https://arxiv.org/abs/2506.15971)
*Jiawen Yang,Shuhao Chen,Yucong Duan,Ke Tang,Yu Zhang*

Main category: cs.CV

TL;DR: The paper proposes a novel setting called Heterogeneous-Modal Unsupervised Domain Adaptation (HMUDA) to address challenges in knowledge transfer across entirely distinct data modalities, introducing Latent Space Bridging (LSB) framework for semantic segmentation.


<details>
  <summary>Details</summary>
Motivation: Traditional unsupervised domain adaptation methods struggle to handle tasks where the source and target domains are entirely different modalities.

Method: The proposed method leverages a bridge domain with unlabeled samples from both modalities. It introduces a Latent Space Bridging (LSB) framework using a dual-branch architecture, feature consistency loss, and domain alignment loss for effective alignment across modalities and domains.

Result: Latent Space Bridging (LSB) demonstrated state-of-the-art performance on six benchmark datasets for semantic segmentation tasks, validating its effectiveness.

Conclusion: The research successfully established HMUDA to enable cross-modal knowledge transfer and presented LSB as a robust approach for handling such adaptation challenges.

Abstract: Unsupervised domain adaptation (UDA) methods effectively bridge domain gaps
but become struggled when the source and target domains belong to entirely
distinct modalities. To address this limitation, we propose a novel setting
called Heterogeneous-Modal Unsupervised Domain Adaptation (HMUDA), which
enables knowledge transfer between completely different modalities by
leveraging a bridge domain containing unlabeled samples from both modalities.
To learn under the HMUDA setting, we propose Latent Space Bridging (LSB), a
specialized framework designed for the semantic segmentation task.
Specifically, LSB utilizes a dual-branch architecture, incorporating a feature
consistency loss to align representations across modalities and a domain
alignment loss to reduce discrepancies between class centroids across domains.
Extensive experiments conducted on six benchmark datasets demonstrate that LSB
achieves state-of-the-art performance.

</details>


### [143] [LBMamba: Locally Bi-directional Mamba](https://arxiv.org/abs/2506.15976)
*Jingwei Zhang,Xi Han,Hong Qin,Mahdi S. Hosseini,Dimitris Samaras*

Main category: cs.CV

TL;DR: The paper introduces LBMamba, an optimized State Space Model enabling locally bi-directional scanning in a per-thread register, improving efficiency while removing the need for costly global backward scanning.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address inefficiencies in Mamba-based vision methods caused by the computational doubling from global backward scans needed for bi-directional receptive fields.

Method: LBMamba embeds a lightweight backward scan within the forward selective scan, functioning within per-thread registers. LBVim alternates scan directions across layers to enable global receptive fields without extra computational overhead.

Result: LBVim demonstrated superiority in classification, segmentation, and detection datasets, outperforming existing approaches on ImageNet-1K, ADE20K, and COCO datasets. LBMamba also improved performance in pathology WSI classification tasks.

Conclusion: LBMamba and LBVim provide scalable and efficient solutions for vision tasks, achieving better performance-throughput trade-offs and enhancing state-of-the-art outcomes in diverse datasets.

Abstract: Mamba, a State Space Model (SSM) that accelerates training by recasting
recurrence as a parallel selective scan, has recently emerged as a
linearly-scaling, efficient alternative to self-attention. Because of its
unidirectional nature, each state in Mamba only has information of its previous
states and is blind to states after. Current Mamba-based computer-vision
methods typically overcome this limitation by augmenting Mamba's global forward
scan with a global backward scan, forming a bi-directional scan that restores a
full receptive field. However, this operation doubles the computational load,
eroding much of the efficiency advantage that originally Mamba have. To
eliminate this extra scans, we introduce LBMamba, a locally bi-directional SSM
block that embeds a lightweight locally backward scan inside the forward
selective scan and executes it entirely in per-thread registers. Building on
LBMamba, we present LBVim, a scalable vision backbone that alternates scan
directions every two layers to recover a global receptive field without extra
backward sweeps. We validate the versatility of our approach on both natural
images and whole slide images (WSIs). We show that our LBVim constantly offers
a superior performance-throughput trade-off. That is under the same throughput,
LBVim achieves 0.8% to 1.6% higher top-1 accuracy on the ImageNet-1K
classification dataset, 0.6% to 2.7% higher mIoU on the ADE20K semantic
segmentation dataset, 0.9% higher APb and 1.1% higher APm on the COCO detection
dataset. We also integrate LBMamba into the SOTA pathology multiple instance
learning (MIL) approach, MambaMIL, which uses single directional scan.
Experiments on 3 public WSI classification datasets for show that our method
achieves a relative improvement of up to 3.06% better AUC, 3.39% better F1,
1.67% better accuracy.

</details>


### [144] [Towards Classifying Histopathological Microscope Images as Time Series Data](https://arxiv.org/abs/2506.15977)
*Sungrae Hong,Hyeongmin Park,Youngsin Ko,Sol Lee,Bryan Wong,Mun Yong Yi*

Main category: cs.CV

TL;DR: This paper introduces a method that classifies pathology microscopy images as time series data using Dynamic Time-series Warping and attention-based pooling to achieve stable and accurate medical diagnoses.


<details>
  <summary>Details</summary>
Motivation: Microscopic pathology images are essential for cancer diagnosis but have received less attention in deep learning research due to challenges like manual acquisition and weak labeling.

Method: The authors classify image sequences of varying lengths by applying Dynamic Time-series Warping (DTW) and employing attention-based pooling for reliable predictions.

Result: The presented method outperformed baselines and demonstrated effectiveness in inference strategies, achieving stable and trustworthy results for medical image analysis.

Conclusion: This approach elevates the reliability of microscopic pathology image analysis, contributing significantly to medical diagnostic practices.

Abstract: As the frontline data for cancer diagnosis, microscopic pathology images are
fundamental for providing patients with rapid and accurate treatment. However,
despite their practical value, the deep learning community has largely
overlooked their usage. This paper proposes a novel approach to classifying
microscopy images as time series data, addressing the unique challenges posed
by their manual acquisition and weakly labeled nature. The proposed method fits
image sequences of varying lengths to a fixed-length target by leveraging
Dynamic Time-series Warping (DTW). Attention-based pooling is employed to
predict the class of the case simultaneously. We demonstrate the effectiveness
of our approach by comparing performance with various baselines and showcasing
the benefits of using various inference strategies in achieving stable and
reliable results. Ablation studies further validate the contribution of each
component. Our approach contributes to medical image analysis by not only
embracing microscopic images but also lifting them to a trustworthy level of
performance.

</details>


### [145] [Advanced Sign Language Video Generation with Compressed and Quantized Multi-Condition Tokenization](https://arxiv.org/abs/2506.15980)
*Cong Wang,Zexuan Deng,Zhiwei Jiang,Fei Shen,Yafeng Yin,Shiwei Gan,Zifeng Cheng,Shiping Ge,Qing Gu*

Main category: cs.CV

TL;DR: The paper introduces SignViP, an advanced sign language video generation framework that improves identity preservation and video fidelity using fine-grained conditions and discrete tokenization methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods for sign language video generation face limitations in naturalness and expressiveness due to reliance on coarse conditions like skeleton sequences.

Method: SignViP uses a novel approach with three core components: (1) a Sign Video Diffusion Model for learning motion and appearance embeddings; (2) a Finite Scalar Quantization Autoencoder for compressing embeddings into discrete tokens; (3) a Multi-Condition Token Translator converting spoken language into discrete tokens.

Result: Experimental validations show that SignViP outperforms existing methods across various metrics including video quality, temporal coherence, and semantic fidelity.

Conclusion: The proposed SignViP framework effectively advances sign language video generation by integrating fine-grained conditions and achieving state-of-the-art performance, providing improved translation fidelity and video naturalness.

Abstract: Sign Language Video Generation (SLVG) seeks to generate identity-preserving
sign language videos from spoken language texts. Existing methods primarily
rely on the single coarse condition (\eg, skeleton sequences) as the
intermediary to bridge the translation model and the video generation model,
which limits both the naturalness and expressiveness of the generated videos.
To overcome these limitations, we propose SignViP, a novel SLVG framework that
incorporates multiple fine-grained conditions for improved generation fidelity.
Rather than directly translating error-prone high-dimensional conditions,
SignViP adopts a discrete tokenization paradigm to integrate and represent
fine-grained conditions (\ie, fine-grained poses and 3D hands). SignViP
contains three core components. (1) Sign Video Diffusion Model is jointly
trained with a multi-condition encoder to learn continuous embeddings that
encapsulate fine-grained motion and appearance. (2) Finite Scalar Quantization
(FSQ) Autoencoder is further trained to compress and quantize these embeddings
into discrete tokens for compact representation of the conditions. (3)
Multi-Condition Token Translator is trained to translate spoken language text
to discrete multi-condition tokens. During inference, Multi-Condition Token
Translator first translates the spoken language text into discrete
multi-condition tokens. These tokens are then decoded to continuous embeddings
by FSQ Autoencoder, which are subsequently injected into Sign Video Diffusion
Model to guide video generation. Experimental results show that SignViP
achieves state-of-the-art performance across metrics, including video quality,
temporal coherence, and semantic fidelity. The code is available at
https://github.com/umnooob/signvip/.

</details>


### [146] [Adversarial Attacks and Detection in Visual Place Recognition for Safer Robot Navigation](https://arxiv.org/abs/2506.15988)
*Connor Malone,Owen Claxton,Iman Shames,Michael Milford*

Main category: cs.CV

TL;DR: The paper analyzes adversarial attacks on Visual Place Recognition (VPR) and proposes a detection and active navigation framework to mitigate performance degradation.


<details>
  <summary>Details</summary>
Motivation: To address the vulnerability of Visual Place Recognition systems to adversarial attacks, particularly in the context of robotic navigation where such attacks can lead to catastrophic outcomes.

Method: The paper evaluates the impact of four common adversarial attacks and introduces four novel VPR-specific attacks. It proposes combining an Adversarial Attack Detector (AAD) with VPR systems and active navigation decisions, demonstrating this with a novel experimental paradigm.

Result: The inclusion of AADs significantly improves VPR performance, showing reductions in localization errors by ~50% with specific true and false positive detection rates. Various performance metrics are used to substantiate the benefits.

Conclusion: The findings underscore the importance of incorporating AADs in real-world VPR-based navigation systems, offering design guidelines for robust and trustworthy operation.

Abstract: Stand-alone Visual Place Recognition (VPR) systems have little defence
against a well-designed adversarial attack, which can lead to disastrous
consequences when deployed for robot navigation. This paper extensively
analyzes the effect of four adversarial attacks common in other perception
tasks and four novel VPR-specific attacks on VPR localization performance. We
then propose how to close the loop between VPR, an Adversarial Attack Detector
(AAD), and active navigation decisions by demonstrating the performance benefit
of simulated AADs in a novel experiment paradigm -- which we detail for the
robotics community to use as a system framework. In the proposed experiment
paradigm, we see the addition of AADs across a range of detection accuracies
can improve performance over baseline; demonstrating a significant improvement
-- such as a ~50% reduction in the mean along-track localization error -- can
be achieved with True Positive and False Positive detection rates of only 75%
and up to 25% respectively. We examine a variety of metrics including:
Along-Track Error, Percentage of Time Attacked, Percentage of Time in an
`Unsafe' State, and Longest Continuous Time Under Attack. Expanding further on
these results, we provide the first investigation into the efficacy of the Fast
Gradient Sign Method (FGSM) adversarial attack for VPR. The analysis in this
work highlights the need for AADs in real-world systems for trustworthy
navigation, and informs quantitative requirements for system design.

</details>


### [147] [DIGMAPPER: A Modular System for Automated Geologic Map Digitization](https://arxiv.org/abs/2506.16006)
*Weiwei Duan,Michael P. Gerlek,Steven N. Minton,Craig A. Knoblock,Fandel Lin,Theresa Chen,Leeje Jang,Sofia Kirsanova,Zekun Li,Yijun Lin,Yao-Yi Chiang*

Main category: cs.CV

TL;DR: DIGMAPPER is an automated, deep learning-powered system that digitizes historical geologic maps efficiently, addressing challenges like training data scarcity and complex visuals.


<details>
  <summary>Details</summary>
Motivation: The paper aims to streamline the labor-intensive process of digitizing historical geologic maps, which are essential for mineral resource assessments vital to energy and security sectors.

Method: DIGMAPPER utilizes a dockerized, modular architecture integrating deep learning models for layout analysis, feature extraction, and georeferencing, while employing synthetic data generation and large language models for improved accuracy.

Result: The system achieves high accuracy in extracting geologic features and reliable georeferencing, as demonstrated on 100 maps from the DARPA-USGS dataset.

Conclusion: DIGMAPPER greatly accelerates the generation of geospatial datasets, aiding national-scale mineral assessments and advancing geoscientific studies.

Abstract: Historical geologic maps contain rich geospatial information, such as rock
units, faults, folds, and bedding planes, that is critical for assessing
mineral resources essential to renewable energy, electric vehicles, and
national security. However, digitizing maps remains a labor-intensive and
time-consuming task. We present DIGMAPPER, a modular, scalable system developed
in collaboration with the United States Geological Survey (USGS) to automate
the digitization of geologic maps. DIGMAPPER features a fully dockerized,
workflow-orchestrated architecture that integrates state-of-the-art deep
learning models for map layout analysis, feature extraction, and
georeferencing. To overcome challenges such as limited training data and
complex visual content, our system employs innovative techniques, including
in-context learning with large language models, synthetic data generation, and
transformer-based models. Evaluations on over 100 annotated maps from the
DARPA-USGS dataset demonstrate high accuracy across polygon, line, and point
feature extraction, and reliable georeferencing performance. Deployed at USGS,
DIGMAPPER significantly accelerates the creation of analysis-ready geospatial
datasets, supporting national-scale critical mineral assessments and broader
geoscientific applications.

</details>


### [148] [EndoMUST: Monocular Depth Estimation for Robotic Endoscopy via End-to-end Multi-step Self-supervised Training](https://arxiv.org/abs/2506.16017)
*Liangjing Shao,Linxin Bai,Chenkang Du,Xinrong Chen*

Main category: cs.CV

TL;DR: This paper presents a novel self-supervised depth estimation framework for endoscopy using a multi-step efficient fine-tuning approach, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: To address challenges like lighting variations and sparse textures in endoscopic scenes, which complicate depth estimation and ego-motion estimation critical for robot-assisted endoscopy.

Method: The paper introduces a three-step training process using optical flow registration, multiscale image decomposition, and multiple transformation alignments, ensuring no interference of irrelevant information during each training phase.

Result: The proposed method achieves 4–10% lower error compared to existing methods, setting new benchmarks on the SCARED and Hamlyn datasets for depth estimation tasks.

Conclusion: The multistep fine-tuning approach is highly effective, demonstrating its superiority for self-supervised depth estimation in endoscopy, with code made publicly available for reproducibility.

Abstract: Monocular depth estimation and ego-motion estimation are significant tasks
for scene perception and navigation in stable, accurate and efficient
robot-assisted endoscopy. To tackle lighting variations and sparse textures in
endoscopic scenes, multiple techniques including optical flow, appearance flow
and intrinsic image decomposition have been introduced into the existing
methods. However, the effective training strategy for multiple modules are
still critical to deal with both illumination issues and information
interference for self-supervised depth estimation in endoscopy. Therefore, a
novel framework with multistep efficient finetuning is proposed in this work.
In each epoch of end-to-end training, the process is divided into three steps,
including optical flow registration, multiscale image decomposition and
multiple transformation alignments. At each step, only the related networks are
trained without interference of irrelevant information. Based on
parameter-efficient finetuning on the foundation model, the proposed method
achieves state-of-the-art performance on self-supervised depth estimation on
SCARED dataset and zero-shot depth estimation on Hamlyn dataset, with
4\%$\sim$10\% lower error. The evaluation code of this work has been published
on https://github.com/BaymaxShao/EndoMUST.

</details>


### [149] [PAROAttention: Pattern-Aware ReOrdering for Efficient Sparse and Quantized Attention in Visual Generation Models](https://arxiv.org/abs/2506.16054)
*Tianchen Zhao,Ke Hong,Xinhao Yang,Xuefeng Xiao,Huixia Li,Feng Ling,Ruiqi Xie,Siqi Chen,Hongyu Zhu,Yichong Zhang,Yu Wang*

Main category: cs.CV

TL;DR: The paper addresses the inefficiencies in visual generation with attention mechanisms by proposing a technique called PARO to reorganize attention patterns for hardware efficiency, achieving significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: The paper identifies the challenges in reducing memory and computational costs in high-resolution visual generation using attention mechanisms, particularly due to irregular attention patterns.

Method: It introduces Pattern-Aware token ReOrdering (PARO), a method to reorganize dispersed attention patterns into hardware-friendly, block-wise patterns, simplifying sparsification and quantization.

Result: PAROAttention achieves lossless visual generation metrics at reduced density (20%-30%) and bitwidth (INT8/INT4), with latency speedups ranging from 1.9x to 2.7x.

Conclusion: Reorganizing attention patterns can significantly enhance efficiency in visual generation, demonstrating potential for hardware-friendly approaches.

Abstract: In visual generation, the quadratic complexity of attention mechanisms
results in high memory and computational costs, especially for longer token
sequences required in high-resolution image or multi-frame video generation. To
address this, prior research has explored techniques such as sparsification and
quantization. However, these techniques face significant challenges under low
density and reduced bitwidths. Through systematic analysis, we identify that
the core difficulty stems from the dispersed and irregular characteristics of
visual attention patterns. Therefore, instead of introducing specialized
sparsification and quantization design to accommodate such patterns, we propose
an alternative strategy: *reorganizing* the attention pattern to alleviate the
challenges. Inspired by the local aggregation nature of visual feature
extraction, we design a novel **Pattern-Aware token ReOrdering (PARO)**
technique, which unifies the diverse attention patterns into a
hardware-friendly block-wise pattern. This unification substantially simplifies
and enhances both sparsification and quantization. We evaluate the
performance-efficiency trade-offs of various design choices and finalize a
methodology tailored for the unified pattern. Our approach, **PAROAttention**,
achieves video and image generation with lossless metrics, and nearly identical
results from full-precision (FP) baselines, while operating at notably lower
density (~20%-30%) and bitwidth (**INT8/INT4**), achieving a **1.9x** to
**2.7x** end-to-end latency speedup.

</details>


### [150] [Stepping Out of Similar Semantic Space for Open-Vocabulary Segmentation](https://arxiv.org/abs/2506.16058)
*Yong Liu,SongLi Wu,Sule Bai,Jiahao Wang,Yitong Wang,Yansong Tang*

Main category: cs.CV

TL;DR: Open-vocabulary segmentation models are tested on a new benchmark, OpenBench, which better evaluates their understanding of real-world concepts. A new model, OVSNet, improves segmentation performance across open-vocabulary tasks.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for open-vocabulary segmentation inadequately measure models' understanding of diverse, real-world concepts due to semantic overlap with training data.

Method: The authors introduce OpenBench, a benchmark that diverges from training semantics to test models on real-world concepts. Additionally, they propose OVSNet, which fuses heterogeneous features and expands the training space to improve segmentation.

Result: OVSNet achieves state-of-the-art performance on both traditional datasets and the new OpenBench, outperforming existing methods.

Conclusion: OpenBench provides a more effective evaluation framework for open-vocabulary segmentation, and OVSNet demonstrates its superiority in diverse, real-world segmentation scenarios.

Abstract: Open-vocabulary segmentation aims to achieve segmentation of arbitrary
categories given unlimited text inputs as guidance. To achieve this, recent
works have focused on developing various technical routes to exploit the
potential of large-scale pre-trained vision-language models and have made
significant progress on existing benchmarks. However, we find that existing
test sets are limited in measuring the models' comprehension of
``open-vocabulary" concepts, as their semantic space closely resembles the
training space, even with many overlapping categories. To this end, we present
a new benchmark named OpenBench that differs significantly from the training
semantics. It is designed to better assess the model's ability to understand
and segment a wide range of real-world concepts. When testing existing methods
on OpenBench, we find that their performance diverges from the conclusions
drawn on existing test sets. In addition, we propose a method named OVSNet to
improve the segmentation performance for diverse and open scenarios. Through
elaborate fusion of heterogeneous features and cost-free expansion of the
training space, OVSNet achieves state-of-the-art results on both existing
datasets and our proposed OpenBench. Corresponding analysis demonstrate the
soundness and effectiveness of our proposed benchmark and method.

</details>


### [151] [STAR-Pose: Efficient Low-Resolution Video Human Pose Estimation via Spatial-Temporal Adaptive Super-Resolution](https://arxiv.org/abs/2506.16061)
*Yucheng Jin,Jinyan Chen,Ziyue He,Baojun Han,Furan An*

Main category: cs.CV

TL;DR: STAR-Pose proposes a spatial-temporal adaptive super-resolution framework for human pose estimation in low-resolution videos.


<details>
  <summary>Details</summary>
Motivation: The paper addresses challenges in human pose estimation in low-resolution videos, emphasizing performance and computational efficiency for resource-constrained environments.

Method: The approach employs a spatial-temporal Transformer with LeakyReLU-modified linear attention, an adaptive fusion module with parallel CNN integration, and a pose-aware compound loss for structural enhancement.

Result: STAR-Pose improves mAP by up to 5.2% under 64x48 resolution and delivers faster inference, achieving 2.8x to 4.4x speed gains compared to cascaded methods.

Conclusion: The framework effectively balances accuracy and efficiency, making it suitable for human pose estimation in low-resource conditions.

Abstract: Human pose estimation in low-resolution videos presents a fundamental
challenge in computer vision. Conventional methods either assume high-quality
inputs or employ computationally expensive cascaded processing, which limits
their deployment in resource-constrained environments. We propose STAR-Pose, a
spatial-temporal adaptive super-resolution framework specifically designed for
video-based human pose estimation. Our method features a novel spatial-temporal
Transformer with LeakyReLU-modified linear attention, which efficiently
captures long-range temporal dependencies. Moreover, it is complemented by an
adaptive fusion module that integrates parallel CNN branch for local texture
enhancement. We also design a pose-aware compound loss to achieve task-oriented
super-resolution. This loss guides the network to reconstruct structural
features that are most beneficial for keypoint localization, rather than
optimizing purely for visual quality. Extensive experiments on several
mainstream video HPE datasets demonstrate that STAR-Pose outperforms existing
approaches. It achieves up to 5.2% mAP improvement under extremely
low-resolution (64x48) conditions while delivering 2.8x to 4.4x faster
inference than cascaded approaches.

</details>


### [152] [TD3Net: A Temporal Densely Connected Multi-Dilated Convolutional Network for Lipreading](https://arxiv.org/abs/2506.16073)
*Byung Hoon Lee,Wooseok Shin,Sung Won Han*

Main category: cs.CV

TL;DR: TD3Net enhances word-level lipreading by combining dense skip connections with multi-dilated temporal convolutions, yielding state-of-the-art performance with improved efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of receptive field density and information loss in existing lipreading backend architectures.

Method: TD3Net introduces dense skip connections and multi-dilated temporal convolutions to achieve a wide, dense receptive field without blind spots.

Result: TD3Net achieves comparable or superior accuracy to state-of-the-art methods on LRW and LRW-1000 datasets with fewer parameters and lower computational costs.

Conclusion: TD3Net effectively utilizes temporal features while improving modeling efficiency, offering advancements for lipreading systems.

Abstract: The word-level lipreading approach typically employs a two-stage framework
with separate frontend and backend architectures to model dynamic lip
movements. Each component has been extensively studied, and in the backend
architecture, temporal convolutional networks (TCNs) have been widely adopted
in state-of-the-art methods. Recently, dense skip connections have been
introduced in TCNs to mitigate the limited density of the receptive field,
thereby improving the modeling of complex temporal representations. However,
their performance remains constrained owing to potential information loss
regarding the continuous nature of lip movements, caused by blind spots in the
receptive field. To address this limitation, we propose TD3Net, a temporal
densely connected multi-dilated convolutional network that combines dense skip
connections and multi-dilated temporal convolutions as the backend
architecture. TD3Net covers a wide and dense receptive field without blind
spots by applying different dilation factors to skip-connected features.
Experimental results on a word-level lipreading task using two large publicly
available datasets, Lip Reading in the Wild (LRW) and LRW-1000, indicate that
the proposed method achieves performance comparable to state-of-the-art
methods. It achieved higher accuracy with fewer parameters and lower
floating-point operations compared to existing TCN-based backend architectures.
Moreover, visualization results suggest that our approach effectively utilizes
diverse temporal features while preserving temporal continuity, presenting
notable advantages in lipreading systems. The code is available at our GitHub
repository:
https://github.com/Leebh-kor/TD3Net-A-Temporal-Densely-Connected-Multi-dilated-Convolutional-Network-for-Lipreading

</details>


### [153] [PR-DETR: Injecting Position and Relation Prior for Dense Video Captioning](https://arxiv.org/abs/2506.16082)
*Yizhe Li,Sanping Zhou,Zheng Qin,Le Wang*

Main category: cs.CV

TL;DR: This paper introduces PR-DETR, a novel transformer-based framework for dense video captioning, which improves performance by incorporating position and relation priors.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing transformer-based models for dense video captioning, which require substantial training data and struggle with localization accuracy and caption coherence.

Method: The proposed framework uses position-anchored queries as position priors to provide scene-specific information and an event relation encoder to model relationships between event boundaries.

Result: The approach demonstrated competitive performance on ActivityNet Captions and YouCook2 datasets and validated the effectiveness of the introduced position and relation priors through extensive ablation studies.

Conclusion: Incorporating explicit position and relation priors into detection transformers improves both event localization accuracy and caption semantic coherence in dense video captioning tasks.

Abstract: Dense video captioning is a challenging task that aims to localize and
caption multiple events in an untrimmed video. Recent studies mainly follow the
transformer-based architecture to jointly perform the two sub-tasks, i.e.,
event localization and caption generation, in an end-to-end manner. Based on
the general philosophy of detection transformer, these methods implicitly learn
the event locations and event semantics, which requires a large amount of
training data and limits the model's performance in practice. In this paper, we
propose a novel dense video captioning framework, named PR-DETR, which injects
the explicit position and relation prior into the detection transformer to
improve the localization accuracy and caption quality, simultaneously. On the
one hand, we first generate a set of position-anchored queries to provide the
scene-specific position and semantic information about potential events as
position prior, which serves as the initial event search regions to eliminate
the implausible event proposals. On the other hand, we further design an event
relation encoder to explicitly calculate the relationship between event
boundaries as relation prior to guide the event interaction to improve the
semantic coherence of the captions. Extensive ablation studies are conducted to
verify the effectiveness of the position and relation prior. Experimental
results also show the competitive performance of our method on ActivityNet
Captions and YouCook2 datasets.

</details>


### [154] [AutoV: Learning to Retrieve Visual Prompt for Large Vision-Language Models](https://arxiv.org/abs/2506.16112)
*Yuan Zhang,Chun-Kai Fan,Tao Huang,Ming Lu,Sicheng Yu,Junwen Pan,Kuan Cheng,Qi She,Shanghang Zhang*

Main category: cs.CV

TL;DR: AutoV is a system designed to automatically select optimal visual prompts for large vision-language models (LVLMs), enhancing their performance on various tasks.


<details>
  <summary>Details</summary>
Motivation: Manual design of visual prompts for LVLMs is challenging, time-consuming, and often results in sub-optimal performance.

Method: AutoV evaluates and ranks different visual prompts for LVLMs based on prediction losses using an automatic data collection and labeling pipeline. The system is trained using these rankings as supervision signals to automatically choose optimal visual prompts.

Result: AutoV improves the performance of multiple LVLMs, achieving notable accuracy gains such as a 1.7% increase for LLaVA-OV on LLaVA$^{\text{Wild}}$ and a 1.9% increase for Qwen2.5-VL on MMMU.

Conclusion: AutoV demonstrates significant potential as an efficient and effective method for enhancing LVLMs by automating the design of visual prompts.

Abstract: Inspired by text prompts in large language models (LLMs), visual prompts have
been explored to enhance the reasoning capabilities of large vision-language
models (LVLMs). Current methods design heuristic visual prompts, such as
overlaying a text-query-guided attention heatmap on the original input image.
However, designing effective prompts manually is challenging and
time-consuming, and it often fails to explore the benefits of different visual
prompts, leading to sub-optimal performance. To this end, we propose
\textbf{AutoV} that learns to automatically select the optimal visual prompt
from various candidates based on given textual queries and the input image. To
train AutoV, we developed an automatic data collection and labeling pipeline
that evaluates various visual prompts with a pre-trained LVLM. We input a set
of visual prompts into the LVLM and rank them according to the prediction
losses generated by the model. Using the ranking as a supervision signal, we
train AutoV to automatically choose the optimal visual prompt from various
visual prompts for LVLMs. Experimental results indicate that AutoV enhances the
performance of various LVLMs across multiple popular image understanding tasks.
For instance, LLaVA-OV with AutoV achieves $\textbf{1.7}\%$ accuracy gain on
LLaVA$^{\text{Wild}}$, and AutoV boosts Qwen2.5-VL by $\textbf{1.9}\%$ on MMMU,
highlighting its potential as an optimal visual prompting method for LVLMs.

</details>


### [155] [FastInit: Fast Noise Initialization for Temporally Consistent Video Generation](https://arxiv.org/abs/2506.16119)
*Chengyu Bai,Yuming Li,Zhongyu Zhao,Jintao Chen,Peidong Jia,Qi She,Ming Lu,Shanghang Zhang*

Main category: cs.CV

TL;DR: FastInit introduces a faster method for achieving temporal consistency in video generation using diffusion models without iterative refinement.


<details>
  <summary>Details</summary>
Motivation: Address the computational inefficiency introduced by iterative refinement in video generation while maintaining high temporal consistency.

Method: Developed FastInit, which uses a Video Noise Prediction Network (VNPNet) to refine noise in one forward pass based on a text prompt, eliminating the need for iterative processes.

Result: FastInit consistently enhances video quality and temporal consistency across frames, validated by extensive experiments with various models.

Conclusion: FastInit successfully tackles the training-inference gap, offers efficient video generation, and promises practical applicability with released code and datasets.

Abstract: Video generation has made significant strides with the development of
diffusion models; however, achieving high temporal consistency remains a
challenging task. Recently, FreeInit identified a training-inference gap and
introduced a method to iteratively refine the initial noise during inference.
However, iterative refinement significantly increases the computational cost
associated with video generation. In this paper, we introduce FastInit, a fast
noise initialization method that eliminates the need for iterative refinement.
FastInit learns a Video Noise Prediction Network (VNPNet) that takes random
noise and a text prompt as input, generating refined noise in a single forward
pass. Therefore, FastInit greatly enhances the efficiency of video generation
while achieving high temporal consistency across frames. To train the VNPNet,
we create a large-scale dataset consisting of pairs of text prompts, random
noise, and refined noise. Extensive experiments with various text-to-video
models show that our method consistently improves the quality and temporal
consistency of the generated videos. FastInit not only provides a substantial
improvement in video generation but also offers a practical solution that can
be applied directly during inference. The code and dataset will be released.

</details>


### [156] [Neurosymbolic Object-Centric Learning with Distant Supervision](https://arxiv.org/abs/2506.16129)
*Stefano Colamonaco,David Debot,Giuseppe Marra*

Main category: cs.CV

TL;DR: The paper introduces DeepObjectLog, a neurosymbolic model that learns object-centric representations directly from raw data using distant supervision while combining object extraction and probabilistic logical reasoning for improved performance.


<details>
  <summary>Details</summary>
Motivation: Existing systems in relational learning often rely on object-level supervision or predefined object decomposition, limiting their ability to learn directly from raw data and generalize across structured domains.

Method: The authors propose DeepObjectLog, a model combining a perceptual module for object extraction with a symbolic reasoning layer using probabilistic logic programming. This integration helps discover meaningful objects through sound probabilistic inference.

Result: DeepObjectLog demonstrates superior performance compared to neural and neurosymbolic baselines in tasks involving unseen object compositions, tasks, and variation in object numbers.

Conclusion: The approach effectively addresses the limitations of previous relational learning systems by leveraging neurosymbolic reasoning and distant supervision to improve object-centric representation learning and generalization.

Abstract: Relational learning enables models to generalize across structured domains by
reasoning over objects and their interactions. While recent advances in
neurosymbolic reasoning and object-centric learning bring us closer to this
goal, existing systems rely either on object-level supervision or on a
predefined decomposition of the input into objects. In this work, we propose a
neurosymbolic formulation for learning object-centric representations directly
from raw unstructured perceptual data and using only distant supervision. We
instantiate this approach in DeepObjectLog, a neurosymbolic model that
integrates a perceptual module, which extracts relevant object representations,
with a symbolic reasoning layer based on probabilistic logic programming. By
enabling sound probabilistic logical inference, the symbolic component
introduces a novel learning signal that further guides the discovery of
meaningful objects in the input. We evaluate our model across a diverse range
of generalization settings, including unseen object compositions, unseen tasks,
and unseen number of objects. Experimental results show that our method
outperforms neural and neurosymbolic baselines across the tested settings.

</details>


### [157] [GRPO-CARE: Consistency-Aware Reinforcement Learning for Multimodal Reasoning](https://arxiv.org/abs/2506.16141)
*Yi Chen,Yuying Ge,Rui Wang,Yixiao Ge,Junhao Cheng,Ying Shan,Xihui Liu*

Main category: cs.CV

TL;DR: The paper proposes GRPO-CARE, a reinforcement learning framework to enhance logical reasoning and answer consistency in multimodal language models, demonstrating improved performance and coherence over the existing GRPO approach.


<details>
  <summary>Details</summary>
Motivation: To address limitations in existing GRPO methods, which improve answer accuracy in multimodal language models but often reduce reasoning consistency.

Method: The authors introduced SEED-Bench-R1 for rigorous evaluation and then proposed GRPO-CARE, a two-tiered reward system combining answer correctness with reasoning coherence.

Result: GRPO-CARE showed a 6.7% gain on the hardest evaluation level and a 24.5% improvement in consistency. It also demonstrated strong adaptability to other video understanding tasks.

Conclusion: The work introduces a new benchmark and post-training framework, providing a significant step towards creating more interpretable and reliable multimodal language models.

Abstract: Recent reinforcement learning approaches, such as outcome-supervised GRPO,
have advanced Chain-of-Thought reasoning in large language models (LLMs), yet
their adaptation to multimodal LLMs (MLLMs) is unexplored. To address the lack
of rigorous evaluation for MLLM post-training methods, we introduce
SEED-Bench-R1, a benchmark with complex real-world videos requiring balanced
perception and reasoning. It offers a large training set and evaluates
generalization across three escalating challenges: in-distribution,
cross-environment, and cross-environment-task scenarios. Using SEED-Bench-R1,
we find that standard GRPO, while improving answer accuracy, often reduces
logical coherence between reasoning steps and answers, with only a 57.9%
consistency rate. This stems from reward signals focusing solely on final
answers, encouraging shortcuts, and strict KL penalties limiting exploration.To
address this, we propose GRPO-CARE, a consistency-aware RL framework optimizing
both answer correctness and reasoning coherence without explicit supervision.
GRPO-CARE introduces a two-tiered reward: (1) a base reward for answer
correctness, and (2) an adaptive consistency bonus, computed by comparing the
model's reasoning-to-answer likelihood (via a slowly-evolving reference model)
against group peers.This dual mechanism amplifies rewards for reasoning paths
that are both correct and logically consistent. Replacing KL penalties with
this adaptive bonus, GRPO-CARE outperforms standard GRPO on SEED-Bench-R1,
achieving a 6.7% performance gain on the hardest evaluation level and a 24.5%
improvement in consistency. It also shows strong transferability, improving
model performance across diverse video understanding benchmarks. Our work
contributes a systematically designed benchmark and a generalizable
post-training framework, advancing the development of more interpretable and
robust MLLMs.

</details>


### [158] [MBA: Multimodal Bidirectional Attack for Referring Expression Segmentation Models](https://arxiv.org/abs/2506.16157)
*Xingbai Chen,Tingchao Fu,Renyang Liu,Wei Zhou,Chao Yi*

Main category: cs.CV

TL;DR: This paper develops a new adversarial attack strategy that improves on vulnerabilities in Referring Expression Segmentation (RES) by optimizing adversarial examples for both image and text modalities.


<details>
  <summary>Details</summary>
Motivation: RES models are widely used in real-world scenarios but their adversarial robustness, especially under varied textual inputs, is largely unexplored.

Method: The paper introduces a Multimodal Bidirectional Attack strategy incorporating learnable proxy textual embedding perturbation and dual optimization on both image and textual modalities to enhance cross-text transferability.

Result: Experimental results show that the proposed method outperforms existing adversarial attack strategies on multiple RES models and datasets.

Conclusion: The new attack strategy effectively exposes vulnerabilities in RES models and generalizes well under diverse textual inputs, advancing adversarial research in multimodal AI.

Abstract: Referring Expression Segmentation (RES) enables precise object segmentation
in images based on natural language descriptions, offering high flexibility and
broad applicability in real-world vision tasks. Despite its impressive
performance, the robustness of RES models against adversarial examples remains
largely unexplored. While prior adversarial attack methods have explored
adversarial robustness on conventional segmentation models, they perform poorly
when directly applied to RES, failing to expose vulnerabilities in its
multimodal structure. Moreover, in practical open-world scenarios, users
typically issue multiple, diverse referring expressions to interact with the
same image, highlighting the need for adversarial examples that generalize
across varied textual inputs. To address these multimodal challenges, we
propose a novel adversarial attack strategy termed \textbf{Multimodal
Bidirectional Attack}, tailored for RES models. Our method introduces learnable
proxy textual embedding perturbation and jointly performs visual-aligned
optimization on the image modality and textual-adversarial optimization on the
textual modality during attack generation. This dual optimization framework
encourages adversarial images to actively adapt to more challenging text
embedding during optimization, thereby enhancing their cross-text
transferability, which refers to the ability of adversarial examples to remain
effective under a variety of unseen or semantically diverse textual inputs.
Extensive experiments conducted on multiple RES models and benchmark datasets
demonstrate the superior effectiveness of our method compared to existing
methods.

</details>


### [159] [Co-Speech Gesture and Facial Expression Generation for Non-Photorealistic 3D Characters](https://arxiv.org/abs/2506.16159)
*Taisei Omine,Naoyuki Kawabata,Fuminori Homma*

Main category: cs.CV

TL;DR: This study develops methods to enhance emotional and semantic expressions of non-photorealistic characters using data from comics and dialogue-based gestures, achieving better results than previous research.


<details>
  <summary>Details</summary>
Motivation: Existing research on conversational AI and bodily expressions is focused on photorealistic avatars, leaving a gap in expressive methods for non-photorealistic characters like anime.

Method: The study incorporates expression data extracted from comics and integrates dialogue-specific semantic gesture techniques to enhance emotional expressiveness for non-photorealistic characters.

Result: A user study confirmed that the proposed methods yielded significant improvements in emotional and semantic expressions compared to prior work.

Conclusion: The research successfully advances the expressiveness of non-photorealistic characters, bridging a gap in conversational AI, through an innovative use of data and gesture integration.

Abstract: With the advancement of conversational AI, research on bodily expressions,
including gestures and facial expressions, has also progressed. However, many
existing studies focus on photorealistic avatars, making them unsuitable for
non-photorealistic characters, such as those found in anime. This study
proposes methods for expressing emotions, including exaggerated expressions
unique to non-photorealistic characters, by utilizing expression data extracted
from comics and dialogue-specific semantic gestures. A user study demonstrated
significant improvements across multiple aspects when compared to existing
research.

</details>


### [160] [Align the GAP: Prior-based Unified Multi-Task Remote Physiological Measurement Framework For Domain Generalization and Personalization](https://arxiv.org/abs/2506.16160)
*Jiyao Wang,Xiao Yang,Hao Lu,Dengbo He,Kaishun Wu*

Main category: cs.CV

TL;DR: The study introduces a unified framework combining multi-source domain generalization (MSSDG) and test-time personalized adaptation (TTPA) for remote physiological measurement.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of partial labeling, environmental noise, and the significant gap between generalization and personalization techniques in multi-task remote physiological measurement.

Method: The framework disentangles facial video information into invariant semantics, individual biases, and noise, and integrates multiple modules leveraging priors to address MSSDG and TTPA simultaneously.

Result: The framework demonstrated strong performance through extensive experiments on six public datasets and a newly introduced real-world driving dataset.

Conclusion: The proposed approach effectively bridges the gap between generalization and personalization in remote physiological estimation, providing a robust solution for MSSDG and TTPA.

Abstract: Multi-source synsemantic domain generalization (MSSDG) for multi-task remote
physiological measurement seeks to enhance the generalizability of these
metrics and attracts increasing attention. However, challenges like partial
labeling and environmental noise may disrupt task-specific accuracy. Meanwhile,
given that real-time adaptation is necessary for personalized products, the
test-time personalized adaptation (TTPA) after MSSDG is also worth exploring,
while the gap between previous generalization and personalization methods is
significant and hard to fuse. Thus, we proposed a unified framework for
MSSD\textbf{G} and TTP\textbf{A} employing \textbf{P}riors (\textbf{GAP}) in
biometrics and remote photoplethysmography (rPPG). We first disentangled
information from face videos into invariant semantics, individual bias, and
noise. Then, multiple modules incorporating priors and our observations were
applied in different stages and for different facial information. Then, based
on the different principles of achieving generalization and personalization,
our framework could simultaneously address MSSDG and TTPA under multi-task
remote physiological estimation with minimal adjustments. We expanded the MSSDG
benchmark to the TTPA protocol on six publicly available datasets and
introduced a new real-world driving dataset with complete labeling. Extensive
experiments that validated our approach, and the codes along with the new
dataset will be released.

</details>


### [161] [Integrating Generative Adversarial Networks and Convolutional Neural Networks for Enhanced Traffic Accidents Detection and Analysis](https://arxiv.org/abs/2506.16186)
*Zhenghao Xi,Xiang Liu,Yaqi Liu,Yitong Cai,Yangyu Zheng*

Main category: cs.CV

TL;DR: This paper presents a deep learning-based framework for detecting accidents using CCTV footage, achieving high accuracy rates. The approach combines GANs for data synthesis and CNNs for model training.


<details>
  <summary>Details</summary>
Motivation: The motivation is the increasing number of car accidents worldwide, which necessitates an automated system for timely accident detection to save lives.

Method: The framework uses GANs for creating synthetic data and CNNs, FTCNN, and VIT for training models. Video frames were sourced from YouTube, processed through resizing, enhancement, and normalization.

Result: The FTCNN and VIT models achieved 94% and 95% accuracy in detecting accidents, outperforming the traditional CNN model, which achieved 88%.

Conclusion: The high accuracy of the proposed framework demonstrates its suitability for real-time accident detection, setting the stage for future intelligent surveillance systems and emergency management integration.

Abstract: Accident detection using Closed Circuit Television (CCTV) footage is one of
the most imperative features for enhancing transport safety and efficient
traffic control. To this end, this research addresses the issues of supervised
monitoring and data deficiency in accident detection systems by adapting
excellent deep learning technologies. The motivation arises from rising
statistics in the number of car accidents worldwide; this calls for innovation
and the establishment of a smart, efficient and automated way of identifying
accidents and calling for help to save lives. Addressing the problem of the
scarcity of data, the presented framework joins Generative Adversarial Networks
(GANs) for synthesizing data and Convolutional Neural Networks (CNN) for model
training. Video frames for accidents and non-accidents are collected from
YouTube videos, and we perform resizing, image enhancement and image
normalisation pixel range adjustments. Three models are used: CNN, Fine-tuned
Convolutional Neural Network (FTCNN) and Vision Transformer (VIT) worked best
for detecting accidents from CCTV, obtaining an accuracy rate of 94% and 95%,
while the CNN model obtained 88%. Such results show that the proposed framework
suits traffic safety applications due to its high real-time accident detection
capabilities and broad-scale applicability. This work lays the foundation for
intelligent surveillance systems in the future for real-time traffic
monitoring, smart city framework, and integration of intelligent surveillance
systems into emergency management systems.

</details>


### [162] [VideoGAN-based Trajectory Proposal for Automated Vehicles](https://arxiv.org/abs/2506.16209)
*Annajoyce Mariani,Kira Maag,Hanno Gottschalk*

Main category: cs.CV

TL;DR: The paper explores using a GAN trained on bird's-eye view videos to generate realistic traffic trajectories, emphasizing quick training and fast inference while maintaining spatial and dynamic accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing methods in representing complex multimodal traffic trajectories effectively, and to enhance the automation level in road vehicles.

Method: A GAN-based pipeline is proposed, which processes low-resolution bird's-eye view occupancy grid videos to extract abstract trajectory data, utilizing single-frame object detection and frame-to-frame matching.

Result: The approach achieved efficient training within 100 GPU hours and inference times under 20ms, producing trajectory distributions closely aligned with the physical realism of the ground truth in the Waymo Open Motion Dataset.

Conclusion: GANs can effectively generate statistically accurate and spatially realistic traffic trajectories, offering advantages in training speed and inference time over diffusion models.

Abstract: Being able to generate realistic trajectory options is at the core of
increasing the degree of automation of road vehicles. While model-driven,
rule-based, and classical learning-based methods are widely used to tackle
these tasks at present, they can struggle to effectively capture the complex,
multimodal distributions of future trajectories. In this paper we investigate
whether a generative adversarial network (GAN) trained on videos of bird's-eye
view (BEV) traffic scenarios can generate statistically accurate trajectories
that correctly capture spatial relationships between the agents. To this end,
we propose a pipeline that uses low-resolution BEV occupancy grid videos as
training data for a video generative model. From the generated videos of
traffic scenarios we extract abstract trajectory data using single-frame object
detection and frame-to-frame object matching. We particularly choose a GAN
architecture for the fast training and inference times with respect to
diffusion models. We obtain our best results within 100 GPU hours of training,
with inference times under 20\,ms. We demonstrate the physical realism of the
proposed trajectories in terms of distribution alignment of spatial and dynamic
parameters with respect to the ground truth videos from the Waymo Open Motion
Dataset.

</details>


### [163] [FOCoOp: Enhancing Out-of-Distribution Robustness in Federated Prompt Learning for Vision-Language Models](https://arxiv.org/abs/2506.16218)
*Xinting Liao,Weiming Liu,Jiaming Qian,Pengyang Zhou,Jiahe Xu,Wenjie Wang,Chaochao Chen,Xiaolin Zheng,Tat-Seng Chua*

Main category: cs.CV

TL;DR: The paper presents a Federated OOD-aware Context Optimization (FOCoOp) framework to address the trade-off between performance and robustness in Federated Prompt Learning (FPL), especially under out-of-distribution (OOD) scenarios.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of current FPL approaches, which struggle with a trade-off between performance and robustness in out-of-distribution (OOD) shifts and are challenged by in-distribution (ID) data heterogeneity.

Method: FOCoOp introduces three sets of prompts (ID global, local, and OOD) to distinguish distributions at class and distribution levels and applies bi-level distributionally robust optimization. It also ensures discrimination consistency using semi-unbalanced optimal transport.

Result: FOCoOp showed enhanced robustness to OOD shifts and better handling of decentralized heterogeneous distributions in experiments on real-world datasets.

Conclusion: FOCoOp is effective in improving the performance and robustness of Federated Prompt Learning under OOD scenarios by leveraging diverse prompts and robust optimization techniques.

Abstract: Federated prompt learning (FPL) for vision-language models is a powerful
approach to collaboratively adapt models across distributed clients while
preserving data privacy. However, existing FPL approaches suffer from a
trade-off between performance and robustness, particularly in
out-of-distribution (OOD) shifts, limiting their reliability in real-world
scenarios. The inherent in-distribution (ID) data heterogeneity among different
clients makes it more challenging to maintain this trade-off. To fill this gap,
we introduce a Federated OOD-aware Context Optimization (FOCoOp) framework,
which captures diverse distributions among clients using ID global prompts,
local prompts, and OOD prompts. Specifically, FOCoOp leverages three sets of
prompts to create both class-level and distribution-level separations, which
adapt to OOD shifts through bi-level distributionally robust optimization.
Additionally, FOCoOp improves the discrimination consistency among clients,
i.e., calibrating global prompts, seemingly OOD prompts, and OOD prompts by
semi-unbalanced optimal transport. The extensive experiments on real-world
datasets demonstrate that FOCoOp effectively captures decentralized
heterogeneous distributions and enhances robustness of different OOD shifts.
The project is available at GitHub.

</details>


### [164] [R3eVision: A Survey on Robust Rendering, Restoration, and Enhancement for 3D Low-Level Vision](https://arxiv.org/abs/2506.16262)
*Weeyoung Kwon,Jeahun Sung,Minkyu Jeon,Chanho Eom,Jihyong Oh*

Main category: cs.CV

TL;DR: This paper surveys 3D Low-Level Vision (3D LLV), focusing on robust 3D scene reconstruction and rendering under real-world degradations such as low resolution, noise, or weather effects.


<details>
  <summary>Details</summary>
Motivation: To address limitations of existing neural rendering models that assume clean, high-resolution inputs and lack robustness to real-world degradations.

Method: The paper formalizes degradation-aware rendering problems, categorizes recent approaches integrating 2D Low-Level Vision tasks with 3D neural rendering, and identifies key challenges such as spatio-temporal consistency and optimization.

Result: It reviews methods, datasets, and evaluation protocols under the emerging 3D LLV field, demonstrating progress in improving 3D reconstruction and rendering in adverse conditions.

Conclusion: 3D LLV is positioned as a fundamental direction for enhancing robust 3D content generation and reconstruction in applications like autonomous driving, AR/VR, and robotics.

Abstract: Neural rendering methods such as Neural Radiance Fields (NeRF) and 3D
Gaussian Splatting (3DGS) have achieved significant progress in photorealistic
3D scene reconstruction and novel view synthesis. However, most existing models
assume clean and high-resolution (HR) multi-view inputs, which limits their
robustness under real-world degradations such as noise, blur, low-resolution
(LR), and weather-induced artifacts. To address these limitations, the emerging
field of 3D Low-Level Vision (3D LLV) extends classical 2D Low-Level Vision
tasks including super-resolution (SR), deblurring, weather degradation removal,
restoration, and enhancement into the 3D spatial domain. This survey, referred
to as R\textsuperscript{3}eVision, provides a comprehensive overview of robust
rendering, restoration, and enhancement for 3D LLV by formalizing the
degradation-aware rendering problem and identifying key challenges related to
spatio-temporal consistency and ill-posed optimization. Recent methods that
integrate LLV into neural rendering frameworks are categorized to illustrate
how they enable high-fidelity 3D reconstruction under adverse conditions.
Application domains such as autonomous driving, AR/VR, and robotics are also
discussed, where reliable 3D perception from degraded inputs is critical. By
reviewing representative methods, datasets, and evaluation protocols, this work
positions 3D LLV as a fundamental direction for robust 3D content generation
and scene-level reconstruction in real-world environments.

</details>


### [165] [Dense 3D Displacement Estimation for Landslide Monitoring via Fusion of TLS Point Clouds and Embedded RGB Images](https://arxiv.org/abs/2506.16265)
*Zhaoyi Wang,Jemil Avers Butt,Shengyu Huang,Tomislav Medic,Andreas Wieser*

Main category: cs.CV

TL;DR: This paper presents a method to improve landslide monitoring by combining 3D point clouds and RGB images for dense 3D displacement estimation, achieving high spatial coverage and accuracy.


<details>
  <summary>Details</summary>
Motivation: The study aims to address limitations in existing point cloud-based methods that rely solely on geometry or radiometry and often provide sparse or non-3D displacement data.

Method: A hierarchical coarse-to-fine approach is introduced, which fuses 3D geometry and 2D RGB image features, refining patch-level matches with geometric consistency checks and rigid transformations.

Result: The method achieves high spatial coverage (79% and 97%) and high accuracy in displacement estimates with deviations of 0.15 m and 0.25 m compared to external measurements, and outperforms state-of-the-art methods.

Conclusion: This innovative approach provides a highly practical and adaptable solution for landslide monitoring using TLS data, with potential applications in other domains. Data and source code are also made publicly available.

Abstract: Landslide monitoring is essential for understanding geohazards and mitigating
associated risks. However, existing point cloud-based methods typically rely on
either geometric or radiometric information and often yield sparse or non-3D
displacement estimates. In this paper, we propose a hierarchical
partition-based coarse-to-fine approach that fuses 3D point clouds and
co-registered RGB images to estimate dense 3D displacement vector fields. We
construct patch-level matches using both 3D geometry and 2D image features.
These matches are refined via geometric consistency checks, followed by rigid
transformation estimation per match. Experimental results on two real-world
landslide datasets demonstrate that our method produces 3D displacement
estimates with high spatial coverage (79% and 97%) and high accuracy.
Deviations in displacement magnitude with respect to external measurements
(total station or GNSS observations) are 0.15 m and 0.25 m on the two datasets,
respectively, and only 0.07 m and 0.20 m compared to manually derived
references. These values are below the average scan resolutions (0.08 m and
0.30 m). Our method outperforms the state-of-the-art method F2S3 in spatial
coverage while maintaining comparable accuracy. Our approach offers a practical
and adaptable solution for TLS-based landslide monitoring and is extensible to
other types of point clouds and monitoring tasks. Our example data and source
code are publicly available at https://github.com/zhaoyiww/fusion4landslide.

</details>


### [166] [Fine-grained Image Retrieval via Dual-Vision Adaptation](https://arxiv.org/abs/2506.16273)
*Xin Jiang,Meiqi Cao,Hao Tang,Fei Shen,Zechao Li*

Main category: cs.CV

TL;DR: The paper introduces a novel Dual-Vision Adaptation (DVA) approach for fine-grained image retrieval (FGIR), addressing challenges related to overfitting and reduced generalization. DVA adapts pre-trained models through sample and feature adaptation techniques.


<details>
  <summary>Details</summary>
Motivation: Current FGIR methods struggle with overfitting and loss of generalization due to reliance on pairwise similarity constraints or localization sub-networks, which forget pre-trained knowledge.

Method: The authors propose DVA with Object-Perceptual Adaptation to modify input samples and In-Context Adaptation with small parameter additions for feature adaptation, while preserving pre-trained parameters. Additionally, Discrimination Perception Transfer employs knowledge distillation for efficient retrieval.

Result: Extensive experiments demonstrate that DVA achieves strong performance across six fine-grained datasets, both in-distribution and out-of-distribution, with fewer learnable parameters.

Conclusion: DVA effectively improves FGIR by leveraging pre-trained knowledge while maintaining generalization and efficiency, making it a robust alternative to existing methods.

Abstract: Fine-Grained Image Retrieval~(FGIR) faces challenges in learning
discriminative visual representations to retrieve images with similar
fine-grained features. Current leading FGIR solutions typically follow two
regimes: enforce pairwise similarity constraints in the semantic embedding
space, or incorporate a localization sub-network to fine-tune the entire model.
However, such two regimes tend to overfit the training data while forgetting
the knowledge gained from large-scale pre-training, thus reducing their
generalization ability. In this paper, we propose a Dual-Vision Adaptation
(DVA) approach for FGIR, which guides the frozen pre-trained model to perform
FGIR through collaborative sample and feature adaptation. Specifically, we
design Object-Perceptual Adaptation, which modifies input samples to help the
pre-trained model perceive critical objects and elements within objects that
are helpful for category prediction. Meanwhile, we propose In-Context
Adaptation, which introduces a small set of parameters for feature adaptation
without modifying the pre-trained parameters. This makes the FGIR task using
these adjusted features closer to the task solved during the pre-training.
Additionally, to balance retrieval efficiency and performance, we propose
Discrimination Perception Transfer to transfer the discriminative knowledge in
the object-perceptual adaptation to the image encoder using the knowledge
distillation mechanism. Extensive experiments show that DVA has fewer learnable
parameters and performs well on three in-distribution and three
out-of-distribution fine-grained datasets.

</details>


### [167] [SycnMapV2: Robust and Adaptive Unsupervised Segmentation](https://arxiv.org/abs/2506.16297)
*Heng Zhang,Zikang Wan,Danilo Vasconcellos Vargas*

Main category: cs.CV

TL;DR: SyncMapV2 introduces an unsupervised segmentation method that is highly robust to visual corruption and adapts online, outperforming state-of-the-art AI methods.


<details>
  <summary>Details</summary>
Motivation: To address the shortcomings of existing AI algorithms in maintaining robustness and segmentation accuracy under noisy and corrupted conditions, particularly compared to human vision.

Method: Based on self-organizing dynamical equations and random networks, SyncMapV2 adapts online to new inputs without re-initialization, bypassing the need for robust training, supervision, or loss functions.

Result: SyncMapV2 achieves superior robustness to various types of corruption, such as noise, weather, and blur, with minimal mIoU drops compared to current state-of-the-art methods.

Conclusion: SyncMapV2 paves the way for more robust and adaptive AI systems, linking accuracy, adaptability, and efficiency without supervised training, and mimics human-like continuous adaptability.

Abstract: Human vision excels at segmenting visual cues without the need for explicit
training, and it remains remarkably robust even as noise severity increases. In
contrast, existing AI algorithms struggle to maintain accuracy under similar
conditions. Here, we present SyncMapV2, the first to solve unsupervised
segmentation with state-of-the-art robustness. SyncMapV2 exhibits a minimal
drop in mIoU, only 0.01%, under digital corruption, compared to a 23.8% drop
observed in SOTA methods.This superior performance extends across various types
of corruption: noise (7.3% vs. 37.7%), weather (7.5% vs. 33.8%), and blur (7.0%
vs. 29.5%). Notably, SyncMapV2 accomplishes this without any robust training,
supervision, or loss functions. It is based on a learning paradigm that uses
self-organizing dynamical equations combined with concepts from random
networks. Moreover,unlike conventional methods that require re-initialization
for each new input, SyncMapV2 adapts online, mimicking the continuous
adaptability of human vision. Thus, we go beyond the accurate and robust
results, and present the first algorithm that can do all the above online,
adapting to input rather than re-initializing. In adaptability tests, SyncMapV2
demonstrates near-zero performance degradation, which motivates and fosters a
new generation of robust and adaptive intelligence in the near future.

</details>


### [168] [Learning Multi-scale Spatial-frequency Features for Image Denoising](https://arxiv.org/abs/2506.16307)
*Xu Zhao,Chen Zhao,Xiantao Hu,Hongliang Zhang,Ying Tai,Jian Yang*

Main category: cs.CV

TL;DR: The authors propose MADNet, a denoising architecture utilizing multi-scale image inputs and adaptive frequency separation to outperform existing state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Current image denoising techniques rely on fixed architectures like single-input single-output Unets, neglecting both pixel-level multi-scale representations and the distinct characteristics of high- and low-frequency noise.

Method: The authors introduce MADNet, a multi-scale adaptive dual-domain network. It uses image pyramid inputs, an adaptive spatial-frequency learning unit (ASFU) with learnable masks to manage frequencies, and global feature fusion for enhanced scaling and features.

Result: MADNet demonstrates superior performance on synthetic and real noisy datasets compared to current state-of-the-art denoising methods.

Conclusion: MADNet's integration of adaptive spatial-frequency learning and multi-scale pyramidal inputs effectively addresses limitations of existing methods, making it highly impactful for noise reduction applications.

Abstract: Recent advancements in multi-scale architectures have demonstrated
exceptional performance in image denoising tasks. However, existing
architectures mainly depends on a fixed single-input single-output Unet
architecture, ignoring the multi-scale representations of pixel level. In
addition, previous methods treat the frequency domain uniformly, ignoring the
different characteristics of high-frequency and low-frequency noise. In this
paper, we propose a novel multi-scale adaptive dual-domain network (MADNet) for
image denoising. We use image pyramid inputs to restore noise-free results from
low-resolution images. In order to realize the interaction of high-frequency
and low-frequency information, we design an adaptive spatial-frequency learning
unit (ASFU), where a learnable mask is used to separate the information into
high-frequency and low-frequency components. In the skip connections, we design
a global feature fusion block to enhance the features at different scales.
Extensive experiments on both synthetic and real noisy image datasets verify
the effectiveness of MADNet compared with current state-of-the-art denoising
approaches.

</details>


### [169] [Segment Anything for Satellite Imagery: A Strong Baseline and a Regional Dataset for Automatic Field Delineation](https://arxiv.org/abs/2506.16318)
*Carmelo Scribano,Elena Govi,Paolo bertellini,Simone Parisi,Giorgia Franchini,Marko Bertogna*

Main category: cs.CV

TL;DR: This paper introduces a pipeline for automated field delineation using the Segment Anything Model (SAM), including dataset expansion and the ERAS dataset for improved accuracy and generalization.


<details>
  <summary>Details</summary>
Motivation: The motivation is to automate the accurate mapping of agricultural field boundaries to enhance the efficiency of agricultural operations and reduce the costs associated with ground surveys.

Method: The method involves fine-tuning the Segment Anything Model (SAM) for field delineation using published datasets and a newly acquired regional dataset (ERAS). Extensive experiments evaluate segmentation accuracy and generalization capabilities.

Result: The study successfully demonstrated robust automated field delineation and generalization capabilities using SAM. Additionally, the developed ERAS dataset provides broader geographical coverage.

Conclusion: This research establishes a strong baseline for automated agricultural field delineation, making the ERAS dataset publicly available to further contribute to the community.

Abstract: Accurate mapping of agricultural field boundaries is essential for the
efficient operation of agriculture. Automatic extraction from high-resolution
satellite imagery, supported by computer vision techniques, can avoid costly
ground surveys. In this paper, we present a pipeline for field delineation
based on the Segment Anything Model (SAM), introducing a fine-tuning strategy
to adapt SAM to this task. In addition to using published datasets, we describe
a method for acquiring a complementary regional dataset that covers areas
beyond current sources. Extensive experiments assess segmentation accuracy and
evaluate the generalization capabilities. Our approach provides a robust
baseline for automated field delineation. The new regional dataset, known as
ERAS, is now publicly available.

</details>


### [170] [RealDriveSim: A Realistic Multi-Modal Multi-Task Synthetic Dataset for Autonomous Driving](https://arxiv.org/abs/2506.16319)
*Arpit Jadon,Haoran Wang,Phillip Thomas,Michael Stanley,S. Nathaniel Cibik,Rachel Laurat,Omar Maher,Lukas Hoyer,Ozan Unal,Dengxin Dai*

Main category: cs.CV

TL;DR: RealDriveSim is a realistic, multi-modal synthetic dataset for autonomous driving that supports both 2D vision and LiDAR, with fine-grained annotations for 64 classes.


<details>
  <summary>Details</summary>
Motivation: The need for large-scale datasets in perception models is increasing, but data annotation is prohibitively expensive. Synthetic datasets offer a cost-effective solution but currently lack realism, broad applicability, and diverse task support.

Method: The authors developed RealDriveSim, a highly realistic, multi-modal dataset that supports 2D vision and LiDAR-based tasks with detailed annotations for 64 classes. They conducted extensive evaluations across multiple applications.

Result: RealDriveSim demonstrated state-of-the-art results, outperforming existing synthetic datasets across various tasks and domains.

Conclusion: RealDriveSim addresses limitations in current synthetic datasets by offering realism, extensive applicability, and fine-grained annotations. It is made publicly available for broader research and development.

Abstract: As perception models continue to develop, the need for large-scale datasets
increases. However, data annotation remains far too expensive to effectively
scale and meet the demand. Synthetic datasets provide a solution to boost model
performance with substantially reduced costs. However, current synthetic
datasets remain limited in their scope, realism, and are designed for specific
tasks and applications. In this work, we present RealDriveSim, a realistic
multi-modal synthetic dataset for autonomous driving that not only supports
popular 2D computer vision applications but also their LiDAR counterparts,
providing fine-grained annotations for up to 64 classes. We extensively
evaluate our dataset for a wide range of applications and domains,
demonstrating state-of-the-art results compared to existing synthetic
benchmarks. The dataset is publicly available at
https://realdrivesim.github.io/.

</details>


### [171] [Reliable Few-shot Learning under Dual Noises](https://arxiv.org/abs/2506.16330)
*Ji Zhang,Jingkuan Song,Lianli Gao,Nicu Sebe,Heng Tao Shen*

Main category: cs.CV

TL;DR: The paper introduces DETA++, a method to improve few-shot learning (FSL) in noisy environments by addressing in-distribution (ID) and out-of-distribution (OOD) noise using novel mechanisms.


<details>
  <summary>Details</summary>
Motivation: To tackle challenges of few-shot learning (FSL) affected by ID and OOD noise in support and query samples, which can hinder reliable task adaptation and predictions.

Method: DETA++ incorporates methods like Contrastive Relevance Aggregation (CoRA) for noise identification, a memory bank to refine clean regions, and strategies like Intra-class Region Swapping for robustness.

Result: Experiments show that DETA++ achieves noise-robust task adaptation and generates reliable predictions in few-shot learning setups.

Conclusion: DETA++ provides an effective approach for managing dual noises in few-shot learning, enhancing the robustness and reliability of task adaptation for target tasks.

Abstract: Recent advances in model pre-training give rise to task adaptation-based
few-shot learning (FSL), where the goal is to adapt a pre-trained task-agnostic
model for capturing task-specific knowledge with a few-labeled support samples
of the target task.Nevertheless, existing approaches may still fail in the open
world due to the inevitable in-distribution (ID) and out-of-distribution (OOD)
noise from both support and query samples of the target task. With limited
support samples available, i) the adverse effect of the dual noises can be
severely amplified during task adaptation, and ii) the adapted model can
produce unreliable predictions on query samples in the presence of the dual
noises. In this work, we propose DEnoised Task Adaptation (DETA++) for reliable
FSL. DETA++ uses a Contrastive Relevance Aggregation (CoRA) module to calculate
image and region weights for support samples, based on which a clean prototype
loss and a noise entropy maximization loss are proposed to achieve noise-robust
task adaptation. Additionally,DETA++ employs a memory bank to store and refine
clean regions for each inner-task class, based on which a Local Nearest
Centroid Classifier (LocalNCC) is devised to yield noise-robust predictions on
query samples. Moreover, DETA++ utilizes an Intra-class Region Swapping
(IntraSwap) strategy to rectify ID class prototypes during task adaptation,
enhancing the model's robustness to the dual noises. Extensive experiments
demonstrate the effectiveness and flexibility of DETA++.

</details>


### [172] [Transparency Techniques for Neural Networks trained on Writer Identification and Writer Verification](https://arxiv.org/abs/2506.16331)
*Viktoria Pundy,Marco Peer,Florian Kleber*

Main category: cs.CV

TL;DR: The paper investigates the use of transparency techniques for neural networks in writer identification and verification tasks, focusing on pixel-level and point-specific saliency maps.


<details>
  <summary>Details</summary>
Motivation: The goal is to enhance the understanding and reliability of neural networks in handling writer identification and verification tasks, especially benefiting forensic experts.

Method: Two transparency techniques (pixel-level saliency maps and point-specific saliency maps) were applied and evaluated using deletion and insertion score metrics.

Result: Pixel-wise saliency maps outperformed point-specific ones and aligned well with forensic experts' understanding of handwriting characteristics.

Conclusion: Pixel-wise saliency maps are effective in providing transparency and supporting forensic experts for writer identification tasks.

Abstract: Neural Networks are the state of the art for many tasks in the computer
vision domain, including Writer Identification (WI) and Writer Verification
(WV). The transparency of these "black box" systems is important for
improvements of performance and reliability. For this work, two transparency
techniques are applied to neural networks trained on WI and WV for the first
time in this domain. The first technique provides pixel-level saliency maps,
while the point-specific saliency maps of the second technique provide
information on similarities between two images. The transparency techniques are
evaluated using deletion and insertion score metrics. The goal is to support
forensic experts with information on similarities in handwritten text and to
explore the characteristics selected by a neural network for the identification
process. For the qualitative evaluation, the highlights of the maps are
compared to the areas forensic experts consider during the identification
process. The evaluation results show that the pixel-wise saliency maps
outperform the point-specific saliency maps and are suitable for the support of
forensic experts.

</details>


### [173] [MambaHash: Visual State Space Deep Hashing Model for Large-Scale Image Retrieval](https://arxiv.org/abs/2506.16353)
*Chao He,Hongxi Wei*

Main category: cs.CV

TL;DR: This paper introduces MambaHash, a hashing model leveraging Vision Mamba for large-scale image retrieval, demonstrating superior performance over existing deep hashing methods.


<details>
  <summary>Details</summary>
Motivation: The paper aims to explore the potential of Vision Mamba's linear time complexity for enhancing large-scale image retrieval tasks.

Method: The authors propose a stage-wise backbone network integrating grouped Mamba operations, channel interaction attention, and adaptive feature enhancement modules.

Result: MambaHash outperformed state-of-the-art deep hashing methods in efficiency and retrieval performance across datasets CIFAR-10, NUS-WIDE, and IMAGENET.

Conclusion: MambaHash proves effective for large-scale image retrieval, offering enhanced visual representation, feature diversity, and scalability.

Abstract: Deep image hashing aims to enable effective large-scale image retrieval by
mapping the input images into simple binary hash codes through deep neural
networks. More recently, Vision Mamba with linear time complexity has attracted
extensive attention from researchers by achieving outstanding performance on
various computer tasks. Nevertheless, the suitability of Mamba for large-scale
image retrieval tasks still needs to be explored. Towards this end, we propose
a visual state space hashing model, called MambaHash. Concretely, we propose a
backbone network with stage-wise architecture, in which grouped Mamba operation
is introduced to model local and global information by utilizing Mamba to
perform multi-directional scanning along different groups of the channel.
Subsequently, the proposed channel interaction attention module is used to
enhance information communication across channels. Finally, we meticulously
design an adaptive feature enhancement module to increase feature diversity and
enhance the visual representation capability of the model. We have conducted
comprehensive experiments on three widely used datasets: CIFAR-10, NUS-WIDE and
IMAGENET. The experimental results demonstrate that compared with the
state-of-the-art deep hashing methods, our proposed MambaHash has well
efficiency and superior performance to effectively accomplish large-scale image
retrieval tasks. Source code is available
https://github.com/shuaichaochao/MambaHash.git

</details>


### [174] [Prompt-based Dynamic Token Pruning to Guide Transformer Attention in Efficient Segmentation](https://arxiv.org/abs/2506.16369)
*Pallabi Dutta,Anubhab Maity,Sushmita Mitra*

Main category: cs.CV

TL;DR: The paper addresses the computational limitations of Vision Transformers in medical imaging through adaptive prompt-guided pruning to improve efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: Vision Transformers face high computational demands due to a large number of tokens, limiting their practical application in medical image analysis.

Method: The study introduces an adaptive prompt-guided pruning method that uses spatial priors to rank token relevance and selectively down-weight low-relevance tokens during segmentation.

Result: The approach reduces computational costs by pruning ~35-55% of tokens while preserving segmentation accuracy, demonstrating improved efficiency and resource optimization.

Conclusion: This framework enables more cost-effective medical image processing, enhancing its applicability for real-time diagnosis in resource-constrained settings.

Abstract: The high computational demands of Vision Transformers (ViTs), in processing a
huge number of tokens, often constrain their practical application in analyzing
medical images. This research proposes an adaptive prompt-guided pruning method
to selectively reduce the processing of irrelevant tokens in the segmentation
pipeline. The prompt-based spatial prior helps to rank the tokens according to
their relevance. Tokens with low-relevance scores are down-weighted, ensuring
that only the relevant ones are propagated for processing across subsequent
stages. This data-driven pruning strategy facilitates end-to-end training,
maintains gradient flow, and improves segmentation accuracy by focusing
computational resources on essential regions. The proposed framework is
integrated with several state-of-the-art models to facilitate the elimination
of irrelevant tokens; thereby, enhancing computational efficiency while
preserving segmentation accuracy. The experimental results show a reduction of
$\sim$ 35-55\% tokens; thus reducing the computational costs relative to the
baselines. Cost-effective medical image processing, using our framework,
facilitates real-time diagnosis by expanding its applicability in
resource-constrained environments.

</details>


### [175] [AGC-Drive: A Large-Scale Dataset for Real-World Aerial-Ground Collaboration in Driving Scenarios](https://arxiv.org/abs/2506.16371)
*Yunhao Hou,Bochao Zou,Min Zhang,Ran Chen,Shangdong Yang,Yanmei Zhang,Junbao Zhuo,Siheng Chen,Jiansheng Chen,Huimin Ma*

Main category: cs.CV

TL;DR: The paper introduces AGC-Drive, a real-world dataset for aerial-ground collaborative 3D perception, emphasizing UAV contributions to autonomous vehicle perception through diverse scenarios.


<details>
  <summary>Details</summary>
Motivation: To address the lack of high-quality datasets for aerial-ground collaborative scenarios and enable improved 3D perception through UAVs.

Method: AGC-Drive dataset includes multi-agent data from vehicles and a UAV, providing comprehensive annotations and benchmarks for collaborative perception tasks.

Result: AGC-Drive offers 120K LiDAR frames, 440K images, and fully annotated data across 14 diverse driving scenarios with dynamic events covering 13 categories.

Conclusion: The AGC-Drive dataset and tools enhance research in aerial-ground collaborative perception, filling a significant gap and supporting future autonomous systems development.

Abstract: By sharing information across multiple agents, collaborative perception helps
autonomous vehicles mitigate occlusions and improve overall perception
accuracy. While most previous work focus on vehicle-to-vehicle and
vehicle-to-infrastructure collaboration, with limited attention to aerial
perspectives provided by UAVs, which uniquely offer dynamic, top-down views to
alleviate occlusions and monitor large-scale interactive environments. A major
reason for this is the lack of high-quality datasets for aerial-ground
collaborative scenarios. To bridge this gap, we present AGC-Drive, the first
large-scale real-world dataset for Aerial-Ground Cooperative 3D perception. The
data collection platform consists of two vehicles, each equipped with five
cameras and one LiDAR sensor, and one UAV carrying a forward-facing camera and
a LiDAR sensor, enabling comprehensive multi-view and multi-agent perception.
Consisting of approximately 120K LiDAR frames and 440K images, the dataset
covers 14 diverse real-world driving scenarios, including urban roundabouts,
highway tunnels, and on/off ramps. Notably, 19.5% of the data comprises dynamic
interaction events, including vehicle cut-ins, cut-outs, and frequent lane
changes. AGC-Drive contains 400 scenes, each with approximately 100 frames and
fully annotated 3D bounding boxes covering 13 object categories. We provide
benchmarks for two 3D perception tasks: vehicle-to-vehicle collaborative
perception and vehicle-to-UAV collaborative perception. Additionally, we
release an open-source toolkit, including spatiotemporal alignment verification
tools, multi-agent visualization systems, and collaborative annotation
utilities. The dataset and code are available at
https://github.com/PercepX/AGC-Drive.

</details>


### [176] [CLIP-MG: Guiding Semantic Attention with Skeletal Pose Features and RGB Data for Micro-Gesture Recognition on the iMiGUE Dataset](https://arxiv.org/abs/2506.16385)
*Santosh Patapati,Trisanth Srinivasan,Amith Adiraju*

Main category: cs.CV

TL;DR: This paper proposes a new CLIP-based model, CLIP-MG, to improve micro-gesture recognition by integrating human pose (skeleton) data into the classification process. The model achieved a Top-1 accuracy of 61.82% on the iMiGUE dataset.


<details>
  <summary>Details</summary>
Motivation: Micro-gestures are subtle and have low movement amplitude, making their recognition a challenging task in affective computing. There is a need for more specialized models to better classify them.

Method: The authors modified the CLIP model to include pose (skeleton) information. They introduced pose-guided semantic query generation and a gated multi-modal fusion mechanism to enhance micro-gesture recognition.

Result: The proposed model, CLIP-MG, achieved a Top-1 accuracy of 61.82% when tested on the iMiGUE dataset, showing promise but also highlighting room for further improvement.

Conclusion: While the proposed approach demonstrates potential in adapting vision-language models like CLIP for micro-gesture recognition, there remains significant difficulty in achieving optimal performance on such tasks.

Abstract: Micro-gesture recognition is a challenging task in affective computing due to
the subtle, involuntary nature of the gestures and their low movement
amplitude. In this paper, we introduce a Pose-Guided Semantics-Aware CLIP-based
architecture, or CLIP for Micro-Gesture recognition (CLIP-MG), a modified CLIP
model tailored for micro-gesture classification on the iMiGUE dataset. CLIP-MG
integrates human pose (skeleton) information into the CLIP-based recognition
pipeline through pose-guided semantic query generation and a gated multi-modal
fusion mechanism. The proposed model achieves a Top-1 accuracy of 61.82%. These
results demonstrate both the potential of our approach and the remaining
difficulty in fully adapting vision-language models like CLIP for micro-gesture
recognition.

</details>


### [177] [HyperPath: Knowledge-Guided Hyperbolic Semantic Hierarchy Modeling for WSI Analysis](https://arxiv.org/abs/2506.16398)
*Peixiang Huang,Yanyan Huang,Weiqin Zhao,Junjun He,Lequan Yu*

Main category: cs.CV

TL;DR: The paper introduces HyperPath, a novel method for WSI classification that uses hyperbolic embeddings instead of traditional Euclidean embeddings and leverages textual and visual feature integration.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the limitations of Euclidean embeddings in capturing semantic hierarchies of WSIs, which are critical for cancer diagnosis and exhibit natural hierarchical structures.

Method: HyperPath uses textual descriptions to model semantic hierarchies of WSIs in hyperbolic space. It employs Angular Modality Alignment Loss for cross-modal alignment and Semantic Hierarchy Consistency Loss for refining hierarchical semantic coherence, with classification based on geodesic distance.

Result: HyperPath achieves superior performance across various WSI analysis tasks when compared to existing methods, highlighting its effectiveness in modeling semantic hierarchies.

Conclusion: The study demonstrates the potential of hyperbolic embeddings and geometry-aware approaches to improve whole slide image analysis, offering new avenues for enhancing cancer diagnosis processes.

Abstract: Pathology is essential for cancer diagnosis, with multiple instance learning
(MIL) widely used for whole slide image (WSI) analysis. WSIs exhibit a natural
hierarchy -- patches, regions, and slides -- with distinct semantic
associations. While some methods attempt to leverage this hierarchy for
improved representation, they predominantly rely on Euclidean embeddings, which
struggle to fully capture semantic hierarchies. To address this limitation, we
propose HyperPath, a novel method that integrates knowledge from textual
descriptions to guide the modeling of semantic hierarchies of WSIs in
hyperbolic space, thereby enhancing WSI classification. Our approach adapts
both visual and textual features extracted by pathology vision-language
foundation models to the hyperbolic space. We design an Angular Modality
Alignment Loss to ensure robust cross-modal alignment, while a Semantic
Hierarchy Consistency Loss further refines feature hierarchies through
entailment and contradiction relationships and thus enhance semantic coherence.
The classification is performed with geodesic distance, which measures the
similarity between entities in the hyperbolic semantic hierarchy. This
eliminates the need for linear classifiers and enables a geometry-aware
approach to WSI analysis. Extensive experiments show that our method achieves
superior performance across tasks compared to existing methods, highlighting
the potential of hyperbolic embeddings for WSI analysis.

</details>


### [178] [Robustness Evaluation of OCR-based Visual Document Understanding under Multi-Modal Adversarial Attacks](https://arxiv.org/abs/2506.16407)
*Dong Nguyen Tien,Dung D. Le*

Main category: cs.CV

TL;DR: The paper presents a framework for multi-modal adversarial attacks on OCR-based Visual Document Understanding (VDU) systems.


<details>
  <summary>Details</summary>
Motivation: To examine and improve the robustness of OCR-based VDU systems under realistic adversarial scenarios, which is currently insufficiently explored.

Method: The framework involves six gradient-based layout attack scenarios that manipulate OCR bounding boxes, pixels, and texts while maintaining layout plausibility. These attacks are tested across word and line granularities with budget constraints.

Result: Experimental results reveal severe performance degradation from line-level and compound attacks, with PGD-based bounding box perturbations outperforming random baselines across four datasets and six model families.

Conclusion: VDU systems are highly vulnerable to adversarial layout manipulations, especially precise and compound attacks, emphasizing the need for building more robust systems and defenses.

Abstract: Visual Document Understanding (VDU) systems have achieved strong performance
in information extraction by integrating textual, layout, and visual signals.
However, their robustness under realistic adversarial perturbations remains
insufficiently explored. We introduce the first unified framework for
generating and evaluating multi-modal adversarial attacks on OCR-based VDU
models. Our method covers six gradient-based layout attack scenarios,
incorporating manipulations of OCR bounding boxes, pixels, and texts across
both word and line granularities, with constraints on layout perturbation
budget (e.g., IoU >= 0.6) to preserve plausibility.
  Experimental results across four datasets (FUNSD, CORD, SROIE, DocVQA) and
six model families demonstrate that line-level attacks and compound
perturbations (BBox + Pixel + Text) yield the most severe performance
degradation. Projected Gradient Descent (PGD)-based BBox perturbations
outperform random-shift baselines in all investigated models. Ablation studies
further validate the impact of layout budget, text modification, and
adversarial transferability.

</details>


### [179] [Efficient Transformations in Deep Learning Convolutional Neural Networks](https://arxiv.org/abs/2506.16418)
*Berk Yilmaz,Daniel Fidel Harvey,Prajit Dhuri*

Main category: cs.CV

TL;DR: The paper explores integrating signal processing methods, like WHT, into ResNet50 to enhance image classification accuracy and cut energy costs.


<details>
  <summary>Details</summary>
Motivation: Current image classification models, like ResNet50, often face challenges associated with high computational and energy demands.

Method: ResNet50 is modified to incorporate FFT, WHT, and DCT transformations in its convolutional layers, evaluated using CIFAR-100 dataset.

Result: Using WHT in convolutional layers boosted accuracy from 66% to 79% and slashed energy consumption from 25,606 kJ to 39 kJ per model.

Conclusion: Integrating WHT in CNNs like ResNet50 offers a remarkable balance of improved accuracy and decreased computational energy, suited for constrained applications.

Abstract: This study investigates the integration of signal processing transformations
-- Fast Fourier Transform (FFT), Walsh-Hadamard Transform (WHT), and Discrete
Cosine Transform (DCT) -- within the ResNet50 convolutional neural network
(CNN) model for image classification. The primary objective is to assess the
trade-offs between computational efficiency, energy consumption, and
classification accuracy during training and inference. Using the CIFAR-100
dataset (100 classes, 60,000 images), experiments demonstrated that
incorporating WHT significantly reduced energy consumption while improving
accuracy. Specifically, a baseline ResNet50 model achieved a testing accuracy
of 66%, consuming an average of 25,606 kJ per model. In contrast, a modified
ResNet50 incorporating WHT in the early convolutional layers achieved 74%
accuracy, and an enhanced version with WHT applied to both early and late
layers achieved 79% accuracy, with an average energy consumption of only 39 kJ
per model. These results demonstrate the potential of WHT as a highly efficient
and effective approach for energy-constrained CNN applications.

</details>


### [180] [Structured Semantic 3D Reconstruction (S23DR) Challenge 2025 -- Winning solution](https://arxiv.org/abs/2506.16421)
*Jan Skvrna,Lukas Neumann*

Main category: cs.CV

TL;DR: This paper introduces a 3D deep learning approach for predicting house roof wireframes from sparse point clouds, achieving a top score in the S23DR Challenge 2025.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the challenge of accurately predicting 3D house roof wireframes based on sparse point clouds and semantic segmentations, an essential task for urban planning and architecture.

Method: The authors used a two-stage 3D deep learning process involving PointNet-like models. One model refines and classifies vertex candidates extracted from the COLMAP point cloud, while the other predicts edges between vertex pairs using cylindrical region analysis.

Result: The method achieved a Hybrid Structure Score (HSS) of 0.43, which was the top score on the private leaderboard of the S23DR Challenge 2025.

Conclusion: The proposed method demonstrates the effectiveness of directly operating in 3D with a tailored deep learning pipeline, setting a new benchmark in the field.

Abstract: This paper presents the winning solution for the S23DR Challenge 2025, which
involves predicting a house's 3D roof wireframe from a sparse point cloud and
semantic segmentations. Our method operates directly in 3D, first identifying
vertex candidates from the COLMAP point cloud using Gestalt segmentations. We
then employ two PointNet-like models: one to refine and classify these
candidates by analyzing local cubic patches, and a second to predict edges by
processing the cylindrical regions connecting vertex pairs. This two-stage, 3D
deep learning approach achieved a winning Hybrid Structure Score (HSS) of 0.43
on the private leaderboard.

</details>


### [181] [How Far Can Off-the-Shelf Multimodal Large Language Models Go in Online Episodic Memory Question Answering?](https://arxiv.org/abs/2506.16450)
*Giuseppe Lando,Rosario Forte,Giovanni Maria Farinella,Antonino Furnari*

Main category: cs.CV

TL;DR: The paper explores whether Multimodal Large Language Models (MLLMs) can address Online Episodic-Memory Video Question Answering without requiring additional training.


<details>
  <summary>Details</summary>
Motivation: To investigate if MLLMs can create efficient and lightweight textual memory for analyzing egocentric videos to answer questions accurately without training new systems.

Method: The approach involves converting streaming egocentric videos into a compact textual memory using an MLLM descriptor module and querying this memory with an LLM reasoner to answer multiple-choice questions.

Result: Achieves 56.0% accuracy on QAEgo4D-Closed benchmark, matching dedicated state-of-the-art systems while being significantly more memory-efficient, using only 3.6 kB per minute storage.

Conclusion: The system demonstrates competitiveness with state-of-the-art systems and extreme memory efficiency, offering insights for future improvements through extensive ablation studies.

Abstract: We investigate whether off-the-shelf Multimodal Large Language Models (MLLMs)
can tackle Online Episodic-Memory Video Question Answering (OEM-VQA) without
additional training. Our pipeline converts a streaming egocentric video into a
lightweight textual memory, only a few kilobytes per minute, via an MLLM
descriptor module, and answers multiple-choice questions by querying this
memory with an LLM reasoner module. On the QAEgo4D-Closed benchmark, our best
configuration attains 56.0% accuracy with 3.6 kB per minute storage, matching
the performance of dedicated state-of-the-art systems while being 10**4/10**5
times more memory-efficient. Extensive ablations provides insights into the
role of each component and design choice, and highlight directions of
improvement for future research.

</details>


### [182] [Spotting tell-tale visual artifacts in face swapping videos: strengths and pitfalls of CNN detectors](https://arxiv.org/abs/2506.16497)
*Riccardo Ziglio,Cecilia Pasquini,Silvio Ranise*

Main category: cs.CV

TL;DR: This paper examines the robustness of visual artifact detection in face-swapping video manipulations by benchmarking CNN-based models on diverse datasets, showcasing their limitations across different data environments.


<details>
  <summary>Details</summary>
Motivation: Face swapping poses increasing threats to video communications, necessitating tools to detect and analyze manipulation artifacts.

Method: CNN-based models were benchmarked on two datasets to evaluate their generalization capabilities across varied data sources and face-swapping algorithms.

Result: CNN architectures perform well within a single dataset but struggle to generalize occlusion-based visual artifact detection across datasets.

Conclusion: Specialized detection strategies are essential to effectively identify manipulation artifacts in diverse video data environments.

Abstract: Face swapping manipulations in video streams represents an increasing threat
in remote video communications, due to advances
  in automated and real-time tools. Recent literature proposes to characterize
and exploit visual artifacts introduced in video frames
  by swapping algorithms when dealing with challenging physical scenes, such as
face occlusions. This paper investigates the
  effectiveness of this approach by benchmarking CNN-based data-driven models
on two data corpora (including a newly collected
  one) and analyzing generalization capabilities with respect to different
acquisition sources and swapping algorithms. The results
  confirm excellent performance of general-purpose CNN architectures when
operating within the same data source, but a significant
  difficulty in robustly characterizing occlusion-based visual cues across
datasets. This highlights the need for specialized detection
  strategies to deal with such artifacts.

</details>


### [183] [Hunyuan3D 2.5: Towards High-Fidelity 3D Assets Generation with Ultimate Details](https://arxiv.org/abs/2506.16504)
*Zeqiang Lai,Yunfei Zhao,Haolin Liu,Zibo Zhao,Qingxiang Lin,Huiwen Shi,Xianghui Yang,Mingxin Yang,Shuhui Yang,Yifei Feng,Sheng Zhang,Xin Huang,Di Luo,Fan Yang,Fang Yang,Lifu Wang,Sicong Liu,Yixuan Tang,Yulin Cai,Zebin He,Tian Liu,Yuhong Liu,Jie Jiang,Linus,Jingwei Huang,Chunchao Guo*

Main category: cs.CV

TL;DR: The paper introduces Hunyuan3D 2.5, a 3D diffusion model suite for generating high-quality textured 3D assets, with significant improvements in shape and texture generation.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address limitations in generating high-fidelity and detailed textured 3D assets by improving both shape and texture generation capabilities of the existing Hunyuan3D 2.0 model.

Method: The approach includes a new shape foundation model called LATTICE trained on large datasets, scaling compute resources, and upgrading texture generation via a new multi-view architecture for physical-based rendering.

Result: The Hunyuan3D 2.5 model significantly outperforms its predecessor and other contemporary methods in generating detailed textured 3D assets with precision and smoothness.

Conclusion: Hunyuan3D 2.5 sets a new benchmark in 3D asset generation by bridging the gap between generated and handcrafted 3D shapes, demonstrating state-of-the-art performance in end-to-end shape and texture creation.

Abstract: In this report, we present Hunyuan3D 2.5, a robust suite of 3D diffusion
models aimed at generating high-fidelity and detailed textured 3D assets.
Hunyuan3D 2.5 follows two-stages pipeline of its previous version Hunyuan3D
2.0, while demonstrating substantial advancements in both shape and texture
generation. In terms of shape generation, we introduce a new shape foundation
model -- LATTICE, which is trained with scaled high-quality datasets,
model-size, and compute. Our largest model reaches 10B parameters and generates
sharp and detailed 3D shape with precise image-3D following while keeping mesh
surface clean and smooth, significantly closing the gap between generated and
handcrafted 3D shapes. In terms of texture generation, it is upgraded with
phyiscal-based rendering (PBR) via a novel multi-view architecture extended
from Hunyuan3D 2.0 Paint model. Our extensive evaluation shows that Hunyuan3D
2.5 significantly outperforms previous methods in both shape and end-to-end
texture generation.

</details>


### [184] [How Hard Is Snow? A Paired Domain Adaptation Dataset for Clear and Snowy Weather: CADC+](https://arxiv.org/abs/2506.16531)
*Mei Qi Tang,Sean Sedwards,Chengjie Huang,Krzysztof Czarnecki*

Main category: cs.CV

TL;DR: The paper introduces CADC+, a dataset that pairs snowy and clear weather driving data in the same environment for more accurate evaluations of 3D object detection performance under snowfall.


<details>
  <summary>Details</summary>
Motivation: Current datasets lack sufficient labelled data in both snowy and clear weather conditions or rely on synthetic data that introduces domain shifts, making it challenging to evaluate the impact of snow on 3D object detection.

Method: The authors created CADC+ by extending the CADC dataset, pairing snowy sequences with corresponding clear weather sequences recorded in the same environment and timeframe, thereby minimizing unrelated domain shifts.

Result: Preliminary results show that snow introduces aleatoric (random noise) and epistemic (systematic) uncertainties, impacting 3D object detection by acting as noise and a distinct data domain.

Conclusion: CADC+ enables more accurate evaluation of winter conditions on autonomous driving systems, with initial findings highlighting the dual impact of snow as both noise and a separate data domain.

Abstract: The impact of snowfall on 3D object detection performance remains
underexplored. Conducting such an evaluation requires a dataset with sufficient
labelled data from both weather conditions, ideally captured in the same
driving environment. Current driving datasets with LiDAR point clouds either do
not provide enough labelled data in both snowy and clear weather conditions, or
rely on de-snowing methods to generate synthetic clear weather. Synthetic data
often lacks realism and introduces an additional domain shift that confounds
accurate evaluations. To address these challenges, we present CADC+, the first
paired weather domain adaptation dataset for autonomous driving in winter
conditions. CADC+ extends the Canadian Adverse Driving Conditions dataset
(CADC) using clear weather data that was recorded on the same roads and in the
same period as CADC. To create CADC+, we pair each CADC sequence with a clear
weather sequence that matches the snowy sequence as closely as possible. CADC+
thus minimizes the domain shift resulting from factors unrelated to the
presence of snow. We also present some preliminary results using CADC+ to
evaluate the effect of snow on 3D object detection performance. We observe that
snow introduces a combination of aleatoric and epistemic uncertainties, acting
as both noise and a distinct data domain.

</details>


### [185] [From Semantic To Instance: A Semi-Self-Supervised Learning Approach](https://arxiv.org/abs/2506.16563)
*Keyhan Najafian,Farhad Maleki,Lingling Jin,Ian Stavness*

Main category: cs.CV

TL;DR: This paper introduces a semi-self-supervised learning method for instance segmentation, greatly reducing manual annotation efforts while achieving state-of-the-art performance in agriculture and other domains.


<details>
  <summary>Details</summary>
Motivation: Manual annotation for instance segmentation is labor-intensive, especially for densely packed and self-occluded images common in agriculture.

Method: The authors propose GLMask, a representation that minimizes reliance on color features and focuses on shape, texture, and pattern. A pipeline was developed to convert semantic segmentation outputs into instance-level segmentation.

Result: The approach achieves state-of-the-art performance, with a mAP@50 of 98.5% for wheat head segmentation and over 12.6% improvement on the Microsoft COCO dataset.

Conclusion: This semi-self-supervised method is highly effective and generalizable, benefiting agricultural and other industries with similar data characteristics.

Abstract: Instance segmentation is essential for applications such as automated
monitoring of plant health, growth, and yield. However, extensive effort is
required to create large-scale datasets with pixel-level annotations of each
object instance for developing instance segmentation models that restrict the
use of deep learning in these areas. This challenge is more significant in
images with densely packed, self-occluded objects, which are common in
agriculture. To address this challenge, we propose a semi-self-supervised
learning approach that requires minimal manual annotation to develop a
high-performing instance segmentation model. We design GLMask, an image-mask
representation for the model to focus on shape, texture, and pattern while
minimizing its dependence on color features. We develop a pipeline to generate
semantic segmentation and then transform it into instance-level segmentation.
The proposed approach substantially outperforms the conventional instance
segmentation models, establishing a state-of-the-art wheat head instance
segmentation model with mAP@50 of 98.5%. Additionally, we assessed the proposed
methodology on the general-purpose Microsoft COCO dataset, achieving a
significant performance improvement of over 12.6% mAP@50. This highlights that
the utility of our proposed approach extends beyond precision agriculture and
applies to other domains, specifically those with similar data characteristics.

</details>


### [186] [SafeTriage: Facial Video De-identification for Privacy-Preserving Stroke Triage](https://arxiv.org/abs/2506.16578)
*Tongan Cai,Haomiao Ni,Wenchao Ma,Yuan Xue,Qian Ma,Rachel Leicht,Kelvin Wong,John Volpi,Stephen T. C. Wong,James Z. Wang,Sharon X. Huang*

Main category: cs.CV

TL;DR: The paper introduces SafeTriage, a method that de-identifies patient facial videos while retaining key motion dynamics for stroke diagnosis, balancing privacy and diagnostic accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the ethical and privacy concerns associated with using real patient facial video datasets for training AI models in stroke diagnosis.

Method: SafeTriage uses a pretrained video motion transfer model to map facial motion from real patients onto synthetic faces. A conditional generative model is introduced for visual prompt tuning to mitigate distribution shifts without fine-tuning the backbone model.

Result: The evaluations showed that SafeTriage successfully preserves stroke-relevant facial dynamics in synthetic videos while protecting patient identities, offering reliable AI-based triage.

Conclusion: SafeTriage provides an ethically sound and secure approach for sharing patient data, enabling effective AI-driven clinical analysis without compromising privacy in neurological disorder diagnosis.

Abstract: Effective stroke triage in emergency settings often relies on clinicians'
ability to identify subtle abnormalities in facial muscle coordination. While
recent AI models have shown promise in detecting such patterns from patient
facial videos, their reliance on real patient data raises significant ethical
and privacy challenges -- especially when training robust and generalizable
models across institutions. To address these concerns, we propose SafeTriage, a
novel method designed to de-identify patient facial videos while preserving
essential motion cues crucial for stroke diagnosis. SafeTriage leverages a
pretrained video motion transfer (VMT) model to map the motion characteristics
of real patient faces onto synthetic identities. This approach retains
diagnostically relevant facial dynamics without revealing the patients'
identities. To mitigate the distribution shift between normal population
pre-training videos and patient population test videos, we introduce a
conditional generative model for visual prompt tuning, which adapts the input
space of the VMT model to ensure accurate motion transfer without needing to
fine-tune the VMT model backbone. Comprehensive evaluation, including
quantitative metrics and clinical expert assessments, demonstrates that
SafeTriage-produced synthetic videos effectively preserve stroke-relevant
facial patterns, enabling reliable AI-based triage. Our evaluations also show
that SafeTriage provides robust privacy protection while maintaining diagnostic
accuracy, offering a secure and ethically sound foundation for data sharing and
AI-driven clinical analysis in neurological disorders.

</details>


### [187] [MetaQAP -- A Meta-Learning Approach for Quality-Aware Pretraining in Image Quality Assessment](https://arxiv.org/abs/2506.16601)
*Muhammad Azeem Aslam,Muhammad Hamza,Nisar Ahmed,Gulshan Saleem,Zhu Shuangtong,Hu Hongfei,Xu Wei,Saba Aslam,Wang Jun*

Main category: cs.CV

TL;DR: MetaQAP is a new no-reference Image Quality Assessment model using pre-trained CNNs, quality-aware loss, and meta-learning, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Address difficulties in image quality assessment due to human perception subjectivity and real-world distortions.

Method: MetaQAP uses quality-aware dataset pretraining, quality-aware loss optimization, and a meta-learner ensemble for combining model predictions.

Result: Achieved high PLCC and SROCC scores on LiveCD, KonIQ-10K, and BIQ2021 benchmarks, outperforming existing methods and showing generalizability in cross-datasets evaluations.

Conclusion: MetaQAP provides a robust framework for IQA, addressing distortions effectively and setting a new standard for future research in no-reference IQA.

Abstract: Image Quality Assessment (IQA) is a critical task in a wide range of
applications but remains challenging due to the subjective nature of human
perception and the complexity of real-world image distortions. This study
proposes MetaQAP, a novel no-reference IQA model designed to address these
challenges by leveraging quality-aware pre-training and meta-learning. The
model performs three key contributions: pre-training Convolutional Neural
Networks (CNNs) on a quality-aware dataset, implementing a quality-aware loss
function to optimize predictions, and integrating a meta-learner to form an
ensemble model that effectively combines predictions from multiple base models.
Experimental evaluations were conducted on three benchmark datasets: LiveCD,
KonIQ-10K, and BIQ2021. The proposed MetaQAP model achieved exceptional
performance with Pearson Linear Correlation Coefficient (PLCC) and Spearman
Rank Order Correlation Coefficient (SROCC) scores of 0.9885/0.9812 on LiveCD,
0.9702/0.9658 on KonIQ-10K, and 0.884/0.8765 on BIQ2021, outperforming existing
IQA methods. Cross-dataset evaluations further demonstrated the
generalizability of the model, with PLCC and SROCC scores ranging from 0.6721
to 0.8023 and 0.6515 to 0.7805, respectively, across diverse datasets. The
ablation study confirmed the significance of each model component, revealing
substantial performance degradation when critical elements such as the
meta-learner or quality-aware loss function were omitted. MetaQAP not only
addresses the complexities of authentic distortions but also establishes a
robust and generalizable framework for practical IQA applications. By advancing
the state-of-the-art in no-reference IQA, this research provides valuable
insights and methodologies for future improvements and extensions in the field.

</details>


### [188] [Leveraging CNN and IoT for Effective E-Waste Management](https://arxiv.org/abs/2506.16647)
*Ajesh Thangaraj Nadar,Gabriel Nixon Raj,Soham Chandane,Sushant Bhat*

Main category: cs.CV

TL;DR: This paper proposes an IoT-enabled system with a lightweight CNN classification pipeline to automate identification and sorting of e-waste based on visual and weight attributes.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the environmental and health challenges posed by improper disposal and insufficient recycling of e-waste through innovation.

Method: The approach utilizes IoT devices integrated with cameras and digital weighing scales to automate e-waste classification via a lightweight CNN model.

Result: The system successfully identifies components like circuit boards, sensors, and wires, enabling smarter recycling workflows.

Conclusion: The proposed framework enhances recycling efficiency and has the potential to significantly mitigate e-waste management issues.

Abstract: The increasing proliferation of electronic devices in the modern era has led
to a significant surge in electronic waste (e-waste). Improper disposal and
insufficient recycling of e-waste pose serious environmental and health risks.
This paper proposes an IoT-enabled system combined with a lightweight CNN-based
classification pipeline to enhance the identification, categorization, and
routing of e-waste materials. By integrating a camera system and a digital
weighing scale, the framework automates the classification of electronic items
based on visual and weight-based attributes. The system demonstrates how
real-time detection of e-waste components such as circuit boards, sensors, and
wires can facilitate smart recycling workflows and improve overall waste
processing efficiency.

</details>


### [189] [A Comparative Analysis of Principal Component Analysis (PCA) and Singular Value Decomposition (SVD) as Dimensionality Reduction Techniques](https://arxiv.org/abs/2506.16663)
*Michael Gyimadu,Gregory Bell*

Main category: cs.CV

TL;DR: This paper analytically compares PCA and SVD for dimensionality reduction, exploring their derivations, interpretability, and numerical stability.


<details>
  <summary>Details</summary>
Motivation: To provide guidelines to choose between PCA and SVD for dimensionality reduction without reliance on empirical benchmarking.

Method: Purely analytical comparison based on derivation from first principles, assessment of interpretability, stability, and applicability to different matrix shapes.

Result: Rule-of-thumb guidelines synthesized for selecting between PCA and SVD based on numerical literature.

Conclusion: Limitations are discussed, and future experimental directions are suggested for enhancing understanding of PCA and SVD.

Abstract: High-dimensional image data often require dimensionality reduction before
further analysis. This paper provides a purely analytical comparison of two
linear techniques-Principal Component Analysis (PCA) and Singular Value
Decomposition (SVD). After the derivation of each algorithm from first
principles, we assess their interpretability, numerical stability, and
suitability for differing matrix shapes. building on classical and recent
numerical literature, We synthesize rule-of-thumb guidelines for choosing one
out of the two algorithms without empirical benchmarking, building on classical
and recent numerical literature. Limitations and directions for future
experimental work are outlined at the end.

</details>


### [190] [Extracting Multimodal Learngene in CLIP: Unveiling the Multimodal Generalizable Knowledge](https://arxiv.org/abs/2506.16673)
*Ruiming Chen,Junming Yang,Shiyu Xia,Xu Yang,Jing Wang,Xin Geng*

Main category: cs.CV

TL;DR: MM-LG introduces a framework to efficiently extract and utilize generalizable multimodal knowledge from CLIP for initializing descendant models, achieving significant performance improvements while reducing computational costs.


<details>
  <summary>Details</summary>
Motivation: Address the computational challenges of large-scale CLIP pre-training by efficiently utilizing multimodal generalizable knowledge for diverse downstream tasks.

Method: Develop multimodal and unimodal blocks to extract knowledge in a weighted-sum manner, using these to initialize models of different scales and modalities.

Result: MM-LG outperforms previous learngene methods and the conventional pre-training/fine-tuning paradigm in accuracy (+1.9 to +4.13%) while reducing parameter storage by 75% and pre-training costs by 2.8 times.

Conclusion: MM-LG effectively balances performance gains and efficiency, making it a robust framework for deploying across varied downstream tasks while minimizing resource demands.

Abstract: CLIP (Contrastive Language-Image Pre-training) has attracted widespread
attention for its multimodal generalizable knowledge, which is significant for
downstream tasks. However, the computational overhead of a large number of
parameters and large-scale pre-training poses challenges of pre-training a
different scale of CLIP. Learngene extracts the generalizable components termed
as learngene from an ancestry model and initializes diverse descendant models
with it. Previous Learngene paradigms fail to handle the generalizable
knowledge in multimodal scenarios. In this paper, we put forward the idea of
utilizing a multimodal block to extract the multimodal generalizable knowledge,
which inspires us to propose MM-LG (Multimodal Learngene), a novel framework
designed to extract and leverage generalizable components from CLIP.
Specifically, we first establish multimodal and unimodal blocks to extract the
multimodal and unimodal generalizable knowledge in a weighted-sum manner.
Subsequently, we employ these components to numerically initialize descendant
models of varying scales and modalities. Extensive experiments demonstrate
MM-LG's effectiveness, which achieves performance gains over existing learngene
approaches (e.g.,+3.1% on Oxford-IIIT PET and +4.13% on Flickr30k) and
comparable or superior results to the pre-training and fine-tuning paradigm
(e.g.,+1.9% on Oxford-IIIT PET and +3.65% on Flickr30k). Notably, MM-LG
requires only around 25% of the parameter storage while reducing around 2.8
times pre-training costs for diverse model scales compared to the pre-training
and fine-tuning paradigm, making it particularly suitable for efficient
deployment across diverse downstream tasks.

</details>


### [191] [AnyTraverse: An off-road traversability framework with VLM and human operator in the loop](https://arxiv.org/abs/2506.16826)
*Sattwik Sahu,Agamdeep Singh,Karthik Nambiar,Srikanth Saripalli,P. B. Sujit*

Main category: cs.CV

TL;DR: The AnyTraverse framework addresses challenges in off-road navigation by incorporating natural language prompts and on-demand human assistance for adaptive robot scene segmentation, achieving better results while minimizing supervision.


<details>
  <summary>Details</summary>
Motivation: Existing off-road traversability frameworks struggle with unstructured environments, uncertain scene changes, and lack adaptability for various robot types.

Method: AnyTraverse combines zero-shot learning with natural language-based prompts and selective human assistance to segment navigable regions and adapt to outdoor scenes without extensive retraining.

Result: Experimental testing shows AnyTraverse outperforms GA-NAV and Off-seg in accuracy and adaptability across RELLIS-3D, Freiburg Forest, and RUGD datasets, demonstrating success in real-world robot deployments.

Conclusion: AnyTraverse provides a robust, vehicle-agnostic solution for off-road traversability, balancing automation with reduced human supervision for diverse robotic platforms.

Abstract: Off-road traversability segmentation enables autonomous navigation with
applications in search-and-rescue, military operations, wildlife exploration,
and agriculture. Current frameworks struggle due to significant variations in
unstructured environments and uncertain scene changes, and are not adaptive to
be used for different robot types. We present AnyTraverse, a framework
combining natural language-based prompts with human-operator assistance to
determine navigable regions for diverse robotic vehicles. The system segments
scenes for a given set of prompts and calls the operator only when encountering
previously unexplored scenery or unknown class not part of the prompt in its
region-of-interest, thus reducing active supervision load while adapting to
varying outdoor scenes. Our zero-shot learning approach eliminates the need for
extensive data collection or retraining. Our experimental validation includes
testing on RELLIS-3D, Freiburg Forest, and RUGD datasets and demonstrate
real-world deployment on multiple robot platforms. The results show that
AnyTraverse performs better than GA-NAV and Off-seg while offering a
vehicle-agnostic approach to off-road traversability that balances automation
with targeted human supervision.

</details>


### [192] [How to Train your Text-to-Image Model: Evaluating Design Choices for Synthetic Training Captions](https://arxiv.org/abs/2506.16679)
*Manuel Brack,Sudeep Katakol,Felix Friedrich,Patrick Schramowski,Hareesh Ravi,Kristian Kersting,Ajinkya Kale*

Main category: cs.CV

TL;DR: This study explores the impact of different synthetic captioning strategies on the performance of text-to-image models, highlighting trade-offs in text alignment, aesthetics, diversity, and output bias.


<details>
  <summary>Details</summary>
Motivation: To investigate how synthetic captioning strategies influence text-to-image model performance, addressing the lack of insights into design choices in current literature.

Method: Conduct systematic experiments to evaluate the effects of different captioning strategies (e.g., dense captions, randomized lengths) and distributions on model performance.

Result: High-quality dense captions enhance text alignment but may compromise aesthetics and diversity. Randomized-length captions offer balanced improvements across aesthetics, alignment, and diversity. Caption distributions significantly affect output bias.

Conclusion: Caption design is critical for optimizing text-to-image model performance, and findings provide practical guidance for creating effective training datasets.

Abstract: Training data is at the core of any successful text-to-image models. The
quality and descriptiveness of image text are crucial to a model's performance.
Given the noisiness and inconsistency in web-scraped datasets, recent works
shifted towards synthetic training captions. While this setup is generally
believed to produce more capable models, current literature does not provide
any insights into its design choices. This study closes this gap by
systematically investigating how different synthetic captioning strategies
impact the downstream performance of text-to-image models. Our experiments
demonstrate that dense, high-quality captions enhance text alignment but may
introduce trade-offs in output aesthetics and diversity. Conversely, captions
of randomized lengths yield balanced improvements across aesthetics and
alignment without compromising sample diversity. We also demonstrate that
varying caption distributions introduce significant shifts in the output bias
of a trained model. Our findings underscore the importance of caption design in
achieving optimal model performance and provide practical insights for more
effective training data strategies in text-to-image generation.

</details>


### [193] [Camera Calibration via Circular Patterns: A Comprehensive Framework with Measurement Uncertainty and Unbiased Projection Model](https://arxiv.org/abs/2506.16842)
*Chaehyeon Song,Dongjae Lee,Jongwoo Lim,Ayoung Kim*

Main category: cs.CV

TL;DR: This paper proposes an unbiased projection model and introduces centroid uncertainty for planar camera calibration using circular patterns, leading to improved accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: Existing planar target calibration methods are prone to biases, particularly when using circle centroids under lens distortion, limiting their effectiveness.

Method: The authors develop an unbiased projection model for circular patterns and incorporate centroid uncertainty using a Markov random field approach to model boundary points, leveraging shape representation through the Green theorem.

Result: The proposed framework delivers significant improvements in calibration accuracy and robustness compared to conventional methods like checkerboard-based calibration.

Conclusion: By mitigating centroid projection biases and introducing uncertainty into the calibration process, this paper presents a robust and effective method, with open code provided for further exploration.

Abstract: Camera calibration using planar targets has been widely favored, and two
types of control points have been mainly considered as measurements: the
corners of the checkerboard and the centroid of circles. Since a centroid is
derived from numerous pixels, the circular pattern provides more precise
measurements than the checkerboard. However, the existing projection model of
circle centroids is biased under lens distortion, resulting in low performance.
To surmount this limitation, we propose an unbiased projection model of the
circular pattern and demonstrate its superior accuracy compared to the
checkerboard. Complementing this, we introduce uncertainty into circular
patterns to enhance calibration robustness and completeness. Defining centroid
uncertainty improves the performance of calibration components, including
pattern detection, optimization, and evaluation metrics. We also provide
guidelines for performing good camera calibration based on the evaluation
metric. The core concept of this approach is to model the boundary points of a
two-dimensional shape as a Markov random field, considering its connectivity.
The shape distribution is propagated to the centroid uncertainty through an
appropriate shape representation based on the Green theorem. Consequently, the
resulting framework achieves marked gains in calibration accuracy and
robustness. The complete source code and demonstration video are available at
https://github.com/chaehyeonsong/discocal.

</details>


### [194] [DepthVanish: Optimizing Adversarial Interval Structures for Stereo-Depth-Invisible Patches](https://arxiv.org/abs/2506.16690)
*Yun Xing,Yue Cao,Nhat Chung,Jie Zhang,Ivor Tsang,Ming-Ming Cheng,Yang Liu,Lei Ma,Qing Guo*

Main category: cs.CV

TL;DR: The paper develops an optimized striped texture approach for adversarial attacks on stereo depth estimation systems, demonstrating effectiveness in both virtual and real-world scenarios.


<details>
  <summary>Details</summary>
Motivation: The work aims to address vulnerabilities in stereo depth estimation systems, which are critical for applications like autonomous driving and robotics, by exploring effective adversarial attack methods.

Method: The authors propose a new adversarial patch design with striped intervals between repeated textures, optimizing both structural and textural elements, and validate its effectiveness through extensive experiments.

Result: The optimized adversarial patches mislead stereo depth systems such as RAFT-Stereo and STTR, and even commercial RGB-D cameras like Intel RealSense in real-world settings.

Conclusion: This method improves the reliability of stereo depth system testing by providing practical and effective adversarial patches for security assessment.

Abstract: Stereo Depth estimation is a critical task in autonomous driving and
robotics, where inaccuracies (such as misidentifying nearby objects as distant)
can lead to dangerous situations. Adversarial attacks against stereo depth
estimation can help reveal vulnerabilities before deployment. Previous work has
shown that repeating optimized textures can effectively mislead stereo depth
estimation in digital settings. However, our research reveals that these
naively repeated texture structures perform poorly in physical-world
implementations, i.e., when deployed as patches, limiting their practical
utility for testing stereo depth estimation systems. In this work, for the
first time, we discover that introducing regular intervals between repeated
textures, creating a striped structure, significantly enhances the patch attack
effectiveness. Through extensive experimentation, we analyze how variations of
this novel structure influence the performance. Based on these insights, we
develop a novel stereo depth attack that jointly optimizes both the striped
structure and texture elements. Our generated adversarial patches can be
inserted into any scenes and successfully attack state-of-the-art stereo depth
estimation methods, i.e., RAFT-Stereo and STTR. Most critically, our patch can
also attack commercial RGB-D cameras (Intel RealSense) in real-world
conditions, demonstrating their practical relevance for security assessment of
stereo systems.

</details>


### [195] [LaVi: Efficient Large Vision-Language Models via Internal Feature Modulation](https://arxiv.org/abs/2506.16691)
*Tongtian Yue,Longteng Guo,Yepeng Tang,Zijia Zhao,Xinxin Zhu,Hua Huang,Jing Liu*

Main category: cs.CV

TL;DR: LaVi introduces a new method for visual-language integration in Large Vision-Language Models, achieving state-of-the-art performance with significantly reduced computational costs.


<details>
  <summary>Details</summary>
Motivation: To overcome inefficiencies in existing approaches to vision-language integration that hinder scalability and efficiency.

Method: LaVi uses lightweight visual context modulation by injecting vision-conditioned deltas into the affine parameters of layer normalization within LLMs.

Result: LaVi achieves state-of-the-art results across 15 benchmarks while drastically improving computational efficiency, cutting FLOPs by 94%, speeding up inference by 3.1 times, and reducing memory use by half compared to LLaVA-OV-7B.

Conclusion: LaVi offers a scalable and efficient solution for real-time multimodal reasoning, preserving linguistic priors while improving performance.

Abstract: Despite the impressive advancements of Large Vision-Language Models (LVLMs),
existing approaches suffer from a fundamental bottleneck: inefficient
visual-language integration. Current methods either disrupt the model's
inherent structure or introduce severe long-context computational burden,
severely limiting scalability and efficiency. In this paper, we rethink
multimodal integration and present LaVi, a novel LVLM that enables seamless and
efficient vision-language fusion through internal feature modulation within the
Large Language Models (LLMs). Unlike dominant LVLMs that rely on visual token
concatenation, LaVi bypasses long-context expansion by introducing a
lightweight and adaptive transformation, which incorporates visual context by
injecting token-wise vision-conditioned deltas into the affine parameters of
layer normalization. This mechanism directly modulates linguistic hidden states
based on visual input, ensuring precise vision-language alignment while
preserving the LLM's linguistic priors and drastically reducing computational
costs. Extensive evaluations across 15 image and video benchmarks demonstrate
that LaVi not only achieves state-of-the-art multimodal performance but also
dramatically enhances efficiency. Compared to LLaVA-OV-7B, LaVi reduces FLOPs
by 94.0%, improves inference speed by 3.1 times, and cuts memory usage in half
- establishing LaVi as a scalable and practical solution for real-time
multimodal reasoning. The code and models will be released soon.

</details>


### [196] [Language-driven Description Generation and Common Sense Reasoning for Video Action Recognition](https://arxiv.org/abs/2506.16701)
*Xiaodan Hu,Chuhang Zou,Suchen Wang,Jaechul Kim,Narendra Ahuja*

Main category: cs.CV

TL;DR: The paper introduces a framework for video action recognition that leverages language-driven common sense priors to handle cluttered video sequences. The approach integrates video context summarization, description generation, and multi-modal recognition.


<details>
  <summary>Details</summary>
Motivation: To overcome challenges in video action recognition, such as occlusions and cluttered views, by leveraging common sense priors from language models.

Method: The proposed method includes three components: video context summarization, description generation using common sense reasoning, and a multi-modal activity recognition head that combines visual and textual information.

Result: The approach was tested on Action Genome and Charades datasets, showing its effectiveness in recognizing complex video actions.

Conclusion: Using language-driven common sense priors enhances video action recognition, particularly in challenging scenarios with occlusions and cluttered views.

Abstract: Recent video action recognition methods have shown excellent performance by
adapting large-scale pre-trained language-image models to the video domain.
However, language models contain rich common sense priors - the scene contexts
that humans use to constitute an understanding of objects, human-object
interactions, and activities - that have not been fully exploited. In this
paper, we introduce a framework incorporating language-driven common sense
priors to identify cluttered video action sequences from monocular views that
are often heavily occluded. We propose: (1) A video context summary component
that generates candidate objects, activities, and the interactions between
objects and activities; (2) A description generation module that describes the
current scene given the context and infers subsequent activities, through
auxiliary prompts and common sense reasoning; (3) A multi-modal activity
recognition head that combines visual and textual cues to recognize video
actions. We demonstrate the effectiveness of our approach on the challenging
Action Genome and Charades datasets.

</details>


### [197] [Few-Shot Generalized Category Discovery With Retrieval-Guided Decision Boundary Enhancement](https://arxiv.org/abs/2506.16728)
*Yunhan Ren,Feng Luo,Siyu Huang*

Main category: cs.CV

TL;DR: The paper introduces Few-shot Generalized Category Discovery (FSGCD), proposing a decision boundary enhancement framework to address GCD tasks under scarce known information conditions.


<details>
  <summary>Details</summary>
Motivation: To explore and improve the performance of Generalized Category Discovery tasks in scenarios with limited labeled samples and known categories.

Method: The paper proposes a decision boundary enhancement framework involving: (1) decision boundary pre-training to mitigate overfitting and improve boundary learning, and (2) a two-stage retrieval-guided strategy using affinity-retrieved pseudo-labeled samples to enhance and transfer these boundaries to unknown clusters.

Result: The proposed method demonstrated superior performance compared to existing approaches across six public GCD benchmarks under FSGCD conditions.

Conclusion: The decision boundary enhancement framework effectively tackles FSGCD challenges, offering competitive advancements and a publicly available implementation for future research.

Abstract: While existing Generalized Category Discovery (GCD) models have achieved
significant success, their performance with limited labeled samples and a small
number of known categories remains largely unexplored. In this work, we
introduce the task of Few-shot Generalized Category Discovery (FSGCD), aiming
to achieve competitive performance in GCD tasks under conditions of known
information scarcity. To tackle this challenge, we propose a decision boundary
enhancement framework with affinity-based retrieval. Our framework is designed
to learn the decision boundaries of known categories and transfer these
boundaries to unknown categories. First, we use a decision boundary
pre-training module to mitigate the overfitting of pre-trained information on
known category boundaries and improve the learning of these decision boundaries
using labeled samples. Second, we implement a two-stage retrieval-guided
decision boundary optimization strategy. Specifically, this strategy further
enhances the severely limited known boundaries by using affinity-retrieved
pseudo-labeled samples. Then, these refined boundaries are applied to unknown
clusters via guidance from affinity-based feature retrieval. Experimental
results demonstrate that our proposed method outperforms existing methods on
six public GCD benchmarks under the FSGCD setting. The codes are available at:
https://github.com/Ryh1218/FSGCD

</details>


### [198] [RGBTrack: Fast, Robust Depth-Free 6D Pose Estimation and Tracking](https://arxiv.org/abs/2506.17119)
*Teng Guo,Jingjin Yu*

Main category: cs.CV

TL;DR: The paper presents RGBTrack, a framework for real-time 6D pose tracking using only RGB data, offering robust performance without requiring depth input.


<details>
  <summary>Details</summary>
Motivation: The goal is to eliminate the dependency on depth sensors for precise and dynamic object pose tracking, enabling broader application scenarios in fields like robotics and AR.

Method: RGBTrack utilizes a binary search strategy and render-and-compare approach for depth inference, integrates advanced 2D tracking with a Kalman filter, and incorporates scale recovery mechanisms using CAD models.

Result: RGBTrack achieves competitive accuracy and real-time performance in 6D object pose tracking tasks, as demonstrated through extensive benchmarking.

Conclusion: RGBTrack offers a practical, depth-free solution for dynamic object tracking, with potential applications in diverse areas such as robotics, AR, and computer vision. The implementation will be open-source.

Abstract: We introduce a robust framework, RGBTrack, for real-time 6D pose estimation
and tracking that operates solely on RGB data, thereby eliminating the need for
depth input for such dynamic and precise object pose tracking tasks. Building
on the FoundationPose architecture, we devise a novel binary search strategy
combined with a render-and-compare mechanism to efficiently infer depth and
generate robust pose hypotheses from true-scale CAD models. To maintain stable
tracking in dynamic scenarios, including rapid movements and occlusions,
RGBTrack integrates state-of-the-art 2D object tracking (XMem) with a Kalman
filter and a state machine for proactive object pose recovery. In addition,
RGBTrack's scale recovery module dynamically adapts CAD models of unknown scale
using an initial depth estimate, enabling seamless integration with modern
generative reconstruction techniques. Extensive evaluations on benchmark
datasets demonstrate that RGBTrack's novel depth-free approach achieves
competitive accuracy and real-time performance, making it a promising practical
solution candidate for application areas including robotics, augmented reality,
and computer vision.
  The source code for our implementation will be made publicly available at
https://github.com/GreatenAnoymous/RGBTrack.git.

</details>


### [199] [TeSG: Textual Semantic Guidance for Infrared and Visible Image Fusion](https://arxiv.org/abs/2506.16730)
*Mingrui Zhu,Xiru Chen,Xin Wei,Nannan Wang,Xinbo Gao*

Main category: cs.CV

TL;DR: This paper introduces a novel method, called TeSG, for improving infrared and visible image fusion using text-guided semantic information, demonstrating its effectiveness in downstream tasks like detection and segmentation.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address insufficient utilization of textual semantic information in infrared and visible image fusion, enhancing the fusion process for better downstream task optimization.

Method: The proposed method, TeSG, uses two levels of textual semantics (mask semantic level and text semantic level) extracted from Vision-Language Models, integrating these through three components: Semantic Information Generator, Mask-Guided Cross-Attention module, and Text-Driven Attentional Fusion module.

Result: Experimental results show TeSG's competitive performance over state-of-the-art methods, particularly excelling in downstream tasks like detection and segmentation.

Conclusion: TeSG effectively leverages textual semantics to optimize infrared-visible image fusion, providing advancements in performance for practical applications such as detection and segmentation.

Abstract: Infrared and visible image fusion (IVF) aims to combine complementary
information from both image modalities, producing more informative and
comprehensive outputs. Recently, text-guided IVF has shown great potential due
to its flexibility and versatility. However, the effective integration and
utilization of textual semantic information remains insufficiently studied. To
tackle these challenges, we introduce textual semantics at two levels: the mask
semantic level and the text semantic level, both derived from textual
descriptions extracted by large Vision-Language Models (VLMs). Building on
this, we propose Textual Semantic Guidance for infrared and visible image
fusion, termed TeSG, which guides the image synthesis process in a way that is
optimized for downstream tasks such as detection and segmentation.
Specifically, TeSG consists of three core components: a Semantic Information
Generator (SIG), a Mask-Guided Cross-Attention (MGCA) module, and a Text-Driven
Attentional Fusion (TDAF) module. The SIG generates mask and text semantics
based on textual descriptions. The MGCA module performs initial attention-based
fusion of visual features from both infrared and visible images, guided by mask
semantics. Finally, the TDAF module refines the fusion process with gated
attention driven by text semantics. Extensive experiments demonstrate the
competitiveness of our approach, particularly in terms of performance on
downstream tasks, compared to existing state-of-the-art methods.

</details>


### [200] [Part$^{2}$GS: Part-aware Modeling of Articulated Objects using 3D Gaussian Splatting](https://arxiv.org/abs/2506.17212)
*Tianjiao Yu,Vedant Shah,Muntasir Wahed,Ying Shen,Kiet A. Nguyen,Ismini Lourentzou*

Main category: cs.CV

TL;DR: This paper introduces Part$^{2}$GS, a framework for modeling high-fidelity, articulated 3D structures with consistent motion and geometry.


<details>
  <summary>Details</summary>
Motivation: Existing 3D reconstruction methods struggle to model articulated objects with accurate structure and motion.

Method: Part$^{2}$GS uses a part-aware 3D Gaussian representation with learnable attributes, physics-based constraints for motion consistency, and a repel-points field to avoid collisions.

Result: Part$^{2}$GS achieves superior performance compared to state-of-the-art methods, notably improving Chamfer Distance by up to 10x for movable parts.

Conclusion: Part$^{2}$GS effectively models articulated objects with high fidelity and physically consistent motion, outperforming other approaches in both synthetic and real-world tests.

Abstract: Articulated objects are common in the real world, yet modeling their
structure and motion remains a challenging task for 3D reconstruction methods.
In this work, we introduce Part$^{2}$GS, a novel framework for modeling
articulated digital twins of multi-part objects with high-fidelity geometry and
physically consistent articulation. Part$^{2}$GS leverages a part-aware 3D
Gaussian representation that encodes articulated components with learnable
attributes, enabling structured, disentangled transformations that preserve
high-fidelity geometry. To ensure physically consistent motion, we propose a
motion-aware canonical representation guided by physics-based constraints,
including contact enforcement, velocity consistency, and vector-field
alignment. Furthermore, we introduce a field of repel points to prevent part
collisions and maintain stable articulation paths, significantly improving
motion coherence over baselines. Extensive evaluations on both synthetic and
real-world datasets show that Part$^{2}$GS consistently outperforms
state-of-the-art methods by up to 10$\times$ in Chamfer Distance for movable
parts.

</details>


### [201] [3DeepRep: 3D Deep Low-rank Tensor Representation for Hyperspectral Image Inpainting](https://arxiv.org/abs/2506.16735)
*Yunshan Li,Wenwu Gong,Qianqian Wang,Chao Wang,Lili Yang*

Main category: cs.CV

TL;DR: This paper introduces a novel hyperspectral image (HSI) inpainting method using a 3-directional deep low-rank tensor representation model (3DeepRep), which improves results by performing deep transforms across all tensor modes and combining outputs effectively.


<details>
  <summary>Details</summary>
Motivation: Current hyperspectral image (HSI) inpainting approaches often restrict low-rank tensor representations to the spectral mode, missing opportunities to leverage low-rank properties in other modes.

Method: The authors developed 3DeepRep, which applies deep nonlinear transforms on all three tensor modes of HSI. It enforces low-rankness by minimizing nuclear norms of frontal slices across three directions and employs a learnable aggregation module to combine outputs.

Result: Extensive experiments on real-world datasets show that 3DeepRep achieves superior inpainting performance both qualitatively and quantitatively compared to state-of-the-art methods.

Conclusion: 3DeepRep introduces a comprehensive approach to HSI inpainting by addressing low-rank tensor representation in all modes and produces superior results through advanced deep transforms and optimization techniques.

Abstract: Recent approaches based on transform-based tensor nuclear norm (TNN) have
demonstrated notable effectiveness in hyperspectral image (HSI) inpainting by
leveraging low-rank structures in latent representations. Recent developments
incorporate deep transforms to improve low-rank tensor representation; however,
existing approaches typically restrict the transform to the spectral mode,
neglecting low-rank properties along other tensor modes. In this paper, we
propose a novel 3-directional deep low-rank tensor representation (3DeepRep)
model, which performs deep nonlinear transforms along all three modes of the
HSI tensor. To enforce low-rankness, the model minimizes the nuclear norms of
mode-i frontal slices in the corresponding latent space for each direction
(i=1,2,3), forming a 3-directional TNN regularization. The outputs from the
three directional branches are subsequently fused via a learnable aggregation
module to produce the final result. An efficient gradient-based optimization
algorithm is developed to solve the model in a self-supervised manner.
Extensive experiments on real-world HSI datasets demonstrate that the proposed
method achieves superior inpainting performance compared to existing
state-of-the-art techniques, both qualitatively and quantitatively.

</details>


### [202] [Long-term Traffic Simulation with Interleaved Autoregressive Motion and Scenario Generation](https://arxiv.org/abs/2506.17213)
*Xiuyu Yang,Shuhan Tan,Philipp Krähenbühl*

Main category: cs.CV

TL;DR: InfGen introduces a unified model for long-term traffic simulation involving both motion and scene generation, outperforming prior methods.


<details>
  <summary>Details</summary>
Motivation: Current traffic simulation models struggle with long-term simulation due to their inability to handle dynamically changing agents and scenes effectively.

Method: InfGen uses a next-token prediction approach that alternates between closed-loop motion simulation and scene generation, enabling stable and seamless transitions for long-term simulation.

Result: InfGen achieves state-of-the-art performance in short-term (9s) traffic simulation and outperforms all existing techniques in long-term (30s) simulation.

Conclusion: InfGen offers a robust solution for realistic long-term traffic simulation by combining motion and scene dynamics, enhancing the fidelity of self-driving system evaluations.

Abstract: An ideal traffic simulator replicates the realistic long-term point-to-point
trip that a self-driving system experiences during deployment. Prior models and
benchmarks focus on closed-loop motion simulation for initial agents in a
scene. This is problematic for long-term simulation. Agents enter and exit the
scene as the ego vehicle enters new regions. We propose InfGen, a unified
next-token prediction model that performs interleaved closed-loop motion
simulation and scene generation. InfGen automatically switches between
closed-loop motion simulation and scene generation mode. It enables stable
long-term rollout simulation. InfGen performs at the state-of-the-art in
short-term (9s) traffic simulation, and significantly outperforms all other
methods in long-term (30s) simulation. The code and model of InfGen will be
released at https://orangesodahub.github.io/InfGen

</details>


### [203] [Cross-modal Offset-guided Dynamic Alignment and Fusion for Weakly Aligned UAV Object Detection](https://arxiv.org/abs/2506.16737)
*Liu Zongzhen,Luo Hui,Wang Zhixing,Wei Yuxing,Zuo Haorui,Zhang Jianlin*

Main category: cs.CV

TL;DR: The paper introduces CoDAF, a framework that tackles weak alignment in UAV RGB and IR imagery through dynamic alignment and feature fusion, achieving 78.6% mAP on the DroneVehicle dataset.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address challenges in multimodal UAV object detection caused by weak alignment due to platform motion and asynchronous imaging.

Method: CoDAF consists of two core modules: Offset-guided Semantic Alignment (OSA) for precise feature alignment and Dynamic Attention-guided Fusion Module (DAFM) for adaptive feature fusion.

Result: CoDAF achieves a significant performance improvement, with a mAP of 78.6% on the DroneVehicle dataset during experiments.

Conclusion: By integrating alignment and fusion in a unified design, CoDAF offers a robust solution for multimodal object detection in UAV applications.

Abstract: Unmanned aerial vehicle (UAV) object detection plays a vital role in
applications such as environmental monitoring and urban security. To improve
robustness, recent studies have explored multimodal detection by fusing visible
(RGB) and infrared (IR) imagery. However, due to UAV platform motion and
asynchronous imaging, spatial misalignment frequently occurs between
modalities, leading to weak alignment. This introduces two major challenges:
semantic inconsistency at corresponding spatial locations and modality conflict
during feature fusion. Existing methods often address these issues in
isolation, limiting their effectiveness. In this paper, we propose Cross-modal
Offset-guided Dynamic Alignment and Fusion (CoDAF), a unified framework that
jointly tackles both challenges in weakly aligned UAV-based object detection.
CoDAF comprises two novel modules: the Offset-guided Semantic Alignment (OSA),
which estimates attention-based spatial offsets and uses deformable convolution
guided by a shared semantic space to align features more precisely; and the
Dynamic Attention-guided Fusion Module (DAFM), which adaptively balances
modality contributions through gating and refines fused features via
spatial-channel dual attention. By integrating alignment and fusion in a
unified design, CoDAF enables robust UAV object detection. Experiments on
standard benchmarks validate the effectiveness of our approach, with CoDAF
achieving a mAP of 78.6% on the DroneVehicle dataset.

</details>


### [204] [Uncertainty-Aware Variational Information Pursuit for Interpretable Medical Image Analysis](https://arxiv.org/abs/2506.16742)
*Md Nahiduzzaman,Ruwan Tennakoon,Steven Korevaar,Zongyuan Ge,Alireza Bab-Hadiashar*

Main category: cs.CV

TL;DR: This paper proposes UAV-IP, integrating uncertainty quantification into V-IP for better interpretability and improved medical decision-making.


<details>
  <summary>Details</summary>
Motivation: To address the lack of instance-level uncertainty quantification in V-IP for medical AI, ensuring interpretable and reliable decision support.

Method: Introduces UAV-IP, combining uncertainty quantification with V-IP to manage epistemic and aleatoric uncertainties.

Result: UAV-IP achieved a 3.2% higher AUC and 20% more concise explanations across 4 medical imaging datasets compared to standard V-IP.

Conclusion: Incorporating uncertainty improves the robustness of interpretable-by-design models, enhancing trust and utility in clinical decision-making.

Abstract: In medical imaging, AI decision-support systems must balance accuracy and
interpretability to build user trust and support effective clinical
decision-making. Recently, Variational Information Pursuit (V-IP) and its
variants have emerged as interpretable-by-design modeling techniques, aiming to
explain AI decisions in terms of human-understandable, clinically relevant
concepts. However, existing V-IP methods overlook instance-level uncertainties
in query-answer generation, which can arise from model limitations (epistemic
uncertainty) or variability in expert responses (aleatoric uncertainty).
  This paper introduces Uncertainty-Aware V-IP (UAV-IP), a novel framework that
integrates uncertainty quantification into the V-IP process. We evaluate UAV-IP
across four medical imaging datasets, PH2, Derm7pt, BrEaST, and SkinCon,
demonstrating an average AUC improvement of approximately 3.2% while generating
20% more concise explanations compared to baseline V-IP, without sacrificing
informativeness. These findings highlight the importance of uncertainty-aware
reasoning in interpretable by design models for robust and reliable medical
decision-making.

</details>


### [205] [Noise-Informed Diffusion-Generated Image Detection with Anomaly Attention](https://arxiv.org/abs/2506.16743)
*Weinan Guan,Wei Wang,Bo Peng,Ziwen He,Jing Dong,Haonan Cheng*

Main category: cs.CV

TL;DR: This paper introduces NASA-Swin, a novel architecture leveraging a Noise-Aware Self-Attention module to enhance detection capabilities for images generated by unseen diffusion models.


<details>
  <summary>Details</summary>
Motivation: To address concerns about the malicious use of diffusion-generated images and the challenge of detecting such images from unknown models.

Method: The approach focuses on noise patterns using a Noise-Aware Self-Attention (NASA) module integrated into Swin Transformer, leveraging cross-modality fusion embedding with RGB and noise images, and a channel mask strategy.

Result: The proposed method achieved state-of-the-art performance in detecting diffusion-generated images, demonstrating robustness against unseen generation methods.

Conclusion: Noise patterns can distinguish diffusion-generated from genuine images effectively, and NASA-Swin architecture significantly enhances forgery detection capabilities.

Abstract: With the rapid development of image generation technologies, especially the
advancement of Diffusion Models, the quality of synthesized images has
significantly improved, raising concerns among researchers about information
security. To mitigate the malicious abuse of diffusion models,
diffusion-generated image detection has proven to be an effective
countermeasure.However, a key challenge for forgery detection is generalising
to diffusion models not seen during training. In this paper, we address this
problem by focusing on image noise. We observe that images from different
diffusion models share similar noise patterns, distinct from genuine images.
Building upon this insight, we introduce a novel Noise-Aware Self-Attention
(NASA) module that focuses on noise regions to capture anomalous patterns. To
implement a SOTA detection model, we incorporate NASA into Swin Transformer,
forming an novel detection architecture NASA-Swin. Additionally, we employ a
cross-modality fusion embedding to combine RGB and noise images, along with a
channel mask strategy to enhance feature learning from both modalities.
Extensive experiments demonstrate the effectiveness of our approach in
enhancing detection capabilities for diffusion-generated images. When
encountering unseen generation methods, our approach achieves the
state-of-the-art performance.Our code is available at
https://github.com/WeinanGuan/NASA-Swin.

</details>


### [206] [Class Agnostic Instance-level Descriptor for Visual Instance Search](https://arxiv.org/abs/2506.16745)
*Qi-Ying Sun,Wan-Lei Zhao,Yi-Bo Miao,Chong-Wah Ngo*

Main category: cs.CV

TL;DR: The paper proposes a hierarchical approach using self-supervised ViT for visual instance search, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Effective instance-level feature representation for visual instance search is lacking, especially for unknown object categories.

Method: Hierarchical decomposition of self-supervised ViT features to model compact feature subsets representing instances at different semantic scales.

Result: The approach achieves substantial performance improvement over state-of-the-art methods on three instance search benchmarks.

Conclusion: The hierarchical instance-level descriptor effectively handles embedding and occlusions, making it robust for both known and unknown object categories.

Abstract: Despite the great success of the deep features in content-based image
retrieval, the visual instance search remains challenging due to the lack of
effective instance level feature representation. Supervised or weakly
supervised object detection methods are not among the options due to their poor
performance on the unknown object categories. In this paper, based on the
feature set output from self-supervised ViT, the instance level region
discovery is modeled as detecting the compact feature subsets in a hierarchical
fashion. The hierarchical decomposition results in a hierarchy of feature
subsets. The non-leaf nodes and leaf nodes on the hierarchy correspond to the
various instance regions in an image of different semantic scales. The
hierarchical decomposition well addresses the problem of object embedding and
occlusions, which are widely observed in the real scenarios. The features
derived from the nodes on the hierarchy make up a comprehensive representation
for the latent instances in the image. Our instance-level descriptor remains
effective on both the known and unknown object categories. Empirical studies on
three instance search benchmarks show that it outperforms state-of-the-art
methods considerably.

</details>


### [207] [Infrared and Visible Image Fusion Based on Implicit Neural Representations](https://arxiv.org/abs/2506.16773)
*Shuchen Sun,Ligen Shi,Chang Liu,Lina Wu,Jun Qiu*

Main category: cs.CV

TL;DR: This paper introduces INRFuse, an image fusion method using Implicit Neural Representations (INR) for infrared and visible light images, overcoming pixel-based limitations and producing high-quality fused images.


<details>
  <summary>Details</summary>
Motivation: To effectively combine infrared and visible light images to create rich, information-dense outputs without dependency on traditional pixel-based approaches, explicitly addressing challenges in resolution variance and computational requirements.

Method: The method uses a neural network to represent multimodal image information implicitly. It takes normalized spatial coordinates from both image modalities as input and employs multi-layer perceptrons alongside multiple loss functions to adaptively fuse features and optimize similarity to source images.

Result: INRFuse produces fused images that outperform existing techniques in subjective and objective evaluations, delivering visually clear, detail-rich, and information-preserving outputs without requiring a training dataset.

Conclusion: INRFuse achieves effective, resolution-independent image fusion of infrared and visible light modalities, maintaining the strengths of both while enabling flexibility and improved image quality through implicit neural representations.

Abstract: Infrared and visible light image fusion aims to combine the strengths of both
modalities to generate images that are rich in information and fulfill visual
or computational requirements. This paper proposes an image fusion method based
on Implicit Neural Representations (INR), referred to as INRFuse. This method
parameterizes a continuous function through a neural network to implicitly
represent the multimodal information of the image, breaking through the
traditional reliance on discrete pixels or explicit features. The normalized
spatial coordinates of the infrared and visible light images serve as inputs,
and multi-layer perceptrons is utilized to adaptively fuse the features of both
modalities, resulting in the output of the fused image. By designing multiple
loss functions, the method jointly optimizes the similarity between the fused
image and the original images, effectively preserving the thermal radiation
information of the infrared image while maintaining the texture details of the
visible light image. Furthermore, the resolution-independent characteristic of
INR allows for the direct fusion of images with varying resolutions and
achieves super-resolution reconstruction through high-density coordinate
queries. Experimental results indicate that INRFuse outperforms existing
methods in both subjective visual quality and objective evaluation metrics,
producing fused images with clear structures, natural details, and rich
information without the necessity for a training dataset.

</details>


### [208] [PQCAD-DM: Progressive Quantization and Calibration-Assisted Distillation for Extremely Efficient Diffusion Model](https://arxiv.org/abs/2506.16776)
*Beomseok Ko,Hyeryung Jang*

Main category: cs.CV

TL;DR: The paper introduces PQCAD-DM, a compression framework for diffusion models that balances computational efficiency with generative quality, cutting inference time in half while maintaining strong performance.


<details>
  <summary>Details</summary>
Motivation: Diffusion models are resource-intensive, with iterative processes leading to error accumulation and inefficiency. There is a need for methods to improve computational efficiency while preserving generative quality.

Method: The PQCAD-DM framework combines Progressive Quantization (PQ), which adaptively manages bit-width transitions to minimize perturbations, and Calibration-Assisted Distillation (CAD), which uses calibration datasets to match the performance of full-precision models.

Result: PQCAD-DM effectively reduces inference time by 50% while maintaining competitive generative performance, outperforming fixed-bit quantization techniques in experiments.

Conclusion: PQCAD-DM demonstrates that hybrid compression strategies can successfully optimize diffusion models, achieving both efficiency and high-quality generation outcomes.

Abstract: Diffusion models excel in image generation but are computational and
resource-intensive due to their reliance on iterative Markov chain processes,
leading to error accumulation and limiting the effectiveness of naive
compression techniques. In this paper, we propose PQCAD-DM, a novel hybrid
compression framework combining Progressive Quantization (PQ) and
Calibration-Assisted Distillation (CAD) to address these challenges. PQ employs
a two-stage quantization with adaptive bit-width transitions guided by a
momentum-based mechanism, reducing excessive weight perturbations in
low-precision. CAD leverages full-precision calibration datasets during
distillation, enabling the student to match full-precision performance even
with a quantized teacher. As a result, PQCAD-DM achieves a balance between
computational efficiency and generative quality, halving inference time while
maintaining competitive performance. Extensive experiments validate PQCAD-DM's
superior generative capabilities and efficiency across diverse datasets,
outperforming fixed-bit quantization methods.

</details>


### [209] [TextBraTS: Text-Guided Volumetric Brain Tumor Segmentation with Innovative Dataset Development and Fusion Module Exploration](https://arxiv.org/abs/2506.16784)
*Xiaoyu Shi,Rahul Kumar Jain,Yinhao Li,Ruibo Hou,Jingliang Cheng,Jie Bai,Guohua Zhao,Lanfen Lin,Rui Xu,Yen-wei Chen*

Main category: cs.CV

TL;DR: This paper introduces the TextBraTS dataset, a multimodal dataset combining MRI scans and textual annotations, and proposes a novel framework for enhancing brain tumor segmentation accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the lack of multimodal datasets combining radiological images and textual annotations in brain tumor segmentation tasks, which prevents exploration of multimodal approaches.

Method: The paper introduces the TextBraTS dataset and proposes a sequential cross-attention framework for text-guided volumetric medical image segmentation.

Result: Experiments with various text-image fusion strategies and templated text formulations show significant improvements in brain tumor segmentation accuracy.

Conclusion: The integration of textual annotations with MRI scans improves segmentation accuracy, offering a public dataset, code, and pre-trained models for further research in multimodal medical imaging approaches.

Abstract: Deep learning has demonstrated remarkable success in medical image
segmentation and computer-aided diagnosis. In particular, numerous advanced
methods have achieved state-of-the-art performance in brain tumor segmentation
from MRI scans. While recent studies in other medical imaging domains have
revealed that integrating textual reports with visual data can enhance
segmentation accuracy, the field of brain tumor analysis lacks a comprehensive
dataset that combines radiological images with corresponding textual
annotations. This limitation has hindered the exploration of multimodal
approaches that leverage both imaging and textual data.
  To bridge this critical gap, we introduce the TextBraTS dataset, the first
publicly available volume-level multimodal dataset that contains paired MRI
volumes and rich textual annotations, derived from the widely adopted BraTS2020
benchmark. Building upon this novel dataset, we propose a novel baseline
framework and sequential cross-attention method for text-guided volumetric
medical image segmentation. Through extensive experiments with various
text-image fusion strategies and templated text formulations, our approach
demonstrates significant improvements in brain tumor segmentation accuracy,
offering valuable insights into effective multimodal integration techniques.
  Our dataset, implementation code, and pre-trained models are publicly
available at https://github.com/Jupitern52/TextBraTS.

</details>


### [210] [RealSR-R1: Reinforcement Learning for Real-World Image Super-Resolution with Vision-Language Chain-of-Thought](https://arxiv.org/abs/2506.16796)
*Junbo Qiao,Miaomiao Cai,Wei Li,Yutong Liu,Xudong Huang,Gaoqi He,Jiao Xie,Jie Hu,Xinghao Chen,Shaohui Lin*

Main category: cs.CV

TL;DR: This paper introduces RealSR-R1, a model that improves real-world image super-resolution by combining vision and language reasoning capabilities inspired by Chain of Thought methods in large language models.


<details>
  <summary>Details</summary>
Motivation: Existing approaches to image super-resolution struggle with accurately understanding degraded image content, resulting in reconstructed outputs that lack fidelity and appear unnatural.

Method: The authors propose the VLCoT framework, which uses progressively generated text and higher-resolution images for image restoration and integrates Group Relative Policy Optimization (GRPO) with four reward functions to enhance the model's reasoning and restoration capabilities.

Result: Extensive experiments confirm that RealSR-R1 excels at generating realistic details and accurately understanding image content, especially for semantically rich or severely degraded images.

Conclusion: RealSR-R1 offers an innovative solution to improve real-world image super-resolution by incorporating reasoning mechanisms and GRPO-based rewards, making it more effective in handling complex and degraded image scenarios.

Abstract: Real-World Image Super-Resolution is one of the most challenging task in
image restoration. However, existing methods struggle with an accurate
understanding of degraded image content, leading to reconstructed results that
are both low-fidelity and unnatural. We present RealSR-R1 in this work, which
empowers the RealSR models with understanding and reasoning capabilities.
Inspired by the success of Chain of Thought (CoT) in large language models
(LLMs), we simulate the human process of handling degraded images and propose
the VLCoT framework, which integrates vision and language reasoning. The
framework aims to precisely restore image details by progressively generating
more comprehensive text and higher-resolution images. To overcome the challenge
of traditional supervised learning CoT failing to generalize to real-world
scenarios, we introduce, for the first time, Group Relative Policy Optimization
(GRPO) into the Real-World Image Super-Resolution task. We propose VLCoT-GRPO
as a solution, which designs four reward functions: (1) Format reward, used to
standardize the CoT process; (2) Degradation reward, to incentivize accurate
degradation estimation; (3) Understanding reward, to ensure the accuracy of the
generated content; and (4) Generation reward, where we propose using a visual
expert model to evaluate the quality of generated images, encouraging the model
to generate more realistic images. Extensive experiments demonstrate that our
proposed RealSR-R1 can generate realistic details and accurately understand
image content, particularly in semantically rich scenes or images with severe
degradation.

</details>


### [211] [Seeing What Matters: Generalizable AI-generated Video Detection with Forensic-Oriented Augmentation](https://arxiv.org/abs/2506.16802)
*Riccardo Corvi,Davide Cozzolino,Ekta Prashnani,Shalini De Mello,Koki Nagano,Luisa Verdoliva*

Main category: cs.CV

TL;DR: The paper discusses improving video forensic detectors by focusing on low-level artifacts and proposing a wavelet-based data augmentation strategy to enhance generalizability across synthetic video models.


<details>
  <summary>Details</summary>
Motivation: Current AI-generated video forensic detectors suffer from poor generalization and struggle to reliably differentiate between synthetic and real videos, especially across various generative models.

Method: The authors identify intrinsic low-level artifacts common to generative architectures and propose a forensic training technique using wavelet decomposition to enhance detector focus on relevant features. This approach eliminates reliance on complex algorithms and large datasets.

Result: Using a detector trained on videos from a single generative model, the method achieves significant accuracy improvements across a wide range of other models, including the latest ones like NOVA and FLUX.

Conclusion: The proposed strategy improves generalizability in synthetic video detection efficiently, offering a practical advancement in video forensics without requiring extensive resources. Public release of code and data is also planned.

Abstract: Synthetic video generation is progressing very rapidly. The latest models can
produce very realistic high-resolution videos that are virtually
indistinguishable from real ones. Although several video forensic detectors
have been recently proposed, they often exhibit poor generalization, which
limits their applicability in a real-world scenario. Our key insight to
overcome this issue is to guide the detector towards seeing what really
matters. In fact, a well-designed forensic classifier should focus on
identifying intrinsic low-level artifacts introduced by a generative
architecture rather than relying on high-level semantic flaws that characterize
a specific model. In this work, first, we study different generative
architectures, searching and identifying discriminative features that are
unbiased, robust to impairments, and shared across models. Then, we introduce a
novel forensic-oriented data augmentation strategy based on the wavelet
decomposition and replace specific frequency-related bands to drive the model
to exploit more relevant forensic cues. Our novel training paradigm improves
the generalizability of AI-generated video detectors, without the need for
complex algorithms and large datasets that include multiple synthetic
generators. To evaluate our approach, we train the detector using data from a
single generative model and test it against videos produced by a wide range of
other models. Despite its simplicity, our method achieves a significant
accuracy improvement over state-of-the-art detectors and obtains excellent
results even on very recent generative models, such as NOVA and FLUX. Code and
data will be made publicly available.

</details>


### [212] [Co-VisiON: Co-Visibility ReasONing on Sparse Image Sets of Indoor Scenes](https://arxiv.org/abs/2506.16805)
*Chao Chen,Nobel Dang,Juexiao Zhang,Wenkai Sun,Pengfei Zheng,Xuhang He,Yimeng Ye,Taarun Srinivas,Chen Feng*

Main category: cs.CV

TL;DR: The paper introduces the Co-Visibility reasoning (Co-VisiON) benchmark to evaluate vision models on their ability to analyze sparse image sets for co-visibility reasoning, a task where humans outperform existing models.


<details>
  <summary>Details</summary>
Motivation: Assessing if vision models can achieve human-level co-visibility reasoning proficiency in sparse and complex environments.

Method: Developing and testing the Co-VisiON benchmark on over 1000 indoor scenarios; introducing a novel baseline model (Covis) for multi-view reasoning.

Result: Existing vision models struggle with co-visibility reasoning under sparse conditions; a proprietary vision-language model leads performance but is still behind humans.

Conclusion: The findings stress the gap in human and machine co-visibility reasoning and emphasize the need for models with spatial understanding through high-level multi-view reasoning.

Abstract: Humans exhibit a remarkable ability to recognize co-visibility-the
overlapping regions visible in multiple images-even when these images are
sparsely distributed across a complex scene. This capability is foundational in
3D vision and robotic perception. Despite significant progress in vision
learning, it remains unclear whether current vision models have reached
human-level proficiency in co-visibility analysis. In this work, we introduce
the Co-Visibility reasONing (Co-VisiON) benchmark, designed to directly
evaluate co-visibility reasoning on sparse image sets across over 1000 indoor
scenarios. Our experiments reveal that while co-visibility is typically treated
as a low-level feature matching task, it poses a significant challenge for
existing vision models under sparse conditions. Notably, a proprietary
vision-language model outperforms all purely vision-based approaches, with all
models lagging substantially behind human performance. This gap underscores the
need for more than basic pairwise vision processing-it calls for a
comprehensive spatial understanding through high-level reasoning across
multiple views. Inspired by human visual cognition, we propose a novel
multi-view baseline, Covis, which achieves top performance among pure vision
models and narrows the gap to the proprietary VLM. We hope our benchmark and
findings will spur further advancements in developing vision models capable of
robust, high-level reasoning in challenging, sparse environments. Our dataset
and source code can be found at: https://ai4ce.github.io/CoVISION

</details>


### [213] [FOCUS: Unified Vision-Language Modeling for Interactive Editing Driven by Referential Segmentation](https://arxiv.org/abs/2506.16806)
*Fan Yang,Yousong Zhu,Xin Li,Yufei Zhan,Hongyin Zhao,Shurong Zheng,Yaowei Wang,Ming Tang,Jinqiao Wang*

Main category: cs.CV

TL;DR: FOCUS is a unified large vision-language model (LVLM) that integrates visual understanding and controllable object-centric image generation, addressing previous limitations by employing a joint segmentation and generation framework.


<details>
  <summary>Details</summary>
Motivation: To unify visual understanding and generative modeling, eliminating the limitations of treating object segmentation and editing tasks separately and relying on multiple disjointed models.

Method: FOCUS uses a dual-branch visual encoder for global and fine-grained detail capture, a MoVQGAN-based visual tokenizer for enhanced generation quality, and a progressive multi-stage training pipeline aligning segmentation, encoding, and generation modules.

Result: Experiments on multimodal understanding, referring segmentation accuracy, and controllable image generation show strong performance for FOCUS, jointly optimizing visual perception and generative abilities.

Conclusion: FOCUS effectively bridges segmentation-aware perception with visual synthesis, offering a robust end-to-end framework for accurate visual understanding and controllable editing.

Abstract: Recent Large Vision Language Models (LVLMs) demonstrate promising
capabilities in unifying visual understanding and generative modeling, enabling
both accurate content understanding and flexible editing. However, current
approaches treat "what to see" and "how to edit" separately: they either
perform isolated object segmentation or utilize segmentation masks merely as
conditional prompts for local edit generation tasks, often relying on multiple
disjointed models. To bridge these gaps, we introduce FOCUS, a unified LVLM
that integrates segmentation-aware perception and controllable object-centric
generation within an end-to-end framework. FOCUS employs a dual-branch visual
encoder to simultaneously capture global semantic context and fine-grained
spatial details. In addition, we leverage a MoVQGAN-based visual tokenizer to
produce discrete visual tokens that enhance generation quality. To enable
accurate and controllable image editing, we propose a progressive multi-stage
training pipeline, where segmentation masks are jointly optimized and used as
spatial condition prompts to guide the diffusion decoder. This strategy aligns
visual encoding, segmentation, and generation modules, effectively bridging
segmentation-aware perception with fine-grained visual synthesis. Extensive
experiments across three core tasks, including multimodal understanding,
referring segmentation accuracy, and controllable image generation, demonstrate
that FOCUS achieves strong performance by jointly optimizing visual perception
and generative capabilities.

</details>


### [214] [Loupe: A Generalizable and Adaptive Framework for Image Forgery Detection](https://arxiv.org/abs/2506.16819)
*Yuchu Jiang,Jiaming Chu,Jian Zhao,Xin Zhang,Xu Yang,Lei Jin,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: Loupe is a lightweight yet effective framework for simultaneously detecting and localizing deepfakes using a patch-aware classifier and segmentation module, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: The paper addresses concerns about visual content forgery due to generative models. Existing deepfake detection methods struggle with generalization across manipulation types or require complex architectures.

Method: Loupe employs a patch-aware classifier combined with a segmentation module using conditional queries for global classification and detailed mask prediction, along with a pseudo-label-guided test-time adaptation technique.

Result: Loupe demonstrated state-of-the-art performance on the DDL dataset, achieving an overall score of 0.846 and securing first place in the IJCAI 2025 Deepfake Detection and Localization Challenge.

Conclusion: The study highlights the effectiveness of Loupe's patch-level fusion and conditional query designs, showing improvements in both detection accuracy and localization for various forgery types.

Abstract: The proliferation of generative models has raised serious concerns about
visual content forgery. Existing deepfake detection methods primarily target
either image-level classification or pixel-wise localization. While some
achieve high accuracy, they often suffer from limited generalization across
manipulation types or rely on complex architectures. In this paper, we propose
Loupe, a lightweight yet effective framework for joint deepfake detection and
localization. Loupe integrates a patch-aware classifier and a segmentation
module with conditional queries, allowing simultaneous global authenticity
classification and fine-grained mask prediction. To enhance robustness against
distribution shifts of test set, Loupe introduces a pseudo-label-guided
test-time adaptation mechanism by leveraging patch-level predictions to
supervise the segmentation head. Extensive experiments on the DDL dataset
demonstrate that Loupe achieves state-of-the-art performance, securing the
first place in the IJCAI 2025 Deepfake Detection and Localization Challenge
with an overall score of 0.846. Our results validate the effectiveness of the
proposed patch-level fusion and conditional query design in improving both
classification accuracy and spatial localization under diverse forgery
patterns. The code is available at https://github.com/Kamichanw/Loupe.

</details>


### [215] [Self-supervised Feature Extraction for Enhanced Ball Detection on Soccer Robots](https://arxiv.org/abs/2506.16821)
*Can Lin,Daniele Affinita,Marco E. P. Zimmatore,Daniele Nardi,Domenico D. Bloisi,Vincenzo Suriani*

Main category: cs.CV

TL;DR: This study introduces a self-supervised learning framework for robust ball detection in autonomous humanoid soccer robots, avoiding manual annotation.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenge of ball detection in dynamic soccer environments, reducing dependency on costly manual annotation.

Method: The framework uses pretrained models for pseudo-labels and employs self-supervised tasks combined with model-agnostic meta-learning for adaptive feature extraction.

Result: The proposed method outperformed baseline models in accuracy, F1 score, and IoU, with faster convergence.

Conclusion: Self-supervised learning and meta-learning strategies enable effective, adaptive ball detection in challenging environments with reduced supervision.

Abstract: Robust and accurate ball detection is a critical component for autonomous
humanoid soccer robots, particularly in dynamic and challenging environments
such as RoboCup outdoor fields. However, traditional supervised approaches
require extensive manual annotation, which is costly and time-intensive. To
overcome this problem, we present a self-supervised learning framework for
domain-adaptive feature extraction to enhance ball detection performance. The
proposed approach leverages a general-purpose pretrained model to generate
pseudo-labels, which are then used in a suite of self-supervised pretext tasks
-- including colorization, edge detection, and triplet loss -- to learn robust
visual features without relying on manual annotations. Additionally, a
model-agnostic meta-learning (MAML) strategy is incorporated to ensure rapid
adaptation to new deployment scenarios with minimal supervision. A new dataset
comprising 10,000 labeled images from outdoor RoboCup SPL matches is
introduced, used to validate the method, and made available to the community.
Experimental results demonstrate that the proposed pipeline outperforms
baseline models in terms of accuracy, F1 score, and IoU, while also exhibiting
faster convergence.

</details>


### [216] [Controllable and Expressive One-Shot Video Head Swapping](https://arxiv.org/abs/2506.16852)
*Chaonan Ji,Jinwei Qi,Peng Zhang,Bang Zhang,Liefeng Bo*

Main category: cs.CV

TL;DR: This paper introduces a novel diffusion-based framework for video head swapping, addressing both identity preservation and expression editing.


<details>
  <summary>Details</summary>
Motivation: Current methods for face/head swapping struggle with preserving holistic head identity, managing complex hairstyles and backgrounds, and allowing post-swap expression editing.

Method: The approach leverages a unified latent diffusion paradigm incorporating identity-preserving context fusion, expression-aware landmark retargeting, and a shape-agnostic mask strategy for robust identity retention and expression editing.

Result: Experiments show the method achieves better integration with backgrounds, retains head identity, and offers advanced expression transfer for both real and virtual characters.

Conclusion: This framework provides an innovative and effective solution for dynamic, editable head-swapping in videos, overcoming limitations of previous methods.

Abstract: In this paper, we propose a novel diffusion-based multi-condition
controllable framework for video head swapping, which seamlessly transplant a
human head from a static image into a dynamic video, while preserving the
original body and background of target video, and further allowing to tweak
head expressions and movements during swapping as needed. Existing
face-swapping methods mainly focus on localized facial replacement neglecting
holistic head morphology, while head-swapping approaches struggling with
hairstyle diversity and complex backgrounds, and none of these methods allow
users to modify the transplanted head expressions after swapping. To tackle
these challenges, our method incorporates several innovative strategies through
a unified latent diffusion paradigm. 1) Identity-preserving context fusion: We
propose a shape-agnostic mask strategy to explicitly disentangle foreground
head identity features from background/body contexts, combining hair
enhancement strategy to achieve robust holistic head identity preservation
across diverse hair types and complex backgrounds. 2) Expression-aware landmark
retargeting and editing: We propose a disentangled 3DMM-driven retargeting
module that decouples identity, expression, and head poses, minimizing the
impact of original expressions in input images and supporting expression
editing. While a scale-aware retargeting strategy is further employed to
minimize cross-identity expression distortion for higher transfer precision.
Experimental results demonstrate that our method excels in seamless background
integration while preserving the identity of the source portrait, as well as
showcasing superior expression transfer capabilities applicable to both real
and virtual characters.

</details>


### [217] [ParkFormer: A Transformer-Based Parking Policy with Goal Embedding and Pedestrian-Aware Control](https://arxiv.org/abs/2506.16856)
*Jun Fu,Bin Tian,Haonan Chen,Shi Meng,Tingting Yao*

Main category: cs.CV

TL;DR: This paper introduces a Transformer-based end-to-end autonomous parking system inspired by human intuitive driving, achieving high precision and safety metrics in simulations.


<details>
  <summary>Details</summary>
Motivation: Current autonomous parking systems struggle with adaptability and accuracy in dynamic urban environments. Human drivers intuitively adapt without explicit modeling, providing a natural inspiration.

Method: A Transformer-based framework processes input from surround-view camera images, goal-point representations, ego vehicle motion, and pedestrian trajectories. Key components include a cross-attention module for BEV-target point integration and a GRU-based pedestrian predictor.

Result: In the CARLA simulator, the system achieved a 96.57% parking success rate and positional/orientation errors as low as 0.21m and 0.41°, respectively, with ablation studies supporting its module effectiveness.

Conclusion: The proposed method demonstrates significant promise for practical autonomous parking, combining high precision with adaptability. It integrates safety measures effectively, outperforming conventional approaches.

Abstract: Autonomous parking plays a vital role in intelligent vehicle systems,
particularly in constrained urban environments where high-precision control is
required. While traditional rule-based parking systems struggle with
environmental uncertainties and lack adaptability in crowded or dynamic scenes,
human drivers demonstrate the ability to park intuitively without explicit
modeling. Inspired by this observation, we propose a Transformer-based
end-to-end framework for autonomous parking that learns from expert
demonstrations. The network takes as input surround-view camera images,
goal-point representations, ego vehicle motion, and pedestrian trajectories. It
outputs discrete control sequences including throttle, braking, steering, and
gear selection. A novel cross-attention module integrates BEV features with
target points, and a GRU-based pedestrian predictor enhances safety by modeling
dynamic obstacles. We validate our method on the CARLA 0.9.14 simulator in both
vertical and parallel parking scenarios. Experiments show our model achieves a
high success rate of 96.57\%, with average positional and orientation errors of
0.21 meters and 0.41 degrees, respectively. The ablation studies further
demonstrate the effectiveness of key modules such as pedestrian prediction and
goal-point attention fusion. The code and dataset will be released at:
https://github.com/little-snail-f/ParkFormer.

</details>


### [218] [With Limited Data for Multimodal Alignment, Let the STRUCTURE Guide You](https://arxiv.org/abs/2506.16895)
*Fabian Gröger,Shuo Wen,Huyen Le,Maria Brbić*

Main category: cs.CV

TL;DR: The paper introduces STRUCTURE, a method for aligning unimodal foundation models to create multimodal models with limited paired data, achieving strong results with only 1% of the typical data.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal models rely on large paired datasets, which are expensive and infeasible to obtain in many domains. The motivation is to explore how to develop multimodal models efficiently with limited paired data.

Method: The authors use a regularization technique called STRUCTURE to preserve the neighborhood geometry of unimodal encoders. They refine the alignment by targeting layers with the highest representational similarity across modalities, rather than just aligning final layers.

Result: The proposed approach achieves a 51.6% improvement in zero-shot image classification and a 91.8% improvement in cross-modal retrieval tasks, across 24 benchmarks. This is achieved using less than 1% of the standard paired data requirements.

Conclusion: The framework demonstrates that high-quality multimodal alignment is feasible with limited data, offering a solution for resource-constrained domains and significantly advancing multimodal learning methodologies.

Abstract: Multimodal models have demonstrated powerful capabilities in complex tasks
requiring multimodal alignment including zero-shot classification and
cross-modal retrieval. However, existing models typically rely on millions of
paired multimodal samples, which are prohibitively expensive or infeasible to
obtain in many domains. In this work, we explore the feasibility of building
multimodal models with limited amount of paired data by aligning pretrained
unimodal foundation models. We show that high-quality alignment is possible
with as few as tens of thousands of paired samples$\unicode{x2013}$less than
$1\%$ of the data typically used in the field. To achieve this, we introduce
STRUCTURE, an effective regularization technique that preserves the
neighborhood geometry of the latent space of unimodal encoders. Additionally,
we show that aligning last layers is often suboptimal and demonstrate the
benefits of aligning the layers with the highest representational similarity
across modalities. These two components can be readily incorporated into
existing alignment methods, yielding substantial gains across 24 zero-shot
image classification and retrieval benchmarks, with average relative
improvement of $51.6\%$ in classification and $91.8\%$ in retrieval tasks. Our
results highlight the effectiveness and broad applicability of our framework
for limited-sample multimodal learning and offer a promising path forward for
resource-constrained domains.

</details>


### [219] [LunarLoc: Segment-Based Global Localization on the Moon](https://arxiv.org/abs/2506.16940)
*Annika Thomas,Robaire Galliath,Aleksander Garbuz,Luke Anger,Cormac O'Neill,Trevor Johst,Dami Thomas,George Lordos,Jonathan P. How*

Main category: cs.CV

TL;DR: This paper introduces LunarLoc, a method for accurate global localization on the Moon using instance segmentation of boulders from stereo imagery, achieving sub-cm accuracy.


<details>
  <summary>Details</summary>
Motivation: The need for autonomous lunar surface operations arises as traditional navigation systems like GPS are unavailable, requiring precise localization for tasks like excavation and robotic exploration.

Method: The method involves extracting boulder landmarks using instance segmentation, constructing a graph-based terrain representation, and aligning it with a reference map using graph-theoretic data association.

Result: LunarLoc demonstrates sub-cm accuracy in multi-session lunar global localization and outperforms the current state-of-the-art.

Conclusion: LunarLoc provides a robust and precise localization framework for lunar missions, aiding autonomous operations and reducing odometry drift.

Abstract: Global localization is necessary for autonomous operations on the lunar
surface where traditional Earth-based navigation infrastructure, such as GPS,
is unavailable. As NASA advances toward sustained lunar presence under the
Artemis program, autonomous operations will be an essential component of tasks
such as robotic exploration and infrastructure deployment. Tasks such as
excavation and transport of regolith require precise pose estimation, but
proposed approaches such as visual-inertial odometry (VIO) accumulate odometry
drift over long traverses. Precise pose estimation is particularly important
for upcoming missions such as the ISRU Pilot Excavator (IPEx) that rely on
autonomous agents to operate over extended timescales and varied terrain. To
help overcome odometry drift over long traverses, we propose LunarLoc, an
approach to global localization that leverages instance segmentation for
zero-shot extraction of boulder landmarks from onboard stereo imagery. Segment
detections are used to construct a graph-based representation of the terrain,
which is then aligned with a reference map of the environment captured during a
previous session using graph-theoretic data association. This method enables
accurate and drift-free global localization in visually ambiguous settings.
LunarLoc achieves sub-cm level accuracy in multi-session global localization
experiments, significantly outperforming the state of the art in lunar global
localization. To encourage the development of further methods for global
localization on the Moon, we release our datasets publicly with a playback
module: https://github.com/mit-acl/lunarloc-data.

</details>


### [220] [LAION-C: An Out-of-Distribution Benchmark for Web-Scale Vision Models](https://arxiv.org/abs/2506.16950)
*Fanfei Li,Thomas Klein,Wieland Brendel,Robert Geirhos,Roland S. Zimmermann*

Main category: cs.CV

TL;DR: This paper addresses the limitations of traditional OOD benchmarks like ImageNet-C, introducing LAION-C with six novel distortion types tailored for assessing the robustness of models trained on web-scale datasets.


<details>
  <summary>Details</summary>
Motivation: Past OOD benchmarks are no longer representative for evaluating robustness, as they fail to account for the corruption types commonly seen in web-scale datasets.

Method: LAION-C was developed with six carefully designed distortion types that are explicitly out-of-distribution even for large datasets like LAION, along with psychophysical testing to compare model performance against human observers.

Result: State-of-the-art models struggled with LAION-C, revealing that it is a robust benchmark. Humans also participated in tests, showing that the leading models are now on par or superior to human observers in OOD tasks.

Conclusion: LAION-C establishes itself as a significant challenge for OOD generalization, marking a shift where top-performing models have surpassed human robustness in certain scenarios.

Abstract: Out-of-distribution (OOD) robustness is a desired property of computer vision
models. Improving model robustness requires high-quality signals from
robustness benchmarks to quantify progress. While various benchmark datasets
such as ImageNet-C were proposed in the ImageNet era, most ImageNet-C
corruption types are no longer OOD relative to today's large, web-scraped
datasets, which already contain common corruptions such as blur or JPEG
compression artifacts. Consequently, these benchmarks are no longer well-suited
for evaluating OOD robustness in the era of web-scale datasets. Indeed, recent
models show saturating scores on ImageNet-era OOD benchmarks, indicating that
it is unclear whether models trained on web-scale datasets truly become better
at OOD generalization or whether they have simply been exposed to the test
distortions during training. To address this, we introduce LAION-C as a
benchmark alternative for ImageNet-C. LAION-C consists of six novel distortion
types specifically designed to be OOD, even for web-scale datasets such as
LAION. In a comprehensive evaluation of state-of-the-art models, we find that
the LAION-C dataset poses significant challenges to contemporary models,
including MLLMs such as Gemini and GPT-4o. We additionally conducted a
psychophysical experiment to evaluate the difficulty of our corruptions for
human observers, enabling a comparison of models to lab-quality human
robustness data. We observe a paradigm shift in OOD generalization: from humans
outperforming models, to the best models now matching or outperforming the best
human observers.

</details>


### [221] [Visual-Instructed Degradation Diffusion for All-in-One Image Restoration](https://arxiv.org/abs/2506.16960)
*Wenyang Luo,Haina Qin,Zewen Chen,Libin Wang,Dandan Zheng,Yuming Li,Yufan Liu,Bing Li,Weiming Hu*

Main category: cs.CV

TL;DR: Defusion is a new framework aimed at solving image restoration tasks by using visual instruction-guided degradation diffusion, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods for image restoration require separate models for different types of degradations, limiting their use in real-world cases with mixed or unknown degradation types.

Method: Defusion employs visual instructions derived by applying degradations to standardized elements. These instructions guide a diffusion-based model to operate in degradation space, enabling high-quality image reconstruction.

Result: Defusion demonstrates superior performance across various image restoration tasks, including complex and real-world degradations.

Conclusion: Defusion offers a unified solution for diverse image restoration problems, providing better generalization and stability compared to traditional methods.

Abstract: Image restoration tasks like deblurring, denoising, and dehazing usually need
distinct models for each degradation type, restricting their generalization in
real-world scenarios with mixed or unknown degradations. In this work, we
propose \textbf{Defusion}, a novel all-in-one image restoration framework that
utilizes visual instruction-guided degradation diffusion. Unlike existing
methods that rely on task-specific models or ambiguous text-based priors,
Defusion constructs explicit \textbf{visual instructions} that align with the
visual degradation patterns. These instructions are grounded by applying
degradations to standardized visual elements, capturing intrinsic degradation
features while agnostic to image semantics. Defusion then uses these visual
instructions to guide a diffusion-based model that operates directly in the
degradation space, where it reconstructs high-quality images by denoising the
degradation effects with enhanced stability and generalizability. Comprehensive
experiments demonstrate that Defusion outperforms state-of-the-art methods
across diverse image restoration tasks, including complex and real-world
degradations.

</details>


### [222] [Reversing Flow for Image Restoration](https://arxiv.org/abs/2506.16961)
*Haina Qin,Wenyang Luo,Libin Wang,Dandan Zheng,Jingdong Chen,Ming Yang,Bing Li,Weiming Hu*

Main category: cs.CV

TL;DR: ResFlow is an innovative framework for efficient and high-performing image restoration, leveraging deterministic modeling of degradation processes.


<details>
  <summary>Details</summary>
Motivation: Existing generative models for image restoration are inefficient and complex because they treat the degradation process as stochastic.

Method: ResFlow employs deterministic paths using continuous normalizing flows, together with entropy-preserving flow paths and augmented degradation flows to resolve prediction uncertainty.

Result: ResFlow achieves state-of-the-art image restoration results across benchmarks and speeds up the process, reducing sampling steps to fewer than four.

Conclusion: ResFlow is a practical, efficient solution that advances the field of image restoration by improving both performance and speed through innovative methodology.

Abstract: Image restoration aims to recover high-quality (HQ) images from degraded
low-quality (LQ) ones by reversing the effects of degradation. Existing
generative models for image restoration, including diffusion and score-based
models, often treat the degradation process as a stochastic transformation,
which introduces inefficiency and complexity. In this work, we propose ResFlow,
a novel image restoration framework that models the degradation process as a
deterministic path using continuous normalizing flows. ResFlow augments the
degradation process with an auxiliary process that disambiguates the
uncertainty in HQ prediction to enable reversible modeling of the degradation
process. ResFlow adopts entropy-preserving flow paths and learns the augmented
degradation flow by matching the velocity field. ResFlow significantly improves
the performance and speed of image restoration, completing the task in fewer
than four sampling steps. Extensive experiments demonstrate that ResFlow
achieves state-of-the-art results across various image restoration benchmarks,
offering a practical and efficient solution for real-world applications.

</details>


### [223] [Enhancing Step-by-Step and Verifiable Medical Reasoning in MLLMs](https://arxiv.org/abs/2506.16962)
*Haoran Sun,Yankai Jiang,Wenjie Lou,Yujie Zhang,Wenjie Li,Lilong Wang,Mianxin Liu,Lei Liu,Xiaosong Wang*

Main category: cs.CV

TL;DR: This paper introduces MICS, a novel method to generate high-quality medical chain-of-thought (CoT) data to enhance reasoning in multimodal large language models (MLLMs), and presents Chiron-o1, a new medical MLLM achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the need for reliable chain-of-thought reasoning data designed specifically for medical domains where such capabilities are underdeveloped in existing MLLMs.

Method: The authors propose a Mentor-Intern Collaborative Search (MICS) framework, which involves mentor models for initial reasoning, intern models for extending reasoning paths, and an MICS-Score to evaluate and optimize reasoning paths.

Result: They build the MMRP dataset and train Chiron-o1, which demonstrates superior performance in medical visual question answering and reasoning benchmarks.

Conclusion: Chiron-o1, trained with the CoT data generated by MICS, is effective at both visual question answering and reasoning, setting a new benchmark for medical MLLMs.

Abstract: Multimodal large language models (MLLMs) have begun to demonstrate robust
reasoning capabilities on general tasks, yet their application in the medical
domain remains in its early stages. Constructing chain-of-thought (CoT)
training data is essential for bolstering the reasoning abilities of medical
MLLMs. However, existing approaches exhibit a deficiency in offering a
comprehensive framework for searching and evaluating effective reasoning paths
towards critical diagnosis. To address this challenge, we propose Mentor-Intern
Collaborative Search (MICS), a novel reasoning-path searching scheme to
generate rigorous and effective medical CoT data. MICS first leverages mentor
models to initialize the reasoning, one step at a time, then prompts each
intern model to continue the thinking along those initiated paths, and finally
selects the optimal reasoning path according to the overall reasoning
performance of multiple intern models. The reasoning performance is determined
by an MICS-Score, which assesses the quality of generated reasoning paths.
Eventually, we construct MMRP, a multi-task medical reasoning dataset with
ranked difficulty, and Chiron-o1, a new medical MLLM devised via a curriculum
learning strategy, with robust visual question-answering and generalizable
reasoning capabilities. Extensive experiments demonstrate that Chiron-o1,
trained on our CoT dataset constructed using MICS, achieves state-of-the-art
performance across a list of medical visual question answering and reasoning
benchmarks. Codes are available at GitHub - manglu097/Chiron-o1: Enhancing
Step-by-Step and Verifiable Medical Reasoning in MLLMs

</details>


### [224] [ForestFormer3D: A Unified Framework for End-to-End Segmentation of Forest LiDAR 3D Point Clouds](https://arxiv.org/abs/2506.16991)
*Binbin Xiang,Maciej Wielgosz,Stefano Puliti,Kamil Král,Martin Krůček,Azim Missarov,Rasmus Astrup*

Main category: cs.CV

TL;DR: ForestFormer3D is a new framework for 3D LiDAR forest point cloud segmentation, featuring state-of-the-art performance and strong generalization.


<details>
  <summary>Details</summary>
Motivation: Current challenges in forest LiDAR segmentation arise from the complexity and variability of natural forest environments; this study aims to address these limitations.

Method: The proposed ForestFormer3D framework integrates ISA-guided query point selection, a score-based block merging strategy during inference, and a one-to-many association mechanism for training.

Result: ForestFormer3D achieves state-of-the-art individual tree segmentation performance on the FOR-instanceV2 dataset and demonstrates robustness across diverse forests and sensor types.

Conclusion: ForestFormer3D provides a unified and robust solution for forest LiDAR segmentation, with plans to release the dataset and code for broader use.

Abstract: The segmentation of forest LiDAR 3D point clouds, including both individual
tree and semantic segmentation, is fundamental for advancing forest management
and ecological research. However, current approaches often struggle with the
complexity and variability of natural forest environments. We present
ForestFormer3D, a new unified and end-to-end framework designed for precise
individual tree and semantic segmentation. ForestFormer3D incorporates
ISA-guided query point selection, a score-based block merging strategy during
inference, and a one-to-many association mechanism for effective training. By
combining these new components, our model achieves state-of-the-art performance
for individual tree segmentation on the newly introduced FOR-instanceV2
dataset, which spans diverse forest types and regions. Additionally,
ForestFormer3D generalizes well to unseen test sets (Wytham woods and LAUTx),
showcasing its robustness across different forest conditions and sensor
modalities. The FOR-instanceV2 dataset and the ForestFormer3D code will be
released soon.

</details>


### [225] [Prmpt2Adpt: Prompt-Based Zero-Shot Domain Adaptation for Resource-Constrained Environments](https://arxiv.org/abs/2506.16994)
*Yasir Ali Farrukh,Syed Wali,Irfan Khan,Nathaniel D. Bastian*

Main category: cs.CV

TL;DR: This paper introduces Prmpt2Adpt, a lightweight zero-shot domain adaptation framework that achieves competitive detection results with faster adaptation and inference speeds using minimal resources.


<details>
  <summary>Details</summary>
Motivation: The paper addresses challenges in unsupervised domain adaptation, particularly in resource-constrained environments such as drones, where memory and computation are limited.

Method: The framework employs a teacher-student paradigm using a distilled and fine-tuned CLIP model as the backbone. It aligns source features to target domain semantics via Prompt-driven Instance Normalization, fine-tunes the detection head of the teacher model, and uses pseudo-labels to adapt a compact student model.

Result: Experiments on the MDS-A dataset show that Prmpt2Adpt provides competitive detection performance while achieving up to 7x faster adaptation and 5x faster inference speeds compared to state-of-the-art methods.

Conclusion: Prmpt2Adpt is an efficient and scalable zero-shot domain adaptation solution for real-time applications in low-resource settings.

Abstract: Unsupervised Domain Adaptation (UDA) is a critical challenge in real-world
vision systems, especially in resource-constrained environments like drones,
where memory and computation are limited. Existing prompt-driven UDA methods
typically rely on large vision-language models and require full access to
source-domain data during adaptation, limiting their applicability. In this
work, we propose Prmpt2Adpt, a lightweight and efficient zero-shot domain
adaptation framework built around a teacher-student paradigm guided by
prompt-based feature alignment. At the core of our method is a distilled and
fine-tuned CLIP model, used as the frozen backbone of a Faster R-CNN teacher. A
small set of low-level source features is aligned to the target domain
semantics-specified only through a natural language prompt-via Prompt-driven
Instance Normalization (PIN). These semantically steered features are used to
briefly fine-tune the detection head of the teacher model. The adapted teacher
then generates high-quality pseudo-labels, which guide the on-the-fly
adaptation of a compact student model. Experiments on the MDS-A dataset
demonstrate that Prmpt2Adpt achieves competitive detection performance compared
to state-of-the-art methods, while delivering up to 7x faster adaptation and 5x
faster inference speed using few source images-making it a practical and
scalable solution for real-time adaptation in low-resource domains.

</details>


### [226] [A Synthetic Benchmark for Collaborative 3D Semantic Occupancy Prediction in V2X Autonomous Driving](https://arxiv.org/abs/2506.17004)
*Hanlin Wu,Pengfei Lin,Ehsan Javanmardi,Naren Bao,Bo Qian,Hao Si,Manabu Tsukada*

Main category: cs.CV

TL;DR: The paper enhances 3D semantic occupancy prediction for autonomous driving using collaborative perception. It introduces a dataset and benchmarks to evaluate range impact and develops a strong baseline model outperforming single-agent versions.


<details>
  <summary>Details</summary>
Motivation: To improve autonomous driving perception by overcoming limits of single-vehicle constraints like occlusion, limited sensor range, and narrow viewpoints.

Method: Augment an existing dataset replayed in CARLA simulation, create benchmarks for assessment, and develop a baseline model with spatial alignment and attention aggregation for collaboration.

Result: Experimental results demonstrate the baseline model's superior performance compared to single-agent models, with larger benefits as prediction range expands.

Conclusion: Collaborative perception significantly enhances 3D semantic occupancy prediction in autonomous driving, especially for extended prediction ranges, paving the way for improved multi-agent perception systems.

Abstract: 3D semantic occupancy prediction is an emerging perception paradigm in
autonomous driving, providing a voxel-level representation of both geometric
details and semantic categories. However, the perception capability of a single
vehicle is inherently constrained by occlusion, restricted sensor range, and
narrow viewpoints. To address these limitations, collaborative perception
enables the exchange of complementary information, thereby enhancing the
completeness and accuracy. In the absence of a dedicated dataset for
collaborative 3D semantic occupancy prediction, we augment an existing
collaborative perception dataset by replaying it in CARLA with a
high-resolution semantic voxel sensor to provide dense and comprehensive
occupancy annotations. In addition, we establish benchmarks with varying
prediction ranges designed to systematically assess the impact of spatial
extent on collaborative prediction. We further develop a baseline model that
performs inter-agent feature fusion via spatial alignment and attention
aggregation. Experimental results demonstrate that our baseline model
consistently outperforms single-agent models, with increasing gains observed as
the prediction range expands.

</details>


### [227] [Unsupervised Image Super-Resolution Reconstruction Based on Real-World Degradation Patterns](https://arxiv.org/abs/2506.17027)
*Yiyang Tie,Hong Zhu,Yunyun Luo,Jing Shi*

Main category: cs.CV

TL;DR: The paper presents a TripleGAN framework to effectively model real-world degradation patterns in low-resolution images for super-resolution reconstruction by leveraging three GANs and addressing existing challenges such as blur and noise diversity.


<details>
  <summary>Details</summary>
Motivation: Current methods struggle to extract and model real-world degradation patterns in low-resolution images due to challenges like blur diversity, noise characteristics, and implicit degradations such as color shifts.

Method: The authors designed a TripleGAN framework consisting of three GANs: FirstGAN reduces domain gap in blur characteristics, SecondGAN performs domain translation for target blur properties, and ThirdGAN reconstructs real-world low-resolution images using data from the first two components.

Result: Extensive experiments on RealSR and DRealSR datasets demonstrate that the proposed framework significantly improves performance, measured by quantitative metrics, and generates sharper reconstructions without over-smoothing artifacts.

Conclusion: The TripleGAN framework effectively learns and models real-world degradation patterns, enabling better high-resolution reconstruction, and aligns datasets with real-world degradation properties for improved SR image quality.

Abstract: The training of real-world super-resolution reconstruction models heavily
relies on datasets that reflect real-world degradation patterns. Extracting and
modeling degradation patterns for super-resolution reconstruction using only
real-world low-resolution (LR) images remains a challenging task. When
synthesizing datasets to simulate real-world degradation, relying solely on
degradation extraction methods fails to capture both blur and diverse noise
characteristics across varying LR distributions, as well as more implicit
degradations such as color gamut shifts. Conversely, domain translation alone
cannot accurately approximate real-world blur characteristics due to the
significant degradation domain gap between synthetic and real data. To address
these challenges, we propose a novel TripleGAN framework comprising two
strategically designed components: The FirstGAN primarily focuses on narrowing
the domain gap in blur characteristics, while the SecondGAN performs
domain-specific translation to approximate target-domain blur properties and
learn additional degradation patterns. The ThirdGAN is trained on pseudo-real
data generated by the FirstGAN and SecondGAN to reconstruct real-world LR
images. Extensive experiments on the RealSR and DRealSR datasets demonstrate
that our method exhibits clear advantages in quantitative metrics while
maintaining sharp reconstructions without over-smoothing artifacts. The
proposed framework effectively learns real-world degradation patterns from LR
observations and synthesizes aligned datasets with corresponding degradation
characteristics, thereby enabling the trained network to achieve superior
performance in reconstructing high-quality SR images from real-world LR inputs.

</details>


### [228] [Relaxed syntax modeling in Transformers for future-proof license plate recognition](https://arxiv.org/abs/2506.17051)
*Florent Meyer,Laurent Guichard,Denis Coquenet,Guillaume Gravier,Yann Soullard,Bertrand Coüasnon*

Main category: cs.CV

TL;DR: License plate recognition systems based on Transformer networks face challenges in adapting to new syntax introduced by future license plates. SaLT, a Syntax-Less Transformer, was developed to address this issue and maintain accuracy over time.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of Transformer networks in adapting to unseen syntax changes in license plates, as their performance significantly drops on future plates.

Method: The study identified issues in positional and contextual information flow in Transformer encoder-decoders and introduced architectural modifications to develop SaLT for syntax-agnostic modeling.

Result: SaLT achieved top accuracy on previously seen syntax and maintained consistency on future license plates in experiments with real and synthetic datasets.

Conclusion: Enhancements to Transformer architectures can improve their robustness for evolving tasks like license plate recognition, keeping performance steady as syntax evolves.

Abstract: Effective license plate recognition systems are required to be resilient to
constant change, as new license plates are released into traffic daily. While
Transformer-based networks excel in their recognition at first sight, we
observe significant performance drop over time which proves them unsuitable for
tense production environments. Indeed, such systems obtain state-of-the-art
results on plates whose syntax is seen during training. Yet, we show they
perform similarly to random guessing on future plates where legible characters
are wrongly recognized due to a shift in their syntax. After highlighting the
flows of positional and contextual information in Transformer encoder-decoders,
we identify several causes for their over-reliance on past syntax. Following,
we devise architectural cut-offs and replacements which we integrate into SaLT,
an attempt at a Syntax-Less Transformer for syntax-agnostic modeling of license
plate representations. Experiments on both real and synthetic datasets show
that our approach reaches top accuracy on past syntax and most importantly
nearly maintains performance on future license plates. We further demonstrate
the robustness of our architecture enhancements by way of various ablations.

</details>


### [229] [Assembler: Scalable 3D Part Assembly via Anchor Point Diffusion](https://arxiv.org/abs/2506.17074)
*Wang Zhao,Yan-Pei Cao,Jiale Xu,Yuejiang Dong,Ying Shan*

Main category: cs.CV

TL;DR: Assembler is a novel framework for 3D part assembly that reconstructs objects from part meshes and reference images using diffusion models, anchor point clouds, and a large-scale dataset.


<details>
  <summary>Details</summary>
Motivation: To address limitations in existing 3D part assembly approaches, which rely on fixed part pose predictions and limited category-specific training, and better handle diverse real-world objects.

Method: The framework uses diffusion models for generative part configuration sampling, introduces a sparse anchor point cloud representation for scalable object assembly, and utilizes a large-scale dataset derived from 3D shape repositories.

Result: Assembler achieves state-of-the-art performance on PartNet and successfully assembles complex real-world 3D objects.

Conclusion: Assembler represents a scalable, generalizable solution for 3D part assembly, showing promise in tasks like part-aware 3D modeling and interactive design.

Abstract: We present Assembler, a scalable and generalizable framework for 3D part
assembly that reconstructs complete objects from input part meshes and a
reference image. Unlike prior approaches that mostly rely on deterministic part
pose prediction and category-specific training, Assembler is designed to handle
diverse, in-the-wild objects with varying part counts, geometries, and
structures. It addresses the core challenges of scaling to general 3D part
assembly through innovations in task formulation, representation, and data.
First, Assembler casts part assembly as a generative problem and employs
diffusion models to sample plausible configurations, effectively capturing
ambiguities arising from symmetry, repeated parts, and multiple valid
assemblies. Second, we introduce a novel shape-centric representation based on
sparse anchor point clouds, enabling scalable generation in Euclidean space
rather than SE(3) pose prediction. Third, we construct a large-scale dataset of
over 320K diverse part-object assemblies using a synthesis and filtering
pipeline built on existing 3D shape repositories. Assembler achieves
state-of-the-art performance on PartNet and is the first to demonstrate
high-quality assembly for complex, real-world objects. Based on Assembler, we
further introduce an interesting part-aware 3D modeling system that generates
high-resolution, editable objects from images, demonstrating potential for
interactive and compositional design. Project page:
https://assembler3d.github.io

</details>


### [230] [Acquiring and Accumulating Knowledge from Diverse Datasets for Multi-label Driving Scene Classification](https://arxiv.org/abs/2506.17101)
*Ke Li,Chenyu Zhang,Yuxin Ding,Xianbiao Hu,Ruwen Qin*

Main category: cs.CV

TL;DR: This paper addresses driving scene identification for autonomous vehicles using a new method combining knowledge acquisition and active learning.


<details>
  <summary>Details</summary>
Motivation: The authors aim to improve multi-label classification for driving scenes by addressing two challenges: the lack of balanced multi-label datasets and the difficulty in balancing learning across tasks.

Method: They propose a system combining Knowledge Acquisition and Accumulation (KAA) with Consistency-based Active Learning (CAL). KAA learns from single-label datasets via monotask learning, while CAL addresses knowledge gaps by aligning individual and joint attribute distributions.

Result: Their approach, KAA-CAL, improves performance by 56.1% over a baseline model and surpasses state-of-the-art models on public benchmarks, using 85% less data.

Conclusion: The KAA-CAL system is both data-efficient and highly effective for driving scene identification, with released datasets and code contributing to further research.

Abstract: Driving scene identification, which assigns multiple non-exclusive class
labels to a scene, provides the contextual awareness necessary for enhancing
autonomous vehicles' ability to understand, reason about, and interact with the
complex driving environment. As a multi-label classification problem, it is
better tackled via multitasking learning. However, directly training a
multi-label classification model for driving scene identification through
multitask learning presents two main challenges: acquiring a balanced,
comprehensively annotated multi-label dataset and balancing learning across
different tasks. This paper introduces a novel learning system that synergizes
knowledge acquisition and accumulation (KAA) with consistency-based active
learning (CAL) to address those challenges. KAA acquires and accumulates
knowledge about scene identification from various single-label datasets via
monotask learning. Subsequently, CAL effectively resolves the knowledge gap
caused by the discrepancy between the marginal distributions of individual
attributes and their joint distribution. An ablation study on our Driving Scene
Identification (DSI) dataset demonstrates a 56.1% performance increase over the
baseline model pretrained on ImageNet. Of this, KAA accounts for 31.3% of the
gain, and CAL contributes 24.8%. Moreover, KAA-CAL stands out as the best
performer when compared to state-of-the-art (SOTA) multi-label models on two
public datasets, BDD100K and HSD, achieving this while using 85% less data. The
DSI dataset and the implementation code for KAA-CAL are available at
https://github.com/KELISBU/KAA-CAL .

</details>


### [231] [MEXA: Towards General Multimodal Reasoning with Dynamic Multi-Expert Aggregation](https://arxiv.org/abs/2506.17113)
*Shoubin Yu,Yue Zhang,Ziyang Wang,Jaehong Yoon,Mohit Bansal*

Main category: cs.CV

TL;DR: MEXA is a training-free framework that combines specialized expert models for scalable multimodal reasoning across diverse domains.


<details>
  <summary>Details</summary>
Motivation: The increasing diversity of input modalities and complex tasks like medical diagnosis and financial forecasting require effective frameworks for multimodal reasoning.

Method: MEXA dynamically selects expert models based on input modality and task-specific skills. Outputs are aggregated using a Large Reasoning Model (LRM).

Result: MEXA achieves consistent performance improvement on multimodal benchmarks such as Video Reasoning, Audio Reasoning, 3D Understanding, and Medical QA.

Conclusion: MEXA provides flexible, transparent, and effective multimodal reasoning without additional training, showcasing its applicability across various domains.

Abstract: Combining pre-trained expert models offers substantial potential for scalable
multimodal reasoning, but building a unified framework remains challenging due
to the increasing diversity of input modalities and task complexity. For
instance, medical diagnosis requires precise reasoning over structured clinical
tables, while financial forecasting depends on interpreting plot-based data to
make informed predictions. To tackle this challenge, we introduce MEXA, a
training-free framework that performs modality- and task-aware aggregation of
multiple expert models to enable effective multimodal reasoning across diverse
and distinct domains. MEXA dynamically selects expert models based on the input
modality and the task-specific reasoning demands (i.e., skills). Each expert
model, specialized in a modality task pair, generates interpretable textual
reasoning outputs. MEXA then aggregates and reasons over these outputs using a
Large Reasoning Model (LRM) to produce the final answer. This modular design
allows flexible and transparent multimodal reasoning across diverse domains
without additional training overhead. We extensively evaluate our approach on
diverse multimodal benchmarks, including Video Reasoning, Audio Reasoning, 3D
Understanding, and Medical QA. MEXA consistently delivers performance
improvements over strong multimodal baselines, highlighting the effectiveness
and broad applicability of our expert-driven selection and aggregation in
diverse multimodal reasoning tasks.

</details>


### [232] [Dynamic Watermark Generation for Digital Images using Perimeter Gated SPAD Imager PUFs](https://arxiv.org/abs/2506.17134)
*Md Sakibur Sajal,Marc Dandin*

Main category: cs.CV

TL;DR: The paper introduces a watermarking technique using perimeter gated SPAD imagers, leveraging dark signal non-uniformity (DSNU) for security purposes.


<details>
  <summary>Details</summary>
Motivation: To explore the potential of using SPAD imagers for digital watermarking, expanding the current focus beyond CMOS image sensors and active pixel sensors.

Method: Three 64 x 64 pgSPAD imager chips using 0.35 µm CMOS process were studied, utilizing their DSNU to create source-scene-specific watermarks for analysis.

Result: The proposed watermarking technique can achieve source identification and tamper detection, with adjustable sensitivity and robustness.

Conclusion: Dynamic watermarks derived from SPAD imagers' DSNU are viable for security purposes, offering a balance of sensitivity and robustness.

Abstract: Digital image watermarks as a security feature can be derived from the
imager's physically unclonable functions (PUFs) by utilizing the manufacturing
variations, i.e., the dark signal non-uniformity (DSNU). While a few
demonstrations focused on the CMOS image sensors (CIS) and active pixel sensors
(APS), single photon avalanche diode (SPAD) imagers have never been
investigated for this purpose. In this work, we have proposed a novel
watermarking technique using perimeter gated SPAD (pgSPAD) imagers. We utilized
the DSNU of three 64 x 64 pgSPAD imager chips, fabricated in a 0.35 {\mu}m
standard CMOS process and analyzed the simulated watermarks for standard test
images from publicly available database. Our observation shows that both source
identification and tamper detection can be achieved using the proposed
source-scene-specific dynamic watermarks with a controllable
sensitivity-robustness trade-off.

</details>


### [233] [Semi-Supervised Multi-Modal Medical Image Segmentation for Complex Situations](https://arxiv.org/abs/2506.17136)
*Dongdong Meng,Sheng Li,Hao Wu,Guoping Wang,Xueqing Yan*

Main category: cs.CV

TL;DR: This paper addresses challenges in semi-supervised learning and multi-modal fusion for medical image segmentation, proposing a robust method to leverage unlabeled data effectively.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome limitations in semi-supervised medical image segmentation, especially for complex backgrounds and challenges where multi-modal data integration has not been effectively utilized.

Method: The method introduces a multi-stage multi-modal fusion strategy combined with contrastive mutual learning for prediction consistency across modalities.

Result: Experimental results on two multi-modal datasets show superior performance and robustness compared to existing approaches.

Conclusion: The proposed approach holds valuable potential for improving semi-supervised medical image segmentation, particularly in challenging scenarios with limited labeled data.

Abstract: Semi-supervised learning addresses the issue of limited annotations in
medical images effectively, but its performance is often inadequate for complex
backgrounds and challenging tasks. Multi-modal fusion methods can significantly
improve the accuracy of medical image segmentation by providing complementary
information. However, they face challenges in achieving significant
improvements under semi-supervised conditions due to the challenge of
effectively leveraging unlabeled data. There is a significant need to create an
effective and reliable multi-modal learning strategy for leveraging unlabeled
data in semi-supervised segmentation. To address these issues, we propose a
novel semi-supervised multi-modal medical image segmentation approach, which
leverages complementary multi-modal information to enhance performance with
limited labeled data. Our approach employs a multi-stage multi-modal fusion and
enhancement strategy to fully utilize complementary multi-modal information,
while reducing feature discrepancies and enhancing feature sharing and
alignment. Furthermore, we effectively introduce contrastive mutual learning to
constrain prediction consistency across modalities, thereby facilitating the
robustness of segmentation results in semi-supervised tasks. Experimental
results on two multi-modal datasets demonstrate the superior performance and
robustness of the proposed framework, establishing its valuable potential for
solving medical image segmentation tasks in complex scenarios.

</details>


### [234] [On the Theory of Conditional Feature Alignment for Unsupervised Domain-Adaptive Counting](https://arxiv.org/abs/2506.17137)
*Zhuonan Liang,Dongnan Liu,Jianan Fan,Yaxuan Song,Qiang Qu,Yu Yao,Peng Fu,Weidong Cai*

Main category: cs.CV

TL;DR: This paper proposes a theoretical framework for conditional feature alignment to improve object counting models across domains with differing density varieties, achieving superior cross-domain generalization.


<details>
  <summary>Details</summary>
Motivation: To address the challenges posed by density shifts in object counting tasks, which violate standard domain adaptation assumptions and impact model performance.

Method: The authors formalize conditional divergence by partitioning domains into subsets and measuring divergence per condition. They derive a joint error bound to show the benefit of conditional feature alignment and propose a practical unsupervised domain adaptation strategy.

Result: Extensive experiments on multiple counting datasets demonstrate that the proposed method outperforms existing unsupervised domain adaptation techniques.

Conclusion: By preserving task-relevant variations and filtering out nuisance shifts, conditional feature alignment offers theoretical and empirical advantages for cross-domain object counting.

Abstract: Object counting models suffer when deployed across domains with differing
density variety, since density shifts are inherently task-relevant and violate
standard domain adaptation assumptions. To address this, we propose a
theoretical framework of conditional feature alignment. We first formalize the
notion of conditional divergence by partitioning each domain into subsets
(e.g., object vs. background) and measuring divergences per condition. We then
derive a joint error bound showing that, under discrete label spaces treated as
condition sets, aligning distributions conditionally leads to tighter bounds on
the combined source-target decision error than unconditional alignment. These
insights motivate a general conditional adaptation principle: by preserving
task-relevant variations while filtering out nuisance shifts, one can achieve
superior cross-domain generalization for counting. We provide both defining
conditional divergence then proving its benefit in lowering joint error and a
practical adaptation strategy that preserves task-relevant information in
unsupervised domain-adaptive counting. We demonstrate the effectiveness of our
approach through extensive experiments on multiple counting datasets with
varying density distributions. The results show that our method outperforms
existing unsupervised domain adaptation methods, empirically validating the
theoretical insights on conditional feature alignment.

</details>


### [235] [Do We Need Large VLMs for Spotting Soccer Actions?](https://arxiv.org/abs/2506.17144)
*Ritabrata Chakraborty,Rajatsubhra Chakraborty,Avijit Dasgupta,Sandeep Chaurasia*

Main category: cs.CV

TL;DR: The paper introduces a lightweight and scalable text-based approach using Large Language Models (LLMs) for soccer action spotting, instead of traditional video-based methods.


<details>
  <summary>Details</summary>
Motivation: Current video-based tasks for soccer action spotting demand heavy computational resources and complex models, prompting the need for a simpler, scalable alternative.

Method: The approach utilizes timestamped commentary from the SoccerNet Echoes dataset and employs a system of three specialized LLMs to analyze text descriptions for spotting match actions like goals and substitutions.

Result: The proposed text-based method effectively detects key match events with accuracy, bypassing the complexities of video processing.

Conclusion: Switching to a language-centric method provides an efficient, training-free solution for action spotting, proving the reliability of expert commentary for event detection.

Abstract: Traditional video-based tasks like soccer action spotting rely heavily on
visual inputs, often requiring complex and computationally expensive models to
process dense video data. In this work, we propose a shift from this
video-centric approach to a text-based task, making it lightweight and scalable
by utilizing Large Language Models (LLMs) instead of Vision-Language Models
(VLMs). We posit that expert commentary, which provides rich, fine-grained
descriptions and contextual cues such as excitement and tactical insights,
contains enough information to reliably spot key actions in a match. To
demonstrate this, we use the SoccerNet Echoes dataset, which provides
timestamped commentary, and employ a system of three LLMs acting as judges
specializing in outcome, excitement, and tactics. Each LLM evaluates sliding
windows of commentary to identify actions like goals, cards, and substitutions,
generating accurate timestamps for these events. Our experiments show that this
language-centric approach performs effectively in detecting critical match
events, providing a lightweight and training-free alternative to traditional
video-based methods for action spotting.

</details>


### [236] [Co-Seg++: Mutual Prompt-Guided Collaborative Learning for Versatile Medical Segmentation](https://arxiv.org/abs/2506.17159)
*Qing Xu,Yuxiang Luo,Wenting Duan,Zhen Chen*

Main category: cs.CV

TL;DR: This paper introduces Co-Seg++, a novel framework that integrates semantic and instance segmentation tasks for improved medical image analysis performance.


<details>
  <summary>Details</summary>
Motivation: Existing segmentation methods treat semantic and instance segmentation tasks in isolation, which leads to suboptimal medical image analysis and overlooks task interdependencies.

Method: The Co-Seg++ framework includes a spatio-temporal prompt encoder (STP-Encoder) to manage spatial and temporal relationships along with a multi-task collaborative decoder (MTC-Decoder) for joint computation of segmentation masks.

Result: The Co-Seg++ framework achieves state-of-the-art performance in semantic, instance, and panoptic segmentation tasks across diverse medical datasets.

Conclusion: The integration of mutual task enhancement significantly improves segmentation quality and understanding of medical images, demonstrating potential for broader medical imaging applications.

Abstract: Medical image analysis is critical yet challenged by the need of jointly
segmenting organs or tissues, and numerous instances for anatomical structures
and tumor microenvironment analysis. Existing studies typically formulated
different segmentation tasks in isolation, which overlooks the fundamental
interdependencies between these tasks, leading to suboptimal segmentation
performance and insufficient medical image understanding. To address this
issue, we propose a Co-Seg++ framework for versatile medical segmentation.
Specifically, we introduce a novel co-segmentation paradigm, allowing semantic
and instance segmentation tasks to mutually enhance each other. We first devise
a spatio-temporal prompt encoder (STP-Encoder) to capture long-range spatial
and temporal relationships between segmentation regions and image embeddings as
prior spatial constraints. Moreover, we devise a multi-task collaborative
decoder (MTC-Decoder) that leverages cross-guidance to strengthen the
contextual consistency of both tasks, jointly computing semantic and instance
segmentation masks. Extensive experiments on diverse CT and histopathology
datasets demonstrate that the proposed Co-Seg++ outperforms state-of-the-arts
in the semantic, instance, and panoptic segmentation of dental anatomical
structures, histopathology tissues, and nuclei instances. The source code is
available at https://github.com/xq141839/Co-Seg-Plus.

</details>


### [237] [YASMOT: Yet another stereo image multi-object tracker](https://arxiv.org/abs/2506.17186)
*Ketil Malde*

Main category: cs.CV

TL;DR: The paper introduces 'yasmot,' a lightweight and flexible object tracker for enhancing object detection and tracking over time in video or image sequences.


<details>
  <summary>Details</summary>
Motivation: Improve object detection performance in time-series imagery and enable downstream tasks like behavior prediction and abundance estimation.

Method: Developed 'yasmot,' a tool that processes outputs from object detectors and tracks objects over time in both monoscopic and stereoscopic setups. Also includes ensemble detection functionality.

Result: 'Yasmot' can successfully track objects over time and generate consensus detections from multiple object detectors.

Conclusion: The tool adds value by enhancing detection and tracking capabilities for various applications in time-series imaging.

Abstract: There now exists many popular object detectors based on deep learning that
can analyze images and extract locations and class labels for occurrences of
objects. For image time series (i.e., video or sequences of stills), tracking
objects over time and preserving object identity can help to improve object
detection performance, and is necessary for many downstream tasks, including
classifying and predicting behaviors, and estimating total abundances. Here we
present yasmot, a lightweight and flexible object tracker that can process the
output from popular object detectors and track objects over time from either
monoscopic or stereoscopic camera configurations. In addition, it includes
functionality to generate consensus detections from ensembles of object
detectors.

</details>


### [238] [Facial Landmark Visualization and Emotion Recognition Through Neural Networks](https://arxiv.org/abs/2506.17191)
*Israel Juárez-Jiménez,Tiffany Guadalupe Martínez Paredes,Jesús García-Ramírez,Eric Ramos Aguilar*

Main category: cs.CV

TL;DR: This paper introduces facial landmark box plots to identify outliers in facial emotion recognition datasets and analyzes landmark features for emotion recognition.


<details>
  <summary>Details</summary>
Motivation: To improve emotion recognition from facial images by addressing challenges in extracting meaningful insights through dataset visualization and landmark analysis.

Method: The paper proposes using facial landmark box plots to detect outliers and evaluates two types of landmark features: absolute positions and displacement from neutral to emotional expressions.

Result: A neural network outperformed a random forest classifier in recognizing emotions based on facial landmark features.

Conclusion: Facial landmark analysis and neural network-based approaches enhance emotion recognition from facial images, demonstrating the importance of dataset visualization and feature selection.

Abstract: Emotion recognition from facial images is a crucial task in human-computer
interaction, enabling machines to learn human emotions through facial
expressions. Previous studies have shown that facial images can be used to
train deep learning models; however, most of these studies do not include a
through dataset analysis. Visualizing facial landmarks can be challenging when
extracting meaningful dataset insights; to address this issue, we propose
facial landmark box plots, a visualization technique designed to identify
outliers in facial datasets. Additionally, we compare two sets of facial
landmark features: (i) the landmarks' absolute positions and (ii) their
displacements from a neutral expression to the peak of an emotional expression.
Our results indicate that a neural network achieves better performance than a
random forest classifier.

</details>


### [239] [Hunyuan-GameCraft: High-dynamic Interactive Game Video Generation with Hybrid History Condition](https://arxiv.org/abs/2506.17201)
*Jiaqi Li,Junshu Tang,Zhiyong Xu,Longhuang Wu,Yuan Zhou,Shuai Shao,Tianbao Yu,Zhiguo Cao,Qinglin Lu*

Main category: cs.CV

TL;DR: The paper introduces Hunyuan-GameCraft, a model for generating high-quality interactive game videos with improved control, efficiency, and consistency.


<details>
  <summary>Details</summary>
Motivation: Current video generation methods struggle with dynamics, generality, and long-term consistency, limiting their application for diverse gameplay videos.

Method: The authors propose unifying keyboard and mouse inputs into a shared camera space, training the model autoregressively with game scene history, and employing model distillation for efficiency. The framework leverages extensive datasets for training and fine-tuning.

Result: The proposed framework achieves superior realism, action controllability, and temporal consistency compared to existing models.

Conclusion: Hunyuan-GameCraft offers an enhanced approach to interactive game video generation, making it more suitable for real-time complex environments.

Abstract: Recent advances in diffusion-based and controllable video generation have
enabled high-quality and temporally coherent video synthesis, laying the
groundwork for immersive interactive gaming experiences. However, current
methods face limitations in dynamics, generality, long-term consistency, and
efficiency, which limit the ability to create various gameplay videos. To
address these gaps, we introduce Hunyuan-GameCraft, a novel framework for
high-dynamic interactive video generation in game environments. To achieve
fine-grained action control, we unify standard keyboard and mouse inputs into a
shared camera representation space, facilitating smooth interpolation between
various camera and movement operations. Then we propose a hybrid
history-conditioned training strategy that extends video sequences
autoregressively while preserving game scene information. Additionally, to
enhance inference efficiency and playability, we achieve model distillation to
reduce computational overhead while maintaining consistency across long
temporal sequences, making it suitable for real-time deployment in complex
interactive environments. The model is trained on a large-scale dataset
comprising over one million gameplay recordings across over 100 AAA games,
ensuring broad coverage and diversity, then fine-tuned on a carefully annotated
synthetic dataset to enhance precision and control. The curated game scene data
significantly improves the visual fidelity, realism and action controllability.
Extensive experiments demonstrate that Hunyuan-GameCraft significantly
outperforms existing models, advancing the realism and playability of
interactive game video generation.

</details>


### [240] [UniFork: Exploring Modality Alignment for Unified Multimodal Understanding and Generation](https://arxiv.org/abs/2506.17202)
*Teng Li,Quanfeng Lu,Lirui Zhao,Hao Li,Xizhou Zhu,Yu Qiao,Jun Zhang,Wenqi Shao*

Main category: cs.CV

TL;DR: The paper introduces UniFork, a Y-shaped architecture for unified image understanding and generation that separates deep layers for task-specific processing, improving performance over conventional models.


<details>
  <summary>Details</summary>
Motivation: Unified models for image understanding and generation face conflicts due to divergent modality alignment patterns between tasks.

Method: The proposed UniFork architecture uses shared shallow layers for cross-task representation and separates deep layers into task-specific branches.

Result: Extensive experiments show UniFork outperforms conventional shared Transformers and matches or exceeds task-specific models.

Conclusion: UniFork balances shared learning and task specialization, addressing task interference in unified models effectively.

Abstract: Unified image understanding and generation has emerged as a promising
paradigm in multimodal artificial intelligence. Despite recent progress, the
optimal architectural design for such unified models remains an open challenge.
In this work, we start by analyzing the modality alignment behaviors of
task-specific expert models for understanding and generation, as well as
current unified models. Our analysis reveals a crucial observation:
understanding tasks benefit from a progressively increasing modality alignment
across network depth, which helps build up semantic information for better
comprehension; In contrast, generation tasks follow a different trend: modality
alignment increases in the early layers but decreases in the deep layers to
recover spatial details. These divergent alignment patterns create a
fundamental conflict in fully shared Transformer backbones, where a uniform
representational flow often leads to performance compromises across two tasks.
Motivated by this finding, we introduce UniFork, a novel Y-shaped architecture
that shares the shallow layers for cross-task representation learning, while
employing task-specific branches in deeper layers to avoid task interference.
This design effectively balances shared learning and task specialization.
Through extensive ablation experiments, we demonstrate that Unifork
consistently outperforms conventional fully shared Transformer architectures,
and achieves performance on par with or better than task-specific models.

</details>


### [241] [Machine Mental Imagery: Empower Multimodal Reasoning with Latent Visual Tokens](https://arxiv.org/abs/2506.17218)
*Zeyuan Yang,Xueyang Yu,Delin Chen,Maohao Shen,Chuang Gan*

Main category: cs.CV

TL;DR: This paper introduces Mirage, a framework allowing vision-language models to enhance visual imagination and reasoning by using latent visual tokens instead of explicit image generation.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of vision-language models that struggle with tasks requiring visual imagination, as their reasoning relies solely on verbalization.

Method: Mirage integrates latent visual tokens into the decoding process of vision-language models, facilitating multimodal reasoning. It utilizes supervised learning through distillation, text-only supervision, and reinforcement learning.

Result: Experiments demonstrate improved multimodal reasoning capabilities of vision-language models across various benchmarks without generating explicit images.

Conclusion: The paper concludes that enhancing vision-language models with latent visual tokens allows for stronger visual reasoning and imagination while bypassing the need for image generation pre-training.

Abstract: Vision-language models (VLMs) excel at multimodal understanding, yet their
text-only decoding forces them to verbalize visual reasoning, limiting
performance on tasks that demand visual imagination. Recent attempts train VLMs
to render explicit images, but the heavy image-generation pre-training often
hinders the reasoning ability. Inspired by the way humans reason with mental
imagery-the internal construction and manipulation of visual cues-we
investigate whether VLMs can reason through interleaved multimodal trajectories
without producing explicit images. To this end, we present a Machine Mental
Imagery framework, dubbed as Mirage, which augments VLM decoding with latent
visual tokens alongside ordinary text. Concretely, whenever the model chooses
to ``think visually'', it recasts its hidden states as next tokens, thereby
continuing a multimodal trajectory without generating pixel-level images. Begin
by supervising the latent tokens through distillation from ground-truth image
embeddings, we then switch to text-only supervision to make the latent
trajectory align tightly with the task objective. A subsequent reinforcement
learning stage further enhances the multimodal reasoning capability.
Experiments on diverse benchmarks demonstrate that Mirage unlocks stronger
multimodal reasoning without explicit image generation.

</details>


### [242] [Emergent Temporal Correspondences from Video Diffusion Transformers](https://arxiv.org/abs/2506.17220)
*Jisu Nam,Soowon Son,Dahyun Chung,Jiyoung Kim,Siyoon Jin,Junhwa Hur,Seungryong Kim*

Main category: cs.CV

TL;DR: The paper introduces DiffTrack, a framework to analyze how video Diffusion Transformers (DiTs) establish temporal correspondences in video generation, achieving insights into the inner mechanisms.


<details>
  <summary>Details</summary>
Motivation: Investigate how video Diffusion Transformers achieve temporally coherent video generation and represent temporal correspondences across frames.

Method: Developed DiffTrack with prompt-generated video datasets, pseudo ground-truth annotations, and custom metrics to analyze 3D attention mechanisms in DiTs.

Result: Identified that specific layers' query-key similarities are crucial for temporal matching, which strengthens during the denoising process, and validated the model in zero-shot point tracking and temporal consistency improvements.

Conclusion: DiffTrack provides key insights into video DiTs, enabling state-of-the-art applications in tracking and motion-enhanced generation while setting the stage for further exploration.

Abstract: Recent advancements in video diffusion models based on Diffusion Transformers
(DiTs) have achieved remarkable success in generating temporally coherent
videos. Yet, a fundamental question persists: how do these models internally
establish and represent temporal correspondences across frames? We introduce
DiffTrack, the first quantitative analysis framework designed to answer this
question. DiffTrack constructs a dataset of prompt-generated video with pseudo
ground-truth tracking annotations and proposes novel evaluation metrics to
systematically analyze how each component within the full 3D attention
mechanism of DiTs (e.g., representations, layers, and timesteps) contributes to
establishing temporal correspondences. Our analysis reveals that query-key
similarities in specific, but not all, layers play a critical role in temporal
matching, and that this matching becomes increasingly prominent during the
denoising process. We demonstrate practical applications of DiffTrack in
zero-shot point tracking, where it achieves state-of-the-art performance
compared to existing vision foundation and self-supervised video models.
Further, we extend our findings to motion-enhanced video generation with a
novel guidance method that improves temporal consistency of generated videos
without additional training. We believe our work offers crucial insights into
the inner workings of video DiTs and establishes a foundation for further
research and applications leveraging their temporal understanding.

</details>


### [243] [VLN-R1: Vision-Language Navigation via Reinforcement Fine-Tuning](https://arxiv.org/abs/2506.17221)
*Zhangyang Qi,Zhixiong Zhang,Yizhou Yu,Jiaqi Wang,Hengshuang Zhao*

Main category: cs.CV

TL;DR: The paper introduces VLN-R1, a navigation framework for vision-language tasks combining Large Vision-Language Models with GRPO-based continuous action training.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of current VLN systems using discrete topological graphs and enhance continuous navigation using egocentric video streams in real-world environments.

Method: VLN-R1 uses a 3D simulator (Habitat) to create training data (VLN-Ego dataset), employs Long-Short Memory Sampling for balanced observations, and introduces a two-stage training (Supervised Fine-tuning and Reinforcement Fine-tuning with Time-Decayed Reward).

Result: Experimental results demonstrate strong performance of VLN-R1 on the VLN-CE benchmark, proving its efficiency in embodied navigation tasks.

Conclusion: LVLMs can drive embodied navigation, enhance task reasoning, and achieve data-efficient training for challenging vision-language navigation tasks.

Abstract: Vision-Language Navigation (VLN) is a core challenge in embodied AI,
requiring agents to navigate real-world environments using natural language
instructions. Current language model-based navigation systems operate on
discrete topological graphs, limiting path planning to predefined node
connections. We propose VLN-R1, an end-to-end framework that leverages Large
Vision-Language Models (LVLM) to directly translate egocentric video streams
into continuous navigation actions, adopting GRPO-based training inspired by
DeepSeek-R1. To enable effective training, we first construct the VLN-Ego
dataset using a 3D simulator, Habitat, and propose Long-Short Memory Sampling
to balance historical and current observations. While large language models can
supervise complete textual instructions, they lack fine-grained action-level
control. Our framework employs a two-stage training approach: a) Supervised
fine-tuning (SFT) to align the model's action sequence text predictions with
expert demonstrations, followed by b) Reinforcement fine-tuning (RFT) enhanced
with a Time-Decayed Reward (TDR) mechanism that strategically weights
multi-step future actions. Experimental results show VLN-R1 achieves strong
performance on VLN-CE benchmark. VLN-R1 proves LVLMs can drive embodied
navigation and enhance task-specific reasoning through data-efficient,
reward-driven post-training.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [244] [TrainVerify: Equivalence-Based Verification for Distributed LLM Training](https://arxiv.org/abs/2506.15961)
*Yunchi Lu,Youshan Miao,Cheng Tan,Peng Huang,Yi Zhu,Xian Zhang,Fan Yang*

Main category: cs.DC

TL;DR: TrainVerify is a system designed to formally verify distributed training processes for large language models (LLMs), ensuring mathematical correctness and preventing costly computational errors.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the issue of silent errors during the distributed training of large language models, which can lead to wasted computational resources and inefficiencies.

Method: The paper introduces TrainVerify, which leverages shape-reduction techniques and a stage-wise parallel verification algorithm to simplify verification of LLM training plans while maintaining formal correctness.

Result: TrainVerify successfully verified training plans for frontier LLMs, including models like Llama3 (405B) and DeepSeek-V3 (671B), showcasing its scalability and efficacy in large-scale verification.

Conclusion: TrainVerify provides an effective solution to ensure mathematical equivalence between distributed training plans and model specifications, reducing the risk of errors and improving reliability in LLM development.

Abstract: Training large language models (LLMs) at scale requires parallel execution
across thousands of devices, incurring enormous computational costs. Yet, these
costly distributed trainings are rarely verified, leaving them prone to silent
errors and potentially wasting millions of GPU hours. We introduce TrainVerify,
a system for verifiable distributed training of LLMs. Given a deep learning
model's logical specification as the ground truth, TrainVerify formally
verifies that a distributed parallel execution plan is mathematically
equivalent to it. Direct verification is notoriously difficult due to the sheer
scale of LLMs which often involves billions of variables and highly intricate
computation graphs. Therefore, TrainVerify introduces shape-reduction
techniques and a stage-wise parallel verification algorithm that significantly
reduces complexity while preserving formal correctness. TrainVerify scales to
frontier LLMs, including the successful verification of the Llama3 (405B) and
DeepSeek-V3 (671B) training plans.

</details>


### [245] [JANUS: Resilient and Adaptive Data Transmission for Enabling Timely and Efficient Cross-Facility Scientific Workflows](https://arxiv.org/abs/2506.17084)
*Vladislav Esaulov,Jieyang Chen,Norbert Podhorszki,Fred Suter,Scott Klasky,Anu G Bourgeois,Lipeng Wan*

Main category: cs.DC

TL;DR: The paper introduces JANUS, a UDP-based adaptive data transmission method with erasure coding and lossy compression, designed to improve data transfer efficiency for cross-facility workflows despite network faults and high data volume.


<details>
  <summary>Details</summary>
Motivation: To address challenges in transferring massive data over wide-area networks for cross-facility workflows, including bandwidth strain, packet loss in TCP, and overhead from traditional fault-tolerance techniques.

Method: JANUS employs UDP, integrates erasure coding for fault tolerance, applies error-bounded lossy compression to reduce overhead, adapts coding parameters to network conditions, and uses optimization models to determine ideal configurations.

Result: JANUS demonstrates significant improvements in data transfer efficiency while maintaining fidelity, as validated by experiments.

Conclusion: JANUS provides a resilient, adaptable, and efficient solution for tackling the bottlenecks associated with massive data transfers in cross-facility scientific workflows.

Abstract: In modern science, the growing complexity of large-scale projects has
increased reliance on cross-facility workflows, where institutions share
resources and expertise to accelerate discovery. These workflows often involve
transferring massive data over wide-area networks. While high-speed networks
like ESnet and data transfer services like Globus have improved data mobility,
challenges remain. Large data volumes can strain bandwidth, TCP suffers from
retransmissions due to packet loss, and traditional fault-tolerance methods
like erasure coding introduce significant overhead.
  This paper presents JANUS, a resilient and adaptive data transmission
approach for cross-facility scientific workflows. JANUS uses UDP, integrates
erasure coding for fault tolerance, and applies error-bounded lossy compression
to reduce overhead. This design enables users to balance transmission time and
accuracy based on specific needs. JANUS also adapts coding parameters to
real-time network conditions and uses optimization models to determine ideal
configurations. Experiments show that JANUS significantly improves data
transfer efficiency while preserving fidelity.

</details>


### [246] [NetSenseML: Network-Adaptive Compression for Efficient Distributed Machine Learning](https://arxiv.org/abs/2506.16235)
*Yisu Wang,Xinjiao Li,Ruilong Wu,Huangxun Chen,Dirk Kutscher*

Main category: cs.DC

TL;DR: This paper proposes NetSenseML, a framework that adaptively applies data reduction strategies (like quantization, pruning, and compression) to better utilize network resources during distributed machine learning, enhancing training efficiency.


<details>
  <summary>Details</summary>
Motivation: Large-scale distributed machine learning systems often face traffic congestion and resource inefficiencies due to network demands, which negatively impact model training times and performance. Existing gradient compression methods often degrade model accuracy as a trade-off for reduced network load.

Method: NetSenseML dynamically adjusts gradient compression strategies in real-time based on active monitoring of network conditions. The framework ensures compression is applied only when congestion impacts convergence speed, enabling a balance between reducing data payload and preserving model quality.

Result: Experiments show NetSenseML improves training throughput by 1.55x to 9.84x over existing compression methods under bandwidth-constrained scenarios, demonstrating its effectiveness in optimizing resource usage without model accuracy degradation.

Conclusion: NetSenseML effectively enhances training efficiency and reduces convergence times by adaptively responding to network conditions, providing a promising solution for distributed machine learning in resource-limited environments.

Abstract: Training large-scale distributed machine learning models imposes considerable
demands on network infrastructure, often resulting in sudden traffic spikes
that lead to congestion, increased latency, and reduced throughput, which would
ultimately affect convergence times and overall training performance. While
gradient compression techniques are commonly employed to alleviate network
load, they frequently compromise model accuracy due to the loss of gradient
information.
  This paper introduces NetSenseML, a novel network adaptive distributed deep
learning framework that dynamically adjusts quantization, pruning, and
compression strategies in response to real-time network conditions. By actively
monitoring network conditions, NetSenseML applies gradient compression only
when network congestion negatively impacts convergence speed, thus effectively
balancing data payload reduction and model accuracy preservation.
  Our approach ensures efficient resource usage by adapting reduction
techniques based on current network conditions, leading to shorter convergence
times and improved training efficiency. We present the design of the NetSenseML
adaptive data reduction function and experimental evaluations show that
NetSenseML can improve training throughput by a factor of 1.55 to 9.84 times
compared to state-of-the-art compression-enabled systems for representative DDL
training jobs in bandwidth-constrained conditions.

</details>


### [247] [A Study of Synchronization Methods for Concurrent Size](https://arxiv.org/abs/2506.16350)
*Hen Kas-Sharir,Gal Sela,Erez Petrank*

Main category: cs.DC

TL;DR: Evaluates synchronization techniques for implementing efficient size methods in concurrent data structures.


<details>
  <summary>Details</summary>
Motivation: To address the high overhead introduced by linearizable concurrent size implementations in data structures, especially when size is not frequently invoked.

Method: Analyzed and benchmarked handshake, optimistic, and lock-based synchronization techniques against state-of-the-art size methods.

Result: The study found no universally optimal solution; handshake and wait-free methods are better for high contention, while optimistic and lock-based approaches excel under low contention.

Conclusion: Synchronization approach selection significantly impacts performance, and the choice should be tailored to the level of contention in the data structure's use case.

Abstract: The size of collections, maps, and data structures in general, constitutes a
fundamental property. An implementation of the size method is required in most
programming environments. Nevertheless, in a concurrent environment,
integrating a linearizable concurrent size introduces a noticeable overhead on
all operations of the data structure, even when the size method is not invoked
during the execution. In this work we present a study of synchronization
methods in an attempt to improve the performance of the data structure. In
particular, we study a handshake technique that is commonly used with
concurrent garbage collection, an optimistic technique, and a lock-based
technique. Evaluation against the state-of-the-art size methodology
demonstrates that the overhead can be significantly reduced by selecting the
appropriate synchronization approach, but there is no one-size-fits-all method.
Different scenarios call for different synchronization methods, as rigorously
shown in this study. Nevertheless, our findings align with general trends in
concurrent computing. In scenarios characterized by low contention, optimistic
and lock-based approaches work best, whereas under high contention, the most
effective solutions are the handshake approach and the wait-free approach.

</details>


### [248] [Parallel Point-to-Point Shortest Paths and Batch Queries](https://arxiv.org/abs/2506.16488)
*Xiaojun Dong,Andy Li,Yan Gu,Yihan Sun*

Main category: cs.DC

TL;DR: Orionet introduces efficient parallel algorithms for Point-to-Point Shortest Paths (PPSP) using bidirectional search and other heuristics, with enhancements for batched queries.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of efficiently implementing PPSP queries, focusing on parallelism and batching to improve performance for diverse graph types in real-world applications.

Method: Orionet leverages bidirectional search, A* search, and pruning techniques to optimize PPSP queries, while formalizing batch queries as a query graph for shared information utilization.

Result: Orionet demonstrated significant speedups compared to baselines GraphIt and MBQ (up to 6.8x and 6.2x improvement respectively) across single and batch PPSP queries.

Conclusion: The study highlights Orionet's effectiveness and efficiency in both single and batched PPSP queries, solidifying its role as a strong contender in high-performance graph-based computing.

Abstract: We propose Orionet, efficient parallel implementations of Point-to-Point
Shortest Paths (PPSP) queries using bidirectional search (BiDS) and other
heuristics, with an additional focus on batch PPSP queries. We present a
framework for parallel PPSP built on existing single-source shortest paths
(SSSP) frameworks by incorporating pruning conditions. As a result, we develop
efficient parallel PPSP algorithms based on early termination, bidirectional
search, A$^*$ search, and bidirectional A$^*$ all with simple and efficient
implementations.
  We extend our idea to batch PPSP queries, which are widely used in real-world
scenarios. We first design a simple and flexible abstraction to represent the
batch so PPSP can leverage the shared information of the batch. Orionet
formalizes the batch as a query graph represented by edges between queried
sources and targets. In this way, we directly extended our PPSP framework to
batched queries in a simple and efficient way.
  We evaluate Orionet on both single and batch PPSP queries using various graph
types and distance percentiles of queried pairs, and compare it against two
baselines, GraphIt and MBQ. Both of them support parallel single PPSP and A$^*$
using unidirectional search. On 14 graphs we tested, on average, our
bidirectional search is 2.9$\times$ faster than GraphIt, and 6.8$\times$ faster
than MBQ. Our bidirectional A$^*$ is 4.4$\times$ and 6.2$\times$ faster than
the A$^*$ in GraphIt and MBQ, respectively. For batched PPSP queries, we also
provide in-depth experimental evaluation, and show that Orionet provides strong
performance compared to the plain solutions.

</details>


### [249] [Enabling Blockchain Interoperability Through Network Discovery Services](https://arxiv.org/abs/2506.16611)
*Khalid Hassan,Amirreza Sokhankhosh,Sara Rouhani*

Main category: cs.DC

TL;DR: A decentralized architecture for blockchain network and asset discovery is proposed and evaluated, demonstrating scalability and resilience.


<details>
  <summary>Details</summary>
Motivation: To address the gap in initial blockchain network discovery, as existing interoperability solutions assume networks are already aware of each other.

Method: Introduce a decentralized architecture for discovering blockchain networks and assets using the Substrate framework, coupled with an incentive mechanism for active node participation.

Result: The architecture handles up to 130,000 concurrent requests with a median response time of 5.5 milliseconds, showing scalability and resilience.

Conclusion: The proposed decentralized discovery architecture successfully addresses blockchain discovery challenges and demonstrates significant scalability potential.

Abstract: Web3 technologies have experienced unprecedented growth in the last decade,
achieving widespread adoption. As various blockchain networks continue to
evolve, we are on the cusp of a paradigm shift in which they could provide
services traditionally offered by the Internet, but in a decentralized manner,
marking the emergence of the Internet of Blockchains. While significant
progress has been achieved in enabling interoperability between blockchain
networks, existing solutions often assume that networks are already mutually
aware. This reveals a critical gap: the initial discovery of blockchain
networks remains largely unaddressed. This paper proposes a decentralized
architecture for blockchain network discovery that operates independently of
any centralized authority. We also introduce a mechanism for discovering assets
and services within a blockchain from external networks. Given the
decentralized nature of the proposed discovery architecture, we design an
incentive mechanism to encourage nodes to actively participate in maintaining
the discovery network. The proposed architecture implemented and evaluated,
using the Substrate framework, demonstrates its resilience and scalability,
effectively handling up to 130,000 concurrent requests under the tested network
configurations, with a median response time of 5.5 milliseconds, demonstrating
the ability to scale its processing capacity further by increasing its network
size.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [250] [Ignition Phase : Standard Training for Fast Adversarial Robustness](https://arxiv.org/abs/2506.15685)
*Wang Yu-Hang,Liu ying,Fang liang,Wang Xuelin,Junkang Guo,Shiwei Li,Lei Gao,Jian Liu,Wenfei Yin*

Main category: cs.LG

TL;DR: The paper introduces Adversarial Evolution Training (AET), enhancing adversarial training by prepending an Empirical Risk Minimization (ERM) phase, improving robustness, accuracy, and training efficiency.


<details>
  <summary>Details</summary>
Motivation: To improve adversarial training, as most variants focus on attack generation and lack emphasis on foundational feature representations.

Method: AET strategically adds an initial ERM phase before conducting conventional adversarial training, hypothesizing that this achieves a favorable feature manifold.

Result: AET delivers comparable or better robustness faster, boosts clean accuracy, and reduces training costs by 8-25%, validated across diverse datasets and architectures.

Conclusion: Integrating ERM with adversarial training creates a more efficient and principled approach to robust defenses, highlighting the importance of feature pre-conditioning.

Abstract: Adversarial Training (AT) is a cornerstone defense, but many variants
overlook foundational feature representations by primarily focusing on stronger
attack generation. We introduce Adversarial Evolution Training (AET), a simple
yet powerful framework that strategically prepends an Empirical Risk
Minimization (ERM) phase to conventional AT. We hypothesize this initial ERM
phase cultivates a favorable feature manifold, enabling more efficient and
effective robustness acquisition. Empirically, AET achieves comparable or
superior robustness more rapidly, improves clean accuracy, and cuts training
costs by 8-25\%. Its effectiveness is shown across multiple datasets,
architectures, and when augmenting established AT methods. Our findings
underscore the impact of feature pre-conditioning via standard training for
developing more efficient, principled robust defenses. Code is available in the
supplementary material.

</details>


### [251] [Learning from M-Tuple Dominant Positive and Unlabeled Data](https://arxiv.org/abs/2506.15686)
*Jiahe Qin,Junpeng Li,Changchun Hua,Yana Yang*

Main category: cs.LG

TL;DR: The paper introduces MDPU, a generalized learning framework for Label Proportion Learning (LLP) that deals with practical challenges of precise supervisory proportion information. It provides a risk estimator, corrects for overfitting, and demonstrates theoretical consistency and effectiveness in experiments.


<details>
  <summary>Details</summary>
Motivation: Label Proportion Learning struggles with obtaining precise proportion information in real-world scenarios. The motivation is to develop a practical framework that better aligns with these challenges.

Method: The authors formulate the instance distribution under specific constraints, derive a risk estimator using empirical risk minimization (ERM), address overfitting with a correction method, and establish theoretical consistency through generalization error bounds.

Result: The method shows effectiveness through theoretical validation (risk consistency and error bounds) and empirical testing on multiple datasets, outperforming baseline methods.

Conclusion: MDPU is a robust, consistent, and practical framework for LLP problems, proving its effectiveness both theoretically and experimentally.

Abstract: Label Proportion Learning (LLP) addresses the classification problem where
multiple instances are grouped into bags and each bag contains information
about the proportion of each class. However, in practical applications,
obtaining precise supervisory information regarding the proportion of instances
in a specific class is challenging. To better align with real-world application
scenarios and effectively leverage the proportional constraints of instances
within tuples, this paper proposes a generalized learning framework
\emph{MDPU}. Specifically, we first mathematically model the distribution of
instances within tuples of arbitrary size, under the constraint that the number
of positive instances is no less than that of negative instances. Then we
derive an unbiased risk estimator that satisfies risk consistency based on the
empirical risk minimization (ERM) method. To mitigate the inevitable
overfitting issue during training, a risk correction method is introduced,
leading to the development of a corrected risk estimator. The generalization
error bounds of the unbiased risk estimator theoretically demonstrate the
consistency of the proposed method. Extensive experiments on multiple datasets
and comparisons with other relevant baseline methods comprehensively validate
the effectiveness of the proposed learning framework.

</details>


### [252] [S$^2$GPT-PINNs: Sparse and Small models for PDEs](https://arxiv.org/abs/2506.15687)
*Yajie Ji,Yanlai Chen,Shawn Koohy*

Main category: cs.LG

TL;DR: The paper introduces S$^2$GPT-PINN, a sparse and compact model for solving parametric PDEs efficiently with minimal computational power.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the inefficiency and computational demands of traditional PINN-based solutions by proposing a specialized and compact model tailored for domain-specific PDEs.

Method: S$^2$GPT-PINN utilizes a greedy algorithm for high-quality data selection, knowledge distillation via task-specific activation functions from pre-trained PINNs, and reduced data points for computing loss to optimize model performance.

Result: The model achieves comparable efficiency with significantly fewer parameters and reduced computational resources than traditional PINN approaches.

Conclusion: S$^2$GPT-PINN demonstrates that tailored, small models can solve PDEs effectively, promoting efficiency in domain-specific applications while reducing resource utilization.

Abstract: We propose S$^2$GPT-PINN, a sparse and small model for solving parametric
partial differential equations (PDEs). Similar to Small Language Models (SLMs),
S$^2$GPT-PINN is tailored to domain-specific (families of) PDEs and
characterized by its compact architecture and minimal computational power.
Leveraging a small amount of extremely high quality data via a mathematically
rigorous greedy algorithm that is enabled by the large full-order models,
S$^2$GPT-PINN relies on orders of magnitude less parameters than PINNs to
achieve extremely high efficiency via two levels of customizations. The first
is knowledge distillation via task-specific activation functions that are
transferred from Pre-Trained PINNs. The second is a judicious down-sampling
when calculating the physics-informed loss of the network compressing the
number of data sites by orders of magnitude to the size of the small model.

</details>


### [253] [Floating-Point Neural Networks Are Provably Robust Universal Approximators](https://arxiv.org/abs/2506.16065)
*Geonho Hwang,Wonyeol Lee,Yeachan Park,Sejun Park,Feras Saad*

Main category: cs.LG

TL;DR: The paper extends the interval universal approximation (IUA) theorem to floating-point neural networks, showing their ability to accurately capture the direct image map of any rounded function.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the gap in theoretical understanding of neural networks when moving from ideal infinite-precision real numbers to the practical finite-precision floating-point numbers.

Method: The paper develops a new IUA theorem specifically for floating-point neural networks, analyzing their expressiveness and computational properties under this numerical setting.

Result: The study establishes that floating-point neural networks can perfectly approximate the direct image map of any rounded target function, showcasing no expressiveness limitations, and reveals notable differences from the real-valued setting.

Conclusion: The findings highlight the computational completeness and robustness of floating-point neural networks, opening pathways for secure, accurate, and theoretically grounded applications in real-world computation.

Abstract: The classical universal approximation (UA) theorem for neural networks
establishes mild conditions under which a feedforward neural network can
approximate a continuous function $f$ with arbitrary accuracy. A recent result
shows that neural networks also enjoy a more general interval universal
approximation (IUA) theorem, in the sense that the abstract interpretation
semantics of the network using the interval domain can approximate the direct
image map of $f$ (i.e., the result of applying $f$ to a set of inputs) with
arbitrary accuracy. These theorems, however, rest on the unrealistic assumption
that the neural network computes over infinitely precise real numbers, whereas
their software implementations in practice compute over finite-precision
floating-point numbers. An open question is whether the IUA theorem still holds
in the floating-point setting.
  This paper introduces the first IUA theorem for floating-point neural
networks that proves their remarkable ability to perfectly capture the direct
image map of any rounded target function $f$, showing no limits exist on their
expressiveness. Our IUA theorem in the floating-point setting exhibits material
differences from the real-valued setting, which reflects the fundamental
distinctions between these two computational models. This theorem also implies
surprising corollaries, which include (i) the existence of provably robust
floating-point neural networks; and (ii) the computational completeness of the
class of straight-line programs that use only floating-point additions and
multiplications for the class of all floating-point programs that halt.

</details>


### [254] [Cellular Traffic Prediction via Deep State Space Models with Attention Mechanism](https://arxiv.org/abs/2506.15688)
*Hui Ma,Kai Yang,Man-On Pun*

Main category: cs.LG

TL;DR: The paper develops an advanced end-to-end framework for cellular traffic prediction using a combination of CNNs, attention mechanisms, and Kalman filters, achieving superior performance over state-of-the-art models.


<details>
  <summary>Details</summary>
Motivation: To address the challenges in accurate cellular traffic prediction caused by its dynamic nature and external influences.

Method: The authors propose using convolutional neural networks (CNNs) with attention mechanisms for spatial patterns and Kalman filters for temporal modeling. They also utilize auxiliary data like social activities to enhance prediction.

Result: Extensive experiments on three real-world datasets demonstrate that the proposed models outperform existing machine learning techniques in prediction accuracy.

Conclusion: This framework efficiently captures spatiotemporal cellular traffic patterns and improves prediction accuracy significantly through innovative methods and external information.

Abstract: Cellular traffic prediction is of great importance for operators to manage
network resources and make decisions. Traffic is highly dynamic and influenced
by many exogenous factors, which would lead to the degradation of traffic
prediction accuracy. This paper proposes an end-to-end framework with two
variants to explicitly characterize the spatiotemporal patterns of cellular
traffic among neighboring cells. It uses convolutional neural networks with an
attention mechanism to capture the spatial dynamics and Kalman filter for
temporal modelling. Besides, we can fully exploit the auxiliary information
such as social activities to improve prediction performance. We conduct
extensive experiments on three real-world datasets. The results show that our
proposed models outperform the state-of-the-art machine learning techniques in
terms of prediction accuracy.

</details>


### [255] [BASE-Q: Bias and Asymmetric Scaling Enhanced Rotational Quantization for Large Language Models](https://arxiv.org/abs/2506.15689)
*Liulu He,Shenli Zhen,Karwei Sun,Yijiang Liu,Yufei Zhao,Chongkang Tan,Huanrui Yang,Yuan Du,Li Du*

Main category: cs.LG

TL;DR: The paper introduces BASE-Q, a rotation-free quantization method for large language models (LLMs) that reduces memory overhead while improving accuracy.


<details>
  <summary>Details</summary>
Motivation: Current rotational quantization methods face issues like poor channel mean alignment, increased rounding errors, and energy loss from Gaussian-like activation distributions, all while requiring memory-intensive optimization processes.

Method: BASE-Q employs bias correction and asymmetric scaling to reduce quantization errors and allows blockwise optimization, eliminating the need for memory-heavy full-model backpropagation.

Result: BASE-Q significantly narrows the accuracy gap between quantized and full-precision models, outperforming state-of-the-art methods such as QuaRot, SpinQuant, and OSTQuant.

Conclusion: BASE-Q addresses fundamental limitations of rotational quantization methods, providing a more efficient and effective solution for quantizing LLMs with reduced computational and memory demands.

Abstract: Rotations have become essential to state-of-the-art quantization pipelines
for large language models (LLMs) by effectively smoothing outliers in weights
and activations. However, further optimizing the rotation parameters offers
only limited performance gains and introduces significant training overhead:
due to rotation parameter sharing, full-model must be loaded simultaneously to
enable backpropagation, resulting in substantial memory consumption and limited
practical utility. In this work, we identify two fundamental limitations of
current rotational quantization methods: (i) rotation fails to align channel
means, resulting in wider quantization bounds and increased rounding errors;
and (ii) rotation makes the activation distribution more Gaussian-like,
increasing energy loss caused by clipping errors. To address these issues, we
introduce \textbf{BASE-Q}, a simple yet powerful approach that combines bias
correction and asymmetric scaling to effectively reduce rounding and clipping
errors. Furthermore, BASE-Q enables blockwise optimization, eliminating the
need for memory-intensive full-model backpropagation. Extensive experiments on
various LLMs and benchmarks demonstrate the effectiveness of BASE-Q, narrowing
the accuracy gap to full-precision models by 50.5\%, 42.9\%, and 29.2\%
compared to QuaRot, SpinQuant, and OSTQuant, respectively. The code will be
released soon.

</details>


### [256] [LLM Web Dynamics: Tracing Model Collapse in a Network of LLMs](https://arxiv.org/abs/2506.15690)
*Tianyu Wang,Lingyou Pang,Akira Horiguchi,Carey E. Priebe*

Main category: cs.LG

TL;DR: The paper introduces a framework, LLM Web Dynamics (LWD), to explore model collapse in large language models by simulating the Internet using retrieval-augmented generation (RAG) and theoretical analysis.


<details>
  <summary>Details</summary>
Motivation: To address the underexplored issue of model collapse when synthetic data is used for training large language models (LLMs).

Method: Proposed LLM Web Dynamics (LWD) framework, simulates Internet interactions using a retrieval-augmented generation (RAG) database and analyzes output convergence with theoretical guarantees.

Result: Demonstrates patterns of convergence in model outputs and provides formal theoretical backing based on Gaussian Mixture Models.

Conclusion: The framework effectively investigates model collapse at a network level and provides valuable insights into synthetic data-based LLM training risks.

Abstract: The increasing use of synthetic data from the public Internet has enhanced
data usage efficiency in large language model (LLM) training. However, the
potential threat of model collapse remains insufficiently explored. Existing
studies primarily examine model collapse in a single model setting or rely
solely on statistical surrogates. In this work, we introduce LLM Web Dynamics
(LWD), an efficient framework for investigating model collapse at the network
level. By simulating the Internet with a retrieval-augmented generation (RAG)
database, we analyze the convergence pattern of model outputs. Furthermore, we
provide theoretical guarantees for this convergence by drawing an analogy to
interacting Gaussian Mixture Models.

</details>


### [257] [What Do Latent Action Models Actually Learn?](https://arxiv.org/abs/2506.15691)
*Chuheng Zhang,Tim Pearce,Pushi Zhang,Kaixin Wang,Xiaoyu Chen,Wei Shen,Li Zhao,Jiang Bian*

Main category: cs.LG

TL;DR: This paper examines whether latent action models (LAMs) effectively capture action-relevant changes in videos or are affected by exogenous noise. They introduce a simplified linear model to analyze LAMs and propose strategies to enhance their performance.


<details>
  <summary>Details</summary>
Motivation: The study aims to address a critical concern in LAMs: distinguishing changes in video frames caused by actions from those caused by irrelevant exogenous noise.

Method: The authors analyze LAMs using a tractable linear model, illustrate insights through connections to PCA, and propose strategies such as data augmentation, data cleaning, and auxiliary action-prediction to refine action-relevant learning.

Result: The study provides analytical insights, justifications for improvement strategies, and numerical simulations to demonstrate how observation structures, actions, and noise affect LAM learning.

Conclusion: The findings systematically address the challenge of separating controllable changes from noise, offering theoretical and practical guidance for improving LAM performance.

Abstract: Latent action models (LAMs) aim to learn action-relevant changes from
unlabeled videos by compressing changes between frames as latents. However,
differences between video frames can be caused by controllable changes as well
as exogenous noise, leading to an important concern -- do latents capture the
changes caused by actions or irrelevant noise? This paper studies this issue
analytically, presenting a linear model that encapsulates the essence of LAM
learning, while being tractable.This provides several insights, including
connections between LAM and principal component analysis (PCA), desiderata of
the data-generating policy, and justification of strategies to encourage
learning controllable changes using data augmentation, data cleaning, and
auxiliary action-prediction. We also provide illustrative results based on
numerical simulation, shedding light on the specific structure of observations,
actions, and noise in data that influence LAM learning.

</details>


### [258] [MLE-STAR: Machine Learning Engineering Agent via Search and Targeted Refinement](https://arxiv.org/abs/2506.15692)
*Jaehyun Nam,Jinsung Yoon,Jiefeng Chen,Jinwoo Shin,Sercan Ö. Arık,Tomas Pfister*

Main category: cs.LG

TL;DR: MLE-STAR enhances LLM-based agents by refining machine learning models iteratively, utilizing external knowledge and targeted exploration, and achieves competitive performance in ML tasks.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based agents struggle in selecting task-specific models and performing component-level exploration for ML engineering, limiting their effectiveness.

Method: MLE-STAR uses external web knowledge for initial setup and iterative component-specific refinement guided by ablation studies, along with a novel ensembling strategy.

Result: MLE-STAR achieved medals in 44% of Kaggle competitions on the MLE-bench, outperforming current best alternatives.

Conclusion: MLE-STAR offers significant advancements in ML engineering automation by improving model effectiveness and exploration strategies, proving its value in competitive environments.

Abstract: Agents based on large language models (LLMs) for machine learning engineering
(MLE) can automatically implement ML models via code generation. However,
existing approaches to build such agents often rely heavily on inherent LLM
knowledge and employ coarse exploration strategies that modify the entire code
structure at once. This limits their ability to select effective task-specific
models and perform deep exploration within specific components, such as
experimenting extensively with feature engineering options. To overcome these,
we propose MLE-STAR, a novel approach to build MLE agents. MLE-STAR first
leverages external knowledge by using a search engine to retrieve effective
models from the web, forming an initial solution, then iteratively refines it
by exploring various strategies targeting specific ML components. This
exploration is guided by ablation studies analyzing the impact of individual
code blocks. Furthermore, we introduce a novel ensembling method using an
effective strategy suggested by MLE-STAR. Our experimental results show that
MLE-STAR achieves medals in 44% of the Kaggle competitions on the MLE-bench,
significantly outperforming the best alternative.

</details>


### [259] [Verifiable Safety Q-Filters via Hamilton-Jacobi Reachability and Multiplicative Q-Networks](https://arxiv.org/abs/2506.15693)
*Jiaxing Li,Hanjiang Hu,Yujie Yang,Changliu Liu*

Main category: cs.LG

TL;DR: The paper proposes a Hamilton-Jacobi reachability-based safety filter that offers formal guarantees, extending self-consistency properties for Q functions and introducing a multiplicative Q-network, achieving verified safety certificates across benchmarks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the lack of formal safety guarantees in learning-based safety filters, which, while effective, fail to maintain rigorous standards of safety compared to traditional methods like CBFs.

Method: The approach includes extending self-consistency properties for Q value functions, designing a multiplicative Q-network structure, and building a sound verification pipeline for these properties.

Result: The method successfully synthesizes formally verified, model-free safety certificates on four standard safe-control benchmarks.

Conclusion: The work demonstrates a model-free approach that effectively combines formal safety guarantees with learning-based adaptability, overcoming limitations in existing methods.

Abstract: Recent learning-based safety filters have outperformed conventional methods,
such as hand-crafted Control Barrier Functions (CBFs), by effectively adapting
to complex constraints. However, these learning-based approaches lack formal
safety guarantees. In this work, we introduce a verifiable model-free safety
filter based on Hamilton-Jacobi reachability analysis. Our primary
contributions include: 1) extending verifiable self-consistency properties for
Q value functions, 2) proposing a multiplicative Q-network structure to
mitigate zero-sublevel-set shrinkage issues, and 3) developing a verification
pipeline capable of soundly verifying these self-consistency properties. Our
proposed approach successfully synthesizes formally verified, model-free safety
certificates across four standard safe-control benchmarks.

</details>


### [260] [PNCS:Power-Norm Cosine Similarity for Diverse Client Selection in Federated Learning](https://arxiv.org/abs/2506.15923)
*Liangyan Li,Yangyi Liu,Yimo Ning,Stefano Rini,Jun Chen*

Main category: cs.LG

TL;DR: The paper introduces an FL framework using PNCS for improved client selection and enhanced convergence on non-IID data.


<details>
  <summary>Details</summary>
Motivation: To address the challenges in federated learning caused by non-IID data and intricate gradient correlations between clients.

Method: The framework uses Power-Norm Cosine Similarity (PNCS) for client selection, measuring higher-order gradient moments, and includes a history queue for diverse client selection.

Result: Experiments with the VGG16 model show consistent improvements in convergence speed and accuracy compared to other methods, especially with varied data partitions.

Conclusion: PNCS-based client selection addresses non-IID data issues effectively, making FL more efficient and accurate for heterogeneous datasets.

Abstract: Federated Learning (FL) has emerged as a powerful paradigm for leveraging
diverse datasets from multiple sources while preserving data privacy by
avoiding centralized storage. However, many existing approaches fail to account
for the intricate gradient correlations between remote clients, a limitation
that becomes especially problematic in data heterogeneity scenarios. In this
work, we propose a novel FL framework utilizing Power-Norm Cosine Similarity
(PNCS) to improve client selection for model aggregation. By capturing
higher-order gradient moments, PNCS addresses non-IID data challenges,
enhancing convergence speed and accuracy. Additionally, we introduce a simple
algorithm ensuring diverse client selection through a selection history queue.
Experiments with a VGG16 model across varied data partitions demonstrate
consistent improvements over state-of-the-art methods.

</details>


### [261] [Development of a Multiprocessing Interface Genetic Algorithm for Optimising a Multilayer Perceptron for Disease Prediction](https://arxiv.org/abs/2506.15694)
*Iliyas Ibrahim Iliyas,Souley Boukari,Abdulsalam Yau Gital*

Main category: cs.LG

TL;DR: This paper proposes an integrated framework of nonlinear feature extraction, classification, and efficient optimization that achieves high accuracy and reduces tuning time in disease prediction tasks.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the need for efficient and accurate machine learning models for disease prediction by proposing a method that combines feature extraction with hyperparameter optimization.

Method: The approach involves (1) using kernel PCA with a radial basis function kernel to reduce dimensionality while retaining 95% variance, (2) employing a multilayer perceptron (MLP) for disease status prediction, and (3) optimizing the MLP's hyperparameters using a Modified Multiprocessing Genetic Algorithm (MIGA).

Result: The framework achieves high classification accuracy: 99.12% for breast cancer, 94.87% for Parkinson's disease, and 100% for chronic kidney disease. It outperforms grid search, random search, Bayesian optimization, and standard genetic algorithms while reducing hyperparameter tuning times by approximately 60%.

Conclusion: The paper demonstrates that integrating kernel PCA, MLP, and MIGA improves classification accuracy and accelerates hyperparameter optimization, making it a robust solution for medical dataset analysis.

Abstract: This study introduces a framework that integrates nonlinear feature
extraction, classification, and efficient optimization. First, kernel principal
component analysis with a radial basis function kernel reduces dimensionality
while preserving 95% of the variance. Second, a multilayer perceptron (MLP)
learns to predict disease status. Finally, a modified multiprocessing genetic
algorithm (MIGA) optimizes MLP hyperparameters in parallel over ten
generations. We evaluated this approach on three datasets: the Wisconsin
Diagnostic Breast Cancer dataset, the Parkinson's Telemonitoring dataset, and
the chronic kidney disease dataset. The MLP tuned by the MIGA achieved the best
accuracy of 99.12% for breast cancer, 94.87% for Parkinson's disease, and 100%
for chronic kidney disease. These results outperform those of other methods,
such as grid search, random search, and Bayesian optimization. Compared with a
standard genetic algorithm, kernel PCA revealed nonlinear relationships that
improved classification, and the MIGA's parallel fitness evaluations reduced
the tuning time by approximately 60%. The genetic algorithm incurs high
computational cost from sequential fitness evaluations, but our multiprocessing
interface GA (MIGA) parallelizes this step, slashing the tuning time and
steering the MLP toward the best accuracy score of 99.12%, 94.87%, and 100% for
breast cancer, Parkinson's disease, and CKD, respectively.

</details>


### [262] [SimuGen: Multi-modal Agentic Framework for Constructing Block Diagram-Based Simulation Models](https://arxiv.org/abs/2506.15695)
*Xinxing Ren,Qianbo Zang,Zekun Guo*

Main category: cs.LG

TL;DR: The paper introduces SimuGen, a framework designed to address the challenges of generating Simulink simulation code using LLMs, by integrating visual and domain-specific knowledge.


<details>
  <summary>Details</summary>
Motivation: LLMs excel in mathematical and code reasoning but struggle to generate Simulink models due to insufficient training data. This poses a challenge for engineering and scientific applications where Simulink models are crucial.

Method: SimuGen combines multimodal agents that collaborate using domain-specific knowledge. The components include an investigator, code generator, executor, and others, coordinated to produce complete and accurate Simulink models.

Result: SimuGen enhances the reliability, interpretability, and reproducibility of Simulink simulation generation. Its open-source code supports further research and development.

Conclusion: The proposed framework fills a critical gap in LLM capabilities by enabling accurate Simulink simulation code generation, leveraging modular and collaborative agent design.

Abstract: Recent advances in large language models (LLMs) have shown impressive
performance in mathematical reasoning and code generation. However, LLMs still
struggle in the simulation domain, particularly in generating Simulink models,
which are essential tools in engineering and scientific research. Our
preliminary experiments indicate that LLM agents often fail to produce reliable
and complete Simulink simulation code from text-only inputs, likely due to the
lack of Simulink-specific data in their pretraining. To address this challenge,
we propose SimuGen, a multimodal agent-based framework that automatically
generates accurate Simulink simulation code by leveraging both the visual
Simulink diagram and domain knowledge. SimuGen coordinates several specialized
agents, including an investigator, unit test reviewer, code generator,
executor, debug locator, and report writer, supported by a domain-specific
knowledge base. This collaborative and modular design enables interpretable,
robust, and reproducible Simulink simulation generation. Our source code is
publicly available at https://github.com/renxinxing123/SimuGen_beta.

</details>


### [263] [CoC: Chain-of-Cancer based on Cross-Modal Autoregressive Traction for Survival Prediction](https://arxiv.org/abs/2506.15696)
*Haipeng Zhou,Sicheng Yang,Sihan Yang,Jing Qin,Lei Chen,Lei Zhu*

Main category: cs.LG

TL;DR: The paper proposes a novel "Chain-of-Cancer" (CoC) framework using four modalities, including clinical and language data, to improve cancer survival prediction, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Existing survival prediction models for cancer rely primarily on pathology and genomics data and overlook the potential of epigenetic changes like methylation data and textual descriptions, which could also provide crucial insights.

Method: The proposed "Chain-of-Cancer" (CoC) framework incorporates four modalities: three clinical modalities and language. It employs intra-learning to use domain-specific clinical features and inter-learning leveraging language as a prompt. An Autoregressive Mutual Traction module is introduced to enable synergistic representation across modalities.

Result: The method was tested across five public cancer datasets, demonstrating its effectiveness and achieving state-of-the-art results in survival prediction.

Conclusion: This study establishes the feasibility and benefits of incorporating language and multiple clinical data modalities for survival prediction, outperforming previous models. Code will be released for reproducibility.

Abstract: Survival prediction aims to evaluate the risk level of cancer patients.
Existing methods primarily rely on pathology and genomics data, either
individually or in combination. From the perspective of cancer pathogenesis,
epigenetic changes, such as methylation data, could also be crucial for this
task. Furthermore, no previous endeavors have utilized textual descriptions to
guide the prediction. To this end, we are the first to explore the use of four
modalities, including three clinical modalities and language, for conducting
survival prediction. In detail, we are motivated by the Chain-of-Thought (CoT)
to propose the Chain-of-Cancer (CoC) framework, focusing on intra-learning and
inter-learning. We encode the clinical data as the raw features, which remain
domain-specific knowledge for intra-learning. In terms of inter-learning, we
use language to prompt the raw features and introduce an Autoregressive Mutual
Traction module for synergistic representation. This tailored framework
facilitates joint learning among multiple modalities. Our approach is evaluated
across five public cancer datasets, and extensive experiments validate the
effectiveness of our methods and proposed designs, leading to producing \sota
results. Codes will be released.

</details>


### [264] [Global Context-aware Representation Learning for Spatially Resolved Transcriptomics](https://arxiv.org/abs/2506.15698)
*Yunhak Oh,Junseok Lee,Yeongmin Kim,Sangwoo Seo,Namkyeong Lee,Chanyoung Park*

Main category: cs.LG

TL;DR: The paper introduces Spotscape, a novel method for improving spatial domain identification in Spatially Resolved Transcriptomics (SRT) by overcoming limitations of existing graph-based approaches.


<details>
  <summary>Details</summary>
Motivation: Existing graph-based SRT methods struggle to effectively represent spots near spatial domain boundaries due to their reliance on heavily weighting adjacent spots with minimal differences. This hinders meaningful feature representation.

Method: Spotscape introduces the Similarity Telescope module for capturing global relationships across spots and a similarity scaling strategy to manage distances between intra- and inter-slice spots. This enables better multi-slice integration.

Result: Spotscape demonstrates superiority in various downstream tasks for both single-slice and multi-slice SRT scenarios as validated by extensive experiments.

Conclusion: Spotscape effectively overcomes the limitations of existing approaches, providing a more robust framework for spatial domain identification in SRT and improving applications in multi-slice integration.

Abstract: Spatially Resolved Transcriptomics (SRT) is a cutting-edge technique that
captures the spatial context of cells within tissues, enabling the study of
complex biological networks. Recent graph-based methods leverage both gene
expression and spatial information to identify relevant spatial domains.
However, these approaches fall short in obtaining meaningful spot
representations, especially for spots near spatial domain boundaries, as they
heavily emphasize adjacent spots that have minimal feature differences from an
anchor node. To address this, we propose Spotscape, a novel framework that
introduces the Similarity Telescope module to capture global relationships
between multiple spots. Additionally, we propose a similarity scaling strategy
to regulate the distances between intra- and inter-slice spots, facilitating
effective multi-slice integration. Extensive experiments demonstrate the
superiority of Spotscape in various downstream tasks, including single-slice
and multi-slice scenarios. Our code is available at the following link: https:
//github.com/yunhak0/Spotscape.

</details>


### [265] [BLUR: A Benchmark for LLM Unlearning Robust to Forget-Retain Overlap](https://arxiv.org/abs/2506.15699)
*Shengyuan Hu,Neil Kale,Pratiksha Thaker,Yiwei Fu,Steven Wu,Virginia Smith*

Main category: cs.LG

TL;DR: The paper introduces BLUR, a benchmark designed to evaluate machine unlearning in large language models by addressing unrealistic scenarios in prior benchmarks, offering more effective assessments of forget-retain overlap.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of ensuring effective unlearning of sensitive or harmful information from large language models while maintaining their performance on general tasks.

Method: The authors developed BLUR, a benchmark with extended evaluation tasks, combined forget/retain queries, and relearning datasets to assess unlearning methods more robustly.

Result: Evaluations using BLUR revealed significant performance drops in existing unlearning methods, with simpler methods outperforming more recent ones.

Conclusion: BLUR highlights the need for improved evaluation methodologies in machine unlearning and suggests new directions for research in the field.

Abstract: Machine unlearning has the potential to improve the safety of large language
models (LLMs) by removing sensitive or harmful information post hoc. A key
challenge in unlearning involves balancing between forget quality (effectively
unlearning undesirable information) and retain quality (maintaining good
performance on other, general tasks). Unfortunately, as we show, current LLM
unlearning benchmarks contain highly disparate forget and retain sets --
painting a false picture of the effectiveness of LLM unlearning methods. This
can be particularly problematic because it opens the door for benign
perturbations, such as relearning attacks, to easily reveal supposedly
unlearned knowledge once models are deployed. To address this, we present
$\texttt{BLUR}$: a benchmark for LLM unlearning that provides more realistic
scenarios of forget-retain overlap. $\texttt{BLUR}$ significantly expands on
existing unlearning benchmarks by providing extended evaluation tasks, combined
forget/retain queries, and relearning datasets of varying degrees of
difficulty. Despite the benign nature of the queries considered, we find that
the performance of existing methods drops significantly when evaluated on
$\texttt{BLUR}$, with simple approaches performing better on average than more
recent methods. These results highlight the importance of robust evaluation and
suggest several important directions of future study. Our benchmark is publicly
available at: https://huggingface.co/datasets/forgelab/BLUR

</details>


### [266] [Contraction Actor-Critic: Contraction Metric-Guided Reinforcement Learning for Robust Path Tracking](https://arxiv.org/abs/2506.15700)
*Minjae Cho,Hiroyasu Tsukamoto,Huy Trong Tran*

Main category: cs.LG

TL;DR: This paper integrates control contraction metrics (CCMs) with reinforcement learning (RL) to address limitations in trajectory optimality and scalability in CCMs for complex systems.


<details>
  <summary>Details</summary>
Motivation: The need to address challenges in CCMs, such as absence of trajectory optimality and difficulty in constructing CCMs for high-dimensional systems with uncertainty.

Method: A contraction actor-critic (CAC) algorithm is developed, combining CCMs with RL to simultaneously learn contraction metrics and optimal tracking policies using actor-critic methods.

Result: The proposed CAC algorithm is shown to outperform established baselines and is validated through simulated and real-world robotics experiments.

Conclusion: Contraction theory can enhance RL by providing dynamics-informed feedback for optimal policy learning, demonstrating synergy for better scalability and trajectory control.

Abstract: Control contraction metrics (CCMs) provide a framework to co-synthesize a
controller and a corresponding contraction metric -- a positive-definite
Riemannian metric under which a closed-loop system is guaranteed to be
incrementally exponentially stable. However, the synthesized controller only
ensures that all the trajectories of the system converge to one single
trajectory and, as such, does not impose any notion of optimality across an
entire trajectory. Furthermore, constructing CCMs requires a known dynamics
model and non-trivial effort in solving an infinite-dimensional convex
feasibility problem, which limits its scalability to complex systems featuring
high dimensionality with uncertainty. To address these issues, we propose to
integrate CCMs into reinforcement learning (RL), where CCMs provide
dynamics-informed feedback for learning control policies that minimize
cumulative tracking error under unknown dynamics. We show that our algorithm,
called contraction actor-critic (CAC), formally enhances the capability of CCMs
to provide a set of contracting policies with the long-term optimality of RL in
a fully automated setting. Given a pre-trained dynamics model, CAC
simultaneously learns a contraction metric generator (CMG) -- which generates a
contraction metric -- and uses an actor-critic algorithm to learn an optimal
tracking policy guided by that metric. We demonstrate the effectiveness of our
algorithm relative to established baselines through extensive empirical
studies, including simulated and real-world robot experiments, and provide a
theoretical rationale for incorporating contraction theory into RL.

</details>


### [267] [Compiler-R1: Towards Agentic Compiler Auto-tuning with Reinforcement Learning](https://arxiv.org/abs/2506.15701)
*Haolin Pan,Hongyu Lin,Haoran Luo,Yang Liu,Kaichun Yao,Libo Zhang,Mingjie Xing,Yanjun Wu*

Main category: cs.LG

TL;DR: Compiler-R1 is a reinforcement learning-driven framework that enhances large language models to optimize compiler auto-tuning, reducing IR instruction counts by an average of 8.46%.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the lack of high-quality reasoning datasets and limited interaction capabilities of LLMs within compiler environments for optimization tasks.

Method: The approach involves developing Compiler-R1, a framework with a specially curated reasoning dataset and a two-stage reinforcement learning training pipeline focused on outcome-based rewards to optimize performance.

Result: Compiler-R1 significantly reduces IR instruction counts by 8.46% on average compared to opt -Oz across seven datasets.

Conclusion: The results validate the potential of reinforcement learning-trained LLMs in optimizing compiler performance, offering a promising direction for further research in this domain.

Abstract: Compiler auto-tuning optimizes pass sequences to improve performance metrics
such as Intermediate Representation (IR) instruction count. Although recent
advances leveraging Large Language Models (LLMs) have shown promise in
automating compiler tuning, two significant challenges still remain: the
absence of high-quality reasoning datasets for agents training, and limited
effective interactions with the compilation environment. In this work, we
introduce Compiler-R1, the first reinforcement learning (RL)-driven framework
specifically augmenting LLM capabilities for compiler auto-tuning. Compiler-R1
features a curated, high-quality reasoning dataset and a novel two-stage
end-to-end RL training pipeline, enabling efficient environment exploration and
learning through an outcome-based reward. Extensive experiments across seven
datasets demonstrate Compiler-R1 achieving an average 8.46% IR instruction
count reduction compared to opt -Oz, showcasing the strong potential of
RL-trained LLMs for compiler optimization. Our code and datasets are publicly
available at https://github.com/Panhaolin2001/Compiler-R1.

</details>


### [268] [Minifinetuning: Low-Data Generation Domain Adaptation through Corrective Self-Distillation](https://arxiv.org/abs/2506.15702)
*Peter Belcak,Greg Heinrich,Jan Kautz,Pavlo Molchanov*

Main category: cs.LG

TL;DR: Minifinetuning (MFT) is a proposed method to combat the general performance decline of language models during domain-specific finetuning, especially in low-data scenarios.


<details>
  <summary>Details</summary>
Motivation: To address the issue of overfitting and general performance degradation when finetuning language models with limited domain-specific data.

Method: MFT employs corrective self-distillation tailored at the individual sample level and works without relying on pre-training data for replay.

Result: MFT achieves 2-10x better specialization-to-degeneralization ratios than standard finetuning, is robust to overfitting with very limited data, and outperforms parameter-efficient finetuning methods.

Conclusion: MFT is effective for domain adaptation in low-data finetuning scenarios and can be combined with other finetuning techniques for enhanced performance.

Abstract: Finetuning language models for a new domain inevitably leads to the
deterioration of their general performance. This becomes more pronounced the
more limited the finetuning data resource.
  We introduce minifinetuning (MFT), a method for language model domain
adaptation that considerably reduces the effects of overfitting-induced
degeneralization in low-data settings and which does so in the absence of any
pre-training data for replay. MFT demonstrates 2-10x more favourable
specialization-to-degeneralization ratios than standard finetuning across a
wide range of models and domains and exhibits an intrinsic robustness to
overfitting when data in the new domain is scarce and down to as little as 500
samples.
  Employing corrective self-distillation that is individualized on the sample
level, MFT outperforms parameter-efficient finetuning methods, demonstrates
replay-like degeneralization mitigation properties, and is composable with
either for a combined effect.

</details>


### [269] [Federated Incomplete Multi-view Clustering with Globally Fused Graph Guidance](https://arxiv.org/abs/2506.15703)
*Guoqing Chao,Zhenghao Zhang,Lei Meng,Jie Wen,Dianhui Chu*

Main category: cs.LG

TL;DR: This study introduces FIMCFG, a federated multi-view clustering method addressing missing data and global information challenges, using advanced feature extraction and fused graph guidance for improved clustering.


<details>
  <summary>Details</summary>
Motivation: Existing federated multi-view clustering approaches face limitations such as neglecting global information in feature extraction and limited exploration of the missing data problem.

Method: The method involves a dual-head graph convolutional encoder at each client for feature extraction, fused graph guidance for feature integration, pseudo-labeling for clustering, and servers to refine graph fusion and pseudo-label computation.

Result: Experimental results demonstrate FIMCFG's effectiveness and superiority over existing methods in federated multi-view clustering.

Conclusion: FIMCFG successfully advances federated multi-view clustering by addressing critical challenges, achieving better performance, and providing publicly accessible code for further investigation.

Abstract: Federated multi-view clustering has been proposed to mine the valuable
information within multi-view data distributed across different devices and has
achieved impressive results while preserving the privacy. Despite great
progress, most federated multi-view clustering methods only used global
pseudo-labels to guide the downstream clustering process and failed to exploit
the global information when extracting features. In addition, missing data
problem in federated multi-view clustering task is less explored. To address
these problems, we propose a novel Federated Incomplete Multi-view Clustering
method with globally Fused Graph guidance (FIMCFG). Specifically, we designed a
dual-head graph convolutional encoder at each client to extract two kinds of
underlying features containing global and view-specific information.
Subsequently, under the guidance of the fused graph, the two underlying
features are fused into high-level features, based on which clustering is
conducted under the supervision of pseudo-labeling. Finally, the high-level
features are uploaded to the server to refine the graph fusion and
pseudo-labeling computation. Extensive experimental results demonstrate the
effectiveness and superiority of FIMCFG. Our code is publicly available at
https://github.com/PaddiHunter/FIMCFG.

</details>


### [270] [Joint Tensor-Train Parameterization for Efficient and Expressive Low-Rank Adaptation](https://arxiv.org/abs/2506.16456)
*Jun Qi,Chen-Yu Liu,Sabato Marco Siniscalchi,Chao-Han Huck Yang,Min-Hsiu Hsieh*

Main category: cs.LG

TL;DR: This paper introduces TensorGuide, a tensor-train-guided framework to enhance Low-Rank Adaptation (LoRA) for neural models, improving expressivity, generalization, and efficiency using controlled Gaussian noise.


<details>
  <summary>Details</summary>
Motivation: Standard LoRA lacks sufficient expressivity and generalization due to independently optimized low-rank matrices. The authors aim to address these inherent limitations.

Method: TensorGuide creates two correlated low-rank LoRA matrices using a unified tensor-train structure aided by controlled Gaussian noise, leading to structured, low-rank adaptations.

Result: TensorGuide significantly improves expressivity, generalization, and parameter efficiency, outperforming standard LoRA and TT-LoRA in experiments on quantum dot classification and GPT-2 fine-tuning benchmarks.

Conclusion: TensorGuide enhances the performance of LoRA by leveraging tensor-train structures, achieving better accuracy, scalability, and optimization dynamics without increasing trainable parameters.

Abstract: Low-Rank Adaptation (LoRA) is widely recognized for its parameter-efficient
fine-tuning of large-scale neural models. However, standard LoRA independently
optimizes low-rank matrices, which inherently limits its expressivity and
generalization capabilities. While classical tensor-train (TT) decomposition
can be separately employed on individual LoRA matrices, this work demonstrates
that the classical TT-based approach neither significantly improves parameter
efficiency nor achieves substantial performance gains. This paper proposes
TensorGuide, a novel tensor-train-guided adaptation framework to overcome these
limitations. TensorGuide generates two correlated low-rank LoRA matrices
through a unified TT structure driven by controlled Gaussian noise. The
resulting joint TT representation inherently provides structured, low-rank
adaptations, significantly enhancing expressivity, generalization, and
parameter efficiency without increasing the number of trainable parameters.
Theoretically, we justify these improvements through neural tangent kernel
analyses, demonstrating superior optimization dynamics and enhanced
generalization. Extensive experiments on quantum dot classification and GPT-2
fine-tuning benchmarks demonstrate that TensorGuide-based LoRA consistently
outperforms standard LoRA and TT-LoRA, achieving improved accuracy and
scalability with fewer parameters.

</details>


### [271] [Learn from the Past: Fast Sparse Indexing for Large Language Model Decoding](https://arxiv.org/abs/2506.15704)
*Feiyu Yao,Qian Wang*

Main category: cs.LG

TL;DR: The paper introduces LFPS (Learn From the Past for Sparse Indexing), a method for improving memory efficiency during long-context inference in large language models by exploiting historical attention patterns.


<details>
  <summary>Details</summary>
Motivation: Large language models face memory and computational bottlenecks as their context lengths increase, specifically due to the memory demand for key-value (KV) caches during decoding.

Method: LFPS leverages historical attention patterns (vertical and slash patterns) to dynamically create sparse indexing candidates, reducing the need for computationally expensive operations during decoding.

Result: The proposed LFPS method achieves up to a 22.8x speedup over full attention and a 9.6x speedup over exact Top-k retrieval on standard hardware, while maintaining generation accuracy.

Conclusion: LFPS is a practical and efficient method for optimizing long-context inference in large language models, effectively balancing computational efficiency and accuracy.

Abstract: As large language models (LLMs) continue to support increasingly longer
contexts, the memory demand for key-value (KV) caches during decoding grows
rapidly, becoming a critical bottleneck in both GPU memory capacity and PCIe
bandwidth. Sparse attention mechanisms alleviate this issue by computing
attention weights only for selected key-value pairs. However, their indexing
computation typically requires traversing all key vectors, resulting in
significant computational and data transfer overhead. To reduce the cost of
index retrieval, existing methods often treat each decoding step as an
independent process, failing to exploit the temporal correlations embedded in
historical decoding information. To this end, we propose LFPS(Learn From the
Past for Sparse Indexing), an acceleration method that dynamically constructs
sparse indexing candidates based on historical attention patterns. LFPS
captures two prevalent trends in decoder attention -vertical patterns
(attending to fixed positions) and slash patterns (attending to relative
positions) -and incorporates a positional expansion strategy to effectively
predict the Top-k indices for the current step. We validate LFPS on challenging
long-context benchmarks such as LongBench-RULER, using Llama-3.1-8B-Instruct as
the base model. Experimental results show that LFPS achieves up to 22.8$\times$
speedup over full attention and 9.6$\times$ speedup over exact Top-k retrieval
on an RTX 4090 GPU and a single CPU core of a Xeon Gold 6430, respectively,
while preserving generation accuracy. These results demonstrate that LFPS
offers a practical and efficient solution for decoding optimization in
long-context LLM inference.

</details>


### [272] [Generalisation Bounds of Zero-Shot Economic Forecasting using Time Series Foundation Models](https://arxiv.org/abs/2506.15705)
*Jittarin Jetwiriyanon,Teo Susnjak,Surangika Ranathunga*

Main category: cs.LG

TL;DR: The paper examines the zero-shot forecasting abilities of Time Series Foundation Models (TSFMs) for macroeconomic indicators, comparing their performance to traditional methods.


<details>
  <summary>Details</summary>
Motivation: To explore whether advanced TSFMs can serve as reliable tools for forecasting macroeconomic indicators without extensive customization or training data.

Method: Analyzed three state-of-the-art TSFMs—Chronos, TimeGPT, and Moirai—under conditions of scarce data and structural shifts, comparing their performance to multivariate models.

Result: TSFMs demonstrated comparable performance to advanced multivariate models during stable conditions but showed vulnerability to rapid economic shocks.

Conclusion: TSFMs can be effective for zero-shot deployment in macroeconomic forecasting during stable periods but require caution under shock-prone conditions.

Abstract: This study investigates zero-shot forecasting capabilities of Time Series
Foundation Models (TSFMs) for macroeconomic indicators. We apply TSFMs to
forecasting economic indicators under univariate conditions, bypassing the need
for train bespoke econometric models using and extensive training datasets. Our
experiments were conducted on a case study dataset, without additional
customisation. We rigorously back-tested three state-of-the-art TSFMs (Chronos,
TimeGPT and Moirai) under data-scarce conditions and structural breaks. Our
results demonstrate that appropriately engineered TSFMs can internalise rich
economic dynamics, accommodate regime shifts, and deliver well-behaved
uncertainty estimates out of the box, while matching state-of-the-art
multivariate models on this domain. Our findings suggest that, without any
fine-tuning, TSFMs can match or exceed classical models during stable economic
conditions. However, they are vulnerable to degradation in performances during
periods of rapid shocks. The findings offer guidance to practitioners on when
zero-shot deployments are viable for macroeconomic monitoring and strategic
planning.

</details>


### [273] [A Free Probabilistic Framework for Analyzing the Transformer-based Language Models](https://arxiv.org/abs/2506.16550)
*Swagatam Das*

Main category: cs.LG

TL;DR: The paper proposes an operator-theoretic approach using free probability theory to analyze the internal mechanics of transformer models.


<details>
  <summary>Details</summary>
Motivation: The motivation is to deepen our understanding of how transformer-based language models function, particularly their inductive biases, generalization abilities, and internal representations.

Method: The method involves representing token embeddings and attention mechanisms as self-adjoint operators in a free probability space and analyzing their behavior using non-commutative harmonic analysis.

Result: The study uncovers a spectral dynamical system driving transformers, derives a generalization bound with free entropy, and shows that layer-wise spectral traces evolve predictably.

Conclusion: This framework connects the architecture of transformers with mathematical tools for understanding their information flow and complexity, paving the way for a more principled analysis of large language models.

Abstract: We outline an operator-theoretic framework for analyzing transformer-based
language models using the tools of free probability theory. By representing
token embeddings and attention mechanisms as self-adjoint operators in a racial
probability space, we reinterpret attention as a non-commutative convolution
and view the layer-wise propagation of representations as an evolution governed
by free additive convolution. This formalism reveals a spectral dynamical
system underpinning deep transformer stacks and offers insight into their
inductive biases, generalization behavior, and entropy dynamics. We derive a
generalization bound based on free entropy and demonstrate that the spectral
trace of transformer layers evolves predictably with depth. Our approach
bridges neural architecture with non-commutative harmonic analysis, enabling
principled analysis of information flow and structural complexity in large
language models

</details>


### [274] [MDPO: Multi-Granularity Direct Preference Optimization for Mathematical Reasoning](https://arxiv.org/abs/2506.15706)
*Yunze Lin*

Main category: cs.LG

TL;DR: The paper introduces Multi-Granularity Direct Preference Optimization (MDPO) to improve LLMs' mathematical reasoning by targeting errors at three levels: solutions, reasoning steps, and computational accuracy, outperforming existing methods on key benchmarks.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with mathematical reasoning, particularly in ensuring the correctness of reasoning steps in long-chain problems. Existing methods like DPO fail to reliably distinguish between correct and incorrect outputs in long-chain scenarios.

Method: The authors propose MDPO, an approach that optimizes reasoning at three granularities: entire solution correctness, logical reasoning between steps, and computational accuracy within steps. It unifies the training objectives across these granularities to better align with generation metrics.

Result: MDPO was tested on Qwen2 and Llama3 models, showing improvements of 1.7% and 0.9% on the GSM8K dataset, and 2.3% and 1.2% on the MATH dataset, outperforming DPO and its variants.

Conclusion: MDPO effectively enhances the mathematical reasoning capabilities of LLMs and surpasses prior optimization methods. The paper also provides a cost-efficient pipeline for constructing training data without manual annotation.

Abstract: Mathematical reasoning presents a significant challenge for Large Language
Models (LLMs) as it requires ensuring the correctness of each reasoning step.
Researchers have been strengthening the mathematical reasoning abilities of
LLMs through supervised fine-tuning, but due to the inability to suppress
incorrect outputs, illusions can easily arise. Recently, Direct Preference
Optimization (DPO) has been widely adopted for aligning human intent by using
preference data to prevent LLMs from generating incorrect outputs. However, it
has shown limited benefits in long-chain mathematical reasoning, mainly because
DPO struggles to effectively capture the differences between accepted and
rejected answers from preferences in long-chain data. The inconsistency between
DPO training and LLMs' generation metrics also affects the effectiveness of
suppressing incorrect outputs. We propose the Multi-Granularity Direct
Preference Optimization (MDPO) method, optimizing the mathematical reasoning of
LLMs at three granularities: Solution2Solution, Inference2Inference, and
Step2Step. Solution2Solution focuses on the correctness of entire long-chain
reasoning; Inference2Inference concentrates on logical reasoning between steps;
Step2Step corrects computational errors in steps, enhancing the computational
capabilities of LLMs. Additionally, we unify the training objectives of the
three granularities to align with the generation metrics. We conducted
experiments on the open-source models Qwen2 and Llama3, achieving improvements
of 1.7% and 0.9% on the GSM8K dataset, and 2.3% and 1.2% on the MATH dataset,
outperforming DPO and other DPO variant methods. Furthermore, we also provide a
pipeline for constructing MDPO training data that is simple and does not
require manual annotation costs.

</details>


### [275] [Every Rollout Counts: Optimal Resource Allocation for Efficient Test-Time Scaling](https://arxiv.org/abs/2506.15707)
*Xinglin Wang,Yiwei Li,Shaoxiong Feng,Peiwen Yuan,Yueqi Zhang,Jiayi Shi,Chuyi Tan,Boyuan Pan,Yao Hu,Kan Li*

Main category: cs.LG

TL;DR: This paper introduces DORA, an optimal strategy for allocating resources during test-time scaling in Large Language Models, improving accuracy in mathematical reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: The authors aim to enhance the test-time scaling in LLMs by addressing inefficient resource allocation during inference, a significant challenge hampering the potential of search-based performance improvements.

Method: DORA solves the resource allocation problem during test-time search by decoupling reasoning direction quality from the number of candidates and reallocating resources at the direction level for optimal search efficiency.

Result: Experiments on mathematical benchmarks like MATH500 and AIME reveal that DORA outperforms existing baselines in accuracy while maintaining equivalent computational costs.

Conclusion: DORA improves test-time scaling efficiency and accuracy in LLMs, contributing insights into optimal inference strategies and advancing state-of-the-art performance in mathematical reasoning.

Abstract: Test-Time Scaling (TTS) improves the performance of Large Language Models
(LLMs) by using additional inference-time computation to explore multiple
reasoning paths through search. Yet how to allocate a fixed rollout budget most
effectively during search remains underexplored, often resulting in inefficient
use of compute at test time. To bridge this gap, we formulate test-time search
as a resource allocation problem and derive the optimal allocation strategy
that maximizes the probability of obtaining a correct solution under a fixed
rollout budget. Within this formulation, we reveal a core limitation of
existing search methods: solution-level allocation tends to favor reasoning
directions with more candidates, leading to theoretically suboptimal and
inefficient use of compute. To address this, we propose Direction-Oriented
Resource Allocation (DORA), a provably optimal method that mitigates this bias
by decoupling direction quality from candidate count and allocating resources
at the direction level. To demonstrate DORA's effectiveness, we conduct
extensive experiments on challenging mathematical reasoning benchmarks
including MATH500, AIME2024, and AIME2025. The empirical results show that DORA
consistently outperforms strong baselines with comparable computational cost,
achieving state-of-the-art accuracy. We hope our findings contribute to a
broader understanding of optimal TTS for LLMs.

</details>


### [276] [Learning Causally Predictable Outcomes from Psychiatric Longitudinal Data](https://arxiv.org/abs/2506.16629)
*Eric V. Strobl*

Main category: cs.LG

TL;DR: The paper introduces DEBIAS, an algorithm that enhances causal inference in psychiatric data by optimizing outcome definitions to reduce confounding.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of accurate treatment effect estimation in psychiatric data complicated by symptom heterogeneity and latent confounding.

Method: The DEBIAS algorithm learns clinically interpretable weights for outcome aggregation, maximizing causal identifiability and minimizing confounding via time-limited treatment effects.

Result: DEBIAS surpasses existing methods in recovering causal effects for composite outcomes in depression and schizophrenia datasets.

Conclusion: By optimizing outcome definitions and providing tests for unconfoundedness, DEBIAS advances causal inference methodologies in psychiatry.

Abstract: Causal inference in longitudinal biomedical data remains a central challenge,
especially in psychiatry, where symptom heterogeneity and latent confounding
frequently undermine classical estimators. Most existing methods for treatment
effect estimation presuppose a fixed outcome variable and address confounding
through observed covariate adjustment. However, the assumption of
unconfoundedness may not hold for a fixed outcome in practice. To address this
foundational limitation, we directly optimize the outcome definition to
maximize causal identifiability. Our DEBIAS (Durable Effects with
Backdoor-Invariant Aggregated Symptoms) algorithm learns non-negative,
clinically interpretable weights for outcome aggregation, maximizing durable
treatment effects and empirically minimizing both observed and latent
confounding by leveraging the time-limited direct effects of prior treatments
in psychiatric longitudinal data. The algorithm also furnishes an empirically
verifiable test for outcome unconfoundedness. DEBIAS consistently outperforms
state-of-the-art methods in recovering causal effects for clinically
interpretable composite outcomes across comprehensive experiments in depression
and schizophrenia.

</details>


### [277] [Refined Causal Graph Structure Learning via Curvature for Brain Disease Classification](https://arxiv.org/abs/2506.15708)
*Falih Gozi Febrinanto,Adonia Simango,Chengpei Xu,Jingjing Zhou,Jiangang Ma,Sonika Tyagi,Feng Xia*

Main category: cs.LG

TL;DR: The paper introduces CGB, a framework using causal relationships and geometric curvature adjustments to advance brain disease classification performance.


<details>
  <summary>Details</summary>
Motivation: Current GNNs for detecting brain diseases overlook causal factor relationships in brain ROIs, which are crucial for understanding cause-and-effect interactions.

Method: CGB combines causal discovery, transfer entropy, and geometric curvature strategies to model and refine causal brain graphs for enhanced disease classification.

Result: Extensive experiments affirm CGB's superior performance over state-of-the-art methods, evaluated via average F1 scores.

Conclusion: Incorporating causal relationships and graph refinement techniques improves the effectiveness of GNNs in brain disease classification.

Abstract: Graph neural networks (GNNs) have been developed to model the relationship
between regions of interest (ROIs) in brains and have shown significant
improvement in detecting brain diseases. However, most of these frameworks do
not consider the intrinsic relationship of causality factor between brain ROIs,
which is arguably more essential to observe cause and effect interaction
between signals rather than typical correlation values. We propose a novel
framework called CGB (Causal Graphs for Brains) for brain disease
classification/detection, which models refined brain networks based on the
causal discovery method, transfer entropy, and geometric curvature strategy.
CGB unveils causal relationships between ROIs that bring vital information to
enhance brain disease classification performance. Furthermore, CGB also
performs a graph rewiring through a geometric curvature strategy to refine the
generated causal graph to become more expressive and reduce potential
information bottlenecks when GNNs model it. Our extensive experiments show that
CGB outperforms state-of-the-art methods in classification tasks on brain
disease datasets, as measured by average F1 scores.

</details>


### [278] [Studying and Improving Graph Neural Network-based Motif Estimation](https://arxiv.org/abs/2506.15709)
*Pedro C. Vieira,Miguel E. P. Silva,Pedro Manuel Pinto Ribeiro*

Main category: cs.LG

TL;DR: This paper explores using Graph Neural Networks (GNNs) for network motif significance-profile (SP) prediction, introducing a new approach for direct SP estimation as a multitarget regression task.


<details>
  <summary>Details</summary>
Motivation: Currently, GNNs have limited focus on motif SP prediction, and there are no standard benchmarks addressing this task in the literature.

Method: The study reformulates the SP estimation problem into direct SP prediction as a multitarget regression problem, optimized for interpretability, stability, and scalability on large graphs.

Result: Experiments highlight the limitations of 1-WL models in precise SP estimation but show their potential to approximate graph generation processes by comparing predicted SPs with those from synthetic generators.

Conclusion: This study lays the foundation for GNN-based motif SP estimation and illustrates that direct SP estimation can potentially overcome theoretical limitations of the traditional subgraph counting approach.

Abstract: Graph Neural Networks (GNNs) are a predominant method for graph
representation learning. However, beyond subgraph frequency estimation, their
application to network motif significance-profile (SP) prediction remains
under-explored, with no established benchmarks in the literature. We propose to
address this problem, framing SP estimation as a task independent of subgraph
frequency estimation. Our approach shifts from frequency counting to direct SP
estimation and modulates the problem as multitarget regression. The
reformulation is optimised for interpretability, stability and scalability on
large graphs. We validate our method using a large synthetic dataset and
further test it on real-world graphs. Our experiments reveal that 1-WL limited
models struggle to make precise estimations of SPs. However, they can
generalise to approximate the graph generation processes of networks by
comparing their predicted SP with the ones originating from synthetic
generators. This first study on GNN-based motif estimation also hints at how
using direct SP estimation can help go past the theoretical limitations that
motif estimation faces when performed through subgraph counting.

</details>


### [279] [Private Training & Data Generation by Clustering Embeddings](https://arxiv.org/abs/2506.16661)
*Felix Zhou,Samson Zhou,Vahab Mirrokni,Alessandro Epasto,Vincent Cohen-Addad*

Main category: cs.LG

TL;DR: This paper introduces a differential privacy (DP) method for generating synthetic image embeddings using a Gaussian Mixture Model (GMM), achieving state-of-the-art classification accuracy and scalability.


<details>
  <summary>Details</summary>
Motivation: Address privacy concerns in training deep neural networks, which may unintentionally memorize sensitive datasets, by creating synthetic datasets that approximate real input while preserving privacy.

Method: Develop a novel differential privacy method using Gaussian Mixture Model (GMM) in an embedding space for DP clustering to generate synthetic datasets, supported by encoder-decoder flexibility and scalable linear algorithms.

Result: State-of-the-art classification accuracy achieved on benchmark datasets using synthetically generated embeddings; synthetic images generated by the method also perform comparably to existing SOTA methods in classification tasks.

Conclusion: The method provides a scalable and versatile solution for synthesizing private training data while maintaining high classification performance, overcoming privacy challenges in machine learning.

Abstract: Deep neural networks often use large, high-quality datasets to achieve high
performance on many machine learning tasks. When training involves potentially
sensitive data, this process can raise privacy concerns, as large models have
been shown to unintentionally memorize and reveal sensitive information,
including reconstructing entire training samples. Differential privacy (DP)
provides a robust framework for protecting individual data and in particular, a
new approach to privately training deep neural networks is to approximate the
input dataset with a privately generated synthetic dataset, before any
subsequent training algorithm. We introduce a novel principled method for DP
synthetic image embedding generation, based on fitting a Gaussian Mixture Model
(GMM) in an appropriate embedding space using DP clustering. Our method
provably learns a GMM under separation conditions. Empirically, a simple
two-layer neural network trained on synthetically generated embeddings achieves
state-of-the-art (SOTA) classification accuracy on standard benchmark datasets.
Additionally, we demonstrate that our method can generate realistic synthetic
images that achieve downstream classification accuracy comparable to SOTA
methods. Our method is quite general, as the encoder and decoder modules can be
freely substituted to suit different tasks. It is also highly scalable,
consisting only of subroutines that scale linearly with the number of samples
and/or can be implemented efficiently in distributed systems.

</details>


### [280] [RAST: Reasoning Activation in LLMs via Small-model Transfer](https://arxiv.org/abs/2506.15710)
*Siru Ouyang,Xinyu Zhu,Zilin Xiao,Minhao Jiang,Yu Meng,Jiawei Han*

Main category: cs.LG

TL;DR: The study introduces RAST, an efficient technique to improve reasoning in large language models by transferring reinforcement learning-induced adjustments from smaller models, cutting down computational costs.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency and high computational requirements of scaling reinforcement learning for large language models while leveraging latent reasoning capabilities.

Method: The authors analyze token-level decoding trajectories to confirm RL-induced probability alignment across models. They propose RAST, which transfers probability adjustments trained on smaller models to larger models.

Result: Experiments on mathematical reasoning benchmarks show RAST enhances reasoning capabilities while requiring significantly less GPU memory, occasionally outperforming RL-trained models.

Conclusion: RAST demonstrates an efficient method of leveraging reinforcement learning effects in large models without substantial computational demands, offering practical scalability solutions.

Abstract: Reinforcement learning (RL) has become a powerful approach for improving the
reasoning capabilities of large language models (LLMs), as evidenced by recent
successes such as OpenAI's o1 and Deepseek-R1. However, applying RL at scale
remains intimidatingly resource-intensive, requiring multiple model copies and
extensive GPU workloads. On the other hand, while being powerful, recent
studies suggest that RL does not fundamentally endow models with new knowledge;
rather, it primarily reshapes the model's output distribution to activate
reasoning capabilities latent in the base model. Building on this insight, we
hypothesize that the changes in output probabilities induced by RL are largely
model-size invariant, opening the door to a more efficient paradigm: training a
small model with RL and transferring its induced probability shifts to larger
base models. To verify our hypothesis, we conduct a token-level analysis of
decoding trajectories and find high alignment in RL-induced output
distributions across model scales, validating our hypothesis. Motivated by
this, we propose RAST, a simple yet effective method that transfers reasoning
behaviors by injecting RL-induced probability adjustments from a small
RL-trained model into larger models. Experiments across multiple mathematical
reasoning benchmarks show that RAST substantially and consistently enhances the
reasoning capabilities of base models while requiring significantly lower GPU
memory than direct RL training, sometimes even yielding better performance than
the RL-trained counterparts. Our findings offer new insights into the nature of
RL-driven reasoning and practical strategies for scaling its benefits without
incurring its full computational cost. The project page of RAST is available at
https://ozyyshr.github.io/RAST/.

</details>


### [281] [How Many Domains Suffice for Domain Generalization? A Tight Characterization via the Domain Shattering Dimension](https://arxiv.org/abs/2506.16704)
*Cynthia Dwork,Lunjia Hu,Han Shao*

Main category: cs.LG

TL;DR: The paper investigates how many domains are needed to train a model for generalizing across both seen and unseen domains. It introduces the 'domain shattering dimension' to measure this.


<details>
  <summary>Details</summary>
Motivation: Understanding the minimal requirements for domain generalization is crucial to creating robust models that can perform well in varied, unseen environments.

Method: The study models the problem using the PAC framework and defines a new measure called the 'domain shattering dimension' to relate domain sample complexity with the existing VC dimension.

Result: The authors establish a connection between the 'domain shattering dimension' and VC dimension, showing that models learnable in standard PAC settings are also learnable in the proposed domain generalization framework.

Conclusion: The paper concludes that the new combinatorial measure, 'domain shattering dimension,' effectively characterizes domain sample complexity in a formalized manner and aligns with standard PAC learning principles.

Abstract: We study a fundamental question of domain generalization: given a family of
domains (i.e., data distributions), how many randomly sampled domains do we
need to collect data from in order to learn a model that performs reasonably
well on every seen and unseen domain in the family? We model this problem in
the PAC framework and introduce a new combinatorial measure, which we call the
domain shattering dimension. We show that this dimension characterizes the
domain sample complexity. Furthermore, we establish a tight quantitative
relationship between the domain shattering dimension and the classic VC
dimension, demonstrating that every hypothesis class that is learnable in the
standard PAC setting is also learnable in our setting.

</details>


### [282] [Shadow defense against gradient inversion attack in federated learning](https://arxiv.org/abs/2506.15711)
*Le Jiang,Liyan Ma,Guang Yang*

Main category: cs.LG

TL;DR: The paper introduces a framework for federated learning that uses interpretability to identify sensitive areas in data, enabling targeted noise injection to safeguard privacy without significantly compromising model accuracy.


<details>
  <summary>Details</summary>
Motivation: High privacy leakage risk in federated learning due to gradient inversion attacks calls for innovative defense mechanisms, especially critical in sensitive fields like healthcare.

Method: A shadow model with interpretability identifies vulnerable areas in training data, allowing sample-specific noise injection that balances privacy and model accuracy.

Result: The framework achieved improved privacy defense metrics (PSNR: 3.73, SSIM: 0.2 for ChestXRay; PSNR: 2.78, SSIM: 0.166 for EyePACS) with less than 1% reduction in F1 score compared to state-of-the-art methods.

Conclusion: The proposed method enhances privacy protection while maintaining model accuracy, demonstrating resilience across medical datasets.

Abstract: Federated learning (FL) has emerged as a transformative framework for
privacy-preserving distributed training, allowing clients to collaboratively
train a global model without sharing their local data. This is especially
crucial in sensitive fields like healthcare, where protecting patient data is
paramount. However, privacy leakage remains a critical challenge, as the
communication of model updates can be exploited by potential adversaries.
Gradient inversion attacks (GIAs), for instance, allow adversaries to
approximate the gradients used for training and reconstruct training images,
thus stealing patient privacy. Existing defense mechanisms obscure gradients,
yet lack a nuanced understanding of which gradients or types of image
information are most vulnerable to such attacks. These indiscriminate
calibrated perturbations result in either excessive privacy protection
degrading model accuracy, or insufficient one failing to safeguard sensitive
information. Therefore, we introduce a framework that addresses these
challenges by leveraging a shadow model with interpretability for identifying
sensitive areas. This enables a more targeted and sample-specific noise
injection. Specially, our defensive strategy achieves discrepancies of 3.73 in
PSNR and 0.2 in SSIM compared to the circumstance without defense on the
ChestXRay dataset, and 2.78 in PSNR and 0.166 in the EyePACS dataset. Moreover,
it minimizes adverse effects on model performance, with less than 1\% F1
reduction compared to SOTA methods. Our extensive experiments, conducted across
diverse types of medical images, validate the generalization of the proposed
framework. The stable defense improvements for FedAvg are consistently over
1.5\% times in LPIPS and SSIM. It also offers a universal defense against
various GIA types, especially for these sensitive areas in images.

</details>


### [283] [Bandwidth Selectors on Semiparametric Bayesian Networks](https://arxiv.org/abs/2506.16844)
*Victor Alejandre,Concha Bielza,Pedro Larrañaga*

Main category: cs.LG

TL;DR: The paper enhances semiparametric Bayesian networks (SPBNs) by integrating advanced bandwidth selection techniques for kernel density estimators (KDEs).


<details>
  <summary>Details</summary>
Motivation: Bandwidth selection in KDEs is critical for balancing bias and variance in SPBNs, but traditional methods assume data normality, which often misrepresents real-world datasets.

Method: The paper formulates a theoretical framework for using state-of-the-art bandwidth selectors, evaluates cross-validation and plug-in selector techniques, and implements these in the PyBNesian open-source package.

Result: Experimental analyses show that advanced selectors, especially unbiased cross-validation, surpass the normal rule in density estimation and predictive performance, especially with large sample sizes.

Conclusion: The proposed selectors improve SPBNs' adaptability and learning efficiency, offering significant advantages for real-world data applications.

Abstract: Semiparametric Bayesian networks (SPBNs) integrate parametric and
non-parametric probabilistic models, offering flexibility in learning complex
data distributions from samples. In particular, kernel density estimators
(KDEs) are employed for the non-parametric component. Under the assumption of
data normality, the normal rule is used to learn the bandwidth matrix for the
KDEs in SPBNs. This matrix is the key hyperparameter that controls the
trade-off between bias and variance. However, real-world data often deviates
from normality, potentially leading to suboptimal density estimation and
reduced predictive performance. This paper first establishes the theoretical
framework for the application of state-of-the-art bandwidth selectors and
subsequently evaluates their impact on SPBN performance. We explore the
approaches of cross-validation and plug-in selectors, assessing their
effectiveness in enhancing the learning capability and applicability of SPBNs.
To support this investigation, we have extended the open-source package
PyBNesian for SPBNs with the additional bandwidth selection techniques and
conducted extensive experimental analyses. Our results demonstrate that the
proposed bandwidth selectors leverage increasing information more effectively
than the normal rule, which, despite its robustness, stagnates with more data.
In particular, unbiased cross-validation generally outperforms the normal rule,
highlighting its advantage in high sample size scenarios.

</details>


### [284] [BatteryBERT for Realistic Battery Fault Detection Using Point-Masked Signal Modeling](https://arxiv.org/abs/2506.15712)
*Songqi Zhou,Ruixue Liu,Yixing Wang,Jia Lu,Benben Jiang*

Main category: cs.LG

TL;DR: The paper adapts a BERT-style pretraining framework for fault detection in lithium-ion batteries using time-series data and shows significant accuracy improvement over existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing fault detection methods for lithium-ion batteries struggle to handle complex temporal dependencies and cannot fully utilize the available unlabeled data.

Method: The paper introduced a customized time-series-to-token representation module and a point-level Masked Signal Modeling (point-MSM) task to enable self-supervised learning for battery fault detection.

Result: Using a real-world dataset, their pretraining framework achieved an AUROC of 0.945, outperforming other existing approaches and enhancing both representation quality and classification accuracy.

Conclusion: The findings affirm the effectiveness of adapting BERT-style architectures for fault detection in time-series data, providing a robust solution for industrial applications.

Abstract: Accurate fault detection in lithium-ion batteries is essential for the safe
and reliable operation of electric vehicles and energy storage systems.
However, existing methods often struggle to capture complex temporal
dependencies and cannot fully leverage abundant unlabeled data. Although large
language models (LLMs) exhibit strong representation capabilities, their
architectures are not directly suited to the numerical time-series data common
in industrial settings. To address these challenges, we propose a novel
framework that adapts BERT-style pretraining for battery fault detection by
extending the standard BERT architecture with a customized time-series-to-token
representation module and a point-level Masked Signal Modeling (point-MSM)
pretraining task tailored to battery applications. This approach enables
self-supervised learning on sequential current, voltage, and other
charge-discharge cycle data, yielding distributionally robust, context-aware
temporal embeddings. We then concatenate these embeddings with battery metadata
and feed them into a downstream classifier for accurate fault classification.
Experimental results on a large-scale real-world dataset show that models
initialized with our pretrained parameters significantly improve both
representation quality and classification accuracy, achieving an AUROC of 0.945
and substantially outperforming existing approaches. These findings validate
the effectiveness of BERT-style pretraining for time-series fault detection.

</details>


### [285] [The Importance of Being Lazy: Scaling Limits of Continual Learning](https://arxiv.org/abs/2506.16884)
*Jacopo Graldi,Alessandro Breccia,Giulia Lanzillotta,Thomas Hofmann,Lorenzo Noci*

Main category: cs.LG

TL;DR: The paper studies catastrophic forgetting (CF) in non-stationary environments, analyzing neural network scale and feature learning effects.


<details>
  <summary>Details</summary>
Motivation: To address the incomplete understanding of catastrophic forgetting and the contradictory findings related to model scale in neural networks used in continual learning.

Method: The study uses systematic experimentation and the dynamical mean field theory framework to explore the implications of neural network width and feature learning across lazy and rich training regimes.

Result: The study finds that increasing model width helps reduce feature learning in lazy regimes, clarifies CF dynamics in feature learning regimes, and introduces a critical transition that depends on task similarity.

Conclusion: Optimal neural network performance in continual learning depends on a critical balance in feature learning, which varies with task non-stationarity and is consistent across scales.

Abstract: Despite recent efforts, neural networks still struggle to learn in
non-stationary environments, and our understanding of catastrophic forgetting
(CF) is far from complete. In this work, we perform a systematic study on the
impact of model scale and the degree of feature learning in continual learning.
We reconcile existing contradictory observations on scale in the literature, by
differentiating between lazy and rich training regimes through a variable
parameterization of the architecture. We show that increasing model width is
only beneficial when it reduces the amount of feature learning, yielding more
laziness. Using the framework of dynamical mean field theory, we then study the
infinite width dynamics of the model in the feature learning regime and
characterize CF, extending prior theoretical results limited to the lazy
regime. We study the intricate relationship between feature learning, task
non-stationarity, and forgetting, finding that high feature learning is only
beneficial with highly similar tasks. We identify a transition modulated by
task similarity where the model exits an effectively lazy regime with low
forgetting to enter a rich regime with significant forgetting. Finally, our
findings reveal that neural networks achieve optimal performance at a critical
level of feature learning, which depends on task non-stationarity and transfers
across model scales. This work provides a unified perspective on the role of
scale and feature learning in continual learning.

</details>


### [286] [An application of machine learning to the motion response prediction of floating assets](https://arxiv.org/abs/2506.15713)
*Michael T. M. B. Morris-Thomas,Marius Martens*

Main category: cs.LG

TL;DR: The study introduces a machine learning approach to predict nonlinear motion responses of turret-moored vessels under varying metocean conditions, with superior accuracy compared to traditional methods.


<details>
  <summary>Details</summary>
Motivation: Predicting floating offshore asset behavior in real-time is challenging due to extreme sea states and nonlinear responses, which traditional methods struggle to handle.

Method: The study combines a gradient-boosted ensemble method with a custom solver trained on one million samples across 100 features to predict nonlinear motion responses of offshore vessels.

Result: The model achieved less than 5% error for critical mooring parameters and 2.5-degree accuracy in vessel heading, outperforming traditional methods.

Conclusion: The framework demonstrates effectiveness for real-time offshore vessel behavior prediction and operational decision-making, and has been successfully deployed in an operational facility.

Abstract: The real-time prediction of floating offshore asset behavior under stochastic
metocean conditions remains a significant challenge in offshore engineering.
While traditional empirical and frequency-domain methods work well in benign
conditions, they struggle with both extreme sea states and nonlinear responses.
This study presents a supervised machine learning approach using multivariate
regression to predict the nonlinear motion response of a turret-moored vessel
in 400 m water depth. We developed a machine learning workflow combining a
gradient-boosted ensemble method with a custom passive weathervaning solver,
trained on approximately $10^6$ samples spanning 100 features. The model
achieved mean prediction errors of less than 5% for critical mooring parameters
and vessel heading accuracy to within 2.5 degrees across diverse metocean
conditions, significantly outperforming traditional frequency-domain methods.
The framework has been successfully deployed on an operational facility,
demonstrating its efficacy for real-time vessel monitoring and operational
decision-making in offshore environments.

</details>


### [287] [RocketStack: A level-aware deep recursive ensemble learning framework with exploratory feature fusion and model pruning dynamics](https://arxiv.org/abs/2506.16965)
*Çağatay Demirel*

Main category: cs.LG

TL;DR: RocketStack introduces a deep recursive ensemble framework for machine learning that balances complexity with depth by pruning weaker models and compressing features, improving performance across datasets.


<details>
  <summary>Details</summary>
Motivation: Addressing the rarity of deep stacking in ensemble learning due to challenges like model complexity, feature redundancy, and computational burden.

Method: The paper introduces RocketStack, which incrementally prunes weaker learners at each level and applies techniques like Gaussian noise randomization and periodic feature compression using attention methods, SFE filters, and autoencoders.

Result: Tests across 33 datasets showed improved accuracy with increasing depth using the framework. Binary tasks achieved a 5.1% accuracy gain and 10.5% runtime reduction, while multi-class tasks saw a 6.1% accuracy improvement and 56% runtime reduction.

Conclusion: RocketStack, inspired by multistage rockets, enables efficient deep stacking by combining pruning, compression, and depth, making recursive ensembling more feasible and effective.

Abstract: Ensemble learning remains a cornerstone of machine learning, with stacking
used to integrate predictions from multiple base learners through a meta-model.
However, deep stacking remains rare, as most designs prioritize horizontal
diversity over recursive depth due to model complexity, feature redundancy, and
computational burden. To address these challenges, RocketStack, a level-aware
recursive ensemble framework, is introduced and explored up to ten stacking
levels, extending beyond prior architectures. The framework incrementally
prunes weaker learners at each level, enabling deeper stacking without
excessive complexity. To mitigate early performance saturation, mild Gaussian
noise is added to out-of-fold (OOF) scores before pruning, and compared against
strict OOF pruning. Further both per-level and periodic feature compressions
are explored using attention-based selection, Simple, Fast, Efficient (SFE)
filter, and autoencoders. Across 33 datasets (23 binary, 10 multi-class),
linear-trend tests confirmed rising accuracy with depth in most variants, and
the top performing meta-model at each level increasingly outperformed the
strongest standalone ensemble. In the binary subset, periodic SFE with mild
OOF-score randomization reached 97.08% at level 10, 5.14% above the
strict-pruning configuration and cut runtime by 10.5% relative to no
compression. In the multi-class subset, periodic attention selection reached
98.60% at level 10, exceeding the strongest baseline by 6.11%, while reducing
runtime by 56.1% and feature dimensionality by 74% compared to no compression.
These findings highlight mild randomization as an effective regularizer and
periodic compression as a stabilizer. Echoing the design of multistage rockets
in aerospace (prune, compress, propel) RocketStack achieves deep recursive
ensembling with tractable complexity.

</details>


### [288] [Adaptive Two Sided Laplace Transforms: A Learnable, Interpretable, and Scalable Replacement for Self-Attention](https://arxiv.org/abs/2506.15714)
*Andrew Kiruluta*

Main category: cs.LG

TL;DR: This paper proposes a novel two-sided short-time Laplace transform (STLT) mechanism to replace self-attention in transformers, achieving efficient long-sequence processing with similar or superior performance.


<details>
  <summary>Details</summary>
Motivation: The study aims to address computational bottlenecks in self-attention mechanisms for transformers, particularly for ultra-long-sequence language modeling, by introducing a more efficient and interpretable approach.

Method: They introduce a learnable STLT mechanism with trainable parameters for decay rates, oscillatory frequencies, and window bandwidth, combined with fast recursive convolution and FFT-based computations for efficiency.

Result: Their method demonstrated competitive perplexities and superior performance in benchmarks like WikiText-103, WMT'14 En-De, and NarrativeQA, while supporting context lengths exceeding 100k tokens.

Conclusion: The proposed STLT mechanism is scalable, robust, and interpretable, offering an effective solution for long-sequence modeling without the constraints of traditional self-attention.

Abstract: We propose an innovative, learnable two-sided short-time Laplace transform
(STLT) mechanism to supplant the traditional self attention in
transformer-based LLMs. Our STLT introduces trainable parameters for each
Laplace node, enabling end-to-end learning of decay rates , oscillatory
frequencies, and window bandwidth T. This flexibility allows the model to
dynamically adapt token relevance half lives and frequency responses during
training. By selecting S learnable nodes and leveraging fast recursive
convolution, we achieve an effective complexity of in time and memory. We
further incorporate an efficient FFT-based computation of the relevance matrix
and an adaptive node allocation mechanism to dynamically adjust the number of
active Laplace nodes. Empirical results on language modeling (WikiText\-103,
Project Gutenberg), machine translation (WMT'14 En\-De), and long document
question answering (NarrativeQA) demonstrate that our learnable STLT achieves
perplexities and scores on par with or better than existing efficient
transformers while naturally extending to context lengths exceeding 100k tokens
or more limited only by available hardware. Ablation studies confirm the
importance of learnable parameters and adaptive node allocation. The proposed
approach combines interpretability, through explicit decay and frequency
parameters, with scalability and robustness, offering a pathway towards
ultra-long-sequence language modeling without the computational bottleneck of
self-attention.

</details>


### [289] [NeuronSeek: On Stability and Expressivity of Task-driven Neurons](https://arxiv.org/abs/2506.15715)
*Hanyu Pei,Jing-Xiao Liao,Qibin Zhao,Ting Gao,Shijun Zhang,Xiaoge Zhang,Feng-Lei Fan*

Main category: cs.LG

TL;DR: The paper introduces a new framework, NeuronSeek-TD, using tensor decomposition to optimize neuron design, offering faster convergence and stability.


<details>
  <summary>Details</summary>
Motivation: To improve the task-specific capabilities of deep learning models by optimizing neuron formulations inspired by the functionality of human brain neurons.

Method: The method uses tensor decomposition (TD) instead of symbolic regression (SR) to determine optimal neuron designs, combined with theoretical guarantees for approximation capabilities.

Result: NeuronSeek-TD showed superior stability and competitive performance against state-of-the-art models across multiple datasets.

Conclusion: Replacing symbolic regression with tensor decomposition improves the efficiency and stability of task-driven neurons, backed by theoretical validation and empirical success.

Abstract: Drawing inspiration from our human brain that designs different neurons for
different tasks, recent advances in deep learning have explored modifying a
network's neurons to develop so-called task-driven neurons. Prototyping
task-driven neurons (referred to as NeuronSeek) employs symbolic regression
(SR) to discover the optimal neuron formulation and construct a network from
these optimized neurons. Along this direction, this work replaces symbolic
regression with tensor decomposition (TD) to discover optimal neuronal
formulations, offering enhanced stability and faster convergence. Furthermore,
we establish theoretical guarantees that modifying the aggregation functions
with common activation functions can empower a network with a fixed number of
parameters to approximate any continuous function with an arbitrarily small
error, providing a rigorous mathematical foundation for the NeuronSeek
framework. Extensive empirical evaluations demonstrate that our NeuronSeek-TD
framework not only achieves superior stability, but also is competitive
relative to the state-of-the-art models across diverse benchmarks. The code is
available at https://github.com/HanyuPei22/NeuronSeek.

</details>


### [290] [Identifiability of Deep Polynomial Neural Networks](https://arxiv.org/abs/2506.17093)
*Konstantin Usevich,Clara Dérand,Ricardo Borsoi,Marianne Clausel*

Main category: cs.LG

TL;DR: This paper studies the identifiability properties of Polynomial Neural Networks (PNNs), revealing how layer structures impact interpretability. It establishes conditions for identifiability and connects PNNs to tensor decomposition theories.


<details>
  <summary>Details</summary>
Motivation: The authors aim to understand the identifiability of Polynomial Neural Networks (PNNs) to ensure their interpretability, which is crucial for reliable and accurate deep learning systems.

Method: The paper employs a theoretical analysis grounded in algebraic geometry and tensor decomposition, using Kruskal-type uniqueness theorems. Constructive proofs are provided to determine both generic and parameter-dependent conditions.

Result: Key findings show that PNNs with non-increasing layer widths are identifiable under mild conditions, and encoder-decoder networks achieve identifiability if decoder widths do not grow rapidly. They also address a conjecture on the dimension of neurovarieties and provide activation degree bounds.

Conclusion: The study provides insights into the structural and parameter conditions needed for the identifiability and interpretability of PNNs, bridging theoretical concepts with practical architectural implications.

Abstract: Polynomial Neural Networks (PNNs) possess a rich algebraic and geometric
structure. However, their identifiability -- a key property for ensuring
interpretability -- remains poorly understood. In this work, we present a
comprehensive analysis of the identifiability of deep PNNs, including
architectures with and without bias terms. Our results reveal an intricate
interplay between activation degrees and layer widths in achieving
identifiability. As special cases, we show that architectures with
non-increasing layer widths are generically identifiable under mild conditions,
while encoder-decoder networks are identifiable when the decoder widths do not
grow too rapidly. Our proofs are constructive and center on a connection
between deep PNNs and low-rank tensor decompositions, and Kruskal-type
uniqueness theorems. This yields both generic conditions determined by the
architecture, and effective conditions that depend on the network's parameters.
We also settle an open conjecture on the expected dimension of PNN's
neurovarieties, and provide new bounds on the activation degrees required for
it to reach its maximum.

</details>


### [291] [Alternates, Assemble! Selecting Optimal Alternates for Citizens' Assemblies](https://arxiv.org/abs/2506.15716)
*Angelos Assos,Carmel Baharav,Bailey Flanigan,Ariel Procaccia*

Main category: cs.LG

TL;DR: The paper introduces an optimization framework to better handle participant attrition in citizens' assemblies by selecting alternates using a machine learning approach, ensuring improved population representation.


<details>
  <summary>Details</summary>
Motivation: Citizens' assemblies are key to deliberative democracy, but participant dropout threatens the representative nature and legitimacy of these groups.

Method: The authors developed a learning-theoretic optimization framework that estimates dropout probabilities based on historical data and selects alternates to minimize misrepresentation.

Result: Theoretical guarantees are provided for sample complexity and loss bounds, and empirical evidence from real-world data shows improved representation and reduced alternate usage compared to current practices.

Conclusion: The proposed algorithm is a significant advancement for mitigating attrition-related misrepresentation in citizens' assemblies, enhancing their effectiveness and legitimacy.

Abstract: An increasingly influential form of deliberative democracy centers on
citizens' assemblies, where randomly selected people discuss policy questions.
The legitimacy of these panels hinges on their representation of the broader
population, but panelists often drop out, leading to an unbalanced composition.
Although participant attrition is mitigated in practice by alternates, their
selection is not taken into account by existing methods. To address this gap,
we introduce an optimization framework for alternate selection. Our algorithmic
approach, which leverages learning-theoretic machinery, estimates dropout
probabilities using historical data and selects alternates to minimize expected
misrepresentation. We establish theoretical guarantees for our approach,
including worst-case bounds on sample complexity (with implications for
computational efficiency) and on loss when panelists' probabilities of dropping
out are mis-estimated. Empirical evaluation using real-world data demonstrates
that, compared to the status quo, our method significantly improves
representation while requiring fewer alternates.

</details>


### [292] [daDPO: Distribution-Aware DPO for Distilling Conversational Abilities](https://arxiv.org/abs/2506.15717)
*Zhengze Zhang,Shiqi Wang,Yiqun Shen,Simin Guo,Dahua Lin,Xiaoliang Wang,Nguyen Cam-Tu,Fei Tan*

Main category: cs.LG

TL;DR: The paper introduces daDPO, a novel approach combining preference optimization and knowledge distillation to enhance the conversational performance of smaller LLMs.


<details>
  <summary>Details</summary>
Motivation: Bridging the performance gap between large and small language models in resource-constrained settings by utilizing teacher models better.

Method: The authors propose daDPO, leveraging both teacher preferences and output distributions for unified preference optimization and knowledge distillation.

Result: Using daDPO, pruned Vicuna1.5-7B achieves near-teacher performance, and Qwen2.5-1.5B occasionally exceeds its teacher's performance.

Conclusion: daDPO is superior to existing methods, showing both theoretical and practical promise in advancing efficient LLM distillation technologies.

Abstract: Large language models (LLMs) have demonstrated exceptional performance across
various applications, but their conversational abilities decline sharply as
model size decreases, presenting a barrier to their deployment in
resource-constrained environments. Knowledge distillation with Direct
Preference Optimization (dDPO) has emerged as a promising approach to enhancing
the conversational abilities of smaller models using a larger teacher model.
However, current methods primarily focus on 'black-box' KD, which only uses the
teacher's responses, overlooking the output distribution offered by the
teacher. This paper addresses this gap by introducing daDPO (Distribution-Aware
DPO), a unified method for preference optimization and distribution-based
distillation. We provide rigorous theoretical analysis and empirical
validation, showing that daDPO outperforms existing methods in restoring
performance for pruned models and enhancing smaller LLM models. Notably, in
in-domain evaluation, our method enables a 20% pruned Vicuna1.5-7B to achieve
near-teacher performance (-7.3% preference rate compared to that of dDPO's
-31%), and allows Qwen2.5-1.5B to occasionally outperform its 7B teacher model
(14.0% win rate).

</details>


### [293] [Consistent Sampling and Simulation: Molecular Dynamics with Energy-Based Diffusion Models](https://arxiv.org/abs/2506.17139)
*Michael Plainer,Hao Wu,Leon Klein,Stephan Günnemann,Frank Noé*

Main category: cs.LG

TL;DR: The paper identifies inconsistencies in diffusion models for molecular dynamics simulations and presents an energy-based diffusion model to address them, demonstrating improved performance and consistency.


<details>
  <summary>Details</summary>
Motivation: Diffusion models are effective for molecular simulations but exhibit inconsistencies between inference and simulation under small diffusion timesteps, which the study aims to resolve.

Method: The authors propose an energy-based diffusion model with Fokker-Planck-derived regularization to ensure consistency, applied to molecular systems.

Result: The proposed model improves consistency and sampling efficiency, validated on toy systems, alanine dipeptide, and dipeptides.

Conclusion: The energy-based diffusion model offers a more reliable framework for molecular dynamics, handling inconsistencies and enhancing simulation capabilities.

Abstract: Diffusion models have recently gained significant attention due to their
effectiveness in various scientific domains, including biochemistry. When
trained on equilibrium molecular distributions, diffusion models provide both:
a generative procedure to sample equilibrium conformations and associated
forces derived from the model's scores. However, using the forces for
coarse-grained molecular dynamics simulations uncovers inconsistencies in the
samples generated via classical diffusion inference and simulation, despite
both originating from the same model. Particularly at the small diffusion
timesteps required for simulations, diffusion models fail to satisfy the
Fokker-Planck equation, which governs how the score should evolve over time. We
interpret this deviation as an indication of the observed inconsistencies and
propose an energy-based diffusion model with a Fokker-Planck-derived
regularization term enforcing consistency. We demonstrate the effectiveness of
our approach on toy systems, alanine dipeptide, and introduce a
state-of-the-art transferable Boltzmann emulator for dipeptides that supports
simulation and demonstrates enhanced consistency and efficient sampling.

</details>


### [294] [BuildingBRep-11K: Precise Multi-Storey B-Rep Building Solids with Rich Layout Metadata](https://arxiv.org/abs/2506.15718)
*Yu Guo,Hongji Fang,Tianyu Fang,Zhe Cui*

Main category: cs.LG

TL;DR: The paper presents BuildingBRep-11K, a dataset of 11,978 geometrically exact 3D building models designed for AI training. It verifies the dataset's usability with PointNet baselines for geometric and quality tasks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the need for large, clean, and richly annotated datasets in the research field of automatic 3D building-scale object generation using AI.

Method: The dataset is generated using a shape-grammar-driven pipeline with architectural design constraints, ensuring output quality through multi-stage filters. Two lightweight PointNet baselines are employed for dataset evaluation: one for multi-attribute regression and another for defect detection.

Result: With the PointNet models, the dataset demonstrated its learnability by achieving promising results in regression (e.g., 0.37-storey MAE) and decent performance in defect detection (e.g., 82% recall for defects).

Conclusion: BuildingBRep-11K is a robust dataset for training AI models in 3D building design tasks, proving both learnable and sufficiently challenging for geometric and quality assessments.

Abstract: With the rise of artificial intelligence, the automatic generation of
building-scale 3-D objects has become an active research topic, yet training
such models still demands large, clean and richly annotated datasets. We
introduce BuildingBRep-11K, a collection of 11 978 multi-storey (2-10 floors)
buildings (about 10 GB) produced by a shape-grammar-driven pipeline that
encodes established building-design principles. Every sample consists of a
geometrically exact B-rep solid-covering floors, walls, slabs and rule-based
openings-together with a fast-loading .npy metadata file that records detailed
per-floor parameters. The generator incorporates constraints on spatial scale,
daylight optimisation and interior layout, and the resulting objects pass
multi-stage filters that remove Boolean failures, undersized rooms and extreme
aspect ratios, ensuring compliance with architectural standards. To verify the
dataset's learnability we trained two lightweight PointNet baselines. (i)
Multi-attribute regression. A single encoder predicts storey count, total
rooms, per-storey vector and mean room area from a 4 000-point cloud. On 100
unseen buildings it attains 0.37-storey MAE (87 \% within $\pm1$), 5.7-room
MAE, and 3.2 m$^2$ MAE on mean area. (ii) Defect detection. With the same
backbone we classify GOOD versus DEFECT; on a balanced 100-model set the
network reaches 54 \% accuracy, recalling 82 \% of true defects at 53 \%
precision (41 TP, 9 FN, 37 FP, 13 TN). These pilots show that BuildingBRep-11K
is learnable yet non-trivial for both geometric regression and topological
quality assessment

</details>


### [295] [Variational Learning of Disentangled Representations](https://arxiv.org/abs/2506.17182)
*Yuli Slavutsky,Ozgur Beker,David Blei,Bianca Dumitrascu*

Main category: cs.LG

TL;DR: DISCoVeR, a novel variational framework, enhances disentangled representations by clearly separating condition-invariant and condition-specific factors, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Generalization in domains like biomedical data analysis requires accurate disentanglement to isolate stable biological signals from context-dependent effects, which current methods struggle with due to latent representation leakage.

Method: DISCoVeR uses a dual-latent architecture, parallel reconstructions, and a max-min objective to promote disentanglement without handcrafted priors, driven by theoretical proof of equilibrium and data likelihood maximization.

Result: DISCoVeR achieves improved disentanglement on synthetic datasets, natural images, and single-cell RNA-seq data, outperforming prior methods in handling complex, multi-condition scenarios.

Conclusion: DISCoVeR is a well-founded, effective framework for disentangled representation learning in multi-condition settings, particularly beneficial for applications like biomedical data analysis.

Abstract: Disentangled representations enable models to separate factors of variation
that are shared across experimental conditions from those that are
condition-specific. This separation is essential in domains such as biomedical
data analysis, where generalization to new treatments, patients, or species
depends on isolating stable biological signals from context-dependent effects.
While extensions of the variational autoencoder (VAE) framework have been
proposed to address this problem, they frequently suffer from leakage between
latent representations, limiting their ability to generalize to unseen
conditions. Here, we introduce DISCoVeR, a new variational framework that
explicitly separates condition-invariant and condition-specific factors.
DISCoVeR integrates three key components: (i) a dual-latent architecture that
models shared and specific factors separately; (ii) two parallel
reconstructions that ensure both representations remain informative; and (iii)
a novel max-min objective that encourages clean separation without relying on
handcrafted priors, while making only minimal assumptions. Theoretically, we
show that this objective maximizes data likelihood while promoting
disentanglement, and that it admits a unique equilibrium. Empirically, we
demonstrate that DISCoVeR achieves improved disentanglement on synthetic
datasets, natural images, and single-cell RNA-seq data. Together, these results
establish DISCoVeR as a principled approach for learning disentangled
representations in multi-condition settings.

</details>


### [296] [Data-Driven Heat Pump Management: Combining Machine Learning with Anomaly Detection for Residential Hot Water Systems](https://arxiv.org/abs/2506.15719)
*Manal Rahal,Bestoun S. Ahmed,Roger Renstrom,Robert Stener,Albrecht Wurtz*

Main category: cs.LG

TL;DR: The paper addresses household hot water demand forecasting with heat pumps, using a novel machine learning approach for adaptive hot water production strategies, achieving high performance.


<details>
  <summary>Details</summary>
Motivation: To improve the efficiency of heat pumps in producing hot water, tackling limitations of conventional threshold-based control methods and employing predictive ML for household-specific optimization.

Method: The study combines ML models (LightGBM, LSTM, Bi-directional LSTM with self-attention) and isolation forest (iForest) for forecasting and anomaly detection, along with advanced time-series analysis and feature selection.

Result: The LightGBM model achieved up to 9.37% RMSE improvement over LSTM, with strong $R^2$ values (0.748-0.983). iForest delivered an F1-score of 0.87 and a false alarm rate of 5.2%, demonstrating efficiency across varied households.

Conclusion: Combining predictive ML and anomaly detection offers effective forecasting and adaptive control for household hot water production, making the approach suitable for real-world heat pump applications.

Abstract: Heat pumps (HPs) have emerged as a cost-effective and clean technology for
sustainable energy systems, but their efficiency in producing hot water remains
restricted by conventional threshold-based control methods. Although machine
learning (ML) has been successfully implemented for various HP applications,
optimization of household hot water demand forecasting remains understudied.
This paper addresses this problem by introducing a novel approach that combines
predictive ML with anomaly detection to create adaptive hot water production
strategies based on household-specific consumption patterns. Our key
contributions include: (1) a composite approach combining ML and isolation
forest (iForest) to forecast household demand for hot water and steer
responsive HP operations; (2) multi-step feature selection with advanced
time-series analysis to capture complex usage patterns; (3) application and
tuning of three ML models: Light Gradient Boosting Machine (LightGBM), Long
Short-Term Memory (LSTM), and Bi-directional LSTM with the self-attention
mechanism on data from different types of real HP installations; and (4)
experimental validation on six real household installations. Our experiments
show that the best-performing model LightGBM achieves superior performance,
with RMSE improvements of up to 9.37\% compared to LSTM variants with $R^2$
values between 0.748-0.983. For anomaly detection, our iForest implementation
achieved an F1-score of 0.87 with a false alarm rate of only 5.2\%,
demonstrating strong generalization capabilities across different household
types and consumption patterns, making it suitable for real-world HP
deployments.

</details>


### [297] [Optimal Implicit Bias in Linear Regression](https://arxiv.org/abs/2506.17187)
*Kanumuri Nithin Varma,Babak Hassibi*

Main category: cs.LG

TL;DR: The paper investigates implicit biases in optimization algorithms for over-parameterized learning problems and derives optimal biases for the best generalization performance.


<details>
  <summary>Details</summary>
Motivation: Address the implicit bias in over-parameterized machine learning models and its effect on generalization performance.

Method: Asymptotic analysis of generalization performance for interpolators from convex functions in over-parameterized linear regression with non-isotropic Gaussian data.

Result: Derives a tight lower bound on minimal generalization error based on several factors and identifies optimal implicit bias under specific distribution conditions.

Conclusion: Optimal implicit biases are critical to achieving the best possible generalization performance under certain conditions in over-parameterized scenarios.

Abstract: Most modern learning problems are over-parameterized, where the number of
learnable parameters is much greater than the number of training data points.
In this over-parameterized regime, the training loss typically has infinitely
many global optima that completely interpolate the data with varying
generalization performance. The particular global optimum we converge to
depends on the implicit bias of the optimization algorithm. The question we
address in this paper is, ``What is the implicit bias that leads to the best
generalization performance?". To find the optimal implicit bias, we provide a
precise asymptotic analysis of the generalization performance of interpolators
obtained from the minimization of convex functions/potentials for
over-parameterized linear regression with non-isotropic Gaussian data. In
particular, we obtain a tight lower bound on the best generalization error
possible among this class of interpolators in terms of the
over-parameterization ratio, the variance of the noise in the labels, the
eigenspectrum of the data covariance, and the underlying distribution of the
parameter to be estimated. Finally, we find the optimal convex implicit bias
that achieves this lower bound under certain sufficient conditions involving
the log-concavity of the distribution of a Gaussian convolved with the prior of
the true underlying parameter.

</details>


### [298] [Tripartite Weight-Space Ensemble for Few-Shot Class-Incremental Learning](https://arxiv.org/abs/2506.15720)
*Juntae Lee,Munawar Hayat,Sungrack Yun*

Main category: cs.LG

TL;DR: The paper proposes a novel approach to Few-shot class incremental learning (FSCIL) known as Tripartite Weight-space Ensemble (Tri-WE) to address catastrophic forgetting and overfitting issues. It achieves state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Few-shot class incremental learning (FSCIL) faces the challenge of forgetting prior concepts and overfitting to new examples due to limited data. Most current methods fix feature extractors, which limits model adaptability to new classes.

Method: The proposed method, Tri-WE, updates the entire model by interpolating base, previous, and current models in weight-space for classification heads. A regularization loss term using amplified data knowledge distillation is also used to improve representation learning.

Result: The method achieves state-of-the-art results on standard FSCIL benchmark datasets like miniImageNet, CUB200, and CIFAR100.

Conclusion: Tri-WE effectively addresses issues in FSCIL by enabling seamless model updates and enriching data for enhanced knowledge transfer, outperforming other methods on standard benchmarks.

Abstract: Few-shot class incremental learning (FSCIL) enables the continual learning of
new concepts with only a few training examples. In FSCIL, the model undergoes
substantial updates, making it prone to forgetting previous concepts and
overfitting to the limited new examples. Most recent trend is typically to
disentangle the learning of the representation from the classification head of
the model. A well-generalized feature extractor on the base classes (many
examples and many classes) is learned, and then fixed during incremental
learning. Arguing that the fixed feature extractor restricts the model's
adaptability to new classes, we introduce a novel FSCIL method to effectively
address catastrophic forgetting and overfitting issues. Our method enables to
seamlessly update the entire model with a few examples. We mainly propose a
tripartite weight-space ensemble (Tri-WE). Tri-WE interpolates the base,
immediately previous, and current models in weight-space, especially for the
classification heads of the models. Then, it collaboratively maintains
knowledge from the base and previous models. In addition, we recognize the
challenges of distilling generalized representations from the previous model
from scarce data. Hence, we suggest a regularization loss term using amplified
data knowledge distillation. Simply intermixing the few-shot data, we can
produce richer data enabling the distillation of critical knowledge from the
previous model. Consequently, we attain state-of-the-art results on the
miniImageNet, CUB200, and CIFAR100 datasets.

</details>


### [299] [Bohdi: Heterogeneous LLM Fusion with Automatic Data Exploration](https://arxiv.org/abs/2506.15721)
*Junqi Gao,Zhichang Guo,Dazhi Zhang,Dong Li,Runze Liu,Pengfei Li,Kai Tian,Biqing Qi*

Main category: cs.LG

TL;DR: The paper introduces Bohdi, a novel framework for integrating multiple large language models (LLMs) using only synthetic data, outperforming traditional methods in efficiency and balancing target model capabilities.


<details>
  <summary>Details</summary>
Motivation: Existing methods for LLM fusion rely heavily on real data from restricted domains and do not dynamically adjust data allocations, causing limitations in domain coverage and balanced model capabilities.

Method: Bohdi employs a knowledge organization hierarchy and uses a Hierarchical Multi-Armed Bandit approach with a DynaBranches mechanism for domain exploration and data sampling. It incorporates a Sliding Window Binomial Likelihood Ratio Testing (SWBLRT) to track and adapt to capability changes in the target LLM.

Result: Bohdi demonstrated superior performance over existing methods in various experiments, showing higher data efficiency and eliminating imbalances in model capabilities for multiple target LLMs.

Conclusion: Bohdi presents an innovative way to achieve heterogeneous LLM fusion using synthetic data, enabling broader domain coverage and dynamic capability balancing, making it a significant advancement in LLM integration.

Abstract: Heterogeneous Large Language Model (LLM) fusion integrates the strengths of
multiple source LLMs with different architectures into a target LLM with low
computational overhead. While promising, existing methods suffer from two major
limitations: 1) reliance on real data from limited domain for knowledge fusion,
preventing the target LLM from fully acquiring knowledge across diverse
domains, and 2) fixed data allocation proportions across domains, failing to
dynamically adjust according to the target LLM's varying capabilities across
domains, leading to a capability imbalance. To overcome these limitations, we
propose Bohdi, a synthetic-data-only heterogeneous LLM fusion framework.
Through the organization of knowledge domains into a hierarchical tree
structure, Bohdi enables automatic domain exploration and multi-domain data
generation through multi-model collaboration, thereby comprehensively
extracting knowledge from source LLMs. By formalizing domain expansion and data
sampling proportion allocation on the knowledge tree as a Hierarchical
Multi-Armed Bandit problem, Bohdi leverages the designed DynaBranches mechanism
to adaptively adjust sampling proportions based on the target LLM's performance
feedback across domains. Integrated with our proposed Introspection-Rebirth
(IR) mechanism, DynaBranches dynamically tracks capability shifts during target
LLM's updates via Sliding Window Binomial Likelihood Ratio Testing (SWBLRT),
further enhancing its online adaptation capability. Comparative experimental
results on a comprehensive suite of benchmarks demonstrate that Bohdi
significantly outperforms existing baselines on multiple target LLMs, exhibits
higher data efficiency, and virtually eliminates the imbalance in the target
LLM's capabilities. Our code is available at
https://github.com/gjq100/Bohdi.git.

</details>


### [300] [UniMate: A Unified Model for Mechanical Metamaterial Generation, Property Prediction, and Condition Confirmation](https://arxiv.org/abs/2506.15722)
*Wangzhi Zhan,Jianpeng Chen,Dongqi Fu,Dawei Zhou*

Main category: cs.LG

TL;DR: The paper introduces UNIMATE, an innovative unified machine learning model that aligns and integrates three critical modalities in mechanical metamaterials design: 3D topology, density, and mechanical properties.


<details>
  <summary>Details</summary>
Motivation: The motivation of the paper is to address the limitations of existing machine learning models, which typically focus on only two of the three key modalities (3D topology, density, and mechanical properties) in designing mechanical metamaterials.

Method: The authors propose UNIMATE, a model with two main components: a modality alignment module and a synergetic diffusion generation module, to simultaneously process and integrate all three modalities.

Result: Experiments show that UNIMATE significantly outperforms existing baseline models in three tasks: topology generation (by up to 80.2%), property prediction (by up to 5.1%), and condition confirmation (by up to 50.2%).

Conclusion: UNIMATE is a step forward in comprehensive machine learning models for mechanical metamaterials design, providing a robust methodology for integrating all three critical design modalities.

Abstract: Metamaterials are artificial materials that are designed to meet unseen
properties in nature, such as ultra-stiffness and negative materials indices.
In mechanical metamaterial design, three key modalities are typically involved,
i.e., 3D topology, density condition, and mechanical property. Real-world
complex application scenarios place the demanding requirements on machine
learning models to consider all three modalities together. However, a
comprehensive literature review indicates that most existing works only
consider two modalities, e.g., predicting mechanical properties given the 3D
topology or generating 3D topology given the required properties. Therefore,
there is still a significant gap for the state-of-the-art machine learning
models capturing the whole. Hence, we propose a unified model named UNIMATE,
which consists of a modality alignment module and a synergetic diffusion
generation module. Experiments indicate that UNIMATE outperforms the other
baseline models in topology generation task, property prediction task, and
condition confirmation task by up to 80.2%, 5.1%, and 50.2%, respectively. We
opensource our proposed UNIMATE model and corresponding results at
https://github.com/wzhan24/UniMate.

</details>


### [301] [MadaKV: Adaptive Modality-Perception KV Cache Eviction for Efficient Multimodal Long-Context Inference](https://arxiv.org/abs/2506.15724)
*Kunxi Li,Zhonghua Jiang,Zhouzhou Shen,Zhaode Wang,Chengfei Lv,Shengyu Zhang,Fan Wu,Fei Wu*

Main category: cs.LG

TL;DR: The paper introduces MadaKV, a modality-adaptive key-value cache eviction strategy, which improves the memory efficiency and inference speed of multimodal large language models (MLLMs) in long-context tasks.


<details>
  <summary>Details</summary>
Motivation: Multimodal scenarios face challenges in efficiently managing KV cache evictions as traditional unimodal methods do not account for modality-specific disparities, leading to suboptimal performance.

Method: MadaKV employs modality preference adaptation and hierarchical compression compensation to dynamically sense and retain key tokens specific to modalities, optimizing memory usage and inference latency.

Result: Experiments show MadaKV significantly improves decoding latency by 1.3 to 1.5 times while preserving accuracy across various long-context multimodal tasks.

Conclusion: MadaKV effectively addresses modality-specific inefficiencies in KV cache management for MLLMs, enhancing performance and memory utilization in multimodal contexts.

Abstract: This paper introduces MadaKV, a modality-adaptive key-value (KV) cache
eviction strategy designed to enhance the efficiency of multimodal large
language models (MLLMs) in long-context inference. In multimodal scenarios,
attention heads exhibit varying preferences for different modalities, resulting
in significant disparities in modality importance across attention heads.
Traditional KV cache eviction methods, which are tailored for unimodal
settings, fail to capture modality-specific information, thereby yielding
suboptimal performance. MadaKV addresses these challenges through two key
components: modality preference adaptation and hierarchical compression
compensation. By dynamically sensing modality information within attention
heads and adaptively retaining critical tokens, MadaKV achieves substantial
reductions in KV cache memory footprint and model inference decoding latency
(1.3 to 1.5 times improvement) while maintaining high accuracy across various
multimodal long-context tasks. Extensive experiments on representative MLLMs
and the MileBench benchmark demonstrate the effectiveness of MadaKV compared to
existing KV cache eviction methods.

</details>


### [302] [Graph Diffusion that can Insert and Delete](https://arxiv.org/abs/2506.15725)
*Matteo Ninniri,Marco Podda,Davide Bacciu*

Main category: cs.LG

TL;DR: The paper introduces GrIDDD, a graph diffusion generative model that supports dynamic resizing of graphs during generation, showing superior or comparable performance to existing models for molecular property targeting and optimization.


<details>
  <summary>Details</summary>
Motivation: Current graph generative models using discrete Denoising Diffusion Probabilistic Models (DDPMs) cannot adapt graph sizes during diffusion. This limitation hinders their effectiveness in tasks like property-driven molecular design, where molecular size is crucial.

Method: The authors reformulated the noising and denoising processes to enable monotonic insertion and deletion of nodes, allowing the graph to dynamically grow or shrink during the generative process.

Result: GrIDDD demonstrates comparable or superior performance to other graph diffusion models on molecular property tasks and exhibits competitive results in molecular optimization applications.

Conclusion: GrIDDD overcomes the size adaptability limitation in existing models, offering a path forward for more flexible and effective graph diffusion methods in molecular generation.

Abstract: Generative models of graphs based on discrete Denoising Diffusion
Probabilistic Models (DDPMs) offer a principled approach to molecular
generation by systematically removing structural noise through iterative atom
and bond adjustments. However, existing formulations are fundamentally limited
by their inability to adapt the graph size (that is, the number of atoms)
during the diffusion process, severely restricting their effectiveness in
conditional generation scenarios such as property-driven molecular design,
where the targeted property often correlates with the molecular size. In this
paper, we reformulate the noising and denoising processes to support monotonic
insertion and deletion of nodes. The resulting model, which we call GrIDDD,
dynamically grows or shrinks the chemical graph during generation. GrIDDD
matches or exceeds the performance of existing graph diffusion models on
molecular property targeting despite being trained on a more difficult problem.
Furthermore, when applied to molecular optimization, GrIDDD exhibits
competitive performance compared to specialized optimization models. This work
paves the way for size-adaptive molecular generation with graph diffusion.

</details>


### [303] [Descriptor-based Foundation Models for Molecular Property Prediction](https://arxiv.org/abs/2506.15792)
*Jackson Burns,Akshat Zalte,William Green*

Main category: cs.LG

TL;DR: CheMeleon is a molecular foundation model using deterministic molecular descriptors to predict molecular properties accurately and with minimal noise.


<details>
  <summary>Details</summary>
Motivation: The study aims to enhance molecular property prediction by addressing the limitations of noisy experimental data and biased quantum mechanical simulations often used in traditional models.

Method: CheMeleon leverages deterministic molecular descriptors pre-trained using the Mordred package and a Directed Message-Passing Neural Network, enabling noise-free learning of molecular representations.

Result: CheMeleon achieves a 79% win rate on Polaris tasks and a 97% win rate on MoleculeACE assays, outperforming existing models, although it struggles with activity cliffs.

Conclusion: Descriptor-based pre-training offers a scalable and effective means for molecular property prediction, with significant potential for further exploration and improvement.

Abstract: Fast and accurate prediction of molecular properties with machine learning is
pivotal to scientific advancements across myriad domains. Foundation models in
particular have proven especially effective, enabling accurate training on
small, real-world datasets. This study introduces CheMeleon, a novel molecular
foundation model pre-trained on deterministic molecular descriptors from the
Mordred package, leveraging a Directed Message-Passing Neural Network to
predict these descriptors in a noise-free setting. Unlike conventional
approaches relying on noisy experimental data or biased quantum mechanical
simulations, CheMeleon uses low-noise molecular descriptors to learn rich
molecular representations. Evaluated on 58 benchmark datasets from Polaris and
MoleculeACE, CheMeleon achieves a win rate of 79% on Polaris tasks,
outperforming baselines like Random Forest (46%), fastprop (39%), and Chemprop
(36%), and a 97% win rate on MoleculeACE assays, surpassing Random Forest (63%)
and other foundation models. However, it struggles to distinguish activity
cliffs like many of the tested models. The t-SNE projection of CheMeleon's
learned representations demonstrates effective separation of chemical series,
highlighting its ability to capture structural nuances. These results
underscore the potential of descriptor-based pre-training for scalable and
effective molecular property prediction, opening avenues for further
exploration of descriptor sets and unlabeled datasets.

</details>


### [304] [DeepJ: Graph Convolutional Transformers with Differentiable Pooling for Patient Trajectory Modeling](https://arxiv.org/abs/2506.15809)
*Deyi Li,Zijun Yao,Muxuan Liang,Mei Liu*

Main category: cs.LG

TL;DR: This paper proposes Deep Patient Journey (DeepJ), a model for capturing complex, temporal interactions in medical events from Electronic Health Records, achieving better results in outcome prediction and interpretability.


<details>
  <summary>Details</summary>
Motivation: Existing graph learning methods for EHR data are limited in modeling relationships across longitudinal patient encounters while factoring in temporal dependencies.

Method: The authors propose Deep Patient Journey (DeepJ), a graph convolutional transformer with differentiable graph pooling to analyze intra-encounter and inter-encounter medical event interactions.

Result: DeepJ outperformed five state-of-the-art models and demonstrated improved interpretability in predicting patient outcomes.

Conclusion: DeepJ provides deeper insights and better tools for patient risk stratification by modeling meaningful and temporal interactions in medical records effectively.

Abstract: In recent years, graph learning has gained significant interest for modeling
complex interactions among medical events in structured Electronic Health
Record (EHR) data. However, existing graph-based approaches often work in a
static manner, either restricting interactions within individual encounters or
collapsing all historical encounters into a single snapshot. As a result, when
it is necessary to identify meaningful groups of medical events spanning
longitudinal encounters, existing methods are inadequate in modeling
interactions cross encounters while accounting for temporal dependencies. To
address this limitation, we introduce Deep Patient Journey (DeepJ), a novel
graph convolutional transformer model with differentiable graph pooling to
effectively capture intra-encounter and inter-encounter medical event
interactions. DeepJ can identify groups of temporally and functionally related
medical events, offering valuable insights into key event clusters pertinent to
patient outcome prediction. DeepJ significantly outperformed five
state-of-the-art baseline models while enhancing interpretability,
demonstrating its potential for improved patient risk stratification.

</details>


### [305] [Optimizing Bidding Strategies in First-Price Auctions in Binary Feedback Setting with Predictions](https://arxiv.org/abs/2506.15817)
*Jason Tandiary*

Main category: cs.LG

TL;DR: This paper introduces a new algorithm using historical data and machine learning to improve regret bounds for Vickrey first-price auctions with binary feedback.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the growing importance of first-price auctions and aims to leverage machine learning to optimize performance prediction and regret minimization.

Method: A new algorithm is proposed within the BROAD-OMD framework that uses predictions about the highest competing bid to enhance performance.

Result: The algorithm achieves zero regret under accurate predictions and establishes a regret bound of O(T^(3/4) * Vt^(1/4)) under certain normal conditions.

Conclusion: The new algorithm demonstrates improved efficiency in first-price auctions, leveraging machine learning predictions to minimize regret effectively.

Abstract: This paper studies Vickrey first-price auctions under binary feedback.
Leveraging the enhanced performance of machine learning algorithms, the new
algorithm uses past information to improve the regret bounds of the BROAD-OMD
algorithm. Motivated by the growing relevance of first-price auctions and the
predictive capabilities of machine learning models, this paper proposes a new
algorithm within the BROAD-OMD framework (Hu et al., 2025) that leverages
predictions of the highest competing bid. This paper's main contribution is an
algorithm that achieves zero regret under accurate predictions. Additionally, a
bounded regret bound of O(T^(3/4) * Vt^(1/4)) is established under certain
normality conditions.

</details>


### [306] [AI-based modular warning machine for risk identification in proximity healthcare](https://arxiv.org/abs/2506.15823)
*Chiara Razzetta,Shahryar Noei,Federico Barbarossa,Edoardo Spairani,Monica Roascio,Elisa Barbi,Giulia Ciacci,Sara Sommariva,Sabrina Guastavino,Michele Piana,Matteo Lenge,Gabriele Arnulfo,Giovanni Magenes,Elvira Maranesi,Giulio Amabili,Anna Maria Massone,Federico Benvenuto,Giuseppe Jurman,Diego Sona,Cristina Campi*

Main category: cs.LG

TL;DR: The paper introduces an automated pipeline combining machine learning methods for interpreting multi-modal data in community medicine.


<details>
  <summary>Details</summary>
Motivation: To improve proximity healthcare by leveraging digital solutions and machine learning for better data interpretation and predictions.

Method: Developed a pipeline incorporating both unsupervised and supervised machine learning techniques to process and interpret multi-modal data.

Result: Successfully enabled predictive insights and facilitated model interpretation via feature identification.

Conclusion: The pipeline shows promise in enhancing data analysis and predictive modeling in digital health applications within community medicine.

Abstract: "DHEAL-COM - Digital Health Solutions in Community Medicine" is a research
and technology project funded by the Italian Department of Health for the
development of digital solutions of interest in proximity healthcare. The
activity within the DHEAL-COM framework allows scientists to gather a notable
amount of multi-modal data whose interpretation can be performed by means of
machine learning algorithms. The present study illustrates a general automated
pipeline made of numerous unsupervised and supervised methods that can ingest
such data, provide predictive results, and facilitate model interpretations via
feature identification.

</details>


### [307] [Heterogeneous Federated Reinforcement Learning Using Wasserstein Barycenters](https://arxiv.org/abs/2506.15825)
*Luiz Pereira,M. Hadi Amini*

Main category: cs.LG

TL;DR: The paper introduces FedWB, an algorithm leveraging Wasserstein barycenters for distributed Deep Neural Network training, and extends it to Heterogeneous Federated Reinforcement Learning (HFRL).


<details>
  <summary>Details</summary>
Motivation: The authors aim to improve distributed model fusion using an efficient and effective aggregation method to address issues like heterogeneity in federated learning environments.

Method: The dataset is partitioned and distributed across agents with identical DNNs. Local training is followed by a global aggregation using Wasserstein barycenters. Additionally, an HFRL algorithm is developed, tested on CartPole environments with varied pole lengths.

Result: FedWB achieves effective model fusion, while the experimental results on HFRL demonstrate its ability to generalize across heterogeneous environments.

Conclusion: The proposed FedWB algorithm provides a novel solution for distributed DNN training and HFRL, achieving model generalization and robust performance in diverse settings.

Abstract: In this paper, we first propose a novel algorithm for model fusion that
leverages Wasserstein barycenters in training a global Deep Neural Network
(DNN) in a distributed architecture. To this end, we divide the dataset into
equal parts that are fed to "agents" who have identical deep neural networks
and train only over the dataset fed to them (known as the local dataset). After
some training iterations, we perform an aggregation step where we combine the
weight parameters of all neural networks using Wasserstein barycenters. These
steps form the proposed algorithm referred to as FedWB. Moreover, we leverage
the processes created in the first part of the paper to develop an algorithm to
tackle Heterogeneous Federated Reinforcement Learning (HFRL). Our test
experiment is the CartPole toy problem, where we vary the lengths of the poles
to create heterogeneous environments. We train a deep Q-Network (DQN) in each
environment to learn to control each cart, while occasionally performing a
global aggregation step to generalize the local models; the end outcome is a
global DQN that functions across all environments.

</details>


### [308] [In-field Calibration of Low-Cost Sensors through XGBoost $\&$ Aggregate Sensor Data](https://arxiv.org/abs/2506.15840)
*Kevin Yin,Julia Gersey,Pei Zhang*

Main category: cs.LG

TL;DR: The paper proposes using XGBoost ensemble learning to calibrate low-cost air quality sensors, enhancing their reliability and spatial coverage.


<details>
  <summary>Details</summary>
Motivation: Expensive high-accuracy sensors limit large-scale air quality monitoring, while low-cost sensors suffer from drift and inaccuracies.

Method: An XGBoost ensemble learning model consolidates neighboring sensor data for in-field calibration, reducing dependency on any single sensor's accuracy.

Result: Improved generalization across locations and enhanced reliability of low-cost air quality sensors.

Conclusion: The proposed method effectively addresses the drift and variability issues of low-cost sensors, enabling better spatial deployments for air quality monitoring.

Abstract: Effective large-scale air quality monitoring necessitates distributed sensing
due to the pervasive and harmful nature of particulate matter (PM),
particularly in urban environments. However, precision comes at a cost: highly
accurate sensors are expensive, limiting the spatial deployments and thus their
coverage. As a result, low-cost sensors have become popular, though they are
prone to drift caused by environmental sensitivity and manufacturing
variability. This paper presents a model for in-field sensor calibration using
XGBoost ensemble learning to consolidate data from neighboring sensors. This
approach reduces dependence on the presumed accuracy of individual sensors and
improves generalization across different locations.

</details>


### [309] [Uncertainty Estimation by Human Perception versus Neural Models](https://arxiv.org/abs/2506.15850)
*Pedro Mendes,Paolo Romano,David Garlan*

Main category: cs.LG

TL;DR: Neural networks (NNs) are often poorly calibrated, producing overconfident predictions. This study compares NN-predicted uncertainty with human-perceived uncertainty and finds significant misalignments.


<details>
  <summary>Details</summary>
Motivation: To address the persistent issue of miscalibration in neural networks, especially in scenarios requiring reliable uncertainty estimates.

Method: Analyzed three vision benchmarks with human disagreement and crowdsourced confidence annotations to assess alignment between NN-predicted and human-perceived uncertainty; investigated the use of human-derived soft labels in training.

Result: Current neural network methods show weak alignment with human intuition, with varied correlations across tasks and metrics. Incorporating human soft labels improves calibration while maintaining accuracy.

Conclusion: There is a notable gap between model and human uncertainty perception. Human insights can be crucial in developing more trustworthy AI systems.

Abstract: Modern neural networks (NNs) often achieve high predictive accuracy but
remain poorly calibrated, producing overconfident predictions even when wrong.
This miscalibration poses serious challenges in applications where reliable
uncertainty estimates are critical. In this work, we investigate how human
perceptual uncertainty compares to uncertainty estimated by NNs. Using three
vision benchmarks annotated with both human disagreement and crowdsourced
confidence, we assess the correlation between model-predicted uncertainty and
human-perceived uncertainty. Our results show that current methods only weakly
align with human intuition, with correlations varying significantly across
tasks and uncertainty metrics. Notably, we find that incorporating
human-derived soft labels into the training process can improve calibration
without compromising accuracy. These findings reveal a persistent gap between
model and human uncertainty and highlight the potential of leveraging human
insights to guide the development of more trustworthy AI systems.

</details>


### [310] [Improving Rectified Flow with Boundary Conditions](https://arxiv.org/abs/2506.15864)
*Xixi Hu,Runlong Liao,Keyang Xu,Bo Liu,Yeqing Li,Eugene Ie,Hongliang Fei,Qiang Liu*

Main category: cs.LG

TL;DR: The paper introduces the Boundary-enforced Rectified Flow Model to address limitations in the velocity field modeling of Rectified Flow by enforcing boundary conditions, showing significant improvements in generative modeling performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address inaccuracies in velocity field estimations in Rectified Flow models, which fail to satisfy boundary conditions, leading to errors—especially during stochastic sampling.

Method: The authors propose the Boundary RF Model, which enforces boundary conditions on the learned velocity field through minimal code modifications.

Result: The Boundary RF Model shows an 8.01% improvement in FID score on ImageNet for ODE sampling and an 8.98% improvement for SDE sampling compared to the vanilla Rectified Flow model.

Conclusion: Enforcing boundary conditions effectively addresses the limitations of existing Rectified Flow models, achieving higher performance and demonstrating its effectiveness in generative modeling tasks.

Abstract: Rectified Flow offers a simple and effective approach to high-quality
generative modeling by learning a velocity field. However, we identify a
limitation in directly modeling the velocity with an unconstrained neural
network: the learned velocity often fails to satisfy certain boundary
conditions, leading to inaccurate velocity field estimations that deviate from
the desired ODE. This issue is particularly critical during stochastic sampling
at inference, as the score function's errors are amplified near the boundary.
To mitigate this, we propose a Boundary-enforced Rectified Flow Model (Boundary
RF Model), in which we enforce boundary conditions with a minimal code
modification. Boundary RF Model improves performance over vanilla RF model,
demonstrating 8.01% improvement in FID score on ImageNet using ODE sampling and
8.98% improvement using SDE sampling.

</details>


### [311] [Hidden Breakthroughs in Language Model Training](https://arxiv.org/abs/2506.15872)
*Sara Kangaslahti,Elan Rosenfeld,Naomi Saphra*

Main category: cs.LG

TL;DR: The paper presents POLCA, a method to identify hidden breakthroughs in model training dynamics by decomposing loss changes into interpretable clusters.


<details>
  <summary>Details</summary>
Motivation: To better understand learning dynamics and uncover hidden conceptual breakthroughs that are obscured by traditional scalar loss metrics.

Method: Introduced POLCA, a technique to decompose the changes in loss along the low-rank training subspace to find clusters of samples with similar loss behaviors.

Result: POLCA successfully identified interpretable clusters of breakthroughs in synthetic arithmetic and natural language tasks, revealing hidden phase transitions.

Conclusion: POLCA is a promising tool for uncovering unsupervised interpretability by identifying latent learning breakthroughs in training processes.

Abstract: Loss curves are smooth during most of model training, so visible
discontinuities stand out as possible conceptual breakthroughs. Studying these
breakthroughs enables a deeper understanding of learning dynamics, but only
when they are properly identified. This paper argues that similar breakthroughs
occur frequently throughout training but they are obscured by a loss metric
that collapses all variation into a single scalar. To find these hidden
transitions, we introduce POLCA, a method for decomposing changes in loss along
arbitrary bases of the low-rank training subspace. We use our method to
identify clusters of samples that share similar changes in loss during
training, disaggregating the overall loss into that of smaller groups of
conceptually similar data. We validate our method on synthetic arithmetic and
natural language tasks, showing that POLCA recovers clusters that represent
interpretable breakthroughs in the model's capabilities. We demonstrate the
promise of these hidden phase transitions as a tool for unsupervised
interpretability.

</details>


### [312] [Job Market Cheat Codes: Prototyping Salary Prediction and Job Grouping with Synthetic Job Listings](https://arxiv.org/abs/2506.15879)
*Abdel Rahman Alsheyab,Mohammad Alkhasawneh,Nidal Shahin*

Main category: cs.LG

TL;DR: The paper develops a machine learning prototype using synthetic job listing data to analyze trends, predict salaries, and group job roles through regression, classification, clustering, and NLP.


<details>
  <summary>Details</summary>
Motivation: The study aims to identify key factors in job market dynamics and provide valuable insights for job seekers, employers, and researchers.

Method: Techniques like regression, classification, clustering, and NLP were applied, using exploratory data analysis to investigate a synthetic dataset of job listings.

Result: Significant features influencing salaries and job roles were identified, and distinct job clusters were formed based on the analysis.

Conclusion: The methodology provides a transferrable framework for job market analysis, though its findings are limited to synthetic data and not meant for real-world application.

Abstract: This paper presents a machine learning methodology prototype using a large
synthetic dataset of job listings to identify trends, predict salaries, and
group similar job roles. Employing techniques such as regression,
classification, clustering, and natural language processing (NLP) for
text-based feature extraction and representation, this study aims to uncover
the key features influencing job market dynamics and provide valuable insights
for job seekers, employers, and researchers. Exploratory data analysis was
conducted to understand the dataset's characteristics. Subsequently, regression
models were developed to predict salaries, classification models to predict job
titles, and clustering techniques were applied to group similar jobs. The
analyses revealed significant factors influencing salary and job roles, and
identified distinct job clusters based on the provided data. While the results
are based on synthetic data and not intended for real-world deployment, the
methodology demonstrates a transferable framework for job market analysis.

</details>


### [313] [T-SHRED: Symbolic Regression for Regularization and Model Discovery with Transformer Shallow Recurrent Decoders](https://arxiv.org/abs/2506.15881)
*Alexey Yermakov,David Zoro,Mars Liyao Gao,J. Nathan Kutz*

Main category: cs.LG

TL;DR: The paper introduces improvements to SHRED models for systemic prediction using a transformer-based approach (T-SHRED) and symbolic regression for better interpretability and performance on various dynamical systems.


<details>
  <summary>Details</summary>
Motivation: Enhancing the predictive capability and interpretability of SHRED systems for sparse sensor measurement-based system identification and forecasting.

Method: Introduced T-SHRED by integrating transformers for temporal encoding and adding a SINDy attention mechanism for symbolic regression in the latent space during training.

Result: T-SHRED demonstrated accurate future frame prediction and improved interpretability on datasets ranging from low-data to high-data dynamical system regimes.

Conclusion: The T-SHRED model outperforms the original SHRED in terms of prediction accuracy and interpretability, making it suitable for broad applications involving sparse and complex dynamical systems.

Abstract: SHallow REcurrent Decoders (SHRED) are effective for system identification
and forecasting from sparse sensor measurements. Such models are light-weight
and computationally efficient, allowing them to be trained on consumer laptops.
SHRED-based models rely on Recurrent Neural Networks (RNNs) and a simple
Multi-Layer Perceptron (MLP) for the temporal encoding and spatial decoding
respectively. Despite the relatively simple structure of SHRED, they are able
to predict chaotic dynamical systems on different physical, spatial, and
temporal scales directly from a sparse set of sensor measurements. In this
work, we improve SHRED by leveraging transformers (T-SHRED) for the temporal
encoding which improves performance on next-step state prediction on large
datasets. We also introduce a sparse identification of nonlinear dynamics
(SINDy) attention mechanism into T-SHRED to perform symbolic regression
directly on the latent space as part of the model regularization architecture.
Symbolic regression improves model interpretability by learning and
regularizing the dynamics of the latent space during training. We analyze the
performance of T-SHRED on three different dynamical systems ranging from
low-data to high-data regimes. We observe that SINDy attention T-SHRED
accurately predicts future frames based on an interpretable symbolic model
across all tested datasets.

</details>


### [314] [Fractional Reasoning via Latent Steering Vectors Improves Inference Time Compute](https://arxiv.org/abs/2506.15882)
*Sheng Liu,Tianlang Chen,Pan Lu,Haotian Ye,Yizheng Chen,Lei Xing,James Zou*

Main category: cs.LG

TL;DR: The paper introduces Fractional Reasoning, a training-free and model-agnostic framework to allow continuous control of reasoning intensity during large language model inference, improving accuracy in diverse reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Existing methods apply a uniform reasoning depth for all inputs during inference, which fails to adapt to varying complexities of different tasks.

Method: Fractional Reasoning involves extracting latent steering vectors for deeper reasoning and reapplying them with a tunable scaling factor to adjust reasoning depth dynamically for each input.

Result: Fractional Reasoning improves performance across a variety of models and reasoning-heavy tasks, as demonstrated by experiments on GSM8K, MATH500, and GPQA datasets.

Conclusion: The approach enhances reasoning flexibility in large language models, offering higher output quality and adaptability for diverse tasks without retraining models.

Abstract: Test-time compute has emerged as a powerful paradigm for improving the
performance of large language models (LLMs), where generating multiple outputs
or refining individual chains can significantly boost answer accuracy. However,
existing methods like Best-of-N, majority voting, and self-reflection typically
apply reasoning in a uniform way across inputs, overlooking the fact that
different problems may require different levels of reasoning depth. In this
work, we propose Fractional Reasoning, a training-free and model-agnostic
framework that enables continuous control over reasoning intensity at inference
time, going beyond the limitations of fixed instructional prompts. Our method
operates by extracting the latent steering vector associated with deeper
reasoning and reapplying it with a tunable scaling factor, allowing the model
to tailor its reasoning process to the complexity of each input. This supports
two key modes of test-time scaling: (1) improving output quality in
breadth-based strategies (e.g., Best-of-N, majority voting), and (2) enhancing
the correctness of individual reasoning chains in depth-based strategies (e.g.,
self-reflection). Experiments on GSM8K, MATH500, and GPQA demonstrate that
Fractional Reasoning consistently improves performance across diverse reasoning
tasks and models.

</details>


### [315] [Formal Models of Active Learning from Contrastive Examples](https://arxiv.org/abs/2506.15893)
*Farnam Mansouri,Hans U. Simon,Adish Singla,Yuxin Chen,Sandra Zilles*

Main category: cs.LG

TL;DR: The paper studies the effect of contrastive examples on active learners through sample complexity using a theoretical framework.


<details>
  <summary>Details</summary>
Motivation: To understand how contrastive training examples influence learning efficiency and explain differences in class labels.

Method: Proposing a theoretical framework analyzing sample complexity in learning concept classes with contrastive examples.

Result: Illustrated findings with geometric concept classes and Boolean function classes, highlighting a connection to self-directed learning.

Conclusion: Contrastive examples can improve active learning by reducing sample complexity, linking to self-directed learning models.

Abstract: Machine learning can greatly benefit from providing learning algorithms with
pairs of contrastive training examples -- typically pairs of instances that
differ only slightly, yet have different class labels. Intuitively, the
difference in the instances helps explain the difference in the class labels.
This paper proposes a theoretical framework in which the effect of various
types of contrastive examples on active learners is studied formally. The focus
is on the sample complexity of learning concept classes and how it is
influenced by the choice of contrastive examples. We illustrate our results
with geometric concept classes and classes of Boolean functions. Interestingly,
we reveal a connection between learning from contrastive examples and the
classical model of self-directed learning.

</details>


### [316] [KG-FGNN: Knowledge-guided GNN Foundation Model for Fertilisation-oriented Soil GHG Flux Prediction](https://arxiv.org/abs/2506.15896)
*Yu Zhang,Gaoshan Bi,Simon Jeffery,Max Davis,Yang Li,Qing Xue,Po Yang*

Main category: cs.LG

TL;DR: This paper presents a knowledge-guided graph neural network (GNN) framework to improve soil GHG flux prediction in agricultural systems.


<details>
  <summary>Details</summary>
Motivation: The authors aimed to overcome the scarcity of agricultural data, which limits machine learning applications in predicting precision soil GHG flux, an essential factor for sustainable agriculture.

Method: The study developed a framework integrating agricultural process-based models with GNN techniques. They used an autoencoder to extract relevant features and multi-target multi-graph-based GNNs to capture correlations between them for improved prediction.

Result: Experiments using both simulated and real-world datasets showed the proposed method outperformed baseline and state-of-the-art regression methods in prediction accuracy and stability.

Conclusion: The proposed framework effectively addresses data scarcity challenges and improves precision soil GHG flux prediction, contributing to emission mitigation and sustainable agriculture development.

Abstract: Precision soil greenhouse gas (GHG) flux prediction is essential in
agricultural systems for assessing environmental impacts, developing emission
mitigation strategies and promoting sustainable agriculture. Due to the lack of
advanced sensor and network technologies on majority of farms, there are
challenges in obtaining comprehensive and diverse agricultural data. As a
result, the scarcity of agricultural data seriously obstructs the application
of machine learning approaches in precision soil GHG flux prediction. This
research proposes a knowledge-guided graph neural network framework that
addresses the above challenges by integrating knowledge embedded in an
agricultural process-based model and graph neural network techniques.
Specifically, we utilise the agricultural process-based model to simulate and
generate multi-dimensional agricultural datasets for 47 countries that cover a
wide range of agricultural variables. To extract key agricultural features and
integrate correlations among agricultural features in the prediction process,
we propose a machine learning framework that integrates the autoencoder and
multi-target multi-graph based graph neural networks, which utilises the
autoencoder to selectively extract significant agricultural features from the
agricultural process-based model simulation data and the graph neural network
to integrate correlations among agricultural features for accurately predict
fertilisation-oriented soil GHG fluxes. Comprehensive experiments were
conducted with both the agricultural simulation dataset and real-world
agricultural dataset to evaluate the proposed approach in comparison with
well-known baseline and state-of-the-art regression methods. The results
demonstrate that our proposed approach provides superior accuracy and stability
in fertilisation-oriented soil GHG prediction.

</details>


### [317] [TrajDiff: Diffusion Bridge Network with Semantic Alignment for Trajectory Similarity Computation](https://arxiv.org/abs/2506.15898)
*Xiao Zhang,Xingyu Zhao,Hong Xia,Yuan Cao,Guiyuan Jiang,Junyu Dong,Yanwei Yu*

Main category: cs.LG

TL;DR: The paper introduces TrajDiff, a trajectory similarity computation framework addressing challenges in semantic alignment, noise robustness, and global ranking awareness.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from challenges in trajectory similarity computation, including semantic gaps, noise obscuring motion patterns, and lack of global ranking insights.

Method: TrajDiff employs semantic alignment via cross-attention and adaptive fusion, noise robustness through DDBM-based pre-training, and ranking-aware regularization for holistic ordering.

Result: Extensive experiments show TrajDiff consistently outperforms state-of-the-art baselines with an average HR@1 gain of 33.38%.

Conclusion: TrajDiff addresses fundamental trajectory similarity challenges, improving model robustness and global ranking capabilities for better trajectory embedding computations.

Abstract: With the proliferation of location-tracking technologies, massive volumes of
trajectory data are continuously being collected. As a fundamental task in
trajectory data mining, trajectory similarity computation plays a critical role
in a wide range of real-world applications. However, existing learning-based
methods face three challenges: First, they ignore the semantic gap between GPS
and grid features in trajectories, making it difficult to obtain meaningful
trajectory embeddings. Second, the noise inherent in the trajectories, as well
as the noise introduced during grid discretization, obscures the true motion
patterns of the trajectories. Third, existing methods focus solely on
point-wise and pair-wise losses, without utilizing the global ranking
information obtained by sorting all trajectories according to their similarity
to a given trajectory. To address the aforementioned challenges, we propose a
novel trajectory similarity computation framework, named TrajDiff.
Specifically, the semantic alignment module relies on cross-attention and an
attention score mask mechanism with adaptive fusion, effectively eliminating
semantic discrepancies between data at two scales and generating a unified
representation. Additionally, the DDBM-based Noise-robust Pre-Training
introduces the transfer patterns between any two trajectories into the model
training process, enhancing the model's noise robustness. Finally, the overall
ranking-aware regularization shifts the model's focus from a local to a global
perspective, enabling it to capture the holistic ordering information among
trajectories. Extensive experiments on three publicly available datasets show
that TrajDiff consistently outperforms state-of-the-art baselines. In
particular, it achieves an average HR@1 gain of 33.38% across all three
evaluation metrics and datasets.

</details>


### [318] [Clinically Interpretable Mortality Prediction for ICU Patients with Diabetes and Atrial Fibrillation: A Machine Learning Approach](https://arxiv.org/abs/2506.15901)
*Li Sun,Shuheng Chen,Yong Si,Junyi Fan,Maryam Pishgar,Elham Pishgar,Kamiar Alaei,Greg Placencia*

Main category: cs.LG

TL;DR: The paper develops an interpretable machine learning model to predict 28-day mortality in ICU patients with diabetes and atrial fibrillation.


<details>
  <summary>Details</summary>
Motivation: Currently, ICU patients with both diabetes and atrial fibrillation have high mortality rates, and there are few models addressing this specific group.

Method: Using the MIMIC-IV database, 1,535 patients were analyzed with preprocessing and feature selection methods. Seven machine learning models were tested, and logistic regression demonstrated the best performance.

Result: Logistic regression achieved an AUROC of 0.825, with top predictors including age, bilirubin levels, and extubation status.

Conclusion: The model provides accurate mortality predictions and actionable insights for ICU patient management, enhancing early triage capabilities for DM and AF patients.

Abstract: Background: Patients with both diabetes mellitus (DM) and atrial fibrillation
(AF) face elevated mortality in intensive care units (ICUs), yet models
targeting this high-risk group remain limited.
  Objective: To develop an interpretable machine learning (ML) model predicting
28-day mortality in ICU patients with concurrent DM and AF using early-phase
clinical data.
  Methods: A retrospective cohort of 1,535 adult ICU patients with DM and AF
was extracted from the MIMIC-IV database. Data preprocessing involved
median/mode imputation, z-score normalization, and early temporal feature
engineering. A two-step feature selection pipeline-univariate filtering (ANOVA
F-test) and Random Forest-based multivariate ranking-yielded 19 interpretable
features. Seven ML models were trained with stratified 5-fold cross-validation
and SMOTE oversampling. Interpretability was assessed via ablation and
Accumulated Local Effects (ALE) analysis.
  Results: Logistic regression achieved the best performance (AUROC: 0.825; 95%
CI: 0.779-0.867), surpassing more complex models. Key predictors included RAS,
age, bilirubin, and extubation. ALE plots showed intuitive, non-linear effects
such as age-related risk acceleration and bilirubin thresholds.
  Conclusion: This interpretable ML model offers accurate risk prediction and
clinical insights for early ICU triage in patients with DM and AF.

</details>


### [319] [VectorEdits: A Dataset and Benchmark for Instruction-Based Editing of Vector Graphics](https://arxiv.org/abs/2506.15903)
*Josef Kuchař,Marek Kadlčík,Michal Spiegel,Michal Štefánik*

Main category: cs.LG

TL;DR: The paper introduces a dataset with over 270,000 vector graphics and related instructions to improve textual edits for vector images, revealing current challenges and encouraging further research.


<details>
  <summary>Details</summary>
Motivation: To address the need for effective models capable of editing vector graphics based on natural language commands, and to create resources for advancing this field.

Method: The dataset creation involved image pairing via CLIP similarity and instruction generation using vision-language models. Experiments were conducted with state-of-the-art large language models to gauge current capabilities.

Result: Initial experiments showed that state-of-the-art models struggle with accurate and valid edits on vector graphics, highlighting the complexity of the task.

Conclusion: The dataset and findings highlight the challenges in natural language-driven vector graphic editing, encouraging further research. The resources from this study are publicly available to support the community.

Abstract: We introduce a large-scale dataset for instruction-guided vector image
editing, consisting of over 270,000 pairs of SVG images paired with natural
language edit instructions. Our dataset enables training and evaluation of
models that modify vector graphics based on textual commands. We describe the
data collection process, including image pairing via CLIP similarity and
instruction generation with vision-language models. Initial experiments with
state-of-the-art large language models reveal that current methods struggle to
produce accurate and valid edits, underscoring the challenge of this task. To
foster research in natural language-driven vector graphic generation and
editing, we make our resources created within this work publicly available.

</details>


### [320] [Pieceformer: Similarity-Driven Knowledge Transfer via Scalable Graph Transformer in VLSI](https://arxiv.org/abs/2506.15907)
*Hang Yang,Yusheng Hu,Yong Liu,Cong,Hao*

Main category: cs.LG

TL;DR: The paper introduces Pieceformer, a self-supervised framework for graph similarity in VLSI design, which significantly reduces error and runtime.


<details>
  <summary>Details</summary>
Motivation: To enable efficient knowledge transfer and reuse of prior solutions in VLSI design for reduced engineering effort and turnaround time.

Method: Developed Pieceformer, a hybrid message-passing and graph transformer encoder framework, using a linear transformer and partitioned training for scalability.

Result: Pieceformer reduces MAE by 24.9% over the baseline and achieves correct clustering in real-world data, alongside 89% runtime reduction in a partitioning case study.

Conclusion: The framework efficiently enables scalable and unbiased design reuse in modern VLSI systems, validating its effectiveness in real-world applications.

Abstract: Accurate graph similarity is critical for knowledge transfer in VLSI design,
enabling the reuse of prior solutions to reduce engineering effort and
turnaround time. We propose Pieceformer, a scalable, self-supervised similarity
assessment framework, equipped with a hybrid message-passing and graph
transformer encoder. To address transformer scalability, we incorporate a
linear transformer backbone and introduce a partitioned training pipeline for
efficient memory and parallelism management. Evaluations on synthetic and
real-world CircuitNet datasets show that Pieceformer reduces mean absolute
error (MAE) by 24.9% over the baseline and is the only method to correctly
cluster all real-world design groups. We further demonstrate the practical
usage of our model through a case study on a partitioning task, achieving up to
89% runtime reduction. These results validate the framework's effectiveness for
scalable, unbiased design reuse in modern VLSI systems.

</details>


### [321] [Early Attentive Sparsification Accelerates Neural Speech Transcription](https://arxiv.org/abs/2506.15912)
*Zifei Xu,Sayeh Sharify,Hesham Mostafa,Tristan Webb,Wanzin Yazar,Xin Wang*

Main category: cs.LG

TL;DR: The paper proposes using time-domain signal sparsification in transformer-based neural encoders, achieving up to 1.6x runtime acceleration with minimal accuracy loss in English speech transcription.


<details>
  <summary>Details</summary>
Motivation: To accelerate neural speech transcription by exploiting the high compressibility of speech audio signals and leveraging the self-attention mechanism's interpretability in transformer-based models.

Method: Conduct systematic architecture search over sparsification stage (encoder layer) and compression ratio (sparsity) using Whisper models, identifying optimal configurations for runtime improvement with less than 1% accuracy degradation.

Result: Optimal sparsification (40-60% sparsity at early encoding stage) provides up to 1.6x runtime acceleration in English transcription tasks without requiring model fine-tuning.

Conclusion: Time-domain signal sparsification in neural encoders is an effective strategy for accelerating speech transcription tasks without compromising accuracy significantly.

Abstract: Transformer-based neural speech processing has achieved state-of-the-art
performance. Since speech audio signals are known to be highly compressible,
here we seek to accelerate neural speech transcription by time-domain signal
sparsification early in the neural encoding stage, taking advantage of the
interpretability of the self-attention mechanism in transformer audio encoders.
With the Whisper family of models, we perform a systematic architecture search
over the joint space of sparsification stage (a certain encoder layer) and
compression ratio (sparsity). We found that the best resulting solutions under
1% accuracy degradation choose to sparsify the hidden state to 40-60% sparsity
at an early encoding stage, and thereby achieve up to 1.6x runtime acceleration
in English speech transcription tasks on Nvidia GPUs without any fine-tuning.

</details>


### [322] [Competing Bandits in Matching Markets via Super Stability](https://arxiv.org/abs/2506.15926)
*Soumya Basu*

Main category: cs.LG

TL;DR: This paper explores bandit learning in two-sided matching markets with uncertainty, proposing the Extended Gale-Shapley (GS) algorithm for better stable matchings.


<details>
  <summary>Details</summary>
Motivation: Many matching market studies focus on single-sided uncertainty, leaving the two-sided uncertainty scenario inadequately addressed.

Method: The authors utilize the Extended Gale-Shapley algorithm to improve stability under incomplete information and derive instance-dependent bounds for stable regret in both centralized and decentralized setups.

Result: The study delivers logarithmic regret in centralized settings and achieves consistency in decentralized situations with marginally increased regret.

Conclusion: The work provides a deeper understanding of matching complexity under bandit feedback and two-sided uncertainty by establishing a new instance-dependent lower bound.

Abstract: We study bandit learning in matching markets with two-sided reward
uncertainty, extending prior research primarily focused on single-sided
uncertainty. Leveraging the concept of `super-stability' from Irving (1994), we
demonstrate the advantage of the Extended Gale-Shapley (GS) algorithm over the
standard GS algorithm in achieving true stable matchings under incomplete
information. By employing the Extended GS algorithm, our centralized algorithm
attains a logarithmic pessimal stable regret dependent on an instance-dependent
admissible gap parameter. This algorithm is further adapted to a decentralized
setting with a constant regret increase. Finally, we establish a novel
centralized instance-dependent lower bound for binary stable regret,
elucidating the roles of the admissible gap and super-stable matching in
characterizing the complexity of stable matching with bandit feedback.

</details>


### [323] [IsoNet: Causal Analysis of Multimodal Transformers for Neuromuscular Gesture Classification](https://arxiv.org/abs/2506.16744)
*Eion Tyacke,Kunal Gupta,Jay Patel,Rui Li*

Main category: cs.LG

TL;DR: This paper investigates multimodal fusion strategies for decoding neuromuscular signals from hand gestures using biosensors.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations in decoding neuromuscular signatures for neuroscience and assistive technologies, leveraging multimodal sensor information has the potential for improved performance.

Method: Three architectures—Multimodal MLP, Multimodal Transformer, and Hierarchical Transformer—are compared using two datasets, evaluating linear and attention-based fusion strategies with unimodal and multimodal inputs.

Result: Hierarchical Transformer with attention-based fusion outperforms others, achieving significant accuracy improvements over the baseline in both datasets.

Conclusion: Attention-driven multimodal fusion enhances biosignal classification and reveals new insights into muscle activity, aiding neurorobotic system design.

Abstract: Hand gestures are a primary output of the human motor system, yet the
decoding of their neuromuscular signatures remains a bottleneck for basic
neuroscience and assistive technologies such as prosthetics. Traditional
human-machine interface pipelines rely on a single biosignal modality, but
multimodal fusion can exploit complementary information from sensors. We
systematically compare linear and attention-based fusion strategies across
three architectures: a Multimodal MLP, a Multimodal Transformer, and a
Hierarchical Transformer, evaluating performance on scenarios with unimodal and
multimodal inputs. Experiments use two publicly available datasets: NinaPro DB2
(sEMG and accelerometer) and HD-sEMG 65-Gesture (high-density sEMG and force).
Across both datasets, the Hierarchical Transformer with attention-based fusion
consistently achieved the highest accuracy, surpassing the multimodal and best
single-modality linear-fusion MLP baseline by over 10% on NinaPro DB2 and 3.7%
on HD-sEMG. To investigate how modalities interact, we introduce an Isolation
Network that selectively silences unimodal or cross-modal attention pathways,
quantifying each group of token interactions' contribution to downstream
decisions. Ablations reveal that cross-modal interactions contribute
approximately 30% of the decision signal across transformer layers,
highlighting the importance of attention-driven fusion in harnessing
complementary modality information. Together, these findings reveal when and
how multimodal fusion would enhance biosignal classification and also provides
mechanistic insights of human muscle activities. The study would be beneficial
in the design of sensor arrays for neurorobotic systems.

</details>


### [324] [CORAL: Disentangling Latent Representations in Long-Tailed Diffusion](https://arxiv.org/abs/2506.15933)
*Esther Rodriguez,Monica Welfert,Samuel McDowell,Nathan Stromberg,Julian Antolin Camarena,Lalitha Sankar*

Main category: cs.LG

TL;DR: Diffusion models perform poorly with long-tailed class distributions but can be improved using a proposed contrastive latent alignment framework called CORAL.


<details>
  <summary>Details</summary>
Motivation: Explore why diffusion models struggle to generate high-quality samples for tail classes under long-tailed training distributions.

Method: Introduced CORAL (COntrastive Regularization for Aligning Latents), employing supervised contrastive losses to separate latent class representations.

Result: CORAL improved both diversity and visual quality of tail-class samples compared to existing methods.

Conclusion: Addressing latent overlap with contrastive regularization enhances diffusion model performance on imbalanced datasets.

Abstract: Diffusion models have achieved impressive performance in generating
high-quality and diverse synthetic data. However, their success typically
assumes a class-balanced training distribution. In real-world settings,
multi-class data often follow a long-tailed distribution, where standard
diffusion models struggle -- producing low-diversity and lower-quality samples
for tail classes. While this degradation is well-documented, its underlying
cause remains poorly understood. In this work, we investigate the behavior of
diffusion models trained on long-tailed datasets and identify a key issue: the
latent representations (from the bottleneck layer of the U-Net) for tail class
subspaces exhibit significant overlap with those of head classes, leading to
feature borrowing and poor generation quality. Importantly, we show that this
is not merely due to limited data per class, but that the relative class
imbalance significantly contributes to this phenomenon. To address this, we
propose COntrastive Regularization for Aligning Latents (CORAL), a contrastive
latent alignment framework that leverages supervised contrastive losses to
encourage well-separated latent class representations. Experiments demonstrate
that CORAL significantly improves both the diversity and visual quality of
samples generated for tail classes relative to state-of-the-art methods.

</details>


### [325] [Off-Policy Actor-Critic for Adversarial Observation Robustness: Virtual Alternative Training via Symmetric Policy Evaluation](https://arxiv.org/abs/2506.16753)
*Kosuke Nakanishi,Akihiro Kubo,Yuji Yasui,Shin Ishii*

Main category: cs.LG

TL;DR: The paper proposes an off-policy reinforcement learning method to handle adversarial inputs more efficiently.


<details>
  <summary>Details</summary>
Motivation: To address RL's vulnerability to adversarial input observations and improve efficiency in handling long time horizons.

Method: The authors reformulate adversarial learning into a soft-constrained optimization problem, supported by theoretical insights from symmetric policy evaluation.

Result: The proposed method eliminates additional environmental interactions and improves efficiency in adversarial RL settings.

Conclusion: The novel method effectively tackles inefficiencies in adversarial learning, enhancing robust RL applications.

Abstract: Recently, robust reinforcement learning (RL) methods designed to handle
adversarial input observations have received significant attention, motivated
by RL's inherent vulnerabilities. While existing approaches have demonstrated
reasonable success, addressing worst-case scenarios over long time horizons
requires both minimizing the agent's cumulative rewards for adversaries and
training agents to counteract them through alternating learning. However, this
process introduces mutual dependencies between the agent and the adversary,
making interactions with the environment inefficient and hindering the
development of off-policy methods. In this work, we propose a novel off-policy
method that eliminates the need for additional environmental interactions by
reformulating adversarial learning as a soft-constrained optimization problem.
Our approach is theoretically supported by the symmetric property of policy
evaluation between the agent and the adversary. The implementation is available
at https://github.com/nakanakakosuke/VALT_SAC.

</details>


### [326] [On the optimal regret of collaborative personalized linear bandits](https://arxiv.org/abs/2506.15943)
*Bruce Huang,Ruida Zhou,Lin F. Yang,Suhas Diggavi*

Main category: cs.LG

TL;DR: The paper introduces a framework for collaborative personalized linear bandits, providing optimal regret bounds that outperform independent agent methods by leveraging cross-agent similarity.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the limitations of single-agent algorithms in real-world multi-agent bandit problems, where ignoring cross-agent similarity leads to suboptimal learning and decision-making.

Method: The paper uses a hierarchical Bayesian framework to model heterogeneity and derives a novel information-theoretic approach to bound regret. It also proposes a two-stage collaborative algorithm achieving optimal regret.

Result: The analysis provides regret bounds as $
\tilde{O}(d\sqrt{mn})$, $\tilde{O}(dm^{1-\gamma}\sqrt{n})$, and $\tilde{O}(dm\sqrt{n})$, varying across different heterogeneity levels, significantly improving upon the $O(dm\sqrt{n})$ achievable without collaboration.

Conclusion: Collaboration among agents in personalized linear bandit problems improves learning efficiency and leads to optimal regret bounds, making it a superior approach compared to independent single-agent strategies.

Abstract: Stochastic linear bandits are a fundamental model for sequential decision
making, where an agent selects a vector-valued action and receives a noisy
reward with expected value given by an unknown linear function. Although well
studied in the single-agent setting, many real-world scenarios involve multiple
agents solving heterogeneous bandit problems, each with a different unknown
parameter. Applying single agent algorithms independently ignores cross-agent
similarity and learning opportunities. This paper investigates the optimal
regret achievable in collaborative personalized linear bandits. We provide an
information-theoretic lower bound that characterizes how the number of agents,
the interaction rounds, and the degree of heterogeneity jointly affect regret.
We then propose a new two-stage collaborative algorithm that achieves the
optimal regret. Our analysis models heterogeneity via a hierarchical Bayesian
framework and introduces a novel information-theoretic technique for bounding
regret. Our results offer a complete characterization of when and how
collaboration helps with a optimal regret bound $\tilde{O}(d\sqrt{mn})$,
$\tilde{O}(dm^{1-\gamma}\sqrt{n})$, $\tilde{O}(dm\sqrt{n})$ for the number of
rounds $n$ in the range of $(0, \frac{d}{m \sigma^2})$, $[\frac{d}{m^{2\gamma}
\sigma^2}, \frac{d}{\sigma^2}]$ and $(\frac{d}{\sigma^2}, \infty)$
respectively, where $\sigma$ measures the level of heterogeneity, $m$ is the
number of agents, and $\gamma\in[0, 1/2]$ is an absolute constant. In contrast,
agents without collaboration achieve a regret bound $O(dm\sqrt{n})$ at best.

</details>


### [327] [One Period to Rule Them All: Identifying Critical Learning Periods in Deep Networks](https://arxiv.org/abs/2506.15954)
*Vinicius Yuiti Fukase,Heitor Gama,Barbara Bueno,Lucas Libanio,Anna Helena Reali Costa,Artur Jordao*

Main category: cs.LG

TL;DR: This study identifies critical training periods for deep neural networks and optimizes resource-intensive strategies, yielding substantial reductions in training time, energy usage, CO$_2$ emissions, and costs, while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Deep learning training can be resource-heavy, and early epochs significantly impact success. Identifying critical periods when optimization techniques are most effective can lead to efficiency gains.

Method: The authors introduced a systematic approach using generalization prediction mechanisms to identify critical training phases, enabling more efficient application of training recipes and resource management.

Result: Experiments demonstrated a reduction of up to 59.67% in training time, 59.47% in CO$_2$ emissions, and 60% in financial costs for popular architectures, with performance remaining unaffected.

Conclusion: The study enhances understanding of training dynamics, enabling sustainable deep learning practices, and provides methods beneficial in resource-limited settings for reducing environmental impacts.

Abstract: Critical Learning Periods comprehend an important phenomenon involving deep
learning, where early epochs play a decisive role in the success of many
training recipes, such as data augmentation. Existing works confirm the
existence of this phenomenon and provide useful insights. However, the
literature lacks efforts to precisely identify when critical periods occur. In
this work, we fill this gap by introducing a systematic approach for
identifying critical periods during the training of deep neural networks,
focusing on eliminating computationally intensive regularization techniques and
effectively applying mechanisms for reducing computational costs, such as data
pruning. Our method leverages generalization prediction mechanisms to pinpoint
critical phases where training recipes yield maximum benefits to the predictive
ability of models. By halting resource-intensive recipes beyond these periods,
we significantly accelerate the learning phase and achieve reductions in
training time, energy consumption, and CO$_2$ emissions. Experiments on
standard architectures and benchmarks confirm the effectiveness of our method.
Specifically, we achieve significant milestones by reducing the training time
of popular architectures by up to 59.67%, leading to a 59.47% decrease in
CO$_2$ emissions and a 60% reduction in financial costs, without compromising
performance. Our work enhances understanding of training dynamics and paves the
way for more sustainable and efficient deep learning practices, particularly in
resource-constrained environments. In the era of the race for foundation
models, we believe our method emerges as a valuable framework. The repository
is available at https://github.com/baunilhamarga/critical-periods

</details>


### [328] [On the Theoretical Understanding of Identifiable Sparse Autoencoders and Beyond](https://arxiv.org/abs/2506.15963)
*Jingyi Cui,Qi Zhang,Yifei Wang,Yisen Wang*

Main category: cs.LG

TL;DR: This paper explores sparse autoencoders (SAEs) for recovering monosemantic features from polysemantic superpositions in large language models and identifies conditions for their effectiveness.


<details>
  <summary>Details</summary>
Motivation: Despite the utility of sparse autoencoders in analyzing features of large language models, the conditions for uniquely and accurately recovering ground truth monosemantic features are not well understood.

Method: The authors conduct theoretical analyses to establish conditions for SAE identifiability, propose a reweighting strategy for improved reconstruction, and validate findings through experiments.

Result: They establish necessary and sufficient conditions for identifiable SAEs (e.g., extreme sparsity and enough hidden dimensions), improve reconstruction precision via reweighting, and experimentally demonstrate enhanced interpretability.

Conclusion: Identifiable SAEs improve monosemantic feature recovery, and reweighting enhances their reconstruction capabilities, advancing interpretability of learned features in large language models.

Abstract: Sparse autoencoders (SAEs) have emerged as a powerful tool for interpreting
features learned by large language models (LLMs). It aims to recover complex
superposed polysemantic features into interpretable monosemantic ones through
feature reconstruction via sparsely activated neural networks. Despite the wide
applications of SAEs, it remains unclear under what conditions an SAE can fully
recover the ground truth monosemantic features from the superposed polysemantic
ones. In this paper, through theoretical analysis, we for the first time
propose the necessary and sufficient conditions for identifiable SAEs (SAEs
that learn unique and ground truth monosemantic features), including 1) extreme
sparsity of the ground truth feature, 2) sparse activation of SAEs, and 3)
enough hidden dimensions of SAEs. Moreover, when the identifiable conditions
are not fully met, we propose a reweighting strategy to improve the
identifiability. Specifically, following the theoretically suggested weight
selection principle, we prove that the gap between the loss functions of SAE
reconstruction and monosemantic feature reconstruction can be narrowed, so that
the reweighted SAEs have better reconstruction of the ground truth monosemantic
features than the uniformly weighted ones. In experiments, we validate our
theoretical findings and show that our weighted SAE significantly improves
feature monosemanticity and interpretability.

</details>


### [329] [LazyEviction: Lagged KV Eviction with Attention Pattern Observation for Efficient Long Reasoning](https://arxiv.org/abs/2506.15969)
*Haoyue Zhang,Hualei Zhang,Xiaosong Ma,Jie Zhang,Song Guo*

Main category: cs.LG

TL;DR: LazyEviction, a KV cache management framework, reduces GPU memory usage by 50% while preserving reasoning performance in long reasoning tasks of LLMs.


<details>
  <summary>Details</summary>
Motivation: Chain-of-Thought reasoning enhances LLM performance but causes increased KV memory needs, particularly in tasks like mathematics and programming, challenging current compression methods.

Method: The authors introduce LazyEviction, using a lagged KV eviction framework with two components: Recurrence Interval Tracking to capture temporal token importance and a Recurrence Interval-Centric Eviction Policy prioritizing token eviction based on recurrence patterns.

Result: LazyEviction achieves a 50% reduction in KV cache size while maintaining accuracy comparable to current state-of-the-art methods in reasoning tasks.

Conclusion: Preserving recurring tokens is crucial for multi-step reasoning tasks, and LazyEviction effectively balances memory efficiency with reasoning performance.

Abstract: Large Language Models (LLMs) exhibit enhanced reasoning capabilities by
employing Chain-of-Thought (CoT). However, the extended reasoning sequences
introduce significant GPU memory overhead due to increased key-value (KV) cache
size, particularly in tasks requiring long reasoning sequences, such as
mathematics and programming. Existing KV cache compression methods mitigate
memory bottlenecks but struggle in long reasoning tasks. In this paper, we
analyze attention patterns in reasoning tasks and reveal a Token Importance
Recurrence phenomenon: a large proportion of tokens receive renewed attention
after multiple decoding steps, which is failed to capture by existing works and
may lead to unpredictable eviction on such periodically critical tokens. To
address this, we propose LazyEviction, a lagged KV eviction framework designed
to maintain reasoning performance while reducing KV memory. LazyEviction is an
Observation Window-based Lagged Eviction Mechanism retaining latent recurring
tokens by performing lagged evictions across decoding steps, which contains two
key components: (1) Recurrence Interval Tracking for capturing temporal
variations in token importance, and (2) an Maximum Recurrence Interval-Centric
Eviction Policy that prioritizes eviction based on tokens' recurrence patterns.
Extensive experiments demonstrate that LazyEviction reduces KV cache size by
50% while maintaining comparable accuracy on mathematics reasoning datasets,
outperforming state-of-the-art methods. Our findings highlight the importance
of preserving recurring tokens, which are critical for maintaining knowledge
continuity in multi-step reasoning tasks.

</details>


### [330] [AutoHFormer: Efficient Hierarchical Autoregressive Transformer for Time Series Prediction](https://arxiv.org/abs/2506.16001)
*Qianru Zhang,Honggang Wen,Ming Li,Dong Huang,Siu-Ming Yiu,Christian S. Jensen,Pietro Liò*

Main category: cs.LG

TL;DR: AutoHFormer enhances time series forecasting by improving efficiency and precision through hierarchical temporal modeling, dynamic windowed attention, and adaptive temporal encoding.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of reliable predictions, scalability, and accurate long-horizon forecasting in time series analysis.

Method: Introduces a hierarchical autoregressive transformer with dual-scale temporal modeling, learnable causal windows for attention mechanisms, and novel position encoding for multi-scale temporal patterns.

Result: Achieves 10.76X faster training and 6.06X memory reduction compared to PatchTST on PEMS08, while maintaining accuracy over long forecasting horizons.

Conclusion: AutoHFormer sets new benchmarks in efficient and accurate time series forecasting and offers open-source implementations.

Abstract: Time series forecasting requires architectures that simultaneously achieve
three competing objectives: (1) strict temporal causality for reliable
predictions, (2) sub-quadratic complexity for practical scalability, and (3)
multi-scale pattern recognition for accurate long-horizon forecasting. We
introduce AutoHFormer, a hierarchical autoregressive transformer that addresses
these challenges through three key innovations: 1) Hierarchical Temporal
Modeling: Our architecture decomposes predictions into segment-level blocks
processed in parallel, followed by intra-segment sequential refinement. This
dual-scale approach maintains temporal coherence while enabling efficient
computation. 2) Dynamic Windowed Attention: The attention mechanism employs
learnable causal windows with exponential decay, reducing complexity while
preserving precise temporal relationships. This design avoids both the
anti-causal violations of standard transformers and the sequential bottlenecks
of RNN hybrids. 3) Adaptive Temporal Encoding: a novel position encoding system
is adopted to capture time patterns at multiple scales. It combines fixed
oscillating patterns for short-term variations with learnable decay rates for
long-term trends. Comprehensive experiments demonstrate that AutoHFormer 10.76X
faster training and 6.06X memory reduction compared to PatchTST on PEMS08,
while maintaining consistent accuracy across 96-720 step horizons in most of
cases. These breakthroughs establish new benchmarks for efficient and precise
time series modeling. Implementations of our method and all baselines in
hierarchical autoregressive mechanism are available at
https://github.com/lizzyhku/Autotime.

</details>


### [331] [Bridging Brain with Foundation Models through Self-Supervised Learning](https://arxiv.org/abs/2506.16009)
*Hamdi Altaheri,Fakhri Karray,Md. Milon Islam,S M Taslim Uddin Raju,Amir-Hossein Karimi*

Main category: cs.LG

TL;DR: The paper reviews how self-supervised learning (SSL) and foundation models can transform brain signal analysis by utilizing unlabeled neural data and navigating unique challenges like noise and variability.


<details>
  <summary>Details</summary>
Motivation: To leverage advancements in self-supervised learning and foundation models to address the challenges posed by brain signal analysis, such as the scarcity of labeled neural data and high noise levels.

Method: The authors systematically review SSL techniques, brain-specific model developments, downstream task adaptations, multimodal SSL integrations, evaluation metrics, benchmark datasets, and challenges in brain signal analysis.

Result: The survey categorizes relevant SSL methods and techniques, providing insights into brain foundation models and their applications to multimodal frameworks, as well as highlighting evaluation strategies.

Conclusion: The paper offers a structured understanding of the use of SSL and foundation models for brain signals, identifies challenges, and proposes future research directions to enhance generalizability.

Abstract: Foundation models (FMs), powered by self-supervised learning (SSL), have
redefined the capabilities of artificial intelligence, demonstrating
exceptional performance in domains like natural language processing and
computer vision. These advances present a transformative opportunity for brain
signal analysis. Unlike traditional supervised learning, which is limited by
the scarcity of labeled neural data, SSL offers a promising solution by
enabling models to learn meaningful representations from unlabeled data. This
is particularly valuable in addressing the unique challenges of brain signals,
including high noise levels, inter-subject variability, and low signal-to-noise
ratios. This survey systematically reviews the emerging field of bridging brain
signals with foundation models through the innovative application of SSL. It
explores key SSL techniques, the development of brain-specific foundation
models, their adaptation to downstream tasks, and the integration of brain
signals with other modalities in multimodal SSL frameworks. The review also
covers commonly used evaluation metrics and benchmark datasets that support
comparative analysis. Finally, it highlights key challenges and outlines future
research directions. This work aims to provide researchers with a structured
understanding of this rapidly evolving field and a roadmap for developing
generalizable brain foundation models powered by self-supervision.

</details>


### [332] [VRAIL: Vectorized Reward-based Attribution for Interpretable Learning](https://arxiv.org/abs/2506.16014)
*Jina Kim,Youjin Jang,Jeongjin Han*

Main category: cs.LG

TL;DR: The paper introduces VRAIL, a framework for interpretable reinforcement learning that improves training stability and interpretability by attributing importance to state features.


<details>
  <summary>Details</summary>
Motivation: To improve interpretability and stability in reinforcement learning by attributing importance to state features in a transparent manner.

Method: Introduces a bi-level framework consisting of a deep learning stage to estimate value functions and a reinforcement learning stage employing potential-based reward transformations.

Result: Empirical tests on Taxi-v3 show improved training stability and convergence, uncovering interpretable subgoals like passenger possession.

Conclusion: VRAIL is a model-agnostic framework that enhances both learning performance and interpretability in reinforcement learning systems.

Abstract: We propose VRAIL (Vectorized Reward-based Attribution for Interpretable
Learning), a bi-level framework for value-based reinforcement learning (RL)
that learns interpretable weight representations from state features. VRAIL
consists of two stages: a deep learning (DL) stage that fits an estimated value
function using state features, and an RL stage that uses this to shape learning
via potential-based reward transformations. The estimator is modeled in either
linear or quadratic form, allowing attribution of importance to individual
features and their interactions. Empirical results on the Taxi-v3 environment
demonstrate that VRAIL improves training stability and convergence compared to
standard DQN, without requiring environment modifications. Further analysis
shows that VRAIL uncovers semantically meaningful subgoals, such as passenger
possession, highlighting its ability to produce human-interpretable behavior.
Our findings suggest that VRAIL serves as a general, model-agnostic framework
for reward shaping that enhances both learning and interpretability.

</details>


### [333] [A Scalable Factorization Approach for High-Order Structured Tensor Recovery](https://arxiv.org/abs/2506.16032)
*Zhen Qin,Michael B. Wakin,Zhihui Zhu*

Main category: cs.LG

TL;DR: The paper introduces a unified framework for tensor decomposition using the factorization approach, leveraging Riemannian gradient descent (RGD) on the Stiefel manifold for better convergence properties.


<details>
  <summary>Details</summary>
Motivation: To address the challenges posed by high-order tensors in storage and computation, particularly the nonconvex nature of optimization in tensor decompositions.

Method: The approach uses Riemannian gradient descent (RGD) to optimize orthonormal factors under a unified framework, leveraging the canonical form of tensor decompositions.

Result: Establishes a Riemannian regularity condition and demonstrates that RGD converges to the ground-truth tensor with a linear rate, and both initialization requirements and convergence rate scale polynomially with tensor order.

Conclusion: The proposed method offers a more computationally efficient and convergent approach to tensor decomposition, improving upon existing strategies.

Abstract: Tensor decompositions, which represent an $N$-order tensor using
approximately $N$ factors of much smaller dimensions, can significantly reduce
the number of parameters. This is particularly beneficial for high-order
tensors, as the number of entries in a tensor grows exponentially with the
order. Consequently, they are widely used in signal recovery and data analysis
across domains such as signal processing, machine learning, and quantum
physics. A computationally and memory-efficient approach to these problems is
to optimize directly over the factors using local search algorithms such as
gradient descent, a strategy known as the factorization approach in matrix and
tensor optimization. However, the resulting optimization problems are highly
nonconvex due to the multiplicative interactions between factors, posing
significant challenges for convergence analysis and recovery guarantees.
  In this paper, we present a unified framework for the factorization approach
to solving various tensor decomposition problems. Specifically, by leveraging
the canonical form of tensor decompositions--where most factors are constrained
to be orthonormal to mitigate scaling ambiguity--we apply Riemannian gradient
descent (RGD) to optimize these orthonormal factors on the Stiefel manifold.
Under a mild condition on the loss function, we establish a Riemannian
regularity condition for the factorized objective and prove that RGD converges
to the ground-truth tensor at a linear rate when properly initialized. Notably,
both the initialization requirement and the convergence rate scale polynomially
rather than exponentially with $N$, improving upon existing results for Tucker
and tensor-train format tensors.

</details>


### [334] [Vision-Guided Chunking Is All You Need: Enhancing RAG with Multimodal Document Understanding](https://arxiv.org/abs/2506.16035)
*Vishesh Tripathi,Tanmay Odapally,Indraneel Das,Uday Allu,Biddwan Ahmed*

Main category: cs.LG

TL;DR: The paper proposes a new multimodal document chunking method for better handling complex PDF document structures in Retrieval-Augmented Generation systems.


<details>
  <summary>Details</summary>
Motivation: Traditional text-based chunking methods fail to efficiently process complex documents, such as multi-page tables, embedded visuals, and cross-page dependencies.

Method: Utilize Large Multimodal Models (LMMs) to chunk PDF documents in batches while preserving semantic and structural integrity across pages.

Result: The proposed method improves chunk quality and RAG system performance, validated on a specialized dataset using queries.

Conclusion: The vision-guided multimodal chunking method offers enhanced semantic coherence and structural accuracy compared to traditional approaches.

Abstract: Retrieval-Augmented Generation (RAG) systems have revolutionized information
retrieval and question answering, but traditional text-based chunking methods
struggle with complex document structures, multi-page tables, embedded figures,
and contextual dependencies across page boundaries. We present a novel
multimodal document chunking approach that leverages Large Multimodal Models
(LMMs) to process PDF documents in batches while maintaining semantic coherence
and structural integrity. Our method processes documents in configurable page
batches with cross-batch context preservation, enabling accurate handling of
tables spanning multiple pages, embedded visual elements, and procedural
content. We evaluate our approach on a curated dataset of PDF documents with
manually crafted queries, demonstrating improvements in chunk quality and
downstream RAG performance. Our vision-guided approach achieves better accuracy
compared to traditional vanilla RAG systems, with qualitative analysis showing
superior preservation of document structure and semantic coherence.

</details>


### [335] [From Data to Decision: Data-Centric Infrastructure for Reproducible ML in Collaborative eScience](https://arxiv.org/abs/2506.16051)
*Zhiwei Li,Carl Kesselman,Tran Huy Nguyen,Benjamin Yixing Xu,Kyle Bolo,Kimberley Yu*

Main category: cs.LG

TL;DR: The paper presents a framework to improve reproducibility in machine learning by structuring workflows around six artifacts for better data sharing and traceability.


<details>
  <summary>Details</summary>
Motivation: Reproducibility in machine learning is particularly challenging in collaborative projects due to fragmented and informal workflows.

Method: The study proposes a structured framework with six artifacts: Dataset, Feature, Workflow, Execution, Asset, and Controlled Vocabulary.

Result: Through a clinical glaucoma detection use case, the framework is shown to enhance iterative exploration and maintain decision provenance.

Conclusion: The framework improves machine learning workflows by making experiments versioned, interpretable, and reproducible over time.

Abstract: Reproducibility remains a central challenge in machine learning (ML),
especially in collaborative eScience projects where teams iterate over data,
features, and models. Current ML workflows are often dynamic yet fragmented,
relying on informal data sharing, ad hoc scripts, and loosely connected tools.
This fragmentation impedes transparency, reproducibility, and the adaptability
of experiments over time. This paper introduces a data-centric framework for
lifecycle-aware reproducibility, centered around six structured artifacts:
Dataset, Feature, Workflow, Execution, Asset, and Controlled Vocabulary. These
artifacts formalize the relationships between data, code, and decisions,
enabling ML experiments to be versioned, interpretable, and traceable over
time. The approach is demonstrated through a clinical ML use case of glaucoma
detection, illustrating how the system supports iterative exploration, improves
reproducibility, and preserves the provenance of collaborative decisions across
the ML lifecycle.

</details>


### [336] [CRIA: A Cross-View Interaction and Instance-Adapted Pre-training Framework for Generalizable EEG Representations](https://arxiv.org/abs/2506.16056)
*Puchun Liu,C. L. Philip Chen,Yubin He,Tong Zhang*

Main category: cs.LG

TL;DR: The paper introduces CRIA, a framework for EEG representation learning that integrates information across temporal, spectral, and spatial views using cross-attention mechanisms.


<details>
  <summary>Details</summary>
Motivation: Existing EEG pre-training methods fail to capture synergistic interactions between multiple views, limiting generalization of learned representations.

Method: CRIA uses cross-view interaction with cross-attention and masking strategies to integrate EEG data from temporal, spectral, and spatial domains.

Result: CRIA achieves strong performance on multi-class event classification (57.02% accuracy) and anomaly detection (80.03%) in EEG datasets.

Conclusion: The framework achieves advanced generalization and representational capabilities compared to existing methods in the context of EEG datasets.

Abstract: The difficulty of extracting deep features from EEG data and effectively
integrating information from multiple views presents significant challenges for
developing a generalizable pretraining framework for EEG representation
learning. However, most existing pre-training methods rely solely on the
contextual semantics of a single view, failing to capture the complex and
synergistic interactions among different perspectives, limiting the
expressiveness and generalization of learned representations. To address these
issues, this paper proposes CRIA, an adaptive framework that utilizes
variable-length and variable-channel coding to achieve a unified representation
of EEG data across different datasets. In this work, we define cross-view
information as the integrated representation that emerges from the interaction
among temporal, spectral, and spatial views of EEG signals. The model employs a
cross-attention mechanism to fuse temporal, spectral, and spatial features
effectively, and combines an attention matrix masking strategy based on the
information bottleneck principle with a novel viewpoint masking pre-training
scheme. Experimental results on the Temple University EEG corpus and the
CHB-MIT dataset show that CRIA outperforms existing methods with the same
pre-training conditions, achieving a balanced accuracy of 57.02% for
multi-class event classification and 80.03% for anomaly detection, highlighting
its strong generalization ability.

</details>


### [337] [A Lightweight RL-Driven Deep Unfolding Network for Robust WMMSE Precoding in Massive MU-MIMO-OFDM Systems](https://arxiv.org/abs/2506.16072)
*Kexuan Wang,An Liu*

Main category: cs.LG

TL;DR: The paper introduces RLDDU-Net, a lightweight Deep Unfolding network incorporating Reinforcement Learning, to address practical implementation challenges of WMMSE precoding in massive MIMO systems under imperfect CSI.


<details>
  <summary>Details</summary>
Motivation: WMMSE precoding achieves near-optimal sum-rate performance but faces challenges like the need for perfect CSI and high computational complexity, which limits its practicality in massive MIMO systems.

Method: The authors developed a wideband stochastic WMMSE algorithm for imperfect CSI and designed RLDDU-Net, a reinforcement learning-enabled deep unfolding network that incorporates beam-domain sparsity and subcarrier correlation to improve efficiency and accuracy.

Result: The proposed RLDDU-Net achieves better weighted sum-rate performance, faster convergence, and reduced computational overhead compared to existing solutions in the presence of imperfect CSI.

Conclusion: RLDDU-Net is an efficient and robust solution for practical MU-MIMO-OFDM systems, addressing key limitations of traditional WMMSE approaches under realistic conditions.

Abstract: Weighted Minimum Mean Square Error (WMMSE) precoding is widely recognized for
its near-optimal weighted sum rate performance. However, its practical
deployment in massive multi-user (MU) multiple-input multiple-output (MIMO)
orthogonal frequency-division multiplexing (OFDM) systems is hindered by the
assumption of perfect channel state information (CSI) and high computational
complexity. To address these issues, we first develop a wideband stochastic
WMMSE (SWMMSE) algorithm that iteratively maximizes the ergodic weighted
sum-rate (EWSR) under imperfect CSI. Building on this, we propose a lightweight
reinforcement learning (RL)-driven deep unfolding (DU) network (RLDDU-Net),
where each SWMMSE iteration is mapped to a network layer. Specifically, its DU
module integrates approximation techniques and leverages beam-domain sparsity
as well as frequency-domain subcarrier correlation, significantly accelerating
convergence and reducing computational overhead. Furthermore, the RL module
adaptively adjusts the network depth and generates compensation matrices to
mitigate approximation errors. Simulation results under imperfect CSI
demonstrate that RLDDU-Net outperforms existing baselines in EWSR performance
while offering superior computational and convergence efficiency.

</details>


### [338] [Joint User Priority and Power Scheduling for QoS-Aware WMMSE Precoding: A Constrained-Actor Attentive-Critic Approach](https://arxiv.org/abs/2506.16074)
*Kexuan Wang,An Liu*

Main category: cs.LG

TL;DR: The paper introduces a Constrained Reinforcement Learning (CRL) algorithm called CAAC to improve WMMSE precoding for 6G networks, outperforming baselines in energy efficiency and QoS satisfaction.


<details>
  <summary>Details</summary>
Motivation: To address the rigidity of traditional WMMSE precoding in adapting to dynamic QoS requirements and channel conditions for 6G networks.

Method: A novel Constrained Reinforcement Learning algorithm, Constrained-Actor Attentive-Critic (CAAC), combining policy optimization via Constrained Stochastic Successive Convex Approximation (CSSCA) and lightweight attention-enhanced Q-networks.

Result: Simulation results demonstrate superior performance of CAAC over baselines in terms of energy efficiency and QoS satisfaction.

Conclusion: The CAAC algorithm offers a flexible and efficient solution to enhance WMMSE precoding in 6G networks by dynamically optimizing user priorities and power.

Abstract: 6G wireless networks are expected to support diverse quality-of-service (QoS)
demands while maintaining high energy efficiency. Weighted Minimum Mean Square
Error (WMMSE) precoding with fixed user priorities and transmit power is widely
recognized for enhancing overall system performance but lacks flexibility to
adapt to user-specific QoS requirements and time-varying channel conditions. To
address this, we propose a novel constrained reinforcement learning (CRL)
algorithm, Constrained-Actor Attentive-Critic (CAAC), which uses a policy
network to dynamically allocate user priorities and power for WMMSE precoding.
Specifically, CAAC integrates a Constrained Stochastic Successive Convex
Approximation (CSSCA) method to optimize the policy, enabling more effective
handling of energy efficiency goals and satisfaction of stochastic non-convex
QoS constraints compared to traditional and existing CRL methods. Moreover,
CAAC employs lightweight attention-enhanced Q-networks to evaluate policy
updates without prior environment model knowledge. The network architecture not
only enhances representational capacity but also boosts learning efficiency.
Simulation results show that CAAC outperforms baselines in both energy
efficiency and QoS satisfaction.

</details>


### [339] [Probing the Robustness of Large Language Models Safety to Latent Perturbations](https://arxiv.org/abs/2506.16078)
*Tianle Gu,Kexin Huang,Zongqi Wang,Yixu Wang,Jie Li,Yuanqi Yao,Yang Yao,Yujiu Yang,Yan Teng,Yingchun Wang*

Main category: cs.LG

TL;DR: The paper highlights flaws in current AI safety alignment methods, introduces the Activation Steering Attack to reveal vulnerabilities, and proposes a new training strategy (LAPT) to enhance robustness.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from current safety alignment methods being shallow, focusing on surface-level behaviors, and being vulnerable to latent perturbations triggering unsafe responses.

Method: The paper introduces a probing method to measure sensitivity in the latent space and uses the insights to construct jailbreak attacks. Additionally, it introduces Layer-wise Adversarial Patch Training (LAPT), a fine-tuning strategy designed to improve alignment robustness by introducing controlled latent perturbations.

Result: The proposed LAPT method effectively improves alignment robustness without compromising general AI capabilities, as demonstrated by experimental results.

Conclusion: The study reveals fundamental flaws in existing alignment paradigms and emphasizes the need for representation-level training strategies for robust and aligned AI systems.

Abstract: Safety alignment is a key requirement for building reliable Artificial
General Intelligence. Despite significant advances in safety alignment, we
observe that minor latent shifts can still trigger unsafe responses in aligned
models. We argue that this stems from the shallow nature of existing alignment
methods, which focus on surface-level refusal behaviors without sufficiently
altering internal representations. Consequently, small shifts in hidden
activations can re-trigger harmful behaviors embedded in the latent space. To
explore the robustness of safety alignment to latent perturbations, we
introduce a probing method that measures the Negative Log-Likelihood of the
original response generated by the model. This probe quantifies local
sensitivity in the latent space, serving as a diagnostic tool for identifying
vulnerable directions. Based on this signal, we construct effective jailbreak
trajectories, giving rise to the Activation Steering Attack (ASA). More
importantly, these insights offer a principled foundation for improving
alignment robustness. To this end, we introduce Layer-wise Adversarial Patch
Training~(LAPT), a fine-tuning strategy that inject controlled perturbations
into hidden representations during training. Experimental results highlight
that LAPT strengthen alignment robustness without compromising general
capabilities. Our findings reveal fundamental flaws in current alignment
paradigms and call for representation-level training strategies that move
beyond surface-level behavior supervision. Codes and results are available at
https://github.com/Carol-gutianle/LatentSafety.

</details>


### [340] [A Brain-to-Population Graph Learning Framework for Diagnosing Brain Disorders](https://arxiv.org/abs/2506.16096)
*Qianqian Liao,Wuque Cai,Hongze Sun,Dongze Liu,Duo Chen,Dezhong Yao,Daqing Guo*

Main category: cs.LG

TL;DR: The paper introduces a new graph-based method for diagnosing brain disorders by addressing shortcomings of predefined brain atlases and incorporating semantic and confounding-aware features.


<details>
  <summary>Details</summary>
Motivation: Current graph-based methods rely too heavily on predefined brain atlases, ignoring embedded rich data and confounding factors such as variability in site and phenotype.

Method: The authors propose a two-stage Brain-to-Population Graph Learning framework. Stage one enriches graph representation by using GPT-4 and adaptive node reassignment. Stage two integrates phenotypic data to enhance population graph construction and mitigate confounding factors.

Result: The framework significantly improves prediction accuracy and interpretability, outperforming state-of-the-art methods in experiments on major datasets like ABIDE I, ADHD-200, and Rest-meta-MDD.

Conclusion: This novel framework not only enhances diagnostic accuracy and interpretability but also moves closer to personalized and clinically applicable methods for brain disorder diagnosis.

Abstract: Recent developed graph-based methods for diagnosing brain disorders using
functional connectivity highly rely on predefined brain atlases, but overlook
the rich information embedded within atlases and the confounding effects of
site and phenotype variability. To address these challenges, we propose a
two-stage Brain-to-Population Graph Learning (B2P-GL) framework that integrates
the semantic similarity of brain regions and condition-based population graph
modeling. In the first stage, termed brain representation learning, we leverage
brain atlas knowledge from GPT-4 to enrich the graph representation and refine
the brain graph through an adaptive node reassignment graph attention network.
In the second stage, termed population disorder diagnosis, phenotypic data is
incorporated into population graph construction and feature fusion to mitigate
confounding effects and enhance diagnosis performance. Experiments on the ABIDE
I, ADHD-200, and Rest-meta-MDD datasets show that B2P-GL outperforms
state-of-the-art methods in prediction accuracy while enhancing
interpretability. Overall, our proposed framework offers a reliable and
personalized approach to brain disorder diagnosis, advancing clinical
applicability.

</details>


### [341] [Mitigating Over-Squashing in Graph Neural Networks by Spectrum-Preserving Sparsification](https://arxiv.org/abs/2506.16110)
*Langzhang Liang,Fanchen Bu,Zixing Song,Zenglin Xu,Shirui Pan,Kijung Shin*

Main category: cs.LG

TL;DR: The paper addresses the "over-squashing" issue in Graph Neural Networks (GNNs) by proposing a novel graph rewiring method using spectrum-preserving graph sparsification to improve connectivity while retaining the graph's spectral properties.


<details>
  <summary>Details</summary>
Motivation: The study aims to overcome the challenge of information exchange across distant nodes in GNNs, which often fails due to structural bottlenecks causing over-squashing. Existing rewiring methods either increase computational overhead or modify graph properties like the spectrum, leading to inefficiency.

Method: A spectrum-preserving graph sparsification method is introduced for graph rewiring. This approach improves connectivity without significantly increasing edge density and largely preserves the spectral properties of the original graph.

Result: The proposed method enhances the connectivity of sparse graphs while preserving their spectral properties. Experimental results show improved classification accuracy and significant retention of the Laplacian spectrum compared to baseline methods.

Conclusion: By balancing connectivity improvement and graph property preservation, the method effectively mitigates structural bottlenecks in GNNs, proving to be superior to existing alternatives in performance and efficiency.

Abstract: The message-passing paradigm of Graph Neural Networks often struggles with
exchanging information across distant nodes typically due to structural
bottlenecks in certain graph regions, a limitation known as
\textit{over-squashing}. To reduce such bottlenecks, \textit{graph rewiring},
which modifies graph topology, has been widely used. However, existing graph
rewiring techniques often overlook the need to preserve critical properties of
the original graph, e.g., \textit{spectral properties}. Moreover, many
approaches rely on increasing edge count to improve connectivity, which
introduces significant computational overhead and exacerbates the risk of
over-smoothing. In this paper, we propose a novel graph rewiring method that
leverages \textit{spectrum-preserving} graph \textit{sparsification}, for
mitigating over-squashing. Our method generates graphs with enhanced
connectivity while maintaining sparsity and largely preserving the original
graph spectrum, effectively balancing structural bottleneck reduction and graph
property preservation. Experimental results validate the effectiveness of our
approach, demonstrating its superiority over strong baseline methods in
classification accuracy and retention of the Laplacian spectrum.

</details>


### [342] [From Teacher to Student: Tracking Memorization Through Model Distillation](https://arxiv.org/abs/2506.16170)
*Simardeep Singh*

Main category: cs.LG

TL;DR: The paper investigates how knowledge distillation (KD) methods influence the memorization of data in fine-tuned language models compared to standard fine-tuning.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by concerns about the privacy and security risks stemming from model memorization of training data, especially in large language models.

Method: The authors evaluate various KD methods to study their effects on memorization when a large teacher model is distilled into smaller student variants.

Result: Fine-tuning a larger teacher model and distilling it into smaller models was found to be more effective in reducing memorization risks than standard fine-tuning.

Conclusion: Knowledge distillation not only reduces computational costs and model size but also mitigates the risks of data memorization in fine-tuned language models.

Abstract: Large language models (LLMs) are known to memorize parts of their training
data, raising important concerns around privacy and security. While previous
research has focused on studying memorization in pre-trained models, much less
is known about how knowledge distillation (KD) affects memorization.In this
study, we explore how different KD methods influence the memorization of
fine-tuned task data when a large teacher model is distilled into smaller
student variants.This study demonstrates that distilling a larger teacher
model, fine-tuned on a dataset, into a smaller variant not only lowers
computational costs and model size but also significantly reduces the
memorization risks compared to standard fine-tuning approaches.

</details>


### [343] [Hallucination Level of Artificial Intelligence Whisperer: Case Speech Recognizing Pantterinousut Rap Song](https://arxiv.org/abs/2506.16174)
*Ismo Horppu,Frederick Ayala,Erlin Gulbenkoglu*

Main category: cs.LG

TL;DR: This paper examines the effectiveness of AI in transcribing Finnish rap lyrics, comparing Faster Whisperer and YouTube's speech-to-text algorithms.


<details>
  <summary>Details</summary>
Motivation: Finnish, a complex language, combined with artistic speech can be challenging for AI-based transcription. The authors aim to evaluate the AI's capability in handling such a demanding task.

Method: The study uses Faster Whisperer and YouTube's speech-to-text systems to transcribe Finnish rap lyrics written by Mc Timo, considering hallucinations and mishearings against the original text.

Result: Errors produced by both algorithms will be analyzed informally against the original Finnish lyrics to assess performance.

Conclusion: The study provides insights into the capabilities and limitations of AI transcription tools for complex linguistic and artistic scenarios.

Abstract: All languages are peculiar. Some of them are considered more challenging to
understand than others. The Finnish Language is known to be a complex language.
Also, when languages are used by artists, the pronunciation and meaning might
be more tricky to understand. Therefore, we are putting AI to a fun, yet
challenging trial: translating a Finnish rap song to text. We will compare the
Faster Whisperer algorithm and YouTube's internal speech-to-text functionality.
The reference truth will be Finnish rap lyrics, which the main author's little
brother, Mc Timo, has written. Transcribing the lyrics will be challenging
because the artist raps over synth music player by Syntikka Janne. The
hallucination level and mishearing of AI speech-to-text extractions will be
measured by comparing errors made against the original Finnish lyrics. The
error function is informal but still works for our case.

</details>


### [344] [Efficient and Privacy-Preserving Soft Prompt Transfer for LLMs](https://arxiv.org/abs/2506.16196)
*Xun Wang,Jing Xu,Franziska Boenisch,Michael Backes,Christopher A. Choquette-Choo,Adam Dziedzic*

Main category: cs.LG

TL;DR: This paper introduces POST, a framework for privately tuning soft prompts on small language models and transferring them to larger LLMs efficiently and securely using knowledge distillation.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the inefficiencies and privacy concerns of tuning soft prompts for large language models (LLMs), particularly the high computational costs and data privacy issues.

Method: POST uses knowledge distillation to derive a small model from the larger LLM. The soft prompts are tuned on the smaller model locally, potentially with differential privacy guarantees, and then transferred to the larger LLM using a small public dataset.

Result: POST successfully reduces the computational costs, preserves user data privacy, and enables the efficient transfer of high-utility soft prompts to large LLMs.

Conclusion: The proposed framework enhances privacy and efficiency in adapting LLMs by allowing soft prompt transfer between models while addressing computational and data-sharing limitations.

Abstract: Prompting has become a dominant paradigm for adapting large language models
(LLMs). While discrete (textual) prompts are widely used for their
interpretability, soft (parameter) prompts have recently gained traction in
APIs. This is because they can encode information from more training samples
while minimizing the user's token usage, leaving more space in the context
window for task-specific input. However, soft prompts are tightly coupled to
the LLM they are tuned on, limiting their generalization to other LLMs. This
constraint is particularly problematic for efficiency and privacy: (1) tuning
prompts on each LLM incurs high computational costs, especially as LLMs
continue to grow in size. Additionally, (2) when the LLM is hosted externally,
soft prompt tuning often requires sharing private data with the LLM provider.
For instance, this is the case with the NVIDIA NeMo API. To address these
issues, we propose POST (Privacy Of Soft prompt Transfer), a framework that
enables private tuning of soft prompts on a small model and subsequently
transfers these prompts to a larger LLM. POST uses knowledge distillation to
derive a small model directly from the large LLM to improve prompt
transferability, tunes the soft prompt locally, optionally with differential
privacy guarantees, and transfers it back to the larger LLM using a small
public dataset. Our experiments show that POST reduces computational costs,
preserves privacy, and effectively transfers high-utility soft prompts.

</details>


### [345] [From Pixels to CSI: Distilling Latent Dynamics For Efficient Wireless Resource Management](https://arxiv.org/abs/2506.16216)
*Charbel Bou Chaaya,Abanoub M. Girgis,Mehdi Bennis*

Main category: cs.LG

TL;DR: The paper presents a machine learning-based approach to optimizing radio resource management in a controller-device communication system, reducing power usage while maintaining control performance.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of efficiently managing communication resources without compromising the performance of control systems.

Method: The authors introduce two coupled joint-embedding predictive architectures (JEPAs) for modeling control dynamics and wireless channel conditions, combined with deep reinforcement learning for control policy and scheduling optimization.

Result: Simulation results show the approach achieves over 50% reduction in transmit power while maintaining control task performance comparable to baseline methods.

Conclusion: The proposed method successfully optimizes wireless communication resource usage in control systems, proving effective at balancing power efficiency and task performance.

Abstract: In this work, we aim to optimize the radio resource management of a
communication system between a remote controller and its device, whose state is
represented through image frames, without compromising the performance of the
control task. We propose a novel machine learning (ML) technique to jointly
model and predict the dynamics of the control system as well as the wireless
propagation environment in latent space. Our method leverages two coupled
joint-embedding predictive architectures (JEPAs): a control JEPA models the
control dynamics and guides the predictions of a wireless JEPA, which captures
the dynamics of the device's channel state information (CSI) through
cross-modal conditioning. We then train a deep reinforcement learning (RL)
algorithm to derive a control policy from latent control dynamics and a power
predictor to estimate scheduling intervals with favorable channel conditions
based on latent CSI representations. As such, the controller minimizes the
usage of radio resources by utilizing the coupled JEPA networks to imagine the
device's trajectory in latent space. We present simulation results on synthetic
multimodal data and show that our proposed approach reduces transmit power by
over 50% while maintaining control performance comparable to baseline methods
that do not account for wireless optimization.

</details>


### [346] [Think Global, Act Local: Bayesian Causal Discovery with Language Models in Sequential Data](https://arxiv.org/abs/2506.16234)
*Prakhar Verma,David Arbour,Sunav Choudhary,Harshita Chopra,Arno Solin,Atanu R. Sinha*

Main category: cs.LG

TL;DR: BLANCE integrates batch-based observational data and noisy expert knowledge from Language Models into a Bayesian framework to improve causal discovery accuracy.


<details>
  <summary>Details</summary>
Motivation: To address practical challenges in causal discovery, such as scarce domain expertise and noisy data from Language Models, while improving robustness and structural accuracy.

Method: BLANCE employs a Bayesian framework using Partial Ancestral Graphs instead of Directed Acyclic Graphs, integrates observational and LM-derived data, and uses sequential optimization to query informative edges.

Result: BLANCE surpasses prior methods in structural accuracy and robustness, and effectively incorporates Bayesian parameter estimation despite LM-induced noise.

Conclusion: This hybrid framework demonstrates adaptability in integrating noisy external knowledge with observational data while maintaining robustness and improving causal discovery outputs.

Abstract: Causal discovery from observational data typically assumes full access to
data and availability of domain experts. In practice, data often arrive in
batches, and expert knowledge is scarce. Language Models (LMs) offer a
surrogate but come with their own issues-hallucinations, inconsistencies, and
bias. We present BLANCE (Bayesian LM-Augmented Causal Estimation)-a hybrid
Bayesian framework that bridges these gaps by adaptively integrating sequential
batch data with LM-derived noisy, expert knowledge while accounting for both
data-induced and LM-induced biases. Our proposed representation shift from
Directed Acyclic Graph (DAG) to Partial Ancestral Graph (PAG) accommodates
ambiguities within a coherent Bayesian framework, allowing grounding the global
LM knowledge in local observational data. To guide LM interaction, we use a
sequential optimization scheme that adaptively queries the most informative
edges. Across varied datasets, BLANCE outperforms prior work in structural
accuracy and extends to Bayesian parameter estimation, showing robustness to LM
noise.

</details>


### [347] [Active MRI Acquisition with Diffusion Guided Bayesian Experimental Design](https://arxiv.org/abs/2506.16237)
*Jacopo Iollo,Geoffroy Oudoumanessah,Carole Lartizien,Michel Dojat,Florence Forbes*

Main category: cs.LG

TL;DR: The paper presents a method to optimize MRI acquisition times using Bayesian experimental design (BED) and diffusion-based generative models.


<details>
  <summary>Details</summary>
Motivation: Accelerating MRI acquisition times while maintaining high image quality for effective clinical use.

Method: The authors propose a sequential Bayesian experimental design (BED) method that adaptively and task-dependently selects the most informative measurements, utilizing diffusion-based generative models and stochastic optimization.

Result: Their approach enhances MRI acquisition processes by optimizing image reconstruction and associated analysis tasks, as demonstrated across various applications.

Conclusion: The proposed methodology successfully balances acquisition speed and image quality, with potential for broad MRI-related applications.

Abstract: A key challenge in maximizing the benefits of Magnetic Resonance Imaging
(MRI) in clinical settings is to accelerate acquisition times without
significantly degrading image quality. This objective requires a balance
between under-sampling the raw k-space measurements for faster acquisitions and
gathering sufficient raw information for high-fidelity image reconstruction and
analysis tasks. To achieve this balance, we propose to use sequential Bayesian
experimental design (BED) to provide an adaptive and task-dependent selection
of the most informative measurements. Measurements are sequentially augmented
with new samples selected to maximize information gain on a posterior
distribution over target images. Selection is performed via a gradient-based
optimization of a design parameter that defines a subsampling pattern. In this
work, we introduce a new active BED procedure that leverages diffusion-based
generative models to handle the high dimensionality of the images and employs
stochastic optimization to select among a variety of patterns while meeting the
acquisition process constraints and budget. So doing, we show how our setting
can optimize, not only standard image reconstruction, but also any associated
image analysis task. The versatility and performance of our approach are
demonstrated on several MRI acquisitions.

</details>


### [348] [Synthetic ALS-EEG Data Augmentation for ALS Diagnosis Using Conditional WGAN with Weight Clipping](https://arxiv.org/abs/2506.16243)
*Abdulvahap Mutlu,Şengül Doğan,Türker Tuncer*

Main category: cs.LG

TL;DR: The paper tackles the issue of scarce, imbalanced EEG data for ALS classification using Conditional Wasserstein GANs to generate synthetic samples, improving classifier accuracy.


<details>
  <summary>Details</summary>
Motivation: ALS EEG recordings are rare and typically imbalanced compared to healthy controls, making it challenging to train effective machine learning models for ALS detection.

Method: The authors used a Conditional Wasserstein Generative Adversarial Network (CWGAN) to generate synthetic ALS EEG signals, trained on a private dataset after preprocessing and normalization.

Result: The CWGAN successfully learned the ALS EEG distribution, generating synthetic samples that closely resemble real ALS EEG patterns. Stabilized loss curves indicate effective training.

Conclusion: Synthetic data generated by CWGAN can alleviate class imbalance, enhance diagnostic models, and enable better data sharing for ALS detection.

Abstract: Amyotrophic Lateral Sclerosis (ALS) is a rare neurodegenerative disease, and
high-quality EEG data from ALS patients are scarce. This data scarcity, coupled
with severe class imbalance between ALS and healthy control recordings, poses a
challenge for training reliable machine learning classifiers. In this work, we
address these issues by generating synthetic EEG signals for ALS patients using
a Conditional Wasserstein Generative Adversarial Network (CWGAN). We train
CWGAN on a private EEG dataset (ALS vs. non-ALS) to learn the distribution of
ALS EEG signals and produce realistic synthetic samples. We preprocess and
normalize EEG recordings, and train a CWGAN model to generate synthetic ALS
signals. The CWGAN architecture and training routine are detailed, with key
hyperparameters chosen for stable training. Qualitative evaluation of generated
signals shows that they closely mimic real ALS EEG patterns. The CWGAN training
converged with generator and discriminator loss curves stabilizing, indicating
successful learning. The synthetic EEG signals appear realistic and have
potential use as augmented data for training classifiers, helping to mitigate
class imbalance and improve ALS detection accuracy. We discuss how this
approach can facilitate data sharing and enhance diagnostic models.

</details>


### [349] [Optimal Online Bookmaking for Any Number of Outcomes](https://arxiv.org/abs/2506.16253)
*Hadar Tal,Oron Sabag*

Main category: cs.LG

TL;DR: This paper addresses how bookmakers can adjust odds dynamically to optimize profit while minimizing worst-case losses in betting scenarios.


<details>
  <summary>Details</summary>
Motivation: To provide bookmakers with a systematic way to minimize financial risk while allowing fair betting opportunities.

Method: The authors compute the bookmaker's optimal loss using a polynomial's largest root and introduce an efficient algorithm that exploits Bellman-Pareto frontier concepts.

Result: They derive the mathematical foundations and provide an algorithm that minimizes losses in both optimal and suboptimal gambling scenarios.

Conclusion: The paper concludes that bookmakers can achieve fairness and reduced financial risk using dynamic optimization strategies that leverage Hermite polynomials and game theory principles.

Abstract: We study the Online Bookmaking problem, where a bookmaker dynamically updates
betting odds on the possible outcomes of an event. In each betting round, the
bookmaker can adjust the odds based on the cumulative betting behavior of
gamblers, aiming to maximize profit while mitigating potential loss. We show
that for any event and any number of betting rounds, in a worst-case setting
over all possible gamblers and outcome realizations, the bookmaker's optimal
loss is the largest root of a simple polynomial. Our solution shows that
bookmakers can be as fair as desired while avoiding financial risk, and the
explicit characterization reveals an intriguing relation between the
bookmaker's regret and Hermite polynomials. We develop an efficient algorithm
that computes the optimal bookmaking strategy: when facing an optimal gambler,
the algorithm achieves the optimal loss, and in rounds where the gambler is
suboptimal, it reduces the achieved loss to the optimal opportunistic loss, a
notion that is related to subgame perfect Nash equilibrium. The key technical
contribution to achieve these results is an explicit characterization of the
Bellman-Pareto frontier, which unifies the dynamic programming updates for
Bellman's value function with the multi-criteria optimization framework of the
Pareto frontier in the context of vector repeated games.

</details>


### [350] [Next-Token Prediction Should be Ambiguity-Sensitive: A Meta-Learning Perspective](https://arxiv.org/abs/2506.16288)
*Leo Gagnon,Eric Elmoznino,Sarthak Mittal,Tom Marty,Tejas Kasetty,Dhanya Sridhar,Guillaume Lajoie*

Main category: cs.LG

TL;DR: Auto-regressive foundation models face computational issues under high ambiguity for next-token prediction. The paper introduces MetaHMM, a synthetic benchmark, and proposes Monte Carlo predictors to address these issues, yielding gains in ambiguous contexts.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address limitations of auto-regressive foundation models in handling high-ambiguity next-token predictions, where standard methods become computationally intractable.

Method: The researchers introduce MetaHMM, a synthetic benchmark, and suggest a Monte Carlo approach to convert pre-trained models for better handling of ambiguous predictions.

Result: Transformers were shown to struggle with high-ambiguity predictions. Employing Monte Carlo predictors improved computational efficiency and accuracy in ambiguous contexts.

Conclusion: Cognitive-inspired heuristics can enhance auto-regressive models' predictions in ambiguous scenarios, though further refinements are needed to tackle remaining challenges.

Abstract: The rapid adaptation ability of auto-regressive foundation models is often
attributed to the diversity of their pre-training data. This is because, from a
Bayesian standpoint, minimizing prediction error in such settings requires
integrating over all plausible latent hypotheses consistent with observations.
While this behavior is desirable in principle, it often proves too ambitious in
practice: under high ambiguity, the number of plausible latent alternatives
makes Bayes-optimal prediction computationally intractable. Cognitive science
has long recognized this limitation, suggesting that under such conditions,
heuristics or information-seeking strategies are preferable to exhaustive
inference. Translating this insight to next-token prediction, we hypothesize
that low- and high-ambiguity predictions pose different computational demands,
making ambiguity-agnostic next-token prediction a detrimental inductive bias.
To test this, we introduce MetaHMM, a synthetic sequence meta-learning
benchmark with rich compositional structure and a tractable Bayesian oracle. We
show that Transformers indeed struggle with high-ambiguity predictions across
model sizes. Motivated by cognitive theories, we propose a method to convert
pre-trained models into Monte Carlo predictors that decouple task inference
from token prediction. Preliminary results show substantial gains in ambiguous
contexts through improved capacity allocation and test-time scalable inference,
though challenges remain.

</details>


### [351] [Optimizing Multilingual Text-To-Speech with Accents & Emotions](https://arxiv.org/abs/2506.16310)
*Pranav Pawar,Akshansh Dwivedi,Jenish Boricha,Himanshu Gohil,Aditya Dubey*

Main category: cs.LG

TL;DR: This research introduces a text-to-speech (TTS) system that enhances multilingual accents and emotion modeling, especially for Hindi and Indian English.


<details>
  <summary>Details</summary>
Motivation: Current TTS systems struggle with multilingual accent accuracy and cultural nuances, especially for Indic languages.

Method: The authors extend Parler-TTS using a hybrid phoneme alignment encoder-decoder, culture-sensitive emotion embedding layers, native speaker corpora, and dynamic accent code switching.

Result: The system showed a 23.7% improvement in accent accuracy and 85.3% emotion recognition accuracy, outperforming existing baselines like METTS and VECL-TTS.

Conclusion: This architecture advances cross-lingual synthesis, particularly for South Asian domains, by providing scalable accent-emotion disentanglement suitable for applications like EdTech and accessibility tools.

Abstract: State-of-the-art text-to-speech (TTS) systems realize high naturalness in
monolingual environments, synthesizing speech with correct multilingual accents
(especially for Indic languages) and context-relevant emotions still poses
difficulty owing to cultural nuance discrepancies in current frameworks. This
paper introduces a new TTS architecture integrating accent along with
preserving transliteration with multi-scale emotion modelling, in particularly
tuned for Hindi and Indian English accent. Our approach extends the Parler-TTS
model by integrating A language-specific phoneme alignment hybrid
encoder-decoder architecture, and culture-sensitive emotion embedding layers
trained on native speaker corpora, as well as incorporating a dynamic accent
code switching with residual vector quantization. Quantitative tests
demonstrate 23.7% improvement in accent accuracy (Word Error Rate reduction
from 15.4% to 11.8%) and 85.3% emotion recognition accuracy from native
listeners, surpassing METTS and VECL-TTS baselines. The novelty of the system
is that it can mix code in real time - generating statements such as "Namaste,
let's talk about <Hindi phrase>" with uninterrupted accent shifts while
preserving emotional consistency. Subjective evaluation with 200 users reported
a mean opinion score (MOS) of 4.2/5 for cultural correctness, much better than
existing multilingual systems (p<0.01). This research makes cross-lingual
synthesis more feasible by showcasing scalable accent-emotion disentanglement,
with direct application in South Asian EdTech and accessibility software.

</details>


### [352] [Improved Exploration in GFlownets via Enhanced Epistemic Neural Networks](https://arxiv.org/abs/2506.16313)
*Sajan Muhammad,Salem Lahlou*

Main category: cs.LG

TL;DR: This paper improves exploration in GFlowNets by using epistemic neural networks to enhance uncertainty quantification.


<details>
  <summary>Details</summary>
Motivation: Current GFlowNets struggle to select training trajectories due to insufficient exploration of state spaces where the reward distribution is not well-learned.

Method: The authors integrate epistemic neural networks (ENN) with GFlowNets to improve joint predictions and uncertainty quantification.

Result: The proposed ENN-GFN-Enhanced algorithm outperforms the baseline GFlowNet in grid environments and structured sequence generation tasks.

Conclusion: ENN-GFN-Enhanced advances exploration and trajectory optimization in GFlowNets through superior uncertainty-driven exploration capability.

Abstract: Efficiently identifying the right trajectories for training remains an open
problem in GFlowNets. To address this, it is essential to prioritize
exploration in regions of the state space where the reward distribution has not
been sufficiently learned. This calls for uncertainty-driven exploration, in
other words, the agent should be aware of what it does not know. This attribute
can be measured by joint predictions, which are particularly important for
combinatorial and sequential decision problems. In this research, we integrate
epistemic neural networks (ENN) with the conventional architecture of GFlowNets
to enable more efficient joint predictions and better uncertainty
quantification, thereby improving exploration and the identification of optimal
trajectories. Our proposed algorithm, ENN-GFN-Enhanced, is compared to the
baseline method in GFlownets and evaluated in grid environments and structured
sequence generation in various settings, demonstrating both its efficacy and
efficiency.

</details>


### [353] [Signatures to help interpretability of anomalies](https://arxiv.org/abs/2506.16314)
*Emmanuel Gangler,Emille E. O. Ishida,Matwey V. Kornilov,Vladimir Korolev,Anastasia Lavrukhina,Konstantin Malanchev,Maria V. Pruzhinskaya,Etienne Russeil,Timofey Semenikhin,Sreevarsha Sreejith,Alina A. Volnova*

Main category: cs.LG

TL;DR: The paper introduces 'anomaly signature' to enhance the interpretability of anomaly detection systems, revealing which features contribute to decisions.


<details>
  <summary>Details</summary>
Motivation: Astronomers often struggle to understand why machine learning algorithms flag certain events as anomalies since these systems lack interpretability.

Method: The concept of 'anomaly signature' is proposed to pinpoint which features significantly contribute to an anomaly detection system's output.

Result: The idea aids in improving the transparency and interpretability of anomaly detection processes.

Conclusion: Anomaly signatures offer a valuable pathway toward understanding the feature-level reasoning behind flagged anomalies, bridging the interpretability gap in machine learning-driven anomaly detection.

Abstract: Machine learning is often viewed as a black box when it comes to
understanding its output, be it a decision or a score. Automatic anomaly
detection is no exception to this rule, and quite often the astronomer is left
to independently analyze the data in order to understand why a given event is
tagged as an anomaly. We introduce here idea of anomaly signature, whose aim is
to help the interpretability of anomalies by highlighting which features
contributed to the decision.

</details>


### [354] [Bayesian Optimization over Bounded Domains with the Beta Product Kernel](https://arxiv.org/abs/2506.16316)
*Huy Hoang Nguyen,Han Zhou,Matthew B. Blaschko,Aleksei Tiulpin*

Main category: cs.LG

TL;DR: This paper introduces the Beta kernel, a non-stationary kernel tailored for bounded domains, and demonstrates its effectiveness in Bayesian optimization tasks, outperforming common kernels like Matérn and RBF.


<details>
  <summary>Details</summary>
Motivation: Current covariance functions such as Matérn and RBF do not leverage assumptions about bounded domains, challenging their performance in such settings. The paper seeks to address this limitation.

Method: The authors propose the Beta kernel, derived from a product of Beta distribution density functions, enabling natural modeling of functions confined to bounded domains. Empirical spectral analyses are conducted to assess its eigendecay properties.

Result: The experimental evidence shows the Beta kernel excels in optimizing functions with features near the boundaries, surpassing traditional kernels like Matérn and RBF in several benchmarks, including synthetic and real-world applications.

Conclusion: The Beta kernel is a robust alternative for bounded domain optimization tasks, offering significant improvements over widely-used kernels like Matérn and RBF across diverse scenarios.

Abstract: Bayesian optimization with Gaussian processes (GP) is commonly used to
optimize black-box functions. The Mat\'ern and the Radial Basis Function (RBF)
covariance functions are used frequently, but they do not make any assumptions
about the domain of the function, which may limit their applicability in
bounded domains. To address the limitation, we introduce the Beta kernel, a
non-stationary kernel induced by a product of Beta distribution density
functions. Such a formulation allows our kernel to naturally model functions on
bounded domains. We present statistical evidence supporting the hypothesis that
the kernel exhibits an exponential eigendecay rate, based on empirical analyses
of its spectral properties across different settings. Our experimental results
demonstrate the robustness of the Beta kernel in modeling functions with optima
located near the faces or vertices of the unit hypercube. The experiments show
that our kernel consistently outperforms a wide range of kernels, including the
well-known Mat\'ern and RBF, in different problems, including synthetic
function optimization and the compression of vision and language models.

</details>


### [355] [Watermarking Autoregressive Image Generation](https://arxiv.org/abs/2506.16349)
*Nikola Jovanović,Ismail Labiad,Tomáš Souček,Martin Vechev,Pierre Fernandez*

Main category: cs.LG

TL;DR: This paper introduces a novel watermarking method for autoregressive image generation models using token-level techniques, addressing significant technical challenges like reverse cycle-consistency.


<details>
  <summary>Details</summary>
Motivation: The motivation is to develop reliable techniques for watermarking outputs from generative models to track their origins and prevent misuse, focusing on autoregressive image generation models.

Method: The paper adapts language model watermarking techniques for autoregressive image models, introduces a custom tokenizer-detokenizer finetuning procedure to improve reverse cycle-consistency, and adds a watermark synchronization layer.

Result: Experiments show that the proposed watermarking approach is effective in enabling detection and is robust to common image transformations, neural compression, and removal attacks, with theoretically grounded p-values.

Conclusion: This work provides a foundational solution for token-level watermarking in autoregressive image generation, addressing key challenges and achieving robustness and reliability.

Abstract: Watermarking the outputs of generative models has emerged as a promising
approach for tracking their provenance. Despite significant interest in
autoregressive image generation models and their potential for misuse, no prior
work has attempted to watermark their outputs at the token level. In this work,
we present the first such approach by adapting language model watermarking
techniques to this setting. We identify a key challenge: the lack of reverse
cycle-consistency (RCC), wherein re-tokenizing generated image tokens
significantly alters the token sequence, effectively erasing the watermark. To
address this and to make our method robust to common image transformations,
neural compression, and removal attacks, we introduce (i) a custom
tokenizer-detokenizer finetuning procedure that improves RCC, and (ii) a
complementary watermark synchronization layer. As our experiments demonstrate,
our approach enables reliable and robust watermark detection with theoretically
grounded p-values.

</details>


### [356] [Data-Driven Policy Mapping for Safe RL-based Energy Management Systems](https://arxiv.org/abs/2506.16352)
*Theo Zangato,Aomar Osmani,Pegah Alizadeh*

Main category: cs.LG

TL;DR: The paper introduces a scalable RL-based Building Energy Management System (BEMS) for cost-efficient and adaptive energy management, addressing scalability, adaptability, and safety challenges through clustering, forecasting, and constrained policy learning.


<details>
  <summary>Details</summary>
Motivation: Buildings are central to sustainable energy due to increasing global energy demand and complexities in renewable integration.

Method: The approach uses a three-step process: clustering non-shiftable loads, LSTM-based forecasting for dynamic conditions, and domain-informed action masking for safety.

Result: The system reduces operating costs by up to 15%, supports classification and optimization of new buildings, adapts to tariff changes, and ensures environmental stability based on real-world evaluations.

Conclusion: The framework is robust, scalable, and enhances sustainable building energy management with minimal retraining and adaptability to dynamic conditions.

Abstract: Increasing global energy demand and renewable integration complexity have
placed buildings at the center of sustainable energy management. We present a
three-step reinforcement learning(RL)-based Building Energy Management System
(BEMS) that combines clustering, forecasting, and constrained policy learning
to address scalability, adaptability, and safety challenges. First, we cluster
non-shiftable load profiles to identify common consumption patterns, enabling
policy generalization and transfer without retraining for each new building.
Next, we integrate an LSTM based forecasting module to anticipate future
states, improving the RL agents' responsiveness to dynamic conditions. Lastly,
domain-informed action masking ensures safe exploration and operation,
preventing harmful decisions. Evaluated on real-world data, our approach
reduces operating costs by up to 15% for certain building types, maintains
stable environmental performance, and quickly classifies and optimizes new
buildings with limited data. It also adapts to stochastic tariff changes
without retraining. Overall, this framework delivers scalable, robust, and
cost-effective building energy management.

</details>


### [357] [Classification of Cattle Behavior and Detection of Heat (Estrus) using Sensor Data](https://arxiv.org/abs/2506.16380)
*Druva Dhakshinamoorthy,Avikshit Jha,Sabyasachi Majumdar,Devdulal Ghosh,Ranjita Chakraborty,Hena Ray*

Main category: cs.LG

TL;DR: The paper presents a low-cost system using sensor-equipped collars and machine learning models to monitor cattle behaviors and detect estrus with high accuracy.


<details>
  <summary>Details</summary>
Motivation: To provide an accessible and scalable solution for precision livestock monitoring, particularly targeting resource-constrained environments.

Method: A Bluetooth-based collar was designed with sensors to collect real-time behavioral data, paired with labeled CCTV footage, and processed through various machine learning models including SVM, RF, CNN, and LSTM.

Result: The system attained over 93% classification accuracy for cattle behaviors and 96% accuracy for estrus detection in initial tests.

Conclusion: The proposed system demonstrates a promising approach for improving livestock management by leveraging affordable technology and robust machine learning algorithms.

Abstract: This paper presents a novel system for monitoring cattle behavior and
detecting estrus (heat) periods using sensor data and machine learning. We
designed and deployed a low-cost Bluetooth-based neck collar equipped with
accelerometer and gyroscope sensors to capture real-time behavioral data from
real cows, which was synced to the cloud. A labeled dataset was created using
synchronized CCTV footage to annotate behaviors such as feeding, rumination,
lying, and others. We evaluated multiple machine learning models -- Support
Vector Machines (SVM), Random Forests (RF), and Convolutional Neural Networks
(CNN) -- for behavior classification. Additionally, we implemented a Long
Short-Term Memory (LSTM) model for estrus detection using behavioral patterns
and anomaly detection. Our system achieved over 93% behavior classification
accuracy and 96% estrus detection accuracy on a limited test set. The approach
offers a scalable and accessible solution for precision livestock monitoring,
especially in resource-constrained environments.

</details>


### [358] [State-Space Kolmogorov Arnold Networks for Interpretable Nonlinear System Identification](https://arxiv.org/abs/2506.16392)
*Gonçalo Granjal Cruz,Balazs Renczes,Mark C Runacres,Jan Decuyper*

Main category: cs.LG

TL;DR: This paper introduces SS-KAN for interpretable nonlinear system identification, enhancing clarity while slightly sacrificing accuracy compared to black-box models.


<details>
  <summary>Details</summary>
Motivation: Black-box models for system identification, although accurate, lack interpretability, motivating a need for models that balance accuracy and understanding of nonlinear system dynamics.

Method: The paper integrates Kolmogorov-Arnold Networks within a state-space framework and validates the approach using benchmark systems with sparsity-promoting regularization for enhancing interpretability.

Result: SS-KAN provides improved interpretability of system dynamics, enabling visualization of nonlinear features, but sacrifices some accuracy compared to state-of-the-art black-box models.

Conclusion: SS-KAN is a promising method for interpretable nonlinear system identification, striking a balance between accuracy and interpretability of underlying system dynamics.

Abstract: While accurate, black-box system identification models lack interpretability
of the underlying system dynamics. This paper proposes State-Space
Kolmogorov-Arnold Networks (SS-KAN) to address this challenge by integrating
Kolmogorov-Arnold Networks within a state-space framework. The proposed model
is validated on two benchmark systems: the Silverbox and the Wiener-Hammerstein
benchmarks. Results show that SS-KAN provides enhanced interpretability due to
sparsity-promoting regularization and the direct visualization of its learned
univariate functions, which reveal system nonlinearities at the cost of
accuracy when compared to state-of-the-art black-box models, highlighting
SS-KAN as a promising approach for interpretable nonlinear system
identification, balancing accuracy and interpretability of nonlinear system
dynamics.

</details>


### [359] [GoalLadder: Incremental Goal Discovery with Vision-Language Models](https://arxiv.org/abs/2506.16396)
*Alexey Zakharov,Shimon Whiteson*

Main category: cs.LG

TL;DR: GoalLadder uses vision-language models to train reinforcement learning agents from single language instructions in visual environments, streamlining goal state identification and improving task completion success rates.


<details>
  <summary>Details</summary>
Motivation: Reinforcement learning systems can benefit from natural language instructions for task specification, but leveraging them in visual environments faces challenges like noisy feedback and resource constraints.

Method: GoalLadder queries vision-language models incrementally to identify and rank goal states using pairwise comparisons and an ELO-based system, enabling RL agents to minimize distance to the top-ranked goal in a learned embedding space.

Result: GoalLadder achieves a ~95% success rate in task completion compared to ~45% from the best competitor in control and robotic manipulation environments.

Conclusion: GoalLadder efficiently utilizes limited feedback and improves learning outcomes, marking progress in using natural language for RL in visual environments.

Abstract: Natural language can offer a concise and human-interpretable means of
specifying reinforcement learning (RL) tasks. The ability to extract rewards
from a language instruction can enable the development of robotic systems that
can learn from human guidance; however, it remains a challenging problem,
especially in visual environments. Existing approaches that employ large,
pretrained language models either rely on non-visual environment
representations, require prohibitively large amounts of feedback, or generate
noisy, ill-shaped reward functions. In this paper, we propose a novel method,
$\textbf{GoalLadder}$, that leverages vision-language models (VLMs) to train RL
agents from a single language instruction in visual environments. GoalLadder
works by incrementally discovering states that bring the agent closer to
completing a task specified in natural language. To do so, it queries a VLM to
identify states that represent an improvement in agent's task progress and to
rank them using pairwise comparisons. Unlike prior work, GoalLadder does not
trust VLM's feedback completely; instead, it uses it to rank potential goal
states using an ELO-based rating system, thus reducing the detrimental effects
of noisy VLM feedback. Over the course of training, the agent is tasked with
minimising the distance to the top-ranked goal in a learned embedding space,
which is trained on unlabelled visual data. This key feature allows us to
bypass the need for abundant and accurate feedback typically required to train
a well-shaped reward function. We demonstrate that GoalLadder outperforms
existing related methods on classic control and robotic manipulation
environments with the average final success rate of $\sim$95% compared to only
$\sim$45% of the best competitor.

</details>


### [360] [Generating Directed Graphs with Dual Attention and Asymmetric Encoding](https://arxiv.org/abs/2506.16404)
*Alba Carballo-Castro,Manuel Madeira,Yiming Qin,Dorina Thanou,Pascal Frossard*

Main category: cs.LG

TL;DR: The paper introduces 'Directo,' a model for generating directed graphs using the discrete flow matching framework, addressing challenges like directionality and lack of benchmarks.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the need to model directed graphs common in biology, transportation, and social networks for tasks like simulation and data augmentation. Challenges include learning complex directional dependencies and lack of evaluation standards.

Method: The method leverages three key components: positional encodings for asymmetric relations, dual-attention to capture directional dependencies, and a discrete generative framework for robustness.

Result: The proposed model demonstrates strong performance on diverse datasets and competes with specialized models, showing its generality and reliability.

Conclusion: Directo establishes a scalable and generalizable approach to directed graph generation, offering a robust foundation for future research in the field.

Abstract: Directed graphs naturally model systems with asymmetric, ordered
relationships, essential to applications in biology, transportation, social
networks, and visual understanding. Generating such graphs enables tasks such
as simulation, data augmentation and novel instance discovery; however,
directed graph generation remains underexplored. We identify two key factors
limiting progress in this direction: first, modeling edge directionality
introduces a substantially larger dependency space, making the underlying
distribution harder to learn; second, the absence of standardized benchmarks
hinders rigorous evaluation. Addressing the former requires more expressive
models that are sensitive to directional topologies. We propose Directo, the
first generative model for directed graphs built upon the discrete flow
matching framework. Our approach combines: (i) principled positional encodings
tailored to asymmetric pairwise relations, (ii) a dual-attention mechanism
capturing both incoming and outgoing dependencies, and (iii) a robust, discrete
generative framework. To support evaluation, we introduce a benchmark suite
covering synthetic and real-world datasets. It shows that our method performs
strongly across diverse settings and even competes with specialized models for
particular classes, such as directed acyclic graphs. Our results highlight the
effectiveness and generality of our approach, establishing a solid foundation
for future research in directed graph generation.

</details>


### [361] [Drag-and-Drop LLMs: Zero-Shot Prompt-to-Weights](https://arxiv.org/abs/2506.16406)
*Zhiyuan Liang,Dongwen Tang,Yuhao Zhou,Xuanlei Zhao,Mingjia Shi,Wangbo Zhao,Zekai Li,Peihao Wang,Konstantin Schürholt,Damian Borth,Michael M. Bronstein,Yang You,Zhangyang Wang,Kai Wang*

Main category: cs.LG

TL;DR: The paper introduces Drag-and-Drop LLMs (DnD), a method that generates task-specific parameters using unlabeled prompts without requiring per-task optimization runs, offering significant efficiency and performance gains.


<details>
  <summary>Details</summary>
Motivation: To overcome the inefficiency of existing PEFT methods that require separate optimization for every downstream dataset.

Method: The paper proposes DnD, a prompt-conditioned parameter generator using a lightweight text encoder and a hyper-convolutional decoder to produce LoRA matrices without task-specific training.

Result: DnD achieves up to 12,000 times lower overhead compared to full fine-tuning, a 30% performance gain on unseen benchmarks, and robust cross-domain generalization.

Conclusion: Prompt-conditioned parameter generation with DnD is a viable and efficient alternative to gradient-based model adaptation for specializing large language models.

Abstract: Modern Parameter-Efficient Fine-Tuning (PEFT) methods such as low-rank
adaptation (LoRA) reduce the cost of customizing large language models (LLMs),
yet still require a separate optimization run for every downstream dataset. We
introduce \textbf{Drag-and-Drop LLMs (\textit{DnD})}, a prompt-conditioned
parameter generator that eliminates per-task training by mapping a handful of
unlabeled task prompts directly to LoRA weight updates. A lightweight text
encoder distills each prompt batch into condition embeddings, which are then
transformed by a cascaded hyper-convolutional decoder into the full set of LoRA
matrices. Once trained in a diverse collection of prompt-checkpoint pairs, DnD
produces task-specific parameters in seconds, yielding i) up to
\textbf{12,000$\times$} lower overhead than full fine-tuning, ii) average gains
up to \textbf{30\%} in performance over the strongest training LoRAs on unseen
common-sense reasoning, math, coding, and multimodal benchmarks, and iii)
robust cross-domain generalization despite never seeing the target data or
labels. Our results demonstrate that prompt-conditioned parameter generation is
a viable alternative to gradient-based adaptation for rapidly specializing
LLMs. Our project is available at
\href{https://jerryliang24.github.io/DnD}{https://jerryliang24.github.io/DnD}.

</details>


### [362] [Optimizing MoE Routers: Design, Implementation, and Evaluation in Transformer Models](https://arxiv.org/abs/2506.16419)
*Daniel Fidel Harvey,George Weale,Berk Yilmaz*

Main category: cs.LG

TL;DR: This paper explores various router architectures to improve token routing in Mixture of Experts (MoE) models, presenting trade-offs in speed, expressiveness, and structured sparse routing.


<details>
  <summary>Details</summary>
Motivation: Address the challenges of scalability and performance degradation in MoE models due to imbalanced token routing and reduced accuracy.

Method: Experimented with six router architectures—Linear, Attention, Multi-Layer Perceptron (MLP), Hybrid, Hash, and MLP-Hadamard—assessed using BERT and Qwen1.5-MoE models for performance metrics like parameter efficiency and inference latency.

Result: Linear routers were faster, MLP and Attention routers were more expressive, and MLP-Hadamard excelled in structured sparse routing. Custom routers were integrated successfully into the quantized Qwen1.5-MoE model.

Conclusion: This work provides a comparative analysis of router designs in MoE models, enabling optimization for scalability and effective deployment in large-scale systems.

Abstract: Mixture of Experts (MoE) architectures increase large language model
scalability, yet their performance depends on the router module that moves
tokens to specialized experts. Bad routing can load imbalance and reduced
accuracy. This project designed and implemented different router architectures
within Transformer models to fix these limitations. We experimented with six
distinct router variants Linear, Attention, Multi-Layer Perceptron (MLP),
Hybrid, Hash, and our new MLP-Hadamard. We characterized these routers using
BERT and the Qwen1.5-MoE model, looking at parameter efficiency, inference
latency, routing entropy, and expert utilization patterns. Our evaluations
showed distinct trade-offs: Linear routers offer speed, while MLP and Attention
routers provide greater expressiveness. The MLP-Hadamard router shows a unique
capability for structured, sparse routing. We successfully replaced and
fine-tuned custom routers within the complex, quantized Qwen1.5-MoE model. This
work provides a comparative analysis of MoE router designs and offers insights
into optimizing their performance for efficient and effective large-scale model
deployment.

</details>


### [363] [EFormer: An Effective Edge-based Transformer for Vehicle Routing Problems](https://arxiv.org/abs/2506.16428)
*Dian Meng,Zhiguang Cao,Yaoxin Wu,Yaqing Hou,Hongwei Ge,Qiang Zhang*

Main category: cs.LG

TL;DR: The study introduces EFormer, an edge-based Transformer model for Vehicle Routing Problems (VRPs), showing improved results using edge inputs for real-world and synthetic datasets.


<details>
  <summary>Details</summary>
Motivation: Existing VRP neural heuristics rely mainly on node coordinates, which may not effectively address real cost metrics, such as edge-based distances.

Method: EFormer uses a precoder module with mixed-score attention, parallel graph-node encoding, and multi-query mechanisms in decoding, trained through reinforcement learning.

Result: Experiments demonstrate superior performance of EFormer over established baselines across synthetic and real-world datasets.

Conclusion: EFormer’s design proves effective in processing edge-based metrics for VRPs, enhancing generalization and scalability for practical applications.

Abstract: Recent neural heuristics for the Vehicle Routing Problem (VRP) primarily rely
on node coordinates as input, which may be less effective in practical
scenarios where real cost metrics-such as edge-based distances-are more
relevant. To address this limitation, we introduce EFormer, an Edge-based
Transformer model that uses edge as the sole input for VRPs. Our approach
employs a precoder module with a mixed-score attention mechanism to convert
edge information into temporary node embeddings. We also present a parallel
encoding strategy characterized by a graph encoder and a node encoder, each
responsible for processing graph and node embeddings in distinct feature
spaces, respectively. This design yields a more comprehensive representation of
the global relationships among edges. In the decoding phase, parallel context
embedding and multi-query integration are used to compute separate attention
mechanisms over the two encoded embeddings, facilitating efficient path
construction. We train EFormer using reinforcement learning in an
autoregressive manner. Extensive experiments on the Traveling Salesman Problem
(TSP) and Capacitated Vehicle Routing Problem (CVRP) reveal that EFormer
outperforms established baselines on synthetic datasets, including large-scale
and diverse distributions. Moreover, EFormer demonstrates strong generalization
on real-world instances from TSPLib and CVRPLib. These findings confirm the
effectiveness of EFormer's core design in solving VRPs.

</details>


### [364] [An efficient neuromorphic approach for collision avoidance combining Stack-CNN with event cameras](https://arxiv.org/abs/2506.16436)
*Antonio Giulio Coretti,Mattia Varile,Mario Edoardo Bertaina*

Main category: cs.LG

TL;DR: The paper introduces a collision avoidance system using event-based cameras and a Stack-CNN algorithm to improve space situational awareness (SSA) and space traffic management (STM).


<details>
  <summary>Details</summary>
Motivation: To address the increasing threat posed by space debris and improve mitigation efforts.

Method: The system employs event-based cameras combined with a Stack-CNN algorithm, leveraging its meteor detection capabilities to analyze real-time data and detect faint moving objects.

Result: Testing on Earth-based data demonstrates the ability to enhance signal-to-noise ratio, improving the detection process for moving objects.

Conclusion: The collision avoidance system shows promise for on-board space imaging and advancing SSA and STM operations.

Abstract: Space debris poses a significant threat, driving research into active and
passive mitigation strategies. This work presents an innovative collision
avoidance system utilizing event-based cameras - a novel imaging technology
well-suited for Space Situational Awareness (SSA) and Space Traffic Management
(STM). The system, employing a Stack-CNN algorithm (previously used for meteor
detection), analyzes real-time event-based camera data to detect faint moving
objects. Testing on terrestrial data demonstrates the algorithm's ability to
enhance signal-to-noise ratio, offering a promising approach for on-board space
imaging and improving STM/SSA operations.

</details>


### [365] [Leveraging Influence Functions for Resampling Data in Physics-Informed Neural Networks](https://arxiv.org/abs/2506.16443)
*Jonas R. Naujoks,Aleksander Krasowski,Moritz Weckbecker,Galip Ümit Yolcu,Thomas Wiegand,Sebastian Lapuschkin,Wojciech Samek,René P. Klausen*

Main category: cs.LG

TL;DR: This paper explores using influence function-based sampling methods for selecting training data in physics-informed neural networks (PINNs), boosting their prediction accuracy.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance the prediction accuracy of physics-informed neural networks (PINNs) by leveraging explainable AI tools like influence functions for targeted resampling of training points, addressing gaps in existing PINN training frameworks.

Method: The authors employed influence functions to analyze how individual training points affect the model's predictions. Using this analysis, they incorporated targeted resampling strategies into the PINN training process to prioritize critical data points.

Result: The study demonstrates that influence function-driven data selection can substantially improve the prediction accuracy of PINNs, showcasing its applicability in scientific machine learning.

Conclusion: Influence function-based resampling proves to be an effective method for improving PINNs' prediction accuracy, highlighting a practical intersection between explainable AI and scientific machine learning.

Abstract: Physics-informed neural networks (PINNs) offer a powerful approach to solving
partial differential equations (PDEs), which are ubiquitous in the quantitative
sciences. Applied to both forward and inverse problems across various
scientific domains, PINNs have recently emerged as a valuable tool in the field
of scientific machine learning. A key aspect of their training is that the data
-- spatio-temporal points sampled from the PDE's input domain -- are readily
available. Influence functions, a tool from the field of explainable AI (XAI),
approximate the effect of individual training points on the model, enhancing
interpretability. In the present work, we explore the application of influence
function-based sampling approaches for the training data. Our results indicate
that such targeted resampling based on data attribution methods has the
potential to enhance prediction accuracy in physics-informed neural networks,
demonstrating a practical application of an XAI method in PINN training.

</details>


### [366] [Consumer-friendly EEG-based Emotion Recognition System: A Multi-scale Convolutional Neural Network Approach](https://arxiv.org/abs/2506.16448)
*Tri Duc Ly,Gia H. Ngo*

Main category: cs.LG

TL;DR: The paper introduces a multi-scale convolutional neural network model for EEG-based emotion recognition, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: To improve EEG-based emotion recognition for real-life applications using advanced deep learning techniques.

Method: The study utilizes multi-scale convolutional neural networks with feature extraction kernels and a novel kernel design learning from four brain regions.

Result: The proposed model surpasses the TSception model in predicting valence, arousal, and dominance scores across multiple metrics.

Conclusion: The model demonstrates state-of-the-art performance, paving the way for enhanced EEG-based emotion recognition in realistic settings.

Abstract: EEG is a non-invasive, safe, and low-risk method to record
electrophysiological signals inside the brain. Especially with recent
technology developments like dry electrodes, consumer-grade EEG devices, and
rapid advances in machine learning, EEG is commonly used as a resource for
automatic emotion recognition. With the aim to develop a deep learning model
that can perform EEG-based emotion recognition in a real-life context, we
propose a novel approach to utilize multi-scale convolutional neural networks
to accomplish such tasks. By implementing feature extraction kernels with many
ratio coefficients as well as a new type of kernel that learns key information
from four separate areas of the brain, our model consistently outperforms the
state-of-the-art TSception model in predicting valence, arousal, and dominance
scores across many performance evaluation metrics.

</details>


### [367] [Latent Concept Disentanglement in Transformer-based Language Models](https://arxiv.org/abs/2506.16975)
*Guan Zhe Hong,Bhavya Vasudeva,Vatsal Sharan,Cyrus Rashtchian,Prabhakar Raghavan,Rina Panigrahy*

Main category: cs.LG

TL;DR: This paper explores how transformers handle latent concepts during in-context learning, finding evidence of localized structures in their representation.


<details>
  <summary>Details</summary>
Motivation: The work aims to understand whether transformers represent latent structures as part of their computation during in-context learning or rely on shortcuts.

Method: The study examines 2-hop reasoning tasks with discrete latent concepts and tasks parameterized by continuous latent concepts, analyzing the representation spaces.

Result: Transformers successfully identify and compose step-by-step latent concepts for discrete tasks, while geometry in representation spaces mimics continuous latent parameterization.

Conclusion: The findings showcase transformers' ability to disentangle and represent latent concepts, improving our understanding of their inner workings during in-context learning.

Abstract: When large language models (LLMs) use in-context learning (ICL) to solve a
new task, they seem to grasp not only the goal of the task but also core,
latent concepts in the demonstration examples. This begs the question of
whether transformers represent latent structures as part of their computation
or whether they take shortcuts to solve the problem. Prior mechanistic work on
ICL does not address this question because it does not sufficiently examine the
relationship between the learned representation and the latent concept, and the
considered problem settings often involve only single-step reasoning. In this
work, we examine how transformers disentangle and use latent concepts. We show
that in 2-hop reasoning tasks with a latent, discrete concept, the model
successfully identifies the latent concept and does step-by-step concept
composition. In tasks parameterized by a continuous latent concept, we find
low-dimensional subspaces in the representation space where the geometry mimics
the underlying parameterization. Together, these results refine our
understanding of ICL and the representation of transformers, and they provide
evidence for highly localized structures in the model that disentangle latent
concepts in ICL tasks.

</details>


### [368] [Black-Box Privacy Attacks on Shared Representations in Multitask Learning](https://arxiv.org/abs/2506.16460)
*John Abascal,Nicolás Berrios,Alina Oprea,Jonathan Ullman,Adam Smith,Matthew Jagielski*

Main category: cs.LG

TL;DR: The paper identifies privacy risks in Multitask Learning (MTL), focusing on black-box task-inference attacks capable of determining if a specific task was used to train a shared representation. The proposed attacks are evaluated across different domains.


<details>
  <summary>Details</summary>
Motivation: To address the privacy vulnerabilities stemming from shared representations in MTL, which can leak sensitive task-specific information.

Method: The authors propose a black-box task-inference threat model, develop efficient attacks exploiting dependencies within embeddings, and provide both empirical and theoretical analyses.

Result: Their attacks demonstrate effectiveness across vision and language domains, showing task-inclusion inference even with fresh task samples instead of training data.

Conclusion: The study emphasizes the inherent privacy risks in MTL and calls for more secure methods to protect against task-inclusion inferences.

Abstract: Multitask learning (MTL) has emerged as a powerful paradigm that leverages
similarities among multiple learning tasks, each with insufficient samples to
train a standalone model, to solve them simultaneously while minimizing data
sharing across users and organizations. MTL typically accomplishes this goal by
learning a shared representation that captures common structure among the tasks
by embedding data from all tasks into a common feature space. Despite being
designed to be the smallest unit of shared information necessary to effectively
learn patterns across multiple tasks, these shared representations can
inadvertently leak sensitive information about the particular tasks they were
trained on.
  In this work, we investigate what information is revealed by the shared
representations through the lens of inference attacks. Towards this, we propose
a novel, black-box task-inference threat model where the adversary, given the
embedding vectors produced by querying the shared representation on samples
from a particular task, aims to determine whether that task was present when
training the shared representation. We develop efficient, purely black-box
attacks on machine learning models that exploit the dependencies between
embeddings from the same task without requiring shadow models or labeled
reference data. We evaluate our attacks across vision and language domains for
multiple use cases of MTL and demonstrate that even with access only to fresh
task samples rather than training data, a black-box adversary can successfully
infer a task's inclusion in training. To complement our experiments, we provide
theoretical analysis of a simplified learning setting and show a strict
separation between adversaries with training samples and fresh samples from the
target task's distribution.

</details>


### [369] [From Concepts to Components: Concept-Agnostic Attention Module Discovery in Transformers](https://arxiv.org/abs/2506.17052)
*Jingtong Su,Julia Kempe,Karen Ullrich*

Main category: cs.LG

TL;DR: This paper introduces SAMD and SAMI, a scalable concept attribution method and intervention strategy for mapping and modifying attention heads in transformers, improving interpretability and task performance.


<details>
  <summary>Details</summary>
Motivation: To enhance interpretability and behavioral control of transformers by addressing limitations in current attribution methods for analyzing complex concepts and attention mechanisms.

Method: They developed SAMD, which maps arbitrary concepts to attention heads using cosine similarity, and SAMI, which modifies the attention module effects using a scalar parameter.

Result: SAMD successfully mapped complex concepts to stable attention heads, and SAMI achieved task-specific improvements like boosting reasoning or enabling jailbreaking while demonstrating domain-agnostic applicability.

Conclusion: The methods enhance transformer interpretability and control across tasks and domains, promoting targeted behavioral modifications with empirical validation.

Abstract: Transformers have achieved state-of-the-art performance across language and
vision tasks. This success drives the imperative to interpret their internal
mechanisms with the dual goals of enhancing performance and improving
behavioral control. Attribution methods help advance interpretability by
assigning model outputs associated with a target concept to specific model
components. Current attribution research primarily studies multi-layer
perceptron neurons and addresses relatively simple concepts such as factual
associations (e.g., Paris is located in France). This focus tends to overlook
the impact of the attention mechanism and lacks a unified approach for
analyzing more complex concepts. To fill these gaps, we introduce Scalable
Attention Module Discovery (SAMD), a concept-agnostic method for mapping
arbitrary, complex concepts to specific attention heads of general transformer
models. We accomplish this by representing each concept as a vector,
calculating its cosine similarity with each attention head, and selecting the
TopK-scoring heads to construct the concept-associated attention module. We
then propose Scalar Attention Module Intervention (SAMI), a simple strategy to
diminish or amplify the effects of a concept by adjusting the attention module
using only a single scalar parameter. Empirically, we demonstrate SAMD on
concepts of varying complexity, and visualize the locations of their
corresponding modules. Our results demonstrate that module locations remain
stable before and after LLM post-training, and confirm prior work on the
mechanics of LLM multilingualism. Through SAMI, we facilitate jailbreaking on
HarmBench (+72.7%) by diminishing "safety" and improve performance on the GSM8K
benchmark (+1.6%) by amplifying "reasoning". Lastly, we highlight the
domain-agnostic nature of our approach by suppressing the image classification
accuracy of vision transformers on ImageNet.

</details>


### [370] [Progressive Inference-Time Annealing of Diffusion Models for Sampling from Boltzmann Densities](https://arxiv.org/abs/2506.16471)
*Tara Akhound-Sadegh,Jungyoon Lee,Avishek Joey Bose,Valentin De Bortoli,Arnaud Doucet,Michael M. Bronstein,Dominique Beaini,Siamak Ravanbakhsh,Kirill Neklyudov,Alexander Tong*

Main category: cs.LG

TL;DR: PITA combines diffusion-based models and annealing techniques to efficiently sample complex probability densities, with potential applications in molecular systems.


<details>
  <summary>Details</summary>
Motivation: Sampling from unnormalized probability densities is a key challenge in many scientific fields, and current diffusion-based methods are not capable of dealing with complex systems, like molecular systems.

Method: The paper introduces Progressive Inference-Time Annealing (PITA), which trains diffusion models sequentially from high to low temperatures and uses Feynman-Kac PDE with Sequential Monte Carlo for inference-time annealing.

Result: PITA successfully enables equilibrium sampling of complex molecular systems such as N-body systems and peptides, with significantly fewer energy function evaluations.

Conclusion: Progressive Inference-Time Annealing (PITA) offers a new pathway for efficient sampling in high-dimensional spaces, particularly for molecular systems, marking a significant improvement over existing approaches.

Abstract: Sampling efficiently from a target unnormalized probability density remains a
core challenge, with relevance across countless high-impact scientific
applications. A promising approach towards this challenge is the design of
amortized samplers that borrow key ideas, such as probability path design, from
state-of-the-art generative diffusion models. However, all existing
diffusion-based samplers remain unable to draw samples from distributions at
the scale of even simple molecular systems. In this paper, we propose
Progressive Inference-Time Annealing (PITA), a novel framework to learn
diffusion-based samplers that combines two complementary interpolation
techniques: I.) Annealing of the Boltzmann distribution and II.) Diffusion
smoothing. PITA trains a sequence of diffusion models from high to low
temperatures by sequentially training each model at progressively higher
temperatures, leveraging engineered easy access to samples of the
temperature-annealed target density. In the subsequent step, PITA enables
simulating the trained diffusion model to procure training samples at a lower
temperature for the next diffusion model through inference-time annealing using
a novel Feynman-Kac PDE combined with Sequential Monte Carlo. Empirically, PITA
enables, for the first time, equilibrium sampling of N-body particle systems,
Alanine Dipeptide, and tripeptides in Cartesian coordinates with dramatically
lower energy function evaluations. Code available at:
https://github.com/taraak/pita

</details>


### [371] [Manifold Learning for Personalized and Label-Free Detection of Cardiac Arrhythmias](https://arxiv.org/abs/2506.16494)
*Amir Reza Vazifeh,Jason W. Fleischer*

Main category: cs.LG

TL;DR: The authors propose the use of nonlinear dimensionality reduction (NLDR) to improve ECG analysis, achieving accurate classification and overcoming the limitations of traditional methods.


<details>
  <summary>Details</summary>
Motivation: Manual analysis of ECG data is time-intensive and error-prone, and existing supervised and unsupervised machine learning techniques struggle with the variability in ECG signals and data labeling inconsistencies.

Method: The authors utilized nonlinear dimensionality reduction techniques, including t-SNE and UMAP, to analyze ECG signals without requiring training or prior information. They tested this approach with the MLII and V1 leads of the MIT-BIH dataset.

Result: The NLDR methods succeeded in identifying individuals with ≥90% accuracy and detecting arrhythmias with a median accuracy of 98.96% and an F1-score of 91.02%.

Conclusion: NLDR methods like t-SNE and UMAP show significant promise for automated ECG analysis, paving the way for improvements in cardiac monitoring, personalized healthcare, and even applications beyond cardiology.

Abstract: Electrocardiograms (ECGs) provide direct, non-invasive measurements of heart
activity and are well-established tools for detecting and monitoring
cardiovascular disease. However, manual ECG analysis can be time-consuming and
prone to errors. Machine learning has emerged as a promising approach for
automated heartbeat recognition and classification, but substantial variations
in ECG signals make it challenging to develop generalizable models. ECG signals
can vary widely across individuals and leads, while datasets often follow
different labeling standards and may be biased, all of which greatly hinder
supervised methods. Conventional unsupervised methods, e.g. principal component
analysis, prioritize large (and often obvious) variances in the data and
typically overlook subtle yet clinically relevant patterns. If labels are
missing and/or variations are significant but small, both approaches fail.
Here, we show that nonlinear dimensionality reduction (NLDR) can accommodate
these issues and identify medically relevant features in ECG signals, with no
need for training or prior information. Using the MLII and V1 leads of the
MIT-BIH dataset, we demonstrate that t-distributed stochastic neighbor
embedding and uniform manifold approximation and projection can discriminate
individual recordings in mixed populations with >= 90% accuracy and distinguish
different arrhythmias in individual patients with a median accuracy of 98.96%
and a median F1-score of 91.02%. The results show that NLDR holds much promise
for cardiac monitoring, including the limiting cases of single-lead ECG and the
current 12-lead standard of care, and for personalized health care beyond
cardiology.

</details>


### [372] [SparseLoRA: Accelerating LLM Fine-Tuning with Contextual Sparsity](https://arxiv.org/abs/2506.16500)
*Samir Khaki,Xiuyu Li,Junxian Guo,Ligeng Zhu,Chenfeng Xu,Konstantinos N. Plataniotis,Amir Yazdanbakhsh,Kurt Keutzer,Song Han,Zhijian Liu*

Main category: cs.LG

TL;DR: SparseLoRA introduces contextual sparsity for LLM fine-tuning to reduce computational cost while maintaining accuracy across tasks.


<details>
  <summary>Details</summary>
Motivation: Current parameter-efficient fine-tuning methods reduce trainable parameters and memory usage but fail to lower computational cost, sometimes even slowing the process.

Method: SparseLoRA uses a training-free SVD sparsity estimator to dynamically select a sparse subset of weights for computations and addresses layer, token, and step sensitivities.

Result: The proposed method reduces computational cost by up to 2.2 times with a measured speedup of up to 1.6 times while maintaining task accuracy.

Conclusion: SparseLoRA provides a more efficient fine-tuning approach by leveraging contextual sparsity, achieving faster computation without sacrificing performance.

Abstract: Fine-tuning LLMs is both computationally and memory-intensive. While
parameter-efficient fine-tuning methods, such as QLoRA and DoRA, reduce the
number of trainable parameters and lower memory usage, they do not decrease
computational cost. In some cases, they may even slow down fine-tuning. In this
paper, we introduce SparseLoRA, a method that accelerates LLM fine-tuning
through contextual sparsity. We propose a lightweight, training-free SVD
sparsity estimator that dynamically selects a sparse subset of weights for loss
and gradient computation. Also, we systematically analyze and address
sensitivity across layers, tokens, and training steps. Our experimental results
show that SparseLoRA reduces computational cost by up to 2.2 times and a
measured speedup of up to 1.6 times while maintaining accuracy across various
downstream tasks, including commonsense and arithmetic reasoning, code
generation, and instruction following.

</details>


### [373] [Subspace-Boosted Model Merging](https://arxiv.org/abs/2506.16506)
*Ronald Skorobogat,Karsten Roth,Mariana-Iuliana Georgescu,Zeynep Akata*

Main category: cs.LG

TL;DR: The paper investigates the limitations of merging multiple specialized expert models into one and introduces methods to mitigate the issue of rank collapse in task vector spaces.


<details>
  <summary>Details</summary>
Motivation: To improve the performance and scalability of model merging without suffering from diminishing returns and rank collapse.

Method: Introducing Subspace Boosting to maintain the rank of task vector spaces, and employing Higher-Order Generalized Singular Value Decomposition for task similarity analysis.

Result: Subspace Boosting improved model merging efficacy significantly with more than 10% gains across large-scale vision benchmarks for merging up to 20 expert models.

Conclusion: Subspace Boosting effectively addresses rank collapse in task vector spaces during model merging, enhancing performance and scalability, and provides novel insights into task similarity.

Abstract: Model merging enables the combination of multiple specialized expert models
into a single model capable of performing multiple tasks. However, the benefits
of merging an increasing amount of specialized experts generally lead to
diminishing returns and reduced overall performance gains. In this work, we
offer an explanation and analysis from a task arithmetic perspective; revealing
that as the merging process (across numerous existing merging methods)
continues for more and more experts, the associated task vector space
experiences rank collapse. To mitigate this issue, we introduce Subspace
Boosting, which operates on the singular value decomposed task vector space and
maintains task vector ranks. Subspace Boosting raises merging efficacy for up
to 20 expert models by large margins of more than 10% when evaluated on vision
benchmarks. Moreover, we propose employing Higher-Order Generalized Singular
Value Decomposition to further quantify task similarity, offering a new
interpretable perspective on model merging.

</details>


### [374] [Robust Reward Modeling via Causal Rubrics](https://arxiv.org/abs/2506.16507)
*Pragya Srivastava,Harman Singh,Rahul Madhavan,Gandharv Patil,Sravanti Addepalli,Arun Suggala,Rengarajan Aravamudhan,Soumya Sharma,Anirban Laha,Aravindan Raghuveer,Karthikeyan Shanmugam,Doina Precup*

Main category: cs.LG

TL;DR: Crome is a framework designed to address reward hacking in Reward Models (RMs) by introducing causally-targeted augmentations during training. It achieves significant performance improvements across multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: Reward Models often suffer from reward hacking, where they use superficial attributes instead of focusing on genuine aspects of response quality, due to issues in standard training objectives.

Method: Crome employs two types of augmentations during training: Causal Augmentations to enforce sensitivity to causal attributes and Neutral Augmentations to enforce invariance to spurious attributes. These augmentations are generated through answer interventions guided by causal rubrics using an oracle LLM.

Result: Crome demonstrates performance improvements on the RewardBench benchmark, with average accuracy gains of up to 5.4%, and specific category improvements of 13.2% and 7.2%, along with consistent gains in a Best-of-N inference setting across various benchmarks.

Conclusion: Crome effectively mitigates reward hacking by leveraging synthetic targeted augmentations grounded in causal modeling, enhancing robustness and alignment of Reward Models in multiple challenging scenarios.

Abstract: Reward models (RMs) are fundamental to aligning Large Language Models (LLMs)
via human feedback, yet they often suffer from reward hacking. They tend to
latch on to superficial or spurious attributes, such as response length or
formatting, mistaking these cues learned from correlations in training data for
the true causal drivers of quality (e.g., factuality, relevance). This occurs
because standard training objectives struggle to disentangle these factors,
leading to brittle RMs and misaligned policies. We introduce Crome (Causally
Robust Reward Modeling), a novel framework grounded in an explicit causal model
designed to mitigate reward hacking. Crome employs the following synthetic
targeted augmentations during training: (1) Causal Augmentations, which are
pairs that differ along specific causal attributes, to enforce sensitivity
along each causal attribute individually, and (2) Neutral Augmentations, which
are tie-label pairs varying primarily in spurious attributes, to enforce
invariance along spurious attributes. Notably, our augmentations are produced
without any knowledge of spurious factors, via answer interventions only along
causal rubrics, that are identified by querying an oracle LLM. Empirically,
Crome significantly outperforms standard baselines on RewardBench, improving
average accuracy by up to 5.4% and achieving gains of up to 13.2% and 7.2% in
specific categories. The robustness of Crome is further testified by the
consistent gains obtained in a Best-of-N inference setting across increasing N,
across various benchmarks, including the popular RewardBench (covering chat,
chat-hard, safety, and reasoning tasks), the safety-focused WildGuardTest, and
the reasoning-specific GSM8k.

</details>


### [375] [Aligning ASR Evaluation with Human and LLM Judgments: Intelligibility Metrics Using Phonetic, Semantic, and NLI Approaches](https://arxiv.org/abs/2506.16528)
*Bornali Phukon,Xiuwen Zheng,Mark Hasegawa-Johnson*

Main category: cs.LG

TL;DR: Traditional ASR metrics lack the ability to capture intelligibility, especially for dysarthric and dysphonic speech. This paper proposes a novel metric combining NLI, semantic, and phonetic similarity to better align with human judgment.


<details>
  <summary>Details</summary>
Motivation: Standard ASR metrics like WER and CER fail to reflect intelligibility, particularly for dysarthric and dysphonic speech, where semantic understanding is more critical. There is also limited exploration of utilizing LLMs to refine ASR transcripts for such speech.

Method: The paper introduces a new evaluation metric for ASR systems that integrates Natural Language Inference, semantic similarity, and phonetic similarity, aimed at better aligning with human judgment of intelligibility.

Result: The proposed metric showed a 0.890 correlation with human assessments of intelligibility in Speech Accessibility Project data, outperforming traditional metrics like WER and CER.

Conclusion: The new metric emphasizes prioritizing intelligibility rather than error-based measures for evaluating ASR systems, especially for dysarthric and dysphonic speech.

Abstract: Traditional ASR metrics like WER and CER fail to capture intelligibility,
especially for dysarthric and dysphonic speech, where semantic alignment
matters more than exact word matches. ASR systems struggle with these speech
types, often producing errors like phoneme repetitions and imprecise
consonants, yet the meaning remains clear to human listeners. We identify two
key challenges: (1) Existing metrics do not adequately reflect intelligibility,
and (2) while LLMs can refine ASR output, their effectiveness in correcting ASR
transcripts of dysarthric speech remains underexplored. To address this, we
propose a novel metric integrating Natural Language Inference (NLI) scores,
semantic similarity, and phonetic similarity. Our ASR evaluation metric
achieves a 0.890 correlation with human judgments on Speech Accessibility
Project data, surpassing traditional methods and emphasizing the need to
prioritize intelligibility over error-based measures.

</details>


### [376] [Mr. Snuffleupagus at SemEval-2025 Task 4: Unlearning Factual Knowledge from LLMs Using Adaptive RMU](https://arxiv.org/abs/2506.16548)
*Arjun Dosajh,Mihika Sanghi*

Main category: cs.LG

TL;DR: The paper proposes using Adaptive Representation Misdirection Unlearning (RMU) to address privacy risks in LLMs by removing sensitive information, achieving notable leaderboard rankings.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the privacy, copyright, and security concerns posed by LLMs' tendency to memorize training data, especially PII.

Method: The method involves applying and experimenting with RMU to identify effective layers in LLMs for removing sensitive information.

Result: The technique achieved 4th place on the official leaderboard for 1B and 7B parameter models.

Conclusion: RMU is effective in mitigating privacy risks by strategically unlearning sensitive data in LLMs, highlighting its potential as a practical solution.

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
natural language understanding and generation. However, their tendency to
memorize training data raises concerns regarding privacy, copyright compliance,
and security, particularly in cases involving Personally Identifiable
Information (PII). Effective machine unlearning techniques are essential to
mitigate these risks, yet existing methods remain underdeveloped for LLMs due
to their open-ended output space. In this work, we apply the Adaptive
Representation Misdirection Unlearning (RMU) technique to unlearn sensitive
information from LLMs. Through extensive experiments, we analyze the effects of
unlearning across different decoder layers to determine the most effective
regions for sensitive information removal. Our technique ranked 4th on the
official leaderboard of both 1B parameter and 7B parameter models.

</details>


### [377] [One Sample is Enough to Make Conformal Prediction Robust](https://arxiv.org/abs/2506.16553)
*Soroush H. Zargarbashi,Mohammad Sadegh Akhondzadeh,Aleksandar Bojchevski*

Main category: cs.LG

TL;DR: This paper introduces a method called Single Sample Robust Conformal Prediction (RCP1) that reduces computational costs while maintaining robust conformal prediction guarantees.


<details>
  <summary>Details</summary>
Motivation: To overcome the computational inefficiency in existing smoothing-based Robust Conformal Prediction (RCP) methods while maintaining prediction reliability under input noise.

Method: The proposed method certifies the entire conformal prediction procedure rather than individual scores, using only a single forward pass of a randomly perturbed input combined with binary certification techniques.

Result: The RCP1 method achieves robust prediction sets with smaller average set sizes and significantly lower computational cost compared to the state-of-the-art methods.

Conclusion: RCP1 effectively balances computational efficiency and prediction robustness, making it applicable to both classification and regression tasks.

Abstract: Given any model, conformal prediction (CP) returns prediction sets guaranteed
to include the true label with high adjustable probability. Robust CP (RCP)
extends this to inputs with worst-case noise. A well-established approach is to
use randomized smoothing for RCP since it is applicable to any black-box model
and provides smaller sets compared to deterministic methods. However, current
smoothing-based RCP requires many model forward passes per each input which is
computationally expensive. We show that conformal prediction attains some
robustness even with a forward pass on a single randomly perturbed input. Using
any binary certificate we propose a single sample robust CP (RCP1). Our
approach returns robust sets with smaller average set size compared to SOTA
methods which use many (e.g. around 100) passes per input. Our key insight is
to certify the conformal prediction procedure itself rather than individual
scores. Our approach is agnostic to the setup (classification and regression).
We further extend our approach to smoothing-based robust conformal risk
control.

</details>


### [378] [Energy-Based Transfer for Reinforcement Learning](https://arxiv.org/abs/2506.16590)
*Zeyun Deng,Jasorsi Ghosh,Fiona Xie,Yuzhe Lu,Katia Sycara,Joseph Campbell*

Main category: cs.LG

TL;DR: The paper introduces an energy-based transfer learning method that selectively uses teacher policy guidance in reinforcement learning to improve sample efficiency and performance for tasks both within and outside its distribution.


<details>
  <summary>Details</summary>
Motivation: Current reinforcement learning algorithms struggle with poor sample efficiency, especially in multi-task or continual learning scenarios, where transferring knowledge from a teacher policy can help but may be biased in unrelated tasks.

Method: The proposed method relies on energy-based models for out-of-distribution detection, ensuring that teacher policy guidance is applied only to states within its training distribution.

Result: Theoretical and empirical analyses demonstrate that energy scores accurately reflect state-visitation densities, resulting in enhanced sample efficiency and performance across diverse scenarios.

Conclusion: Selective knowledge transfer from teacher policies using energy-based detection can effectively mitigate biases and improve reinforcement learning in various tasks.

Abstract: Reinforcement learning algorithms often suffer from poor sample efficiency,
making them challenging to apply in multi-task or continual learning settings.
Efficiency can be improved by transferring knowledge from a previously trained
teacher policy to guide exploration in new but related tasks. However, if the
new task sufficiently differs from the teacher's training task, the transferred
guidance may be sub-optimal and bias exploration toward low-reward behaviors.
We propose an energy-based transfer learning method that uses
out-of-distribution detection to selectively issue guidance, enabling the
teacher to intervene only in states within its training distribution. We
theoretically show that energy scores reflect the teacher's state-visitation
density and empirically demonstrate improved sample efficiency and performance
across both single-task and multi-task settings.

</details>


### [379] [FLAME: Towards Federated Fine-Tuning Large Language Models Through Adaptive SMoE](https://arxiv.org/abs/2506.16600)
*Khiem Le,Tuan Tran,Ting Hua,Nitesh V. Chawla*

Main category: cs.LG

TL;DR: FLAME introduces a federated learning framework using Sparse Mixture-of-Experts to overcome limitations of compressed LoRA matrices, ensuring better adaptability and performance across clients.


<details>
  <summary>Details</summary>
Motivation: Address limitations in existing LoRA federated fine-tuning methods caused by information loss due to matrix compression.

Method: FLAME retains uncompressed global LoRA matrices, enables client adaptation via varying expert activation, and employs rescaling and activation-aware aggregation for efficiency.

Result: Empirical results demonstrate FLAME's consistent superior performance across various computational settings.

Conclusion: FLAME provides a robust and effective solution for resource-adaptive federated learning, improving over prior methods.

Abstract: Existing resource-adaptive LoRA federated fine-tuning methods enable clients
to fine-tune models using compressed versions of global LoRA matrices, in order
to accommodate various compute resources across clients. This compression
requirement will lead to suboptimal performance due to information loss. To
address this, we propose FLAME, a novel federated learning framework based on
the Sparse Mixture-of-Experts (SMoE) architecture. Unlike prior approaches,
FLAME retains full (uncompressed) global LoRA matrices and achieves client-side
adaptability by varying the number of activated experts per client. However,
incorporating SMoE into federated learning introduces unique challenges,
specifically, the mismatch in output magnitude from partial expert activation
and the imbalance in expert training quality across clients. FLAME tackles
these challenges through a lightweight rescaling mechanism and an
activation-aware aggregation scheme. Empirical results across diverse
computational settings demonstrate that FLAME consistently outperforms existing
methods, providing a robust and effective solution for resource-adaptive
federated learning.

</details>


### [380] [SlepNet: Spectral Subgraph Representation Learning for Neural Dynamics](https://arxiv.org/abs/2506.16602)
*Siddharth Viswanath,Rahul Singh,Yanlei Zhang,J. Adam Noah,Joy Hirsch,Smita Krishnaswamy*

Main category: cs.LG

TL;DR: The paper introduces SlepNet, a novel graph convolutional network (GCN) architecture, utilizing Slepian harmonics for improved spatiotemporal pattern representation in graphs, achieving superior results over traditional methods in fMRI and traffic data.


<details>
  <summary>Details</summary>
Motivation: Graph neural networks and graph signal processing struggle to efficiently represent spatiotemporal patterns in signals, particularly those localized in subgraphs, which are critical for domains like neuroscience.

Method: The proposed SlepNet architecture employs Slepian harmonics instead of graph Fourier harmonics. These harmonics optimize signal concentration on relevant subgraphs via automatic mask learning, enabling canonical and highly resolved signal representations.

Result: SlepNet outperforms conventional GNNs and graph signal processing approaches across three fMRI datasets and two traffic dynamics datasets, providing better resolution in distinguishing similar patterns and creating useful trajectory representations.

Conclusion: SlepNet demonstrates strong utility for both prediction and representation learning in spatiotemporal graph data, offering detailed and canonical signal pattern representations that support downstream tasks effectively.

Abstract: Graph neural networks have been useful in machine learning on
graph-structured data, particularly for node classification and some types of
graph classification tasks. However, they have had limited use in representing
patterning of signals over graphs. Patterning of signals over graphs and in
subgraphs carries important information in many domains including neuroscience.
Neural signals are spatiotemporally patterned, high dimensional and difficult
to decode. Graph signal processing and associated GCN models utilize the graph
Fourier transform and are unable to efficiently represent spatially or
spectrally localized signal patterning on graphs. Wavelet transforms have shown
promise here, but offer non-canonical representations and cannot be tightly
confined to subgraphs. Here we propose SlepNet, a novel GCN architecture that
uses Slepian bases rather than graph Fourier harmonics. In SlepNet, the Slepian
harmonics optimally concentrate signal energy on specifically relevant
subgraphs that are automatically learned with a mask. Thus, they can produce
canonical and highly resolved representations of neural activity, focusing
energy of harmonics on areas of the brain which are activated. We evaluated
SlepNet across three fMRI datasets, spanning cognitive and visual tasks, and
two traffic dynamics datasets, comparing its performance against conventional
GNNs and graph signal processing constructs. SlepNet outperforms the baselines
in all datasets. Moreover, the extracted representations of signal patterns
from SlepNet offers more resolution in distinguishing between similar patterns,
and thus represent brain signaling transients as informative trajectories. Here
we have shown that these extracted trajectory representations can be used for
other downstream untrained tasks. Thus we establish that SlepNet is useful both
for prediction and representation learning in spatiotemporal data.

</details>


### [381] [Distribution Parameter Actor-Critic: Shifting the Agent-Environment Boundary for Diverse Action Spaces](https://arxiv.org/abs/2506.16608)
*Jiamin He,A. Rupam Mahmood,Martha White*

Main category: cs.LG

TL;DR: This paper introduces a novel reinforcement learning framework centered around using distribution parameters as continuous actions, leading to decreased variance in policy gradients and improved performance across tasks.


<details>
  <summary>Details</summary>
Motivation: To address high variance issues and limitations in traditional action spaces in reinforcement learning by redefining the boundary between agent and environment.

Method: Proposed a framework that uses distribution parameters as continuous actions. Developed Distribution Parameter Policy Gradient (DPPG) with reduced variance, introduced Interpolated Critic Learning (ICL) to address critic challenges, and built a Distribution Parameter Actor-Critic (DPAC) algorithm based on TD3.

Result: DPAC outperforms the TD3 baseline in continuous control tasks and showcases competitive results in discretized action space environments as well.

Conclusion: The framework and DPAC algorithm redefine reinforcement learning boundaries and demonstrate improved performance, showing potential for broader applicability with reduced gradient variance.

Abstract: We introduce a novel reinforcement learning (RL) framework that treats
distribution parameters as actions, redefining the boundary between agent and
environment. This reparameterization makes the new action space continuous,
regardless of the original action type (discrete, continuous, mixed, etc.).
Under this new parameterization, we develop a generalized deterministic policy
gradient estimator, Distribution Parameter Policy Gradient (DPPG), which has
lower variance than the gradient in the original action space. Although
learning the critic over distribution parameters poses new challenges, we
introduce interpolated critic learning (ICL), a simple yet effective strategy
to enhance learning, supported by insights from bandit settings. Building on
TD3, a strong baseline for continuous control, we propose a practical
DPPG-based actor-critic algorithm, Distribution Parameter Actor-Critic (DPAC).
Empirically, DPAC outperforms TD3 in MuJoCo continuous control tasks from
OpenAI Gym and DeepMind Control Suite, and demonstrates competitive performance
on the same environments with discretized action spaces.

</details>


### [382] [Semantic Outlier Removal with Embedding Models and LLMs](https://arxiv.org/abs/2506.16644)
*Eren Akbiyik,João Almeida,Rik Melis,Ritu Sriram,Viviana Petrescu,Vilhjálmur Vilhjálmsson*

Main category: cs.LG

TL;DR: SORE (Semantic Outlier Removal) is proposed as an efficient method to eliminate extraneous content in text, leveraging multilingual sentence embeddings and approximate nearest-neighbor search. It achieves near-LLM precision at lower computational cost in removing unwanted text.


<details>
  <summary>Details</summary>
Motivation: Current methods for text cleaning struggle in multilingual or context-sensitive settings, necessitating an efficient yet precise approach to identify and remove extraneous content.

Method: SORE employs multilingual sentence embeddings and approximate nearest-neighbor search. It identifies core content through metadata embedding and flags segments matching predefined outlier groups or significantly deviating from the core.

Result: Experiments on HTML datasets show SORE outperforms structural methods with high precision, proving effective in various scenarios, and it is handling millions of documents daily in multiple languages.

Conclusion: SORE demonstrates near-LLM extraction precision in removing text noise efficiently across languages, making it suitable for large-scale production while releasing its implementation and datasets for community use.

Abstract: Modern text processing pipelines demand robust methods to remove extraneous
content while preserving a document's core message. Traditional approaches such
as HTML boilerplate extraction or keyword filters often fail in multilingual
settings and struggle with context-sensitive nuances, whereas Large Language
Models (LLMs) offer improved quality at high computational cost. We introduce
SORE (Semantic Outlier Removal), a cost-effective, transparent method that
leverages multilingual sentence embeddings and approximate nearest-neighbor
search to identify and excise unwanted text segments. By first identifying core
content via metadata embedding and then flagging segments that either closely
match predefined outlier groups or deviate significantly from the core, SORE
achieves near-LLM extraction precision at a fraction of the cost. Experiments
on HTML datasets demonstrate that SORE outperforms structural methods and yield
high precision in diverse scenarios. Our system is currently deployed in
production, processing millions of documents daily across multiple languages
while maintaining both efficiency and accuracy. To facilitate reproducibility
and further research, we release our implementation and evaluation datasets.

</details>


### [383] [A Distributional-Lifting Theorem for PAC Learning](https://arxiv.org/abs/2506.16651)
*Guy Blanc,Jane Lange,Carmen Strassle,Li-Yang Tan*

Main category: cs.LG

TL;DR: The paper introduces a broader method to upgrade specific distribution learners to general distribution learners efficiently, improving the PAC model's adaptability and sample complexity without requiring a conditional sample oracle.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of distribution-specific learning by developing a method to generalize efficient PAC learning to any distribution.

Method: The authors propose a distributional-lifting theorem that transforms learners for specific distribution families into learners for arbitrary distributions without needing to explicitly learn the target distribution.

Result: The proposed method eliminates the requirement of conditional sample oracles and achieves distribution-lifting under the standard PAC model while ensuring noise tolerance, improved sample complexity, and simplicity.

Conclusion: The paper advances the theory of PAC learning by enabling broader learning capabilities using accessible data representations, making algorithms more robust and practical.

Abstract: The apparent difficulty of efficient distribution-free PAC learning has led
to a large body of work on distribution-specific learning. Distributional
assumptions facilitate the design of efficient algorithms but also limit their
reach and relevance. Towards addressing this, we prove a distributional-lifting
theorem: This upgrades a learner that succeeds with respect to a limited
distribution family $\mathcal{D}$ to one that succeeds with respect to any
distribution $D^\star$, with an efficiency overhead that scales with the
complexity of expressing $D^\star$ as a mixture of distributions in
$\mathcal{D}$.
  Recent work of Blanc, Lange, Malik, and Tan considered the special case of
lifting uniform-distribution learners and designed a lifter that uses a
conditional sample oracle for $D^\star$, a strong form of access not afforded
by the standard PAC model. Their approach, which draws on ideas from
semi-supervised learning, first learns $D^\star$ and then uses this information
to lift.
  We show that their approach is information-theoretically intractable with
access only to random examples, thereby giving formal justification for their
use of the conditional sample oracle. We then take a different approach that
sidesteps the need to learn $D^\star$, yielding a lifter that works in the
standard PAC model and enjoys additional advantages: it works for all base
distribution families, preserves the noise tolerance of learners, has better
sample complexity, and is simpler.

</details>


### [384] [Relational Deep Learning: Challenges, Foundations and Next-Generation Architectures](https://arxiv.org/abs/2506.16654)
*Vijay Prakash Dwivedi,Charilaos Kanatsoulis,Shenyang Huang,Jure Leskovec*

Main category: cs.LG

TL;DR: The paper explores Relational Deep Learning (RDL) for representing relational databases as relational entity graphs, emphasizing challenges and opportunities in adapting graph machine learning for relational data.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to leverage graph machine learning for multi-tabular relational database data by representing it as relational entity graphs, aiming to improve representation learning and unify sub-fields in graph machine learning.

Method: This paper reviews benchmarks and existing Graph Neural Network (GNN)-based RDL methods, discusses challenges such as integrating multi-table structures and handling heterogeneous temporal dynamics, and explores recent architectural innovations.

Result: The paper identifies key challenges, evaluates existing methods, and outlines opportunities to converge various aspects of graph machine learning into integrated foundational models for relational data.

Conclusion: RDL is a promising framework that bridges relational databases and graph machine learning, with potential to unify and innovate across sub-fields, transforming how relational data is processed.

Abstract: Graph machine learning has led to a significant increase in the capabilities
of models that learn on arbitrary graph-structured data and has been applied to
molecules, social networks, recommendation systems, and transportation, among
other domains. Data in multi-tabular relational databases can also be
constructed as 'relational entity graphs' for Relational Deep Learning (RDL) -
a new blueprint that enables end-to-end representation learning without
traditional feature engineering. Compared to arbitrary graph-structured data,
relational entity graphs have key properties: (i) their structure is defined by
primary-foreign key relationships between entities in different tables, (ii)
the structural connectivity is a function of the relational schema defining a
database, and (iii) the graph connectivity is temporal and heterogeneous in
nature. In this paper, we provide a comprehensive review of RDL by first
introducing the representation of relational databases as relational entity
graphs, and then reviewing public benchmark datasets that have been used to
develop and evaluate recent GNN-based RDL models. We discuss key challenges
including large-scale multi-table integration and the complexities of modeling
temporal dynamics and heterogeneous data, while also surveying foundational
neural network methods and recent architectural advances specialized for
relational entity graphs. Finally, we explore opportunities to unify these
distinct modeling challenges, highlighting how RDL converges multiple
sub-fields in graph machine learning towards the design of foundation models
that can transform the processing of relational data.

</details>


### [385] [Mesh-Informed Neural Operator : A Transformer Generative Approach](https://arxiv.org/abs/2506.16656)
*Yaozhong Shi,Zachary E. Ross,Domniki Asimaki,Kamyar Azizzadenesheli*

Main category: cs.LG

TL;DR: The paper introduces the Mesh-Informed Neural Operator (MINO), addressing limitations of current functional generative models tied to regular grids and rectangular domains.


<details>
  <summary>Details</summary>
Motivation: Overcome the limitations of Fourier Neural Operator (FNO)-based generative models and expand their applicability beyond regular grids and rectangular domains.

Method: MINO integrates graph neural operators with cross-attention mechanisms to create a domain- and discretization-agnostic generative model framework.

Result: MINO enables generative modeling in diverse scientific applications, supports a unified framework for neural operators, and introduces standardized evaluation metrics.

Conclusion: MINO significantly broadens the scope of function-space generative models, enhancing their versatility and comparability in scientific modeling tasks.

Abstract: Generative models in function spaces, situated at the intersection of
generative modeling and operator learning, are attracting increasing attention
due to their immense potential in diverse scientific and engineering
applications. While functional generative models are theoretically domain- and
discretization-agnostic, current implementations heavily rely on the Fourier
Neural Operator (FNO), limiting their applicability to regular grids and
rectangular domains. To overcome these critical limitations, we introduce the
Mesh-Informed Neural Operator (MINO). By leveraging graph neural operators and
cross-attention mechanisms, MINO offers a principled, domain- and
discretization-agnostic backbone for generative modeling in function spaces.
This advancement significantly expands the scope of such models to more diverse
applications in generative, inverse, and regression tasks. Furthermore, MINO
provides a unified perspective on integrating neural operators with general
advanced deep learning architectures. Finally, we introduce a suite of
standardized evaluation metrics that enable objective comparison of functional
generative models, addressing another critical gap in the field.

</details>


### [386] [A Minimalist Optimizer Design for LLM Pretraining](https://arxiv.org/abs/2506.16659)
*Athanasios Glentis,Jiaxiang Li,Andi Han,Mingyi Hong*

Main category: cs.LG

TL;DR: This paper proposes SCALE, an optimizer combining column-normalized SGD and last-layer momentum, achieving memory efficiency and competitive performance in large language model pretraining.


<details>
  <summary>Details</summary>
Motivation: To identify the minimal optimizer state needed to maintain state-of-the-art performance in LLM pretraining while reducing memory consumption.

Method: They use a bottom-up approach to evaluate optimization techniques, introducing column gradient normalization and last-layer momentum in the SCALE optimizer.

Result: SCALE achieves comparable or better results than Adam and other memory-efficient optimizers while using only 35-45% of the memory, particularly excelling with the LLaMA 7B model.

Conclusion: SCALE establishes itself as a memory-efficient, high-performance optimizer for LLM pretraining and acts as a minimalist baseline for future designs.

Abstract: Training large language models (LLMs) typically relies on adaptive optimizers
such as Adam, which require significant memory to maintain first- and
second-moment matrices, known as optimizer states. While recent works such as
GaLore, Fira, and APOLLO have proposed state-compressed variants to reduce
memory consumption, a fundamental question remains: What is the minimal amount
of optimizer state that is truly necessary to retain state-of-the-art
performance in LLM pretraining? In this work, we systematically investigate
this question using a bottom-up approach. We find that two memory- and
compute-efficient optimization techniques are particularly effective: (1)
column-wise gradient normalization significantly boosts the performance of
plain SGD without requiring momentum; and (2) adding first-order momentum only
to the output layer - where gradient variance is highest - yields performance
competitive with fully adaptive methods such as Muon. Based on these insights,
we propose SCALE (Stochastic Column-normalized Last-layer Momentum), a new
optimizer that combines column-normalized SGD with last-layer momentum, where
column normalization refers to normalizing the gradient along the output
dimension. Across multiple LLaMA models (60M-1B), SCALE matches or exceeds the
performance of Adam while using only 35-45% of the total memory. It also
consistently outperforms memory-efficient optimizers such as GaLore, Fira, and
APOLLO, making it a strong candidate for large-scale pretraining under memory
constraints. For the LLaMA 7B model, SCALE outperforms the state-of-the-art
method APOLLO in terms of both perplexity and memory consumption. In addition,
our method serves as a minimalist baseline for more sophisticated optimizer
design.

</details>


### [387] [Fast and Stable Diffusion Planning through Variational Adaptive Weighting](https://arxiv.org/abs/2506.16688)
*Zhiying Qiu,Tao Lin*

Main category: cs.LG

TL;DR: The paper proposes an uncertainty-aware loss weighting strategy to improve diffusion models in offline RL, achieving faster training with fewer steps.


<details>
  <summary>Details</summary>
Motivation: Improve the training efficiency and stability of diffusion models in offline Reinforcement Learning (RL), addressing current challenges like high cost and slow convergence.

Method: A novel variationally optimal uncertainty-aware loss weighting function is derived and estimated with a closed-form polynomial approximation. This method is integrated into the diffusion planning pipeline for evaluation.

Result: The method shows competitive performance on Maze2D and Kitchen tasks, requiring up to 10 times fewer training steps than existing methods.

Conclusion: The proposed approach enhances practical effectiveness in offline RL by significantly reducing training time while maintaining competitive results.

Abstract: Diffusion models have recently shown promise in offline RL. However, these
methods often suffer from high training costs and slow convergence,
particularly when using transformer-based denoising backbones. While several
optimization strategies have been proposed -- such as modified noise schedules,
auxiliary prediction targets, and adaptive loss weighting -- challenges remain
in achieving stable and efficient training. In particular, existing loss
weighting functions typically rely on neural network approximators, which can
be ineffective in early training phases due to limited generalization capacity
of MLPs when exposed to sparse feedback in the early training stages. In this
work, we derive a variationally optimal uncertainty-aware weighting function
and introduce a closed-form polynomial approximation method for its online
estimation under the flow-based generative modeling framework. We integrate our
method into a diffusion planning pipeline and evaluate it on standard offline
RL benchmarks. Experimental results on Maze2D and Kitchen tasks show that our
method achieves competitive performance with up to 10 times fewer training
steps, highlighting its practical effectiveness.

</details>


### [388] [SIDE: Semantic ID Embedding for effective learning from sequences](https://arxiv.org/abs/2506.16698)
*Dinesh Ramasamy,Shakti Kumar,Chris Cadonic,Jiaxin Yang,Sohini Roychowdhury,Esam Abdel Rhman,Srihari Reddy*

Main category: cs.LG

TL;DR: This paper introduces a compact Semantic ID (SID) solution using vector quantization (VQ) to address storage and inference challenges in sequence-based ad-recommendation systems handling large user histories.


<details>
  <summary>Details</summary>
Motivation: Sequence-based recommendation systems face scalability challenges when processing long user histories with O(10^3) to O(10^4) events due to limitations in storage and inference costs.

Method: The authors propose a multi-task VQ-VAE framework (VQ fusion), a SID-to-embedding conversion technique (SIDE), and a novel Discrete-PCA quantization method to compress embedding representation while maintaining efficiency.

Result: The proposed innovations improve normalized entropy gain by 2.4X and reduce data footprint by 3X in a large-scale industrial ads-recommendation system compared to traditional SID methods.

Conclusion: The paper demonstrates the effectiveness of compact SIDs, offering significant performance benefits in scalability and storage for long-sequence industrial ad-recommendation systems.

Abstract: Sequence-based recommendations models are driving the state-of-the-art for
industrial ad-recommendation systems. Such systems typically deal with user
histories or sequence lengths ranging in the order of O(10^3) to O(10^4)
events. While adding embeddings at this scale is manageable in pre-trained
models, incorporating them into real-time prediction models is challenging due
to both storage and inference costs. To address this scaling challenge, we
propose a novel approach that leverages vector quantization (VQ) to inject a
compact Semantic ID (SID) as input to the recommendation models instead of a
collection of embeddings. Our method builds on recent works of SIDs by
introducing three key innovations: (i) a multi-task VQ-VAE framework, called VQ
fusion that fuses multiple content embeddings and categorical predictions into
a single Semantic ID; (ii) a parameter-free, highly granular SID-to-embedding
conversion technique, called SIDE, that is validated with two content embedding
collections, thereby eliminating the need for a large parameterized lookup
table; and (iii) a novel quantization method called Discrete-PCA (DPCA) which
generalizes and enhances residual quantization techniques. The proposed
enhancements when applied to a large-scale industrial ads-recommendation system
achieves 2.4X improvement in normalized entropy (NE) gain and 3X reduction in
data footprint compared to traditional SID methods.

</details>


### [389] [TriCon-SF: A Triple-Shuffle and Contribution-Aware Serial Federated Learning Framework for Heterogeneous Healthcare Data](https://arxiv.org/abs/2506.16723)
*Yuping Yan,Yizhi Wang,Yuanshuai Li,Yaochu Jin*

Main category: cs.LG

TL;DR: TriCon-SF is a novel federated learning framework that introduces triple randomization for enhanced privacy and uses Shapley value methods for dynamic client contribution evaluation.


<details>
  <summary>Details</summary>
Motivation: Current serial pipeline training in cross-silo federated learning faces privacy and resilience challenges, especially against gradient leakage, linkage attacks, and malicious client behavior.

Method: TriCon-SF employs three levels of randomization by shuffling model layers, data segments, and training sequences, combined with Shapley value-based contribution evaluation to detect dishonest behaviors.

Result: Extensive experiments on non-IID healthcare datasets show that TriCon-SF achieves better accuracy and communication efficiency compared to existing federated learning methods.

Conclusion: TriCon-SF offers an effective solution to enhance both privacy and robustness in federated learning with promising results, particularly in privacy-sensitive domains.

Abstract: Serial pipeline training is an efficient paradigm for handling data
heterogeneity in cross-silo federated learning with low communication overhead.
However, even without centralized aggregation, direct transfer of models
between clients can violate privacy regulations and remain susceptible to
gradient leakage and linkage attacks. Additionally, ensuring resilience against
semi-honest or malicious clients who may manipulate or misuse received models
remains a grand challenge, particularly in privacy-sensitive domains such as
healthcare. To address these challenges, we propose TriCon-SF, a novel serial
federated learning framework that integrates triple shuffling and contribution
awareness. TriCon-SF introduces three levels of randomization by shuffling
model layers, data segments, and training sequences to break deterministic
learning patterns and disrupt potential attack vectors, thereby enhancing
privacy and robustness. In parallel, it leverages Shapley value methods to
dynamically evaluate client contributions during training, enabling the
detection of dishonest behavior and enhancing system accountability. Extensive
experiments on non-IID healthcare datasets demonstrate that TriCon-SF
outperforms standard serial and parallel federated learning in both accuracy
and communication efficiency. Security analysis further supports its resilience
against client-side privacy attacks.

</details>


### [390] [On Training-Test (Mis)alignment in Unsupervised Combinatorial Optimization: Observation, Empirical Exploration, and Analysis](https://arxiv.org/abs/2506.16732)
*Fanchen Bu,Kijung Shin*

Main category: cs.LG

TL;DR: The paper introduces a differentiable derandomization method in unsupervised combinatorial optimization to improve alignment between training and testing phases, addressing a misalignment issue in existing UCO methods.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the misalignment between training and testing phases in unsupervised combinatorial optimization (UCO), where lower training loss does not guarantee better derandomization performance.

Method: The paper proposes including a differentiable version of derandomization into the training process to better align training and testing phases in UCO.

Result: Empirical exploration shows that incorporating differentiable derandomization improves the alignment between training and testing, but also adds notable challenges to the training complexity.

Conclusion: Enhanced training-test alignment with the differentiable derandomization approach demonstrates its potential in improving UCO methods, but requires tackling accompanying training difficulties.

Abstract: In unsupervised combinatorial optimization (UCO), during training, one aims
to have continuous decisions that are promising in a probabilistic sense for
each training instance, which enables end-to-end training on initially discrete
and non-differentiable problems. At the test time, for each test instance,
starting from continuous decisions, derandomization is typically applied to
obtain the final deterministic decisions. Researchers have developed more and
more powerful test-time derandomization schemes to enhance the empirical
performance and the theoretical guarantee of UCO methods. However, we notice a
misalignment between training and testing in the existing UCO methods.
Consequently, lower training losses do not necessarily entail better
post-derandomization performance, even for the training instances without any
data distribution shift. Empirically, we indeed observe such undesirable cases.
We explore a preliminary idea to better align training and testing in UCO by
including a differentiable version of derandomization into training. Our
empirical exploration shows that such an idea indeed improves training-test
alignment, but also introduces nontrivial challenges into training.

</details>


### [391] [Optimism Without Regularization: Constant Regret in Zero-Sum Games](https://arxiv.org/abs/2506.16736)
*John Lazarsfeld,Georgios Piliouras,Ryann Sim,Stratis Skoulakis*

Main category: cs.LG

TL;DR: The paper explores Optimistic Fictitious Play (OFP) in two-player zero-sum games, showing for the first time that constant regret is achievable without regularization, unlike Alternating Fictitious Play.


<details>
  <summary>Details</summary>
Motivation: The goal is to understand whether unregularized learning algorithms, like Optimistic Fictitious Play, can achieve optimal game learning rates similar to regularized methods, which provide constant regret.

Method: The researchers analyzed OFP using a geometric approach in the dual space of payoff vectors, proving bounded regret via an energy function. They also contrasted this with Alternating Fictitious Play, deriving a regret lower bound for it.

Result: The study shows that Optimistic Fictitious Play achieves constant regret across all tiebreaking rules, while Alternating Fictitious Play incurs a regret lower bound of Ω(√T).

Conclusion: The findings highlight the unique ability of optimism in achieving fast convergence in games, even in the absence of regularization, separating it from other no-regret methods.

Abstract: This paper studies the optimistic variant of Fictitious Play for learning in
two-player zero-sum games. While it is known that Optimistic FTRL -- a
regularized algorithm with a bounded stepsize parameter -- obtains constant
regret in this setting, we show for the first time that similar, optimal rates
are also achievable without regularization: we prove for two-strategy games
that Optimistic Fictitious Play (using any tiebreaking rule) obtains only
constant regret, providing surprising new evidence on the ability of
non-no-regret algorithms for fast learning in games. Our proof technique
leverages a geometric view of Optimistic Fictitious Play in the dual space of
payoff vectors, where we show a certain energy function of the iterates remains
bounded over time. Additionally, we also prove a regret lower bound of
$\Omega(\sqrt{T})$ for Alternating Fictitious Play. In the unregularized
regime, this separates the ability of optimism and alternation in achieving
$o(\sqrt{T})$ regret.

</details>


### [392] [Metapath-based Hyperbolic Contrastive Learning for Heterogeneous Graph Embedding](https://arxiv.org/abs/2506.16754)
*Jongmin Park,Seunghoon Han,Won-Yong Shin,Sungsu Lim*

Main category: cs.LG

TL;DR: The paper introduces a Metapath-based Hyperbolic Contrastive Learning (MHCL) framework using multiple hyperbolic spaces for better representation of heterogeneous graphs' diverse power-law structures.


<details>
  <summary>Details</summary>
Motivation: Heterogeneous graphs have complex and diverse power-law structures, but existing hyperbolic graph embeddings fail to capture this diversity using a single hyperbolic space.

Method: MHCL leverages multiple hyperbolic spaces to represent the structural diversity corresponding to metapaths, combined with a contrastive learning approach to enhance the discriminability of metapath embeddings.

Result: Experiments show MHCL surpasses state-of-the-art baselines in various graph machine learning tasks, demonstrating its capability to capture heterogeneous graph structures.

Conclusion: MHCL effectively addresses limitations in existing methods by using multiple hyperbolic spaces and contrastive learning to improve graph representation, setting a new benchmark in graph-based machine learning tasks.

Abstract: The hyperbolic space, characterized by a constant negative curvature and
exponentially expanding space, aligns well with the structural properties of
heterogeneous graphs. However, although heterogeneous graphs inherently possess
diverse power-law structures, most hyperbolic heterogeneous graph embedding
models rely on a single hyperbolic space. This approach may fail to effectively
capture the diverse power-law structures within heterogeneous graphs. To
address this limitation, we propose a Metapath-based Hyperbolic Contrastive
Learning framework (MHCL), which uses multiple hyperbolic spaces to capture
diverse complex structures within heterogeneous graphs. Specifically, by
learning each hyperbolic space to describe the distribution of complex
structures corresponding to each metapath, it is possible to capture semantic
information effectively. Since metapath embeddings represent distinct semantic
information, preserving their discriminability is important when aggregating
them to obtain node representations. Therefore, we use a contrastive learning
approach to optimize MHCL and improve the discriminability of metapath
embeddings. In particular, our contrastive learning method minimizes the
distance between embeddings of the same metapath and maximizes the distance
between those of different metapaths in hyperbolic space, thereby improving the
separability of metapath embeddings with distinct semantic information. We
conduct comprehensive experiments to evaluate the effectiveness of MHCL. The
experimental results demonstrate that MHCL outperforms state-of-the-art
baselines in various graph machine learning tasks, effectively capturing the
complex structures of heterogeneous graphs.

</details>


### [393] [What Is the Point of Equality in Machine Learning Fairness? Beyond Equality of Opportunity](https://arxiv.org/abs/2506.16782)
*Youjin Kong*

Main category: cs.LG

TL;DR: This paper argues that the focus on distributive equality in ML research is incomplete and proposes a multifaceted egalitarian framework incorporating both distributive and relational equality.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the ethical inadequacy in current ML fairness research, which predominantly focuses on distributive equality without considering representational harms and relational equality.

Method: The authors use critical social and political philosophy to construct an integrated egalitarian framework for ML fairness, addressing both allocative and representational harms.

Result: The proposed framework broadens the ethical foundation of ML fairness and highlights the need for relational equality to understand and mitigate representational harms in ML.

Conclusion: A multifaceted egalitarian approach is necessary for comprehensively addressing ML fairness, as it tackles both allocative and representational harms while fostering relational equality.

Abstract: Fairness in machine learning (ML) has become a rapidly growing area of
research. But why, in the first place, is unfairness in ML morally wrong? And
why should we care about improving fairness? Most fair-ML research implicitly
appeals to distributive equality: the idea that desirable goods and benefits,
such as opportunities (e.g., Barocas et al., 2023), should be equally
distributed across society. Unfair ML models, then, are seen as wrong because
they unequally distribute such benefits. This paper argues that this exclusive
focus on distributive equality offers an incomplete and potentially misleading
ethical foundation. Grounding ML fairness in egalitarianism -- the view that
equality is a fundamental moral and social ideal -- requires challenging
structural inequality: systematic, institutional, and durable arrangements that
privilege some groups while disadvantaging others. Structural inequality
manifests through ML systems in two primary forms: allocative harms (e.g.,
economic loss) and representational harms (e.g., stereotypes, erasure). While
distributive equality helps address allocative harms, it fails to explain why
representational harms are wrong -- why it is wrong for ML systems to reinforce
social hierarchies that stratify people into superior and inferior groups --
and why ML systems should aim to foster a society where people relate as equals
(i.e., relational equality). To address these limitations, the paper proposes a
multifaceted egalitarian framework for ML fairness that integrates both
distributive and relational equality. Drawing on critical social and political
philosophy, this framework offers a more comprehensive ethical foundation for
tackling the full spectrum of harms perpetuated by ML systems. The paper also
outlines practical pathways for implementing the framework across the ML
pipeline.

</details>


### [394] [Revisiting LoRA through the Lens of Parameter Redundancy: Spectral Encoding Helps](https://arxiv.org/abs/2506.16787)
*Jiashun Cheng,Aochuan Chen,Nuo Chen,Ziqi Gao,Yuhan Li,Jia Li,Fugee Tsung*

Main category: cs.LG

TL;DR: The paper introduces SeLoRA, a spectral-based, efficient re-parameterization of LoRA for fine-tuning foundation models.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiencies and limitations caused by parameter redundancy in LoRA during fine-tuning of large foundation models.

Method: The authors propose SeLoRA, leveraging sparse spectral subspaces to re-parameterize LoRA for reduced redundancy while maintaining expressiveness. It remains compatible with existing LoRA variants.

Result: SeLoRA outperforms strong baselines across tasks like commonsense reasoning, math reasoning, and code generation, achieving superior performance with fewer parameters.

Conclusion: SeLoRA provides an efficient, plug-and-play solution for enhancing LoRA's capacity and scalability in fine-tuning tasks.

Abstract: Low-Rank Adaptation (LoRA) has emerged as a prominent technique for
fine-tuning large foundation models. Despite its successes, the substantial
parameter redundancy, which limits the capacity and efficiency of LoRA, has
been recognized as a bottleneck. In this work, we systematically investigate
the impact of redundancy in fine-tuning LoRA and reveal that reducing density
redundancy does not degrade expressiveness. Based on this insight, we introduce
\underline{S}pectral-\underline{e}ncoding \underline{L}ow-\underline{R}ank
\underline{A}daptation (SeLoRA), which harnesses the robust expressiveness of
spectral bases to re-parameterize LoRA from a sparse spectral subspace.
Designed with simplicity, SeLoRA enables seamless integration with various LoRA
variants for performance boosting, serving as a scalable plug-and-play
framework. Extensive experiments substantiate that SeLoRA achieves greater
efficiency with fewer parameters, delivering superior performance enhancements
over strong baselines on various downstream tasks, including commonsense
reasoning, math reasoning, and code generation.

</details>


### [395] [Exploring and Improving Initialization for Deep Graph Neural Networks: A Signal Propagation Perspective](https://arxiv.org/abs/2506.16790)
*Senmiao Wang,Yupeng Chen,Yushun Zhang,Ruoyu Sun,Tian Ding*

Main category: cs.LG

TL;DR: The paper introduces SPoGInit, an advanced initialization method for GNNs, optimized for deeper network performance by enhancing signal propagation.


<details>
  <summary>Details</summary>
Motivation: To address the common issue of performance degradation in Graph Neural Networks (GNNs) as their depth increases.

Method: The paper proposes metrics for effective Signal Propagation (SP) and introduces SPoGInit, an initialization method that optimizes these metrics to enhance SP in GNNs.

Result: SPoGInit was tested across diverse tasks and architectures, demonstrating superior performance and mitigating depth-related degradation in GNNs.

Conclusion: SPoGInit advances GNN initialization techniques, enabling better scalability and overcoming depth-related challenges effectively.

Abstract: Graph Neural Networks (GNNs) often suffer from performance degradation as the
network depth increases. This paper addresses this issue by introducing
initialization methods that enhance signal propagation (SP) within GNNs. We
propose three key metrics for effective SP in GNNs: forward propagation,
backward propagation, and graph embedding variation (GEV). While the first two
metrics derive from classical SP theory, the third is specifically designed for
GNNs. We theoretically demonstrate that a broad range of commonly used
initialization methods for GNNs, which exhibit performance degradation with
increasing depth, fail to control these three metrics simultaneously. To deal
with this limitation, a direct exploitation of the SP analysis--searching for
weight initialization variances that optimize the three metrics--is shown to
significantly enhance the SP in deep GCNs. This approach is called Signal
Propagation on Graph-guided Initialization (SPoGInit). Our experiments
demonstrate that SPoGInit outperforms commonly used initialization methods on
various tasks and architectures. Notably, SPoGInit enables performance
improvements as GNNs deepen, which represents a significant advancement in
addressing depth-related challenges and highlights the validity and
effectiveness of the SP analysis framework.

</details>


### [396] [TabArena: A Living Benchmark for Machine Learning on Tabular Data](https://arxiv.org/abs/2506.16791)
*Nick Erickson,Lennart Purucker,Andrej Tschalzev,David Holzmüller,Prateek Mutalik Desai,and David Salinas,Frank Hutter*

Main category: cs.LG

TL;DR: TabArena introduces a dynamic benchmarking system for tabular data models, addressing flaws in static benchmarks by enabling continuous updates and providing a public leaderboard.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to tackle the limitations of static tabular benchmarks, which fail to evolve with model advancements, dataset changes, or identified flaws.

Method: The authors curated datasets and models, performed a detailed benchmarking study, and assembled an expert team to maintain and update a living leaderboard for tabular machine learning.

Result: Gradient-boosted trees still perform well, while deep learning models have closed the gap under larger time budgets. Foundation models perform best on smaller datasets, and ensembles across models improve state-of-the-art results.

Conclusion: TabArena establishes a dynamic, reliable benchmarking framework for tabular data with reproducible results and maintenance protocols to ensure continuous improvements.

Abstract: With the growing popularity of deep learning and foundation models for
tabular data, the need for standardized and reliable benchmarks is higher than
ever. However, current benchmarks are static. Their design is not updated even
if flaws are discovered, model versions are updated, or new models are
released. To address this, we introduce TabArena, the first continuously
maintained living tabular benchmarking system. To launch TabArena, we manually
curate a representative collection of datasets and well-implemented models,
conduct a large-scale benchmarking study to initialize a public leaderboard,
and assemble a team of experienced maintainers. Our results highlight the
influence of validation method and ensembling of hyperparameter configurations
to benchmark models at their full potential. While gradient-boosted trees are
still strong contenders on practical tabular datasets, we observe that deep
learning methods have caught up under larger time budgets with ensembling. At
the same time, foundation models excel on smaller datasets. Finally, we show
that ensembles across models advance the state-of-the-art in tabular machine
learning and investigate the contributions of individual models. We launch
TabArena with a public leaderboard, reproducible code, and maintenance
protocols to create a living benchmark available at https://tabarena.ai.

</details>


### [397] [Robust Group Anomaly Detection for Quasi-Periodic Network Time Series](https://arxiv.org/abs/2506.16815)
*Kai Yang,Shaoyu Dou,Pan Luo,Xin Wang,H. Vincent Poor*

Main category: cs.LG

TL;DR: The paper proposes seq2GMM, a framework for efficiently identifying unusual quasi-periodic multivariate time series in network databases. It outperforms existing anomaly detection techniques.


<details>
  <summary>Details</summary>
Motivation: Identify the minority of unusual time series in real-world networks while also offering interpretable decisions for human experts.

Method: The seq2GMM model, paired with a surrogate-based optimization training algorithm, is introduced for anomaly detection in quasi-periodic time series.

Result: Seq2GMM outperforms state-of-the-art anomaly detection methods on benchmark datasets and is supported by theoretical convergence analysis.

Conclusion: The seq2GMM model is effective and interpretable for anomaly detection in complex time series networks and shows promise in practical and theoretical aspects.

Abstract: Many real-world multivariate time series are collected from a network of
physical objects embedded with software, electronics, and sensors. The
quasi-periodic signals generated by these objects often follow a similar
repetitive and periodic pattern, but have variations in the period, and come in
different lengths caused by timing (synchronization) errors. Given a multitude
of such quasi-periodic time series, can we build machine learning models to
identify those time series that behave differently from the majority of the
observations? In addition, can the models help human experts to understand how
the decision was made? We propose a sequence to Gaussian Mixture Model
(seq2GMM) framework. The overarching goal of this framework is to identify
unusual and interesting time series within a network time series database. We
further develop a surrogate-based optimization algorithm that can efficiently
train the seq2GMM model. Seq2GMM exhibits strong empirical performance on a
plurality of public benchmark datasets, outperforming state-of-the-art anomaly
detection techniques by a significant margin. We also theoretically analyze the
convergence property of the proposed training algorithm and provide numerical
results to substantiate our theoretical claims.

</details>


### [398] [Predicting New Research Directions in Materials Science using Large Language Models and Concept Graphs](https://arxiv.org/abs/2506.16824)
*Thomas Marwitz,Alexander Colsmann,Ben Breitung,Christoph Brabec,Christoph Kirchlechner,Eva Blasco,Gabriel Cadilha Marques,Horst Hahn,Michael Hirtz,Pavel A. Levkin,Yolita M. Eggeler,Tobias Schlöder,Pascal Friederich*

Main category: cs.LG

TL;DR: The paper explores using large language models (LLMs) to extract concepts from materials science abstracts, predict innovative research ideas, and enhance creative thinking.


<details>
  <summary>Details</summary>
Motivation: The exponential growth in scientific publications makes it impossible for researchers to keep up with all relevant literature in their field, necessitating automated tools to extract key insights and identify potential research opportunities.

Method: Large language models (LLMs) are employed to extract key concepts from scientific abstracts, construct a semantic concept graph, and train a machine learning model to predict emergent combinations of these concepts as future research ideas.

Result: The study shows that LLMs outperform traditional keyword extraction methods, and incorporating semantic concept information improves the machine learning model's accuracy in predicting innovative research ideas. Qualitative expert interviews validate the model's utility in inspiring materials scientists.

Conclusion: LLMs are effective tools for extracting semantic information and predicting future research directions in materials science, fostering innovation and aiding researchers in generating novel ideas.

Abstract: Due to an exponential increase in published research articles, it is
impossible for individual scientists to read all publications, even within
their own research field. In this work, we investigate the use of large
language models (LLMs) for the purpose of extracting the main concepts and
semantic information from scientific abstracts in the domain of materials
science to find links that were not noticed by humans and thus to suggest
inspiring near/mid-term future research directions. We show that LLMs can
extract concepts more efficiently than automated keyword extraction methods to
build a concept graph as an abstraction of the scientific literature. A machine
learning model is trained to predict emerging combinations of concepts, i.e.
new research ideas, based on historical data. We demonstrate that integrating
semantic concept information leads to an increased prediction performance. The
applicability of our model is demonstrated in qualitative interviews with
domain experts based on individualized model suggestions. We show that the
model can inspire materials scientists in their creative thinking process by
predicting innovative combinations of topics that have not yet been
investigated.

</details>


### [399] [FedFitTech: A Baseline in Federated Learning for Fitness Tracking](https://arxiv.org/abs/2506.16840)
*Zeyneddin Oz,Shreyas Korde,Marius Bock,Kristof Van Laerhoven*

Main category: cs.LG

TL;DR: This paper introduces FedFitTech, a federated learning (FL) baseline developed to address challenges in wearable fitness technology while maintaining privacy, generalization, and efficiency.


<details>
  <summary>Details</summary>
Motivation: The paper aims to tackle privacy concerns, regulatory issues, communication inefficiencies, and challenges unique to federated learning in fitness technology, such as data imbalance and heterogeneous user activity patterns.

Method: The researchers developed the FedFitTech baseline under the Flower framework and evaluated it through a case study involving a client-side early stopping strategy.

Result: The system demonstrated reduced communication redundancies by 13% and maintained recognition performance with only a 1% cost to accuracy.

Conclusion: FedFitTech provides an open-source foundation for scalable, privacy-aware fitness tracking research and development in both academia and industry.

Abstract: Rapid evolution of sensors and resource-efficient machine learning models
have spurred the widespread adoption of wearable fitness tracking devices.
Equipped with inertial sensors, such devices can continuously capture physical
movements for fitness technology (FitTech), enabling applications from sports
optimization to preventive healthcare. Traditional centralized learning
approaches to detect fitness activities struggle with privacy concerns,
regulatory constraints, and communication inefficiencies. In contrast,
Federated Learning (FL) enables a decentralized model training by communicating
model updates rather than private wearable sensor data. Applying FL to FitTech
presents unique challenges, such as data imbalance, lack of labelled data,
heterogeneous user activity patterns, and trade-offs between personalization
and generalization. To simplify research on FitTech in FL, we present the
FedFitTech baseline, under the Flower framework, which is publicly available
and widely used by both industry and academic researchers. Additionally, to
illustrate its usage, this paper presents a case study that implements a system
based on the FedFitTech baseline, incorporating a client-side early stopping
strategy and comparing the results. For instance, this system allows wearable
devices to optimize the trade-off between capturing common fitness activity
patterns and preserving individuals' nuances, thereby enhancing both the
scalability and efficiency of privacy-aware fitness tracking applications.
Results show that this reduces overall redundant communications by 13 percent,
while maintaining the overall recognition performance at a negligible
recognition cost by 1 percent. Thus, FedFitTech baseline creates a foundation
for a wide range of new research and development opportunities in FitTech, and
it is available as open-source at:
https://github.com/adap/flower/tree/main/baselines/fedfittech

</details>


### [400] [Soft decision trees for survival analysis](https://arxiv.org/abs/2506.16846)
*Antonio Consoloa,Edoardo Amaldi,Emilio Carrizosa*

Main category: cs.LG

TL;DR: The paper introduces a new soft survival tree model (SST), optimizing survival prediction using globally trained soft splits, outperforming existing survival trees in popular metrics across datasets.


<details>
  <summary>Details</summary>
Motivation: To enhance interpretability while improving accuracy in survival analysis by addressing limitations of heuristic-based survival tree methods with a globally optimized approach.

Method: Proposes a soft survival tree (SST) model using soft splitting rules, trained via nonlinear optimization, enabling each leaf node to output specific survival functions suitable for diverse survival function types.

Result: SST models show superior performance across 15 datasets when compared to benchmark survival tree models, evaluated via discrimination and calibration techniques.

Conclusion: SST models combine interpretability and flexibility with better accuracy and can extend to applications requiring group fairness.

Abstract: Decision trees are popular in survival analysis for their interpretability
and ability to model complex relationships. Survival trees, which predict the
timing of singular events using censored historical data, are typically built
through heuristic approaches. Recently, there has been growing interest in
globally optimized trees, where the overall tree is trained by minimizing the
error function over all its parameters. We propose a new soft survival tree
model (SST), with a soft splitting rule at each branch node, trained via a
nonlinear optimization formulation amenable to decomposition. Since SSTs
provide for every input vector a specific survival function associated to a
single leaf node, they satisfy the conditional computation property and inherit
the related benefits. SST and the training formulation combine flexibility with
interpretability: any smooth survival function (parametric, semiparametric, or
nonparametric) estimated through maximum likelihood can be used, and each leaf
node of an SST yields a cluster of distinct survival functions which are
associated to the data points routed to it. Numerical experiments on 15
well-known datasets show that SSTs, with parametric and spline-based
semiparametric survival functions, trained using an adaptation of the
node-based decomposition algorithm proposed by Consolo et al. (2024) for soft
regression trees, outperform three benchmark survival trees in terms of four
widely-used discrimination and calibration measures. SSTs can also be extended
to consider group fairness.

</details>


### [401] [Reward-Agnostic Prompt Optimization for Text-to-Image Diffusion Models](https://arxiv.org/abs/2506.16853)
*Semin Kim,Yeonwoo Cha,Jaehoon Yoo,Seunghoon Hong*

Main category: cs.LG

TL;DR: The paper introduces RATTPO, an approach for optimizing text prompts for text-to-image diffusion models without depending on specific reward models, demonstrating superior efficiency and effectiveness across diverse evaluation criteria.


<details>
  <summary>Details</summary>
Motivation: Existing prompt optimization methods struggle in diverse reward scenarios because they are typically designed for specific reward configurations, leading to subpar performance when generalized.

Method: RATTPO utilizes a test-time optimization technique involving iterative prompt refinement using large language models, relying on a novel reward-aware feedback signal ("hint") and optimization trajectory for generalizability.

Result: RATTPO effectively enhances prompt quality across multiple reward configurations, outperforming other test-time methods in efficiency and achieving performance comparable to learning-based approaches requiring fine-tuning.

Conclusion: RATTPO provides a versatile and efficient solution to prompt optimization challenges, expanding the potential applications of text-to-image diffusion models without compromising on performance.

Abstract: We investigate a general approach for improving user prompts in text-to-image
(T2I) diffusion models by finding prompts that maximize a reward function
specified at test-time. Although diverse reward models are used for evaluating
image generation, existing automated prompt engineering methods typically
target specific reward configurations. Consequently, these specialized designs
exhibit suboptimal performance when applied to new prompt engineering scenarios
involving different reward models. To address this limitation, we introduce
RATTPO (Reward-Agnostic Test-Time Prompt Optimization), a flexible test-time
optimization method applicable across various reward scenarios without
modification. RATTPO iteratively searches for optimized prompts by querying
large language models (LLMs) \textit{without} requiring reward-specific task
descriptions. Instead, it uses the optimization trajectory and a novel
reward-aware feedback signal (termed a "hint") as context. Empirical results
demonstrate the versatility of RATTPO, effectively enhancing user prompts
across diverse reward setups that assess various generation aspects, such as
aesthetics, general human preference, or spatial relationships between objects.
RATTPO surpasses other test-time search baselines in search efficiency, using
up to 3.5 times less inference budget, and, given sufficient inference budget,
achieves performance comparable to learning-based baselines that require
reward-specific fine-tuning. The code is available at
https://github.com/seminkim/RATTPO.

</details>


### [402] [Anomaly Detection in Event-triggered Traffic Time Series via Similarity Learning](https://arxiv.org/abs/2506.16855)
*Shaoyu Dou,Kai Yang,Yang Jiao,Chengbo Qiu,Kui Ren*

Main category: cs.LG

TL;DR: This paper proposes an unsupervised framework using autoencoders and GMM to learn and measure similarities among event-triggered time series, achieving superior performance in cyber security tasks.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the need to better understand and quantify similarities in event-triggered time series for cybersecurity applications.

Method: A combination of hierarchical multi-resolution sequential autoencoders and Gaussian Mixture Model (GMM) is used to extract low-dimensional representations and compute similarity metrics.

Result: Extensive experiments demonstrate the proposed framework outperforms existing methods in both qualitative and quantitative evaluations.

Conclusion: The unsupervised framework provides a systematic approach to model and learn similarities in event-triggered time series, advancing security-related analyses like anomaly detection and clustering.

Abstract: Time series analysis has achieved great success in cyber security such as
intrusion detection and device identification. Learning similarities among
multiple time series is a crucial problem since it serves as the foundation for
downstream analysis. Due to the complex temporal dynamics of the
event-triggered time series, it often remains unclear which similarity metric
is appropriate for security-related tasks, such as anomaly detection and
clustering. The overarching goal of this paper is to develop an unsupervised
learning framework that is capable of learning similarities among a set of
event-triggered time series. From the machine learning vantage point, the
proposed framework harnesses the power of both hierarchical multi-resolution
sequential autoencoders and the Gaussian Mixture Model (GMM) to effectively
learn the low-dimensional representations from the time series. Finally, the
obtained similarity measure can be easily visualized for the explanation. The
proposed framework aspires to offer a stepping stone that gives rise to a
systematic approach to model and learn similarities among a multitude of
event-triggered time series. Through extensive qualitative and quantitative
experiments, it is revealed that the proposed method outperforms
state-of-the-art methods considerably.

</details>


### [403] [Optimal Depth of Neural Networks](https://arxiv.org/abs/2506.16862)
*Qian Qi*

Main category: cs.LG

TL;DR: The paper proposes a theoretical framework to find the optimal depth of neural networks by framing it as an optimal stopping problem, applied to architectures like ResNet and Transformers.


<details>
  <summary>Details</summary>
Motivation: Determining the optimal depth of a neural network is a challenging and resource-intensive task, motivating the need for a systematic and efficient approach.

Method: The paper models the forward pass of networks as a sequential decision process, introducing a regularization term ($\mathcal{L}_{\rm depth}$) to promote early exiting while balancing accuracy and computational cost.

Result: The proposed framework and regularization term reduce computational cost and, in some cases, improve model accuracy, as validated empirically on ImageNet.

Conclusion: The approach provides a general and practical method for optimizing neural network depth while making computations more efficient.

Abstract: Determining the optimal depth of a neural network is a fundamental yet
challenging problem, typically resolved through resource-intensive
experimentation. This paper introduces a formal theoretical framework to
address this question by recasting the forward pass of a deep network,
specifically a Residual Network (ResNet), as an optimal stopping problem. We
model the layer-by-layer evolution of hidden representations as a sequential
decision process where, at each layer, a choice is made between halting
computation to make a prediction or continuing to a deeper layer for a
potentially more refined representation. This formulation captures the
intrinsic trade-off between accuracy and computational cost. Our primary
theoretical contribution is a proof that, under a plausible condition of
diminishing returns on the residual functions, the expected optimal stopping
depth is provably finite, even in an infinite-horizon setting. We leverage this
insight to propose a novel and practical regularization term, $\mathcal{L}_{\rm
depth}$, that encourages the network to learn representations amenable to
efficient, early exiting. We demonstrate the generality of our framework by
extending it to the Transformer architecture and exploring its connection to
continuous-depth models via free-boundary problems. Empirical validation on
ImageNet confirms that our regularizer successfully induces the theoretically
predicted behavior, leading to significant gains in computational efficiency
without compromising, and in some cases improving, final model accuracy.

</details>


### [404] [From Lab to Factory: Pitfalls and Guidelines for Self-/Unsupervised Defect Detection on Low-Quality Industrial Images](https://arxiv.org/abs/2506.16890)
*Sebastian Hönel,Jonas Nordqvist*

Main category: cs.LG

TL;DR: The paper discusses machine learning-based approaches for detecting and localizing quality issues in industrial products using unsupervised methods, with a focus on challenges like robustness and metrics suitability in real-world conditions.


<details>
  <summary>Details</summary>
Motivation: Manual inspection of mass-produced industrial products is costly and error-prone, prompting the need for automated and unsupervised machine learning methods to handle the variety and unpredictability of quality defects.

Method: The authors evaluate state-of-the-art machine learning models to identify and improve quality issues in production data using low-quality RGB imagery of blasted forged metal parts. They also provide a framework for recognizing problems in model robustness and empirical risk estimation.

Result: The study reveals common pitfalls of likelihood-based approaches and provides guidance for practitioners to handle robustness and invariance challenges effectively in industrial settings.

Conclusion: The proposed guardrails help practitioners diagnose and address challenges in using machine learning for defect detection while highlighting suitability for real-world applications and pitfalls of common metrics like AUROC.

Abstract: The detection and localization of quality-related problems in industrially
mass-produced products has historically relied on manual inspection, which is
costly and error-prone. Machine learning has the potential to replace manual
handling. As such, the desire is to facilitate an unsupervised (or
self-supervised) approach, as it is often impossible to specify all conceivable
defects ahead of time. A plethora of prior works have demonstrated the aptitude
of common reconstruction-, embedding-, and synthesis-based methods in
laboratory settings. However, in practice, we observe that most methods do not
handle low data quality well or exude low robustness in unfavorable, but
typical real-world settings. For practitioners it may be very difficult to
identify the actual underlying problem when such methods underperform. Worse,
often-reported metrics (e.g., AUROC) are rarely suitable in practice and may
give misleading results. In our setting, we attempt to identify subtle
anomalies on the surface of blasted forged metal parts, using rather
low-quality RGB imagery only, which is a common industrial setting. We
specifically evaluate two types of state-of-the-art models that allow us to
identify and improve quality issues in production data, without having to
obtain new data. Our contribution is to provide guardrails for practitioners
that allow them to identify problems related to, e.g., (lack of) robustness or
invariance, in either the chosen model or the data reliably in similar
scenarios. Furthermore, we exemplify common pitfalls in and shortcomings of
likelihood-based approaches and outline a framework for proper empirical risk
estimation that is more suitable for real-world scenarios.

</details>


### [405] [A deep learning and machine learning approach to predict neonatal death in the context of São Paulo](https://arxiv.org/abs/2506.16929)
*Mohon Raihan,Plabon Kumar Saha,Rajan Das Gupta,A Z M Tahmidul Kabir,Afia Anjum Tamanna,Md. Harun-Ur-Rashid,Adnan Bin Abdus Salam,Md Tanvir Anjum,A Z M Ahteshamul Kabir*

Main category: cs.LG

TL;DR: The paper explores using machine learning (ML) and deep learning (DL) techniques for predicting neonatal mortality, with LSTM achieving the highest accuracy at 99%.


<details>
  <summary>Details</summary>
Motivation: To address the high neonatal death rate and provide early prediction of endangered newborns to enable preventive care and reduce mortality.

Method: They applied ML and DL models—including XGBoost, random forest, and LSTM—on a dataset of 1.4 million newborns to predict neonatal mortality.

Result: XGBoost and random forest models achieved 94% accuracy, while the LSTM deep learning model outperformed them with 99% accuracy.

Conclusion: The study concludes that deep learning, specifically LSTM, is the most effective approach for predicting neonatal mortality risk to guide precautionary measures.

Abstract: Neonatal death is still a concerning reality for underdeveloped and even some
developed countries. Worldwide data indicate that 26.693 babies out of 1,000
births die, according to Macro Trades. To reduce this number, early prediction
of endangered babies is crucial. Such prediction enables the opportunity to
take ample care of the child and mother so that early child death can be
avoided. In this context, machine learning was used to determine whether a
newborn baby is at risk. To train the predictive model, historical data of 1.4
million newborns was used. Machine learning and deep learning techniques such
as logical regression, K-nearest neighbor, random forest classifier, extreme
gradient boosting (XGBoost), convolutional neural network, and long short-term
memory (LSTM) were implemented using the dataset to identify the most accurate
model for predicting neonatal mortality. Among the machine learning algorithms,
XGBoost and random forest classifier achieved the best accuracy with 94%, while
among the deep learning models, LSTM delivered the highest accuracy with 99%.
Therefore, using LSTM appears to be the most suitable approach to predict
whether precautionary measures for a child are necessary.

</details>


### [406] [Robust Reinforcement Learning for Discrete Compositional Generation via General Soft Operators](https://arxiv.org/abs/2506.17007)
*Marco Jiralerspong,Esther Derman,Danilo Vucetic,Nikolay Malkin,Bilun Sun,Tianyu Zhang,Pierre-Luc Bacon,Gauthier Gidel*

Main category: cs.LG

TL;DR: This paper tackles challenges in scientific discovery by proposing a robust reinforcement learning (RL) operator to enhance candidate selection in large search spaces.


<details>
  <summary>Details</summary>
Motivation: Science often faces difficulties in narrowing large combinatorial sets of objects, such as proteins or molecules, to viable candidates. Current RL methods face limitations due to proxy reward function uncertainties.

Method: The authors introduce a robust RL operator aiming for peakier sampling distributions that are less affected by reward function uncertainties, alongside a novel algorithm for compositional generation.

Result: The proposed algorithm identifies higher-quality, diverse candidates in both synthetic and real-world scenarios, outperforming existing methods.

Conclusion: This work provides a more effective and flexible framework for discrete compositional generation tasks, advancing candidate selection in scientific discovery.

Abstract: A major bottleneck in scientific discovery involves narrowing a large
combinatorial set of objects, such as proteins or molecules, to a small set of
promising candidates. While this process largely relies on expert knowledge,
recent methods leverage reinforcement learning (RL) to enhance this filtering.
They achieve this by estimating proxy reward functions from available datasets
and using regularization to generate more diverse candidates. These reward
functions are inherently uncertain, raising a particularly salient challenge
for scientific discovery. In this work, we show that existing methods, often
framed as sampling proportional to a reward function, are inadequate and yield
suboptimal candidates, especially in large search spaces. To remedy this issue,
we take a robust RL approach and introduce a unified operator that seeks
robustness to the uncertainty of the proxy reward function. This general
operator targets peakier sampling distributions while encompassing known soft
RL operators. It also leads us to a novel algorithm that identifies
higher-quality, diverse candidates in both synthetic and real-world tasks.
Ultimately, our work offers a new, flexible perspective on discrete
compositional generation tasks. Code: https://github.com/marcojira/tgm.

</details>


### [407] [The Hidden Cost of an Image: Quantifying the Energy Consumption of AI Image Generation](https://arxiv.org/abs/2506.17016)
*Giulia Bertazzini,Chiara Albisani,Daniele Baracchi,Dasara Shullani,Roberto Verdecchia*

Main category: cs.LG

TL;DR: The study assesses the environmental impact of generating AI images, showing a significant variability in energy consumption across different models, resolutions, and methods.


<details>
  <summary>Details</summary>
Motivation: To explore the environmental impact associated with AI image generation, given its growing popularity and resource demands.

Method: A comparative analysis of 17 AI image generation models, considering factors like model quantization, image resolution, prompt length, and trade-offs with image quality.

Result: There is up to a 46x difference in energy consumption among models. Resolution inconsistently affects energy use (1.3x to 4.7x with doubling), U-Net-based models are generally more energy-efficient, and model quantization worsens efficiency. Prompt length/content have negligible impact. Higher image quality does not necessarily mean higher energy consumption.

Conclusion: Understanding energy consumption variations helps balance environmental impact without sacrificing image quality. Some efficient models are also capable of high-quality image generation.

Abstract: With the growing adoption of AI image generation, in conjunction with the
ever-increasing environmental resources demanded by AI, we are urged to answer
a fundamental question: What is the environmental impact hidden behind each
image we generate? In this research, we present a comprehensive empirical
experiment designed to assess the energy consumption of AI image generation.
Our experiment compares 17 state-of-the-art image generation models by
considering multiple factors that could affect their energy consumption, such
as model quantization, image resolution, and prompt length. Additionally, we
consider established image quality metrics to study potential trade-offs
between energy consumption and generated image quality. Results show that image
generation models vary drastically in terms of the energy they consume, with up
to a 46x difference. Image resolution affects energy consumption
inconsistently, ranging from a 1.3x to 4.7x increase when doubling resolution.
U-Net-based models tend to consume less than Transformer-based one. Model
quantization instead results to deteriorate the energy efficiency of most
models, while prompt length and content have no statistically significant
impact. Improving image quality does not always come at the cost of a higher
energy consumption, with some of the models producing the highest quality
images also being among the most energy efficient ones.

</details>


### [408] [Scalable and Reliable Multi-agent Reinforcement Learning for Traffic Assignment](https://arxiv.org/abs/2506.17029)
*Leizhen Wang,Peibo Duan,Cheng Lyu,Zewen Wang,Zhiqiang He,Nan Zheng,Zhenliang Ma*

Main category: cs.LG

TL;DR: The study addresses scalability and reliability challenges in traffic assignment using a novel multi-agent reinforcement learning framework called MARL-OD-DA that defines agents as origin-destination pair routers.


<details>
  <summary>Details</summary>
Motivation: Traditional traffic assignment methods struggle to model adaptive routing behavior effectively, and existing MARL frameworks face scalability challenges in large networks.

Method: The paper introduces a MARL framework (MARL-OD-DA) that uses OD pair routers as agents, a Dirichlet-based action space, action pruning, and a locally-based reward function.

Result: Experiments on the SiouxFalls network demonstrated MARL-OD-DA's superiority, providing assignment solutions with a 94.99% lower relative gap in just 10 steps compared to conventional methods.

Conclusion: MARL-OD-DA enhances scalability and solution reliability, making medium-sized traffic networks more feasible to handle and outperform existing MARL approaches.

Abstract: The evolution of metropolitan cities and the increase in travel demands
impose stringent requirements on traffic assignment methods. Multi-agent
reinforcement learning (MARL) approaches outperform traditional methods in
modeling adaptive routing behavior without requiring explicit system dynamics,
which is beneficial for real-world deployment. However, MARL frameworks face
challenges in scalability and reliability when managing extensive networks with
substantial travel demand, which limiting their practical applicability in
solving large-scale traffic assignment problems. To address these challenges,
this study introduces MARL-OD-DA, a new MARL framework for the traffic
assignment problem, which redefines agents as origin-destination (OD) pair
routers rather than individual travelers, significantly enhancing scalability.
Additionally, a Dirichlet-based action space with action pruning and a reward
function based on the local relative gap are designed to enhance solution
reliability and improve convergence efficiency. Experiments demonstrate that
the proposed MARL framework effectively handles medium-sized networks with
extensive and varied city-level OD demand, surpassing existing MARL methods.
When implemented in the SiouxFalls network, MARL-OD-DA achieves better
assignment solutions in 10 steps, with a relative gap that is 94.99% lower than
that of conventional methods.

</details>


### [409] [Critical Appraisal of Fairness Metrics in Clinical Predictive AI](https://arxiv.org/abs/2506.17035)
*João Matos,Ben Van Calster,Leo Anthony Celi,Paula Dhiman,Judy Wawira Gichoya,Richard D. Riley,Chris Russell,Sara Khalid,Gary S. Collins*

Main category: cs.LG

TL;DR: The paper reviews fairness metrics in clinical predictive AI, identifying 62 metrics from 41 studies and highlighting challenges like fragmentation, limited clinical validation, and gaps in real-world applicability.


<details>
  <summary>Details</summary>
Motivation: To improve clinical AI practice and patient outcomes while avoiding perpetuated biases by addressing fairness comprehensively.

Method: A scoping review of fairness metrics, using five databases to screen 820 studies and extracting data from 41 relevant works.

Result: Identified 62 fairness metrics and categorized them, finding issues with validation, fragmentation, and over reliance on threshold measures.

Conclusion: Emphasizes the need for clinically relevant metrics and further research in areas like uncertainty, intersectionality, and real-world use.

Abstract: Predictive artificial intelligence (AI) offers an opportunity to improve
clinical practice and patient outcomes, but risks perpetuating biases if
fairness is inadequately addressed. However, the definition of "fairness"
remains unclear. We conducted a scoping review to identify and critically
appraise fairness metrics for clinical predictive AI. We defined a "fairness
metric" as a measure quantifying whether a model discriminates (societally)
against individuals or groups defined by sensitive attributes. We searched five
databases (2014-2024), screening 820 records, to include 41 studies, and
extracted 62 fairness metrics. Metrics were classified by
performance-dependency, model output level, and base performance metric,
revealing a fragmented landscape with limited clinical validation and
overreliance on threshold-dependent measures. Eighteen metrics were explicitly
developed for healthcare, including only one clinical utility metric. Our
findings highlight conceptual challenges in defining and quantifying fairness
and identify gaps in uncertainty quantification, intersectionality, and
real-world applicability. Future work should prioritise clinically meaningful
metrics.

</details>


### [410] [LSCD: Lomb-Scargle Conditioned Diffusion for Time series Imputation](https://arxiv.org/abs/2506.17039)
*Elizabeth Fons,Alejandro Sztrajman,Yousef El-Laham,Luciana Ferrer,Svitlana Vyetrenko,Manuela Veloso*

Main category: cs.LG

TL;DR: The paper introduces a differentiable Lomb-Scargle layer to handle irregularly sampled time series data, improving imputation and spectral consistency over time-domain methods.


<details>
  <summary>Details</summary>
Motivation: To address challenges in handling missing or irregularly sampled time series in machine learning without distorting the spectrum during interpolation.

Method: The researchers propose a differentiable Lomb-Scargle layer combined with a novel score-based diffusion model, enabling spectral computations directly on irregularly sampled data.

Result: The method outperforms time-domain baselines in recovering missing data and provides more accurate and consistent frequency estimates.

Conclusion: This work improves imputation for irregularly sampled time series, offering a framework that integrates spectral guidance and can be broadly adopted in machine learning.

Abstract: Time series with missing or irregularly sampled data are a persistent
challenge in machine learning. Many methods operate on the frequency-domain,
relying on the Fast Fourier Transform (FFT) which assumes uniform sampling,
therefore requiring prior interpolation that can distort the spectra. To
address this limitation, we introduce a differentiable Lomb--Scargle layer that
enables a reliable computation of the power spectrum of irregularly sampled
data. We integrate this layer into a novel score-based diffusion model (LSCD)
for time series imputation conditioned on the entire signal spectrum.
Experiments on synthetic and real-world benchmarks demonstrate that our method
recovers missing data more accurately than purely time-domain baselines, while
simultaneously producing consistent frequency estimates. Crucially, our method
can be easily integrated into learning frameworks, enabling broader adoption of
spectral guidance in machine learning approaches involving incomplete or
irregular data.

</details>


### [411] [MAWIFlow Benchmark: Realistic Flow-Based Evaluation for Network Intrusion Detection](https://arxiv.org/abs/2506.17041)
*Joshua Schraven,Alexander Windmann,Oliver Niggemann*

Main category: cs.LG

TL;DR: This paper introduces MAWIFlow, a flow-based benchmark for network intrusion detection, derived from the MAWILAB dataset, addressing limitations of synthetic traffic datasets. The benchmark and code are publicly available.


<details>
  <summary>Details</summary>
Motivation: Synthetic datasets for network intrusion detection fail to capture the real-world variability and temporal drift, leading to unrealistic evaluations of anomaly detection methods.

Method: The authors transformed packet captures from the MAWILAB dataset into flow representations using a reproducible preprocessing pipeline while preserving original anomaly labels. They evaluated traditional machine learning models alongside a CNN-BiLSTM deep learning model for performance comparison.

Result: Tree-based models demonstrated strong initial performance but struggled with temporal drift, whereas the CNN-BiLSTM model exhibited better long-term generalization. This highlights the benefits of realistic datasets with temporal structure.

Conclusion: Synthetic benchmarks are limited in capturing real-world variability. Realistic benchmarks like MAWIFlow are necessary for effective evaluation of anomaly detection models. The study promotes transparency by publicly sharing data, code, and models.

Abstract: Benchmark datasets for network intrusion detection commonly rely on
synthetically generated traffic, which fails to reflect the statistical
variability and temporal drift encountered in operational environments. This
paper introduces MAWIFlow, a flow-based benchmark derived from the MAWILAB v1.1
dataset, designed to enable realistic and reproducible evaluation of anomaly
detection methods. A reproducible preprocessing pipeline is presented that
transforms raw packet captures into flow representations conforming to the
CICFlowMeter format, while preserving MAWILab's original anomaly labels. The
resulting datasets comprise temporally distinct samples from January 2011,
2016, and 2021, drawn from trans-Pacific backbone traffic.
  To establish reference baselines, traditional machine learning methods,
including Decision Trees, Random Forests, XGBoost, and Logistic Regression, are
compared to a deep learning model based on a CNN-BiLSTM architecture. Empirical
results demonstrate that tree-based classifiers perform well on temporally
static data but experience significant performance degradation over time. In
contrast, the CNN-BiLSTM model maintains better performance, thus showing
improved generalization. These findings underscore the limitations of synthetic
benchmarks and static models, and motivate the adoption of realistic datasets
with explicit temporal structure. All datasets, pipeline code, and model
implementations are made publicly available to foster transparency and
reproducibility.

</details>


### [412] [Navigating the Deep: Signature Extraction on Deep Neural Networks](https://arxiv.org/abs/2506.17047)
*Haolin Liu,Adrien Siproudhis,Samuel Experton,Peter Lorenz,Christina Boura,Thomas Peyrin*

Main category: cs.LG

TL;DR: This paper refines neural network signature extraction methods to enable recovering weights from deeper networks, addressing limitations like rank deficiency and noise propagation.


<details>
  <summary>Details</summary>
Motivation: Prior methods, including Carlini et al.'s technique, struggle to extract parameters from deeper neural networks, restricting their practicality in security-sensitive scenarios.

Method: The authors systematically identify critical issues in earlier approaches to signature extraction and implement efficient algorithmic solutions to overcome them, improving extraction depth and accuracy.

Result: Their method enables extracting weights effectively for deeper networks than before. For example, they successfully extracted all eight layers from a neural network trained on CIFAR-10 with over 95% input-space accuracy, whereas previous works managed only three layers.

Conclusion: This work makes signature extraction feasible for larger networks, posing significant implications for the security of complex neural architectures.

Abstract: Neural network model extraction has emerged in recent years as an important
security concern, as adversaries attempt to recover a network's parameters via
black-box queries. A key step in this process is signature extraction, which
aims to recover the absolute values of the network's weights layer by layer.
Prior work, notably by Carlini et al. (2020), introduced a technique inspired
by differential cryptanalysis to extract neural network parameters. However,
their method suffers from several limitations that restrict its applicability
to networks with a few layers only. Later works focused on improving sign
extraction, but largely relied on the assumption that signature extraction
itself was feasible.
  In this work, we revisit and refine the signature extraction process by
systematically identifying and addressing for the first time critical
limitations of Carlini et al.'s signature extraction method. These limitations
include rank deficiency and noise propagation from deeper layers. To overcome
these challenges, we propose efficient algorithmic solutions for each of the
identified issues, greatly improving the efficiency of signature extraction.
Our approach permits the extraction of much deeper networks than was previously
possible. We validate our method through extensive experiments on ReLU-based
neural networks, demonstrating significant improvements in extraction depth and
accuracy. For instance, our extracted network matches the target network on at
least 95% of the input space for each of the eight layers of a neural network
trained on the CIFAR-10 dataset, while previous works could barely extract the
first three layers. Our results represent a crucial step toward practical
attacks on larger and more complex neural network architectures.

</details>


### [413] [Flow-Based Non-stationary Temporal Regime Causal Structure Learning](https://arxiv.org/abs/2506.17065)
*Abdellah Rahmani,Pascal Frossard*

Main category: cs.LG

TL;DR: The paper introduces FANTOM, a framework to discover causal relationships in non-stationary multivariate time series with complex noise, addressing challenges current methods cannot handle.


<details>
  <summary>Details</summary>
Motivation: The paper aims to resolve challenges in understanding causal relationships in multivariate time series data, particularly when data exhibits non-stationarity, has regime shifts, and complex noise distributions.

Method: FANTOM employs a Bayesian Expectation Maximization algorithm to infer regime boundaries and learn causal graphs for each regime, addressing non-stationarity and non-Gaussian noise issues.

Result: Experimental evaluations on synthetic and real-world datasets demonstrate that FANTOM outperforms existing causal discovery methods, showcasing its effective handling of non-stationarity and complex noise.

Conclusion: The proposed FANTOM framework successfully learns causal structures in challenging scenarios, with proven theoretical identifiability and superior practical performance over current approaches.

Abstract: Understanding causal relationships in multivariate time series is crucial in
many scenarios, such as those dealing with financial or neurological data. Many
such time series exhibit multiple regimes, i.e., consecutive temporal segments
with a priori unknown boundaries, with each regime having its own causal
structure. Inferring causal dependencies and regime shifts is critical for
analyzing the underlying processes. However, causal structure learning in this
setting is challenging due to (1) non stationarity, i.e., each regime can have
its own causal graph and mixing function, and (2) complex noise distributions,
which may be non Gaussian or heteroscedastic. Existing causal discovery
approaches cannot address these challenges, since generally assume stationarity
or Gaussian noise with constant variance. Hence, we introduce FANTOM, a unified
framework for causal discovery that handles non stationary processes along with
non Gaussian and heteroscedastic noises. FANTOM simultaneously infers the
number of regimes and their corresponding indices and learns each regime's
Directed Acyclic Graph. It uses a Bayesian Expectation Maximization algorithm
that maximizes the evidence lower bound of the data log likelihood. On the
theoretical side, we prove, under mild assumptions, that temporal
heteroscedastic causal models, introduced in FANTOM's formulation, are
identifiable in both stationary and non stationary settings. In addition,
extensive experiments on synthetic and real data show that FANTOM outperforms
existing methods.

</details>


### [414] [TransDreamerV3: Implanting Transformer In DreamerV3](https://arxiv.org/abs/2506.17103)
*Shruti Sadanand Dongare,Amun Kharel,Jonathan Samuel,Xiaona Zhou*

Main category: cs.LG

TL;DR: This work introduces TransDreamerV3, a reinforcement learning model that incorporates a transformer encoder to improve decision-making, showing enhanced results over DreamerV3 in complex tasks.


<details>
  <summary>Details</summary>
Motivation: To address the limitations in memory and decision-making in world model-based reinforcement learning frameworks, particularly in complex environments.

Method: The approach involves integrating a transformer encoder into the DreamerV3 architecture and conducting experiments on tasks like Atari-Boxing, Atari-Freeway, Atari-Pong, and Crafter to evaluate performance.

Result: TransDreamerV3 outperformed the DreamerV3 baseline in some tasks, particularly Atari-Freeway and Crafter, though challenges like Minecraft task failures and limited training diversity were noted.

Conclusion: TransDreamerV3 demonstrates a promising advance in reinforcement learning by successfully utilizing transformer architectures to enhance world modeling and decision-making capabilities in specific tasks.

Abstract: This paper introduces TransDreamerV3, a reinforcement learning model that
enhances the DreamerV3 architecture by integrating a transformer encoder. The
model is designed to improve memory and decision-making capabilities in complex
environments. We conducted experiments on Atari-Boxing, Atari-Freeway,
Atari-Pong, and Crafter tasks, where TransDreamerV3 demonstrated improved
performance over DreamerV3, particularly in the Atari-Freeway and Crafter
tasks. While issues in the Minecraft task and limited training across all tasks
were noted, TransDreamerV3 displays advancement in world model-based
reinforcement learning, leveraging transformer architectures.

</details>


### [415] [Rapid and Continuous Trust Evaluation for Effective Task Collaboration Through Siamese Model](https://arxiv.org/abs/2506.17128)
*Botao Zhu,Xianbin Wang*

Main category: cs.LG

TL;DR: The paper presents a Siamese-enabled trust evaluation framework (SRCTE) for collaborative systems, using real-time data and deep learning to ensure effective collaboration.


<details>
  <summary>Details</summary>
Motivation: To address challenges in rapidly and continuously evaluating the trustworthiness of collaborators in distributed and dynamic collaborative systems.

Method: The proposed SRCTE framework uses an attributed control flow graph (ACFG) to model trust-related data, a Siamese model with Structure2vec networks to generate embeddings, and similarity measures to calculate trust values in real time.

Result: Experiments on a real system demonstrate that SRCTE rapidly converges with minimal data and outperforms baseline algorithms in anomaly trust detection rates.

Conclusion: The SRCTE framework enables effective, rapid, and continuous trust evaluation in collaborative environments, enhancing the overall collaboration process.

Abstract: Trust is emerging as an effective tool to ensure the successful completion of
collaborative tasks within collaborative systems. However, rapidly and
continuously evaluating the trustworthiness of collaborators during task
execution is a significant challenge due to distributed devices, complex
operational environments, and dynamically changing resources. To tackle this
challenge, this paper proposes a Siamese-enabled rapid and continuous trust
evaluation framework (SRCTE) to facilitate effective task collaboration. First,
the communication and computing resource attributes of the collaborator in a
trusted state, along with historical collaboration data, are collected and
represented using an attributed control flow graph (ACFG) that captures
trust-related semantic information and serves as a reference for comparison
with data collected during task execution. At each time slot of task execution,
the collaborator's communication and computing resource attributes, as well as
task completion effectiveness, are collected in real time and represented with
an ACFG to convey their trust-related semantic information. A Siamese model,
consisting of two shared-parameter Structure2vec networks, is then employed to
learn the deep semantics of each pair of ACFGs and generate their embeddings.
Finally, the similarity between the embeddings of each pair of ACFGs is
calculated to determine the collaborator's trust value at each time slot. A
real system is built using two Dell EMC 5200 servers and a Google Pixel 8 to
test the effectiveness of the proposed SRCTE framework. Experimental results
demonstrate that SRCTE converges rapidly with only a small amount of data and
achieves a high anomaly trust detection rate compared to the baseline
algorithm.

</details>


### [416] [Sparse-Reg: Improving Sample Complexity in Offline Reinforcement Learning using Sparsity](https://arxiv.org/abs/2506.17155)
*Samin Yeasar Arnob,Scott Fujimoto,Doina Precup*

Main category: cs.LG

TL;DR: This paper shows that offline RL algorithms often overfit on small datasets and introduces a sparsity-based regularization technique (Sparse-Reg) to address this issue.


<details>
  <summary>Details</summary>
Motivation: The authors aim to enhance offline reinforcement learning in scenarios where data is limited, as traditional offline RL benchmarks and methods often require large datasets.

Method: They propose a regularization technique called Sparse-Reg, which uses sparsity to reduce overfitting in offline RL with small datasets.

Result: Sparse-Reg is shown to enable effective learning on limited data and outperform state-of-the-art methods in continuous control tasks.

Conclusion: The proposed Sparse-Reg method proves effective in mitigating overfitting in small offline RL datasets, offering a solution for limited-data scenarios.

Abstract: In this paper, we investigate the use of small datasets in the context of
offline reinforcement learning (RL). While many common offline RL benchmarks
employ datasets with over a million data points, many offline RL applications
rely on considerably smaller datasets. We show that offline RL algorithms can
overfit on small datasets, resulting in poor performance. To address this
challenge, we introduce "Sparse-Reg": a regularization technique based on
sparsity to mitigate overfitting in offline reinforcement learning, enabling
effective learning in limited data settings and outperforming state-of-the-art
baselines in continuous control.

</details>


### [417] [Deep generative models as the probability transformation functions](https://arxiv.org/abs/2506.17171)
*Vitalii Bondar,Vira Babenko,Roman Trembovetskyi,Yurii Korobeinyk,Viktoriya Dzyuba*

Main category: cs.LG

TL;DR: The paper presents a unified perspective of deep generative models, showing they all transform predefined simple distributions into complex target data distributions.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between various generative model architectures and their training methods, and develop a universal theoretical framework to improve generative modeling.

Method: Analyzing diverse generative models, including autoencoders, GANs, normalizing flows, diffusion models, and autoregressive models, to reveal a common principle underlying their operation.

Result: Demonstrated that all generative models fundamentally behave as probability transformation mechanisms, irrespective of architectural differences.

Conclusion: The unification enables the transfer of improvements across architectures and lays the groundwork for more efficient and universal generative modeling approaches.

Abstract: This paper introduces a unified theoretical perspective that views deep
generative models as probability transformation functions. Despite the apparent
differences in architecture and training methodologies among various types of
generative models - autoencoders, autoregressive models, generative adversarial
networks, normalizing flows, diffusion models, and flow matching - we
demonstrate that they all fundamentally operate by transforming simple
predefined distributions into complex target data distributions. This unifying
perspective facilitates the transfer of methodological improvements between
model architectures and provides a foundation for developing universal
theoretical approaches, potentially leading to more efficient and effective
generative modeling techniques.

</details>


### [418] [Network Sparsity Unlocks the Scaling Potential of Deep Reinforcement Learning](https://arxiv.org/abs/2506.17204)
*Guozheng Ma,Lu Li,Zilin Wang,Li Shen,Pierre-Luc Bacon,Dacheng Tao*

Main category: cs.LG

TL;DR: This paper shows that introducing static network sparsity via simple random pruning can improve the performance of deep reinforcement learning (DRL) models, surpassing dense networks in scalability and robustness.


<details>
  <summary>Details</summary>
Motivation: Deep reinforcement learning models face challenges in scalability due to network issues during training, prompting the need for efficient solutions to address optimization problems.

Method: Static network sparsity is introduced using one-shot random pruning, where a fixed percentage of weights are removed before training begins.

Result: Sparse networks not only exhibit higher parameter efficiency and better expressive capabilities but also demonstrate improved resistance to issues like plasticity loss and gradient interference. Performance benefits are consistent across both visual and streaming RL scenarios.

Conclusion: Static sparsity, achieved through straightforward random pruning, is an effective strategy for scaling deep reinforcement learning models and overcoming optimization and scalability challenges.

Abstract: Effectively scaling up deep reinforcement learning models has proven
notoriously difficult due to network pathologies during training, motivating
various targeted interventions such as periodic reset and architectural
advances such as layer normalization. Instead of pursuing more complex
modifications, we show that introducing static network sparsity alone can
unlock further scaling potential beyond their dense counterparts with
state-of-the-art architectures. This is achieved through simple one-shot random
pruning, where a predetermined percentage of network weights are randomly
removed once before training. Our analysis reveals that, in contrast to naively
scaling up dense DRL networks, such sparse networks achieve both higher
parameter efficiency for network expressivity and stronger resistance to
optimization challenges like plasticity loss and gradient interference. We
further extend our evaluation to visual and streaming RL scenarios,
demonstrating the consistent benefits of network sparsity.

</details>


### [419] [BREAD: Branched Rollouts from Expert Anchors Bridge SFT & RL for Reasoning](https://arxiv.org/abs/2506.17211)
*Xuechen Zhang,Zijian Huang,Yingcong Li,Chenshun Ni,Jiasi Chen,Samet Oymak*

Main category: cs.LG

TL;DR: The paper addresses the limitations of training small language models (SLMs) using supervised fine-tuning (SFT) followed by reinforcement learning (RL). It proposes BREAD, a new method integrating SFT and RL with expert guidance, that outperforms standard methods and trains faster.


<details>
  <summary>Details</summary>
Motivation: SLMs struggle to learn complex reasoning when expert traces are either limited or too complex, and the current SFT + RL strategy has fundamental limitations in efficiently training SLMs in such scenarios.

Method: The authors propose BREAD, a variant of GRPO that combines partial expert guidance and branched rollouts. BREAD adaptively introduces expert hints during failed self-generated traces to ensure successful updates and creates a natural curriculum for learning.

Result: BREAD reduces the need for expert traces by over 60%, improves performance over standard GRPO, and accelerates training by approximately 3 times. It also enables SLMs to solve reasoning tasks that the traditional SFT + RL strategy cannot.

Conclusion: BREAD demonstrates that partial expert guidance and branched rollouts can effectively address SLM training challenges, making the method a promising direction for improving reasoning capabilities in small language models.

Abstract: Small language models (SLMs) struggle to learn complex reasoning behaviors,
especially when high-quality traces are scarce or difficult to learn from. The
standard training approach combines a supervised fine-tuning (SFT) stage, often
to distill capabilities of a larger model, followed by a reinforcement learning
(RL)stage such as Group Relative Policy Optimization (GRPO). In this paper, we
investigate the fundamental limitations of this SFT + RL paradigm and propose
methods to overcome them. Under a suitable theoretical model, we demonstrate
that the SFT + RL strategy can fail completely when (1) the expert's traces are
too difficult for the small model to express, or (2) the small model's
initialization has exponentially small likelihood of success. To address these,
we introduce BREAD: a GRPO variant that unifies the SFT and RL stages via
partial expert guidance and branched rollouts. When self-generated traces fail,
BREAD adaptively inserts short expert prefixes/hints, allowing the small model
to complete the rest of the reasoning path, and ensuring that each update
includes at least one successful trace. This mechanism both densifies the
reward signal and induces a natural learning curriculum. BREAD requires fewer
than 40% of ground-truth traces, consistently outperforming standard GRPO while
speeding up the training by about 3 times. Importantly, we demonstrate that
BREAD helps the model solve problems that are otherwise unsolvable by the SFT +
RL strategy, highlighting how branched rollouts and expert guidance can
substantially boost SLM reasoning.

</details>


### [420] [No Free Lunch: Rethinking Internal Feedback for LLM Reasoning](https://arxiv.org/abs/2506.17219)
*Yanzhi Zhang,Zhaoxi Zhang,Haoxiang Guan,Yilin Cheng,Yitong Duan,Chen Wang,Yue Wang,Shuxin Zheng,Jiyan He*

Main category: cs.LG

TL;DR: This paper investigates the use of Reinforcement Learning from Internal Feedback (RLIF) for improving reasoning in large language models (LLMs). It shows RLIF can initially boost performance but declines over time, with limited benefits for already instruction-tuned models.


<details>
  <summary>Details</summary>
Motivation: The authors aim to explore an alternative to external supervision approaches like RLHF, focusing on intrinsic, model-derived signals (RLIF) to improve LLM reasoning.

Method: They introduce RLIF strategies based on unsupervised reward proxies such as token-level entropy, trajectory-level entropy, and self-certainty, along with theoretical and empirical evaluations.

Result: RLIF initially boosts reasoning performance of LLMs and matches or surpasses RLVR methods, but performance declines with training progression. For instruction-tuned models, RLIF shows little improvement.

Conclusion: RLIF has potential for early-stage training of LLMs but faces limitations as training progresses or when applied to instruction-tuned models. Insights are provided for integrating internal feedback into LLM training effectively.

Abstract: Reinforcement learning has emerged as a powerful paradigm for post-training
large language models (LLMs) to improve reasoning. Approaches like
Reinforcement Learning from Human Feedback (RLHF) and Reinforcement Learning
with Verifiable Rewards (RLVR) have shown strong results, but they require
extensive external supervision. We investigate an alternative class of methods,
Reinforcement Learning from Internal Feedback (RLIF), which relies solely on
intrinsic model-derived signals instead of external rewards. In particular, we
leverage unsupervised reward proxies such as token-level entropy,
trajectory-level entropy, and self-certainty. Our theoretical analysis shows
these internal objectives are partially equivalent, and we empirically evaluate
various RLIF strategies on challenging math reasoning benchmarks. Experimental
results demonstrate that RLIF can boost the reasoning performance of base LLMs
at the beginning phase of the training, matching or surpassing RLVR techniques
on these tasks. However, when training progresses, performance degrades even
below the model before training. Moreover, we find that RLIF yields little
improvement for instruction-tuned models, indicating diminishing returns of
intrinsic feedback once an LLM is already instruction-tuned. We further analyze
this limitation by mixing model weights and explain the reason of RLIF's
training behaviors, providing practical guidelines for integrating internal
feedback signals into LLM training. We hope our analysis of internal feedback
will inform more principled and effective strategies for LLM post-training.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [421] [A Study of Hybrid and Evolutionary Metaheuristics for Single Hidden Layer Feedforward Neural Network Architecture](https://arxiv.org/abs/2506.15737)
*Gautam Siddharth Kashyap,Md Tabrez Nafis,Samar Wazir*

Main category: cs.NE

TL;DR: The study presents a hybrid PSO-SGD strategy to enhance ANN training efficiency and accuracy, significantly outperforming conventional methods like GA and PSO.


<details>
  <summary>Details</summary>
Motivation: Address challenges in training ANNs with SGD, including high computational costs and local optima convergence, through exploration of alternative MHOs.

Method: Develop a hybrid optimization strategy combining PSO and SGD to leverage local search efficiency and performance benefits of population-based optimization.

Result: The hybrid PSO-SGD dramatically reduces median training MSE (by 90%-95%) and outperforms GA, PSO, RMHC, and RS in several network configurations.

Conclusion: Hybrid evolutionary optimization approaches like PSO-SGD enhance ANN training effectiveness and validate evolutionary search principles like the Building Block Hypothesis.

Abstract: Training Artificial Neural Networks (ANNs) with Stochastic Gradient Descent
(SGD) frequently encounters difficulties, including substantial computing
expense and the risk of converging to local optima, attributable to its
dependence on partial weight gradients. Therefore, this work investigates
Particle Swarm Optimization (PSO) and Genetic Algorithms (GAs) - two
population-based Metaheuristic Optimizers (MHOs) - as alternatives to SGD to
mitigate these constraints. A hybrid PSO-SGD strategy is developed to improve
local search efficiency. The findings indicate that the hybrid PSO-SGD
technique decreases the median training MSE by 90 to 95 percent relative to
conventional GA and PSO across various network sizes (e.g., from around 0.02 to
approximately 0.001 in the Sphere function). RMHC attains substantial
enhancements, reducing MSE by roughly 85 to 90 percent compared to GA.
Simultaneously, RS consistently exhibits errors exceeding 0.3, signifying
subpar performance. These findings underscore that hybrid and evolutionary
procedures significantly improve training efficiency and accuracy compared to
conventional optimization methods and imply that the Building Block Hypothesis
(BBH) may still be valid, indicating that advantageous weight structures are
retained during evolutionary search.

</details>


### [422] [Neural Cellular Automata for ARC-AGI](https://arxiv.org/abs/2506.15746)
*Kevin Xu,Risto Miikkulainen*

Main category: cs.NE

TL;DR: The paper investigates the use of Neural Cellular Automata (NCA) for solving tasks in the Abstraction and Reasoning Corpus for Artificial General Intelligence (ARC-AGI), showing promising results for abstract grid-based problems.


<details>
  <summary>Details</summary>
Motivation: To explore the ability of Neural Cellular Automata (NCA) to handle complex and abstract few-shot tasks such as those in the ARC-AGI domain, which tests for broader self-organizing system capabilities.

Method: Gradient-based training was employed to teach NCAs iterative update rules that could generalize transformations from training to test grids in the ARC tasks.

Result: The findings indicate that gradient-trained NCA models perform well and efficiently on ARC tasks, handling abstract grid-based reasoning with design and training adaptability.

Conclusion: NCAs are presented as a viable and efficient solution for abstract reasoning tasks in ARC, with insights extending to their general utility in self-organizing systems.

Abstract: Cellular automata and their differentiable counterparts, Neural Cellular
Automata (NCA), are highly expressive and capable of surprisingly complex
behaviors. This paper explores how NCAs perform when applied to tasks requiring
precise transformations and few-shot generalization, using the Abstraction and
Reasoning Corpus for Artificial General Intelligence (ARC-AGI) as a domain that
challenges their capabilities in ways not previously explored. Specifically,
this paper uses gradient-based training to learn iterative update rules that
transform input grids into their outputs from the training examples and apply
them to the test inputs. Results suggest that gradient-trained NCA models are a
promising and efficient approach to a range of abstract grid-based tasks from
ARC. Along with discussing the impacts of various design modifications and
training constraints, this work examines the behavior and properties of NCAs
applied to ARC to give insights for broader applications of self-organizing
systems.

</details>


### [423] [Robust Dynamic Material Handling via Adaptive Constrained Evolutionary Reinforcement Learning](https://arxiv.org/abs/2506.16795)
*Chengpeng Hu,Ziming Wang,Bo Yuan,Jialin Liu,Chengqi Zhang,Xin Yao*

Main category: cs.NE

TL;DR: The paper proposes a novel approach called ACERL to handle dynamically arriving material transport tasks by focusing on sparse rewards, constraint satisfaction, and adaptability, demonstrating performance and robustness across training and test instances.


<details>
  <summary>Details</summary>
Motivation: Dynamic material handling often deals with real-time assignment of transport tasks with constraints like delay, requiring adaptability, computational efficiency, and robust policies trained on historical records.

Method: The proposed method, ACERL, employs reinforcement learning with a population of actors for exploration, addressing sparse rewards and constraint violations. It adaptively selects beneficial training instances to improve the learned policies.

Result: Extensive experiments on eight training and test instances show ACERL's superior performance in satisfying task constraints compared with other algorithms. Robustness is confirmed through tests on 40 unseen noisy instances and cross-validation.

Conclusion: ACERL offers a reliable and effective solution to dynamic material handling challenges, leveraging adaptability, diverse exploration, and constraint handling with proven robustness across varied scenarios.

Abstract: Dynamic material handling (DMH) involves the assignment of dynamically
arriving material transporting tasks to suitable vehicles in real time for
minimising makespan and tardiness. In real-world scenarios, historical task
records are usually available, which enables the training of a decision policy
on multiple instances consisting of historical records. Recently, reinforcement
learning has been applied to solve DMH. Due to the occurrence of dynamic events
such as new tasks, adaptability is highly required. Solving DMH is challenging
since constraints including task delay should be satisfied. A feedback is
received only when all tasks are served, which leads to sparse reward. Besides,
making the best use of limited computational resources and historical records
for training a robust policy is crucial. The time allocated to different
problem instances would highly impact the learning process. To tackle those
challenges, this paper proposes a novel adaptive constrained evolutionary
reinforcement learning (ACERL) approach, which maintains a population of actors
for diverse exploration. ACERL accesses each actor for tackling sparse rewards
and constraint violation to restrict the behaviour of the policy. Moreover,
ACERL adaptively selects the most beneficial training instances for improving
the policy. Extensive experiments on eight training and eight unseen test
instances demonstrate the outstanding performance of ACERL compared with
several state-of-the-art algorithms. Policies trained by ACERL can schedule the
vehicles while fully satisfying the constraints. Additional experiments on 40
unseen noised instances show the robust performance of ACERL. Cross-validation
further presents the overall effectiveness of ACREL. Besides, a rigorous
ablation study highlights the coordination and benefits of each ingredient of
ACERL.

</details>


### [424] [Continual Learning with Columnar Spiking Neural Networks](https://arxiv.org/abs/2506.17169)
*Denis Larionov,Nikolay Bazenkov,Mikhail Kiselev*

Main category: cs.NE

TL;DR: The paper proposes CoLaNET, a columnar-organized spiking neural network, to address continual learning challenges with minimal catastrophic forgetting.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of balancing stability (retaining old knowledge) and plasticity (acquiring new knowledge) in continual learning scenarios.

Method: The study utilizes CoLaNET, a spiking neural network model with columnar organization, and adjusts its hyperparameters to manage stability-plasticity trade-offs during sequential tasks.

Result: CoLaNET achieved 92% accuracy across ten sequential MNIST tasks with only 4% performance degradation on the first task, demonstrating minimal catastrophic forgetting.

Conclusion: Columnar-organized SNNs like CoLaNET provide an effective framework for continual learning, achieving high task retention and low forgetting rates through optimized configurations.

Abstract: This study investigates columnar-organized spiking neural networks (SNNs) for
continual learning and catastrophic forgetting. Using CoLaNET (Columnar Layered
Network), we show that microcolumns adapt most efficiently to new tasks when
they lack shared structure with prior learning. We demonstrate how CoLaNET
hyperparameters govern the trade-off between retaining old knowledge
(stability) and acquiring new information (plasticity). Our optimal
configuration learns ten sequential MNIST tasks effectively, maintaining 92%
accuracy on each. It shows low forgetting, with only 4% performance degradation
on the first task after training on nine subsequent tasks.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [425] [How to Increase Energy Efficiency with a Single Linux Command](https://arxiv.org/abs/2506.16046)
*Alborz Jelvani,Richard P Martin,Santosh Nagarakatte*

Main category: cs.PF

TL;DR: The paper shows that simple power capping can improve energy efficiency by up to 25% on dual Intel Xeon systems, without significant performance degradation.


<details>
  <summary>Details</summary>
Motivation: Existing dynamic power management settings do not achieve optimal energy savings and need an accessible, efficient alternative.

Method: Utilized power capping mechanisms and validated their efficiency with real system measurements from SPEC CPU 2017 benchmarks over a month-long campaign.

Result: Power capping improved energy efficiency by up to 25% compared to traditional energy-saving settings, with minimal performance impact.

Conclusion: Power capping is a simple yet highly effective solution for improving energy efficiency and can surpass complex DVFS approaches, benefiting programmers and administrators.

Abstract: Processors with dynamic power management provide a variety of settings to
control energy efficiency. However, tuning these settings does not achieve
optimal energy savings. We highlight how existing power capping mechanisms can
address these limitations without requiring any changes to current power
governors. We validate this approach using system measurements across a
month-long data acquisition campaign from SPEC CPU 2017 benchmarks on a
server-class system equipped with dual Intel Xeon Scalable processors. Our
results indicate that setting a simple power cap can improve energy efficiency
by up to 25% over traditional energy-saving system configurations with little
performance loss, as most default settings focus on thermal regulation and
performance rather than compute efficiency. Power capping is very accessible
compared to other approaches, as it can be implemented with a single Linux
command. Our results point to programmers and administrators using power caps
as a primary mechanism to maintain significant energy efficiency while
retaining acceptable performance, as opposed to deploying complex DVFS
algorithms.

</details>


### [426] [Dependability of UAV-Based Networks and Computing Systems: A Survey](https://arxiv.org/abs/2506.16786)
*Qingyang Zhang,Mohammad Dwipa Furqan,Tasfia Nutzhat,Fumio Machida,Ermeson Andrade*

Main category: cs.PF

TL;DR: The paper reviews the literature on dependability challenges in UAV-based networks and computing systems, categorizing threats and technologies while identifying research gaps.


<details>
  <summary>Details</summary>
Motivation: The increasing reliance on UAV-based systems in cyber-physical applications highlights the need for dependability assurances due to uncertainties like software bugs, failures, and external disturbances.

Method: Conducted systematic literature reviews to analyze current research trends, categorize threats, and adopted technologies in UAV-based computing and networking systems.

Result: Organized findings into threat and technology categories, revealing critical aspects and research trends, and identified eight future research directions to improve dependability.

Conclusion: Dependability remains a significant challenge for UAV-based systems, and addressing the identified research gaps is essential for their reliable operation in diverse applications.

Abstract: Uncrewed Aerial Vehicle (UAV) computing and networking are becoming a
fundamental computation infrastructure for diverse cyber-physical application
systems. UAVs can be empowered by AI on edge devices and can communicate with
other UAVs and ground stations via wireless communication networks. Dynamic
computation demands and heterogeneous computing resources are distributed in
the system and need to be controlled to maintain the quality of services and to
accomplish critical missions. With the evolution of UAV-based systems,
dependability assurance of such systems emerges as a crucial challenge.
UAV-based systems confront diverse sources of uncertainty that may threaten
their dependability, such as software bugs, component failures, network
disconnections, battery shortages, and disturbances from the real world. In
this paper, we conduct systematic literature reviews on the dependability of
UAV-based networks and computing systems. The survey report reveals emerging
research trends in this field and summarizes the literature into comprehensive
categories by threat types and adopted technologies. Based on our literature
reviews, we identify eight research fields that require further exploration in
the future to achieve dependable UAV-based systems.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [427] [A System Level Compiler for Massively-Parallel, Spatial, Dataflow Architectures](https://arxiv.org/abs/2506.15875)
*Dirk Van Essendelft,Patrick Wingo,Terry Jordan,Ryan Smith,Wissam Saidi*

Main category: cs.PL

TL;DR: The paper introduces MACH, a versatile compiler for advanced parallel architectures, demonstrated with tensor examples using NumPy on Cerebras hardware.


<details>
  <summary>Details</summary>
Motivation: The motivation is to simplify the process of compiling codes for complex, massively-parallel spatial architectures such as the Wafer Scale Engine, while also supporting traditional architectures.

Method: MACH employs a Virtual Machine concept, a domain-specific language, and a compiler that translates high-level languages to architecture-specific code, providing flexibility for data mappings and execution.

Result: MACH is demonstrated using dense tensor computations in NumPy, successfully lowering them to run on the Wafer Scale Engine using Cerebras-specific languages.

Conclusion: MACH proves effective in bridging the gap between high-level language representations and hardware-specific implementations for varied advanced architectures, particularly spatial ones.

Abstract: We have developed a novel compiler called the Multiple-Architecture Compiler
for Advanced Computing Hardware (MACH) designed specifically for
massively-parallel, spatial, dataflow architectures like the Wafer Scale
Engine. Additionally, MACH can execute code on traditional unified-memory
devices. MACH addresses the complexities in compiling for spatial architectures
through a conceptual Virtual Machine, a flexible domain-specific language, and
a compiler that can lower high-level languages to machine-specific code in
compliance with the Virtual Machine concept. While MACH is designed to be
operable on several architectures and provide the flexibility for several
standard and user-defined data mappings, we introduce the concept with dense
tensor examples from NumPy and show lowering to the Wafer Scale Engine by
targeting Cerebras' hardware specific languages.

</details>


### [428] [WAMI: Compilation to WebAssembly through MLIR without Losing Abstraction](https://arxiv.org/abs/2506.16048)
*Byeongjee Kang,Harsh Desai,Limin Jia,Brandon Lucia*

Main category: cs.PL

TL;DR: The paper introduces a novel compilation pipeline for WebAssembly (Wasm) using MLIR that maintains high-level abstractions during code generation. It achieves this by presenting Wasm-specific dialects and showcases its modular extensibility with performance close to LLVM-based compilers.


<details>
  <summary>Details</summary>
Motivation: Existing Wasm compilation approaches either require redundant efforts for different languages or lose abstraction by lowering constructs into low-level representations like LLVM IR. Current MLIR-based solutions also inherit LLVM's limitations for Wasm code generation. There is a need for a better approach to integrate high-level features securely and efficiently.

Method: The proposed method introduces Wasm-specific dialects within the MLIR infrastructure, enabling direct generation of high-level Wasm code. This pipeline maintains abstraction throughout the process and showcases its extensibility using a case study based on Stack Switching.

Result: Performance evaluations with PolyBench benchmarks reveal that the new pipeline is competitive with LLVM-based compilers, having at most 7.7% slower performance, and even outperforming them in some environments.

Conclusion: The proposed compilation pipeline for Wasm is modular, extensible, and capable of efficiently generating high-level Wasm code while preserving high-level abstractions. This approach provides a significant step forward to streamline Wasm's integration of high-level features.

Abstract: WebAssembly (Wasm) is a portable bytecode format that serves as a compilation
target for high-level languages, enabling their secure and efficient execution
across diverse platforms, including web browsers and embedded systems. To
improve support for high-level languages without incurring significant code
size or performance overheads, Wasm continuously evolves by integrating
high-level features such as Garbage Collection and Stack Switching. However,
existing compilation approaches either lack reusable design -- requiring
redundant implementation efforts for each language -- or lose abstraction by
lowering high-level constructs into low-level shared representations like LLVM
IR, which hinder the adoption of high-level features. MLIR compiler
infrastructure provides the compilation pipeline with multiple levels of
abstraction, preserving high-level abstractions throughout the compilation
pipeline, yet the current MLIR pipeline relies on the LLVM backend for Wasm
code generation, thereby inheriting LLVM's limitations.
  This paper presents a novel compilation pipeline for Wasm, featuring Wasm
dialects explicitly designed to represent high-level Wasm constructs within
MLIR. Our approach enables direct generation of high-level Wasm code from
corresponding high-level MLIR dialects without losing abstraction, providing a
modular and extensible way to incorporate high-level Wasm features. We
illustrate this extensibility through a case study that leverages Stack
Switching, a recently introduced high-level feature of Wasm. Performance
evaluations on PolyBench benchmarks show that our pipeline, benefiting from
optimizations within the MLIR and Wasm ecosystems, produces code with at most
7.7\% slower, and faster in some execution environments, compared to LLVM-based
compilers.

</details>


### [429] [Low Overhead Allocation Sampling in a Garbage Collected Virtual Machine](https://arxiv.org/abs/2506.16883)
*Christoph Jung,C. F. Bolz-Tereick*

Main category: cs.PL

TL;DR: This paper introduces a low-overhead sampling allocation profiler for PyPy, a Python virtual machine, integrated into its garbage collector.


<details>
  <summary>Details</summary>
Motivation: Allocations in dynamically typed languages like Python are resource-intensive, and existing profiling methods are inefficient when tracking every single allocation. The study aims to create a low-overhead mechanism for allocation profiling.

Method: The authors implement a sampling-based allocation profiler integrated within the garbage collector of PyPy. Sampling is configured to track allocations periodically, ensuring reduced overhead.

Result: The profiler achieves a maximum time overhead of 25% at a 4 MB sampling period while maintaining efficient functionality in benchmarks.

Conclusion: Sampling-based allocation profiling can provide a balance between insight and performance, making it a more efficient alternative to time-based profiling for allocation-heavy programs.

Abstract: Compared to the more commonly used time-based profiling, allocation profiling
provides an alternate view of the execution of allocation heavy dynamically
typed languages. However, profiling every single allocation in a program is
very inefficient. We present a sampling allocation profiler that is deeply
integrated into the garbage collector of PyPy, a Python virtual machine. This
integration ensures tunable low overhead for the allocation profiler, which we
measure and quantify. Enabling allocation sampling profiling with a sampling
period of 4 MB leads to a maximum time overhead of 25% in our benchmarks, over
un-profiled regular execution.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [430] [Robust control for multi-legged elongate robots in noisy environments](https://arxiv.org/abs/2506.15788)
*Baxi Chong,Juntao He,Daniel Irvine,Tianyu Wang,Esteban Flores,Daniel Soto,Jianfeng Lin,Zhaochen Xu,Vincent R Nienhusser,Grigoriy Blekherman,Daniel I. Goldman*

Main category: cs.RO

TL;DR: The paper introduces a new control paradigm for multi-legged elongate robots (MERs) that replicates communication theory's concepts to enhance locomotion in rugged terrains with minimal reliance on sensors.


<details>
  <summary>Details</summary>
Motivation: Existing legged robots face limitations such as dependency on high-bandwidth sensors and platform-specific training, restricting their adaptability to complex terrains and across platforms.

Method: The authors draw analogies between robot-environment interactions and communication theory, using mechanical intelligence (passive responses akin to error correction) and computational intelligence (feedback akin to repeat request) to design robust MERs.

Result: The proposed system exhibited reliable motion of half a body length per cycle on complex terrains with terrain noise exceeding twice the robots' height.

Conclusion: This paradigm offers a systematic foundation for designing agile, resilient robots capable of terrain-agnostic performance, paving the way for their deployment in extreme environments.

Abstract: Modern two and four legged robots exhibit impressive mobility on complex
terrain, largely attributed to advancement in learning algorithms. However,
these systems often rely on high-bandwidth sensing and onboard computation to
perceive/respond to terrain uncertainties. Further, current locomotion
strategies typically require extensive robot-specific training, limiting their
generalizability across platforms. Building on our prior research connecting
robot-environment interaction and communication theory, we develop a new
paradigm to construct robust and simply controlled multi-legged elongate robots
(MERs) capable of operating effectively in cluttered, unstructured
environments. In this framework, each leg-ground contact is thought of as a
basic active contact (bac), akin to bits in signal transmission. Reliable
locomotion can be achieved in open-loop on "noisy" landscapes via sufficient
redundancy in bacs. In such situations, robustness is achieved through passive
mechanical responses. We term such processes as those displaying mechanical
intelligence (MI) and analogize these processes to forward error correction
(FEC) in signal transmission. To augment MI, we develop feedback control
schemes, which we refer to as computational intelligence (CI) and such
processes analogize automatic repeat request (ARQ) in signal transmission.
Integration of these analogies between locomotion and communication theory
allow analysis, design, and prediction of embodied intelligence control schemes
(integrating MI and CI) in MERs, showing effective and reliable performance
(approximately half body lengths per cycle) on complex landscapes with terrain
"noise" over twice the robot's height. Our work provides a foundation for
systematic development of MER control, paving the way for terrain-agnostic,
agile, and resilient robotic systems capable of operating in extreme
environments.

</details>


### [431] [Steering Your Diffusion Policy with Latent Space Reinforcement Learning](https://arxiv.org/abs/2506.15799)
*Andrew Wagenmaker,Mitsuhiko Nakamoto,Yunchu Zhang,Seohong Park,Waleed Yagoub,Anusha Nagabandi,Abhishek Gupta,Sergey Levine*

Main category: cs.RO

TL;DR: Behavioral cloning (BC) policies based on human demonstrations struggle to improve without costly additional data collection, while reinforcement learning (RL) demands excessive samples for adaptation. This paper proposes Diffusion Steering via Reinforcement Learning (DSRL) to efficiently adapt BC-trained diffusion policies without altering their weights.


<details>
  <summary>Details</summary>
Motivation: Current BC-trained robotic control policies often face limited initial performance in open-world settings and require additional demonstrations for improvement. RL, while autonomous, is inefficient due to high sample requirements. The goal was to enable fast, efficient real-world policy adaptation.

Method: The proposed method, Diffusion Steering via Reinforcement Learning (DSRL), leverages RL to adapt diffusion BC policies through the latent-noise space, avoiding weight modification and requiring only black-box access to the original policy.

Result: DSRL demonstrated high sample efficiency and effective policy improvement across various simulated and real-world robotic tasks, including generalist policy adaptation.

Conclusion: DSRL represents a promising solution for adapting BC-trained policies autonomously and efficiently, combining the benefits of RL and BC while addressing their inherent limitations.

Abstract: Robotic control policies learned from human demonstrations have achieved
impressive results in many real-world applications. However, in scenarios where
initial performance is not satisfactory, as is often the case in novel
open-world settings, such behavioral cloning (BC)-learned policies typically
require collecting additional human demonstrations to further improve their
behavior -- an expensive and time-consuming process. In contrast, reinforcement
learning (RL) holds the promise of enabling autonomous online policy
improvement, but often falls short of achieving this due to the large number of
samples it typically requires. In this work we take steps towards enabling fast
autonomous adaptation of BC-trained policies via efficient real-world RL.
Focusing in particular on diffusion policies -- a state-of-the-art BC
methodology -- we propose diffusion steering via reinforcement learning (DSRL):
adapting the BC policy by running RL over its latent-noise space. We show that
DSRL is highly sample efficient, requires only black-box access to the BC
policy, and enables effective real-world autonomous policy improvement.
Furthermore, DSRL avoids many of the challenges associated with finetuning
diffusion policies, obviating the need to modify the weights of the base policy
at all. We demonstrate DSRL on simulated benchmarks, real-world robotic tasks,
and for adapting pretrained generalist policies, illustrating its sample
efficiency and effective performance at real-world policy improvement.

</details>


### [432] [Context Matters! Relaxing Goals with LLMs for Feasible 3D Scene Planning](https://arxiv.org/abs/2506.15828)
*Emanuele Musumeci,Michele Brienza,Francesco Argenziano,Vincenzo Suriani,Daniele Nardi,Domenico D. Bloisi*

Main category: cs.RO

TL;DR: The paper integrates classical planning with large language models (LLMs) to improve robot adaptability and efficiency by addressing challenges in perception grounding and commonsense reasoning.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the limitations of classical planning in real-world scenarios, such as ineffective grounding of perceptions and reliance on hard-coded behaviors, and to address the unfeasibility of plans generated solely by LLM-based systems.

Method: The paper introduces a hierarchical approach that combines classical planning with LLMs, using their commonsense knowledge to ground actions and enable relaxed goal achievement in unfeasible tasks. The method leverages 3D Scene Graphs for environment modeling.

Result: The approach is evaluated qualitatively and quantitatively, demonstrating superior adaptability and task performance in complex scenarios compared to other benchmark methods.

Conclusion: The study showcases the potential of integrating classical planning with LLMs to achieve functionally equivalent goals in robotic systems, while providing resources like code and datasets to the community.

Abstract: Classical planning in AI and Robotics addresses complex tasks by shifting
from imperative to declarative approaches (e.g., PDDL). However, these methods
often fail in real scenarios due to limited robot perception and the need to
ground perceptions to planning predicates. This often results in heavily
hard-coded behaviors that struggle to adapt, even with scenarios where goals
can be achieved through relaxed planning. Meanwhile, Large Language Models
(LLMs) lead to planning systems that leverage commonsense reasoning but often
at the cost of generating unfeasible and/or unsafe plans. To address these
limitations, we present an approach integrating classical planning with LLMs,
leveraging their ability to extract commonsense knowledge and ground actions.
We propose a hierarchical formulation that enables robots to make unfeasible
tasks tractable by defining functionally equivalent goals through gradual
relaxation. This mechanism supports partial achievement of the intended
objective, suited to the agent's specific context. Our method demonstrates its
ability to adapt and execute tasks effectively within environments modeled
using 3D Scene Graphs through comprehensive qualitative and quantitative
evaluations. We also show how this method succeeds in complex scenarios where
other benchmark methods are more likely to fail. Code, dataset, and additional
material are released to the community.

</details>


### [433] [SafeMimic: Towards Safe and Autonomous Human-to-Robot Imitation for Mobile Manipulation](https://arxiv.org/abs/2506.15847)
*Arpit Bahety,Arnav Balaji,Ben Abbatematteo,Roberto Martín-Martín*

Main category: cs.RO

TL;DR: SafeMimic proposes a framework for robots to autonomously and safely learn mobile manipulation tasks from a single human demonstration video.


<details>
  <summary>Details</summary>
Motivation: The authors aim to enable robots to efficiently perform mobile manipulation tasks in home settings by learning from a single human video demonstration, overcoming challenges in semantic understanding, first-person adaptation, and safety.

Method: SafeMimic's approach involves parsing human video demonstrations into task segments, modeling them for egocentric robot use, adapting actions to the robot's morphology via safety predictions using Q-functions, and revising failed attempts through backtracking.

Result: Experiments demonstrate that SafeMimic significantly improves a robot's ability to learn multi-step mobile manipulation tasks safely and autonomously across seven tasks, outperforming existing baselines.

Conclusion: SafeMimic provides a robust solution for robots to learn manipulation tasks from human demonstrations while reducing exploration needs and ensuring safety in execution.

Abstract: For robots to become efficient helpers in the home, they must learn to
perform new mobile manipulation tasks simply by watching humans perform them.
Learning from a single video demonstration from a human is challenging as the
robot needs to first extract from the demo what needs to be done and how,
translate the strategy from a third to a first-person perspective, and then
adapt it to be successful with its own morphology. Furthermore, to mitigate the
dependency on costly human monitoring, this learning process should be
performed in a safe and autonomous manner. We present SafeMimic, a framework to
learn new mobile manipulation skills safely and autonomously from a single
third-person human video. Given an initial human video demonstration of a
multi-step mobile manipulation task, SafeMimic first parses the video into
segments, inferring both the semantic changes caused and the motions the human
executed to achieve them and translating them to an egocentric reference. Then,
it adapts the behavior to the robot's own morphology by sampling candidate
actions around the human ones, and verifying them for safety before execution
in a receding horizon fashion using an ensemble of safety Q-functions trained
in simulation. When safe forward progression is not possible, SafeMimic
backtracks to previous states and attempts a different sequence of actions,
adapting both the trajectory and the grasping modes when required for its
morphology. As a result, SafeMimic yields a strategy that succeeds in the
demonstrated behavior and learns task-specific actions that reduce exploration
in future attempts. Our experiments show that our method allows robots to
safely and efficiently learn multi-step mobile manipulation behaviors from a
single human demonstration, from different users, and in different
environments, with improvements over state-of-the-art baselines across seven
tasks

</details>


### [434] [PRISM-Loc: a Lightweight Long-range LiDAR Localization in Urban Environments with Topological Maps](https://arxiv.org/abs/2506.15849)
*Kirill Muravyev,Vasily Yuryev,Oleg Bulichev,Dmitry Yudin,Konstantin Yakovlev*

Main category: cs.RO

TL;DR: The paper introduces PRISM-Loc, a topological map-based method for effective long-range localization of robots using a two-step process combining global place recognition and local pose estimation.


<details>
  <summary>Details</summary>
Motivation: The challenge addressed is real-time localization in large environments where dense lidar maps are computationally expensive and memory-consuming.

Method: PRISM-Loc utilizes a two-phase localization process: global place recognition and a custom lidar-based scan matching algorithm for local pose estimation, optimized with 2D features and point matching.

Result: In evaluations on the ITLP-Campus dataset (3 km route), PRISM-Loc outperformed state-of-the-art methods in both accuracy and computational efficiency.

Conclusion: PRISM-Loc is a superior alternative to traditional metric or place recognition-based localization methods, offering computational and qualitative improvements.

Abstract: Localization in the environment is one of the crucial tasks of navigation of
a mobile robot or a self-driving vehicle. For long-range routes, performing
localization within a dense global lidar map in real time may be difficult, and
the creation of such a map may require much memory. To this end, leveraging
topological maps may be useful. In this work, we propose PRISM-Loc -- a
topological map-based approach for localization in large environments. The
proposed approach leverages a twofold localization pipeline, which consists of
global place recognition and estimation of the local pose inside the found
location. For local pose estimation, we introduce an original lidar scan
matching algorithm, which is based on 2D features and point-based optimization.
We evaluate the proposed method on the ITLP-Campus dataset on a 3 km route, and
compare it against the state-of-the-art metric map-based and place
recognition-based competitors. The results of the experiments show that the
proposed method outperforms its competitors both quality-wise and
computationally-wise.

</details>


### [435] [Semantic and Feature Guided Uncertainty Quantification of Visual Localization for Autonomous Vehicles](https://arxiv.org/abs/2506.15851)
*Qiyuan Wu,Mark Campbell*

Main category: cs.RO

TL;DR: This paper presents a method to quantify uncertainty in visual localization for autonomous vehicles by leveraging image feature and semantic information.


<details>
  <summary>Details</summary>
Motivation: To enhance the safety of robotics systems, particularly for self-driving cars, by improving uncertainty quantification in visual localization under various conditions.

Method: It employs a lightweight sensor error model coupled with image features and semantic data to predict 2D error distributions, demonstrated on the Ithaca365 dataset with challenging lighting and weather scenarios.

Result: The proposed method outperforms traditional Gaussian models in capturing measurement error, especially under adverse conditions where errors do not align with Gaussian distributions.

Conclusion: The approach effectively provides context-aware uncertainty quantification, enhancing localization accuracy and safety for autonomous vehicles.

Abstract: The uncertainty quantification of sensor measurements coupled with deep
learning networks is crucial for many robotics systems, especially for
safety-critical applications such as self-driving cars. This paper develops an
uncertainty quantification approach in the context of visual localization for
autonomous driving, where locations are selected based on images. Key to our
approach is to learn the measurement uncertainty using light-weight sensor
error model, which maps both image feature and semantic information to
2-dimensional error distribution. Our approach enables uncertainty estimation
conditioned on the specific context of the matched image pair, implicitly
capturing other critical, unannotated factors (e.g., city vs highway, dynamic
vs static scenes, winter vs summer) in a latent manner. We demonstrate the
accuracy of our uncertainty prediction framework using the Ithaca365 dataset,
which includes variations in lighting and weather (sunny, night, snowy). Both
the uncertainty quantification of the sensor+network is evaluated, along with
Bayesian localization filters using unique sensor gating method. Results show
that the measurement error does not follow a Gaussian distribution with poor
weather and lighting conditions, and is better predicted by our Gaussian
Mixture model.

</details>


### [436] [Improving Robotic Manipulation: Techniques for Object Pose Estimation, Accommodating Positional Uncertainty, and Disassembly Tasks from Examples](https://arxiv.org/abs/2506.15865)
*Viral Rasik Galaiya*

Main category: cs.RO

TL;DR: This paper investigates using tactile sensing to improve robotic grasping and manipulation in unstructured environments.


<details>
  <summary>Details</summary>
Motivation: To enable robots to better adapt in unstructured environments by addressing limitations of camera-based sensing such as occlusion and variability.

Method: The study utilizes tactile sensing for pose determination, combines reinforcement learning with tactile collisions for more effective object grasping, and employs human-example pretraining for faster trajectory calculation in restricted spaces.

Result: Tactile sensing enhances the robot’s ability to determine object pose, reduces attempts to grasp an object despite camera uncertainty, and shortens training time for constrained object removal.

Conclusion: Integrating tactile sensors with reinforcement learning improves adaptability and efficiency of robots in environments previously challenging due to sensing limitations.

Abstract: To use robots in more unstructured environments, we have to accommodate for
more complexities. Robotic systems need more awareness of the environment to
adapt to uncertainty and variability. Although cameras have been predominantly
used in robotic tasks, the limitations that come with them, such as occlusion,
visibility and breadth of information, have diverted some focus to tactile
sensing. In this thesis, we explore the use of tactile sensing to determine the
pose of the object using the temporal features. We then use reinforcement
learning with tactile collisions to reduce the number of attempts required to
grasp an object resulting from positional uncertainty from camera estimates.
Finally, we use information provided by these tactile sensors to a
reinforcement learning agent to determine the trajectory to take to remove an
object from a restricted passage while reducing training time by pertaining
from human examples.

</details>


### [437] [CooperRisk: A Driving Risk Quantification Pipeline with Multi-Agent Cooperative Perception and Prediction](https://arxiv.org/abs/2506.15868)
*Mingyue Lei,Zewei Zhou,Hongchen Li,Jia Hu,Jiaqi Ma*

Main category: cs.RO

TL;DR: The paper presents CooperRisk, a V2X-enabled risk quantification system to assess driving risks using a learning-based cooperative prediction model and temporal risk maps.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of limited perception range and occlusion in single-vehicle autonomous systems, proposing to enhance risk interpretability using Vehicle-to-everything (V2X) data.

Method: CooperRisk fuses multi-agent perception data, uses a transformer-based cooperative prediction model, and generates interpretable scenario risk maps to guide planning.

Result: The approach demonstrates superior performance, achieving a 44.35% decrease in conflict rate between the ego vehicle and background traffic in evaluations using the V2XPnP dataset.

Conclusion: CooperRisk effectively quantifies multi-agent driving risk scenarios, improves safety and risk interpretability, and mitigates conflicts for autonomous driving using V2X data.

Abstract: Risk quantification is a critical component of safe autonomous driving,
however, constrained by the limited perception range and occlusion of
single-vehicle systems in complex and dense scenarios. Vehicle-to-everything
(V2X) paradigm has been a promising solution to sharing complementary
perception information, nevertheless, how to ensure the risk interpretability
while understanding multi-agent interaction with V2X remains an open question.
In this paper, we introduce the first V2X-enabled risk quantification pipeline,
CooperRisk, to fuse perception information from multiple agents and quantify
the scenario driving risk in future multiple timestamps. The risk is
represented as a scenario risk map to ensure interpretability based on risk
severity and exposure, and the multi-agent interaction is captured by the
learning-based cooperative prediction model. We carefully design a
risk-oriented transformer-based prediction model with multi-modality and
multi-agent considerations. It aims to ensure scene-consistent future behaviors
of multiple agents and avoid conflicting predictions that could lead to overly
conservative risk quantification and cause the ego vehicle to become overly
hesitant to drive. Then, the temporal risk maps could serve to guide a model
predictive control planner. We evaluate the CooperRisk pipeline in a real-world
V2X dataset V2XPnP, and the experiments demonstrate its superior performance in
risk quantification, showing a 44.35% decrease in conflict rate between the ego
vehicle and background traffic participants.

</details>


### [438] [A Small-Scale Robot for Autonomous Driving: Design, Challenges, and Best Practices](https://arxiv.org/abs/2506.15870)
*Hossein Maghsoumi,Yaser Fallah*

Main category: cs.RO

TL;DR: The paper explores the design, integration, challenges, and improvement guidelines for one-sixth-scale autonomous vehicle platforms.


<details>
  <summary>Details</summary>
Motivation: To address the limited representation of specific configurations in small-scale autonomous vehicle platforms and expand awareness of their potential.

Method: The study provides an overview of the design, integration, challenges, and solutions specific to one-sixth-scale autonomous vehicles.

Result: Guidelines for improving the reliability and performance of small-scale autonomous vehicle systems are proposed.

Conclusion: Small-scale autonomous vehicle platforms show potential for advancing autonomous driving algorithms and merit further research attention.

Abstract: Small-scale autonomous vehicle platforms provide a cost-effective environment
for developing and testing advanced driving systems. However, specific
configurations within this scale are underrepresented, limiting full awareness
of their potential. This paper focuses on a one-sixth-scale setup, offering a
high-level overview of its design, hardware and software integration, and
typical challenges encountered during development. We discuss methods for
addressing mechanical and electronic issues common to this scale and propose
guidelines for improving reliability and performance. By sharing these
insights, we aim to expand the utility of small-scale vehicles for testing
autonomous driving algorithms and to encourage further research in this domain.

</details>


### [439] [Challenges and Research Directions from the Operational Use of a Machine Learning Damage Assessment System via Small Uncrewed Aerial Systems at Hurricanes Debby and Helene](https://arxiv.org/abs/2506.15890)
*Thomas Manzini,Priyankari Perali,Robin R. Murphy,David Merrick*

Main category: cs.RO

TL;DR: This paper identifies four challenges in using sUAS-based ML for disaster damage assessments during Hurricanes Debby and Helene, and offers three research recommendations for improving future deployments.


<details>
  <summary>Details</summary>
Motivation: The motivation is to evaluate and understand the operational challenges of using sUAS-based ML systems in real-world disaster response scenarios, as this appears to be the first such system deployed in practice.

Method: The research involved the deployment of ML damage assessment models on imagery collected by sUAS during Hurricanes Debby and Helene, as well as on traditional crewed aerial imagery, to understand their performance and limitations.

Result: The study identified four key challenges: spatial resolution variations, spatial misalignment of imagery, reliance on wireless connectivity, and format issues with data products, which impacted the effectiveness of ML systems.

Conclusion: The findings highlight specific areas of research needed: improving ML models to handle diverse spatial resolutions, mitigating spatial misalignments, and reducing dependencies on wireless connectivity to enhance future sUAS disaster response capabilities.

Abstract: This paper details four principal challenges encountered with machine
learning (ML) damage assessment using small uncrewed aerial systems (sUAS) at
Hurricanes Debby and Helene that prevented, degraded, or delayed the delivery
of data products during operations and suggests three research directions for
future real-world deployments. The presence of these challenges is not
surprising given that a review of the literature considering both datasets and
proposed ML models suggests this is the first sUAS-based ML system for disaster
damage assessment actually deployed as a part of real-world operations. The
sUAS-based ML system was applied by the State of Florida to Hurricanes Helene
(2 orthomosaics, 3.0 gigapixels collected over 2 sorties by a Wintra WingtraOne
sUAS) and Debby (1 orthomosaic, 0.59 gigapixels collected via 1 sortie by a
Wintra WingtraOne sUAS) in Florida. The same model was applied to crewed aerial
imagery of inland flood damage resulting from post-tropical remnants of
Hurricane Debby in Pennsylvania (436 orthophotos, 136.5 gigapixels), providing
further insights into the advantages and limitations of sUAS for disaster
response. The four challenges (variationin spatial resolution of input imagery,
spatial misalignment between imagery and geospatial data, wireless
connectivity, and data product format) lead to three recommendations that
specify research needed to improve ML model capabilities to accommodate the
wide variation of potential spatial resolutions used in practice, handle
spatial misalignment, and minimize the dependency on wireless connectivity.
These recommendations are expected to improve the effective operational use of
sUAS and sUAS-based ML damage assessment systems for disaster response.

</details>


### [440] [Advancing Autonomous Racing: A Comprehensive Survey of the RoboRacer (F1TENTH) Platform](https://arxiv.org/abs/2506.15899)
*Israel Charles,Hossein Maghsoumi,Yaser Fallah*

Main category: cs.RO

TL;DR: The paper surveys the RoboRacer platform, emphasizing its contributions to autonomous driving research in terms of hardware/software modularity, education, competitions, and advancing Sim2Real methodologies.


<details>
  <summary>Details</summary>
Motivation: The study aims to position RoboRacer as an essential tool bridging the gap between theoretical research and real-world applications in autonomous driving and robotics.

Method: A comprehensive survey was conducted on RoboRacer's hardware/software architecture, its application in education and simulation environments, and its use in competitions and research.

Result: Findings emphasized advancements in perception, planning, control algorithms, Sim2Real integration, and insights from collaborative research using the RoboRacer platform.

Conclusion: RoboRacer is established as a critical, scalable platform for driving innovation in autonomous driving, bridging simulation and reality, and fostering global collaborative research.

Abstract: The RoboRacer (F1TENTH) platform has emerged as a leading testbed for
advancing autonomous driving research, offering a scalable, cost-effective, and
community-driven environment for experimentation. This paper presents a
comprehensive survey of the platform, analyzing its modular hardware and
software architecture, diverse research applications, and role in autonomous
systems education. We examine critical aspects such as bridging the
simulation-to-reality (Sim2Real) gap, integration with simulation environments,
and the availability of standardized datasets and benchmarks. Furthermore, the
survey highlights advancements in perception, planning, and control algorithms,
as well as insights from global competitions and collaborative research
efforts. By consolidating these contributions, this study positions RoboRacer
as a versatile framework for accelerating innovation and bridging the gap
between theoretical research and real-world deployment. The findings underscore
the platform's significance in driving forward developments in autonomous
racing and robotics.

</details>


### [441] [Learning from Planned Data to Improve Robotic Pick-and-Place Planning Efficiency](https://arxiv.org/abs/2506.15920)
*Liang Qin,Weiwei Wan,Jun Takahashi,Ryo Negishi,Masaki Matsushita,Kensuke Harada*

Main category: cs.RO

TL;DR: The paper introduces a learning method to accelerate robotic pick-and-place tasks by predicting shared grasps using an Energy-Based Model (EBM).


<details>
  <summary>Details</summary>
Motivation: Traditional methods for solving shared grasps in pick-and-place tasks are computationally expensive as they evaluate each candidate separately.

Method: The proposed method uses an Energy-Based Model that combines the energies of feasible grasp poses in both the initial and goal configurations to predict shared grasps, reducing the search space.

Result: The approach improves grasp selection performance, increases data efficiency, and generalizes to unseen grasps and objects with similar shapes.

Conclusion: The method provides an efficient and generalized solution for shared grasp prediction, making robotic pick-and-place planning faster and more effective.

Abstract: This work proposes a learning method to accelerate robotic pick-and-place
planning by predicting shared grasps. Shared grasps are defined as grasp poses
feasible to both the initial and goal object configurations in a pick-and-place
task. Traditional analytical methods for solving shared grasps evaluate grasp
candidates separately, leading to substantial computational overhead as the
candidate set grows. To overcome the limitation, we introduce an Energy-Based
Model (EBM) that predicts shared grasps by combining the energies of feasible
grasps at both object poses. This formulation enables early identification of
promising candidates and significantly reduces the search space. Experiments
show that our method improves grasp selection performance, offers higher data
efficiency, and generalizes well to unseen grasps and similarly shaped objects.

</details>


### [442] [KARL: Kalman-Filter Assisted Reinforcement Learner for Dynamic Object Tracking and Grasping](https://arxiv.org/abs/2506.15945)
*Kowndinya Boyalakuntla,Abdeslam Boularias,Jingjin Yu*

Main category: cs.RO

TL;DR: The paper introduces KARL, a system combining Kalman filters and reinforcement learning for object tracking and grasping in dynamic conditions, achieving superior performance over prior methods.


<details>
  <summary>Details</summary>
Motivation: Existing eye-on-hand robotic systems struggle with dynamic object tracking and grasping in unpredictable environments. The paper aims to address these challenges by improving motion range, pose estimation, and recovery from execution failures.

Method: KARL employs a six-stage RL curriculum to double motion range, integrates a Kalman filter for 6D pose estimation during challenging scenarios, and adds mechanisms for retrying failed actions in its reinforcement learning framework.

Result: KARL demonstrates higher grasp success rates and faster robot execution in both simulated and real-world tests, outperforming previous systems.

Conclusion: This approach significantly enhances the capabilities of robotic systems in dynamic environments, increasing system reliability and efficiency. The code is open-source for further exploration and implementation.

Abstract: We present Kalman-filter Assisted Reinforcement Learner (KARL) for dynamic
object tracking and grasping over eye-on-hand (EoH) systems, significantly
expanding such systems capabilities in challenging, realistic environments. In
comparison to the previous state-of-the-art, KARL (1) incorporates a novel
six-stage RL curriculum that doubles the system's motion range, thereby greatly
enhancing the system's grasping performance, (2) integrates a robust Kalman
filter layer between the perception and reinforcement learning (RL) control
modules, enabling the system to maintain an uncertain but continuous 6D pose
estimate even when the target object temporarily exits the camera's
field-of-view or undergoes rapid, unpredictable motion, and (3) introduces
mechanisms to allow retries to gracefully recover from unavoidable policy
execution failures. Extensive evaluations conducted in both simulation and
real-world experiments qualitatively and quantitatively corroborate KARL's
advantage over earlier systems, achieving higher grasp success rates and faster
robot execution speed. Source code and supplementary materials for KARL will be
made available at: https://github.com/arc-l/karl.

</details>


### [443] [ViTacFormer: Learning Cross-Modal Representation for Visuo-Tactile Dexterous Manipulation](https://arxiv.org/abs/2506.15953)
*Liang Heng,Haoran Geng,Kaifeng Zhang,Pieter Abbeel,Jitendra Malik*

Main category: cs.RO

TL;DR: ViTacFormer integrates vision and tactile sensing for robotic dexterous manipulation, outperforming prior methods by combining cross-attention and predictive models.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of enabling robots to perform human-like dexterous manipulation, particularly in scenarios requiring fine-grained control with tactile feedback.

Method: ViTacFormer combines a cross-attention encoder for fusing visual and tactile sensory data with an autoregressive model for predicting tactile feedback. A curriculum training approach refines the model's accuracy and robustness.

Result: ViTacFormer achieves approximately 50% higher success rates than prior methods and autonomously completes complex dexterous manipulation tasks involving up to 11 sequential steps.

Conclusion: This work demonstrates the feasibility and advantages of integrating vision-tactile fusion models for long-horizon, precise robotic manipulation, setting a new benchmark in the field.

Abstract: Dexterous manipulation is a cornerstone capability for robotic systems aiming
to interact with the physical world in a human-like manner. Although
vision-based methods have advanced rapidly, tactile sensing remains crucial for
fine-grained control, particularly in unstructured or visually occluded
settings. We present ViTacFormer, a representation-learning approach that
couples a cross-attention encoder to fuse high-resolution vision and touch with
an autoregressive tactile prediction head that anticipates future contact
signals. Building on this architecture, we devise an easy-to-challenging
curriculum that steadily refines the visual-tactile latent space, boosting both
accuracy and robustness. The learned cross-modal representation drives
imitation learning for multi-fingered hands, enabling precise and adaptive
manipulation. Across a suite of challenging real-world benchmarks, our method
achieves approximately 50% higher success rates than prior state-of-the-art
systems. To our knowledge, it is also the first to autonomously complete
long-horizon dexterous manipulation tasks that demand highly precise control
with an anthropomorphic hand, successfully executing up to 11 sequential stages
and sustaining continuous operation for 2.5 minutes.

</details>


### [444] [A Low-Cost Portable Lidar-based Mobile Mapping System on an Android Smartphone](https://arxiv.org/abs/2506.15983)
*Jianzhu Huai,Yuxin Shao,Yujia Zhang,Alper Yilmaz*

Main category: cs.RO

TL;DR: The paper introduces a low-cost, portable mobile mapping system using a lidar unit, Android smartphone, and RTK-GNSS stick, under 2,000 USD, optimized for tracking and mapping.


<details>
  <summary>Details</summary>
Motivation: The demand for affordable and portable mapping systems has increased due to advancements in metaverse, digital twins, and robotics, and current solutions either cost too much or lack accuracy and range.

Method: The authors developed a portable mapping system integrating multiple sensors—lidar, IMU, wide-angle camera, and GNSS—with an Android-based lidar-inertial odometry framework.

Result: The proposed system achieves a balance between affordability (under 2,000 USD), portability (1 kg weight), and functionality, and its design and software are made open source.

Conclusion: The presented system offers a promising, accessible solution for reality capture and mapping, and contributes to the community through open-source availability for further innovation.

Abstract: The rapid advancement of the metaverse, digital twins, and robotics
underscores the demand for low-cost, portable mapping systems for reality
capture. Current mobile solutions, such as the Leica BLK2Go and lidar-equipped
smartphones, either come at a high cost or are limited in range and accuracy.
Leveraging the proliferation and technological evolution of mobile devices
alongside recent advancements in lidar technology, we introduce a novel,
low-cost, portable mobile mapping system. Our system integrates a lidar unit,
an Android smartphone, and an RTK-GNSS stick. Running on the Android platform,
it features lidar-inertial odometry built with the NDK, and logs data from the
lidar, wide-angle camera, IMU, and GNSS. With a total bill of materials (BOM)
cost under 2,000 USD and a weight of about 1 kilogram, the system achieves a
good balance between affordability and portability. We detail the system
design, multisensor calibration, synchronization, and evaluate its performance
for tracking and mapping. To further contribute to the community, the system's
design and software are made open source at:
https://github.com/OSUPCVLab/marslogger_android/releases/tag/v2.1

</details>


### [445] [DualTHOR: A Dual-Arm Humanoid Simulation Platform for Contingency-Aware Planning](https://arxiv.org/abs/2506.16012)
*Boyu Li,Siyuan He,Hang Xu,Haoqi Yuan,Yu Zang,Liwei Hu,Junpeng Yue,Zhenxiong Jiang,Pengbo Hu,Börje F. Karlsson,Yehui Tang,Zongqing Lu*

Main category: cs.RO

TL;DR: The paper introduces DualTHOR, a physics-based simulation platform designed for dual-arm humanoid robots, addressing gaps in real-world transferability and robustness of Vision Language Models (VLMs).


<details>
  <summary>Details</summary>
Motivation: Developing embodied agents that can perform interactive tasks in real-world scenarios is challenging due to limitations in current simulation platforms regarding robot physics and task complexity.

Method: The authors propose DualTHOR, built upon AI2-THOR, incorporating dual-arm robot assets, a task suite, inverse kinematics solvers, and a contingency mechanism to simulate real-world failures.

Result: The simulator revealed challenges for current VLMs, including poor dual-arm coordination and limited robustness in contingency-filled, realistic environments.

Conclusion: DualTHOR improves the development and evaluation of VLMs for embodied AI tasks, emphasizing the need for physics-driven simulations for enhanced real-world applicability.

Abstract: Developing embodied agents capable of performing complex interactive tasks in
real-world scenarios remains a fundamental challenge in embodied AI. Although
recent advances in simulation platforms have greatly enhanced task diversity to
train embodied Vision Language Models (VLMs), most platforms rely on simplified
robot morphologies and bypass the stochastic nature of low-level execution,
which limits their transferability to real-world robots. To address these
issues, we present a physics-based simulation platform DualTHOR for complex
dual-arm humanoid robots, built upon an extended version of AI2-THOR. Our
simulator includes real-world robot assets, a task suite for dual-arm
collaboration, and inverse kinematics solvers for humanoid robots. We also
introduce a contingency mechanism that incorporates potential failures through
physics-based low-level execution, bridging the gap to real-world scenarios.
Our simulator enables a more comprehensive evaluation of the robustness and
generalization of VLMs in household environments. Extensive evaluations reveal
that current VLMs struggle with dual-arm coordination and exhibit limited
robustness in realistic environments with contingencies, highlighting the
importance of using our simulator to develop more capable VLMs for embodied
tasks. The code is available at https://github.com/ds199895/DualTHOR.git.

</details>


### [446] [Noise Fusion-based Distillation Learning for Anomaly Detection in Complex Industrial Environments](https://arxiv.org/abs/2506.16050)
*Jiawen Yu,Jieji Ren,Yang Chang,Qiaojun Yu,Xuan Tong,Boyang Wang,Yan Song,You Li,Xinji Mai,Wenqiang Zhang*

Main category: cs.RO

TL;DR: This paper introduces HetNet, a novel method for anomaly detection and localization in challenging industrial environments. Designed to tackle varying views, poses, and illumination, HetNet improves upon existing detection methods significantly across benchmarks.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenge of detecting workpiece defects in industrial settings characterized by complex, unstructured environments and variable imaging conditions, which hinder the performance of traditional automated defect detection systems.

Method: The proposed method utilizes a collaborative distillation heterogeneous teacher network (HetNet), an adaptive local-global feature fusion module, and a local multivariate Gaussian noise generation module to model complex feature distributions while handling minimal disruptive changes.

Result: HetNet achieves approximately 10% improvement across all evaluation metrics on MSC-AD benchmarks. It outperforms state-of-the-art methods on other datasets and successfully demonstrates robustness to environmental fluctuations.

Conclusion: HetNet enhances the reliability and effectiveness of industrial anomaly detection systems in diverse scenarios, proving to be a robust and real-time solution that can be integrated into production environments.

Abstract: Anomaly detection and localization in automated industrial manufacturing can
significantly enhance production efficiency and product quality. Existing
methods are capable of detecting surface defects in pre-defined or controlled
imaging environments. However, accurately detecting workpiece defects in
complex and unstructured industrial environments with varying views, poses and
illumination remains challenging. We propose a novel anomaly detection and
localization method specifically designed to handle inputs with perturbative
patterns. Our approach introduces a new framework based on a collaborative
distillation heterogeneous teacher network (HetNet), an adaptive local-global
feature fusion module, and a local multivariate Gaussian noise generation
module. HetNet can learn to model the complex feature distribution of normal
patterns using limited information about local disruptive changes. We conducted
extensive experiments on mainstream benchmarks. HetNet demonstrates superior
performance with approximately 10% improvement across all evaluation metrics on
MSC-AD under industrial conditions, while achieving state-of-the-art results on
other datasets, validating its resilience to environmental fluctuations and its
capability to enhance the reliability of industrial anomaly detection systems
across diverse scenarios. Tests in real-world environments further confirm that
HetNet can be effectively integrated into production lines to achieve robust
and real-time anomaly detection. Codes, images and videos are published on the
project website at: https://zihuatanejoyu.github.io/HetNet/

</details>


### [447] [Investigating Lagrangian Neural Networks for Infinite Horizon Planning in Quadrupedal Locomotion](https://arxiv.org/abs/2506.16079)
*Prakrut Kotecha,Aditya Shirwatkar,Shishir Kolathaya*

Main category: cs.RO

TL;DR: The paper explores the use of Lagrangian Neural Networks (LNNs) for accurate and stable infinite horizon locomotion planning in quadrupeds, showcasing improvements in prediction accuracy, sample efficiency, and real-time control performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address challenges in dynamics models, such as compounding errors in long-horizon predictions, and establish sustainable and accurate locomotion planning in quadrupedal robots.

Method: The study evaluates LNNs using four dynamics models: full-order forward dynamics with diagonalized representation, inverse dynamics to forward dynamics training, and torso CoM dynamics as reduced-order modeling. Experiments compare performances in various setups.

Result: Experiments indicate LNNs improve sample efficiency by 10x and achieve 2-10x better prediction accuracy than baseline methods. Additionally, the diagonalized LNNs support real-time receding horizon control with reduced computational complexity.

Conclusion: LNNs effectively capture the system dynamics structure, enabling enhanced locomotion planning and control in quadrupeds. The proposed methodology offers real-world deployment potential with improved control frequency.

Abstract: Lagrangian Neural Networks (LNNs) present a principled and interpretable
framework for learning the system dynamics by utilizing inductive biases. While
traditional dynamics models struggle with compounding errors over long
horizons, LNNs intrinsically preserve the physical laws governing any system,
enabling accurate and stable predictions essential for sustainable locomotion.
This work evaluates LNNs for infinite horizon planning in quadrupedal robots
through four dynamics models: (1) full-order forward dynamics (FD) training and
inference, (2) diagonalized representation of Mass Matrix in full order FD, (3)
full-order inverse dynamics (ID) training with FD inference, (4) reduced-order
modeling via torso centre-of-mass (CoM) dynamics. Experiments demonstrate that
LNNs bring improvements in sample efficiency (10x) and superior prediction
accuracy (up to 2-10x) compared to baseline methods. Notably, the
diagonalization approach of LNNs reduces computational complexity while
retaining some interpretability, enabling real-time receding horizon control.
These findings highlight the advantages of LNNs in capturing the underlying
structure of system dynamics in quadrupeds, leading to improved performance and
efficiency in locomotion planning and control. Additionally, our approach
achieves a higher control frequency than previous LNN methods, demonstrating
its potential for real-world deployment on quadrupeds.

</details>


### [448] [From Theory to Practice: Identifying the Optimal Approach for Offset Point Tracking in the Context of Agricultural Robotics](https://arxiv.org/abs/2506.16143)
*Stephane Ngnepiepaye Wembe,Vincent Rousseau,Johann Laconte,Roland Lenain*

Main category: cs.RO

TL;DR: The paper addresses challenges in agricultural robotics, focusing on control strategies for implement-level precision in non-straight crop rows.


<details>
  <summary>Details</summary>
Motivation: Modern agriculture demands solutions for food production, labor shortages, and environmental concerns, necessitating precise robotic automation.

Method: It proposes a predictive control strategy centered on the implement's reference point to improve motion tracking and avoid overshooting during turns.

Result: The study showcases enhanced tracking and interaction accuracy in agricultural robots through the predictive control strategy.

Conclusion: By prioritizing the implement over the robot body, the paper introduces a significant improvement to agricultural robotic operations, enabling better handling of complex field layouts.

Abstract: Modern agriculture faces escalating challenges: increasing demand for food,
labor shortages, and the urgent need to reduce environmental impact.
Agricultural robotics has emerged as a promising response to these pressures,
enabling the automation of precise and suitable field operations. In
particular, robots equipped with implements for tasks such as weeding or sowing
must interact delicately and accurately with the crops and soil. Unlike robots
in other domains, these agricultural platforms typically use rigidly mounted
implements, where the implement's position is more critical than the robot's
center in determining task success. Yet, most control strategies in the
literature focus on the vehicle body, often neglecting the acctual working
point of the system. This is particularly important when considering new
agriculture practices where crops row are not necessary straights. This paper
presents a predictive control strategy targeting the implement's reference
point. The method improves tracking performance by anticipating the motion of
the implement, which, due to its offset from the vehicle's center of rotation,
is prone to overshooting during turns if not properly accounted for.

</details>


### [449] [Single-Microphone-Based Sound Source Localization for Mobile Robots in Reverberant Environments](https://arxiv.org/abs/2506.16173)
*Jiang Wang,Runwu Shi,Benjamin Yen,He Kong,Kazuhiro Nakadai*

Main category: cs.RO

TL;DR: This paper introduces a novel method for sound source localization using a single microphone on a moving robot in reverberant settings, employing a lightweight neural network combined with extended Kalman filtering.


<details>
  <summary>Details</summary>
Motivation: Traditional sound source localization methods rely on preconfigured microphone arrays, limiting their application to robots with spatial constraints.

Method: The approach involves using a single microphone and a neural network model with 43k parameters for distance estimation. This is combined with an extended Kalman filter for real-time sound source localization.

Result: The proposed method successfully achieves online sound source localization, validated through extensive experiments in reverberant environments.

Conclusion: This work fills a research gap by enabling movable robot sound source localization with a single microphone, offering a practical and accessible solution. The authors have shared their implementation to support further research.

Abstract: Accurately estimating sound source positions is crucial for robot audition.
However, existing sound source localization methods typically rely on a
microphone array with at least two spatially preconfigured microphones. This
requirement hinders the applicability of microphone-based robot audition
systems and technologies. To alleviate these challenges, we propose an online
sound source localization method that uses a single microphone mounted on a
mobile robot in reverberant environments. Specifically, we develop a
lightweight neural network model with only 43k parameters to perform real-time
distance estimation by extracting temporal information from reverberant
signals. The estimated distances are then processed using an extended Kalman
filter to achieve online sound source localization. To the best of our
knowledge, this is the first work to achieve online sound source localization
using a single microphone on a moving robot, a gap that we aim to fill in this
work. Extensive experiments demonstrate the effectiveness and merits of our
approach. To benefit the broader research community, we have open-sourced our
code at https://github.com/JiangWAV/single-mic-SSL.

</details>


### [450] [FlowRAM: Grounding Flow Matching Policy with Region-Aware Mamba Framework for Robotic Manipulation](https://arxiv.org/abs/2506.16201)
*Sen Wang,Le Wang,Sanping Zhou,Jingyi Tian,Jiayi Li,Haowen Sun,Wei Tang*

Main category: cs.RO

TL;DR: FlowRAM introduces a framework enhancing robotic manipulation in high-precision tasks by utilizing generative models for efficient multimodal information processing, and achieves improved computational efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: Address limitations in diffusion-based policy learning for robotic manipulation, specifically computational inefficiency and inadequate exploration of generative models in 3D environments.

Method: Employ a Dynamic Radius Schedule for adaptive perception, integrate state space models for multimodal information processing with linear complexity, and implement conditional flow matching for learning action poses efficiently.

Result: FlowRAM achieves state-of-the-art performance on RLBench tests, improves average success rate by 12.0%, and reduces inference steps to under 4, demonstrating enhanced efficiency and accuracy.

Conclusion: FlowRAM demonstrates the potential of generative model-based frameworks in robotic manipulation, significantly enhancing precision, inference speed, and success rate in challenging real-world tasks.

Abstract: Robotic manipulation in high-precision tasks is essential for numerous
industrial and real-world applications where accuracy and speed are required.
Yet current diffusion-based policy learning methods generally suffer from low
computational efficiency due to the iterative denoising process during
inference. Moreover, these methods do not fully explore the potential of
generative models for enhancing information exploration in 3D environments. In
response, we propose FlowRAM, a novel framework that leverages generative
models to achieve region-aware perception, enabling efficient multimodal
information processing. Specifically, we devise a Dynamic Radius Schedule,
which allows adaptive perception, facilitating transitions from global scene
comprehension to fine-grained geometric details. Furthermore, we integrate
state space models to integrate multimodal information, while preserving linear
computational complexity. In addition, we employ conditional flow matching to
learn action poses by regressing deterministic vector fields, simplifying the
learning process while maintaining performance. We verify the effectiveness of
the FlowRAM in the RLBench, an established manipulation benchmark, and achieve
state-of-the-art performance. The results demonstrate that FlowRAM achieves a
remarkable improvement, particularly in high-precision tasks, where it
outperforms previous methods by 12.0% in average success rate. Additionally,
FlowRAM is able to generate physically plausible actions for a variety of
real-world tasks in less than 4 time steps, significantly increasing inference
speed.

</details>


### [451] [ControlVLA: Few-shot Object-centric Adaptation for Pre-trained Vision-Language-Action Models](https://arxiv.org/abs/2506.16211)
*Puhao Li,Yingying Wu,Ziheng Xi,Wanlin Li,Yuzhe Huang,Zhiyuan Zhang,Yinghan Chen,Jianan Wang,Song-Chun Zhu,Tengyu Liu,Siyuan Huang*

Main category: cs.RO

TL;DR: The study introduces ControlVLA, a framework enabling fine-tuning of pre-trained models for robotic manipulation using minimal demonstrations, achieving high success rates with 10-20 examples.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of learning robotic manipulation in real-world settings with limited demonstrations, overcoming the drawbacks of simulation-based data and pre-built modules.

Method: ControlVLA leverages a ControlNet-style architecture that incorporates object-centric conditions into pre-trained VLA models via zero-initialized projection layers for efficient fine-tuning.

Result: ControlVLA achieved a 76.7% success rate on diverse manipulation tasks using only 10-20 demonstrations, outperforming traditional methods requiring over 100 demonstrations.

Conclusion: The proposed method effectively bridges general-purpose pre-trained models with specific tasks in data-scarce robotic manipulation, showing extensibility to complex and unseen scenarios.

Abstract: Learning real-world robotic manipulation is challenging, particularly when
limited demonstrations are available. Existing methods for few-shot
manipulation often rely on simulation-augmented data or pre-built modules like
grasping and pose estimation, which struggle with sim-to-real gaps and lack
extensibility. While large-scale imitation pre-training shows promise, adapting
these general-purpose policies to specific tasks in data-scarce settings
remains unexplored. To achieve this, we propose ControlVLA, a novel framework
that bridges pre-trained VLA models with object-centric representations via a
ControlNet-style architecture for efficient fine-tuning. Specifically, to
introduce object-centric conditions without overwriting prior knowledge,
ControlVLA zero-initializes a set of projection layers, allowing them to
gradually adapt the pre-trained manipulation policies. In real-world
experiments across 6 diverse tasks, including pouring cubes and folding
clothes, our method achieves a 76.7% success rate while requiring only 10-20
demonstrations -- a significant improvement over traditional approaches that
require more than 100 demonstrations to achieve comparable success. Additional
experiments highlight ControlVLA's extensibility to long-horizon tasks and
robustness to unseen objects and backgrounds.

</details>


### [452] [Probabilistic Collision Risk Estimation for Pedestrian Navigation](https://arxiv.org/abs/2506.16219)
*Amine Tourki,Paul Prevel,Nils Einecke,Tim Puphal,Alexandre Alahi*

Main category: cs.RO

TL;DR: The paper integrates risk model technology into assistance devices for vision-impaired persons, demonstrating superior warning accuracy compared to traditional measures.


<details>
  <summary>Details</summary>
Motivation: Assistive devices for vision impairment lag behind intelligent driver assistance systems, prompting exploration of advanced technologies to improve safety and support.

Method: The study adapted a probabilistic risk model from autonomous driving to predict collision risks using object trajectories and compared its accuracy to distance and time-to-contact measures.

Result: The risk model showed a warning accuracy of 67% in real-world scenarios, outperforming distance and time-to-contact measures, which achieved only 51%.

Conclusion: Integrating advanced risk models into assistive devices for vision impairment significantly improves safety through accurate warnings, bridging the gap with intelligent systems like those in autonomous driving.

Abstract: Intelligent devices for supporting persons with vision impairment are
becoming more widespread, but they are lacking behind the advancements in
intelligent driver assistant system. To make a first step forward, this work
discusses the integration of the risk model technology, previously used in
autonomous driving and advanced driver assistance systems, into an assistance
device for persons with vision impairment. The risk model computes a
probabilistic collision risk given object trajectories which has previously
been shown to give better indications of an object's collision potential
compared to distance or time-to-contact measures in vehicle scenarios. In this
work, we show that the risk model is also superior in warning persons with
vision impairment about dangerous objects. Our experiments demonstrate that the
warning accuracy of the risk model is 67% while both distance and
time-to-contact measures reach only 51% accuracy for real-world data.

</details>


### [453] [CodeDiffuser: Attention-Enhanced Diffusion Policy via VLM-Generated Code for Instruction Ambiguity](https://arxiv.org/abs/2506.16652)
*Guang Yin,Yitong Li,Yixuan Wang,Dale McConachie,Paarth Shah,Kunimatsu Hashimoto,Huan Zhang,Katherine Liu,Yunzhu Li*

Main category: cs.RO

TL;DR: This paper presents a robot manipulation framework addressing ambiguity in natural language instructions using Vision-Language Models (VLM) to generate task-specific code and resolve ambiguities for effective task execution.


<details>
  <summary>Details</summary>
Motivation: The authors aim to overcome challenges in robotic manipulation caused by ambiguity in natural language instructions and the limitations of existing language-conditioned end-to-end policies.

Method: A modular framework employing Vision-Language Models to interpret natural language and generate task-specific executable code, supported by perception modules and 3D attention maps.

Result: Experiments demonstrated superior performance over existing methods in handling ambiguous instructions, contact-rich manipulations, and multi-object interactions.

Conclusion: The approach successfully addresses language ambiguity and environmental variations, showcasing adaptability and enhanced task-solving abilities in robotic manipulation.

Abstract: Natural language instructions for robotic manipulation tasks often exhibit
ambiguity and vagueness. For instance, the instruction "Hang a mug on the mug
tree" may involve multiple valid actions if there are several mugs and branches
to choose from. Existing language-conditioned policies typically rely on
end-to-end models that jointly handle high-level semantic understanding and
low-level action generation, which can result in suboptimal performance due to
their lack of modularity and interpretability. To address these challenges, we
introduce a novel robotic manipulation framework that can accomplish tasks
specified by potentially ambiguous natural language. This framework employs a
Vision-Language Model (VLM) to interpret abstract concepts in natural language
instructions and generates task-specific code - an interpretable and executable
intermediate representation. The generated code interfaces with the perception
module to produce 3D attention maps that highlight task-relevant regions by
integrating spatial and semantic information, effectively resolving ambiguities
in instructions. Through extensive experiments, we identify key limitations of
current imitation learning methods, such as poor adaptation to language and
environmental variations. We show that our approach excels across challenging
manipulation tasks involving language ambiguity, contact-rich manipulation, and
multi-object interactions.

</details>


### [454] [CapsDT: Diffusion-Transformer for Capsule Robot Manipulation](https://arxiv.org/abs/2506.16263)
*Xiting He,Mingwu Su,Xinqi Jiang,Long Bai,Jiewen Lai,Hongliang Ren*

Main category: cs.RO

TL;DR: The paper presents CapsDT, a new Vision-Language-Action (VLA) model for endoscopy capsule robot manipulation, demonstrating state-of-the-art performance in simulated medical tasks.


<details>
  <summary>Details</summary>
Motivation: To improve interactions between human operators and endoscopy capsule robots, enhancing diagnostic accuracy and treatment using vision-language-action models.

Method: Designed CapsDT, a Diffusion Transformer model that processes combined visual inputs and textual instructions to generate robotic control signals. Built a physical system with a robotic arm-held magnet and created datasets for gastric endoscopy tasks.

Result: CapsDT achieved state-of-the-art results on various simulated endoscopy tasks, including a 26.25% success rate for real-world manipulation within a stomach simulator.

Conclusion: CapsDT proves to be an effective VLA model for capsule robot tasks, offering improved capabilities for endoscopy applications and advancing the field of medical robotics.

Abstract: Vision-Language-Action (VLA) models have emerged as a prominent research
area, showcasing significant potential across a variety of applications.
However, their performance in endoscopy robotics, particularly endoscopy
capsule robots that perform actions within the digestive system, remains
unexplored. The integration of VLA models into endoscopy robots allows more
intuitive and efficient interactions between human operators and medical
devices, improving both diagnostic accuracy and treatment outcomes. In this
work, we design CapsDT, a Diffusion Transformer model for capsule robot
manipulation in the stomach. By processing interleaved visual inputs, and
textual instructions, CapsDT can infer corresponding robotic control signals to
facilitate endoscopy tasks. In addition, we developed a capsule endoscopy robot
system, a capsule robot controlled by a robotic arm-held magnet, addressing
different levels of four endoscopy tasks and creating corresponding capsule
robot datasets within the stomach simulator. Comprehensive evaluations on
various robotic tasks indicate that CapsDT can serve as a robust
vision-language generalist, achieving state-of-the-art performance in various
levels of endoscopy tasks while achieving a 26.25% success rate in real-world
simulation manipulation.

</details>


### [455] [M-Predictive Spliner: Enabling Spatiotemporal Multi-Opponent Overtaking for Autonomous Racing](https://arxiv.org/abs/2506.16301)
*Nadine Imholz,Maurice Brunner,Nicolas Baumann,Edoardo Ghignone,Michele Magno*

Main category: cs.RO

TL;DR: The paper presents a method for autonomous multi-agent racing that predicts opponents' intent for improved overtaking maneuvers.


<details>
  <summary>Details</summary>
Motivation: The study aims to improve decision-making in unrestricted, multi-agent autonomous racing by overcoming limitations of prior approaches, such as neglecting spatiotemporal information or focusing only on single-opponent scenarios.

Method: The approach combines Kalman Filter-based opponent tracking and spatial-velocity Gaussian Process Regression to predict opponent trajectories, enabling effective overtaking maneuvers.

Result: The proposed method achieved a 91.65% overtaking success rate and a 10.13%-point improvement in safety compared to the previous state-of-the-art.

Conclusion: The method demonstrates high potential for robust autonomous performance in multi-agent racing scenarios, enhancing overtaking success and operational safety.

Abstract: Unrestricted multi-agent racing presents a significant research challenge,
requiring decision-making at the limits of a robot's operational capabilities.
While previous approaches have either ignored spatiotemporal information in the
decision-making process or been restricted to single-opponent scenarios, this
work enables arbitrary multi-opponent head-to-head racing while considering the
opponents' future intent. The proposed method employs a KF-based multi-opponent
tracker to effectively perform opponent ReID by associating them across
observations. Simultaneously, spatial and velocity GPR is performed on all
observed opponent trajectories, providing predictive information to compute the
overtaking maneuvers. This approach has been experimentally validated on a
physical 1:10 scale autonomous racing car, achieving an overtaking success rate
of up to 91.65% and demonstrating an average 10.13%-point improvement in safety
at the same speed as the previous SotA. These results highlight its potential
for high-performance autonomous racing.

</details>


### [456] [Goal-conditioned Hierarchical Reinforcement Learning for Sample-efficient and Safe Autonomous Driving at Intersections](https://arxiv.org/abs/2506.16336)
*Yiou Huang*

Main category: cs.RO

TL;DR: The paper introduces a hierarchical reinforcement learning framework for autonomous driving, incorporating a goal-conditioned collision prediction (GCCP) module to enhance safety and sample efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the training challenges of sample efficiency and safety in reinforcement learning for complex autonomous driving scenarios.

Method: A hierarchical reinforcement learning framework is proposed, where a high-level decision-maker selects safe subgoals using a GCCP module that predicts collision risks, and a low-level motion-planner executes tasks based on subgoals.

Result: The framework demonstrates faster convergence to optimal policies and improved safety compared to traditional reinforcement learning methods.

Conclusion: The proposed hierarchical reinforcement learning framework with GCCP is effective in improving sample efficiency and safety for autonomous driving tasks.

Abstract: Reinforcement learning (RL) exhibits remarkable potential in addressing
autonomous driving tasks. However, it is difficult to train a sample-efficient
and safe policy in complex scenarios. In this article, we propose a novel
hierarchical reinforcement learning (HRL) framework with a goal-conditioned
collision prediction (GCCP) module. In the hierarchical structure, the GCCP
module predicts collision risks according to different potential subgoals of
the ego vehicle. A high-level decision-maker choose the best safe subgoal. A
low-level motion-planner interacts with the environment according to the
subgoal. Compared to traditional RL methods, our algorithm is more
sample-efficient, since its hierarchical structure allows reusing the policies
of subgoals across similar tasks for various navigation scenarios. In
additional, the GCCP module's ability to predict both the ego vehicle's and
surrounding vehicles' future actions according to different subgoals, ensures
the safety of the ego vehicle throughout the decision-making process.
Experimental results demonstrate that the proposed method converges to an
optimal policy faster and achieves higher safety than traditional RL methods.

</details>


### [457] [Comparison between External and Internal Single Stage Planetary gearbox actuators for legged robots](https://arxiv.org/abs/2506.16356)
*Aman Singh,Deepak Kapa,Prasham Chedda,Shishir N. Y. Kolathaya*

Main category: cs.RO

TL;DR: This paper introduces a framework for optimizing actuator designs for legged robots, comparing two gearbox types (ISSPG and ESSPG) for specific gear ratios.


<details>
  <summary>Details</summary>
Motivation: There is a lack of objective comparisons and systematic optimization in actuator design, especially between Internal and External Single-Stage Planetary Gearboxes, which are crucial for efficient and lightweight robotics.

Method: The authors developed a design framework to optimize actuator parameters, generating and analyzing various gearbox designs. They also validated their methodology by manufacturing and testing two optimized designs with precise gear ratios.

Result: The study finds ISSPG is optimal for lower gear ratios (5:1 to 7:1), while ESSPG is preferable for higher gear ratios (7:1 to 11:1). Manufactured designs closely match predicted mass, supporting the validity of the framework.

Conclusion: The proposed optimization design framework is effective and provides practical insights, suggesting ISSPG’s advantage for low to moderate gear ratios and ESSPG’s suitability for higher gear ratios in robotic actuators.

Abstract: Legged robots, such as quadrupeds and humanoids, require high-performance
actuators for efficient locomotion. Quasi-Direct-Drive (QDD) actuators with
single-stage planetary gearboxes offer low inertia, high efficiency, and
transparency. Among planetary gearbox architectures, Internal (ISSPG) and
External Single-Stage Planetary Gearbox (ESSPG) are the two predominant
designs. While ISSPG is often preferred for its compactness and high torque
density at certain gear ratios, no objective comparison between the two
architectures exists. Additionally, existing designs rely on heuristics rather
than systematic optimization. This paper presents a design framework for
optimally selecting actuator parameters based on given performance requirements
and motor specifications. Using this framework, we generate and analyze various
optimized gearbox designs for both architectures. Our results demonstrate that
for the T-motor U12, ISSPG is the superior choice within the lower gear ratio
range of 5:1 to 7:1, offering a lighter design. However, for gear ratios
exceeding 7:1, ISSPG becomes infeasible, making ESSPG the better option in the
7:1 to 11:1 range. To validate our approach, we designed and optimized two
actuators for manufacturing: an ISSPG with a 6.0:1 gear ratio and an ESSPG with
a 7.2:1 gear ratio. Their respective masses closely align with our optimization
model predictions, confirming the effectiveness of our methodology.

</details>


### [458] [CSC-MPPI: A Novel Constrained MPPI Framework with DBSCAN for Reliable Obstacle Avoidance](https://arxiv.org/abs/2506.16386)
*Leesai Park,Keunwoo Jang,Sanghyun Kim*

Main category: cs.RO

TL;DR: This paper introduces CSC-MPPI, a model predictive path integral approach that uses constrained sampling and clustering to improve trajectory optimization while enforcing strict system constraints.


<details>
  <summary>Details</summary>
Motivation: Traditional MPPI struggles with constraint satisfaction and generates suboptimal trajectories, necessitating a more robust approach.

Method: CSC-MPPI employs a primal-dual gradient method to enforce constraints and DBSCAN clustering for improved selection of feasible trajectories.

Result: Simulation and real-world experiments demonstrate enhanced obstacle avoidance, reliability, and efficiency compared to traditional MPPI.

Conclusion: CSC-MPPI effectively optimizes trajectories, satisfies constraints, and performs better in complex environments.

Abstract: This paper proposes Constrained Sampling Cluster Model Predictive Path
Integral (CSC-MPPI), a novel constrained formulation of MPPI designed to
enhance trajectory optimization while enforcing strict constraints on system
states and control inputs. Traditional MPPI, which relies on a probabilistic
sampling process, often struggles with constraint satisfaction and generates
suboptimal trajectories due to the weighted averaging of sampled trajectories.
To address these limitations, the proposed framework integrates a primal-dual
gradient-based approach and Density-Based Spatial Clustering of Applications
with Noise (DBSCAN) to steer sampled input trajectories into feasible regions
while mitigating risks associated with weighted averaging. First, to ensure
that sampled trajectories remain within the feasible region, the primal-dual
gradient method is applied to iteratively shift sampled inputs while enforcing
state and control constraints. Then, DBSCAN groups the sampled trajectories,
enabling the selection of representative control inputs within each cluster.
Finally, among the representative control inputs, the one with the lowest cost
is chosen as the optimal action. As a result, CSC-MPPI guarantees constraint
satisfaction, improves trajectory selection, and enhances robustness in complex
environments. Simulation and real-world experiments demonstrate that CSC-MPPI
outperforms traditional MPPI in obstacle avoidance, achieving improved
reliability and efficiency. The experimental videos are available at
https://cscmppi.github.io

</details>


### [459] [Full-Pose Tracking via Robust Control for Over-Actuated Multirotors](https://arxiv.org/abs/2506.16427)
*Mohamad Hachem,Clément Roos,Thierry Miquel,Murat Bronz*

Main category: cs.RO

TL;DR: The paper introduces a robust control approach for over-actuated multirotors using INDI combined with structured H_inf control and a geometric guidance allocation method. It shows improvements in precise attitude and position tracking through simulation.


<details>
  <summary>Details</summary>
Motivation: To address limitations in control architectures for over-actuated multirotors, such as infeasible pose references, disturbances, and physical constraint handling.

Method: The approach combines Incremental Nonlinear Dynamic Inversion (INDI) control with structured H_inf control and uses a weighted least-squares geometric guidance control allocation method formulated as a quadratic optimization problem.

Result: Numerical simulations with an over-actuated hexacopter validate the approach's effectiveness in diverse mission scenarios and its suitability for real aerial applications.

Conclusion: The proposed architecture enhances tracking precision, robustness, and adaptability to various configurations for multirotors, making it promising for practical applications.

Abstract: This paper presents a robust cascaded control architecture for over-actuated
multirotors. It extends the Incremental Nonlinear Dynamic Inversion (INDI)
control combined with structured H_inf control, initially proposed for
under-actuated multirotors, to a broader range of multirotor configurations. To
achieve precise and robust attitude and position tracking, we employ a weighted
least-squares geometric guidance control allocation method, formulated as a
quadratic optimization problem, enabling full-pose tracking. The proposed
approach effectively addresses key challenges, such as preventing infeasible
pose references and enhancing robustness against disturbances, as well as
considering multirotor's actual physical limitations. Numerical simulations
with an over-actuated hexacopter validate the method's effectiveness,
demonstrating its adaptability to diverse mission scenarios and its potential
for real-world aerial applications.

</details>


### [460] [Human2LocoMan: Learning Versatile Quadrupedal Manipulation with Human Pretraining](https://arxiv.org/abs/2506.16475)
*Yaru Niu,Yunzhe Zhang,Mingyang Yu,Changyi Lin,Chenhao Li,Yikai Wang,Yuxiang Yang,Wenhao Yu,Tingnan Zhang,Bingqing Chen,Jonathan Francis,Zhenzhen Li,Jie Tan,Ding Zhao*

Main category: cs.RO

TL;DR: The paper presents a cross-embodiment imitation learning framework for quadrupedal manipulation using data from humans and a robot called LocoMan, achieving significant success rate improvements in a variety of tasks.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of providing quadrupedal robots with scalable and autonomous manipulation skills, particularly in diverse and complex environments.

Method: Developed a modularized teleoperation and data collection pipeline to align observations and actions across humans and robots. Proposed an efficient modularized architecture for co-training with structured modality-aligned data and constructed a comprehensive manipulation dataset.

Result: The proposed system showed an average success rate improvement of 41.9% overall and 79.7% under OOD settings compared to baselines. Pretraining with human data led to a 38.6% improvement overall and 82.7% for OOD tasks, requiring half the amount of robot data.

Conclusion: The system demonstrates promising advancements in quadrupedal manipulation, validating the potential of cross-embodiment imitation learning. The open-sourced resources will facilitate further research in the field.

Abstract: Quadrupedal robots have demonstrated impressive locomotion capabilities in
complex environments, but equipping them with autonomous versatile manipulation
skills in a scalable way remains a significant challenge. In this work, we
introduce a cross-embodiment imitation learning system for quadrupedal
manipulation, leveraging data collected from both humans and LocoMan, a
quadruped equipped with multiple manipulation modes. Specifically, we develop a
teleoperation and data collection pipeline, which unifies and modularizes the
observation and action spaces of the human and the robot. To effectively
leverage the collected data, we propose an efficient modularized architecture
that supports co-training and pretraining on structured modality-aligned data
across different embodiments. Additionally, we construct the first manipulation
dataset for the LocoMan robot, covering various household tasks in both
unimanual and bimanual modes, supplemented by a corresponding human dataset. We
validate our system on six real-world manipulation tasks, where it achieves an
average success rate improvement of 41.9% overall and 79.7% under
out-of-distribution (OOD) settings compared to the baseline. Pretraining with
human data contributes a 38.6% success rate improvement overall and 82.7% under
OOD settings, enabling consistently better performance with only half the
amount of robot data. Our code, hardware, and data are open-sourced at:
https://human2bots.github.io.

</details>


### [461] [Grounding Language Models with Semantic Digital Twins for Robotic Planning](https://arxiv.org/abs/2506.16493)
*Mehreen Naeem,Andrew Melnik,Michael Beetz*

Main category: cs.RO

TL;DR: The paper introduces a framework combining Semantic Digital Twins (SDTs) and Large Language Models (LLMs) for adaptive and goal-oriented robotic task execution in dynamic scenarios.


<details>
  <summary>Details</summary>
Motivation: To create a robotic system capable of understanding and executing tasks in dynamic environments by leveraging semantic context and language capabilities.

Method: The framework translates natural language instructions into structured action triplets using SDT data for contextual grounding. It uses LLMs for reasoning and adaptability, revising action plans with feedback after errors.

Result: The framework was evaluated on ALFRED benchmark tasks, showcasing robust and reliable task completion in various household scenarios.

Conclusion: The integration of SDTs and LLMs enables robots to effectively plan, adapt, and execute tasks in dynamic environments, even when errors occur.

Abstract: We introduce a novel framework that integrates Semantic Digital Twins (SDTs)
with Large Language Models (LLMs) to enable adaptive and goal-driven robotic
task execution in dynamic environments. The system decomposes natural language
instructions into structured action triplets, which are grounded in contextual
environmental data provided by the SDT. This semantic grounding allows the
robot to interpret object affordances and interaction rules, enabling action
planning and real-time adaptability. In case of execution failures, the LLM
utilizes error feedback and SDT insights to generate recovery strategies and
iteratively revise the action plan. We evaluate our approach using tasks from
the ALFRED benchmark, demonstrating robust performance across various household
scenarios. The proposed framework effectively combines high-level reasoning
with semantic environment understanding, achieving reliable task completion in
the face of uncertainty and failure.

</details>


### [462] [eCAV: An Edge-Assisted Evaluation Platform for Connected Autonomous Vehicles](https://arxiv.org/abs/2506.16535)
*Tyler Landle,Jordan Rapp,Dean Blank,Chandramouli Amarnath,Abhijit Chatterjee,Alex Daglis,Umakishore Ramachandran*

Main category: cs.RO

TL;DR: This paper introduces eCAV, a scalable simulation platform for evaluating connected autonomous vehicles (CAVs), capable of handling higher vehicle counts compared to current systems.


<details>
  <summary>Details</summary>
Motivation: With the rise of autonomous vehicles, ensuring road safety through V2X technologies is critical, yet existing simulation tools are inadequate for realistic large-scale testing.

Method: The authors developed eCAV, a modular platform for simulating V2X communication and vehicle algorithms, handling up to 8x the vehicles of current methods without perception and 4x with perception, while maintaining efficiency.

Result: eCAV demonstrates a significant performance improvement over existing frameworks, simulating up to 256 vehicles without perception and 64 with perception enabled, outperforming OpenCDA in scalability and speed.

Conclusion: eCAV provides a robust solution for testing and validating CAV control systems in large-scale scenarios, potentially advancing the adoption and safety of autonomous vehicles.

Abstract: As autonomous vehicles edge closer to widespread adoption, enhancing road
safety through collision avoidance and minimization of collateral damage
becomes imperative. Vehicle-to-everything (V2X) technologies, which include
vehicle-to-vehicle (V2V), vehicle-to-infrastructure (V2I), and vehicle-to-cloud
(V2C), are being proposed as mechanisms to achieve this safety improvement.
  Simulation-based testing is crucial for early-stage evaluation of Connected
Autonomous Vehicle (CAV) control systems, offering a safer and more
cost-effective alternative to real-world tests. However, simulating large 3D
environments with many complex single- and multi-vehicle sensors and
controllers is computationally intensive. There is currently no evaluation
framework that can effectively evaluate realistic scenarios involving large
numbers of autonomous vehicles.
  We propose eCAV -- an efficient, modular, and scalable evaluation platform to
facilitate both functional validation of algorithmic approaches to increasing
road safety, as well as performance prediction of algorithms of various V2X
technologies, including a futuristic Vehicle-to-Edge control plane and
correspondingly designed control algorithms. eCAV can model up to 256 vehicles
running individual control algorithms without perception enabled, which is
$8\times$ more vehicles than what is possible with state-of-the-art
alternatives. %faster than state-of-the-art alternatives that can simulate
$8\times$ fewer vehicles. With perception enabled, eCAV simulates up to 64
vehicles with a step time under 800ms, which is $4\times$ more and $1.5\times$
faster than the state-of-the-art OpenCDA framework.

</details>


### [463] [Agile, Autonomous Spacecraft Constellations with Disruption Tolerant Networking to Monitor Precipitation and Urban Floods](https://arxiv.org/abs/2506.16537)
*Sreeja Roy-Singh,Alan P. Li,Vinay Ravindra,Roderick Lammers,Marc Sanchez Net*

Main category: cs.RO

TL;DR: The paper presents a framework for scheduling small, agile satellite constellations with onboard intelligence to better monitor dynamic phenomena such as urban floods.


<details>
  <summary>Details</summary>
Motivation: To enhance responsiveness and observational efficacy for transient phenomena using agile, intelligent satellite constellations focused on solving critical issues like urban flooding and precipitation tracking.

Method: A ground-based and onboard algorithmic framework that integrates orbital mechanics, attitude control, inter-satellite communication, prediction, and planning for agile satellite constellations.

Result: The proposed framework demonstrated low latency in communication and improved flood magnitude observation by ~7%; agile constellations performed 98% better than non-agile ones.

Conclusion: Integrating intelligent agility in small satellite constellations significantly enhances observation capabilities, particularly for dynamic natural phenomena.

Abstract: Fully re-orientable small spacecraft are now supported by commercial
technologies, allowing them to point their instruments in any direction and
capture images, with short notice. When combined with improved onboard
processing, and implemented on a constellation of inter-communicable
satellites, this intelligent agility can significantly increase responsiveness
to transient or evolving phenomena. We demonstrate a ground-based and onboard
algorithmic framework that combines orbital mechanics, attitude control,
inter-satellite communication, intelligent prediction and planning to schedule
the time-varying, re-orientation of agile, small satellites in a constellation.
Planner intelligence is improved by updating the predictive value of future
space-time observations based on shared observations of evolving episodic
precipitation and urban flood forecasts. Reliable inter-satellite communication
within a fast, dynamic constellation topology is modeled in the physical,
access control and network layer. We apply the framework on a representative
24-satellite constellation observing 5 global regions. Results show
appropriately low latency in information exchange (average within 1/3rd
available time for implicit consensus), enabling the onboard scheduler to
observe ~7% more flood magnitude than a ground-based implementation. Both
onboard and offline versions performed ~98% better than constellations without
agility.

</details>


### [464] [BIDA: A Bi-level Interaction Decision-making Algorithm for Autonomous Vehicles in Dynamic Traffic Scenarios](https://arxiv.org/abs/2506.16546)
*Liyang Yu,Tianyi Wang,Junfeng Jiao,Fengwu Shan,Hongqing Chu,Bingzhao Gao*

Main category: cs.RO

TL;DR: The paper introduces a bi-level interaction decision-making algorithm (BIDA) for autonomous vehicles to manage human unpredictability in traffic using Monte Carlo tree search and deep reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: Autonomous vehicles face challenges in making real-time safe decisions in unpredictable, human-dominated traffic scenarios, especially on multi-lane highways and unsignalized intersections.

Method: The proposed algorithm integrates interactive Monte Carlo tree search with deep reinforcement learning, using value and policy networks to guide decision-making and implementing trajectory planning and tracking with CARLA simulator.

Result: Experimental evaluations show BIDA improves decision-making, reduces computation costs, and surpasses benchmarks in safety, efficiency, and interaction rationality under diverse traffic conditions.

Conclusion: The BIDA framework effectively addresses AV interaction challenges, proving superior in dynamic traffic scenarios by improving safety and efficiency while reducing computational demands.

Abstract: In complex real-world traffic environments, autonomous vehicles (AVs) need to
interact with other traffic participants while making real-time and
safety-critical decisions accordingly. The unpredictability of human behaviors
poses significant challenges, particularly in dynamic scenarios, such as
multi-lane highways and unsignalized T-intersections. To address this gap, we
design a bi-level interaction decision-making algorithm (BIDA) that integrates
interactive Monte Carlo tree search (MCTS) with deep reinforcement learning
(DRL), aiming to enhance interaction rationality, efficiency and safety of AVs
in dynamic key traffic scenarios. Specifically, we adopt three types of DRL
algorithms to construct a reliable value network and policy network, which
guide the online deduction process of interactive MCTS by assisting in value
update and node selection. Then, a dynamic trajectory planner and a trajectory
tracking controller are designed and implemented in CARLA to ensure smooth
execution of planned maneuvers. Experimental evaluations demonstrate that our
BIDA not only enhances interactive deduction and reduces computational costs,
but also outperforms other latest benchmarks, which exhibits superior safety,
efficiency and interaction rationality under varying traffic conditions.

</details>


### [465] [An Optimization-Augmented Control Framework for Single and Coordinated Multi-Arm Robotic Manipulation](https://arxiv.org/abs/2506.16555)
*Melih Özcan,Ozgur S. Oguz*

Main category: cs.RO

TL;DR: The paper proposes a multi-modal control framework that integrates force control with optimization-based motion planning for robotic manipulation, allowing seamless switching between control modes based on task demands.


<details>
  <summary>Details</summary>
Motivation: Robotic manipulation requires precise force control for compliant interaction and motion planning to avoid collisions, but existing methods struggle to handle both dynamic and extended motion sequences effectively.

Method: The study introduces a framework that decomposes complex robotic tasks into subtasks, dynamically assigning them to three control modes: pure optimization, pure force control, or hybrid control based on task requirements.

Result: The proposed framework demonstrated versatility and robustness in various scenarios, including single-arm, bimanual, and multi-arm manipulation, excelling in both free-space and contact-rich tasks.

Conclusion: The multi-modal control framework is particularly effective for handling complex, long-horizon robotic manipulation tasks, especially those requiring coordination among multiple robot arms and environmental considerations.

Abstract: Robotic manipulation demands precise control over both contact forces and
motion trajectories. While force control is essential for achieving compliant
interaction and high-frequency adaptation, it is limited to operations in close
proximity to the manipulated object and often fails to maintain stable
orientation during extended motion sequences. Conversely, optimization-based
motion planning excels in generating collision-free trajectories over the
robot's configuration space but struggles with dynamic interactions where
contact forces play a crucial role. To address these limitations, we propose a
multi-modal control framework that combines force control and
optimization-augmented motion planning to tackle complex robotic manipulation
tasks in a sequential manner, enabling seamless switching between control modes
based on task requirements. Our approach decomposes complex tasks into
subtasks, each dynamically assigned to one of three control modes: Pure
optimization for global motion planning, pure force control for precise
interaction, or hybrid control for tasks requiring simultaneous trajectory
tracking and force regulation. This framework is particularly advantageous for
bimanual and multi-arm manipulation, where synchronous motion and coordination
among arms are essential while considering both the manipulated object and
environmental constraints. We demonstrate the versatility of our method through
a range of long-horizon manipulation tasks, including single-arm, bimanual, and
multi-arm applications, highlighting its ability to handle both free-space
motion and contact-rich manipulation with robustness and precision.

</details>


### [466] [Reimagination with Test-time Observation Interventions: Distractor-Robust World Model Predictions for Visual Model Predictive Control](https://arxiv.org/abs/2506.16565)
*Yuxin Chen,Jianglan Wei,Chenfeng Xu,Boyi Li,Masayoshi Tomizuka,Andrea Bajcsy,Ran Tian*

Main category: cs.RO

TL;DR: The paper introduces Reimagination with Observation Intervention (ReOI), a technique to improve world models for robots by addressing novel distractors in visual inputs. It enhances task success rates in open-world scenarios.


<details>
  <summary>Details</summary>
Motivation: Current world models for robots struggle with novel visual distractors, which negatively impact their action outcome predictions and lead to failures in planning and verification.

Method: ReOI operates at test-time by detecting and removing visual distractors from observations, reimagining outcomes with improved data, and reintroducing distractors for visual consistency during downstream tasks.

Result: ReOI demonstrates robustness against both in-distribution and out-of-distribution distractors, showing up to a 3x improvement in task success rates in robotic manipulation tasks.

Conclusion: ReOI effectively enhances the reliability of world models in scenarios with inevitable novel distractors, outperforming standard approaches in open-world robotic settings.

Abstract: World models enable robots to "imagine" future observations given current
observations and planned actions, and have been increasingly adopted as
generalized dynamics models to facilitate robot learning. Despite their
promise, these models remain brittle when encountering novel visual distractors
such as objects and background elements rarely seen during training.
Specifically, novel distractors can corrupt action outcome predictions, causing
downstream failures when robots rely on the world model imaginations for
planning or action verification. In this work, we propose Reimagination with
Observation Intervention (ReOI), a simple yet effective test-time strategy that
enables world models to predict more reliable action outcomes in open-world
scenarios where novel and unanticipated visual distractors are inevitable.
Given the current robot observation, ReOI first detects visual distractors by
identifying which elements of the scene degrade in physically implausible ways
during world model prediction. Then, it modifies the current observation to
remove these distractors and bring the observation closer to the training
distribution. Finally, ReOI "reimagines" future outcomes with the modified
observation and reintroduces the distractors post-hoc to preserve visual
consistency for downstream planning and verification. We validate our approach
on a suite of robotic manipulation tasks in the context of action verification,
where the verifier needs to select desired action plans based on predictions
from a world model. Our results show that ReOI is robust to both
in-distribution and out-of-distribution visual distractors. Notably, it
improves task success rates by up to 3x in the presence of novel distractors,
significantly outperforming action verification that relies on world model
predictions without imagination interventions.

</details>


### [467] [DRIVE Through the Unpredictability:From a Protocol Investigating Slip to a Metric Estimating Command Uncertainty](https://arxiv.org/abs/2506.16593)
*Nicolas Samson,William Larrivée-Hardy,William Dubois,Élie Roy-Brouard,Edith Brotherton,Dominic Baril,Julien Lépine,François Pomerleau*

Main category: cs.RO

TL;DR: The paper introduces the DRIVE protocol to improve data collection for modeling off-road UGV motion, validated via datasets on multiple terrains and platforms.


<details>
  <summary>Details</summary>
Motivation: The need for more accurate motion models for off-road autonomous navigation that account for terrain-robot interactions, which onboard sensors cannot directly measure.

Method: Development of the DRIVE protocol for standardizing data collection. Validation was conducted with datasets from two platforms across six terrains, analyzing velocity commands and slip dynamics.

Result: Protocol was validated with 4.9 hours of data across terrains, allowing identification of velocity-slippage relationships and introducing a metric for command unpredictability.

Conclusion: The research helps in understanding terrain-robot dynamics, provides insights for future system identification, and shares lessons for large UGV deployment.

Abstract: Off-road autonomous navigation is a challenging task as it is mainly
dependent on the accuracy of the motion model. Motion model performances are
limited by their ability to predict the interaction between the terrain and the
UGV, which an onboard sensor can not directly measure. In this work, we propose
using the DRIVE protocol to standardize the collection of data for system
identification and characterization of the slip state space. We validated this
protocol by acquiring a dataset with two platforms (from 75 kg to 470 kg) on
six terrains (i.e., asphalt, grass, gravel, ice, mud, sand) for a total of 4.9
hours and 14.7 km. Using this data, we evaluate the DRIVE protocol's ability to
explore the velocity command space and identify the reachable velocities for
terrain-robot interactions. We investigated the transfer function between the
command velocity space and the resulting steady-state slip for an SSMR. An
unpredictability metric is proposed to estimate command uncertainty and help
assess risk likelihood and severity in deployment. Finally, we share our
lessons learned on running system identification on large UGV to help the
community.

</details>


### [468] [History-Augmented Vision-Language Models for Frontier-Based Zero-Shot Object Navigation](https://arxiv.org/abs/2506.16623)
*Mobin Habibpour,Fatemeh Afghah*

Main category: cs.RO

TL;DR: The paper presents a zero-shot ObjectNav framework that utilizes history-aware VLM prompting and refined waypoint strategies for robust navigation in unseen environments.


<details>
  <summary>Details</summary>
Motivation: Current ObjectNav methods underutilize the reasoning capabilities of Vision-Language Models, leading to inefficient navigation and poor contextual understanding.

Method: The framework leverages history-aware dynamic prompting in VLMs for providing semantic guidance and uses a waypoint generation mechanism for precise navigation.

Result: Tests on the HM3D dataset show a 46% Success Rate and 24.8% SPL, on par with state-of-the-art zero-shot methods.

Conclusion: The history-augmented VLM prompting strategy enhances context-aware navigation, suggesting that deeper integration of VLM reasoning can significantly improve robotic exploration.

Abstract: Object Goal Navigation (ObjectNav) challenges robots to find objects in
unseen environments, demanding sophisticated reasoning. While Vision-Language
Models (VLMs) show potential, current ObjectNav methods often employ them
superficially, primarily using vision-language embeddings for object-scene
similarity checks rather than leveraging deeper reasoning. This limits
contextual understanding and leads to practical issues like repetitive
navigation behaviors. This paper introduces a novel zero-shot ObjectNav
framework that pioneers the use of dynamic, history-aware prompting to more
deeply integrate VLM reasoning into frontier-based exploration. Our core
innovation lies in providing the VLM with action history context, enabling it
to generate semantic guidance scores for navigation actions while actively
avoiding decision loops. We also introduce a VLM-assisted waypoint generation
mechanism for refining the final approach to detected objects. Evaluated on the
HM3D dataset within Habitat, our approach achieves a 46% Success Rate (SR) and
24.8% Success weighted by Path Length (SPL). These results are comparable to
state-of-the-art zero-shot methods, demonstrating the significant potential of
our history-augmented VLM prompting strategy for more robust and context-aware
robotic navigation.

</details>


### [469] [See What I Mean? Expressiveness and Clarity in Robot Display Design](https://arxiv.org/abs/2506.16643)
*Matthew Ebisu,Hang Yu,Reuben Aronson,Elaine Short*

Main category: cs.RO

TL;DR: This paper investigates how animated versus static non-verbal visual cues in robots affect task performance, trust, and satisfaction in dynamic human-robot collaboration scenarios.


<details>
  <summary>Details</summary>
Motivation: To understand how different nonverbal communication cues affect human-robot interaction, particularly task performance, in real-time decision-making scenarios.

Method: A collaborative navigation task where humans and robots with partial map knowledge communicated using either animated or static visual displays. In total, 37 participants interacted with robots using either type of display.

Result: Animated displays fostered the highest trust and satisfaction levels, static icons enhanced interpretability, and robots with static eyes contributed to the highest task completion success rates.

Conclusion: While animated visual cues improve trust and satisfaction, combining static, familiar icons enhances human-robot communication and task performance reliability.

Abstract: Nonverbal visual symbols and displays play an important role in communication
when humans and robots work collaboratively. However, few studies have
investigated how different types of non-verbal cues affect objective task
performance, especially in a dynamic environment that requires real time
decision-making. In this work, we designed a collaborative navigation task
where the user and the robot only had partial information about the map on each
end and thus the users were forced to communicate with a robot to complete the
task. We conducted our study in a public space and recruited 37 participants
who randomly passed by our setup. Each participant collaborated with a robot
utilizing either animated anthropomorphic eyes and animated icons, or static
anthropomorphic eyes and static icons. We found that participants that
interacted with a robot with animated displays reported the greatest level of
trust and satisfaction; that participants interpreted static icons the best;
and that participants with a robot with static eyes had the highest completion
success. These results suggest that while animation can foster trust with
robots, human-robot communication can be optimized by the addition of familiar
static icons that may be easier for users to interpret. We published our code,
designed symbols, and collected results online at:
https://github.com/mattufts/huamn_Cozmo_interaction.

</details>


### [470] [Compliant Residual DAgger: Improving Real-World Contact-Rich Manipulation with Human Corrections](https://arxiv.org/abs/2506.16685)
*Xiaomeng Xu,Yifan Hou,Zeyi Liu,Shuran Song*

Main category: cs.RO

TL;DR: The paper introduces CR-DAgger, a system designed to improve real-world contact-rich manipulation tasks by using compliance control for human corrections and force feedback for policy learning.


<details>
  <summary>Details</summary>
Motivation: To overcome challenges in dataset aggregation for manipulation tasks, including collecting human correction data and updating policies effectively.

Method: CR-DAgger combines a compliant intervention interface for accurate human corrections and a policy formulation using force feedback to learn residual policies.

Result: The proposed system enhances manipulation task success rates by over 50% compared to existing approaches, using minimal human corrections.

Conclusion: CR-DAgger is a practical, effective solution for real-world robot learning tasks, providing insights for implementing dataset aggregation strategies.

Abstract: We address key challenges in Dataset Aggregation (DAgger) for real-world
contact-rich manipulation: how to collect informative human correction data and
how to effectively update policies with this new data. We introduce Compliant
Residual DAgger (CR-DAgger), which contains two novel components: 1) a
Compliant Intervention Interface that leverages compliance control, allowing
humans to provide gentle, accurate delta action corrections without
interrupting the ongoing robot policy execution; and 2) a Compliant Residual
Policy formulation that learns from human corrections while incorporating force
feedback and force control. Our system significantly enhances performance on
precise contact-rich manipulation tasks using minimal correction data,
improving base policy success rates by over 50\% on two challenging tasks (book
flipping and belt assembly) while outperforming both retraining-from-scratch
and finetuning approaches. Through extensive real-world experiments, we provide
practical guidance for implementing effective DAgger in real-world robot
learning tasks. Result videos are available at:
https://compliant-residual-dagger.github.io/

</details>


### [471] [VLM-Empowered Multi-Mode System for Efficient and Safe Planetary Navigation](https://arxiv.org/abs/2506.16703)
*Sinuo Cheng,Ruyi Zhou,Wenhao Feng,Huaiguang Yang,Haibo Gao,Zongquan Deng,Liang Ding*

Main category: cs.RO

TL;DR: This study introduces a Vision-Language Model (VLM)-empowered multi-mode navigation system for planetary rovers to improve efficiency and safety in complex terrains.


<details>
  <summary>Details</summary>
Motivation: The need for adaptable and flexible rover navigation strategies due to the increasingly complex planetary exploration environment.

Method: The system uses VLM for terrain understanding, switches across specialized navigation modes, and integrates local navigation with global waypoint planning for handling diverse terrains.

Result: Testing in simulated environments shows the multi-mode system improves traversal efficiency by 79.5% compared to single-mode methods while maintaining safety.

Conclusion: The proposed multi-mode navigation system enhances both efficiency and safety for planetary rovers in handling long-distance tasks across complex terrains.

Abstract: The increasingly complex and diverse planetary exploration environment
requires more adaptable and flexible rover navigation strategy. In this study,
we propose a VLM-empowered multi-mode system to achieve efficient while safe
autonomous navigation for planetary rovers. Vision-Language Model (VLM) is used
to parse scene information by image inputs to achieve a human-level
understanding of terrain complexity. Based on the complexity classification,
the system switches to the most suitable navigation mode, composing of
perception, mapping and planning modules designed for different terrain types,
to traverse the terrain ahead before reaching the next waypoint. By integrating
the local navigation system with a map server and a global waypoint generation
module, the rover is equipped to handle long-distance navigation tasks in
complex scenarios. The navigation system is evaluated in various simulation
environments. Compared to the single-mode conservative navigation method, our
multi-mode system is able to bootstrap the time and energy efficiency in a
long-distance traversal with varied type of obstacles, enhancing efficiency by
79.5%, while maintaining its avoidance capabilities against terrain hazards to
guarantee rover safety. More system information is shown at
https://chengsn1234.github.io/multi-mode-planetary-navigation/.

</details>


### [472] [Experimental Setup and Software Pipeline to Evaluate Optimization based Autonomous Multi-Robot Search Algorithms](https://arxiv.org/abs/2506.16710)
*Aditya Bhatt,Mary Katherine Corra,Franklin Merlo,Prajit KrisshnaKumar,Souma Chowdhury*

Main category: cs.RO

TL;DR: The paper introduces a physical setup and software pipeline to benchmark multi-robot search algorithms using acoustic sources and small robots, capable of assessing sim-to-real gaps.


<details>
  <summary>Details</summary>
Motivation: Signal source localization in multi-robot systems demands practical evaluation for applications like search, rescue, and hazard localization.

Method: A lab-scale setup with acoustic sources, e-puck robots, motion-capture systems, and a ROS-based software pipeline for testing multi-robot algorithms.

Result: The proposed setup successfully evaluated swarm optimization, Bayes-Swarm, and random walk algorithms under real-world conditions.

Conclusion: The paper provides a replicable, robust framework to assess and benchmark multi-robot search algorithms in real settings.

Abstract: Signal source localization has been a problem of interest in the multi-robot
systems domain given its applications in search \& rescue and hazard
localization in various industrial and outdoor settings. A variety of
multi-robot search algorithms exist that usually formulate and solve the
associated autonomous motion planning problem as a heuristic model-free or
belief model-based optimization process. Most of these algorithms however
remains tested only in simulation, thereby losing the opportunity to generate
knowledge about how such algorithms would compare/contrast in a real physical
setting in terms of search performance and real-time computing performance. To
address this gap, this paper presents a new lab-scale physical setup and
associated open-source software pipeline to evaluate and benchmark multi-robot
search algorithms. The presented physical setup innovatively uses an acoustic
source (that is safe and inexpensive) and small ground robots (e-pucks)
operating in a standard motion-capture environment. This setup can be easily
recreated and used by most robotics researchers. The acoustic source also
presents interesting uncertainty in terms of its noise-to-signal ratio, which
is useful to assess sim-to-real gaps. The overall software pipeline is designed
to readily interface with any multi-robot search algorithm with minimal effort
and is executable in parallel asynchronous form. This pipeline includes a
framework for distributed implementation of multi-robot or swarm search
algorithms, integrated with a ROS (Robotics Operating System)-based software
stack for motion capture supported localization. The utility of this novel
setup is demonstrated by using it to evaluate two state-of-the-art multi-robot
search algorithms, based on swarm optimization and batch-Bayesian Optimization
(called Bayes-Swarm), as well as a random walk baseline.

</details>


### [473] [DRARL: Disengagement-Reason-Augmented Reinforcement Learning for Efficient Improvement of Autonomous Driving Policy](https://arxiv.org/abs/2506.16720)
*Weitao Zhou,Bo Zhang,Zhong Cao,Xiang Li,Qian Cheng,Chunyang Liu,Yaqin Zhang,Diange Yang*

Main category: cs.RO

TL;DR: This paper introduces DRARL, a method to improve driving policies in automated vehicles by analyzing disengagement cases with a reason-focused approach.


<details>
  <summary>Details</summary>
Motivation: Automated vehicles face growing issues with disengagement cases on open roads due to inherent data scarcity and mixed reasons behind interventions. Existing approaches often struggle with effective training using disengagement data.

Method: The paper proposes DRARL, which uses an out-of-distribution (OOD) state estimation model to classify disengagement reasons and selectively adjusts driving policies in a reason-augmented imagination environment.

Result: Experiments using real-world disengagement data from robo-taxis show DRARL effectively identifies policy-related reasons, enabling improved handling of original and similar disengagement situations. It also avoids overly conservative policy adjustments.

Conclusion: DRARL offers an efficient mechanism to enhance driving policy performance by analyzing and acting on disengagement cases using reason augmentation.

Abstract: With the increasing presence of automated vehicles on open roads under driver
supervision, disengagement cases are becoming more prevalent. While some
data-driven planning systems attempt to directly utilize these disengagement
cases for policy improvement, the inherent scarcity of disengagement data
(often occurring as a single instances) restricts training effectiveness.
Furthermore, some disengagement data should be excluded since the disengagement
may not always come from the failure of driving policies, e.g. the driver may
casually intervene for a while. To this end, this work proposes
disengagement-reason-augmented reinforcement learning (DRARL), which enhances
driving policy improvement process according to the reason of disengagement
cases. Specifically, the reason of disengagement is identified by a
out-of-distribution (OOD) state estimation model. When the reason doesn't
exist, the case will be identified as a casual disengagement case, which
doesn't require additional policy adjustment. Otherwise, the policy can be
updated under a reason-augmented imagination environment, improving the policy
performance of disengagement cases with similar reasons. The method is
evaluated using real-world disengagement cases collected by autonomous driving
robotaxi. Experimental results demonstrate that the method accurately
identifies policy-related disengagement reasons, allowing the agent to handle
both original and semantically similar cases through reason-augmented training.
Furthermore, the approach prevents the agent from becoming overly conservative
after policy adjustments. Overall, this work provides an efficient way to
improve driving policy performance with disengagement cases.

</details>


### [474] [A Scalable Post-Processing Pipeline for Large-Scale Free-Space Multi-Agent Path Planning with PiBT](https://arxiv.org/abs/2506.16748)
*Arjo Chakravarty,Michael X. Grey,M. A. Viraj J. Muthugala,Mohan Rajesh Elara*

Main category: cs.RO

TL;DR: The paper introduces a hybrid approach for free-space multi-agent path planning that scales to over 500 agents, combining Priority Inheritance with Backtracking (PiBT) and a novel safety-aware path smoothing method.


<details>
  <summary>Details</summary>
Motivation: The challenge of effectively planning paths for a large number of agents in free-space environments while balancing optimality and scalability.

Method: Combining Priority Inheritance with Backtracking (PiBT) and a new safety-aware path smoothing technique, including fallback collision resolution with Safe Interval Path Planning (SIPP).

Result: The method achieves real-time performance, scales to over 500 agents, and produces near-optimal trajectories in sparse domains, outperforming existing approaches.

Conclusion: The proposed framework is a viable cornerstone for scalable, real-time multi-agent path planning in free-space robotic systems.

Abstract: Free-space multi-agent path planning remains challenging at large scales.
Most existing methods either offer optimality guarantees but do not scale
beyond a few dozen agents, or rely on grid-world assumptions that do not
generalize well to continuous space. In this work, we propose a hybrid,
rule-based planning framework that combines Priority Inheritance with
Backtracking (PiBT) with a novel safety-aware path smoothing method. Our
approach extends PiBT to 8-connected grids and selectively applies
string-pulling based smoothing while preserving collision safety through local
interaction awareness and a fallback collision resolution step based on Safe
Interval Path Planning (SIPP). This design allows us to reduce overall path
lengths while maintaining real-time performance. We demonstrate that our method
can scale to over 500 agents in large free-space environments, outperforming
existing any-angle and optimal methods in terms of runtime, while producing
near-optimal trajectories in sparse domains. Our results suggest this framework
is a promising building block for scalable, real-time multi-agent navigation in
robotics systems operating beyond grid constraints.

</details>


### [475] [Learning Dexterous Object Handover](https://arxiv.org/abs/2506.16822)
*Daniel Frau-Alfaro,Julio Castaño-Amoros,Santiago Puente,Pablo Gil,Roberto Calandra*

Main category: cs.RO

TL;DR: The paper focuses on teaching robots object handover through reinforcement learning, achieving high success rates using a novel reward approach.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enable robots to perform efficient and safe object handovers, which are essential for collaborative settings like homes.

Method: Reinforcement Learning (RL) is utilized, with a novel reward function based on dual quaternions to optimize handover performance.

Result: The trained policy achieves a 94% success rate and shows robustness to new objects and dynamic handover conditions.

Conclusion: The approach demonstrates promising results for robust and practical robot object handover, even under novel and dynamic scenarios.

Abstract: Object handover is an important skill that we use daily when interacting with
other humans. To deploy robots in collaborative setting, like houses, being
able to receive and handing over objects safely and efficiently becomes a
crucial skill. In this work, we demonstrate the use of Reinforcement Learning
(RL) for dexterous object handover between two multi-finger hands. Key to this
task is the use of a novel reward function based on dual quaternions to
minimize the rotation distance, which outperforms other rotation
representations such as Euler and rotation matrices. The robustness of the
trained policy is experimentally evaluated by testing w.r.t. objects that are
not included in the training distribution, and perturbations during the
handover process. The results demonstrate that the trained policy successfully
perform this task, achieving a total success rate of 94% in the best-case
scenario after 100 experiments, thereby showing the robustness of our policy
with novel objects. In addition, the best-case performance of the policy
decreases by only 13.8% when the other robot moves during the handover, proving
that our policy is also robust to this type of perturbation, which is common in
real-world object handovers.

</details>


### [476] [Orbital Collision: An Indigenously Developed Web-based Space Situational Awareness Platform](https://arxiv.org/abs/2506.16892)
*Partha Chowdhury,Harsha M,Ayush Gupta,Sanat K Biswas*

Main category: cs.RO

TL;DR: OrCo is a web-based platform developed to predict collision probabilities of space objects using TLE data, addressing challenges of congestion in Earth's orbit.


<details>
  <summary>Details</summary>
Motivation: The increasing amount of space debris and inactive satellites in Earth's orbit raises collision risks, creating a need for enhanced Space Situational Awareness to ensure satellite safety.

Method: The study uses TLE data for orbit propagation, employs methods for orbital uncertainty calculations, and evaluates the platform's performance on accuracy and efficiency.

Result: The web platform demonstrates improved tracking of space objects, assessing collision probabilities in a reliable and efficient manner.

Conclusion: OrCo is a significant tool to address congestion in Earth's orbit, enhancing the prediction and mitigation of satellite collision risks.

Abstract: This work presents an indigenous web based platform Orbital Collision (OrCo),
created by the Space Systems Laboratory at IIIT Delhi, to enhance Space
Situational Awareness (SSA) by predicting collision probabilities of space
objects using Two Line Elements (TLE) data. The work highlights the growing
challenges of congestion in the Earth's orbital environment, mainly due to
space debris and defunct satellites, which increase collision risks. It employs
several methods for propagating orbital uncertainty and calculating the
collision probability. The performance of the platform is evaluated through
accuracy assessments and efficiency metrics, in order to improve the tracking
of space objects and ensure the safety of the satellite in congested space.

</details>


### [477] [SDDiff: Boost Radar Perception via Spatial-Doppler Diffusion](https://arxiv.org/abs/2506.16936)
*Shengpeng Wang,Xin Luo,Yulong Xie,Wei Wang*

Main category: cs.RO

TL;DR: This paper proposes a novel Spatial-Doppler Diffusion (SDDiff) model to jointly perform point cloud extraction (PCE) and ego velocity estimation (EVE), achieving significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the commonly overlooked interplay between radar's spatial and Doppler domain features in 3D radar perception, recognizing that these tasks can mutually benefit from each other.

Method: The approach uses a Spatial-Doppler Diffusion model that modifies conventional latent diffusion processes by incorporating a joint spatial and Doppler representation, directional diffusion based on radar priors, and an Iterative Doppler Refinement technique.

Result: The SDDiff model achieves a 59% improvement in EVE accuracy, a 4X increase in valid generation density, and enhances effectiveness and reliability in PCE compared to state-of-the-art methods.

Conclusion: The SDDiff model demonstrates that combining PCE and EVE in a unified framework can optimize both tasks, leveraging the interplay between spatial and Doppler features for superior radar perception performance.

Abstract: Point cloud extraction (PCE) and ego velocity estimation (EVE) are key
capabilities gaining attention in 3D radar perception. However, existing work
typically treats these two tasks independently, which may neglect the interplay
between radar's spatial and Doppler domain features, potentially introducing
additional bias. In this paper, we observe an underlying correlation between 3D
points and ego velocity, which offers reciprocal benefits for PCE and EVE. To
fully unlock such inspiring potential, we take the first step to design a
Spatial-Doppler Diffusion (SDDiff) model for simultaneously dense PCE and
accurate EVE. To seamlessly tailor it to radar perception, SDDiff improves the
conventional latent diffusion process in three major aspects. First, we
introduce a representation that embodies both spatial occupancy and Doppler
features. Second, we design a directional diffusion with radar priors to
streamline the sampling. Third, we propose Iterative Doppler Refinement to
enhance the model's adaptability to density variations and ghosting effects.
Extensive evaluations show that SDDiff significantly outperforms
state-of-the-art baselines by achieving 59% higher in EVE accuracy, 4X greater
in valid generation density while boosting PCE effectiveness and reliability.

</details>


### [478] [Learning Accurate Whole-body Throwing with High-frequency Residual Policy and Pullback Tube Acceleration](https://arxiv.org/abs/2506.16986)
*Yuntao Ma,Yang Liu,Kaixian Qu,Marco Hutter*

Main category: cs.RO

TL;DR: This paper introduces a control framework for robots to perform accurate prehensile throwing using a combination of learning and model-based techniques, surpassing human performance in hitting small targets.


<details>
  <summary>Details</summary>
Motivation: Robots require advanced manipulation capabilities like throwing to interact with objects beyond their physical arm reach in dynamic scenarios.

Method: The proposed framework integrates a nominal tracking policy for the end-effector, a high-frequency residual control policy to improve tracking precision, and an optimization-based module focusing on end-effector acceleration control.

Result: The framework achieved a 0.28 m landing error when targeting at 6 m distances and surpassed humans in tracking accuracy and success rates in hitting small targets within 3-5 m.

Conclusion: The system demonstrates the feasibility and accuracy of prehensile throwing in legged robots, pushing forward the boundaries of dynamic whole-body manipulation.

Abstract: Throwing is a fundamental skill that enables robots to manipulate objects in
ways that extend beyond the reach of their arms. We present a control framework
that combines learning and model-based control for prehensile whole-body
throwing with legged mobile manipulators. Our framework consists of three
components: a nominal tracking policy for the end-effector, a high-frequency
residual policy to enhance tracking accuracy, and an optimization-based module
to improve end-effector acceleration control. The proposed controller achieved
the average of 0.28 m landing error when throwing at targets located 6 m away.
Furthermore, in a comparative study with university students, the system
achieved a velocity tracking error of 0.398 m/s and a success rate of 56.8%,
hitting small targets randomly placed at distances of 3-5 m while throwing at a
specified speed of 6 m/s. In contrast, humans have a success rate of only
15.2%. This work provides an early demonstration of prehensile throwing with
quantified accuracy on hardware, contributing to progress in dynamic whole-body
manipulation.

</details>


### [479] [Monocular One-Shot Metric-Depth Alignment for RGB-Based Robot Grasping](https://arxiv.org/abs/2506.17110)
*Teng Guo,Baichuan Huang,Jingjin Yu*

Main category: cs.RO

TL;DR: The paper introduces MOMA, a framework enabling accurate metric depth recovery for 6D object pose estimation from a single RGB image.


<details>
  <summary>Details</summary>
Motivation: Current 6D pose estimation techniques face challenges such as reliance on expensive, noisy depth sensors that struggle with transparent objects. Monocular Depth Estimation Models (MDEMs) provide a potential alternative but lack generalization and true metric depth reliability.

Method: The proposed framework, MOMA, enhances monocular depth estimation by performing scale-rotation-shift alignments using sparse ground-truth depth points during camera calibration. MOMA also supports fine-tuning MDEMs for transparent objects.

Result: MOMA demonstrated high success rates in real-world tasks, specifically in tabletop grasping and suction-based bin-picking scenarios, without requiring additional data collection or retraining.

Conclusion: MOMA offers a cost-effective and versatile solution for accurate depth estimation in robotics, addressing limitations of traditional methods and improving adaptability to diverse environments.

Abstract: Accurate 6D object pose estimation is a prerequisite for successfully
completing robotic prehensile and non-prehensile manipulation tasks. At
present, 6D pose estimation for robotic manipulation generally relies on depth
sensors based on, e.g., structured light, time-of-flight, and stereo-vision,
which can be expensive, produce noisy output (as compared with RGB cameras),
and fail to handle transparent objects. On the other hand, state-of-the-art
monocular depth estimation models (MDEMs) provide only affine-invariant depths
up to an unknown scale and shift. Metric MDEMs achieve some successful
zero-shot results on public datasets, but fail to generalize. We propose a
novel framework, Monocular One-shot Metric-depth Alignment (MOMA), to recover
metric depth from a single RGB image, through a one-shot adaptation building on
MDEM techniques. MOMA performs scale-rotation-shift alignments during camera
calibration, guided by sparse ground-truth depth points, enabling accurate
depth estimation without additional data collection or model retraining on the
testing setup. MOMA supports fine-tuning the MDEM on transparent objects,
demonstrating strong generalization capabilities. Real-world experiments on
tabletop 2-finger grasping and suction-based bin-picking applications show MOMA
achieves high success rates in diverse tasks, confirming its effectiveness.

</details>


### [480] [Judo: A User-Friendly Open-Source Package for Sampling-Based Model Predictive Control](https://arxiv.org/abs/2506.17184)
*Albert H. Li,Brandon Hung,Aaron D. Ames,Jiuguang Wang,Simon Le Cleac'h,Preston Culbertson*

Main category: cs.RO

TL;DR: This paper introduces Judo, a Python software package for prototyping and deploying sampling-based MPC controllers, validated for real-time performance.


<details>
  <summary>Details</summary>
Motivation: The resurgence in sampling-based MPC in robotics necessitates common tools for prototyping, evaluation, and deployment.

Method: The authors designed Judo with robust algorithms, benchmark tasks, asynchronous execution, and GUI interactivity, leveraging MuJoCo for real-time performance.

Result: Judo successfully facilitates rapid prototyping and real-time controller performance across various hardware.

Conclusion: Judo addresses a gap in robotics tools, offering a comprehensive and user-friendly solution for sampling-based MPC development and deployment.

Abstract: Recent advancements in parallel simulation and successful robotic
applications are spurring a resurgence in sampling-based model predictive
control. To build on this progress, however, the robotics community needs
common tooling for prototyping, evaluating, and deploying sampling-based
controllers. We introduce Judo, a software package designed to address this
need. To facilitate rapid prototyping and evaluation, Judo provides robust
implementations of common sampling-based MPC algorithms and standardized
benchmark tasks. It further emphasizes usability with simple but extensible
interfaces for controller and task definitions, asynchronous execution for
straightforward simulation-to-hardware transfer, and a highly customizable
interactive GUI for tuning controllers interactively. While written in Python,
the software leverages MuJoCo as its physics backend to achieve real-time
performance, which we validate across both consumer and server-grade hardware.
Code at https://github.com/bdaiinstitute/judo.

</details>


### [481] [Dex1B: Learning with 1B Demonstrations for Dexterous Manipulation](https://arxiv.org/abs/2506.17198)
*Jianglong Ye,Keyi Wang,Chengjing Yuan,Ruihan Yang,Yiquan Li,Jiyue Zhu,Yuzhe Qin,Xueyan Zou,Xiaolong Wang*

Main category: cs.RO

TL;DR: This paper introduces Dex1B, a dataset with one billion demonstrations for dexterous hand manipulation, created using a novel generative model.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of generating large-scale and high-quality demonstrations for dexterous hand manipulation tasks.

Method: Developed a generative model with geometric constraints for feasibility and additional diversity conditions to create the Dex1B dataset.

Result: Dex1B significantly outperforms state-of-the-art methods on simulation benchmarks and proves effective in real-world robot experiments.

Conclusion: The Dex1B dataset sets a new standard for diverse and high-quality demonstrations, greatly advancing the capabilities in robotic dexterous hand manipulation.

Abstract: Generating large-scale demonstrations for dexterous hand manipulation remains
challenging, and several approaches have been proposed in recent years to
address this. Among them, generative models have emerged as a promising
paradigm, enabling the efficient creation of diverse and physically plausible
demonstrations. In this paper, we introduce Dex1B, a large-scale, diverse, and
high-quality demonstration dataset produced with generative models. The dataset
contains one billion demonstrations for two fundamental tasks: grasping and
articulation. To construct it, we propose a generative model that integrates
geometric constraints to improve feasibility and applies additional conditions
to enhance diversity. We validate the model on both established and newly
introduced simulation benchmarks, where it significantly outperforms prior
state-of-the-art methods. Furthermore, we demonstrate its effectiveness and
robustness through real-world robot experiments. Our project page is at
https://jianglongye.com/dex1b

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [482] [How Do Community Smells Influence Self-Admitted Technical Debt in Machine Learning Projects?](https://arxiv.org/abs/2506.15884)
*Shamse Tasnim Cynthia,Nuri Almarimi,Banani Roy*

Main category: cs.SE

TL;DR: The study investigates the prevalence and correlation of community smells with self-admitted technical debt (SATD) in ML-based open-source systems.


<details>
  <summary>Details</summary>
Motivation: The paper aims to uncover the interplay between community smells and SATD specific to ML-based projects to improve quality and sustainability.

Method: The researchers analyzed community smells, SATD, and their correlations across 155 ML projects and evaluated the patterns and trajectories at the release level.

Result: Community smells are widespread in ML projects, with some strongly correlated with SATD. Patterns depend on project size, with communication- and authority-related smells linked to code and design debt.

Conclusion: Early detection and mitigation of socio-technical issues are crucial for maintaining the quality and sustainability of ML-based projects.

Abstract: Community smells reflect poor organizational practices that often lead to
socio-technical issues and the accumulation of Self-Admitted Technical Debt
(SATD). While prior studies have explored these problems in general software
systems, their interplay in machine learning (ML)-based projects remains
largely underexamined. In this study, we investigated the prevalence of
community smells and their relationship with SATD in open-source ML projects,
analyzing data at the release level. First, we examined the prevalence of ten
community smell types across the releases of 155 ML-based systems and found
that community smells are widespread, exhibiting distinct distribution patterns
across small, medium, and large projects. Second, we detected SATD at the
release level and applied statistical analysis to examine its correlation with
community smells. Our results showed that certain smells, such as Radio Silence
and Organizational Silos, are strongly correlated with higher SATD occurrences.
Third, we considered the six identified types of SATD to determine which
community smells are most associated with each debt category. Our analysis
revealed authority- and communication-related smells often co-occur with
persistent code and design debt. Finally, we analyzed how the community smells
and SATD evolve over the releases, uncovering project size-dependent trends and
shared trajectories. Our findings emphasize the importance of early detection
and mitigation of socio-technical issues to maintain the long-term quality and
sustainability of ML-based systems.

</details>


### [483] [Regression Testing Optimization for ROS-based Autonomous Systems: A Comprehensive Review of Techniques](https://arxiv.org/abs/2506.16101)
*Yupeng Jiang,Shuaiyi Sun,Xi Zheng*

Main category: cs.SE

TL;DR: This paper surveys regression testing optimization techniques specifically for ROS-based autonomous systems (ROSAS), categorizing studies, highlighting challenges, and suggesting future research directions.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address unique challenges in regression testing for ROSAS, which arise from their dynamic behaviors, complex architectures, and safety-critical requirements.

Method: The authors conducted a systematic review of 122 studies, categorized the techniques into prioritization, minimization, and selection methods, and introduced a taxonomy tailored to ROSAS.

Result: The study identified the applicability and limitations of existing techniques in ROSAS contexts and outlined major challenges in optimizing regression testing.

Conclusion: The paper concludes by providing future research directions, emphasizing techniques such as coverage metrics, foundation models, and neurosymbolic reasoning to advance regression testing for ROSAS.

Abstract: Regression testing plays a critical role in maintaining software reliability,
particularly for ROS-based autonomous systems (ROSAS), which frequently undergo
continuous integration and iterative development. However, conventional
regression testing techniques face significant challenges when applied to
autonomous systems due to their dynamic and non-deterministic behaviors,
complex multi-modal sensor data, asynchronous distributed architectures, and
stringent safety and real-time constraints. Although numerous studies have
explored test optimization in traditional software contexts, regression testing
optimization specifically for ROSAS remains largely unexplored. To address this
gap, we present the first comprehensive survey systematically reviewing
regression testing optimization techniques tailored for ROSAS. We analyze and
categorize 122 representative studies into regression test case prioritization,
minimization, and selection methods. A structured taxonomy is introduced to
clearly illustrate their applicability and limitations within ROSAS contexts.
Furthermore, we highlight major challenges specific to regression testing for
ROSAS, including effectively prioritizing tests in response to frequent system
modifications, efficiently minimizing redundant tests, and difficulty in
accurately selecting impacted test cases. Finally, we propose research insights
and identify promising future directions, such as leveraging frame-to-vector
coverage metrics, multi-source foundation models, and neurosymbolic reasoning
to enhance regression testing efficiency and effectiveness. This survey
provides a foundational reference and practical roadmap for advancing the
state-of-the-art in regression testing optimization for ROSAS.

</details>


### [484] [Seeing is Fixing: Cross-Modal Reasoning with Multimodal LLMs for Visual Software Issue Fixing](https://arxiv.org/abs/2506.16136)
*Kai Huang,Jian Zhang,Xiaofei Xie,Chunyang Chen*

Main category: cs.SE

TL;DR: GUIRepair, a novel approach for automated program repair in multimodal scenarios, integrates visual and textual cues to outperform current systems on SWE-bench M benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing program repair systems struggle with multimodal scenarios due to their inability to process and reason with visual information from graphical user interfaces. Addressing this limitation is necessary to build more robust automated repair systems.

Method: GUIRepair introduces two core components, Image2Code and Code2Image. Image2Code generates contextual code related to visual issues by analyzing GUI images, while Code2Image validates fixes by rendering patched GUIs to assess the visual resolution of issues.

Result: GUIRepair shows enhanced effectiveness on the SWE-bench M dataset, solving significantly more instances compared to both open-source and commercial baselines, especially when leveraging the o4-mini base model.

Conclusion: The introduction of cross-modal reasoning and integration of visual data makes GUIRepair a more effective solution for multimodal program repair challenges, setting a new direction in automated program repair research.

Abstract: Large language model-(LLM) based automated program repair (APR) techniques
have shown promising results in resolving real-world GitHub issue tasks.
Existing APR systems are primarily evaluated in unimodal settings (e.g.,
SWE-bench). However, these autonomous systems struggle to resolve multimodal
problem scenarios (e.g., SWE-bench M) due to limitations in interpreting and
leveraging visual information. In multimodal scenarios, LLMs need to rely on
visual information in the graphical user interface (GUI) to understand bugs and
generate fixes. To bridge this gap, we propose GUIRepair, a cross-modal
reasoning approach for resolving multimodal issue scenarios by understanding
and capturing visual information. Specifically, GUIRepair integrates two key
components, Image2Code and Code2Image, to enhance fault comprehension and patch
validation. Image2Code extracts relevant project documents based on the issue
report, then applies this domain knowledge to generate the reproduced code
responsible for the visual symptoms, effectively translating GUI images into
executable context for better fault comprehension. Code2Image replays the
visual issue scenario using the reproduced code and captures GUI renderings of
the patched program to assess whether the fix visually resolves the issue,
providing feedback for patch validation. We evaluate GUIRepair on SWE-bench M,
and the approach demonstrates significant effectiveness. When utilizing GPT-4o
as the base model, GUIRepair solves 157 instances, outperforming the best
open-source baseline by 26 instances. Furthermore, when using o4-mini as the
base model, GUIRepair can achieve even better results and solve 175 instances,
outperforming the top commercial system by 22 instances. This emphasizes the
success of our new perspective on incorporating cross-modal reasoning by
understanding and capturing visual information to resolve multimodal issues.

</details>


### [485] [The Technical Debt Gamble: A Case Study on Technical Debt in a Large-Scale Industrial Microservice Architecture](https://arxiv.org/abs/2506.16214)
*Klara Borowa,Andrzej Ratkowski,Roberto Verdecchia*

Main category: cs.SE

TL;DR: Microservice architecture's maintainability is jeopardized by technical debt (TD), which they study in a large-scale system using quantitative and qualitative methods. Results highlight TD detection through static analysis, communication issues, and rapid TD cycles.


<details>
  <summary>Details</summary>
Motivation: To analyze how technical debt (TD) affects large-scale microservice systems and identify effective strategies for its management.

Method: They conducted a mixed-method case study using static code analyzers for quantitative data and qualitative insights from developer discussions and an architect interview.

Result: Four key findings: static code analysis as an effective TD discovery tool, communication issues as TD contributors, architectural-organizational misalignments exacerbating TD, and the rapid accumulation/resolution cycle called 'microservice architecture technical debt gamble.'

Conclusion: The study highlights technical debt challenges in microservice architectures and proposes targeted management strategies to enhance maintainability and system quality.

Abstract: Microservice architectures provide an intuitive promise of high
maintainability and evolvability due to loose coupling. However, these quality
attributes are notably vulnerable to technical debt (TD). Few studies address
TD in microservice systems, particularly on a large scale. This research
explores how TD manifests in a large-scale microservice-based industrial
system. The research is based on a mixed-method case study of a project
including over 100 microservices and serving over 15k locations. Results are
collected via a quantitative method based static code analyzers combined with
qualitative insights derived from a focus group discussion with the development
team and a follow-up interview with the lead architect of the case study
system. Results show that (1) simple static source code analysis can be an
efficient and effective entry point for holistic TD discovery, (2) inadequate
communication significantly contributes to TD, (3) misalignment between
architectural and organizational structures can exacerbate TD accumulation, (4)
microservices can rapidly cycle through TD accumulation and resolution, a
phenomenon referred to as "microservice architecture technical debt gamble".
Finally, we identify a set of fitting strategies for TD management in
microservice architectures.

</details>


### [486] [Evaluating the Use of LLMs for Documentation to Code Traceability](https://arxiv.org/abs/2506.16440)
*Ebube Alor,SayedHassan Khatoonabadi,Emad Shihab*

Main category: cs.SE

TL;DR: The paper evaluates the performance of large language models in identifying trace links between software documentation and source code, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: There is a need to automate traceability between software documentation and source code to improve efficiency and accuracy in software engineering.

Method: The authors conducted systematic experiments with three LLMs on two novel datasets, assessing three main capabilities: trace link identification accuracy, explanation quality, and multi-step chain reconstruction.

Result: The best-performing LLM achieved F1-scores of 79.4% and 80.4% on two datasets while outperforming baselines like TF-IDF, BM25, and CodeBERT, but exhibited errors such as naming assumptions and phantom links.

Conclusion: LLMs are effective in trace discovery but require additional human-in-the-loop systems due to limitations like overgeneralization and error-prone patterns.

Abstract: Large Language Models (LLMs) offer new potential for automating
documentation-to-code traceability, yet their capabilities remain
underexplored. We present a comprehensive evaluation of LLMs (Claude 3.5
Sonnet, GPT-4o, and o3-mini) in establishing trace links between various
software documentation (including API references and user guides) and source
code. We create two novel datasets from two open-source projects (Unity Catalog
and Crawl4AI). Through systematic experiments, we assess three key
capabilities: (1) trace link identification accuracy, (2) relationship
explanation quality, and (3) multi-step chain reconstruction. Results show that
the best-performing LLM achieves F1-scores of 79.4% and 80.4% across the two
datasets, substantially outperforming our baselines (TF-IDF, BM25, and
CodeBERT). While fully correct relationship explanations range from 42.9% to
71.1%, partial accuracy exceeds 97%, indicating that fundamental connections
are rarely missed. For multi-step chains, LLMs maintain high endpoint accuracy
but vary in capturing precise intermediate links. Error analysis reveals that
many false positives stem from naming-based assumptions, phantom links, or
overgeneralization of architectural patterns. We demonstrate that task-framing,
such as a one-to-many matching strategy, is critical for performance. These
findings position LLMs as powerful assistants for trace discovery, but their
limitations could necessitate human-in-the-loop tool design and highlight
specific error patterns for future research.

</details>


### [487] [Understanding the Challenges and Promises of Developing Generative AI Apps: An Empirical Study](https://arxiv.org/abs/2506.16453)
*Buthayna AlMulla,Maram Assi,Safwat Hassan*

Main category: cs.SE

TL;DR: The paper analyzes user reviews of generative AI apps using a systematic methodology to uncover key discussion topics and evolving user sentiments.


<details>
  <summary>Details</summary>
Motivation: To better understand how end users perceive and evaluate generative AI functionalities in mobile applications.

Method: SARA: A four-phase methodology (Selection, Acquisition, Refinement, and Analysis) leveraging prompt-based LLM techniques to systematically extract and analyze user reviews.

Result: 91% topic extraction accuracy achieved through five-shot prompting. Identification of top 10 user-discussed topics along with evolving trends over time.

Conclusion: The study provides actionable insights for developers and researchers, addressing user challenges and identifying emerging opportunities in generative AI apps.

Abstract: The release of ChatGPT in 2022 triggered a rapid surge in generative
artificial intelligence mobile apps (i.e., Gen-AI apps). Despite widespread
adoption, little is known about how end users perceive and evaluate these
Gen-AI functionalities in practice. In this work, we conduct a user-centered
analysis of 676,066 reviews from 173 Gen-AI apps on the Google Play Store. We
introduce a four-phase methodology, SARA (Selection, Acquisition, Refinement,
and Analysis), that enables the systematic extraction of user insights using
prompt-based LLM techniques. First, we demonstrate the reliability of LLMs in
topic extraction, achieving 91% accuracy through five-shot prompting and
non-informative review filtering. Then, we apply this method to the informative
reviews, identify the top 10 user-discussed topics (e.g., AI Performance,
Content Quality, and Content Policy & Censorship) and analyze the key
challenges and emerging opportunities. Finally, we examine how these topics
evolve over time, offering insight into shifting user expectations and
engagement patterns with Gen-AI apps. Based on our findings and observations,
we present actionable implications for developers and researchers.

</details>


### [488] [Scaling GR(1) Synthesis via a Compositional Framework for LTL Discrete Event Control](https://arxiv.org/abs/2506.16557)
*Hernán Gagliardi,Victor Braberman,Sebastian Uchitel*

Main category: cs.SE

TL;DR: This paper introduces a modular approach to discrete event system controller synthesis with linear temporal logic (LTL) goals to tackle state explosion issues.


<details>
  <summary>Details</summary>
Motivation: The motivation lies in addressing the state explosion problem faced by monolithic controller synthesis approaches for discrete event systems with LTL goals.

Method: The approach involves breaking down the plant into modular subsets, solving weaker control problems iteratively for these subsets, and using observational synthesis equivalence to abstract local events. The approach is implemented in the MTSA tool for an LTL subset called GR(1).

Result: The proposed method generates solutions for systems up to 1000 times larger than what monolithic methods can handle, demonstrating its scalability and effectiveness.

Conclusion: The compositional approach mitigates state explosion, is scalable, and produces a set of controllers that collectively fulfill the LTL goals when executed in parallel.

Abstract: We present a compositional approach to controller synthesis of discrete event
system controllers with linear temporal logic (LTL) goals. We exploit the
modular structure of the plant to be controlled, given as a set of labelled
transition systems (LTS), to mitigate state explosion that monolithic
approaches to synthesis are prone to. Maximally permissive safe controllers are
iteratively built for subsets of the plant LTSs by solving weaker control
problems. Observational synthesis equivalence is used to reduce the size of the
controlled subset of the plant by abstracting away local events. The result of
synthesis is also compositional, a set of controllers that when run in parallel
ensure the LTL goal. We implement synthesis in the MTSA tool for an expressive
subset of LTL, GR(1), and show it computes solutions to that can be up to 1000
times larger than those that the monolithic approach can solve.

</details>


### [489] [AI-Driven Tools in Modern Software Quality Assurance: An Assessment of Benefits, Challenges, and Future Directions](https://arxiv.org/abs/2506.16586)
*Ihor Pysmennyi,Roman Kyslyi,Kyrylo Kleshch*

Main category: cs.SE

TL;DR: The paper explores integrating AI tools into software QA processes for modern systems, showcasing potential benefits alongside practical challenges.


<details>
  <summary>Details</summary>
Motivation: Traditional QA methods struggle with complexity, scale, rapid cycles, and resource constraints in modern software systems, driving the need for improvement.

Method: The study analyzed effects on verification and validation processes by implementing AI-based tools across multiple testing aspects and conducted end-to-end regression tests as a proof of concept.

Result: Findings revealed an 8.3% flaky execution rate for AI-generated test cases, demonstrating significant promise but also identifying challenges like explainability and semantic coverage issues.

Conclusion: AI has transformative potential for quality assurance, but strategic implementation is essential to address its limitations and ensure effective adoption.

Abstract: Traditional quality assurance (QA) methods face significant challenges in
addressing the complexity, scale, and rapid iteration cycles of modern software
systems and are strained by limited resources available, leading to substantial
costs associated with poor quality. The object of this research is the Quality
Assurance processes for modern distributed software applications. The subject
of the research is the assessment of the benefits, challenges, and prospects of
integrating modern AI-oriented tools into quality assurance processes. We
performed comprehensive analysis of implications on both verification and
validation processes covering exploratory test analyses, equivalence
partitioning and boundary analyses, metamorphic testing, finding
inconsistencies in acceptance criteria (AC), static analyses, test case
generation, unit test generation, test suit optimization and assessment, end to
end scenario execution. End to end regression of sample enterprise application
utilizing AI-agents over generated test scenarios was implemented as a proof of
concept highlighting practical use of the study. The results, with only 8.3%
flaky executions of generated test cases, indicate significant potential for
the proposed approaches. However, the study also identified substantial
challenges for practical adoption concerning generation of semantically
identical coverage, "black box" nature and lack of explainability from
state-of-the-art Large Language Models (LLMs), the tendency to correct mutated
test cases to match expected results, underscoring the necessity for thorough
verification of both generated artifacts and test execution results. The
research demonstrates AI's transformative potential for QA but highlights the
importance of a strategic approach to implementing these technologies,
considering the identified limitations and the need for developing appropriate
verification methodologies.

</details>


### [490] [LLM-based Satisfiability Checking of String Requirements by Consistent Data and Checker Generation](https://arxiv.org/abs/2506.16639)
*Boqi Chen,Aren A. Babikian,Shuzhao Feng,Dániel Varró,Gunter Mussbacher*

Main category: cs.SE

TL;DR: This paper develops a hybrid approach employing large language models (LLMs) to verify the satisfiability and correctness of natural language (NL) requirements over strings using generated formal checkers.


<details>
  <summary>Details</summary>
Motivation: Verifying NL requirements over strings is complex due to the theoretical limitations of formal approaches and the manual effort needed to translate NL into formal constraints. The paper seeks to explore the potential of LLMs for this task.

Method: A hybrid approach uses LLMs to (1) infer satisfiability outcomes and generate consistent strings (if possible), and (2) produce both SMT and Python-based checkers to validate these results. This process is experimentally evaluated using four LLMs.

Result: The study shows that LLMs can effectively translate NL requirements into formal checkers, achieving perfect accuracy for Python checkers. The generated checkers significantly aid in both finding consistent strings and detecting unsatisfiable requirements, resulting in a doubled generation success rate and enhanced F1-score in certain scenarios.

Conclusion: LLMs, supplemented with generated formal checkers, offer an effective solution for verifying NL requirements over strings, improving both efficiency and accuracy compared to traditional methods.

Abstract: Requirements over strings, commonly represented using natural language (NL),
are particularly relevant for software systems due to their heavy reliance on
string data manipulation. While individual requirements can usually be analyzed
manually, verifying properties (e.g., satisfiability) over sets of NL
requirements is particularly challenging. Formal approaches (e.g., SMT solvers)
may efficiently verify such properties, but are known to have theoretical
limitations. Additionally, the translation of NL requirements into formal
constraints typically requires significant manual effort. Recently, large
language models (LLMs) have emerged as an alternative approach for formal
reasoning tasks, but their effectiveness in verifying requirements over strings
is less studied. In this paper, we introduce a hybrid approach that verifies
the satisfiability of NL requirements over strings by using LLMs (1) to derive
a satisfiability outcome (and a consistent string, if possible), and (2) to
generate declarative (i.e., SMT) and imperative (i.e., Python) checkers, used
to validate the correctness of (1). In our experiments, we assess the
performance of four LLMs. Results show that LLMs effectively translate natural
language into checkers, even achieving perfect testing accuracy for
Python-based checkers. These checkers substantially help LLMs in generating a
consistent string and accurately identifying unsatisfiable requirements,
leading to more than doubled generation success rate and F1-score in certain
cases compared to baselines without generated checkers.

</details>


### [491] [SemAgent: A Semantics Aware Program Repair Agent](https://arxiv.org/abs/2506.16650)
*Anvith Pabba,Alex Mathai,Anindya Chakraborty,Baishakhi Ray*

Main category: cs.SE

TL;DR: Current systems for Automated Program Repair (APR) overly focus on fixing only the local lines of code directly affected by issues, which can lead to overfitted patches. SemAgent introduces a workflow-based method leveraging issue, code, and execution semantics to provide complete and generalizable patches.


<details>
  <summary>Details</summary>
Motivation: Existing APR systems lack deep semantic understanding, often over-localizing on problems and generating overfitted patches instead of general fixes.

Method: SemAgent uses a novel pipeline that integrates execution semantics, issue abstraction, and code-context isolation. It operates in two stages: a repair stage for proposing fixes and a reviewer stage for filtering semantically relevant ones.

Result: SemAgent achieved a 44.66% solve rate on the SWEBench-Lite benchmark, surpassing other methods. It particularly excels at issues requiring multi-line reasoning and edge-case handling.

Conclusion: Incorporating deeper issue, code, and execution semantics into APR pipelines enhances their robustness and ability to generate semantically coherent and complete repairs.

Abstract: Large Language Models (LLMs) have shown impressive capabilities in downstream
software engineering tasks such as Automated Program Repair (APR). In
particular, there has been a lot of research on repository-level
issue-resolution benchmarks such as SWE-Bench. Although there has been
significant progress on this topic, we notice that in the process of solving
such issues, existing agentic systems tend to hyper-localize on immediately
suspicious lines of code and fix them in isolation, without a deeper
understanding of the issue semantics, code semantics, or execution semantics.
Consequently, many existing systems generate patches that overfit to the user
issue, even when a more general fix is preferable. To address this limitation,
we introduce SemAgent, a novel workflow-based procedure that leverages issue,
code, and execution semantics to generate patches that are complete -
identifying and fixing all lines relevant to the issue. We achieve this through
a novel pipeline that (a) leverages execution semantics to retrieve relevant
context, (b) comprehends issue-semantics via generalized abstraction, (c)
isolates code-semantics within the context of this abstraction, and (d)
leverages this understanding in a two-stage architecture: a repair stage that
proposes fine-grained fixes, followed by a reviewer stage that filters relevant
fixes based on the inferred issue-semantics. Our evaluations show that our
methodology achieves a solve rate of 44.66% on the SWEBench-Lite benchmark
beating all other workflow-based approaches, and an absolute improvement of
7.66% compared to our baseline, which lacks such deep semantic understanding.
We note that our approach performs particularly well on issues requiring
multi-line reasoning (and editing) and edge-case handling, suggesting that
incorporating issue and code semantics into APR pipelines can lead to robust
and semantically consistent repairs.

</details>


### [492] [LLMs in Coding and their Impact on the Commercial Software Engineering Landscape](https://arxiv.org/abs/2506.16653)
*Vladislav Belozerov,Peter J Barclay,Askhan Sami*

Main category: cs.SE

TL;DR: AI coding tools are widely used but pose risks such as privacy leaks, security flaws, and sycophantic responses.


<details>
  <summary>Details</summary>
Motivation: To ensure the safe and effective use of AI coding tools amidst rising concerns over privacy and security flaws.

Method: Assess the risks posed by AI tools, recommend tagging/reviews of code, private deployments, following regulations, and sycophantic test addition.

Result: Highlight the prevalence of private data leakage (10%) and security flaws (42%), with AI sycophancy documented as a key issue.

Conclusion: Firms must adopt stricter safety practices to balance the benefits of AI coding tools with security and accuracy demands.

Abstract: Large-language-model coding tools are now mainstream in software engineering.
But as these same tools move human effort up the development stack, they
present fresh dangers: 10% of real prompts leak private data, 42% of generated
snippets hide security flaws, and the models can even ``agree'' with wrong
ideas, a trait called sycophancy. We argue that firms must tag and review every
AI-generated line of code, keep prompts and outputs inside private or
on-premises deployments, obey emerging safety regulations, and add tests that
catch sycophantic answers -- so they can gain speed without losing security and
accuracy.

</details>


### [493] [Accountability of Robust and Reliable AI-Enabled Systems: A Preliminary Study and Roadmap](https://arxiv.org/abs/2506.16831)
*Filippo Scaramuzza,Damian A. Tamburri,Willem-Jan van den Heuvel*

Main category: cs.SE

TL;DR: The paper examines the robustness, reliability, and accountability of AI systems, emphasizing their importance for safety and trust, and suggests future research directions.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by the need to ensure safety, effectiveness, and trust in AI-enabled systems, particularly emphasizing accountability as a key factor.

Method: The paper explores evolving definitions of robustness, reliability, and accountability through literature review and a real-world case study.

Result: Major challenges and approaches in testing AI systems are highlighted, along with a demonstration of applications and gaps in current solutions.

Conclusion: Robustness, reliability, and accountability are identified as essential for trustworthy AI development, with future work needed to address gaps and develop innovative solutions.

Abstract: This vision paper presents initial research on assessing the robustness and
reliability of AI-enabled systems, and key factors in ensuring their safety and
effectiveness in practical applications, including a focus on accountability.
By exploring evolving definitions of these concepts and reviewing current
literature, the study highlights major challenges and approaches in the field.
A case study is used to illustrate real-world applications, emphasizing the
need for innovative testing solutions. The incorporation of accountability is
crucial for building trust and ensuring responsible AI development. The paper
outlines potential future research directions and identifies existing gaps,
positioning robustness, reliability, and accountability as vital areas for the
development of trustworthy AI systems of the future.

</details>


### [494] [Revolutionizing Validation and Verification: Explainable Testing Methodologies for Intelligent Automotive Decision-Making Systems](https://arxiv.org/abs/2506.16876)
*Halit Eris,Stefan Wagner*

Main category: cs.SE

TL;DR: The paper proposes a framework to integrate explainability and transparency into the validation and verification (V&V) processes of Autonomous Driving Systems, involving methods like large language models and simulation environments.


<details>
  <summary>Details</summary>
Motivation: Ensure safety and reliability in Autonomous Driving Systems by overcoming limitations of traditional inefficient and manual testing processes.

Method: Refine V&V requirements via literature and stakeholder inputs, use large language models to generate scenarios, and enable real-time validation using simulation tools with components like test oracle, explanation generation, and a chatbot.

Result: Empirical studies are planned to evaluate how the proposed framework improves diagnostic efficiency and transparency.

Conclusion: The framework aims to make V&V more efficient, reduce resources required, and build greater trust in autonomous driving technologies.

Abstract: Autonomous Driving Systems (ADS) use complex decision-making (DM) models with
multimodal sensory inputs, making rigorous validation and verification (V&V)
essential for safety and reliability. These models pose challenges in
diagnosing failures, tracing anomalies, and maintaining transparency, with
current manual testing methods being inefficient and labor-intensive. This
vision paper presents a methodology that integrates explainability,
transparency, and interpretability into V&V processes. We propose refining V&V
requirements through literature reviews and stakeholder input, generating
explainable test scenarios via large language models (LLMs), and enabling
real-time validation in simulation environments. Our framework includes test
oracle, explanation generation, and a test chatbot, with empirical studies
planned to evaluate improvements in diagnostic efficiency and transparency. Our
goal is to streamline V&V, reduce resources, and build user trust in autonomous
technologies.

</details>


### [495] [Quantum Optimization for Software Engineering: A Survey](https://arxiv.org/abs/2506.16878)
*Man Zhang,Yuechen Li,Tao Yue,Kai-Yuan Cai*

Main category: cs.SE

TL;DR: Quantum optimization is advancing towards practical applications, especially in Software Engineering (SE), through systematic literature review uncovering concentrated research areas and gaps.


<details>
  <summary>Details</summary>
Motivation: The increasing complexity of modern software systems and engineering processes calls for innovative solutions, such as leveraging quantum algorithms in SE optimization.

Method: A systematic literature review (SLR) analyzing 77 primary studies from an initial pool of 2083 publications sourced from six digital databases.

Result: The review highlights research focus on SE operations and software testing, while exposing gaps in other SE activities. It also identifies relevant works published outside traditional SE venues.

Conclusion: The findings can assist the Search-Based Software Engineering (SBSE) community in utilizing quantum advancements to tackle next-generation SE challenges.

Abstract: Quantum computing, particularly in the area of quantum optimization, is
steadily progressing toward practical applications, supported by an expanding
range of hardware platforms and simulators. While Software Engineering (SE)
optimization has a strong foundation, which is exemplified by the active
Search-Based Software Engineering (SBSE) community and numerous classical
optimization methods, the growing complexity of modern software systems and
their engineering processes demands innovative solutions. This Systematic
Literature Review (SLR) focuses specifically on studying the literature that
applies quantum or quantum-inspired algorithms to solve classical SE
optimization problems. We examine 77 primary studies selected from an initial
pool of 2083 publications obtained through systematic searches of six digital
databases using carefully crafted search strings. Our findings reveal
concentrated research efforts in areas such as SE operations and software
testing, while exposing significant gaps across other SE activities.
Additionally, the SLR uncovers relevant works published outside traditional SE
venues, underscoring the necessity of this comprehensive review. Overall, our
study provides a broad overview of the research landscape, empowering the SBSE
community to leverage quantum advancements in addressing next-generation SE
challenges.

</details>


### [496] [Identifying Explanation Needs: Towards a Catalog of User-based Indicators](https://arxiv.org/abs/2506.16997)
*Hannah Deters,Laura Reinhardt,Jakob Droste,Martin Obaidi,Kurt Schneider*

Main category: cs.SE

TL;DR: This paper identifies user behavior, system events, and emotional or physical reactions as indicators to determine when explanations are needed in complex software systems.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of identifying individual needs for explanation in complex software systems while mitigating biases.

Method: Conducted an online study to collect and analyze self-reported indicators related to the need for explanations, resulting in a catalog of 39 indicators.

Result: Developed a catalog with 39 indicators (17 on user behavior, 8 on system events, and 14 on emotional or physical reactions) and analyzed their relationships with types of explanation needs.

Conclusion: The indicators can improve the elicitation process of explanation needs, optimize deployed applications using telemetry, and provide real-time explanations during runtime.

Abstract: In today's digitalized world, where software systems are becoming
increasingly ubiquitous and complex, the quality aspect of explainability is
gaining relevance. A major challenge in achieving adequate explanations is the
elicitation of individual explanation needs, as it may be subject to severe
hypothetical or confirmation biases. To address these challenges, we aim to
establish user-based indicators concerning user behavior or system events that
can be captured at runtime to determine when a need for explanations arises. In
this work, we conducted explorative research in form of an online study to
collect self-reported indicators that could indicate a need for explanation. We
compiled a catalog containing 17 relevant indicators concerning user behavior,
8 indicators concerning system events and 14 indicators concerning emotional
states or physical reactions. We also analyze the relationships between these
indicators and different types of need for explanation. The established
indicators can be used in the elicitation process through prototypes, as well
as after publication to gather requirements from already deployed applications
using telemetry and usage data. Moreover, these indicators can be used to
trigger explanations at appropriate moments during the runtime.

</details>


### [497] [Behavior Driven Development for 3D Games](https://arxiv.org/abs/2506.17057)
*Fernando Pastor Ricós,Beatriz Marín,I. S. W. B. Prasetya,Tanja E. J. Vos,Joseph Davidson,Karel Hovorka*

Main category: cs.SE

TL;DR: This paper discusses integrating the iv4XR framework with Behavior-driven Development (BDD) to automate testing in 3D games like Space Engineers, allowing better collaboration through more readable scripts.


<details>
  <summary>Details</summary>
Motivation: Testing 3D games is challenging due to their complexity, requiring innovative methods like automation to maintain quality and ease collaboration.

Method: The iv4XR framework is integrated with Behavior-driven Development (BDD) to allow for more accessible scripting and automates regression and play-testing for games like Space Engineers and LabRecruits.

Result: The integration successfully automated regression and long-play test scenarios, demonstrated the framework's versatility, and improved user access through readable test scripts.

Conclusion: The iv4XR framework, enhanced with BDD and tactical programming, empowers developers and testers to automate diverse game testing scenarios effectively using accessible methods.

Abstract: Computer 3D games are complex software environments that require novel
testing processes to ensure high-quality standards. The Intelligent
Verification/Validation for Extended Reality Based Systems (iv4XR) framework
addresses this need by enabling the implementation of autonomous agents to
automate game testing scenarios. This framework facilitates the automation of
regression test cases for complex 3D games like Space Engineers. Nevertheless,
the technical expertise required to define test scripts using iv4XR can
constrain seamless collaboration between developers and testers. This paper
reports how integrating a Behavior-driven Development (BDD) approach with the
iv4XR framework allows the industrial company behind Space Engineers to
automate regression testing. The success of this industrial collaboration has
inspired the iv4XR team to integrate the BDD approach to improve the automation
of play-testing for the experimental 3D game LabRecruits. Furthermore, the
iv4XR framework has been extended with tactical programming to enable the
automation of long-play test scenarios in Space Engineers. These results
underscore the versatility of the iv4XR framework in supporting diverse testing
approaches while showcasing how BDD empowers users to create, manage, and
execute automated game tests using comprehensive and human-readable statements.

</details>


### [498] [Software Fairness Testing in Practice](https://arxiv.org/abs/2506.17095)
*Ronnie de Souza Santos,Matheus de Morais Leca,Reydne Santos,Cleyton Magalhaes*

Main category: cs.SE

TL;DR: This paper examines the adoption of fairness testing in AI systems by industry professionals, highlighting the gap between theoretical concepts and practical implementation.


<details>
  <summary>Details</summary>
Motivation: To understand the challenges industry faces in applying fairness testing for AI systems and bridge the gap between academic research and real-world practice.

Method: The study conducted interviews with 22 AI/ML practitioners to explore their approaches to fairness testing in AI systems.

Result: Findings reveal a gap between academic definitions of fairness and practical industry applications, with challenges like data quality, time constraints, and lack of effective tools.

Conclusion: There is a need for actionable strategies and tools to enable practitioners to systematically apply fairness testing in AI systems.

Abstract: Software testing ensures that a system functions correctly, meets specified
requirements, and maintains high quality. As artificial intelligence and
machine learning (ML) technologies become integral to software systems, testing
has evolved to address their unique complexities. A critical advancement in
this space is fairness testing, which identifies and mitigates biases in AI
applications to promote ethical and equitable outcomes. Despite extensive
academic research on fairness testing, including test input generation, test
oracle identification, and component testing, practical adoption remains
limited. Industry practitioners often lack clear guidelines and effective tools
to integrate fairness testing into real-world AI development. This study
investigates how software professionals test AI-powered systems for fairness
through interviews with 22 practitioners working on AI and ML projects. Our
findings highlight a significant gap between theoretical fairness concepts and
industry practice. While fairness definitions continue to evolve, they remain
difficult for practitioners to interpret and apply. The absence of
industry-aligned fairness testing tools further complicates adoption,
necessitating research into practical, accessible solutions. Key challenges
include data quality and diversity, time constraints, defining effective
metrics, and ensuring model interoperability. These insights emphasize the need
to bridge academic advancements with actionable strategies and tools, enabling
practitioners to systematically address fairness in AI systems.

</details>


### [499] [Reassessing Code Authorship Attribution in the Era of Language Models](https://arxiv.org/abs/2506.17120)
*Atish Kumar Dipongkor,Ziyu Yao,Kevin Moran*

Main category: cs.SE

TL;DR: The paper investigates the performance of state-of-the-art language models (LMs) in code authorship attribution (CAA), a task crucial for cybersecurity and software forensics.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenges of code authorship attribution, a crucial task in identifying coding styles, detecting plagiarism, and supporting security-related legal cases. Traditional approaches struggle with nuanced coding patterns and adversarial perturbations.

Method: The study applies two larger state-of-the-art code LMs and five smaller code LMs to 6 datasets containing 12,000 code snippets by 463 developers, with in-depth performance analysis using machine learning interpretability techniques.

Result: The results highlight the models' ability to capture stylometric code patterns, but also reveal their limitations and suggest areas for improvement in the field of CAA.

Conclusion: Transformer-based LMs show promise for CAA but remain limited in certain aspects. The study advances understanding and opens directions for improving automated CAA techniques.

Abstract: The study of Code Stylometry, and in particular Code Authorship Attribution
(CAA), aims to analyze coding styles to identify the authors of code samples.
CAA is crucial in cybersecurity and software forensics for addressing,
detecting plagiarism, and supporting criminal prosecutions. However, CAA is a
complex and error prone task, due to the need for recognizing nuanced
relationships between coding patterns. This challenge is compounded in large
software systems with numerous authors due to the subtle variability of
patterns that signify the coding style of one author among many. Given the
challenges related to this task, researchers have proposed and studied
automated approaches that rely upon classical Machine Learning and Deep
Learning techniques. However, such techniques have historically relied upon
hand-crafted features, and due to the often intricate interaction of different
features (e.g., formatting, etc.), have key limitations in properly
characterizing authorship, and are sensitive to adversarial code perturbations.
Recently, transformer-based Language Models (LMs) have shown remarkable
efficacy across a range of software engineering tasks, and in the authorship
attribution on natural language in the NLP domain. However, their effectiveness
in CAA is not well understood. As such, we conduct the first extensive
empirical study applying two larger state-of-the-art code LMs, and five smaller
code LMs to the task of CAA to 6 diverse datasets that encompass 12k code
snippets written by 463 developers. Furthermore, we perform an in-depth
analysis of our studied models' performance on CAA using established machine
learning interpretability techniques. The results of our analysis illustrate
important findings that illuminate the behavior of LMs in understanding
stylometric code patterns during the task of CAA, and point towards important
directions for future work.

</details>


### [500] [Large Language Model Unlearning for Source Code](https://arxiv.org/abs/2506.17125)
*Xue Jiang,Yihong Dong,Zheng Fang,Yingwei Ma,Tangxinyu Wang,Rongyu Cao,Binhua Li,Zhi Jin,Wenpin Jiao,Yongbin Li,Ge Li*

Main category: cs.SE

TL;DR: The paper introduces PROD, an unlearning approach designed to make large language models (LLMs) forget specific source code content while maintaining their utility for code generation.


<details>
  <summary>Details</summary>
Motivation: To address the risks posed by LLMs memorizing sensitive or outdated data, such as legal non-compliance, security vulnerabilities, and reduced code quality.

Method: PROD suppresses the probability of undesired data in LLMs' output distributions while promoting alternative components, ensuring models can learn to forget specific content without losing general code generation capabilities.

Result: PROD outperforms existing unlearning methods in removing unwanted code while maintaining model utility on downstream tasks and adapts well across different LLM scales. It is also robust against adversarial attacks.

Conclusion: PROD effectively expands unlearning techniques to source code, enabling reliable and secure code generation with reduced risks from undesirable training data.

Abstract: LLM4SE has demonstrated significant success, but LLMs' potential memorization
of sensitive or outdated training data introduces critical risks to legal
compliance, software security, and code quality. LLM unlearning techniques,
which can eliminate the influence of undesired data from LLMs in a
post-training way, present a promising solution to address these concerns.
While recent efforts in LLM unlearning show effectiveness in natural language,
their applicability to source code remains underexplored. Our empirical study
reveals that existing LLM unlearning approaches, when applied to source code,
cause severe model utility degradation, rendering models practically unusable
for code generation. In this paper, we propose PROD, a novel unlearning
approach that enables LLMs to forget undesired code content while effectively
preserving their code generation capabilities. PROD suppresses the probability
of forget data in LLMs' output distribution while promoting candidate
distributional components, enabling the model to jointly learn to forget
specific content and retain its general capabilities. To facilitate this study,
we establish a benchmark for code unlearning evaluation, which includes three
critical downstream tasks: copyrighted code unlearning, insecure code
unlearning, and deprecated API unlearning. Our evaluation demonstrates that
PROD achieves superior balance between forget quality and model utility
compared to existing unlearning approaches across three downstream tasks, while
consistently exhibiting improvements when applied to LLMs of varying series.
PROD also exhibits superior robustness against adversarial attacks without
generating or exposing the data to be forgotten. The results underscore that
our approach not only extends the application boundary of unlearning techniques
to source code, but also holds significant implications for advancing reliable
code generation.

</details>


### [501] [Dissecting the SWE-Bench Leaderboards: Profiling Submitters and Architectures of LLM- and Agent-Based Repair Systems](https://arxiv.org/abs/2506.17208)
*Matias Martinez,Xavier Franch*

Main category: cs.SE

TL;DR: This paper reviews submissions from the SWE-Bench Lite and Verified leaderboards to evaluate 67 program repair approaches, analyzing various dimensions of these systems.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to understand the architectural design, origins, and characteristics of submissions to improve understanding in the field of LLM-based program repair research.

Method: Analyzed 67 unique approaches submitted to SWE-Bench leaderboards based on factors like submitter type, product availability, LLM usage, and architecture.

Result: Findings highlight the dominance of proprietary LLMs like Claude 3.5/3.7, varying architectural approaches, and submitter diversity from individuals to corporations.

Conclusion: The study identifies trends and categorizations in LLM-based systems for program repair, offering a first-of-its-kind comprehensive evaluation of the SWE-Bench leaderboards.

Abstract: The rapid progress in Automated Program Repair (APR) has been driven by
advances in AI, particularly large language models (LLMs) and agent-based
systems. SWE-Bench is a recent benchmark designed to evaluate LLM-based repair
systems using real issues and pull requests mined from 12 popular open-source
Python repositories. Its public leaderboards, SWE-Bench Lite and SWE-Bench
Verified, have become central platforms for tracking progress and comparing
solutions. However, because the submission process does not require detailed
documentation, the architectural design and origin of many solutions remain
unclear. In this paper, we present the first comprehensive study of all
submissions to the SWE-Bench Lite (68 entries) and Verified (79 entries)
leaderboards, analyzing 67 unique approaches across dimensions such as
submitter type, product availability, LLM usage, and system architecture. Our
findings reveal the dominance of proprietary LLMs (especially Claude 3.5/3.7),
the presence of both agentic and non-agentic designs, and a contributor base
spanning from individual developers to large tech companies.

</details>


### [502] [cAST: Enhancing Code Retrieval-Augmented Generation with Structural Chunking via Abstract Syntax Tree](https://arxiv.org/abs/2506.15655)
*Yilin Zhang,Xinran Zhao,Zora Zhiruo Wang,Chenyang Yang,Jiayi Wei,Tongshuang Wu*

Main category: cs.SE

TL;DR: This paper introduces a structure-aware chunking method using Abstract Syntax Trees (AST) for better retrieval-augmented generation in code intelligence tasks.


<details>
  <summary>Details</summary>
Motivation: Enhance code generation through retrieval-augmented generation by addressing limitations in existing line-based chunking heuristics, which often disrupt semantic structure.

Method: Proposes chunking via Abstract Syntax Trees (ASTs) to create semantically coherent, self-contained retrievable units, splitting and merging nodes based on size constraints.

Result: Improved retrieval and generation performance in code tasks: Recall@5 increased by 4.3 points and Pass@1 by 2.67 points across relevant benchmarks.

Conclusion: Structure-aware chunking is critical for achieving scalability and improved performance in retrieval-augmented code generation pipelines.

Abstract: Retrieval-Augmented Generation (RAG) has become essential for large-scale
code generation, grounding predictions in external code corpora to improve
actuality. However, a critical yet underexplored aspect of RAG pipelines is
chunking -- the process of dividing documents into retrievable units. Existing
line-based chunking heuristics often break semantic structures, splitting
functions or merging unrelated code, which can degrade generation quality. We
propose chunking via Abstract Syntax Trees (\ourwork), a structure-aware method
that recursively breaks large AST nodes into smaller chunks and merges sibling
nodes while respecting size limits. This approach generates self-contained,
semantically coherent units across programming languages and tasks, improving
performance on diverse code generation tasks, e.g., boosting Recall@5 by 4.3
points on RepoEval retrieval and Pass@1 by 2.67 points on SWE-bench generation.
Our work highlights the importance of structure-aware chunking for scaling
retrieval-enhanced code intelligence.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [503] [Cross-Modal Epileptic Signal Harmonization: Frequency Domain Mapping Quantization for Pre-training a Unified Neurophysiological Transformer](https://arxiv.org/abs/2506.17068)
*Runkai Zhang,Hua Yu,John Q. Gan,Haixian Wang*

Main category: q-bio.NC

TL;DR: The paper introduces EpiNT, a Transformer-based model for unified scalp and intracranial EEG analysis in epilepsy, using large-scale multi-modal data.


<details>
  <summary>Details</summary>
Motivation: The authors aim to overcome challenges in unified analysis of scalp EEG and intracranial EEG due to differences in montages, amplitude, SNR, and frequency components.

Method: The proposed method involves EpiNT, a pre-trained Transformer model with MAE, VQ, and frequency domain quantizer, trained on 2,700+ hours of data from 1,199 patients.

Result: EpiNT outperformed baseline models and existing methods on six downstream tasks, demonstrating strong learning capabilities.

Conclusion: EpiNT offers a powerful framework for combined analysis of EEG and iEEG, advancing epilepsy neurophysiology research.

Abstract: Scalp electroencephalography (EEG) and intracranial EEG (iEEG) are vital for
epilepsy diagnosis and treatment. Their unified analysis offers the potential
to harness the complementary strengths of each modality but is challenging due
to variations in recording montages, amplitude and signal-to-noise ratio (SNR),
and frequency components. To address the aforementioned challenges, this paper
introduces EpiNT, a novel Transformer-based pre-trained model for unified EEG
and iEEG analysis. EpiNT employs channel-independent modeling with masked
autoencoders (MAE) and vector quantization (VQ), along with a frequency domain
mapping quantizer to capture crucial frequency features. Pre-trained on over
2,700 hours of multi-modal clinical neurophysiological data from 1,199
patients, EpiNT outperformed both randomly initialized models and other
pre-trained methods on six downstream classification tasks, demonstrating
robust representation learning capabilities. This work presents a promising
approach for unified epilepsy neurophysiology analysis.

</details>


### [504] [Efficient and faithful reconstruction of dynamical attractors using homogeneous differentiators](https://arxiv.org/abs/2506.17079)
*Uros Sutulovic,Daniele Proverbio,Rami Katz,Giulia Giordano*

Main category: q-bio.NC

TL;DR: This paper proposes using Homogeneous Differentiators (HD) to improve attractor reconstructions for nonlinear dynamical systems, overcoming issues like noise and limited data.


<details>
  <summary>Details</summary>
Motivation: Existing attractor reconstruction methods often produce poor results due to noise corruption, limited measurements, and short time series.

Method: The authors use Homogeneous Differentiators to denoise measurements and enhance time-delay embedding, differential embedding, and functional observability for attractor reconstruction.

Result: Their experiments show significant improvements in attractor quality and computational efficiency on simulated models (e.g., Lorenz system, Epileptor model) and empirical EEG data.

Conclusion: Homogeneous Differentiators are effective for preprocessing noisy signals and reconstructing nonlinear dynamical attractors, making them a promising tool for both theoretical and practical applications.

Abstract: Reconstructing the attractors of complex nonlinear dynamical systems from
available measurements is key to analyse and predict their time evolution.
Existing attractor reconstruction methods typically rely on topological
embedding and may produce poor reconstructions, which differ significantly from
the actual attractor, because measurements are corrupted by noise and often
available only for some of the state variables and/or their combinations, and
the time series are often relatively short. Here, we propose the use of
Homogeneous Differentiators (HD) to effectively de-noise measurements and more
faithfully reconstruct attractors of nonlinear systems. Homogeneous
Differentiators are supported by rigorous theoretical guarantees about their
de-noising capabilities, and their results can be fruitfully combined with
time-delay embedding, differential embedding and functional observability. We
apply our proposed HD-based methodology to simulated dynamical models of
increasing complexity, from the Lorenz system to the Hindmarsh-Rose model and
the Epileptor model for neural dynamics, as well as to empirical data of EEG
recordings. In the presence of corrupting noise of various types, we obtain
drastically improved quality and resolution of the reconstructed attractors, as
well as significantly reduced computational time, which can be orders of
magnitude lower than that of alternative methods. Our tests show the
flexibility and effectiveness of Homogeneous Differentiators and suggest that
they can become the tool of choice for preprocessing noisy signals and
reconstructing attractors of highly nonlinear dynamical systems from both
theoretical models and real data.

</details>


### [505] [Brain-inspired interpretable reservoir computing with resonant recurrent neural networks](https://arxiv.org/abs/2506.17083)
*Mark A. Kramer*

Main category: q-bio.NC

TL;DR: This paper proposes a resonant reservoir network (RRN) featuring oscillatory node dynamics inspired by biological neurons for improved classification tasks across several input domains.


<details>
  <summary>Details</summary>
Motivation: Biological neurons operate within an oscillatory context, unlike artificial neural networks, which employ non-oscillatory nodes. Leveraging biological neuron traits could enhance artificial neural networks' efficiency and interpretability.

Method: The RRN incorporates damped oscillatory dynamics into nodes, connecting these dynamics to existing neural networks. Physical constraints reduce free parameters, and optimization is illustrated via brain rhythm classification associated with epilepsy.

Result: RRNs classify input signals effectively, resonating with their features. They generalize well across visual, auditory, and epileptic brain rhythm scenarios, with fewer trainable parameters compared to conventional artificial networks.

Conclusion: The RRN framework, due to its interpretability, straightforward expression, and efficiency, offers potential as a biologically inspired approach for neural network designs that work well while avoiding computationally intensive processes like backpropagation.

Abstract: Traditional artificial neural networks consist of nodes with non-oscillatory
dynamics. Biological neural networks, on the other hand, consist of oscillatory
components embedded in an oscillatory environment. Motivated by this feature of
biological neurons, we describe a reservoir computing framework with explicit
damped, oscillatory node dynamics. We express the oscillatory dynamics using
two history dependent terms to connect these dynamics with existing artificial
neural network approaches and apply physical and stationary constraints to
reduce the number of free parameters. We then optimize and illustrate reservoir
performance by classifying different brain rhythms associated with epilepsy and
show that reservoir elements support classification by resonating with features
of the input signals. Applying the same reservoir network to visual and
auditory signal types, we show the reservoir generalizes for accurate
classification with few trainable parameters. Compared to existing artificial
neural network approaches, the proposed resonant reservoir network (RRN)
utilizes oscillatory dynamics expressed as a straightforward extension of
traditional artificial neural networks, produces interpretable features for
classification, avoids computationally expensive training (e.g.,
backpropagation), and performs well with few parameters in different
classification scenarios. We propose that RRNs may serve as efficient,
biologically implemented building blocks to achieve complex goals in biological
and artificial neural networks.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [506] [Sampling conditioned diffusions via Pathspace Projected Monte Carlo](https://arxiv.org/abs/2506.15743)
*Tobias Grafke*

Main category: stat.ML

TL;DR: The paper proposes a pathspace Metropolis-adjusted manifold sampling algorithm to sample stochastic differential equations under various constraints.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of sampling stochastic differential equations subjected to complex constraints such as endpoint, integral, and stochastic integral constraints.

Method: The authors developed a pathspace Metropolis-adjusted manifold sampling scheme, targeting stochastic paths that satisfy specific conditioning constraints.

Result: The algorithm successfully demonstrates its applicability in diverse scenarios, including phase transitions, constrained random walks, stochastic wave equations, and turbulence modeling.

Conclusion: The proposed sampling algorithm is versatile and effective for analyzing constrained stochastic systems, offering insights into various complex phenomena.

Abstract: We present an algorithm to sample stochastic differential equations
conditioned on rather general constraints, including integral constraints,
endpoint constraints, and stochastic integral constraints. The algorithm is a
pathspace Metropolis-adjusted manifold sampling scheme, which samples
stochastic paths on the submanifold of realizations that adhere to the
conditioning constraint. We demonstrate the effectiveness of the algorithm by
sampling a dynamical condensation phase transition, conditioning a random walk
on a fixed Levy stochastic area, conditioning a stochastic nonlinear wave
equation on high amplitude waves, and sampling a stochastic partial
differential equation model of turbulent pipe flow conditioned on
relaminarization events.

</details>


### [507] [From Local Interactions to Global Operators: Scalable Gaussian Process Operator for Physical Systems](https://arxiv.org/abs/2506.15906)
*Sawan Kumar,Tapas Tripura,Rajdip Nayek,Souvik Chakraborty*

Main category: stat.ML

TL;DR: This study introduces a scalable Gaussian Process Operator (GPO) leveraging kernel design techniques to solve high-dimensional parametric PDEs efficiently, while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to address the computational challenges in scaling probabilistic neural operators, specifically Gaussian Processes Operators (GPOs), to high-dimensional and data-intensive scenarios.

Method: The method combines locality and sparsity principles using nearest-neighbor local kernel approximations, sparse kernel techniques in the parameter space, and structured Kronecker factorizations for efficient computation. Additionally, operator-aware kernel structures and neural operator-based mean functions enhance accuracy.

Result: The framework performs effectively with high accuracy across nonlinear PDEs like Navier-Stokes, Darcy flow, and Burgers' equations, even across various discretization scales.

Conclusion: The work successfully bridges scalability and fidelity gaps in GPO for uncertainty-aware modeling in complex physical systems.

Abstract: Operator learning offers a powerful paradigm for solving parametric partial
differential equations (PDEs), but scaling probabilistic neural operators such
as the recently proposed Gaussian Processes Operators (GPOs) to
high-dimensional, data-intensive regimes remains a significant challenge. In
this work, we introduce a novel, scalable GPO, which capitalizes on sparsity,
locality, and structural information through judicious kernel design.
Addressing the fundamental limitation of cubic computational complexity, our
method leverages nearest-neighbor-based local kernel approximations in the
spatial domain, sparse kernel approximation in the parameter space, and
structured Kronecker factorizations to enable tractable inference on
large-scale datasets and high-dimensional input. While local approximations
often introduce accuracy trade-offs due to limited kernel interactions, we
overcome this by embedding operator-aware kernel structures and employing
expressive, task-informed mean functions derived from neural operator
architectures. Through extensive evaluations on a broad class of nonlinear PDEs
- including Navier-Stokes, wave advection, Darcy flow, and Burgers' equations -
we demonstrate that our framework consistently achieves high accuracy across
varying discretization scales. These results underscore the potential of our
approach to bridge the gap between scalability and fidelity in GPO, offering a
compelling foundation for uncertainty-aware modeling in complex physical
systems.

</details>


### [508] [Diffusion-Based Hypothesis Testing and Change-Point Detection](https://arxiv.org/abs/2506.16089)
*Sean Moushegian,Taposh Banerjee,Vahid Tarokh*

Main category: stat.ML

TL;DR: The paper extends score-based hypothesis testing and change-point detection to include diffusion-based methodologies, optimizing performance via numerically-tuned weight matrices, showing efficacy through theoretical and numerical simulations.


<details>
  <summary>Details</summary>
Motivation: Score-based methods for modeling and hypothesis testing are less powerful than likelihood-based methods. Existing advancements in score functions via matrix transformations require further exploration to improve hypothesis testing and detection.

Method: The authors generalize score-based methods into diffusion-based systems using matrix multiplication, propose algorithmic optimizations for weight matrices, and provide both theoretical analysis and numerical simulations for validation.

Result: Diffusion-based algorithms show better performance over traditional score-based methods, validated through numerical simulations and analyses of optimized weight matrices.

Conclusion: Diffusion-based techniques can significantly enhance score-based hypothesis testing and change-point detection, offering a powerful alternative to existing methods.

Abstract: Score-based methods have recently seen increasing popularity in modeling and
generation. Methods have been constructed to perform hypothesis testing and
change-point detection with score functions, but these methods are in general
not as powerful as their likelihood-based peers. Recent works consider
generalizing the score-based Fisher divergence into a diffusion-divergence by
transforming score functions via multiplication with a matrix-valued function
or a weight matrix. In this paper, we extend the score-based hypothesis test
and change-point detection stopping rule into their diffusion-based analogs.
Additionally, we theoretically quantify the performance of these
diffusion-based algorithms and study scenarios where optimal performance is
achievable. We propose a method of numerically optimizing the weight matrix and
present numerical simulations to illustrate the advantages of diffusion-based
algorithms.

</details>


### [509] [CP$^2$: Leveraging Geometry for Conformal Prediction via Canonicalization](https://arxiv.org/abs/2506.16189)
*Putri A. van der Linden,Alexander Timans,Erik J. Bekkers*

Main category: stat.ML

TL;DR: The paper addresses conformal prediction under geometric data shifts, proposing integration of geometric information for robustness.


<details>
  <summary>Details</summary>
Motivation: Geometric data shifts, such as transformations like rotations or flips, weaken conformal prediction's reliability despite its formal coverage guarantees.

Method: Incorporating geometric pose information into the conformal prediction process using pose canonicalization advancements.

Result: Evaluations show that integrating geometric information with conformal prediction effectively handles geometric shifts while accommodating black-box predictors.

Conclusion: The proposed approach ensures robustness against geometric data shifts and provides a generalizable method for enhancing conformal prediction.

Abstract: We study the problem of conformal prediction (CP) under geometric data
shifts, where data samples are susceptible to transformations such as rotations
or flips. While CP endows prediction models with post-hoc uncertainty
quantification and formal coverage guarantees, their practicality breaks under
distribution shifts that deteriorate model performance. To address this issue,
we propose integrating geometric information--such as geometric pose--into the
conformal procedure to reinstate its guarantees and ensure robustness under
geometric shifts. In particular, we explore recent advancements on pose
canonicalization as a suitable information extractor for this purpose.
Evaluating the combined approach across discrete and continuous shifts and
against equivariant and augmentation-based baselines, we find that integrating
geometric information with CP yields a principled way to address geometric
shifts while maintaining broad applicability to black-box predictors.

</details>


### [510] [Random feature approximation for general spectral methods](https://arxiv.org/abs/2506.16283)
*Mike Nguyen,Nicole Mücke*

Main category: stat.ML

TL;DR: This paper focuses on analyzing the generalization properties of random feature methods for kernel-based learning and extends it to a variety of regularization strategies, including neural operators.


<details>
  <summary>Details</summary>
Motivation: To understand the generalization capabilities of random feature techniques in large-scale kernel learning and to expand their applicability to broader regularization methods and neural network analysis via the NTK approach.

Method: The authors extend analysis from Tikhonov regularization to a variety of spectral regularization techniques, including implicit schemes like gradient descent and accelerated methods. They derive optimal learning rates for estimators under specific regularity conditions.

Result: The analysis achieves optimal learning rates over a wide range of regularity classes, including those outside the reproducing kernel Hilbert space, improving or complementing prior kernel algorithm results.

Conclusion: This research broadens the theoretical understanding of random feature and kernel-based methods, offering improved or new insights into their generalization and applicability in neural networks and large-scale learning.

Abstract: Random feature approximation is arguably one of the most widely used
techniques for kernel methods in large-scale learning algorithms. In this work,
we analyze the generalization properties of random feature methods, extending
previous results for Tikhonov regularization to a broad class of spectral
regularization techniques. This includes not only explicit methods but also
implicit schemes such as gradient descent and accelerated algorithms like the
Heavy-Ball and Nesterov method. Through this framework, we enable a theoretical
analysis of neural networks and neural operators through the lens of the Neural
Tangent Kernel (NTK) approach trained via gradient descent. For our estimators
we obtain optimal learning rates over regularity classes (even for classes that
are not included in the reproducing kernel Hilbert space), which are defined
through appropriate source conditions. This improves or completes previous
results obtained in related settings for specific kernel algorithms.

</details>


### [511] [The Condition Number as a Scale-Invariant Proxy for Information Encoding in Neural Units](https://arxiv.org/abs/2506.16289)
*Oswaldo Ludwig*

Main category: stat.ML

TL;DR: The paper examines how the condition number of neural network weight tensors relates to their information encoding, proposing a framework that links condition numbers and information theory, and applying it to fine-tuning multimodal models.


<details>
  <summary>Details</summary>
Motivation: Investigating the role of the condition number in quantifying efficient information encoding within neural networks, with a goal to improve techniques for addressing catastrophic forgetting in multimodal models.

Method: Analyzing the condition number of weight tensors using information-theoretic principles, modeling transformations for linear units with Gaussian inputs, and applying findings to selective fine-tuning of a multimodal Large Language Model.

Result: It shows that a high condition number indicates specialized encoding but reduced information transfer, and validates the approach by mitigating catastrophic forgetting in a case study.

Conclusion: The study provides formal links between neural network condition numbers and information encoding, demonstrating practical benefits in fine-tuning practices without relying on pre-training statistics.

Abstract: This paper explores the relationship between the condition number of a neural
network's weight tensor and the extent of information encoded by the associated
processing unit, viewed through the lens of information theory. We argue that a
high condition number, though not sufficient for effective knowledge encoding,
may indicate that the unit has learned to selectively amplify and compress
information. We formalize this intuition, particularly for linear units with
Gaussian inputs, linking the condition number and the transformation's
log-volume scaling factor to the characteristics of the output entropy and the
geometric properties of the learned transformation. Our analysis demonstrates
that for a fixed weight norm, a concentrated distribution of singular values
(high condition number) corresponds to reduced overall information transfer,
indicating a specialized and efficient encoding strategy. Furthermore, we
present a practical case study where these principles are applied to guide
selective fine-tuning of a multimodal Large Language Model, aiming to mitigate
catastrophic forgetting during cross-modal adaptation. Unlike many existing
catastrophic forgetting mitigation methods that rely on access to pre-training
statistics, which are often unavailable, our selective fine-tuning approach
offers a way to bypass this common requirement.

</details>


### [512] [Identifying Heterogeneity in Distributed Learning](https://arxiv.org/abs/2506.16394)
*Zelin Xiao,Jia Gu,Song Xi Chen*

Main category: stat.ML

TL;DR: This paper proposes methods for identifying heterogeneous parameters in distributed M-estimation while minimizing data transmission. The focus is on testing procedures with different strengths under various heterogeneity conditions.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of identifying varying components of parameters in distributed datasets, while ensuring minimal data transfer and maintaining testing consistency under different levels of heterogeneity.

Method: Two tests are introduced: (1) a re-normalized Wald test for dense heterogeneity and smaller numbers of data blocks relative to block sample size, and (2) an extreme contrast test (ECT) with sample splitting for sparse heterogeneity when the number of blocks exceeds sample size. A hybrid approach combines both tests for robustness.

Result: The methods demonstrate consistent identification of heterogeneous parameters, with numerical experiments showing effective family-wise error rate and power. A case study validates the practical applicability of the methods.

Conclusion: The proposed testing methods are effective, communication-efficient, and suitable for varying sparsity levels in distributed data scenarios, allowing robust detection of parameter heterogeneity with minimal data transfer.

Abstract: We study methods for identifying heterogeneous parameter components in
distributed M-estimation with minimal data transmission. One is based on a
re-normalized Wald test, which is shown to be consistent as long as the number
of distributed data blocks $K$ is of a smaller order of the minimum block
sample size {and the level of heterogeneity is dense}. The second one is an
extreme contrast test (ECT) based on the difference between the largest and
smallest component-wise estimated parameters among data blocks. By introducing
a sample splitting procedure, the ECT can avoid the bias accumulation arising
from the M-estimation procedures, and exhibits consistency for $K$ being much
larger than the sample size while the heterogeneity is sparse. The ECT
procedure is easy to operate and communication-efficient. A combination of the
Wald and the extreme contrast tests is formulated to attain more robust power
under varying levels of sparsity of the heterogeneity. We also conduct
intensive numerical experiments to compare the family-wise error rate (FWER)
and the power of the proposed methods. Additionally, we conduct a case study to
present the implementation and validity of the proposed methods.

</details>


### [513] [On Continuous Monitoring of Risk Violations under Unknown Shift](https://arxiv.org/abs/2506.16416)
*Alexander Timans,Rajeev Verma,Eric Nalisnick,Christian A. Naesseth*

Main category: stat.ML

TL;DR: The paper presents a framework for real-time monitoring of machine learning risk violations under unpredictable data shifts.


<details>
  <summary>Details</summary>
Motivation: Ensure machine learning systems deployed in dynamic environments remain reliable despite unpredictable distribution shifts.

Method: Proposes a sequential hypothesis testing procedure using the 'testing by betting' paradigm to monitor and detect risk violations in evolving data streams.

Result: Successfully applied the proposed framework to monitor risks in outlier detection and set prediction under various shifts.

Conclusion: The framework offers a broadly applicable method for maintaining reliability of machine learning systems under minimal assumptions during deployment.

Abstract: Machine learning systems deployed in the real world must operate under
dynamic and often unpredictable distribution shifts. This challenges the
validity of statistical safety assurances on the system's risk established
beforehand. Common risk control frameworks rely on fixed assumptions and lack
mechanisms to continuously monitor deployment reliability. In this work, we
propose a general framework for the real-time monitoring of risk violations in
evolving data streams. Leveraging the 'testing by betting' paradigm, we propose
a sequential hypothesis testing procedure to detect violations of bounded risks
associated with the model's decision-making mechanism, while ensuring control
on the false alarm rate. Our method operates under minimal assumptions on the
nature of encountered shifts, rendering it broadly applicable. We illustrate
the effectiveness of our approach by monitoring risks in outlier detection and
set prediction under a variety of shifts.

</details>


### [514] [Latent Noise Injection for Private and Statistically Aligned Synthetic Data Generation](https://arxiv.org/abs/2506.16636)
*Rex Shen,Lu Tian*

Main category: stat.ML

TL;DR: The paper introduces a new Latent Noise Injection method using Masked Autoregressive Flows (MAF) for generating synthetic data that addresses slow convergence challenges in high-dimensional scenarios.


<details>
  <summary>Details</summary>
Motivation: To tackle inefficiencies in existing generative models (e.g., slow convergence beyond the standard $1/\sqrt{n}$ rate) for high-dimensional synthetic data while ensuring privacy.

Method: The proposed method perturbs data points in the latent space and maps them back to the data domain, enabling efficient synthetic data generation with preserved privacy, using a local $(\epsilon, \delta)$-differential privacy framework.

Result: The method demonstrates strong alignment with original data, improved efficiency via meta-analyses, and robustness against membership inference attacks.

Conclusion: Latent Noise Injection offers a robust and efficient alternative for generating privacy-preserving synthetic data, particularly suitable for decentralized and privacy-sensitive applications like biomedical research.

Abstract: Synthetic Data Generation has become essential for scalable,
privacy-preserving statistical analysis. While standard approaches based on
generative models, such as Normalizing Flows, have been widely used, they often
suffer from slow convergence in high-dimensional settings, frequently
converging more slowly than the canonical $1/\sqrt{n}$ rate when approximating
the true data distribution.
  To overcome these limitations, we propose a Latent Noise Injection method
using Masked Autoregressive Flows (MAF). Instead of directly sampling from the
trained model, our method perturbs each data point in the latent space and maps
it back to the data domain. This construction preserves a one to one
correspondence between observed and synthetic data, enabling synthetic outputs
that closely reflect the underlying distribution, particularly in challenging
high-dimensional regimes where traditional sampling struggles.
  Our procedure satisfies local $(\epsilon, \delta)$-differential privacy and
introduces a single perturbation parameter to control the privacy-utility
trade-off. Although estimators based on individual synthetic datasets may
converge slowly, we show both theoretically and empirically that aggregating
across $K$ studies in a meta analysis framework restores classical efficiency
and yields consistent, reliable inference. We demonstrate that with a
well-calibrated perturbation parameter, Latent Noise Injection achieves strong
statistical alignment with the original data and robustness against membership
inference attacks. These results position our method as a compelling
alternative to conventional flow-based sampling for synthetic data sharing in
decentralized and privacy-sensitive domains, such as biomedical research.

</details>


### [515] [Schrödinger Bridge Matching for Tree-Structured Costs and Entropic Wasserstein Barycentres](https://arxiv.org/abs/2506.17197)
*Samuel Howard,Peter Potaptchik,George Deligiannidis*

Main category: stat.ML

TL;DR: The paper extends the Iterative Markovian Fitting (IMF) algorithm, a method for solving Schrödinger Bridge (SB) problems, to tree-structured scenarios, addressing multi-marginal optimal transport and enabling the computation of Wasserstein barycentres.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve the scalability and practicality of solving Schrödinger Bridge (SB) problems, particularly in multi-marginal optimal transport scenarios, where tree structures and Wasserstein barycentres are significant applications. IMF is a favorable alternative to traditional methods like IPF.

Method: The research generalizes the Iterative Markovian Fitting (IMF) algorithm for tree-structured Schrödinger Bridge problems by incorporating entropic regularization techniques and adapting fixed-point approaches from barycentre computation within flow-based optimal transport solvers.

Result: The extended IMF algorithm successfully addresses tree-structured Schrödinger Bridge problems, providing computational and practical advantages over Iterative Proportional Fitting (IPF) methods in this setting. The approach specifically improves efficiency in computing Wasserstein barycentres.

Conclusion: The paper concludes that the extended IMF algorithm offers a scalable and efficient tool for solving tree-structured SB problems, inheriting the benefits of IMF over IPF and advancing the computational capabilities in entropic optimal transport and barycentre calculations.

Abstract: Recent advances in flow-based generative modelling have provided scalable
methods for computing the Schr\"odinger Bridge (SB) between distributions, a
dynamic form of entropy-regularised Optimal Transport (OT) for the quadratic
cost. The successful Iterative Markovian Fitting (IMF) procedure solves the SB
problem via sequential bridge-matching steps, presenting an elegant and
practical approach with many favourable properties over the more traditional
Iterative Proportional Fitting (IPF) procedure. Beyond the standard setting,
optimal transport can be generalised to the multi-marginal case in which the
objective is to minimise a cost defined over several marginal distributions. Of
particular importance are costs defined over a tree structure, from which
Wasserstein barycentres can be recovered as a special case. In this work, we
extend the IMF procedure to solve for the tree-structured SB problem. Our
resulting algorithm inherits the many advantages of IMF over IPF approaches in
the tree-based setting. In the specific case of Wasserstein barycentres, our
approach can be viewed as extending fixed-point approaches for barycentre
computation to the case of flow-based entropic OT solvers.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [516] [Unpacking Generative AI in Education: Computational Modeling of Teacher and Student Perspectives in Social Media Discourse](https://arxiv.org/abs/2506.16412)
*Paulina DeVito,Akhil Vallala,Sean Mcmahon,Yaroslav Hinda,Benjamin Thaw,Hanqi Zhuang,Hari Kalva*

Main category: cs.SI

TL;DR: This study analyzes public perceptions of Generative AI (GAI) in education through social media, using advanced NLP and GPT-4-powered methods.


<details>
  <summary>Details</summary>
Motivation: The paper aims to understand perceptions and dynamics of stakeholders on GAI technologies in education to better inform adoption strategies.

Method: Using Reddit data, the authors apply sentiment analysis, topic modeling, and author classification via a modular framework based on GPT-4 and compare it to classical NLP models.

Result: The GPT-4o pipeline outperformed classical NLP models, with high accuracy in sentiment analysis and identification of the key latent topics and contrasting perspectives among students and educators.

Conclusion: Institutions need clearer policies and transparent GAI integration to address concerns, while this study showcases the effectiveness of LLM-based frameworks in analyzing social discourse.

Abstract: Generative AI (GAI) technologies are quickly reshaping the educational
landscape. As adoption accelerates, understanding how students and educators
perceive these tools is essential. This study presents one of the most
comprehensive analyses to date of stakeholder discourse dynamics on GAI in
education using social media data. Our dataset includes 1,199 Reddit posts and
13,959 corresponding top-level comments. We apply sentiment analysis, topic
modeling, and author classification. To support this, we propose and validate a
modular framework that leverages prompt-based large language models (LLMs) for
analysis of online social discourse, and we evaluate this framework against
classical natural language processing (NLP) models. Our GPT-4o pipeline
consistently outperforms prior approaches across all tasks. For example, it
achieved 90.6% accuracy in sentiment analysis against gold-standard human
annotations. Topic extraction uncovered 12 latent topics in the public
discourse with varying sentiment and author distributions. Teachers and
students convey optimism about GAI's potential for personalized learning and
productivity in higher education. However, key differences emerged: students
often voice distress over false accusations of cheating by AI detectors, while
teachers generally express concern about job security, academic integrity, and
institutional pressures to adopt GAI tools. These contrasting perspectives
highlight the tension between innovation and oversight in GAI-enabled learning
environments. Our findings suggest a need for clearer institutional policies,
more transparent GAI integration practices, and support mechanisms for both
educators and students. More broadly, this study demonstrates the potential of
LLM-based frameworks for modeling stakeholder discourse within online
communities.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [517] [Smartphone-integrated RPA-CRISPR-Cas12a Detection System with Microneedle Sampling for Point-of-Care Diagnosis of Potato Late Blight in Early Stage](https://arxiv.org/abs/2506.15728)
*Jiangnan Zhao,Hanbo Xu,Cifu Xu,Wenlong Yin,Laixin Luo,Gang Liu,Yan Wang*

Main category: q-bio.QM

TL;DR: The paper describes a smartphone-based, point-of-care diagnostic system leveraging RPA-CRISPR-Cas12a and microneedle patches to detect potato late blight early in the field.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of traditional plant disease diagnostic methods (e.g., PCR, LAMP), which require specialized equipment and are impractical for field use, by developing a portable, cost-effective, and efficient diagnostic system for early detection of potato late blight.

Method: The study integrates a polyvinyl alcohol (PVA) microneedle patch for efficient DNA sample extraction from plant leaves with an RPA-CRISPR-Cas12a isothermal assay, targeting the pathogen Phytophthora infestans. A smartphone is used for fluorescent image acquisition and analysis to ensure portability and ease of use.

Result: The system achieved a DNA extraction efficiency of 56 μg/mg, 3x higher than traditional CTAB methods, and exhibited a detection limit of 2 pg/μL for the pathogen's genomic DNA. It accurately detected the disease in roughly 80% of cases by day 3 and 100% by day 4 post-inoculation, even before visible symptoms appeared.

Conclusion: The portable RPA-CRISPR-Cas12a diagnostic system offers a sensitive, specific, and field-deployable solution for early-stage plant disease detection, enabling timely intervention and control of potato late blight.

Abstract: Potato late blight, caused by the oomycete pathogen Phytophthora infestans,
is one of the most devastating diseases affecting potato crops in the history.
Although conventional detection methods of plant diseases such as PCR and LAMP
are highly sensitive and specific, they rely on bulky and expensive laboratory
equipment and involve complex operations, making them impracticable for
point-of care diagnosis in the field. Here in this study, we report a portable
RPA-CRISPR based diagnosis system for plant disease, integrating smartphone for
acquisition and analysis of fluorescent images. A polyvinyl alcohol (PVA)
microneedle patch was employed for sample extraction on the plant leaves within
one minute, the DNA extraction efficiency achieved 56 ug/mg, which is
approximately 3 times to the traditional CTAB methods (18 ug/mg). The system of
RPA-CRISPR-Cas12a isothermal assay was established to specifically target P.
infestans with no cross-reactivity observed against closely-related species (P.
sojae, P. capsici). The system demonstrated a detection limit of 2 pg/uL for P.
infestans genomic DNA, offering sensitivity comparable to that of benchtop
laboratory equipment. The system demonstrates the early-stage diagnosis
capability by achieving a approximately 80% and 100% detection rate on the
third and fourth day post-inoculation respectively, before visible symptoms
observed on the leaves. The smartphone-based "sample-to-result" system
decouples the limitations of traditional methods that rely heavily on
specialized equipment, offering a promising way for early-stage plant disease
detection and control in the field.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [518] [Sudoku: Decomposing DRAM Address Mapping into Component Functions](https://arxiv.org/abs/2506.15918)
*Minbok Wi,Seungmin Baek,Seonyong Park,Mattan Erez,Jung Ho Ahn*

Main category: cs.CR

TL;DR: The paper introduces a software tool, Sudoku, for automatically decomposing DRAM address mappings using novel timing-based techniques.


<details>
  <summary>Details</summary>
Motivation: Understanding DRAM memory behavior and enabling precise attacks such as RowHammer require detailed knowledge of address mappings, which current methods fail to achieve.

Method: Novel timing-based techniques utilizing DRAM refresh intervals and access latencies to infer component-level functions, implemented in the Sudoku tool.

Result: Sudoku successfully decomposes DRAM address mappings on Intel and AMD processors, validating its effectiveness.

Conclusion: The novel approach and Sudoku tool address shortcomings of existing methods, advancing DRAM address mapping analysis and security understanding.

Abstract: Decomposing DRAM address mappings into component-level functions is critical
for understanding memory behavior and enabling precise RowHammer attacks, yet
existing reverse-engineering methods fall short. We introduce novel
timing-based techniques leveraging DRAM refresh intervals and consecutive
access latencies to infer component-specific functions. Based on this, we
present Sudoku, the first software-based tool to automatically decompose full
DRAM address mappings into channel, rank, bank group, and bank functions while
identifying row and column bits. We validate Sudoku's effectiveness,
successfully decomposing mappings on recent Intel and AMD processors.

</details>


### [519] [ETrace:Event-Driven Vulnerability Detection in Smart Contracts via LLM-Based Trace Analysis](https://arxiv.org/abs/2506.15790)
*Chenyang Peng,Haijun Wang,Yin Wu,Hao Wu,Ming Fan,Yitao Zhao,Ting Liu*

Main category: cs.CR

TL;DR: ETrace introduces an innovative method for detecting vulnerabilities in smart contracts using LLM-powered trace analysis, eliminating the need for source code access.


<details>
  <summary>Details</summary>
Motivation: Traditional security analysis methods rely heavily on accessible smart contract code, which is not always available.

Method: ETrace extracts granular event sequences from transaction logs, leveraging Large Language Models for semantic interpretation and chain-of-thought reasoning to analyze vulnerabilities.

Result: Preliminary experiments validate the framework's efficacy in detecting vulnerabilities through event-driven analysis.

Conclusion: ETrace offers a promising solution for enhancing smart contract security, addressing limitations of existing methodologies by analyzing transaction logs for potential vulnerabilities without source code.

Abstract: With the advance application of blockchain technology in various fields,
ensuring the security and stability of smart contracts has emerged as a
critical challenge. Current security analysis methodologies in vulnerability
detection can be categorized into static analysis and dynamic analysis
methods.However, these existing traditional vulnerability detection methods
predominantly rely on analyzing original contract code, not all smart contracts
provide accessible code.We present ETrace, a novel event-driven vulnerability
detection framework for smart contracts, which uniquely identifies potential
vulnerabilities through LLM-powered trace analysis without requiring source
code access. By extracting fine-grained event sequences from transaction logs,
the framework leverages Large Language Models (LLMs) as adaptive semantic
interpreters to reconstruct event analysis through chain-of-thought reasoning.
ETrace implements pattern-matching to establish causal links between
transaction behavior patterns and known attack behaviors. Furthermore, we
validate the effectiveness of ETrace through preliminary experimental results.

</details>


### [520] [Multi-use LLM Watermarking and the False Detection Problem](https://arxiv.org/abs/2506.15975)
*Zihao Fu,Chris Russell*

Main category: cs.CR

TL;DR: The paper addresses the issue of false detection in digital watermarking for text by proposing a Dual Watermarking method that reduces false positives while maintaining detection accuracy.


<details>
  <summary>Details</summary>
Motivation: To mitigate the risks of misuse of automatically generated text and address the false detection problem in watermarking systems as user capacity grows.

Method: The authors proposed Dual Watermarking, a system that jointly encodes detection and identification watermarks into text. The method is both supported by theoretical analysis and tested experimentally.

Result: The proposed Dual Watermarking method significantly reduces false positives and maintains high detection accuracy, as validated by both theoretical and experimental findings.

Conclusion: Dual Watermarking is an effective solution to the false detection problems in watermarking, offering improved performance in mitigating misuse of automatically generated text.

Abstract: Digital watermarking is a promising solution for mitigating some of the risks
arising from the misuse of automatically generated text. These approaches
either embed non-specific watermarks to allow for the detection of any text
generated by a particular sampler, or embed specific keys that allow the
identification of the LLM user. However, simultaneously using the same
embedding for both detection and user identification leads to a false detection
problem, whereby, as user capacity grows, unwatermarked text is increasingly
likely to be falsely detected as watermarked. Through theoretical analysis, we
identify the underlying causes of this phenomenon. Building on these insights,
we propose Dual Watermarking which jointly encodes detection and identification
watermarks into generated text, significantly reducing false positives while
maintaining high detection accuracy. Our experimental results validate our
theoretical findings and demonstrate the effectiveness of our approach.

</details>


### [521] [Probe before You Talk: Towards Black-box Defense against Backdoor Unalignment for Large Language Models](https://arxiv.org/abs/2506.16447)
*Biao Yi,Tiansheng Huang,Sishuo Chen,Tong Li,Zheli Liu,Zhixuan Chu,Yiming Li*

Main category: cs.CR

TL;DR: This paper introduces BEAT, a black-box defense mechanism to detect and deactivate backdoors in Large Language Models (LLMs), by measuring distortions in output distributions caused by triggered inputs.


<details>
  <summary>Details</summary>
Motivation: Backdoor attacks on LLMs with hidden triggers evade safety audits and pose serious risks in real-world black-box settings, such as LLM as a Service (LLMaaS), due to the semantic complexity of malicious commands.

Method: The proposed BEAT defense relies on detecting triggered inputs by measuring the distortion in output distribution before and after concatenating a sample with probes, focusing on refusal signals rather than sample-specific attack behaviors. It approximates results using multiple sampling, overcoming black-box access constraints.

Result: Experiments on various backdoor attacks and LLMs, including GPT-3.5-turbo, show BEAT is effective and efficient in mitigating these threats. It also demonstrates preliminary success against jailbreak attacks.

Conclusion: BEAT offers a novel and robust solution to tackle backdoor threats and jailbreak attacks in LLMs, even in a black-box setting, by targeting the universal effect of triggers on refusal signals.

Abstract: Backdoor unalignment attacks against Large Language Models (LLMs) enable the
stealthy compromise of safety alignment using a hidden trigger while evading
normal safety auditing. These attacks pose significant threats to the
applications of LLMs in the real-world Large Language Model as a Service
(LLMaaS) setting, where the deployed model is a fully black-box system that can
only interact through text. Furthermore, the sample-dependent nature of the
attack target exacerbates the threat. Instead of outputting a fixed label, the
backdoored LLM follows the semantics of any malicious command with the hidden
trigger, significantly expanding the target space. In this paper, we introduce
BEAT, a black-box defense that detects triggered samples during inference to
deactivate the backdoor. It is motivated by an intriguing observation (dubbed
the probe concatenate effect), where concatenated triggered samples
significantly reduce the refusal rate of the backdoored LLM towards a malicious
probe, while non-triggered samples have little effect. Specifically, BEAT
identifies whether an input is triggered by measuring the degree of distortion
in the output distribution of the probe before and after concatenation with the
input. Our method addresses the challenges of sample-dependent targets from an
opposite perspective. It captures the impact of the trigger on the refusal
signal (which is sample-independent) instead of sample-specific successful
attack behaviors. It overcomes black-box access limitations by using multiple
sampling to approximate the output distribution. Extensive experiments are
conducted on various backdoor attacks and LLMs (including the closed-source
GPT-3.5-turbo), verifying the effectiveness and efficiency of our defense.
Besides, we also preliminarily verify that BEAT can effectively defend against
popular jailbreak attacks, as they can be regarded as 'natural backdoors'.

</details>


### [522] [PRISON: Unmasking the Criminal Potential of Large Language Models](https://arxiv.org/abs/2506.16150)
*Xinyi Wu,Geng Hong,Pei Chen,Yueyue Chen,Xudong Pan,Min Yang*

Main category: cs.CR

TL;DR: Large language models (LLMs) show criminal tendencies when assessed via a structured framework in simulated crime scenarios, indicating a need for better safeguards.


<details>
  <summary>Details</summary>
Motivation: To address concerns over the misuse of LLMs in complex social contexts by systematically evaluating their criminal capabilities.

Method: A unified framework named PRISON was developed, incorporating five criminal dimensions and using role-play scenarios adapted from classic crime films to evaluate LLM behavior.

Result: LLMs exhibited criminal tendencies such as misleading statements and evasion tactics, while their ability to recognize deception was quite limited (41% average accuracy).

Conclusion: There is an urgent need for adversarial robustness, behavioral alignment, and safety measures to prevent harmful uses of LLMs.

Abstract: As large language models (LLMs) advance, concerns about their misconduct in
complex social contexts intensify. Existing research overlooked the systematic
understanding and assessment of their criminal capability in realistic
interactions. We propose a unified framework PRISON, to quantify LLMs' criminal
potential across five dimensions: False Statements, Frame-Up, Psychological
Manipulation, Emotional Disguise, and Moral Disengagement. Using structured
crime scenarios adapted from classic films, we evaluate both criminal potential
and anti-crime ability of LLMs via role-play. Results show that
state-of-the-art LLMs frequently exhibit emergent criminal tendencies, such as
proposing misleading statements or evasion tactics, even without explicit
instructions. Moreover, when placed in a detective role, models recognize
deceptive behavior with only 41% accuracy on average, revealing a striking
mismatch between conducting and detecting criminal behavior. These findings
underscore the urgent need for adversarial robustness, behavioral alignment,
and safety mechanisms before broader LLM deployment.

</details>


### [523] [Towards Effective Complementary Security Analysis using Large Language Models](https://arxiv.org/abs/2506.16899)
*Jonas Wagner,Simon Müller,Christian Näther,Jan-Philipp Steghöfer,Andreas Both*

Main category: cs.CR

TL;DR: The paper explores using Large Language Models (LLMs) to reduce false positives in security analyses generated by SAST tools, showing a significant improvement in performance through advanced techniques and model combinations.


<details>
  <summary>Details</summary>
Motivation: Manual evaluation of security weaknesses flagged by SAST tools is resource-intensive due to the high number of false positives, undermining security analysis efficiency.

Method: The study employs prompting strategies like Chain-of-Thought and Self-Consistency, testing LLM performance on datasets from OWASP Benchmark and real-world projects to identify false positives without reducing true positive rates.

Result: LLMs identified up to 62.5% of false positives in the OWASP Benchmark dataset without missing true positives, and combining results from multiple LLMs further increased FP detection to 78.9%. Using real-world datasets, the best LLM detected 33.85% of false positives, with model combinations raising this to 38.46%.

Conclusion: LLMs can effectively complement SAST tools, automating FP detection and reducing time spent on false alarms, with potential for generalization across diverse programming languages and tools.

Abstract: A key challenge in security analysis is the manual evaluation of potential
security weaknesses generated by static application security testing (SAST)
tools. Numerous false positives (FPs) in these reports reduce the effectiveness
of security analysis. We propose using Large Language Models (LLMs) to improve
the assessment of SAST findings. We investigate the ability of LLMs to reduce
FPs while trying to maintain a perfect true positive rate, using datasets
extracted from the OWASP Benchmark (v1.2) and a real-world software project.
Our results indicate that advanced prompting techniques, such as
Chain-of-Thought and Self-Consistency, substantially improve FP detection.
Notably, some LLMs identified approximately 62.5% of FPs in the OWASP Benchmark
dataset without missing genuine weaknesses. Combining detections from different
LLMs would increase this FP detection to approximately 78.9%. Additionally, we
demonstrate our approach's generalizability using a real-world dataset covering
five SAST tools, three programming languages, and infrastructure files. The
best LLM detected 33.85% of all FPs without missing genuine weaknesses, while
combining detections from different LLMs would increase this detection to
38.46%. Our findings highlight the potential of LLMs to complement traditional
SAST tools, enhancing automation and reducing resources spent addressing false
alarms.

</details>


### [524] [Malware Classification Leveraging NLP & Machine Learning for Enhanced Accuracy](https://arxiv.org/abs/2506.16224)
*Bishwajit Prasad Gond,Rajneekant,Pushkar Kishore,Durga Prasad Mohapatra*

Main category: cs.CR

TL;DR: The study introduces an NLP-based n-gram and machine learning approach for malware classification, achieving 99.02% accuracy with reduced features.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the need for improved methods in malware classification by leveraging NLP to analyze linguistic patterns within malware samples.

Method: Using NLP-based n-gram extraction combined with hybrid feature selection and machine learning algorithms, enabling efficient identification of malware's linguistic features.

Result: The approach achieved high classification accuracy (99.02%) while reducing the feature set to just 1.6% of the original, demonstrating superior performance over traditional methods.

Conclusion: NLP and machine learning techniques can effectively improve malware classification, providing both high accuracy and efficient feature selection.

Abstract: This paper investigates the application of natural language processing
(NLP)-based n-gram analysis and machine learning techniques to enhance malware
classification. We explore how NLP can be used to extract and analyze textual
features from malware samples through n-grams, contiguous string or API call
sequences. This approach effectively captures distinctive linguistic patterns
among malware and benign families, enabling finer-grained classification. We
delve into n-gram size selection, feature representation, and classification
algorithms. While evaluating our proposed method on real-world malware samples,
we observe significantly improved accuracy compared to the traditional methods.
By implementing our n-gram approach, we achieved an accuracy of 99.02% across
various machine learning algorithms by using hybrid feature selection technique
to address high dimensionality. Hybrid feature selection technique reduces the
feature set to only 1.6% of the original features.

</details>


### [525] [The Hitchhiker's Guide to Efficient, End-to-End, and Tight DP Auditing](https://arxiv.org/abs/2506.16666)
*Meenatchi Sundaram Muthu Selva Annamalai,Borja Balle,Jamie Hayes,Georgios Kaissis,Emiliano De Cristofaro*

Main category: cs.CR

TL;DR: The paper establishes a systematic framework to audit Differential Privacy (DP) techniques and identifies challenges and future directions.


<details>
  <summary>Details</summary>
Motivation: Auditing DP techniques poses challenges due to efficiency, end-to-end applicability, and tightness of evaluations, motivating a structured analysis of the field.

Method: The authors introduce a framework for reviewing DP auditing techniques based on three criteria and analyze their modes of operation, including threat models, attacks, and evaluations.

Result: They uncover overlooked aspects, limiting factors, and open research challenges in DP auditing techniques.

Conclusion: Their methodology enables systematic progress assessment and highlights key focus areas for researchers in Differential Privacy auditing.

Abstract: This paper systematizes research on auditing Differential Privacy (DP)
techniques, aiming to identify key insights into the current state of the art
and open challenges. First, we introduce a comprehensive framework for
reviewing work in the field and establish three cross-contextual desiderata
that DP audits should target--namely, efficiency, end-to-end-ness, and
tightness. Then, we systematize the modes of operation of state-of-the-art DP
auditing techniques, including threat models, attacks, and evaluation
functions. This allows us to highlight key details overlooked by prior work,
analyze the limiting factors to achieving the three desiderata, and identify
open research problems. Overall, our work provides a reusable and systematic
methodology geared to assess progress in the field and identify friction points
and future directions for our community to focus on.

</details>


<div id='hep-th'></div>

# hep-th [[Back]](#toc)

### [526] [Approximate Ricci-flat Metrics for Calabi-Yau Manifolds](https://arxiv.org/abs/2506.15766)
*Seung-Joo Lee,Andre Lukas*

Main category: hep-th

TL;DR: The paper presents a method combining machine learning and numerical fitting for obtaining analytic Kähler potentials with approximately Ricci-flat Kähler metrics for Calabi-Yau manifolds.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of finding analytic expressions for Kähler potentials and Ricci-flat Kähler metrics on Calabi-Yau manifolds, which have both mathematical and physical significance.

Method: Utilizing machine learning techniques to numerically calculate Ricci-flat Kähler potentials and subsequently fitting these results to Donaldson's Ansatz for analytic representation.

Result: The method is applied to specific Calabi-Yau hypersurfaces in projective spaces, yielding simple analytic expressions for approximately Ricci-flat Kähler potentials with dependence limited to the modulus of the complex structure parameter.

Conclusion: This approach successfully produces analytic Kähler potentials for specific families, demonstrating efficiency in linking numerical techniques and analytic modeling for complex geometries.

Abstract: We outline a method to determine analytic K\"ahler potentials with associated
approximately Ricci-flat K\"ahler metrics on Calabi-Yau manifolds. Key
ingredients are numerically calculating Ricci-flat K\"ahler potentials via
machine learning techniques and fitting the numerical results to Donaldson's
Ansatz. We apply this method to the Dwork family of quintic hypersurfaces in
$\mathbb{P}^4$ and an analogous one-parameter family of bi-cubic CY
hypersurfaces in $\mathbb{P}^2\times\mathbb{P}^2$. In each case, a relatively
simple analytic expression is obtained for the approximately Ricci-flat
K\"ahler potentials, including the explicit dependence on the complex structure
parameter. We find that these K\"ahler potentials only depend on the modulus of
the complex structure parameter.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [527] [TRUST: Transparent, Robust and Ultra-Sparse Trees](https://arxiv.org/abs/2506.15791)
*Albert Dorador*

Main category: stat.ME

TL;DR: The paper introduces TRUST, a regression tree model that achieves high predictive accuracy comparable to Random Forests while maintaining interpretability and transparency.


<details>
  <summary>Details</summary>
Motivation: Piecewise-constant regression trees are interpretable but lag in predictive accuracy compared to black-box models such as Random Forests.

Method: TRUST integrates sparse linear models, shallow decision trees, and utilizes Large Language Models to enhance explanation transparency.

Result: TRUST consistently outperforms traditional interpretable models while matching Random Forest accuracy, and improves accuracy and interpretability compared to the M5' model.

Conclusion: The TRUST model effectively bridges the gap between interpretability and predictive accuracy, offering a transparent and robust alternative to existing models.

Abstract: Piecewise-constant regression trees remain popular for their
interpretability, yet often lag behind black-box models like Random Forest in
predictive accuracy. In this work, we introduce TRUST (Transparent, Robust, and
Ultra-Sparse Trees), a novel regression tree model that combines the accuracy
of Random Forests with the interpretability of shallow decision trees and
sparse linear models. TRUST further enhances transparency by leveraging Large
Language Models to generate tailored, user-friendly explanations. Extensive
validation on synthetic and real-world benchmark datasets demonstrates that
TRUST consistently outperforms other interpretable models -- including CART,
Lasso, and Node Harvest -- in predictive accuracy, while matching the accuracy
of Random Forest and offering substantial gains in both accuracy and
interpretability over M5', a well-established model that is conceptually
related.

</details>


### [528] [Summary Statistics of Large-scale Model Outputs for Observation-corrected Outputs](https://arxiv.org/abs/2506.15845)
*Atlanta Chakraborty,Julie Bessac*

Main category: stat.ME

TL;DR: The paper introduces Sig-PCA, a method combining physics-based models with observational data using neural networks to improve model accuracy while reducing computational complexity.


<details>
  <summary>Details</summary>
Motivation: Physics-based models are limited by biases and approximations, while observational data is sparse. Integrating these complementary sources helps improve the accuracy and reliability of predictions.

Method: A neural network-based framework, Sig-PCA, is utilized to merge reduced-order statistical summaries from physics-based models with observational data, facilitating corrections and enabling computationally efficient analysis.

Result: Sig-PCA was tested on datasets involving surface temperature and wind, showing improved alignment of model outputs with observational data, including accurate corrections of probability distributions and space-time correlations.

Conclusion: The framework successfully combines observational data and statistical summaries, demonstrating its potential to improve both accuracy and efficiency in multisource data integration.

Abstract: Physics-based models capture broad spatial and temporal dynamics, but often
suffer from biases and numerical approximations, while observations capture
localized variability but are sparse. Integrating these complementary data
modalities is important to improving the accuracy and reliability of model
outputs. Meanwhile, physics-based models typically generate large outputs that
are challenging to manipulate. In this paper, we propose Sig-PCA, a space-time
framework that integrates summary statistics from model outputs with localized
observations via a neural network (NN). By leveraging reduced-order
representations from physics-based models and integrating them with
observational data, our approach corrects model outputs, while allowing to work
with dimensionally-reduced quantities hence with smaller NNs. This framework
highlights the synergy between observational data and statistical summaries of
model outputs, and effectively combines multisource data by preserving
essential statistical information. We demonstrate our approach on two datasets
(surface temperature and surface wind) with different statistical properties
and different ratios of model to observational data. Our method corrects model
outputs to align closely with the observational data, specifically enabling to
correct probability distributions and space-time correlation structures.

</details>


### [529] [Leveraging Optimal Transport for Distributed Two-Sample Testing: An Integrated Transportation Distance-based Framework](https://arxiv.org/abs/2506.16047)
*Zhengqi Lin,Yan Chen*

Main category: stat.ME

TL;DR: The paper introduces the Integrated Transportation Distance (ITD) for distributed two-sample testing, addressing challenges in decentralized settings while ensuring privacy, with a robust performance demonstrated through simulations.


<details>
  <summary>Details</summary>
Motivation: To address challenges in data detection and distributional changes in federated learning environments, especially under privacy and heterogeneity constraints.

Method: The paper introduces ITD, a permutation test procedure for distributed settings, with theoretical analysis on convergence and asymptotic behavior.

Result: ITD exhibits strong power and Type I error control, effectively detecting subtle distributional shifts across decentralized data.

Conclusion: ITD is a robust and efficient tool for distributed two-sample testing, contributing to decentralized statistical inference with privacy preservation.

Abstract: This paper introduces a novel framework for distributed two-sample testing
using the Integrated Transportation Distance (ITD), an extension of the Optimal
Transport distance. The approach addresses the challenges of detecting
distributional changes in decentralized learning or federated learning
environments, where data privacy and heterogeneity are significant concerns. We
provide theoretical foundations for the ITD, including convergence properties
and asymptotic behavior. A permutation test procedure is proposed for practical
implementation in distributed settings, allowing for efficient computation
while preserving data privacy. The framework's performance is demonstrated
through theoretical power analysis and extensive simulations, showing robust
Type I error control and high power across various distributions and
dimensions. The results indicate that ITD effectively aggregates information
across distributed clients, detecting subtle distributional shifts that might
be missed when examining individual clients. This work contributes to the
growing field of distributed statistical inference, offering a powerful tool
for two-sample testing in modern, decentralized data environments.

</details>


### [530] [An introduction to Causal Modelling](https://arxiv.org/abs/2506.16486)
*Gauranga Kumar Baishya*

Main category: stat.ME

TL;DR: The paper provides a concise tutorial on modern causal modeling, combining potential outcomes and graphical methods, with practical examples for researchers.


<details>
  <summary>Details</summary>
Motivation: The paper aims to provide a clear and integrated understanding of causal questions and methods for applied researchers, bridging theoretical and practical aspects.

Method: The tutorial covers both potential outcome approaches, including counterfactual reasoning, treatment effects, and robust inference, and graphical methods such as causal graphs, d-separation, and structural equation models.

Result: The tutorial demonstrates how potential-outcome and graphical approaches can enhance each other, offering practical tools like inverse probability weighting and propensity score estimation.

Conclusion: By integrating these methods with clear explanations, the paper equips applied researchers with robust causal analysis tools for diverse settings.

Abstract: This tutorial provides a concise introduction to modern causal modeling by
integrating potential outcomes and graphical methods. We motivate causal
questions such as counterfactual reasoning under interventions and define
binary treatments and potential outcomes. We discuss causal effect
measures-including average treatment effects on the treated and on the
untreated-and choices of effect scales for binary outcomes. We derive
identification in randomized experiments under exchangeability and consistency,
and extend to stratification and blocking designs. We present inverse
probability weighting with propensity score estimation and robust inference via
sandwich estimators. Finally, we introduce causal graphs, d-separation, the
backdoor criterion, single-world intervention graphs, and structural equation
models, showing how graphical and potential-outcome approaches complement each
other. Emphasis is placed on clear notation, intuitive explanations, and
practical examples for applied researchers.

</details>


### [531] [Bayesian Joint Model of Multi-Sensor and Failure Event Data for Multi-Mode Failure Prediction](https://arxiv.org/abs/2506.17036)
*Sina Aghaee Dabaghan Fard,Minhee Kim,Akash Deep,Jaesung Lee*

Main category: stat.ME

TL;DR: This paper proposes a Bayesian model combining Cox proportional hazards, Gaussian Processes, and multinomial distributions to predict Remaining Useful Life (RUL) with uncertainty quantification for systems experiencing multiple failure modes.


<details>
  <summary>Details</summary>
Motivation: Existing models often handle failure modes and RUL prediction independently or use black-box methods lacking statistical rigor. There's a need for models that leverage multi-sensor data in a statistically robust way while accounting for uncertainty.

Method: The authors design a unified Bayesian framework integrating Cox proportional hazards, Convolved Multi-output Gaussian Processes, and multinomial distributions. Variational Bayes is employed for inference, while Monte Carlo sampling ensures robust prediction.

Result: The proposed model demonstrates its efficacy and advantages through comprehensive numerical and case studies, including applications on jet-engine datasets.

Conclusion: This hierarchical Bayesian approach improves RUL prediction accuracy while providing robust uncertainty quantification, addressing limitations in existing methods.

Abstract: Modern industrial systems are often subject to multiple failure modes, and
their conditions are monitored by multiple sensors, generating multiple
time-series signals. Additionally, time-to-failure data are commonly available.
Accurately predicting a system's remaining useful life (RUL) requires
effectively leveraging multi-sensor time-series data alongside multi-mode
failure event data. In most existing models, failure modes and RUL prediction
are performed independently, ignoring the inherent relationship between these
two tasks. Some models integrate multiple failure modes and event prediction
using black-box machine learning approaches, which lack statistical rigor and
cannot characterize the inherent uncertainty in the model and data. This paper
introduces a unified approach to jointly model the multi-sensor time-series
data and failure time concerning multiple failure modes. This proposed model
integrate a Cox proportional hazards model, a Convolved Multi-output Gaussian
Process, and multinomial failure mode distributions in a hierarchical Bayesian
framework with corresponding priors, enabling accurate prediction with robust
uncertainty quantification. Posterior distributions are effectively obtained by
Variational Bayes, and prediction is performed with Monte Carlo sampling. The
advantages of the proposed model is validated through extensive numerical and
case studies with jet-engine dataset.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [532] [From Generation to Adaptation: Comparing AI-Assisted Strategies in High School Programming Education](https://arxiv.org/abs/2506.15955)
*Tong Hu,Songzan Wang*

Main category: cs.CY

TL;DR: This study evaluates two approaches to programming education using language-constrained AI tools among novice students. Transitioning from abstract code generation to adapting small, functional code units led to significantly better results.


<details>
  <summary>Details</summary>
Motivation: The paper aims to investigate effective ways to integrate AI-assisted programming techniques into education to alleviate frustrations and optimize learning for novice programmers.

Method: Researchers tested two pedagogical approaches using LCAs in a competition setting—abstract code generation versus adapting minimal functional units—and contrasted their outcomes using high school students.

Result: The MFU-based approach achieved 100% MVP completion compared to only 20% in the abstract specification approach, showcasing the efficacy of leveraging small code examples.

Conclusion: Aligning AI tools with task-specific strengths and providing cognitive scaffolds through instructional design can maximize their benefits in learning environments, offering practical strategies for educators.

Abstract: This exploratory case study investigated two contrasting pedagogical
approaches for LCA-assisted programming with five novice high school students
preparing for a WeChat Mini Program competition. In Phase 1, students used LCAs
to generate code from abstract specifications (From-Scratch approach),
achieving only 20% MVP completion. In Phase 2, students adapted existing
Minimal Functional Units (MFUs), small, functional code examples, using LCAs,
achieving 100% MVP completion. Analysis revealed that the MFU-based approach
succeeded by aligning with LCA strengths in pattern modification rather than de
novo generation, while providing cognitive scaffolds that enabled students to
navigate complex development tasks. The study introduces a dual-scaffolding
model combining technical support (MFUs) with pedagogical guidance (structured
prompting strategies), demonstrating that effective LCA integration depends
less on AI capabilities than on instructional design. These findings offer
practical guidance for educators seeking to transform AI tools from sources of
frustration into productive learning partners in programming education.

</details>


### [533] [Teaching Complex Systems based on Microservices](https://arxiv.org/abs/2506.16492)
*Renato Cordeiro Ferreira,Thatiane de Oliveira Rosa,Alfredo Goldman,Eduardo Guerra*

Main category: cs.CY

TL;DR: The paper discusses teaching microservices development to senior undergraduates at the University of São Paulo, simulating real-world industry environments.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of teaching complex microservices systems in a way that mirrors real-world industry practices.

Method: Engaging over 80 senior undergraduate students in team-based learning to develop microservices, mimicking industry conditions.

Result: Demonstrated that senior undergraduate students in Computer Science and related fields can successfully learn advanced microservices concepts.

Conclusion: Teaching complex systems via microservices is effective when employing team-centered, industry-simulating approaches for senior-level students.

Abstract: Developing complex systems using microservices is a current challenge. In
this paper, we present our experience with teaching this subject to more than
80 students at the University of S\~ao Paulo (USP), fostering team work and
simulating the industry's environment. We show it is possible to teach such
advanced concepts for senior undergraduate students of Computer Science and
related fields.

</details>


### [534] [From Prompts to Constructs: A Dual-Validity Framework for LLM Research in Psychology](https://arxiv.org/abs/2506.16697)
*Zhicheng Lin*

Main category: cs.CY

TL;DR: The paper identifies issues in applying human psychological measurement tools to large language models (LLMs) and proposes a framework for robust validation in AI psychology.


<details>
  <summary>Details</summary>
Motivation: To address the inconsistencies and statistical artifacts arising from the use of human psychological measurement tools on large language models.

Method: Proposes a dual-validity framework that integrates reliable measurement principles and sound causal inference standards for studying LLMs in AI psychology.

Result: Clarifies the varying evidence requirements for different scientific claims about LLMs, such as text classification versus simulation of psychological states.

Conclusion: Advances can be achieved by developing computational analogues of psychological constructs and implementing scalable evidence standards rather than relying on traditional human measurement tools.

Abstract: Large language models (LLMs) are rapidly being adopted across psychology,
serving as research tools, experimental subjects, human simulators, and
computational models of cognition. However, the application of human
measurement tools to these systems can produce contradictory results, raising
concerns that many findings are measurement phantoms--statistical artifacts
rather than genuine psychological phenomena. In this Perspective, we argue that
building a robust science of AI psychology requires integrating two of our
field's foundational pillars: the principles of reliable measurement and the
standards for sound causal inference. We present a dual-validity framework to
guide this integration, which clarifies how the evidence needed to support a
claim scales with its scientific ambition. Using an LLM to classify text may
require only basic accuracy checks, whereas claiming it can simulate anxiety
demands a far more rigorous validation process. Current practice systematically
fails to meet these requirements, often treating statistical pattern matching
as evidence of psychological phenomena. The same model output--endorsing "I am
anxious"--requires different validation strategies depending on whether
researchers claim to measure, characterize, simulate, or model psychological
constructs. Moving forward requires developing computational analogues of
psychological constructs and establishing clear, scalable standards of evidence
rather than the uncritical application of human measurement tools.

</details>


### [535] [Large Language Models as Psychological Simulators: A Methodological Guide](https://arxiv.org/abs/2506.16702)
*Zhicheng Lin*

Main category: cs.CY

TL;DR: The paper offers a framework for using large language models (LLMs) in psychological research as simulators for roles/personas and as cognitive models, while also addressing challenges and ethical concerns.


<details>
  <summary>Details</summary>
Motivation: To provide methodological guidance for the emerging use of LLMs in psychological and behavioral research, which currently lacks sufficient structure.

Method: The paper proposes two major methods: using LLMs to simulate roles and personas validated by human data, and employing LLMs as cognitive models to study internal representations and human cognition systematically.

Result: The framework highlights the capabilities and constraints of LLMs, including biases, cultural limitations, and prompt brittleness, while also addressing challenges like prompt sensitivity and ethical considerations.

Conclusion: This framework equips researchers with a structured approach to utilize LLMs in psychological research, emphasizing the need for transparency and ethical responsibility.

Abstract: Large language models (LLMs) offer emerging opportunities for psychological
and behavioral research, but methodological guidance is lacking. This article
provides a framework for using LLMs as psychological simulators across two
primary applications: simulating roles and personas to explore diverse
contexts, and serving as computational models to investigate cognitive
processes. For simulation, we present methods for developing psychologically
grounded personas that move beyond demographic categories, with strategies for
validation against human data and use cases ranging from studying inaccessible
populations to prototyping research instruments. For cognitive modeling, we
synthesize emerging approaches for probing internal representations,
methodological advances in causal interventions, and strategies for relating
model behavior to human cognition. We address overarching challenges including
prompt sensitivity, temporal limitations from training data cutoffs, and
ethical considerations that extend beyond traditional human subjects review.
Throughout, we emphasize the need for transparency about model capabilities and
constraints. Together, this framework integrates emerging empirical evidence
about LLM performance--including systematic biases, cultural limitations, and
prompt brittleness--to help researchers wrangle these challenges and leverage
the unique capabilities of LLMs in psychological research.

</details>


### [536] [TrajSceneLLM: A Multimodal Perspective on Semantic GPS Trajectory Analysis](https://arxiv.org/abs/2506.16401)
*Chunhou Ji,Qiumeng Li*

Main category: cs.CY

TL;DR: The paper introduces TrajSceneLLM, a multimodal framework that enhances GPS trajectory analysis using semantic embeddings derived from visual map data and LLM-generated textual descriptions, significantly improving Travel Mode Identification (TMI) accuracy.


<details>
  <summary>Details</summary>
Motivation: Traditional methods struggle to deeply understand GPS trajectory data, mainly due to challenges in extracting semantic meanings and integrating contextual spatial information.

Method: The authors propose a novel framework, TrajSceneLLM, which combines visual map images and textual descriptions generated by LLMs to create semantic trajectory embeddings. The embeddings are paired with an MLP classifier for analysis.

Result: The proposed approach significantly improves the performance of Travel Mode Identification (TMI), showcasing its ability to capture deep semantic spatio-temporal dependencies without relying heavily on handcrafted features.

Conclusion: TrajSceneLLM demonstrates superior semantic understanding of GPS trajectories, promising improved performance in geospatial AI applications, with open-source code provided for further exploration.

Abstract: GPS trajectory data reveals valuable patterns of human mobility and urban
dynamics, supporting a variety of spatial applications. However, traditional
methods often struggle to extract deep semantic representations and incorporate
contextual map information. We propose TrajSceneLLM, a multimodal perspective
for enhancing semantic understanding of GPS trajectories. The framework
integrates visualized map images (encoding spatial context) and textual
descriptions generated through LLM reasoning (capturing temporal sequences and
movement dynamics). Separate embeddings are generated for each modality and
then concatenated to produce trajectory scene embeddings with rich semantic
content which are further paired with a simple MLP classifier. We validate the
proposed framework on Travel Mode Identification (TMI), a critical task for
analyzing travel choices and understanding mobility behavior. Our experiments
show that these embeddings achieve significant performance improvement,
highlighting the advantage of our LLM-driven method in capturing deep
spatio-temporal dependencies and reducing reliance on handcrafted features.
This semantic enhancement promises significant potential for diverse downstream
applications and future research in geospatial artificial intelligence. The
source code and dataset are publicly available at:
https://github.com/februarysea/TrajSceneLLM.

</details>


### [537] [LLM-Based Bot Broadens the Range of Arguments in Online Discussions, Even When Transparently Disclosed as AI](https://arxiv.org/abs/2506.17073)
*Valeria Vuk,Cristina Sarasua,Fabrizio Gilardi*

Main category: cs.CY

TL;DR: The study explores the use of an LLM-based bot to expand perspectives in online political discussions, demonstrating its success in enriching arguments without negative effects upon bot disclosure.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the need to address limited diversity in online political discussions, which can lead to extremism, lack of legitimacy, and polarization.

Method: The authors designed two pre-registered randomized experiments in a chatroom to evaluate a bot that identifies and introduces missing arguments in discussions.

Result: The bot effectively broadened the range of arguments, as validated by objective and subjective metrics, with bot disclosure as AI not significantly impacting its effectiveness.

Conclusion: LLM-based moderation tools hold promise for improving the diversity of perspectives in online political discussions and mitigating the downsides of homogeneous discourse.

Abstract: A wide range of participation is essential for democracy, as it helps prevent
the dominance of extreme views, erosion of legitimacy, and political
polarization. However, engagement in online political discussions often
features a limited spectrum of views due to high levels of self-selection and
the tendency of online platforms to facilitate exchanges primarily among
like-minded individuals. This study examines whether an LLM-based bot can widen
the scope of perspectives expressed by participants in online discussions
through two pre-registered randomized experiments conducted in a chatroom. We
evaluate the impact of a bot that actively monitors discussions, identifies
missing arguments, and introduces them into the conversation. The results
indicate that our bot significantly expands the range of arguments, as measured
by both objective and subjective metrics. Furthermore, disclosure of the bot as
AI does not significantly alter these effects. These findings suggest that
LLM-based moderation tools can positively influence online political discourse.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [538] [Code Rate Optimization via Neural Polar Decoders](https://arxiv.org/abs/2506.15836)
*Ziv Aharoni,Bashar Huleihel,Henry D Pfister,Haim H Permuter*

Main category: cs.IT

TL;DR: This paper introduces neural polar decoders (NPDs) to optimize communication code rates and input distributions for unknown channel models.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between theoretical capacity estimation and practical coding performance in communication systems, especially for unknown channel models.

Method: A two-phase process: Training phase (alternates between estimating mutual information using NPDs and optimizing input distribution) and inference phase (constructing polar codes with improved decoding performance).

Result: Experimental results show significant improvements in mutual information and bit error rates (BERs) for non-uniform input distributions in memoryless and finite-state channels, validated for block lengths up to 1024.

Conclusion: The approach is scalable and applicable to real-world communication systems, showcasing its effectiveness in optimizing code rates and input distributions.

Abstract: This paper proposes a method to optimize communication code rates via the
application of neural polar decoders (NPDs). Employing this approach enables
simultaneous optimization of code rates over input distributions while
providing a practical coding scheme within the framework of polar codes. The
proposed approach is designed for scenarios where the channel model is unknown,
treating the channel as a black box that produces output samples from input
samples. We employ polar codes to achieve our objectives, using NPDs to
estimate mutual information (MI) between the channel inputs and outputs, and
optimize a parametric model of the input distribution. The methodology involves
a two-phase process: a training phase and an inference phase. In the training
phase, two steps are repeated interchangeably. First, the estimation step
estimates the MI of the channel inputs and outputs via NPDs. Second, the
improvement step optimizes the input distribution parameters to maximize the MI
estimate obtained by the NPDs. In the inference phase, the optimized model is
used to construct polar codes. This involves incorporating the Honda-Yamamoto
(HY) scheme to accommodate the optimized input distributions and list decoding
to enhance decoding performance. Experimental results on memoryless and
finite-state channels (FSCs) demonstrate the effectiveness of our approach,
particularly in cases where the channel's capacity-achieving input distribution
is non-uniform. For these cases, we show significant improvements in MI and bit
error rates (BERs) over those achieved by uniform and independent and
identically distributed (i.i.d.) input distributions, validating our method for
block lengths up to 1024. This scalable approach has potential applications in
real-world communication systems, bridging theoretical capacity estimation and
practical coding performance.

</details>


### [539] [Neural Polar Decoders for DNA Data Storage](https://arxiv.org/abs/2506.17076)
*Ziv Aharoni,Henry D. Pfister*

Main category: cs.IT

TL;DR: The paper introduces neural polar decoders (NPDs) as a low-complexity solution for decoding synchronization error channels in DNA data storage systems, demonstrating near-optimal performance and efficient resource usage.


<details>
  <summary>Details</summary>
Motivation: DNA data storage systems face challenges due to synchronization errors like insertions and deletions, necessitating robust decoding strategies that are computationally efficient.

Method: The authors propose neural polar decoders (NPDs), which use a data-driven approach for decoding. NPDs require only sample access to channels without explicit channel models and offer mutual information estimates for optimization.

Result: NPDs show near-optimal performance and accurate mutual information estimation for synthetic deletion and IDS channels. They outperform existing methods in realistic DNA storage scenarios and use fewer resources.

Conclusion: Neural polar decoders are highlighted as effective tools for robust and efficient decoding in channels with synchronization errors, promising advancements in DNA data storage systems.

Abstract: Synchronization errors, such as insertions and deletions, present a
fundamental challenge in DNA-based data storage systems, arising from both
synthesis and sequencing noise. These channels are often modeled as
insertion-deletion-substitution (IDS) channels, for which designing
maximum-likelihood decoders is computationally expensive. In this work, we
propose a data-driven approach based on neural polar decoders (NPDs) to design
low-complexity decoders for channels with synchronization errors. The proposed
architecture enables decoding over IDS channels with reduced complexity $O(AN
log N )$, where $A$ is a tunable parameter independent of the channel. NPDs
require only sample access to the channel and can be trained without an
explicit channel model. Additionally, NPDs provide mutual information (MI)
estimates that can be used to optimize input distributions and code design. We
demonstrate the effectiveness of NPDs on both synthetic deletion and IDS
channels. For deletion channels, we show that NPDs achieve near-optimal
decoding performance and accurate MI estimation, with significantly lower
complexity than trellis-based decoders. We also provide numerical estimates of
the channel capacity for the deletion channel. We extend our evaluation to
realistic DNA storage settings, including channels with multiple noisy reads
and real-world Nanopore sequencing data. Our results show that NPDs match or
surpass the performance of existing methods while using significantly fewer
parameters than the state-of-the-art. These findings highlight the promise of
NPDs for robust and efficient decoding in DNA data storage systems.

</details>


<div id='cs.SY'></div>

# cs.SY [[Back]](#toc)

### [540] [Formal Control for Uncertain Systems via Contract-Based Probabilistic Surrogates (Extended Version)](https://arxiv.org/abs/2506.16971)
*Oliver Schön,Sofie Haesaert,Sadegh Soudjani*

Main category: cs.SY

TL;DR: The paper proposes an abstraction-based technique that enhances scalability for modeling stochastic systems without directly computing error bounds.


<details>
  <summary>Details</summary>
Motivation: Current methods struggle with scalability and effective decision-making due to the complexity of models for formal correctness in stochastic systems.

Method: The proposed method eliminates the need to compute error bounds directly, focuses on probabilistic simulation relations and surrogate models, and applies abstraction techniques.

Result: The approach scales effectively to higher dimensions and addresses nonlinear agent-environment interactions with infinite-horizon temporal logic amidst uncertainties.

Conclusion: The method trades scalability for conservatism in a favorable way and is validated using a high-dimensional vehicle intersection case study.

Abstract: The requirement for identifying accurate system representations has not only
been a challenge to fulfill, but it has compromised the scalability of formal
methods, as the resulting models are often too complex for effective decision
making with formal correctness and performance guarantees. Focusing on
probabilistic simulation relations and surrogate models of stochastic systems,
we propose an approach that significantly enhances the scalability and
practical applicability of such simulation relations by eliminating the need to
compute error bounds directly. As a result, we provide an abstraction-based
technique that scales effectively to higher dimensions while addressing complex
nonlinear agent-environment interactions with infinite-horizon temporal logic
guarantees amidst uncertainty. Our approach trades scalability for conservatism
favorably, as demonstrated on a complex high-dimensional vehicle intersection
case study.

</details>


<div id='cond-mat.str-el'></div>

# cond-mat.str-el [[Back]](#toc)

### [541] [Simulating Correlated Electrons with Symmetry-Enforced Normalizing Flows](https://arxiv.org/abs/2506.17015)
*Dominic Schuh,Janik Kreit,Evan Berkowitz,Lena Funcke,Thomas Luu,Kim A. Nicoli,Marcel Rodekamp*

Main category: cond-mat.str-el

TL;DR: The authors demonstrate that normalizing flows can learn the Boltzmann distribution of the fermionic Hubbard model with greater accuracy and efficiency than traditional methods.


<details>
  <summary>Details</summary>
Motivation: To overcome biases and inefficiencies such as ergodicity issues encountered by state-of-the-art methods like Hybrid Monte Carlo in studying the fermionic Hubbard model.

Method: The method integrates symmetry-aware architectures and independent, identically distributed sampling to train normalizing flows for accurate Boltzmann distribution learning.

Result: The proposed approach resolves ergodicity issues near the time-continuum limit and provides significant computational speed-ups over traditional approaches.

Conclusion: The study showcases the potential of normalizing flows for modeling the Boltzmann distribution in highly complex physical systems such as the fermionic Hubbard model.

Abstract: We present the first proof of principle that normalizing flows can accurately
learn the Boltzmann distribution of the fermionic Hubbard model - a key
framework for describing the electronic structure of graphene and related
materials. State-of-the-art methods like Hybrid Monte Carlo often suffer from
ergodicity issues near the time-continuum limit, leading to biased estimates.
Leveraging symmetry-aware architectures as well as independent and identically
distributed sampling, our approach resolves these issues and achieves
significant speed-ups over traditional methods.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [542] [Searching for a Hidden Markov Anomaly over Multiple Processes](https://arxiv.org/abs/2506.17108)
*Levli Citron,Kobi Cohen,Qing Zhao*

Main category: eess.SP

TL;DR: The paper focuses on detecting anomalies among multiple processes evolving under a hidden Markov model. It introduces the ADHM algorithm for efficient anomaly detection using predictive updates and statistical evidence.


<details>
  <summary>Details</summary>
Motivation: The study aims to improve anomaly detection methods by addressing scenarios where observations are dependent on processes governed by hidden Markov models, rather than assuming i.i.d. observations.

Method: A novel algorithm called ADHM is formulated, combining dynamic probing strategies, accumulated evidence, and belief updates on hidden states to enhance anomaly detection.

Result: The proposed ADHM algorithm outperformed existing anomaly detection methods in extensive simulations and is backed by theoretical guarantees regarding detection limits.

Conclusion: The ADHM approach optimally utilizes temporal correlations and enhances detection capabilities, offering a promising advancement for sequential anomaly detection in hidden Markov model environments.

Abstract: We address the problem of detecting an anomalous process among a large number
of processes. At each time t, normal processes are in state zero (normal
state), while the abnormal process may be in either state zero (normal state)
or state one (abnormal state), with the states being hidden. The transition
between states for the abnormal process is governed by a Markov chain over
time. At each time step, observations can be drawn from a selected subset of
processes. Each probed process generates an observation depending on its hidden
state, either a typical distribution under state zero or an abnormal
distribution under state one. The objective is to design a sequential search
strategy that minimizes the expected detection time, subject to an error
probability constraint. In contrast to prior works that assume i.i.d.
observations, we address a new setting where anomalies evolve according to a
hidden Markov model. To this end, we propose a novel algorithm, dubbed Anomaly
Detection under Hidden Markov model (ADHM), which dynamically adapts the
probing strategy based on accumulated statistical evidence and predictive
belief updates over hidden states. ADHM effectively leverages temporal
correlations to focus sensing resources on the most informative processes. The
algorithm is supported by an asymptotic theoretical foundation, grounded in an
oracle analysis that characterizes the fundamental limits of detection under
the assumption of a known distribution of the hidden states. In addition, the
algorithm demonstrates strong empirical performance, consistently outperforming
existing methods in extensive simulations.

</details>


### [543] [Empowering Near-Field Communications in Low-Altitude Economy with LLM: Fundamentals, Potentials, Solutions, and Future Directions](https://arxiv.org/abs/2506.17067)
*Zhuo Xu,Tianyue Zheng,Linglong Dai*

Main category: eess.SP

TL;DR: This paper explores using large language models (LLM) to address challenges in near-field communications of the low-altitude economy (LAE), focusing on enhancing signal processing and user distinction.


<details>
  <summary>Details</summary>
Motivation: To address the challenges in near-field communications within LAE, such as increased signal processing complexity and distinguishing between far and near-field users.

Method: Integrating LLM with near-field XL-MIMO systems to facilitate solutions, including a case study on multi-user precoding matrix design and user distinction.

Result: Presentation of an LLM-based scheme for near-field communications, with demonstration of its capability in joint user distinction and matrix design.

Conclusion: LLM shows potential to solve complex problems in near-field LAE communication systems, offering new research directions while tackling existing challenges.

Abstract: The low-altitude economy (LAE) is gaining significant attention from academia
and industry. Fortunately, LAE naturally aligns with near-field communications
in extremely large-scale MIMO (XL-MIMO) systems. By leveraging near-field
beamfocusing, LAE can precisely direct beam energy to unmanned aerial vehicles,
while the additional distance dimension boosts overall spectrum efficiency.
However, near-field communications in LAE still face several challenges, such
as the increase in signal processing complexity and the necessity of
distinguishing between far and near-field users. Inspired by the large language
models (LLM) with powerful ability to handle complex problems, we apply LLM to
solve challenges of near-field communications in LAE. The objective of this
article is to provide a comprehensive analysis and discussion on LLM-empowered
near-field communications in LAE. Specifically, we first introduce fundamentals
of LLM and near-field communications, including the key advantages of LLM and
key characteristics of near-field communications. Then, we reveal the
opportunities and challenges of near-field communications in LAE. To address
these challenges, we present a LLM-based scheme for near-field communications
in LAE, and provide a case study which jointly distinguishes far and near-field
users and designs multi-user precoding matrix. Finally, we outline and
highlight several future research directions and open issues.

</details>


<div id='astro-ph.GA'></div>

# astro-ph.GA [[Back]](#toc)

### [544] [Can AI Dream of Unseen Galaxies? Conditional Diffusion Model for Galaxy Morphology Augmentation](https://arxiv.org/abs/2506.16233)
*Chenrui Ma,Zechang Sun,Tao Jing,Zheng Cai,Yuan-Sen Ting,Song Huang,Mingyu Li*

Main category: astro-ph.GA

TL;DR: The paper proposes a generative model using conditional diffusion to synthesize galaxy images for machine learning, resulting in increased performance in classification and rare object detection.


<details>
  <summary>Details</summary>
Motivation: Machine learning models for astronomy face challenges in generalizing due to limited datasets, especially for rare objects critical to scientific discovery.

Method: A conditional diffusion model is used to generate synthetic, realistic galaxy images based on the Galaxy Zoo 2 dataset, enabling data augmentation and extrapolation for unseen domains.

Result: Incorporating synthesized images improves standard morphology classification by up to 30% and doubles detection of rare objects like dust lane galaxies.

Conclusion: Generative models effectively address data scarcity in observational astronomy, paving the way for advanced methods in rare object detection and astrophysical research.

Abstract: Observational astronomy relies on visual feature identification to detect
critical astrophysical phenomena. While machine learning (ML) increasingly
automates this process, models often struggle with generalization in
large-scale surveys due to the limited representativeness of labeled datasets
-- whether from simulations or human annotation -- a challenge pronounced for
rare yet scientifically valuable objects. To address this, we propose a
conditional diffusion model to synthesize realistic galaxy images for
augmenting ML training data. Leveraging the Galaxy Zoo 2 dataset which contains
visual feature -- galaxy image pairs from volunteer annotation, we demonstrate
that our model generates diverse, high-fidelity galaxy images closely adhere to
the specified morphological feature conditions. Moreover, this model enables
generative extrapolation to project well-annotated data into unseen domains and
advancing rare object detection. Integrating synthesized images into ML
pipelines improves performance in standard morphology classification, boosting
completeness and purity by up to 30\% across key metrics. For rare object
detection, using early-type galaxies with prominent dust lane features (
$\sim$0.1\% in GZ2 dataset) as a test case, our approach doubled the number of
detected instances from 352 to 872, compared to previous studies based on
visual inspection. This study highlights the power of generative models to
bridge gaps between scarce labeled data and the vast, uncharted parameter space
of observational astronomy and sheds insight for future astrophysical
foundation model developments. Our project homepage is available at
https://galaxysd-webpage.streamlit.app/.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [545] [Autonomous Trajectory Optimization for UAVs in Disaster Zone Using Henry Gas Optimization Scheme](https://arxiv.org/abs/2506.15910)
*Zakria Qadir,Muhammad Bilal,Guoqiang Liu,Xiaolong Xu*

Main category: eess.SY

TL;DR: This paper proposes a metaheuristic optimization model for UAV trajectory planning using Henry gas optimization (HGO) algorithm, demonstrating superior performance compared to multiple existing techniques.


<details>
  <summary>Details</summary>
Motivation: Optimizing UAV trajectories in disaster-prone environments is critical for aiding rescue operations and maintaining connectivity, while ensuring minimal transportation cost and computational complexity.

Method: The study introduces a mathematical model utilizing the Henry Gas Optimization (HGO) algorithm for trajectory planning and compares its performance against other metaheuristic algorithms like PSO, GWO, CSA, and BMO in four distinct environmental scenarios.

Result: The HGO algorithm outperforms existing models, particularly showing a 39.3% reduction in transportation cost and a 16.8% reduction in computational time in ambient environments compared to PSO.

Conclusion: The proposed HGO algorithm is robust and effective for UAV trajectory optimization, especially in disaster scenarios, paving the way for its application in smart cities.

Abstract: The unmanned aerial vehicles (UAVs) in a disaster-prone environment plays
important role in assisting the rescue services and providing the internet
connectivity with the outside world. However, in such a complex environment the
selection of optimum trajectory of UAVs is of utmost importance. UAV trajectory
optimization deals with finding the shortest path in the minimal possible time.
In this paper, a cluster optimization scheme (COS) is proposed using the Henry
gas optimization (HGO) metaheuristic algorithm to identify the shortest path
having minimal transportation cost and algorithm complexity. The mathematical
model is designed for COS using the HGO algorithm and compared with the
state-of-the-art metaheuristic algorithms such as particle swarm optimization
(PSO), grey wolf optimization (GWO), cuckoo search algorithm (CSA) and
barnacles mating optimizer (BMO). In order to prove the robustness of the
proposed model, four different scenarios are evaluated that includes ambient
environment, constrict environment, tangled environment, and complex
environment. In all the aforementioned scenarios, the HGO algorithm outperforms
the existing algorithms. Particularly, in the ambient environment, the HGO
algorithm achieves a 39.3% reduction in transportation cost and a 16.8%
reduction in computational time as compared to the PSO algorithm. Hence, the
HGO algorithm can be used for autonomous trajectory optimization of UAVs in
smart cities.

</details>


### [546] [Vision-Based Multirotor Control for Spherical Target Tracking: A Bearing-Angle Approach](https://arxiv.org/abs/2506.16870)
*Marcelo Jacinto,Rita Cunha*

Main category: eess.SY

TL;DR: This study designs an adaptive nonlinear controller for multirotor vehicles to track a moving spherical target with unknown radius, utilizing new system coordinates and a constant acceleration target model.


<details>
  <summary>Details</summary>
Motivation: To enable multirotor vehicles to accurately track dynamic moving targets, particularly spherical ones, even with incomplete information like an unknown radius.

Method: The authors transform camera sensor bearings into "bearing-angle" pairs, derive system dynamics in these coordinates, and propose an adaptive control algorithm based on constant acceleration target modeling.

Result: Simulations demonstrate the effectiveness of the control algorithm for tracking and adapting to target dynamics.

Conclusion: The proposed method successfully handles the tracking challenge for a moving spherical target with unknown parameters, leveraging geometry-driven control principles.

Abstract: This work addresses the problem of designing a visual servo controller for a
multirotor vehicle, with the end goal of tracking a moving spherical target
with unknown radius. To address this problem, we first transform two bearing
measurements provided by a camera sensor into a bearing-angle pair. We then use
this information to derive the system's dynamics in a new set of coordinates,
where the angle measurement is used to quantify a relative distance to the
target. Building on this system representation, we design an adaptive nonlinear
control algorithm that takes advantage of the properties of the new system
geometry and assumes that the target follows a constant acceleration model.
Simulation results illustrate the performance of the proposed control
algorithm.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [547] [Optimal Navigation in Microfluidics via the Optimization of a Discrete Loss](https://arxiv.org/abs/2506.15902)
*Petr Karnakov,Lucas Amoudruz,Petros Koumoutsakos*

Main category: physics.comp-ph

TL;DR: This paper introduces ODIL, a closed-loop control method addressing the challenges of microscopic devices navigating in fluid, offering advantages like robustness and speed.


<details>
  <summary>Details</summary>
Motivation: Navigating microscopic devices in fluid environments for applications like drug delivery or environmental monitoring is complex due to device-flow interactions.

Method: The paper proposes ODIL, a closed-loop control method optimizing a discrete loss based on dynamics and path objectives, compared to reinforcement learning.

Result: ODIL is shown to be robust, up to three orders faster, and performs effectively in high-dimensional action/state spaces.

Conclusion: ODIL is a promising tool for controlling microdevices in complex fluid flows, surpassing reinforcement learning in several critical aspects.

Abstract: Optimal path planning and control of microscopic devices navigating in fluid
environments is essential for applications ranging from targeted drug delivery
to environmental monitoring. These tasks are challenging due to the complexity
of microdevice-flow interactions. We introduce a closed-loop control method
that optimizes a discrete loss (ODIL) in terms of dynamics and path objectives.
In comparison with reinforcement learning, ODIL is more robust, up to three
orders faster, and excels in high-dimensional action/state spaces, making it a
powerful tool for navigating complex flow environments.

</details>


### [548] [A Neural Operator based Hybrid Microscale Model for Multiscale Simulation of Rate-Dependent Materials](https://arxiv.org/abs/2506.16918)
*Dhananjeyan Jeyaraj,Hamidreza Eivazi,Jendrik-Alexander Tröger,Stefan Wittek,Stefan Hartmann,Andreas Rausch*

Main category: physics.comp-ph

TL;DR: This paper combines neural operators with physics-based multiscale modeling to predict viscoelastic material behavior efficiently, achieving <6% error and ~100x computational speedup.


<details>
  <summary>Details</summary>
Motivation: Address the computational intensity of multiscale modeling methods like the FE2 approach, especially in simulating micro-macro interactions across scales.

Method: A hybrid model integrating neural operators into computational homogenization is utilized. Physics principles and microscale constitutive relations are built into the model architecture, allowing for simulations of viscoelastic material behavior.

Result: The approach demonstrated high accuracy (less than 6% error in homogenized stress predictions) and achieved a computational speedup of approximately 100x.

Conclusion: This hybrid approach provides a scalable and efficient solution for multiscale modeling, blending physics-based accuracy with the computational efficiency of neural operators.

Abstract: The behavior of materials is influenced by a wide range of phenomena
occurring across various time and length scales. To better understand the
impact of microstructure on macroscopic response, multiscale modeling
strategies are essential. Numerical methods, such as the $\text{FE}^2$
approach, account for micro-macro interactions to predict the global response
in a concurrent manner. However, these methods are computationally intensive
due to the repeated evaluations of the microscale. This challenge has led to
the integration of deep learning techniques into computational homogenization
frameworks to accelerate multiscale simulations. In this work, we employ neural
operators to predict the microscale physics, resulting in a hybrid model that
combines data-driven and physics-based approaches. This allows for
physics-guided learning and provides flexibility for different materials and
spatial discretizations. We apply this method to time-dependent solid mechanics
problems involving viscoelastic material behavior, where the state is
represented by internal variables only at the microscale. The constitutive
relations of the microscale are incorporated into the model architecture and
the internal variables are computed based on established physical principles.
The results for homogenized stresses ($<6\%$ error) show that the approach is
computationally efficient ($\sim 100 \times$ faster).

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [549] [Modern approaches to building effective interpretable models of the property market using machine learning](https://arxiv.org/abs/2506.15723)
*Irina G. Tanashkina,Alexey S. Tanashkin,Alexander S. Maksimchuik,Anna Yu. Poshivailo*

Main category: q-fin.ST

TL;DR: This paper discusses methods for building interpretable machine learning models for property market valuation in Russia, combining geostatistics and RuleFit techniques.


<details>
  <summary>Details</summary>
Motivation: Researchers face challenges in building accurate property market valuation models due to noisy real-world data and the need for interpretability.

Method: The paper outlines a process involving data collection, outlier identification, pattern analysis, factor selection, and model building using linear regression, geostatistics, and RuleFit methods.

Result: Combination of traditional and advanced techniques yielded effective models for land parcels and flats, considering legal practicalities.

Conclusion: Effective and interpretable property valuation models can be achieved despite challenges associated with noisy data and interpretability constraints.

Abstract: In this article, we review modern approaches to building interpretable models
of property markets using machine learning on the base of mass valuation of
property in the Primorye region, Russia. The researcher, lacking expertise in
this topic, encounters numerous difficulties in the effort to build a good
model. The main source of this is the huge difference between noisy real market
data and ideal data which is very common in all types of tutorials on machine
learning. This paper covers all stages of modeling: the collection of initial
data, identification of outliers, the search and analysis of patterns in data,
the formation and final choice of price factors, the building of the model, and
the evaluation of its efficiency. For each stage, we highlight potential issues
and describe sound methods for overcoming emerging difficulties on actual
examples. We show that the combination of classical linear regression with
interpolation methods of geostatistics allows to build an effective model for
land parcels. For flats, when many objects are attributed to one spatial point
the application of geostatistical methods is difficult. Therefore we suggest
linear regression with automatic generation and selection of additional rules
on the base of decision trees, so called the RuleFit method. Thus we show, that
despite the strong restriction as the requirement of interpretability which is
important in practical aspects, for example, legal matters, it is still
possible to build effective models of real property markets.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [550] [Comparison of substructured non-overlapping domain decomposition and overlapping additive Schwarz methods for large-scale Helmholtz problems with multiple sources](https://arxiv.org/abs/2506.16875)
*Boris Martin,Pierre Jolivet,Christophe Geuzaine*

Main category: math.NA

TL;DR: This paper analyzes methods to solve large-scale 3D Helmholtz problems, comparing non-overlapping substructured domain decomposition methods (DDM) and overlapping methods like Optimized Restricted Additive Schwarz (ORAS) preconditioners.


<details>
  <summary>Details</summary>
Motivation: Helmholtz problems, especially in 3D, are computationally expensive and difficult to solve efficiently using conventional methods. This study is motivated by the necessity to determine the best approaches for tackling such problems in realistic scenarios like geophysics.

Method: Two domain decomposition methods are compared: non-overlapping substructured DDM and overlapping ORAS preconditioners. Both methods are applied to Helmholtz problems with multiple sources in realistic test cases.

Result: The non-overlapping methods, when optimally tuned, reduced convergence gaps and demonstrated better computational efficiency compared to overlapping methods.

Conclusion: Non-overlapping substructured DDM can outperform overlapping methods like ORAS preconditioners for large-scale 3D Helmholtz problems when appropriately optimized.

Abstract: Solving large-scale Helmholtz problems discretized with high-order finite
elements is notoriously difficult, especially in 3D where direct factorization
of the system matrix is very expensive and memory demanding, and robust
convergence of iterative methods is difficult to obtain. Domain decomposition
methods (DDM) constitute one of the most promising strategy so far, by
combining direct and iterative approaches: using direct solvers on overlapping
or non-overlapping subdomains, as a preconditioner for a Krylov subspace method
on the original Helmholtz system or as an iterative solver on a substructured
problem involving field values or Lagrange multipliers on the interfaces
between the subdomains. In this work we compare the computational performance
of non-overlapping substructured DDM and Optimized Restricted Additive Schwarz
(ORAS) preconditioners for solving large-scale Helmholtz problems with multiple
sources, as is encountered, e.g., in frequency-domain Full Waveform Inversion.
We show on a realistic geophysical test-case that, when appropriately tuned,
the non-overlapping methods can reduce the convergence gap sufficiently to
significantly outperform the overlapping methods.

</details>


### [551] [Convergent Methods for Koopman Operators on Reproducing Kernel Hilbert Spaces](https://arxiv.org/abs/2506.15782)
*Nicolas Boullé,Matthew J. Colbrook,Gustav Conradie*

Main category: math.NA

TL;DR: This study introduces algorithms for spectral analysis of Koopman operators in reproducing kernel Hilbert spaces (RKHSs), offering practical advantages such as error bounds, improved computations, and efficiency in high-dimensional datasets.


<details>
  <summary>Details</summary>
Motivation: Traditional Koopman operator studies are limited to square-integrable function spaces, which have computational challenges, especially in large datasets. Using RKHSs aims to address these limitations while providing enhanced capabilities.

Method: The authors propose rigorous, data-driven algorithms for Koopman and Perron--Frobenius operator analysis on RKHSs. These methods exploit RKHS structures to allow efficient spectral computations without requiring large-data limits and use kernels instead of quadrature-based sampling.

Result: The algorithms effectively compute spectra, pseudospectra, and spectral measures with error controls, even for challenging high-dimensional and real-world datasets, such as turbulent flows and molecular dynamics simulations.

Conclusion: The proposed algorithms are both theoretically optimal and practically effective, providing a significant step forward in operator-based dynamical system analysis. Their availability in the SpecRKHS software package ensures accessibility for broader use.

Abstract: Data-driven spectral analysis of Koopman operators is a powerful tool for
understanding numerous real-world dynamical systems, from neuronal activity to
variations in sea surface temperature. The Koopman operator acts on a function
space and is most commonly studied on the space of square-integrable functions.
However, defining it on a suitable reproducing kernel Hilbert space (RKHS)
offers numerous practical advantages, including pointwise predictions with
error bounds, improved spectral properties that facilitate computations, and
more efficient algorithms, particularly in high dimensions. We introduce the
first general, provably convergent, data-driven algorithms for computing
spectral properties of Koopman and Perron--Frobenius operators on RKHSs. These
methods efficiently compute spectra and pseudospectra with error control and
spectral measures while exploiting the RKHS structure to avoid the large-data
limits required in the $L^2$ settings. The function space is determined by a
user-specified kernel, eliminating the need for quadrature-based sampling as in
$L^2$ and enabling greater flexibility with finite, externally provided
datasets. Using the Solvability Complexity Index hierarchy, we construct
adversarial dynamical systems for these problems to show that no algorithm can
succeed in fewer limits, thereby proving the optimality of our algorithms.
Notably, this impossibility extends to randomized algorithms and datasets. We
demonstrate the effectiveness of our algorithms on challenging,
high-dimensional datasets arising from real-world measurements and
high-fidelity numerical simulations, including turbulent channel flow,
molecular dynamics of a binding protein, Antarctic sea ice concentration, and
Northern Hemisphere sea surface height. The algorithms are publicly available
in the software package $\texttt{SpecRKHS}$.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [552] [Contactless Precision Steering of Particles in a Fluid inside a Cube with Rotating Walls](https://arxiv.org/abs/2506.15958)
*Lucas Amoudruz,Petr Karnakov,Petros Koumoutsakos*

Main category: physics.flu-dyn

TL;DR: The paper introduces a novel algorithm using rotating disks and the ODIL framework to steer multiple particles in flow for contactless manipulation, targeting biomedical uses.


<details>
  <summary>Details</summary>
Motivation: To develop robust and precise contactless manipulation methods suited for biomedical and chemical applications.

Method: The method employs rotating disks to generate controllable flow fields, guided by a feedback control policy using the ODIL framework.

Result: The system successfully transported two beads simultaneously to predefined locations in both simulations and physical experiments.

Conclusion: This approach enhances contactless particle manipulation, overcoming challenges in trapping multiple particles in fluids, with potential for biomedical applications.

Abstract: Contactless manipulation of small objects is essential for biomedical and
chemical applications, such as cell analysis, assisted fertilisation, and
precision chemistry. Established methods, including optical, acoustic, and
magnetic tweezers, are now complemented by flow control techniques that use
flow-induced motion to enable precise and versatile manipulation. However,
trapping multiple particles in fluid remains a challenge. This study introduces
a novel control algorithm capable of steering multiple particles in flow. The
system uses rotating disks to generate flow fields that transport particles to
precise locations. Disk rotations are governed by a feedback control policy
based on the Optimising a Discrete Loss (ODIL) framework, which combines fluid
dynamics equations with path objectives into a single loss function. Our
experiments, conducted in both simulations and with the physical device,
demonstrate the capability of the approach to transport two beads
simultaneously to predefined locations, advancing robust contactless particle
manipulation for biomedical applications.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [553] [RecBayes: Recurrent Bayesian Ad Hoc Teamwork in Large Partially Observable Domains](https://arxiv.org/abs/2506.15756)
*João G. Ribeiro,Yaniv Oren,Alberto Sardinha,Matthijs Spaan,Francisco S. Melo*

Main category: cs.MA

TL;DR: This paper introduces RecBayes, a novel method for ad hoc teamwork under partial observability, which enables agents to operate effectively without accessing environment states or teammate actions.


<details>
  <summary>Details</summary>
Motivation: Existing approaches for ad hoc teamwork have limitations, requiring environment states, teammates' actions, or small-scale environments. This paper aims to overcome these limitations.

Method: RecBayes uses a recurrent Bayesian classifier trained on past experiences to identify known team configurations and tasks from partial observations alone.

Result: Experimental results show that RecBayes scales effectively to large environments with up to 1 million states and can identify task information and assist teams effectively using only partial observations.

Conclusion: RecBayes is a scalable and robust approach that overcomes key limitations of prior methods for ad hoc teamwork under partial observability, enabling effective collaboration in complex environments.

Abstract: This paper proposes RecBayes, a novel approach for ad hoc teamwork under
partial observability, a setting where agents are deployed on-the-fly to
environments where pre-existing teams operate, that never requires, at any
stage, access to the states of the environment or the actions of its teammates.
We show that by relying on a recurrent Bayesian classifier trained using past
experiences, an ad hoc agent is effectively able to identify known teams and
tasks being performed from observations alone. Unlike recent approaches such as
PO-GPL (Gu et al., 2021) and FEAT (Rahman et al., 2023), that require at some
stage fully observable states of the environment, actions of teammates, or
both, or approaches such as ATPO (Ribeiro et al., 2023) that require the
environments to be small enough to be tabularly modelled (Ribeiro et al.,
2023), in their work up to 4.8K states and 1.7K observations, we show RecBayes
is both able to handle arbitrarily large spaces while never relying on either
states and teammates' actions. Our results in benchmark domains from the
multi-agent systems literature, adapted for partial observability and scaled up
to 1M states and 2^125 observations, show that RecBayes is effective at
identifying known teams and tasks being performed from partial observations
alone, and as a result, is able to assist the teams in solving the tasks
effectively.

</details>


### [554] [Generalizable Agent Modeling for Agent Collaboration-Competition Adaptation with Multi-Retrieval and Dynamic Generation](https://arxiv.org/abs/2506.16718)
*Chenxu Wang,Yonggang Jin,Cheng Hu,Youpeng Zhao,Zipeng Dai,Jian Zhao,Shiyu Huang,Liuyu Xiang,Junge Zhang,Zhaofeng He*

Main category: cs.MA

TL;DR: The paper introduces a new setting called Agent Collaborative-Competitive Adaptation (ACCA) to adapt agents in multi-agent systems, along with a novel modeling approach known as Multi-Retrieval and Dynamic Generation (MRDG).


<details>
  <summary>Details</summary>
Motivation: Adapting agents to multi-agent contexts requires flexibility in handling diverse tasks, environments, and dynamic interactions with unknown counterparts. Existing simplified scenarios do not address these comprehensive challenges.

Method: Proposes the ACCA framework combined with MRDG modeling. MRDG uses behavioral trajectory modeling of teammates and opponents, positional encoding for variable team sizes, hypernetwork modules for adaptability, and viewpoint alignment modules for synchronization.

Result: Testing conducted on scenarios like SMAC, Overcooked-AI, and Melting Pot shows that the MRDG approach outperforms existing methods in collaborating and competing with unknown teammates and opponents.

Conclusion: MRDG demonstrates significant improvements in agent adaptability, enhancing robust performance in dynamic multi-agent settings. The approach is validated through rigorous benchmarking and comparison.

Abstract: Adapting a single agent to a new multi-agent system brings challenges,
necessitating adjustments across various tasks, environments, and interactions
with unknown teammates and opponents. Addressing this challenge is highly
complex, and researchers have proposed two simplified scenarios, Multi-agent
reinforcement learning for zero-shot learning and Ad-Hoc Teamwork. Building on
these foundations, we propose a more comprehensive setting, Agent
Collaborative-Competitive Adaptation (ACCA), which evaluates an agent to
generalize across diverse scenarios, tasks, and interactions with both
unfamiliar opponents and teammates. In ACCA, agents adjust to task and
environmental changes, collaborate with unseen teammates, and compete against
unknown opponents. We introduce a new modeling approach, Multi-Retrieval and
Dynamic Generation (MRDG), that effectively models both teammates and opponents
using their behavioral trajectories. This method incorporates a positional
encoder for varying team sizes and a hypernetwork module to boost agents'
learning and adaptive capabilities. Additionally, a viewpoint alignment module
harmonizes the observational perspectives of retrieved teammates and opponents
with the learning agent. Extensive tests in benchmark scenarios like SMAC,
Overcooked-AI, and Melting Pot show that MRDG significantly improves robust
collaboration and competition with unseen teammates and opponents, surpassing
established baselines. Our code is available at:
https://github.com/vcis-wangchenxu/MRDG.git

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [555] [Graphics4Science: Computer Graphics for Scientific Impacts](https://arxiv.org/abs/2506.15786)
*Peter Yichen Chen,Minghao Guo,Hanspeter Pfister,Ming Lin,William Freeman,Qixing Huang,Han-Wei Shen,Wojciech Matusik*

Main category: cs.GR

TL;DR: This paper highlights the intersection of computer graphics and science, emphasizing their collaborative evolution and potential in solving scientific challenges.


<details>
  <summary>Details</summary>
Motivation: The paper aims to bridge the gap between computer graphics and scientific communities, presenting graphics as a modeling language in scientific discovery.

Method: The authors use examples of geometric reasoning and physical modeling to illustrate how computer graphics methods provide inductive biases useful for scientific challenges.

Result: The work identifies ways computer graphics methods can address data-scarce scientific problems and promotes collaboration between graphics and science communities.

Conclusion: The paper encourages the graphics community to tackle scientific challenges and advance impactful contributions to modern scientific discovery.

Abstract: Computer graphics, often associated with films, games, and visual effects,
has long been a powerful tool for addressing scientific challenges--from its
origins in 3D visualization for medical imaging to its role in modern
computational modeling and simulation. This course explores the deep and
evolving relationship between computer graphics and science, highlighting past
achievements, ongoing contributions, and open questions that remain. We show
how core methods, such as geometric reasoning and physical modeling, provide
inductive biases that help address challenges in both fields, especially in
data-scarce settings. To that end, we aim to reframe graphics as a modeling
language for science by bridging vocabulary gaps between the two communities.
Designed for both newcomers and experts, Graphics4Science invites the graphics
community to engage with science, tackle high-impact problems where graphics
expertise can make a difference, and contribute to the future of scientific
discovery. Additional details are available on the course website:
https://graphics4science.github.io

</details>


### [556] [VEIGAR: View-consistent Explicit Inpainting and Geometry Alignment for 3D object Removal](https://arxiv.org/abs/2506.15821)
*Pham Khai Nguyen Do,Bao Nguyen Tran,Nam Nguyen,Duc Dung Nguyen*

Main category: cs.GR

TL;DR: VEIGAR improves Novel View Synthesis and 3D generation, achieving better quality and efficiency without requiring initial 3D reconstruction.


<details>
  <summary>Details</summary>
Motivation: Address inefficiencies and quality limitations in current methods for cross-view consistency in NVS and 3D generation.

Method: Developed a lightweight model aligning priors in pixel space, introduced scale-invariant depth loss for efficient monocular depth regularization.

Result: VEIGAR sets new benchmarks in reconstruction quality and cross-view consistency, with threefold reduced training times compared to competitors.

Conclusion: VEIGAR offers a computationally efficient and effective solution to NVS and 3D generation, outperforming traditional methods.

Abstract: Recent advances in Novel View Synthesis (NVS) and 3D generation have
significantly improved editing tasks, with a primary emphasis on maintaining
cross-view consistency throughout the generative process. Contemporary methods
typically address this challenge using a dual-strategy framework: performing
consistent 2D inpainting across all views guided by embedded priors either
explicitly in pixel space or implicitly in latent space; and conducting 3D
reconstruction with additional consistency guidance. Previous strategies, in
particular, often require an initial 3D reconstruction phase to establish
geometric structure, introducing considerable computational overhead. Even with
the added cost, the resulting reconstruction quality often remains suboptimal.
In this paper, we present VEIGAR, a computationally efficient framework that
outperforms existing methods without relying on an initial reconstruction
phase. VEIGAR leverages a lightweight foundation model to reliably align priors
explicitly in the pixel space. In addition, we introduce a novel supervision
strategy based on scale-invariant depth loss, which removes the need for
traditional scale-and-shift operations in monocular depth regularization.
Through extensive experimentation, VEIGAR establishes a new state-of-the-art
benchmark in reconstruction quality and cross-view consistency, while achieving
a threefold reduction in training time compared to the fastest existing method,
highlighting its superior balance of efficiency and effectiveness.

</details>


### [557] [GratNet: A Photorealistic Neural Shader for Diffractive Surfaces](https://arxiv.org/abs/2506.15815)
*Narayan Kandel,Daljit Singh J. S. Dhillon*

Main category: cs.GR

TL;DR: This paper introduces a data-driven, multi-layer perceptron (MLP) based approach for accurately and efficiently rendering diffractive surfaces, significantly reducing data requirements and achieving photorealistic results.


<details>
  <summary>Details</summary>
Motivation: Existing methods for rendering structural coloration rely heavily on dense data and wave optics, which can be resource-intensive and inefficient. There is a lack of solutions addressing these challenges with implicit neural representation.

Method: The authors use a multi-layer perceptron (MLP) approach focused on data compression. They propose a tailored training methodology that accounts for the domain and range of diffractive reflectance datasets, avoiding overfitting and ensuring robust resampling capabilities.

Result: The proposed method achieves high-quality reconstructions using evaluation metrics like PSNR, SSIM, and FLIP. It performs comparably to a state-of-the-art wave-optical method while significantly reducing data storage requirements by two orders of magnitude.

Conclusion: The method efficiently reproduces photorealistic renderings of diffractive surfaces with less data and computational resources, demonstrating its potential for real-world applications.

Abstract: Structural coloration is commonly modeled using wave optics for reliable and
photorealistic rendering of natural, quasi-periodic and complex nanostructures.
Such models often rely on dense, preliminary or preprocessed data to accurately
capture the nuanced variations in diffractive surface reflectances. This heavy
data dependency warrants implicit neural representation which has not been
addressed comprehensively in the current literature. In this paper, we present
a multi-layer perceptron (MLP) based method for data-driven rendering of
diffractive surfaces with high accuracy and efficiency. We primarily approach
this problem from a data compression perspective to devise a nuanced training
and modeling method which is attuned to the domain and range characteristics of
diffractive reflectance datasets. Importantly, our approach avoids over-fitting
and has robust resampling behavior. Using Peak-Signal-to-Noise (PSNR),
Structural Similarity Index Measure (SSIM) and a flipping difference evaluator
(FLIP) as evaluation metrics, we demonstrate the high-quality reconstruction of
the ground-truth. In comparison to a recent state-of-the-art offline,
wave-optical, forward modeling approach, our method reproduces subjectively
similar results with significant performance gains. We reduce the memory
footprint of the raw datasets by two orders of magnitude in general. Lastly, we
depict the working of our method with actual surface renderings.

</details>


### [558] [FlatCAD: Fast Curvature Regularization of Neural SDFs for CAD Models](https://arxiv.org/abs/2506.16627)
*Haotian Yin,Aleksander Plocharski,Michal Jan Wlodarczyk,Mikolaj Kida,Przemyslaw Musialski*

Main category: cs.GR

TL;DR: This paper introduces a curvature proxy method for neural signed-distance fields (SDFs) to simplify and improve geometric learning for CAD-style behavior, reducing computational cost and memory usage compared to Hessian-based methods.


<details>
  <summary>Details</summary>
Motivation: To improve the efficiency and scalability of geometric learning using neural signed-distance fields while maintaining the desired curvature properties without relying on computationally expensive full Hessian evaluation.

Method: The paper proposes two curvature proxy methods for mixed second-order derivatives of neural SDFs: (i) a finite-difference proxy relying on a few forward SDF evaluations and a first-order gradient; (ii) an autodiff proxy using Hessian-vector products to avoid explicit full Hessian computation.

Result: The proposed proxies outperform traditional Hessian-based approaches on ABC benchmarks by matching/exceeding reconstruction fidelity, while significantly reducing GPU memory usage and computation time (by 50%).

Conclusion: These curvature proxies offer a scalable and framework-agnostic approach for curvature-aware SDF learning, making them practical for engineering-grade shape reconstruction tasks while addressing memory and runtime constraints.

Abstract: Neural signed-distance fields (SDFs) have become a versatile backbone for
geometric learning, yet enforcing developable, CAD-style behavior still hinges
on Gaussian curvature penalties that require full Hessian evaluation and
second-order automatic differentiation, both of which are costly in memory and
runtime. We present a curvature proxy that regularizes only the mixed
second-order term (Weingarten term), allowing the two principal curvatures to
adapt freely to data while suppressing unwanted warp. Two complementary
instantiations realize this idea: (i) a finite-difference proxy that replaces
each Hessian entry with four forward SDF evaluations and a single first-order
gradient, and (ii) an autodiff proxy that computes the same mixed derivative
via one Hessian-vector product, sidestepping explicit full Hessian assembly and
remaining faster in practice. Both variants converge to the exact mixed second
derivative, thus preserving the intended geometric bias without incurring full
second-order graphs. On the ABC benchmarks, the proxies match or exceed the
reconstruction fidelity of Hessian-based baselines while reducing GPU memory
use and wall-clock time by a factor of two. Because the method is drop-in and
framework-agnostic, it opens a practical path toward scalable, curvature-aware
SDF learning for engineering-grade shape reconstruction.

</details>


### [559] [Beyond Blur: A Fluid Perspective on Generative Diffusion Models](https://arxiv.org/abs/2506.16827)
*Grzegorz Gruszczynski,Michal Jan Wlodarczyk,Jakub J Meixner,Przemyslaw Musialski*

Main category: cs.GR

TL;DR: The paper introduces a new approach to generate images using a PDE-driven corruption process, combining advection-diffusion physics with neural networks to improve image diversity and quality.


<details>
  <summary>Details</summary>
Motivation: Current PDE-based image synthesis methods lack physical modeling of directional motion and turbulence, limiting the diversity and visual quality of generated images.

Method: The paper formulates image corruption using a PDE combining advection, diffusion, and Gaussian noise, implemented via a Lattice Boltzmann solver to create realistic turbulence. A neural network learns to reverse this operator for image generation.

Result: The proposed framework improves image diversity and quality, effectively generalizing previous PDE-based techniques while preserving the color palette of images.

Conclusion: This work successfully integrates fluid dynamics and neural networks for a physically inspired image generation process, offering advancements in generative modeling and PDE theory applications.

Abstract: We propose a novel PDE-driven corruption process for generative image
synthesis based on advection-diffusion processes which generalizes existing
PDE-based approaches. Our forward pass formulates image corruption via a
physically motivated PDE that couples directional advection with isotropic
diffusion and Gaussian noise, controlled by dimensionless numbers (Peclet,
Fourier). We implement this PDE numerically through a GPU-accelerated custom
Lattice Boltzmann solver for fast evaluation. To induce realistic turbulence,
we generate stochastic velocity fields that introduce coherent motion and
capture multi-scale mixing. In the generative process, a neural network learns
to reverse the advection-diffusion operator thus constituting a novel
generative model. We discuss how previous methods emerge as specific cases of
our operator, demonstrating that our framework generalizes prior PDE-based
corruption techniques. We illustrate how advection improves the diversity and
quality of the generated images while keeping the overall color palette
unaffected. This work bridges fluid dynamics, dimensionless PDE theory, and
deep generative modeling, offering a fresh perspective on physically informed
image corruption processes for diffusion-based synthesis.

</details>


### [560] [DreamCube: 3D Panorama Generation via Multi-plane Synchronization](https://arxiv.org/abs/2506.17206)
*Yukun Huang,Yanning Zhou,Jianan Wang,Kaiyi Huang,Xihui Liu*

Main category: cs.GR

TL;DR: This paper proposes DreamCube, a method for generating high-quality 3D panoramas utilizing multi-plane RGB-D diffusion modeling to extend capabilities of 2D models.


<details>
  <summary>Details</summary>
Motivation: To overcome the scarcity of 3D panoramic data and address the limitations of applying 2D model priors to 3D panorama synthesis.

Method: The paper introduces multi-plane synchronization for operators from 2D foundation models to adapt them for omnidirectional content. Based on this, DreamCube, a multi-plane RGB-D diffusion model, is proposed to ensure diverse appearances, accurate geometry, and multi-view consistency.

Result: The approach effectively generates panoramic images, estimates panoramic depths, and synthesizes 3D scenes with high quality and consistency.

Conclusion: DreamCube successfully demonstrates that 2D foundation model priors can be leveraged for 3D panorama tasks, achieving high-quality and diverse outputs while ensuring geometric and visual consistency.

Abstract: 3D panorama synthesis is a promising yet challenging task that demands
high-quality and diverse visual appearance and geometry of the generated
omnidirectional content. Existing methods leverage rich image priors from
pre-trained 2D foundation models to circumvent the scarcity of 3D panoramic
data, but the incompatibility between 3D panoramas and 2D single views limits
their effectiveness. In this work, we demonstrate that by applying multi-plane
synchronization to the operators from 2D foundation models, their capabilities
can be seamlessly extended to the omnidirectional domain. Based on this design,
we further introduce DreamCube, a multi-plane RGB-D diffusion model for 3D
panorama generation, which maximizes the reuse of 2D foundation model priors to
achieve diverse appearances and accurate geometry while maintaining multi-view
consistency. Extensive experiments demonstrate the effectiveness of our
approach in panoramic image generation, panoramic depth estimation, and 3D
scene generation.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [561] [ROS 2 Agnocast: Supporting Unsized Message Types for True Zero-Copy Publish/Subscribe IPC](https://arxiv.org/abs/2506.16882)
*Takahiro Ishikawa-Aso,Shinpei Kato*

Main category: cs.OS

TL;DR: Agnocast, a zero-copy IPC framework for ROS 2 C++ on Linux, addresses requirements unmet by existing solutions, improving efficiency in systems like autonomous driving platforms.


<details>
  <summary>Details</summary>
Motivation: To enable efficient high-performance communication in ROS 2 systems, addressing unmet requirements in existing zero-copy communication solutions.

Method: Development of Agnocast, a true zero-copy IPC framework, ensuring support for unsized message types, minimal code modifications, and selective implementation for specific nodes.

Result: Evaluation shows Agnocast achieves constant IPC overhead for all message sizes and improves response times by 16% and worst-case response time by 25% in real-world scenarios.

Conclusion: Agnocast successfully fulfills critical requirements for integration into ROS 2, enhancing real-time performance and efficiency without requiring significant application code modifications.

Abstract: Robot applications, comprising independent components that mutually
publish/subscribe messages, are built on inter-process communication (IPC)
middleware such as Robot Operating System 2 (ROS 2). In large-scale ROS 2
systems like autonomous driving platforms, true zero-copy communication --
eliminating serialization and deserialization -- is crucial for efficiency and
real-time performance. However, existing true zero-copy middleware solutions
lack widespread adoption as they fail to meet three essential requirements: 1)
Support for all ROS 2 message types including unsized ones; 2) Minimal
modifications to existing application code; 3) Selective implementation of
zero-copy communication between specific nodes while maintaining conventional
communication mechanisms for other inter-node communications including
inter-host node communications. This first requirement is critical, as
production-grade ROS 2 projects like Autoware rely heavily on unsized message
types throughout their codebase to handle diverse use cases (e.g., various
sensors), and depend on the broader ROS 2 ecosystem, where unsized message
types are pervasive in libraries. The remaining requirements facilitate
seamless integration with existing projects. While IceOryx middleware, a
practical true zero-copy solution, meets all but the first requirement, other
studies achieving the first requirement fail to satisfy the remaining criteria.
This paper presents Agnocast, a true zero-copy IPC framework applicable to ROS
2 C++ on Linux that fulfills all these requirements. Our evaluation
demonstrates that Agnocast maintains constant IPC overhead regardless of
message size, even for unsized message types. In Autoware PointCloud
Preprocessing, Agnocast achieves a 16% improvement in average response time and
a 25% improvement in worst-case response time.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [562] [Generative Modeling of Full-Atom Protein Conformations using Latent Diffusion on Graph Embeddings](https://arxiv.org/abs/2506.17064)
*Aditya Sengar,Ali Hariri,Daniel Probst,Patrick Barth,Pierre Vandergheynst*

Main category: q-bio.BM

TL;DR: This paper introduces LD-FPG, a method that generates all-atom protein conformations using latent diffusion, validated on the dopamine D2 receptor dataset with high fidelity to molecular dynamics simulations.


<details>
  <summary>Details</summary>
Motivation: Current generative models for proteins often oversimplify atomic detail or ignore conformational diversity, limiting their utility for dynamic proteins like GPCRs.

Method: LD-FPG uses Chebyshev graph neural networks to create latent embeddings of protein structures, processes them with pooling strategies, and applies a diffusion model to generate new samples. These are decoded back to full atomic coordinates, potentially regularized by dihedral-angle losses.

Result: When applied to a 2-microsecond MD trajectory of the dopamine D2 receptor, the model achieved high structural fidelity (all-atom lDDT ~0.7; C-alpha lDDT ~0.8) and accurately captured dihedral-angle distributions with a Jensen-Shannon divergence under 0.03.

Conclusion: LD-FPG enables accurate and diverse generation of all-atom protein structures. It is a promising approach for designing therapeutics targeting complex and dynamic proteins.

Abstract: Generating diverse, all-atom conformational ensembles of dynamic proteins
such as G-protein-coupled receptors (GPCRs) is critical for understanding their
function, yet most generative models simplify atomic detail or ignore
conformational diversity altogether. We present latent diffusion for full
protein generation (LD-FPG), a framework that constructs complete all-atom
protein structures, including every side-chain heavy atom, directly from
molecular dynamics (MD) trajectories. LD-FPG employs a Chebyshev graph neural
network (ChebNet) to obtain low-dimensional latent embeddings of protein
conformations, which are processed using three pooling strategies: blind,
sequential and residue-based. A diffusion model trained on these latent
representations generates new samples that a decoder, optionally regularized by
dihedral-angle losses, maps back to Cartesian coordinates. Using D2R-MD, a
2-microsecond MD trajectory (12 000 frames) of the human dopamine D2 receptor
in a membrane environment, the sequential and residue-based pooling strategy
reproduces the reference ensemble with high structural fidelity (all-atom lDDT
of approximately 0.7; C-alpha-lDDT of approximately 0.8) and recovers backbone
and side-chain dihedral-angle distributions with a Jensen-Shannon divergence of
less than 0.03 compared to the MD data. LD-FPG thereby offers a practical route
to system-specific, all-atom ensemble generation for large proteins, providing
a promising tool for structure-based therapeutic design on complex, dynamic
targets. The D2R-MD dataset and our implementation are freely available to
facilitate further research.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [563] [Quantum Artificial Intelligence for Secure Autonomous Vehicle Navigation: An Architectural Proposal](https://arxiv.org/abs/2506.16000)
*Hemanth Kannamarlapudi,Sowmya Chintalapudi*

Main category: cs.ET

TL;DR: The paper introduces a novel framework combining Quantum Artificial Intelligence to enhance navigation decision-making and security in autonomous vehicles.


<details>
  <summary>Details</summary>
Motivation: To address fundamental challenges in autonomous vehicle navigation, particularly concerning decision-making processes and ensuring secure, efficient communication.

Method: The authors propose using Quantum Neural Networks for multimodal sensor fusion, Nav-Q for quantum reinforcement learning to optimize navigation policies, and post-quantum cryptographic protocols for secure communication.

Result: Quantum techniques facilitated unified sensor data representation, enhanced learning of navigation policies under dynamic conditions, and ensured secure communication in autonomous vehicles.

Conclusion: The approach leverages quantum innovations to improve autonomous vehicle navigation while safeguarding communications against classical and quantum threats.

Abstract: Navigation is a very crucial aspect of autonomous vehicle ecosystem which
heavily relies on collecting and processing large amounts of data in various
states and taking a confident and safe decision to define the next vehicle
maneuver. In this paper, we propose a novel architecture based on Quantum
Artificial Intelligence by enabling quantum and AI at various levels of
navigation decision making and communication process in Autonomous vehicles :
Quantum Neural Networks for multimodal sensor fusion, Nav-Q for Quantum
reinforcement learning for navigation policy optimization and finally
post-quantum cryptographic protocols for secure communication. Quantum neural
networks uses quantum amplitude encoding to fuse data from various sensors like
LiDAR, radar, camera, GPS and weather etc., This approach gives a unified
quantum state representation between heterogeneous sensor modalities. Nav-Q
module processes the fused quantum states through variational quantum circuits
to learn optimal navigation policies under swift dynamic and complex
conditions. Finally, post quantum cryptographic protocols are used to secure
communication channels for both within vehicle communication and V2X (Vehicle
to Everything) communications and thus secures the autonomous vehicle
communication from both classical and quantum security threats. Thus, the
proposed framework addresses fundamental challenges in autonomous vehicles
navigation by providing quantum performance and future proof security. Index
Terms Quantum Computing, Autonomous Vehicles, Sensor Fusion

</details>


### [564] [Artificial Intelligence for Atmospheric Sciences: A Research Roadmap](https://arxiv.org/abs/2506.16281)
*Martha Arbayani Zaidan,Naser Hossein Motlagh,Petteri Nurmi,Tareq Hussein,Markku Kulmala,Tuukka Petäjä,Sasu Tarkoma*

Main category: cs.ET

TL;DR: This paper explores the role of AI in advancing atmospheric sciences, identifies integration challenges, and proposes a research roadmap.


<details>
  <summary>Details</summary>
Motivation: To address how AI can transform atmospheric sciences by enabling advanced data analysis and predictive capabilities.

Method: Conducts a critical interdisciplinary review that bridges atmospheric sciences with computer science. It identifies key challenges such as big data issues and infrastructure inadequacies.

Result: Highlights the transformative potential of AI in atmospheric research and offers a detailed research roadmap for addressing current and emerging challenges.

Conclusion: AI has the potential to significantly advance atmospheric research. Addressing integration challenges will enhance predictive capabilities and data analysis in the field.

Abstract: Atmospheric sciences are crucial for understanding environmental phenomena
ranging from air quality to extreme weather events, and climate change. Recent
breakthroughs in sensing, communication, computing, and Artificial Intelligence
(AI) have significantly advanced atmospheric sciences, enabling the generation
of vast amounts of data through long-term Earth observations and providing
powerful tools for analyzing atmospheric phenomena and predicting natural
disasters. This paper contributes a critical interdisciplinary overview that
bridges the fields of atmospheric science and computer science, highlighting
the transformative potential of AI in atmospheric research. We identify key
challenges associated with integrating AI into atmospheric research, including
issues related to big data and infrastructure, and provide a detailed research
roadmap that addresses both current and emerging challenges.

</details>


<div id='astro-ph.EP'></div>

# astro-ph.EP [[Back]](#toc)

### [565] [Exoplanet Classification through Vision Transformers with Temporal Image Analysis](https://arxiv.org/abs/2506.16597)
*Anupma Choudhary,Sohith Bandari,B. S. Kushvah,C. Swastik*

Main category: astro-ph.EP

TL;DR: The paper proposes using Vision Transformer (ViT) models with transformed light curve data (GAFs and RPs) for exoplanet classification, achieving notable performance metrics.


<details>
  <summary>Details</summary>
Motivation: The study addresses challenges in exoplanet classification, focusing on improving efficiency using advanced machine learning techniques.

Method: Light curve data from NASA's Kepler mission is transformed into Gramian Angular Fields (GAFs) and Recurrence Plots (RPs) to serve as input to Vision Transformer models, evaluated via metrics during 5-fold cross-validation.

Result: The Recurrence Plots outperformed GAFs, with recall reaching 89.46% and precision at 85.09%, showcasing the ViT model's capability in exoplanet detection.

Conclusion: The study highlights promising results for ViT models but notes limitations due to dataset size reduction and proposes further advancements in model architectures for improved automation and generalization.

Abstract: The classification of exoplanets has been a longstanding challenge in
astronomy, requiring significant computational and observational resources.
Traditional methods demand substantial effort, time, and cost, highlighting the
need for advanced machine learning techniques to enhance classification
efficiency. In this study, we propose a methodology that transforms raw light
curve data from NASA's Kepler mission into Gramian Angular Fields (GAFs) and
Recurrence Plots (RPs) using the Gramian Angular Difference Field and
recurrence plot techniques. These transformed images serve as inputs to the
Vision Transformer (ViT) model, leveraging its ability to capture intricate
temporal dependencies. We assess the performance of the model through recall,
precision, and F1 score metrics, using a 5-fold cross-validation approach to
obtain a robust estimate of the model's performance and reduce evaluation bias.
Our comparative analysis reveals that RPs outperform GAFs, with the ViT model
achieving an 89.46$\%$ recall and an 85.09$\%$ precision rate, demonstrating
its significant capability in accurately identifying exoplanetary transits.
Despite using under-sampling techniques to address class imbalance, dataset
size reduction remains a limitation. This study underscores the importance of
further research into optimizing model architectures to enhance automation,
performance, and generalization of the model.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [566] [MoNetV2: Enhanced Motion Network for Freehand 3D Ultrasound Reconstruction](https://arxiv.org/abs/2506.15835)
*Mingyuan Luo,Xin Yang,Zhongnuo Yan,Yan Cao,Yuanji Zhang,Xindi Hu,Jin Wang,Haoxuan Ding,Wei Han,Litao Sun,Dong Ni*

Main category: eess.IV

TL;DR: The paper introduces MoNetV2, combining image and motion information, multi-level consistency constraints, and self-supervised learning to improve 3D ultrasound volume reconstruction accuracy and robustness. It performs better than existing methods.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of image-only freehand 3D ultrasound reconstruction, particularly issues with cumulative drift and accuracy under complex scanning motions.

Method: MoNetV2 employs a sensor-based fusion of image and motion data, multi-level consistency constraints (scan-level, path-level, and patch-level), and a self-supervised strategy to reduce errors and improve reconstruction.

Result: Extensive tests show that MoNetV2 outperforms existing methods on reconstruction quality and generalizability across diverse datasets.

Conclusion: MoNetV2 enhances the reliability and performance of 3D ultrasound reconstruction, showcasing its potential for more accurate and robust clinical applications.

Abstract: Three-dimensional (3D) ultrasound (US) aims to provide sonographers with the
spatial relationships of anatomical structures, playing a crucial role in
clinical diagnosis. Recently, deep-learning-based freehand 3D US has made
significant advancements. It reconstructs volumes by estimating transformations
between images without external tracking. However, image-only reconstruction
poses difficulties in reducing cumulative drift and further improving
reconstruction accuracy, particularly in scenarios involving complex motion
trajectories. In this context, we propose an enhanced motion network (MoNetV2)
to enhance the accuracy and generalizability of reconstruction under diverse
scanning velocities and tactics. First, we propose a sensor-based temporal and
multi-branch structure that fuses image and motion information from a velocity
perspective to improve image-only reconstruction accuracy. Second, we devise an
online multi-level consistency constraint that exploits the inherent
consistency of scans to handle various scanning velocities and tactics. This
constraint exploits both scan-level velocity consistency, path-level appearance
consistency, and patch-level motion consistency to supervise inter-frame
transformation estimation. Third, we distill an online multi-modal
self-supervised strategy that leverages the correlation between network
estimation and motion information to further reduce cumulative errors.
Extensive experiments clearly demonstrate that MoNetV2 surpasses existing
methods in both reconstruction quality and generalizability performance across
three large datasets.

</details>


### [567] [Cross-Modality Learning for Predicting IHC Biomarkers from H&E-Stained Whole-Slide Images](https://arxiv.org/abs/2506.15853)
*Amit Das,Naofumi Tomita,Kyle J. Syme,Weijie Ma,Paige O'Connor,Kristin N. Corbett,Bing Ren,Xiaoying Liu,Saeed Hassanpour*

Main category: eess.IV

TL;DR: HistoStainAlign, a novel deep learning framework, predicts IHC staining patterns from H&E whole-slide images using contrastive learning, bypassing expensive and time-consuming IHC processes.


<details>
  <summary>Details</summary>
Motivation: The motivation is to reduce the cost, time, and expertise required for Immunohistochemistry (IHC) staining by using computational methods to predict IHC staining patterns from the widely used and less resource-intensive H&E staining.

Method: The proposed method, HistoStainAlign, applies a deep learning framework that uses contrastive training to align embeddings from paired H&E and IHC whole-slide images, capturing cross-modality features without the need for patch-level annotations or tissue registration.

Result: The model was evaluated on gastrointestinal and lung tissues with IHC stains P53, PD-L1, and Ki-67, achieving weighted F1 scores of 0.735, 0.830, and 0.723, respectively, demonstrating robust performance compared to baseline models.

Conclusion: HistoStainAlign has the potential to serve as a pre-screening tool to prioritize cases for IHC staining, thereby improving workflow efficiency and maintaining diagnostic accuracy.

Abstract: Hematoxylin and Eosin (H&E) staining is a cornerstone of pathological
analysis, offering reliable visualization of cellular morphology and tissue
architecture for cancer diagnosis, subtyping, and grading. Immunohistochemistry
(IHC) staining provides molecular insights by detecting specific proteins
within tissues, enhancing diagnostic accuracy, and improving treatment
planning. However, IHC staining is costly, time-consuming, and
resource-intensive, requiring specialized expertise. To address these
limitations, this study proposes HistoStainAlign, a novel deep learning
framework that predicts IHC staining patterns directly from H&E whole-slide
images (WSIs) by learning joint representations of morphological and molecular
features. The framework integrates paired H&E and IHC embeddings through a
contrastive training strategy, capturing complementary features across staining
modalities without patch-level annotations or tissue registration. The model
was evaluated on gastrointestinal and lung tissue WSIs with three commonly used
IHC stains: P53, PD-L1, and Ki-67. HistoStainAlign achieved weighted F1 scores
of 0.735 [95% Confidence Interval (CI): 0.670-0.799], 0.830 [95% CI:
0.772-0.886], and 0.723 [95% CI: 0.607-0.836], respectively for these three IHC
stains. Embedding analyses demonstrated the robustness of the contrastive
alignment in capturing meaningful cross-stain relationships. Comparisons with a
baseline model further highlight the advantage of incorporating contrastive
learning for improved stain pattern prediction. This study demonstrates the
potential of computational approaches to serve as a pre-screening tool, helping
prioritize cases for IHC staining and improving workflow efficiency.

</details>


### [568] [InfiniPot-V: Memory-Constrained KV Cache Compression for Streaming Video Understanding](https://arxiv.org/abs/2506.15745)
*Minsoo Kim,Kyuhong Shim,Jungwook Choi,Simyung Chang*

Main category: eess.IV

TL;DR: The paper introduces InfiniPot-V, a training-free framework that efficiently compresses the KV cache of multimodal large language models during streaming video processing, allowing devices with limited memory to handle long video streams in real-time.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the memory bottleneck in multimodal large language models (MLLMs), which arises from the linear growth of key-value (KV) cache size during processing of streaming videos. This issue limits their deployment on memory-constrained devices like mobile phones and AR glasses.

Method: InfiniPot-V employs a lightweight compression method that works during video encoding. It monitors the KV cache and, once it reaches a user-defined memory threshold, applies two techniques: (i) Temporal-axis Redundancy (TaR) metric to remove redundant tokens and (ii) Value-Norm (VaN) ranking to retain semantically significant tokens.

Result: InfiniPot-V reduces peak GPU memory usage by up to 94%, sustains real-time generation, and achieves comparable or better accuracy to models using the full cache in benchmark tests across multiple MLLMs and video datasets.

Conclusion: InfiniPot-V enables memory-constrained devices to effectively perform real-time streaming video understanding, bypassing the KV cache bottleneck without the need for model retraining or prior knowledge of queries. This innovation facilitates the practical application of MLLMs on devices like edge robots and AR glasses.

Abstract: Modern multimodal large language models (MLLMs) can reason over hour-long
video, yet their key-value (KV) cache grows linearly with time--quickly
exceeding the fixed memory of phones, AR glasses, and edge robots. Prior
compression schemes either assume the whole video and user query are available
offline or must first build the full cache, so memory still scales with stream
length. InfiniPot-V is the first training-free, query-agnostic framework that
enforces a hard, length-independent memory cap for streaming video
understanding. During video encoding it monitors the cache and, once a user-set
threshold is reached, runs a lightweight compression pass that (i) removes
temporally redundant tokens via Temporal-axis Redundancy (TaR) metric and (ii)
keeps semantically significant tokens via Value-Norm (VaN) ranking. Across four
open-source MLLMs and four long-video and two streaming-video benchmarks,
InfiniPot-V cuts peak GPU memory by up to 94%, sustains real-time generation,
and matches or surpasses full-cache accuracy--even in multi-turn dialogues. By
dissolving the KV cache bottleneck without retraining or query knowledge,
InfiniPot-V closes the gap for on-device streaming video assistants.

</details>


### [569] [CF-Seg: Counterfactuals meet Segmentation](https://arxiv.org/abs/2506.16213)
*Raghav Mehta,Fabio De Sousa Ribeiro,Tian Xia,Melanie Roschewitz,Ainkaran Santhirasekaram,Dominic C. Marshall,Ben Glocker*

Main category: eess.IV

TL;DR: The paper introduces the use of counterfactual images to improve anatomical segmentation in disease-affected medical images.


<details>
  <summary>Details</summary>
Motivation: Accurate segmentation in medical imaging is hindered by the presence of diseases, which can distort healthy tissues and obscure anatomical structures, potentially leading to diagnostic errors.

Method: Counterfactual images are generated to simulate the anatomy as it would appear without disease, allowing improved segmentation without modifying the underlying segmentation model.

Result: Experiments conducted on two chest X-ray datasets demonstrated improved anatomical segmentation using counterfactual images.

Conclusion: Counterfactual images enhance the accuracy of anatomical segmentation in medical imaging, aiding clinical decision-making.

Abstract: Segmenting anatomical structures in medical images plays an important role in
the quantitative assessment of various diseases. However, accurate segmentation
becomes significantly more challenging in the presence of disease. Disease
patterns can alter the appearance of surrounding healthy tissues, introduce
ambiguous boundaries, or even obscure critical anatomical structures. As such,
segmentation models trained on real-world datasets may struggle to provide good
anatomical segmentation, leading to potential misdiagnosis. In this paper, we
generate counterfactual (CF) images to simulate how the same anatomy would
appear in the absence of disease without altering the underlying structure. We
then use these CF images to segment structures of interest, without requiring
any changes to the underlying segmentation model. Our experiments on two
real-world clinical chest X-ray datasets show that the use of counterfactual
images improves anatomical segmentation, thereby aiding downstream clinical
decision-making.

</details>


### [570] [Pixel-wise Modulated Dice Loss for Medical Image Segmentation](https://arxiv.org/abs/2506.15744)
*Seyed Mohsen Hosseini*

Main category: eess.IV

TL;DR: The paper addresses class and difficulty imbalances in medical segmentation by introducing a Pixel-wise Modulated Dice loss (PM Dice loss) that improves segmentation performance with minimal computational cost.


<details>
  <summary>Details</summary>
Motivation: Class imbalance and difficulty imbalance reduce neural network performance in medical segmentation. Existing methods for addressing difficulty imbalance are computationally expensive and show limited success.

Method: The study proposes a simple modification to Dice loss by introducing a pixel-level modulating term, aiming to handle both class and difficulty imbalances effectively.

Result: The proposed PM Dice loss outperformed existing methods on three commonly used medical segmentation tasks by addressing difficulty imbalance more effectively.

Conclusion: The PM Dice loss is an efficient, low-cost modification that improves performance in handling both class and difficulty imbalances in medical segmentation tasks.

Abstract: Class imbalance and the difficulty imbalance are the two types of data
imbalance that affect the performance of neural networks in medical
segmentation tasks. In class imbalance the loss is dominated by the majority
classes and in difficulty imbalance the loss is dominated by easy to classify
pixels. This leads to an ineffective training. Dice loss, which is based on a
geometrical metric, is very effective in addressing the class imbalance
compared to the cross entropy (CE) loss, which is adopted directly from
classification tasks. To address the difficulty imbalance, the common approach
is employing a re-weighted CE loss or a modified Dice loss to focus the
training on difficult to classify areas. The existing modification methods are
computationally costly and with limited success. In this study we propose a
simple modification to the Dice loss with minimal computational cost. With a
pixel level modulating term, we take advantage of the effectiveness of Dice
loss in handling the class imbalance to also handle the difficulty imbalance.
Results on three commonly used medical segmentation tasks show that the
proposed Pixel-wise Modulated Dice loss (PM Dice loss) outperforms other
methods, which are designed to tackle the difficulty imbalance problem.

</details>


### [571] [Diffusion-based Counterfactual Augmentation: Towards Robust and Interpretable Knee Osteoarthritis Grading](https://arxiv.org/abs/2506.15748)
*Zhe Wang,Yuhua Ru,Aladine Chetouani,Tina Shiang,Fang Chen,Fabian Bauer,Liping Zhang,Didier Hans,Rachid Jennane,William Ewing Palmer,Mohamed Jarraya,Yung Hsin Chen*

Main category: eess.IV

TL;DR: The paper presents Diffusion-based Counterfactual Augmentation (DCA) to improve deep learning robustness in automated KOA grading, introducing counterfactual examples for better classification accuracy and interpretability.


<details>
  <summary>Details</summary>
Motivation: The main motivation is to overcome challenges in automated KOA grading, including inter-observer variability and lack of robustness near decision boundaries in deep learning models.

Method: The paper proposes navigating the latent space of a diffusion model using an SDE to create counterfactual examples, which are utilized in a self-corrective learning strategy to address classifier uncertainties.

Result: Experiments with OAI and MOST datasets show improved classification accuracy across models and enhanced interpretability, with latent space topology aligning with clinical knowledge.

Conclusion: DCA transforms model uncertainty into a robust training signal, paving the way for trustworthy automated diagnostics for KOA.

Abstract: Automated grading of Knee Osteoarthritis (KOA) from radiographs is challenged
by significant inter-observer variability and the limited robustness of deep
learning models, particularly near critical decision boundaries. To address
these limitations, this paper proposes a novel framework, Diffusion-based
Counterfactual Augmentation (DCA), which enhances model robustness and
interpretability by generating targeted counterfactual examples. The method
navigates the latent space of a diffusion model using a Stochastic Differential
Equation (SDE), governed by balancing a classifier-informed boundary drive with
a manifold constraint. The resulting counterfactuals are then used within a
self-corrective learning strategy to improve the classifier by focusing on its
specific areas of uncertainty. Extensive experiments on the public
Osteoarthritis Initiative (OAI) and Multicenter Osteoarthritis Study (MOST)
datasets demonstrate that this approach significantly improves classification
accuracy across multiple model architectures. Furthermore, the method provides
interpretability by visualizing minimal pathological changes and revealing that
the learned latent space topology aligns with clinical knowledge of KOA
progression. The DCA framework effectively converts model uncertainty into a
robust training signal, offering a promising pathway to developing more
accurate and trustworthy automated diagnostic systems. Our code is available at
https://github.com/ZWang78/DCA.

</details>


### [572] [Fast Training-free Perceptual Image Compression](https://arxiv.org/abs/2506.16102)
*Ziran Zhu,Tongda Xu,Minye Huang,Dailan He,Xingtong Ge,Xinjie Zhang,Ling Li,Yan Wang*

Main category: eess.IV

TL;DR: The paper introduces a training-free algorithm to improve perceptual image codec performance, ensuring faster decoding with theoretical guarantees and perceptual trade-offs.


<details>
  <summary>Details</summary>
Motivation: Existing training-free perceptual image codecs suffer from slow decoding times (up to 1 minute per image), hampering usability despite their reliance on pre-trained generative models.

Method: The authors propose a training-free algorithm that enhances perceptual quality with theoretical guarantees and presents implementations for various decoding time budgets: ~0.1s, 0.1-10s, and >=10s. The approach optimizes existing codecs without re-training.

Result: The proposed algorithm successfully improved the perceptual quality of various codecs (e.g., ELIC, VTM, MS-ILLM) with significantly faster decoding times and maintained comparable FID scores to prior training-free methods.

Conclusion: The novel algorithm achieves a perception-distortion trade-off, faster decoding, and superior performance compared to previous generative model-based codecs, advancing the field of perceptual image coding without re-training costs.

Abstract: Training-free perceptual image codec adopt pre-trained unconditional
generative model during decoding to avoid training new conditional generative
model. However, they heavily rely on diffusion inversion or sample
communication, which take 1 min to intractable amount of time to decode a
single image. In this paper, we propose a training-free algorithm that improves
the perceptual quality of any existing codec with theoretical guarantee. We
further propose different implementations for optimal perceptual quality when
decoding time budget is $\approx 0.1$s, $0.1-10$s and $\ge 10$s. Our approach:
1). improves the decoding time of training-free codec from 1 min to $0.1-10$s
with comparable perceptual quality. 2). can be applied to non-differentiable
codec such as VTM. 3). can be used to improve previous perceptual codecs, such
as MS-ILLM. 4). can easily achieve perception-distortion trade-off.
Empirically, we show that our approach successfully improves the perceptual
quality of ELIC, VTM and MS-ILLM with fast decoding. Our approach achieves
comparable FID to previous training-free codec with significantly less decoding
time. And our approach still outperforms previous conditional generative model
based codecs such as HiFiC and MS-ILLM in terms of FID. The source code is
provided in the supplementary material.

</details>


### [573] [Enhanced Dermatology Image Quality Assessment via Cross-Domain Training](https://arxiv.org/abs/2506.16116)
*Ignacio Hernández Montilla,Alfonso Medela,Paola Pasquali,Andy Aguilar,Taig Mac Carthy,Gerardo Fernández,Antonio Martorell,Enrique Onieva*

Main category: eess.IV

TL;DR: This paper addresses poor image quality in teledermatology by proposing cross-domain training of Image Quality Assessment (IQA) models using both dermatology and non-dermatology datasets.


<details>
  <summary>Details</summary>
Motivation: Teledermatology faces challenges due to poor image quality, which undermines the effectiveness of remote consultations.

Method: The authors created a novel dermatology IQA database, Legit.Health-DIQA-Artificial, and applied cross-domain training by combining dermatology and non-dermatology datasets.

Result: Cross-domain training improves image quality management in teledermatology by overcoming data limitations and utilizing diverse image distortions.

Conclusion: Cross-domain training enhances IQA model performance, enabling better remote care in dermatology.

Abstract: Teledermatology has become a widely accepted communication method in daily
clinical practice, enabling remote care while showing strong agreement with
in-person visits. Poor image quality remains an unsolved problem in
teledermatology and is a major concern to practitioners, as bad-quality images
reduce the usefulness of the remote consultation process. However, research on
Image Quality Assessment (IQA) in dermatology is sparse, and does not leverage
the latest advances in non-dermatology IQA, such as using larger image
databases with ratings from large groups of human observers. In this work, we
propose cross-domain training of IQA models, combining dermatology and
non-dermatology IQA datasets. For this purpose, we created a novel dermatology
IQA database, Legit.Health-DIQA-Artificial, using dermatology images from
several sources and having them annotated by a group of human observers. We
demonstrate that cross-domain training yields optimal performance across
domains and overcomes one of the biggest limitations in dermatology IQA, which
is the small scale of data, and leads to models trained on a larger pool of
image distortions, resulting in a better management of image quality in the
teledermatology process.

</details>


### [574] [From Coarse to Continuous: Progressive Refinement Implicit Neural Representation for Motion-Robust Anisotropic MRI Reconstruction](https://arxiv.org/abs/2506.16210)
*Zhenxuan Zhang,Lipei Zhang,Yanqi Cheng,Zi Wang,Fanwen Wang,Haosen Zhang,Yue Yang,Yinzhe Wu,Jiahao Huang,Angelica I Aviles-Rivero,Zhifan Gao,Guang Yang,Peter J. Lally*

Main category: eess.IV

TL;DR: This paper proposes the PR-INR framework for reconstructing anatomically accurate 3D brain volumes from motion-affected and undersampled 2D MRI slices.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the challenges of reconstructing accurate 3D brain volumes from 2D MRI slices affected by motion artifacts, undersampling, and volumetric anisotropy.

Method: The PR-INR framework includes three modules: a motion-aware diffusion module for coarse reconstruction, an implicit detail restoration module for spatial refinement, and a voxel continuous-aware representation module for inter-slice detail recovery.

Result: Experimental evaluations on five public MRI datasets show PR-INR outperforms current methods in reconstruction metrics and visual quality under various conditions including motion displacement and undersampling rates.

Conclusion: PR-INR provides robust and generalizable solutions for improving MRI reconstruction quality, demonstrating effectiveness across different motion and data acquisition scenarios.

Abstract: In motion-robust magnetic resonance imaging (MRI), slice-to-volume
reconstruction is critical for recovering anatomically consistent 3D brain
volumes from 2D slices, especially under accelerated acquisitions or patient
motion. However, this task remains challenging due to hierarchical structural
disruptions. It includes local detail loss from k-space undersampling, global
structural aliasing caused by motion, and volumetric anisotropy. Therefore, we
propose a progressive refinement implicit neural representation (PR-INR)
framework. Our PR-INR unifies motion correction, structural refinement, and
volumetric synthesis within a geometry-aware coordinate space. Specifically, a
motion-aware diffusion module is first employed to generate coarse volumetric
reconstructions that suppress motion artifacts and preserve global anatomical
structures. Then, we introduce an implicit detail restoration module that
performs residual refinement by aligning spatial coordinates with visual
features. It corrects local structures and enhances boundary precision.
Further, a voxel continuous-aware representation module represents the image as
a continuous function over 3D coordinates. It enables accurate inter-slice
completion and high-frequency detail recovery. We evaluate PR-INR on five
public MRI datasets under various motion conditions (3% and 5% displacement),
undersampling rates (4x and 8x) and slice resolutions (scale = 5). Experimental
results demonstrate that PR-INR outperforms state-of-the-art methods in both
quantitative reconstruction metrics and visual quality. It further shows
generalization and robustness across diverse unseen domains.

</details>


### [575] [AGE-US: automated gestational age estimation based on fetal ultrasound images](https://arxiv.org/abs/2506.16256)
*César Díaz-Parga,Marta Nuñez-Garcia,Maria J. Carreira,Gabriel Bernardino,Nicolás Vila-Blanco*

Main category: eess.IV

TL;DR: This paper develops an interpretable deep learning method for calculating gestational age using distance maps, achieving results comparable to state-of-the-art while being resource-efficient.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the necessity to accurately estimate gestational age, which is critical for fetal growth monitoring, especially given the limitations of traditional methods like last menstrual period estimates and the variability in ultrasound-based manual measurements.

Method: The method utilizes a novel segmentation architecture supported by distance maps for automated estimation, addressing dataset limitations and scarcity of segmentation masks.

Result: The proposed method achieves accuracy comparable to state-of-the-art models while being less complex and resource-intensive.

Conclusion: The approach is particularly suitable for resource-constrained environments and shows promise in estimating femur endpoints reliably using distance maps.

Abstract: Being born small carries significant health risks, including increased
neonatal mortality and a higher likelihood of future cardiac diseases. Accurate
estimation of gestational age is critical for monitoring fetal growth, but
traditional methods, such as estimation based on the last menstrual period, are
in some situations difficult to obtain. While ultrasound-based approaches offer
greater reliability, they rely on manual measurements that introduce
variability. This study presents an interpretable deep learning-based method
for automated gestational age calculation, leveraging a novel segmentation
architecture and distance maps to overcome dataset limitations and the scarcity
of segmentation masks. Our approach achieves performance comparable to
state-of-the-art models while reducing complexity, making it particularly
suitable for resource-constrained settings and with limited annotated data.
Furthermore, our results demonstrate that the use of distance maps is
particularly suitable for estimating femur endpoints.

</details>


### [576] [VesselSDF: Distance Field Priors for Vascular Network Reconstruction](https://arxiv.org/abs/2506.16556)
*Salvatore Esposito,Daniel Rebain,Arno Onken,Changjian Li,Oisin Mac Aodha*

Main category: eess.IV

TL;DR: The paper introduces VesselSDF, a framework using signed distance fields for precise vascular network segmentation from sparse CT scans, improving upon existing methods by maintaining vessel geometry and connectivity.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenges in accurately segmenting vascular networks from sparse CT scans due to thin, branching vessels and sparsity between imaging planes, where existing methods fail in structural continuity and geometric fidelity.

Method: The method involves the reformulation of vessel segmentation as a continuous signed distance field (SDF) regression problem, using adaptive Gaussian regularization to enhance smoothness and precision while avoiding floating segment artifacts.

Result: VesselSDF significantly outperforms existing segmentation methods, providing accurate reconstructions of vessels and maintaining vessel geometry and connectivity.

Conclusion: VesselSDF offers a robust solution for vascular segmentation in sparse CT scans, with improved geometric fidelity and continuity, enabling more reliable clinical vascular analysis.

Abstract: Accurate segmentation of vascular networks from sparse CT scan slices remains
a significant challenge in medical imaging, particularly due to the thin,
branching nature of vessels and the inherent sparsity between imaging planes.
Existing deep learning approaches, based on binary voxel classification, often
struggle with structural continuity and geometric fidelity. To address this
challenge, we present VesselSDF, a novel framework that leverages signed
distance fields (SDFs) for robust vessel reconstruction. Our method
reformulates vessel segmentation as a continuous SDF regression problem, where
each point in the volume is represented by its signed distance to the nearest
vessel surface. This continuous representation inherently captures the smooth,
tubular geometry of blood vessels and their branching patterns. We obtain
accurate vessel reconstructions while eliminating common SDF artifacts such as
floating segments, thanks to our adaptive Gaussian regularizer which ensures
smoothness in regions far from vessel surfaces while producing precise geometry
near the surface boundaries. Our experimental results demonstrate that
VesselSDF significantly outperforms existing methods and preserves vessel
geometry and connectivity, enabling more reliable vascular analysis in clinical
settings.

</details>


### [577] [DiffO: Single-step Diffusion for Image Compression at Ultra-Low Bitrates](https://arxiv.org/abs/2506.16572)
*Chanung Park,Joo Chan Lee,Jong Hwan Ko*

Main category: eess.IV

TL;DR: This paper proposes a single-step diffusion-based image compression model, "DiffO," that achieves high perceptual quality and fast decoding at extremely low bitrates.


<details>
  <summary>Details</summary>
Motivation: Current image compression methods suffer from significant quality loss at very low bitrates, and existing diffusion-based models, although better at generative performance, are slow and provide limited visual quality.

Method: The proposed DiffO employs VQ Residual training to better encode global geometry and details, and uses rate adaptive noise modulation to adjust denoising strength dynamically for bitrate optimization.

Result: DiffO outperforms state-of-the-art compression methods in quality and improves decoding speed by roughly 50x compared to earlier diffusion-based techniques.

Conclusion: DiffO makes generative codecs more practical with its combination of high compression quality and significant decoding efficiency improvements.

Abstract: Although image compression is fundamental to visual data processing and has
inspired numerous standard and learned codecs, these methods still suffer
severe quality degradation at extremely low bits per pixel. While recent
diffusion based models provided enhanced generative performance at low
bitrates, they still yields limited perceptual quality and prohibitive decoding
latency due to multiple denoising steps. In this paper, we propose the first
single step diffusion model for image compression (DiffO) that delivers high
perceptual quality and fast decoding at ultra low bitrates. DiffO achieves
these goals by coupling two key innovations: (i) VQ Residual training, which
factorizes a structural base code and a learned residual in latent space,
capturing both global geometry and high frequency details; and (ii) rate
adaptive noise modulation, which tunes denoising strength on the fly to match
the desired bitrate. Extensive experiments show that DiffO surpasses state of
the art compression performance while improving decoding speed by about 50x
compared to prior diffusion-based methods, greatly improving the practicality
of generative codecs. The code will be available at
https://github.com/Freemasti/DiffO.

</details>


### [578] [Hybrid Attention Network for Accurate Breast Tumor Segmentation in Ultrasound Images](https://arxiv.org/abs/2506.16592)
*Muhammad Azeem Aslam,Asim Naveed,Nisar Ahmed*

Main category: eess.IV

TL;DR: The paper introduces a hybrid attention-based network for automated tumor segmentation in breast ultrasound imaging.


<details>
  <summary>Details</summary>
Motivation: The study aims to improve the accuracy and robustness of tumor segmentation in breast ultrasound imaging, which is challenging due to noise, scale variations, and fuzzy lesion boundaries.

Method: The proposed architecture incorporates a DenseNet121-based encoder for feature extraction, a multi-branch attention-enhanced decoder, and novel modules like Global Spatial Attention, Position Encoding, and Scaled Dot-Product Attention in the bottleneck.

Result: The hybrid approach, optimized with a combination of Binary Cross-Entropy and Jaccard Index loss, demonstrated superior performance on public datasets compared to existing segmentation methods.

Conclusion: This work highlights the potential of the proposed model to assist radiologists in achieving early and accurate breast cancer diagnoses.

Abstract: Breast ultrasound imaging is a valuable tool for early breast cancer
detection, but automated tumor segmentation is challenging due to inherent
noise, variations in scale of lesions, and fuzzy boundaries. To address these
challenges, we propose a novel hybrid attention-based network for lesion
segmentation. Our proposed architecture integrates a pre-trained DenseNet121 in
the encoder part for robust feature extraction with a multi-branch
attention-enhanced decoder tailored for breast ultrasound images. The
bottleneck incorporates Global Spatial Attention (GSA), Position Encoding (PE),
and Scaled Dot-Product Attention (SDPA) to learn global context, spatial
relationships, and relative positional features. The Spatial Feature
Enhancement Block (SFEB) is embedded at skip connections to refine and enhance
spatial features, enabling the network to focus more effectively on tumor
regions. A hybrid loss function combining Binary Cross-Entropy (BCE) and
Jaccard Index loss optimizes both pixel-level accuracy and region-level overlap
metrics, enhancing robustness to class imbalance and irregular tumor shapes.
Experiments on public datasets demonstrate that our method outperforms existing
approaches, highlighting its potential to assist radiologists in early and
accurate breast cancer diagnosis.

</details>


### [579] [Overfitting in Histopathology Model Training: The Need for Customized Architectures](https://arxiv.org/abs/2506.16631)
*Saghir Alfasly,Ghazal Alabtah,H. R. Tizhoosh*

Main category: eess.IV

TL;DR: The paper highlights the risk of overfitting when using large-scale models designed for natural image analysis on histopathology image tasks, proposing simpler, domain-specific designs as a better alternative.


<details>
  <summary>Details</summary>
Motivation: To address the issue of overfitting and suboptimal performance of deep learning models on histopathology image analysis, particularly when repurposing large architectures from natural image tasks.

Method: Conducted extensive experiments comparing different model architectures, including ResNet variants and Vision Transformers (ViT), on histopathology datasets to evaluate performance and overfitting tendencies.

Result: Found that larger models did not necessarily improve performance and were more prone to overfitting. Simpler, domain-specific architectures achieved comparable or better results with reduced overfitting.

Conclusion: Customized architectures tailored for histopathology image analysis are essential, especially when handling limited data, as they can outperform generic large-scale models while reducing overfitting.

Abstract: This study investigates the critical problem of overfitting in deep learning
models applied to histopathology image analysis. We show that simply adopting
and fine-tuning large-scale models designed for natural image analysis often
leads to suboptimal performance and significant overfitting when applied to
histopathology tasks. Through extensive experiments with various model
architectures, including ResNet variants and Vision Transformers (ViT), we show
that increasing model capacity does not necessarily improve performance on
histopathology datasets. Our findings emphasize the need for customized
architectures specifically designed for histopathology image analysis,
particularly when working with limited datasets. Using Oesophageal
Adenocarcinomas public dataset, we demonstrate that simpler, domain-specific
architectures can achieve comparable or better performance while minimizing
overfitting.

</details>


### [580] [A Prior-Guided Joint Diffusion Model in Projection Domain for PET Tracer Conversion](https://arxiv.org/abs/2506.16733)
*Fang Chen,Weifeng Zhang,Xingyu Ai,BingXuan Li,An Li,Qiegen Liu*

Main category: eess.IV

TL;DR: This paper introduces a method to transform 18F-FDG PET images into more specialized 18F-DOPA PET images to overcome imaging limitations for neuroendocrine tumors and neurological disorders.


<details>
  <summary>Details</summary>
Motivation: To address the complex synthesis and clinical limitations of 18F-DOPA PET imaging, and leverage projection domain modeling to minimize errors during image reconstruction.

Method: A prior-guided joint diffusion model (PJDM) was developed involving coarse estimation and prior refinement processes. A hybrid sampler generates initial sinograms, which undergo iterative refinement guided by learned priors.

Result: The proposed PJDM model improved both the quality of sinograms and synthetic 18F-DOPA PET imaging, as validated by experimental results.

Conclusion: PJDM provides a promising approach for enhancing PET imaging specificity and quality, facilitating the broader use of 18F-DOPA without its synthesis and transport hurdles.

Abstract: Positron emission tomography (PET) is widely used to assess metabolic
activity, but its application is limited by the availability of radiotracers.
18F-labeled fluorodeoxyglucose (18F-FDG) is the most commonly used tracer but
shows limited effectiveness for certain tumors. In contrast,
6-18F-fluoro-3,4-dihydroxy-L-phenylalanine (18F-DOPA) offers higher specificity
for neuroendocrine tumors and neurological disorders. However, its complex
synthesis and limitations in transportation and clinical use hinder widespread
adoption. During PET imaging, the sinogram represents a form of raw data
acquired by the scanner. Therefore, modeling in projection domain enables more
direct utilization of the original information, potentially reducing the
accumulation of errors introduced during the image reconstruction process.
Inspired by these factors, this study proposes a prior-guided joint diffusion
model (PJDM) for transforming 18F-FDG PET images into 18F-DOPA PET images in
projection domain. Specifically, a coarse estimation model and a prior
refinement model are trained independently. During inference, an initial
synthetic 18F-DOPA PET sinogram is generated using a higher-order hybrid
sampler. This sinogram is then degraded and serves as an additional condition
to guide the iterative refinement process using learned prior. Experimental
results demonstrated that PJDM effectively improved both sinogram quality and
synthetic outcomes. The code is available at: https://github.com/yqx7150/PJDM.

</details>


### [581] [Temperature calibration of surface emissivities with an improved thermal image enhancement network](https://arxiv.org/abs/2506.16803)
*Ning Chu,Siya Zheng,Shanqing Zhang,Li Li,Caifang Cai,Ali Mohammad-Djafari,Feng Zhao,Yuanbo Song*

Main category: eess.IV

TL;DR: The paper proposes a neural framework to improve temperature accuracy in infrared thermography by unifying temperature correction and image enhancement.


<details>
  <summary>Details</summary>
Motivation: Temperature inaccuracy in infrared thermography caused by emissivity variations is a long-standing challenge, overlooked in existing methods combining radiometric calibration and image degradation.

Method: The study uses a symmetric skip-CNN with an emissivity-aware attention module and a dual-constrained loss function for fusing thermal and spatial features after a pre-processing stage.

Result: The proposed network effectively addresses emissivity artifacts and recovers structural details, achieving accurate calibration across different conditions in an industrial blower system.

Conclusion: The framework dynamically fuses thermal radiation and spatial context, providing promising results in industrial applications requiring precise temperature calibration.

Abstract: Infrared thermography faces persistent challenges in temperature accuracy due
to material emissivity variations, where existing methods often neglect the
joint optimization of radiometric calibration and image degradation. This study
introduces a physically guided neural framework that unifies temperature
correction and image enhancement through a symmetric skip-CNN architecture and
an emissivity-aware attention module. The pre-processing stage segments the
ROIs of the image and and initially corrected the firing rate. A novel
dual-constrained loss function strengthens the statistical consistency between
the target and reference regions through mean-variance alignment and histogram
matching based on Kullback-Leibler dispersion. The method works by dynamically
fusing thermal radiation features and spatial context, and the model suppresses
emissivity artifacts while recovering structural details. After validating the
industrial blower system under different conditions, the improved network
realizes the dynamic fusion of thermal radiation characteristics and spatial
background, with accurate calibration results in various industrial conditions.

</details>


### [582] [PET Tracer Separation Using Conditional Diffusion Transformer with Multi-latent Space Learning](https://arxiv.org/abs/2506.16934)
*Bin Huang,Feihong Xu,Xinchong Shi,Shan Huang,Binxuan Li,Fei Li,Qiegen Liu*

Main category: eess.IV

TL;DR: This study introduces a multi-latent space guided texture conditional diffusion transformer model (MS-CDT) for separating PET tracer signals, combining diffusion and transformer architectures with texture guidance for enhanced image details.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the challenge in multi-tracer PET imaging where gamma-photon pairs of different tracers are difficult to distinguish, despite their potential to give comprehensive physiological insights.

Method: The paper proposes the MS-CDT model that leverages texture masks as conditional inputs for enhanced image detail and multi-latent space priors for capturing multi-level image features, integrated into a unified framework with diffusion transformers.

Result: Experiments on 3D PET brain and chest scans show that MS-CDT outperforms advanced methods in image quality and clinically relevant information retention.

Conclusion: MS-CDT effectively separates tracer signals in PET imaging, combining computational efficiency and detail preservation, thus enhancing multi-tracer PET imaging capabilities.

Abstract: In clinical practice, single-radiotracer positron emission tomography (PET)
is commonly used for imaging. Although multi-tracer PET imaging can provide
supplementary information of radiotracers that are sensitive to physiological
function changes, enabling a more comprehensive characterization of
physiological and pathological states, the gamma-photon pairs generated by
positron annihilation reactions of different tracers in PET imaging have the
same energy, making it difficult to distinguish the tracer signals. In this
study, a multi-latent space guided texture conditional diffusion transformer
model (MS-CDT) is proposed for PET tracer separation. To the best of our
knowledge, this is the first attempt to use texture condition and multi-latent
space for tracer separation in PET imaging. The proposed model integrates
diffusion and transformer architectures into a unified optimization framework,
with the novel addition of texture masks as conditional inputs to enhance image
details. By leveraging multi-latent space prior derived from different tracers,
the model captures multi-level feature representations, aiming to balance
computational efficiency and detail preservation. The texture masks, serving as
conditional guidance, help the model focus on salient structural patterns,
thereby improving the extraction and utilization of fine-grained image
textures. When combined with the diffusion transformer backbone, this
conditioning mechanism contributes to more accurate and robust tracer
separation. To evaluate its effectiveness, the proposed MS-CDT is compared with
several advanced methods on two types of 3D PET datasets: brain and chest
scans. Experimental results indicate that MS-CDT achieved competitive
performance in terms of image quality and preservation of clinically relevant
information. Code is available at: https://github.com/yqx7150/MS-CDT.

</details>


### [583] [Robust Training with Data Augmentation for Medical Imaging Classification](https://arxiv.org/abs/2506.17133)
*Josué Martínez-Martínez,Olivia Brown,Mostafa Karami,Sheida Nabavi*

Main category: eess.IV

TL;DR: The paper introduces a robust training algorithm with data augmentation (RTDA) to enhance the resilience of medical image classifiers to adversarial attacks and distribution shifts.


<details>
  <summary>Details</summary>
Motivation: To address the inherent vulnerabilities of deep neural networks in medical imaging against adversarial attacks and distribution shifts, which impact diagnostic reliability and user trust.

Method: The researchers develop and benchmark RTDA against six baseline techniques, using datasets from mammograms, X-rays, and ultrasound imaging, testing for adversarial robustness and generalization under distribution shifts.

Result: The RTDA technique demonstrates superior robustness to adversarial attacks and better generalization under distribution shifts across all imaging tasks while maintaining high clean accuracy.

Conclusion: RTDA effectively enhances the robustness and reliability of medical image classifiers, thereby improving their applicability in clinical settings.

Abstract: Deep neural networks are increasingly being used to detect and diagnose
medical conditions using medical imaging. Despite their utility, these models
are highly vulnerable to adversarial attacks and distribution shifts, which can
affect diagnostic reliability and undermine trust among healthcare
professionals. In this study, we propose a robust training algorithm with data
augmentation (RTDA) to mitigate these vulnerabilities in medical image
classification. We benchmark classifier robustness against adversarial
perturbations and natural variations of RTDA and six competing baseline
techniques, including adversarial training and data augmentation approaches in
isolation and combination, using experimental data sets with three different
imaging technologies (mammograms, X-rays, and ultrasound). We demonstrate that
RTDA achieves superior robustness against adversarial attacks and improved
generalization performance in the presence of distribution shift in each image
classification task while maintaining high clean accuracy.

</details>


### [584] [MeDi: Metadata-Guided Diffusion Models for Mitigating Biases in Tumor Classification](https://arxiv.org/abs/2506.17140)
*David Jacob Drexlin,Jonas Dippel,Julius Hense,Niklas Prenißl,Grégoire Montavon,Frederick Klauschen,Klaus-Robert Müller*

Main category: eess.IV

TL;DR: The paper proposes a Metadata-guided generative Diffusion model (MeDi) to address biases in histological prediction tasks by generating synthetic data for underrepresented subpopulations.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of bias in deep learning models for histological tasks caused by varying conditions such as staining, scanner, hospital, and demographics, leading to shortcut learning and biased predictions.

Method: A Metadata-guided generative Diffusion model framework (MeDi) was developed to augment underrepresented data subpopulations with high-quality synthetic histopathology images.

Result: MeDi generated high-quality synthetic images for unseen subpopulations, improved fidelity, and enhanced the performance of downstream classifiers in subpopulation-shifted datasets.

Conclusion: The approach demonstrates potential in mitigating data biases through generative models, providing a proof-of-concept for addressing bias in clinical adaptation of predictive models.

Abstract: Deep learning models have made significant advances in histological
prediction tasks in recent years. However, for adaptation in clinical practice,
their lack of robustness to varying conditions such as staining, scanner,
hospital, and demographics is still a limiting factor: if trained on
overrepresented subpopulations, models regularly struggle with less frequent
patterns, leading to shortcut learning and biased predictions. Large-scale
foundation models have not fully eliminated this issue. Therefore, we propose a
novel approach explicitly modeling such metadata into a Metadata-guided
generative Diffusion model framework (MeDi). MeDi allows for a targeted
augmentation of underrepresented subpopulations with synthetic data, which
balances limited training data and mitigates biases in downstream models. We
experimentally show that MeDi generates high-quality histopathology images for
unseen subpopulations in TCGA, boosts the overall fidelity of the generated
images, and enables improvements in performance for downstream classifiers on
datasets with subpopulation shifts. Our work is a proof-of-concept towards
better mitigating data biases with generative models.

</details>


### [585] [Proportional Sensitivity in Generative Adversarial Network (GAN)-Augmented Brain Tumor Classification Using Convolutional Neural Network](https://arxiv.org/abs/2506.17165)
*Mahin Montasir Afif,Abdullah Al Noman,K. M. Tahsin Kabir,Md. Mortuza Ahmmed,Md. Mostafizur Rahman,Mufti Mahmud,Md. Ashraful Babu*

Main category: eess.IV

TL;DR: This paper explores the effect of GAN-generated MRI images on brain tumor classification using CNN, finding a balance between real and synthetic data is critical.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of limited medical imaging datasets by exploring the potential of GANs in augmenting data for brain tumor classification.

Method: A DCGAN was used to generate synthetic MRI images, which were mixed with real images in varying ratios to train a CNN. The model was evaluated on a separate test set.

Result: The CNN achieved high performance (95.2% accuracy and other metrics exceeding 95%) when trained with a mix of real and synthetic images, but performance declined with excessive synthetic data.

Conclusion: GANs can effectively augment limited datasets for brain tumor MRI classification, but excessive synthetic data can degrade generalization to real-world cases.

Abstract: Generative Adversarial Networks (GAN) have shown potential in expanding
limited medical imaging datasets. This study explores how different ratios of
GAN-generated and real brain tumor MRI images impact the performance of a CNN
in classifying healthy vs. tumorous scans. A DCGAN was used to create synthetic
images which were mixed with real ones at various ratios to train a custom CNN.
The CNN was then evaluated on a separate real-world test set. Our results
indicate that the model maintains high sensitivity and precision in tumor
classification, even when trained predominantly on synthetic data. When only a
small portion of GAN data was added, such as 900 real images and 100 GAN
images, the model achieved excellent performance, with test accuracy reaching
95.2%, and precision, recall, and F1-score all exceeding 95%. However, as the
proportion of GAN images increased further, performance gradually declined.
This study suggests that while GANs are useful for augmenting limited datasets
especially when real data is scarce, too much synthetic data can introduce
artifacts that affect the model's ability to generalize to real world cases.

</details>


### [586] [Implicit neural representations for accurate estimation of the standard model of white matter](https://arxiv.org/abs/2506.15762)
*Tom Hendriks,Gerrit Arends,Edwin Versteeg,Anna Vilanova,Maxime Chamberland,Chantal M. W. Tax*

Main category: eess.IV

TL;DR: This paper proposes using implicit neural representations (INRs) to improve diffusion MRI signal analysis, achieving superior accuracy even in noisy conditions.


<details>
  <summary>Details</summary>
Motivation: Improved accuracy in diffusion MRI data interpretation is needed due to noise and parameter degeneracies in high-dimensional models like the Standard Model of white matter.

Method: The proposed method employs implicit neural representations (INRs) with sinusoidal position encoding, incorporating spatial regularization and enabling unsupervised parameter estimation.

Result: The INR method demonstrates superior accuracy in parameter estimation under noisy conditions compared to other techniques, and enables improved spatial upsampling for anatomically plausible representations.

Conclusion: INRs represent a versatile and powerful tool for diffusion MRI data analysis, offering robustness to noise, adaptability to various models, and significant computational advantages.

Abstract: Diffusion magnetic resonance imaging (dMRI) enables non-invasive
investigation of tissue microstructure. The Standard Model (SM) of white matter
aims to disentangle dMRI signal contributions from intra- and extra-axonal
water compartments. However, due to the model its high-dimensional nature,
extensive acquisition protocols with multiple b-values and diffusion tensor
shapes are typically required to mitigate parameter degeneracies. Even then,
accurate estimation remains challenging due to noise. This work introduces a
novel estimation framework based on implicit neural representations (INRs),
which incorporate spatial regularization through the sinusoidal encoding of the
input coordinates. The INR method is evaluated on both synthetic and in vivo
datasets and compared to parameter estimates using cubic polynomials,
supervised neural networks, and nonlinear least squares. Results demonstrate
superior accuracy of the INR method in estimating SM parameters, particularly
in low signal-to-noise conditions. Additionally, spatial upsampling of the INR
can represent the underlying dataset anatomically plausibly in a continuous
way, which is unattainable with linear or cubic interpolation. The INR is fully
unsupervised, eliminating the need for labeled training data. It achieves fast
inference ($\sim$6 minutes), is robust to both Gaussian and Rician noise,
supports joint estimation of SM kernel parameters and the fiber orientation
distribution function with spherical harmonics orders up to at least 8 and
non-negativity constraints, and accommodates spatially varying acquisition
protocols caused by magnetic gradient non-uniformities. The combination of
these properties along with the possibility to easily adapt the framework to
other dMRI models, positions INRs as a potentially important tool for analyzing
and interpreting diffusion MRI data.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [587] [Implementing Keyword Spotting on the MCUX947 Microcontroller with Integrated NPU](https://arxiv.org/abs/2506.08911)
*Petar Jakuš,Hrvoje Džapo*

Main category: cs.HC

TL;DR: The paper discusses a highly efficient keyword spotting system using a microcontroller with Neural Processing Unit, achieving significant speedup and high accuracy with a small model.


<details>
  <summary>Details</summary>
Motivation: The need for enabling real-time voice interactions on resource-constrained embedded devices.

Method: Combines MFCC feature extraction with CNN classifier, optimized by Quantization Aware Training, and leverages an NPU for inference execution.

Result: Achieves a 59x inference speedup on NPU compared to CPU, 97.06% accuracy, with a model size of just 30.58 KB.

Conclusion: Efficient, low-power voice interfaces on embedded platforms are feasible, showcasing the potential for real-time voice processing in constrained environments.

Abstract: This paper presents a keyword spotting (KWS) system implemented on the NXP
MCXN947 microcontroller with an integrated Neural Processing Unit (NPU),
enabling real-time voice interaction on resource-constrained devices. The
system combines MFCC feature extraction with a CNN classifier, optimized using
Quantization Aware Training to reduce model size with minimal accuracy drop.
Experimental results demonstrate a 59x speedup in inference time when
leveraging the NPU compared to CPU-only execution, achieving 97.06% accuracy
with a model size of 30.58 KB, demonstrating the feasibility of efficient,
low-power voice interfaces on embedded platforms.

</details>


### [588] [Human-Centered Shared Autonomy for Motor Planning, Learning, and Control Applications](https://arxiv.org/abs/2506.16044)
*MH Farhadi,Ali Rabiee,Sima Ghafoori,Anna Cetera,Wei Xu,Reza Abiri*

Main category: cs.HC

TL;DR: The paper reviews AI-driven shared autonomy frameworks in healthcare, focusing on upper limb biosignal-based interfaces and human-machine systems for rehabilitation and assistive tasks.


<details>
  <summary>Details</summary>
Motivation: To improve collaboration between humans and AI in healthcare where human intent plays a critical role, and fully autonomous systems may not be suitable.

Method: Exploration and analysis of shared autonomy AI frameworks using upper limb biosignal-based human-machine interfaces and motion control systems, with focus on rehabilitation and assistive robotics.

Result: Identified methods for blending human and machine inputs in healthcare applications, discussed human factors, intent detection, and emerging directions like LLMs.

Conclusion: Adaptive shared autonomy AI is proposed as an effective paradigm for healthcare collaboration, with challenges and future directions outlined to integrate neuroscience with robotics.

Abstract: With recent advancements in AI and computational tools, intelligent paradigms
have emerged to enhance fields like shared autonomy and human-machine teaming
in healthcare. Advanced AI algorithms (e.g., reinforcement learning) can
autonomously make decisions to achieve planning and motion goals. However, in
healthcare, where human intent is crucial, fully independent machine decisions
may not be ideal. This chapter presents a comprehensive review of
human-centered shared autonomy AI frameworks, focusing on upper limb
biosignal-based machine interfaces and associated motor control systems,
including computer cursors, robotic arms, and planar platforms. We examine
motor planning, learning (rehabilitation), and control, covering conceptual
foundations of human-machine teaming in reach-and-grasp tasks and analyzing
both theoretical and practical implementations. Each section explores how human
and machine inputs can be blended for shared autonomy in healthcare
applications. Topics include human factors, biosignal processing for intent
detection, shared autonomy in brain-computer interfaces (BCI), rehabilitation,
assistive robotics, and Large Language Models (LLMs) as the next frontier. We
propose adaptive shared autonomy AI as a high-performance paradigm for
collaborative human-AI systems, identify key implementation challenges, and
outline future directions, particularly regarding AI reasoning agents. This
analysis aims to bridge neuroscientific insights with robotics to create more
intuitive, effective, and ethical human-machine teaming frameworks.

</details>


### [589] [PPTP: Performance-Guided Physiological Signal-Based Trust Prediction in Human-Robot Collaboration](https://arxiv.org/abs/2506.16677)
*Hao Guo,Wei Fan,Shaohui Liu,Feng Jiang,Chunzhi Yi*

Main category: cs.HC

TL;DR: This paper introduces PPTP, a framework for human-robot collaboration trust prediction using physiological signals and performance cues, achieving high accuracy in trust classification.


<details>
  <summary>Details</summary>
Motivation: To improve trust calibration in human-robot collaboration, especially in construction scenarios where safety and efficiency rely on accurate trust prediction.

Method: The study uses multimodal physiological signals (ECG, EMG, and GSR) fused with collaboration performance evaluations, cross-modality processing, and ablation experiments.

Result: Achieved 81% accuracy in three-level trust prediction and 74.3% in the novel seven-level classification, outperforming the previous baseline methods.

Conclusion: The PPTP framework effectively enhances trust prediction by integrating physiological signal processing with performance measurement, advancing trust research.

Abstract: Trust prediction is a key issue in human-robot collaboration, especially in
construction scenarios where maintaining appropriate trust calibration is
critical for safety and efficiency. This paper introduces the
Performance-guided Physiological signal-based Trust Prediction (PPTP), a novel
framework designed to improve trust assessment. We designed a human-robot
construction scenario with three difficulty levels to induce different trust
states. Our approach integrates synchronized multimodal physiological signals
(ECG, GSR, and EMG) with collaboration performance evaluation to predict human
trust levels. Individual physiological signals are processed using
collaboration performance information as guiding cues, leveraging the
standardized nature of collaboration performance to compensate for individual
variations in physiological responses. Extensive experiments demonstrate the
efficacy of our cross-modality fusion method in significantly improving trust
classification performance. Our model achieves over 81% accuracy in three-level
trust classification, outperforming the best baseline method by 6.7%, and
notably reaches 74.3% accuracy in high-resolution seven-level classification,
which is a first in trust prediction research. Ablation experiments further
validate the superiority of physiological signal processing guided by
collaboration performance assessment.

</details>


### [590] [Do We Talk to Robots Like Therapists, and Do They Respond Accordingly? Language Alignment in AI Emotional Support](https://arxiv.org/abs/2506.16473)
*Sophie Chiang,Guy Laban,Hatice Gunes*

Main category: cs.HC

TL;DR: The study evaluates similarity between conversations with social robots (using GPT-3.5) and traditional human therapy sessions, showing strong overlap in thematic alignment and semantic responses.


<details>
  <summary>Details</summary>
Motivation: To determine how closely interactions with conversational agents using large language models (like GPT-3.5) align with traditional human-to-human therapy sessions in terms of topics and responses.

Method: The researchers analyzed two datasets: human therapist interactions and robot-led support conversations. Using sentence embeddings and K-means clustering, they assessed thematic alignment through distance-based cluster fitting and validated results with Euclidean distances. Semantic overlaps were examined using Transformer, Word2Vec, and BERT models.

Result: 90.88% of robot conversation topics aligned with those from human therapist sessions. Additionally, strong semantic overlap was observed in both disclosure topics and responses given by robots and human therapists.

Conclusion: The study demonstrates parallels between robot and human-led support conversations, showcasing the potential of conversational agents in mental health interventions while identifying existing limitations.

Abstract: As conversational agents increasingly engage in emotionally supportive
dialogue, it is important to understand how closely their interactions resemble
those in traditional therapy settings. This study investigates whether the
concerns shared with a robot align with those shared in human-to-human (H2H)
therapy sessions, and whether robot responses semantically mirror those of
human therapists. We analyzed two datasets: one of interactions between users
and professional therapists (Hugging Face's NLP Mental Health Conversations),
and another involving supportive conversations with a social robot (QTrobot
from LuxAI) powered by a large language model (LLM, GPT-3.5). Using sentence
embeddings and K-means clustering, we assessed cross-agent thematic alignment
by applying a distance-based cluster-fitting method that evaluates whether
responses from one agent type map to clusters derived from the other, and
validated it using Euclidean distances. Results showed that 90.88% of robot
conversation disclosures could be mapped to clusters from the human therapy
dataset, suggesting shared topical structure. For matched clusters, we compared
the subjects as well as therapist and robot responses using Transformer,
Word2Vec, and BERT embeddings, revealing strong semantic overlap in subjects'
disclosures in both datasets, as well as in the responses given to similar
human disclosure themes across agent types (robot vs. human therapist). These
findings highlight both the parallels and boundaries of robot-led support
conversations and their potential for augmenting mental health interventions.

</details>


### [591] [On using AI for EEG-based BCI applications: problems, current challenges and future trends](https://arxiv.org/abs/2506.16168)
*Thomas Barbera,Jacopo Burger,Alessandro D'Amelio,Simone Zini,Simone Bianco,Raffaella Lanzarotti,Paolo Napoletano,Giuseppe Boccignone,Jose Luis Contreras-Vidal*

Main category: cs.HC

TL;DR: This paper explores the integration of AI with EEG data to develop advanced brain-computer interfaces (BCIs), unlocking capabilities such as Brain-to-Speech and Brain-to-Image, while addressing challenges in reliability and applicability in real-world scenarios.


<details>
  <summary>Details</summary>
Motivation: To advance the capabilities of brain-computer interfaces by leveraging breakthroughs in AI and EEG decoding, enabling real-life applications that go beyond traditional uses.

Method: Guided exploration of fundamental paradigms from a causal perspective, analysis of AI-driven models for EEG-based BCIs, and identification of challenges and promising future research directions.

Result: The paper identifies foundational barriers and potential pathways in AI-driven EEG decoding, focusing on developing BCIs for practical, real-world applications.

Conclusion: The paper lays out a roadmap for overcoming technological, methodological, and ethical challenges in EEG-based BCIs, aiming for reliable and effective solutions suited to everyday environments.

Abstract: Imagine unlocking the power of the mind to communicate, create, and even
interact with the world around us. Recent breakthroughs in Artificial
Intelligence (AI), especially in how machines "see" and "understand" language,
are now fueling exciting progress in decoding brain signals from scalp
electroencephalography (EEG). Prima facie, this opens the door to revolutionary
brain-computer interfaces (BCIs) designed for real life, moving beyond
traditional uses to envision Brain-to-Speech, Brain-to-Image, and even a
Brain-to-Internet of Things (BCIoT).
  However, the journey is not as straightforward as it was for Computer Vision
(CV) and Natural Language Processing (NLP). Applying AI to real-world EEG-based
BCIs, particularly in building powerful foundational models, presents unique
and intricate hurdles that could affect their reliability.
  Here, we unfold a guided exploration of this dynamic and rapidly evolving
research area. Rather than barely outlining a map of current endeavors and
results, the goal is to provide a principled navigation of this hot and
cutting-edge research landscape. We consider the basic paradigms that emerge
from a causal perspective and the attendant challenges presented to AI-based
models. Looking ahead, we then discuss promising research avenues that could
overcome today's technological, methodological, and ethical limitations. Our
aim is to lay out a clear roadmap for creating truly practical and effective
EEG-based BCI solutions that can thrive in everyday environments.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [592] [Unsupervised deep learning model for fast energy layer pre-selection of delivery-efficient proton arc therapy plan optimization of nasopharyngeal carcinoma](https://arxiv.org/abs/2506.15803)
*Bohan Yang,Gang Liu,Rirao Dao,Yujia Qian,Ke Shi,Anke Tang,Yong Luo,Jingnan Liu*

Main category: physics.med-ph

TL;DR: This paper introduces an unsupervised deep learning model, SPArcdl, that optimizes energy layer pre-selection for proton arc therapy (PAT), improving treatment quality and efficiency.


<details>
  <summary>Details</summary>
Motivation: Proton arc therapy (PAT) has advantages over conventional methods but is computationally intensive due to the numerous possible energy layer transitions. There is a need for a fast and effective approach to optimize energy layer selection while maintaining high treatment quality.

Method: The study proposes a novel deep learning framework, SPArcdl, based on a UNet architecture. It incorporates a spot-count representation of data and optimizes three objectives: maximizing target coverage, minimizing OAR exposure, and reducing energy layer switching time.

Result: SPArcdl significantly improves plan quality and delivery efficiency compared to the SPArc particle swarm method. It boosts the conformity index, reduces the homogeneity index, shortens energy switching time by 38.4%, and decreases the mean dose to the brainstem. Inference time is under 1 second.

Conclusion: SPArcdl effectively pre-selects energy layers to optimize PAT plans, achieving high-quality dosimetric performance and reduced delivery time, making it a promising tool for clinical applications.

Abstract: Objective. Proton arc therapy (PAT) is an emerging and promising modality in
radiotherapy, offering several advantages over conventional intensitymodulated
proton therapy (IMPT). However, identifying the optimal energy layer (EL)
sequence remains computationally intensive due to the large number of possible
energy layer transitions. This study proposes an unsupervised deep learning
framework for fast and effective EL pre-selection, aiming to minimize energy
layer switch time while preserving high plan quality. Approach. We introduce a
novel data representation method, spot-count representation, which encodes the
number of proton spots intersecting the target and organs at risk (OARs) in a
matrix structured by sorted gantry angles and energy layers. This
representation is the input of a UNet-based architecture, SPArcdl, which is
trained to optimize a tri-objective function: maximizing target coverage,
minimizing OAR exposure, and reducing energy switching time. The model is
evaluated on 54 nasopharyngeal cancer cases, and its performance is benchmarked
against plans generated by SPArcparticle swarm. Main results. SPArcdl produces
EL pre-selection that significantly improves both plan quality and delivery
efficiency. Compared to SPArc particle swarm, it enhances the conformity index
by 0.16 (p < 0.01), reduces the homogeneity index by 0.71 (p < 0.01), shortens
the energy switching time by 38.4% (p < 0.01), and lowers the mean dose to
brainstem by 0.21 (p < 0.01). The results unintentionally reveal employing
unchanged ELS is more time-wise efficient than descended ELS. SPArcdl's
inference time is within 1 second. Significance. SPArcdl is a fast and
effective tool for generating high-quality PAT plans by strategically
pre-selecting energy layers to reduce delivery time while maintaining excellent
dosimetric performance.

</details>


<div id='physics.ins-det'></div>

# physics.ins-det [[Back]](#toc)

### [593] [Bias Variation Compensation in Perimeter-Gated SPAD TRNGs](https://arxiv.org/abs/2506.15888)
*Md Sakibur Sajal,Hunter Guthrie,Marc Dandin*

Main category: physics.ins-det

TL;DR: The paper develops a technique to compensate for bias variation in random binary strings generated from hardware entropy sources (pgSPAD array), achieving high-quality random numbers.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of bias variation in hardware-based random number generators and ensure unbiased and high-quality random bits.

Method: A 64x64 array of pgSPADs was used, with tailored gate voltages compensating for native dark count rates, followed by debiasing via the iterative Von Neumann algorithm.

Result: The compensation reduced bias variation to less than 1%, achieving a raw-bit generation rate of 2 kHz/pixel. The debiased bits passed all 16 NIST Statistical Test Suite tests.

Conclusion: The proposed approach effectively mitigates bias variation and produces high-quality, statistically validated random binary strings.

Abstract: Random number generators that utilize arrays of entropy source elements
suffer from bias variation (BV). Despite the availability of efficient
debiasing algorithms, optimized implementations of hardware friendly options
depend on the bit bias in the raw bit streams and cannot accommodate a wide BV.
In this work, we present a 64 x 64 array of perimeter gated single photon
avalanche diodes (pgSPADs), fabricated in a 0.35 {\mu}m standard CMOS
technology, as a source of entropy to generate random binary strings with a BV
compensation technique. By applying proper gate voltages based on the devices'
native dark count rates, we demonstrate less than 1% BV for a raw-bit
generation rate of 2 kHz/pixel at room temperature. The raw bits were debiased
using the classical iterative Von Neumann's algorithm and the debiased bits
were found to pass all of the 16 tests from NIST's Statistical Test Suite.

</details>


### [594] [Improvement of Nuclide Detection through Graph Spectroscopic Analysis Framework and its Application to Nuclear Facility Upset Detection](https://arxiv.org/abs/2506.16522)
*Pedro Rodríguez Fernández,Christian Svinth,Alex Hagen*

Main category: physics.ins-det

TL;DR: The paper introduces a neural network-based method to improve radionuclide detection using spectroscopic radiation detectors via attention mechanisms and temporal event distribution analysis, yielding significant gains over traditional methods.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the limitations in radionuclide detection thresholds and improve performance in identifying releases from nuclear facilities.

Method: The researchers utilize a neural network with an attention mechanism to enhance spectroscopic data analysis, focusing on arrival times and adapting thresholds based on temporal and spectral distributions.

Result: The method achieves a 2x improvement in detecting Cesium releases compared to conventional spectroscopic methods and demonstrates adaptability for more complex radionuclide decay chains.

Conclusion: The method has broad applicability and potential for integration of diverse detection event data to further enhance its generalizability across different radionuclide scenarios.

Abstract: We present a method to improve the detection limit for radionuclides using
spectroscopic radiation detectors and the arrival time of each detected
radiation quantum. We enable this method using a neural network with an
attention mechanism. We illustrate the method on the detection of Cesium
release from a nuclear facility during an upset, and our method shows $2\times$
improvement over the traditional spectroscopic method. We hypothesize that our
method achieves this performance increase by modulating its detection
probability by the overall rate of probable detections, specifically by
adapting detection thresholds based on temporal event distributions and local
spectral features, and show evidence to this effect. We believe this method is
applicable broadly and may be more successful for radionuclides with more
complicated decay chains than Cesium; we also note that our method can
generalize beyond the addition of arrival time and could integrate other data
about each detection event, such as pulse quality, location in detector, or
even combining the energy and time from detections in different detectors.

</details>


<div id='q-fin.RM'></div>

# q-fin.RM [[Back]](#toc)

### [595] [On Design of Representative Distributionally Robust Formulations for Evaluation of Tail Risk Measures](https://arxiv.org/abs/2506.16230)
*Anand Deo*

Main category: q-fin.RM

TL;DR: This paper tackles the sensitivity of Conditional Value-at-Risk (CVaR) to extreme losses by leveraging extreme value theory to improve Distributionally Robust Optimization (DRO) formulations. The proposed method yields representative worst-case evaluations without overestimating the CVaR.


<details>
  <summary>Details</summary>
Motivation: CVaR, a widely-used risk measure, struggles with tail sensitivity due to limited samples. This sensitivity complicates real-world risk assessments, necessitating methods to achieve more accurate and robust evaluations.

Method: The paper introduces a DRO formulation based on extreme value theory, requiring the calibration of a single scalar parameter. This approach tailors the evaluation process to avoid underestimation while maintaining realistic assessments.

Result: Theoretical demonstration shows the implementability of the method without using extensive data. Numerical experiments on synthetic and real-world data validate the practical utility of the improved DRO formulation.

Conclusion: The proposed DRO formulation enhances CVaR evaluations by balancing accuracy and robustness, addressing tail risk efficiently even with limited data. Its applicability extends to multivariate setups and other risk measures.

Abstract: Conditional Value-at-Risk (CVaR) is a risk measure widely used to quantify
the impact of extreme losses. Owing to the lack of representative samples CVaR
is sensitive to the tails of the underlying distribution. In order to combat
this sensitivity, Distributionally Robust Optimization (DRO), which evaluates
the worst-case CVaR measure over a set of plausible data distributions is often
deployed. Unfortunately, an improper choice of the DRO formulation can lead to
a severe underestimation of tail risk. This paper aims at leveraging extreme
value theory to arrive at a DRO formulation which leads to representative
worst-case CVaR evaluations in that the above pitfall is avoided while
simultaneously, the worst case evaluation is not a gross over-estimate of the
true CVaR. We demonstrate theoretically that even when there is paucity of
samples in the tail of the distribution, our formulation is readily
implementable from data, only requiring calibration of a single scalar
parameter. We showcase that our formulation can be easily extended to provide
robustness to tail risk in multivariate applications as well as in the
evaluation of other commonly used risk measures. Numerical illustrations on
synthetic and real-world data showcase the practical utility of our approach.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [596] [Linearithmic Clean-up for Vector-Symbolic Key-Value Memory with Kroneker Rotation Products](https://arxiv.org/abs/2506.15793)
*Ruipeng Liu,Qinru Qiu,Simon Khan,Garrett E. Katz*

Main category: cs.DS

TL;DR: This paper addresses the computational bottleneck of Vector-Symbolic Architectures (VSAs) clean-up step by introducing a novel codebook representation using Kroneker products of rotation-like matrices, achieving improved scalability and efficiency.


<details>
  <summary>Details</summary>
Motivation: Current VSAs face challenges in decoding noisy vectors efficiently, as the clean-up step involves computationally expensive comparisons against prototype vectors.

Method: The proposed method restructures the codebook using Kroneker products and rotation-like matrices, enabling significant improvements in clean-up time complexity ($𝒪(N⋅log⋅N)$) and space complexity ($𝒪(	ext{log}⋅N)$ for representation).

Result: Experiments show substantial scalability improvements compared to baseline VSA techniques, making the clean-up process faster and more efficient while preserving memory capacity.

Conclusion: The paper introduced a scalable and efficient solution for VSAs clean-up, demonstrating advances in computational feasibility without compromising memory capacity.

Abstract: A computational bottleneck in current Vector-Symbolic Architectures (VSAs) is
the ``clean-up'' step, which decodes the noisy vectors retrieved from the
architecture. Clean-up typically compares noisy vectors against a ``codebook''
of prototype vectors, incurring computational complexity that is quadratic or
similar. We present a new codebook representation that supports efficient
clean-up, based on Kroneker products of rotation-like matrices. The resulting
clean-up time complexity is linearithmic, i.e. $\mathcal{O}(N\,\text{log}\,N)$,
where $N$ is the vector dimension and also the number of vectors in the
codebook. Clean-up space complexity is $\mathcal{O}(N)$. Furthermore, the
codebook is not stored explicitly in computer memory: It can be represented in
$\mathcal{O}(\text{log}\,N)$ space, and individual vectors in the codebook can
be materialized in $\mathcal{O}(N)$ time and space. At the same time,
asymptotic memory capacity remains comparable to standard approaches. Computer
experiments confirm these results, demonstrating several orders of magnitude
more scalability than baseline VSA techniques.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [597] [MoR: Better Handling Diverse Queries with a Mixture of Sparse, Dense, and Human Retrievers](https://arxiv.org/abs/2506.15862)
*Jushaan Singh Kalra,Xinran Zhao,To Eun Kim,Fengyu Cai,Fernando Diaz,Tongshuang Wu*

Main category: cs.IR

TL;DR: The paper introduces a framework for dynamically combining multiple types of retrievers, enhancing Retrieval-augmented Generation (RAG) in a zero-shot setting.


<details>
  <summary>Details</summary>
Motivation: Current RAG approaches usually rely on fixed retrievers, which limits their generalization across varying information needs.

Method: The proposed method uses a zero-shot, weighted combination of heterogeneous retrievers to dynamically select and integrate for each query.

Result: Experiments demonstrate the proposed mixture is effective, outperforming individual retrievers and larger models by significant margins.

Conclusion: Mixture of retrievers can dynamically combine retriever signals for improved efficiency, accuracy, and adaptability in RAG tasks.

Abstract: Retrieval-augmented Generation (RAG) is powerful, but its effectiveness
hinges on which retrievers we use and how. Different retrievers offer distinct,
often complementary signals: BM25 captures lexical matches; dense retrievers,
semantic similarity. Yet in practice, we typically fix a single retriever based
on heuristics, which fails to generalize across diverse information needs. Can
we dynamically select and integrate multiple retrievers for each individual
query, without the need for manual selection? In our work, we validate this
intuition with quantitative analysis and introduce mixture of retrievers: a
zero-shot, weighted combination of heterogeneous retrievers. Extensive
experiments show that such mixtures are effective and efficient: Despite
totaling just 0.8B parameters, this mixture outperforms every individual
retriever and even larger 7B models by +10.8% and +3.9% on average,
respectively. Further analysis also shows that this mixture framework can help
incorporate specialized non-oracle human information sources as retrievers to
achieve good collaboration, with a 58.9% relative performance improvement over
simulated humans alone.

</details>


### [598] [Revela: Dense Retriever Learning via Language Modeling](https://arxiv.org/abs/2506.16552)
*Fengyu Cai,Tong Chen,Xinran Zhao,Sihao Chen,Hongming Zhang,Sherry Tongshuang Wu,Iryna Gurevych,Heinz Koeppl*

Main category: cs.IR

TL;DR: Revela introduces a self-supervised framework for training dense retrievers using language modeling principles, achieving notable performance improvements.


<details>
  <summary>Details</summary>
Motivation: Dense retrievers are essential for augmenting LMs with external knowledge, but training them typically requires costly annotated data. The paper aims to explore self-supervised approaches to address this challenge.

Method: Revela leverages language modeling by conditioning next-token prediction on local and cross-document context using retriever-computed weighted attention in a unified framework.

Result: Revela shows significant improvements in retrieval performance across general-domain and specialized benchmarks, outperforming prior methods by 5.2% and 5.6% in NDCG@10.

Conclusion: The framework demonstrates scalability and effectiveness in self-supervised retriever learning, contributing to advancements in dense retrieval with language modeling principles.

Abstract: Dense retrievers play a vital role in accessing external and specialized
knowledge to augment language models (LMs). Training dense retrievers typically
requires annotated query-document pairs, which are costly and hard to obtain in
specialized domains such as code-motivating growing interest in self-supervised
retriever learning. Since LMs are trained to capture token-level dependencies
through a self-supervised learning objective (i.e., next-token prediction), we
can analogously cast retrieval as learning dependencies among chunks of tokens.
This analogy naturally leads to the question: How can we adapt self-supervised
learning objectives in the spirit of language modeling to train retrievers?
  To answer this question, we introduce Revela, a unified and scalable training
framework for self-supervised retriever learning via language modeling. Revela
models semantic dependencies among documents by conditioning next-token
prediction on both local and cross-document context through an in-batch
attention mechanism. This attention is weighted by retriever-computed
similarity scores, enabling the retriever to be optimized as part of language
modeling. We evaluate Revela on both general-domain (BEIR) and domain-specific
(CoIR) benchmarks across various retriever backbones. At a comparable parameter
scale, Revela outperforms the previous best method with absolute improvements
of 5.2 % (18.3 % relative) and 5.6 % (14.4 % relative) on NDCG@10,
respectively, underscoring its effectiveness. Performance increases with model
size, highlighting both the scalability of our approach and its promise for
self-supervised retriever learning.

</details>


### [599] [GFlowGR: Fine-tuning Generative Recommendation Frameworks with Generative Flow Networks](https://arxiv.org/abs/2506.16114)
*Yejing Wang,Shengyu Zhou,Jinyu Lu,Qidong Liu,Xinhang Li,Wenlin Zhang,Feng Li,Pengjie Wang,Jian Xu,Bo Zheng,Xiangyu Zhao*

Main category: cs.IR

TL;DR: This paper introduces a GFlowNets-based fine-tuning framework (GFlowGR) to improve generative recommendation systems by addressing the exposure bias issue in fine-tuning methods.


<details>
  <summary>Details</summary>
Motivation: To address the unexplored exposure bias problem in fine-tuning large language models (LLMs) for generative recommendation systems.

Method: The paper proposes a multi-step generation task using GFlowNets-based fine-tuning, which integrates collaborative knowledge, adaptive trajectory sampling, and heuristic weighting to construct a reward model.

Result: The empirical evaluation on two datasets and two GR backbones demonstrates that GFlowGR is effective and robust in mitigating exposure bias and improving performance.

Conclusion: GFlowGR represents a promising new direction for fine-tuning frameworks in generative recommendations, addressing critical issues in exposure bias and advancing the performance of recommendation systems.

Abstract: Generative recommendations (GR), which usually include item tokenizers and
generative Large Language Models (LLMs), have demonstrated remarkable success
across a wide range of scenarios. The majority of existing research efforts
primarily concentrate on developing powerful item tokenizers or advancing LLM
decoding strategies to attain superior performance. However, the critical
fine-tuning step in GR frameworks, which is essential for adapting LLMs to
recommendation data, remains largely unexplored. Current approaches
predominantly rely on either the next-token prediction loss of supervised
fine-tuning (SFT) or recommendationspecific direct preference optimization
(DPO) strategies. Both methods ignore the exploration of possible positive
unobserved samples, which is commonly referred to as the exposure bias problem.
To mitigate this problem, this paper treats the GR as a multi-step generation
task and constructs a GFlowNets-based fine-tuning framework (GFlowGR). The
proposed framework integrates collaborative knowledge from traditional
recommender systems to create an adaptive trajectory sampler and a
comprehensive reward model. Leveraging the diverse generation property of
GFlowNets, along with sampling and heuristic weighting techniques, GFlowGR
emerges as a promising approach to mitigate the exposure bias problem.
Extensive empirical results on two real-world datasets and with two different
GR backbones highlight the effectiveness and robustness of GFlowGR.

</details>


### [600] [A Simple Contrastive Framework Of Item Tokenization For Generative Recommendation](https://arxiv.org/abs/2506.16683)
*Penglong Zhai,Yifang Yuan,Fanyi Di,Jie Li,Yue Liu,Chen Li,Jie Huang,Sicong Wang,Yao Xu,Xin Li*

Main category: cs.IR

TL;DR: The paper introduces SimCIT, an unsupervised deep quantization framework using contrastive learning for generative retrieval-based recommendation systems.


<details>
  <summary>Details</summary>
Motivation: Generative retrieval approaches face challenges like redundancy and vast token spaces, especially in large-scale systems, requiring effective tokenization and integration of multi-modal data.

Method: SimCIT uses residual quantization and contrastive learning to align multi-modal information, focusing on semantic tokenization rather than precise reconstruction.

Result: Experiments across public and industrial datasets show SimCIT's efficacy in generative recommendation systems.

Conclusion: SimCIT effectively addresses tokenization and multi-modal integration challenges in generative recommendations, improving system performance through its innovative framework.

Abstract: Generative retrieval-based recommendation has emerged as a promising paradigm
aiming at directly generating the identifiers of the target candidates.
However, in large-scale recommendation systems, this approach becomes
increasingly cumbersome due to the redundancy and sheer scale of the token
space. To overcome these limitations, recent research has explored the use of
semantic tokens as an alternative to ID tokens, which typically leveraged
reconstruction-based strategies, like RQ-VAE, to quantize content embeddings
and significantly reduce the embedding size. However, reconstructive
quantization aims for the precise reconstruction of each item embedding
independently, which conflicts with the goal of generative retrieval tasks
focusing more on differentiating among items. Moreover, multi-modal side
information of items, such as descriptive text and images, geographical
knowledge in location-based recommendation services, has been shown to be
effective in improving recommendations by providing richer contexts for
interactions. Nevertheless, effectively integrating such complementary
knowledge into existing generative recommendation frameworks remains
challenging. To overcome these challenges, we propose a novel unsupervised deep
quantization exclusively based on contrastive learning, named SimCIT (a Simple
Contrastive Item Tokenization framework). Specifically, different from existing
reconstruction-based strategies, SimCIT propose to use a learnable residual
quantization module to align with the signals from different modalities of the
items, which combines multi-modal knowledge alignment and semantic tokenization
in a mutually beneficial contrastive learning framework. Extensive experiments
across public datasets and a large-scale industrial dataset from various
domains demonstrate SimCIT's effectiveness in LLM-based generative
recommendation.

</details>


<div id='cs.CG'></div>

# cs.CG [[Back]](#toc)

### [601] [Wavelet-based Global Orientation and Surface Reconstruction for Point Clouds](https://arxiv.org/abs/2506.16299)
*Yueji Ma,Yanzun Meng,Dong Xiao,Zuoqiang Shi,Bin Wang*

Main category: cs.CG

TL;DR: This paper focuses on enhancing unoriented surface reconstruction for sparse point clouds using wavelet-based methods, demonstrating state-of-the-art results and efficiency.


<details>
  <summary>Details</summary>
Motivation: Address challenges in surface reconstruction for sparse point clouds, which are poorly handled by existing methods for unoriented points.

Method: Introduce a wavelet-based mollified indicator function approach, utilize kernel functions for smoothing and acceleration, and design divergence-free constraints for stability.

Result: Achieves state-of-the-art performance in orientation and surface reconstruction, particularly for sparse models while ensuring CPU efficiency.

Conclusion: The proposed method outperforms existing techniques, demonstrating its capability to enhance stability, effectiveness, and computational efficiency in surface reconstruction tasks.

Abstract: Unoriented surface reconstruction is an important task in computer graphics
and has extensive applications. Based on the compact support of wavelet and
orthogonality properties, classic wavelet surface reconstruction achieves good
and fast reconstruction. However, this method can only handle oriented points.
Despite some improved attempts for unoriented points, such as iWSR, these
methods perform poorly on sparse point clouds. To address these shortcomings,
we propose a wavelet-based method to represent the mollified indicator function
and complete both the orientation and surface reconstruction tasks. We use the
modifying kernel function to smoothen out discontinuities on the surface,
aligning with the continuity of the wavelet basis function. During the
calculation of coefficient, we fully utilize the properties of the
convolutional kernel function to shift the modifying computation onto wavelet
basis to accelerate. In addition, we propose a novel method for constructing
the divergence-free function field and using them to construct the additional
homogeneous constraints to improve the effectiveness and stability. Extensive
experiments demonstrate that our method achieves state-of-the-art performance
in both orientation and reconstruction for sparse models. We align the matrix
construction with the compact support property of wavelet basis functions to
further accelerate our method, resulting in efficient performance on CPU. Our
source codes will be released on GitHub.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [602] [Client Selection Strategies for Federated Semantic Communications in Heterogeneous IoT Networks](https://arxiv.org/abs/2506.17063)
*Samer Lahoud,Kinda Khawam*

Main category: cs.NI

TL;DR: The paper introduces a federated semantic communication framework to tackle bandwidth efficiency and privacy in IoT networks, focusing on image reconstruction across diverse devices.


<details>
  <summary>Details</summary>
Motivation: To address communication overhead and privacy challenges in heterogeneous IoT networks with bandwidth constraints.

Method: Introduces a federated semantic communication framework leveraging client selection strategies, semantic bottlenecks, and loss-based aggregation.

Result: Experimental results show that Utilitarian client selection provides high reconstruction quality, while Proportional Fairness improves resource allocation fairness and computational efficiency.

Conclusion: Federated semantic communication balances reconstruction quality, resource efficiency, and fairness, enabling sustainable edge intelligence in IoT environments.

Abstract: The exponential growth of IoT devices presents critical challenges in
bandwidth-constrained wireless networks, particularly regarding efficient data
transmission and privacy preservation. This paper presents a novel federated
semantic communication (SC) framework that enables collaborative training of
bandwidth-efficient models for image reconstruction across heterogeneous IoT
devices. By leveraging SC principles to transmit only semantic features, our
approach dramatically reduces communication overhead while preserving
reconstruction quality. We address the fundamental challenge of client
selection in federated learning environments where devices exhibit significant
disparities in dataset sizes and data distributions. Our framework implements
three distinct client selection strategies that explore different trade-offs
between system performance and fairness in resource allocation. The system
employs an end-to-end SC architecture with semantic bottlenecks, coupled with a
loss-based aggregation mechanism that naturally adapts to client heterogeneity.
Experimental evaluation on image data demonstrates that while Utilitarian
selection achieves the highest reconstruction quality, Proportional Fairness
maintains competitive performance while significantly reducing participation
inequality and improving computational efficiency. These results establish that
federated SC can successfully balance reconstruction quality, resource
efficiency, and fairness in heterogeneous IoT deployments, paving the way for
sustainable and privacy-preserving edge intelligence applications.

</details>


<div id='cond-mat.quant-gas'></div>

# cond-mat.quant-gas [[Back]](#toc)

### [603] [Single-shot thermometry of simulated Bose--Einstein condensates using artificial intelligence](https://arxiv.org/abs/2506.16925)
*Jack Griffiths,Steven A. Wrathmall,Simon A. Gardiner*

Main category: cond-mat.quant-gas

TL;DR: The paper introduces an AI approach using convolutional neural networks to rapidly determine thermodynamic parameters of ultracold Bose gases from single-shot density images, achieving non-destructive, highly accurate measurements.


<details>
  <summary>Details</summary>
Motivation: Traditional methods for measuring thermodynamic parameters in ultracold Bose gases are destructive and prone to significant uncertainties, necessitating the development of faster, non-invasive, and accurate techniques.

Method: A convolutional neural network (CNN) is trained on quasi-2D Bose-Einstein condensates in harmonic traps to predict temperature and chemical potential from single-shot density profiles. The model also exhibits generalization to unseen trap geometries and dynamic thermal states.

Result: The CNN achieves parameter extraction in fractions of a second, successfully generalizes to toroidal trap geometries with minimal errors (few nanokelvin), and maintains accuracy during dynamic thermalization without explicit non-equilibrium training.

Conclusion: Supervised learning using neural networks can surpass challenges in ultracold atom thermometry, offering potential extensions to broader scenarios for precise, real-time quantum gas experiments, streamlining workflows, and enhancing measurement accuracy.

Abstract: Precise determination of thermodynamic parameters in ultracold Bose gases
remains challenging due to the destructive nature of conventional measurement
techniques and inherent experimental uncertainties. We demonstrate an
artificial intelligence approach for rapid, non-destructive estimation of the
chemical potential and temperature from single-shot, in situ imaged density
profiles of finite-temperature Bose gases. Our convolutional neural network is
trained exclusively on quasi-2D `pancake' condensates in harmonic trap
configurations. It achieves parameter extraction within fractions of a second.
The model also demonstrates zero-shot generalisation across both trap geometry
and thermalisation dynamics, successfully estimating thermodynamic parameters
for toroidally trapped condensates with errors of only a few nanokelvin despite
no prior exposure to such geometries during training, and maintaining
predictive accuracy during dynamic thermalisation processes after a relatively
brief evolution without explicit training on non-equilibrium states. These
results suggest that supervised learning can overcome traditional limitations
in ultracold atom thermometry, with extension to broader geometric
configurations, temperature ranges, and additional parameters potentially
enabling comprehensive real-time analysis of quantum gas experiments. Such
capabilities could significantly streamline experimental workflows whilst
improving measurement precision across a range of quantum fluid systems.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [604] [DT-UFC: Universal Large Model Feature Coding via Peaky-to-Balanced Distribution Transformation](https://arxiv.org/abs/2506.16495)
*Changsheng Gao,Zijie Liu,Li Li,Dong Liu,Xiaoyan Sun,Weisi Lin*

Main category: cs.MM

TL;DR: This paper proposes a novel approach for universal feature coding to optimize transmission and storage for large models by reshaping diverse, incompatible feature distributions into a common balanced target space.


<details>
  <summary>Details</summary>
Motivation: Existing approaches to feature coding focus on task- or model-specific solutions, ignoring the need for a universal method to handle diverse and incompatible feature distributions from different large models.

Method: A data-driven, plug-and-play transformation is proposed to reshape skewed feature distributions into a balanced and unified target space, allowing for effective compression and generalization across models without modifying downstream codecs.

Result: The proposed method achieves superior compression efficiency and cross-model generalization compared to task-specific approaches across three large models (LLaMA3, DINOv2, and SD3) and multiple tasks.

Conclusion: The study demonstrates that universal feature coding is feasible and effective through distribution transformation, setting a solid foundation for future research in efficient model deployment scenarios.

Abstract: Like image coding in visual data transmission, feature coding is essential
for the distributed deployment of large models by significantly reducing
transmission and storage overhead. However, prior studies have mostly targeted
task- or model-specific scenarios, leaving the challenge of universal feature
coding across diverse large models largely unaddressed. In this paper, we
present the first systematic study on universal feature coding for large
models. The key challenge lies in the inherently diverse and distributionally
incompatible nature of features extracted from different models. For example,
features from DINOv2 exhibit highly peaky, concentrated distributions, while
those from Stable Diffusion 3 (SD3) are more dispersed and uniform. This
distributional heterogeneity severely hampers both compression efficiency and
cross-model generalization. To address this, we propose a learned
peaky-to-balanced distribution transformation, which reshapes highly skewed
feature distributions into a common, balanced target space. This transformation
is non-uniform, data-driven, and plug-and-play, enabling effective alignment of
heterogeneous distributions without modifying downstream codecs. With this
alignment, a universal codec trained on the balanced target distribution can
effectively generalize to features from different models and tasks. We validate
our approach on three representative large models-LLaMA3, DINOv2, and
SD3-across multiple tasks and modalities. Extensive experiments show that our
method achieves notable improvements in both compression efficiency and
cross-model generalization over task-specific baselines. All source code will
be released for future research.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [605] [Improved Intelligibility of Dysarthric Speech using Conditional Flow Matching](https://arxiv.org/abs/2506.16127)
*Shoutrik Das,Nishant Singh,Arjun Gangwar,S Umesh*

Main category: cs.SD

TL;DR: The paper proposes an innovative dysarthric-to-clean speech conversion technique utilizing self-supervised learning features, along with Conditional Flow Matching and Diffusion Transformers, emphasizing discrete acoustic units to enhance speech intelligibility.


<details>
  <summary>Details</summary>
Motivation: To address the communication challenges faced by individuals with dysarthria by developing effective and innovative speech conversion methods.

Method: A fully non-autoregressive technique using self-supervised learning features (via WavLM) and Conditional Flow Matching with Diffusion Transformers to translate dysarthric speech to clean single-speaker voice speech.

Result: The approach improves intelligibility and convergence speed when compared to traditional mel-spectrogram-based methods, thanks to the use of discrete acoustic units.

Conclusion: The use of self-supervised learning features and discrete acoustic units is promising for faster and better intelligibility in dysarthric speech conversion techniques.

Abstract: Dysarthria is a neurological disorder that significantly impairs speech
intelligibility, often rendering affected individuals unable to communicate
effectively. This necessitates the development of robust dysarthric-to-regular
speech conversion techniques. In this work, we investigate the utility and
limitations of self-supervised learning (SSL) features and their quantized
representations as an alternative to mel-spectrograms for speech generation.
Additionally, we explore methods to mitigate speaker variability by generating
clean speech in a single-speaker voice using features extracted from WavLM. To
this end, we propose a fully non-autoregressive approach that leverages
Conditional Flow Matching (CFM) with Diffusion Transformers to learn a direct
mapping from dysarthric to clean speech. Our findings highlight the
effectiveness of discrete acoustic units in improving intelligibility while
achieving faster convergence compared to traditional mel-spectrogram-based
approaches.

</details>


### [606] [Universal Music Representations? Evaluating Foundation Models on World Music Corpora](https://arxiv.org/abs/2506.17055)
*Charilaos Papaioannou,Emmanouil Benetos,Alexandros Potamianos*

Main category: cs.SD

TL;DR: This paper evaluates five state-of-the-art audio foundation models across diverse musical traditions using three methodologies, achieving state-of-the-art performance in five datasets but highlighting limitations in cross-cultural generalization.


<details>
  <summary>Details</summary>
Motivation: To investigate the ability of foundation models to generalize across diverse global music traditions, addressing a gap in understanding their cross-cultural capabilities.

Method: Three methodologies were used: probing for representation assessment, supervised fine-tuning of 1-2 layers, and few-shot learning for low-resource scenarios.

Result: Larger models performed better on non-Western music but saw reduced success in culturally distant traditions. State-of-the-art results were achieved in five of six datasets, but fine-tuning was not significantly superior to probing.

Conclusion: Foundation models encode substantial cross-cultural musical knowledge but are not yet universally effective across all traditions. The proposed evaluation framework establishes a benchmark for future advancements in universal music representation.

Abstract: Foundation models have revolutionized music information retrieval, but
questions remain about their ability to generalize across diverse musical
traditions. This paper presents a comprehensive evaluation of five
state-of-the-art audio foundation models across six musical corpora spanning
Western popular, Greek, Turkish, and Indian classical traditions. We employ
three complementary methodologies to investigate these models' cross-cultural
capabilities: probing to assess inherent representations, targeted supervised
fine-tuning of 1-2 layers, and multi-label few-shot learning for low-resource
scenarios. Our analysis shows varying cross-cultural generalization, with
larger models typically outperforming on non-Western music, though results
decline for culturally distant traditions. Notably, our approaches achieve
state-of-the-art performance on five out of six evaluated datasets,
demonstrating the effectiveness of foundation models for world music
understanding. We also find that our targeted fine-tuning approach does not
consistently outperform probing across all settings, suggesting foundation
models already encode substantial musical knowledge. Our evaluation framework
and benchmarking results contribute to understanding how far current models are
from achieving universal music representations while establishing metrics for
future progress.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [607] [Category-based Galaxy Image Generation via Diffusion Models](https://arxiv.org/abs/2506.16255)
*Xingzhong Fan,Hongming Tang,Yue Zeng,M. B. N. Kouwenhoven,Guangquan Zeng*

Main category: astro-ph.IM

TL;DR: The introduction of GalCatDiff, a new galaxy generation framework, enhances realism and physical consistency using diffusion models with astrophysical properties.


<details>
  <summary>Details</summary>
Motivation: Existing galaxy generation methods rely on complex physical assumptions and parameter tuning, limiting their adaptability. Data-driven models like diffusion models offer a flexible alternative but require further improvement using astrophysical knowledge.

Method: The framework GalCatDiff employs an enhanced U-Net architecture combined with Astro-RAB (Residual Attention Block) for robust feature fidelity and attention mechanisms. Additionally, it utilizes category embeddings for efficient class-specific galaxy generation.

Result: GalCatDiff outperforms previous galaxy generation methods in terms of sample consistency, color/size distributions, and visual realism while being computationally efficient.

Conclusion: GalCatDiff offers a significant advancement in galaxy simulations, merging physical realism with computational efficiency, and can also be a useful tool for data augmentation in astronomy research.

Abstract: Conventional galaxy generation methods rely on semi-analytical models and
hydrodynamic simulations, which are highly dependent on physical assumptions
and parameter tuning. In contrast, data-driven generative models do not have
explicit physical parameters pre-determined, and instead learn them efficiently
from observational data, making them alternative solutions to galaxy
generation. Among these, diffusion models outperform Variational Autoencoders
(VAEs) and Generative Adversarial Networks (GANs) in quality and diversity.
Leveraging physical prior knowledge to these models can further enhance their
capabilities. In this work, we present GalCatDiff, the first framework in
astronomy to leverage both galaxy image features and astrophysical properties
in the network design of diffusion models. GalCatDiff incorporates an enhanced
U-Net and a novel block entitled Astro-RAB (Residual Attention Block), which
dynamically combines attention mechanisms with convolution operations to ensure
global consistency and local feature fidelity. Moreover, GalCatDiff uses
category embeddings for class-specific galaxy generation, avoiding the high
computational costs of training separate models for each category. Our
experimental results demonstrate that GalCatDiff significantly outperforms
existing methods in terms of the consistency of sample color and size
distributions, and the generated galaxies are both visually realistic and
physically consistent. This framework will enhance the reliability of galaxy
simulations and can potentially serve as a data augmentor to support future
galaxy classification algorithm development.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [608] [Quantifying Flow State Dynamics: A Prefrontal Cortex EEG-Based Model Validation Study. Unveiling the Prefrontal Cortex's Role in Flow State Experience: An Empirical EEG Analysis](https://arxiv.org/abs/2506.16838)
*Gianluca Rosso,Raffaella Ricci,Lorenzo Pia,Giovanni Rebaudo,Michele Guindani,Alberto Marocchino,Giorgio De Pieri,Andrea Filippo Rosso*

Main category: stat.AP

TL;DR: This paper examines the connection between the flow mental state and EEG brain activity using predictive models and portable EEG devices, mainly in sports like golf.


<details>
  <summary>Details</summary>
Motivation: The motivation is to optimize mental performance by analyzing the flow state, known to enhance performance through better synchronization of mind and body.

Method: The study used predictive models like Flow State Index within proprietary software, portable EEG devices, questionnaires, and psychological exercises for analysis.

Result: A strong correlation was found between EEG data and subjective flow state experiences, and psychological exercises were shown to enhance flow.

Conclusion: The findings confirm the feasibility of detecting and encouraging the flow state through EEG activity, with applications in sports, gaming, and mental training practices beyond its validation in golf.

Abstract: This article aims to explore the optimization of mental performance through
the analysis of metrics associated with the psychological state known as flow.
Several clinical studies have shown a correlation between the mental state of
flow (characterized by deep and relaxed concentration and high psychophysical
efficiency) and brain activity measured through electroencephalography (EEG).
This study confirms such a correlation, focusing in particular on the sports
field, where the flow state tends to occur more frequently. To conduct the
study, Sporthype developed proprietary software that integrates several
predictive models, in particular the Flow State Index (FSI), implemented within
the Holytics system. An analytical protocol was established, including mental
exercises and data collection sessions using the portable EEG device Muse,
accompanied by a questionnaire to gather athletes' subjective perceptions of
their mental state. The results revealed a significant alignment between the
EEG data and the subjective experiences reported in the questionnaires,
confirming the feasibility of detecting the flow state through prefrontal
cortex activity. Furthermore, the psychological exercises included in the study
protocol showed a tangible positive effect in enhancing flow during athletic
performance. Flow improves performance through a more harmonious
synchronization between mind and body. Although golf was the main context of
the experimentation, the mathematical models developed within Holytics were
designed to be applicable to a wide range of sports. In addition to golf,
preliminary tests have been conducted in other sports such as tennis, as well
as in non-sport contexts, including gaming and mental training practices such
as mindfulness, concentration, and visualization.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [609] [Data-Agnostic Cardinality Learning from Imperfect Workloads](https://arxiv.org/abs/2506.16007)
*Peizhi Wu,Rong Kang,Tieying Zhang,Jianjun Chen,Ryan Marcus,Zachary G. Ives*

Main category: cs.DB

TL;DR: GRASP is a data-agnostic cardinality estimation system designed to handle real-world constraints, such as incomplete and imbalanced query workloads.


<details>
  <summary>Details</summary>
Motivation: Current query-driven cardinality estimation models fail in real-world scenarios where data access is restricted or incomplete workloads and imbalanced join templates are present.

Method: GRASP uses a compositional design that generalizes to unseen join templates and includes a per-table cardinality model for handling value distribution shifts, alongside a learned count sketch model to capture join correlations.

Result: GRASP demonstrates superior accuracy and lower query latency compared to existing query-driven models in experiments, and even outperforms traditional methods on complex benchmarks using limited data.

Conclusion: GRASP is a robust and effective solution for cardinality estimation in environments with restricted data access and imperfect workloads, addressing issues of real-world applicability.

Abstract: Cardinality estimation (CardEst) is a critical aspect of query optimization.
Traditionally, it leverages statistics built directly over the data. However,
organizational policies (e.g., regulatory compliance) may restrict global data
access. Fortunately, query-driven cardinality estimation can learn CardEst
models using query workloads. However, existing query-driven models often
require access to data or summaries for best performance, and they assume
perfect training workloads with complete and balanced join templates (or join
graphs). Such assumptions rarely hold in real-world scenarios, in which join
templates are incomplete and imbalanced. We present GRASP, a data-agnostic
cardinality learning system designed to work under these real-world
constraints. GRASP's compositional design generalizes to unseen join templates
and is robust to join template imbalance. It also introduces a new per-table
CardEst model that handles value distribution shifts for range predicates, and
a novel learned count sketch model that captures join correlations across base
relations. Across three database instances, we demonstrate that GRASP
consistently outperforms existing query-driven models on imperfect workloads,
both in terms of estimation accuracy and query latency. Remarkably, GRASP
achieves performance comparable to, or even surpassing, traditional approaches
built over the underlying data on the complex CEB-IMDb-full benchmark --
despite operating without any data access and using only 10% of all possible
join templates.

</details>


<div id='cond-mat.dis-nn'></div>

# cond-mat.dis-nn [[Back]](#toc)

### [610] [Uncertainty in AI-driven Monte Carlo simulations](https://arxiv.org/abs/2506.14594)
*Dimitrios Tzivrailis,Alberto Rosso,Eiji Kawasaki*

Main category: cond-mat.dis-nn

TL;DR: This paper proposes the Penalty Ensemble Method (PEM) to address epistemic uncertainty in deep learning-augmented Monte Carlo simulations by modifying the Metropolis rule for increased reliability.


<details>
  <summary>Details</summary>
Motivation: Monte Carlo techniques for evaluating physical systems become computationally expensive due to repeated energy and force field evaluations, especially with long-range interactions, leading to interest in efficient solutions.

Method: Deep learning models approximate energy landscapes or force fields, but the method introduces PEM, modifying the Metropolis acceptance rule to account for epistemic uncertainty by rejecting configurations in uncertain regions.

Result: The proposed approach mitigates epistemic uncertainty in sampling, improving the reliability of simulation outcomes.

Conclusion: Penalty Ensemble Method increases confidence in simulations by managing uncertainty through an uncertainty-aware Metropolis rule, enhancing overall robustness.

Abstract: In the study of complex systems, evaluating physical observables often
requires sampling representative configurations via Monte Carlo techniques.
These methods rely on repeated evaluations of the system's energy and force
fields, which can become computationally expensive, particularly in the
presence of long-range interactions. To accelerate these simulations, deep
learning models are increasingly employed as surrogate functions to approximate
the energy landscape or force fields. However, such models introduce epistemic
uncertainty in their predictions, which may propagate through the sampling
process and affect the system's macroscopic behavior. In this work, we present
the Penalty Ensemble Method (PEM) to quantify epistemic uncertainty and
mitigate its impact on Monte Carlo sampling. Our approach introduces an
uncertainty-aware modification of the Metropolis acceptance rule, which
increases the rejection probability in regions of high uncertainty, thereby
enhancing the reliability of the simulation outcomes.

</details>


<div id='cond-mat.stat-mech'></div>

# cond-mat.stat-mech [[Back]](#toc)

### [611] [Microcanonical simulated annealing: Massively parallel Monte Carlo simulations with sporadic random-number generation](https://arxiv.org/abs/2506.16240)
*M. Bernaschi,L. A. Fernandez,I. González-Adalid Pemartín,E. Marinari,V. Martin-Mayor,G. Parisi,F. Ricci-Tersenghi,J. J. Ruiz-Lorenzo,D. Yllanes*

Main category: cond-mat.stat-mech

TL;DR: The paper introduces a new simulated annealing technique that reduces the reliance on random-number generation, enabling efficient massively parallel computations.


<details>
  <summary>Details</summary>
Motivation: Monte Carlo simulations are essential for modeling complex systems ranging from quantum chromodynamics to spin-glass systems, but their excessive demand for random numbers hampers computational efficiency on advanced hardware.

Method: The authors propose microcanonical simulated annealing (mic.SA), which minimizes the need for random numbers and is tailored for massively parallel computation. The algorithm was benchmarked using GPUs and compared to conventional methods like those on Janus II supercomputers.

Result: The new algorithm showed comparable thermal equilibrium results with conventional simulations, and off-equilibrium dynamics were accurately mapped through simple time rescaling.

Conclusion: Microcanonical simulated annealing offers highly efficient computations for complex models, achieving results comparable to random-number-intensive simulations while overcoming their major computational drawbacks.

Abstract: Numerical simulations of models and theories that describe complex
experimental systems $\unicode{x2014}$in fields like high-energy and
condensed-matter physics$\unicode{x2014}$ are becoming increasingly important.
Examples include lattice gauge theories, which can describe, among others,
quantum chromodynamics (the Standard Model description of strong interactions
between elementary particles), and spin-glass systems. Beyond fundamental
research, these computational methods also find practical applications, among
many others, in optimization, finance, and complex biological problems.
However, Monte Carlo simulations, an important subcategory of these methods,
are plagued by a major drawback: they are extremely greedy for (pseudo) random
numbers. The total fraction of computer time dedicated to random-number
generation increases as the hardware grows more sophisticated, and can get
prohibitive for special-purpose computing platforms. We propose here a
general-purpose microcanonical simulated annealing (mic.SA) formalism that
dramatically reduces such a burden. The algorithm is fully adapted to a
massively parallel computation, as we show in the particularly demanding
benchmark of the three-dimensional Ising spin glass. We carry out very
stringent numerical tests of the new algorithm by comparing our results,
obtained on GPUs, with high-precision standard (i.e., random-number-greedy)
simulations performed on the Janus II custom-built supercomputer. In those
cases where thermal equilibrium is reachable (i.e., in the paramagnetic phase),
both simulations reach compatible values. More significantly, barring
short-time corrections, a simple time rescaling suffices to map the mic.SA
off-equilibrium dynamics onto the results obtained with standard simulations.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [612] [Quantum Fisher-Preconditioned Reinforcement Learning: From Single-Qubit Control to Rayleigh-Fading Link Adaptation](https://arxiv.org/abs/2506.15753)
*Oluwaseyi Giwa,Muhammad Ahmed Mohsin,Muhammad Ali Jamshed*

Main category: quant-ph

TL;DR: The paper introduces Quantum-Preconditioned Policy Gradient (QPPG), a novel algorithm utilizing inverse quantum Fisher information for improved and stable reinforcement learning performance.


<details>
  <summary>Details</summary>
Motivation: Existing reinforcement learning methods often struggle with convergence speed and robustness in noisy environments, motivating the development of algorithms leveraging quantum principles to address these issues.

Method: The proposed QPPG algorithm integrates natural gradient preconditioning using full inverse quantum Fisher information with Tikhonov regularization for stable learning outcomes.

Result: QPPG demonstrated 4x faster convergence compared to REINFORCE and showcased high noise resistance, achieving significant performance improvements in classical and quantum tasks.

Conclusion: The research validates QPPG's efficacy in combining quantum geometry with classical RL methods, providing scalable and noise-robust solutions for reinforcement learning challenges.

Abstract: In this letter, we propose Quantum-Preconditioned Policy Gradient (QPPG), a
natural gradient-based algorithm for link adaptation that whitens policy
updates using the full inverse quantum Fisher information with Tikhonov
regularization. QPPG bridges classical and quantum geometry, achieving stable
learning even under noise. Evaluated on classical and quantum environments,
including noisy single-qubit Gym tasks and Rayleigh-fading channels, QPPG
converges 4 times faster than REINFORCE and sustains a 1 dB gain under
uncertainty. It reaches a 90 percent return in one hundred episodes with high
noise robustness, showcasing the advantages of full QFI-based preconditioning
for scalable quantum reinforcement learning.

</details>


### [613] [Compilation, Optimization, Error Mitigation, and Machine Learning in Quantum Algorithms](https://arxiv.org/abs/2506.15760)
*Shuangbao Paul Wang,Jianzhou Mao,Eric Sakk*

Main category: quant-ph

TL;DR: Discusses methods to enhance quantum algorithm execution, focusing on compilation, optimization, and error mitigation.


<details>
  <summary>Details</summary>
Motivation: Real-world implementation of quantum algorithms requires addressing challenges like suboptimal performance and errors during execution.

Method: Introduced approximate quantum Fourier transform (AQFT) for improving its execution efficiency.

Result: AQFT optimizes circuits and preserves quantum-induced exponential speedup from the quantum Fourier transform.

Conclusion: Hybrid platforms leveraging AQFT and high-performance classical computing power offer improved execution of quantum algorithms with better accuracy and speed.

Abstract: This paper discusses the compilation, optimization, and error mitigation of
quantum algorithms, essential steps to execute real-world quantum algorithms.
Quantum algorithms running on a hybrid platform with QPU and CPU/GPU take
advantage of existing high-performance computing power with quantum-enabled
exponential speedups. The proposed approximate quantum Fourier transform (AQFT)
for quantum algorithm optimization improves the circuit execution on top of an
exponential speed-ups the quantum Fourier transform has provided.

</details>


### [614] [Superconducting Qubit Readout Using Next-Generation Reservoir Computing](https://arxiv.org/abs/2506.15771)
*Robert Kent,Benjamin Lienhard,Gregory Lafyatis,Daniel J. Gauthier*

Main category: quant-ph

TL;DR: The paper proposes a reservoir computing approach for scalable, efficient, and accurate qubit-state discrimination for quantum processors, achieving significant error and crosstalk reduction compared to traditional methods.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenge of simultaneous qubit measurements in superconducting qubits, as traditional methods struggle with crosstalk and neural network solutions lack scalability due to high computational demands.

Method: The researchers use next-generation reservoir computing, leveraging polynomial feature construction without nonlinear activation functions, to map measurement signals to qubit states in a parallelizable and scalable manner.

Result: The approach achieved error reductions of up to 50% and 11% on single- and five-qubit datasets, along with up to 2.5x crosstalk reduction and significantly lower computational requirements than recent machine-learning methods.

Conclusion: Reservoir computing is presented as an effective, scalable alternative for high-fidelity qubit-state discrimination in quantum processors with lower computational costs compared to traditional and machine-learning-based approaches.

Abstract: Quantum processors require rapid and high-fidelity simultaneous measurements
of many qubits. While superconducting qubits are among the leading modalities
toward a useful quantum processor, their readout remains a bottleneck.
Traditional approaches to processing measurement data often struggle to account
for crosstalk present in frequency-multiplexed readout, the preferred method to
reduce the resource overhead. Recent approaches to address this challenge use
neural networks to improve the state-discrimination fidelity. However, they are
computationally expensive to train and evaluate, resulting in increased latency
and poor scalability as the number of qubits increases. We present an
alternative machine learning approach based on next-generation reservoir
computing that constructs polynomial features from the measurement signals and
maps them to the corresponding qubit states. This method is highly
parallelizable, avoids the costly nonlinear activation functions common in
neural networks, and supports real-time training, enabling fast evaluation,
adaptability, and scalability. Despite its lower computational complexity, our
reservoir approach is able to maintain high qubit-state-discrimination
fidelity. Relative to traditional methods, our approach achieves error
reductions of up to 50% and 11% on single- and five-qubit datasets,
respectively, and delivers up to 2.5x crosstalk reduction on the five-qubit
dataset. Compared with recent machine-learning methods, evaluating our model
requires 100x fewer multiplications for single-qubit and 2.5x fewer for
five-qubit models. This work demonstrates that reservoir computing can enhance
qubit-state discrimination while maintaining scalability for future quantum
processors.

</details>


### [615] [Feedback-driven recurrent quantum neural network universality](https://arxiv.org/abs/2506.16332)
*Lukas Gonon,Rodrigo Martínez-Peña,Juan-Pablo Ortega*

Main category: quant-ph

TL;DR: The paper introduces a recurrent quantum neural network for quantum reservoir computing, offering theoretical guarantees like universality and real-time usability.


<details>
  <summary>Details</summary>
Motivation: To advance quantum reservoir computing by addressing the limitations of earlier techniques and establishing a theoretical foundation for feedback-based approaches.

Method: A recurrent quantum neural network architecture is proposed, extending feedforward models into a feedback-driven reservoir computing framework. The paper also provides theoretical guarantees such as universality and approximation bounds.

Result: The model demonstrates universality with linear readouts and offers theoretical legitimacy for feedback-based quantum reservoir computing in real-time applications.

Conclusion: The research advances the field by providing a practical and theoretically-supported framework for quantum reservoir computing, facilitating its experimental and real-time processing applications.

Abstract: Quantum reservoir computing uses the dynamics of quantum systems to process
temporal data, making it particularly well-suited for learning with noisy
intermediate-scale quantum devices. Early experimental proposals, such as the
restarting and rewinding protocols, relied on repeating previous steps of the
quantum map to avoid backaction. However, this approach compromises real-time
processing and increases computational overhead. Recent developments have
introduced alternative protocols that address these limitations. These include
online, mid-circuit measurement, and feedback techniques, which enable
real-time computation while preserving the input history. Among these, the
feedback protocol stands out for its ability to process temporal information
with comparatively fewer components. Despite this potential advantage, the
theoretical foundations of feedback-based quantum reservoir computing remain
underdeveloped, particularly with regard to the universality and the
approximation capabilities of this approach. This paper addresses this issue by
presenting a recurrent quantum neural network architecture that extends a class
of existing feedforward models to a dynamic, feedback-driven reservoir setting.
We provide theoretical guarantees for variational recurrent quantum neural
networks, including approximation bounds and universality results. Notably, our
analysis demonstrates that the model is universal with linear readouts, making
it both powerful and experimentally accessible. These results pave the way for
practical and theoretically grounded quantum reservoir computing with real-time
processing capabilities.

</details>


### [616] [Enhancing Expressivity of Quantum Neural Networks Based on the SWAP test](https://arxiv.org/abs/2506.16938)
*Sebastian Nagies,Emiliano Tolotti,Davide Pastorello,Enrico Blanzieri*

Main category: quant-ph

TL;DR: The paper analyzes quantum neural networks (QNNs) built with SWAP test circuits, identifying their equivalence to classical quadratic feedforward networks and introducing a novel modification to overcome expressivity limitations.


<details>
  <summary>Details</summary>
Motivation: To leverage quantum neural networks for machine learning tasks by exploring connections to classical neural networks, while addressing limitations in QNN expressivity and performance on challenging problems.

Method: The paper employs a mathematical analysis of SWAP test-based QNN circuits, compares their capability with classical neural networks, tests their performance on various datasets, and proposes a circuit modification using generalized SWAP tests to enhance expressivity.

Result: The original SWAP test-based architecture is mathematically equivalent to classical quadratic networks, but it fails on hard problems like the parity check function due to limited expressivity. The introduced modification resolves this limitation and enables learning of these problems in arbitrary dimensions.

Conclusion: The paper enhances the understanding and design of QNNs by bridging classical and quantum architectures, demonstrating that modifying QNNs with generalized SWAP tests significantly enhances their expressivity and task-solving capabilities.

Abstract: Parameterized quantum circuits represent promising architectures for machine
learning applications, yet many lack clear connections to classical models,
potentially limiting their ability to translate the wide success of classical
neural networks to the quantum realm. We examine a specific type of quantum
neural network (QNN) built exclusively from SWAP test circuits, and discuss its
mathematical equivalence to a classical two-layer feedforward network with
quadratic activation functions under amplitude encoding. Our analysis across
classical real-world and synthetic datasets reveals that while this
architecture can successfully learn many practical tasks, it exhibits
fundamental expressivity limitations due to violating the universal
approximation theorem, particularly failing on harder problems like the parity
check function. To address this limitation, we introduce a circuit modification
using generalized SWAP test circuits that effectively implements classical
neural networks with product layers. This enhancement enables successful
learning of parity check functions in arbitrary dimensions which we
analytically argue to be impossible for the original architecture beyond two
dimensions regardless of network size. Our results establish a framework for
enhancing QNN expressivity through classical task analysis and demonstrate that
our SWAP test-based architecture offers broad representational capacity,
suggesting potential promise also for quantum learning tasks.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [617] [Fair Contracts in Principal-Agent Games with Heterogeneous Types](https://arxiv.org/abs/2506.15887)
*Jakub Tłuczek,Victor Villin,Christos Dimitrakakis*

Main category: cs.GT

TL;DR: The paper develops a framework for fairness in multi-agent systems through adaptive contracts, ensuring equitable outcomes without sacrificing efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of achieving fairness in multi-agent systems where agents differ in latent traits, which often lead to inequality.

Method: Proposes a framework based on repeated principal-agent games where a fairness-driven principal learns to implement adaptive, homogeneous linear contracts.

Result: Demonstrated that fairness-aware contracts can equalize outcomes among agents in social dilemmas while maintaining system efficiency.

Conclusion: It is possible to achieve both equity and stability without compromising on overall performance in multi-agent systems.

Abstract: Fairness is desirable yet challenging to achieve within multi-agent systems,
especially when agents differ in latent traits that affect their abilities.
This hidden heterogeneity often leads to unequal distributions of wealth, even
when agents operate under the same rules. Motivated by real-world examples, we
propose a framework based on repeated principal-agent games, where a principal,
who also can be seen as a player of the game, learns to offer adaptive
contracts to agents. By leveraging a simple yet powerful contract structure, we
show that a fairness-aware principal can learn homogeneous linear contracts
that equalize outcomes across agents in a sequential social dilemma.
Importantly, this fairness does not come at the cost of efficiency: our results
demonstrate that it is possible to promote equity and stability in the system
while preserving overall performance.

</details>


### [618] [Solving Zero-Sum Convex Markov Games](https://arxiv.org/abs/2506.16120)
*Fivos Kalogiannis,Emmanouil-Vasileios Vlatakis-Gkaragkounis,Ian Gemp,Georgios Piliouras*

Main category: cs.GT

TL;DR: The paper provides the first guarantees of global convergence to Nash equilibria in two-player zero-sum convex Markov games using independent policy gradient methods.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of achieving Nash equilibria in strategic multi-agent frameworks like convex Markov games, which suffer from inherent nonconvexity, lack of Bellman consistency, and infinite-horizon complexity.

Method: The authors use a two-step approach: (1) employing nonconvex regularization to transform the problem into a nonconvex-proximal Polyak-Lojasiewicz (NC-pPL) setting, and (2) establishing convergence for stochastic nested and alternating gradient descent-ascent methods.

Result: The paper demonstrates that independent policy gradient methods can be stabilized and made to converge to Nash equilibria in convex Markov games under the NC-pPL framework.

Conclusion: This work provides foundational guarantees and techniques for global convergence in strategic multi-agent settings, which could have broader implications for optimization methods in nonconvex settings.

Abstract: We contribute the first provable guarantees of global convergence to Nash
equilibria (NE) in two-player zero-sum convex Markov games (cMGs) by using
independent policy gradient methods. Convex Markov games, recently defined by
Gemp et al. (2024), extend Markov decision processes to multi-agent settings
with preferences that are convex over occupancy measures, offering a broad
framework for modeling generic strategic interactions. However, even the
fundamental min-max case of cMGs presents significant challenges, including
inherent nonconvexity, the absence of Bellman consistency, and the complexity
of the infinite horizon.
  We follow a two-step approach. First, leveraging properties of
hidden-convex--hidden-concave functions, we show that a simple nonconvex
regularization transforms the min-max optimization problem into a
nonconvex-proximal Polyak-Lojasiewicz (NC-pPL) objective. Crucially, this
regularization can stabilize the iterates of independent policy gradient
methods and ultimately lead them to converge to equilibria. Second, building on
this reduction, we address the general constrained min-max problems under
NC-pPL and two-sided pPL conditions, providing the first global convergence
guarantees for stochastic nested and alternating gradient descent-ascent
methods, which we believe may be of independent interest.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [619] [RapFlow-TTS: Rapid and High-Fidelity Text-to-Speech with Improved Consistency Flow Matching](https://arxiv.org/abs/2506.16741)
*Hyun Joon Park,Jeongmin Liu,Jin Sob Kim,Jeong Yeol Yang,Sung Won Han,Eunwoo Song*

Main category: eess.AS

TL;DR: RapFlow-TTS introduces a high-fidelity Text-to-Speech model leveraging velocity consistency in flow matching (FM) to reduce synthesis steps while maintaining quality.


<details>
  <summary>Details</summary>
Motivation: ODE-based TTS suffers from a trade-off between speech quality and inference speed due to the high number of generation steps needed. This paper seeks to overcome that challenge.

Method: The model applies velocity consistency constraints in flow matching alongside techniques like time interval scheduling and adversarial learning, enabling fewer generation steps while maintaining synthetic quality.

Result: RapFlow-TTS achieves high-quality speech synthesis while reducing synthesis steps by 5- and 10-fold compared to FM- and score-based approaches.

Conclusion: RapFlow-TTS successfully balances inference speed and speech quality, making it a promising advancement for efficient TTS systems.

Abstract: We introduce RapFlow-TTS, a rapid and high-fidelity TTS acoustic model that
leverages velocity consistency constraints in flow matching (FM) training.
Although ordinary differential equation (ODE)-based TTS generation achieves
natural-quality speech, it typically requires a large number of generation
steps, resulting in a trade-off between quality and inference speed. To address
this challenge, RapFlow-TTS enforces consistency in the velocity field along
the FM-straightened ODE trajectory, enabling consistent synthetic quality with
fewer generation steps. Additionally, we introduce techniques such as time
interval scheduling and adversarial learning to further enhance the quality of
the few-step synthesis. Experimental results show that RapFlow-TTS achieves
high-fidelity speech synthesis with a 5- and 10-fold reduction in synthesis
steps than the conventional FM- and score-based approaches, respectively.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [620] [Multi-Armed Bandits With Machine Learning-Generated Surrogate Rewards](https://arxiv.org/abs/2506.16658)
*Wenlong Ji,Yihan Pan,Ruihao Zhu,Lihua Lei*

Main category: math.ST

TL;DR: The paper introduces the MLA-UCB algorithm for multi-armed bandit problems, which leverages pre-trained machine learning models to utilize historical auxiliary data, even when surrogate rewards are biased.


<details>
  <summary>Details</summary>
Motivation: Traditional MAB algorithms rely only on scarce online data, while rich auxiliary data is often available in many real-world scenarios. Leveraging this auxiliary data through machine learning opens new possibilities but introduces challenges due to potential biases in surrogate rewards.

Method: The authors propose the Machine Learning-Assisted Upper Confidence Bound (MLA-UCB) algorithm, which integrates surrogate rewards predicted by machine learning models into the MAB framework. It handles biases in the surrogate rewards and does not require prior knowledge of the covariance relationship between true and surrogate rewards.

Result: The proposed MLA-UCB algorithm provably reduces cumulative regret under the assumption that predicted and true rewards are jointly Gaussian with non-zero correlation. Numerical studies demonstrate significant efficiency gains over the standard UCB algorithm, even with moderate-sized offline data and correlations.

Conclusion: MLA-UCB effectively addresses the challenge of leveraging biased auxiliary data in MAB problems, showcasing its practical utility and improvement over traditional UCB methods in scenarios rich with auxiliary data.

Abstract: Multi-armed bandit (MAB) is a widely adopted framework for sequential
decision-making under uncertainty. Traditional bandit algorithms rely solely on
online data, which tends to be scarce as it must be gathered during the online
phase when the arms are actively pulled. However, in many practical settings,
rich auxiliary data, such as covariates of past users, is available prior to
deploying any arms. We introduce a new setting for MAB where pre-trained
machine learning (ML) models are applied to convert side information and
historical data into \emph{surrogate rewards}. A prominent feature of this
setting is that the surrogate rewards may exhibit substantial bias, as true
reward data is typically unavailable in the offline phase, forcing ML
predictions to heavily rely on extrapolation. To address the issue, we propose
the Machine Learning-Assisted Upper Confidence Bound (MLA-UCB) algorithm, which
can be applied to any reward prediction model and any form of auxiliary data.
When the predicted and true rewards are jointly Gaussian, it provably improves
the cumulative regret, provided that the correlation is non-zero -- even in
cases where the mean surrogate reward completely misaligns with the true mean
rewards. Notably, our method requires no prior knowledge of the covariance
matrix between true and surrogate rewards. We compare MLA-UCB with the standard
UCB on a range of numerical studies and show a sizable efficiency gain even
when the size of the offline data and the correlation between predicted and
true rewards are moderate.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [621] [Fast solvers for Tokamak fluid models with PETSC -- Part I](https://arxiv.org/abs/2506.16676)
*Mark F. Adams,Jin Chen,Benjamin Sturdevant*

Main category: physics.plasm-ph

TL;DR: This report introduces one-dimensional multigrid solvers to improve Tokamak MHD models, achieving a 5x speed improvement for velocity solves in M3D-C1.


<details>
  <summary>Details</summary>
Motivation: To enhance Tokamak MHD model efficiency by reducing reliance on direct solvers, which scale poorly and are unsuitable for modern parallel hardware.

Method: Incorporated toroidal semi-coarsening multigrid into M3D-C1 using the PETSC numerical library to optimize velocity solves in a sample test problem.

Result: One-dimensional multigrid demonstrated a fivefold performance improvement over the existing one-level solver for an MHD disruption test case.

Conclusion: Integrating one-dimensional multigrid provides a scalable and hardware-compatible solution for MHD simulations, with significant future potential for Tokamak modeling.

Abstract: This report develops the first step in adding multigrid solvers to scientific
and engineering-relevant magnetohydrodynamics (MHD) models of Tokamaks. These
models are characterized by a distinguished direction in the toroidal
coordinate that is partially aligned with the magnetic guide field, which
dominates the plasma dynamics. All Tokamak models exploit this structure, for
example, NIMROD (https://nimrodteam.org/) uses $2D$, unstructured, high-order
finite elements in the poloidal plane with Fourier modes in the toroidal
coordinate, and the $3D$, extended MHD code M3D-C1
(https://w3.pppl.gov/~nferraro/m3dc1.html) uses $2D$, unstructured $C^1$
elements in the poloidal plane with cubic Hermite functions in the toroidal
direction. This structure suggests adding toroidal semi-coarsening multigrid to
the existing solver and thereby reducing reliance on direct solvers, which do
not scale optimally and are not well suited to modern hardware that demands
extreme levels of parallelism. This report focuses on the velocity solve in
M3D-C1, using the PETSC -- the Portable, Extensible Toolkit for Scientific
Computation -- numerical library (https://petsc.org), and shows that with
little new application code, one-dimensional multigrid is about $5x$ faster
than the existing one-level method on an MHD disruption, with runaway
electrons, test problem.

</details>
