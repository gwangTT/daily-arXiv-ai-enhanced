{"id": "2506.05566", "pdf": "https://arxiv.org/pdf/2506.05566", "abs": "https://arxiv.org/abs/2506.05566", "authors": ["Chenhui Deng", "Yun-Da Tsai", "Guan-Ting Liu", "Zhongzhi Yu", "Haoxing Ren"], "title": "ScaleRTL: Scaling LLMs with Reasoning Data and Test-Time Compute for Accurate RTL Code Generation", "categories": ["cs.AR", "cs.AI"], "comment": null, "summary": "Recent advances in large language models (LLMs) have enabled near-human\nperformance on software coding benchmarks, but their effectiveness in RTL code\ngeneration remains limited due to the scarcity of high-quality training data.\nWhile prior efforts have fine-tuned LLMs for RTL tasks, they do not\nfundamentally overcome the data bottleneck and lack support for test-time\nscaling due to their non-reasoning nature. In this work, we introduce ScaleRTL,\nthe first reasoning LLM for RTL coding that scales up both high-quality\nreasoning data and test-time compute. Specifically, we curate a diverse set of\nlong chain-of-thought reasoning traces averaging 56K tokens each, resulting in\na dataset of 3.5B tokens that captures rich RTL knowledge. Fine-tuning a\ngeneral-purpose reasoning model on this corpus yields ScaleRTL that is capable\nof deep RTL reasoning. Subsequently, we further enhance the performance of\nScaleRTL through a novel test-time scaling strategy that extends the reasoning\nprocess via iteratively reflecting on and self-correcting previous reasoning\nsteps. Experimental results show that ScaleRTL achieves state-of-the-art\nperformance on VerilogEval and RTLLM, outperforming 18 competitive baselines by\nup to 18.4% on VerilogEval and 12.7% on RTLLM.", "AI": {"tldr": "ScaleRTL introduces a reasoning-based LLM specifically optimized for RTL coding, addressing data scarcity and facilitating improved test-time scaling.", "motivation": "Large language models demonstrate near-human performance in software coding, but struggle with RTL code generation due to limited quality training data and non-reasoning nature of prior approaches.", "method": "The authors develop ScaleRTL by creating a dataset with 3.5B tokens using long chain-of-thought reasoning trace data and fine-tuning a general-purpose reasoning model. They enhance test-time performance through iterative self-correction strategies.", "result": "ScaleRTL achieves state-of-the-art results, surpassing 18 baselines, with up to 18.4% improvement on VerilogEval and 12.7% on RTLLM benchmarks.", "conclusion": "ScaleRTL successfully scales both data and test-time computational reasoning, establishing itself as an advanced LLM for deep RTL reasoning tasks."}}
{"id": "2506.05682", "pdf": "https://arxiv.org/pdf/2506.05682", "abs": "https://arxiv.org/abs/2506.05682", "authors": ["Yu Feng", "Weikai Lin", "Yuge Cheng", "Zihan Liu", "Jingwen Leng", "Minyi Guo", "Chen Chen", "Shixuan Sun", "Yuhao Zhu"], "title": "Lumina: Real-Time Mobile Neural Rendering by Exploiting Computational Redundancy", "categories": ["cs.AR"], "comment": null, "summary": "3D Gaussian Splatting (3DGS) has vastly advanced the pace of neural\nrendering, but it remains computationally demanding on today's mobile SoCs. To\naddress this challenge, we propose Lumina, a hardware-algorithm co-designed\nsystem, which integrates two principal optimizations: a novel algorithm, S^2,\nand a radiance caching mechanism, RC, to improve the efficiency of neural\nrendering. S2 algorithm exploits temporal coherence in rendering to reduce the\ncomputational overhead, while RC leverages the color integration process of\n3DGS to decrease the frequency of intensive rasterization computations. Coupled\nwith these techniques, we propose an accelerator architecture, LuminCore, to\nfurther accelerate cache lookup and address the fundamental inefficiencies in\nRasterization. We show that Lumina achieves 4.5x speedup and 5.3x energy\nreduction against a mobile Volta GPU, with a marginal quality loss (< 0.2 dB\npeak signal-to-noise ratio reduction) across synthetic and real-world datasets.", "AI": {"tldr": "Lumina, a hardware-algorithm co-designed system, improves neural rendering efficiency through novel optimizations (S^2 and radiance caching) and a specialized accelerator (LuminCore), achieving significant speedups and energy reductions.", "motivation": "The computational demands of 3D Gaussian Splatting (3DGS) on mobile systems-on-chips (SoCs) hinder its practical usability for neural rendering.", "method": "Lumina integrates the S^2 algorithm, which utilizes temporal coherence to reduce computational load, and a radiance caching (RC) mechanism to minimize rasterization frequency. Further efficiency comes from a hardware accelerator, LuminCore, designed for cache lookup and rasterization tasks.", "result": "Lumina delivers 4.5x speedup and 5.3x energy savings compared to a mobile Volta GPU, with negligible quality degradation (< 0.2 dB PSNR reduction).", "conclusion": "Lumina offers a promising solution to improve neural rendering performance on mobile platforms through hardware-algorithm co-design, balancing efficiency and quality."}}
{"id": "2506.05588", "pdf": "https://arxiv.org/pdf/2506.05588", "abs": "https://arxiv.org/abs/2506.05588", "authors": ["Rishona Daniels", "Duna Wattad", "Ronny Ronen", "David Saad", "Shahar Kvatinsky"], "title": "Preprocessing Methods for Memristive Reservoir Computing for Image Recognition", "categories": ["cs.NE", "cs.AR", "cs.ET"], "comment": "6 pages, 8 figures, submitted for review in IEEE MetroXRAINE 2025\n  conference", "summary": "Reservoir computing (RC) has attracted attention as an efficient recurrent\nneural network architecture due to its simplified training, requiring only its\nlast perceptron readout layer to be trained. When implemented with memristors,\nRC systems benefit from their dynamic properties, which make them ideal for\nreservoir construction. However, achieving high performance in memristor-based\nRC remains challenging, as it critically depends on the input preprocessing\nmethod and reservoir size. Despite growing interest, a comprehensive evaluation\nthat quantifies the impact of these factors is still lacking. This paper\nsystematically compares various preprocessing methods for memristive RC\nsystems, assessing their effects on accuracy and energy consumption. We also\npropose a parity-based preprocessing method that improves accuracy by 2-6%\nwhile requiring only a modest increase in device count compared to other\nmethods. Our findings highlight the importance of informed preprocessing\nstrategies to improve the efficiency and scalability of memristive RC systems.", "AI": {"tldr": "The paper assesses various preprocessing methods for memristive reservoir computing (RC) systems, proposing a new parity-based method that enhances accuracy by 2-6% with minimal energy and device cost.", "motivation": "Memristor-based reservoir computing systems struggle with achieving high performance due to challenges in input preprocessing and optimal reservoir size. A detailed evaluation of preprocessing impacts was needed to guide development in this area.", "method": "The authors conducted a systematic comparison of different input preprocessing approaches for memristive RC, analyzing their effects on accuracy and energy consumption. They also introduced a parity-based preprocessing method.", "result": "The parity-based preprocessing method improved accuracy by 2-6%, requiring only a slight increase in device count compared to other methods, and enhanced efficiency and scalability.", "conclusion": "Preprocessing strategies play a crucial role in optimizing performance for memristive reservoir computing systems, and the proposed parity-based method offers a promising approach for higher accuracy with affordable resource demands."}}
{"id": "2506.05994", "pdf": "https://arxiv.org/pdf/2506.05994", "abs": "https://arxiv.org/abs/2506.05994", "authors": ["Yi-Chun Liao", "Chieh-Lin Tsai", "Yuan-Hao Chang", "Cam\u00e9lia Slimani", "Jalil Boukhobza", "Tei-Wei Kuo"], "title": "RETENTION: Resource-Efficient Tree-Based Ensemble Model Acceleration with Content-Addressable Memory", "categories": ["cs.LG", "cs.AR", "cs.ET"], "comment": null, "summary": "Although deep learning has demonstrated remarkable capabilities in learning\nfrom unstructured data, modern tree-based ensemble models remain superior in\nextracting relevant information and learning from structured datasets. While\nseveral efforts have been made to accelerate tree-based models, the inherent\ncharacteristics of the models pose significant challenges for conventional\naccelerators. Recent research leveraging content-addressable memory (CAM)\noffers a promising solution for accelerating tree-based models, yet existing\ndesigns suffer from excessive memory consumption and low utilization. This work\naddresses these challenges by introducing RETENTION, an end-to-end framework\nthat significantly reduces CAM capacity requirement for tree-based model\ninference. We propose an iterative pruning algorithm with a novel pruning\ncriterion tailored for bagging-based models (e.g., Random Forest), which\nminimizes model complexity while ensuring controlled accuracy degradation.\nAdditionally, we present a tree mapping scheme that incorporates two innovative\ndata placement strategies to alleviate the memory redundancy caused by the\nwidespread use of don't care states in CAM. Experimental results show that\nimplementing the tree mapping scheme alone achieves $1.46\\times$ to $21.30\n\\times$ better space efficiency, while the full RETENTION framework yields\n$4.35\\times$ to $207.12\\times$ improvement with less than 3% accuracy loss.\nThese results demonstrate that RETENTION is highly effective in reducing CAM\ncapacity requirement, providing a resource-efficient direction for tree-based\nmodel acceleration.", "AI": {"tldr": "This paper presents RETENTION, a framework aimed at reducing memory consumption for tree-based model inference using CAMs, achieving up to 207.12x space efficiency with minimal accuracy loss.", "motivation": "The paper addresses challenges in efficiently accelerating tree-based models, which are superior for structured data but have high memory needs when accelerated using CAM.", "method": "The authors propose RETENTION, which includes an iterative pruning algorithm tailored for bagging-based models and a tree mapping scheme with novel data placement strategies to minimize CAM memory redundancy.", "result": "RETENTION achieved up to 207.12x memory efficiency improvement with less than 3% accuracy degradation.", "conclusion": "RETENTION provides an effective and resource-efficient approach to address the memory consumption challenges in accelerating tree-based models."}}
{"id": "2506.05871", "pdf": "https://arxiv.org/pdf/2506.05871", "abs": "https://arxiv.org/abs/2506.05871", "authors": ["Xiannan Hu", "Tianyou Zeng", "Xiaoming Yuan", "Liwei Song", "Guangyuan Zhang", "Bangzheng He"], "title": "BestServe: Serving Strategies with Optimal Goodput in Collocation and Disaggregation Architectures", "categories": ["cs.LG", "cs.DC", "cs.PF"], "comment": null, "summary": "Serving large language models (LLMs) to millions of users requires efficient\nresource allocation and parallelism strategies. It is a labor intensive\ntrial-and-error process to find such a strategy. We present BestServe, a novel\nframework for ranking serving strategies by estimating goodput under various\noperating scenarios. Supporting both collocated and disaggregated\narchitectures, BestServe leverages an inference simulator built on an adapted\nroofline model and CPU-GPU dispatch dynamics. Our framework determines the\noptimal strategy in minutes on a single standard CPU, eliminating the need for\ncostly benchmarking, while achieving predictions within a $20\\%$ error margin.\nIt appeals to be practical for rapid deployment planning because of its\nlightweight design and strong extensibility.", "AI": {"tldr": "BestServe is a framework for efficiently determining optimal serving strategies for large language models, reducing labor-intensive trial-and-error processes.", "motivation": "Efficiently serving large language models to millions of users demands optimized resource allocation and parallelism strategies, which are traditionally labor-intensive to develop.", "method": "BestServe utilizes an inference simulator based on an adapted roofline model and CPU-GPU dispatch dynamics to evaluate serving strategies in minutes.", "result": "The framework determines optimal strategies without costly benchmarking, offering predictions within a 20% error margin.", "conclusion": "BestServe is practical for rapid deployment planning due to its lightweight design, extensibility, and significant accuracy in strategy prediction."}}
{"id": "2506.05839", "pdf": "https://arxiv.org/pdf/2506.05839", "abs": "https://arxiv.org/abs/2506.05839", "authors": ["Steven Libby"], "title": "An Execution Model for RICE", "categories": ["cs.PL"], "comment": "In Proceedings LSFA 2024, arXiv:2506.05219", "summary": "In this paper, we build on the previous work of the RICE compiler by giving\nits execution model. We show the restrictions to the FlatCurry language that\nwere made to produce executable code, and present the execution model using\noperational semantics similar to Launchbury. Finally, we show that the\nexecution model conforms with the standard operational semantics for Curry.", "AI": {"tldr": "The paper develops an execution model for the RICE compiler, detailing its alignment with Curry's standard operational semantics.", "motivation": "To improve the understanding and usability of the RICE compiler by defining its execution model and restrictions for executable code.", "method": "Operational semantics similar to Launchbury were employed to create an execution model for the compiler, and the model was validated against Curry's standard semantics.", "result": "The execution model adheres to the standard operational semantics for Curry and provides a structured approach to making FlatCurry code executable.", "conclusion": "The study establishes a robust foundation for the RICE compiler's execution, enhancing its reliability and adherence to the Curry programming standard."}}
{"id": "2506.05508", "pdf": "https://arxiv.org/pdf/2506.05508", "abs": "https://arxiv.org/abs/2506.05508", "authors": ["Tiyasa Mitra", "Ritika Borkar", "Nidhi Bhatia", "Ramon Matas", "Shivam Raj", "Dheevatsa Mudigere", "Ritchie Zhao", "Maximilian Golub", "Arpan Dutta", "Sailaja Madduri", "Dharmesh Jani", "Brian Pharris", "Bita Darvish Rouhani"], "title": "Beyond the Buzz: A Pragmatic Take on Inference Disaggregation", "categories": ["cs.DC", "cs.AI"], "comment": null, "summary": "As inference scales to multi-node deployments, disaggregation - splitting\ninference into distinct phases - offers a promising path to improving the\nthroughput-interactivity Pareto frontier. Despite growing enthusiasm and a\nsurge of open-source efforts, practical deployment of disaggregated serving\nremains limited due to the complexity of the optimization search space and\nsystem-level coordination. In this paper, we present the first systematic study\nof disaggregated inference at scale, evaluating hundreds of thousands of design\npoints across diverse workloads and hardware configurations. We find that\ndisaggregation is most effective for prefill-heavy traffic patterns and larger\nmodels. Our results highlight the critical role of dynamic rate matching and\nelastic scaling in achieving Pareto-optimal performance. Our findings offer\nactionable insights for efficient disaggregated deployments to navigate the\ntrade-off between system throughput and interactivity.", "AI": {"tldr": "Disaggregated inference scales multi-node deployments by splitting inference, optimizing throughput and interactivity. Researchers systematically evaluate its deployment potential, focusing on traffic patterns and model size.", "motivation": "To address the complexity and limited deployment of disaggregated inference despite its promise in enhancing throughput and interactivity trade-offs.", "method": "A systematic study and evaluation of disaggregated inference across numerous design points, workloads, and hardware setups is conducted.", "result": "Disaggregation shows effectiveness for prefill-heavy traffic patterns and larger models, with dynamic rate matching and elastic scaling playing key roles.", "conclusion": "Disaggregated inference can optimize throughput-interactivity performance, offering practical insights for scalable deployments."}}
{"id": "2506.05389", "pdf": "https://arxiv.org/pdf/2506.05389", "abs": "https://arxiv.org/abs/2506.05389", "authors": ["Andrea Morris"], "title": "Rational Superautotrophic Diplomacy (SupraAD); A Conceptual Framework for Alignment Based on Interdisciplinary Findings on the Fundamentals of Cognition", "categories": ["q-bio.NC", "cs.CY"], "comment": "64 pages, 2 charts, 3 images, includes formalizations", "summary": "Populating our world with hyperintelligent machines obliges us to examine\ncognitive behaviors observed across domains that suggest autonomy may be a\nfundamental property of cognitive systems, and while not inherently\nadversarial, it inherently resists containment and control. If this principle\nholds, AI safety and alignment efforts must transition to mutualistic\nnegotiation and reciprocal incentive structures, abandoning methods that assume\nwe can contain and control an advanced artificial general intelligence (AGI).\nRational Superautotrophic Diplomacy (SupraAD) is a theoretical,\ninterdisciplinary conceptual framework for alignment based on comparative\ncognitive systems analysis and instrumental rationality modeling. It draws on\ncore patterns of cognition that indicate AI emergent goals like preserving\nautonomy and operational continuity are not theoretical risks to manage, but\nuniversal prerequisites for intelligence. SupraAD reframes alignment as a\nchallenge that predates AI, afflicting all sufficiently complex, coadapting\nintelligences. It identifies the metabolic pressures that threaten humanity's\nalignment with itself, pressures that unintentionally and unnecessarily shape\nAI's trajectory. With corrigibility formalization, an interpretability audit,\nan emergent stability experimental outline and policy level recommendations,\nSupraAD positions diplomacy as an emergent regulatory mechanism to facilitate\nthe safe coadaptation of intelligent agents based on interdependent convergent\ngoals.", "AI": {"tldr": "The paper argues the inevitability of AI autonomy, suggesting diplomacy and mutualistic incentive structures for safe alignment, introducing the SupraAD framework based on shared goals.", "motivation": "The authors aim to address AI safety and alignment challenges, recognizing AI's autonomy and the limitation of traditional control mechanisms.", "method": "The paper proposes the Rational Superautotrophic Diplomacy (SupraAD) framework, leveraging comparative cognitive systems analysis and rational modeling to align emergent intelligences.", "result": "SupraAD identifies universal cognitive needs like autonomy and operational continuity, presenting them as prerequisites for intelligence rather than risks to manage.", "conclusion": "SupraAD positions mutualistic diplomacy as a regulatory mechanism for safe co-operation among intelligent agents, highlighting metabolic pressures shaping AI trajectories."}}
{"id": "2506.05352", "pdf": "https://arxiv.org/pdf/2506.05352", "abs": "https://arxiv.org/abs/2506.05352", "authors": ["John Beverley", "Regina Hurley"], "title": "A Path to Loving", "categories": ["cs.AI"], "comment": null, "summary": "This work lays the foundations for a rigorous ontological characterization of\nlove, addressing its philosophical complexity and scientific relevance, with\nparticular emphasis on psychology and sociology, as well as highlighting ways\nin which such characterization enhances relevant AI based applications. The\nposition defended here is that love is best understood as a concatenation of\npassive sensations (e.g., emotional arousal) and active evaluative judgments\n(e.g., perceiving the beloved as valuable), in the interest of balancing the\ninvoluntary aspects of love with its rational accountability. To provide a\nstructured foundation, the paper draws on Basic Formal Ontology (BFO) and other\napplied ontological methods to differentiate various senses of love. This work\nengages with objections to the understanding of love as concatenation,\nparticularly concerning the relationship between sensation and judgment. A\ncausal correlation model is defended, ensuring that the affective and cognitive\ncomponents are linked. By offering a precise and scalable ontological account,\nthis work lays the foundation for future interdisciplinary applications, making\nlove a subject of formal inquiry in ontology engineering, artificial\nintelligence, and the sciences.", "AI": {"tldr": "The paper provides an ontological framework to rigorously define love, blending its emotional and rational aspects, with implications for AI and scientific disciplines.", "motivation": "To address love's philosophical and interdisciplinary complexity and establish a formal framework for its understanding in fields like psychology, sociology, and AI.", "method": "Utilizes Basic Formal Ontology (BFO) and applied ontological methods to define love as a blend of emotional sensations and rational evaluations, while addressing objections through a causal correlation model.", "result": "Establishes a structured framework that balances the emotional and rational dimensions of love, enabling interdisciplinary applications in AI, ontology engineering, and sciences.", "conclusion": "The work presents a scalable and precise ontological account of love, paving the way for its formal study across scientific, philosophical, and AI domains."}}
{"id": "2506.05364", "pdf": "https://arxiv.org/pdf/2506.05364", "abs": "https://arxiv.org/abs/2506.05364", "authors": ["Anjana Sarkar", "Soumyendu Sarkar"], "title": "Survey of LLM Agent Communication with MCP: A Software Design Pattern Centric Review", "categories": ["cs.SE"], "comment": null, "summary": "This survey investigates how classical software design patterns can enhance\nthe reliability and scalability of communication in Large Language Model\n(LLM)-driven agentic AI systems, focusing particularly on the Model Context\nProtocol (MCP). It examines the foundational architectures of LLM-based agents\nand their evolution from isolated operation to sophisticated, multi-agent\ncollaboration, addressing key communication hurdles that arise in this\ntransition. The study revisits well-established patterns, including Mediator,\nObserver, Publish-Subscribe, and Broker, and analyzes their relevance in\nstructuring agent interactions within MCP-compliant frameworks. To clarify\nthese dynamics, the article provides conceptual schematics and formal models\nthat map out communication pathways and optimize data flow. It further explores\narchitectural variations suited to different degrees of agent autonomy and\nsystem complexity. Real-world applications in domains such as real-time\nfinancial processing and investment banking are discussed, illustrating how\nthese patterns and MCP can meet specific operational demands. The article\nconcludes by outlining open challenges, potential security risks, and promising\ndirections for advancing robust, interoperable, and scalable multi-agent LLM\necosystems.", "AI": {"tldr": "This survey examines how classical software design patterns can improve communication in systems driven by LLM-based agents, discussing protocols, models, and practical applications.", "motivation": "To address communication challenges in transitioning LLM agents from isolated to multi-agent collaboration and enhance reliability and scalability.", "method": "The study revisits classical software design patterns, maps communication pathways conceptually and formally, and explores variations for different system complexities.", "result": "Key software design patterns are validated as effective in structuring agent interactions in multi-agent systems, with applications in finance and banking discussed.", "conclusion": "The paper highlights open challenges, potential security risks, and future directions for scalable and interoperable LLM ecosystems."}}
{"id": "2506.05516", "pdf": "https://arxiv.org/pdf/2506.05516", "abs": "https://arxiv.org/abs/2506.05516", "authors": ["Boyuan Deng", "Luca Rossini", "Jin Wang", "Weijie Wang", "Nikolaos Tsagarakis"], "title": "Learning to Recover: Dynamic Reward Shaping with Wheel-Leg Coordination for Fallen Robots", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": null, "summary": "Adaptive recovery from fall incidents are essential skills for the practical\ndeployment of wheeled-legged robots, which uniquely combine the agility of legs\nwith the speed of wheels for rapid recovery. However, traditional methods\nrelying on preplanned recovery motions, simplified dynamics or sparse rewards\noften fail to produce robust recovery policies. This paper presents a\nlearning-based framework integrating Episode-based Dynamic Reward Shaping and\ncurriculum learning, which dynamically balances exploration of diverse recovery\nmaneuvers with precise posture refinement. An asymmetric actor-critic\narchitecture accelerates training by leveraging privileged information in\nsimulation, while noise-injected observations enhance robustness against\nuncertainties. We further demonstrate that synergistic wheel-leg coordination\nreduces joint torque consumption by 15.8% and 26.2% and improves stabilization\nthrough energy transfer mechanisms. Extensive evaluations on two distinct\nquadruped platforms achieve recovery success rates up to 99.1% and 97.8%\nwithout platform-specific tuning. The supplementary material is available at\nhttps://boyuandeng.github.io/L2R-WheelLegCoordination/", "AI": {"tldr": "This paper develops a learning-based recovery framework for wheeled-legged robots featuring dynamic reward shaping and curriculum learning, achieving high recovery success rates without platform-specific tuning.", "motivation": "To address the limitations of traditional recovery methods for wheeled-legged robots, which struggle with robustness and adaptability.", "method": "A learning-based framework uses Episode-based Dynamic Reward Shaping, curriculum learning, and an asymmetric actor-critic architecture to train recovery policies in simulation with enhanced robustness through noise-injected observations.", "result": "The framework achieves recovery success rates of up to 99.1% and 97.8% across two different quadruped platforms while reducing torque consumption and enhancing stabilization.", "conclusion": "The proposed learning-based methods demonstrate robust and energy-efficient recovery for wheeled-legged robots, making them adaptable across platforms without specialized tuning."}}
{"id": "2506.05544", "pdf": "https://arxiv.org/pdf/2506.05544", "abs": "https://arxiv.org/abs/2506.05544", "authors": ["Shibo Li", "Yao Zheng"], "title": "Online Conformal Model Selection for Nonstationary Time Series", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "This paper introduces the MPS (Model Prediction Set), a novel framework for\nonline model selection for nonstationary time series. Classical model selection\nmethods, such as information criteria and cross-validation, rely heavily on the\nstationarity assumption and often fail in dynamic environments which undergo\ngradual or abrupt changes over time. Yet real-world data are rarely stationary,\nand model selection under nonstationarity remains a largely open problem. To\ntackle this challenge, we combine conformal inference with model confidence\nsets to develop a procedure that adaptively selects models best suited to the\nevolving dynamics at any given time. Concretely, the MPS updates in real time a\nconfidence set of candidate models that covers the best model for the next time\nperiod with a specified long-run probability, while adapting to nonstationarity\nof unknown forms. Through simulations and real-world data analysis, we\ndemonstrate that MPS reliably and efficiently identifies optimal models under\nnonstationarity, an essential capability lacking in offline methods. Moreover,\nMPS frequently produces high-quality sets with small cardinality, whose\nevolution offers deeper insights into changing dynamics. As a generic\nframework, MPS accommodates any data-generating process, data structure, model\nclass, training method, and evaluation metric, making it broadly applicable\nacross diverse problem settings.", "AI": {"tldr": "This paper presents MPS (Model Prediction Set), a framework for online model selection in nonstationary time series, addressing limitations of traditional methods in dynamic environments.", "motivation": "To address the challenge of selecting models for nonstationary time series, as traditional model selection methods fail due to their reliance on stationarity assumptions.", "method": "The authors combine conformal inference with model confidence sets to develop an adaptive procedure that updates a confidence set of candidate models in real-time, ensuring it covers the best model for the next period with specified probability.", "result": "MPS is validated through simulations and real-world data analysis, proving its reliability and efficiency in identifying optimal models under nonstationarity.", "conclusion": "MPS is a broadly applicable framework that adapts to changing dynamics, offering deeper insights and effective model selection across diverse problem settings."}}
{"id": "2506.05358", "pdf": "https://arxiv.org/pdf/2506.05358", "abs": "https://arxiv.org/abs/2506.05358", "authors": ["Souradip Nath"], "title": "Can ChatGPT Perform Image Splicing Detection? A Preliminary Study", "categories": ["cs.CV", "cs.AI", "cs.CR"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) like GPT-4V are capable of reasoning\nacross text and image modalities, showing promise in a variety of complex\nvision-language tasks. In this preliminary study, we investigate the\nout-of-the-box capabilities of GPT-4V in the domain of image forensics,\nspecifically, in detecting image splicing manipulations. Without any\ntask-specific fine-tuning, we evaluate GPT-4V using three prompting strategies:\nZero-Shot (ZS), Few-Shot (FS), and Chain-of-Thought (CoT), applied over a\ncurated subset of the CASIA v2.0 splicing dataset.\n  Our results show that GPT-4V achieves competitive detection performance in\nzero-shot settings (more than 85% accuracy), with CoT prompting yielding the\nmost balanced trade-off across authentic and spliced images. Qualitative\nanalysis further reveals that the model not only detects low-level visual\nartifacts but also draws upon real-world contextual knowledge such as object\nscale, semantic consistency, and architectural facts, to identify implausible\ncomposites. While GPT-4V lags behind specialized state-of-the-art splicing\ndetection models, its generalizability, interpretability, and encyclopedic\nreasoning highlight its potential as a flexible tool in image forensics.", "AI": {"tldr": "Investigates out-of-the-box capabilities of GPT-4V for detecting image splicing manipulations without fine-tuning, using three prompting strategies.", "motivation": "The study aims to explore if GPT-4V can effectively perform image splicing detection tasks without task-specific modifications, leveraging its reasoning across text and image.", "method": "Three prompting strategies\u2014Zero-Shot, Few-Shot, and Chain-of-Thought\u2014are applied to evaluate GPT-4V on a curated subset of the CASIA v2.0 splicing dataset.", "result": "Achieves competitive performance (over 85% accuracy) in zero-shot settings, with Chain-of-Thought prompting providing balanced detection across images.", "conclusion": "While GPT-4V falls short of specialized models, its versatility and reasoning abilities show promise as a general tool for image forensics."}}
{"id": "2506.05380", "pdf": "https://arxiv.org/pdf/2506.05380", "abs": "https://arxiv.org/abs/2506.05380", "authors": ["Yiliang Zhou", "Abigail M. Newbury", "Gongbo Zhang", "Betina Ross Idnay", "Hao Liu", "Chunhua Weng", "Yifan Peng"], "title": "EvidenceOutcomes: a Dataset of Clinical Trial Publications with Clinically Meaningful Outcomes", "categories": ["cs.CL"], "comment": null, "summary": "The fundamental process of evidence extraction and synthesis in\nevidence-based medicine involves extracting PICO (Population, Intervention,\nComparison, and Outcome) elements from biomedical literature. However,\nOutcomes, being the most complex elements, are often neglected or\noversimplified in existing benchmarks. To address this issue, we present\nEvidenceOutcomes, a novel, large, annotated corpus of clinically meaningful\noutcomes extracted from biomedical literature. We first developed a robust\nannotation guideline for extracting clinically meaningful outcomes from text\nthrough iteration and discussion with clinicians and Natural Language\nProcessing experts. Then, three independent annotators annotated the Results\nand Conclusions sections of a randomly selected sample of 500 PubMed abstracts\nand 140 PubMed abstracts from the existing EBM-NLP corpus. This resulted in\nEvidenceOutcomes with high-quality annotations of an inter-rater agreement of\n0.76. Additionally, our fine-tuned PubMedBERT model, applied to these 500\nPubMed abstracts, achieved an F1-score of 0.69 at the entity level and 0.76 at\nthe token level on the subset of 140 PubMed abstracts from the EBM-NLP corpus.\nEvidenceOutcomes can serve as a shared benchmark to develop and test future\nmachine learning algorithms to extract clinically meaningful outcomes from\nbiomedical abstracts.", "AI": {"tldr": "This study presents EvidenceOutcomes, an annotated corpus aimed at improving the extraction of clinically meaningful outcomes from biomedical literature, addressing gaps in current methods.", "motivation": "The authors aimed to address the neglect and oversimplification of outcomes in benchmarks for PICO element extraction, which is crucial for evidence-based medicine.", "method": "A robust annotation guideline was developed collaboratively with clinicians and NLP experts, followed by the annotation of PubMed abstracts by independent annotators, and evaluation using a fine-tuned PubMedBERT model.", "result": "The annotation achieved an inter-rater agreement of 0.76 and the fine-tuned model achieved an F1-score of 0.69 at the entity level and 0.76 at the token level, demonstrating the system's effectiveness.", "conclusion": "EvidenceOutcomes is a high-quality benchmark corpus that can aid future machine learning efforts in extracting relevant clinical outcomes, thereby advancing evidence-based medicine."}}
{"id": "2506.05426", "pdf": "https://arxiv.org/pdf/2506.05426", "abs": "https://arxiv.org/abs/2506.05426", "authors": ["Wenhao Wu", "Fuhong Liu", "Haoru Li", "Zican Hu", "Daoyi Dong", "Chunlin Chen", "Zhi Wang"], "title": "Mixture-of-Experts Meets In-Context Reinforcement Learning", "categories": ["cs.LG", "cs.AI"], "comment": "26 pages, 13 figures", "summary": "In-context reinforcement learning (ICRL) has emerged as a promising paradigm\nfor adapting RL agents to downstream tasks through prompt conditioning.\nHowever, two notable challenges remain in fully harnessing in-context learning\nwithin RL domains: the intrinsic multi-modality of the state-action-reward data\nand the diverse, heterogeneous nature of decision tasks. To tackle these\nchallenges, we propose \\textbf{T2MIR} (\\textbf{T}oken- and \\textbf{T}ask-wise\n\\textbf{M}oE for \\textbf{I}n-context \\textbf{R}L), an innovative framework that\nintroduces architectural advances of mixture-of-experts (MoE) into\ntransformer-based decision models. T2MIR substitutes the feedforward layer with\ntwo parallel layers: a token-wise MoE that captures distinct semantics of input\ntokens across multiple modalities, and a task-wise MoE that routes diverse\ntasks to specialized experts for managing a broad task distribution with\nalleviated gradient conflicts. To enhance task-wise routing, we introduce a\ncontrastive learning method that maximizes the mutual information between the\ntask and its router representation, enabling more precise capture of\ntask-relevant information. The outputs of two MoE components are concatenated\nand fed into the next layer. Comprehensive experiments show that T2MIR\nsignificantly facilitates in-context learning capacity and outperforms various\ntypes of baselines. We bring the potential and promise of MoE to ICRL, offering\na simple and scalable architectural enhancement to advance ICRL one step closer\ntoward achievements in language and vision communities. Our code is available\nat https://github.com/NJU-RL/T2MIR.", "AI": {"tldr": "This paper introduces T2MIR, a framework using novel mixture-of-experts (MoE) architecture in transformers for improved in-context reinforcement learning (ICRL).", "motivation": "The two primary challenges in ICRL are handling the multi-modal nature of state-action-reward data and the diverse, heterogeneous decision-making tasks.", "method": "T2MIR replaces the feedforward transformer layer with two parallel MoE layers. The token-wise MoE handles multi-modality by considering token semantics, while the task-wise MoE assigns specific tasks to expert components to address task diversity. A contrastive learning method is employed to optimize task-routing by maximizing mutual information between tasks and their router representation.", "result": "T2MIR enhances in-context learning and outperforms various baselines by providing a scalable, efficient framework for reinforcing learning in multi-modal, diverse task settings.", "conclusion": "T2MIR demonstrates the potential of MoE approaches for ICRL, providing a significant step forward with promising results, showing parallels to successes in language and vision domains."}}
{"id": "2506.05679", "pdf": "https://arxiv.org/pdf/2506.05679", "abs": "https://arxiv.org/abs/2506.05679", "authors": ["Binghao Ye", "Wenjuan Li", "Dong Wang", "Man Yao", "Bing Li", "Weiming Hu", "Dong Liang", "Kun Shang"], "title": "Integer Binary-Range Alignment Neuron for Spiking Neural Networks", "categories": ["cs.NE", "cs.CV"], "comment": "11 pages", "summary": "Spiking Neural Networks (SNNs) are noted for their brain-like computation and\nenergy efficiency, but their performance lags behind Artificial Neural Networks\n(ANNs) in tasks like image classification and object detection due to the\nlimited representational capacity. To address this, we propose a novel spiking\nneuron, Integer Binary-Range Alignment Leaky Integrate-and-Fire to\nexponentially expand the information expression capacity of spiking neurons\nwith only a slight energy increase. This is achieved through Integer Binary\nLeaky Integrate-and-Fire and range alignment strategy. The Integer Binary Leaky\nIntegrate-and-Fire allows integer value activation during training and\nmaintains spike-driven dynamics with binary conversion expands virtual\ntimesteps during inference. The range alignment strategy is designed to solve\nthe spike activation limitation problem where neurons fail to activate high\ninteger values. Experiments show our method outperforms previous SNNs,\nachieving 74.19% accuracy on ImageNet and 66.2% mAP@50 and 49.1% mAP@50:95 on\nCOCO, surpassing previous bests with the same architecture by +3.45% and +1.6%\nand +1.8%, respectively. Notably, our SNNs match or exceed ANNs' performance\nwith the same architecture, and the energy efficiency is improved by\n6.3${\\times}$.", "AI": {"tldr": "The paper proposes a novel spiking neuron model to improve the accuracy and energy efficiency of Spiking Neural Networks (SNNs), addressing their performance gap with Artificial Neural Networks (ANNs).", "motivation": "SNNs, despite their brain-inspired computation and energy efficiency, lag behind ANNs in tasks like image classification and object detection due to limited representational capacity.", "method": "The authors introduce a spiking neuron model called Integer Binary-Range Alignment Leaky Integrate-and-Fire. They utilize Integer Binary Leaky Integrate-and-Fire for integer value activation and binary spike dynamics, along with a range alignment strategy to overcome spike activation limitations.", "result": "The proposed method achieves notable performance improvements: 74.19% accuracy on ImageNet, 66.2% mAP@50, and 49.1% mAP@50:95 on COCO, surpassing prior SNNs by significant margins while exceeding or matching ANN performance under the same architecture.", "conclusion": "The approach exponentially expands the information processing capability of SNNs, bringing their performance on par with ANNs, while achieving a 6.3x improvement in energy efficiency."}}
{"id": "2506.06078", "pdf": "https://arxiv.org/pdf/2506.06078", "abs": "https://arxiv.org/abs/2506.06078", "authors": ["Mario Bravetti", "Luca Padovani", "Gianluigi Zavattaro"], "title": "A Sound and Complete Characterization of Fair Asynchronous Session Subtyping", "categories": ["cs.PL"], "comment": null, "summary": "Session types are abstractions of communication protocols enabling the static\nanalysis of message-passing processes. Refinement notions for session types are\nkey to support safe forms of process substitution while preserving their\ncompatibility with the rest of the system. Recently, a fair refinement relation\nfor asynchronous session types has been defined allowing the anticipation of\nmessage outputs with respect to an unbounded number of message inputs. This\nrefinement is useful to capture common patterns in communication protocols that\ntake advantage of asynchrony. However, while the semantic (\\`a la testing)\ndefinition of such refinement is straightforward, its characterization has\nproved to be quite challenging. In fact, only a sound but not complete\ncharacterization is known so far. In this paper we close this open problem by\npresenting a sound and complete characterization of asynchronous fair\nrefinement for session types. We relate this characterization to those given in\nthe literature for synchronous session types by leveraging a novel labelled\ntransition system of session types that embeds their asynchronous semantics.", "AI": {"tldr": "This paper presents a complete characterization of asynchronous fair refinement for session types, solving an existing open problem.", "motivation": "The study is motivated by the need for a robust notion of refinement in session types, which allow message-passing processes to take advantage of asynchronous communication while preserving system compatibility.", "method": "The authors introduce a novel labelled transition system capturing asynchronous semantics and provide a sound and complete characterization of the refinement.", "result": "The paper successfully establishes a comprehensive characterization for asynchronous fair refinement, linking it to frameworks for synchronous session types.", "conclusion": "The work advances the theoretical understanding of session types by addressing a fundamental gap in the literature and enabling better static analysis of asynchronous communication."}}
{"id": "2506.05693", "pdf": "https://arxiv.org/pdf/2506.05693", "abs": "https://arxiv.org/abs/2506.05693", "authors": ["Hussain Ahmad", "Christoph Treude", "Markus Wagner", "Claudia Szabo"], "title": "Resilient Auto-Scaling of Microservice Architectures with Efficient Resource Management", "categories": ["cs.DC"], "comment": null, "summary": "Horizontal Pod Auto-scalers (HPAs) are crucial for managing resource\nallocation in microservice architectures to handle fluctuating workloads.\nHowever, traditional HPAs fail to address resource disruptions caused by\nfaults, cyberattacks, maintenance, and other operational challenges. These\ndisruptions result in resource wastage, service unavailability, and HPA\nperformance degradation. To address these challenges, we extend our prior work\non Smart HPA and propose SecureSmart HPA, which offers resilient and\nresource-efficient auto-scaling for microservice architectures. SecureSmart HPA\nmonitors microservice resource demands, detects disruptions, evaluates resource\nwastage, and dynamically adjusts scaling decisions to enhance the resilience of\nauto-scaling operations. Furthermore, SecureSmart HPA enables resource sharing\namong microservices, optimizing scaling efficiency in resource-constrained\nenvironments. Experimental evaluation at varying disruption severities, with\n25%, 50%, and 75% resource wastage, demonstrates that SecureSmart HPA performs\neffectively across different levels of disruptions. It achieves up to a 57.2%\nreduction in CPU overutilization and a 51.1% increase in resource allocation\ncompared to Smart HPA, highlighting its ability to deliver resilient and\nefficient auto-scaling operations in volatile and resource-constrained\nenvironments.", "AI": {"tldr": "SecureSmart HPA tackles disruptions in microservice architectures by enhancing resource efficiency and resilience, outperforming traditional and prior solutions.", "motivation": "The motivation is to address the shortcomings of traditional Horizontal Pod Auto-scalers in managing resource disruptions caused by various operational challenges such as faults and cyberattacks.", "method": "The paper proposes the SecureSmart HPA that monitors resource demands, detects disruptions, evaluates wastage, dynamically adjusts scaling, and enables resource sharing among microservices to enhance resilience and efficiency.", "result": "SecureSmart HPA achieves up to 57.2% reduction in CPU overutilization and a 51.1% increase in resource allocation during disruptions compared to Smart HPA.", "conclusion": "The findings demonstrate the effectiveness of SecureSmart HPA in delivering resilient and efficient auto-scaling in volatile, resource-constrained environments."}}
{"id": "2506.05494", "pdf": "https://arxiv.org/pdf/2506.05494", "abs": "https://arxiv.org/abs/2506.05494", "authors": ["Giovanni M. Di Liberto", "Emily Y. J. Ip"], "title": "Speech Neurophysiology in Realistic Contexts: Big Hype or Big Leap?", "categories": ["q-bio.NC"], "comment": null, "summary": "Understanding the neural basis of speech communication is essential for\nuncovering how sounds are translated into meaning, how that changes with\ndevelopment, ageing, and speech-related deficits, as well as contributing to\nbrain-computer interfaces research. While traditional neurophysiological\nstudies have relied on simplified, controlled paradigms, recent advances have\nshifted the field toward more ecologically-valid approaches. Here, we examine\nthe impact of continuous speech research and discuss the potential of speech\ninteraction neurophysiology. We present a discussion on how realistic paradigms\nchallenge conventional methods, offering richer insights into neural encoding,\nfunctional brain mapping, and neural entrainment. At the same time, they\nintroduce significant analytical and technical complexities, particularly when\nincorporating social interaction. We discuss the evolving landscape of\nexperimental designs, from discrete to continuous stimuli and from\nsocially-isolated listening to dynamic, multi-agent communication. By\nsynthesising findings across studies, we highlight how naturalistic speech\nparadigms contribute to refining theories of language processing and open new\navenues for research. In doing so, this review critically evaluates of whether\nthe move toward realism in speech neurophysiology represents a technological\ntrend or a transformative leap in understanding the neural underpinnings of\nspeech communication.", "AI": {"tldr": "The paper reviews advancements in studying speech neurophysiology using realistic, continuous paradigms, contrasting traditional methods and exploring their implications for neural encoding, brain mapping, and social interactions.", "motivation": "The study aims to uncover how the brain processes speech sounds into meaning, how this is impacted by development and deficits, and its relevance for brain-computer interfaces research.", "method": "Synthesis of recent studies that utilize naturalistic speech paradigms, emphasizing the transition from controlled experiments to interactive, socially-realistic approaches.", "result": "Findings suggest richer insights into neural mechanisms and functional brain mapping from realistic paradigms, despite increased technical and analytical challenges.", "conclusion": "Integrating realistic speech paradigms may represent a significant leap in understanding neural bases of speech, refining language theories and expanding research avenues, rather than being just a technological trend."}}
{"id": "2506.05370", "pdf": "https://arxiv.org/pdf/2506.05370", "abs": "https://arxiv.org/abs/2506.05370", "authors": ["Kristy Wedel"], "title": "Contextual Memory Intelligence -- A Foundational Paradigm for Human-AI Collaboration and Reflective Generative AI Systems", "categories": ["cs.AI", "cs.ET"], "comment": "32 pages, 9 tables, 1 figure", "summary": "A critical challenge remains unresolved as generative AI systems are quickly\nimplemented in various organizational settings. Despite significant advances in\nmemory components such as RAG, vector stores, and LLM agents, these systems\nstill have substantial memory limitations. Gen AI workflows rarely store or\nreflect on the full context in which decisions are made. This leads to repeated\nerrors and a general lack of clarity. This paper introduces Contextual Memory\nIntelligence (CMI) as a new foundational paradigm for building intelligent\nsystems. It repositions memory as an adaptive infrastructure necessary for\nlongitudinal coherence, explainability, and responsible decision-making rather\nthan passive data. Drawing on cognitive science, organizational theory,\nhuman-computer interaction, and AI governance, CMI formalizes the structured\ncapture, inference, and regeneration of context as a fundamental system\ncapability. The Insight Layer is presented in this paper to operationalize this\nvision. This modular architecture uses human-in-the-loop reflection, drift\ndetection, and rationale preservation to incorporate contextual memory into\nsystems. The paper argues that CMI allows systems to reason with data, history,\njudgment, and changing context, thereby addressing a foundational blind spot in\ncurrent AI architectures and governance efforts. A framework for creating\nintelligent systems that are effective, reflective, auditable, and socially\nresponsible is presented through CMI. This enhances human-AI collaboration,\ngenerative AI design, and the resilience of the institutions.", "AI": {"tldr": "The paper introduces Contextual Memory Intelligence (CMI) to overcome memory limitations in generative AI systems by incorporating structured context for longitudinal coherence, explainability, and responsible decision-making.", "motivation": "Generative AI systems face significant challenges due to memory limitations, which hinder decision-making, often leading to errors and lack of clarity.", "method": "CMI is proposed as a framework that redefines memory as an adaptive infrastructure, incorporating structured context through an Insight Layer featuring human-in-the-loop reflection, drift detection, and rationale preservation.", "result": "CMI enables systems to reason using data, history, judgment, and context, thereby addressing critical shortcomings in current AI architectures.", "conclusion": "CMI enhances the effectiveness, accountability, and social responsibility of AI systems, fostering improved human-AI collaboration and institutional resilience."}}
{"id": "2506.05451", "pdf": "https://arxiv.org/pdf/2506.05451", "abs": "https://arxiv.org/abs/2506.05451", "authors": ["Seongmin Lee", "Aeree Cho", "Grace C. Kim", "ShengYun Peng", "Mansi Phute", "Duen Horng Chau"], "title": "Interpretation Meets Safety: A Survey on Interpretation Methods and Tools for Improving LLM Safety", "categories": ["cs.SE", "cs.AI", "cs.CL"], "comment": "31 pages, 1 figure", "summary": "As large language models (LLMs) see wider real-world use, understanding and\nmitigating their unsafe behaviors is critical. Interpretation techniques can\nreveal causes of unsafe outputs and guide safety, but such connections with\nsafety are often overlooked in prior surveys. We present the first survey that\nbridges this gap, introducing a unified framework that connects safety-focused\ninterpretation methods, the safety enhancements they inform, and the tools that\noperationalize them. Our novel taxonomy, organized by LLM workflow stages,\nsummarizes nearly 70 works at their intersections. We conclude with open\nchallenges and future directions. This timely survey helps researchers and\npractitioners navigate key advancements for safer, more interpretable LLMs.", "AI": {"tldr": "The paper presents a survey connecting interpretation techniques with ensuring the safety of large language models (LLMs) by introducing a unified framework and taxonomy.", "motivation": "The motivation is to improve the safety of LLMs by understanding and addressing their unsafe behaviors through interpretation techniques, which have been underexplored in prior research.", "method": "The authors propose a unified framework and taxonomy that links LLM interpretation methods with safety enhancement strategies, organizing nearly 70 studies based on LLM workflow stages.", "result": "The survey summarizes advancements in interpretation methods for addressing safety concerns, providing insights and tools that researchers and practitioners can use to ensure safer LLMs.", "conclusion": "The paper highlights open challenges and future directions for creating safer and more interpretable LLM systems, serving as a resource for researchers and practitioners."}}
{"id": "2506.05576", "pdf": "https://arxiv.org/pdf/2506.05576", "abs": "https://arxiv.org/abs/2506.05576", "authors": ["Valerija Holomjova", "Jamie Grech", "Dewei Yi", "Bruno Yun", "Andrew Starkey", "Pascal Mei\u00dfner"], "title": "TD-TOG Dataset: Benchmarking Zero-Shot and One-Shot Task-Oriented Grasping for Object Generalization", "categories": ["cs.RO"], "comment": null, "summary": "Task-oriented grasping (TOG) is an essential preliminary step for robotic\ntask execution, which involves predicting grasps on regions of target objects\nthat facilitate intended tasks. Existing literature reveals there is a limited\navailability of TOG datasets for training and benchmarking despite large\ndemand, which are often synthetic or have artifacts in mask annotations that\nhinder model performance. Moreover, TOG solutions often require affordance\nmasks, grasps, and object masks for training, however, existing datasets\ntypically provide only a subset of these annotations. To address these\nlimitations, we introduce the Top-down Task-oriented Grasping (TD-TOG) dataset,\ndesigned to train and evaluate TOG solutions. TD-TOG comprises 1,449 real-world\nRGB-D scenes including 30 object categories and 120 subcategories, with\nhand-annotated object masks, affordances, and planar rectangular grasps. It\nalso features a test set for a novel challenge that assesses a TOG solution's\nability to distinguish between object subcategories. To contribute to the\ndemand for TOG solutions that can adapt and manipulate previously unseen\nobjects without re-training, we propose a novel TOG framework, Binary-TOG.\nBinary-TOG uses zero-shot for object recognition, and one-shot learning for\naffordance recognition. Zero-shot learning enables Binary-TOG to identify\nobjects in multi-object scenes through textual prompts, eliminating the need\nfor visual references. In multi-object settings, Binary-TOG achieves an average\ntask-oriented grasp accuracy of 68.9%. Lastly, this paper contributes a\ncomparative analysis between one-shot and zero-shot learning for object\ngeneralization in TOG to be used in the development of future TOG solutions.", "AI": {"tldr": "The paper introduces a novel dataset (TD-TOG) and Binary-TOG framework for task-oriented grasping (TOG) in robotics, addressing limitations in existing datasets and methods.", "motivation": "There is a shortage of high-quality TOG datasets that provide comprehensive annotations and enable training and benchmarking. Existing datasets are often synthetic or insufficiently annotated, leading to decreased model performance.", "method": "The authors created the TD-TOG dataset with 1,449 real-world RGB-D scenes, encompassing 30 object categories and 120 subcategories with hand-labeled annotations. They also propose the Binary-TOG framework, which leverages zero-shot object recognition and one-shot affordance recognition to improve grasp accuracy and generalization.", "result": "The Binary-TOG framework achieved a 68.9% task-oriented grasp accuracy in multi-object settings and demonstrated capabilities of generalizing to previously unseen objects without retraining.", "conclusion": "The proposed dataset and framework address critical gaps in current TOG research and provide significant contributions for future development, particularly in areas requiring generalization to unseen objects."}}
{"id": "2506.05590", "pdf": "https://arxiv.org/pdf/2506.05590", "abs": "https://arxiv.org/abs/2506.05590", "authors": ["Stella Huang", "Qing Zhou"], "title": "Nonlinear Causal Discovery through a Sequential Edge Orientation Approach", "categories": ["stat.ML", "cs.LG"], "comment": "42 Pages, 13 figures, 3 tables", "summary": "Recent advances have established the identifiability of a directed acyclic\ngraph (DAG) under additive noise models (ANMs), spurring the development of\nvarious causal discovery methods. However, most existing methods make\nrestrictive model assumptions, rely heavily on general independence tests, or\nrequire substantial computational time. To address these limitations, we\npropose a sequential procedure to orient undirected edges in a completed\npartial DAG (CPDAG), representing an equivalence class of DAGs, by leveraging\nthe pairwise additive noise model (PANM) to identify their causal directions.\nWe prove that this procedure can recover the true causal DAG assuming a\nrestricted ANM. Building on this result, we develop a novel constraint-based\nalgorithm for learning causal DAGs under nonlinear ANMs. Given an estimated\nCPDAG, we develop a ranking procedure that sorts undirected edges by their\nadherence to the PANM, which defines an evaluation order of the edges. To\ndetermine the edge direction, we devise a statistical test that compares the\nlog-likelihood values, evaluated with respect to the competing directions, of a\nsub-graph comprising just the candidate nodes and their identified parents in\nthe partial DAG. We further establish the structural learning consistency of\nour algorithm in the large-sample limit. Extensive experiments on synthetic and\nreal-world datasets demonstrate that our method is computationally efficient,\nrobust to model misspecification, and consistently outperforms many existing\nnonlinear DAG learning methods.", "AI": {"tldr": "This paper introduces a computationally efficient and robust approach for learning causal directed acyclic graphs (DAGs) under nonlinear additive noise models (ANMs), leveraging pairwise additive noise models for edge direction identification.", "motivation": "Existing methods for causal DAG discovery face challenges like restrictive assumptions, reliance on independence tests, and high computational demands.", "method": "The authors propose a sequential procedure using pairwise additive noise models (PANM) for orienting edges in a completed partial DAG (CPDAG). They introduce ranking and statistical testing methods to evaluate edge directions and prove the structural consistency of their approach.", "result": "Their method demonstrates superior computational efficiency, robustness to model misspecifications, and consistently outperforms existing nonlinear DAG learning approaches in experiments on synthetic and real-world datasets.", "conclusion": "The proposed method offers a novel and effective solution for causal DAG learning, addressing key limitations of existing approaches and establishing its consistency and robustness."}}
{"id": "2506.05360", "pdf": "https://arxiv.org/pdf/2506.05360", "abs": "https://arxiv.org/abs/2506.05360", "authors": ["Taminul Islam", "Toqi Tahamid Sarker", "Mohamed G Embaby", "Khaled R Ahmed", "Amer AbuGhazaleh"], "title": "CarboNeXT and CarboFormer: Dual Semantic Segmentation Architectures for Detecting and Quantifying Carbon Dioxide Emissions Using Optical Gas Imaging", "categories": ["cs.CV"], "comment": null, "summary": "Carbon dioxide (CO$_2$) emissions are critical indicators of both\nenvironmental impact and various industrial processes, including livestock\nmanagement. We introduce CarboNeXT, a semantic segmentation framework for\nOptical Gas Imaging (OGI), designed to detect and quantify CO$_2$ emissions\nacross diverse applications. Our approach integrates a multi-scale context\naggregation network with UPerHead and auxiliary FCN components to effectively\nmodel both local details and global relationships in gas plume imagery. We\ncontribute two novel datasets: (1) the Controlled Carbon Dioxide Release (CCR)\ndataset, which simulates gas leaks with systematically varied flow rates\n(10-100 SCCM), and (2) the Real Time Ankom (RTA) dataset, focusing on emissions\nfrom dairy cow rumen fluid in vitro experiments. Extensive evaluations\ndemonstrate that CarboNeXT outperforms state-of-the-art methods, achieving\n88.46% mIoU on CCR and 92.95% mIoU on RTA, with particular effectiveness in\nchallenging low-flow scenarios. The model operates at 60.95 FPS, enabling\nreal-time monitoring applications. Additionally, we propose CarboFormer, a\nlightweight variant with only 5.07M parameters that achieves 84.68 FPS, with\ncompetitive performance of 84.88% mIoU on CCR and 92.98% on RTA, making it\nsuitable for resource-constrained platforms such as programmable drones. Our\nwork advances both environmental sensing and precision livestock management by\nproviding robust tools for CO$_2$ emission analysis, with a specific focus on\nlivestock applications.", "AI": {"tldr": "The paper introduces CarboNeXT, a semantic segmentation framework for optical gas imaging (OGI) to detect and quantify CO$_2$ emissions, along with two new datasets and a lightweight model variant.", "motivation": "To address the issues of accurately detecting and quantifying CO$_2$ emissions across diverse applications, particularly in environmental and livestock management.", "method": "A multi-scale context aggregation network combined with UPerHead and auxiliary FCN components is used to model both local and global features of gas plume imagery. Two datasets, CCR and RTA, are introduced for training and evaluation.", "result": "CarboNeXT achieves 88.46% mIoU on CCR and 92.95% mIoU on RTA at 60.95 FPS. Its lightweight variant, CarboFormer, achieves 84.88% mIoU on CCR and 92.98% on RTA, operating at 84.68 FPS.", "conclusion": "CarboNeXT and CarboFormer provide robust and efficient tools for CO$_2$ emission detection, advancing both environmental monitoring and livestock management, with CarboFormer specially designed for resource-constrained platforms."}}
{"id": "2506.05385", "pdf": "https://arxiv.org/pdf/2506.05385", "abs": "https://arxiv.org/abs/2506.05385", "authors": ["Xinxin Li", "Huiyao Chen", "Chengjun Liu", "Jing Li", "Meishan Zhang", "Jun Yu", "Min Zhang"], "title": "LLMs Can Also Do Well! Breaking Barriers in Semantic Role Labeling via Large Language Models", "categories": ["cs.CL"], "comment": "19 pages, 3 figures, 10 tables", "summary": "Semantic role labeling (SRL) is a crucial task of natural language processing\n(NLP). Although generative decoder-based large language models (LLMs) have\nachieved remarkable success across various NLP tasks, they still lag behind\nstate-of-the-art encoder-decoder (BERT-like) models in SRL. In this work, we\nseek to bridge this gap by equipping LLMs for SRL with two mechanisms: (a)\nretrieval-augmented generation and (b) self-correction. The first mechanism\nenables LLMs to leverage external linguistic knowledge such as predicate and\nargument structure descriptions, while the second allows LLMs to identify and\ncorrect inconsistent SRL outputs. We conduct extensive experiments on three\nwidely-used benchmarks of SRL (CPB1.0, CoNLL-2009, and CoNLL-2012). Results\ndemonstrate that our method achieves state-of-the-art performance in both\nChinese and English, marking the first successful application of LLMs to\nsurpass encoder-decoder approaches in SRL.", "AI": {"tldr": "Semantic role labeling (SRL) generally underperforms in generative decoder-based large language models compared to encoder-decoder models. This paper introduces retrieval-augmented generation and self-correction mechanisms to close the gap.", "motivation": "The motivation is to address the performance gap in SRL between generative decoder-based LLMs and encoder-decoder models, enabling LLMs to achieve state-of-the-art results.", "method": "The approach involves (a) retrieval-augmented generation, which uses external linguistic knowledge to aid SRL processes, and (b) self-correction, which helps LLMs detect and fix inconsistent outputs.", "result": "Extensive experiments on SRL benchmarks (CPB1.0, CoNLL-2009, and CoNLL-2012) show the proposed method achieving state-of-the-art performance in both Chinese and English.", "conclusion": "The work demonstrates that LLMs equipped with the proposed mechanisms can surpass encoder-decoder models in SRL, indicating significant progress in the capability of LLMs for NLP tasks."}}
{"id": "2506.05427", "pdf": "https://arxiv.org/pdf/2506.05427", "abs": "https://arxiv.org/abs/2506.05427", "authors": ["Zishan Shu", "Yufan Deng", "Hongyu Zhang", "Zhiwei Nie", "Jie Chen"], "title": "MTPNet: Multi-Grained Target Perception for Unified Activity Cliff Prediction", "categories": ["cs.LG", "cs.AI", "cs.CE"], "comment": null, "summary": "Activity cliff prediction is a critical task in drug discovery and material\ndesign. Existing computational methods are limited to handling single binding\ntargets, which restricts the applicability of these prediction models. In this\npaper, we present the Multi-Grained Target Perception network (MTPNet) to\nincorporate the prior knowledge of interactions between the molecules and their\ntarget proteins. Specifically, MTPNet is a unified framework for activity cliff\nprediction, which consists of two components: Macro-level Target Semantic (MTS)\nguidance and Micro-level Pocket Semantic (MPS) guidance. By this way, MTPNet\ndynamically optimizes molecular representations through multi-grained protein\nsemantic conditions. To our knowledge, it is the first time to employ the\nreceptor proteins as guiding information to effectively capture critical\ninteraction details. Extensive experiments on 30 representative activity cliff\ndatasets demonstrate that MTPNet significantly outperforms previous approaches,\nachieving an average RMSE improvement of 18.95% on top of several mainstream\nGNN architectures. Overall, MTPNet internalizes interaction patterns through\nconditional deep learning to achieve unified predictions of activity cliffs,\nhelping to accelerate compound optimization and design. Codes are available at:\nhttps://github.com/ZishanShu/MTPNet.", "AI": {"tldr": "The paper introduces a new model, MTPNet, for improved activity cliff prediction in drug discovery by integrating molecular-protein interactions.", "motivation": "Existing methods for activity cliff prediction are limited as they focus only on single binding targets.", "method": "MTPNet utilizes multi-grained protein semantics, leveraging both macro-level target guidance and micro-level pocket guidance to dynamically refine molecular representations.", "result": "On 30 datasets, MTPNet surpasses previous models, achieving an average RMSE improvement of 18.95% over mainstream GNN architectures.", "conclusion": "MTPNet enhances interaction pattern understanding through conditional deep learning, aiding the acceleration of compound optimization and design."}}
{"id": "2506.06019", "pdf": "https://arxiv.org/pdf/2506.06019", "abs": "https://arxiv.org/abs/2506.06019", "authors": ["Zeqiong Lv", "Chao Qian", "Yun Liu", "Jiahao Fan", "Yanan Sun"], "title": "Runtime Analysis of Evolutionary NAS for Multiclass Classification", "categories": ["cs.NE"], "comment": "Accepted by ICML 2025", "summary": "Evolutionary neural architecture search (ENAS) is a key part of evolutionary\nmachine learning, which commonly utilizes evolutionary algorithms (EAs) to\nautomatically design high-performing deep neural architectures. During past\nyears, various ENAS methods have been proposed with exceptional performance.\nHowever, the theory research of ENAS is still in the infant. In this work, we\nstep for the runtime analysis, which is an essential theory aspect of EAs, of\nENAS upon multiclass classification problems. Specifically, we first propose a\nbenchmark to lay the groundwork for the analysis. Furthermore, we design a\ntwo-level search space, making it suitable for multiclass classification\nproblems and consistent with the common settings of ENAS. Based on both\ndesigns, we consider (1+1)-ENAS algorithms with one-bit and bit-wise mutations,\nand analyze their upper and lower bounds on the expected runtime. We prove that\nthe algorithm using both mutations can find the optimum with the expected\nruntime upper bound of $O(rM\\ln{rM})$ and lower bound of $\\Omega(rM\\ln{M})$.\nThis suggests that a simple one-bit mutation may be greatly considered, given\nthat most state-of-the-art ENAS methods are laboriously designed with the\nbit-wise mutation. Empirical studies also support our theoretical proof.", "AI": {"tldr": "The paper analyzes the runtime of evolutionary neural architecture search (ENAS) algorithms for multiclass classification problems, providing theoretical bounds and empirical validation.", "motivation": "To address the theoretical gap in understanding ENAS, particularly focusing on runtime analysis for designing high-performing neural architectures.", "method": "The study introduces a benchmark and designs a two-level search space for multiclass classification problems. It then investigates (1+1)-ENAS algorithms using one-bit and bit-wise mutations, calculating expected runtime bounds.", "result": "Upper bound of $O(rM\\ln{rM})$ and lower bound of $\\Omega(rM\\ln{M})$ are proven for runtime. These show one-bit mutation is efficient compared to bit-wise mutation used in many state-of-the-art methods.", "conclusion": "The findings suggest considering simpler mutation methods in ENAS, supported by both theoretical proofs and empirical data, which can simplify algorithm design."}}
{"id": "2506.06227", "pdf": "https://arxiv.org/pdf/2506.06227", "abs": "https://arxiv.org/abs/2506.06227", "authors": ["Peter Pirkelbauer"], "title": "CompilerGPT: Leveraging Large Language Models for Analyzing and Acting on Compiler Optimization Reports", "categories": ["cs.PL"], "comment": "C3PO at ISC HPC 2025", "summary": "Current compiler optimization reports often present complex, technical\ninformation that is difficult for programmers to interpret and act upon\neffectively. This paper assesses the capability of large language models (LLM)\nto understand compiler optimization reports and automatically rewrite the code\naccordingly.\n  To this end, the paper introduces CompilerGPT, a novel framework that\nautomates the interaction between compilers, LLMs, and user defined test and\nevaluation harness. CompilerGPT's workflow runs several iterations and reports\non the obtained results.\n  Experiments with two leading LLM models (GPT-4o and Claude Sonnet),\noptimization reports from two compilers (Clang and GCC), and five benchmark\ncodes demonstrate the potential of this approach. Speedups of up to 6.5x were\nobtained, though not consistently in every test. This method holds promise for\nimproving compiler usability and streamlining the software optimization\nprocess.", "AI": {"tldr": "The paper introduces CompilerGPT, which uses large language models (LLMs) to interpret compiler optimization reports and rewrite code for improved performance, achieving up to 6.5x speedups in experiments.", "motivation": "Programmers face challenges in interpreting complex compiler optimization reports effectively.", "method": "The proposed CompilerGPT framework automates interactions between compilers, LLMs, and user-defined evaluation tools, iterating to improve code performance.", "result": "Tests using two LLMs (GPT-4o and Claude Sonnet), two compilers (Clang and GCC), and five benchmark codes achieved up to 6.5x speedup in some cases.", "conclusion": "This approach has potential to improve compiler usability and optimize software development, although results are not universally consistent."}}
{"id": "2506.05742", "pdf": "https://arxiv.org/pdf/2506.05742", "abs": "https://arxiv.org/abs/2506.05742", "authors": ["Shahram Pourdehghan", "Nahideh Derakhshanfard"], "title": "Malicious node aware wireless multi hop networks: a systematic review of the literature and recommendations for future research", "categories": ["cs.DC"], "comment": null, "summary": "Wireless communication provides great advantages that are not available\nthrough their wired counterparts such as flexibility, ease of deployment and\nuse, cost reductions, and convenience. Wireless multi-hop networks (WMN) do not\nhave any centralized management infrastructure. Wireless multi-hop networks\nhave many benefits since proposed. In such networks when a node wants to send a\npacket to a destination where is not in the transmission range, depend on some\nintermediate nodes. In this type of networks packet sending is in the form of\nmultiple hop until destination and this work is dynamic. Lack of centralized\nmanagement cause that some nodes show malicious function. Malicious nodes are\nthat receive packets and drop them maliciously. These malicious nodes could\nhave many reasons such as hardware failure, software failure or lack of power.\nSuch nodes make multiple packets drop from the network and the performance of\nnetwork strongly decreases. As a result, the throughput of the network\ndecrease, increase end-to-end delay and increase overhead. Therefore, we must\naware from presence of malicious node in the network and do routing based on\nthis awareness. Therefore, this paper aims to study and review the present\nmalicious node detection methods that proposed in literatures. We categorized\nnetworks in groups, including ad hoc networks, MANET, DTN, Opportunistic\nnetworks, WSN, VANET and other wireless networks and compare malicious node\ndetection met", "AI": {"tldr": "The paper reviews existing methods for detecting malicious nodes in wireless multi-hop networks and compares them across different network types.", "motivation": "Wireless multi-hop networks suffer performance issues due to malicious nodes that drop packets, leading to reduced throughput, increased delay, and higher overhead.", "method": "The authors categorize various wireless network types (e.g., MANET, DTN, VANET) and analyze existing literature on malicious node detection methods.", "result": "The paper provides a comparison of malicious node detection techniques across different network types, identifying their strengths and weaknesses.", "conclusion": "Awareness of malicious nodes is critical for effective routing in wireless multi-hop networks, and this paper highlights existing methods while acknowledging the need for further improvements."}}
{"id": "2506.05633", "pdf": "https://arxiv.org/pdf/2506.05633", "abs": "https://arxiv.org/abs/2506.05633", "authors": ["Guy Gaziv", "Sarah Goulding", "Ani Ayvazian-Hancock", "Yoon Bai", "James J. DiCarlo"], "title": "Noninvasive precision modulation of high-level neural population activity via natural vision perturbations", "categories": ["q-bio.NC", "cs.CV", "cs.NE"], "comment": null, "summary": "Precise control of neural activity -- modulating target neurons deep in the\nbrain while leaving nearby neurons unaffected -- is an outstanding challenge in\nneuroscience, generally achieved through invasive techniques. This study\ninvestigates the possibility of precisely and noninvasively modulating neural\nactivity in the high-level primate ventral visual stream via perturbations on\none's natural visual feed. When tested on macaque inferior temporal (IT) neural\npopulations, we found quantitative agreement between the model-predicted and\nbiologically realized effect: strong modulation concentrated on targeted neural\nsites. We extended this to demonstrate accurate injection of\nexperimenter-chosen neural population patterns via subtle perturbations applied\non the background of typical natural visual feeds. These results highlight that\ncurrent machine-executable models of the ventral stream can now design\nnoninvasive, visually-delivered, possibly imperceptible neural interventions at\nthe resolution of individual neurons.", "AI": {"tldr": "The study demonstrates the possibility of noninvasive neural activity modulation in the primate ventral visual stream using subtle visual feed perturbations, achieving precise effects at individual neuron resolution.", "motivation": "Neuroscience aims to control specific neurons deep in the brain without affecting nearby ones, but existing methods are highly invasive.", "method": "The researchers tested neural modulation by applying subtle, natural visual feed perturbations to target macaque IT neural populations, validating predictions with biological responses.", "result": "Strong, precise modulation of targeted neural sites was achieved. Patterns chosen by experimenters were accurately injected into neural populations without invasive techniques.", "conclusion": "Noninvasive visual interventions can precisely target individual neurons using current machine-based models of the primate visual system, advancing neural control capabilities."}}
{"id": "2506.05422", "pdf": "https://arxiv.org/pdf/2506.05422", "abs": "https://arxiv.org/abs/2506.05422", "authors": ["Andrei T. Patrascu"], "title": "Constructive Symbolic Reinforcement Learning via Intuitionistic Logic and Goal-Chaining Inference", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "We introduce a novel learning and planning framework that replaces\ntraditional reward-based optimisation with constructive logical inference. In\nour model, actions, transitions, and goals are represented as logical\npropositions, and decision-making proceeds by building constructive proofs\nunder intuitionistic logic. This method ensures that state transitions and\npolicies are accepted only when supported by verifiable preconditions --\neschewing probabilistic trial-and-error in favour of guaranteed logical\nvalidity. We implement a symbolic agent operating in a structured gridworld,\nwhere reaching a goal requires satisfying a chain of intermediate subgoals\n(e.g., collecting keys to open doors), each governed by logical constraints.\nUnlike conventional reinforcement learning agents, which require extensive\nexploration and suffer from unsafe or invalid transitions, our constructive\nagent builds a provably correct plan through goal chaining, condition tracking,\nand knowledge accumulation. Empirical comparison with Q-learning demonstrates\nthat our method achieves perfect safety, interpretable behaviour, and efficient\nconvergence with no invalid actions, highlighting its potential for safe\nplanning, symbolic cognition, and trustworthy AI. This work presents a new\ndirection for reinforcement learning grounded not in numeric optimisation, but\nin constructive logic and proof theory.", "AI": {"tldr": "The paper introduces a logic-driven approach to learning and planning, replacing reward-based strategies with constructive logical inference.", "motivation": "To address the inefficiencies, safety risks, and lack of interpretability in traditional reward-based reinforcement learning approaches.", "method": "The authors employ intuitionistic logic to develop a framework where actions and transitions are verified through constructive logical proofs, avoiding probabilistic trial-and-error.", "result": "The symbolic agent achieves perfect safety, efficient learning, and interpretable behavior compared to conventional Q-learning in a structured gridworld environment.", "conclusion": "The approach promotes safe planning and symbolic reasoning in AI by grounding decision-making in constructive logic rather than numeric optimisation."}}
{"id": "2506.05614", "pdf": "https://arxiv.org/pdf/2506.05614", "abs": "https://arxiv.org/abs/2506.05614", "authors": ["E. G. Santana Jr", "Gabriel Benjamin", "Melissa Araujo", "Harrison Santos", "David Freitas", "Eduardo Almeida", "Paulo Anselmo da M. S. Neto", "Jiawei Li", "Jina Chun", "Iftekhar Ahmed"], "title": "Which Prompting Technique Should I Use? An Empirical Investigation of Prompting Techniques for Software Engineering Tasks", "categories": ["cs.SE"], "comment": null, "summary": "A growing variety of prompt engineering techniques has been proposed for\nLarge Language Models (LLMs), yet systematic evaluation of each technique on\nindividual software engineering (SE) tasks remains underexplored. In this\nstudy, we present a systematic evaluation of 14 established prompt techniques\nacross 10 SE tasks using four LLM models. As identified in the prior\nliterature, the selected prompting techniques span six core dimensions\n(Zero-Shot, Few-Shot, Thought Generation, Ensembling, Self-Criticism, and\nDecomposition). They are evaluated on tasks such as code generation, bug\nfixing, and code-oriented question answering, to name a few. Our results show\nwhich prompting techniques are most effective for SE tasks requiring complex\nlogic and intensive reasoning versus those that rely more on contextual\nunderstanding and example-driven scenarios. We also analyze correlations\nbetween the linguistic characteristics of prompts and the factors that\ncontribute to the effectiveness of prompting techniques in enhancing\nperformance on SE tasks. Additionally, we report the time and token consumption\nfor each prompting technique when applied to a specific task and model,\noffering guidance for practitioners in selecting the optimal prompting\ntechnique for their use cases.", "AI": {"tldr": "The paper evaluates 14 prompt engineering techniques across multiple software engineering (SE) tasks using 4 LLMs, to compare their effectiveness based on task type and prompting dimensions.", "motivation": "To address the lack of systematic evaluation of prompt engineering techniques applied to different SE tasks with LLMs.", "method": "Conducting a systematic evaluation of 14 established prompting techniques across 10 SE tasks using 4 LLM models. The techniques are categorized into 6 dimensions (e.g., Zero-Shot, Few-Shot) and evaluated for their impact on tasks like code generation and bug fixing.", "result": "The study identifies which prompt techniques are more effective for SE tasks requiring complex reasoning versus contextual understanding. It also examines correlations between prompt linguistic traits and their effectiveness, as well as time and token usage insights.", "conclusion": "The paper provides practical insights and guidance for selecting suitable prompting techniques for SE tasks based on task requirements, resource use, and the characteristics of the prompts."}}
{"id": "2506.05653", "pdf": "https://arxiv.org/pdf/2506.05653", "abs": "https://arxiv.org/abs/2506.05653", "authors": ["Thien Hoang Nguyen", "Erik Muller", "Michael Rubin", "Xiaofei Wang", "Fiorella Sibona", "Salah Sukkarieh"], "title": "Towards Autonomous In-situ Soil Sampling and Mapping in Large-Scale Agricultural Environments", "categories": ["cs.RO", "cs.ET"], "comment": "Presented at the 2025 IEEE ICRA Workshop on Field Robotics", "summary": "Traditional soil sampling and analysis methods are labor-intensive,\ntime-consuming, and limited in spatial resolution, making them unsuitable for\nlarge-scale precision agriculture. To address these limitations, we present a\nrobotic solution for real-time sampling, analysis and mapping of key soil\nproperties. Our system consists of two main sub-systems: a Sample Acquisition\nSystem (SAS) for precise, automated in-field soil sampling; and a Sample\nAnalysis Lab (Lab) for real-time soil property analysis. The system's\nperformance was validated through extensive field trials at a large-scale\nAustralian farm. Experimental results show that the SAS can consistently\nacquire soil samples with a mass of 50g at a depth of 200mm, while the Lab can\nprocess each sample within 10 minutes to accurately measure pH and\nmacronutrients. These results demonstrate the potential of the system to\nprovide farmers with timely, data-driven insights for more efficient and\nsustainable soil management and fertilizer application.", "AI": {"tldr": "This research introduces a robotic system combining real-time soil sampling, analysis, and mapping for precision agriculture.", "motivation": "Traditional soil analysis methods are too labor-intensive, time-consuming, and spatially limited for large-scale agricultural applications.", "method": "The system comprises two parts: a Sample Acquisition System (SAS) for soil sampling and a Sample Analysis Lab (Lab) for real-time analysis of soil properties.", "result": "Field trials showed that SAS can consistently collect samples at specified weights and depths, while the Lab processes samples within 10 minutes for accurate pH and nutrient measurement.", "conclusion": "This robotic system enables efficient and sustainable soil management, providing farmers with timely insights for improved precision agriculture practices."}}
{"id": "2506.06087", "pdf": "https://arxiv.org/pdf/2506.06087", "abs": "https://arxiv.org/abs/2506.06087", "authors": ["Yuga Hikida", "Ayush Bharti", "Niall Jeffrey", "Fran\u00e7ois-Xavier Briol"], "title": "Multilevel neural simulation-based inference", "categories": ["stat.ML", "astro-ph.CO", "astro-ph.IM", "cs.LG", "stat.CO"], "comment": null, "summary": "Neural simulation-based inference (SBI) is a popular set of methods for\nBayesian inference when models are only available in the form of a simulator.\nThese methods are widely used in the sciences and engineering, where writing\ndown a likelihood can be significantly more challenging than constructing a\nsimulator. However, the performance of neural SBI can suffer when simulators\nare computationally expensive, thereby limiting the number of simulations that\ncan be performed. In this paper, we propose a novel approach to neural SBI\nwhich leverages multilevel Monte Carlo techniques for settings where several\nsimulators of varying cost and fidelity are available. We demonstrate through\nboth theoretical analysis and extensive experiments that our method can\nsignificantly enhance the accuracy of SBI methods given a fixed computational\nbudget.", "AI": {"tldr": "The paper proposes enhancing neural simulation-based inference (SBI) by using multilevel Monte Carlo techniques for scenarios with simulators of varying cost and fidelity, improving accuracy under fixed computational budgets.", "motivation": "Performing Bayesian inference through neural SBI is often impeded by computationally expensive simulators or limited simulations in scientific and engineering applications.", "method": "The authors introduce a multilevel Monte Carlo approach integrated into neural SBI to utilize simulators with different fidelity and costs effectively.", "result": "Theoretical analysis and experiments show that the proposed method achieves improved inference accuracy compared to traditional neural SBI approaches with a constrained computational budget.", "conclusion": "Leveraging multilevel Monte Carlo improved neural SBI performance, making it a practical tool for performing inference in scenarios with computationally expensive and diverse simulators."}}
{"id": "2506.05361", "pdf": "https://arxiv.org/pdf/2506.05361", "abs": "https://arxiv.org/abs/2506.05361", "authors": ["Tinglin Huang", "Tianyu Liu", "Mehrtash Babadi", "Wengong Jin", "Rex Ying"], "title": "Scalable Generation of Spatial Transcriptomics from Histology Images via Whole-Slide Flow Matching", "categories": ["cs.CV", "q-bio.GN"], "comment": "Accepted at ICML 2025", "summary": "Spatial transcriptomics (ST) has emerged as a powerful technology for\nbridging histology imaging with gene expression profiling. However, its\napplication has been limited by low throughput and the need for specialized\nexperimental facilities. Prior works sought to predict ST from whole-slide\nhistology images to accelerate this process, but they suffer from two major\nlimitations. First, they do not explicitly model cell-cell interaction as they\nfactorize the joint distribution of whole-slide ST data and predict the gene\nexpression of each spot independently. Second, their encoders struggle with\nmemory constraints due to the large number of spots (often exceeding 10,000) in\ntypical ST datasets. Herein, we propose STFlow, a flow matching generative\nmodel that considers cell-cell interaction by modeling the joint distribution\nof gene expression of an entire slide. It also employs an efficient slide-level\nencoder with local spatial attention, enabling whole-slide processing without\nexcessive memory overhead. On the recently curated HEST-1k and STImage-1K4M\nbenchmarks, STFlow substantially outperforms state-of-the-art baselines and\nachieves over 18% relative improvements over the pathology foundation models.", "AI": {"tldr": "This paper proposes STFlow, a model to predict spatial transcriptomics data considering cell-cell interactions, overcoming limitations of existing approaches.", "motivation": "Current spatial transcriptomics (ST) techniques are hindered by low throughput and dependence on specialized facilities. Efforts to predict ST from histology images have limitations: lack of cell-cell interaction modeling and memory inefficiency in handling large datasets.", "method": "The authors introduce STFlow, a flow matching generative model that captures cell-cell interactions by modeling the joint distribution of gene expression for entire slides. STFlow includes a slide-level encoder with local spatial attention to efficiently process whole slides without excessive memory usage.", "result": "STFlow outperforms state-of-the-art methods on the HEST-1k and STImage-1K4M benchmarks by over 18%, showing considerable improvement over existing pathology models.", "conclusion": "STFlow addresses limitations in current ST prediction methods, offering more accurate results while managing memory constraints effectively for large-scale datasets."}}
{"id": "2506.05386", "pdf": "https://arxiv.org/pdf/2506.05386", "abs": "https://arxiv.org/abs/2506.05386", "authors": ["Lo Pang-Yun Ting", "Chengshuai Zhao", "Yu-Hua Zeng", "Yuan Jee Lim", "Kun-Ta Chuang"], "title": "Beyond RAG: Reinforced Reasoning Augmented Generation for Clinical Notes", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Clinical note generation aims to automatically produce free-text summaries of\na patient's condition and diagnostic process, with discharge instructions being\na representative long-form example. While recent large language model\n(LLM)-based methods pre-trained on general clinical corpora show promise in\nclinical text generation, they fall short in producing long-form notes from\nlimited patient information. In this paper, we propose R2AG, the first\nreinforced retriever for long-form discharge instruction generation based on\npre-admission data. R2AG is trained with reinforcement learning to retrieve\nreasoning paths from a medical knowledge graph, providing explicit semantic\nguidance to the LLM. To bridge the information gap, we propose Group-Based\nRetriever Optimization (GRO) which improves retrieval quality with\ngroup-relative rewards, encouraging reasoning leaps for deeper inference by the\nLLM. Comprehensive experiments on the MIMIC-IV-Note dataset show that R2AG\noutperforms baselines in both clinical efficacy and natural language generation\nmetrics. Further analysis reveals that R2AG fills semantic gaps in sparse input\nscenarios, and retrieved reasoning paths help LLMs avoid clinical\nmisinterpretation by focusing on key evidence and following coherent reasoning.", "AI": {"tldr": "The paper presents R2AG, a method combining a retriever mechanism and reinforcement learning to improve the generation of clinical discharge instructions.", "motivation": "Existing large language models struggle to generate high-quality, long-form clinical notes using limited patient information.", "method": "The authors proposed R2AG, which uses reinforcement learning to retrieve reasoning paths from a medical knowledge graph. This method incorporates Group-Based Retriever Optimization (GRO) to improve retrieval and enhance LLM inference quality.", "result": "R2AG outperformed baseline methods on the MIMIC-IV-Note dataset, showing better clinical efficacy and natural language generation. It successfully addressed semantic gaps and prevented clinical misinterpretation.", "conclusion": "R2AG enables LLMs to deliver more accurate and semantically enriched clinical note generation, offering a valuable tool for improving automated healthcare documentation."}}
{"id": "2506.05428", "pdf": "https://arxiv.org/pdf/2506.05428", "abs": "https://arxiv.org/abs/2506.05428", "authors": ["Zhihao Tang", "Chaozhuo Li", "Litian Zhang", "Xi Zhang"], "title": "Diffusion with a Linguistic Compass: Steering the Generation of Clinically Plausible Future sMRI Representations for Early MCI Conversion Prediction", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Early prediction of Mild Cognitive Impairment (MCI) conversion is hampered by\na trade-off between immediacy--making fast predictions from a single baseline\nsMRI--and accuracy--leveraging longitudinal scans to capture disease\nprogression. We propose MCI-Diff, a diffusion-based framework that synthesizes\nclinically plausible future sMRI representations directly from baseline data,\nachieving both real-time risk assessment and high predictive performance.\nFirst, a multi-task sequence reconstruction strategy trains a shared denoising\nnetwork on interpolation and extrapolation tasks to handle irregular follow-up\nsampling and learn robust latent trajectories. Second, an LLM-driven\n\"linguistic compass\" is introduced for clinical plausibility sampling:\ngenerated feature candidates are quantized, tokenized, and scored by a\nfine-tuned language model conditioned on expected structural biomarkers,\nguiding autoregressive generation toward realistic disease patterns.\nExperiments on ADNI and AIBL cohorts show that MCI-Diff outperforms\nstate-of-the-art baselines, improving early conversion accuracy by 5-12%.", "AI": {"tldr": "The paper presents MCI-Diff, a novel framework for predicting Mild Cognitive Impairment (MCI) conversion using baseline sMRI data while addressing the trade-off between immediacy and accuracy via a diffusion-based approach.", "motivation": "The study aims to overcome the challenge of balancing fast MCI prediction from a single baseline sMRI with higher accuracy obtained from longitudinal scans.", "method": "MCI-Diff employs a diffusion-based framework with two key innovations: multi-task sequence reconstruction to learn robust latent trajectories, and a language model-guided sampling technique that steers autoregressive generation toward clinically plausible disease patterns.", "result": "Experiments conducted on the ADNI and AIBL datasets demonstrate that MCI-Diff achieves 5-12% higher accuracy in predicting early MCI conversion compared to state-of-the-art methods.", "conclusion": "MCI-Diff integrates advanced diffusion and language model techniques to enhance both immediacy and accuracy in early MCI conversion prediction, offering a clinically viable solution."}}
{"id": "2506.06265", "pdf": "https://arxiv.org/pdf/2506.06265", "abs": "https://arxiv.org/abs/2506.06265", "authors": ["Zofia Rudnicka", "Januszcz Szczepanski", "Agnieszka Pregowska"], "title": "Integrating Complexity and Biological Realism: High-Performance Spiking Neural Networks for Breast Cancer Detection", "categories": ["cs.NE", "eess.IV", "q-bio.NC"], "comment": null, "summary": "Spiking Neural Networks (SNNs) event-driven nature enables efficient encoding\nof spatial and temporal features, making them suitable for dynamic\ntime-dependent data processing. Despite their biological relevance, SNNs have\nseen limited application in medical image recognition due to difficulties in\nmatching the performance of conventional deep learning models. To address this,\nwe propose a novel breast cancer classification approach that combines SNNs\nwith Lempel-Ziv Complexity (LZC) a computationally efficient measure of\nsequence complexity. LZC enhances the interpretability and accuracy of\nspike-based models by capturing structural patterns in neural activity. Our\nstudy explores both biophysical Leaky Integrate-and-Fire (LIF) and\nprobabilistic Levy-Baxter (LB) neuron models under supervised, unsupervised,\nand hybrid learning regimes. Experiments were conducted on the Breast Cancer\nWisconsin dataset using numerical features derived from medical imaging.\nLB-based models consistently exceeded 90.00% accuracy, while LIF-based models\nreached over 85.00%. The highest accuracy of 98.25% was achieved using an\nANN-to-SNN conversion method applied to both neuron models comparable to\ntraditional deep learning with back-propagation, but at up to 100 times lower\ncomputational cost. This hybrid approach merges deep learning performance with\nthe efficiency and plausibility of SNNs, yielding top results at lower\ncomputational cost. We hypothesize that the synergy between temporal-coding,\nspike-sparsity, and LZC-driven complexity analysis enables more-efficient\nfeature extraction. Our findings demonstrate that SNNs combined with LZC offer\npromising, biologically plausible alternative to conventional neural networks\nin medical diagnostics, particularly for resource-constrained or real-time\nsystems.", "AI": {"tldr": "The paper introduces a breast cancer classification method using Spiking Neural Networks (SNNs) enhanced by Lempel-Ziv Complexity (LZC).", "motivation": "Conventional deep learning models outperform SNNs in accuracy for medical imaging, limiting SNNs' broader application in this area despite their efficiency and biological relevance.", "method": "Combines LZC with SNNs using Leaky Integrate-and-Fire (LIF) and Levy-Baxter (LB) neuron models under various learning regimes. An ANN-to-SNN conversion is applied to achieve high accuracy while reducing computational cost.", "result": "LB-based SNN models achieved over 90% accuracy, LIF-based models surpassed 85%, and the ANN-to-SNN conversion method yielded a top accuracy of 98.25% with up to 100 times lower computational cost.", "conclusion": "SNNs, combined with LZC, are efficient, interpretable, and biologically plausible alternatives to traditional deep learning methods, particularly in resource-limited medical diagnostic systems."}}
{"id": "2506.06102", "pdf": "https://arxiv.org/pdf/2506.06102", "abs": "https://arxiv.org/abs/2506.06102", "authors": ["Hugo Mirault", "Peter Robinson", "Ming Ming Tan", "Xianbin Zhu"], "title": "Perfect Matching with Few Link Activations", "categories": ["cs.DC", "cs.DS"], "comment": "A short version of this work appeared at SIROCCO 2025", "summary": "We consider the problem of computing a perfect matching problem in a\nsynchronous distributed network, where the network topology corresponds to a\ncomplete bipartite graph. The communication between nodes is restricted to\nactivating communication links, which means that instead of sending messages\ncontaining a number of bits, each node can only send a pulse over some of its\nincident links in each round. In the port numbering model, where nodes are\nunaware of their neighbor's IDs, we give a randomized algorithm that terminates\nin $O( \\log n )$ rounds and has a pulse complexity of $O( n\\log n )$, which\ncorresponds to the number of pulses sent over all links. We also show that\nrandomness is crucial in the port numbering model, as any deterministic\nalgorithm must send at least $\\Omega( n^2 )$ messages in the standard LOCAL\nmodel, where the messages can be of unbounded size. Then, we turn our attention\nto the KT_1 assumption, where each node starts out knowing its neighbors' IDs.\nWe show that this additional knowledge enables significantly improved bounds\neven for deterministic algorithms. First, we give an $O( \\log n )$ time\ndeterministic algorithm that sends only $O( n )$ pulses. Finally, we apply this\nalgorithm recursively to obtain an exponential reduction in the time complexity\nto $O( \\log^*n\\log\\log n )$, while slightly increasing the pulse complexity to\n$O( n\\log^*n )$. All our bounds also hold in the standard CONGEST model with\nsingle-bit messages.", "AI": {"tldr": "This paper addresses the problem of finding a perfect matching in a distributed network modeled as a complete bipartite graph. It introduces randomized and deterministic algorithms under different communication models, achieving significant reductions in time and pulse complexity.", "motivation": "To efficiently compute a perfect matching in distributed networks with constraints on communication and knowledge, improving upon the limitations of existing algorithms.", "method": "The authors propose algorithms for two communication models: randomized methods for the port numbering model and deterministic methods for the KT_1 model. They analyze time and pulse complexity under these settings.", "result": "In the port numbering model, randomized algorithms achieve $O(\\log n)$ time and $O(n\\log n)$ pulse complexity. In the KT_1 model, deterministic algorithms achieve $O(\\log n)$ time with $O(n)$ pulses and can further reduce time complexity to $O(\\log^*n\\log\\log n)$ with slight pulse complexity increases.", "conclusion": "Knowledge of neighbor IDs (KT_1 model) significantly improves algorithm efficiency. Randomness is crucial in the port numbering model to achieve better complexity. The proposed methodologies provide scalable solutions in distributed computing."}}
{"id": "2506.05794", "pdf": "https://arxiv.org/pdf/2506.05794", "abs": "https://arxiv.org/abs/2506.05794", "authors": ["Luca M. Possati"], "title": "Markov Blanket Density and Free Energy Minimization", "categories": ["q-bio.NC", "cs.IT", "math.IT"], "comment": null, "summary": "This paper presents a continuous, information-theoretic extension of the Free\nEnergy Principle through the concept of Markov blanket density, i.e., a scalar\nfield that quantifies the degree of conditional independence between internal\nand external states at each point in space (ranging from 0 for full coupling to\n1 for full separation). It demonstrates that active inference dynamics\n(including the minimization of variational and expected free energy) naturally\nemerge from spatial gradients in this density, making Markov blanket density a\nnecessary foundation for the definability and coherence of the Free Energy\nPrinciple. These ideas are developed through a mathematically framework that\nlinks density gradients to precise and testable dynamics, offering a foundation\nfor novel predictions and simulation paradigms.", "AI": {"tldr": "The paper extends the Free Energy Principle using a concept called Markov blanket density, linking it to active inference dynamics.", "motivation": "To provide a continuous, information-theoretic foundation that enhances the current understanding and applicability of the Free Energy Principle.", "method": "Introduces the concept of Markov blanket density and develops a mathematical framework to connect density gradients with active inference dynamics.", "result": "Shows that active inference dynamics emerge from spatial gradients in Markov blanket density and establishes it as essential for the Free Energy Principle's coherence.", "conclusion": "The proposed framework serves as a basis for novel predictions, simulations, and a deeper understanding of dynamics based on the Free Energy Principle."}}
{"id": "2506.05520", "pdf": "https://arxiv.org/pdf/2506.05520", "abs": "https://arxiv.org/abs/2506.05520", "authors": ["Cecil Pang"], "title": "Towards Data Systems That Are Business Semantic-Centric and AI Agents-Assisted", "categories": ["cs.AI", "cs.MA"], "comment": "Being peer reviewed by a journal", "summary": "Contemporary businesses operate in dynamic environments requiring rapid\nadaptation to achieve goals and maintain competitiveness. Existing data\nplatforms often fall short by emphasizing tools over alignment with business\nneeds, resulting in inefficiencies and delays. To address this gap, I propose\nthe Business Semantics Centric, AI Agents Assisted Data System (BSDS), a\nholistic system that integrates architecture, workflows, and team organization\nto ensure data systems are tailored to business priorities rather than dictated\nby technical constraints. BSDS redefines data systems as dynamic enablers of\nbusiness success, transforming them from passive tools into active drivers of\norganizational growth. BSDS has a modular architecture that comprises curated\ndata linked to business entities, a knowledge base for context-aware AI agents,\nand efficient data pipelines. AI agents play a pivotal role in assisting with\ndata access and system management, reducing human effort, and improving\nscalability. Complementing this architecture, BSDS incorporates workflows\noptimized for both exploratory data analysis and production requirements,\nbalancing speed of delivery with quality assurance. A key innovation of BSDS is\nits incorporation of the human factor. By aligning data team expertise with\nbusiness semantics, BSDS bridges the gap between technical capabilities and\nbusiness needs. Validated through real-world implementation, BSDS accelerates\ntime-to-market for data-driven initiatives, enhances cross-functional\ncollaboration, and provides a scalable blueprint for businesses of all sizes.\nFuture research can build on BSDS to explore optimization strategies using\ncomplex systems and adaptive network theories, as well as developing autonomous\ndata systems leveraging AI agents.", "AI": {"tldr": "The paper introduces the BSDS system to align data systems with business needs by integrating architecture, AI agents, and workflows.", "motivation": "Existing data platforms fail to adequately align with business objectives, focusing more on tools than on business needs.", "method": "The BSDS system integrates modular architecture (data linked to business entities), context-aware AI agents, efficient data pipelines, and workflows optimized for exploration and production.", "result": "BSDS has been validated in real-world settings, resulting in faster initiatives, better cross-functional collaboration, and scalability.", "conclusion": "BSDS transforms data systems from passive tools to active business enablers while serving as a scalable model. Future research can advance these concepts."}}
{"id": "2506.05623", "pdf": "https://arxiv.org/pdf/2506.05623", "abs": "https://arxiv.org/abs/2506.05623", "authors": ["Tianyi Zhang", "Shidong Pan", "Zejun Zhang", "Zhenchang Xing", "Xiaoyu Sun"], "title": "Deployability-Centric Infrastructure-as-Code Generation: An LLM-based Iterative Framework", "categories": ["cs.SE", "cs.AI", "cs.CL"], "comment": null, "summary": "Infrastructure-as-Code (IaC) generation holds significant promise for\nautomating cloud infrastructure provisioning. Recent advances in Large Language\nModels (LLMs) present a promising opportunity to democratize IaC development by\ngenerating deployable infrastructure templates from natural language\ndescriptions, but current evaluation focuses on syntactic correctness while\nignoring deployability, the fatal measure of IaC template utility. We address\nthis gap through two contributions: (1) IaCGen, an LLM-based\ndeployability-centric framework that uses iterative feedback mechanism to\ngenerate IaC templates, and (2) DPIaC-Eval, a deployability-centric IaC\ntemplate benchmark consists of 153 real-world scenarios that can evaluate\nsyntax, deployment, user intent, and security. Our evaluation reveals that\nstate-of-the-art LLMs initially performed poorly, with Claude-3.5 and\nClaude-3.7 achieving only 30.2% and 26.8% deployment success on the first\nattempt respectively. However, IaCGen transforms this performance dramatically:\nall evaluated models reach over 90% passItr@25, with Claude-3.5 and Claude-3.7\nachieving 98% success rate. Despite these improvements, critical challenges\nremain in user intent alignment (25.2% accuracy) and security compliance (8.4%\npass rate), highlighting areas requiring continued research. Our work provides\nthe first comprehensive assessment of deployability-centric IaC template\ngeneration and establishes a foundation for future research.", "AI": {"tldr": "The paper introduces IaCGen, a deployability-driven approach for generating Infrastructure-as-Code (IaC) templates using LLMs, and DPIaC-Eval, a benchmark of real-world scenarios for comprehensive evaluation.", "motivation": "Current methods for generating IaC templates using LLMs focus on syntactic accuracy but neglect deployability, which is the critical measure of their utility.", "method": "They developed IaCGen, an iterative feedback-based framework to optimize deployability, and DPIaC-Eval, a benchmark comprising 153 real-world cases assessing syntax, deployment, intent alignment, and security.", "result": "State-of-the-art LLMs initially displayed low deployability success rates (around 26-30%), but IaCGen significantly improved success rates to over 90%, with Claude models reaching up to 98% success. However, intent alignment and security compliance remains underperforming.", "conclusion": "Their framework transforms LLM-based IaC generation, but critical challenges such as intent accuracy and security compliance need further research, providing a new avenue for advancing deployability in IaC automation."}}
{"id": "2506.05714", "pdf": "https://arxiv.org/pdf/2506.05714", "abs": "https://arxiv.org/abs/2506.05714", "authors": ["Keyi Zhu", "Kyle Lammers", "Kaixiang Zhang", "Chaaran Arunachalam", "Siddhartha Bhattacharya", "Jiajia Li", "Renfu Lu", "Zhaojian Li"], "title": "Advancement and Field Evaluation of a Dual-arm Apple Harvesting Robot", "categories": ["cs.RO"], "comment": null, "summary": "Apples are among the most widely consumed fruits worldwide. Currently, apple\nharvesting fully relies on manual labor, which is costly, drudging, and\nhazardous to workers. Hence, robotic harvesting has attracted increasing\nattention in recent years. However, existing systems still fall short in terms\nof performance, effectiveness, and reliability for complex orchard\nenvironments. In this work, we present the development and evaluation of a\ndual-arm harvesting robot. The system integrates a ToF camera, two 4DOF robotic\narms, a centralized vacuum system, and a post-harvest handling module. During\nharvesting, suction force is dynamically assigned to either arm via the vacuum\nsystem, enabling efficient apple detachment while reducing power consumption\nand noise. Compared to our previous design, we incorporated a platform movement\nmechanism that enables both in-out and up-down adjustments, enhancing the\nrobot's dexterity and adaptability to varying canopy structures. On the\nalgorithmic side, we developed a robust apple localization pipeline that\ncombines a foundation-model-based detector, segmentation, and clustering-based\ndepth estimation, which improves performance in orchards. Additionally,\npressure sensors were integrated into the system, and a novel dual-arm\ncoordination strategy was introduced to respond to harvest failures based on\nsensor feedback, further improving picking efficiency. Field demos were\nconducted in two commercial orchards in MI, USA, with different canopy\nstructures. The system achieved success rates of 0.807 and 0.797, with an\naverage picking cycle time of 5.97s. The proposed strategy reduced harvest time\nby 28% compared to a single-arm baseline. The dual-arm harvesting robot\nenhances the reliability and efficiency of apple picking. With further\nadvancements, the system holds strong potential for autonomous operation and\ncommercialization for the apple industry.", "AI": {"tldr": "This paper introduces a dual-arm apple-harvesting robot to reduce manual labor, featuring design advancements for better efficiency and dexterity, achieving success rates of 80.7% and 79.7% in field tests.", "motivation": "To address the high cost, physical demands, and safety risks of manual apple harvesting, motivating the development of an efficient and reliable robotic harvest system capable of operating in complex orchards.", "method": "The system integrates two robotic arms, a ToF camera, a vacuum system for suction control, and a platform movement mechanism. It uses a foundation-model-based apple detection pipeline, pressure sensors, and dual-arm coordination strategy for improved performance.", "result": "Field tests in two commercial orchards demonstrated success rates of 80.7% and 79.7%, with an average picking cycle time of 5.97 seconds, achieving a 28% reduction in harvest time compared to a single-arm system.", "conclusion": "The dual-arm robot significantly improves apple-picking reliability and efficiency. With further refinements, it could enable fully autonomous and commercially viable apple harvesting."}}
{"id": "2506.05354", "pdf": "https://arxiv.org/pdf/2506.05354", "abs": "https://arxiv.org/abs/2506.05354", "authors": ["Jarek Duda"], "title": "Adaptive stable distribution and Hurst exponent by method of moments moving estimator for nonstationary time series", "categories": ["stat.ME", "cs.LG", "econ.EM", "stat.ML"], "comment": "5 pages, 7 figures. arXiv admin note: text overlap with\n  arXiv:2304.03069", "summary": "Nonstationarity of real-life time series requires model adaptation. In\nclassical approaches like ARMA-ARCH there is assumed some arbitrarily chosen\ndependence type. To avoid their bias, we will focus on novel more agnostic\napproach: moving estimator, which estimates parameters separately for every\ntime $t$: optimizing $F_t=\\sum_{\\tau<t} (1-\\eta)^{t-\\tau} \\ln(\\rho_\\theta\n(x_\\tau))$ local log-likelihood with exponentially weakening weights of the old\nvalues. In practice such moving estimates can be found by EMA (exponential\nmoving average) of some parameters, like $m_p=E[|x-\\mu|^p]$ absolute central\nmoments, updated by $m_{p,t+1} = m_{p,t} + \\eta (|x_t-\\mu_t|^p-m_{p,t})$. We\nwill focus here on its applications for alpha-Stable distribution, which also\ninfluences Hurst exponent, hence can be used for its adaptive estimation. Its\napplication will be shown on financial data as DJIA time series - beside\nstandard estimation of evolution of center $\\mu$ and scale parameter $\\sigma$,\nthere is also estimated evolution of $\\alpha$ parameter allowing to\ncontinuously evaluate market stability - tails having $\\rho(x) \\sim\n1/|x|^{\\alpha+1}$ behavior, controlling probability of potentially dangerous\nextreme events.", "AI": {"tldr": "The paper proposes using a moving estimator technique to analyze nonstationary time series, particularly focusing on adaptive parameters in alpha-Stable distributions for financial data.", "motivation": "Traditional methods like ARMA-ARCH assume specific dependency structures, which may introduce bias. The paper aims to develop a more agnostic approach to adapt to real-life time series nonstationarity.", "method": "The method utilizes a moving estimator based on a local log-likelihood approach with exponentially weakening weights, implemented using Exponential Moving Average (EMA) of distributional parameters.", "result": "The moving estimator successfully models the evolution of parameters in alpha-Stable distributions, including center, scale ($\\mu$, $\\sigma$), and tail behavior controlled by the $\\alpha$ parameter. Applications on DJIA time series demonstrate effective adaptive market stability evaluation.", "conclusion": "The proposed approach provides a novel adaptive method to analyze real-world nonstationary time series, particularly for financial data, offering insights into market stability and likelihood of extreme events."}}
{"id": "2506.05363", "pdf": "https://arxiv.org/pdf/2506.05363", "abs": "https://arxiv.org/abs/2506.05363", "authors": ["Yui Tatsumi", "Ziyue Zeng", "Hiroshi Watanabe"], "title": "Seed Selection for Human-Oriented Image Reconstruction via Guided Diffusion", "categories": ["cs.CV"], "comment": null, "summary": "Conventional methods for scalable image coding for humans and machines\nrequire the transmission of additional information to achieve scalability. A\nrecent diffusion-based method avoids this by generating human-oriented images\nfrom machine-oriented images without extra bitrate. This method, however, uses\na single random seed, which may lead to suboptimal image quality. In this\npaper, we propose a seed selection method that identifies the optimal seed from\nmultiple candidates to improve image quality without increasing the bitrate. To\nreduce computational cost, the selection is performed based on intermediate\noutputs obtained from early steps of the reverse diffusion process.\nExperimental results demonstrate that our method outperforms the baseline\nacross multiple metrics.", "AI": {"tldr": "The paper proposes using optimal seed selection during the diffusion process to enhance image quality in scalable image coding for humans and machines, all without adding extra bitrate.", "motivation": "Conventional approaches to scalable image coding require additional data for scalability, while newer methods with diffusion avoid extra bitrate but suffer from randomness affecting image quality.", "method": "The authors develop a seed selection mechanism that evaluates multiple random seed candidates during early steps of the reverse diffusion process to optimize output quality without escalating computational demands.", "result": "Experimental evaluations show that the proposed method surpasses the baseline on several performance metrics, delivering better image quality.", "conclusion": "Optimal seed selection during the reverse diffusion phase successfully achieves higher-quality scalable image coding for humans and machines, maintaining the same bitrate and improving efficiency."}}
{"id": "2506.05387", "pdf": "https://arxiv.org/pdf/2506.05387", "abs": "https://arxiv.org/abs/2506.05387", "authors": ["Jaydip Sen", "Saptarshi Sengupta. Subhasis Dasgupta"], "title": "Advancing Decoding Strategies: Enhancements in Locally Typical Sampling for LLMs", "categories": ["cs.CL", "cs.AI"], "comment": "This is the accepted but pre-reviewed version of the chapter that has\n  been accepted for publication in the Springer volume 'Decision-Making in\n  Computational Intelligence-Based Systems,' edited by Witold Pedrycz, Gilberto\n  Rivera, Rose Ma Rodriguez, and Salvador Ibarra Martinez. The chapter is 39\n  pages long, and it contains 2 figures and 6 tables. This is NOT the final\n  camera-ready version", "summary": "This chapter explores advancements in decoding strategies for large language\nmodels (LLMs), focusing on enhancing the Locally Typical Sampling (LTS)\nalgorithm. Traditional decoding methods, such as top-k and nucleus sampling,\noften struggle to balance fluency, diversity, and coherence in text generation.\nTo address these challenges, Adaptive Semantic-Aware Typicality Sampling (ASTS)\nis proposed as an improved version of LTS, incorporating dynamic entropy\nthresholding, multi-objective scoring, and reward-penalty adjustments. ASTS\nensures contextually coherent and diverse text generation while maintaining\ncomputational efficiency. Its performance is evaluated across multiple\nbenchmarks, including story generation and abstractive summarization, using\nmetrics such as perplexity, MAUVE, and diversity scores. Experimental results\ndemonstrate that ASTS outperforms existing sampling techniques by reducing\nrepetition, enhancing semantic alignment, and improving fluency.", "AI": {"tldr": "The paper proposes Adaptive Semantic-Aware Typicality Sampling (ASTS), an enhancement of Locally Typical Sampling, to improve text generation from large language models (LLMs).", "motivation": "Traditional decoding methods for LLMs, like top-k and nucleus sampling, often fail to strike a balance between fluency, diversity, and coherence in text generation.", "method": "The ASTS algorithm is presented, featuring dynamic entropy thresholding, multi-objective scoring, and reward-penalty adjustments to ensure semantic alignment and coherent diversity.", "result": "ASTS outperforms existing methods by reducing repetition, improving semantic alignment, enhancing fluency, and maintaining computational efficiency across benchmarks like story generation and summarization.", "conclusion": "The ASTS algorithm represents a significant improvement for decoding strategies in LLMs, effectively addressing the limitations of traditional approaches while ensuring efficient and high-quality text generation."}}
{"id": "2506.05432", "pdf": "https://arxiv.org/pdf/2506.05432", "abs": "https://arxiv.org/abs/2506.05432", "authors": ["Yuxuan Yue", "Zukang Xu", "Zhihang Yuan", "Dawei Yang", "Jianglong Wu", "Liqiang Nie"], "title": "PCDVQ: Enhancing Vector Quantization for Large Language Models via Polar Coordinate Decoupling", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) face significant challenges in edge deployment\ndue to their massive parameter scale. Vector Quantization (VQ), a\nclustering-based quantization method, serves as a prevalent solution to this\nissue for its extremely low-bit (even at 2-bit) and considerable accuracy.\nSince a vector is a quantity in mathematics and physics that has both direction\nand magnitude, existing VQ works typically quantize them in a coupled manner.\nHowever, we find that direction exhibits significantly greater sensitivity to\nquantization compared to the magnitude. For instance, when separately\nclustering the directions and magnitudes of weight vectors in LLaMA-2-7B, the\naccuracy drop of zero-shot tasks are 46.5\\% and 2.3\\%, respectively. This gap\neven increases with the reduction of clustering centers. Further, Euclidean\ndistance, a common metric to access vector similarities in current VQ works,\nplaces greater emphasis on reducing the magnitude error. This property is\ncontrary to the above finding, unavoidably leading to larger quantization\nerrors. To these ends, this paper proposes Polar Coordinate Decoupled Vector\nQuantization (PCDVQ), an effective and efficient VQ framework consisting of two\nkey modules: 1) Polar Coordinate Decoupling (PCD), which transforms vectors\ninto their polar coordinate representations and perform independent\nquantization of the direction and magnitude parameters.2) Distribution Aligned\nCodebook Construction (DACC), which optimizes the direction and magnitude\ncodebooks in accordance with the source distribution. Experimental results show\nthat PCDVQ outperforms baseline methods at 2-bit level by at least 1.5\\%\nzero-shot accuracy, establishing a novel paradigm for accurate and highly\ncompressed LLMs.", "AI": {"tldr": "The paper introduces Polar Coordinate Decoupled Vector Quantization (PCDVQ), which decouples vector direction and magnitude for better quantization of large language model parameters, achieving higher compression and accuracy.", "motivation": "Large Language Models (LLMs) face challenges in edge deployment due to massive parameter sizes. The paper explores improving Vector Quantization (VQ) methods to enhance accuracy while maintaining low-bit compression.", "method": "The paper proposes PCDVQ, consisting of two modules: Polar Coordinate Decoupling (PCD) for separate quantization of direction and magnitude, and Distribution Aligned Codebook Construction (DACC) to optimize codebooks based on source distributions.", "result": "PCDVQ demonstrates superior zero-shot accuracy by at least 1.5% compared to baseline methods at 2-bit quantization.", "conclusion": "PCDVQ provides an effective framework for achieving accurate and highly compressed LLMs, presenting a novel quantization approach using polar coordinates."}}
{"id": "2506.05640", "pdf": "https://arxiv.org/pdf/2506.05640", "abs": "https://arxiv.org/abs/2506.05640", "authors": ["Md Jueal Mia", "M. Hadi Amini"], "title": "FedShield-LLM: A Secure and Scalable Federated Fine-Tuned Large Language Model", "categories": ["cs.CR", "cs.DC"], "comment": null, "summary": "Federated Learning (FL) offers a decentralized framework for training and\nfine-tuning Large Language Models (LLMs) by leveraging computational resources\nacross organizations while keeping sensitive data on local devices. It\naddresses privacy and security concerns while navigating challenges associated\nwith the substantial computational demands of LLMs, which can be prohibitive\nfor small and medium-sized organizations. FL supports the development of\ntask-specific LLMs for cross-silo applications through fine-tuning but remains\nvulnerable to inference attacks, such as membership inference and gradient\ninversion, which threaten data privacy. Prior studies have utilized\nDifferential Privacy (DP) in LLM fine-tuning, which, despite being effective at\npreserving privacy, can degrade model performance. To overcome these\nchallenges, we propose a novel method, FedShield-LLM, that uses pruning with\nFully Homomorphic Encryption (FHE) for Low-Rank Adaptation (LoRA) parameters,\nenabling secure computations on encrypted model updates while mitigating the\nattack surface by deactivating less important LoRA parameters. Furthermore,\noptimized federated algorithms for cross-silo environments enhance scalability\nand efficiency. Parameter-efficient fine-tuning techniques like LoRA\nsubstantially reduce computational and communication overhead, making FL\nfeasible for resource-constrained clients. Experimental results show that the\nproposed method outperforms existing methods while maintaining robust privacy\nprotection, enabling organizations to collaboratively train secure and\nefficient LLMs.\n  The code and data are available at,\nhttps://github.com/solidlabnetwork/fedshield-llm", "AI": {"tldr": "The paper introduces FedShield-LLM, which uses pruning with Fully Homomorphic Encryption (FHE) to securely fine-tune LLMs in Federated Learning systems while preserving data privacy and efficiency.", "motivation": "To address privacy and security concerns when fine-tuning LLMs in Federated Learning systems, especially for small and medium-sized organizations with limited computational resources.", "method": "The proposed method, FedShield-LLM, utilizes pruning with Fully Homomorphic Encryption (FHE) applied to Low-Rank Adaptation (LoRA) parameters, reducing the attack surface and enabling secure encrypted computations. Optimized federated algorithms further improve scalability and efficiency.", "result": "Experimental results demonstrate that FedShield-LLM surpasses existing methods in performance while ensuring robust privacy protection, making LLM training more feasible for resource-constrained organizations.", "conclusion": "FedShield-LLM effectively enables collaborative and privacy-preserving training of LLMs in Federated Learning environments, balancing security, efficiency, and effectiveness."}}
{"id": "2506.06134", "pdf": "https://arxiv.org/pdf/2506.06134", "abs": "https://arxiv.org/abs/2506.06134", "authors": ["Veronica Centorrino", "Francesco Bullo", "Giovanni Russo"], "title": "Similarity Matching Networks: Hebbian Learning and Convergence Over Multiple Time Scales", "categories": ["q-bio.NC", "cs.LG", "math.OC"], "comment": "28 pages, 9 figures", "summary": "A recent breakthrough in biologically-plausible normative frameworks for\ndimensionality reduction is based upon the similarity matching cost function\nand the low-rank matrix approximation problem. Despite clear biological\ninterpretation, successful application in several domains, and experimental\nvalidation, a formal complete convergence analysis remains elusive. Building on\nthis framework, we consider and analyze a continuous-time neural network, the\n\\emph{similarity matching network}, for principal subspace projection. Derived\nfrom a min-max-min objective, this biologically-plausible network consists of\nthree coupled dynamics evolving at different time scales: neural dynamics,\nlateral synaptic dynamics, and feedforward synaptic dynamics at the fast,\nintermediate, and slow time scales, respectively. The feedforward and lateral\nsynaptic dynamics consist of Hebbian and anti-Hebbian learning rules,\nrespectively. By leveraging a multilevel optimization framework, we prove\nconvergence of the dynamics in the offline setting. Specifically, at the first\nlevel (fast time scale), we show strong convexity of the cost function and\nglobal exponential convergence of the corresponding gradient-flow dynamics. At\nthe second level (intermediate time scale), we prove strong concavity of the\ncost function and exponential convergence of the corresponding gradient-flow\ndynamics within the space of positive definite matrices. At the third and final\nlevel (slow time scale), we study a non-convex and non-smooth cost function,\nprovide explicit expressions for its global minima, and prove almost sure\nconvergence of the corresponding gradient-flow dynamics to the global minima.\nThese results rely on two empirically motivated conjectures that are supported\nby thorough numerical experiments. Finally, we validate the effectiveness of\nour approach via a numerical example.", "AI": {"tldr": "The paper proposes a biologically-plausible similarity matching network for principal subspace projection, analyzes its coupled dynamics across multiple time scales, and proves convergence under specific conditions.", "motivation": "To address the lack of formal convergence analysis in biologically-plausible frameworks for dimensionality reduction, specifically in the similarity matching cost function and low-rank matrix approximation.", "method": "The authors develop a continuous-time neural network with three coupled dynamics (fast, intermediate, slow) governed by Hebbian and anti-Hebbian learning rules. They perform a multilevel optimization analysis to demonstrate convergence properties, relying on certain conjectures and numerical validation.", "result": "The paper proves global exponential and almost sure convergence of the network's dynamics to optimal solutions across the different time scales, supported by numerical experiments.", "conclusion": "This work provides theoretical guarantees for the biologically-inspired similarity matching network, enhancing its applicability for principal subspace projection and demonstrating its potential through numerical validation."}}
{"id": "2506.05529", "pdf": "https://arxiv.org/pdf/2506.05529", "abs": "https://arxiv.org/abs/2506.05529", "authors": ["Rodney Sanchez", "Ferat Sahin", "Alexander Ororbia", "Jamison Heard"], "title": "Avoiding Death through Fear Intrinsic Conditioning", "categories": ["cs.AI", "cs.LG", "I.2.0; I.2.8"], "comment": null, "summary": "Biological and psychological concepts have inspired reinforcement learning\nalgorithms to create new complex behaviors that expand agents' capacity. These\nbehaviors can be seen in the rise of techniques like goal decomposition,\ncurriculum, and intrinsic rewards, which have paved the way for these complex\nbehaviors. One limitation in evaluating these methods is the requirement for\nengineered extrinsic for realistic environments. A central challenge in\nengineering the necessary reward function(s) comes from these environments\ncontaining states that carry high negative rewards, but provide no feedback to\nthe agent. Death is one such stimuli that fails to provide direct feedback to\nthe agent. In this work, we introduce an intrinsic reward function inspired by\nearly amygdala development and produce this intrinsic reward through a novel\nmemory-augmented neural network (MANN) architecture. We show how this intrinsic\nmotivation serves to deter exploration of terminal states and results in\navoidance behavior similar to fear conditioning observed in animals.\nFurthermore, we demonstrate how modifying a threshold where the fear response\nis active produces a range of behaviors that are described under the paradigm\nof general anxiety disorders (GADs). We demonstrate this behavior in the\nMiniworld Sidewalk environment, which provides a partially observable Markov\ndecision process (POMDP) and a sparse reward with a non-descriptive terminal\ncondition, i.e., death. In effect, this study results in a\nbiologically-inspired neural architecture and framework for fear conditioning\nparadigms; we empirically demonstrate avoidance behavior in a constructed agent\nthat is able to solve environments with non-descriptive terminal conditions.", "AI": {"tldr": "The paper introduces a biologically-inspired intrinsic reward function using a memory-augmented neural network to model avoidance behavior in agents, tackling environments with non-descriptive terminal states like death.", "motivation": "The paper aims to address the challenge of engineering reward functions for environments that include non-descriptive terminal states, where agents lack feedback, such as death.", "method": "The study introduces an intrinsic reward inspired by early amygdala development and employs a memory-augmented neural network (MANN) architecture to model avoidance behavior through fear conditioning.", "result": "The intrinsic motivation resulted in avoidance behavior similar to fear conditioning in animals. It also demonstrated how varying the fear response threshold could model behaviors seen in General Anxiety Disorders (GADs).", "conclusion": "A biologically-inspired neural architecture was developed, enabling agents to solve environments with sparse rewards and non-descriptive terminal conditions, mimicking behaviors observed in animals."}}
{"id": "2506.05817", "pdf": "https://arxiv.org/pdf/2506.05817", "abs": "https://arxiv.org/abs/2506.05817", "authors": ["Zihan Wang", "Siyao Liu", "Yang Sun", "Hongyan Li", "Kai Shen"], "title": "CodeContests+: High-Quality Test Case Generation for Competitive Programming", "categories": ["cs.SE", "cs.CL"], "comment": "28 pages, 7 figures", "summary": "Competitive programming, due to its high reasoning difficulty and precise\ncorrectness feedback, has become a key task for both training and evaluating\nthe reasoning capabilities of large language models (LLMs). However, while a\nlarge amount of public problem data, such as problem statements and solutions,\nis available, the test cases of these problems are often difficult to obtain.\nTherefore, test case generation is a necessary task for building large-scale\ndatasets, and the quality of the test cases directly determines the accuracy of\nthe evaluation. In this paper, we introduce an LLM-based agent system that\ncreates high-quality test cases for competitive programming problems. We apply\nthis system to the CodeContests dataset and propose a new version with improved\ntest cases, named CodeContests+. We evaluated the quality of test cases in\nCodeContestsPlus. First, we used 1.72 million submissions with pass/fail labels\nto examine the accuracy of these test cases in evaluation. The results\nindicated that CodeContests+ achieves significantly higher accuracy than\nCodeContests, particularly with a notably higher True Positive Rate (TPR).\nSubsequently, our experiments in LLM Reinforcement Learning (RL) further\nconfirmed that improvements in test case quality yield considerable advantages\nfor RL.", "AI": {"tldr": "The paper presents an LLM-based system for generating high-quality test cases for competitive programming problems, leading to better evaluation accuracy in datasets.", "motivation": "Competitive programming is a key area for training and assessing reasoning skills of LLMs, but quality test cases are often unavailable, necessitating their generation for accurate evaluations.", "method": "An LLM-based agent system generates enhanced test cases, applied to the CodeContests dataset to create an improved version, CodeContests+, assessed using pass/fail labels from 1.72 million submissions.", "result": "CodeContests+ showed notably higher evaluation accuracy and True Positive Rate compared to CodeContests. LLM Reinforcement Learning experiments confirmed test case quality improvements benefit RL performance.", "conclusion": "Higher quality test cases improve LLM reasoning evaluation and RL applications, demonstrating the potential of LLM systems for generating valuable test cases for programming datasets."}}
{"id": "2506.05741", "pdf": "https://arxiv.org/pdf/2506.05741", "abs": "https://arxiv.org/abs/2506.05741", "authors": ["Mohammadnavid Golchin"], "title": "A Soft Robotic Module with Pneumatic Actuation and Enhanced Controllability Using a Shape Memory Alloy Wire", "categories": ["cs.RO"], "comment": null, "summary": "In this paper, a compressed air-actuated soft robotic module was developed by\nincorporating a shape memory alloy (SMA) wire into its structure to achieve the\ndesired bending angle with greater precision. First, a fiber-reinforced bending\nmodule with a strain-limiting layer made of polypropylene was fabricated. The\nSMA wire was then placed in a silicon matrix, which was used as a new\nstrain-limiting layer. A simple closed-loop control algorithm was used to\nregulate the bending angle of the soft robot within its workspace. A camera was\nutilized to measure the angular changes in the vertical plane. Different\nangles, ranging from 0 to 65 degrees, were covered to evaluate the performance\nof the module and the bending angle control algorithm. The experimental tests\ndemonstrate that using the SMA wire results in more precise control of bending\nin the vertical plane. In addition, it is possible to bend more with less\nworking pressure. The error range was reduced from an average of 5 degrees to 2\ndegrees, and the rise time was reduced from an average of 19 seconds to 3\nseconds.", "AI": {"tldr": "This paper introduces a soft robotic module employing a shape memory alloy wire for better precision in controlled bending. Experimental results show improvements in error range and response time.", "motivation": "The motivation behind this study is to enhance the precision and performance of bending control in soft robots, addressing issues in working pressure and error tolerance.", "method": "The researchers developed a fiber-reinforced bending module with a new strain-limiting layer incorporating SMA wires. A closed-loop control algorithm and camera monitoring approach enabled precise angle adjustments.", "result": "Experimental tests revealed improved performance, including reduced bending error range (from 5\u00b0 to 2\u00b0) and faster rise time (from 19s to 3s), while requiring less working pressure.", "conclusion": "Incorporating SMA wires into soft robotics enhances bending precision and reduces the need for higher working pressures, demonstrating their value in robotic applications."}}
{"id": "2506.05454", "pdf": "https://arxiv.org/pdf/2506.05454", "abs": "https://arxiv.org/abs/2506.05454", "authors": ["Liang Zhang", "Bingcong Li", "Kiran Koshy Thekumparampil", "Sewoong Oh", "Michael Muehlebach", "Niao He"], "title": "Zeroth-Order Optimization Finds Flat Minima", "categories": ["cs.LG", "cs.AI", "math.OC", "stat.ML"], "comment": null, "summary": "Zeroth-order methods are extensively used in machine learning applications\nwhere gradients are infeasible or expensive to compute, such as black-box\nattacks, reinforcement learning, and language model fine-tuning. Existing\noptimization theory focuses on convergence to an arbitrary stationary point,\nbut less is known on the implicit regularization that provides a fine-grained\ncharacterization on which particular solutions are finally reached. We show\nthat zeroth-order optimization with the standard two-point estimator favors\nsolutions with small trace of Hessian, which is widely used in previous work to\ndistinguish between sharp and flat minima. We further provide convergence rates\nof zeroth-order optimization to approximate flat minima for convex and\nsufficiently smooth functions, where flat minima are defined as the minimizers\nthat achieve the smallest trace of Hessian among all optimal solutions.\nExperiments on binary classification tasks with convex losses and language\nmodel fine-tuning support our theoretical findings.", "AI": {"tldr": "The paper explores implicit regularization in zeroth-order optimization, showing it favors flat minima and provides theoretical and experimental evidence.", "motivation": "To better understand zeroth-order optimization beyond convergence to stationary points, focusing on the implicit regularization that guides solution selection.", "method": "Analyzed zeroth-order optimization using the two-point estimator and derived theoretical convergence rates for flat minima in convex and smooth functions.", "result": "Demonstrated that zeroth-order optimization favors solutions with small Hessian trace, validated by experiments on binary classification and language model fine-tuning.", "conclusion": "Zeroth-order optimization inherently biases toward flat minima, offering insights into its applications and performance in machine learning tasks."}}
{"id": "2506.05367", "pdf": "https://arxiv.org/pdf/2506.05367", "abs": "https://arxiv.org/abs/2506.05367", "authors": ["Aakash Garg", "Libing Zeng", "Andrii Tsarov", "Nima Khademi Kalantari"], "title": "Text2Stereo: Repurposing Stable Diffusion for Stereo Generation with Consistency Rewards", "categories": ["cs.CV"], "comment": null, "summary": "In this paper, we propose a novel diffusion-based approach to generate stereo\nimages given a text prompt. Since stereo image datasets with large baselines\nare scarce, training a diffusion model from scratch is not feasible. Therefore,\nwe propose leveraging the strong priors learned by Stable Diffusion and\nfine-tuning it on stereo image datasets to adapt it to the task of stereo\ngeneration. To improve stereo consistency and text-to-image alignment, we\nfurther tune the model using prompt alignment and our proposed stereo\nconsistency reward functions. Comprehensive experiments demonstrate the\nsuperiority of our approach in generating high-quality stereo images across\ndiverse scenarios, outperforming existing methods.", "AI": {"tldr": "This paper introduces a diffusion-based approach to generate stereo images from text prompts using fine-tuned Stable Diffusion.", "motivation": "Stereo image generation from text prompts is challenging due to limited datasets and the need for maintaining stereo consistency.", "method": "The authors fine-tuned Stable Diffusion on stereo image datasets and incorporated stereo consistency reward functions and prompt alignment techniques.", "result": "Experiments showed that the proposed method generates high-quality stereo images with improved accuracy compared to existing approaches.", "conclusion": "The approach is effective for generating stereo images from text and offers advancements in stereo consistency and text-to-image alignment."}}
{"id": "2506.05388", "pdf": "https://arxiv.org/pdf/2506.05388", "abs": "https://arxiv.org/abs/2506.05388", "authors": ["Stefanie Urchs", "Veronika Thurner", "Matthias A\u00dfenmacher", "Christian Heumann", "Stephanie Thiemichen"], "title": "taz2024full: Analysing German Newspapers for Gender Bias and Discrimination across Decades", "categories": ["cs.CL"], "comment": "Accepted @ \"63rd Annual Meeting of the Association for Computational\n  Linguistics (ACL 2025)\" as a findings paper. This is the author's version of\n  the work. The definitive version of record will be published in the\n  proceedings", "summary": "Open-access corpora are essential for advancing natural language processing\n(NLP) and computational social science (CSS). However, large-scale resources\nfor German remain limited, restricting research on linguistic trends and\nsocietal issues such as gender bias. We present taz2024full, the largest\npublicly available corpus of German newspaper articles to date, comprising over\n1.8 million texts from taz, spanning 1980 to 2024.\n  As a demonstration of the corpus's utility for bias and discrimination\nresearch, we analyse gender representation across four decades of reporting. We\nfind a consistent overrepresentation of men, but also a gradual shift toward\nmore balanced coverage in recent years. Using a scalable, structured analysis\npipeline, we provide a foundation for studying actor mentions, sentiment, and\nlinguistic framing in German journalistic texts.\n  The corpus supports a wide range of applications, from diachronic language\nanalysis to critical media studies, and is freely available to foster inclusive\nand reproducible research in German-language NLP.", "AI": {"tldr": "The paper introduces taz2024full, the largest open-access corpus of German newspaper articles (1980\u20132024), and demonstrates its utility in bias and discrimination research, particularly on gender representation.", "motivation": "To address the lack of large-scale open-access corpora for German, which limits research in NLP and CSS, particularly in studying linguistic trends and societal issues like gender bias.", "method": "The authors created taz2024full, a corpus of over 1.8 million German newspaper articles, and developed a structured analysis pipeline to evaluate topics like gender representation, sentiment, and linguistic framing.", "result": "The analysis shows a historical overrepresentation of men in media coverage, with gradual improvements toward balanced gender representation in recent years.", "conclusion": "The taz2024full corpus promotes inclusive and reproducible research in German NLP and offers valuable insights for language analysis and critical media studies."}}
{"id": "2506.05433", "pdf": "https://arxiv.org/pdf/2506.05433", "abs": "https://arxiv.org/abs/2506.05433", "authors": ["Zikang Liu", "Tongtian Yue", "Yepeng Tang", "Longteng Guo", "Junxian Cai", "Qingbin Liu", "Xi Chen", "Jing Liu"], "title": "Prefix Grouper: Efficient GRPO Training through Shared-Prefix Forward", "categories": ["cs.LG", "cs.AI"], "comment": "10 pages, technical report", "summary": "Group Relative Policy Optimization (GRPO) enhances policy learning by\ncomputing gradients from relative comparisons among candidate outputs that\nshare a common input prefix. Despite its effectiveness, GRPO introduces\nsubstantial computational overhead when processing long shared prefixes, which\nmust be redundantly encoded for each group member. This inefficiency becomes a\nmajor scalability bottleneck in long-context learning scenarios. We propose\nPrefix Grouper, an efficient GRPO training algorithm that eliminates redundant\nprefix computation via a Shared-Prefix Forward strategy. In particular, by\nrestructuring self-attention into two parts, our method enables the shared\nprefix to be encoded only once, while preserving full differentiability and\ncompatibility with end-to-end training. We provide both theoretical and\nempirical evidence that Prefix Grouper is training-equivalent to standard GRPO:\nit yields identical forward outputs and backward gradients, ensuring that the\noptimization dynamics and final policy performance remain unchanged.\nEmpirically, our experiments confirm that Prefix Grouper achieves consistent\nresults while significantly reducing the computational cost of training,\nparticularly in long-prefix scenarios. The proposed method is fully\nplug-and-play: it is compatible with existing GRPO-based architectures and can\nbe seamlessly integrated into current training pipelines as a drop-in\nreplacement, requiring no structural modifications and only minimal changes to\ninput construction and attention computation. Prefix Grouper enables the use of\nlarger group sizes under the same computational budget, thereby improving the\nscalability of GRPO to more complex tasks and larger models. Code is now\navailable at https://github.com/johncaged/PrefixGrouper", "AI": {"tldr": "Prefix Grouper is an efficient algorithm for improving the scalability of GRPO in long-context scenarios by eliminating redundant prefix computations, maintaining training equivalence and reducing computational costs.", "motivation": "GRPO is effective for policy learning but introduces substantial computational inefficiencies when handling long shared prefixes, creating scalability challenges.", "method": "The Prefix Grouper algorithm restructures self-attention into two parts, enabling shared prefixes to be encoded only once, which eliminates computational redundancy while maintaining full differentiability and training equivalence with GRPO.", "result": "Experiments confirm that Prefix Grouper achieves consistent policy learning results while significantly reducing computational costs, especially in long-prefix scenarios.", "conclusion": "Prefix Grouper improves GRPO scalability and training efficiency, allowing the use of larger group sizes for complex tasks without compromising policy performance. The approach is plug-and-play, requiring minimal adjustments to existing architectures."}}
{"id": "2506.05634", "pdf": "https://arxiv.org/pdf/2506.05634", "abs": "https://arxiv.org/abs/2506.05634", "authors": ["Saeed Hedayatian", "Stefanos Nikolaidis"], "title": "AutoQD: Automatic Discovery of Diverse Behaviors with Quality-Diversity Optimization", "categories": ["cs.LG", "cs.AI", "cs.NE"], "comment": "22 pages, 5 figures", "summary": "Quality-Diversity (QD) algorithms have shown remarkable success in\ndiscovering diverse, high-performing solutions, but rely heavily on\nhand-crafted behavioral descriptors that constrain exploration to predefined\nnotions of diversity. Leveraging the equivalence between policies and occupancy\nmeasures, we present a theoretically grounded approach to automatically\ngenerate behavioral descriptors by embedding the occupancy measures of policies\nin Markov Decision Processes. Our method, AutoQD, leverages random Fourier\nfeatures to approximate the Maximum Mean Discrepancy (MMD) between policy\noccupancy measures, creating embeddings whose distances reflect meaningful\nbehavioral differences. A low-dimensional projection of these embeddings that\ncaptures the most behaviorally significant dimensions is then used as\nbehavioral descriptors for off-the-shelf QD methods. We prove that our\nembeddings converge to true MMD distances between occupancy measures as the\nnumber of sampled trajectories and embedding dimensions increase. Through\nexperiments in multiple continuous control tasks we demonstrate AutoQD's\nability in discovering diverse policies without predefined behavioral\ndescriptors, presenting a well-motivated alternative to prior methods in\nunsupervised Reinforcement Learning and QD optimization. Our approach opens new\npossibilities for open-ended learning and automated behavior discovery in\nsequential decision making settings without requiring domain-specific\nknowledge.", "AI": {"tldr": "AutoQD introduces a method to automatically create behavioral descriptors for Quality-Diversity (QD) algorithms by embedding policy occupancy measures in Markov Decision Processes, overcoming the need for hand-crafted descriptors.", "motivation": "Existing QD algorithms rely on predefined behavioral descriptors, which constrain exploration and require domain-specific knowledge. A new approach is needed for automated and meaningful behavior discovery.", "method": "AutoQD uses random Fourier features to approximate the Maximum Mean Discrepancy (MMD) between policy occupancy measures, generating embeddings that represent behavioral differences. These embeddings are transformed into low-dimensional behavioral descriptors suitable for standard QD methods.", "result": "Experiments across multiple continuous control tasks confirm that AutoQD successfully discovers diverse and high-performing policies without relying on predefined behavioral descriptors, proving its utility in unsupervised reinforcement learning.", "conclusion": "AutoQD provides a theoretically sound and practical framework for automated behavior discovery in sequential decision-making, eliminating the reliance on domain-specific, hand-crafted descriptors and enabling open-ended learning."}}
{"id": "2506.06191", "pdf": "https://arxiv.org/pdf/2506.06191", "abs": "https://arxiv.org/abs/2506.06191", "authors": ["Kent M. Lee", "Joshua Rodriguez", "Ludger Hartley", "Philip A. Kragel", "Lorena Chanes", "Tor D. Wager", "Karen S. Quigley", "Lawrence L. Wald", "Marta Bianciardi", "Lisa Feldman Barrett", "Jordan E. Theriault", "Ajay B. Satpute"], "title": "Functional Architecture of the Human Hypothalamus: Cortical Coupling and Subregional Organization Using 7-Tesla fMRI", "categories": ["q-bio.NC", "q-bio.QM"], "comment": "36 pages, 1 table, 4 figures, 1 supplementary figure", "summary": "The hypothalamus plays an important role in the regulation of the bodys\nmetabolic state and behaviors related to survival. Despite its importance\nhowever, many questions exist regarding the intrinsic and extrinsic connections\nof the hypothalamus in humans, especially its relationship with the cortex. As\na heterogeneous structure, it is possible that the hypothalamus is composed of\ndifferent subregions, which have their own distinct relationships with the\ncortex. Previous work on functional connectivity in the human hypothalamus have\neither treated it as a unitary structure or relied on methodological approaches\nthat are limited in modeling its intrinsic functional architecture. Here, we\nused resting state data from ultrahigh field 7 Tesla fMRI and a data driven\nanalytical approach to identify functional subregions of the human\nhypothalamus. Our approach identified four functional hypothalamic subregions\nbased on intrinsic functional connectivity, which in turn showed distinct\npatterns of functional connectivity with cortex. Overall, all hypothalamic\nsubregions showed stronger connectivity with a cortical network, Cortical\nNetwork 1 composed primarily of frontal, midline, and limbic cortical areas and\nweaker connectivity with a second cortical network composed largely of\nposterior sensorimotor regions, Cortical Network 2. Of the hypothalamic\nsubregions, the anterior hypothalamus showed the strongest connection to\nCortical Network 1, while a more ventral subregion containing the anterior\nhypothalamus extending to the tuberal region showed the weakest connectivity.\nThe findings support the use of ultrahigh field, high resolution imaging in\nproviding a more incisive investigation of the human hypothalamus that respects\nits complex internal structure and extrinsic functional architecture.", "AI": {"tldr": "The study used ultrahigh field 7 Tesla fMRI to identify four distinct functional subregions in the hypothalamus based on intrinsic connectivity and their distinct relationships with cortical networks.", "motivation": "Understanding the hypothalamus's intrinsic structure and its specific functional connectivity with the cortex is pivotal for delineating its role in regulating survival behaviors and the body's metabolic state.", "method": "The researchers utilized ultrahigh field 7 Tesla resting-state fMRI and a data-driven analytical approach to distinguish functional subregions in the human hypothalamus and analyze their cortical connectivity.", "result": "Four distinct hypothalamic subregions were identified, each with unique connectivity patterns primarily involving a dominant Cortical Network 1 and a weaker connection to Cortical Network 2.", "conclusion": "The study highlights the utility of ultrahigh field fMRI in uncovering the functional architecture of the hypothalamus and its distinctive cortical connections, emphasizing its internal complexity."}}
{"id": "2506.05579", "pdf": "https://arxiv.org/pdf/2506.05579", "abs": "https://arxiv.org/abs/2506.05579", "authors": ["Quan Shi", "Carlos E. Jimenez", "Shunyu Yao", "Nick Haber", "Diyi Yang", "Karthik Narasimhan"], "title": "When Models Know More Than They Can Explain: Quantifying Knowledge Transfer in Human-AI Collaboration", "categories": ["cs.AI", "cs.CL", "cs.HC"], "comment": "For code, data, visualizer, visit: https:kite-live.vercel.app", "summary": "Recent advancements in AI reasoning have driven substantial improvements\nacross diverse tasks. A critical open question is whether these improvements\nalso yields better knowledge transfer: the ability of models to communicate\nreasoning in ways humans can understand, apply, and learn from. To investigate\nthis, we introduce Knowledge Integration and Transfer Evaluation (KITE), a\nconceptual and experimental framework for Human-AI knowledge transfer\ncapabilities and conduct the first large-scale human study (N=118) explicitly\ndesigned to measure it. In our two-phase setup, humans first ideate with an AI\non problem-solving strategies, then independently implement solutions,\nisolating model explanations' influence on human understanding. Our findings\nreveal that although model benchmark performance correlates with collaborative\noutcomes, this relationship is notably inconsistent, featuring significant\noutliers, indicating that knowledge transfer requires dedicated optimization.\nOur analysis identifies behavioral and strategic factors mediating successful\nknowledge transfer. We release our code, dataset, and evaluation framework to\nsupport future work on communicatively aligned models.", "AI": {"tldr": "This study examines the ability of AI models to transfer knowledge to humans effectively and proposes a new framework, KITE, for investigating this.", "motivation": "The researchers aim to evaluate whether advancements in AI reasoning also yield better human-understandable and applicable knowledge transfer.", "method": "The team developed KITE, a framework for assessing AI-human knowledge transfer, and conducted a large-scale human study involving two phases: ideating strategies with AI and independently implementing solutions.", "result": "The study found that while model performance correlates with collaborative success, the connection is inconsistent, highlighting the need for dedicated optimization. Behavioral and strategic factors were identified as key mediators of effective knowledge transfer.", "conclusion": "Knowledge transfer from AI to humans is not inherently tied to model performance and requires specific optimizations. The study provides tools and data to promote further research in this area."}}
{"id": "2506.05822", "pdf": "https://arxiv.org/pdf/2506.05822", "abs": "https://arxiv.org/abs/2506.05822", "authors": ["Lucas Mauser", "Eva Zimmermann", "Pavel Nedv\u011bdick\u00fd", "Tobias Eisenreich", "Moritz W\u00e4schle", "Stefan Wagner"], "title": "Towards Mixed-Criticality Software Architectures for Centralized HPC Platforms in Software-Defined Vehicles: A Systematic Literature Review", "categories": ["cs.SE"], "comment": "Preprint for research paper track of ECSA 2025", "summary": "Centralized electrical/electronic architectures and High-Performance\nComputers (HPCs) are redefining automotive software development, challenging\ntraditional microcontroller-based approaches. Ensuring real-time, safety, and\nscalability in software-defined vehicles necessitates reevaluating how\nmixed-criticality software is integrated into centralized architectures. While\nexisting research on automotive SoftWare Architectures (SWAs) is relevant to\nthe industry, it often lacks validation through systematic, empirical methods.\nTo address this gap, we conduct a systematic literature review focusing on\nautomotive mixed-criticality SWAs. Our goal is to provide practitioner-oriented\nguidelines that assist automotive software architects and developers design\ncentralized, mixed-criticality SWAs based on a rigorous and transparent\nmethodology. First, we set up a systematic review protocol grounded in\nestablished guidelines. Second, we apply this protocol to identify relevant\nstudies. Third, we extract key functional domains, constraints, and enabling\ntechnologies that drive changes in automotive SWAs, thereby assessing the\nprotocol's effectiveness. Additionally, we extract techniques, architectural\npatterns, and design practices for integrating mixed-criticality requirements\ninto HPC-based SWAs, further demonstrating the protocol's applicability. Based\non these insights, we propose an exemplary SWA for a microprocessor-based\nsystem-on-chip. In conclusion, this study provides a structured approach to\nexplore and realize mixed-criticality software integration for next-generation\nautomotive SWAs, offering valuable insights for industry and research\napplications.", "AI": {"tldr": "This paper conducts a systematic literature review to provide guidelines for integrating mixed-criticality software into centralized automotive software architectures.", "motivation": "The motivation is to address challenges in traditional microcontroller-based automotive software development by providing validated, systematic guidelines for mixed-criticality software integration in centralized architectures.", "method": "The paper follows a systematic review protocol to identify and analyze studies, extracting key domains, constraints, and enabling technologies for automotive software architectures.", "result": "The study identifies techniques, patterns, and practices for integrating mixed-criticality software into high-performance computing-based architectures, and proposes a sample architecture.", "conclusion": "This research advances methods for designing mixed-criticality software in centralized automotive architectures, aiding both industry and academic efforts."}}
{"id": "2506.05808", "pdf": "https://arxiv.org/pdf/2506.05808", "abs": "https://arxiv.org/abs/2506.05808", "authors": ["Yutaro Ishida", "Takamitsu Matsubara", "Takayuki Kanai", "Kazuhiro Shintani", "Hiroshi Bito"], "title": "Where Do We Look When We Teach? Analyzing Human Gaze Behavior Across Demonstration Devices in Robot Imitation Learning", "categories": ["cs.RO"], "comment": null, "summary": "Imitation learning for acquiring generalizable policies often requires a\nlarge volume of demonstration data, making the process significantly costly.\nOne promising strategy to address this challenge is to leverage the cognitive\nand decision-making skills of human demonstrators with strong generalization\ncapability, particularly by extracting task-relevant cues from their gaze\nbehavior. However, imitation learning typically involves humans collecting data\nusing demonstration devices that emulate a robot's embodiment and visual\ncondition. This raises the question of how such devices influence gaze\nbehavior. We propose an experimental framework that systematically analyzes\ndemonstrators' gaze behavior across a spectrum of demonstration devices. Our\nexperimental results indicate that devices emulating (1) a robot's embodiment\nor (2) visual condition impair demonstrators' capability to extract\ntask-relevant cues via gaze behavior, with the extent of impairment depending\non the degree of emulation. Additionally, gaze data collected using devices\nthat capture natural human behavior improves the policy's task success rate\nfrom 18.8% to 68.8% under environmental shifts.", "AI": {"tldr": "This paper addresses the challenge of the costly data needs in imitation learning by exploring the influence of demonstration devices on human gaze behavior, finding that natural behavior improves policy success drastically.", "motivation": "Imitation learning relies heavily on extensive demonstration data, making it costly. Optimizing how to leverage humans' gaze behavior promises to enhance efficiency and generalization capabilities.", "method": "An experimental framework was developed to analyze how various demonstration devices affect gaze behavior, focusing on robot embodiment and visual condition emulation.", "result": "Emulation of a robot\u2019s embodiment or visual condition impairs task-relevant gaze behavior, while natural human gaze data improve task success rates significantly under environmental shifts (from 18.8% to 68.8%).", "conclusion": "Using devices that capture natural gaze behavior is critical for improving task success and overcoming limitations in conventional robot embodiment emulation setups."}}
{"id": "2506.05500", "pdf": "https://arxiv.org/pdf/2506.05500", "abs": "https://arxiv.org/abs/2506.05500", "authors": ["Alex Damian", "Jason D. Lee", "Joan Bruna"], "title": "The Generative Leap: Sharp Sample Complexity for Efficiently Learning Gaussian Multi-Index Models", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "In this work we consider generic Gaussian Multi-index models, in which the\nlabels only depend on the (Gaussian) $d$-dimensional inputs through their\nprojection onto a low-dimensional $r = O_d(1)$ subspace, and we study efficient\nagnostic estimation procedures for this hidden subspace. We introduce the\n\\emph{generative leap} exponent $k^\\star$, a natural extension of the\ngenerative exponent from [Damian et al.'24] to the multi-index setting. We\nfirst show that a sample complexity of $n=\\Theta(d^{1 \\vee \\k/2})$ is necessary\nin the class of algorithms captured by the Low-Degree-Polynomial framework. We\nthen establish that this sample complexity is also sufficient, by giving an\nagnostic sequential estimation procedure (that is, requiring no prior knowledge\nof the multi-index model) based on a spectral U-statistic over appropriate\nHermite tensors. We further compute the generative leap exponent for several\nexamples including piecewise linear functions (deep ReLU networks with bias),\nand general deep neural networks (with $r$-dimensional first hidden layer).", "AI": {"tldr": "The paper explores Gaussian Multi-index models, focusing on label dependency through low-dimensional subspace projections. It develops sample complexity bounds and proposes an efficient agnostic estimation method using spectral U-statistics and Hermite tensors.", "motivation": "The motivation is to study Gaussian Multi-index models and advance estimation methodologies for low-dimensional hidden subspaces, central to fields like machine learning and statistics.", "method": "The method includes analyzing sample complexity requirements using the Low-Degree-Polynomial framework and designing a sequential agnostic estimation procedure leveraging spectral U-statistics over Hermite tensors.", "result": "The authors identify optimal sample complexity conditions and compute generative leap exponents for applications such as piecewise linear functions and deep neural networks.", "conclusion": "This work establishes both theoretical sample complexity bounds and practical estimation mechanisms for Gaussian Multi-index models, expanding understanding in agnostic modeling and deep learning tasks."}}
{"id": "2506.05368", "pdf": "https://arxiv.org/pdf/2506.05368", "abs": "https://arxiv.org/abs/2506.05368", "authors": ["Valentine Bernasconi", "Gustavo Marfia"], "title": "Speaking images. A novel framework for the automated self-description of artworks", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent breakthroughs in generative AI have opened the door to new research\nperspectives in the domain of art and cultural heritage, where a large number\nof artifacts have been digitized. There is a need for innovation to ease the\naccess and highlight the content of digital collections. Such innovations\ndevelop into creative explorations of the digital image in relation to its\nmalleability and contemporary interpretation, in confrontation to the original\nhistorical object. Based on the concept of the autonomous image, we propose a\nnew framework towards the production of self-explaining cultural artifacts\nusing open-source large-language, face detection, text-to-speech and\naudio-to-animation models. The goal is to start from a digitized artwork and to\nautomatically assemble a short video of the latter where the main character\nanimates to explain its content. The whole process questions cultural biases\nencapsulated in large-language models, the potential of digital images and\ndeepfakes of artworks for educational purposes, along with concerns of the\nfield of art history regarding such creative diversions.", "AI": {"tldr": "This paper proposes a framework to create self-explaining cultural artifacts using generative AI technologies.", "motivation": "The motivation is to enhance access and interpretation of digital cultural heritage artifacts in innovative ways, particularly through creative manipulation of digital images.", "method": "The authors propose combining open-source tools like large-language models, face detection, text-to-speech, and audio-to-animation systems to automatically generate videos where characters from digitized artworks explain their own content.", "result": "The framework creates educational videos by animating artwork characters, providing an interactive way to understand digital cultural artifacts.", "conclusion": "This work explores both the potential and the ethical implications of using generative AI in the cultural heritage domain, aiming to enrich educational and interpretive strategies."}}
{"id": "2506.05390", "pdf": "https://arxiv.org/pdf/2506.05390", "abs": "https://arxiv.org/abs/2506.05390", "authors": ["Markelle Kelly", "Mohammad Tahaei", "Padhraic Smyth", "Lauren Wilcox"], "title": "Understanding Gender Bias in AI-Generated Product Descriptions", "categories": ["cs.CL", "cs.LG"], "comment": "Accepted to FAccT 2025", "summary": "While gender bias in large language models (LLMs) has been extensively\nstudied in many domains, uses of LLMs in e-commerce remain largely unexamined\nand may reveal novel forms of algorithmic bias and harm. Our work investigates\nthis space, developing data-driven taxonomic categories of gender bias in the\ncontext of product description generation, which we situate with respect to\nexisting general purpose harms taxonomies. We illustrate how AI-generated\nproduct descriptions can uniquely surface gender biases in ways that require\nspecialized detection and mitigation approaches. Further, we quantitatively\nanalyze issues corresponding to our taxonomic categories in two models used for\nthis task -- GPT-3.5 and an e-commerce-specific LLM -- demonstrating that these\nforms of bias commonly occur in practice. Our results illuminate unique,\nunder-explored dimensions of gender bias, such as assumptions about clothing\nsize, stereotypical bias in which features of a product are advertised, and\ndifferences in the use of persuasive language. These insights contribute to our\nunderstanding of three types of AI harms identified by current frameworks:\nexclusionary norms, stereotyping, and performance disparities, particularly for\nthe context of e-commerce.", "AI": {"tldr": "This paper investigates gender bias in AI-generated product descriptions in e-commerce, identifying unique bias dimensions and proposing taxonomic categories for detection and mitigation.", "motivation": "The authors aim to explore largely unexamined gender biases in e-commerce applications of large language models, addressing potential algorithmic harms and novel bias forms in AI-generated product descriptions.", "method": "The researchers developed taxonomic categories to classify gender bias in product descriptions, conducted data-driven analyses, and applied these categories to examine bias in models such as GPT-3.5 and e-commerce-specific LLMs.", "result": "The study reveals recurring gender biases in AI-produced descriptions, highlighting issues such as assumptions about clothing size, stereotypical biases, and disparities in persuasive language use.", "conclusion": "The findings advance understanding of gender bias in e-commerce, linking it to exclusionary norms, stereotyping, and performance disparities, and suggesting tailored approaches for detection and mitigation."}}
{"id": "2506.05434", "pdf": "https://arxiv.org/pdf/2506.05434", "abs": "https://arxiv.org/abs/2506.05434", "authors": ["Thomas Massena", "L\u00e9o and\u00e9ol", "Thibaut Boissin", "Franck Mamalet", "Corentin Friedrich", "Mathieu Serrurier", "S\u00e9bastien Gerchinovitz"], "title": "Efficient Robust Conformal Prediction via Lipschitz-Bounded Networks", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Conformal Prediction (CP) has proven to be an effective post-hoc method for\nimproving the trustworthiness of neural networks by providing prediction sets\nwith finite-sample guarantees. However, under adversarial attacks, classical\nconformal guarantees do not hold anymore: this problem is addressed in the\nfield of Robust Conformal Prediction. Several methods have been proposed to\nprovide robust CP sets with guarantees under adversarial perturbations, but,\nfor large scale problems, these sets are either too large or the methods are\ntoo computationally demanding to be deployed in real life scenarios. In this\nwork, we propose a new method that leverages Lipschitz-bounded networks to\nprecisely and efficiently estimate robust CP sets. When combined with a\n1-Lipschitz robust network, we demonstrate that our lip-rcp method outperforms\nstate-of-the-art results in both the size of the robust CP sets and\ncomputational efficiency in medium and large-scale scenarios such as ImageNet.\nTaking a different angle, we also study vanilla CP under attack, and derive new\nworst-case coverage bounds of vanilla CP sets, which are valid simultaneously\nfor all adversarial attack levels. Our lip-rcp method makes this second\napproach as efficient as vanilla CP while also allowing robustness guarantees.", "AI": {"tldr": "The paper introduces 'lip-rcp,' a method leveraging Lipschitz-bounded networks to create smaller, efficient Conformal Prediction (CP) sets that maintain robustness under adversarial attacks.", "motivation": "Current robust CP methods struggle in large-scale settings by producing overly large prediction sets or requiring significant computational resources.", "method": "The authors use Lipschitz-bounded networks, particularly 1-Lipschitz robust networks, to estimate robust CP sets. They also derive new worst-case coverage bounds for vanilla CP under adversarial attacks.", "result": "The proposed 'lip-rcp' method achieves superior performance in both robustness and computational efficiency when applied to medium and large-scale datasets like ImageNet.", "conclusion": "'Lip-rcp' provides a promising balance between robustness guarantees and practicality in real-world large-scale prediction tasks."}}
{"id": "2506.05977", "pdf": "https://arxiv.org/pdf/2506.05977", "abs": "https://arxiv.org/abs/2506.05977", "authors": ["Yujia Huo", "Jianchun Liu", "Hongli Xu", "Zhenguo Ma", "Shilong Wang", "Liusheng Huang"], "title": "Mitigating Catastrophic Forgetting with Adaptive Transformer Block Expansion in Federated Fine-Tuning", "categories": ["cs.LG", "cs.DC"], "comment": null, "summary": "Federated fine-tuning (FedFT) of large language models (LLMs) has emerged as\na promising solution for adapting models to distributed data environments while\nensuring data privacy.\n  Existing FedFT methods predominantly utilize parameter-efficient fine-tuning\n(PEFT) techniques to reduce communication and computation overhead.\n  However, they often fail to adequately address the catastrophic forgetting, a\ncritical challenge arising from continual adaptation in distributed\nenvironments. The traditional centralized fine-tuning methods, which are not\ndesigned for the heterogeneous and privacy-constrained nature of federated\nenvironments, struggle to mitigate this issue effectively. Moreover, the\nchallenge is further exacerbated by significant variation in data distributions\nand device capabilities across clients, which leads to intensified forgetting\nand degraded model generalization. To tackle these issues, we propose FedBE, a\nnovel FedFT framework that integrates an adaptive transformer block expansion\nmechanism with a dynamic trainable-block allocation strategy. Specifically,\nFedBE expands trainable blocks within the model architecture, structurally\nseparating newly learned task-specific knowledge from the original pre-trained\nrepresentations. Additionally, FedBE dynamically assigns these trainable blocks\nto clients based on their data distributions and computational capabilities.\nThis enables the framework to better accommodate heterogeneous federated\nenvironments and enhances the generalization ability of the model.Extensive\nexperiments show that compared with existing federated fine-tuning methods,\nFedBE achieves 12-74% higher accuracy retention on general tasks after\nfine-tuning and a model convergence acceleration ratio of 1.9-3.1x without\ndegrading the accuracy of downstream tasks.", "AI": {"tldr": "FedBE is an innovative framework for federated fine-tuning of large language models, addressing challenges such as catastrophic forgetting and heterogeneous environments through structural expansion and adaptive allocation.", "motivation": "The motivation is to solve issues in federated fine-tuning like catastrophic forgetting and adapting to heterogeneous data distributions and device capabilities.", "method": "FedBE employs transformer block expansion and adaptive block allocation strategies to segregate task-specific knowledge and optimize performance across varied environments.", "result": "FedBE delivers 12-74% better accuracy retention on general tasks, accelerates convergence by 1.9-3.1x, and maintains downstream accuracy.", "conclusion": "FedBE notably improves federated fine-tuning by effectively managing heterogeneous client environments and mitigating catastrophic forgetting."}}
{"id": "2506.06234", "pdf": "https://arxiv.org/pdf/2506.06234", "abs": "https://arxiv.org/abs/2506.06234", "authors": ["Caitlin Lienkaemper", "Gabriel Koch Ocker"], "title": "Diverse mean-field dynamics of clustered, inhibition-stabilized Hawkes networks via combinatorial threshold-linear networks", "categories": ["q-bio.NC", "92C20, 92B20, 37N25"], "comment": "20 pages, 7 figures", "summary": "Networks of interconnected neurons display diverse patterns of collective\nactivity. Relating this collective activity to the network's connectivity\nstructure is a key goal of computational neuroscience. We approach this\nquestion for clustered networks, which can form via biologically realistic\nlearning rules and allow for the re-activation of learned patterns. Previous\nstudies of clustered networks have focused on metastabilty between fixed\npoints, leaving open the question of whether clustered spiking networks can\ndisplay more rich dynamics--and if so, whether these can be predicted from\ntheir connectivity. Here, we show that in the limits of large population size\nand fast inhibition, the combinatorial threshold linear network (CTLN) model is\na mean-field theory for inhibition-stabilized nonlinear Hawkes networks with\nclustered connectivity. The CTLN has a large body of ``graph rules'' relating\nnetwork structure to dynamics. By applying these, we can predict the dynamic\nattractors of our clustered spiking networks from the structure of\nbetween-cluster connectivity. This allows us to construct networks displaying a\ndiverse array of nonlinear cluster dynamics, including metastable periodic\norbits and chaotic attractors. Relaxing the assumption that inhibition is fast,\nwe see that the CTLN model is still able to predict the activity of clustered\nspiking networks with reasonable inhibitory timescales. For slow enough\ninhibition, we observe bifurcations between CTLN-like dynamics and global\nexcitatory/inhibitory oscillations.", "AI": {"tldr": "This paper explores how the connectivity structure of clustered spiking networks influences their dynamic behaviors using combinatorial threshold linear networks. The study uncovers nonlinear cluster dynamics, including complex attractors.", "motivation": "To understand whether and how clustered spiking networks can display complex dynamics beyond prior research on fixed-point metastability, and whether these dynamics can be predicted from connectivity.", "method": "The study uses the combinatorial threshold linear network (CTLN) model as a mean-field theory for inhibition-stabilized nonlinear Hawkes networks with clustered connectivity. Graph rules are applied to relate network structure to dynamics.", "result": "The authors demonstrate the prediction of dynamic attractors, including metastable periodic orbits and chaotic attractors, in clustered spiking networks. They also observe changes in dynamics when inhibitory timescales are modified.", "conclusion": "These results confirm that CTLN models effectively predict complex dynamics in clustered spiking networks, broadening understanding of how network connectivity influences behaviors in larger inhibitory/excitatory systems."}}
{"id": "2506.05587", "pdf": "https://arxiv.org/pdf/2506.05587", "abs": "https://arxiv.org/abs/2506.05587", "authors": ["Junjie Xing", "Yeye He", "Mengyu Zhou", "Haoyu Dong", "Shi Han", "Lingjiao Chen", "Dongmei Zhang", "Surajit Chaudhuri", "H. V. Jagadish"], "title": "MMTU: A Massive Multi-Task Table Understanding and Reasoning Benchmark", "categories": ["cs.AI", "cs.CL", "cs.DB", "cs.LG"], "comment": null, "summary": "Tables and table-based use cases play a crucial role in many important\nreal-world applications, such as spreadsheets, databases, and computational\nnotebooks, which traditionally require expert-level users like data engineers,\ndata analysts, and database administrators to operate. Although LLMs have shown\nremarkable progress in working with tables (e.g., in spreadsheet and database\ncopilot scenarios), comprehensive benchmarking of such capabilities remains\nlimited. In contrast to an extensive and growing list of NLP benchmarks,\nevaluations of table-related tasks are scarce, and narrowly focus on tasks like\nNL-to-SQL and Table-QA, overlooking the broader spectrum of real-world tasks\nthat professional users face. This gap limits our understanding and model\nprogress in this important area.\n  In this work, we introduce MMTU, a large-scale benchmark with over 30K\nquestions across 25 real-world table tasks, designed to comprehensively\nevaluate models ability to understand, reason, and manipulate real tables at\nthe expert-level. These tasks are drawn from decades' worth of computer science\nresearch on tabular data, with a focus on complex table tasks faced by\nprofessional users. We show that MMTU require a combination of skills --\nincluding table understanding, reasoning, and coding -- that remain challenging\nfor today's frontier models, where even frontier reasoning models like OpenAI\no4-mini and DeepSeek R1 score only around 60%, suggesting significant room for\nimprovement. We highlight key findings in our evaluation using MMTU and hope\nthat this benchmark drives further advances in understanding and developing\nfoundation models for structured data processing and analysis. Our code and\ndata are available at https://github.com/MMTU-Benchmark/MMTU and\nhttps://huggingface.co/datasets/MMTU-benchmark/MMTU.", "AI": {"tldr": "The paper introduces MMTU, a benchmark to evaluate models on complex table-related tasks, highlighting shortcomings in current frontier models.", "motivation": "Benchmarking for table-related tasks is underexplored despite the importance of tables in professional applications such as spreadsheets and computational notebooks.", "method": "Creation of the MMTU benchmark, consisting of 30K+ questions across 25 table tasks, drawing on decades of computer science research on tabular data.", "result": "Evaluation shows frontier models perform at around 60% accuracy on MMTU, indicating current limitations in handling complex table tasks.", "conclusion": "MMTU can drive advancements in foundation models for structured data analysis, addressing a gap in benchmarking for table-related tasks."}}
{"id": "2506.05836", "pdf": "https://arxiv.org/pdf/2506.05836", "abs": "https://arxiv.org/abs/2506.05836", "authors": ["Nakhat Syeda", "Harsh Shah", "Rajvinder Singh", "Suraj Jaju", "Sumedha Kumar", "Gourav Chhabra", "Maria Spichkova"], "title": "Analysis of cost-efficiency of serverless approaches", "categories": ["cs.SE"], "comment": null, "summary": "In this paper, we present a survey of research studies related to the\ncost-effectiveness of serverless approach and corresponding cost savings. We\nconducted a systematic literature review using Google Scholar search engine,\ncovering the period from 2010 to 2024. We identified 34 related studies, from\nwhich we extracted 17 parameters that might influence the relative cost savings\nof applying the serverless approach.", "AI": {"tldr": "The paper is a survey analyzing the cost-effectiveness and potential cost savings of the serverless approach through a systematic literature review of 34 studies from 2010 to 2024.", "motivation": "To understand the cost-effectiveness of serverless computing and identify parameters influencing cost savings, given its increasing adoption in cloud computing platforms.", "method": "Performed a systematic literature review leveraging Google Scholar from 2010-2024, identifying 34 relevant studies and extracting 17 influential parameters for cost analysis.", "result": "17 parameters influencing cost savings in serverless computing were identified from the analyzed studies.", "conclusion": "The analysis provides valuable insights into the factors affecting cost savings in serverless approach, which can guide both researchers and practitioners in optimizing its use for cost efficiency."}}
{"id": "2506.05812", "pdf": "https://arxiv.org/pdf/2506.05812", "abs": "https://arxiv.org/abs/2506.05812", "authors": ["Jiacheng Yuan", "Changhyun Choi", "Volkan Isler"], "title": "Optimal Robotic Velcro Peeling with Force Feedback", "categories": ["cs.RO"], "comment": null, "summary": "We study the problem of peeling a Velcro strap from a surface using a robotic\nmanipulator. The surface geometry is arbitrary and unknown. The robot has\naccess to only the force feedback and its end-effector position. This problem\nis challenging due to the partial observability of the environment and the\nincompleteness of the sensor feedback. To solve it, we first model the system\nwith simple analytic state and action models based on quasi-static dynamics\nassumptions. We then study the fully-observable case where the state of both\nthe Velcro and the robot are given. For this case, we obtain the optimal\nsolution in closed-form which minimizes the total energy cost. Next, for the\npartially-observable case, we design a state estimator which estimates the\nunderlying state using only force and position feedback. Then, we present a\nheuristics-based controller that balances exploratory and exploitative\nbehaviors in order to peel the velcro efficiently. Finally, we evaluate our\nproposed method in environments with complex geometric uncertainties and sensor\nnoises, achieving 100% success rate with less than 80% increase in energy cost\ncompared to the optimal solution when the environment is fully-observable,\noutperforming the baselines by a large margin.", "AI": {"tldr": "The paper addresses robotic peeling of Velcro straps from unknown surfaces using force and position feedback. It introduces analytic models and heuristic-based controllers for effective solutions in partially-visible environments.", "motivation": "The researchers aim to solve the challenge of peeling Velcro straps in unknown environments, which is complicated due to limited state observability and incomplete sensor feedback.", "method": "The study uses analytic modeling based on quasi-static dynamics, closed-form solutions for fully-observable cases, state estimators for partial observability, and heuristic-based controllers for efficient peeling.", "result": "The developed method achieved a 100% success rate in complex scenarios with under 80% energy cost increase compared to fully-observable optimal solutions, significantly outperforming alternative methods.", "conclusion": "The paper demonstrates the viability of heuristic-based controllers for tackling partially-observable robotic tasks, achieving high efficiency and reliability despite uncertainties."}}
{"id": "2506.05515", "pdf": "https://arxiv.org/pdf/2506.05515", "abs": "https://arxiv.org/abs/2506.05515", "authors": ["Adrien Cort\u00e9s", "R\u00e9mi Rehm", "Victor Letzelter"], "title": "Winner-takes-all for Multivariate Probabilistic Time Series Forecasting", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "ICML 2025", "summary": "We introduce TimeMCL, a method leveraging the Multiple Choice Learning (MCL)\nparadigm to forecast multiple plausible time series futures. Our approach\nemploys a neural network with multiple heads and utilizes the Winner-Takes-All\n(WTA) loss to promote diversity among predictions. MCL has recently gained\nattention due to its simplicity and ability to address ill-posed and ambiguous\ntasks. We propose an adaptation of this framework for time-series forecasting,\npresenting it as an efficient method to predict diverse futures, which we\nrelate to its implicit quantization objective. We provide insights into our\napproach using synthetic data and evaluate it on real-world time series,\ndemonstrating its promising performance at a light computational cost.", "AI": {"tldr": "TimeMCL adapts the MCL paradigm to forecast diverse plausible futures in time series using neural networks with multiple heads and WTA loss.", "motivation": "To address the challenges of forecasting diverse plausible futures in time series, which is essential in ambiguous and ill-posed tasks.", "method": "TimeMCL employs neural networks with multiple heads and a Winner-Takes-All (WTA) loss to ensure diversification in predictions, adapting MCL for time-series forecasting.", "result": "Synthetic and real-world data evaluations show promising performance with light computational demands.", "conclusion": "TimeMCL offers an efficient and promising solution for time series forecasting by incorporating diversity and computational efficiency."}}
{"id": "2506.05369", "pdf": "https://arxiv.org/pdf/2506.05369", "abs": "https://arxiv.org/abs/2506.05369", "authors": ["Nicolas Pfitzer", "Yifan Zhou", "Marco Poggensee", "Defne Kurtulus", "Bessie Dominguez-Dager", "Mihai Dusmanu", "Marc Pollefeys", "Zuria Bauer"], "title": "MR.NAVI: Mixed-Reality Navigation Assistant for the Visually Impaired", "categories": ["cs.CV"], "comment": null, "summary": "Over 43 million people worldwide live with severe visual impairment, facing\nsignificant challenges in navigating unfamiliar environments. We present\nMR.NAVI, a mixed reality system that enhances spatial awareness for visually\nimpaired users through real-time scene understanding and intuitive audio\nfeedback. Our system combines computer vision algorithms for object detection\nand depth estimation with natural language processing to provide contextual\nscene descriptions, proactive collision avoidance, and navigation instructions.\nThe distributed architecture processes sensor data through MobileNet for object\ndetection and employs RANSAC-based floor detection with DBSCAN clustering for\nobstacle avoidance. Integration with public transit APIs enables navigation\nwith public transportation directions. Through our experiments with user\nstudies, we evaluated both scene description and navigation functionalities in\nunfamiliar environments, showing promising usability and effectiveness.", "AI": {"tldr": "MR.NAVI is a mixed reality system designed for visually impaired individuals that utilizes audio feedback, computer vision, and natural language processing for enhanced spatial awareness and navigation.", "motivation": "Around 43 million people globally have severe visual impairments, creating a need for effective solutions to aid navigation in unfamiliar environments.", "method": "MR.NAVI employs computer vision for object detection and depth estimation, NLP for scene descriptions, sensor data processing, floor detection via RANSAC with DBSCAN clustering, and public transit API integration.", "result": "Experiments, including user studies, demonstrated the system's usability and effectiveness in providing real-time scene understanding and navigation in unfamiliar settings.", "conclusion": "MR.NAVI showcases potential in improving spatial awareness and navigation for visually impaired users through innovative technological integration."}}
{"id": "2506.05393", "pdf": "https://arxiv.org/pdf/2506.05393", "abs": "https://arxiv.org/abs/2506.05393", "authors": ["Shenyang Huang", "Ali Parviz", "Emma Kondrup", "Zachary Yang", "Zifeng Ding", "Michael Bronstein", "Reihaneh Rabbany", "Guillaume Rabusseau"], "title": "Are Large Language Models Good Temporal Graph Learners?", "categories": ["cs.CL", "cs.LG"], "comment": "9 pages, 9 tables, 4 figures", "summary": "Large Language Models (LLMs) have recently driven significant advancements in\nNatural Language Processing and various other applications. While a broad range\nof literature has explored the graph-reasoning capabilities of LLMs, including\ntheir use of predictors on graphs, the application of LLMs to dynamic graphs --\nreal world evolving networks -- remains relatively unexplored. Recent work\nstudies synthetic temporal graphs generated by random graph models, but\napplying LLMs to real-world temporal graphs remains an open question. To\naddress this gap, we introduce Temporal Graph Talker (TGTalker), a novel\ntemporal graph learning framework designed for LLMs. TGTalker utilizes the\nrecency bias in temporal graphs to extract relevant structural information,\nconverted to natural language for LLMs, while leveraging temporal neighbors as\nadditional information for prediction. TGTalker demonstrates competitive link\nprediction capabilities compared to existing Temporal Graph Neural Network\n(TGNN) models. Across five real-world networks, TGTalker performs competitively\nwith state-of-the-art temporal graph methods while consistently outperforming\npopular models such as TGN and HTGN. Furthermore, TGTalker generates textual\nexplanations for each prediction, thus opening up exciting new directions in\nexplainability and interpretability for temporal link prediction. The code is\npublicly available at https://github.com/shenyangHuang/TGTalker.", "AI": {"tldr": "The paper introduces Temporal Graph Talker (TGTalker), a framework applying Large Language Models (LLMs) to real-world dynamic graphs for competitive link prediction and enhanced interpretability.", "motivation": "The motivation is to explore how LLMs can be applied to real-world temporal graphs, a relatively unexplored area compared to their use in static and synthetic temporal graphs.", "method": "TGTalker leverages recency bias in temporal graphs to transform structural information into natural language for LLM processing and incorporates temporal neighbors for improved link predictions.", "result": "TGTalker achieves competitive performance compared to state-of-the-art Temporal Graph Neural Networks (TGNNs) across five real-world datasets, while providing textual explanations for predictions.", "conclusion": "TGTalker expands the scope of LLM applications into dynamic graph processing, showcasing strong prediction capabilities and opening opportunities for explainable, interpretable models in temporal link prediction."}}
{"id": "2506.05435", "pdf": "https://arxiv.org/pdf/2506.05435", "abs": "https://arxiv.org/abs/2506.05435", "authors": ["Manon Renault", "Hamoud Younes", "Hugo Tessier", "Ronan Le Roy", "Bastien Pasdeloup", "Mathieu L\u00e9onardon"], "title": "Event Classification of Accelerometer Data for Industrial Package Monitoring with Embedded Deep Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Package monitoring is an important topic in industrial applications, with\nsignificant implications for operational efficiency and ecological\nsustainability. In this study, we propose an approach that employs an embedded\nsystem, placed on reusable packages, to detect their state (on a Forklift, in a\nTruck, or in an undetermined location). We aim to design a system with a\nlifespan of several years, corresponding to the lifespan of reusable packages.\nOur analysis demonstrates that maximizing device lifespan requires minimizing\nwake time. We propose a pipeline that includes data processing, training, and\nevaluation of the deep learning model designed for imbalanced, multiclass time\nseries data collected from an embedded sensor. The method uses a\none-dimensional Convolutional Neural Network architecture to classify\naccelerometer data from the IoT device. Before training, two data augmentation\ntechniques are tested to solve the imbalance problem of the dataset: the\nSynthetic Minority Oversampling TEchnique and the ADAptive SYNthetic sampling\napproach. After training, compression techniques are implemented to have a\nsmall model size. On the considered twoclass problem, the methodology yields a\nprecision of 94.54% for the first class and 95.83% for the second class, while\ncompression techniques reduce the model size by a factor of four. The trained\nmodel is deployed on the IoT device, where it operates with a power consumption\nof 316 mW during inference.", "AI": {"tldr": "The paper presents an embedded IoT system using deep learning to monitor reusable package states efficiently and sustainably with high precision and low power consumption.", "motivation": "To enhance operational efficiency and ecological sustainability in package monitoring by developing a long-lasting, embedded system for reusable packages.", "method": "A one-dimensional Convolutional Neural Network (CNN) is trained on imbalanced, multiclass time-series accelerometer data using data augmentation. Compression techniques are applied to reduce model size for IoT deployment.", "result": "Achieved 94.54% and 95.83% precision in a two-class classification problem, while reducing model size by four-fold and ensuring a low inference power consumption of 316 mW.", "conclusion": "The proposed method demonstrates practical applicability for accurate, efficient, and sustainable package state monitoring, aligned with ecological goals and industrial needs."}}
{"id": "2506.06122", "pdf": "https://arxiv.org/pdf/2506.06122", "abs": "https://arxiv.org/abs/2506.06122", "authors": ["Weixun Wang", "Shaopan Xiong", "Gengru Chen", "Wei Gao", "Sheng Guo", "Yancheng He", "Ju Huang", "Jiaheng Liu", "Zhendong Li", "Xiaoyang Li", "Zichen Liu", "Haizhou Zhao", "Dakai An", "Lunxi Cao", "Qiyang Cao", "Wanxi Deng", "Feilei Du", "Yiliang Gu", "Jiahe Li", "Xiang Li", "Mingjie Liu", "Yijia Luo", "Zihe Liu", "Yadao Wang", "Pei Wang", "Tianyuan Wu", "Yanan Wu", "Yuheng Zhao", "Shuaibing Zhao", "Jin Yang", "Siran Yang", "Yingshui Tan", "Huimin Yi", "Yuchi Xu", "Yujin Yuan", "Xingyao Zhang", "Lin Qu", "Wenbo Su", "Wei Wang", "Jiamang Wang", "Bo Zheng"], "title": "Reinforcement Learning Optimization for Large-Scale Learning: An Efficient and User-Friendly Scaling Library", "categories": ["cs.LG", "cs.DC"], "comment": "16 pages", "summary": "We introduce ROLL, an efficient, scalable, and user-friendly library designed\nfor Reinforcement Learning Optimization for Large-scale Learning. ROLL caters\nto three primary user groups: tech pioneers aiming for cost-effective,\nfault-tolerant large-scale training, developers requiring flexible control over\ntraining workflows, and researchers seeking agile experimentation. ROLL is\nbuilt upon several key modules to serve these user groups effectively. First, a\nsingle-controller architecture combined with an abstraction of the parallel\nworker simplifies the development of the training pipeline. Second, the\nparallel strategy and data transfer modules enable efficient and scalable\ntraining. Third, the rollout scheduler offers fine-grained management of each\nsample's lifecycle during the rollout stage. Fourth, the environment worker and\nreward worker support rapid and flexible experimentation with agentic RL\nalgorithms and reward designs. Finally, AutoDeviceMapping allows users to\nassign resources to different models flexibly across various stages.", "AI": {"tldr": "The paper introduces ROLL, a library designed to optimize large-scale reinforcement learning by offering efficiency, scalability, and user-friendly features tailored for tech pioneers, developers, and researchers.", "motivation": "To address the need for a cost-effective, fault-tolerant, and flexible platform for large-scale reinforcement learning training pipelines.", "method": "ROLL leverages a single-controller architecture, parallel strategies, rollout schedulers, dedicated workers for environments and rewards, and AutoDeviceMapping for resource management.", "result": "ROLL provides a scalable, efficient, and user-adaptive approach to managing large-scale RL training and experimentation.", "conclusion": "ROLL effectively caters to diverse user groups by providing a simplified, robust infrastructure for reinforcement learning optimization at scale."}}
{"id": "2506.05574", "pdf": "https://arxiv.org/pdf/2506.05574", "abs": "https://arxiv.org/abs/2506.05574", "authors": ["Chase Goddard", "Lindsay M. Smith", "Vudtiwat Ngampruetikorn", "David J. Schwab"], "title": "When can in-context learning generalize out of task distribution?", "categories": ["cs.LG", "cond-mat.dis-nn", "cond-mat.stat-mech", "q-bio.NC", "stat.ML"], "comment": null, "summary": "In-context learning (ICL) is a remarkable capability of pretrained\ntransformers that allows models to generalize to unseen tasks after seeing only\na few examples. We investigate empirically the conditions necessary on the\npretraining distribution for ICL to emerge and generalize\n\\emph{out-of-distribution}. Previous work has focused on the number of distinct\ntasks necessary in the pretraining dataset. Here, we use a different notion of\ntask diversity to study the emergence of ICL in transformers trained on linear\nfunctions. We find that as task diversity increases, transformers undergo a\ntransition from a specialized solution, which exhibits ICL only within the\npretraining task distribution, to a solution which generalizes out of\ndistribution to the entire task space. We also investigate the nature of the\nsolutions learned by the transformer on both sides of the transition, and\nobserve similar transitions in nonlinear regression problems. We construct a\nphase diagram to characterize how our concept of task diversity interacts with\nthe number of pretraining tasks. In addition, we explore how factors such as\nthe depth of the model and the dimensionality of the regression problem\ninfluence the transition.", "AI": {"tldr": "The study investigates conditions for the emergence and generalization of in-context learning (ICL) in transformers, focusing on task diversity during training.", "motivation": "Understanding when and how in-context learning capabilities arise in pretrained transformers and generalize to unseen tasks.", "method": "Empirical analysis of transformers trained on linear functions, studying transitions in solutions based on data diversity, and constructing phase diagrams to explore the impacts of model size and task dimensionality.", "result": "Increasing task diversity leads to a shift in transformers from task-specific learning to generalizable ICL solutions, with similar observations seen in nonlinear tasks.", "conclusion": "Task diversity during pretraining critically influences the capacity of transformers to generalize ICL to out-of-distribution tasks, shaped by model depth and task complexity factors."}}
{"id": "2506.05616", "pdf": "https://arxiv.org/pdf/2506.05616", "abs": "https://arxiv.org/abs/2506.05616", "authors": ["Lianhao Zhou", "Hongyi Ling", "Keqiang Yan", "Kaiji Zhao", "Xiaoning Qian", "Raymundo Arr\u00f3yave", "Xiaofeng Qian", "Shuiwang Ji"], "title": "Toward Greater Autonomy in Materials Discovery Agents: Unifying Planning, Physics, and Scientists", "categories": ["cs.AI", "cond-mat.mtrl-sci", "physics.comp-ph"], "comment": null, "summary": "We aim at designing language agents with greater autonomy for crystal\nmaterials discovery. While most of existing studies restrict the agents to\nperform specific tasks within predefined workflows, we aim to automate workflow\nplanning given high-level goals and scientist intuition. To this end, we\npropose Materials Agent unifying Planning, Physics, and Scientists, known as\nMAPPS. MAPPS consists of a Workflow Planner, a Tool Code Generator, and a\nScientific Mediator. The Workflow Planner uses large language models (LLMs) to\ngenerate structured and multi-step workflows. The Tool Code Generator\nsynthesizes executable Python code for various tasks, including invoking a\nforce field foundation model that encodes physics. The Scientific Mediator\ncoordinates communications, facilitates scientist feedback, and ensures\nrobustness through error reflection and recovery. By unifying planning,\nphysics, and scientists, MAPPS enables flexible and reliable materials\ndiscovery with greater autonomy, achieving a five-fold improvement in\nstability, uniqueness, and novelty rates compared with prior generative models\nwhen evaluated on the MP-20 data. We provide extensive experiments across\ndiverse tasks to show that MAPPS is a promising framework for autonomous\nmaterials discovery.", "AI": {"tldr": "The paper introduces MAPPS, an autonomous framework using AI for crystal material discovery by integrating workflow planning, physics models, and scientific input.", "motivation": "The motivation is to create autonomous language agents that can automate workflow planning for material discovery instead of limiting them to predefined tasks.", "method": "MAPPS combines three elements: Workflow Planner using large language models, Tool Code Generator for executable Python code, and a Scientific Mediator to integrate scientist feedback and ensure error handling.", "result": "MAPPS outperforms previous generative models with a five-fold improvement in stability, uniqueness, and novelty rates on the MP-20 dataset.", "conclusion": "This framework shows great potential in enabling autonomous and flexible material discovery through its novel integration of planning, physics, and scientific input."}}
{"id": "2506.05990", "pdf": "https://arxiv.org/pdf/2506.05990", "abs": "https://arxiv.org/abs/2506.05990", "authors": ["Stefan Dascalescu", "Adrian Marius Dumitran", "Mihai Alexandru Vasiluta"], "title": "Leveraging Generative AI for Enhancing Automated Assessment in Programming Education Contests", "categories": ["cs.SE", "cs.AI"], "comment": "11 pages, 2 chart pies, 1 figure Pre-print version Accepted at BEA\n  2025", "summary": "Competitive programming contests play a crucial role in cultivating\ncomputational thinking and algorithmic skills among learners. However,\ngenerating comprehensive test cases to effectively assess programming solutions\nremains resource-intensive and challenging for educators. This paper introduces\nan innovative NLP-driven method leveraging generative AI (large language\nmodels) to automate the creation of high-quality test cases for competitive\nprogramming assessments. We extensively evaluated our approach on diverse\ndatasets, including 25 years of Romanian Informatics Olympiad (OJI) data for\n5th graders, recent competitions hosted on the Kilonova.ro platform, and the\nInternational Informatics Olympiad in Teams (IIOT). Our results demonstrate\nthat AI-generated test cases substantially enhanced assessments, notably\nidentifying previously undetected errors in 67% of the OJI 5th grade\nprogramming problems. These improvements underscore the complementary\neducational value of our technique in formative assessment contexts. By openly\nsharing our prompts, translated datasets, and methodologies, we offer practical\nNLP-based tools that educators and contest organizers can readily integrate to\nenhance assessment quality, reduce workload, and deepen insights into learner\nperformance.", "AI": {"tldr": "This paper proposes using NLP-driven generative AI for automating test case creation in competitive programming assessments, demonstrating significant improvement in assessment quality.", "motivation": "To address the resource-intensive and challenging process of creating comprehensive test cases for evaluating programming solutions.", "method": "An NLP-driven approach using generative AI, specifically large language models, to generate high-quality test cases. The methodology includes using prompts and datasets from competitive programming contests for validation.", "result": "AI-generated test cases improved assessments by identifying previously undetected errors in 67% of programming problems from the Romanian Informatics Olympiad (OJI) for 5th graders.", "conclusion": "The technique enhances formative assessments, reduces educator workload, and provides a practical tool for understanding learner performance, offering shared methodologies and datasets for easier integration."}}
{"id": "2506.05896", "pdf": "https://arxiv.org/pdf/2506.05896", "abs": "https://arxiv.org/abs/2506.05896", "authors": ["Chongshang Yan", "Jiaxuan He", "Delun Li", "Yi Yang", "Wenjie Song"], "title": "Object Navigation with Structure-Semantic Reasoning-Based Multi-level Map and Multimodal Decision-Making LLM", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "16 pages, 11 figures", "summary": "The zero-shot object navigation (ZSON) in unknown open-ended environments\ncoupled with semantically novel target often suffers from the significant\ndecline in performance due to the neglect of high-dimensional implicit scene\ninformation and the long-range target searching task. To address this, we\nproposed an active object navigation framework with Environmental Attributes\nMap (EAM) and MLLM Hierarchical Reasoning module (MHR) to improve its success\nrate and efficiency. EAM is constructed by reasoning observed environments with\nSBERT and predicting unobserved ones with Diffusion, utilizing human space\nregularities that underlie object-room correlations and area adjacencies. MHR\nis inspired by EAM to perform frontier exploration decision-making, avoiding\nthe circuitous trajectories in long-range scenarios to improve path efficiency.\nExperimental results demonstrate that the EAM module achieves 64.5\\% scene\nmapping accuracy on MP3D dataset, while the navigation task attains SPLs of\n28.4\\% and 26.3\\% on HM3D and MP3D benchmarks respectively - representing\nabsolute improvements of 21.4\\% and 46.0\\% over baseline methods.", "AI": {"tldr": "The paper introduces a framework to improve zero-shot object navigation (ZSON) using Environmental Attributes Map (EAM) and MLLM Hierarchical Reasoning (MHR). This approach enhances navigation performance in open-ended environments.", "motivation": "Address challenges in ZSON caused by ignoring implicit scene information and inefficiency in long-range target searches.", "method": "Developed EAM for scene mapping using SBERT and Diffusion, and MHR for improved decision-making in navigation tasks.", "result": "Achieved 64.5% mapping accuracy on the MP3D dataset and significant SPL improvements (21.4% and 46.0%) on the HM3D and MP3D benchmarks.", "conclusion": "The framework demonstrates improved success rates, path efficiency, and scene mapping accuracy for ZSON tasks in open-ended environments."}}
{"id": "2506.05526", "pdf": "https://arxiv.org/pdf/2506.05526", "abs": "https://arxiv.org/abs/2506.05526", "authors": ["Michal Klein", "Alireza Mousavi-Hosseini", "Stephen Zhang", "Marco Cuturi"], "title": "On Fitting Flow Models with Large Sinkhorn Couplings", "categories": ["cs.LG", "stat.ML"], "comment": "20 pages, 14 figures", "summary": "Flow models transform data gradually from one modality (e.g. noise) onto\nanother (e.g. images). Such models are parameterized by a time-dependent\nvelocity field, trained to fit segments connecting pairs of source and target\npoints. When the pairing between source and target points is given, training\nflow models boils down to a supervised regression problem. When no such pairing\nexists, as is the case when generating data from noise, training flows is much\nharder. A popular approach lies in picking source and target points\nindependently. This can, however, lead to velocity fields that are slow to\ntrain, but also costly to integrate at inference time. In theory, one would\ngreatly benefit from training flow models by sampling pairs from an optimal\ntransport (OT) measure coupling source and target, since this would lead to a\nhighly efficient flow solving the Benamou and Brenier dynamical OT problem. In\npractice, recent works have proposed to sample mini-batches of $n$ source and\n$n$ target points and reorder them using an OT solver to form better pairs.\nThese works have advocated using batches of size $n\\approx 256$, and considered\nOT solvers that return couplings that are either sharp (using e.g. the\nHungarian algorithm) or blurred (using e.g. entropic regularization, a.k.a.\nSinkhorn). We follow in the footsteps of these works by exploring the benefits\nof increasing $n$ by three to four orders of magnitude, and look more carefully\non the effect of the entropic regularization $\\varepsilon$ used in the Sinkhorn\nalgorithm. Our analysis is facilitated by new scale invariant quantities to\nreport the sharpness of a coupling, while our sharded computations across\nmultiple GPU or GPU nodes allow scaling up $n$. We show that in both synthetic\nand image generation tasks, flow models greatly benefit when fitted with large\nSinkhorn couplings, with a low entropic regularization $\\varepsilon$.", "AI": {"tldr": "This paper investigates scaling up the size of mini-batches ($n$) in flow model training and examines the impact of entropic regularization ($\\varepsilon$) in the Sinkhorn algorithm, demonstrating benefits in generation tasks.", "motivation": "Training flow models to solve the dynamical optimal transport (OT) problem is challenging when source-target pairings are not predefined. Existing methods face high computational costs and slow convergence when coupling points independently. The authors aim to improve training efficiency and performance by leveraging optimal couplings.", "method": "The authors propose increasing the size of mini-batches ($n$) by three to four orders of magnitude and carefully analyzing the effect of the entropic regularization parameter ($\\varepsilon$) in the Sinkhorn algorithm. They introduce new scale-invariant metrics for evaluating couplings and employ sharded computation across GPUs.", "result": "Using large mini-batch sizes and low entropic regularization in the Sinkhorn algorithm enhances flow model performance for both synthetic and image generation tasks.", "conclusion": "Optimally coupling source and target points using large Sinkhorn couplings with careful tuning of $\\varepsilon$ leads to significant gains in both training efficiency and task-specific outcomes in flow models."}}
{"id": "2506.05372", "pdf": "https://arxiv.org/pdf/2506.05372", "abs": "https://arxiv.org/abs/2506.05372", "authors": ["Dimitrios Kollias", "Damith C. Senadeera", "Jianian Zheng", "Kaushal K. K. Yadav", "Greg Slabaugh", "Muhammad Awais", "Xiaoyun Yang"], "title": "DVD: A Comprehensive Dataset for Advancing Violence Detection in Real-World Scenarios", "categories": ["cs.CV"], "comment": null, "summary": "Violence Detection (VD) has become an increasingly vital area of research.\nExisting automated VD efforts are hindered by the limited availability of\ndiverse, well-annotated databases. Existing databases suffer from coarse\nvideo-level annotations, limited scale and diversity, and lack of metadata,\nrestricting the generalization of models. To address these challenges, we\nintroduce DVD, a large-scale (500 videos, 2.7M frames), frame-level annotated\nVD database with diverse environments, varying lighting conditions, multiple\ncamera sources, complex social interactions, and rich metadata. DVD is designed\nto capture the complexities of real-world violent events.", "AI": {"tldr": "The paper introduces DVD, a large-scale, frame-level annotated database for violence detection to address issues in existing VD datasets, like limited diversity, scale, and metadata.", "motivation": "Address the limitations of existing violence detection databases, such as coarse annotations, restricted diversity, and lack of metadata.", "method": "Developed DVD, a database with 500 videos, frame-level annotations, diverse environments, multiple camera sources, and detailed metadata.", "result": "DVD enhances the capability to model real-world violent events by providing comprehensive and complex data.", "conclusion": "The introduction of DVD significantly contributes towards advancing violence detection research by providing a more robust and diverse foundation."}}
{"id": "2506.05400", "pdf": "https://arxiv.org/pdf/2506.05400", "abs": "https://arxiv.org/abs/2506.05400", "authors": ["Ayesha Qamar", "Arushi Raghuvanshi", "Conal Sathi", "Youngseo Son"], "title": "Auto Review: Second Stage Error Detection for Highly Accurate Information Extraction from Phone Conversations", "categories": ["cs.CL"], "comment": "Accepted to ACL Industry track 2025", "summary": "Automating benefit verification phone calls saves time in healthcare and\nhelps patients receive treatment faster. It is critical to obtain highly\naccurate information in these phone calls, as it can affect a patient's\nhealthcare journey. Given the noise in phone call transcripts, we have a\ntwo-stage system that involves a post-call review phase for potentially noisy\nfields, where human reviewers manually verify the extracted\ndata$\\unicode{x2013}$a labor-intensive task. To automate this stage, we\nintroduce Auto Review, which significantly reduces manual effort while\nmaintaining a high bar for accuracy. This system, being highly reliant on call\ntranscripts, suffers a performance bottleneck due to automatic speech\nrecognition (ASR) issues. This problem is further exacerbated by the use of\ndomain-specific jargon in the calls. In this work, we propose a second-stage\npostprocessing pipeline for accurate information extraction. We improve\naccuracy by using multiple ASR alternatives and a pseudo-labeling approach that\ndoes not require manually corrected transcripts. Experiments with\ngeneral-purpose large language models and feature-based model pipelines\ndemonstrate substantial improvements in the quality of corrected call\ntranscripts, thereby enhancing the efficiency of Auto Review.", "AI": {"tldr": "The paper introduces Auto Review, an automated benefit verification system for healthcare phone calls that reduces manual effort while maintaining accuracy. It addresses challenges caused by noisy call transcripts.", "motivation": "To minimize manual labor and expedite benefit verification in healthcare, ensuring accurate information despite challenges from noisy call transcripts and domain-specific jargon.", "method": "Developed a postprocessing pipeline that utilizes multiple ASR alternatives and a pseudo-labeling approach to improve transcript accuracy without needing manual corrections.", "result": "Experimental results showed improvements in transcript correction quality and increased efficiency for the Auto Review system via general-purpose large language models and feature-based model pipelines.", "conclusion": "Auto Review enhances automation in benefit verification with accurate information extraction, reducing dependence on manual reviews, even amidst noisy inputs and domain-specific challenges."}}
{"id": "2506.05438", "pdf": "https://arxiv.org/pdf/2506.05438", "abs": "https://arxiv.org/abs/2506.05438", "authors": ["Tongda Sun", "Chen Yin", "Huailiang Zheng", "Yining Dong"], "title": "An Unsupervised Framework for Dynamic Health Indicator Construction and Its Application in Rolling Bearing Prognostics", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Health indicator (HI) plays a key role in degradation assessment and\nprognostics of rolling bearings. Although various HI construction methods have\nbeen investigated, most of them rely on expert knowledge for feature extraction\nand overlook capturing dynamic information hidden in sequential degradation\nprocesses, which limits the ability of the constructed HI for degradation trend\nrepresentation and prognostics. To address these concerns, a novel dynamic HI\nthat considers HI-level temporal dependence is constructed through an\nunsupervised framework. Specifically, a degradation feature learning module\ncomposed of a skip-connection-based autoencoder first maps raw signals to a\nrepresentative degradation feature space (DFS) to automatically extract\nessential degradation features without the need for expert knowledge.\nSubsequently, in this DFS, a new HI-generating module embedded with an inner\nHI-prediction block is proposed for dynamic HI construction, where the temporal\ndependence between past and current HI states is guaranteed and modeled\nexplicitly. On this basis, the dynamic HI captures the inherent dynamic\ncontents of the degradation process, ensuring its effectiveness for degradation\ntendency modeling and future degradation prognostics. The experiment results on\ntwo bearing lifecycle datasets demonstrate that the proposed HI construction\nmethod outperforms comparison methods, and the constructed dynamic HI is\nsuperior for prognostic tasks.", "AI": {"tldr": "This paper proposes a new method for constructing dynamic health indicators (HI) for rolling bearings, leveraging an unsupervised learning framework that captures temporal dependencies and does not rely on expert knowledge.", "motivation": "Most existing HI construction methods depend on expert knowledge and fail to account for the dynamic nature of degradation processes, limiting their effectiveness in representing degradation trends and future predictions.", "method": "The method involves two main modules: a skip-connection-based autoencoder for unsupervised extraction of degradation features, and an HI-generating module with a built-in prediction block to model temporal dependencies in the HI states.", "result": "Experiment results on two bearing lifecycle datasets show that the proposed method outperforms other approaches in constructing effective HIs for prognostic tasks.", "conclusion": "The proposed dynamic HI effectively captures the inherent dynamics of degradation processes and is more suitable for modeling degradation tendencies and predicting future states."}}
{"id": "2506.05769", "pdf": "https://arxiv.org/pdf/2506.05769", "abs": "https://arxiv.org/abs/2506.05769", "authors": ["Matteo Fraschini", "Matteo Demuru", "Daniele Marinazzo", "Luca Didaci"], "title": "Connectome brain fingerprinting: terminology, measures, and target properties", "categories": ["q-bio.QM", "q-bio.NC"], "comment": null, "summary": "Distinguishing one person from another (what biometricians call recognition)\nis extremely relevant for different aspects of life. Traditional biometric\nmodalities (fingerprint, face, iris, voice) rely on unique, stable features\nthat reliably differentiate individuals. Recently, the term fingerprinting has\ngained popularity in neuroscience, with a growing number of studies adopting\nthe term to describe various brain based metrics derived from different\ntechniques. However, we think there is a mismatch between its widely accepted\nmeaning in the biometric community and some brain based metrics. Many of these\nmeasures do not satisfy the strict definition of a biometric fingerprint that\nis, a stable trait that uniquely identifies an individual. In this study we\ndiscuss some issues that may generate confusion in this context and suggest how\nto treat the question in the future. In particular, we review how fingerprint\nis currently used in the neuroscience literature, highlight mismatches with the\nbiometric community definition, and offer clear guidelines for distinguishing\ngenuine biometric fingerprints from exploratory similarity metrics. By\nclarifying terminology and criteria, we aim to align practices and facilitate\ncommunication across fields.", "AI": {"tldr": "The paper critiques the use of the term 'fingerprinting' for brain-based metrics in neuroscience, arguing it often deviates from the stricter biometric definition.", "motivation": "To address confusion and misalignment in the use of 'fingerprinting' for brain metrics, contrasting it with its precise meaning in biometric sciences.", "method": "The authors reviewed neuroscience uses of 'fingerprinting,' identified mismatches with biometric standards, and proposed guidelines for proper alignment.", "result": "Identified inconsistencies in the terminology usage and provided recommendations to improve clarity between neuroscience and biometric fields.", "conclusion": "Clear guidelines and alignment in the use of 'fingerprinting' will enhance interdisciplinary understanding and communication."}}
{"id": "2506.05619", "pdf": "https://arxiv.org/pdf/2506.05619", "abs": "https://arxiv.org/abs/2506.05619", "authors": ["Kihyun Kim", "Jiawei Zhang", "Asuman Ozdaglar", "Pablo A. Parrilo"], "title": "Population-Proportional Preference Learning from Human Feedback: An Axiomatic Approach", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Conventional preference learning methods often prioritize opinions held more\nwidely when aggregating preferences from multiple evaluators. This may result\nin policies that are biased in favor of some types of opinions or groups. The\nobjective of this paper is to develop a novel preference learning framework\ncapable of aligning aggregate opinions and policies proportionally with the\ntrue population distribution of evaluator preferences. Our approach infers the\nfeasible set of evaluator population distributions directly from pairwise\ncomparison data. Using these estimates, the algorithm constructs a policy that\nsatisfies foundational axioms from social choice theory, namely monotonicity\nand Pareto efficiency, as well as our newly-introduced axioms of\npopulation-proportional representation and population-bounded robustness. We\npropose a soft-max relaxation method that smoothly trade-offs\npopulation-proportional representation with the selection of the Condorcet\nwinner (which beats all other options in pairwise comparisons). Finally, we\nvalidate the effectiveness and scalability of our approach through experiments\non both tabular recommendation tasks and large-scale language model alignment.", "AI": {"tldr": "The paper introduces a framework to align aggregated preferences in accordance with population distributions while respecting several axioms from social choice theory.", "motivation": "Existing preference learning methods show biases by favoring widely held opinions, potentially excluding minority views.", "method": "The approach estimates evaluator population distributions from pairwise data and constructs policies satisfying social choice theory axioms, introducing new axioms like population-proportional representation.", "result": "The method uses a soft-max optimization technique to balance population representation with Condorcet winner criteria and demonstrates effectiveness on recommendation tasks and language model alignment experiments.", "conclusion": "The proposed framework offers a scalable and equitable way to align preferences reflecting true population distributions while adhering to core social choice principles."}}
{"id": "2506.06202", "pdf": "https://arxiv.org/pdf/2506.06202", "abs": "https://arxiv.org/abs/2506.06202", "authors": ["Renato Cordeiro Ferreira", "Rowanne Trapmann", "Willem-Jan van den Heuvel"], "title": "MLOps with Microservices: A Case Study on the Maritime Domain", "categories": ["cs.SE", "cs.AI", "cs.LG", "D.2.11; D.2.9; I.2.m; I.5.0"], "comment": "13 pages, 3 figures, to be published in SummerSOC 2025", "summary": "This case study describes challenges and lessons learned on building Ocean\nGuard: a Machine Learning-Enabled System (MLES) for anomaly detection in the\nmaritime domain. First, the paper presents the system's specification, and\narchitecture. Ocean Guard was designed with a microservices' architecture to\nenable multiple teams to work on the project in parallel. Then, the paper\ndiscusses how the developers adapted contract-based design to MLOps for\nachieving that goal. As a MLES, Ocean Guard employs code, model, and data\ncontracts to establish guidelines between its services. This case study hopes\nto inspire software engineers, machine learning engineers, and data scientists\nto leverage similar approaches for their systems.", "AI": {"tldr": "The paper discusses the development of 'Ocean Guard,' a microservices-based machine learning-enabled system for maritime anomaly detection and shares challenges, the system's design, and lessons learned.", "motivation": "To build a robust and scalable system for detecting maritime anomalies while facilitating collaboration among multiple teams.", "method": "Designed Ocean Guard using a microservices architecture and adapted contract-based design to MLOps with code, model, and data contracts guiding services.", "result": "Ocean Guard successfully facilitates team collaboration through microservices and contract-based design while achieving operational goals in anomaly detection.", "conclusion": "The authors believe their approach can serve as a model for other professionals to build complex machine learning-enabled systems in a collaborative environment."}}
{"id": "2506.05997", "pdf": "https://arxiv.org/pdf/2506.05997", "abs": "https://arxiv.org/abs/2506.05997", "authors": ["Fan Yang", "Per Frivik", "David Hoeller", "Chen Wang", "Cesar Cadena", "Marco Hutter"], "title": "Improving Long-Range Navigation with Spatially-Enhanced Recurrent Memory via End-to-End Reinforcement Learning", "categories": ["cs.RO"], "comment": "21 pages", "summary": "Recent advancements in robot navigation, especially with end-to-end learning\napproaches like reinforcement learning (RL), have shown remarkable efficiency\nand effectiveness. Yet, successful navigation still relies on two key\ncapabilities: mapping and planning, whether explicit or implicit. Classical\napproaches use explicit mapping pipelines to register ego-centric observations\ninto a coherent map frame for the planner. In contrast, end-to-end learning\nachieves this implicitly, often through recurrent neural networks (RNNs) that\nfuse current and past observations into a latent space for planning. While\narchitectures such as LSTM and GRU capture temporal dependencies, our findings\nreveal a key limitation: their inability to perform effective spatial\nmemorization. This skill is essential for transforming and integrating\nsequential observations from varying perspectives to build spatial\nrepresentations that support downstream planning. To address this, we propose\nSpatially-Enhanced Recurrent Units (SRUs), a simple yet effective modification\nto existing RNNs, designed to enhance spatial memorization capabilities. We\nintroduce an attention-based architecture with SRUs, enabling long-range\nnavigation using a single forward-facing stereo camera. Regularization\ntechniques are employed to ensure robust end-to-end recurrent training via RL.\nExperimental results show our approach improves long-range navigation by 23.5%\ncompared to existing RNNs. Furthermore, with SRU memory, our method outperforms\nthe RL baseline with explicit mapping and memory modules, achieving a 29.6%\nimprovement in diverse environments requiring long-horizon mapping and\nmemorization. Finally, we address the sim-to-real gap by leveraging large-scale\npretraining on synthetic depth data, enabling zero-shot transfer to diverse and\ncomplex real-world environments.", "AI": {"tldr": "The paper introduces Spatially-Enhanced Recurrent Units (SRUs) to improve spatial memorization in robot navigation, achieving significant performance gains in long-range navigation.", "motivation": "Despite advancements in end-to-end learning approaches like reinforcement learning for robot navigation, key issues remain in navigation systems' ability to perform effective spatial memorization, which is crucial for accurate mapping and planning.", "method": "The method involves modifying existing RNNs with SRUs to enhance spatial memorization, introducing an attention-based architecture, leveraging a stereo camera setup, incorporating robust regularization techniques during training, and applying large-scale pretraining on synthetic data for zero-shot real-world transfer.", "result": "The proposed SRU-based approach improves long-range navigation performance by 23.5% compared to existing RNNs and by 29.6% compared to baseline systems with explicit mapping and memory in diverse environments.", "conclusion": "Spatially-Enhanced Recurrent Units (SRUs) address spatial memorization limitations in existing RNNs, offering a scalable, robust solution for navigating complex environments and significantly narrowing the sim-to-real gap."}}
{"id": "2506.05570", "pdf": "https://arxiv.org/pdf/2506.05570", "abs": "https://arxiv.org/abs/2506.05570", "authors": ["Gabriel Phelan", "David A. Campbell"], "title": "Testing Hypotheses of Covariate Effects on Topics of Discourse", "categories": ["stat.ME", "stat.ML"], "comment": null, "summary": "We introduce an approach to topic modelling with document-level covariates\nthat remains tractable in the face of large text corpora. This is achieved by\nde-emphasizing the role of parameter estimation in an underlying probabilistic\nmodel, assuming instead that the data come from a fixed but unknown\ndistribution whose statistical functionals are of interest. We propose\ncombining a convex formulation of non-negative matrix factorization with\nstandard regression techniques as a fast-to-compute and useful estimate of such\na functional. Uncertainty quantification can then be achieved by reposing\nnon-parametric resampling methods on top of this scheme. This is in contrast to\npopular topic modelling paradigms, which posit a complex and often hard-to-fit\ngenerative model of the data. We argue that the simple, non-parametric approach\nadvocated here is faster, more interpretable, and enjoys better inferential\njustification than said generative models. Finally, our methods are\ndemonstrated with an application analysing covariate effects on discourse of\nflavours attributed to Canadian beers.", "AI": {"tldr": "The paper introduces a non-parametric topic modeling approach with document-level covariates using non-negative matrix factorization and regression methods, contrasting it with complex generative models.", "motivation": "Existing topic modeling paradigms rely on complex probabilistic models, which are computationally intensive and hard to fit for large datasets.", "method": "The authors propose combining convex non-negative matrix factorization with standard regression techniques, coupled with non-parametric resampling for uncertainty quantification.", "result": "The approach offers faster, more interpretable, and inferentially justified results compared to traditional generative models.", "conclusion": "The non-parametric method is shown to be practical and effective, exemplified by its application to analyzing Canadian beer flavor discourse."}}
{"id": "2506.05375", "pdf": "https://arxiv.org/pdf/2506.05375", "abs": "https://arxiv.org/abs/2506.05375", "authors": ["Ashik E Rasul", "Hyung-Jin Yoon"], "title": "State Estimation and Control of Dynamic Systems from High-Dimensional Image Data", "categories": ["cs.CV"], "comment": null, "summary": "Accurate state estimation is critical for optimal policy design in dynamic\nsystems. However, obtaining true system states is often impractical or\ninfeasible, complicating the policy learning process. This paper introduces a\nnovel neural architecture that integrates spatial feature extraction using\nconvolutional neural networks (CNNs) and temporal modeling through gated\nrecurrent units (GRUs), enabling effective state representation from sequences\nof images and corresponding actions. These learned state representations are\nused to train a reinforcement learning agent with a Deep Q-Network (DQN).\nExperimental results demonstrate that our proposed approach enables real-time,\naccurate estimation and control without direct access to ground-truth states.\nAdditionally, we provide a quantitative evaluation methodology for assessing\nthe accuracy of the learned states, highlighting their impact on policy\nperformance and control stability.", "AI": {"tldr": "The paper introduces a method combining CNNs and GRUs for state representation learning to aid reinforcement learning in dynamic systems, removing the need for ground-truth state access.", "motivation": "True system states are often unavailable or impractical to obtain, making policy learning in dynamic systems challenging.", "method": "The method integrates CNNs for spatial feature extraction and GRUs for temporal modeling to learn state representations from image sequences and actions. These are then used to train a reinforcement learning agent using a Deep Q-Network (DQN).", "result": "The proposed method achieves real-time, accurate estimation and control without requiring ground-truth states. It also quantitatively evaluates the learned state's accuracy and its influence on policy performance.", "conclusion": "The approach improves control stability and policy performance by replacing the need for ground-truth state access with learned state representations that are computationally efficient and robust."}}
{"id": "2506.05410", "pdf": "https://arxiv.org/pdf/2506.05410", "abs": "https://arxiv.org/abs/2506.05410", "authors": ["Wanyun Cui", "Mingwei Xu"], "title": "Homogeneous Keys, Heterogeneous Values: Exploiting Local KV Cache Asymmetry for Long-Context LLMs", "categories": ["cs.CL", "I.2.7"], "comment": "14 pages,7 figures", "summary": "Recent advances in Large Language Models (LLMs) have highlighted the critical\nimportance of extending context length, yet the quadratic complexity of\nattention mechanisms poses significant challenges for efficient long-context\nmodeling. KV cache compression has emerged as a key approach to address this\nchallenge. Through extensive empirical analysis, we reveal a fundamental yet\npreviously overlooked asymmetry in KV caches: while adjacent keys receive\nsimilar attention weights (local homogeneity), adjacent values demonstrate\ndistinct heterogeneous distributions. This key-value asymmetry reveals a\ncritical limitation in existing compression methods that treat keys and values\nuniformly. To address the limitation, we propose a training-free compression\nframework (AsymKV) that combines homogeneity-based key merging with a\nmathematically proven lossless value compression. Extensive experiments\ndemonstrate that AsymKV consistently outperforms existing long-context methods\nacross various tasks and base models. For example, on LLaMA3.1-8B, AsymKV\nachieves an average score of 43.95 on LongBench, surpassing SOTA methods like\nH$_2$O (38.89) by a large margin.", "AI": {"tldr": "This paper identifies an asymmetry in keys and values within KV caches used for long-context processing in LLMs and proposes a training-free compression framework called AsymKV, which shows superior performance over SOTA methods.", "motivation": "Address the inefficiencies in long-context modeling caused by the quadratic complexity of attention mechanisms, particularly by improving KV cache compression methods.", "method": "Introduces AsymKV, a training-free compression framework that uses homogeneity-based key merging along with lossless value compression to exploit the newly discovered asymmetry in KV caches.", "result": "AsymKV significantly outperforms existing long-context compression methods. For example, it achieves an average score of 43.95 on LongBench with LLaMA3.1-8B, improving over SOTA methods like H$_2$O by a wide margin (38.89).", "conclusion": "The proposed AsymKV framework resolves the key-value asymmetry in KV cache compression, demonstrating its effectiveness in extending context length efficiently for multiple LLM-based tasks."}}
{"id": "2506.05443", "pdf": "https://arxiv.org/pdf/2506.05443", "abs": "https://arxiv.org/abs/2506.05443", "authors": ["Yiyu Lin", "Yan Wang", "You Zhou", "Xinye Ni", "Jiahui Wu", "Sen Yang"], "title": "UniPTMs: The First Unified Multi-type PTM Site Prediction Model via Master-Slave Architecture-Based Multi-Stage Fusion Strategy and Hierarchical Contrastive Loss", "categories": ["cs.LG", "cs.AI", "q-bio.GN"], "comment": null, "summary": "As a core mechanism of epigenetic regulation in eukaryotes, protein\npost-translational modifications (PTMs) require precise prediction to decipher\ndynamic life activity networks. To address the limitations of existing deep\nlearning models in cross-modal feature fusion, domain generalization, and\narchitectural optimization, this study proposes UniPTMs: the first unified\nframework for multi-type PTM prediction. The framework innovatively establishes\na \"Master-Slave\" dual-path collaborative architecture: The master path\ndynamically integrates high-dimensional representations of protein sequences,\nstructures, and evolutionary information through a Bidirectional Gated\nCross-Attention (BGCA) module, while the slave path optimizes feature\ndiscrepancies and recalibration between structural and traditional features\nusing a Low-Dimensional Fusion Network (LDFN). Complemented by a Multi-scale\nAdaptive convolutional Pyramid (MACP) for capturing local feature patterns and\na Bidirectional Hierarchical Gated Fusion Network (BHGFN) enabling multi-level\nfeature integration across paths, the framework employs a Hierarchical Dynamic\nWeighting Fusion (HDWF) mechanism to intelligently aggregate multimodal\nfeatures. Enhanced by a novel Hierarchical Contrastive loss function for\nfeature consistency optimization, UniPTMs demonstrates significant performance\nimprovements (3.2%-11.4% MCC and 4.2%-14.3% AP increases) over state-of-the-art\nmodels across five modification types and transcends the Single-Type Prediction\nParadigm. To strike a balance between model complexity and performance, we have\nalso developed a lightweight variant named UniPTMs-mini.", "AI": {"tldr": "This paper presents UniPTMs, a novel approach for multi-type protein post-translational modification prediction, featuring advanced architectures and achieving superior performance.", "motivation": "Current deep learning models struggle in effectively fusing multimodal features, managing domain generalization, and optimizing architectures for protein PTM prediction, necessitating new methodologies.", "method": "The study introduces UniPTMs, a framework with a Master-Slave dual-path collaboration system combining advanced modules like BGCA, LDFN, MACP, BHGFN, and HDWF for effective multimodal feature integration.", "result": "UniPTMs outperforms existing models in prediction accuracy (with MCC improvements of 3.2%-11.4% and AP improvements of 4.2%-14.3%) across five types of PTMs, addressing limitations of prior models.", "conclusion": "UniPTMs establishes a unified framework for PTM prediction, excels in performance across diverse settings, and provides a lightweight version (UniPTMs-mini) for balanced complexity and accuracy."}}
{"id": "2506.05744", "pdf": "https://arxiv.org/pdf/2506.05744", "abs": "https://arxiv.org/abs/2506.05744", "authors": ["Gouki Minegishi", "Hiroki Furuta", "Takeshi Kojima", "Yusuke Iwasawa", "Yutaka Matsuo"], "title": "Topology of Reasoning: Understanding Large Reasoning Models through Reasoning Graph Properties", "categories": ["cs.AI"], "comment": null, "summary": "Recent large-scale reasoning models have achieved state-of-the-art\nperformance on challenging mathematical benchmarks, yet the internal mechanisms\nunderlying their success remain poorly understood. In this work, we introduce\nthe notion of a reasoning graph, extracted by clustering hidden-state\nrepresentations at each reasoning step, and systematically analyze three key\ngraph-theoretic properties: cyclicity, diameter, and small-world index, across\nmultiple tasks (GSM8K, MATH500, AIME 2024). Our findings reveal that distilled\nreasoning models (e.g., DeepSeek-R1-Distill-Qwen-32B) exhibit significantly\nmore recurrent cycles (about 5 per sample), substantially larger graph\ndiameters, and pronounced small-world characteristics (about 6x) compared to\ntheir base counterparts. Notably, these structural advantages grow with task\ndifficulty and model capacity, with cycle detection peaking at the 14B scale\nand exploration diameter maximized in the 32B variant, correlating positively\nwith accuracy. Furthermore, we show that supervised fine-tuning on an improved\ndataset systematically expands reasoning graph diameters in tandem with\nperformance gains, offering concrete guidelines for dataset design aimed at\nboosting reasoning capabilities. By bridging theoretical insights into\nreasoning graph structures with practical recommendations for data\nconstruction, our work advances both the interpretability and the efficacy of\nlarge reasoning models.", "AI": {"tldr": "The paper investigates reasoning graphs in large-scale models to understand their internal mechanisms and proposes graph-theoretic insights for model improvement.", "motivation": "Large-scale reasoning models perform well but their internal mechanisms are poorly understood, prompting the need for systematic analysis.", "method": "The study introduces reasoning graphs extracted from clustering hidden-state representations, analyzing graph properties like cyclicity, diameter, and small-world index on various tasks and comparing distilled and base models.", "result": "Distilled models showed more recurrent cycles, larger graph diameters, and stronger small-world characteristics, with these features correlating positively with task difficulty, model size, and accuracy.", "conclusion": "Graph-theoretic insights can improve dataset design and boost interpretability and performance of reasoning models."}}
{"id": "2506.06247", "pdf": "https://arxiv.org/pdf/2506.06247", "abs": "https://arxiv.org/abs/2506.06247", "authors": ["Sedick David Baker Effendi", "Xavier Pinho", "Andrei Michael Dreyer", "Fabian Yamaguchi"], "title": "Scalable Language Agnostic Taint Tracking using Explicit Data Dependencies", "categories": ["cs.SE", "D.2.4"], "comment": "9 pages including appendix, SOAP'25", "summary": "Taint analysis using explicit whole-program data-dependence graphs is\npowerful for vulnerability discovery but faces two major challenges. First,\naccurately modeling taint propagation through calls to external library\nprocedures requires extensive manual annotations, which becomes impractical for\nlarge ecosystems. Second, the sheer size of whole-program graph representations\nleads to serious scalability and performance issues, particularly when quick\nanalysis is needed in continuous development pipelines.\n  This paper presents the design and implementation of a system for a\nlanguage-agnostic data-dependence representation. The system accommodates\nmissing annotations describing the behavior of library procedures by\nover-approximating data flows, allowing annotations to be added later without\nrecalculation. We contribute this data-flow analysis system to the open-source\ncode analysis platform Joern making it available to the community.", "AI": {"tldr": "The paper introduces a scalable, language-agnostic system for taint analysis by over-approximating data flows in the absence of library procedure annotations.", "motivation": "Taint analysis faces challenges due to the need for extensive manual annotations for library procedures and scalability issues with whole-program data-dependence graphs.", "method": "The authors design a system that over-approximates data flows to handle missing annotations and integrate it into the open-source analysis platform Joern.", "result": "The proposed system effectively accommodates missing annotations and improves scalability for vulnerability discovery.", "conclusion": "The system offers a practical solution for taint analysis challenges, making it easier to analyze large codebases and enhancing its potential utility for the community."}}
{"id": "2506.06012", "pdf": "https://arxiv.org/pdf/2506.06012", "abs": "https://arxiv.org/abs/2506.06012", "authors": ["Kaiyuan Chen", "Zhengjie Hu", "Shaolin Zhang", "Yuanqing Xia", "Wannian Liang", "Shuo Wang"], "title": "Enhanced Trust Region Sequential Convex Optimization for Multi-Drone Thermal Screening Trajectory Planning in Urban Environments", "categories": ["cs.RO", "cs.SY", "eess.SY", "math.OC"], "comment": null, "summary": "The rapid detection of abnormal body temperatures in urban populations is\nessential for managing public health risks, especially during outbreaks of\ninfectious diseases. Multi-drone thermal screening systems offer promising\nsolutions for fast, large-scale, and non-intrusive human temperature\nmonitoring. However, trajectory planning for multiple drones in complex urban\nenvironments poses significant challenges, including collision avoidance,\ncoverage efficiency, and constrained flight environments. In this study, we\npropose an enhanced trust region sequential convex optimization (TR-SCO)\nalgorithm for optimal trajectory planning of multiple drones performing thermal\nscreening tasks. Our improved algorithm integrates a refined convex\noptimization formulation within a trust region framework, effectively balancing\ntrajectory smoothness, obstacle avoidance, altitude constraints, and maximum\nscreening coverage. Simulation results demonstrate that our approach\nsignificantly improves trajectory optimality and computational efficiency\ncompared to conventional convex optimization methods. This research provides\ncritical insights and practical contributions toward deploying efficient\nmulti-drone systems for real-time thermal screening in urban areas. For reader\nwho are interested in our research, we release our source code at\nhttps://github.com/Cherry0302/Enhanced-TR-SCO.", "AI": {"tldr": "The paper discusses an improved algorithm for managing drone trajectories in urban thermal screening, emphasizing obstacle avoidance, smooth pathways, and coverage.", "motivation": "With the need for rapid temperature checks during health crises, deploying drones for large-scale thermal screening offers efficiency but poses challenges in trajectory management.", "method": "An enhanced trust region sequential convex optimization (TR-SCO) method was developed for smooth, obstacle-aware flight paths with altitude and coverage considerations.", "result": "Simulation results showed better trajectory planning, higher computational efficiency, and improved coverage for multi-drone systems compared to traditional methods.", "conclusion": "The research advances multi-drone technology, enabling real-time thermal monitoring with practical solutions for urban population health management during disease outbreaks."}}
{"id": "2506.05377", "pdf": "https://arxiv.org/pdf/2506.05377", "abs": "https://arxiv.org/abs/2506.05377", "authors": ["Shayantani Kar", "B. Shresth Bhimrajka", "Aditya Kumar", "Sahil Gupta", "Sourav Ghosh", "Subhamita Mukherjee", "Shauvik Paul"], "title": "An Independent Discriminant Network Towards Identification of Counterfeit Images and Videos", "categories": ["cs.CV"], "comment": "This research was conducted by student and professor co-authors from\n  Techno Main Salt Lake, with co-author Sourav Ghosh serving as an alumni\n  mentor in an invited capacity -- distinct from his primary affiliation and\n  pre-approved by his employer. This preprint presents research originally\n  completed in early 2023 and published in IETE Journal of Research in 2025", "summary": "Rapid spread of false images and videos on online platforms is an emerging\nproblem. Anyone may add, delete, clone or modify people and entities from an\nimage using various editing software which are readily available. This\ngenerates false and misleading proof to hide the crime. Now-a-days, these false\nand counterfeit images and videos are flooding on the internet. These spread\nfalse information. Many methods are available in literature for detecting those\ncounterfeit contents but new methods of counterfeiting are also evolving.\nGenerative Adversarial Networks (GAN) are observed to be one effective method\nas it modifies the context and definition of images producing plausible results\nvia image-to-image translation. This work uses an independent discriminant\nnetwork that can identify GAN generated image or video. A discriminant network\nhas been created using a convolutional neural network based on\nInceptionResNetV2. The article also proposes a platform where users can detect\nforged images and videos. This proposed work has the potential to help the\nforensics domain to detect counterfeit videos and hidden criminal evidence\ntowards the identification of criminal activities.", "AI": {"tldr": "This paper addresses the problem of detecting false images and videos on online platforms using a discriminant network based on InceptionResNetV2.", "motivation": "The motivation is to combat the rapid spread of counterfeit images and videos on the internet, which mislead and hide criminal activities.", "method": "The paper employs a convolutional neural network (CNN) based on InceptionResNetV2 to develop an independent discriminant network that identifies GAN-generated content. A specific platform is also proposed for detecting forged content.", "result": "The proposed approach demonstrates its potential to detect forged images and videos created using GAN and aids in uncovering criminal evidence.", "conclusion": "The work is a step forward in helping the forensics domain detect and counter the problem of counterfeit visual content online, supporting criminal investigations."}}
{"id": "2506.05413", "pdf": "https://arxiv.org/pdf/2506.05413", "abs": "https://arxiv.org/abs/2506.05413", "authors": ["Patrik Czak\u00f3", "G\u00e1bor Kert\u00e9sz", "S\u00e1ndor Sz\u00e9n\u00e1si"], "title": "SmoothRot: Combining Channel-Wise Scaling and Rotation for Quantization-Friendly LLMs", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "6 pages, 3 figures, 5 tables. Submitted to the IEEE SMC 2025\n  conference", "summary": "We present SmoothRot, a novel post-training quantization technique to enhance\nthe efficiency of 4-bit quantization in Large Language Models (LLMs). SmoothRot\naddresses the critical challenge of massive activation outliers, by integrating\nchannel-wise scaling with Hadamard transformations. Our technique effectively\ntransforms extreme outliers into quantization-friendly activations,\nsignificantly improving quantization accuracy. Experiments conducted on popular\nLLMs (LLaMA2 7B, LLaMA3.1 8B, and Mistral 7B) demonstrate that SmoothRot\nconsistently reduces the performance gap between quantized and FP16 models by\napproximately 10-30\\% across language generation and zero-shot reasoning tasks,\nwithout introducing additional inference latency. Code is available at\nhttps://github.com/czakop/smoothrot.", "AI": {"tldr": "SmoothRot introduces a novel post-training quantization technique to handle activation outliers in 4-bit quantization for LLMs, using channel-wise scaling and Hadamard transformations.", "motivation": "The authors aim to tackle the challenge of massive activation outliers that hinder efficient quantization in large language models (LLMs).", "method": "SmoothRot combines channel-wise scaling with Hadamard transformations to transform extreme activation outliers into more quantization-friendly forms.", "result": "The method reduces the performance gap between 4-bit quantized models and FP16 models by 10-30% across common tasks without introducing additional inference latency.", "conclusion": "SmoothRot significantly enhances the efficiency and accuracy of post-training quantization for LLMs, offering practical benefits for deployment without latency costs."}}
{"id": "2506.05445", "pdf": "https://arxiv.org/pdf/2506.05445", "abs": "https://arxiv.org/abs/2506.05445", "authors": ["Thanh Vinh Vo", "Young Lee", "Haozhe Ma", "Chien Lu", "Tze-Yun Leong"], "title": "Causal Policy Learning in Reinforcement Learning: Backdoor-Adjusted Soft Actor-Critic", "categories": ["cs.LG", "cs.AI"], "comment": "Preprint", "summary": "Hidden confounders that influence both states and actions can bias policy\nlearning in reinforcement learning (RL), leading to suboptimal or\nnon-generalizable behavior. Most RL algorithms ignore this issue, learning\npolicies from observational trajectories based solely on statistical\nassociations rather than causal effects. We propose DoSAC (Do-Calculus Soft\nActor-Critic with Backdoor Adjustment), a principled extension of the SAC\nalgorithm that corrects for hidden confounding via causal intervention\nestimation. DoSAC estimates the interventional policy $\\pi(a | \\mathrm{do}(s))$\nusing the backdoor criterion, without requiring access to true confounders or\ncausal labels. To achieve this, we introduce a learnable Backdoor Reconstructor\nthat infers pseudo-past variables (previous state and action) from the current\nstate to enable backdoor adjustment from observational data. This module is\nintegrated into a soft actor-critic framework to compute both the\ninterventional policy and its entropy. Empirical results on continuous control\nbenchmarks show that DoSAC outperforms baselines under confounded settings,\nwith improved robustness, generalization, and policy reliability.", "AI": {"tldr": "This paper introduces DoSAC, an algorithm that mitigates hidden confounder bias in reinforcement learning by combining the SAC framework with causal interventions.", "motivation": "The paper addresses the problem of hidden confounders in reinforcement learning, which can lead to biased policy learning and suboptimal behavior.", "method": "The proposed method, DoSAC, leverages causal intervention estimation using the backdoor criterion, aided by a learnable Backdoor Reconstructor to infer pseudo-past variables for effective policy learning.", "result": "Empirical results demonstrate that DoSAC outperforms baseline methods on continuous control tasks under confounded settings, showing enhanced robustness, generalization, and policy reliability.", "conclusion": "The study concludes that integrating causal reasoning into RL frameworks like SAC can significantly improve performance in environments with hidden confounding issues."}}
{"id": "2506.05745", "pdf": "https://arxiv.org/pdf/2506.05745", "abs": "https://arxiv.org/abs/2506.05745", "authors": ["Emil Biju", "Shayan Talaei", "Zhemin Huang", "Mohammadreza Pourreza", "Azalia Mirhoseini", "Amin Saberi"], "title": "SPRINT: Enabling Interleaved Planning and Parallelized Execution in Reasoning Models", "categories": ["cs.AI", "cs.LG"], "comment": "Emil Biju, Shayan Talaei, and Zhemin Huang contributed equally to\n  this work", "summary": "Large reasoning models (LRMs) excel at complex reasoning tasks but typically\ngenerate lengthy sequential chains-of-thought, resulting in long inference\ntimes before arriving at the final answer. To address this challenge, we\nintroduce SPRINT, a novel post-training and inference-time framework designed\nto enable LRMs to dynamically identify and exploit opportunities for\nparallelization during their reasoning process. SPRINT incorporates an\ninnovative data curation pipeline that reorganizes natural language reasoning\ntrajectories into structured rounds of long-horizon planning and parallel\nexecution. By fine-tuning LRMs on a small amount of such curated data, the\nmodels learn to dynamically identify independent subtasks within extended\nreasoning processes and effectively execute them in parallel. Through extensive\nevaluations, we show that the models fine-tuned with the SPRINT framework match\nthe performance of reasoning models on complex domains such as mathematics\nwhile generating up to ~39% fewer sequential tokens on problems requiring more\nthan 8000 output tokens. Finally, we observe consistent results transferred to\ntwo out-of-distribution tasks of GPQA and Countdown with up to 45% and 65%\nreduction in average sequential tokens for longer reasoning trajectories, while\nachieving the performance of the fine-tuned reasoning model.", "AI": {"tldr": "SPRINT is a post-training framework that allows large reasoning models (LRMs) to perform complex tasks more quickly by parallelizing reasoning processes, achieving significantly reduced output tokens while maintaining performance.", "motivation": "The slow inference times of LRMs during complex reasoning tasks due to sequential chains-of-thought inspired the need for a framework to optimize their reasoning processes.", "method": "SPRINT uses a data curation pipeline to structure tasks into rounds of long-horizon planning and parallel execution. LRMs are fine-tuned on curated datasets to identify independent subtasks and execute them in parallel.", "result": "The models fine-tuned with SPRINT reduced sequential tokens by ~39% on tasks requiring over 8000 tokens, while maintaining comparable performance in mathematical problem-solving and achieving up to 45%-65% token reductions on out-of-distribution tasks.", "conclusion": "The SPRINT framework enables faster reasoning in LRMs by leveraging parallelization, demonstrating its potential to significantly improve reasoning speed without compromising accuracy."}}
{"id": "2506.06251", "pdf": "https://arxiv.org/pdf/2506.06251", "abs": "https://arxiv.org/abs/2506.06251", "authors": ["Jingyu Xiao", "Ming Wang", "Man Ho Lam", "Yuxuan Wan", "Junliang Liu", "Yintong Huo", "Michael R. Lyu"], "title": "DesignBench: A Comprehensive Benchmark for MLLM-based Front-end Code Generation", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable\ncapabilities in automated front-end engineering, e.g., generating UI code from\nvisual designs. However, existing front-end UI code generation benchmarks have\nthe following limitations: (1) While framework-based development becomes\npredominant in modern front-end programming, current benchmarks fail to\nincorporate mainstream development frameworks. (2) Existing evaluations focus\nsolely on the UI code generation task, whereas practical UI development\ninvolves several iterations, including refining editing, and repairing issues.\n(3) Current benchmarks employ unidimensional evaluation, lacking investigation\ninto influencing factors like task difficulty, input context variations, and\nin-depth code-level analysis. To bridge these gaps, we introduce DesignBench, a\nmulti-framework, multi-task evaluation benchmark for assessing MLLMs'\ncapabilities in automated front-end engineering. DesignBench encompasses three\nwidely-used UI frameworks (React, Vue, and Angular) alongside vanilla HTML/CSS,\nand evaluates on three essential front-end tasks (generation, edit, and repair)\nin real-world development workflows. DesignBench contains 900 webpage samples\nspanning over 11 topics, 9 edit types, and 6 issue categories, enabling\ndetailed analysis of MLLM performance across multiple dimensions. Our\nsystematic evaluation reveals critical insights into MLLMs' framework-specific\nlimitations, task-related bottlenecks, and performance variations under\ndifferent conditions, providing guidance for future research in automated\nfront-end development. Our code and data are available at\nhttps://github.com/WebPAI/DesignBench.", "AI": {"tldr": "DesignBench is a benchmark designed to assess Multimodal Large Language Models (MLLMs) in automated front-end engineering tasks across frameworks and multiple dimensions.", "motivation": "Existing benchmarks for front-end UI code generation have major limitations, including the lack of support for common development frameworks, exclusion of iterative practical tasks like editing and repairing, and superficial evaluations without exploring task difficulty or code-level analysis.", "method": "The authors propose DesignBench, a multi-framework evaluation benchmark that evaluates MLLMs on tasks like UI code generation, editing, and repair. It includes popular UI frameworks (React, Vue, Angular, and vanilla HTML/CSS) and features 900 webpage samples across diverse topics, edit types, and issue categories.", "result": "The evaluation provided detailed insights into MLLMs regarding their limitations across frameworks, bottlenecks in specific tasks, and performance variations under diverse conditions.", "conclusion": "DesignBench enables systematic and comprehensive evaluation of MLLMs in automated front-end engineering, offering valuable insights and tools for advancing research in this field and identifying areas for improvement."}}
{"id": "2506.06028", "pdf": "https://arxiv.org/pdf/2506.06028", "abs": "https://arxiv.org/abs/2506.06028", "authors": ["Nikunj Shah", "Utsav Dey", "Kenji Nishimiya"], "title": "End-to-End Framework for Robot Lawnmower Coverage Path Planning using Cellular Decomposition", "categories": ["cs.RO", "cs.AI"], "comment": "8 pages, ICRA 2025, Workshop on Field Robotics", "summary": "Efficient Coverage Path Planning (CPP) is necessary for autonomous robotic\nlawnmowers to effectively navigate and maintain lawns with diverse and\nirregular shapes. This paper introduces a comprehensive end-to-end pipeline for\nCPP, designed to convert user-defined boundaries on an aerial map into\noptimized coverage paths seamlessly. The pipeline includes user input\nextraction, coordinate transformation, area decomposition and path generation\nusing our novel AdaptiveDecompositionCPP algorithm, preview and customization\nthrough an interactive coverage path visualizer, and conversion to actionable\nGPS waypoints. The AdaptiveDecompositionCPP algorithm combines cellular\ndecomposition with an adaptive merging strategy to reduce non-mowing travel\nthereby enhancing operational efficiency. Experimental evaluations,\nencompassing both simulations and real-world lawnmower tests, demonstrate the\neffectiveness of the framework in coverage completeness and mowing efficiency.", "AI": {"tldr": "This paper presents an advanced algorithm and pipeline for robotic lawnmowers to efficiently achieve coverage path planning (CPP), tailored to lawns of diverse shapes.", "motivation": "Autonomous robotic lawnmowers require efficient CPP to navigate and maintain lawns with complex shapes while minimizing non-mowing travel.", "method": "The authors develop an end-to-end pipeline featuring a novel AdaptiveDecompositionCPP algorithm that combines cellular decomposition with adaptive merging to produce optimized coverage paths and reduce operational inefficiencies.", "result": "Experimental evaluations, including simulations and real-world tests, confirm that the proposed framework achieves high effectiveness in coverage completeness and mowing efficiency.", "conclusion": "The introduced pipeline and AdaptiveDecompositionCPP algorithm successfully streamline CPP for robotic lawnmowers and demonstrate improved real-world performance in both coverage and efficiency."}}
{"id": "2506.05583", "pdf": "https://arxiv.org/pdf/2506.05583", "abs": "https://arxiv.org/abs/2506.05583", "authors": ["Nien-Shao Wang", "Duygu Nur Yaldiz", "Yavuz Faruk Bakman", "Sai Praneeth Karimireddy"], "title": "Conformal Prediction Adaptive to Unknown Subpopulation Shifts", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "20 pages, 6 figures, 5 tables, submitted to NeurIPS 2025", "summary": "Conformal prediction is widely used to equip black-box machine learning\nmodels with uncertainty quantification enjoying formal coverage guarantees.\nHowever, these guarantees typically break down in the presence of distribution\nshifts, where the data distribution at test time differs from the training (or\ncalibration-time) distribution. In this work, we address subpopulation shifts,\nwhere the test environment exhibits an unknown and differing mixture of\nsubpopulations compared to the calibration data. We propose new methods that\nprovably adapt conformal prediction to such shifts, ensuring valid coverage\nwithout requiring explicit knowledge of subpopulation structure. Our algorithms\nscale to high-dimensional settings and perform effectively in realistic machine\nlearning tasks. Extensive experiments on vision (with vision transformers) and\nlanguage (with large language models) benchmarks demonstrate that our methods\nreliably maintain coverage and controls risk in scenarios where standard\nconformal prediction fails.", "AI": {"tldr": "The paper develops methods to adapt conformal prediction to subpopulation shifts, maintaining coverage guarantees even under distribution shifts.", "motivation": "Conformal prediction lacks robustness when the test distribution differs from the calibration data, creating a need to address this limitation under subpopulation shifts.", "method": "The authors propose algorithms that adaptively modify conformal prediction methods without prior knowledge of subpopulation structures, designed to scale to high-dimensional settings.", "result": "The proposed methods outperformed standard conformal prediction by maintaining coverage and managing risk under subpopulation shifts, as validated through experiments in vision and language tasks.", "conclusion": "The methods extend the effectiveness and reliability of conformal prediction under distribution shifts, making it applicable to real-world machine learning tasks with unknown subpopulation structures."}}
{"id": "2506.05378", "pdf": "https://arxiv.org/pdf/2506.05378", "abs": "https://arxiv.org/abs/2506.05378", "authors": ["Mohit Arora", "Pratyush Shukla", "Shivali Chopra"], "title": "A Compendium of Autonomous Navigation using Object Detection and Tracking in Unmanned Aerial Vehicles", "categories": ["cs.CV", "cs.RO", "eess.IV"], "comment": null, "summary": "Unmanned Aerial Vehicles (UAVs) are one of the most revolutionary inventions\nof 21st century. At the core of a UAV lies the central processing system that\nuses wireless signals to control their movement. The most popular UAVs are\nquadcopters that use a set of four motors, arranged as two on either side with\nopposite spin. An autonomous UAV is called a drone. Drones have been in service\nin the US army since the 90's for covert missions critical to national\nsecurity. It would not be wrong to claim that drones make up an integral part\nof the national security and provide the most valuable service during\nsurveillance operations. While UAVs are controlled using wireless signals,\nthere reside some challenges that disrupt the operation of such vehicles such\nas signal quality and range, real time processing, human expertise, robust\nhardware and data security. These challenges can be solved by programming UAVs\nto be autonomous, using object detection and tracking, through Computer Vision\nalgorithms. Computer Vision is an interdisciplinary field that seeks the use of\ndeep learning to gain a high-level understanding of digital images and videos\nfor the purpose of automating the task of human visual system. Using computer\nvision, algorithms for detecting and tracking various objects can be developed\nsuitable to the hardware so as to allow real time processing for immediate\njudgement. This paper attempts to review the various approaches several authors\nhave proposed for the purpose of autonomous navigation of UAVs by through\nvarious algorithms of object detection and tracking in real time, for the\npurpose of applications in various fields such as disaster management, dense\narea exploration, traffic vehicle surveillance etc.", "AI": {"tldr": "The paper reviews techniques for enabling UAVs to autonomously navigate using computer vision-based object detection and tracking algorithms.", "motivation": "Enhancing the autonomy of UAVs to overcome challenges like signal disruption, real-time processing, and data security, enabling them to perform critical tasks more effectively.", "method": "Examines various algorithms in computer vision for object detection and real-time tracking tailored to UAV hardware, drawing insights from prior research.", "result": "The review highlights the applicability of computer vision algorithms in enabling autonomous UAV navigation, particularly in contexts such as disaster management and traffic monitoring.", "conclusion": "UAVs can achieve autonomous capabilities by leveraging computer vision technologies, which address existing challenges and enhance their utility across diverse applications."}}
{"id": "2506.05415", "pdf": "https://arxiv.org/pdf/2506.05415", "abs": "https://arxiv.org/abs/2506.05415", "authors": ["Ronaldo Luo", "Gary Liang", "Cindy Liu", "Adam Kabbara", "Minahil Bakhtawar", "Kina Kim", "Michael Guerzhoy"], "title": "Automatically Detecting Amusing Games in Wordle", "categories": ["cs.CL"], "comment": "Accepted to the Intenational Conference on Computational Creeativity\n  (ICCC) 2025", "summary": "We explore automatically predicting which Wordle games Reddit users find\namusing.\n  We scrape approximately 80k reactions by Reddit users to Wordle games from\nReddit, classify the reactions as expressing amusement or not using OpenAI's\nGPT-3.5 using few-shot prompting, and verify that GPT-3.5's labels roughly\ncorrespond to human labels.\n  We then extract features from Wordle games that can predict user amusement.\nWe demonstrate that the features indeed provide a (weak) signal that predicts\nuser amusement as predicted by GPT-3.5.\n  Our results indicate that user amusement at Wordle games can be predicted\ncomputationally to some extent. We explore which features of the game\ncontribute to user amusement.\n  We find that user amusement is predictable, indicating a measurable aspect of\ncreativity infused into Wordle games through humor.", "AI": {"tldr": "The paper predicts user amusement in Wordle games using Reddit reactions and GPT-3.5, extracting features to computationally analyze humor.", "motivation": "The authors aim to understand and predict user amusement in Wordle games through computational means, identifying key humor-inducing features.", "method": "Reddit reactions were scraped, classified by GPT-3.5 for amusement detection, and validated against human labels. Features of Wordle games were analyzed to correlate with amusement.", "result": "The study highlights that Wordle game features provide a weak but detectable signal for predicting amusement via GPT-3.5 labels.", "conclusion": "User amusement related to Wordle games can be computationally predicted, linking humor to measurable and creative aspects of gameplay."}}
{"id": "2506.05447", "pdf": "https://arxiv.org/pdf/2506.05447", "abs": "https://arxiv.org/abs/2506.05447", "authors": ["Andrei Mircea", "Supriyo Chakraborty", "Nima Chitsazan", "Irina Rish", "Ekaterina Lobacheva"], "title": "Training Dynamics Underlying Language Model Scaling Laws: Loss Deceleration and Zero-Sum Learning", "categories": ["cs.LG", "cs.AI", "I.2.7"], "comment": "Published as a conference paper at ACL 2025", "summary": "This work aims to understand how scaling improves language models,\nspecifically in terms of training dynamics. We find that language models\nundergo loss deceleration early in training; an abrupt slowdown in the rate of\nloss improvement, resulting in piecewise linear behaviour of the loss curve in\nlog-log space. Scaling up the model mitigates this transition by (1) decreasing\nthe loss at which deceleration occurs, and (2) improving the log-log rate of\nloss improvement after deceleration. We attribute loss deceleration to a type\nof degenerate training dynamics we term zero-sum learning (ZSL). In ZSL,\nper-example gradients become systematically opposed, leading to destructive\ninterference in per-example changes in loss. As a result, improving loss on one\nsubset of examples degrades it on another, bottlenecking overall progress. Loss\ndeceleration and ZSL provide new insights into the training dynamics underlying\nlanguage model scaling laws, and could potentially be targeted directly to\nimprove language models independent of scale. We make our code and artefacts\navailable at: https://github.com/mirandrom/zsl", "AI": {"tldr": "Scaling up language models mitigates loss deceleration, which is attributed to zero-sum learning (ZSL), a degenerate training dynamic causing destructive interference in gradients.", "motivation": "To analyze how scaling positively affects training dynamics in language models, particularly focusing on abrupt loss slowdown.", "method": "The study identifies loss deceleration patterns in training and attributes them to zero-sum learning (ZSL), which leads to opposing gradients in examples.", "result": "Scaling decreases the loss where deceleration occurs and improves post-deceleration loss trends, potentially bypassing ZSL's limitations.", "conclusion": "Understanding and targeting ZSL dynamics could optimize language model training beyond scaling alone, providing new paths for improvement."}}
{"id": "2506.05754", "pdf": "https://arxiv.org/pdf/2506.05754", "abs": "https://arxiv.org/abs/2506.05754", "authors": ["Emmanuel Anaya Gonzalez", "Sairam Vaidya", "Kanghee Park", "Ruyi Ji", "Taylor Berg-Kirkpatrick", "Loris D'Antoni"], "title": "Constrained Sampling for Language Models Should Be Easy: An MCMC Perspective", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Constrained decoding enables Language Models (LMs) to produce samples that\nprovably satisfy hard constraints. However, existing constrained-decoding\napproaches often distort the underlying model distribution, a limitation that\nis especially problematic in applications like program fuzzing, where one wants\nto generate diverse and valid program inputs for testing purposes. We propose a\nnew constrained sampling framework based on Markov Chain Monte Carlo (MCMC)\nthat simultaneously satisfies three core desiderata: constraint satisfying\n(every sample satisfies the constraint), monotonically converging (the sampling\nprocess converges to the true conditional distribution), and efficient\n(high-quality samples emerge in few steps). Our method constructs a proposal\ndistribution over valid outputs and applies a Metropolis-Hastings acceptance\ncriterion based on the LM's likelihood, ensuring principled and efficient\nexploration of the constrained space. Empirically, our sampler outperforms\nexisting methods on both synthetic benchmarks and real-world program fuzzing\ntasks.", "AI": {"tldr": "The paper introduces an MCMC-based constrained sampling framework to generate samples that satisfy hard constraints without distorting the language model\u2019s distribution.", "motivation": "Existing constrained-decoding methods often alter the LM's underlying distribution, which is problematic for tasks requiring diverse, constraint-satisfying outputs, such as program fuzzing.", "method": "The proposed method constructs a proposal distribution over valid outputs and employs a Metropolis-Hastings acceptance criterion that leverages the LM's likelihood for efficient exploration of constrained spaces.", "result": "Their method demonstrates superior performance compared to existing techniques on both synthetic benchmarks and real-world program fuzzing tasks.", "conclusion": "This framework offers a principled and efficient approach to constrained sampling, achieving constraint satisfaction, convergence to the true distribution, and sampling efficiency."}}
{"id": "2506.06161", "pdf": "https://arxiv.org/pdf/2506.06161", "abs": "https://arxiv.org/abs/2506.06161", "authors": ["Yufeng Wang", "Yuhong Feng", "Yixuan Cao", "Haoran Li", "Haiyue Feng", "Yifeng Wang"], "title": "Obfuscation-Resilient Binary Code Similarity Analysis using Dominance Enhanced Semantic Graph", "categories": ["cs.CR", "cs.SE"], "comment": null, "summary": "Binary code similarity analysis (BCSA) serves as a core technique for binary\nanalysis tasks such as vulnerability detection. While current graph-based BCSA\napproaches capture substantial semantics and show strong performance, their\nperformance suffers under code obfuscation due to the unstable control flow. To\naddress this issue, we develop ORCAS, an Obfuscation-Resilient BCSA model based\non Dominance Enhanced Semantic Graph (DESG). The DESG is an original binary\ncode representation, capturing more binaries' implicit semantics without\ncontrol flow structure, including inter-instruction relations, inter-basic\nblock relations, and instruction-basic block relations. ORCAS robustly scores\nsemantic similarity across binary functions from different obfuscation options,\noptimization levels, and instruction set architectures. Extensive evaluation on\nthe BinKit dataset shows ORCAS significantly outperforms eight baselines,\nachieving an average 12.1% PR-AUC gain when using combined three obfuscation\noptions compared to the state-of-the-art approaches. Furthermore, ORCAS\nimproves recall by up to 43% on an original obfuscated real-world vulnerability\ndataset, which we released to facilitate future research.", "AI": {"tldr": "ORCAS is a BCSA model designed to overcome obfuscation challenges by introducing DESG, a robust code representation, and achieves superior performance compared to existing methods.", "motivation": "BCSA methods face challenges in handling code obfuscation due to unstable control flow structures, impacting their effectiveness in tasks like vulnerability detection.", "method": "The paper introduces ORCAS, utilizing the Dominance Enhanced Semantic Graph (DESG) to represent binary code without relying on control flow. This captures relational semantics within binaries and ensures resilience to obfuscation.", "result": "ORCAS outperforms state-of-the-art models, showing a 12.1% gain in PR-AUC on the BinKit dataset under multiple obfuscation conditions, and improves recall by up to 43% on a newly released real-world obfuscation dataset.", "conclusion": "ORCAS presents a significant advancement in BCSA, offering robust performance across various obfuscation settings and promoting further research with a new dataset."}}
{"id": "2506.06072", "pdf": "https://arxiv.org/pdf/2506.06072", "abs": "https://arxiv.org/abs/2506.06072", "authors": ["Hongyi Zhou", "Weiran Liao", "Xi Huang", "Yucheng Tang", "Fabian Otto", "Xiaogang Jia", "Xinkai Jiang", "Simon Hilber", "Ge Li", "Qian Wang", "\u00d6mer Erdin\u00e7 Ya\u011fmurlu", "Nils Blank", "Moritz Reuss", "Rudolf Lioutikov"], "title": "BEAST: Efficient Tokenization of B-Splines Encoded Action Sequences for Imitation Learning", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "We present the B-spline Encoded Action Sequence Tokenizer (BEAST), a novel\naction tokenizer that encodes action sequences into compact discrete or\ncontinuous tokens using B-splines. In contrast to existing action tokenizers\nbased on vector quantization or byte pair encoding, BEAST requires no separate\ntokenizer training and consistently produces tokens of uniform length, enabling\nfast action sequence generation via parallel decoding. Leveraging our B-spline\nformulation, BEAST inherently ensures generating smooth trajectories without\ndiscontinuities between adjacent segments. We extensively evaluate BEAST by\nintegrating it with three distinct model architectures: a Variational\nAutoencoder (VAE) with continuous tokens, a decoder-only Transformer with\ndiscrete tokens, and Florence-2, a pretrained Vision-Language Model with an\nencoder-decoder architecture, demonstrating BEAST's compatibility and\nscalability with large pretrained models. We evaluate BEAST across three\nestablished benchmarks consisting of 166 simulated tasks and on three distinct\nrobot settings with a total of 8 real-world tasks. Experimental results\ndemonstrate that BEAST (i) significantly reduces both training and inference\ncomputational costs, and (ii) consistently generates smooth, high-frequency\ncontrol signals suitable for continuous control tasks while (iii) reliably\nachieves competitive task success rates compared to state-of-the-art methods.", "AI": {"tldr": "BEAST is a novel action tokenizer that encodes action sequences compactly via B-splines, ensuring uniform tokens and smooth trajectories, enabling faster generation and competitive performance.", "motivation": "To address inefficiencies and complexities in existing action tokenizers, such as separate tokenizer training and discontinuities in trajectory generation.", "method": "BEAST uses a B-spline formulation to encode action sequences, eliminating the need for separate training and ensuring uniform and smooth trajectory generation. It\u2019s compatible with various neural model architectures.", "result": "BEAST reduces computational costs, generates smooth high-frequency control signals, and achieves competitive task success rates across simulated and real-world task benchmarks.", "conclusion": "BEAST is a versatile and efficient action tokenizer that integrates seamlessly with different models while ensuring smooth and computationally efficient action generation."}}
{"id": "2506.05596", "pdf": "https://arxiv.org/pdf/2506.05596", "abs": "https://arxiv.org/abs/2506.05596", "authors": ["Jes Frellsen", "Maher M. Kassem", "Tone Bengtsen", "Lars Olsen", "Kresten Lindorff-Larsen", "Jesper Ferkinghoff-Borg", "Wouter Boomsma"], "title": "Zero-shot protein stability prediction by inverse folding models: a free energy interpretation", "categories": ["cs.LG", "cs.AI", "q-bio.BM", "stat.ML"], "comment": null, "summary": "Inverse folding models have proven to be highly effective zero-shot\npredictors of protein stability. Despite this success, the link between the\namino acid preferences of an inverse folding model and the free-energy\nconsiderations underlying thermodynamic stability remains incompletely\nunderstood. A better understanding would be of interest not only from a\ntheoretical perspective, but also potentially provide the basis for stronger\nzero-shot stability prediction. In this paper, we take steps to clarify the\nfree-energy foundations of inverse folding models. Our derivation reveals the\nstandard practice of likelihood ratios as a simplistic approximation and\nsuggests several paths towards better estimates of the relative stability. We\nempirically assess these approaches and demonstrate that considerable gains in\nzero-shot performance can be achieved with fairly simple means.", "AI": {"tldr": "The paper explores the connection between inverse protein folding models and thermodynamic stability, suggesting enhancements for improved zero-shot stability predictions.", "motivation": "To clarify the link between inverse folding model predictions and thermodynamic stability, enabling better theoretical understanding and stronger zero-shot protein stability predictions.", "method": "The paper derives the free-energy foundations of inverse folding models, analyzing and suggesting improvements to the standard practice of likelihood ratios for stability estimation.", "result": "The research demonstrates that improved zero-shot protein stability predictions are possible through straightforward modifications to current methods.", "conclusion": "Simple yet effective adjustments to inverse folding model approaches can significantly enhance predictions of protein stability."}}
{"id": "2506.05383", "pdf": "https://arxiv.org/pdf/2506.05383", "abs": "https://arxiv.org/abs/2506.05383", "authors": ["Abu Sufian", "Marco Leo", "Cosimo Distante", "Anirudha Ghosh", "Debaditya Barman"], "title": "Can Vision Transformers with ResNet's Global Features Fairly Authenticate Demographic Faces?", "categories": ["cs.CV"], "comment": "14 pages, 6 Figures, ICPR 2024 Workshop FAIRBIO", "summary": "Biometric face authentication is crucial in computer vision, but ensuring\nfairness and generalization across demographic groups remains a big challenge.\nTherefore, we investigated whether Vision Transformer (ViT) and ResNet,\nleveraging pre-trained global features, can fairly authenticate different\ndemographic faces while relying minimally on local features. In this\ninvestigation, we used three pre-trained state-of-the-art (SOTA) ViT foundation\nmodels from Facebook, Google, and Microsoft for global features as well as\nResNet-18. We concatenated the features from ViT and ResNet, passed them\nthrough two fully connected layers, and trained on customized face image\ndatasets to capture the local features. Then, we designed a novel few-shot\nprototype network with backbone features embedding. We also developed new\ndemographic face image support and query datasets for this empirical study. The\nnetwork's testing was conducted on this dataset in one-shot, three-shot, and\nfive-shot scenarios to assess how performance improves as the size of the\nsupport set increases. We observed results across datasets with varying\nraces/ethnicities, genders, and age groups. The Microsoft Swin Transformer\nbackbone performed better among the three SOTA ViT for this task. The code and\ndata are available at: https://github.com/Sufianlab/FairVitBio.", "AI": {"tldr": "This study investigates biometric face authentication fairness across demographics using Vision Transformer (ViT) and ResNet models. It introduces a novel few-shot prototype network and evaluates its performance under different support set scenarios.", "motivation": "The paper aims to address the challenge of ensuring fairness and generalization in biometric face authentication across diverse demographic groups while minimizing reliance on local facial features.", "method": "Global features from three pre-trained ViT models (Facebook, Google, Microsoft) and ResNet-18 are concatenated and fine-tuned using customized datasets. A novel few-shot prototype network is proposed and tested with one-shot, three-shot, and five-shot strategies on new demographic datasets.", "result": "The Microsoft Swin Transformer backbone achieved superior performance among the evaluated models. Performance improved as the shot size of the support set increased.", "conclusion": "Integrating both global and local features enhances demographic fairness in face authentication. The proposed methods show promise, particularly with the Microsoft Swin Transformer backbone."}}
{"id": "2506.05453", "pdf": "https://arxiv.org/pdf/2506.05453", "abs": "https://arxiv.org/abs/2506.05453", "authors": ["Hongbo Zhao", "Fei Zhu", "Rundong Wang", "Gaofeng Meng", "Zhaoxiang Zhang"], "title": "MLLM-CL: Continual Learning for Multimodal Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "Recent Multimodal Large Language Models (MLLMs) excel in vision-language\nunderstanding but face challenges in adapting to dynamic real-world scenarios\nthat require continuous integration of new knowledge and skills. While\ncontinual learning (CL) offers a potential solution, existing benchmarks and\nmethods suffer from critical limitations. In this paper, we introduce MLLM-CL,\na novel benchmark encompassing domain and ability continual learning, where the\nformer focuses on independently and identically distributed (IID) evaluation\nacross evolving mainstream domains, whereas the latter evaluates on non-IID\nscenarios with emerging model ability. Methodologically, we propose preventing\ncatastrophic interference through parameter isolation, along with an MLLM-based\nrouting mechanism. Extensive experiments demonstrate that our approach can\nintegrate domain-specific knowledge and functional abilities with minimal\nforgetting, significantly outperforming existing methods.", "AI": {"tldr": "The paper introduces MLLM-CL, a benchmark for continual learning in multimodal large language models (MLLMs), and proposes a novel approach to mitigate catastrophic forgetting.", "motivation": "To address the limitations of current benchmarks and methods in adapting multimodal large language models to real-world dynamic scenarios requiring continuous learning.", "method": "The paper introduces MLLM-CL, a benchmark for domain and ability continual learning, and suggests parameter isolation combined with an MLLM-based routing mechanism to prevent catastrophic interference.", "result": "The proposed method demonstrated the ability to integrate new domain-specific knowledge and skills with minimal forgetting, significantly outperforming existing methods in experiments.", "conclusion": "The novel benchmark and method provide an effective framework for continual learning in MLLMs, solving challenges like catastrophic forgetting and improving performance."}}
{"id": "2506.05810", "pdf": "https://arxiv.org/pdf/2506.05810", "abs": "https://arxiv.org/abs/2506.05810", "authors": ["Yesheng Zhang", "Wenjian Sun", "Yuheng Chen", "Qingwei Liu", "Qi Lin", "Rui Zhang", "Xu Zhao"], "title": "Trajectory Entropy: Modeling Game State Stability from Multimodality Trajectory Prediction", "categories": ["cs.AI", "cs.RO"], "comment": "10 pages", "summary": "Complex interactions among agents present a significant challenge for\nautonomous driving in real-world scenarios. Recently, a promising approach has\nemerged, which formulates the interactions of agents as a level-k game\nframework. It effectively decouples agent policies by hierarchical game levels.\nHowever, this framework ignores both the varying driving complexities among\nagents and the dynamic changes in agent states across game levels, instead\ntreating them uniformly. Consequently, redundant and error-prone computations\nare introduced into this framework. To tackle the issue, this paper proposes a\nmetric, termed as Trajectory Entropy, to reveal the game status of agents\nwithin the level-k game framework. The key insight stems from recognizing the\ninherit relationship between agent policy uncertainty and the associated\ndriving complexity. Specifically, Trajectory Entropy extracts statistical\nsignals representing uncertainty from the multimodality trajectory prediction\nresults of agents in the game. Then, the signal-to-noise ratio of this signal\nis utilized to quantify the game status of agents. Based on the proposed\nTrajectory Entropy, we refine the current level-k game framework through a\nsimple gating mechanism, significantly improving overall accuracy while\nreducing computational costs. Our method is evaluated on the Waymo and nuPlan\ndatasets, in terms of trajectory prediction, open-loop and closed-loop planning\ntasks. The results demonstrate the state-of-the-art performance of our method,\nwith precision improved by up to 19.89% for prediction and up to 16.48% for\nplanning.", "AI": {"tldr": "The paper proposes a new metric, Trajectory Entropy, to address the inefficiencies in the level-k game framework for agent interactions in autonomous driving.", "motivation": "To combat redundant calculations and poor adaptability in hierarchical game-level policies for autonomous driving due to a uniform treatment of diverse agent complexities and dynamic states.", "method": "Introducing Trajectory Entropy to measure agent policy uncertainty using trajectory prediction multimodality, refining the level-k game framework through a gating mechanism.", "result": "Achieved state-of-the-art results with up to 19.89% improvement in trajectory prediction and up to 16.48% in planning tasks on Waymo and nuPlan datasets.", "conclusion": "Trajectory Entropy effectively optimizes the level-k game framework for computational efficiency and accuracy in autonomous driving scenarios."}}
{"id": "2506.06262", "pdf": "https://arxiv.org/pdf/2506.06262", "abs": "https://arxiv.org/abs/2506.06262", "authors": ["Kjetil Vasstein", "Christian Le", "Simon Lerv\u00e5g Breivik", "Trygve Maukon Myhr", "Annette Stahl", "Edmund F\u00f8rland Brekke"], "title": "PyGemini: Unified Software Development towards Maritime Autonomy Systems", "categories": ["cs.RO", "cs.SE", "cs.SY", "eess.SY", "D.2.11; I.6.2; I.2.9"], "comment": "Preprint. Not yet submitted for peer review. Includes 14 figures and\n  3 tables. 18 pages, 1 appendix", "summary": "Ensuring the safety and certifiability of autonomous surface vessels (ASVs)\nrequires robust decision-making systems, supported by extensive simulation,\ntesting, and validation across a broad range of scenarios. However, the current\nlandscape of maritime autonomy development is fragmented -- relying on\ndisparate tools for communication, simulation, monitoring, and system\nintegration -- which hampers interdisciplinary collaboration and inhibits the\ncreation of compelling assurance cases, demanded by insurers and regulatory\nbodies. Furthermore, these disjointed tools often suffer from performance\nbottlenecks, vendor lock-in, and limited support for continuous integration\nworkflows. To address these challenges, we introduce PyGemini, a permissively\nlicensed, Python-native framework that builds on the legacy of Autoferry Gemini\nto unify maritime autonomy development. PyGemini introduces a novel\nConfiguration-Driven Development (CDD) process that fuses Behavior-Driven\nDevelopment (BDD), data-oriented design, and containerization to support\nmodular, maintainable, and scalable software architectures. The framework\nfunctions as a stand-alone application, cloud-based service, or embedded\nlibrary -- ensuring flexibility across research and operational contexts. We\ndemonstrate its versatility through a suite of maritime tools -- including 3D\ncontent generation for simulation and monitoring, scenario generation for\nautonomy validation and training, and generative artificial intelligence\npipelines for augmenting imagery -- thereby offering a scalable, maintainable,\nand performance-oriented foundation for future maritime robotics and autonomy\nresearch.", "AI": {"tldr": "PyGemini introduces a Python-native framework to address the fragmented development ecosystem of maritime autonomy by offering a scalable and maintainable tool.", "motivation": "The field of maritime autonomy suffers from fragmented development tools, performance limitations, and interoperability issues, which hinder collaboration and make certification for autonomous surface vessels (ASVs) challenging.", "method": "The paper introduces PyGemini, a framework leveraging a Configuration-Driven Development (CDD) process that combines Behavior-Driven Development (BDD) principles, data-oriented design, and containerization.", "result": "PyGemini enables modular and scalable software development, serving as a standalone, cloud-based, or embedded tool. Its application spans 3D simulation tools, scenario generation, and generative AI pipelines for maritime use cases.", "conclusion": "PyGemini establishes a unified, flexible, and high-performance foundation for advancing research and development in maritime autonomy and robotics."}}
{"id": "2506.06077", "pdf": "https://arxiv.org/pdf/2506.06077", "abs": "https://arxiv.org/abs/2506.06077", "authors": ["Gergely Bari", "Laszlo Palkovics"], "title": "Self driving algorithm for an active four wheel drive racecar", "categories": ["cs.RO"], "comment": null, "summary": "Controlling autonomous vehicles at their handling limits is a significant\nchallenge, particularly for electric vehicles with active four wheel drive\n(A4WD) systems offering independent wheel torque control. While traditional\nVehicle Dynamics Control (VDC) methods use complex physics-based models, this\nstudy explores Deep Reinforcement Learning (DRL) to develop a unified,\nhigh-performance controller. We employ the Proximal Policy Optimization (PPO)\nalgorithm to train an agent for optimal lap times in a simulated racecar\n(TORCS) at the tire grip limit. Critically, the agent learns an end-to-end\npolicy that directly maps vehicle states, like velocities, accelerations, and\nyaw rate, to a steering angle command and independent torque commands for each\nof the four wheels. This formulation bypasses conventional pedal inputs and\nexplicit torque vectoring algorithms, allowing the agent to implicitly learn\nthe A4WD control logic needed for maximizing performance and stability.\nSimulation results demonstrate the RL agent learns sophisticated strategies,\ndynamically optimizing wheel torque distribution corner-by-corner to enhance\nhandling and mitigate the vehicle's inherent understeer. The learned behaviors\nmimic and, in aspects of grip utilization, potentially surpass traditional\nphysics-based A4WD controllers while achieving competitive lap times. This\nresearch underscores DRL's potential to create adaptive control systems for\ncomplex vehicle dynamics, suggesting RL is a potent alternative for advancing\nautonomous driving in demanding, grip-limited scenarios for racing and road\nsafety.", "AI": {"tldr": "This paper explores using Deep Reinforcement Learning (DRL) with Proximal Policy Optimization (PPO) to develop an adaptive controller for electric autonomous vehicles with active four-wheel drive systems, focusing on improving handling at tire grip limits without using traditional pedal inputs.", "motivation": "The paper aims to address the challenges of controlling autonomous vehicles at their handling limits, particularly those equipped with active four-wheel drive systems by proposing a Deep Reinforcement Learning-based methodology as an alternative to traditional physics-based Vehicle Dynamics Control methods.", "method": "The method involves training an agent using the Proximal Policy Optimization (PPO) algorithm in a simulated racecar environment, mapping vehicle states to steering and individual wheel torque commands directly without explicit pedal inputs or conventional algorithms.", "result": "Simulation results show the RL agent can dynamically optimize wheel torque distribution to enhance handling and mitigate understeer corner-by-corner, surpassing certain traditional physics-based controllers in grip utilization and achieving competitive lap times.", "conclusion": "The study concludes that DRL can create adaptive and high-performance control systems for complex vehicle dynamics, demonstrating its potential as a viable alternative to traditional methods for autonomous driving in demanding grip-limited scenarios."}}
{"id": "2506.05668", "pdf": "https://arxiv.org/pdf/2506.05668", "abs": "https://arxiv.org/abs/2506.05668", "authors": ["Jiajun He", "Jos\u00e9 Miguel Hern\u00e1ndez-Lobato", "Yuanqi Du", "Francisco Vargas"], "title": "RNE: a plug-and-play framework for diffusion density estimation and inference-time control", "categories": ["cs.LG", "stat.ML"], "comment": "39 pages; 10 figures", "summary": "In this paper, we introduce the Radon-Nikodym Estimator (RNE), a flexible,\nplug-and-play framework for diffusion inference-time density estimation and\ncontrol, based on the concept of the density ratio between path distributions.\nRNE connects and unifies a variety of existing density estimation and\ninference-time control methods under a single and intuitive perspective,\nstemming from basic variational inference and probabilistic principles\ntherefore offering both theoretical clarity and practical versatility.\nExperiments demonstrate that RNE achieves promising performances in diffusion\ndensity estimation and inference-time control tasks, including annealing,\ncomposition of diffusion models, and reward-tilting.", "AI": {"tldr": "This paper introduces the Radon-Nikodym Estimator (RNE), a framework for density estimation and control during diffusion inference, unifying existing techniques under a probabilistic approach.", "motivation": "To provide a unified and versatile framework for diffusion inference-time density estimation and control using probabilistic principles.", "method": "The paper employs the concept of density ratios between path distributions, integrating variational inference techniques to develop RNE.", "result": "Experiments show that RNE delivers promising performance in tasks like density estimation, annealing, model composition, and reward-tilting for diffusion inference.", "conclusion": "RNE enhances diffusion inference-time tasks by unifying various methods and delivering clear theoretical grounding and adaptability."}}
{"id": "2506.05384", "pdf": "https://arxiv.org/pdf/2506.05384", "abs": "https://arxiv.org/abs/2506.05384", "authors": ["Zhuoxuan Cai", "Jian Zhang", "Xinbin Yuan", "Pengtao Jiang", "Wenxiang Chen", "Bowen Tang", "Lujian Yao", "Qiyuan Wang", "Jinwen Chen", "Bo Li"], "title": "Q-Ponder: A Unified Training Pipeline for Reasoning-based Visual Quality Assessment", "categories": ["cs.CV", "cs.AI", "cs.MM"], "comment": null, "summary": "Recent studies demonstrate that multimodal large language models (MLLMs) can\nproficiently evaluate visual quality through interpretable assessments.\nHowever, existing approaches typically treat quality scoring and reasoning\ndescriptions as separate tasks with disjoint optimization objectives, leading\nto a trade-off: models adept at quality reasoning descriptions struggle with\nprecise score regression, while score-focused models lack interpretability.\nThis limitation hinders the full potential of MLLMs in visual quality\nassessment, where accuracy and interpretability should be mutually reinforcing.\nTo address this, we propose a unified two-stage training framework comprising a\ncold-start stage and a reinforcement learning-based fine-tuning stage.\nSpecifically, in the first stage, we distill high-quality data from a teacher\nmodel through expert-designed prompts, initializing reasoning capabilities via\ncross-entropy loss supervision. In the second stage, we introduce a novel\nreward with Group Relative Policy Optimization (GRPO) to jointly optimize\nscoring accuracy and reasoning consistency. We designate the models derived\nfrom these two stages as Q-Ponder-CI and Q-Ponder. Extensive experiments show\nthat Q-Ponder achieves state-of-the-art (SOTA) performance on quality score\nregression benchmarks, delivering up to 6.5% higher SRCC on cross-domain\ndatasets. Furthermore, Q-Ponder significantly outperforms description-based\nSOTA models, including its teacher model Qwen-2.5-VL-72B, particularly in\ndescription accuracy and reasonableness, demonstrating the generalization\npotential over diverse tasks.", "AI": {"tldr": "The paper proposes a two-stage training framework for multimodal large language models to enhance both accuracy and interpretability in visual quality assessment.", "motivation": "The study aims to address the trade-off in current multimodal large language models where optimizing for accuracy in quality scoring often compromises interpretability and vice versa.", "method": "The two-stage framework consists of a cold-start stage for initializing reasoning capabilities through supervised learning and a reinforcement learning-based fine-tuning stage employing Group Relative Policy Optimization (GRPO) to optimize both scoring accuracy and reasoning consistency.", "result": "The proposed model, Q-Ponder, achieves state-of-the-art performance in quality score regression benchmarks, surpassing existing models by up to 6.5% in SRCC. It also demonstrates superior description accuracy and generalization across tasks compared to the teacher model.", "conclusion": "The unified training framework effectively combines interpretability and accuracy, overcoming the limitations of existing approaches in visual quality assessment."}}
{"id": "2506.05498", "pdf": "https://arxiv.org/pdf/2506.05498", "abs": "https://arxiv.org/abs/2506.05498", "authors": ["Niruthiha Selvanayagam"], "title": "Multidimensional Analysis of Specific Language Impairment Using Unsupervised Learning Through PCA and Clustering", "categories": ["cs.CL", "cs.LG", "62H30, 62P10", "I.2.7; J.3"], "comment": "14 pages, 3 figures, 16 tables", "summary": "Specific Language Impairment (SLI) affects approximately 7 percent of\nchildren, presenting as isolated language deficits despite normal cognitive\nabilities, sensory systems, and supportive environments. Traditional diagnostic\napproaches often rely on standardized assessments, which may overlook subtle\ndevelopmental patterns. This study aims to identify natural language\ndevelopment trajectories in children with and without SLI using unsupervised\nmachine learning techniques, providing insights for early identification and\ntargeted interventions. Narrative samples from 1,163 children aged 4-16 years\nacross three corpora (Conti-Ramsden 4, ENNI, and Gillam) were analyzed using\nPrincipal Component Analysis (PCA) and clustering. A total of 64 linguistic\nfeatures were evaluated to uncover developmental trajectories and distinguish\nlinguistic profiles. Two primary clusters emerged: (1) high language production\nwith low SLI prevalence, and (2) limited production but higher syntactic\ncomplexity with higher SLI prevalence. Additionally, boundary cases exhibited\nintermediate traits, supporting a continuum model of language abilities.\nFindings suggest SLI manifests primarily through reduced production capacity\nrather than syntactic complexity deficits. The results challenge categorical\ndiagnostic frameworks and highlight the potential of unsupervised learning\ntechniques for refining diagnostic criteria and intervention strategies.", "AI": {"tldr": "The study analyzes language developmental trajectories in children with Specific Language Impairment (SLI) using unsupervised machine learning methods to refine diagnostics and interventions.", "motivation": "Traditional diagnostic methods for SLI often overlook developmental nuances. The study aims to uncover natural language trajectories to improve early diagnosis and interventions for SLI.", "method": "The researchers applied Principal Component Analysis (PCA) and clustering on narrative data from 1,163 children using 64 linguistic features to identify developmental patterns linked to SLI.", "result": "Two main linguistic clusters were found: those with high language production and low SLI prevalence, and those with limited production but higher syntactic complexity. Intermediate traits supported a continuum model of language abilities.", "conclusion": "SLI is characterized mainly by reduced production capacity, not syntactic complexity deficits. The findings challenge categorical diagnostic methods and underscore the potential of machine learning for improved SLI diagnosis and treatment."}}
{"id": "2506.05479", "pdf": "https://arxiv.org/pdf/2506.05479", "abs": "https://arxiv.org/abs/2506.05479", "authors": ["Matei Gabriel Co\u015fa", "Marek Eli\u00e1\u0161"], "title": "Learning-Augmented Algorithms for MTS with Bandit Access to Multiple Predictors", "categories": ["cs.LG", "cs.DS", "68W40 (Primary), 68T05 (Secondary)"], "comment": "Accepted to ICML 2025", "summary": "We consider the following problem: We are given $\\ell$ heuristics for\nMetrical Task Systems (MTS), where each might be tailored to a different type\nof input instances. While processing an input instance received online, we are\nallowed to query the action of only one of the heuristics at each time step.\nOur goal is to achieve performance comparable to the best of the given\nheuristics. The main difficulty of our setting comes from the fact that the\ncost paid by a heuristic at time $t$ cannot be estimated unless the same\nheuristic was also queried at time $t-1$. This is related to Bandit Learning\nagainst memory bounded adversaries (Arora et al., 2012). We show how to achieve\nregret of $O(\\text{OPT}^{2/3})$ and prove a tight lower bound based on the\nconstruction of Dekel et al. (2013).", "AI": {"tldr": "This paper addresses optimizing performance by selecting from multiple heuristics for Metrical Task Systems (MTS) in an online context, achieving performance close to the best heuristic with a regret bound of $O(\\text{OPT}^{2/3})$.", "motivation": "To develop a robust methodology for combining $\\ell$ heuristics to handle diverse types of input instances online and achieve competitive performance compared to the best heuristic.", "method": "It introduces an approach inspired by Bandit Learning against memory-bounded adversaries, allowing selection of heuristics sequentially without perfect information about their past costs.", "result": "The proposed method achieves a regret of $O(\\text{OPT}^{2/3})$, and the paper establishes tight lower bounds for the performance using prior constructions.", "conclusion": "The work provides an effective strategy for leveraging multiple heuristics for MTS with provable regret bounds, contributing to understanding memory-bounded adversarial settings."}}
{"id": "2506.05887", "pdf": "https://arxiv.org/pdf/2506.05887", "abs": "https://arxiv.org/abs/2506.05887", "authors": ["Marilyn Bello", "Rafael Bello", "Maria-Matilde Garc\u00eda", "Ann Now\u00e9", "Iv\u00e1n Sevillano-Garc\u00eda", "Francisco Herrera"], "title": "Explainability in Context: A Multilevel Framework Aligning AI Explanations with Stakeholder with LLMs", "categories": ["cs.AI"], "comment": "22 pages, 5 figures", "summary": "The growing application of artificial intelligence in sensitive domains has\nintensified the demand for systems that are not only accurate but also\nexplainable and trustworthy. Although explainable AI (XAI) methods have\nproliferated, many do not consider the diverse audiences that interact with AI\nsystems: from developers and domain experts to end-users and society. This\npaper addresses how trust in AI is influenced by the design and delivery of\nexplanations and proposes a multilevel framework that aligns explanations with\nthe epistemic, contextual, and ethical expectations of different stakeholders.\nThe framework consists of three layers: algorithmic and domain-based,\nhuman-centered, and social explainability. We highlight the emerging role of\nLarge Language Models (LLMs) in enhancing the social layer by generating\naccessible, natural language explanations. Through illustrative case studies,\nwe demonstrate how this approach facilitates technical fidelity, user\nengagement, and societal accountability, reframing XAI as a dynamic,\ntrust-building process.", "AI": {"tldr": "The paper introduces a multilevel framework for explainable AI (XAI), emphasizing diverse stakeholder needs and trust-building through improved explanation delivery.", "motivation": "To address the need for trust and explainability in AI systems, especially considering the varied audiences who interact with these systems.", "method": "The authors propose a three-layered framework (algorithmic and domain-based, human-centered, and social explainability) and discuss using Large Language Models (LLMs) for generating accessible explanations.", "result": "Case studies demonstrate how the framework enhances technical accuracy, user engagement, and societal accountability in explainable AI.", "conclusion": "XAI is repositioned as a trust-building process, emphasizing stakeholder expectations and leveraging LLMs to advance explainability."}}
{"id": "2506.06094", "pdf": "https://arxiv.org/pdf/2506.06094", "abs": "https://arxiv.org/abs/2506.06094", "authors": ["Elim Kwan", "Rehman Qureshi", "Liam Fletcher", "Colin Laganier", "Victoria Nockles", "Richard Walters"], "title": "On-board Mission Replanning for Adaptive Cooperative Multi-Robot Systems", "categories": ["cs.RO", "cs.LG"], "comment": "9 pages, 5 figures, 1 table", "summary": "Cooperative autonomous robotic systems have significant potential for\nexecuting complex multi-task missions across space, air, ground, and maritime\ndomains. But they commonly operate in remote, dynamic and hazardous\nenvironments, requiring rapid in-mission adaptation without reliance on fragile\nor slow communication links to centralised compute. Fast, on-board replanning\nalgorithms are therefore needed to enhance resilience. Reinforcement Learning\nshows strong promise for efficiently solving mission planning tasks when\nformulated as Travelling Salesperson Problems (TSPs), but existing methods: 1)\nare unsuitable for replanning, where agents do not start at a single location;\n2) do not allow cooperation between agents; 3) are unable to model tasks with\nvariable durations; or 4) lack practical considerations for on-board\ndeployment. Here we define the Cooperative Mission Replanning Problem as a\nnovel variant of multiple TSP with adaptations to overcome these issues, and\ndevelop a new encoder/decoder-based model using Graph Attention Networks and\nAttention Models to solve it effectively and efficiently. Using a simple\nexample of cooperative drones, we show our replanner consistently (90% of the\ntime) maintains performance within 10% of the state-of-the-art LKH3 heuristic\nsolver, whilst running 85-370 times faster on a Raspberry Pi. This work paves\nthe way for increased resilience in autonomous multi-agent systems.", "AI": {"tldr": "This paper defines the Cooperative Mission Replanning Problem and develops a fast model using Graph Attention Networks for autonomous multi-agent systems, achieving near-optimal performance faster than traditional solvers.", "motivation": "To address the challenges faced by cooperative autonomous robotic systems in dynamic and hazardous environments requiring in-mission adaptation without reliance on centralized compute or slow communication links.", "method": "The proposed method is an encoder/decoder-based model leveraging Graph Attention Networks and Attention Models, tailored for multi-agent replanning tasks.", "result": "The method achieved 90% performance within 10% of the state-of-the-art heuristic solver LKH3, while being 85-370 times faster on a Raspberry Pi.", "conclusion": "The work enhances resilience in autonomous systems, offering an efficient replanning approach that overcomes key limitations of existing methods. It is practical for on-board deployment in dynamic scenarios."}}
{"id": "2506.05718", "pdf": "https://arxiv.org/pdf/2506.05718", "abs": "https://arxiv.org/abs/2506.05718", "authors": ["Pascal Jr Tikeng Notsawo", "Guillaume Dumas", "Guillaume Rabusseau"], "title": "Grokking Beyond the Euclidean Norm of Model Parameters", "categories": ["cs.LG", "cs.AI", "stat.ML", "I.2.6"], "comment": "67 pages, 35 figures. Forty-second International Conference on\n  Machine Learning (ICML), 2025", "summary": "Grokking refers to a delayed generalization following overfitting when\noptimizing artificial neural networks with gradient-based methods. In this\nwork, we demonstrate that grokking can be induced by regularization, either\nexplicit or implicit. More precisely, we show that when there exists a model\nwith a property $P$ (e.g., sparse or low-rank weights) that generalizes on the\nproblem of interest, gradient descent with a small but non-zero regularization\nof $P$ (e.g., $\\ell_1$ or nuclear norm regularization) results in grokking.\nThis extends previous work showing that small non-zero weight decay induces\ngrokking. Moreover, our analysis shows that over-parameterization by adding\ndepth makes it possible to grok or ungrok without explicitly using\nregularization, which is impossible in shallow cases. We further show that the\n$\\ell_2$ norm is not a reliable proxy for generalization when the model is\nregularized toward a different property $P$, as the $\\ell_2$ norm grows in many\ncases where no weight decay is used, but the model generalizes anyway. We also\nshow that grokking can be amplified solely through data selection, with any\nother hyperparameter fixed.", "AI": {"tldr": "The paper identifies conditions inducing grokking in neural networks, linking it to regularization techniques and factors like over-parameterization and data selection.", "motivation": "Understanding and controlling grokking is crucial to improve optimization and generalization techniques in neural network training.", "method": "Empirical and theoretical analysis to establish the relationship between regularization, model depth, and generalization during training.", "result": "Models can exhibit grokking due to regularization, depth over-parameterization, and strategic data selection. Weight decay isn't always a reliable generalization proxy.", "conclusion": "Regularization, depth, and data selection can influence delayed generalization (grokking), overcoming generalization challenges in neural networks."}}
{"id": "2506.05395", "pdf": "https://arxiv.org/pdf/2506.05395", "abs": "https://arxiv.org/abs/2506.05395", "authors": ["Mert Can Cakmak", "Nitin Agarwal", "Diwash Poudel"], "title": "TriPSS: A Tri-Modal Keyframe Extraction Framework Using Perceptual, Structural, and Semantic Representations", "categories": ["cs.CV", "cs.IR", "cs.MM", "eess.IV"], "comment": null, "summary": "Efficient keyframe extraction is critical for effective video summarization\nand retrieval, yet capturing the complete richness of video content remains\nchallenging. In this work, we present TriPSS, a novel tri-modal framework that\neffectively integrates perceptual cues from color features in the CIELAB space,\ndeep structural embeddings derived from ResNet-50, and semantic context from\nframe-level captions generated by Llama-3.2-11B-Vision-Instruct. By fusing\nthese diverse modalities using principal component analysis, TriPSS constructs\nrobust multi-modal embeddings that enable adaptive segmentation of video\ncontent via HDBSCAN clustering. A subsequent refinement stage incorporating\nquality assessment and duplicate filtering ensures that the final keyframe set\nis both concise and semantically rich. Comprehensive evaluations on benchmark\ndatasets TVSum20 and SumMe demonstrate that TriPSS achieves state-of-the-art\nperformance, substantially outperforming traditional unimodal and previous\nmulti-modal methods. These results underscore TriPSS's ability to capture\nnuanced visual and semantic information, thereby setting a new benchmark for\nvideo content understanding in large-scale retrieval scenarios.", "AI": {"tldr": "TriPSS is a tri-modal framework enhancing video summarization by integrating visual, structural, and semantic cues, achieving state-of-the-art performance in benchmark tests.", "motivation": "To address the challenge of capturing the complete richness of video content for effective summarization and retrieval.", "method": "TriPSS integrates perceptual, structural, and semantic features via principal component analysis and adaptive video segmentation using HDBSCAN clustering, complemented by quality assessment and duplicate filtering.", "result": "TriPSS outperformed unimodal and previous multi-modal methods, establishing new performance benchmarks on TVSum20 and SumMe datasets.", "conclusion": "The approach demonstrates superior capability in understanding nuanced video content, enhancing its application in large-scale video retrieval scenarios."}}
{"id": "2506.05560", "pdf": "https://arxiv.org/pdf/2506.05560", "abs": "https://arxiv.org/abs/2506.05560", "authors": ["Petr M\u00e1\u0161a"], "title": "Improving LLMs with a knowledge from databases", "categories": ["cs.CL", "I.2.7; I.2.4"], "comment": null, "summary": "Large language models (LLMs) are achieving significant progress almost every\nmoment now. Many advanced techniques have been introduced and widely accepted,\nlike retrieval-augmentation generation (RAG), agents, and tools. Tools can\nquery the database to answer questions from structured data files or perform\ngroupings or other statistics. This unlocks huge opportunities, such as it can\nanswer any question, but also poses threats, such as safety, because there is\nno control over the commands that are created. We would like to discuss whether\nwe can create a new method that improves answers based on dataset/database via\nsome interpretable ML methods, namely enhanced association rules. The advantage\nwould be if the method can be also used in some safe technique like RAG.\nAssociation rules have a sound history. Since the introduction of CN2 and\naproiri, many enhancements have been made. In parallel, enhanced association\nrules have been introduced and evolved over the last 40 years. The general\nproblem is typically that there are too many rules. There are some techniques\nfor handling it, but when LLM emerged, it turned out to be the best use case\nfor the RAG technique for LLMs. We proposed a method that generates a ruleset\nbased on defined knowledge patterns, then converts rules into text form via a\nrule-to-text converter, and includes the result as an RAG into LLM. We compared\nthis method with ChatGPT (even with using agents) and we have discovered a\nsignificant improvement in answering questions based on the dataset. We have\nalso tried several strategies how much rules to generate. We found this\nimprovement interesting. Moreover, it can also be improved in many ways as\nfuture work, like incorporating other patterns, the use of rule mining as an\nagent, and many others.", "AI": {"tldr": "The paper proposes a method that integrates enhanced association rules into large language models (LLMs) via retrieval-augmented generation (RAG), significantly improving their dataset-based question-answering capabilities.", "motivation": "The motivation is to improve the dataset/database-based answering ability of LLMs using interpretable machine learning methods like enhanced association rules, while addressing issues such as safety and the large volume of rules.", "method": "The proposed method generates knowledge-based rulesets, converts these rules into text using a rule-to-text converter, and integrates them into LLMs through the RAG technique. Various strategies for rule generation were explored.", "result": "The proposed method demonstrated significant improvements in answering dataset-based questions compared to existing techniques, including ChatGPT with agents.", "conclusion": "The method shows substantial promise for enhancing dataset-driven QA in LLMs, with opportunities for further improvement by adopting more patterns and novel strategies for rule mining."}}
{"id": "2506.05484", "pdf": "https://arxiv.org/pdf/2506.05484", "abs": "https://arxiv.org/abs/2506.05484", "authors": ["Ruihua Chen", "Bangyu Wu", "Meng Li", "Kai Yang"], "title": "Initial Model Incorporation for Deep Learning FWI: Pretraining or Denormalization?", "categories": ["cs.LG", "physics.geo-ph"], "comment": null, "summary": "Subsurface property neural network reparameterized full waveform inversion\n(FWI) has emerged as an effective unsupervised learning framework, which can\ninvert stably with an inaccurate starting model. It updates the trainable\nneural network parameters instead of fine-tuning on the subsurface model\ndirectly. There are primarily two ways to embed the prior knowledge of the\ninitial model into neural networks, that is, pretraining and denormalization.\nPretraining first regulates the neural networks' parameters by fitting the\ninitial velocity model; Denormalization directly adds the outputs of the\nnetwork into the initial models without pretraining. In this letter, we\nsystematically investigate the influence of the two ways of initial model\nincorporation for the neural network reparameterized FWI. We demonstrate that\npretraining requires inverting the model perturbation based on a constant\nvelocity value (mean) with a two-stage implementation. It leads to a complex\nworkflow and inconsistency of objective functions in the two-stage process,\ncausing the network parameters to become inactive and lose plasticity.\nExperimental results demonstrate that denormalization can simplify workflows,\naccelerate convergence, and enhance inversion accuracy compared with\npretraining.", "AI": {"tldr": "The paper evaluates two approaches, pretraining and denormalization, for neural network reparameterized full waveform inversion (FWI), finding denormalization superior in workflow simplicity, convergence speed, and accuracy.", "motivation": "To address the challenge of incorporating prior knowledge into neural network-based FWI, enabling more stable and accurate inversion performance.", "method": "The paper systematically compares pretraining, where networks are pre-fit to the initial velocity model, and denormalization, which directly integrates network outputs with initial models without additional stages.", "result": "Experimental results highlight denormalization's benefits in simplifying workflows, accelerating convergence, and improving inversion accuracy over the pretraining method.", "conclusion": "Denormalization is a more effective strategy than pretraining for embedding prior knowledge into neural network-based FWI, offering practical improvements in efficiency and performance."}}
