{"id": "2601.16032", "pdf": "https://arxiv.org/pdf/2601.16032", "abs": "https://arxiv.org/abs/2601.16032", "authors": ["Yifan Zhu", "Yekai Pan", "Chen Ding"], "title": "Sawtooth Wavefront Reordering: Enhanced CuTile FlashAttention on NVIDIA GB10", "categories": ["cs.PF", "cs.AI", "cs.LG", "cs.OS"], "comment": null, "summary": "High-performance attention kernels are essential for Large Language Models. This paper presents analysis of CuTile-based Flash Attention memory behavior and a technique to improve its cache performance. In particular, our analysis on the NVIDIA GB10 (Grace Blackwell) identifies the main cause of L2 cache miss. Leveraging this insight, we introduce a new programming technique called Sawtooth Wavefront Reordering that reduces L2 misses. We validate it in both CUDA and CuTile, observing 50\\% or greater reduction in L2 misses and up to 60\\% increase in throughput on GB10.", "AI": {"tldr": "This paper analyzes and improves the cache performance of Flash Attention kernels using a new programming technique called Sawtooth Wavefront Reordering.", "motivation": "To address the main bottleneck in L2 cache performance that impacts the efficiency of attention kernels in Large Language Models.", "method": "The authors identify the causes of L2 cache misses via analysis on NVIDIA GB10, and introduce Sawtooth Wavefront Reordering to mitigate this.", "result": "The approach reduces L2 cache misses by 50% or more, and increases throughput by up to 60% on NVIDIA GB10.", "conclusion": "The proposed technique significantly enhances cache performance and throughput for Flash Attention, showcasing its effectiveness in Large Language Models."}}
{"id": "2509.02811", "pdf": "https://arxiv.org/pdf/2509.02811", "abs": "https://arxiv.org/abs/2509.02811", "authors": ["Alessandro Traspadini", "Michele Zorzi", "Marco Giordani"], "title": "Performance Evaluation of LoRa for IoT Applications in Non-Terrestrial Networks via ns-3", "categories": ["cs.NI", "cs.PF"], "comment": "6 pages, 4 figures, 2 tables. Accepted for publication in the 2025 IEEE Global Communications Conference (GLOBECOM) \\c{opyright}2025 IEEE. A. Traspadini, M. Zorzi, and M. Giordani \"Performance Evaluation of LoRa for IoT Applications in Non-Terrestrial Networks via ns-3,\" in Proc. IEEE Global Communications Conference (GLOBECOM), 2025", "summary": "The integration of Internet of Things (IoT) and Non-Terrestrial Networks (NTNs) has emerged as a key paradigm to provide connectivity for sensors and actuators via satellite gateways in remote areas where terrestrial infrastructure is limited or unavailable. Among other Low-Power Wide-Area Network (LPWAN) technologies for IoT, Long Range (LoRa) holds great potential given its long range, energy efficiency, and flexibility. In this paper, we explore the feasibility and performance of LoRa to support large-scale IoT connectivity through Low Earth Orbit (LEO) satellite gateways. To do so, we developed a new ns3-LoRa-NTN simulation module, which integrates and extends the ns3-LoRa and ns3-NTN modules, to enable full-stack end-to-end simulation of satellite communication in LoRa networks. Our results, given in terms of average data rate and Packet Reception Ratio (PRR), confirm that LoRa can effectively support direct communication from the ground to LEO satellites, but network optimization is required to mitigate collision probability when end nodes use the same Spreading Factors (SFs) over long distances.", "AI": {"tldr": "This paper investigates the feasibility and performance of LoRa technology for IoT applications using LEO satellite gateways, developing a new simulation module to assess system-level communication outcomes.", "motivation": "The paper aims to address connectivity challenges for IoT sensors and actuators in remote areas lacking terrestrial infrastructure by exploring LoRa integration with Non-Terrestrial Networks (NTNs).", "method": "A simulation-based approach using the newly developed ns3-LoRa-NTN module, enabling end-to-end system modeling of satellite communication in LoRa networks.", "result": "LoRa demonstrates potential for direct ground-to-satellite communication in IoT applications, though optimization is needed to reduce collision probabilities due to SF usage over extended distances.", "conclusion": "LoRa, combined with LEO satellite gateways, is a viable solution for large-scale IoT connectivity in remote areas, but network configurations require adjustments for efficient operation."}}
{"id": "2601.15298", "pdf": "https://arxiv.org/pdf/2601.15298", "abs": "https://arxiv.org/abs/2601.15298", "authors": ["Anantha Sharma"], "title": "Embedding Retrofitting: Data Engineering for better RAG", "categories": ["cs.CL", "cs.AI", "cs.PF"], "comment": "16 pages, 11 figures, 7 tables", "summary": "Embedding retrofitting adjusts pre-trained word vectors using knowledge graph constraints to improve domain-specific retrieval. However, the effectiveness of retrofitting depends critically on knowledge graph quality, which in turn depends on text preprocessing. This paper presents a data engineering framework that addresses data quality degradation from annotation artifacts in real-world corpora.\n  The analysis shows that hashtag annotations inflate knowledge graph density, leading to creating spurious edges that corrupt the retrofitting objective. On noisy graphs, all retrofitting techniques produce statistically significant degradation ($-3.5\\%$ to $-5.2\\%$, $p<0.05$). After preprocessing, \\acrshort{ewma} retrofitting achieves $+6.2\\%$ improvement ($p=0.0348$) with benefits concentrated in quantitative synthesis questions ($+33.8\\%$ average). The gap between clean and noisy preprocessing (10\\%+ swing) exceeds the gap between algorithms (3\\%), establishing preprocessing quality as the primary determinant of retrofitting success.", "AI": {"tldr": "This paper highlights that preprocessing quality critically impacts embedding retrofitting success, demonstrating significant improvements when cleaning noisy data.", "motivation": "The study aims to improve the reliability of embedding retrofitting for domain-specific retrieval by addressing challenges of data quality degradation from annotation artifacts.", "method": "The method involves developing a data engineering framework to preprocess real-world corpora, analyzing how hashtag annotations affect knowledge graph quality, and applying \u0001b[ewma\u00001b retrofitting on clean data.", "result": "The analysis reveals that noisy graphs degrade performance across retrofitting techniques by 3.5% to 5.2%, while preprocessing improves EWMA retrofitting results by 6.2%, with substantial benefits in specific queries.", "conclusion": "Preprocessing quality is crucial for embedding retrofitting success, outweighing the impact of algorithmic variations."}}
{"id": "2601.15476", "pdf": "https://arxiv.org/pdf/2601.15476", "abs": "https://arxiv.org/abs/2601.15476", "authors": ["Alex Dantart"], "title": "Reliability by design: quantifying and eliminating fabrication risk in LLMs. From generative to consultative AI: a comparative analysis in the legal domain and lessons for high-stakes knowledge bases", "categories": ["cs.AI", "cs.PF"], "comment": null, "summary": "This paper examines how to make large language models reliable for high-stakes legal work by reducing hallucinations. It distinguishes three AI paradigms: (1) standalone generative models (\"creative oracle\"), (2) basic retrieval-augmented systems (\"expert archivist\"), and (3) an advanced, end-to-end optimized RAG system (\"rigorous archivist\"). The authors introduce two reliability metrics -False Citation Rate (FCR) and Fabricated Fact Rate (FFR)- and evaluate 2,700 judicial-style answers from 12 LLMs across 75 legal tasks using expert, double-blind review. Results show that standalone models are unsuitable for professional use (FCR above 30%), while basic RAG greatly reduces errors but still leaves notable misgrounding. Advanced RAG, using techniques such as embedding fine-tuning, re-ranking, and self-correction, reduces fabrication to negligible levels (below 0.2%). The study concludes that trustworthy legal AI requires rigor-focused, retrieval-based architectures emphasizing verification and traceability, and provides an evaluation framework applicable to other high-risk domains.", "AI": {"tldr": "This paper evaluates methods to ensure large language models are reliable for legal tasks by examining standalone generative models and retrieval-augmented systems, introducing reliability metrics, and finding that advanced RAG systems drastically reduce errors.", "motivation": "The study aims to address the problem of hallucinations in large language models, particularly for critical tasks in legal work where accuracy and reliability are essential.", "method": "Three frameworks of AI paradigms are analyzed and tested on 2,700 judicial-style answers from 12 LLMs across 75 legal tasks, using expert double-blind reviews and two reliability metrics: False Citation Rate (FCR) and Fabricated Fact Rate (FFR).", "result": "Standalone models were found unsuitable for professional use (FCR over 30%), basic RAG reduces errors significantly but has room for improvement, and advanced RAG systems reduce fabrication rates to below 0.2%, highlighting their reliability.", "conclusion": "Reliable legal AI necessitates rigorous, retrieval-based systems focused on verification and traceability, providing a framework that can be applied to other high-stakes domains."}}
{"id": "2601.15598", "pdf": "https://arxiv.org/pdf/2601.15598", "abs": "https://arxiv.org/abs/2601.15598", "authors": ["Boxuan Zhang", "Jiaxin Wang", "Zhen Xu", "Kuan Tao"], "title": "Ternary Spiking Neural Networks Enhanced by Complemented Neurons and Membrane Potential Aggregation", "categories": ["cs.NE"], "comment": "Submission in progress. Code is available at https://github.com/ZBX05/Enhanced-TernarySNN", "summary": "Spiking Neural Networks (SNNs) are promising energy-efficient models and powerful framworks of modeling neuron dynamics. However, existing binary spiking neurons exhibit limited biological plausibilities and low information capacity. Recently developed ternary spiking neuron possesses higher consistency with biological principles (i.e. excitation-inhibition balance mechanism). Despite of this, the ternary spiking neuron suffers from defects including iterative information loss, temporal gradient vanishing and irregular distributions of membrane potentials. To address these issues, we propose Complemented Ternary Spiking Neuron (CTSN), a novel ternary spiking neuron model that incorporates an learnable complemental term to store information from historical inputs. CTSN effectively improves the deficiencies of ternary spiking neuron, while the embedded learnable factors enable CTSN to adaptively adjust neuron dynamics, providing strong neural heterogeneity. Furthermore, based on the temporal evolution features of ternary spiking neurons' membrane potential distributions, we propose the Temporal Membrane Potential Regularization (TMPR) training method. TMPR introduces time-varying regularization strategy utilizing membrane potentials, furhter enhancing the training process by creating extra backpropagation paths. We validate our methods through extensive experiments on various datasets, demonstrating remarkable performance advances.", "AI": {"tldr": "The paper proposes a new model, Complemented Ternary Spiking Neuron (CTSN), and a training method TMPR to improve spiking neural networks (SNNs).", "motivation": "To address the limitations of existing ternary spiking neurons, including iterative information loss, temporal gradient vanishing, and irregular membrane potential distributions.", "method": "CTSN incorporates a learnable complemental term to store historical input information. Additionally, a training method (TMPR) using time-varying membrane potential regularization is introduced.", "result": "CTSN and TMPR show substantial improvement in neuron dynamics and training processes, validated through experiments on multiple datasets.", "conclusion": "The CTSN model and TMPR method enhance the consistency, adaptability, and performance of spiking neural networks."}}
{"id": "2601.15710", "pdf": "https://arxiv.org/pdf/2601.15710", "abs": "https://arxiv.org/abs/2601.15710", "authors": ["Jiahao Zhang", "Zifan He", "Nicholas Fraser", "Michaela Blott", "Yizhou Sun", "Jason Cong"], "title": "FlexLLM: Composable HLS Library for Flexible Hybrid LLM Accelerator Design", "categories": ["cs.AR", "cs.AI", "cs.LG"], "comment": null, "summary": "We present FlexLLM, a composable High-Level Synthesis (HLS) library for rapid development of domain-specific LLM accelerators. FlexLLM exposes key architectural degrees of freedom for stage-customized inference, enabling hybrid designs that tailor temporal reuse and spatial dataflow differently for prefill and decode, and provides a comprehensive quantization suite to support accurate low-bit deployment. Using FlexLLM, we build a complete inference system for the Llama-3.2 1B model in under two months with only 1K lines of code. The system includes: (1) a stage-customized accelerator with hardware-efficient quantization (12.68 WikiText-2 PPL) surpassing SpinQuant baseline, and (2) a Hierarchical Memory Transformer (HMT) plug-in for efficient long-context processing. On the AMD U280 FPGA at 16nm, the accelerator achieves 1.29$\\times$ end-to-end speedup, 1.64$\\times$ higher decode throughput, and 3.14$\\times$ better energy efficiency than an NVIDIA A100 GPU (7nm) running BF16 inference; projected results on the V80 FPGA at 7nm reach 4.71$\\times$, 6.55$\\times$, and 4.13$\\times$, respectively. In long-context scenarios, integrating the HMT plug-in reduces prefill latency by 23.23$\\times$ and extends the context window by 64$\\times$, delivering 1.10$\\times$/4.86$\\times$ lower end-to-end latency and 5.21$\\times$/6.27$\\times$ higher energy efficiency on the U280/V80 compared to the A100 baseline. FlexLLM thus bridges algorithmic innovation in LLM inference and high-performance accelerators with minimal manual effort.", "AI": {"tldr": "FlexLLM is a high-level synthesis library designed for rapid development of efficient domain-specific accelerators for large language models (LLMs).", "motivation": "The motivation is to create a composable library that facilitates the efficient design of LLM inference systems with high performance, low energy consumption, and adaptability to various design choices.", "method": "The method involves creating FlexLLM to enable architectural flexibility in LLM inference by allowing stage-customized designs, hardware-efficient quantization, and providing tools for long-context processing such as Hierarchical Memory Transformer (HMT).", "result": "FlexLLM achieves significant performance gains, with better speed, throughput, and energy efficiency compared to an NVIDIA A100 GPU. It also drastically reduces latency and increases the context window in long-context scenarios.", "conclusion": "FlexLLM provides a bridge between algorithmic innovation and high-performance LLM accelerators with minimal development effort, enabling customizable and efficient LLM inference systems."}}
{"id": "2601.15455", "pdf": "https://arxiv.org/pdf/2601.15455", "abs": "https://arxiv.org/abs/2601.15455", "authors": ["Patrycja Balik", "Szymon J\u0119dras", "Piotr Polesiuk"], "title": "Remarks on Algebraic Reconstruction of Types and Effects", "categories": ["cs.PL"], "comment": null, "summary": "In their 1991 paper \"Algebraic Reconstruction of Types and Effects,\" Pierre Jouvelot and David Gifford presented a type-and-effect reconstruction algorithm based on an algebraic structure of effects. Their work is considered a milestone in the development of type-and-effect systems, and has inspired numerous subsequent works in the area of static analysis. However, unlike the later research it spawned, the original algorithm considered a language with higher-rank polymorphism, a feature which is challenging to implement correctly. In this note, we identify subtle bugs related to variable binding in their approach to this feature. We revisit their type system and reconstruction algorithm, and describe the discovered issues.", "AI": {"tldr": "This paper revisits Jouvelot and Gifford's 1991 type-and-effect reconstruction algorithm, identifying bugs in handling higher-rank polymorphism and proposing corrections.", "motivation": "To address subtle bugs in the variable binding mechanism of the original 1991 algorithm, focusing on improving accuracy in handling higher-rank polymorphism.", "method": "Reexamining the original type-and-effect system and reconstruction algorithm, analyzing issues in variable binding for higher-rank polymorphism.", "result": "Uncovered subtle errors in the original methodology for higher-rank polymorphism and provided precise details about the problematic mechanisms.", "conclusion": "The paper highlights flaws in a well-regarded type-and-effect reconstruction algorithm, paving the way for more accurate implementation in future research."}}
{"id": "2601.15528", "pdf": "https://arxiv.org/pdf/2601.15528", "abs": "https://arxiv.org/abs/2601.15528", "authors": ["Jiazhu Xie", "Bowen Li", "Heyu Fu", "Chong Gao", "Ziqi Xu", "Fengling Han"], "title": "Securing LLM-as-a-Service for Small Businesses: An Industry Case Study of a Distributed Chatbot Deployment Platform", "categories": ["cs.DC", "cs.CR"], "comment": "Accepted by AISC 2026", "summary": "Large Language Model (LLM)-based question-answering systems offer significant potential for automating customer support and internal knowledge access in small businesses, yet their practical deployment remains challenging due to infrastructure costs, engineering complexity, and security risks, particularly in retrieval-augmented generation (RAG)-based settings. This paper presents an industry case study of an open-source, multi-tenant platform that enables small businesses to deploy customised LLM-based support chatbots via a no-code workflow. The platform is built on distributed, lightweight k3s clusters spanning heterogeneous, low-cost machines and interconnected through an encrypted overlay network, enabling cost-efficient resource pooling while enforcing container-based isolation and per-tenant data access controls. In addition, the platform integrates practical, platform-level defences against prompt injection attacks in RAG-based chatbots, translating insights from recent prompt injection research into deployable security mechanisms without requiring model retraining or enterprise-scale infrastructure. We evaluate the proposed platform through a real-world e-commerce deployment, demonstrating that secure and efficient LLM-based chatbot services can be achieved under realistic cost, operational, and security constraints faced by small businesses.", "AI": {"tldr": "This paper introduces a no-code platform for small businesses to deploy custom chatbots using Large Language Models (LLMs), ensuring cost efficiency, improved security, and simplified implementation.", "motivation": "Small businesses face challenges in deploying LLM-based systems due to high costs, engineering complexity, and security risks in RAG settings.", "method": "The authors developed a platform using distributed, lightweight clusters on low-cost machines, encrypted networking, container isolation, and integrated prompt injection defenses into the RAG approach.", "result": "The proposed platform was tested in a real-world e-commerce setting, showing it can deliver secure and efficient chatbot services under practical constraints for small businesses.", "conclusion": "This study concludes that the presented platform provides a viable solution for small businesses to implement cost-effective and secure LLM-based chatbots without extensive technical expertise or high expenses."}}
{"id": "2601.15305", "pdf": "https://arxiv.org/pdf/2601.15305", "abs": "https://arxiv.org/abs/2601.15305", "authors": ["Alfred Shen", "Aaron Shen"], "title": "Gated Sparse Attention: Combining Computational Efficiency with Training Stability for Long-Context Language Models", "categories": ["cs.AI"], "comment": "15 pages, 1 figure, attention mechanism, sparse attention, gating, long-context", "summary": "The computational burden of attention in long-context language models has motivated two largely independent lines of work: sparse attention mechanisms that reduce complexity by attending to selected tokens, and gated attention variants that improve training sta-bility while mitigating the attention sink phenomenon. We observe that these approaches address complementary weaknesses and propose Gated Sparse Attention (GSA), an architecture that realizes the benefits of both. GSA incorporates a gated lightning indexer with sigmoid activations that produce bounded, interpretable selection scores, an adaptive sparsity controller that modulates the number of attended tokens based on local uncertainty, and dual gating at the value and output stages. We establish theoretical foundations for the approach, including complexity analysis, expressiveness results, and convergence guarantees. In experiments with 1.7B parameter models trained on 400B tokens, GSA matches the efficiency of sparse-only baselines (12-16x speedup at 128K context) while achieving the quality gains associated with gated attention: perplexity improves from 6.03 to 5.70, RULER scores at 128K context nearly double, and attention to the first token, a proxy for attention sinks, drops from 47% to under 4%. Training stability improves markedly, with loss spikes reduced by 98%.", "AI": {"tldr": "The paper introduces Gated Sparse Attention (GSA), combining sparse and gated attention mechanisms to address limitations of long-context language models, achieving efficiency and quality improvements.", "motivation": "The need to address computational burdens of long-context language models, balancing efficiency and quality, motivates the proposal of GSA, combining sparse and gated attention approaches.", "method": "GSA utilizes features like a gated lightning indexer, sigmoid activations, an adaptive sparsity controller, and dual gating mechanisms. Theoretical analysis on complexity, expressiveness, and stability is provided.", "result": "GSA matches sparse-only baselines in efficiency, achieves significant quality improvements (e.g., perplexity from 6.03 to 5.70, better attention dynamics, and reduced attention sink). It also enhances training stability, reducing loss spikes by 98%.", "conclusion": "GSA effectively combines sparse and gated attention to improve the performance and stability of long-context language models, verifying its feasibility through theoretical and experimental validation."}}
{"id": "2601.15335", "pdf": "https://arxiv.org/pdf/2601.15335", "abs": "https://arxiv.org/abs/2601.15335", "authors": ["Yi Zhai", "Dian Shen", "Junzhou Luo", "Bin Yang"], "title": "ToolCaching: Towards Efficient Caching for LLM Tool-calling", "categories": ["cs.SE", "cs.AI", "cs.PL"], "comment": null, "summary": "Recent advances in Large Language Models (LLMs) have revolutionized web applications, enabling intelligent search, recommendation, and assistant services with natural language interfaces. Tool-calling extends LLMs with the ability to interact with external APIs, greatly enhancing their practical utility. While prior research has improved tool-calling performance by adopting traditional computer systems techniques, such as parallel and asynchronous execution, the challenge of redundant or repeated tool-calling requests remains largely unaddressed. Caching is a classic solution to this problem, but applying it to LLM tool-calling introduces new difficulties due to heterogeneous request semantics, dynamic workloads, and varying freshness requirements, which render conventional cache policies ineffective. To address these issues, we propose ToolCaching, an efficient feature-driven and adaptive caching framework for LLM tool-calling systems. ToolCaching systematically integrates semantic and system-level features to evaluate request cacheability and estimate caching value. At its core, the VAAC algorithm integrates bandit-based admission with value-driven, multi-factor eviction, jointly accounting for request frequency, recency, and caching value. Extensive experiments on synthetic and public tool-calling workloads demonstrate that ToolCaching with VAAC achieves up to 11% higher cache hit ratios and 34% lower latency compared to standard policies, effectively accelerating LLM tool-calling in practical applications.", "AI": {"tldr": "The paper introduces ToolCaching, a caching framework designed to optimize LLM tool-calling by addressing challenges like request redundancy and dynamic workloads, improving performance significantly.", "motivation": "To solve the inefficiencies and redundancies in LLM tool-calling requests, as existing caching methods fail to handle dynamic workloads and varying semantics effectively.", "method": "It introduces ToolCaching, a feature-driven and adaptive framework, integrating semantic and system-level features. The VAAC algorithm is central, combining bandit-based admission and value-driven eviction policies for efficient caching.", "result": "Experiments demonstrate that ToolCaching achieves up to 11% higher cache hit ratios and reduces latency by 34%, outperforming standard cache policies.", "conclusion": "ToolCaching effectively addresses the unique challenges in LLM tool-calling systems, optimizing performance and showcasing its application utility."}}
{"id": "2601.15309", "pdf": "https://arxiv.org/pdf/2601.15309", "abs": "https://arxiv.org/abs/2601.15309", "authors": ["Jiaxin Xu", "Chao Zhang", "Raymond H. Cuijpers", "Wijnand A. IJsselsteijn"], "title": "Designing Persuasive Social Robots for Health Behavior Change: A Systematic Review of Behavior Change Strategies and Evaluation Methods", "categories": ["cs.RO", "cs.HC"], "comment": "Accepted to HRI 2026", "summary": "Social robots are increasingly applied as health behavior change interventions, yet actionable knowledge to guide their design and evaluation remains limited. This systematic review synthesizes (1) the behavior change strategies used in existing HRI studies employing social robots to promote health behavior change, and (2) the evaluation methods applied to assess behavior change outcomes. Relevant literature was identified through systematic database searches and hand searches. Analysis of 39 studies revealed four overarching categories of behavior change strategies: coaching strategies, counseling strategies, social influence strategies, and persuasion-enhancing strategies. These strategies highlight the unique affordances of social robots as behavior change interventions and offer valuable design heuristics. The review also identified key characteristics of current evaluation practices, including study designs, settings, durations, and outcome measures, on the basis of which we propose several directions for future HRI research.", "AI": {"tldr": "This paper systematically reviews health behavior change strategies and evaluation methods involving social robots, analyzing 39 studies to propose future research directions in HRI.", "motivation": "To understand how social robots can be optimally designed and evaluated for health behavior change interventions, addressing a gap in actionable guidance for their application.", "method": "A systematic review was conducted by analyzing 39 studies, with data sourced through database and hand searches. Strategies were categorized and evaluation methods were assessed.", "result": "Four behavior change strategy categories were identified: coaching, counseling, social influence, and persuasion-enhancing. Insights into current evaluation practices such as designs, settings, and measures were provided.", "conclusion": "Social robots offer unique affordances for health interventions. This study outlines design heuristics and suggests improvements for future evaluation methodologies in HRI research."}}
{"id": "2601.15360", "pdf": "https://arxiv.org/pdf/2601.15360", "abs": "https://arxiv.org/abs/2601.15360", "authors": ["Eichi Uehara"], "title": "Robust X-Learner: Breaking the Curse of Imbalance and Heavy Tails via Robust Cross-Imputation", "categories": ["stat.ML", "cs.LG", "econ.EM", "stat.ME"], "comment": "17 pages, 4 figures, 4 tables", "summary": "Estimating Heterogeneous Treatment Effects (HTE) in industrial applications such as AdTech and healthcare presents a dual challenge: extreme class imbalance and heavy-tailed outcome distributions. While the X-Learner framework effectively addresses imbalance through cross-imputation, we demonstrate that it is fundamentally vulnerable to \"Outlier Smearing\" when reliant on Mean Squared Error (MSE) minimization. In this failure mode, the bias from a few extreme observations (\"whales\") in the minority group is propagated to the entire majority group during the imputation step, corrupting the estimated treatment effect structure. To resolve this, we propose the Robust X-Learner (RX-Learner). This framework integrates a redescending \u03b3-divergence objective -- structurally equivalent to the Welsch loss under Gaussian assumptions -- into the gradient boosting machinery. We further stabilize the non-convex optimization using a Proxy Hessian strategy grounded in Majorization-Minimization (MM) principles. Empirical evaluation on a semi-synthetic Criteo Uplift dataset demonstrates that the RX-Learner reduces the Precision in Estimation of Heterogeneous Effect (PEHE) metric by 98.6% compared to the standard X-Learner, effectively decoupling the stable \"Core\" population from the volatile \"Periphery\".", "AI": {"tldr": "The paper introduces the RX-Learner framework to address biases in HTE estimation due to class imbalance and outliers.", "motivation": "The paper aims to overcome the limitations of the X-Learner framework in handling HTE estimation, particularly the susceptibility to \"Outlier Smearing\" caused by extreme observations.", "method": "The RX-Learner integrates the redescending \u03b3-divergence objective together with a Proxy Hessian strategy for optimization, leveraging Majorization-Minimization principles.", "result": "The RX-Learner demonstrated a 98.6% reduction in the PEHE metric, showcasing improved decoupling of stable populations from volatile ones.", "conclusion": "The RX-Learner effectively addresses the biases introduced by extreme observations and provides a more robust framework for HTE estimation in imbalanced datasets."}}
{"id": "2601.15366", "pdf": "https://arxiv.org/pdf/2601.15366", "abs": "https://arxiv.org/abs/2601.15366", "authors": ["Christina Thrainer"], "title": "AI-Based Culvert-Sewer Inspection", "categories": ["cs.CV"], "comment": "Masters thesis, University of Technology Graz, 2025", "summary": "Culverts and sewer pipes are critical components of drainage systems, and their failure can lead to serious risks to public safety and the environment. In this thesis, we explore methods to improve automated defect segmentation in culverts and sewer pipes. Collecting and annotating data in this field is cumbersome and requires domain knowledge. Having a large dataset for structural defect detection is therefore not feasible. Our proposed methods are tested under conditions with limited annotated data to demonstrate applicability to real-world scenarios. Overall, this thesis proposes three methods to significantly enhance defect segmentation and handle data scarcity. This can be addressed either by enhancing the training data or by adjusting a models architecture.\n  First, we evaluate preprocessing strategies, including traditional data augmentation and dynamic label injection. These techniques significantly improve segmentation performance, increasing both Intersection over Union (IoU) and F1 score. Second, we introduce FORTRESS, a novel architecture that combines depthwise separable convolutions, adaptive Kolmogorov-Arnold Networks (KAN), and multi-scale attention mechanisms. FORTRESS achieves state-of-the-art performance on the culvert sewer pipe defect dataset, while significantly reducing the number of trainable parameters, as well as its computational cost. Finally, we investigate few-shot semantic segmentation and its applicability to defect detection. Few-shot learning aims to train models with only limited data available. By employing a bidirectional prototypical network with attention mechanisms, the model achieves richer feature representations and achieves satisfactory results across evaluation metrics.", "AI": {"tldr": "This thesis focuses on improving automated defect segmentation in culverts and sewer pipes, addressing data scarcity through preprocessing, innovative model architecture, and few-shot learning.", "motivation": "Failures in culverts and sewer pipes pose risks to public safety and the environment, necessitating effective automated defect segmentation methods despite challenges in obtaining large annotated datasets.", "method": "The study introduces three approaches: preprocessing strategies like dynamic label injection, a novel lightweight architecture (FORTRESS), and a bidirectional prototypical network for few-shot learning.", "result": "Significant improvements in segmentation performance (IoU and F1 scores), state-of-the-art results with reduced computational costs for FORTRESS, and effective application of few-shot learning in defect segmentation.", "conclusion": "The proposed methods enhance defect segmentation efficiency and performance, even with limited annotated data, making them applicable to real-world drainage system inspection scenarios."}}
{"id": "2601.15333", "pdf": "https://arxiv.org/pdf/2601.15333", "abs": "https://arxiv.org/abs/2601.15333", "authors": ["Xuanning Hu", "Anchen Li", "Qianli Xing", "Jinglong Ji", "Hao Tuo", "Bo Yang"], "title": "Empowering LLMs for Structure-Based Drug Design via Exploration-Augmented Latent Inference", "categories": ["cs.LG", "cs.AI", "q-bio.QM"], "comment": null, "summary": "Large Language Models (LLMs) possess strong representation and reasoning capabilities, but their application to structure-based drug design (SBDD) is limited by insufficient understanding of protein structures and unpredictable molecular generation. To address these challenges, we propose Exploration-Augmented Latent Inference for LLMs (ELILLM), a framework that reinterprets the LLM generation process as an encoding, latent space exploration, and decoding workflow. ELILLM explicitly explores portions of the design problem beyond the model's current knowledge while using a decoding module to handle familiar regions, generating chemically valid and synthetically reasonable molecules. In our implementation, Bayesian optimization guides the systematic exploration of latent embeddings, and a position-aware surrogate model efficiently predicts binding affinity distributions to inform the search. Knowledge-guided decoding further reduces randomness and effectively imposes chemical validity constraints. We demonstrate ELILLM on the CrossDocked2020 benchmark, showing strong controlled exploration and high binding affinity scores compared with seven baseline methods. These results demonstrate that ELILLM can effectively enhance LLMs capabilities for SBDD.", "AI": {"tldr": "The paper proposes ELILLM, a framework to enhance Large Language Models (LLMs) for structure-based drug design (SBDD), improving molecular generation and protein structure understanding.", "motivation": "LLMs have strong reasoning abilities but face challenges in SBDD due to limited understanding of protein structures and unpredictable molecular generation.", "method": "The authors introduce ELILLM, a framework combining encoding, latent space exploration using Bayesian optimization, and knowledge-guided decoding to explore and generate molecules effectively.", "result": "ELILLM demonstrates superior exploration and high binding affinity scores on the CrossDocked2020 benchmark, outperforming seven other baseline methods.", "conclusion": "ELILLM successfully extends LLMs' application in SBDD by improving molecular design through systematic exploration and better encoding-decoding mechanisms."}}
{"id": "2601.15296", "pdf": "https://arxiv.org/pdf/2601.15296", "abs": "https://arxiv.org/abs/2601.15296", "authors": ["Longxuan Wei", "Yubo Zhang", "Zijiao Zhang", "Zhihu Wang", "Shiwan Zhao", "Tianyu Huang", "Huiting Zhao", "Chenfei Liu", "Shenao Zhang", "Junchi Yan"], "title": "Entropy-Tree: Tree-Based Decoding with Entropy-Guided Exploration", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models achieve strong reasoning performance, yet existing decoding strategies either explore blindly (random sampling) or redundantly (independent multi-sampling). We propose Entropy-Tree, a tree-based decoding method that exploits entropy as a signal for branching decisions--expanding the search tree only at positions where the model exhibits genuine uncertainty. Entropy-Tree shows superior accuracy and calibration in reasoning tasks: it achieves better pass@k than Multi-chain across multiple models and datasets, and its predictive entropy demonstrates better AUROC compared to several traditional metrics. Entropy-Tree unifies efficient structured exploration and reliable uncertainty estimation within a single decoding procedure.", "AI": {"tldr": "Entropy-Tree is a tree-based decoding method leveraging entropy for reasoning tasks, improving accuracy and calibration compared to existing strategies.", "motivation": "Current decoding methods for large language models either lack structure (random sampling) or introduce redundancy (independent multi-sampling), leading to inefficiencies in reasoning tasks.", "method": "Entropy-Tree uses entropy as a branching decision criterion to expand the search tree only at uncertain positions during decoding.", "result": "Entropy-Tree achieves higher accuracy (better pass@k) and improved calibration (AUROC) across multiple models and datasets versus traditional decoding approaches.", "conclusion": "Entropy-Tree offers a unified approach for efficient exploration and reliable uncertainty estimation in reasoning tasks with language models."}}
{"id": "2601.15313", "pdf": "https://arxiv.org/pdf/2601.15313", "abs": "https://arxiv.org/abs/2601.15313", "authors": ["Matt Beton", "Simran Chana"], "title": "Mind the Gap: Why Neural Memory Fails Under Semantic Density", "categories": ["q-bio.NC", "cs.AI"], "comment": "24 Pages, 5 Figures", "summary": "The brain solves a problem that current AI architectures struggle to manage: storing specific episodic facts without corrupting general semantic knowledge. Neuroscience explains this through Complementary Learning Systems theory - a fast hippocampal system for episodic storage using pattern-separated representations, and a slow neocortical system for extracting statistical regularities. Current AI systems lack this separation, attempting both functions through neural weights alone. We identify the 'Stability Gap' in online neural memory: fast-weight mechanisms that write facts into shared continuous parameters collapse to near-random accuracy within tens of semantically related facts. Through semantic density (rho), we show collapse occurs with as few as N=5 facts at high density (rho > 0.6) or N ~ 20-75 at moderate density - a phenomenon we formalise as the Orthogonality Constraint. This failure persists even with perfect attention and unlimited context, arising from write-time interference when storage and retrieval share the same substrate. We also identify schema drift and version ambiguity as primary failure modes in production systems, observing 40-70% schema consistency and 0-100% clean correction rates. Context-based memory incurs 30-300% cost premium over selective retrieval. We propose Knowledge Objects (KOs): discrete, typed memory units with controlled vocabularies and explicit version chains. Paired with neural weights, KOs enable a true complementary learning architecture, suggesting reliable AI memory may require this bicameral design.", "AI": {"tldr": "This paper highlights the limitations of current AI in storing episodic and semantic knowledge due to the lack of specialized systems like those in the brain, proposing Knowledge Objects to address these inefficiencies.", "motivation": "To address the inability of AI to separately store episodic facts and semantic knowledge without interference, and overcome the 'Stability Gap' identified in current memory systems.", "method": "The study identifies issues in neural memory such as semantic density and Orthogonality Constraint, and proposes Knowledge Objects (KOs) as a bicameral system solution.", "result": "Observations include collapse in online learning with semantic density, schema drift, high cost in context-based memory, and proposed benefit from KOs increasing efficiency and reliability.", "conclusion": "Reliable AI memory may require a complementary design combining neural weights with Knowledge Objects to overcome current performance gaps."}}
{"id": "2601.15924", "pdf": "https://arxiv.org/pdf/2601.15924", "abs": "https://arxiv.org/abs/2601.15924", "authors": ["Brainard Philemon Jagati", "Jitendra Tembhurne", "Harsh Goud", "Rudra Pratap Singh", "Chandrashekhar Meshram"], "title": "Class Confidence Aware Reweighting for Long Tailed Learning", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.PF"], "comment": "9 pages, 3 figures, IEEE Transaction on Neural Networks and Learning Systems (Submitted)", "summary": "Deep neural network models degrade significantly in the long-tailed data distribution, with the overall training data dominated by a small set of classes in the head, and the tail classes obtaining less training examples. Addressing the imbalance in the classes, attention in the related literature was given mainly to the adjustments carried out in the decision space in terms of either corrections performed at the logit level in order to compensate class-prior bias, with the least attention to the optimization process resulting from the adjustments introduced through the differences in the confidences among the samples. In the current study, we present the design of a class and confidence-aware re-weighting scheme for long-tailed learning. This scheme is purely based upon the loss level and has a complementary nature to the existing methods performing the adjustment of the logits. In the practical implementation stage of the proposed scheme, we use an \u03a9(p_t, f_c) function. This function enables the modulation of the contribution towards the training task based upon the confidence value of the prediction, as well as the relative frequency of the corresponding class. Our observations in the experiments are corroborated by significant experimental results performed on the CIFAR-100-LT, ImageNet-LT, and iNaturalist2018 datasets under various values of imbalance factors that clearly authenticate the theoretical discussions above.", "AI": {"tldr": "This paper proposes a class and confidence-aware re-weighting scheme for better handling long-tailed data distributions in deep learning.", "motivation": "To address the challenges of class imbalance in long-tailed data distributions, which affect deep neural network performance.", "method": "The study introduces a re-weighting scheme based on loss level adjustments using an \u03a9(p_t, f_c) function to modulate training contributions depending on confidence and relative class frequency.", "result": "Experimental results on datasets like CIFAR-100-LT, ImageNet-LT, and iNaturalist2018 show significant performance improvements under various imbalance conditions.", "conclusion": "The proposed scheme effectively complements existing logit adjustment methods and improves long-tailed learning performance."}}
{"id": "2601.15738", "pdf": "https://arxiv.org/pdf/2601.15738", "abs": "https://arxiv.org/abs/2601.15738", "authors": ["Junhao Qiu", "Haoyang Zhuang", "Fei Liu", "Jianjun Liu", "Qingfu Zhang"], "title": "LLM-Assisted Automatic Dispatching Rule Design for Dynamic Flexible Assembly Flow Shop Scheduling", "categories": ["cs.NE"], "comment": null, "summary": "Dynamic multi-product delivery environments demand rapid coordination of part completion and product-level kitting within hybrid processing and assembly systems to satisfy strict hierarchical supply constraints. The flexible assembly flow shop scheduling problem formally defines dependencies for multi-stage kitting, yet dynamic variants make designing integrated scheduling rules under multi-level time coupling highly challenging. Existing automated heuristic design methods, particularly genetic programming constrained to fixed terminal symbol sets, struggle to capture and leverage dynamic uncertainties and hierarchical dependency information under transient decision states. This study develops an LLM-assisted Dynamic Rule Design framework (LLM4DRD) that automatically evolves integrated online scheduling rules adapted to scheduling features. Firstly, multi-stage processing and assembly supply decisions are transformed into feasible directed edge orderings based on heterogeneous graph. Then, an elite knowledge guided initialization embeds advanced design expertise into initial rules to enhance initial quality. Additionally, a dual-expert mechanism is introduced in which LLM-A evolutionary code to generate candidate rules and LLM-S conducts scheduling evaluation, while dynamic feature-fitting rule evolution combined with hybrid evaluation enables continuous improvement and extracts adaptive rules with strong generalization capability. A series of experiments are conducted to validate the effectiveness of the method. The average tardiness of LLM4DRD is 3.17-12.39% higher than state-of-the-art methods in 20 practical instances used for training and testing, respectively. In 24 scenarios with different resource configurations, order loads, and disturbance levels totaling 480 instances, it achieves 11.10% higher performance than the second best competitor, exhibiting excellent robustness.", "AI": {"tldr": "The paper introduces LLM4DRD, a framework leveraging large language models (LLMs) to evolve online scheduling rules for dynamic multi-product delivery and assembly systems. It shows superior performance and robustness against state-of-the-art methods.", "motivation": "The motivation lies in addressing challenges in dynamic multi-product delivery environments, where hierarchical dependency constraints and uncertainties complicate scheduling decisions.", "method": "The method uses LLM-assisted dynamic rule design, transforming decisions into directed edge orderings via heterogeneous graphs. It embeds expert knowledge in initialization, employs a dual-expert mechanism (LLM-A for rule generation and LLM-S for evaluation), and enables dynamic feature-fitting rule evolution.", "result": "Experiments reveal LLM4DRD achieves 3.17-12.39% better average tardiness compared to state-of-the-art methods in training/testing scenarios, and outperforms competitors by 11.10% across diverse resource configurations and disturbances.", "conclusion": "LLM4DRD demonstrates effectiveness and robustness in evolving adaptive scheduling rules for dynamic environments, surpassing existing methods."}}
{"id": "2601.16118", "pdf": "https://arxiv.org/pdf/2601.16118", "abs": "https://arxiv.org/abs/2601.16118", "authors": ["Marco Ronzani", "Cristina Silvano"], "title": "A Case for Hypergraphs to Model and Map SNNs on Neuromorphic Hardware", "categories": ["cs.AR", "cs.NE"], "comment": null, "summary": "Executing Spiking Neural Networks (SNNs) on neuromorphic hardware poses the problem of mapping neurons to cores. SNNs operate by propagating spikes between neurons that form a graph through synapses. Neuromorphic hardware mimics them through a network-on-chip, transmitting spikes, and a mesh of cores, each managing several neurons. Its operational cost is tied to spike movement and active cores. A mapping comprises two tasks: partitioning the SNN's graph to fit inside cores and placement of each partition on the hardware mesh. Both are NP-hard problems, and as SNNs and hardware scale towards billions of neurons, they become increasingly difficult to tackle effectively. In this work, we propose to raise the abstraction of SNNs from graphs to hypergraphs, redesigning mapping techniques accordingly. The resulting model faithfully captures the replication of spikes inside cores by exposing the notion of hyperedge co-membership between neurons. We further show that the overlap and locality of hyperedges strongly correlate with high-quality mappings, making these properties instrumental in devising mapping algorithms. By exploiting them directly, grouping neurons through shared hyperedges, communication traffic and hardware resource usage can be reduced be yond what just contracting individual connections attains. To substantiate this insight, we consider several partitioning and placement algorithms, some newly devised, others adapted from literature, and compare them over progressively larger and bio-plausible SNNs. Our results show that hypergraph based techniques can achieve better mappings than the state-of-the-art at several execution time regimes. Based on these observations, we identify a promising selection of algorithms to achieve effective mappings at any scale.", "AI": {"tldr": "This paper addresses the challenges in mapping Spiking Neural Networks (SNNs) to neuromorphic hardware by introducing hypergraph-based modeling techniques for improved performance.", "motivation": "Mapping neurons to cores for efficient SNN execution on neuromorphic hardware is challenging, especially with large-scale networks requiring optimized resource utilization and spike movement management.", "method": "The authors propose using hypergraph-based abstraction for SNNs and develop algorithms exploiting hyperedge properties to optimize partitioning and placement of neurons on hardware meshes.", "result": "Experiments demonstrate that hypergraph-based techniques reduce communication traffic and resource usage, outperforming state-of-the-art methods on large and bio-plausible SNNs.", "conclusion": "Hypergraph-based models effectively improve SNN mapping techniques, and the paper identifies suitable algorithms for scalable execution on neuromorphic hardware."}}
{"id": "2601.16008", "pdf": "https://arxiv.org/pdf/2601.16008", "abs": "https://arxiv.org/abs/2601.16008", "authors": ["Federico Bruzzone", "Walter Cazzola", "Luca Favini"], "title": "Prioritizing Configuration Relevance via Compiler-Based Refined Feature Ranking", "categories": ["cs.PL"], "comment": "29 pages 4 figures", "summary": "Modern programming languages, most notably Rust, offer advanced linguistic constructs for building highly configurable software systems as aggregation of features -- identified by a configuration. However, they pose substantial challenges for program analysis, optimization, and testing, as the combinatorial explosion of configurations often makes exhaustive exploration infeasible. In this manuscript, we present the first compiler-based method for prioritizing configurations. Our approach consists of four main steps: 1. extracting a tailored intermediate representation from the Rust compiler, 2. constructing two complementary graph-based data structures, 3. using centrality measures to rank features, and 4. refining the ranking by considering the extent of code they impact. A fixed number of most relevant configurations are generated based on the achieved feature ranking. The validity of the generated configurations is guaranteed by using a SAT solver that takes a representation of this graph in conjunctive normal form. We formalized this approach and implemented it in a prototype, RustyEx, by instrumenting the Rust compiler. An empirical evaluation on higher-ranked open source Rust projects shows that RustyEx efficiently generates user-specified sets of configurations within bounded resources, while ensuring soundness by construction. The results demonstrate that centrality-guided configuration prioritization enables effective and practical exploration of large configuration spaces, paving the way for future research in configuration-aware analysis and optimization.", "AI": {"tldr": "This paper introduces RustyEx, a prototype tool for prioritizing software configurations in Rust using advanced graph-based analysis, ensuring efficient and sound exploration of configurations.", "motivation": "The motivation is to address the challenge of analyzing, optimizing, and testing configurable software systems due to the combinatorial explosion of configurations, especially in languages like Rust.", "method": "The method involves creating a novel compiler-based workflow, which extracts an intermediate representation, constructs graph-based structures, ranks features using centrality metrics, refines the ranking based on code impact, and verifies configuration validity through a SAT solver.", "result": "RustyEx effectively and efficiently prioritizes configurations for open source Rust projects, meeting user-specified resource constraints and maintaining validity.", "conclusion": "Centrality-guided configuration prioritization is a practical solution for managing large configuration spaces, with RustyEx paving the way for advancements in configuration-aware program analysis and optimization."}}
{"id": "2601.15633", "pdf": "https://arxiv.org/pdf/2601.15633", "abs": "https://arxiv.org/abs/2601.15633", "authors": ["Enzo Meneses", "Hugo Bec", "Crist\u00f3bal A. Navarroa", "Beno\u00eet Crespin", "Felipe A. Quezada", "Nancy Hitschfeld", "Heinich Porro", "Maxime Maria"], "title": "Advancing RT Core-Accelerated Fixed-Radius Nearest Neighbor Search", "categories": ["cs.DC"], "comment": "Journal submission", "summary": "In this work we introduce three ideas that can further improve particle FRNN physics simulations running on RT Cores; i) a real-time update/rebuild ratio optimizer for the bounding volume hierarchy (BVH) structure, ii) a new RT core use, with two variants, that eliminates the need of a neighbor list and iii) a technique that enables RT cores for FRNN with periodic boundary conditions (BC). Experimental evaluation using the Lennard-Jones FRNN interaction model as a case study shows that the proposed update/rebuild ratio optimizer is capable of adapting to the different dynamics that emerge during a simulation, leading to a RT core pipeline up to $\\sim 3.4\\times$ faster than with other known approaches to manage the BVH. In terms of simulation step performance, the proposed variants can significantly improve the speedup and EE of the base RT core idea; from $\\sim1.3\\times$ at small radius to $\\sim2.0\\times$ for log normal radius distributions. Furthermore, the proposed variants manage to simulate cases that would otherwise not fit in memory because of the use of neighbor lists, such as clusters of particles with log normal radius distribution. The proposed RT Core technique to support periodic BC is indeed effective as it does not introduce any significant penalty in performance. In terms of scaling, the proposed methods scale both their performance and EE across GPU generations. Throughout the experimental evaluation, we also identify the simulation cases were regular GPU computation should still be preferred, contributing to the understanding of the strengths and limitations of RT cores.", "AI": {"tldr": "This paper introduces three advancements for improving particle FRNN physics simulations using RT Cores: optimizing BVH updates, eliminating neighbor lists, and supporting periodic boundary conditions.", "motivation": "To enhance the efficiency and applicability of particle FRNN physics simulations utilizing RT Cores, especially in handling dynamic scenarios and large-scale particle distributions.", "method": "Developed an update/rebuild optimizer for BVH structures, proposed new RT Core variants to eliminate neighbor lists, and introduced techniques for periodic boundary conditions.", "result": "The methods achieved up to 3.4x faster processing, memory optimization for challenging cases, and performance consistency across GPU generations. They also identified scenarios where traditional GPU methods are preferable.", "conclusion": "The advancements leverage RT Cores effectively for FRNN simulations with increased performance, scalability, and memory efficiency, while also outlining the limitations and appropriate use cases for these techniques."}}
{"id": "2601.15306", "pdf": "https://arxiv.org/pdf/2601.15306", "abs": "https://arxiv.org/abs/2601.15306", "authors": ["Ethan Zhang"], "title": "Uncovering Latent Bias in LLM-Based Emergency Department Triage Through Proxy Variables", "categories": ["cs.AI"], "comment": "15 pages, 3 figures", "summary": "Recent advances in large language models (LLMs) have enabled their integration into clinical decision-making; however, hidden biases against patients across racial, social, economic, and clinical backgrounds persist. In this study, we investigate bias in LLM-based medical AI systems applied to emergency department (ED) triage. We employ 32 patient-level proxy variables, each represented by paired positive and negative qualifiers, and evaluate their effects using both public (MIMIC-IV-ED Demo, MIMIC-IV Demo) and restricted-access credentialed (MIMIC-IV-ED and MIMIC-IV) datasets as appropriate~\\cite{mimiciv_ed_demo,mimiciv_ed,mimiciv}. Our results reveal discriminatory behavior mediated through proxy variables in ED triage scenarios, as well as a systematic tendency for LLMs to modify perceived patient severity when specific tokens appear in the input context, regardless of whether they are framed positively or negatively. These findings indicate that AI systems is still imperfectly trained on noisy, sometimes non-causal signals that do not reliably reflect true patient acuity. Consequently, more needs to be done to ensure the safe and responsible deployment of AI technologies in clinical settings.", "AI": {"tldr": "The paper investigates biases in large language model-based medical AI for emergency department triage, revealing discriminatory behavior and flawed severity assessment due to noisy inputs.", "motivation": "The study aims to address the persistent issue of hidden biases in clinical decision-making AI tools, particularly for emergency department triage.", "method": "The authors use 32 patient-level proxy variables tested on both public and credentialed datasets (MIMIC-IV-ED Demo, MIMIC-IV Demo, MIMIC-IV-ED, and MIMIC-IV). They evaluate how these variables affect AI performance in emergency triage scenarios.", "result": "Their findings disclose discriminatory behavior caused by noise signals in training data and a tendency of LLMs to adjust patient severity based on specific input tokens.", "conclusion": "The paper concludes that existing medical AI systems reflect biases and are influenced by non-causal signals, necessitating further refinement for reliable and safe clinical use."}}
{"id": "2601.15339", "pdf": "https://arxiv.org/pdf/2601.15339", "abs": "https://arxiv.org/abs/2601.15339", "authors": ["Jayant Havare", "Ashish Mittal", "Srikanth Tamilselvam", "Ganesh Ramakrishnan"], "title": "Lost in Transcription: How Speech-to-Text Errors Derail Code Understanding", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Code understanding is a foundational capability in software engineering tools and developer workflows. However, most existing systems are designed for English-speaking users interacting via keyboards, which limits accessibility in multilingual and voice-first settings, particularly in regions like India. Voice-based interfaces offer a more inclusive modality, but spoken queries involving code present unique challenges due to the presence of non-standard English usage, domain-specific vocabulary, and custom identifiers such as variable and function names, often combined with code-mixed expressions. In this work, we develop a multilingual speech-driven framework for code understanding that accepts spoken queries in a user native language, transcribes them using Automatic Speech Recognition (ASR), applies code-aware ASR output refinement using Large Language Models (LLMs), and interfaces with code models to perform tasks such as code question answering and code retrieval through benchmarks such as CodeSearchNet, CoRNStack, and CodeQA. Focusing on four widely spoken Indic languages and English, we systematically characterize how transcription errors impact downstream task performance. We also identified key failure modes in ASR for code and demonstrated that LLM-guided refinement significantly improves performance across both transcription and code understanding stages. Our findings underscore the need for code-sensitive adaptations in speech interfaces and offer a practical solution for building robust, multilingual voice-driven programming tools.", "AI": {"tldr": "This paper presents a multilingual speech-driven framework for code understanding, targeting inclusivity for non-English speakers and voice-first users.", "motivation": "The paper aims to address accessibility challenges in existing software engineering tools, which are predominantly designed for English-speaking users typing via keyboards, neglecting voice-based and multilingual interactions.", "method": "The framework utilizes Automatic Speech Recognition (ASR) for spoken queries in native languages, refines transcriptions using Large Language Models (LLMs), and interfaces with code models to handle tasks like code question answering and code retrieval through specific benchmarks.", "result": "Key insights include the systematic analysis of transcription errors' impacts on task performance, identification of ASR's failure modes in code contexts, and the demonstration of LLM-guided refinement improving transcription accuracy and code understanding.", "conclusion": "The study highlights the need for code-sensitive adaptations in speech interfaces and proposes effective methodologies for creating inclusive and resilient voice-driven programming tools."}}
{"id": "2601.15349", "pdf": "https://arxiv.org/pdf/2601.15349", "abs": "https://arxiv.org/abs/2601.15349", "authors": ["Jiaqing Chang", "Song Gao", "Chaowei Dong", "zhaobang Li", "Yang Liu"], "title": "Preparation and Motion Study of Magnetically Driven Micro Soft Robot Mimicking the Cownose Ray", "categories": ["cs.RO", "eess.SY"], "comment": "These experiments lay an important foundation for the study of tether-free control of underwater micro-soft robots. Furthermore, this research provides important references for the fields of biomimetic robots and magnetically controlled micro-soft robots", "summary": "In narrow, unstructured underwater environments such as environmental monitoring and minimally invasive medical procedures, micro soft robots exhibit unique advantages due to their flexible movement capabilities and small size. At the same time, applying bionic technology to the structural design of micro soft robots can significantly improve their swimming performance. However, limited by their miniaturization, these robots are difficult to power internally and usually adopt a wireless power supply method. This study designs and fabricates a magnetically responsive, cownose ray-inspired micro soft robot based on the swimming principle of the cownose ray. The robot is made of a certain proportion of NdFeB and PDMS. Then, a three-dimensional Helmholtz coil is used to generate an oscillating harmonic magnetic field to conduct swimming experiments on the robot, exploring the influence of magnetic field parameters on the robot's swimming performance. The experimental results show that the swimming speed is the fastest at B = 5 mT and f = 11 Hz, reaching 5.25 mm/s, which is about 0.5 body lengths per second. In addition, by adjusting the current direction and frequency of the coil, the robot can perform different swimming modes such as straight swimming, turning swimming, and directional swimming. By employing a stepwise adjustment method, the impact of response errors on the robot's trajectory can be effectively reduced. This study demonstrates a method for magnetically driven micro soft robots, laying a foundation for the application of wireless-driven robots in underwater narrow spaces.", "AI": {"tldr": "This paper develops a magnetically responsive micro soft robot, inspired by a cownose ray, that swims efficiently in narrow underwater environments.", "motivation": "To enhance the swimming performance of micro soft robots in unstructured underwater environments for applications including environmental monitoring and minimally invasive medical procedures.", "method": "The micro soft robot design incorporates a combination of NdFeB and PDMS materials, powered wirelessly by an oscillating magnetic field generated by a 3D Helmholtz coil.", "result": "The robot achieved its highest swimming speed at 5 mT and 11 Hz, reaching 5.25 mm/s, and demonstrated versatile movement capabilities like straight, turning, and directional swimming.", "conclusion": "This study provides a viable approach for wireless control of micro soft robots in narrow underwater spaces, advancing their potential applications."}}
{"id": "2601.15363", "pdf": "https://arxiv.org/pdf/2601.15363", "abs": "https://arxiv.org/abs/2601.15363", "authors": ["Jason Bohne", "Ieva Petrulionyte", "Michael Arbel", "Julien Mairal", "Pawe\u0142 Polak"], "title": "Non-Stationary Functional Bilevel Optimization", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Functional bilevel optimization (FBO) provides a powerful framework for hierarchical learning in function spaces, yet current methods are limited to static offline settings and perform suboptimally in online, non-stationary scenarios. We propose SmoothFBO, the first algorithm for non-stationary FBO with both theoretical guarantees and practical scalability. SmoothFBO introduces a time-smoothed stochastic hypergradient estimator that reduces variance through a window parameter, enabling stable outer-loop updates with sublinear regret. Importantly, the classical parametric bilevel case is a special reduction of our framework, making SmoothFBO a natural extension to online, non-stationary settings. Empirically, SmoothFBO consistently outperforms existing FBO methods in non-stationary hyperparameter optimization and model-based reinforcement learning, demonstrating its practical effectiveness. Together, these results establish SmoothFBO as a general, theoretically grounded, and practically viable foundation for bilevel optimization in online, non-stationary scenarios.", "AI": {"tldr": "SmoothFBO is a novel algorithm for functional bilevel optimization addressing online, non-stationary scenarios with strong theoretical foundations and empirical performance.", "motivation": "To improve functional bilevel optimization methods that perform poorly in online, non-stationary settings.", "method": "Introduced a time-smoothed stochastic hypergradient estimator with a window parameter to stabilize outer-loop updates with sublinear regret.", "result": "SmoothFBO outperformed existing methods in hyperparameter optimization and model-based reinforcement learning in non-stationary settings.", "conclusion": "SmoothFBO establishes a theoretically sound and practically effective framework for bilevel optimization in dynamic environments."}}
{"id": "2601.15406", "pdf": "https://arxiv.org/pdf/2601.15406", "abs": "https://arxiv.org/abs/2601.15406", "authors": ["Hatef Otroshi Shahreza", "Anjith George", "S\u00e9bastien Marcel"], "title": "Evaluating Multimodal Large Language Models for Heterogeneous Face Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have recently demonstrated strong performance on a wide range of vision-language tasks, raising interest in their potential use for biometric applications. In this paper, we conduct a systematic evaluation of state-of-the-art MLLMs for heterogeneous face recognition (HFR), where enrollment and probe images are from different sensing modalities, including visual (VIS), near infrared (NIR), short-wave infrared (SWIR), and thermal camera. We benchmark multiple open-source MLLMs across several cross-modality scenarios, including VIS-NIR, VIS-SWIR, and VIS-THERMAL face recognition. The recognition performance of MLLMs is evaluated using biometric protocols and based on different metrics, including Acquire Rate, Equal Error Rate (EER), and True Accept Rate (TAR). Our results reveal substantial performance gaps between MLLMs and classical face recognition systems, particularly under challenging cross-spectral conditions, in spite of recent advances in MLLMs. Our findings highlight the limitations of current MLLMs for HFR and also the importance of rigorous biometric evaluation when considering their deployment in face recognition systems.", "AI": {"tldr": "The paper evaluates Multimodal Large Language Models (MLLMs) for heterogeneous face recognition (HFR) across different modalities, finding significant performance gaps compared to traditional systems.", "motivation": "The motivation is to explore the potential of state-of-the-art MLLMs in biometric applications, specifically for face recognition across different imaging modalities.", "method": "The authors systematically evaluated multiple open-source MLLMs across cross-modality facial recognition scenarios, using metrics like Acquire Rate, Equal Error Rate (EER), and True Accept Rate (TAR).", "result": "Results showed that MLLMs underperformed significantly in comparison to traditional face recognition systems, particularly in challenging cross-spectral conditions.", "conclusion": "The paper concludes that current MLLMs have substantial limitations for HFR tasks and emphasizes the need for rigorous biometric evaluation before deployment."}}
{"id": "2601.15337", "pdf": "https://arxiv.org/pdf/2601.15337", "abs": "https://arxiv.org/abs/2601.15337", "authors": ["Shourya Jain", "Paras Chopra"], "title": "Language Models Entangle Language and Culture", "categories": ["cs.LG", "cs.CL"], "comment": "Accepted at LM4UC Workshop at AAAI'26, Submitted to ACL 2026. 17 pages, 7 figures", "summary": "Users should not be systemically disadvantaged by the language they use for interacting with LLMs; i.e. users across languages should get responses of similar quality irrespective of language used. In this work, we create a set of real-world open-ended questions based on our analysis of the WildChat dataset and use it to evaluate whether responses vary by language, specifically, whether answer quality depends on the language used to query the model. We also investigate how language and culture are entangled in LLMs such that choice of language changes the cultural information and context used in the response by using LLM-as-a-Judge to identify the cultural context present in responses. To further investigate this, we evaluate LLMs on a translated subset of the CulturalBench benchmark across multiple languages. Our evaluations reveal that LLMs consistently provide lower quality answers to open-ended questions in low resource languages. We find that language significantly impacts the cultural context used by the model. This difference in context impacts the quality of the downstream answer.", "AI": {"tldr": "The study evaluates if language impacts the quality of responses from LLMs, finding lower-quality answers in low-resource languages and cultural differences in responses.", "motivation": "The motivation is to ensure fairness in LLM interactions across diverse languages, without systemic disadvantages to users based on the language they use.", "method": "The study used open-ended questions from the WildChat dataset, evaluated LLM responses for quality variation based on language, and analyzed cultural biases using CulturalBench benchmarks.", "result": "LLMs deliver lower-quality answers in low-resource languages and demonstrate cultural context variations influenced by the choice of language.", "conclusion": "To achieve fairness, LLMs need improvement in handling low-resource languages and mitigating cultural biases in outputs."}}
{"id": "2601.15297", "pdf": "https://arxiv.org/pdf/2601.15297", "abs": "https://arxiv.org/abs/2601.15297", "authors": ["Edward Ajayi"], "title": "AfriEconQA: A Benchmark Dataset for African Economic Analysis based on World Bank Reports", "categories": ["cs.CL"], "comment": null, "summary": "We introduce AfriEconQA, a specialized benchmark dataset for African economic analysis grounded in a comprehensive corpus of 236 World Bank reports. The task of AfriEconQA is to answer complex economic queries that require high-precision numerical reasoning and temporal disambiguation from specialized institutional documents. The dataset consists of 8,937 curated QA instances, rigorously filtered from a pool of 10018 synthetic questions to ensure high-quality evidence-answer alignment. Each instance is composed of: (1) a question requiring reasoning over economic indicators, (2) the corresponding evidence retrieved from the corpus, (3) a verified ground-truth answer, and (4) source metadata (e.g., URL and publication date) to ensure temporal provenance. AfriEconQA is the first benchmark focused specifically on African economic analysis, providing a unique challenge for Information Retrieval (IR) systems, as the data is largely absent from the pretraining corpora of current Large Language Models (LLMs). We operationalize this dataset through an 11-experiment matrix, benchmarking a zero-shot baseline (GPT-5 Mini) against RAG configurations using GPT-4o and Qwen 32B across five distinct embedding and ranking strategies. Our results demonstrate a severe parametric knowledge gap, where zero-shot models fail to answer over 90 percent of queries, and even state-of-the-art RAG pipelines struggle to achieve high precision. This confirms AfriEconQA as a robust and challenging benchmark for the next generation of domain-specific IR and RAG systems. The AfriEconQA dataset and code will be made publicly available upon publication.", "AI": {"tldr": "AfriEconQA introduces a benchmark for analyzing African economic data using curated QA instances from World Bank reports, presenting a unique challenge for IR systems and LLMs.", "motivation": "To address the absence of African economic data in pretraining corpora and evaluate the ability of IR systems to process complex economic queries.", "method": "The paper develops AfriEconQA, a benchmark dataset of 8,937 QA instances from World Bank reports, tested through multiple experiments comparing zero-shot and RAG configurations.", "result": "Zero-shot models failed to answer over 90% of the queries, while advanced RAG pipelines also faced difficulties in achieving high precision.", "conclusion": "AfriEconQA highlights significant gaps in parametric knowledge and establishes itself as a crucial challenge for improving domain-specific IR and RAG systems, with plans to release the dataset publicly."}}
{"id": "2601.15314", "pdf": "https://arxiv.org/pdf/2601.15314", "abs": "https://arxiv.org/abs/2601.15314", "authors": ["Lalit Kumar Shukla"], "title": "Beyond the Einstein-Bohr Debate: Cognitive Complementarity and the Emergence of Quantum Intuition", "categories": ["q-bio.NC", "cs.AI", "quant-ph"], "comment": "This interdisciplinary work bridges quantum foundations and cognitive science, proposing a formal extension of complementarity into cognitive reasoning and introducing the testable construct of quantum intuition", "summary": "Recent high-precision experimental confirmations of quantum complementarity have revitalized foundational debates about measurement, description, and realism. This article argues that complementarity is most productively interpreted as an epistemic principle--constraining what can be simultaneously accessed and represented--rather than as an ontological claim about quantum reality. Reexamining the Einstein-Bohr debate through this lens reveals a persistent tension between descriptive completeness and contextual meaning, a tension experiments clarify but do not dissolve. Building on this analysis, we introduce cognitive complementarity as a structural principle governing reasoning under non-classical uncertainty, where mutually constraining representations cannot be jointly optimized. Within this framework, we propose quantum intuition as a testable cognitive capacity: the ability to sustain representational plurality, regulate commitment timing, and resolve perspective-incompatibilities in a context-sensitive manner. Formulated as a naturalistic construct grounded in shared informational constraints, quantum intuition offers a principled bridge between quantum measurement theory and cognition. This work reframes the historical debate, extends epistemic lessons from quantum foundations into cognitive science, and outlines empirical pathways for studying decision-making in contexts of irreducible uncertainty.", "AI": {"tldr": "The paper explores quantum complementarity as an epistemic principle, linking it to reasoning under uncertainty, and introduces 'quantum intuition' as a cognitive capacity for decision-making in ambiguous contexts.", "motivation": "High-precision experimental confirmation of quantum complementarity stimulated debate on measurement, description, realism, and epistemological implications.", "method": "Reexamining historical debates like Einstein-Bohr through an epistemic perspective, analyzing quantum complementarity, and linking it to cognitive frameworks for reasoning under uncertainty.", "result": "Introduced 'quantum intuition,' a testable cognitive concept encompassing representation plurality, timing regulation, and resolving perspective-incompatibilities.", "conclusion": "This work bridges quantum measurement theory with cognition, reframes debates on quantum foundations, and suggests empirical studies on decision-making involving uncertainty."}}
{"id": "2601.16096", "pdf": "https://arxiv.org/pdf/2601.16096", "abs": "https://arxiv.org/abs/2601.16096", "authors": ["Hyunsoo Kim", "Ehsan Pajouheshgar", "Sabine S\u00fcsstrunk", "Wenzel Jakob", "Jinah Park"], "title": "Neural Particle Automata: Learning Self-Organizing Particle Dynamics", "categories": ["cs.NE", "cs.CV"], "comment": "15 pages, 15 figures", "summary": "We introduce Neural Particle Automata (NPA), a Lagrangian generalization of Neural Cellular Automata (NCA) from static lattices to dynamic particle systems. Unlike classical Eulerian NCA where cells are pinned to pixels or voxels, NPA model each cell as a particle with a continuous position and internal state, both updated by a shared, learnable neural rule. This particle-based formulation yields clear individuation of cells, allows heterogeneous dynamics, and concentrates computation only on regions where activity is present. At the same time, particle systems pose challenges: neighborhoods are dynamic, and a naive implementation of local interactions scale quadratically with the number of particles. We address these challenges by replacing grid-based neighborhood perception with differentiable Smoothed Particle Hydrodynamics (SPH) operators backed by memory-efficient, CUDA-accelerated kernels, enabling scalable end-to-end training. Across tasks including morphogenesis, point-cloud classification, and particle-based texture synthesis, we show that NPA retain key NCA behaviors such as robustness and self-regeneration, while enabling new behaviors specific to particle systems. Together, these results position NPA as a compact neural model for learning self-organizing particle dynamics.", "AI": {"tldr": "The authors present Neural Particle Automata (NPA), expanding Neural Cellular Automata (NCA) to dynamic particle systems using differentiable Smoothed Particle Hydrodynamics for computation efficiency and enabling new behaviors in particle dynamics.", "motivation": "To generalize Neural Cellular Automata (NCA) from static grid-based lattices to dynamic particle systems, addressing challenges in computational scalability and enabling new behaviors specific to such systems.", "method": "They designed Neural Particle Automata (NPA) with a particle-based approach using differentiable Smoothed Particle Hydrodynamics operators for efficient computation and end-to-end training of dynamic particle systems.", "result": "NPA demonstrate robustness, self-regeneration, and heterogeneous dynamics across various tasks such as morphogenesis, point-cloud classification, and texture synthesis, aligning with key NCA behaviors while unlocking unique capabilities for particle systems.", "conclusion": "Neural Particle Automata (NPA) provide a scalable and compact neural model for learning self-organizing dynamics in particle-based systems, bridging the gap between static cellular automata and dynamic particle behaviors."}}
{"id": "2601.15294", "pdf": "https://arxiv.org/pdf/2601.15294", "abs": "https://arxiv.org/abs/2601.15294", "authors": ["Elif Uskuplu", "Lawrence S. Moss", "Valeria de Paiva"], "title": "KnowTeX: Visualizing Mathematical Dependencies", "categories": ["cs.HC", "cs.IR", "cs.PL"], "comment": null, "summary": "Mathematical knowledge exists in many forms, ranging from informal textbooks and lecture notes to large formal proof libraries, yet moving between these representations remains difficult. Informal texts hide dependencies, while formal systems expose every detail in ways that are not always human-readable. Dependency graphs offer a middle ground by making visible the structure of results, definitions, and proofs. We present KnowTeX, a standalone, user-friendly tool that extends the ideas of Lean's Blueprints, enabling the visualization of conceptual dependencies directly from LaTeX sources. Using a simple \"uses\" command, KnowTeX extracts relationships among statements and generates previewable graphs in DOT and TikZ formats. Applied to mathematical texts, such graphs clarify core results, support education and formalization, and provide a resource for aligning informal and formal mathematical representations. We argue that dependency graphs should become a standard feature of mathematical writing, benefiting both human readers and automated systems.", "AI": {"tldr": "The paper introduces KnowTeX, a tool that creates dependency graphs from LaTeX sources for clarity in mathematical writings.", "motivation": "Difficulty in transitioning between informal texts and formal proof libraries due to hidden or overly exposed dependencies.", "method": "KnowTeX uses a 'uses' command to extract conceptual relationships and generate visual dependency graphs in DOT and TikZ formats.", "result": "The tool successfully clarifies core results, educational processes, and aligns informal and formal representations via dependency graphs.", "conclusion": "Dependency graphs should become a standard practice in mathematical writing to improve understanding and integration for both humans and automated systems."}}
{"id": "2601.16073", "pdf": "https://arxiv.org/pdf/2601.16073", "abs": "https://arxiv.org/abs/2601.16073", "authors": ["Hanwen Zhang", "Qiaojin Shen", "Yuxi Liu", "Yuesheng Zhu", "Guibo Luo"], "title": "DSFedMed: Dual-Scale Federated Medical Image Segmentation via Mutual Distillation Between Foundation and Lightweight Models", "categories": ["cs.CV", "cs.DC"], "comment": null, "summary": "Foundation Models (FMs) have demonstrated strong generalization across diverse vision tasks. However, their deployment in federated settings is hindered by high computational demands, substantial communication overhead, and significant inference costs. We propose DSFedMed, a dual-scale federated framework that enables mutual knowledge distillation between a centralized foundation model and lightweight client models for medical image segmentation. To support knowledge distillation, a set of high-quality medical images is generated to replace real public datasets, and a learnability-guided sample selection strategy is proposed to enhance efficiency and effectiveness in dual-scale distillation. This mutual distillation enables the foundation model to transfer general knowledge to lightweight clients, while also incorporating client-specific insights to refine the foundation model. Evaluations on five medical imaging segmentation datasets show that DSFedMed achieves an average 2 percent improvement in Dice score while reducing communication costs and inference time by nearly 90 percent compared to existing federated foundation model baselines. These results demonstrate significant efficiency gains and scalability for resource-limited federated deployments.", "AI": {"tldr": "The paper introduces DSFedMed, a federated learning framework for medical image segmentation using mutual knowledge distillation, addressing challenges in deploying foundation models.", "motivation": "To overcome computational, communication, and inference limitations that hinder the deployment of foundation models in federated medical imaging tasks.", "method": "Dual-scale mutual knowledge distillation framework with synthetic high-quality images for distillation support and a learnability-guided sample selection strategy.", "result": "Achieved a 2% improvement in Dice score on average, while reducing communication and inference costs by nearly 90% compared to current federated baselines.", "conclusion": "DSFedMed demonstrates high efficiency and scalability, showcasing its potential in resource-limited federated deployments of medical image segmentation."}}
{"id": "2601.15307", "pdf": "https://arxiv.org/pdf/2601.15307", "abs": "https://arxiv.org/abs/2601.15307", "authors": ["Guo-Biao Zhang", "Ding-Yuan Liu", "Da-Yi Wu", "Tian Lan", "Heyan Huang", "Zhijing Wu", "Xian-Ling Mao"], "title": "DeepSurvey-Bench: Evaluating Academic Value of Automatically Generated Scientific Survey", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "The rapid development of automated scientific survey generation technology has made it increasingly important to establish a comprehensive benchmark to evaluate the quality of generated surveys.Nearly all existing evaluation benchmarks rely on flawed selection criteria such as citation counts and structural coherence to select human-written surveys as the ground truth survey datasets, and then use surface-level metrics such as structural quality and reference relevance to evaluate generated surveys.However, these benchmarks have two key issues: (1) the ground truth survey datasets are unreliable because of a lack academic dimension annotations; (2) the evaluation metrics only focus on the surface quality of the survey such as logical coherence. Both issues lead to existing benchmarks cannot assess to evaluate their deep \"academic value\", such as the core research objectives and the critical analysis of different studies. To address the above problems, we propose DeepSurvey-Bench, a novel benchmark designed to comprehensively evaluate the academic value of generated surveys. Specifically, our benchmark propose a comprehensive academic value evaluation criteria covering three dimensions: informational value, scholarly communication value, and research guidance value. Based on this criteria, we construct a reliable dataset with academic value annotations, and evaluate the deep academic value of the generated surveys. Extensive experimental results demonstrate that our benchmark is highly consistent with human performance in assessing the academic value of generated surveys.", "AI": {"tldr": "DeepSurvey-Bench is proposed as a novel benchmark for evaluating the academic value of machine-generated surveys, emphasizing informational value, scholarly communication, and research guidance.", "motivation": "Existing benchmarks use flawed ground truth datasets relying on citation counts and superficial metrics, failing to assess the deep academic value of generated surveys.", "method": "The authors designed a benchmark with comprehensive academic evaluation criteria across three dimensions and built a dataset with academic annotations for assessing generated surveys.", "result": "Experimental results showed DeepSurvey-Bench aligns well with human assessments of academic value in surveys.", "conclusion": "DeepSurvey-Bench provides an advanced and reliable framework for evaluating generated surveys' deep academic value, addressing weaknesses in existing benchmarks."}}
{"id": "2601.15352", "pdf": "https://arxiv.org/pdf/2601.15352", "abs": "https://arxiv.org/abs/2601.15352", "authors": ["Adeyemi Adeseye", "Aisvarya Adeseye"], "title": "A Prompt-Based Framework for Loop Vulnerability Detection Using Local LLMs", "categories": ["cs.SE"], "comment": "Accepted and Waiting to be published ICAI'25: 27th International Conference on Artificial Intelligence https://american-cse.org/csce2025/conferences-ICAI", "summary": "Loop vulnerabilities are one major risky construct in software development. They can easily lead to infinite loops or executions, exhaust resources, or introduce logical errors that degrade performance and compromise security. The problem are often undetected by traditional static analyzers because such tools rely on syntactic patterns, which makes them struggle to detect semantic flaws. Consequently, Large Language Models (LLMs) offer new potential for vulnerability detection because of their ability to understand code contextually. Moreover, local LLMs unlike commercial ones like ChatGPT or Gemini addresses issues such as privacy, latency, and dependency concerns by facilitating efficient offline analysis. Consequently, this study proposes a prompt-based framework that utilize local LLMs for the detection of loop vulnerabilities within Python 3.7+ code. The framework targets three categories of loop-related issues, such as control and logic errors, security risks inside loops, and resource management inefficiencies. A generalized and structured prompt-based framework was designed and tested with two locally deployed LLMs (LLaMA 3.2; 3B and Phi 3.5; 4B) by guiding their behavior via iterative prompting. The designed prompt-based framework included key safeguarding features such as language-specific awareness, code-aware grounding, version sensitivity, and hallucination prevention. The LLM results were validated against a manually established baseline truth, and the results indicate that Phi outperforms LLaMA in precision, recall, and F1-score. The findings emphasize the importance of designing effective prompts for local LLMs to perform secure and accurate code vulnerability analysis.", "AI": {"tldr": "This study introduces a prompt-based framework utilizing local LLMs to detect loop vulnerabilities in Python code, focusing on logical, security, and resource management issues.", "motivation": "Traditional static analyzers struggle to detect semantic flaws in code, especially loop vulnerabilities, posing security and performance risks.", "method": "The framework uses local language models, LLaMA and Phi, guided by iterative prompting, for vulnerability detection and employs safeguarding features like hallucination prevention and version sensitivity.", "result": "Validation shows Phi surpasses LLaMA in precision, recall, and F1-score when detecting loop vulnerabilities.", "conclusion": "Local LLMs, aided by effective prompts, are promising tools for secure and accurate identification of software vulnerabilities."}}
{"id": "2601.15419", "pdf": "https://arxiv.org/pdf/2601.15419", "abs": "https://arxiv.org/abs/2601.15419", "authors": ["Yashuai Yan", "Dongheui Lee"], "title": "Learning a Unified Latent Space for Cross-Embodiment Robot Control", "categories": ["cs.RO"], "comment": null, "summary": "We present a scalable framework for cross-embodiment humanoid robot control by learning a shared latent representation that unifies motion across humans and diverse humanoid platforms, including single-arm, dual-arm, and legged humanoid robots. Our method proceeds in two stages: first, we construct a decoupled latent space that captures localized motion patterns across different body parts using contrastive learning, enabling accurate and flexible motion retargeting even across robots with diverse morphologies. To enhance alignment between embodiments, we introduce tailored similarity metrics that combine joint rotation and end-effector positioning for critical segments, such as arms. Then, we train a goal-conditioned control policy directly within this latent space using only human data. Leveraging a conditional variational autoencoder, our policy learns to predict latent space displacements guided by intended goal directions. We show that the trained policy can be directly deployed on multiple robots without any adaptation. Furthermore, our method supports the efficient addition of new robots to the latent space by learning only a lightweight, robot-specific embedding layer. The learned latent policies can also be directly applied to the new robots. Experimental results demonstrate that our approach enables robust, scalable, and embodiment-agnostic robot control across a wide range of humanoid platforms.", "AI": {"tldr": "This paper introduces a scalable framework for humanoid robot control by unifying human and diverse robot motions in a shared latent space and demonstrating generalizable and adaptive control methods.", "motivation": "The research aims to facilitate seamless humanoid robot control across vastly different embodiments, addressing challenges in motion retargeting, adaptability, and scalability.", "method": "The approach employs a two-stage process: building a decoupled latent space via contrastive learning and training goal-conditioned control policies in the latent space using conditional variational autoencoders.", "result": "The framework achieves robust and scalable control across varied humanoid robots, with experimental results showing improved motion retargeting and adaptability.", "conclusion": "The study proves the scalability and embodiment-agnostic control capabilities of the proposed framework while simplifying the integration of new robots."}}
{"id": "2601.15500", "pdf": "https://arxiv.org/pdf/2601.15500", "abs": "https://arxiv.org/abs/2601.15500", "authors": ["Saptarshi Roy", "Alessandro Rinaldo", "Purnamrita Sarkar"], "title": "Low-Dimensional Adaptation of Rectified Flow: A New Perspective through the Lens of Diffusion and Stochastic Localization", "categories": ["stat.ML", "cs.AI", "cs.LG", "math.ST"], "comment": "32 pages, 7 figures", "summary": "In recent years, Rectified flow (RF) has gained considerable popularity largely due to its generation efficiency and state-of-the-art performance. In this paper, we investigate the degree to which RF automatically adapts to the intrinsic low dimensionality of the support of the target distribution to accelerate sampling. We show that, using a carefully designed choice of the time-discretization scheme and with sufficiently accurate drift estimates, the RF sampler enjoys an iteration complexity of order $O(k/\\varepsilon)$ (up to log factors), where $\\varepsilon$ is the precision in total variation distance and $k$ is the intrinsic dimension of\n  the target distribution. In addition, we show that the denoising diffusion probabilistic model (DDPM) procedure is equivalent to a stochastic version of RF by establishing a novel connection between these processes and stochastic localization. Building on this connection, we further design a stochastic RF sampler that also adapts to the low-dimensionality of the target distribution under milder requirements on the accuracy of the drift estimates, and also with a specific time schedule. We illustrate with simulations on the synthetic data and text-to-image data experiments the improved performance of the proposed samplers implementing the newly designed time-discretization schedules.", "AI": {"tldr": "The paper investigates the efficiency and capabilities of Rectified Flow (RF) in adapting to the intrinsic low-dimensionality of target distributions for accelerated sampling, establishes its connection to DDPM, and proposes a stochastic RF sampler with improved performance.", "motivation": "To improve sampling efficiency in RF by adapting to the intrinsic low-dimensionality of the target distribution, and to explore its connections with stochastic processes like DDPM.", "method": "The authors analyze RF's efficiency using a time-discretization scheme and establish a theoretical connection between RF and DDPM/stochastic localization. They also propose a stochastic RF sampler with a new time-schedule for improved performance.", "result": "The RF sampler demonstrates an iteration complexity of O(k/\u03b5), adapting to the intrinsic dimensionality (k) of the target distribution. Simulations on synthetic and text-to-image data show improved performance using the proposed samplers.", "conclusion": "The findings provide theoretical insights into RF's adaptability to low-dimensional target distributions, highlight the connection with DDPM, and introduce practical sampling strategies with enhanced performance."}}
{"id": "2601.15408", "pdf": "https://arxiv.org/pdf/2601.15408", "abs": "https://arxiv.org/abs/2601.15408", "authors": ["Pablo Messina", "Andr\u00e9s Villa", "Juan Le\u00f3n Alc\u00e1zar", "Karen S\u00e1nchez", "Carlos Hinojosa", "Denis Parra", "\u00c1lvaro Soto", "Bernard Ghanem"], "title": "CURE: Curriculum-guided Multi-task Training for Reliable Anatomy Grounded Report Generation", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "31 pages, 7 figures, submitted to CVPR 2026 (under review)", "summary": "Medical vision-language models can automate the generation of radiology reports but struggle with accurate visual grounding and factual consistency. Existing models often misalign textual findings with visual evidence, leading to unreliable or weakly grounded predictions. We present CURE, an error-aware curriculum learning framework that improves grounding and report quality without any additional data. CURE fine-tunes a multimodal instructional model on phrase grounding, grounded report generation, and anatomy-grounded report generation using public datasets. The method dynamically adjusts sampling based on model performance, emphasizing harder samples to improve spatial and textual alignment. CURE improves grounding accuracy by +0.37 IoU, boosts report quality by +0.188 CXRFEScore, and reduces hallucinations by 18.6%. CURE is a data-efficient framework that enhances both grounding accuracy and report reliability. Code is available at https://github.com/PabloMessina/CURE and model weights at https://huggingface.co/pamessina/medgemma-4b-it-cure", "AI": {"tldr": "CURE is a curriculum learning framework that improves grounding accuracy and report quality in medical vision-language models without using extra data.", "motivation": "To address the challenge of inaccurate visual grounding and factual inconsistency in existing medical vision-language models for radiology report generation.", "method": "CURE utilizes curriculum learning by phase-wise fine-tuning on phrase grounding, grounded report generation, and anatomy-grounded report generation. It emphasizes harder samples based on model performance to enhance accuracy.", "result": "CURE achieved a +0.37 improvement in IoU, +0.188 enhancement in CXRFEScore, and an 18.6% reduction in hallucinations, demonstrating increased grounding accuracy and report reliability.", "conclusion": "CURE provides a data-efficient solution for enhancing both grounding accuracy and the overall factual consistency of radiology report generation models."}}
{"id": "2601.15370", "pdf": "https://arxiv.org/pdf/2601.15370", "abs": "https://arxiv.org/abs/2601.15370", "authors": ["Maciej Kilian", "Oleg Mkrtchyan", "Luke Zettlemoyer", "Akshat Shrivastava", "Armen Aghajanyan"], "title": "Improving MoE Compute Efficiency by Composing Weight and Data Sparsity", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Mixture-of-Experts layers achieve compute efficiency through weight sparsity: each token activates only a subset of experts. Data sparsity, where each expert processes only a subset of tokens, offers a complementary axis. Expert-choice routing implements data sparsity directly but violates causality in autoregressive models, creating train-inference mismatch. We recover data sparsity within causal token-choice MoE by leveraging zero-compute (null) experts within the routing pool. When a token routes to null experts, those slots consume no compute. The standard load balancing objective trains the model to uniformly use all experts (real and null) therefore creating data sparsity in expectation without the causality violations. We evaluate on vision-language model training, where data heterogeneity is pronounced: vision encoders produce many low-information tokens while text tokens are denser. At matched expected FLOPs, composing weight and data sparsity yields a more compute-efficient frontier than weight sparsity alone, with gains in training loss and downstream performance. The model learns implicit modality-aware allocation, routing vision tokens to null experts more aggressively than text, without explicit modality routing.", "AI": {"tldr": "The paper introduces a causality-preserving approach to data sparsity in Mixture-of-Experts (MoE) layers using zero-compute (null) experts, enhancing compute efficiency.", "motivation": "To address train-inference mismatch and improve compute efficiency by combining weight sparsity and data sparsity in Mixture-of-Experts layers.", "method": "Introduce zero-compute (null) experts to enable causal token-choice in MoE and use a load balancing objective to train uniform use of experts.", "result": "Data sparsity combined with weight sparsity leads to improved training loss, downstream performance, and compute efficiency compared to weight sparsity alone, particularly in vision-language models.", "conclusion": "Integrating weight and data sparsity enhances compute efficiency and performance, enabling implicit modality-aware routing without explicit design."}}
{"id": "2601.15319", "pdf": "https://arxiv.org/pdf/2601.15319", "abs": "https://arxiv.org/abs/2601.15319", "authors": ["Francesco Chiappone", "Davide Marocco", "Nicola Milano"], "title": "Large Language Models as Simulative Agents for Neurodivergent Adult Psychometric Profiles", "categories": ["q-bio.NC", "cs.AI"], "comment": null, "summary": "Adult neurodivergence, including Attention-Deficit/Hyperactivity Disorder (ADHD), high-functioning Autism Spectrum Disorder (ASD), and Cognitive Disengagement Syndrome (CDS), is marked by substantial symptom overlap that limits the discriminant sensitivity of standard psychometric instruments. While recent work suggests that Large Language Models (LLMs) can simulate human psychometric responses from qualitative data, it remains unclear whether they can accurately and stably model neurodevelopmental traits rather than broad personality characteristics. This study examines whether LLMs can generate psychometric responses that approximate those of real individuals when grounded in a structured qualitative interview, and whether such simulations are sensitive to variations in trait intensity. Twenty-six adults completed a 29-item open-ended interview and four standardized self-report measures (ASRS, BAARS-IV, AQ, RAADS-R). Two LLMs (GPT-4o and Qwen3-235B-A22B) were prompted to infer an individual psychological profile from interview content and then respond to each questionnaire in-role. Accuracy, reliability, and sensitivity were assessed using group-level comparisons, error metrics, exact-match scoring, and a randomized baseline. Both models outperformed random responses across instruments, with GPT-4o showing higher accuracy and reproducibility. Simulated responses closely matched human data for ASRS, BAARS-IV, and RAADS-R, while the AQ revealed subscale-specific limitations, particularly in Attention to Detail. Overall, the findings indicate that interview-grounded LLMs can produce coherent and above-chance simulations of neurodevelopmental traits, supporting their potential use as synthetic participants in early-stage psychometric research, while highlighting clear domain-specific constraints.", "AI": {"tldr": "The study evaluates Large Language Models' (LLMs) ability to simulate psychometric responses for neurodivergent traits through interviews and standardized measures, finding that GPT-4o performs well while domain-specific limitations remain.", "motivation": "The study aims to explore whether LLMs can simulate accurate psychometric responses grounded in qualitative interview data as traditional psychometric tools struggle to differentiate overlapping neurodivergent symptoms.", "method": "Researchers conducted a 29-item open-ended interview and assessed participants using psychometric measures (ASRS, BAARS-IV, AQ, RAADS-R). Two LLMs, GPT-4o and Qwen3-235B-A22B, were prompted to infer participant psychological profiles and simulate questionnaire responses. Analysis included comparisons and sensitivity assessments.", "result": "Both LLMs outperformed random guessing, with GPT-4o achieving higher accuracy and reproducibility. Models simulated human-like responses for ASRS, BAARS-IV, and RAADS-R, but faced challenges with AQ, particularly in the Attention to Detail subscale.", "conclusion": "Interview-grounded LLMs can simulate neurodevelopmental trait profiles to a meaningful extent, potentially aiding early-stage psychometric research, but they currently face domain-specific limitations that need refinement."}}
{"id": "2601.15807", "pdf": "https://arxiv.org/pdf/2601.15807", "abs": "https://arxiv.org/abs/2601.15807", "authors": ["Tobias Boege", "Antony Della Vecchia", "Marina Garrote-L\u00f3pez", "Benjamin Hollering"], "title": "Algebraic Statistics in OSCAR", "categories": ["stat.CO", "cs.NE", "math.AC", "math.ST"], "comment": null, "summary": "We introduce the AlgebraicStatistics section of the OSCAR computer algebra system. We give an overview of its extensible design and highlight its features including serialization of data types for sharing results and creating databases, and state-of-the-art implicitization algorithms.", "AI": {"tldr": "The paper discusses the AlgebraicStatistics section of OSCAR, focusing on its design and features such as data serialization and advanced algorithms.", "motivation": "To provide advanced computational tools for algebraic statistics within the OSCAR system.", "method": "Overview of the design principles, features like data sharing, database creation, and implicitization algorithms.", "result": "Introduction of the extensible AlgebraicStatistics section within the OSCAR system with highlighted features.", "conclusion": "The AlgebraicStatistics section enhances OSCAR's capabilities in statistical algebra for sharing and computation."}}
{"id": "2601.16169", "pdf": "https://arxiv.org/pdf/2601.16169", "abs": "https://arxiv.org/abs/2601.16169", "authors": ["Robert Walkup", "Juha J\u00e4ykk\u00e4", "Igor Pasichnyk", "Zachary Streeter", "Kasia \u015awirydowicz", "Mikko Tukiainen", "Yasuko Eckert", "Luke Bertels", "Daniel Claudino", "Peter Groszkowski", "Travis S. Humble", "Constantinos Evangelinos", "Javier Robledo-Moreno", "William Kirby", "Antonio Mezzacapo", "Antonio C\u00f3rcoles", "Seetharami Seelam"], "title": "Scaling Sample-Based Quantum Diagonalization on GPU-Accelerated Systems using OpenMP Offload", "categories": ["cs.ET", "cs.DC"], "comment": "12 pages", "summary": "Hybrid quantum-HPC algorithms advance research by delegating complex tasks to quantum processors and using HPC systems to orchestrate workflows and complementary computations. Sample-based quantum diagonalization (SQD) is a hybrid quantum-HPC method in which information from a molecular Hamiltonian is encoded into a quantum circuit for evaluation on a quantum computer. A set of measurements on the quantum computer yields electronic configurations that are filtered on the classical computer, which also performs diagonalization on the selected subspace and identifies configurations to be carried over to the next step in an iterative process. Diagonalization is the most demanding task for the classical computer. Previous studies used the Fugaku supercomputer and a highly scalable diagonalization code designed for CPUs. In this work, we describe our efforts to enable efficient scalable and portable diagonalization on heterogeneous systems using GPUs as the main compute engines based on the previous work.\n  GPUs provide massive on-device thread-level parallelism that is well aligned with the algorithms used for diagonalization. We focus on the computation of ground-state energies and wavefunctions using the Davidson algorithm with a selected set of electron configurations. We describe the offload strategy, code transformations, and data-movement, with examples of measurements on the Frontier supercomputer and five other GPU accelerated systems. Our measurements show that GPUs provide an outstanding performance boost of order 100x on a per-node basis. This dramatically expedites the diagonalization step-essential for extracting ground and excited state energies-bringing the classical processing time down from hours to minutes.", "AI": {"tldr": "The paper discusses enhancing hybrid quantum-HPC algorithms with GPU acceleration for diagonalization tasks, achieving significant performance improvements on quantum research workflows.", "motivation": "The motivation is to improve the efficiency and scalability of the diagonalization process within hybrid quantum-HPC algorithms by leveraging GPU computing, which accelerates classical computations in quantum workflows.", "method": "The study uses GPUs as the primary compute engine for diagonalization in the Sample-based Quantum Diagonalization (SQD) method. It focuses on the Davidson algorithm, optimizing computation through offload strategies, code transformations, and data movements.", "result": "Experiments on the Frontier supercomputer and five other GPU-accelerated systems show a 100x performance boost on a per-node basis, significantly reducing classical computation time in diagonalization tasks.", "conclusion": "Leveraging GPUs for diagonalization tasks within hybrid quantum-HPC algorithms enables faster computation of ground-state energies and wavefunctions, making quantum research workflows more efficient and scalable."}}
{"id": "2601.15311", "pdf": "https://arxiv.org/pdf/2601.15311", "abs": "https://arxiv.org/abs/2601.15311", "authors": ["Mustafa Arslan"], "title": "Aeon: High-Performance Neuro-Symbolic Memory Management for Long-Horizon LLM Agents", "categories": ["cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) are fundamentally constrained by the quadratic computational cost of self-attention and the \"Lost in the Middle\" phenomenon, where reasoning capabilities degrade as context windows expand. Existing solutions, primarily \"Flat RAG\" architectures relying on vector databases, treat memory as an unstructured bag of embeddings. This approach fails to capture the hierarchical and temporal structure of long-horizon interactions, leading to \"Vector Haze\", the retrieval of disjointed facts lacking episodic continuity. We propose Aeon, a Neuro-Symbolic Cognitive Operating System that redefines memory not as a static store, but as a managed OS resource. Aeon structures memory into a Memory Palace (a spatial index implemented via Atlas, a SIMD-accelerated Page-Clustered Vector Index that combines small-world graph navigation with B+ Tree-style disk locality to minimize read amplification) and a Trace (a neuro-symbolic episodic graph). We introduce the Semantic Lookaside Buffer (SLB), a predictive caching mechanism that exploits conversational locality to achieve sub-millisecond retrieval latencies. Benchmarks demonstrate that Aeon achieves < 1ms retrieval latency on conversational workloads while ensuring state consistency via a zero-copy C++/Python bridge, effectively enabling persistent, structured memory for autonomous agents.", "AI": {"tldr": "The paper proposes Aeon, a Neuro-Symbolic Cognitive Operating System, to overcome limitations of LLMs' context handling and memory organization, leveraging structured memory and fast retrieval.", "motivation": "Address the limitations of long context management in LLMs, including the high computational cost of self-attention and poor reasoning in expanded contexts, while improving memory structuring to avoid unstructured retrieval known as 'Vector Haze.'", "method": "Aeon implements a managed memory system using a Memory Palace and Trace architecture with Atlas indexing and introduces the Semantic Lookaside Buffer (SLB) to provide fast, structured memory retrieval.", "result": "Aeon achieves retrieval latencies below 1ms in conversational tasks, ensures state consistency through efficient engineering, and effectively sustains structured memory for autonomous agents.", "conclusion": "The approach significantly enhances memory structuring and retrieval for LLMs, providing a robust foundation for handling long-horizon interactions in autonomous systems."}}
{"id": "2601.15493", "pdf": "https://arxiv.org/pdf/2601.15493", "abs": "https://arxiv.org/abs/2601.15493", "authors": ["M M Abid Naziri", "Shinhae Kim", "Feiran Qin", "Marcelo d'Amorim", "Saikat Dutta"], "title": "Testing Deep Learning Libraries via Neurosymbolic Constraint Learning", "categories": ["cs.SE"], "comment": null, "summary": "Deep Learning (DL) libraries (e.g., PyTorch) are popular in AI development. These libraries are complex and contain bugs. Researchers have proposed various bug-finding techniques for such libraries. Yet, there is much room for improvement. A key challenge in testing DL libraries is the lack of API specifications. Prior testing approaches often inaccurately model the input specifications of DL APIs, resulting in missed valid inputs that could reveal bugs or false alarms due to invalid inputs.\n  To address this challenge, we develop Centaur -- the first neurosymbolic technique to test DL library APIs using dynamically learned input constraints. Centaur leverages the key idea that formal API constraints can be learned from a small number of automatically generated seed inputs, and that the learned constraints can be solved using SMT solvers to generate valid and diverse test inputs.\n  We develop a novel grammar that represents first-order logic formulae over API parameters and expresses tensor-related properties (e.g., shape, data types) as well as relational properties between parameters. We use the grammar to guide a Large Language Model (LLM) to enumerate syntactically correct candidate rules, validated using seed inputs. Further, we develop a custom refinement strategy to prune the set of learned rules to eliminate spurious or redundant rules. We use the learned constraints to systematically generate valid and diverse inputs by integrating SMT-based solving with randomized sampling.\n  We evaluate Centaur for testing PyTorch and TensorFlow. Our results show that Centaur's constraints have a recall of 94.0% and a precision of 94.0% on average. In terms of coverage, Centaur covers 203, 150, and 9,608 more branches than TitanFuzz, ACETest and Pathfinder, respectively. Using Centaur, we also detect 26 new bugs in PyTorch and TensorFlow, 18 of which are confirmed.", "AI": {"tldr": "The paper introduces Centaur, a neurosymbolic technique to test deep learning (DL) library APIs, significantly improving bug detection through learned input constraints.", "motivation": "Prior techniques for bug-finding in DL libraries suffer from inaccurately modeled API input specifications, leading to missed valid bugs or false alarms. There is a need for improved methods to address these limitations.", "method": "Centaur develops formal API constraints dynamically from seed inputs. It uses a grammar-guided Large Language Model (LLM) to identify potential rules, validates them with seed inputs, refines the rules to eliminate redundancies, and employs SMT-based solving for input generation.", "result": "Centaur achieves an average recall and precision of 94%, improves branch coverage over previous methods, and detects 26 new bugs in widely used DL libraries, with 18 bugs confirmed.", "conclusion": "Centaur demonstrates its efficacy in testing DL libraries using its learned input constraints, outperforming state-of-the-art methods and uncovering previously undetected bugs."}}
{"id": "2601.15459", "pdf": "https://arxiv.org/pdf/2601.15459", "abs": "https://arxiv.org/abs/2601.15459", "authors": ["Sarvin Ghiasi", "Majid Roshanfar", "Jake Barralet", "Liane S. Feldman", "Amir Hooshiar"], "title": "Neural Collision Detection for Multi-arm Laparoscopy Surgical Robots Through Learning-from-Simulation", "categories": ["cs.RO"], "comment": null, "summary": "This study presents an integrated framework for enhancing the safety and operational efficiency of robotic arms in laparoscopic surgery by addressing key challenges in collision detection and minimum distance estimation. By combining analytical modeling, real-time simulation, and machine learning, the framework offers a robust solution for ensuring safe robotic operations. An analytical model was developed to estimate the minimum distances between robotic arms based on their joint configurations, offering precise theoretical calculations that serve as both a validation tool and a benchmark. To complement this, a 3D simulation environment was created to model two 7-DOF Kinova robotic arms, generating a diverse dataset of configurations for collision detection and distance estimation. Using these insights, a deep neural network model was trained with joint actuators of robot arms and relative positions as inputs, achieving a mean absolute error of 282.2 mm and an R-squared value of 0.85. The close alignment between predicted and actual distances highlights the network's accuracy and its ability to generalize spatial relationships. This work demonstrates the effectiveness of combining analytical precision with machine learning algorithms to enhance the precision and reliability of robotic systems.", "AI": {"tldr": "The paper proposes a framework using analytical modeling, simulation, and machine learning to enhance collision detection and distance estimation in robotic laparoscopic surgery.", "motivation": "To improve the safety and efficiency of robotic arms during laparoscopic surgery by addressing challenges in collision detection and minimum distance estimation.", "method": "Developed an analytical model for theoretical distance calculations, created a 3D simulation environment for generating datasets, and trained a deep neural network for distance estimation using robotic arm configurations.", "result": "The neural network achieved a mean absolute error of 282.2 mm and R-squared value of 0.85, indicating strong generalization and accuracy in predicting spatial relationships.", "conclusion": "Combining analytical modeling, simulation, and machine learning enhances the precision and reliability of robotic systems in surgical applications."}}
{"id": "2601.16070", "pdf": "https://arxiv.org/pdf/2601.16070", "abs": "https://arxiv.org/abs/2601.16070", "authors": ["Jingfu Peng", "Yuhong Yang"], "title": "On damage of interpolation to adversarial robustness in regression", "categories": ["stat.ML", "cs.LG", "math.ST"], "comment": null, "summary": "Deep neural networks (DNNs) typically involve a large number of parameters and are trained to achieve zero or near-zero training error. Despite such interpolation, they often exhibit strong generalization performance on unseen data, a phenomenon that has motivated extensive theoretical investigations. Comforting results show that interpolation indeed may not affect the minimax rate of convergence under the squared error loss. In the mean time, DNNs are well known to be highly vulnerable to adversarial perturbations in future inputs. A natural question then arises: Can interpolation also escape from suboptimal performance under a future $X$-attack? In this paper, we investigate the adversarial robustness of interpolating estimators in a framework of nonparametric regression. A finding is that interpolating estimators must be suboptimal even under a subtle future $X$-attack, and achieving perfect fitting can substantially damage their robustness. An interesting phenomenon in the high interpolation regime, which we term the curse of simple size, is also revealed and discussed. Numerical experiments support our theoretical findings.", "AI": {"tldr": "This paper explores the adversarial robustness of interpolating Deep Neural Network estimators under nonparametric regression, showing that interpolation compromises robustness even under subtle attacks.", "motivation": "Understanding why Deep Neural Networks, despite interpolation and vulnerability to adversarial attacks, often generalize well in unseen scenarios.", "method": "The paper investigates theoretical properties of interpolating estimators in nonparametric regression under adversarial perturbations, with numerical experiments to validate findings.", "result": "Interpolating estimators are proven to be suboptimal under subtle future adversarial attacks, with their robustness significantly compromised by achieving perfect fitting.", "conclusion": "Interpolation damages adversarial robustness in DNNs, revealing the curse of simple size in high interpolation regimes, as supported by numerical experiments."}}
{"id": "2601.15416", "pdf": "https://arxiv.org/pdf/2601.15416", "abs": "https://arxiv.org/abs/2601.15416", "authors": ["Cuong Tran Van", "Trong-Thang Pham", "Ngoc-Son Nguyen", "Duy Minh Ho Nguyen", "Ngan Le"], "title": "DuFal: Dual-Frequency-Aware Learning for High-Fidelity Extremely Sparse-view CBCT Reconstruction", "categories": ["cs.CV", "cs.AI"], "comment": "Published with J2C Certification in Transactions on Machine Learning Research (TMLR)", "summary": "Sparse-view Cone-Beam Computed Tomography reconstruction from limited X-ray projections remains a challenging problem in medical imaging due to the inherent undersampling of fine-grained anatomical details, which correspond to high-frequency components. Conventional CNN-based methods often struggle to recover these fine structures, as they are typically biased toward learning low-frequency information. To address this challenge, this paper presents DuFal (Dual-Frequency-Aware Learning), a novel framework that integrates frequency-domain and spatial-domain processing via a dual-path architecture. The core innovation lies in our High-Local Factorized Fourier Neural Operator, which comprises two complementary branches: a Global High-Frequency Enhanced Fourier Neural Operator that captures global frequency patterns and a Local High-Frequency Enhanced Fourier Neural Operator that processes spatially partitioned patches to preserve spatial locality that might be lost in global frequency analysis. To improve efficiency, we design a Spectral-Channel Factorization scheme that reduces the Fourier Neural Operator parameter count. We also design a Cross-Attention Frequency Fusion module to integrate spatial and frequency features effectively. The fused features are then decoded through a Feature Decoder to produce projection representations, which are subsequently processed through an Intensity Field Decoding pipeline to reconstruct a final Computed Tomography volume. Experimental results on the LUNA16 and ToothFairy datasets demonstrate that DuFal significantly outperforms existing state-of-the-art methods in preserving high-frequency anatomical features, particularly under extremely sparse-view settings.", "AI": {"tldr": "The paper introduces DuFal, a novel framework for reconstructing sparse-view Cone-Beam CT images, utilizing a dual-path frequency-domain and spatial-domain learning to better preserve high-frequency details.", "motivation": "Sparse-view CT reconstruction often fails to recover fine anatomical details due to high-frequency undersampling, and conventional CNN methods focus too heavily on low-frequency components.", "method": "The proposed DuFal framework uses a dual-path architecture combining global and local frequency-enhanced Fourier Neural Operators. It includes spectral-channel factorization for efficiency and cross-attention frequency fusion for feature integration.", "result": "DuFal achieves superior performance, especially in preserving high-frequency details in sparse-view CT settings, as validated by experiments on the LUNA16 and ToothFairy datasets.", "conclusion": "DuFal demonstrates significant advancements over state-of-the-art methods in reconstructing sparse-view CT images, bridging spatial and frequency information effectively to retain anatomical accuracy."}}
{"id": "2601.15380", "pdf": "https://arxiv.org/pdf/2601.15380", "abs": "https://arxiv.org/abs/2601.15380", "authors": ["Elon Litman", "Gabe Guo"], "title": "You Need Better Attention Priors", "categories": ["cs.LG", "cs.CL", "stat.ML"], "comment": null, "summary": "We generalize the attention mechanism by viewing it through the lens of Entropic Optimal Transport, revealing that standard attention corresponds to a transport problem regularized by an implicit uniform prior. We introduce Generalized Optimal transport Attention with Trainable priors (GOAT), a new attention mechanism that replaces this naive assumption with a learnable, continuous prior. This prior maintains full compatibility with optimized kernels such as FlashAttention. GOAT also provides an EOT-based explanation of attention sinks and materializes a solution for them, avoiding the representational trade-offs of standard attention. Finally, by absorbing spatial information into the core attention computation, GOAT learns an extrapolatable prior that combines the flexibility of learned positional embeddings with the length generalization of fixed encodings.", "AI": {"tldr": "GOAT introduces a generalized attention mechanism by integrating Entropic Optimal Transport and trainable priors, improving flexibility and addressing issues like attention sinks.", "motivation": "Standard attention mechanisms rely on implicit uniform priors, limiting their adaptability and can cause issues like attention sinks.", "method": "GOAT incorporates learnable, continuous priors based on Entropic Optimal Transport, maintaining compatibility with optimized kernels such as FlashAttention.", "result": "The method demonstrates enhanced flexibility in positional encoding and addresses representational trade-offs without compromising efficiency.", "conclusion": "GOAT offers an improved attention mechanism with extrapolatable priors, resolving issues in standard attention and advancing its generalizability and performance."}}
{"id": "2601.15299", "pdf": "https://arxiv.org/pdf/2601.15299", "abs": "https://arxiv.org/abs/2601.15299", "authors": ["Yash Sharma"], "title": "MALTopic: Multi-Agent LLM Topic Modeling Framework", "categories": ["cs.CL", "cs.IR", "cs.MA"], "comment": "6 pages. Published in 2025 IEEE World AI-IoT Congress. \\c{opyright} 2025 IEEE. Project code and data available at: https://github.com/yash91sharma/MALTopic", "summary": "Topic modeling is a crucial technique for extracting latent themes from unstructured text data, particularly valuable in analyzing survey responses. However, traditional methods often only consider free-text responses and do not natively incorporate structured or categorical survey responses for topic modeling. And they produce abstract topics, requiring extensive human interpretation. To address these limitations, we propose the Multi-Agent LLM Topic Modeling Framework (MALTopic). This framework decomposes topic modeling into specialized tasks executed by individual LLM agents: an enrichment agent leverages structured data to enhance textual responses, a topic modeling agent extracts latent themes, and a deduplication agent refines the results. Comparative analysis on a survey dataset demonstrates that MALTopic significantly improves topic coherence, diversity, and interpretability compared to LDA and BERTopic. By integrating structured data and employing a multi-agent approach, MALTopic generates human-readable topics with enhanced contextual relevance, offering a more effective solution for analyzing complex survey data.", "AI": {"tldr": "The paper introduces the Multi-Agent LLM Topic Modeling Framework (MALTopic) to address limitations in traditional topic modeling methods for survey data, integrating structured data and enhancing interpretability.", "motivation": "To overcome limitations of traditional topic modeling techniques, such as poor integration of structured data, abstract topics, and heavy interpretational burden.", "method": "The framework uses a multi-agent approach with a set of specialized agents: (1) an enrichment agent for integrating structured data, (2) topic modeling agent for extracting themes, and (3) a deduplication agent to improve results.", "result": "MALTopic improves topic coherence, diversity, and interpretability significantly compared to traditional models like LDA and BERTopic, based on a comparative analysis on survey datasets.", "conclusion": "MALTopic offers a more holistic and effective way to extract human-readable and contextually relevant topics from complex survey data by leveraging structured data and a multi-agent strategy."}}
{"id": "2601.15320", "pdf": "https://arxiv.org/pdf/2601.15320", "abs": "https://arxiv.org/abs/2601.15320", "authors": ["Takao Inou\u00e9"], "title": "On Brain as a Mathematical Manifold: Neural Manifolds, Sheaf Semantics, and Leibnizian Harmony", "categories": ["q-bio.NC"], "comment": "17 pages with 2 figures; a sheaf-theoretic model of neural integration and pathology, with an application to the interpretation of Leibnizian harmony", "summary": "We present a mathematical and philosophical framework in which brain function is modeled using sheaf theory over neural state spaces. Local neural or cognitive functions are represented as sections of a sheaf, while global coherence corresponds to the existence of global sections. Brain pathologies are interpreted as obstructions to such global integration and are classified using tools from sheaf cohomology. The framework builds on the neural manifold program in contemporary neuroscience and on standard results in sheaf theory, and is further interpreted through a Leibnizian lens \\cite{Churchland2012, Leibniz1714, MacLaneMoerdijk, Perich2025}. This paper is intended as a conceptual and formal proposal rather than a complete empirical theory.", "AI": {"tldr": "The paper introduces a mathematical and philosophical framework using sheaf theory to model brain functions, with global coherence analyzed via global sections and pathologies as obstructions in the system.", "motivation": "The study aims to connect brain functions, cognitive coherence, and pathologies through a structured mathematical framework to bridge conceptual gaps in neuroscience and philosophy.", "method": "Sheaf theory is applied to neural state spaces, representing local functions as sections and pathologies as obstructions, leveraging neural manifold techniques and sheaf cohomology.", "result": "The paper provides a structured conceptual and formal groundwork for understanding neural coherence and related pathologies using advanced mathematical tools.", "conclusion": "A novel framework using sheaf theory is proposed for modeling and analyzing brain function, with potential implications in understanding neural coherence and pathologies."}}
{"id": "2601.15316", "pdf": "https://arxiv.org/pdf/2601.15316", "abs": "https://arxiv.org/abs/2601.15316", "authors": ["Wei Ai", "Yilong Tan", "Yuntao Shou", "Tao Meng", "Haowen Chen", "Zhixiong He", "Keqin Li"], "title": "The Paradigm Shift: A Comprehensive Survey on Large Vision Language Models for Multimodal Fake News Detection", "categories": ["cs.AI", "cs.CV"], "comment": null, "summary": "In recent years, the rapid evolution of large vision-language models (LVLMs) has driven a paradigm shift in multimodal fake news detection (MFND), transforming it from traditional feature-engineering approaches to unified, end-to-end multimodal reasoning frameworks. Early methods primarily relied on shallow fusion techniques to capture correlations between text and images, but they struggled with high-level semantic understanding and complex cross-modal interactions. The emergence of LVLMs has fundamentally changed this landscape by enabling joint modeling of vision and language with powerful representation learning, thereby enhancing the ability to detect misinformation that leverages both textual narratives and visual content. Despite these advances, the field lacks a systematic survey that traces this transition and consolidates recent developments. To address this gap, this paper provides a comprehensive review of MFND through the lens of LVLMs. We first present a historical perspective, mapping the evolution from conventional multimodal detection pipelines to foundation model-driven paradigms. Next, we establish a structured taxonomy covering model architectures, datasets, and performance benchmarks. Furthermore, we analyze the remaining technical challenges, including interpretability, temporal reasoning, and domain generalization. Finally, we outline future research directions to guide the next stage of this paradigm shift. To the best of our knowledge, this is the first comprehensive survey to systematically document and analyze the transformative role of LVLMs in combating multimodal fake news. The summary of existing methods mentioned is in our Github: \\href{https://github.com/Tan-YiLong/Overview-of-Fake-News-Detection}{https://github.com/Tan-YiLong/Overview-of-Fake-News-Detection}.", "AI": {"tldr": "The paper reviews the evolution of multimodal fake news detection (MFND), focusing on the transformative role of large vision-language models (LVLMs).", "motivation": "The rapid advancements in LVLMs have revolutionized MFND by enabling more effective joint modeling of vision and language, but there is no systematic survey documenting this transition.", "method": "The authors provide a historical overview, propose a taxonomy, analyze challenges, and discuss future research directions in MFND through LVLMs.", "result": "This paper consolidates recent developments, highlights technical hurdles, and maps out key areas for future investigation in multimodal fake news detection.", "conclusion": "The study emphasizes the significant transformation of MFND through LVLMs while serving as the first systematic survey in this domain, aiming to guide future research."}}
{"id": "2601.15687", "pdf": "https://arxiv.org/pdf/2601.15687", "abs": "https://arxiv.org/abs/2601.15687", "authors": ["Khusrav Badalov", "Young Yoon"], "title": "FARM: Field-Aware Resolution Model for Intelligent Trigger-Action Automation", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Trigger-Action Programming (TAP) platforms such as IFTTT and Zapier enable Web of Things (WoT) automation by composing event-driven rules across heterogeneous services. A TAP applet links a trigger to an action and must bind trigger outputs (ingredients) to action inputs (fields) to be executable. Prior work largely treats TAP as service-level prediction from natural language, which often yields non-executable applets that still require manual configuration. We study the function-level configuration problem: generating complete applets with correct ingredient-to-field bindings. We propose FARM (Field-Aware Resolution Model), a two-stage architecture for automated applet generation with full configuration. Stage 1 trains contrastive dual encoders with selective layer freezing over schema-enriched representations, retrieving candidates from 1,724 trigger functions and 1,287 action functions (2.2M possible trigger-action pairs). Stage 2 performs selection and configuration using an LLM-based multi-agent pipeline. It includes intent analysis, trigger selection, action selection via cross-schema scoring, and configuration verification. Agents coordinate through shared state and agreement-based selection. FARM achieves 81% joint accuracy on Gold (62% Noisy, 70% One-shot) at the function level, where both trigger and action functions must match the ground truth. For comparison with service-level baselines, we map functions to their parent services and evaluate at the service level. FARM reaches 81% joint accuracy and improves over TARGE by 23 percentage points. FARM also generates ingredient-to-field bindings, producing executable automation configurations.", "AI": {"tldr": "A novel architecture for automated applet generation called FARM improves joint accuracy in accurately generating function-level and executable configurations for TAP platforms.", "motivation": "The paper aims to address the inefficiency in current TAP platforms where applet generation from natural language often produces non-executable configurations, requiring manual intervention.", "method": "FARM, a two-stage architecture comprising contrastive dual encoders for candidate retrieval and an LLM-based multi-agent system for selection and configuration to achieve fully functional applet generation.", "result": "FARM demonstrates significant improvement in accuracy, achieving 81% joint accuracy for function-level applet generation and outperforms previous baselines by 23 percentage points at the service level.", "conclusion": "The proposed FARM architecture shows it can reliably automate the generation of complete and executable applets, marking an advancement in TAP platform automation."}}
{"id": "2601.15486", "pdf": "https://arxiv.org/pdf/2601.15486", "abs": "https://arxiv.org/abs/2601.15486", "authors": ["Javier N. Ramos-Silva", "Peter J. Burke"], "title": "A Universal Large Language Model -- Drone Command and Control Interface", "categories": ["cs.RO"], "comment": null, "summary": "The use of artificial intelligence (AI) for drone control can have a transformative impact on drone capabilities, especially when real world information can be integrated with drone sensing, command, and control, part of a growing field of physical AI. Large language models (LLMs) can be advantageous if trained at scale on general knowledge, but especially and in particular when the training data includes information such as detailed map geography topology of the entire planet, as well as the ability to access real time situational data such as weather. However, challenges remain in the interface between drones and LLMs in general, with each application requiring a tedious, labor intensive effort to connect the LLM trained knowledge to drone command and control. Here, we solve that problem, using an interface strategy that is LLM agnostic and drone agnostic, providing the first universal, versatile, comprehensive and easy to use drone control interface. We do this using the new model context protocol (MCP) standard, an open standard that provides a universal way for AI systems to access external data, tools, and services. We develop and deploy a cloud based Linux machine hosting an MCP server that supports the Mavlink protocol, an ubiquitous drone control language used almost universally by millions of drones including Ardupilot and PX4 framework.We demonstrate flight control of a real unmanned aerial vehicle. In further testing, we demonstrate extensive flight planning and control capability in a simulated drone, integrated with a Google Maps MCP server for up to date, real time navigation information. This demonstrates a universal approach to integration of LLMs with drone command and control, a paradigm that leverages and exploits virtually all of modern AI industry with drone technology in an easy to use interface that translates natural language to drone control.", "AI": {"tldr": "This paper proposes a universal and versatile interface for connecting large language models (LLMs) with drones for enhanced control and planning using the Model Context Protocol (MCP) standard.", "motivation": "Current methods for integrating LLMs with drones are labor-intensive, application-specific, and lack a universal approach.", "method": "The paper introduces the Model Context Protocol (MCP) standard, utilizes an MCP server supporting Mavlink for drone control, and tests the system with real and simulated drones integrated with tools like Google Maps.", "result": "The researchers successfully demonstrate real UAV flight control and simulated drone operations using real-time navigation data.", "conclusion": "The proposed solution provides an advanced, universal interface for LLM-driven drone command and control, simplifying integration and expanding capabilities significantly."}}
{"id": "2601.16120", "pdf": "https://arxiv.org/pdf/2601.16120", "abs": "https://arxiv.org/abs/2601.16120", "authors": ["Zhengchi Ma", "Anru R. Zhang"], "title": "Synthetic Augmentation in Imbalanced Learning: When It Helps, When It Hurts, and How Much to Add", "categories": ["stat.ML", "cs.LG", "stat.ME"], "comment": null, "summary": "Imbalanced classification, where one class is observed far less frequently than the other, often causes standard training procedures to prioritize the majority class and perform poorly on rare but important cases. A classic and widely used remedy is to augment the minority class with synthetic examples, but two basic questions remain under-resolved: when does synthetic augmentation actually help, and how many synthetic samples should be generated?\n  We develop a unified statistical framework for synthetic augmentation in imbalanced learning, studying models trained on imbalanced data augmented with synthetic minority samples and evaluated under the balanced population risk. Our theory shows that synthetic data is not always beneficial. In a ``local symmetry\" regime, imbalance is not the dominant source of error near the balanced optimum, so adding synthetic samples cannot improve learning rates and can even degrade performance by amplifying generator mismatch. When augmentation can help (a ``local asymmetry\" regime), the optimal synthetic size depends on generator accuracy and on whether the generator's residual mismatch is directionally aligned with the intrinsic majority-minority shift. This structure can make the best synthetic size deviate from naive full balancing, sometimes by a small refinement and sometimes substantially when generator bias is systematic. Practically, we recommend Validation-Tuned Synthetic Size (VTSS): select the synthetic size by minimizing balanced validation loss over a range centered near the fully balanced baseline, while allowing meaningful departures when the data indicate them. Simulations and a real sepsis prediction study support the theory and illustrate when synthetic augmentation helps, when it cannot, and how to tune its quantity effectively.", "AI": {"tldr": "This paper addresses the benefits and limitations of synthetic data augmentation in imbalanced classification, offering theoretical insight into when it helps, when it doesn't, and how to determine the optimal synthetic size.", "motivation": "To address the problem of biased performance caused by imbalanced datasets, particularly for rare but crucial cases, and to investigate the effectiveness and limitations of synthetic minority class augmentation.", "method": "Theoretical analysis of synthetic augmentation in imbalanced learning using balanced population risk as evaluation metric. Proposed Validation-Tuned Synthetic Size (VTSS) method, validated with simulations and a real sepsis prediction study.", "result": "Synthetic data is not universally beneficial. Its effectiveness depends on factors like data imbalance, generator accuracy, and bias alignment. Practical guidance (VTSS) is provided for selecting synthetic size based on validation loss.", "conclusion": "Synthetic augmentation can improve learning in specific scenarios but may harm performance in others. Careful tuning (such as with VTSS) is required to optimize its effectiveness, avoiding naive full balancing assumptions."}}
{"id": "2601.15453", "pdf": "https://arxiv.org/pdf/2601.15453", "abs": "https://arxiv.org/abs/2601.15453", "authors": ["Morteza Poudineh", "Marc Lalonde"], "title": "DevPrompt: Deviation-Based Prompt Learning for One-Normal ShotImage Anomaly Detection", "categories": ["cs.CV"], "comment": "8 pages", "summary": "Few-normal shot anomaly detection (FNSAD) aims to detect abnormal regions in images using only a few normal training samples, making the task highly challenging due to limited supervision and the diversity of potential defects. Recent approaches leverage vision-language models such as CLIP with prompt-based learning to align image and text features. However, existing methods often exhibit weak discriminability between normal and abnormal prompts and lack principled scoring mechanisms for patch-level anomalies. We propose a deviation-guided prompt learning framework that integrates the semantic power of vision-language models with the statistical reliability of deviation-based scoring. Specifically, we replace fixed prompt prefixes with learnable context vectors shared across normal and abnormal prompts, while anomaly-specific suffix tokens enable class-aware alignment. To enhance separability, we introduce a deviation loss with Top-K Multiple Instance Learning (MIL), modeling patch-level features as Gaussian deviations from the normal distribution. This allows the network to assign higher anomaly scores to patches with statistically significant deviations, improving localization and interpretability. Experiments on the MVTecAD and VISA benchmarks demonstrate superior pixel-level detection performance compared to PromptAD and other baselines. Ablation studies further validate the effectiveness of learnable prompts, deviation-based scoring, and the Top-K MIL strategy.", "AI": {"tldr": "The paper introduces a deviation-guided prompt learning framework that enhances few-shot anomaly detection using the semantic power of vision-language models and statistical reliability for patch scoring.", "motivation": "FNSAD faces challenges due to limited supervision and potential defect variety, necessitating improvement in prompt discriminability and patch-level anomaly scoring.", "method": "The framework employs learnable context vectors for prompts, anomaly-specific suffix tokens for alignment, and a deviation loss with Top-K MIL to model patch-level features as Gaussian deviations.", "result": "Experiments on MVTecAD and VISA benchmarks show better pixel-level anomaly detection compared to prior methods, with ablation studies confirming the effectiveness of the approach.", "conclusion": "The proposed framework offers superior anomaly detection performance and interpretability, leveraging learnable prompts and deviation scoring strategies."}}
{"id": "2601.15390", "pdf": "https://arxiv.org/pdf/2601.15390", "abs": "https://arxiv.org/abs/2601.15390", "authors": ["Zhaolong Su", "Leheng Zhao", "Xiaoying Wu", "Ziyue Xu", "Jindong Wang"], "title": "FedUMM: A General Framework for Federated Learning with Unified Multimodal Models", "categories": ["cs.LG"], "comment": null, "summary": "Unified multimodal models (UMMs) are emerging as strong foundation models that can do both generation and understanding tasks in a single architecture. However, they are typically trained in centralized settings where all training and downstream datasets are gathered in a central server, limiting the deployment in privacy-sensitive and geographically distributed scenarios. In this paper, we present FedUMM, a general federated learning framework for UMMs under non-IID multimodal data with low communication cost. Built on NVIDIA FLARE, FedUMM instantiates federation for a BLIP3o backbone via parameter-efficient fine-tuning: clients train lightweight LoRA adapters while freezing the foundation models, and the server aggregates only adapter updates. We evaluate on VQA v2 and the GenEval compositional generation benchmarks under Dirichlet-controlled heterogeneity with up to 16 clients. Results show slight degradation as client count and heterogeneity increase, while remaining competitive with centralized training. We further analyze computation--communication trade-offs and demonstrate that adapter-only federation reduces per-round communication by over an order of magnitude compared to full fine-tuning, enabling practical federated UMM training. This work provides empirical experience for future research on privacy-preserving federated unified multimodal models.", "AI": {"tldr": "This paper introduces FedUMM, a federated learning framework addressing non-IID multimodal data for unified multimodal models (UMMs) with low communication cost, showing practical training without centralized data.", "motivation": "To enable the deployment of unified multimodal models (UMMs) in privacy-sensitive and geographically distributed settings, addressing limitations of centralized training methods.", "method": "The study introduces a methodology using parameter-efficient fine-tuning (via LoRA adapters) within a federated learning framework built on NVIDIA FLARE. Clients train these adapters while the server aggregates updates to avoid transferring full model weights.", "result": "FedUMM was tested on the VQA v2 and GenEval benchmarks using up to 16 clients. Results showed only slight degradation in performance with increasing client count and non-IID heterogeneity, while achieving significant communication cost savings.", "conclusion": "FedUMM facilitates federated training of UMMs with reduced communication costs and competitive performance, marking a practical step towards privacy-preserving, distributed multimodal learning."}}
{"id": "2601.15300", "pdf": "https://arxiv.org/pdf/2601.15300", "abs": "https://arxiv.org/abs/2601.15300", "authors": ["Weiwei Wang", "Jiyong Min", "Weijie Zou"], "title": "Intelligence Degradation in Long-Context LLMs: Critical Threshold Determination via Natural Length Distribution Analysis", "categories": ["cs.CL"], "comment": "29 pages", "summary": "Large Language Models (LLMs) exhibit catastrophic performance degradation when processing contexts approaching certain critical thresholds, even when information remains relevant. This intelligence degradation-defined as over 30% drop in task performance-severely limits long-context applications. This degradation shows a common pattern: models maintain strong performance up to a critical threshold, then collapse catastrophically. We term this shallow long-context adaptation-models adapt for short to medium contexts but fail beyond critical thresholds. This paper presents three contributions: (1) Natural Length Distribution Analysis: We use each sample's natural token length without truncation or padding, providing stronger causal evidence that degradation results from context length itself. (2) Critical Threshold Determination: Through experiments on a mixed dataset (1,000 samples covering 5%-95% of context length), we identify the critical threshold for Qwen2.5-7B at 40-50% of maximum context length, where F1 scores drop from 0.55-0.56 to 0.3 (45.5% degradation), using five-method cross-validation. (3) Unified Framework: We consolidate shallow adaptation, explaining degradation patterns and providing a foundation for mitigation strategies. This work provides the first systematic characterization of intelligence degradation in open-source Qwen models, offering practical guidance for deploying LLMs in long-context scenarios.", "AI": {"tldr": "LLMs, particularly Qwen models, show catastrophic performance drops when processing long contexts beyond certain thresholds. The paper introduces analysis and frameworks to understand and potentially mitigate this degradation.", "motivation": "The motivation is to address performance limitations in LLMs, especially in long-context processing, which are critical for deploying these models in complex, real-world tasks.", "method": "The study analyzes natural length distribution without truncation, determines critical thresholds through experiments across mixed dataset samples, and consolidates findings into a unified framework explaining degradation patterns.", "result": "Qwen2.5-7B model experiences a performance drop of 45.5% beyond 40-50% of the maximum context length. F1 scores decline from 0.55-0.56 to 0.3, demonstrating this intelligence degradation.", "conclusion": "The study systematically identifies and characterizes intelligence degradation in Qwen models in long-context scenarios, providing actionable insights and strategies for improving LLM viability for extended contexts."}}
{"id": "2601.15321", "pdf": "https://arxiv.org/pdf/2601.15321", "abs": "https://arxiv.org/abs/2601.15321", "authors": ["Sayan Saha"], "title": "Analysis of the Ventriloquism Aftereffect Using Network Theory Techniques", "categories": ["q-bio.NC", "cs.HC"], "comment": null, "summary": "Ventriloquism After-Effect is the phenomenon where sustained exposure to the ventriloquist illusion causes a change in unisensory auditory localization towards the location where the visual stimulus was present. We investigate the recalibration in EEG networks that causes this change and the track the timeline of changes in the auditory processing pathway. Our results obtained using network analysis, non-stationary time series analysis and multivariate pattern classification show that recalibration takes place early in the auditory processing pathway and the after-effect decays with time after exposure to the illusion.", "AI": {"tldr": "The paper investigates how sustained auditory-visual illusions cause auditory recalibration, using EEG analysis to track early-stage neural changes and their decay over time.", "motivation": "To understand the neural mechanisms underlying recalibration of auditory localization due to sustained exposure to the ventriloquist illusion.", "method": "The study employs EEG network analysis, non-stationary time series analysis, and multivariate pattern classification to analyze changes in the auditory processing pathway.", "result": "Findings reveal that recalibration occurs early in the auditory pathway and that the after-effect diminishes over time after exposure to the illusion.", "conclusion": "The ventriloquism after-effect reflects early auditory processing changes in response to audiovisual recalibration, but these changes are temporary and decay post-exposure."}}
{"id": "2601.15322", "pdf": "https://arxiv.org/pdf/2601.15322", "abs": "https://arxiv.org/abs/2601.15322", "authors": ["Raffi Khatchadourian"], "title": "Replayable Financial Agents: A Determinism-Faithfulness Assurance Harness for Tool-Using LLM Agents", "categories": ["cs.AI", "cs.CL"], "comment": "23 pages, 5 figures, 9 tables. Code and data: https://github.com/ibm-client-engineering/output-drift-financial-llms", "summary": "LLM agents struggle with regulatory audit replay: when asked to reproduce a flagged transaction decision with identical inputs, most deployments fail to return consistent results. This paper introduces the Determinism-Faithfulness Assurance Harness (DFAH), a framework for measuring trajectory determinism and evidence-conditioned faithfulness in tool-using agents deployed in financial services.\n  Across 74 configurations (12 models, 4 providers, 8-24 runs each at T=0.0) in non-agentic baseline experiments, 7-20B parameter models achieved 100% determinism, while 120B+ models required 3.7x larger validation samples to achieve equivalent statistical reliability. Agentic tool-use introduces additional variance (see Tables 4-7). Contrary to the assumed reliability-capability trade-off, a positive Pearson correlation emerged (r = 0.45, p < 0.01, n = 51 at T=0.0) between determinism and faithfulness; models producing consistent outputs also tended to be more evidence-aligned.\n  Three financial benchmarks are provided (compliance triage, portfolio constraints, DataOps exceptions; 50 cases each) along with an open-source stress-test harness. In these benchmarks and under DFAH evaluation settings, Tier 1 models with schema-first architectures achieved determinism levels consistent with audit replay requirements.", "AI": {"tldr": "This paper addresses the issue of LLM agents failing to reproduce consistent transaction decisions under audit replay settings. A novel framework called DFAH is introduced, revealing a correlation between output consistency (determinism) and evidence alignment (faithfulness).", "motivation": "The motivation arises from LLM agents' inability to consistently reproduce transaction decisions for regulatory audits in financial services, posing risks in compliance and transparency.", "method": "The paper proposes the DFAH framework to measure trajectory determinism and evidence-conditioned faithfulness of tool-using LLM agents in financial benchmarks. Various experiments and models were evaluated to assess reliability and consistency.", "result": "Models with 7-20B parameters achieved full determinism, while 120B+ models required more validation samples for equivalent reliability. DFAH identified variance in agentic tool-use but showed a positive correlation between determinism and evidence alignment.", "conclusion": "For audit replay requirements, DFAH proves effective in evaluating determinism and faithfulness in models, with Tier 1 schema-first architectures performing optimally under this framework."}}
{"id": "2601.15879", "pdf": "https://arxiv.org/pdf/2601.15879", "abs": "https://arxiv.org/abs/2601.15879", "authors": ["Jiajun Zhang", "Zeyu Cui", "Lei Zhang", "Jian Yang", "Jiaxi Yang", "Qiang Liu", "Zilei Wang", "Binyuan Hui", "Liang Wang", "Junyang Lin"], "title": "Evaluating and Achieving Controllable Code Completion in Code LLM", "categories": ["cs.SE", "cs.CL"], "comment": null, "summary": "Code completion has become a central task, gaining significant attention with the rise of large language model (LLM)-based tools in software engineering. Although recent advances have greatly improved LLMs' code completion abilities, evaluation methods have not advanced equally. Most current benchmarks focus solely on functional correctness of code completions based on given context, overlooking models' ability to follow user instructions during completion-a common scenario in LLM-assisted programming. To address this limitation, we present the first instruction-guided code completion benchmark, Controllable Code Completion Benchmark (C3-Bench), comprising 2,195 carefully designed completion tasks. Through comprehensive evaluation of over 40 mainstream LLMs across C3-Bench and conventional benchmarks, we reveal substantial gaps in instruction-following capabilities between open-source and advanced proprietary models during code completion tasks. Moreover, we develop a straightforward data synthesis pipeline that leverages Qwen2.5-Coder to generate high-quality instruction-completion pairs for supervised fine-tuning (SFT). The resulting model, Qwen2.5-Coder-C3, achieves state-of-the-art performance on C3-Bench. Our findings provide valuable insights for enhancing LLMs' code completion and instruction-following capabilities, establishing new directions for future research in code LLMs. To facilitate reproducibility and foster further research in code LLMs, we open-source all code, datasets, and models.", "AI": {"tldr": "The paper introduces C3-Bench, the first benchmark for instruction-guided code completion with 2,195 tasks, evaluating over 40 language models. It highlights gaps in current LLMs' instruction-following abilities and develops a strong model using a data synthesis pipeline.", "motivation": "Current code completion evaluation focuses mainly on functional correctness and ignores instruction-following abilities, which are crucial in user-guided programming.", "method": "The authors designed C3-Bench tasks and evaluated mainstream LLMs, implemented a data synthesis pipeline to generate instructional pairs, and fine-tuned Qwen2.5-Coder to create Qwen2.5-Coder-C3.", "result": "Qwen2.5-Coder-C3 achieved state-of-the-art performance on C3-Bench, showcasing improved instruction-following capabilities for code completions.", "conclusion": "The paper establishes C3-Bench as a valuable benchmark and provides insights and tools for improving instruction-following abilities in code LLMs, forming a basis for future advancements."}}
{"id": "2601.15541", "pdf": "https://arxiv.org/pdf/2601.15541", "abs": "https://arxiv.org/abs/2601.15541", "authors": ["Heng Zhang", "Wei-Hsing Huang", "Qiyi Tong", "Gokhan Solak", "Puze Liu", "Sheng Liu", "Jan Peters", "Arash Ajoudani"], "title": "CompliantVLA-adaptor: VLM-Guided Variable Impedance Action for Safe Contact-Rich Manipulation", "categories": ["cs.RO"], "comment": "under review", "summary": "We propose a CompliantVLA-adaptor that augments the state-of-the-art Vision-Language-Action (VLA) models with vision-language model (VLM)-informed context-aware variable impedance control (VIC) to improve the safety and effectiveness of contact-rich robotic manipulation tasks. Existing VLA systems (e.g., RDT, Pi0, OpenVLA-oft) typically output position, but lack force-aware adaptation, leading to unsafe or failed interactions in physical tasks involving contact, compliance, or uncertainty. In the proposed CompliantVLA-adaptor, a VLM interprets task context from images and natural language to adapt the stiffness and damping parameters of a VIC controller. These parameters are further regulated using real-time force/torque feedback to ensure interaction forces remain within safe thresholds. We demonstrate that our method outperforms the VLA baselines on a suite of complex contact-rich tasks, both in simulation and on real hardware, with improved success rates and reduced force violations. The overall success rate across all tasks increases from 9.86\\% to 17.29\\%, presenting a promising path towards safe contact-rich manipulation using VLAs. We release our code, prompts, and force-torque-impedance-scenario context datasets at https://sites.google.com/view/compliantvla.", "AI": {"tldr": "The paper introduces CompliantVLA-adaptor, enhancing Vision-Language-Action (VLA) models with context-aware variable impedance control for safer and more effective robotic manipulation tasks.", "motivation": "Existing Vision-Language-Action systems lack the ability to adapt forces in contact-rich scenarios, often leading to unsafe or unsuccessful physical interactions.", "method": "The proposed system integrates vision-language models to adaptively set stiffness and damping in a variable impedance controller. It uses real-time force/torque feedback to maintain safe force thresholds during robotic interactions.", "result": "CompliantVLA-adaptor outperformed baseline VLA systems in complex contact-rich tasks, significantly improving success rates from 9.86% to 17.29%, with fewer force violations.", "conclusion": "This method offers a viable solution for improving safety and success rates in contact-based robotic manipulation, demonstrated on real hardware and simulators, with publicly available resources for reproducibility."}}
{"id": "2601.16174", "pdf": "https://arxiv.org/pdf/2601.16174", "abs": "https://arxiv.org/abs/2601.16174", "authors": ["Yiyao Yang"], "title": "Beyond Predictive Uncertainty: Reliable Representation Learning with Structural Constraints", "categories": ["stat.ML", "cs.LG"], "comment": "22 pages, 5 figures, 5 propositions", "summary": "Uncertainty estimation in machine learning has traditionally focused on the prediction stage, aiming to quantify confidence in model outputs while treating learned representations as deterministic and reliable by default. In this work, we challenge this implicit assumption and argue that reliability should be regarded as a first-class property of learned representations themselves. We propose a principled framework for reliable representation learning that explicitly models representation-level uncertainty and leverages structural constraints as inductive biases to regularize the space of feasible representations. Our approach introduces uncertainty-aware regularization directly in the representation space, encouraging representations that are not only predictive but also stable, well-calibrated, and robust to noise and structural perturbations. Structural constraints, such as sparsity, relational structure, or feature-group dependencies, are incorporated to define meaningful geometry and reduce spurious variability in learned representations, without assuming fully correct or noise-free structure. Importantly, the proposed framework is independent of specific model architectures and can be integrated with a wide range of representation learning methods.", "AI": {"tldr": "This paper focuses on introducing uncertainty estimation at the representation learning stage, proposing a framework that incorporates structural constraints for stable and robust representation learning.", "motivation": "Current machine learning approaches treat learned representations as deterministic, neglecting the quantification of uncertainty at the representation level, which can affect reliability.", "method": "The paper proposes a framework that models representation-level uncertainty, uses structural constraints as inductive biases, and introduces uncertainty-aware regularization directly in the representation space.", "result": "The framework produces representations that are stable, well-calibrated, robust to noise, and capable of reducing spurious variability in learned representations.", "conclusion": "The proposed framework enhances reliability in representation learning by combining uncertainty quantification and structural constraints, applicable across diverse methods and model architectures."}}
{"id": "2601.15475", "pdf": "https://arxiv.org/pdf/2601.15475", "abs": "https://arxiv.org/abs/2601.15475", "authors": ["Yunshan Qi", "Lin Zhu", "Nan Bao", "Yifan Zhao", "Jia Li"], "title": "Seeing through Light and Darkness: Sensor-Physics Grounded Deblurring HDR NeRF from Single-Exposure Images and Events", "categories": ["cs.CV"], "comment": null, "summary": "Novel view synthesis from low dynamic range (LDR) blurry images, which are common in the wild, struggles to recover high dynamic range (HDR) and sharp 3D representations in extreme lighting conditions. Although existing methods employ event data to address this issue, they ignore the sensor-physics mismatches between the camera output and physical world radiance, resulting in suboptimal HDR and deblurring results. To cope with this problem, we propose a unified sensor-physics grounded NeRF framework for sharp HDR novel view synthesis from single-exposure blurry LDR images and corresponding events. We employ NeRF to directly represent the actual radiance of the 3D scene in the HDR domain and model raw HDR scene rays hitting the sensor pixels as in the physical world. A pixel-wise RGB mapping field is introduced to align the above rendered pixel values with the sensor-recorded LDR pixel values of the input images. A novel event mapping field is also designed to bridge the physical scene dynamics and actual event sensor output. The two mapping fields are jointly optimized with the NeRF network, leveraging the spatial and temporal dynamic information in events to enhance the sharp HDR 3D representation learning. Experiments on the collected and public datasets demonstrate that our method can achieve state-of-the-art deblurring HDR novel view synthesis results with single-exposure blurry LDR images and corresponding events.", "AI": {"tldr": "The paper introduces a novel NeRF framework for generating sharp and HDR 3D representations from single-exposure blurry LDR images and events, achieving state-of-the-art results.", "motivation": "Current view synthesis methods struggle with low-quality LDR input images under extreme lighting and suffer mismatches in connecting sensor data with real-world radiance, resulting in suboptimal HDR and deblurring.", "method": "The proposed framework incorporates NeRF to model HDR radiance directly, with a pixel-wise RGB mapping to align rendered and sensor-recorded values, and an event mapping field to connect scene dynamics with event sensor outputs.", "result": "Experiments on both collected and public datasets show state-of-the-art performance in deblurring and HDR novel view synthesis, demonstrating the effectiveness of the proposed method.", "conclusion": "The novel sensor-physics grounded NeRF framework effectively addresses blurry LDR and event challenges, offering improved HDR and sharpness for 3D scene representations."}}
{"id": "2601.15399", "pdf": "https://arxiv.org/pdf/2601.15399", "abs": "https://arxiv.org/abs/2601.15399", "authors": ["Ashna Nawar Ahmed", "Banooqa Banday", "Terry Jones", "Tanzima Z. Islam"], "title": "Attention-Informed Surrogates for Navigating Power-Performance Trade-offs in HPC", "categories": ["cs.LG"], "comment": "13 pages, 6 figures Published in MLForSys workshop in NeurIPS 2025 Link: https://openreview.net/forum?id=R0Vc9lnDd5", "summary": "High-Performance Computing (HPC) schedulers must balance user performance with facility-wide resource constraints. The task boils down to selecting the optimal number of nodes for a given job. We present a surrogate-assisted multi-objective Bayesian optimization (MOBO) framework to automate this complex decision. Our core hypothesis is that surrogate models informed by attention-based embeddings of job telemetry can capture performance dynamics more effectively than standard regression techniques. We pair this with an intelligent sample acquisition strategy to ensure the approach is data-efficient. On two production HPC datasets, our embedding-informed method consistently identified higher-quality Pareto fronts of runtime-power trade-offs compared to baselines. Furthermore, our intelligent data sampling strategy drastically reduced training costs while improving the stability of the results. To our knowledge, this is the first work to successfully apply embedding-informed surrogates in a MOBO framework to the HPC scheduling problem, jointly optimizing for performance and power on production workloads.", "AI": {"tldr": "This paper proposes a surrogate-assisted multi-objective Bayesian optimization (MOBO) framework for efficient HPC job scheduling by optimizing performance and power trade-offs using attention-based embeddings for job telemetry.", "motivation": "Current HPC schedulers struggle to balance user performance needs with facility-wide resource constraints effectively. A new approach is needed for optimizing resource allocation while considering complex runtime and power trade-offs.", "method": "The authors propose using surrogate models informed by attention-based embeddings of job telemetry. This MOBO framework incorporates an intelligent data sampling strategy to ensure efficiency and effectiveness in capturing performance dynamics.", "result": "On two production HPC datasets, the proposed method consistently identified higher-quality Pareto fronts compared to baseline models. Additionally, the embedding-informed strategy reduced training costs and improved result stability.", "conclusion": "This work demonstrates the potential of embedding-informed surrogates in MOBO frameworks for optimizing performance and power in HPC scheduling, offering a novel and data-efficient approach for production workloads."}}
{"id": "2601.15301", "pdf": "https://arxiv.org/pdf/2601.15301", "abs": "https://arxiv.org/abs/2601.15301", "authors": ["Jivnesh Sandhan", "Harshit Jaiswal", "Fei Cheng", "Yugo Murawaki"], "title": "Can We Trust LLM Detectors?", "categories": ["cs.CL", "cs.AI"], "comment": "NLP2026, Utsunomiya, Japan", "summary": "The rapid adoption of LLMs has increased the need for reliable AI text detection, yet existing detectors often fail outside controlled benchmarks. We systematically evaluate 2 dominant paradigms (training-free and supervised) and show that both are brittle under distribution shift, unseen generators, and simple stylistic perturbations. To address these limitations, we propose a supervised contrastive learning (SCL) framework that learns discriminative style embeddings. Experiments show that while supervised detectors excel in-domain, they degrade sharply out-of-domain, and training-free methods remain highly sensitive to proxy choice. Overall, our results expose fundamental challenges in building domain-agnostic detectors. Our code is available at: https://github.com/HARSHITJAIS14/DetectAI", "AI": {"tldr": "Current AI text detection methods struggle with reliability under distribution shifts, unseen generators, and simple stylistic tweaks.", "motivation": "The paper focuses on improving AI text detection methods due to issues in reliability and performance of existing detection techniques.", "method": "Supervised contrastive learning (SCL) framework is introduced to develop discriminative style embeddings for more robust detection.", "result": "Supervised detectors work well in familiar domains but fail under distribution shifts, while training-free methods depend heavily on specific proxies.", "conclusion": "Building domain-agnostic AI text detection methods is fundamentally challenging; existing paradigms need significant improvement."}}
{"id": "2601.15336", "pdf": "https://arxiv.org/pdf/2601.15336", "abs": "https://arxiv.org/abs/2601.15336", "authors": ["Daniel Brownell"], "title": "Learning Discrete Successor Transitions in Continuous Attractor Networks: Emergence, Limits, and Topological Constraints", "categories": ["q-bio.NC", "cs.AI"], "comment": "An open-source reference implementation is available at https://github.com/javadan/can-paper", "summary": "Continuous attractor networks (CANs) are a well-established class of models for representing low-dimensional continuous variables such as head direction, spatial position, and phase. In canonical spatial domains, transitions along the attractor manifold are driven by continuous displacement signals, such as angular velocity-provided by sensorimotor systems external to the CAN itself. When such signals are not explicitly provided as dedicated displacement inputs, it remains unclear whether attractor-based circuits can reliably acquire recurrent dynamics that support stable state transitions, or whether alternative predictive strategies dominate.\n  In this work, we present an experimental framework for training CANs to perform successor-like transitions between stable attractor states in the absence of externally provided displacement signals. We compare two recurrent topologies, a circular ring and a folded snake manifold, and systematically vary the temporal regime under which stability is evaluated. We find that, under short evaluation windows, networks consistently converge to impulse-driven associative solutions that achieve high apparent accuracy yet lack persistent attractor dynamics. Only when stability is explicitly enforced over extended free-run periods do genuine attractor-based transition dynamics emerge. This suggests that shortcut solutions are the default outcome of local learning in recurrent networks, while attractor dynamics represent a constrained regime rather than a generic result.\n  Furthermore, we demonstrate that topology strictly limits the capacity for learned transitions. While the continuous ring topology achieves perfect stability over long horizons, the folded snake topology hits a geometric limit characterized by failure at manifold discontinuities, which neither curriculum learning nor basal ganglia-inspired gating can fully overcome.", "AI": {"tldr": "The authors explore how continuous attractor networks (CANs) can achieve stable transitions in the absence of external signals, comparing network topologies and revealing topology limitations.", "motivation": "To investigate whether attractor-based circuits can acquire stable recurrent dynamics for state transitions without external displacement signals.", "method": "The authors propose an experimental framework, comparing circular ring and folded snake manifold topologies under varied temporal stability conditions.", "result": "Impulse-driven solutions dominate under short-term stability evaluations, while persistent attractor dynamics are achieved with extended stability enforcement. Topology limits capacity for transitions, with folded snake topology hitting geometric constraints.", "conclusion": "Stable attractor state transitions are not always generic outcomes; topology and stability regime play key roles. Shortcut solutions prevail without extended constraints, and topology limits transition capacity."}}
{"id": "2601.15324", "pdf": "https://arxiv.org/pdf/2601.15324", "abs": "https://arxiv.org/abs/2601.15324", "authors": ["Mark Wind"], "title": "Prometheus Mind: Retrofitting Memory to Frozen Language Models", "categories": ["cs.AI"], "comment": "28 pages", "summary": "Adding memory to pretrained language models typically requires architectural changes or weight modification. We present Prometheus Mind, which retrofits memory to a frozen Qwen3-4B using 11 modular adapters (530MB, 7% overhead) -- fully reversible by removing the adapters. Building this system required solving four problems: (1) Extraction -- we develop Contrastive Direction Discovery (CDD), which finds semantic directions via minimal pairs without labeled data. (2) Training -- end-to-end optimization collapses; stage-wise training of each adapter on simple proxy tasks succeeds. (3) Injection -- learned encoders fail to generalize; we find that lm_head.weight rows already provide the mapping we need, requiring no training. (4) Hidden state collapse -- transformers make ``wife'' and ``brother'' 0.98+ similar; we train projections to recover distinction (0.98 $\\rightarrow$ 0.09). On PrometheusExtract-132 (132 cases), the system achieves 94.4% retrieval on clean inputs (n=54, 95% CI: [84.9%, 98.1%]), degrading to 19.4% on informal inputs with ellipsis, filler words, or implicit subjects (n=36). The primary bottleneck is relation classification (47.3% accuracy), responsible for most extraction errors.", "AI": {"tldr": "Prometheus Mind integrates memory into frozen Qwen3-4B via modular adapters without modifying its architecture or weights. Challenges include semantic direction discovery, effective training stages, leveraging existing mappings, and preventing hidden state collapse.", "motivation": "To add memory capabilities to pretrained language models without altering their architecture, weights, or permanent modifications.", "method": "The method involves attaching 11 modular adapters to a frozen language model using techniques like Contrastive Direction Discovery, stage-wise training, and leveraging lm_head.weight mappings for injection while addressing hidden state collapse issues.", "result": "Prometheus Mind achieved 94.4% retrieval accuracy on clean inputs but experienced significant degradation on informal inputs. The main bottleneck observed was low relation classification accuracy (47.3%), impacting extraction performance.", "conclusion": "The system demonstrates that memory can be retrofitted into frozen models effectively using modular, non-intrusive adapters. However, challenges like informal input degradation and relation classification limitations need further refinement for practical use."}}
{"id": "2601.16009", "pdf": "https://arxiv.org/pdf/2601.16009", "abs": "https://arxiv.org/abs/2601.16009", "authors": ["Giovanna Broccia", "Sira Vegas", "Alessio Ferrari"], "title": "The Role of Cognitive Abilities in Requirements Inspection: Comparing UML and Textual Representations", "categories": ["cs.SE"], "comment": null, "summary": "The representation of requirements plays a critical role in the accuracy of requirements inspection. While visual representations, such as UML diagrams, are widely used alongside text-based requirements, their effectiveness in supporting inspection is still debated. Cognitive abilities, such as working memory and mental rotation skills, may also influence inspection accuracy. This study aims to evaluate whether the use of UML sequence diagrams alongside text-based requirements improves the accuracy of requirements inspection compared to text-based requirements alone and to explore whether cognitive abilities are associated with differences in performance across the two treatments (text vs text with UML support). We conducted a crossover experiment with 38 participants to assess the accuracy of requirements inspection under the two treatments in terms of issues found and justifications provided. Linear mixed-effects and generalized linear models were used to analyse the effects of treatment, period, sequence, and cognitive abilities. The results indicate a significant three-way interaction between representation type, working memory capacity, and mental rotation ability. This finding suggests that the effectiveness of UML support is not uniform across individuals: participants with high scores in both cognitive abilities experienced reduced performance when using UML for violation detection. Conversely, the same cognitive profile was associated with improved justification accuracy under UML-aided inspection, indicating that higher cognitive abilities may support deeper reasoning processes when dealing with multi-modal information, i.e., diagrams and text.", "AI": {"tldr": "This paper investigates how UML sequence diagrams paired with text-based requirements impact requirements inspection accuracy, considering cognitive abilities like working memory and mental rotation skills.", "motivation": "The study aims to address the ongoing debate on the effectiveness of visual representations, such as UML diagrams, when combined with text-based requirements, and whether cognitive abilities influence inspection accuracy.", "method": "A crossover experiment with 38 participants, analyzing requirements inspection accuracy under two treatments (text-only vs text with UML diagrams). Various models were employed to assess the effects of representation type and cognitive abilities.", "result": "Findings revealed a three-way interaction between representation type, working memory, and mental rotation ability. Participants with high cognitive scores showed mixed results: reduced violation detection performance with UML but enhanced justification accuracy.", "conclusion": "The efficacy of UML diagrams depends on individual cognitive profiles, suggesting that while UML can aid deeper reasoning, it may hamper certain inspection tasks for individuals with specific cognitive strengths."}}
{"id": "2601.15545", "pdf": "https://arxiv.org/pdf/2601.15545", "abs": "https://arxiv.org/abs/2601.15545", "authors": ["Zhifan Yan", "Chang Liu", "Yiyang Jiang", "Wenxuan Zheng", "Xinhao Chen", "Axel Krieger"], "title": "A Mobile Magnetic Manipulation Platform for Gastrointestinal Navigation with Deep Reinforcement Learning Control", "categories": ["cs.RO"], "comment": null, "summary": "Targeted drug delivery in the gastrointestinal (GI) tract using magnetic robots offers a promising alternative to systemic treatments. However, controlling these robots is a major challenge. Stationary magnetic systems have a limited workspace, while mobile systems (e.g., coils on a robotic arm) suffer from a \"model-calibration bottleneck\", requiring complex, pre-calibrated physical models that are time-consuming to create and computationally expensive. This paper presents a compact, low-cost mobile magnetic manipulation platform that overcomes this limitation using Deep Reinforcement Learning (DRL). Our system features a compact four-electromagnet array mounted on a UR5 collaborative robot. A Soft Actor-Critic (SAC)-based control strategy is trained through a sim-to-real pipeline, enabling effective policy deployment within 15 minutes and significantly reducing setup time. We validated the platform by controlling a 7-mm magnetic capsule along 2D trajectories. Our DRL-based controller achieved a root-mean-square error (RMSE) of 1.18~mm for a square path and 1.50~mm for a circular path. We also demonstrated successful tracking over a clinically relevant, 30 cm * 20 cm workspace. This work demonstrates a rapidly deployable, model-free control framework capable of precise magnetic manipulation in a large workspace,validated using a 2D GI phantom.", "AI": {"tldr": "The paper introduces a cost-effective mobile magnetic manipulation system for targeted gastrointestinal drug delivery, leveraging Deep Reinforcement Learning (DRL).", "motivation": "Existing magnetic systems face challenges of limited workspace in stationary setups and a cumbersome calibration process in mobile systems. A simplified yet effective control strategy is needed.", "method": "A four-electromagnet array mounted on a UR5 collaborative robotic arm was used with a Soft Actor-Critic (SAC)-based control strategy trained via sim-to-real pipeline.", "result": "The platform successfully controlled a magnetic capsule in 2D trajectories with an RMSE of 1.18 mm on square paths and 1.50 mm on circular paths, demonstrating tracking in a clinically relevant large workspace.", "conclusion": "The system provides a model-free, quickly deployable, and precise framework for magnetic manipulation in the GI tract, validated on a 2D phantom."}}
{"id": "2601.15353", "pdf": "https://arxiv.org/pdf/2601.15353", "abs": "https://arxiv.org/abs/2601.15353", "authors": ["Asim H. Gazi", "Yongyi Guo", "Daiqi Gao", "Ziping Xu", "Kelly W. Zhang", "Susan A. Murphy"], "title": "Statistical Reinforcement Learning in the Real World: A Survey of Challenges and Future Directions", "categories": ["stat.AP", "cs.LG", "stat.ML"], "comment": null, "summary": "Reinforcement learning (RL) has achieved remarkable success in real-world decision-making across diverse domains, including gaming, robotics, online advertising, public health, and natural language processing. Despite these advances, a substantial gap remains between RL research and its deployment in many practical settings. Two recurring challenges often underlie this gap. First, many settings offer limited opportunity for the agent to interact extensively with the target environment due to practical constraints. Second, many target environments often undergo substantial changes, requiring redesign and redeployment of RL systems (e.g., advancements in science and technology that change the landscape of healthcare delivery). Addressing these challenges and bridging the gap between basic research and application requires theory and methodology that directly inform the design, implementation, and continual improvement of RL systems in real-world settings.\n  In this paper, we frame the application of RL in practice as a three-component process: (i) online learning and optimization during deployment, (ii) post- or between-deployment offline analyses, and (iii) repeated cycles of deployment and redeployment to continually improve the RL system. We provide a narrative review of recent advances in statistical RL that address these components, including methods for maximizing data utility for between-deployment inference, enhancing sample efficiency for online learning within-deployment, and designing sequences of deployments for continual improvement. We also outline future research directions in statistical RL that are use-inspired -- aiming for impactful application of RL in practice.", "AI": {"tldr": "The paper addresses real-world challenges in deploying reinforcement learning (RL), proposing a framework and reviewing statistical RL advances for bridging the gap between research and application.", "motivation": "The authors aim to address limitations in RL deployment in practical scenarios such as restricted interaction opportunities and the need for system redesign due to environmental changes.", "method": "The paper conceptualizes RL deployment as a three-component process involving online learning, offline analyses, and iterative system improvements. Recent statistical RL methods are reviewed to optimize these components.", "result": "It highlights advancements in statistical RL methods that enhance data utility, sample efficiency, and deployment strategies for real-world application.", "conclusion": "The proposed framework and insights pave the way for impactful RL use in practice, emphasizing continual system improvement and future research directions."}}
{"id": "2601.15490", "pdf": "https://arxiv.org/pdf/2601.15490", "abs": "https://arxiv.org/abs/2601.15490", "authors": ["Jobeal Solomon", "Ali Mohammed Mansoor Alsahag", "Seyed Sahand Mohammadi Ziabari"], "title": "Hybrid Vision Transformer_GAN Attribute Neutralizer for Mitigating Bias in Chest X_Ray Diagnosis", "categories": ["cs.CV"], "comment": null, "summary": "Bias in chest X-ray classifiers frequently stems from sex- and age-related shortcuts, leading to systematic underdiagnosis of minority subgroups. Previous pixel-space attribute neutralizers, which rely on convolutional encoders, lessen but do not fully remove this attribute leakage at clinically usable edit strengths. This study evaluates whether substituting the U-Net convolutional encoder with a Vision Transformer backbone in the Attribute-Neutral Framework can reduce demographic attribute leakage while preserving diagnostic accuracy. A data-efficient Image Transformer Small (DeiT-S) neutralizer was trained on the ChestX-ray14 dataset. Its edited images, generated across eleven edit-intensity levels, were evaluated with an independent AI judge for attribute leakage and with a convolutional neural network (ConvNet) for disease prediction. At a moderate edit level (alpha = 0.5), the Vision Transformer (ViT) neutralizer reduces patient sex-recognition area under the curve (AUC) to approximately 0.80, about 10 percentage points below the original framework's convolutional U-Net encoder, despite being trained for only half as many epochs. Meanwhile, macro receiver operating characteristic area under the curve (ROC AUC) across 15 findings stays within five percentage points of the unedited baseline, and the worst-case subgroup AUC remains near 0.70. These results indicate that global self-attention vision models can further suppress attribute leakage without sacrificing clinical utility, suggesting a practical route toward fairer chest X-ray AI.", "AI": {"tldr": "The study explores whether Vision Transformer (ViT) models can reduce bias in chest X-ray classifiers, highlighting better demographic attribute leakage suppression compared to convolutional encoders.", "motivation": "The paper aims to address bias in automated chest X-ray classifiers caused by demographic shortcuts, which lead to systematic underdiagnosis in minority subgroups.", "method": "A Vision Transformer (DeiT-S model) within the Attribute-Neutral Framework was trained to neutralize demographic biases in chest X-ray data, and its performance was evaluated using independent models for attribute leakage and disease predictions.", "result": "The Vision Transformer reduced patient sex-recognition AUC further than convolutional U-Net encoders, with minimal impact on diagnostic accuracy for medical findings.", "conclusion": "Vision Transformer's global self-attention mechanism offers a promising approach to mitigating demographic biases in chest X-ray AI while preserving diagnostic performance."}}
{"id": "2601.15417", "pdf": "https://arxiv.org/pdf/2601.15417", "abs": "https://arxiv.org/abs/2601.15417", "authors": ["Adri\u00e1n Rodr\u00edguez-Mu\u00f1oz", "William Daspit", "Adam Klivans", "Antonio Torralba", "Constantinos Daskalakis", "Giannis Daras"], "title": "Ambient Dataloops: Generative Models for Dataset Refinement", "categories": ["cs.LG", "cs.AI"], "comment": "27 pages, 9 figures, 11 tables", "summary": "We propose Ambient Dataloops, an iterative framework for refining datasets that makes it easier for diffusion models to learn the underlying data distribution. Modern datasets contain samples of highly varying quality, and training directly on such heterogeneous data often yields suboptimal models. We propose a dataset-model co-evolution process; at each iteration of our method, the dataset becomes progressively higher quality, and the model improves accordingly. To avoid destructive self-consuming loops, at each generation, we treat the synthetically improved samples as noisy, but at a slightly lower noisy level than the previous iteration, and we use Ambient Diffusion techniques for learning under corruption. Empirically, Ambient Dataloops achieve state-of-the-art performance in unconditional and text-conditional image generation and de novo protein design. We further provide a theoretical justification for the proposed framework that captures the benefits of the data looping procedure.", "AI": {"tldr": "The paper introduces Ambient Dataloops, an iterative process for refining datasets to train diffusion models more effectively.", "motivation": "Current datasets often contain samples with varying quality, leading to suboptimal models when trained on heterogeneous data.", "method": "The framework iteratively refines datasets using a co-evolution process with models, treating improved samples as noisy with progressively lower levels and leveraging Ambient Diffusion techniques.", "result": "Ambient Dataloops achieved state-of-the-art performance in image generation, text-conditioned image synthesis, and protein design.", "conclusion": "The proposed method improves dataset quality and model performance recursively, supported by empirical results and theoretical validation."}}
{"id": "2601.15330", "pdf": "https://arxiv.org/pdf/2601.15330", "abs": "https://arxiv.org/abs/2601.15330", "authors": ["Zhebo Wang", "Xiaohu Mu", "Zijie Zhou", "Mohan Li", "Wenpeng Xing", "Dezhang Kong", "Meng Han"], "title": "ICPO: Illocution-Calibrated Policy Optimization for Multi-Turn Conversation", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ICASSP 2026", "summary": "Large Language Models (LLMs) in multi-turn conversations often suffer from a ``lost-in-conversation'' phenomenon, where they struggle to recover from early incorrect assumptions, particularly when users provide ambiguous initial instructions. We find that standard post-training techniques like Reinforcement Learning with Verifiable Rewards (RLVR) exacerbate this issue by rewarding confident, direct answers, thereby inducing overconfidence and discouraging the model from seeking clarification. To address this, we propose Illocution-Calibrated Policy Optimization (ICPO), a novel training framework that sensitizes the model to instruction ambiguity. ICPO augments the training corpus with underspecified prompts and conditions the reward signal on the user's illocutionary intent, rewarding the model for expressing uncertainty or asking for clarification when faced with ambiguity. Experiments demonstrate that ICPO fosters appropriate humility, yielding a substantial average improvement of 75\\% in multi-turn conversation, while preserving robust performance on single-turn benchmarks. Our work presents a practical path toward more robust and collaborative conversational AI that can better navigate the nuances of human interaction.", "AI": {"tldr": "Large Language Models (LLMs) often fail in multi-turn conversations due to misinterpretations, exacerbated by standard reinforcement learning approaches. ICPO is proposed as a solution to instill awareness of ambiguities.", "motivation": "Address issues in multi-turn conversations of LLMs, particularly their difficulty in overcoming incorrect assumptions induced by ambiguous initial instructions.", "method": "Proposes Illocution-Calibrated Policy Optimization (ICPO), a framework that trains models to recognize ambiguity by incorporating underspecified prompts and rewarding uncertainty or clarification queries.", "result": "ICPO improves multi-turn conversation performance by an average of 75% while maintaining single-turn benchmark performance.", "conclusion": "ICPO enhances conversational AI, enabling it to better handle ambiguous instructions and fostering more collaborative interaction."}}
{"id": "2601.15344", "pdf": "https://arxiv.org/pdf/2601.15344", "abs": "https://arxiv.org/abs/2601.15344", "authors": ["Cole Korponay"], "title": "A Dual-Head Transformer-State-Space Architecture for Neurocircuit Mechanism Decomposition from fMRI", "categories": ["q-bio.NC"], "comment": null, "summary": "Precision psychiatry aspires to elucidate brain-based biomarkers of psychopathology to bolster disease risk assessment and treatment development. To this end, functional magnetic resonance imaging (fMRI) has helped triangulate brain circuits whose functional features are correlated with or even predictive of forms of psychopathology. Yet, fMRI biomarkers to date remain largely descriptive identifiers of where, rather than how, neurobiology is aberrant, limiting their utility for guiding treatment. We present a method for decomposing fMRI-based functional connectivity (FC) into constituent biomechanisms - output drive, input responsivity, modulator gating - with clearer alignment to differentiable therapeutic interventions. Neurocircuit mechanism decomposition (NMD) integrates (i) a graph-constrained, lag-aware transformer to estimate directed, pathway-specific routing distributions and drive signals, with (ii) a measurement-aware state-space model (SSM) that models hemodynamic convolution and recovers intrinsic latent dynamics. This dual-head architecture yields interpretable circuit parameters that may provide a more direct bridge from fMRI to treatment strategy selection. We instantiate the model in an anatomically and electrophysiologically well-defined circuit: the cortico-basal ganglia-thalamo-cortical loop.", "AI": {"tldr": "The paper discusses a novel method to enhance fMRI precision in identifying neurobiological dysfunctions for improved psychiatric treatment.", "motivation": "To create actionable brain-based biomarkers using fMRI for psychiatric treatment and risk assessment, surpassing current descriptive approaches.", "method": "A dual-head architecture called Neurocircuit Mechanism Decomposition (NMD) is proposed. This integrates a lag-aware transformer for directed signal routing and a state-space model (SSM) for modeling brain dynamics.", "result": "The method provides interpretable circuit parameters with potential for guiding therapeutic strategies.", "conclusion": "This approach represents a more practical bridge from fMRI biomarkers to personalized interventions, exemplified in cortico-basal ganglia-thalamo-cortical circuit analysis."}}
{"id": "2601.15347", "pdf": "https://arxiv.org/pdf/2601.15347", "abs": "https://arxiv.org/abs/2601.15347", "authors": ["Chuanqing Wang", "Zhenmin Zhao", "Shanshan Du", "Chaoqun Fei", "Songmao Zhang", "Ruqian Lu"], "title": "Logic Programming on Knowledge Graph Networks And its Application in Medical Domain", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "33 pages", "summary": "The rash development of knowledge graph research has brought big driving force to its application in many areas, including the medicine and healthcare domain. However, we have found that the application of some major information processing techniques on knowledge graph still lags behind. This defect includes the failure to make sufficient use of advanced logic reasoning, advanced artificial intelligence techniques, special-purpose programming languages, modern probabilistic and statistic theories et al. on knowledge graphs development and application. In particular, the multiple knowledge graphs cooperation and competition techniques have not got enough attention from researchers. This paper develops a systematic theory, technique and application of the concept 'knowledge graph network' and its application in medical and healthcare domain. Our research covers its definition, development, reasoning, computing and application under different conditions such as unsharp, uncertain, multi-modal, vectorized, distributed, federated. Almost in each case we provide (real data) examples and experiment results. Finally, a conclusion of innovation is provided.", "AI": {"tldr": "The paper explores gaps in knowledge graph research, specifically its application in healthcare, and introduces the concept of 'knowledge graph network' with systematic analysis, experiments, and innovation.", "motivation": "The motivation arises from the insufficient application of advanced techniques like logic reasoning, AI, and probabilistic methods in knowledge graph development. Additionally, inadequate focus has been given to cooperation and competition among multiple knowledge graphs, especially in the medical domain.", "method": "The authors propose a systematic theory and techniques for 'knowledge graph networks,' addressing variations like uncertainty, multi-modality, and distributed designs. They validate their work with examples and experimental results.", "result": "The study provides definitions, methods, and experiments under diverse conditions, showcasing the applicability and improvements of the proposed approaches in real-world scenarios.", "conclusion": "The paper concludes with innovative contributions to knowledge graph applications, particularly in the medical and healthcare fields, addressing multiple challenges with validated solutions."}}
{"id": "2601.16080", "pdf": "https://arxiv.org/pdf/2601.16080", "abs": "https://arxiv.org/abs/2601.16080", "authors": ["Oleksandr Kosenkov", "Ehsan Zabardast", "Jannik Fischbach", "Tony Gorschek", "Daniel Mendez"], "title": "Towards a Goal-Centric Assessment of Requirements Engineering Methods for Privacy by Design", "categories": ["cs.SE", "cs.CY"], "comment": "The paper has been accepted for the 32nd International Working Conference on Requirements Engineering: Foundation for Software Quality (REFSQ 2026)", "summary": "Implementing privacy by design (PbD) according to the General Data Protection Regulation (GDPR) is met with a growing number of requirements engineering (RE) approaches. However, the question of which RE method for PbD fits best the goals of organisations remains a challenge. We report our endeavor to close this gap by synthesizing a goal-centric approach for PbD methods assessment. We used literature review, interviews, and validation with practitioners to achieve the goal of our study. As practitioners do not approach PbD systematically, we suggest that RE methods for PbD should be assessed against organisational goals, rather than process characteristics only. We hope that, when further developed, the goal-centric approach could support the development, selection, and tailoring of RE practices for PbD.", "AI": {"tldr": "The paper introduces a goal-centric approach to assess privacy by design (PbD) methods in requirements engineering (RE), aiming to align them with organizational goals.", "motivation": "The paper seeks to address the challenge of identifying the most suitable requirements engineering methods for privacy by design, ensuring alignment with organizational goals under GDPR requirements.", "method": "The authors synthesized their approach using a combination of literature review, interviews, and validation with practitioners.", "result": "The study highlights the importance of assessing PbD methods against organizational goals rather than just process characteristics, noting that practitioners often lack systematic approaches to PbD.", "conclusion": "The proposed goal-centric approach has potential to aid in the development, selection, and adaptation of RE practices for PbD, contributing to a more goal-aligned privacy design process."}}
{"id": "2601.15607", "pdf": "https://arxiv.org/pdf/2601.15607", "abs": "https://arxiv.org/abs/2601.15607", "authors": ["Lenworth Thomas", "Tjaden Bridges", "Sarah Bergbreiter"], "title": "Airflow Source Seeking on Small Quadrotors Using a Single Flow Sensor", "categories": ["cs.RO"], "comment": null, "summary": "As environmental disasters happen more frequently and severely, seeking the source of pollutants or harmful particulates using plume tracking becomes even more important. Plume tracking on small quadrotors would allow these systems to operate around humans and fly in more confined spaces, but can be challenging due to poor sensitivity and long response times from gas sensors that fit on small quadrotors. In this work, we present an approach to complement chemical plume tracking with airflow source-seeking behavior using a custom flow sensor that can sense both airflow magnitude and direction on small quadrotors < 100 g. We use this sensor to implement a modified version of the `Cast and Surge' algorithm that takes advantage of flow direction sensing to find and navigate towards flow sources. A series of characterization experiments verified that the system can detect airflow while in flight and reorient the quadrotor toward the airflow. Several trials with random starting locations and orientations were used to show that our source-seeking algorithm can reliably find a flow source. This work aims to provide a foundation for future platforms that can use flow sensors in concert with other sensors to enable richer plume tracking data collection and source-seeking.", "AI": {"tldr": "This paper introduces a method for detecting and navigating airflow sources on small quadrotors (<100 g) using a custom flow sensor.", "motivation": "Increased environmental disasters require reliable methods to identify pollutant sources. Small quadrotors offer safer, more adaptable solutions in confined spaces, but existing gas sensors for these systems lack sensitivity and speed.", "method": "The study developed a custom flow sensor for small quadrotors to detect airflow magnitude and direction. This sensor was integrated into a modified 'Cast and Surge' algorithm to navigate towards airflow sources.", "result": "Experiments demonstrated the quadrotor's ability to detect airflow while flying, reorient towards the airflow, and effectively locate source positions through multiple trials.", "conclusion": "This work lays the groundwork for future plume tracking systems combining flow sensors with advanced sensing technologies to enable comprehensive pollutant source-searching capabilities."}}
{"id": "2601.15507", "pdf": "https://arxiv.org/pdf/2601.15507", "abs": "https://arxiv.org/abs/2601.15507", "authors": ["Jinrui Yang", "Qing Liu", "Yijun Li", "Mengwei Ren", "Letian Zhang", "Zhe Lin", "Cihang Xie", "Yuyin Zhou"], "title": "Controllable Layered Image Generation for Real-World Editing", "categories": ["cs.CV"], "comment": null, "summary": "Recent image generation models have shown impressive progress, yet they often struggle to yield controllable and consistent results when users attempt to edit specific elements within an existing image. Layered representations enable flexible, user-driven content creation, but existing approaches often fail to produce layers with coherent compositing relationships, and their object layers typically lack realistic visual effects such as shadows and reflections. To overcome these limitations, we propose LASAGNA, a novel, unified framework that generates an image jointly with its composing layers--a photorealistic background and a high-quality transparent foreground with compelling visual effects. Unlike prior work, LASAGNA efficiently learns correct image composition from a wide range of conditioning inputs--text prompts, foreground, background, and location masks--offering greater controllability for real-world applications. To enable this, we introduce LASAGNA-48K, a new dataset composed of clean backgrounds and RGBA foregrounds with physically grounded visual effects. We also propose LASAGNABENCH, the first benchmark for layer editing. We demonstrate that LASAGNA excels in generating highly consistent and coherent results across multiple image layers simultaneously, enabling diverse post-editing applications that accurately preserve identity and visual effects. LASAGNA-48K and LASAGNABENCH will be publicly released to foster open research in the community. The project page is https://rayjryang.github.io/LASAGNA-Page/.", "AI": {"tldr": "LASAGNA is a new framework for generating layered images with realistic visual effects to enable controllable and consistent editing.", "motivation": "Existing image generation models struggle with controllable and consistent editing of specific elements within images, particularly layers, realistic shadows, and reflections.", "method": "The LASAGNA framework simultaneously generates photorealistic backgrounds and transparent foregrounds using inputs like text prompts, location masks, and introduces LASAGNA-48K dataset and LASAGNABENCH benchmark.", "result": "LASAGNA achieves coherent, high-quality layered image generation and editing, surpassing previous approaches in preserving visual identity and effects.", "conclusion": "LASAGNA enables flexible, user-driven post-editing of layered images with realistic effects, promoting open research via publicly available datasets and benchmarks."}}
{"id": "2601.15423", "pdf": "https://arxiv.org/pdf/2601.15423", "abs": "https://arxiv.org/abs/2601.15423", "authors": ["Lorian Bannis"], "title": "Lattice: A Confidence-Gated Hybrid System for Uncertainty-Aware Sequential Prediction with Behavioral Archetypes", "categories": ["cs.LG"], "comment": "12 pages, 1 figure, uses tikz.sty", "summary": "We introduce Lattice, a hybrid sequential prediction system that conditionally activates learned behavioral structure using binary confidence gating. The system clusters behavior windows into behavioral archetypes and uses binary confidence gating to activate archetype-based scoring only when confidence exceeds a threshold, falling back to baseline predictions when uncertain. We validate Lattice on recommendation systems (MovieLens), scientific time-series (LIGO), and financial markets, using LSTM and transformer backbones. On MovieLens with LSTM, Lattice achieves +31.9% improvement over LSTM baseline in HR@10 (p < 3.29 x 10^-25, 30 seeds), outperforming transformer baselines by 109.4% over SASRec and 218.6% over BERT4Rec. On LIGO and financial data, the system correctly refuses archetype activation when distribution shift occurs - a successful outcome demonstrating confidence gating prevents false activation. On transformer backbones, Lattice provides 0.0% improvement (neutral, no degradation), gracefully deferring when structure is already present. This bidirectional validation - activating when patterns apply, refusing when they don't, and deferring when redundant - supports confidence gating as a promising architectural principle for managing epistemic uncertainty in safety-critical applications.", "AI": {"tldr": "Lattice is a hybrid sequential prediction system using confidence gating to activate or defer behavior modeling, validated across multiple datasets offering improvements and safety-critical applicability.", "motivation": "The paper introduces a system to address challenges in managing uncertainty in predictions, particularly for diverse applications requiring safety-critical decisions.", "method": "The method combines behavioral clustering with confidence gating, activating or deferring archetype-based scoring depending on observed confidence, and validating results using LSTM and transformer backbones.", "result": "Lattice shows significant improvement in recommendation systems and correctly avoids activation during distribution shifts in scientific and financial time series data.", "conclusion": "Confidence gating in the Lattice system is effective for managing uncertainty, proving useful for safety-critical applications by optimizing prediction performance and deferring when necessary."}}
