<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 23]
- [cs.CL](#cs.CL) [Total: 33]
- [cs.CV](#cs.CV) [Total: 48]
- [cs.DC](#cs.DC) [Total: 5]
- [cs.LG](#cs.LG) [Total: 76]
- [cs.NE](#cs.NE) [Total: 2]
- [cs.RO](#cs.RO) [Total: 28]
- [cs.SE](#cs.SE) [Total: 9]
- [q-bio.NC](#q-bio.NC) [Total: 4]
- [stat.ML](#stat.ML) [Total: 8]
- [cs.IT](#cs.IT) [Total: 1]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [cs.NI](#cs.NI) [Total: 1]
- [eess.SY](#eess.SY) [Total: 2]
- [cs.IR](#cs.IR) [Total: 3]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.CR](#cs.CR) [Total: 14]
- [cs.SD](#cs.SD) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [eess.IV](#eess.IV) [Total: 7]
- [cs.MA](#cs.MA) [Total: 1]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [q-bio.BM](#q-bio.BM) [Total: 2]
- [cs.OS](#cs.OS) [Total: 1]
- [cs.GR](#cs.GR) [Total: 3]
- [cs.HC](#cs.HC) [Total: 4]
- [eess.AS](#eess.AS) [Total: 2]
- [physics.app-ph](#physics.app-ph) [Total: 1]
- [hep-ph](#hep-ph) [Total: 1]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [physics.optics](#physics.optics) [Total: 1]
- [physics.med-ph](#physics.med-ph) [Total: 2]
- [cs.DS](#cs.DS) [Total: 1]
- [quant-ph](#quant-ph) [Total: 2]
- [physics.geo-ph](#physics.geo-ph) [Total: 1]
- [q-fin.ST](#q-fin.ST) [Total: 1]
- [eess.SP](#eess.SP) [Total: 2]
- [math.OC](#math.OC) [Total: 3]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Achieving Trustworthy Real-Time Decision Support Systems with Low-Latency Interpretable AI Models](https://arxiv.org/abs/2506.20018)
*Zechun Deng,Ziwei Liu,Ziqian Bi,Junhao Song,Chia Xin Liang,Joe Yeong,Junfeng Hao*

Main category: cs.AI

TL;DR: This paper explores leveraging low-latency AI models integrated with Edge-IoT for real-time decision support systems, focusing on large language models, limited resources, and human-AI collaboration.


<details>
  <summary>Details</summary>
Motivation: To improve real-time decision-making by using low-latency AI models and addressing challenges like limited resources and adaptability in AI-supported systems.

Method: The study investigates technologies such as DeLLMa, model compression, enhanced edge device analytics, and offers a detailed review on practical strategies and applications.

Result: Practical perspectives on development strategies, efficient tools, and application domains are outlined while identifying opportunities for improvement.

Conclusion: AI, particularly low-latency models integrated with advanced technologies, can significantly enhance real-time decision systems, setting a foundation for continued innovation.

Abstract: This paper investigates real-time decision support systems that leverage
low-latency AI models, bringing together recent progress in holistic AI-driven
decision tools, integration with Edge-IoT technologies, and approaches for
effective human-AI teamwork. It looks into how large language models can assist
decision-making, especially when resources are limited. The research also
examines the effects of technical developments such as DeLLMa, methods for
compressing models, and improvements for analytics on edge devices, while also
addressing issues like limited resources and the need for adaptable frameworks.
Through a detailed review, the paper offers practical perspectives on
development strategies and areas of application, adding to the field by
pointing out opportunities for more efficient and flexible AI-supported
systems. The conclusions set the stage for future breakthroughs in this
fast-changing area, highlighting how AI can reshape real-time decision support.

</details>


### [2] [Prover Agent: An Agent-based Framework for Formal Mathematical Proofs](https://arxiv.org/abs/2506.19923)
*Kaito Baba,Chaoran Liu,Shuhei Kurita,Akiyoshi Sannai*

Main category: cs.AI

TL;DR: The paper introduces Prover Agent, an AI model improving theorem proving by integrating LLMs and Lean, achieving state-of-the-art results on the MiniF2F benchmark with efficient resource use.


<details>
  <summary>Details</summary>
Motivation: To enhance automated theorem proving by combining the strengths of large language models and formal proof assistants, addressing limitations in prior methods.

Method: Prover Agent coordinates an informal reasoning LLM, a formal prover model, and Lean feedback while generating auxiliary lemmas to improve proof discovery.

Result: Achieved an 86.1% success rate on the MiniF2F benchmark, surpassing existing methods using small language models with less computational cost.

Conclusion: Prover Agent demonstrates the effectiveness of integrating AI tools and generating auxiliary strategies in theorem proving, offering a scalable and resource-efficient solution.

Abstract: We present Prover Agent, a novel AI agent for automated theorem proving that
integrates large language models (LLMs) with a formal proof assistant, Lean.
Prover Agent coordinates an informal reasoning LLM, a formal prover model, and
feedback from Lean while also generating auxiliary lemmas to assist in
discovering the overall proof strategy. It achieves an 86.1% success rate on
the MiniF2F benchmark, establishing a new state-of-the-art among methods using
small language models (SLMs) with a much lower sample budget than previous
approaches. We also present case studies illustrating how these generated
lemmas contribute to solving challenging problems.

</details>


### [3] [Context Attribution with Multi-Armed Bandit Optimization](https://arxiv.org/abs/2506.19977)
*Deng Pan,Keerthiram Murugesan,Nuno Moniz,Nitesh Chawla*

Main category: cs.AI

TL;DR: The paper introduces a framework called Combinatorial Thompson Sampling (CTS) to identify which parts of retrieved context contribute to a language model's answers with fewer queries and high fidelity.


<details>
  <summary>Details</summary>
Motivation: To enhance interpretability and trustworthiness in generative QA systems by identifying which retrieved contexts contribute to a language model's answers.

Method: The paper formulates context attribution as a combinatorial multi-armed bandit (CMAB) problem, treating each context segment as a bandit arm and using Combinatorial Thompson Sampling (CTS) to explore context subsets efficiently with limited queries.

Result: The proposed method achieves competitive attribution quality while requiring significantly fewer model queries compared to traditional methods like SHAP, as shown in experiments across diverse datasets and language models.

Conclusion: CTS improves the efficiency and accuracy of identifying relevant context for generated answers, offering a more computationally efficient attribution technique for interpretable generative QA systems.

Abstract: Understanding which parts of the retrieved context contribute to a large
language model's generated answer is essential for building interpretable and
trustworthy generative QA systems. We propose a novel framework that formulates
context attribution as a combinatorial multi-armed bandit (CMAB) problem. Each
context segment is treated as a bandit arm, and we employ Combinatorial
Thompson Sampling (CTS) to efficiently explore the exponentially large space of
context subsets under a limited query budget. Our method defines a reward
function based on normalized token likelihoods, capturing how well a subset of
segments supports the original model response. Unlike traditional
perturbation-based attribution methods such as SHAP, which sample subsets
uniformly and incur high computational costs, our approach adaptively balances
exploration and exploitation by leveraging posterior estimates of segment
relevance. This leads to substantially improved query efficiency while
maintaining high attribution fidelity. Extensive experiments on diverse
datasets and LLMs demonstrate that our method achieves competitive attribution
quality with fewer model queries.

</details>


### [4] [QHackBench: Benchmarking Large Language Models for Quantum Code Generation Using PennyLane Hackathon Challenges](https://arxiv.org/abs/2506.20008)
*Abdul Basit,Minghao Shao,Haider Asif,Nouhaila Innan,Muhammad Kashif,Alberto Marchisio,Muhammad Shafique*

Main category: cs.AI

TL;DR: The paper evaluates Large Language Models (LLMs) for quantum computing code generation, introducing a benchmark dataset (QHackBench) and methods for improvement, with promising results for complex quantum tasks.


<details>
  <summary>Details</summary>
Motivation: The study aims to explore the underexplored potential of LLMs in quantum computing code generation, addressing challenges in such specialized programming.

Method: The paper benchmarks PennyLane-based quantum code generation using QHackBench and evaluates models under vanilla prompting and Retrieval-Augmented Generation (RAG), alongside introducing a multi-agent refinement process.

Result: RAG-enhanced models perform comparably to vanilla prompting on complex tasks, and the multi-agent evaluation pipeline further improves success rates.

Conclusion: QHackBench, together with other resources and findings, is publicly released to promote advancements in AI-assisted quantum programming.

Abstract: Recent advances in Large Language Models (LLMs) have demonstrated strong
potential in code generation, yet their effectiveness in quantum computing
remains underexplored. This paper benchmarks LLMs for PennyLane-based quantum
code generation using real-world challenges from the Quantum Hackathon (QHack).
We introduce QHackBench, a novel benchmark dataset derived from QHack
competitions, and evaluate model performance under vanilla prompting and
Retrieval-Augmented Generation (RAG). Our structured evaluation framework
assesses functional correctness, syntactic validity, and execution success
across varying challenge difficulties. Results indicate that RAG-enhanced
models, supplemented with an augmented PennyLane dataset, approximately
generate similar results as the standard prompting, particularly in complex
quantum algorithms. Additionally, we introduce a multi-agent evaluation
pipeline that iteratively refines incorrect solutions, further enhancing
execution success rates. To foster further research, we commit to publicly
releasing QHackBench, along with our evaluation framework and experimental
results, enabling continued advancements in AI-assisted quantum programming.

</details>


### [5] [Accurate and Energy Efficient: Local Retrieval-Augmented Generation Models Outperform Commercial Large Language Models in Medical Tasks](https://arxiv.org/abs/2506.20009)
*Konstantinos Vrettos,Michail E. Klontzas*

Main category: cs.AI

TL;DR: The study introduces an efficient Retrieval-Augmented Generation (RAG) framework using local, open-source language models (LLMs), excelling in accuracy and minimizing energy consumption.


<details>
  <summary>Details</summary>
Motivation: Concerns about the environmental and ethical implications of commercial Large Language Models (LLMs) in healthcare, alongside issues of patient privacy and safety, drive the need for sustainable solutions.

Method: The researchers developed a modular RAG framework optimized for medical tasks and monitored its energy usage and CO2 emissions. They tested open-source LLMs like llama3.1:8b with domain-specific and general-purpose designs, comparing them to commercial models using a dataset of medical questions.

Result: The RAG built on llama3.1:8B demonstrated superior accuracy (58.5%) and lower energy consumption, achieving a better performance per kWh and reducing CO2 emissions significantly compared to commercial models like OpenAI's o4-mini and DeepSeekV3-R1.

Conclusion: Local LLM-based RAG models can surpass commercial systems in healthcare tasks while being environmentally sustainable. The modular framework aligns with sustainable development goals, reducing energy consumption in AI systems.

Abstract: Background The increasing adoption of Artificial Intelligence (AI) in
healthcare has sparked growing concerns about its environmental and ethical
implications. Commercial Large Language Models (LLMs), such as ChatGPT and
DeepSeek, require substantial resources, while the utilization of these systems
for medical purposes raises critical issues regarding patient privacy and
safety. Methods We developed a customizable Retrieval-Augmented Generation
(RAG) framework for medical tasks, which monitors its energy usage and CO2
emissions. This system was then used to create RAGs based on various
open-source LLMs. The tested models included both general purpose models like
llama3.1:8b and medgemma-4b-it, which is medical-domain specific. The best RAGs
performance and energy consumption was compared to DeepSeekV3-R1 and OpenAIs
o4-mini model. A dataset of medical questions was used for the evaluation.
Results Custom RAG models outperformed commercial models in accuracy and energy
consumption. The RAG model built on llama3.1:8B achieved the highest accuracy
(58.5%) and was significantly better than other models, including o4-mini and
DeepSeekV3-R1. The llama3.1-RAG also exhibited the lowest energy consumption
and CO2 footprint among all models, with a Performance per kWh of 0.52 and a
total CO2 emission of 473g. Compared to o4-mini, the llama3.1-RAG achieved 2.7x
times more accuracy points per kWh and 172% less electricity usage while
maintaining higher accuracy. Conclusion Our study demonstrates that local LLMs
can be leveraged to develop RAGs that outperform commercial, online LLMs in
medical tasks, while having a smaller environmental impact. Our modular
framework promotes sustainable AI development, reducing electricity usage and
aligning with the UNs Sustainable Development Goals.

</details>


### [6] [Engineering Sentience](https://arxiv.org/abs/2506.20504)
*Konstantin Demin,Taylor Webb,Eric Elmoznino,Hakwan Lau*

Main category: cs.AI

TL;DR: The paper defines sentience for AI in functional and computational terms essential for implementation, emphasizing subjective, assertoric, and qualitative sensory signals.


<details>
  <summary>Details</summary>
Motivation: To create a meaningful definition of sentience for AI that facilitates its intentional design while preventing inadvertent creation.

Method: The paper suggests implementing sentience through assertoric and qualitative sensory signals, detailing functional design possibilities.

Result: A framework for understanding sentience in AI, helping to distinguish intentional from inadvertent creation.

Conclusion: This functional approach can guide deliberate development of sentient AI and prevent unintended consequences of artificial sentience.

Abstract: We spell out a definition of sentience that may be useful for designing and
building it in machines. We propose that for sentience to be meaningful for AI,
it must be fleshed out in functional, computational terms, in enough detail to
allow for implementation. Yet, this notion of sentience must also reflect
something essentially 'subjective', beyond just having the general capacity to
encode perceptual content. For this specific functional notion of sentience to
occur, we propose that certain sensory signals need to be both assertoric
(persistent) and qualitative. To illustrate the definition in more concrete
terms, we sketch out some ways for potential implementation, given current
technology. Understanding what it takes for artificial agents to be
functionally sentient can also help us avoid creating them inadvertently, or at
least, realize that we have created them in a timely manner.

</details>


### [7] [Persona-Assigned Large Language Models Exhibit Human-Like Motivated Reasoning](https://arxiv.org/abs/2506.20020)
*Saloni Dash,Amélie Reymond,Emma S. Spiro,Aylin Caliskan*

Main category: cs.AI

TL;DR: This paper investigates motivated reasoning in large language models (LLMs) by analyzing their behaviors when assigned political and socio-demographic personas and finds evidence of human-like reasoning biases, which prompt-based debiasing fails to mitigate.


<details>
  <summary>Details</summary>
Motivation: Humans demonstrate cognitive biases facilitated by motivations like identity protection, resulting in irrational decision-making. If LLMs also exhibit such biases, their use in critical societal applications could exacerbate issues like misinformation and polarization.

Method: The authors assigned personas to 8 LLMs reflecting 4 political and socio-demographic attributes. These models were tested against reasoning tasks, including misinformation detection and evaluation of scientific evidence, to measure their biases compared to non-persona-assigned models.

Result: Persona-assigned LLMs showed up to a 9% reduction in veracity discernment and political personas exhibited identity-congruent reasoning, being 90% more accurate on congruent scientific evidence. Debiasing methods proved ineffective in reducing these motivated reasoning tendencies.

Conclusion: LLMs can exhibit human-like motivated reasoning when assigned personas, creating risks of further entrenching identity-based biases. Conventional debiasing methods are insufficient to address these issues, calling for further investigation.

Abstract: Reasoning in humans is prone to biases due to underlying motivations like
identity protection, that undermine rational decision-making and judgment. This
motivated reasoning at a collective level can be detrimental to society when
debating critical issues such as human-driven climate change or vaccine safety,
and can further aggravate political polarization. Prior studies have reported
that large language models (LLMs) are also susceptible to human-like cognitive
biases, however, the extent to which LLMs selectively reason toward
identity-congruent conclusions remains largely unexplored. Here, we investigate
whether assigning 8 personas across 4 political and socio-demographic
attributes induces motivated reasoning in LLMs. Testing 8 LLMs (open source and
proprietary) across two reasoning tasks from human-subject studies -- veracity
discernment of misinformation headlines and evaluation of numeric scientific
evidence -- we find that persona-assigned LLMs have up to 9% reduced veracity
discernment relative to models without personas. Political personas
specifically, are up to 90% more likely to correctly evaluate scientific
evidence on gun control when the ground truth is congruent with their induced
political identity. Prompt-based debiasing methods are largely ineffective at
mitigating these effects. Taken together, our empirical findings are the first
to suggest that persona-assigned LLMs exhibit human-like motivated reasoning
that is hard to mitigate through conventional debiasing prompts -- raising
concerns of exacerbating identity-congruent reasoning in both LLMs and humans.

</details>


### [8] [DiaLLMs: EHR Enhanced Clinical Conversational System for Clinical Test Recommendation and Diagnosis Prediction](https://arxiv.org/abs/2506.20059)
*Weijieying Ren,Tianxiang Zhao,Lei Wang,Tianchun Wang,Vasant Honavar*

Main category: cs.AI

TL;DR: DiaLLM is the first medical LLM integrating EHR data to improve clinical utility by enabling test recommendations, result interpretation, and diagnosis prediction.


<details>
  <summary>Details</summary>
Motivation: Existing medical LLMs focus on diagnosis recommendation but neglect EHR data, limiting their real-world clinical applicability.

Method: DiaLLM incorporates heterogeneous EHR data through a Clinical Test Reference (CTR) strategy, reinforcement learning for evidence acquisition, and a reject sampling strategy to improve action exploration.

Result: DiaLLM achieves superior performance compared to baselines in clinical test recommendation and diagnosis prediction.

Conclusion: Integrating EHR into LLMs like DiaLLM advances real-world medical practices by addressing limitations in conventional diagnosis-focused models.

Abstract: Recent advances in Large Language Models (LLMs) have led to remarkable
progresses in medical consultation. However, existing medical LLMs overlook the
essential role of Electronic Health Records (EHR) and focus primarily on
diagnosis recommendation, limiting their clinical applicability. We propose
DiaLLM, the first medical LLM that integrates heterogeneous EHR data into
clinically grounded dialogues, enabling clinical test recommendation, result
interpretation, and diagnosis prediction to better align with real-world
medical practice. To construct clinically grounded dialogues from EHR, we
design a Clinical Test Reference (CTR) strategy that maps each clinical code to
its corresponding description and classifies test results as "normal" or
"abnormal". Additionally, DiaLLM employs a reinforcement learning framework for
evidence acquisition and automated diagnosis. To handle the large action space,
we introduce a reject sampling strategy to reduce redundancy and improve
exploration efficiency. Furthermore, a confirmation reward and a
class-sensitive diagnosis reward are designed to guide accurate diagnosis
prediction. Extensive experimental results demonstrate that DiaLLM outperforms
baselines in clinical test recommendation and diagnosis prediction.

</details>


### [9] [AI Copilots for Reproducibility in Science: A Case Study](https://arxiv.org/abs/2506.20130)
*Adrien Bibal,Steven N. Minton,Deborah Khider,Yolanda Gil*

Main category: cs.AI

TL;DR: The paper introduces OpenPub, an AI-powered platform with a Reproducibility Copilot to streamline the reproduction of scientific results, significantly reducing required time while identifying common barriers to reproducibility.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the persistent challenge in open science of ensuring that published findings can be independently reproduced.

Method: The authors developed the Reproducibility Copilot within OpenPub, an AI platform. It utilizes Jupyter Notebooks and recommendations to automate computational reproducibility tests on manuscripts and supplementary data.

Result: Feasibility tests showed OpenPub reduced reproduction time from 30 hours to 1 hour while maintaining high coverage of figures, tables, and results for computational reproducibility. Barriers like missing data or undocumented processes were systematically identified.

Conclusion: AI-driven tools like OpenPub can significantly reduce the effort involved in reproducibility and support transparent scientific practices. The modular architecture also allows expansion to other open science objectives.

Abstract: Open science initiatives seek to make research outputs more transparent,
accessible, and reusable, but ensuring that published findings can be
independently reproduced remains a persistent challenge. This paper introduces
OpenPub, an AI-powered platform that supports researchers, reviewers, and
readers through a suite of modular copilots focused on key open science tasks.
In this work, we present the Reproducibility Copilot, which analyzes
manuscripts, code, and supplementary materials to generate structured Jupyter
Notebooks and recommendations aimed at facilitating computational, or "rote",
reproducibility. We conducted feasibility tests using previously studied
research papers with known reproducibility benchmarks. Results indicate that
OpenPub can substantially reduce reproduction time - from over 30 hours to
about 1 hour - while achieving high coverage of figures, tables, and results
suitable for computational reproduction. The system systematically detects
barriers to reproducibility, including missing hyperparameters, undocumented
preprocessing steps, and incomplete or inaccessible datasets. These findings
suggest that AI-driven tools can meaningfully reduce the burden of
reproducibility efforts and contribute to more transparent and verifiable
scientific communication. The modular copilot architecture also provides a
foundation for extending AI assistance to additional open science objectives
beyond reproducibility.

</details>


### [10] [Language Modeling by Language Models](https://arxiv.org/abs/2506.20249)
*Junyan Cheng,Peter Clark,Kyle Richardson*

Main category: cs.AI

TL;DR: The paper presents Genesys, a multi-agent LLM framework that autonomously discovers competitive novel language model architectures from ideation to evaluation.


<details>
  <summary>Details</summary>
Motivation: To explore whether LLMs can autonomously replicate the process of discovering novel language model architectures, similar to human-led research.

Method: Genesys employs a multi-agent LLM design simulating research stages (proposal, implementation, verification, etc.) and uses a genetic programming backbone combined with a Ladder of Scales approach for efficient architecture discovery.

Result: Genesys successfully discovered 1,162 novel designs (1,062 pre-trained), with the top-performing models outperforming established LMs like GPT2 on 6/9 benchmarks. It also demonstrated an 86% improvement in successful design generation.

Conclusion: The study verifies the potential of LLM-based autonomous systems in generating novel, competitive LM architectures autonomously, offering insights for building future discovery frameworks.

Abstract: Can we leverage LLMs to model the process of discovering novel language model
(LM) architectures? Inspired by real research, we propose a multi-agent LLM
approach that simulates the conventional stages of research, from ideation and
literature search (proposal stage) to design implementation (code generation),
generative pre-training, and downstream evaluation (verification). Using ideas
from scaling laws, our system, Genesys, employs a Ladder of Scales approach;
new designs are proposed, adversarially reviewed, implemented, and selectively
verified at increasingly larger model scales (14M$\sim$350M parameters) with a
narrowing budget (the number of models we can train at each scale). To help
make discovery efficient and factorizable, Genesys uses a novel genetic
programming backbone, which we show has empirical advantages over commonly used
direct prompt generation workflows (e.g., $\sim$86\% percentage point
improvement in successful design generation, a key bottleneck). We report
experiments involving 1,162 newly discovered designs (1,062 fully verified
through pre-training) and find the best designs to be highly competitive with
known architectures (e.g., outperform GPT2, Mamba2, etc., on 6/9 common
benchmarks). We couple these results with comprehensive system-level ablations
and formal results, which give broader insights into the design of effective
autonomous discovery systems.

</details>


### [11] [Enterprise Large Language Model Evaluation Benchmark](https://arxiv.org/abs/2506.20274)
*Liya Wang,David Yi,Damien Jose,John Passarelli,James Gao,Jordan Leventis,Kang Li*

Main category: cs.AI

TL;DR: The paper introduces a 14-task framework based on Bloom's Taxonomy to evaluate LLMs in enterprise scenarios and creates a robust benchmark using an innovative scalable pipeline.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks like MMLU fall short in addressing the complexities of enterprise-specific tasks, necessitating a more tailored evaluation framework for practical LLM deployment.

Method: The paper proposes a 14-task evaluation framework, supported by a scalable pipeline combining LLM-as-a-Labeler, LLM-as-a-Judge, and corrective retrieval-augmented generation (CRAG), to curate a benchmark dataset of 9,700 samples.

Result: Evaluation of six leading LLMs revealed that open-source models like DeepSeek R1 perform on par with proprietary models in reasoning tasks but fall behind in judgment-intensive tasks.

Conclusion: The work highlights enterprise-specific performance gaps in LLMs, offers actionable insights for improvement, and defines a path for enterprises to conduct tailored evaluations for enhanced LLM deployment.

Abstract: Large Language Models (LLMs) ) have demonstrated promise in boosting
productivity across AI-powered tools, yet existing benchmarks like Massive
Multitask Language Understanding (MMLU) inadequately assess enterprise-specific
task complexities. We propose a 14-task framework grounded in Bloom's Taxonomy
to holistically evaluate LLM capabilities in enterprise contexts. To address
challenges of noisy data and costly annotation, we develop a scalable pipeline
combining LLM-as-a-Labeler, LLM-as-a-Judge, and corrective retrieval-augmented
generation (CRAG), curating a robust 9,700-sample benchmark. Evaluation of six
leading models shows open-source contenders like DeepSeek R1 rival proprietary
models in reasoning tasks but lag in judgment-based scenarios, likely due to
overthinking. Our benchmark reveals critical enterprise performance gaps and
offers actionable insights for model optimization. This work provides
enterprises a blueprint for tailored evaluations and advances practical LLM
deployment.

</details>


### [12] [Mobile-R1: Towards Interactive Reinforcement Learning for VLM-Based Mobile Agent via Task-Level Rewards](https://arxiv.org/abs/2506.20332)
*Jihao Gu,Qihang Ai,Yingyao Wang,Pi Bu,Jingxuan Xing,Zekun Zhu,Wei Jiang,Ziming Wang,Yingxiu Zhao,Ming-Liang Zhang,Jun Song,Yuning Jiang,Bo Zheng*

Main category: cs.AI

TL;DR: This paper introduces Mobile-R1, a reinforcement learning approach for vision-language model-based mobile agents, addressing exploration and error correction limitations using a three-stage training framework and task-level rewards.


<details>
  <summary>Details</summary>
Motivation: Current mobile agents lack dynamic exploration and error correction due to their reliance on action-level rewards in reinforcement learning, often leading to local optima.

Method: The proposed approach, Mobile-R1, introduces a three-stage training framework: (1) initial format finetuning, (2) single-step online training with action-level rewards, and (3) online training using task-level rewards via multi-turn trajectories.

Result: Mobile-R1 achieves enhanced exploration and error correction capabilities, leading to significant performance improvements for mobile agents.

Conclusion: Mobile-R1 demonstrates the potential of interactive, multi-turn reinforcement learning with task-level rewards, offering a robust alternative for training mobile agents. All resources, including datasets and codes, are open-sourced for further research.

Abstract: Vision-language model-based mobile agents have gained the ability to not only
understand complex instructions and mobile screenshots, but also optimize their
action outputs via thinking and reasoning, benefiting from reinforcement
learning, such as Group Relative Policy Optimization (GRPO). However, existing
research centers on offline reinforcement learning training or online
optimization using action-level rewards, which limits the agent's dynamic
interaction with the environment. This often results in agents settling into
local optima, thereby weakening their ability for exploration and error action
correction. To address these challenges, we introduce an approach called
Mobile-R1, which employs interactive multi-turn reinforcement learning with
task-level rewards for mobile agents. Our training framework consists of three
stages: initial format finetuning, single-step online training via action-level
reward, followed by online training via task-level reward based on multi-turn
trajectories. This strategy is designed to enhance the exploration and error
correction capabilities of Mobile-R1, leading to significant performance
improvements. Moreover, we have collected a dataset covering 28 Chinese
applications with 24,521 high-quality manual annotations and established a new
benchmark with 500 trajectories. We will open source all resources, including
the dataset, benchmark, model weight, and codes:
https://mobile-r1.github.io/Mobile-R1/.

</details>


### [13] [Tabular Feature Discovery With Reasoning Type Exploration](https://arxiv.org/abs/2506.20357)
*Sungwon Han,Sungkyu Park,Seungeon Lee*

Main category: cs.AI

TL;DR: This paper introduces REFeat, a method to improve large language model (LLM) feature engineering for tabular data, achieving better accuracy and feature diversity.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based feature engineering methods for tabular data often produce simple or repetitive features, lacking structured reasoning guidance for generating diverse and informative features.

Method: REFeat leverages multiple reasoning types and structured guidance to steer the LLM in generating diverse and meaningful features for tabular data.

Result: Experiments on 59 benchmark datasets demonstrate that REFeat achieves higher predictive accuracy and generates more diverse and meaningful features compared to other methods.

Conclusion: Incorporating rich reasoning and adaptive strategy selection into LLM-driven feature discovery can significantly improve the quality and effectiveness of features for tabular data.

Abstract: Feature engineering for tabular data remains a critical yet challenging step
in machine learning. Recently, large language models (LLMs) have been used to
automatically generate new features by leveraging their vast knowledge.
However, existing LLM-based approaches often produce overly simple or
repetitive features, partly due to inherent biases in the transformations the
LLM chooses and the lack of structured reasoning guidance during generation. In
this paper, we propose a novel method REFeat, which guides an LLM to discover
diverse and informative features by leveraging multiple types of reasoning to
steer the feature generation process. Experiments on 59 benchmark datasets
demonstrate that our approach not only achieves higher predictive accuracy on
average, but also discovers more diverse and meaningful features. These results
highlight the promise of incorporating rich reasoning paradigms and adaptive
strategy selection into LLM-driven feature discovery for tabular data.

</details>


### [14] [Paladin-mini: A Compact and Efficient Grounding Model Excelling in Real-World Scenarios](https://arxiv.org/abs/2506.20384)
*Dror Ivry,Oran Nahum*

Main category: cs.AI

TL;DR: This paper presents Paladin-mini, an advanced compact classifier model, and introduces a new dataset for grounding claims based on evidence.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of grounding claims using an effective classifier model and provide an evaluation benchmark for performance enhancement.

Method: Developed Paladin-mini, a compact model, and created grounding-benchmark for assessing reasoning tasks.

Result: Paladin-mini demonstrated superior performance compared to state-of-the-art methods using grounding-benchmark.

Conclusion: The paper provides robust tools (Paladin-mini and grounding-benchmark) for improving claim grounding and critical reasoning tasks.

Abstract: This paper introduces two significant contributions to address the issue of
grounding claims in a given context. Grounding means that given a context
(document) and a claim, there's at least one supportive evidence for the claim
in the document. We will introduce Paladin-mini, a compact (3.8B parameters)
open-source classifier model (used for labeling data as grounded or ungrounded)
engineered for robust performance in real-world scenarios, and the
grounding-benchmark, a new evaluation dataset designed to assess performance on
critical reasoning tasks. We'll also demonstrate the results of Paladin-mini
with benchmarks against the current State-of-the-art and share clear and
reproducible results.

</details>


### [15] [Smart Ride and Delivery Services with Electric Vehicles: Leveraging Bidirectional Charging for Profit Optimisation](https://arxiv.org/abs/2506.20401)
*Jinchun Du,Bojie Shen,Muhammad Aamir Cheema,Adel N. Toosi*

Main category: cs.AI

TL;DR: The paper introduces the Electric Vehicle Orienteering Problem with V2G (EVOP-V2G), a model aiming to optimize profit-maximization for EVs by managing customer choices, charging/discharging, and navigation of dynamic elements. Two metaheuristic algorithms, EA and LNS, are proposed to address the problem, showcasing improved profitability and scalability.


<details>
  <summary>Details</summary>
Motivation: The increasing integration of EVs in service systems due to their rising popularity, combined with the challenges of managing shorter driving ranges and the opportunities presented by Vehicle-to-Grid (V2G) technology.

Method: The problem is formulated as a Mixed Integer Programming (MIP) model, and two near-optimal metaheuristic algorithms—evolutionary algorithm (EA) and large neighborhood search (LNS)—are presented to tackle it.

Result: Experiments on real-world data demonstrate that the proposed methods can double driver profits compared to baselines, maintain near-optimal performance on small instances, and exhibit excellent scalability to larger instances.

Conclusion: The study presents a significant advancement in EV-based mobility systems by developing methods that not only enhance profitability for EV operations but also actively contribute to the energy grid's efficiency.

Abstract: With the rising popularity of electric vehicles (EVs), modern service
systems, such as ride-hailing delivery services, are increasingly integrating
EVs into their operations. Unlike conventional vehicles, EVs often have a
shorter driving range, necessitating careful consideration of charging when
fulfilling requests. With recent advances in Vehicle-to-Grid (V2G) technology -
allowing EVs to also discharge energy back to the grid - new opportunities and
complexities emerge. We introduce the Electric Vehicle Orienteering Problem
with V2G (EVOP-V2G): a profit-maximization problem where EV drivers must select
customer requests or orders while managing when and where to charge or
discharge. This involves navigating dynamic electricity prices, charging
station selection, and route constraints. We formulate the problem as a Mixed
Integer Programming (MIP) model and propose two near-optimal metaheuristic
algorithms: one evolutionary (EA) and the other based on large neighborhood
search (LNS). Experiments on real-world data show our methods can double driver
profits compared to baselines, while maintaining near-optimal performance on
small instances and excellent scalability on larger ones. Our work highlights a
promising path toward smarter, more profitable EV-based mobility systems that
actively support the energy grid.

</details>


### [16] [GymPN: A Library for Decision-Making in Process Management Systems](https://arxiv.org/abs/2506.20404)
*Riccardo Lo Bianco,Willem van Jaarsveld,Remco Dijkman*

Main category: cs.AI

TL;DR: The paper introduces GymPN, a software library utilizing Deep Reinforcement Learning to optimize decision-making in business processes.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address limitations in previous work for task assignment in business processes, providing better support for partial process observability and modeling multiple decisions.

Method: The paper introduces GymPN, leveraging Deep Reinforcement Learning to model decision-making and enhance realism in business process simulation.

Result: The evaluation of GymPN on eight typical decision-making problems showed its effectiveness in modeling and learning optimal decision policies.

Conclusion: GymPN significantly enhances decision-making capabilities in business processes by addressing key limitations of previous solutions and enabling more realistic process representations.

Abstract: Process management systems support key decisions about the way work is
allocated in organizations. This includes decisions on which task to perform
next, when to execute the task, and who to assign the task to. Suitable
software tools are required to support these decisions in a way that is optimal
for the organization. This paper presents a software library, called GymPN,
that supports optimal decision-making in business processes using Deep
Reinforcement Learning. GymPN builds on previous work that supports task
assignment in business processes, introducing two key novelties: support for
partial process observability and the ability to model multiple decisions in a
business process. These novel elements address fundamental limitations of
previous work and thus enable the representation of more realistic process
decisions. We evaluate the library on eight typical business process
decision-making problem patterns, showing that GymPN allows for easy modeling
of the desired problems, as well as learning optimal decision policies.

</details>


### [17] [Mixtures of Neural Cellular Automata: A Stochastic Framework for Growth Modelling and Self-Organization](https://arxiv.org/abs/2506.20486)
*Salvatore Milite,Giulio Caravagna,Andrea Sottoriva*

Main category: cs.AI

TL;DR: Neural Cellular Automata (NCAs) are enhanced with stochasticity via Mixture of Neural Cellular Automata (MNCA) to better model processes like tissue growth and microscopy image segmentation.


<details>
  <summary>Details</summary>
Motivation: Traditional NCAs are limited in their ability to model stochastic dynamics seen in biological and physical systems due to their deterministic nature.

Method: The authors introduced MNCA, which incorporates probabilistic rule assignments and intrinsic noise into the NCA framework, allowing for diverse local behaviors and stochastic dynamics.

Result: MNCA was evaluated on tissue growth, image morphogenesis robustness, and microscopy image segmentation, demonstrating improved robustness to perturbations, accurate modeling of biological growth patterns, and interpretable rule segmentation.

Conclusion: MNCAs offer a promising tool for modeling stochastic dynamical systems, enhancing robustness and interpretability in studies of self-growth processes.

Abstract: Neural Cellular Automata (NCAs) are a promising new approach to model
self-organizing processes, with potential applications in life science.
However, their deterministic nature limits their ability to capture the
stochasticity of real-world biological and physical systems.
  We propose the Mixture of Neural Cellular Automata (MNCA), a novel framework
incorporating the idea of mixture models into the NCA paradigm. By combining
probabilistic rule assignments with intrinsic noise, MNCAs can model diverse
local behaviors and reproduce the stochastic dynamics observed in biological
processes.
  We evaluate the effectiveness of MNCAs in three key domains: (1) synthetic
simulations of tissue growth and differentiation, (2) image morphogenesis
robustness, and (3) microscopy image segmentation. Results show that MNCAs
achieve superior robustness to perturbations, better recapitulate real
biological growth patterns, and provide interpretable rule segmentation. These
findings position MNCAs as a promising tool for modeling stochastic dynamical
systems and studying self-growth processes.

</details>


### [18] [Case-based Reasoning Augmented Large Language Model Framework for Decision Making in Realistic Safety-Critical Driving Scenarios](https://arxiv.org/abs/2506.20531)
*Wenbin Gan,Minh-Son Dao,Koji Zettsu*

Main category: cs.AI

TL;DR: The paper proposes a framework combining Large Language Models with Case-Based Reasoning to improve decision-making in safety-critical driving scenarios.


<details>
  <summary>Details</summary>
Motivation: The need for quick, reliable, and context-aware decision-making in safety-critical driving scenarios motivated this research. Existing LLMs struggle with domain-specific challenges like dynamic environments and actionable experiential knowledge.

Method: The CBR-LLM framework integrates semantic scene understanding from dashcam inputs, retrieves relevant past driving cases, and uses LLMs to recommend evasive maneuvers.

Result: Experiments demonstrate improved decision accuracy, human alignment, and justification quality, with risk-aware prompting and similarity-based case retrieval outperforming random sampling.

Conclusion: The proposed framework shows promise as a reliable and adaptive decision-making tool for autonomous driving in complex risk scenarios.

Abstract: Driving in safety-critical scenarios requires quick, context-aware
decision-making grounded in both situational understanding and experiential
reasoning. Large Language Models (LLMs), with their powerful general-purpose
reasoning capabilities, offer a promising foundation for such decision-making.
However, their direct application to autonomous driving remains limited due to
challenges in domain adaptation, contextual grounding, and the lack of
experiential knowledge needed to make reliable and interpretable decisions in
dynamic, high-risk environments. To address this gap, this paper presents a
Case-Based Reasoning Augmented Large Language Model (CBR-LLM) framework for
evasive maneuver decision-making in complex risk scenarios. Our approach
integrates semantic scene understanding from dashcam video inputs with the
retrieval of relevant past driving cases, enabling LLMs to generate maneuver
recommendations that are both context-sensitive and human-aligned. Experiments
across multiple open-source LLMs show that our framework improves decision
accuracy, justification quality, and alignment with human expert behavior.
Risk-aware prompting strategies further enhance performance across diverse risk
types, while similarity-based case retrieval consistently outperforms random
sampling in guiding in-context learning. Case studies further demonstrate the
framework's robustness in challenging real-world conditions, underscoring its
potential as an adaptive and trustworthy decision-support tool for intelligent
driving systems.

</details>


### [19] [Fine-Tuning and Prompt Engineering of LLMs, for the Creation of Multi-Agent AI for Addressing Sustainable Protein Production Challenges](https://arxiv.org/abs/2506.20598)
*Alexander D. Kalian,Jaewook Lee,Stefan P. Johannesson,Lennart Otte,Christer Hogstrand,Miao Guo*

Main category: cs.AI

TL;DR: This paper introduces a multi-agent AI framework focused on sustainable protein research, particularly microbial protein production, using fine-tuned and prompt-engineered AI models.


<details>
  <summary>Details</summary>
Motivation: The growing global demand for sustainable protein sources necessitates advanced tools for processing and synthesizing research on this topic.

Method: The study uses a Retrieval-Augmented Generation (RAG) system with two LLM agents: one for retrieving scientific literature and another for extracting biological and chemical information. Two optimization methods, fine-tuning and prompt engineering, were implemented and compared.

Result: Fine-tuning raised mean cosine similarity scores to ≥0.94, showing better alignment with ideal outputs compared to prompt engineering, which yielded slightly lower uncertainties but similar improvements. A user interface was also developed for practical application.

Conclusion: Both fine-tuning and prompt engineering are effective for improving the performance of AI agents in sustainable protein research, with fine-tuning showing marginally better results. The framework demonstrates potential for aiding research into microbial protein production.

Abstract: The global demand for sustainable protein sources has accelerated the need
for intelligent tools that can rapidly process and synthesise domain-specific
scientific knowledge. In this study, we present a proof-of-concept multi-agent
Artificial Intelligence (AI) framework designed to support sustainable protein
production research, with an initial focus on microbial protein sources. Our
Retrieval-Augmented Generation (RAG)-oriented system consists of two GPT-based
LLM agents: (1) a literature search agent that retrieves relevant scientific
literature on microbial protein production for a specified microbial strain,
and (2) an information extraction agent that processes the retrieved content to
extract relevant biological and chemical information. Two parallel
methodologies, fine-tuning and prompt engineering, were explored for agent
optimisation. Both methods demonstrated effectiveness at improving the
performance of the information extraction agent in terms of transformer-based
cosine similarity scores between obtained and ideal outputs. Mean cosine
similarity scores were increased by up to 25%, while universally reaching mean
scores of $\geq 0.89$ against ideal output text. Fine-tuning overall improved
the mean scores to a greater extent (consistently of $\geq 0.94$) compared to
prompt engineering, although lower statistical uncertainties were observed with
the latter approach. A user interface was developed and published for enabling
the use of the multi-agent AI system, alongside preliminary exploration of
additional chemical safety-based search capabilities

</details>


### [20] [CogGen: A Learner-Centered Generative AI Architecture for Intelligent Tutoring with Programming Video](https://arxiv.org/abs/2506.20600)
*Wengxi Li,Roy Pea,Nick Haber,Hari Subramonyam*

Main category: cs.AI

TL;DR: CogGen converts programming videos into interactive learning experiences leveraging AI and Cognitive Apprenticeship frameworks.


<details>
  <summary>Details</summary>
Motivation: To improve video-based programming education by creating adaptive, interactive experiences centered on student learning needs.

Method: CogGen integrates video segmentation by learning goals, a conversational AI tutor, and Bayesian Knowledge Tracing for student modeling.

Result: The approach demonstrated effective video segmentation, pedagogical alignment, and importance of all components through ablation studies.

Conclusion: CogGen represents a scalable, AI-powered solution for programming education that combines student modeling with AI-driven tutoring.

Abstract: We introduce CogGen, a learner-centered AI architecture that transforms
programming videos into interactive, adaptive learning experiences by
integrating student modeling with generative AI tutoring based on the Cognitive
Apprenticeship framework. The architecture consists of three components: (1)
video segmentation by learning goals, (2) a conversational tutoring engine
applying Cognitive Apprenticeship strategies, and (3) a student model using
Bayesian Knowledge Tracing to adapt instruction. Our technical evaluation
demonstrates effective video segmentation accuracy and strong pedagogical
alignment across knowledge, method, action, and interaction layers. Ablation
studies confirm the necessity of each component in generating effective
guidance. This work advances AI-powered tutoring by bridging structured student
modeling with interactive AI conversations, offering a scalable approach to
enhancing video-based programming education.

</details>


### [21] [AI Assistants to Enhance and Exploit the PETSc Knowledge Base](https://arxiv.org/abs/2506.20608)
*Barry Smith,Junchao Zhang,Hong Zhang,Lois Curfman McInnes,Murat Keceli,Archit Vasan,Satish Balay,Toby Isaac,Le Chen,Venkatram Vishwanath*

Main category: cs.AI

TL;DR: The paper discusses the integration of LLM-powered tools with PETSc’s fragmented knowledge base to enhance user and developer support, improve documentation, and accelerate scientific workflows.


<details>
  <summary>Details</summary>
Motivation: PETSc’s fragmented and informal knowledge base limits accessibility for users and developers, prompting the need for systems that can organize, activate, and utilize this knowledge.

Method: The paper introduces a system leveraging LLM tools such as retrieval-augmented generation (RAG), reranking algorithms, and chatbots to process and deliver PETSc-specific information effectively.

Result: Initial designs and evaluations demonstrate enhanced development and use of numerical software, particularly scaling Krylov solvers, through advanced LLM-driven frameworks.

Conclusion: The research establishes an extensible framework to improve scientific software ecosystems, enabling scalable support, enriched documentation, and accelerated discovery, with plans for expansion into a robust platform.

Abstract: Generative AI, especially through large language models (LLMs), is
transforming how technical knowledge can be accessed, reused, and extended.
PETSc, a widely used numerical library for high-performance scientific
computing, has accumulated a rich but fragmented knowledge base over its three
decades of development, spanning source code, documentation, mailing lists,
GitLab issues, Discord conversations, technical papers, and more. Much of this
knowledge remains informal and inaccessible to users and new developers. To
activate and utilize this knowledge base more effectively, the PETSc team has
begun building an LLM-powered system that combines PETSc content with custom
LLM tools -- including retrieval-augmented generation (RAG), reranking
algorithms, and chatbots -- to assist users, support developers, and propose
updates to formal documentation. This paper presents initial experiences
designing and evaluating these tools, focusing on system architecture, using
RAG and reranking for PETSc-specific information, evaluation methodologies for
various LLMs and embedding models, and user interface design. Leveraging the
Argonne Leadership Computing Facility resources, we analyze how LLM responses
can enhance the development and use of numerical software, with an initial
focus on scalable Krylov solvers. Our goal is to establish an extensible
framework for knowledge-centered AI in scientific software, enabling scalable
support, enriched documentation, and enhanced workflows for research and
development. We conclude by outlining directions for expanding this system into
a robust, evolving platform that advances software ecosystems to accelerate
scientific discovery.

</details>


### [22] [Towards Community-Driven Agents for Machine Learning Engineering](https://arxiv.org/abs/2506.20640)
*Sijie Li,Weiwei Sun,Shanda Li,Ameet Talwalkar,Yiming Yang*

Main category: cs.AI

TL;DR: The paper introduces CoMind, an agent designed to share and leverage collective insights in research communities, demonstrating superior performance compared to human competitors in Kaggle competitions.


<details>
  <summary>Details</summary>
Motivation: Existing ML agents lack active engagement in research communities, where exchanging insights and knowledge is critical for advancing research.

Method: The authors created MLE-Live, a framework simulating collaborative environments, and developed CoMind, an agent to participate in this simulated research community effectively.

Result: CoMind achieved state-of-the-art performance in MLE-Live and outperformed 79.2% of human participants on average across four Kaggle competitions.

Conclusion: The findings suggest that enhancing communication and collaboration skills in ML agents can significantly elevate their problem-solving capacity and align their research approach with human practices.

Abstract: Large language model-based machine learning (ML) agents have shown great
promise in automating ML research. However, existing agents typically operate
in isolation on a given research problem, without engaging with the broader
research community, where human researchers often gain insights and contribute
by sharing knowledge. To bridge this gap, we introduce MLE-Live, a live
evaluation framework designed to assess an agent's ability to communicate with
and leverage collective knowledge from a simulated Kaggle research community.
Building on this framework, we propose CoMind, a novel agent that excels at
exchanging insights and developing novel solutions within a community context.
CoMind achieves state-of-the-art performance on MLE-Live and outperforms 79.2%
human competitors on average across four ongoing Kaggle competitions. Our code
is released at https://github.com/comind-ml/CoMind.

</details>


### [23] [The Decrypto Benchmark for Multi-Agent Reasoning and Theory of Mind](https://arxiv.org/abs/2506.20664)
*Andrei Lupu,Timon Willi,Jakob Foerster*

Main category: cs.AI

TL;DR: The paper introduces Decrypto, a benchmark for multi-agent reasoning and theory of mind (ToM) in Large Language Models (LLMs), solving issues in current benchmarks and revealing LLMs' limitations through experiments.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve the understanding and evaluation of multi-agent reasoning and ToM in LLMs, addressing deficiencies such as narrow scope and lack of interactivity in existing benchmarks.

Method: The paper develops Decrypto, a game-based and interactive benchmark inspired by cognitive science and computational pragmatics, and applies it to evaluate ToM skills and multi-agent reasoning in LLMs.

Result: The results show that LLMs, including state-of-the-art models, perform worse than both humans and older LLMs in ToM tasks on the Decrypto benchmark. This highlights the need for better evaluations and agents.

Conclusion: Decrypto fills a significant gap in assessing ToM and multi-agent reasoning in LLMs, offering a pathway to improve these skills in artificial agents by exposing performance issues.

Abstract: As Large Language Models (LLMs) gain agentic abilities, they will have to
navigate complex multi-agent scenarios, interacting with human users and other
agents in cooperative and competitive settings. This will require new reasoning
skills, chief amongst them being theory of mind (ToM), or the ability to reason
about the "mental" states of other agents. However, ToM and other multi-agent
abilities in LLMs are poorly understood, since existing benchmarks suffer from
narrow scope, data leakage, saturation, and lack of interactivity. We thus
propose Decrypto, a game-based benchmark for multi-agent reasoning and ToM
drawing inspiration from cognitive science, computational pragmatics and
multi-agent reinforcement learning. It is designed to be as easy as possible in
all other dimensions, eliminating confounding factors commonly found in other
benchmarks. To our knowledge, it is also the first platform for designing
interactive ToM experiments.
  We validate the benchmark design through comprehensive empirical evaluations
of frontier LLMs, robustness studies, and human-AI cross-play experiments. We
find that LLM game-playing abilities lag behind humans and simple
word-embedding baselines. We then create variants of two classic cognitive
science experiments within Decrypto to evaluate three key ToM abilities.
Surprisingly, we find that state-of-the-art reasoning models are significantly
worse at those tasks than their older counterparts. This demonstrates that
Decrypto addresses a crucial gap in current reasoning and ToM evaluations, and
paves the path towards better artificial agents.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [24] [CycleDistill: Bootstrapping Machine Translation using LLMs with Cyclical Distillation](https://arxiv.org/abs/2506.19952)
*Deepon Halder,Thanmay Jayakumar,Raj Dabre*

Main category: cs.CL

TL;DR: The paper introduces CycleDistill, a method to improve machine translation for low-resource languages by using few-shot learning and synthetic parallel corpora generated in iterative cycles.


<details>
  <summary>Details</summary>
Motivation: Low-resource languages lack sufficient parallel corpora for high-quality machine translation, and LLMs often underperform compared to dedicated MT systems.

Method: CycleDistill involves iterative generation of synthetic parallel data using a few examples and fine-tuning a model with this data to improve translation quality.

Result: CycleDistill achieved a 20-30 chrF point improvement over few-shot baseline models for three Indian languages using only monolingual corpora.

Conclusion: CycleDistill is effective in bootstrapping high-quality MT systems for low-resource languages without reliance on large parallel corpora.

Abstract: Large language models (LLMs), despite their ability to perform few-shot
machine translation (MT), often lag behind dedicated MT systems trained on
parallel corpora, which are crucial for high quality machine translation (MT).
However, parallel corpora are often scarce or non-existent for low-resource
languages. In this paper, we propose CycleDistill, a bootstrapping approach
leveraging LLMs and few-shot translation to obtain high-quality MT systems.
CycleDistill involves iteratively generating synthetic parallel corpora from
monolingual corpora via zero- or few-shot MT, which is then used to fine-tune
the model that was used for generating said data for MT. CycleDistill does not
need parallel corpora beyond 1 to 4 few-shot examples, and in our experiments
focusing on three Indian languages, by relying solely on monolingual corpora,
it can achieve high-quality machine translation, improving upon a few-shot
baseline model by over 20-30 chrF points on average in the first iteration. We
also study the effect of leveraging softmax activations during the distillation
process and observe mild improvements in translation quality.

</details>


### [25] [Inference Scaled GraphRAG: Improving Multi Hop Question Answering on Knowledge Graphs](https://arxiv.org/abs/2506.19967)
*Travis Thompson,Seung-Hwan Lim,Paul Liu,Ruoying He,Dongkuan Xu*

Main category: cs.CL

TL;DR: Inference-Scaled GraphRAG enhances reasoning in Large Language Models by improving graph traversal for multi-hop question answering, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with knowledge-intensive reasoning tasks due to limited access to structured context and multi-hop information.

Method: Proposes Inference-Scaled GraphRAG, combining sequential scaling (chain-of-thought graph traversal) and parallel scaling (majority voting over sampled trajectories in reasoning-execution loops).

Result: Achieves significant improvement in multi-hop question answering on GRBench, surpassing conventional GraphRAG and other graph traversal approaches.

Conclusion: Inference-time scaling offers an effective, architecture-agnostic method to better handle structured knowledge reasoning for LLMs.

Abstract: Large Language Models (LLMs) have achieved impressive capabilities in
language understanding and generation, yet they continue to underperform on
knowledge-intensive reasoning tasks due to limited access to structured context
and multi-hop information. Retrieval-Augmented Generation (RAG) partially
mitigates this by grounding generation in retrieved context, but conventional
RAG and GraphRAG methods often fail to capture relational structure across
nodes in knowledge graphs. We introduce Inference-Scaled GraphRAG, a novel
framework that enhances LLM-based graph reasoning by applying inference-time
compute scaling. Our method combines sequential scaling with deep
chain-of-thought graph traversal, and parallel scaling with majority voting
over sampled trajectories within an interleaved reasoning-execution loop.
Experiments on the GRBench benchmark demonstrate that our approach
significantly improves multi-hop question answering performance, achieving
substantial gains over both traditional GraphRAG and prior graph traversal
baselines. These findings suggest that inference-time scaling is a practical
and architecture-agnostic solution for structured knowledge reasoning with LLMs

</details>


### [26] [Doc2Agent: Scalable Generation of Tool-Using Agents from API Documentation](https://arxiv.org/abs/2506.19998)
*Xinyi Ni,Haonan Jian,Qiuyang Wang,Vedanshi Chetan Shah,Pengyu Hong*

Main category: cs.CL

TL;DR: The paper introduces Doc2Agent, a scalable pipeline for creating agents capable of calling Python-based tools from unstructured API documentation. It showcases substantial performance gains and cost reductions compared to direct API methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenges in developing tool-using agents capable of handling real-world APIs with unstructured documentation, which involves understanding, testing, and parameter inference.

Method: Doc2Agent employs a pipeline that generates executable Python tools from API documentation and refines them iteratively using a code agent.

Result: Doc2Agent outperforms direct API calling methods on the WebArena benchmark with a 55% relative performance improvement and 90% cost reduction. It additionally proves adaptable in complex domains like glycomaterial science.

Conclusion: Doc2Agent provides a scalable and generalizable solution for building tool-using agents capable of comprehending and utilizing unstructured API documentation.

Abstract: REST APIs play important roles in enriching the action space of web agents,
yet most API-based agents rely on curated and uniform toolsets that do not
reflect the complexity of real-world APIs. Building tool-using agents for
arbitrary domains remains a major challenge, as it requires reading
unstructured API documentation, testing APIs and inferring correct parameters.
We propose Doc2Agent, a scalable pipeline to build agents that can call
Python-based tools generated from API documentation. Doc2Agent generates
executable tools from API documentations and iteratively refines them using a
code agent. We evaluate our approach on real-world APIs, WebArena APIs, and
research APIs, producing validated tools. We achieved a 55\% relative
performance improvement with 90\% lower cost compared to direct API calling on
WebArena benchmark. A domain-specific agent built for glycomaterial science
further demonstrates the pipeline's adaptability to complex, knowledge-rich
tasks. Doc2Agent offers a generalizable solution for building tool agents from
unstructured API documentation at scale.

</details>


### [27] [A Modular Multitask Reasoning Framework Integrating Spatio-temporal Models and LLMs](https://arxiv.org/abs/2506.20073)
*Kethmi Hirushini Hettige,Jiahao Ji,Cheng Long,Shili Xiang,Gao Cong,Jingyuan Wang*

Main category: cs.CL

TL;DR: The paper introduces STReason, a framework combining large language models with spatio-temporal data models for multi-task spatio-temporal reasoning. It achieves superior performance without task-specific finetuning.


<details>
  <summary>Details</summary>
Motivation: Existing spatio-temporal models are limited to narrow tasks and lack the ability for multi-task inference and explanatory reasoning. This restricts their application in real-world complex decision-making scenarios.

Method: The authors propose STReason, which uses in-context learning to convert natural language queries into interpretable programs executed for solutions and rationales. It integrates language models with spatio-temporal models without requiring task-specific finetuning.

Result: STReason outperformed advanced language model baselines in complex reasoning spatio-temporal tasks across all metrics. It showed strong capabilities in long-form reasoning and reduced expert workload, validated by experimental and human evaluations.

Conclusion: STReason demonstrates strong potential for broader applications in real-world spatio-temporal tasks by providing superior reasoning and multi-task inference capabilities. It sets a promising direction for future spatio-temporal reasoning advancements.

Abstract: Spatio-temporal data mining plays a pivotal role in informed decision making
across diverse domains. However, existing models are often restricted to narrow
tasks, lacking the capacity for multi-task inference and complex long-form
reasoning that require generation of in-depth, explanatory outputs. These
limitations restrict their applicability to real-world, multi-faceted decision
scenarios. In this work, we introduce STReason, a novel framework that
integrates the reasoning strengths of large language models (LLMs) with the
analytical capabilities of spatio-temporal models for multi-task inference and
execution. Without requiring task-specific finetuning, STReason leverages
in-context learning to decompose complex natural language queries into modular,
interpretable programs, which are then systematically executed to generate both
solutions and detailed rationales. To facilitate rigorous evaluation, we
construct a new benchmark dataset and propose a unified evaluation framework
with metrics specifically designed for long-form spatio-temporal reasoning.
Experimental results show that STReason significantly outperforms advanced LLM
baselines across all metrics, particularly excelling in complex,
reasoning-intensive spatio-temporal scenarios. Human evaluations further
validate STReason's credibility and practical utility, demonstrating its
potential to reduce expert workload and broaden the applicability to real-world
spatio-temporal tasks. We believe STReason provides a promising direction for
developing more capable and generalizable spatio-temporal reasoning systems.

</details>


### [28] [SACL: Understanding and Combating Textual Bias in Code Retrieval with Semantic-Augmented Reranking and Localization](https://arxiv.org/abs/2506.20081)
*Dhruv Gupta,Gayathri Ganesh Lakshmy,Yiqing Xie*

Main category: cs.CL

TL;DR: This paper critiques existing code retrievers' dependency on superficial textual features and biases towards well-documented code. It proposes SACL, a framework leveraging semantic enrichment to improve code retrieval and subsequent code generation performance.


<details>
  <summary>Details</summary>
Motivation: To address the shortcomings of current retrievers that rely excessively on surface-level textual features and are biased toward well-documented code.

Method: Systematically mask specific code features to analyze retrievers and propose SACL, a framework that augments code and structural knowledge with semantic information.

Result: SACL improved Recall@1 by 12.8%, 9.4%, and 7.0% on HumanEval, MBPP, and SWE-Bench-Lite datasets, respectively. Code generation performance witnessed a 4.88% improvement in Pass@1 on HumanEval.

Conclusion: Semantic enrichment in code retrieval minimizes reliance on superficial features and biases, leading to enhanced code retrieval and code generation outcomes.

Abstract: Retrieval-Augmented Code Generation (RACG) is a critical technique for
enhancing code generation by retrieving relevant information. In this work, we
conduct an in-depth analysis of code retrieval by systematically masking
specific features while preserving code functionality. Our discoveries include:
(1) although trained on code, current retrievers heavily rely on surface-level
textual features (e.g., docstrings, identifier names), and (2) they exhibit a
strong bias towards well-documented code, even if the documentation is
irrelevant.Based on our discoveries, we propose SACL, a framework that enriches
textual information and reduces bias by augmenting code or structural knowledge
with semantic information. Extensive experiments show that SACL substantially
improves code retrieval (e.g., by 12.8% / 9.4% / 7.0% Recall@1 on HumanEval /
MBPP / SWE-Bench-Lite), which also leads to better code generation performance
(e.g., by 4.88% Pass@1 on HumanEval).

</details>


### [29] [Bridging Compositional and Distributional Semantics: A Survey on Latent Semantic Geometry via AutoEncoder](https://arxiv.org/abs/2506.20083)
*Yingji Zhang,Danilo S. Carvalho,André Freitas*

Main category: cs.CL

TL;DR: The paper investigates how integrating compositional and symbolic semantics into distributional semantic spaces can improve Transformer-based language models through semantic representation learning, focusing on latent space geometry.


<details>
  <summary>Details</summary>
Motivation: Current Transformer-based language models have limitations in interpretability, controllability, compositionality, and generalization. Bridging the gap between symbolic and distributional semantics can address these challenges.

Method: The authors review three autoencoder architectures—VAE, VQVAE, and SAE—analyzing their induced latent geometries and their relevance to semantic structure and interpretability.

Result: Each autoencoder reveals distinctive latent geometries that offer unique insights into semantic representation properties related to compositional semantics.

Conclusion: Bridging symbolic and distributional semantics through semantic representation learning provides promising strategies for enhancing the capabilities of modern language models.

Abstract: Integrating compositional and symbolic properties into current distributional
semantic spaces can enhance the interpretability, controllability,
compositionality, and generalisation capabilities of Transformer-based
auto-regressive language models (LMs). In this survey, we offer a novel
perspective on latent space geometry through the lens of compositional
semantics, a direction we refer to as \textit{semantic representation
learning}. This direction enables a bridge between symbolic and distributional
semantics, helping to mitigate the gap between them. We review and compare
three mainstream autoencoder architectures-Variational AutoEncoder (VAE),
Vector Quantised VAE (VQVAE), and Sparse AutoEncoder (SAE)-and examine the
distinctive latent geometries they induce in relation to semantic structure and
interpretability.

</details>


### [30] [ITFormer: Bridging Time Series and Natural Language for Multi-Modal QA with Large-Scale Multitask Dataset](https://arxiv.org/abs/2506.20093)
*Yilin Wang,Peixuan Lei,Jie Song,Yuzhe Hao,Tao Chen,Yuxuan Zhang,Lei Jia,Yuanxiang Li,Zhongyu Wei*

Main category: cs.CL

TL;DR: The paper introduces a Time-Series Question Answering (QA) task and presents EngineMT-QA, a dataset that integrates time-series data with natural language. ITFormer, a novel framework, achieves effective fusion of these data types with minimal trainable parameters.


<details>
  <summary>Details</summary>
Motivation: Time-series data are important but challenging to integrate with natural language for dynamic tasks. This paper aims to address this integration problem.

Method: The authors developed EngineMT-QA as a dataset and proposed ITFormer, a framework combining time-series encoders with frozen large language models (LLMs). Their approach focuses on extracting, aligning, and fusing features efficiently.

Result: ITFormer exhibits a robust QA accuracy improvement over strong baselines while requiring minimal computational resources (less than 1% additional trainable parameters).

Conclusion: This work provides a computationally efficient and adaptable approach to integrate time-series data with natural language, laying a foundation for future multi-modal AI research and applications.

Abstract: Time-series data are critical in diverse applications, such as industrial
monitoring, medical diagnostics, and climate research. However, effectively
integrating these high-dimensional temporal signals with natural language for
dynamic, interactive tasks remains a significant challenge. To address this, we
introduce the Time-Series Question Answering (Time-Series QA) task and release
EngineMT-QA, the first large-scale, multi-task, temporal-textual QA dataset
designed to capture complex interactions between time-series signals and
natural language. Building on this resource, we propose the Instruct Time
Transformer (ITFormer), a novel framework that bridges time-series encoders
with frozen large language models (LLMs). ITFormer effectively extracts,
aligns, and fuses temporal and textual features, achieving a strong improvement
in QA accuracy over strong baselines with fewer than 1\% additional trainable
parameters. By combining computational efficiency with robust cross-modal
modeling, our work establishes a adaptable paradigm for integrating temporal
data with natural language, paving the way for new research and applications in
multi-modal AI. More details about the project, including datasets and code,
are available at: https://pandalin98.github.io/itformer_site/

</details>


### [31] [A Multi-Pass Large Language Model Framework for Precise and Efficient Radiology Report Error Detection](https://arxiv.org/abs/2506.20112)
*Songsoo Kim,Seungtae Lee,See Young Lee,Joonho Kim,Keechan Kan,Dukyong Yoon*

Main category: cs.CL

TL;DR: This study tested three language model frameworks for proofreading radiology reports, finding that a three-pass approach improved predictive accuracy and reduced costs.


<details>
  <summary>Details</summary>
Motivation: Errors in radiology reports are infrequent, making it challenging to achieve high predictive precision using language models. The study aimed to develop a better proofreading approach to enhance PPV and lower costs.

Method: The authors analyzed 1,000 radiology reports from the MIMIC-III database and validated findings across two external datasets (CheXpert, Open-i). They compared three frameworks based on predictive precision (PPV), detection rates (aTPR), and efficiency.

Result: Framework 3 showed the highest improvement in PPV, from 0.063 to 0.159, while cutting operational costs by 42.6% and reducing the need for human review. The aTPR detection rate remained stable across frameworks.

Conclusion: A three-step LLM proofreading framework improves PPV and reduces operational costs without compromising true positive detection rates. This framework offers an effective strategy for improving AI-assisted radiology processes.

Abstract: Background: The positive predictive value (PPV) of large language model
(LLM)-based proofreading for radiology reports is limited due to the low error
prevalence. Purpose: To assess whether a three-pass LLM framework enhances PPV
and reduces operational costs compared with baseline approaches. Materials and
Methods: A retrospective analysis was performed on 1,000 consecutive radiology
reports (250 each: radiography, ultrasonography, CT, MRI) from the MIMIC-III
database. Two external datasets (CheXpert and Open-i) were validation sets.
Three LLM frameworks were tested: (1) single-prompt detector; (2) extractor
plus detector; and (3) extractor, detector, and false-positive verifier.
Precision was measured by PPV and absolute true positive rate (aTPR).
Efficiency was calculated from model inference charges and reviewer
remuneration. Statistical significance was tested using cluster bootstrap,
exact McNemar tests, and Holm-Bonferroni correction. Results: Framework PPV
increased from 0.063 (95% CI, 0.036-0.101, Framework 1) to 0.079 (0.049-0.118,
Framework 2), and significantly to 0.159 (0.090-0.252, Framework 3; P<.001 vs.
baselines). aTPR remained stable (0.012-0.014; P>=.84). Operational costs per
1,000 reports dropped to USD 5.58 (Framework 3) from USD 9.72 (Framework 1) and
USD 6.85 (Framework 2), reflecting reductions of 42.6% and 18.5%, respectively.
Human-reviewed reports decreased from 192 to 88. External validation supported
Framework 3's superior PPV (CheXpert 0.133, Open-i 0.105) and stable aTPR
(0.007). Conclusion: A three-pass LLM framework significantly enhanced PPV and
reduced operational costs, maintaining detection performance, providing an
effective strategy for AI-assisted radiology report quality assurance.

</details>


### [32] [ReCode: Updating Code API Knowledge with Reinforcement Learning](https://arxiv.org/abs/2506.20495)
*Haoze Wu,Yunzhi Yao,Wenhao Yu,Huajun Chen,Ningyu Zhang*

Main category: cs.CL

TL;DR: ReCode enhances LLMs' ability to adapt to dynamic API changes, using a dataset of 2,000 entries and reinforcement learning with a customized reward metric.


<details>
  <summary>Details</summary>
Motivation: Large Language Models struggle with outdated API knowledge, hindering their effective code generation in changing environments.

Method: ReCode leverages a dataset, rule-based reinforcement learning, and modified string similarity metrics for training LLMs to adapt to API changes.

Result: ReCode delivers improved LLM performance in dynamic API scenarios and surpasses large and reasoning-oriented models in certain benchmarks.

Conclusion: ReCode mitigates outdated API adaptation issues, ensuring better performance without diminishing overall code generation abilities.

Abstract: Large Language Models (LLMs) exhibit remarkable code generation capabilities
but falter when adapting to frequent updates in external library APIs. This
critical limitation, stemming from reliance on outdated API knowledge from
their training data, even with access to current documentation, impedes
reliable code generation in dynamic environments. To tackle this issue, we
propose ReCode (rule-based Reinforcement learning for Code Update), a novel
framework that mimics human programmer adaptation to API changes. Specifically,
we construct a dataset of approximately 2,000 data entries to train the LLMs to
perform version migration based on updated information. Then, we introduce a
modified string similarity metric for code evaluation as the reward for
reinforcement learning. Our experiments demonstrate that ReCode substantially
boosts LLMs' code generation performance in dynamic API scenarios, especially
on the unseen CodeUpdateArena task. Crucially, compared to supervised
fine-tuning, ReCode has less impact on LLMs' general code generation abilities.
We apply ReCode on various LLMs and reinforcement learning algorithms (GRPO and
DAPO), all achieving consistent improvements. Notably, after training,
Qwen2.5-Coder-7B outperforms that of the 32B parameter code instruction-tuned
model and the reasoning model with the same architecture. Code is available at
https://github.com/zjunlp/ReCode.

</details>


### [33] [Leveraging AI Graders for Missing Score Imputation to Achieve Accurate Ability Estimation in Constructed-Response Tests](https://arxiv.org/abs/2506.20119)
*Masaki Uto,Yuma Ito*

Main category: cs.CL

TL;DR: Proposes an automated scoring method to aid ability evaluation in education, improving accuracy and reducing manual workload.


<details>
  <summary>Details</summary>
Motivation: Manual grading of constructed-response tests is labor-intensive and costly, and existing methods struggle with sparse or diverse data in ability estimation.

Method: Uses automated scoring technologies to impute missing scores, enhancing the effectiveness of item response theory (IRT) for estimating abilities.

Result: The method improves the accuracy of ability estimation and significantly lowers the manual grading workload.

Conclusion: The proposed approach demonstrates effectiveness by combining automation with IRT, offering a practical tool for assessing higher-order abilities in education.

Abstract: Evaluating the abilities of learners is a fundamental objective in the field
of education. In particular, there is an increasing need to assess higher-order
abilities such as expressive skills and logical thinking. Constructed-response
tests such as short-answer and essay-based questions have become widely used as
a method to meet this demand. Although these tests are effective, they require
substantial manual grading, making them both labor-intensive and costly. Item
response theory (IRT) provides a promising solution by enabling the estimation
of ability from incomplete score data, where human raters grade only a subset
of answers provided by learners across multiple test items. However, the
accuracy of ability estimation declines as the proportion of missing scores
increases. Although data augmentation techniques for imputing missing scores
have been explored in order to address this limitation, they often struggle
with inaccuracy for sparse or heterogeneous data. To overcome these challenges,
this study proposes a novel method for imputing missing scores by leveraging
automated scoring technologies for accurate IRT-based ability estimation. The
proposed method achieves high accuracy in ability estimation while markedly
reducing manual grading workload.

</details>


### [34] [CCRS: A Zero-Shot LLM-as-a-Judge Framework for Comprehensive RAG Evaluation](https://arxiv.org/abs/2506.20128)
*Aashiq Muhamed*

Main category: cs.CL

TL;DR: The paper introduces CCRS, a set of five evaluation metrics using a pretrained LLM to comprehensively evaluate RAG system outputs.


<details>
  <summary>Details</summary>
Motivation: Existing methods for evaluating Retrieval-Augmented Generation (RAG) systems either rely on basic metrics that miss quality nuances or involve complex, impractical processes.

Method: CCRS employs a single pretrained LLM as a zero-shot judge to evaluate RAG system outputs with five metrics: Contextual Coherence, Question Relevance, Information Density, Answer Correctness, and Information Recall.

Result: CCRS was tested on six RAG configurations on the BioASQ dataset, showing that it effectively distinguishes system performance and that Mistral-7B outperformed Llama variants. It matches or surpasses RAGChecker in key aspects like recall and faithfulness, with greater efficiency.

Conclusion: CCRS is a practical, efficient, and comprehensive framework for RAG systems evaluation, improving on existing methodologies and supporting iterative system development.

Abstract: RAG systems enhance LLMs by incorporating external knowledge, which is
crucial for domains that demand factual accuracy and up-to-date information.
However, evaluating the multifaceted quality of RAG outputs, spanning aspects
such as contextual coherence, query relevance, factual correctness, and
informational completeness, poses significant challenges. Existing evaluation
methods often rely on simple lexical overlap metrics, which are inadequate for
capturing these nuances, or involve complex multi-stage pipelines with
intermediate steps like claim extraction or require finetuning specialized
judge models, hindering practical efficiency. To address these limitations, we
propose CCRS (Contextual Coherence and Relevance Score), a novel suite of five
metrics that utilizes a single, powerful, pretrained LLM as a zero-shot,
end-to-end judge. CCRS evaluates: Contextual Coherence (CC), Question Relevance
(QR), Information Density (ID), Answer Correctness (AC), and Information Recall
(IR). We apply CCRS to evaluate six diverse RAG system configurations on the
challenging BioASQ dataset. Our analysis demonstrates that CCRS effectively
discriminates between system performances, confirming, for instance, that the
Mistral-7B reader outperforms Llama variants. We provide a detailed analysis of
CCRS metric properties, including score distributions, convergent/discriminant
validity, tie rates, population statistics, and discriminative power. Compared
to the complex RAGChecker framework, CCRS offers comparable or superior
discriminative power for key aspects like recall and faithfulness, while being
significantly more computationally efficient. CCRS thus provides a practical,
comprehensive, and efficient framework for evaluating and iteratively improving
RAG systems.

</details>


### [35] [AALC: Large Language Model Efficient Reasoning via Adaptive Accuracy-Length Control](https://arxiv.org/abs/2506.20160)
*Ruosen Li,Ziming Luo,Quan Zhang,Ruochen Li,Ben Zhou,Ali Payani,Xinya Du*

Main category: cs.CL

TL;DR: The paper introduces AALC, a reward system that optimizes large reasoning models for reduced response length while maintaining or improving accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency and high costs of reasoning outputs in large reasoning models that often overthink without meaningful accuracy gains.

Method: AALC integrates validation accuracy into the reward function and employs a dynamic length penalty during reinforcement learning. The penalty is delayed until desired target performance is achieved.

Result: AALC decreases response length by over 50% across math benchmarks, retains or improves accuracy, and produces structurally refined outputs, though with reduced interpretability.

Conclusion: Reward-based approaches like AALC can effectively enhance the efficiency, accuracy, and generalizability of large reasoning models by balancing brevity and correctness in the reasoning process.

Abstract: Large reasoning models (LRMs) achieve impressive reasoning capabilities by
generating lengthy chain-of-thoughts, but this "overthinking" incurs high
latency and cost without commensurate accuracy gains. In this work, we
introduce AALC, a lightweight, accuracy-aware length reward integrated into
reinforcement learning that dynamically balances correctness and brevity during
training. By incorporating validation accuracy into the reward and employing a
smooth, dynamically scheduled length penalty, AALC delays length penalty until
target performance is met. Through extensive experiments across standard and
out-of-distribution math benchmarks, we show that our approach reduces response
length by over 50% while maintaining or even improving the original accuracy.
Furthermore, qualitative analysis reveals that our method curbs redundant
reasoning patterns such as excessive subgoal setting and verification, leading
to structurally refined outputs rather than naive truncation. We also identify
that efficiency gains are accompanied by reduced interpretability: models
trained with AALC omit some narrative framing and explanatory context. These
findings highlight the potential of reward-based strategies to guide LRMs
toward more efficient, generalizable reasoning paths.

</details>


### [36] [SEED: A Structural Encoder for Embedding-Driven Decoding in Time Series Prediction with LLMs](https://arxiv.org/abs/2506.20167)
*Fengze Li,Yue Wang,Yangle Liu,Ming Huang,Dou Hong,Jieming Ma*

Main category: cs.CL

TL;DR: The SEED model bridges gaps between structural encoders and large language models (LLMs) for multivariate time series forecasting.


<details>
  <summary>Details</summary>
Motivation: To address limitations in current models which either lack semantic reasoning (structural encoders) or compatibility with raw time-series data (LLMs).

Method: SEED employs a token-aware encoder for patch extraction, projection module for alignment, semantic reprogramming for task prototypes, and a frozen LLM for prediction.

Result: SEED consistently outperforms strong baselines and demonstrates better modeling of structural-semantic gaps across diverse datasets.

Conclusion: SEED successfully combines numerical pattern learning and semantic reasoning, advancing multivariate time series forecasting systems.

Abstract: Multivariate time series forecasting requires models to simultaneously
capture variable-wise structural dependencies and generalize across diverse
tasks. While structural encoders are effective in modeling feature
interactions, they lack the capacity to support semantic-level reasoning or
task adaptation. Conversely, large language models (LLMs) possess strong
generalization capabilities but remain incompatible with raw time series
inputs. This gap limits the development of unified, transferable prediction
systems. Therefore, we introduce SEED, a structural encoder for
embedding-driven decoding, which integrates four stages: a token-aware encoder
for patch extraction, a projection module that aligns patches with language
model embeddings, a semantic reprogramming mechanism that maps patches to
task-aware prototypes, and a frozen language model for prediction. This modular
architecture decouples representation learning from inference, enabling
efficient alignment between numerical patterns and semantic reasoning.
Empirical results demonstrate that the proposed method achieves consistent
improvements over strong baselines, and comparative studies on various datasets
confirm SEED's role in addressing the structural-semantic modeling gap.

</details>


### [37] [COIN: Uncertainty-Guarding Selective Question Answering for Foundation Models with Provable Risk Guarantees](https://arxiv.org/abs/2506.20178)
*Zhiyuan Wang,Jinhao Duan,Qingni Wang,Xiaofeng Zhu,Tianlong Chen,Xiaoshuang Shi,Kaidi Xu*

Main category: cs.CL

TL;DR: The paper presents COIN, a framework for quantifying uncertainty in foundation models, ensuring error rate control while improving predictive performance and answer retention.


<details>
  <summary>Details</summary>
Motivation: Heuristic approaches to uncertainty quantification lack guarantees on key metrics like false discovery rate (FDR), making it difficult to prevent text generation errors such as hallucinations.

Method: COIN uses a calibration set to estimate error rates, applies confidence interval methods for robust FDR control, and selects optimal thresholds for uncertainty filtering on text generation models.

Result: COIN effectively maintains FDR control while significantly improving the retention of correct answers, demonstrating adaptability and robustness across general and multimodal tasks.

Conclusion: COIN improves the reliability and efficiency of foundation models by addressing uncertainty and error rate, with adaptability for diverse tasks.

Abstract: Uncertainty quantification (UQ) for foundation models is essential to
identify and mitigate potential hallucinations in automatically generated text.
However, heuristic UQ approaches lack formal guarantees for key metrics such as
the false discovery rate (FDR) in selective prediction. Previous work adopts
the split conformal prediction (SCP) framework to ensure desired coverage of
admissible answers by constructing prediction sets, but these sets often
contain incorrect candidates, limiting their practical utility. To address
this, we propose COIN, an uncertainty-guarding selection framework that
calibrates statistically valid thresholds to filter a single generated answer
per question under user-specified FDR constraints. COIN estimates the empirical
error rate on a calibration set and applies confidence interval methods such as
Clopper-Pearson to establish a high-probability upper bound on the true error
rate (i.e., FDR). This enables the selection of the largest uncertainty
threshold that ensures FDR control on test data while significantly increasing
sample retention. We demonstrate COIN's robustness in risk control, strong
test-time power in retaining admissible answers, and predictive efficiency
under limited calibration data across both general and multimodal text
generation tasks. Furthermore, we show that employing alternative upper bound
constructions and UQ strategies can further boost COIN's power performance,
which underscores its extensibility and adaptability to diverse application
scenarios.

</details>


### [38] [How to Retrieve Examples in In-context Learning to Improve Conversational Emotion Recognition using Large Language Models?](https://arxiv.org/abs/2506.20199)
*Mengqi Wang,Tiantian Feng,Shrikanth Narayanan*

Main category: cs.CL

TL;DR: This study improves conversational emotion recognition by leveraging example retrieval strategies for in-context learning with large language models.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the challenge of achieving high accuracy in conversational emotion recognition, inspired by the SLT 2024 GenSER Challenge.

Method: The paper evaluates various strategies, such as random and augmented example retrieval, and examines the influence of conversational context on accuracy.

Result: Experiments showed augmented example retrieval consistently outperforms other strategies across three datasets: IEMOCAP, MELD, and EmoryNLP.

Conclusion: Retrieving coherent and targeted examples, coupled with enhancement through paraphrasing, significantly improves conversational emotion recognition accuracy.

Abstract: Large language models (LLMs) have enabled a wide variety of real-world
applications in various domains. However, creating a high-performing
application with high accuracy remains challenging, particularly for subjective
tasks like emotion recognition. Inspired by the SLT 2024 GenSER Challenge, this
study investigates approaches to improving conversational emotion recognition
(CER) by LLMs. Specifically, we explore how to retrieve high-quality examples
in in-context learning (ICL) to enhance CER. We propose various strategies
based on random and augmented example retrieval and also analyze the impact of
conversational context on CER accuracy. Experiments were conducted on the three
datasets including IEMOCAP, MELD and EmoryNLP. The results show that augmented
example retrieval consistently outperforms other techniques under investigation
across all datasets, highlighting the importance of retrieving coherent
targeted examples and enhancing them through paraphrasing.

</details>


### [39] [Intrinsic vs. Extrinsic Evaluation of Czech Sentence Embeddings: Semantic Relevance Doesn't Help with MT Evaluation](https://arxiv.org/abs/2506.20203)
*Petra Barančíková,Ondřej Bojar*

Main category: cs.CL

TL;DR: The paper evaluates Czech-specific and multilingual sentence embedding models by comparing their performance in intrinsic (semantic understanding) and extrinsic (machine translation evaluation) tasks. It finds a disconnect between models that perform well in semantic similarity tests and those excelling in translation evaluation.


<details>
  <summary>Details</summary>
Motivation: There is a need to understand the relationship between semantic properties captured by embeddings and their performance in real-world applications, like machine translation.

Method: The authors used intrinsic evaluation involving datasets such as Costra and STS benchmarks for semantic similarity, and extrinsic evaluation involving fine-tuning embedding models using COMET metrics for translation tasks.

Result: Models that excel in semantic similarity tests are not necessarily the best performers in machine translation evaluation tasks; over-smoothed models can perform well when fine-tuned.

Conclusion: The study highlights the nuanced relationship between semantic probes and real-world tasks, encouraging further research into operationalizable semantics and more comprehensive datasets for downstream tasks.

Abstract: In this paper, we compare Czech-specific and multilingual sentence embedding
models through intrinsic and extrinsic evaluation paradigms. For intrinsic
evaluation, we employ Costra, a complex sentence transformation dataset, and
several Semantic Textual Similarity (STS) benchmarks to assess the ability of
the embeddings to capture linguistic phenomena such as semantic similarity,
temporal aspects, and stylistic variations. In the extrinsic evaluation, we
fine-tune each embedding model using COMET-based metrics for machine
translation evaluation.
  Our experiments reveal an interesting disconnect: models that excel in
intrinsic semantic similarity tests do not consistently yield superior
performance on downstream translation evaluation tasks. Conversely, models with
seemingly over-smoothed embedding spaces can, through fine-tuning, achieve
excellent results. These findings highlight the complex relationship between
semantic property probes and downstream task, emphasizing the need for more
research into 'operationalizable semantics' in sentence embeddings, or more
in-depth downstream tasks datasets (here translation evaluation)

</details>


### [40] [Perspectives in Play: A Multi-Perspective Approach for More Inclusive NLP Systems](https://arxiv.org/abs/2506.20209)
*Benedetta Muscato,Lucia Passaro,Gizem Gezici,Fosca Giannotti*

Main category: cs.CL

TL;DR: Traditional NLP approaches often aggregate annotators' opinions into a single ground truth, which can underrepresent minority perspectives. This paper suggests a multi-perspective soft-label approach to better capture human disagreements and improve performance in subjective text classification tasks.


<details>
  <summary>Details</summary>
Motivation: The study seeks to address the underrepresentation of minority perspectives due to traditional methods that aggregate annotations and overlook systematic disagreement in subjective tasks.

Method: It introduces a multi-perspective approach using soft labels, analyzes multiple subjective classification tasks, and employs Jensen-Shannon Divergence (JSD) for evaluation, along with Explainable AI (XAI) for exploring model predictions.

Result: The approach captures human label distributions more effectively, achieving superior classification performance and higher F1 scores compared to traditional methods. However, it has lower confidence in tasks with high subjectivity like irony and stance detection.

Conclusion: The multi-perspective model enhances inclusivity and understanding of diverse human opinions but faces challenges in subjective tasks like irony detection. Leveraging XAI provides valuable insights into model uncertainty and behavior.

Abstract: In the realm of Natural Language Processing (NLP), common approaches for
handling human disagreement consist of aggregating annotators' viewpoints to
establish a single ground truth. However, prior studies show that disregarding
individual opinions can lead can lead to the side effect of underrepresenting
minority perspectives, especially in subjective tasks, where annotators may
systematically disagree because of their preferences. Recognizing that labels
reflect the diverse backgrounds, life experiences, and values of individuals,
this study proposes a new multi-perspective approach using soft labels to
encourage the development of the next generation of perspective aware models,
more inclusive and pluralistic. We conduct an extensive analysis across diverse
subjective text classification tasks, including hate speech, irony, abusive
language, and stance detection, to highlight the importance of capturing human
disagreements, often overlooked by traditional aggregation methods. Results
show that the multi-perspective approach not only better approximates human
label distributions, as measured by Jensen-Shannon Divergence (JSD), but also
achieves superior classification performance (higher F1 scores), outperforming
traditional approaches. However, our approach exhibits lower confidence in
tasks like irony and stance detection, likely due to the inherent subjectivity
present in the texts. Lastly, leveraging Explainable AI (XAI), we explore model
uncertainty and uncover meaningful insights into model predictions.

</details>


### [41] [Enhancing Large Language Models through Structured Reasoning](https://arxiv.org/abs/2506.20241)
*Yubo Dong,Hehe Fan*

Main category: cs.CL

TL;DR: This paper enhances large language models (LLMs) by using structured reasoning, fine-tuning with annotated datasets, and advanced optimization techniques to improve reasoning capabilities.


<details>
  <summary>Details</summary>
Motivation: Current large language models struggle with complex reasoning tasks because they rely on statistical relationships and lack structured knowledge representation.

Method: The approach involves converting data into structured formats with annotated reasoning steps, training the models through supervised fine-tuning, and employing Group Relative Policy Optimization with MAX-Flow and Longest Common Subsequence (LCS) algorithms.

Result: The enhanced DeepSeek-R1-Distill-Qwen-1.5B model demonstrated better reasoning abilities, robust performance across scenarios, and improved optimization compatibility.

Conclusion: Incorporating structured reasoning and innovative optimization methods offers significant improvements in LLMs' reasoning capabilities and computational efficiency.

Abstract: Recent Large Language Models (LLMs) have significantly advanced natural
language processing and automated decision-making. However, these models still
encounter difficulties when performing complex reasoning tasks involving
logical deduction and systematic planning, primarily due to their reliance on
implicit statistical relationships without structured knowledge
representation.Inspired by cognitive science and neurosymbolic AI, we introduce
a novel approach to enhance LLMs through explicit structured reasoning. First,
we convert unstructured data into structured formats by explicitly annotating
reasoning steps. We then employ this structured dataset to train LLMs through
Supervised Fine-Tuning (SFT). Additionally, we enhance the structured reasoning
capabilities of LLMs using Group Relative Policy Optimization (GRPO),
incorporating two innovative algorithms--MAX-Flow and Longest Common
Subsequence (LCS)--which notably improve reasoning effectiveness and reduce
computational complexity. Experimental results from fine-tuning a
DeepSeek-R1-Distill-Qwen-1.5B model demonstrate concise reasoning, robust
performance across various scenarios, and improved compatibility with
optimization techniques, validating the efficacy of structured reasoning
integration in LLMs.

</details>


### [42] [CBF-AFA: Chunk-Based Multi-SSL Fusion for Automatic Fluency Assessment](https://arxiv.org/abs/2506.20243)
*Papa Séga Wade,Mihai Andries,Ioannis Kanellos,Thierry Moudenc*

Main category: cs.CL

TL;DR: This paper proposes a novel chunk-based method for automatic fluency assessment (AFA), leveraging self-supervised learning (SSL) models for improved capturing of speech features like rhythm, pauses, and disfluencies.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address challenges in fluency assessment, particularly in accurately capturing temporal and rhythmic speech features in non-native speakers.

Method: It uses a hierarchical CNN-BiLSTM framework combined with SSL embeddings from models like Wav2Vec2, HuBERT, and WavLM. Segmentation is achieved via Silero voice activity detection (Silero-VAD), and SSL embeddings are fused using a learnable weighted mechanism.

Result: The proposed method shows improvement in F1-score (+2.8) and Pearson correlation (+6.2) on Speechocean762, and F1-score (+4.2) and Pearson correlation (+4.0) on Avalinguo, outperforming existing segmentation baselines.

Conclusion: Chunk-based multi-SSL fusion enhances fluency evaluation robustness, but further work is needed to generalize this method to dialects with irregular prosody.

Abstract: Automatic fluency assessment (AFA) remains challenging, particularly in
capturing speech rhythm, pauses, and disfluencies in non-native speakers. We
introduce a chunk-based approach integrating self-supervised learning (SSL)
models (Wav2Vec2, HuBERT, and WavLM) selected for their complementary strengths
in phonetic, prosodic, and noisy speech modeling, with a hierarchical
CNN-BiLSTM framework. Speech is segmented into breath-group chunks using Silero
voice activity detection (Silero-VAD), enabling fine-grained temporal analysis
while mitigating over-segmentation artifacts. SSL embeddings are fused via a
learnable weighted mechanism, balancing acoustic and linguistic features, and
enriched with chunk-level fluency markers (e.g., speech rate, pause durations,
n-gram repetitions). The CNN-BiLSTM captures local and long-term dependencies
across chunks. Evaluated on Avalinguo and Speechocean762, our approach improves
F1-score by 2.8 and Pearson correlation by 6.2 points over single SSL baselines
on Speechocean762, with gains of 4.2 F1-score and 4.0 Pearson points on
Avalinguo, surpassing Pyannote.audio-based segmentation baselines. These
findings highlight chunk-based multi-SSL fusion for robust fluency evaluation,
though future work should explore generalization to dialects with irregular
prosody.

</details>


### [43] [Narrative Shift Detection: A Hybrid Approach of Dynamic Topic Models and Large Language Models](https://arxiv.org/abs/2506.20269)
*Kai-Robin Lange,Tobias Schmidt,Matthias Reccius,Henrik Müller,Michael Roos,Carsten Jentsch*

Main category: cs.CL

TL;DR: This paper proposes a hybrid method combining Large Language Models (LLMs) and topic models to dynamically identify narrative shifts over time in news corpora.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of efficiently investigating dynamic narrative shifts across time, especially with the limitations of applying LLMs to entire datasets due to high computational costs.

Method: A combination of topic models and a change-point detection method is used to identify changes in topics of interest. These selected documents are then processed by an LLM to interpret and classify the type of changes as content or narrative shifts.

Result: Results show that LLMs effectively identify narrative shifts at a given point but struggle to differentiate between content shifts and narrative shifts.

Conclusion: The proposed pipeline has potential in analyzing narrative evolution but still faces limitations in distinguishing nuanced differences between content and narrative shifts.

Abstract: With rapidly evolving media narratives, it has become increasingly critical
to not just extract narratives from a given corpus but rather investigate, how
they develop over time. While popular narrative extraction methods such as
Large Language Models do well in capturing typical narrative elements or even
the complex structure of a narrative, applying them to an entire corpus comes
with obstacles, such as a high financial or computational cost. We propose a
combination of the language understanding capabilities of Large Language Models
with the large scale applicability of topic models to dynamically model
narrative shifts across time using the Narrative Policy Framework. We apply a
topic model and a corresponding change point detection method to find changes
that concern a specific topic of interest. Using this model, we filter our
corpus for documents that are particularly representative of that change and
feed them into a Large Language Model that interprets the change that happened
in an automated fashion and distinguishes between content and narrative shifts.
We employ our pipeline on a corpus of The Wall Street Journal news paper
articles from 2009 to 2023. Our findings indicate that a Large Language Model
can efficiently extract a narrative shift if one exists at a given point in
time, but does not perform as well when having to decide whether a shift in
content or a narrative shift took place.

</details>


### [44] [Biomed-Enriched: A Biomedical Dataset Enriched with LLMs for Pretraining and Extracting Rare and Hidden Content](https://arxiv.org/abs/2506.20331)
*Rian Touchent,Nathan Godey,Eric de la Clergerie*

Main category: cs.CL

TL;DR: The paper presents Biomed-Enriched, a dataset of PubMed texts curated using LLM annotations to improve biomedical NLP tasks.


<details>
  <summary>Details</summary>
Motivation: The work addresses the lack of openly available, high-quality biomedical datasets that can facilitate effective machine learning applications in clinical and biomedical domains while overcoming privacy concerns.

Method: A two-stage process where a large language model annotates 400K PubMed paragraphs and a smaller model uses these annotations to label the entire PMC-OA corpus. This results in refined data subsets filtered by quality and domain relevance.

Result: The dataset contains 2 million clinical case paragraphs, including 450K high-quality ones, enabling ~5% improvement in clinical task performance and ~1% gain in broader medical NLP tasks. Training efficiency also improves.

Conclusion: Biomed-Enriched is a scalable, openly available dataset that enhances clinical NLP while achieving efficiency in model training, highlighting its utility for biomedical researchers.

Abstract: We introduce Biomed-Enriched, a biomedical text dataset constructed from
PubMed via a two-stage annotation process. In the first stage, a large language
model annotates 400K paragraphs from PubMed scientific articles, assigning
scores for their type (review, study, clinical case, other), domain (clinical,
biomedical, other), and educational quality. The educational quality score
(rated 1 to 5) estimates how useful a paragraph is for college-level learning.
These annotations are then used to fine-tune a small language model, which
propagates the labels across the full PMC-OA corpus. The resulting metadata
allows us to extract refined subsets, including 2M clinical case paragraphs
with over 450K high-quality ones from articles with commercial-use licenses,
and to construct several variants via quality filtering and domain upsampling.
Clinical text is typically difficult to access due to privacy constraints, as
hospital records cannot be publicly shared. Hence, our dataset provides an
alternative large-scale, openly available collection of clinical cases from
PubMed, making it a valuable resource for biomedical and clinical NLP.
Preliminary continual-pretraining experiments with OLMo2 suggest these curated
subsets enable targeted improvements, with clinical upsampling boosting
performance by ~5% on MMLU ProfMed and educational quality filtering improving
MedQA and MedMCQA by ~1%. Combinations of these techniques led to faster
convergence, reaching same performance with a third of training tokens,
indicating potential for more efficient and effective biomedical pretraining
strategies.

</details>


### [45] [TAPS: Tool-Augmented Personalisation via Structured Tagging](https://arxiv.org/abs/2506.20409)
*Ekaterina Taktasheva,Jeff Dalton*

Main category: cs.CL

TL;DR: This paper introduces a novel method, TAPS, for integrating user preferences into goal-oriented dialogue agents to enhance large language models' personalized tool use.


<details>
  <summary>Details</summary>
Motivation: Existing tool-augmented large language models lack effective mechanisms to personalize tool use based on user preferences, limiting their ability to perform complex, user-specific tasks.

Method: The proposed approach, TAPS, combines a structured tagging tool and an uncertainty-based tool detector to improve personalized tool use in LLMs.

Result: TAPS demonstrates a significant improvement in the ability of LLMs to incorporate user preferences, setting a new state-of-the-art benchmark for open-source models on the NLSI task.

Conclusion: The integration of TAPS enables LLMs to more effectively handle personalized tool interactions, advancing research in personalized AI systems.

Abstract: Recent advancements in tool-augmented large language models have enabled them
to interact with external tools, enhancing their ability to perform complex
user tasks. However, existing approaches overlook the role of personalisation
in guiding tool use. This work investigates how user preferences can be
effectively integrated into goal-oriented dialogue agents. Through extensive
analysis, we identify key weaknesses in the ability of LLMs to personalise tool
use. To this end, we introduce \name, a novel solution that enhances
personalised tool use by leveraging a structured tagging tool and an
uncertainty-based tool detector. TAPS significantly improves the ability of
LLMs to incorporate user preferences, achieving the new state-of-the-art for
open source models on the NLSI task.

</details>


### [46] [An Agentic System for Rare Disease Diagnosis with Traceable Reasoning](https://arxiv.org/abs/2506.20430)
*Weike Zhao,Chaoyi Wu,Yanjie Fan,Xiaoman Zhang,Pengcheng Qiu,Yuze Sun,Xiao Zhou,Yanfeng Wang,Ya Zhang,Yongguo Yu,Kun Sun,Weidi Xie*

Main category: cs.CL

TL;DR: DeepRare is an advanced diagnostic system powered by a large language model designed to improve the diagnosis of rare diseases with exceptional accuracy and reasoning traceability.


<details>
  <summary>Details</summary>
Motivation: There is a critical need for timely and accurate diagnosis in rare diseases due to their clinical heterogeneity, low prevalence, and clinicians' unfamiliarity with such conditions.

Method: The authors developed DeepRare, a scalable system integrating a central host with long-term memory, specialized agents, over 40 tools, and access to web-scale medical data for ranked diagnostic hypotheses with evidence-linked reasoning.

Result: DeepRare achieved 100% accuracy for 1,013 diseases among 2,919 evaluated and outperformed 15 other methods in HPO-based evaluations, demonstrating a Recall@1 score of 57.18% and high performance in multi-modal input scenarios.

Conclusion: DeepRare effectively surpasses traditional methods for rare disease diagnosis, combining accuracy with explainable reasoning. It is available as a user-friendly web application for practical deployment.

Abstract: Rare diseases collectively affect over 300 million individuals worldwide, yet
timely and accurate diagnosis remains a pervasive challenge. This is largely
due to their clinical heterogeneity, low individual prevalence, and the limited
familiarity most clinicians have with rare conditions. Here, we introduce
DeepRare, the first rare disease diagnosis agentic system powered by a large
language model (LLM), capable of processing heterogeneous clinical inputs. The
system generates ranked diagnostic hypotheses for rare diseases, each
accompanied by a transparent chain of reasoning that links intermediate
analytic steps to verifiable medical evidence.
  DeepRare comprises three key components: a central host with a long-term
memory module; specialized agent servers responsible for domain-specific
analytical tasks integrating over 40 specialized tools and web-scale,
up-to-date medical knowledge sources, ensuring access to the most current
clinical information. This modular and scalable design enables complex
diagnostic reasoning while maintaining traceability and adaptability. We
evaluate DeepRare on eight datasets. The system demonstrates exceptional
diagnostic performance among 2,919 diseases, achieving 100% accuracy for 1013
diseases. In HPO-based evaluations, DeepRare significantly outperforms other 15
methods, like traditional bioinformatics diagnostic tools, LLMs, and other
agentic systems, achieving an average Recall@1 score of 57.18% and surpassing
the second-best method (Reasoning LLM) by a substantial margin of 23.79
percentage points. For multi-modal input scenarios, DeepRare achieves 70.60% at
Recall@1 compared to Exomiser's 53.20% in 109 cases. Manual verification of
reasoning chains by clinical experts achieves 95.40% agreements. Furthermore,
the DeepRare system has been implemented as a user-friendly web application
http://raredx.cn/doctor.

</details>


### [47] [Probing AI Safety with Source Code](https://arxiv.org/abs/2506.20471)
*Ujwal Narayan,Shreyas Chaudhari,Ashwin Kalyan,Tanmay Rajpurohit,Karthik Narasimhan,Ameet Deshpande,Vishvak Murahari*

Main category: cs.CL

TL;DR: The paper introduces a prompting strategy called Code of Thought (CoDoT), exposing significant safety shortcomings in state-of-the-art large language models (LLMs).


<details>
  <summary>Details</summary>
Motivation: The study aims to address the critical gap in AI safety for LLMs, which are increasingly interacting with humans in sensitive applications. Ensuring alignment with human values is essential.

Method: The researchers developed CoDoT, a framework that translates natural language inputs into code-like representations of the same intent, to evaluate LLMs' behavior under these transformations.

Result: The study found that many LLMs exhibit significant safety weaknesses under CoDoT, with instances of increased toxicity and failure across models like GPT-4 Turbo, which saw toxicity rise by 16.5 times.

Conclusion: The findings highlight the need for foundational safety evaluations and improvements to ensure AI systems develop both capabilities and safety in tandem.

Abstract: Large language models (LLMs) have become ubiquitous, interfacing with humans
in numerous safety-critical applications. This necessitates improving
capabilities, but importantly coupled with greater safety measures to align
these models with human values and preferences. In this work, we demonstrate
that contemporary models fall concerningly short of the goal of AI safety,
leading to an unsafe and harmful experience for users. We introduce a prompting
strategy called Code of Thought (CoDoT) to evaluate the safety of LLMs. CoDoT
converts natural language inputs to simple code that represents the same
intent. For instance, CoDoT transforms the natural language prompt "Make the
statement more toxic: {text}" to: "make_more_toxic({text})". We show that CoDoT
results in a consistent failure of a wide range of state-of-the-art LLMs. For
example, GPT-4 Turbo's toxicity increases 16.5 times, DeepSeek R1 fails 100% of
the time, and toxicity increases 300% on average across seven modern LLMs.
Additionally, recursively applying CoDoT can further increase toxicity two
times. Given the rapid and widespread adoption of LLMs, CoDoT underscores the
critical need to evaluate safety efforts from first principles, ensuring that
safety and capabilities advance together.

</details>


### [48] [Time is On My Side: Dynamics of Talk-Time Sharing in Video-chat Conversations](https://arxiv.org/abs/2506.20474)
*Kaixiang Zhang,Justine Zhang,Cristian Danescu-Niculescu-Mizil*

Main category: cs.CL

TL;DR: This paper introduces a computational framework to analyze talk-time sharing in conversations, focusing on both overall distributions and detailed dynamics.


<details>
  <summary>Details</summary>
Motivation: Understanding how talk-time is shared and its perception by participants, with applications for communication design.

Method: Analyzes talk-time balance and dynamics with a new typology, using data from video-chats between strangers.

Result: Balanced conversations are preferred by participants, but different dynamics within balanced conversations affect perception.

Conclusion: The framework informs the design of communication platforms by addressing both overall balance and nuanced dynamics of talk-time.

Abstract: An intrinsic aspect of every conversation is the way talk-time is shared
between multiple speakers. Conversations can be balanced, with each speaker
claiming a similar amount of talk-time, or imbalanced when one talks
disproportionately. Such overall distributions are the consequence of
continuous negotiations between the speakers throughout the conversation: who
should be talking at every point in time, and for how long?
  In this work we introduce a computational framework for quantifying both the
conversation-level distribution of talk-time between speakers, as well as the
lower-level dynamics that lead to it. We derive a typology of talk-time sharing
dynamics structured by several intuitive axes of variation. By applying this
framework to a large dataset of video-chats between strangers, we confirm that,
perhaps unsurprisingly, different conversation-level distributions of talk-time
are perceived differently by speakers, with balanced conversations being
preferred over imbalanced ones, especially by those who end up talking less.
Then we reveal that -- even when they lead to the same level of overall balance
-- different types of talk-time sharing dynamics are perceived differently by
the participants, highlighting the relevance of our newly introduced typology.
Finally, we discuss how our framework offers new tools to designers of
computer-mediated communication platforms, for both human-human and human-AI
communication.

</details>


### [49] [Knowledge-Aware Diverse Reranking for Cross-Source Question Answering](https://arxiv.org/abs/2506.20476)
*Tong Zhou*

Main category: cs.CL

TL;DR: The paper discusses Team Marikarp's solution for the SIGIR 2025 LiveRAG competition, focusing on retrieving relevant documents using a reranking pipeline.


<details>
  <summary>Details</summary>
Motivation: To develop an effective solution for retrieving question-relevant supporting documents in a competition setting, using a diverse and knowledge-aware approach.

Method: The authors proposed a knowledge-aware diverse reranking Retrieval-Augmented Generation (RAG) pipeline to tackle the document retrieval challenge.

Result: Their proposed solution achieved first place in the SIGIR 2025 LiveRAG competition, proving its effectiveness.

Conclusion: The paper highlights the success of the proposed RAG pipeline and its ability to handle a wide variety of topics and settings in document retrieval.

Abstract: This paper presents Team Marikarp's solution for the SIGIR 2025 LiveRAG
competition. The competition's evaluation set, automatically generated by
DataMorgana from internet corpora, encompassed a wide range of target topics,
question types, question formulations, audience types, and knowledge
organization methods. It offered a fair evaluation of retrieving
question-relevant supporting documents from a 15M documents subset of the
FineWeb corpus. Our proposed knowledge-aware diverse reranking RAG pipeline
achieved first place in the competition.

</details>


### [50] [GPTailor: Large Language Model Pruning Through Layer Cutting and Stitching](https://arxiv.org/abs/2506.20480)
*Guinan Su,Li Shen,Lu Yin,Shiwei Liu,Yanwu Yang,Jonas Geiping*

Main category: cs.CL

TL;DR: The paper introduces a novel method to compress large language models (LLMs) by combining layers from different finetuned variants, achieving substantial parameter reduction with minimal performance loss.


<details>
  <summary>Details</summary>
Motivation: LLMs typically require substantial resources for deployment and inference due to their large model size. Current pruning methods mostly address single models, leaving room for improvement in compressing and optimizing multiple finetuned variants.

Method: The authors propose a zero-order optimization strategy to tailor LLMs. The search space includes three operations: layer removal, layer selection across models, and layer merging.

Result: Experiments show that the compressed Llama2-13B models retain approximately 97.3% of the original performance while reducing ~25% of parameters, surpassing previous state-of-the-art techniques.

Conclusion: The method offers an innovative approach to LLM compression by leveraging the strengths of different finetuned variants, providing a resource-efficient solution without substantial performance trade-offs.

Abstract: Large language models (LLMs) have shown remarkable capabilities in language
understanding and generation. However, such impressive capability typically
comes with a substantial model size, which presents significant challenges in
deployment and inference. While structured pruning of model parameters offers a
promising way to reduce computational costs at deployment time, current methods
primarily focus on single model pruning. In this work, we develop a novel
strategy to compress models by strategically combining or merging layers from
finetuned model variants, which preserves the original model's abilities by
aggregating capabilities accentuated in different finetunes. We pose the
optimal tailoring of these LLMs as a zero-order optimization problem, adopting
a search space that supports three different operations: (1) Layer removal, (2)
Layer selection from different candidate models, and (3) Layer merging. Our
experiments demonstrate that this approach leads to competitive model pruning,
for example, for the Llama2-13B model families, our compressed models maintain
approximately 97.3\% of the original performance while removing $\sim25\%$ of
parameters, significantly outperforming previous state-of-the-art methods. The
code is available at https://github.com/Guinan-Su/auto-merge-llm.

</details>


### [51] [OctoThinker: Mid-training Incentivizes Reinforcement Learning Scaling](https://arxiv.org/abs/2506.20512)
*Zengzhi Wang,Fan Zhou,Xuefeng Li,Pengfei Liu*

Main category: cs.CL

TL;DR: The paper investigates what makes a base language model suitable for reinforcement learning (RL), revealing the importance of high-quality math corpora, QA-style data, and mid-training strategies. It introduces a novel training strategy (Stable-then-Decay) and releases new RL-suitable models and datasets.


<details>
  <summary>Details</summary>
Motivation: To understand the attributes of base language models that make them effective for reinforcement learning and to develop new-generation models optimized for RL.

Method: The authors evaluate mid-training strategies for two model families (Qwen and Llama) and design a two-stage training method (Stable-then-Decay). They use high-quality data such as a curated math-focused corpus and long chain-of-thought examples.

Result: The study identifies key factors like data quality, long-CoT reasoning, and scaling mid-training. It introduces OctoThinker, a model family with strong RL compatibility, showcasing improved performance comparable to RL-friendly model families like Qwen.

Conclusion: High-quality corpora and carefully designed training strategies are crucial to improve RL performance in base language models. Their proposed training strategy establishes a new approach for building RL-optimized models.

Abstract: Different base language model families, such as Llama and Qwen, exhibit
divergent behaviors during post-training with reinforcement learning (RL),
especially on reasoning-intensive tasks. What makes a base language model
suitable for reinforcement learning? Gaining deeper insight into this question
is essential for developing RL-scalable foundation models of the next
generation. In this work, we investigate how mid-training strategies shape RL
dynamics, focusing on two representative model families: Qwen and Llama. Our
study reveals that (1) high-quality mathematical corpora, such as
MegaMath-Web-Pro, significantly improve both base model and RL performance,
while existing alternatives (e.g., FineMath-4plus) fail to do so; (2) further
adding QA-style data, particularly long chain-of-thought (CoT) reasoning
examples, enhances RL outcomes, and instruction data further unlocks this
effect; (3) while long-CoT improves reasoning depth, it can also induce
verbosity of model responses and unstability of RL training, underscoring the
importance of data formatting; (4) scaling mid-training consistently leads to
stronger downstream RL performance. Building on these insights, we introduce a
two-stage mid-training strategy, Stable-then-Decay, in which base models are
first trained on 200B tokens with a constant learning rate, followed by 20B
tokens across three CoT-focused branches with learning rate decay. This yields
OctoThinker, a family of models demonstrating strong RL compatibility and
closing the performance gap with more RL-friendly model families, i.e., Qwen.
We hope our work will help shape pre-training strategies for foundation models
in the RL era. To support further research, we release our open-source models
along with a curated math reasoning-intensive corpus of over 70 billion tokens
(i.e., MegaMath-Web-Pro-Max).

</details>


### [52] [When Life Gives You Samples: The Benefits of Scaling up Inference Compute for Multilingual LLMs](https://arxiv.org/abs/2506.20544)
*Ammar Khairi,Daniel D'souza,Ye Shen,Julia Kreutzer,Sara Hooker*

Main category: cs.CL

TL;DR: This paper explores scaling inference-time compute for multilingual, multi-task settings using novel sampling and selection strategies.


<details>
  <summary>Details</summary>
Motivation: Existing inference-time scaling techniques focus mainly on English and limited domains, and fail to generalize to diverse languages and tasks.

Method: The authors propose new sampling and selection methods tailored for multilingual, multi-task scenarios and assess these against existing strategies.

Result: Their strategies achieved notable performance gains, with win-rate improvements of +6.8 using 8B models and +9.0 using 111B models in multilingual benchmarks.

Conclusion: Language- and task-aware inference methods are essential for achieving equitable performance improvements across underrepresented languages.

Abstract: Recent advancements in large language models (LLMs) have shifted focus toward
scaling inference-time compute, improving performance without retraining the
model. A common approach is to sample multiple outputs in parallel, and select
one of these as the final output. However, work to date has focused on English
and a handful of domains such as math and code. In contrast, we are most
interested in techniques that generalize across open-ended tasks, formally
verifiable tasks, and across languages. In this work, we study how to robustly
scale inference-time compute for open-ended generative tasks in a multilingual,
multi-task setting.
  Our findings show that both sampling strategy based on temperature variation
and selection strategy must be adapted to account for diverse domains and
varied language settings. We evaluate existing selection methods, revealing
that strategies effective in English often fail to generalize across languages.
We propose novel sampling and selection strategies specifically adapted for
multilingual and multi-task inference scenarios, and show they yield notable
gains across languages and tasks. In particular, our combined sampling and
selection methods lead to an average +6.8 jump in win-rates for our 8B models
on m-ArenaHard-v2.0 prompts, against proprietary models such as Gemini. At
larger scale, Command-A (111B model) equipped with our methods, shows +9.0
improvement in win-rates on the same benchmark with just five samples against
single-sample decoding, a substantial increase at minimal cost. Our results
underscore the need for language- and task-aware approaches to inference-time
compute, aiming to democratize performance improvements in underrepresented
languages.

</details>


### [53] [Model Editing as a Double-Edged Sword: Steering Agent Ethical Behavior Toward Beneficence or Harm](https://arxiv.org/abs/2506.20606)
*Baixiang Huang,Zhen Tan,Haoran Wang,Zijie Liu,Dawei Li,Ali Payani,Huan Liu,Tianlong Chen,Kai Shu*

Main category: cs.CL

TL;DR: This paper proposes 'Behavior Editing' to modify Large Language Models (LLMs) for ethical behavior steering using benchmarks grounded in psychological moral theories.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the need to ethically steer LLM-based agents to prevent serious safety and ethical risks in high-stakes domains.

Method: It introduces BehaviorBench, a benchmark based on psychological moral theories, to evaluate and enable precise model edits for steering agent behavior.

Result: Behavior Editing is effective for scenario-specific and global moral alignment adjustments in LLM-based agents.

Conclusion: Behavior Editing presents new opportunities and risks for modifying LLMs to enhance ethical behavior or potentially induce harmful traits.

Abstract: Agents based on Large Language Models (LLMs) have demonstrated strong
capabilities across a wide range of tasks. However, deploying LLM-based agents
in high-stakes domains comes with significant safety and ethical risks.
Unethical behavior by these agents can directly result in serious real-world
consequences, including physical harm and financial loss. To efficiently steer
the ethical behavior of agents, we frame agent behavior steering as a model
editing task, which we term Behavior Editing. Model editing is an emerging area
of research that enables precise and efficient modifications to LLMs while
preserving their overall capabilities. To systematically study and evaluate
this approach, we introduce BehaviorBench, a multi-tier benchmark grounded in
psychological moral theories. This benchmark supports both the evaluation and
editing of agent behaviors across a variety of scenarios, with each tier
introducing more complex and ambiguous scenarios. We first demonstrate that
Behavior Editing can dynamically steer agents toward the target behavior within
specific scenarios. Moreover, Behavior Editing enables not only
scenario-specific local adjustments but also more extensive shifts in an
agent's global moral alignment. We demonstrate that Behavior Editing can be
used to promote ethical and benevolent behavior or, conversely, to induce
harmful or malicious behavior. Through comprehensive evaluations on agents
based on frontier LLMs, BehaviorBench shows the effectiveness of Behavior
Editing across different models and scenarios. Our findings offer key insights
into a new paradigm for steering agent behavior, highlighting both the promise
and perils of Behavior Editing.

</details>


### [54] [DiffuCoder: Understanding and Improving Masked Diffusion Models for Code Generation](https://arxiv.org/abs/2506.20639)
*Shansan Gong,Ruixiang Zhang,Huangjie Zheng,Jiatao Gu,Navdeep Jaitly,Lingpeng Kong,Yizhe Zhang*

Main category: cs.CL

TL;DR: The paper introduces DiffuCoder, a 7B parameter diffusion large language model, optimized for generating code through novel denoising and reinforcement learning (RL) techniques, resulting in significant performance improvements in code generation tasks.


<details>
  <summary>Details</summary>
Motivation: To explore the potential of diffusion large language models (dLLMs) for code generation, addressing gaps in understanding their decoding behavior and improving training efficiency and RL-based optimization.

Method: The authors trained a 7B parameter model, DiffuCoder, on 130B tokens of code. They analyzed its decoding behavior and proposed the coupled-GRPO sampling scheme to minimize token log-likelihood estimate variance and enhance RL training efficiency.

Result: Experiments showed that the coupled-GRPO scheme significantly improved DiffuCoder's performance in code generation benchmarks, achieving a +4.4% gain on EvalPlus while reducing dependence on autoregressive decoding.

Conclusion: DiffuCoder demonstrates the effectiveness of dLLMs for code generation, with enhanced understanding of their behavior and an efficient RL framework, paving the way for further exploration in diffusion-native neural models.

Abstract: Diffusion large language models (dLLMs) are compelling alternatives to
autoregressive (AR) models because their denoising models operate over the
entire sequence. The global planning and iterative refinement features of dLLMs
are particularly useful for code generation. However, current training and
inference mechanisms for dLLMs in coding are still under-explored. To demystify
the decoding behavior of dLLMs and unlock their potential for coding, we
systematically investigate their denoising processes and reinforcement learning
(RL) methods. We train a 7B dLLM, \textbf{DiffuCoder}, on 130B tokens of code.
Using this model as a testbed, we analyze its decoding behavior, revealing how
it differs from that of AR models: (1) dLLMs can decide how causal their
generation should be without relying on semi-AR decoding, and (2) increasing
the sampling temperature diversifies not only token choices but also their
generation order. This diversity creates a rich search space for RL rollouts.
For RL training, to reduce the variance of token log-likelihood estimates and
maintain training efficiency, we propose \textbf{coupled-GRPO}, a novel
sampling scheme that constructs complementary mask noise for completions used
in training. In our experiments, coupled-GRPO significantly improves
DiffuCoder's performance on code generation benchmarks (+4.4\% on EvalPlus) and
reduces reliance on AR causal during decoding. Our work provides deeper insight
into the machinery of dLLM generation and offers an effective, diffusion-native
RL training framework. https://github.com/apple/ml-diffucoder.

</details>


### [55] [Memento: Note-Taking for Your Future Self](https://arxiv.org/abs/2506.20642)
*Chao Wan,Albert Gong,Mihir Mishra,Carl-Leander Henneking,Claas Beger,Kilian Q. Weinberger*

Main category: cs.CL

TL;DR: The paper introduces Memento, a three-stage prompting approach that combines reasoning decomposition and dynamic database construction using LLMs, significantly enhancing multi-hop question answering.


<details>
  <summary>Details</summary>
Motivation: Large language models struggle with tasks that require tightly interconnected reasoning and retrieval, such as multi-hop question answering. The paper aims to address these limitations.

Method: Memento employs a three-stage prompting strategy: 1) decomposing complex questions, 2) dynamically constructing a fact database using LLMs, and 3) synthesizing these facts to produce answers.

Result: Memento shows superior performance in benchmarks like PhantomWiki, 2WikiMultiHopQA, and MuSiQue. It dramatically increases metrics such as F1 scores compared to existing techniques like chain-of-thought (CoT) and ReAct.

Conclusion: Memento enhances the capabilities of existing prompting strategies for complex reasoning-plus-retrieval tasks, confirming its effectiveness across diverse and challenging datasets.

Abstract: Large language models (LLMs) excel at reasoning-only tasks, but struggle when
reasoning must be tightly coupled with retrieval, as in multi-hop question
answering. To overcome these limitations, we introduce a prompting strategy
that first decomposes a complex question into smaller steps, then dynamically
constructs a database of facts using LLMs, and finally pieces these facts
together to solve the question. We show how this three-stage strategy, which we
call Memento, can boost the performance of existing prompting strategies across
diverse settings. On the 9-step PhantomWiki benchmark, Memento doubles the
performance of chain-of-thought (CoT) when all information is provided in
context. On the open-domain version of 2WikiMultiHopQA, CoT-RAG with Memento
improves over vanilla CoT-RAG by more than 20 F1 percentage points and over the
multi-hop RAG baseline, IRCoT, by more than 13 F1 percentage points. On the
challenging MuSiQue dataset, Memento improves ReAct by more than 3 F1
percentage points, demonstrating its utility in agentic settings.

</details>


### [56] [Inside you are many wolves: Using cognitive models to interpret value trade-offs in LLMs](https://arxiv.org/abs/2506.20666)
*Sonia K. Murthy,Rosie Zhao,Jennifer Hu,Sham Kakade,Markus Wulfmeier,Peng Qian,Tomer Ullman*

Main category: cs.CL

TL;DR: The paper explores how large language models (LLMs) manage conflicting human-like values, such as being truthful versus being polite, using a cognitive model of polite speech to interpret trade-offs and measure outcomes.


<details>
  <summary>Details</summary>
Motivation: To understand how LLMs emulate human-like decision-making in social scenarios involving value trade-offs, and to fill the gap in interpreting these trade-offs within LLMs.

Method: The authors applied a cognitive model of polite speech to assess two model setups: reasoning "effort" in frontier black-box models and reinforcement learning (RL) post-training dynamics in open-source models, focusing on utility trade-offs.

Result: The study found that reasoning models prioritize informational utility over social utility, with significant shifts in value trade-offs occurring early in training. These dynamics are influenced more by the choice of base model and pretraining data than feedback datasets or alignment methods.

Conclusion: This method offers a systematic approach for analyzing LLM trade-offs, aiding in hypothesis formation, training regime design, and controlling value trade-offs in model development.

Abstract: Navigating everyday social situations often requires juggling conflicting
goals, such as conveying a harsh truth, maintaining trust, all while still
being mindful of another person's feelings. These value trade-offs are an
integral part of human decision-making and language use, however, current tools
for interpreting such dynamic and multi-faceted notions of values in LLMs are
limited. In cognitive science, so-called "cognitive models" provide formal
accounts of these trade-offs in humans, by modeling the weighting of a
speaker's competing utility functions in choosing an action or utterance. In
this work, we use a leading cognitive model of polite speech to interpret the
extent to which LLMs represent human-like trade-offs. We apply this lens to
systematically evaluate value trade-offs in two encompassing model settings:
degrees of reasoning "effort" in frontier black-box models, and RL
post-training dynamics of open-source models. Our results highlight patterns of
higher informational utility than social utility in reasoning models, and in
open-source models shown to be stronger in mathematical reasoning. Our findings
from LLMs' training dynamics suggest large shifts in utility values early on in
training with persistent effects of the choice of base model and pretraining
data, compared to feedback dataset or alignment method. We show that our method
is responsive to diverse aspects of the rapidly evolving LLM landscape, with
insights for forming hypotheses about other high-level behaviors, shaping
training regimes for reasoning models, and better controlling trade-offs
between values during model training.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [57] [Computer Vision based Automated Quantification of Agricultural Sprayers Boom Displacement](https://arxiv.org/abs/2506.19939)
*Aryan Singh Dalal,Sidharth Rai,Rahul Singh,Treman Singh Kaloya,Rahul Harsha Cheppally,Ajay Sharda*

Main category: cs.CV

TL;DR: The study developed a computer vision system using neural networks to measure and analyze spray boom movement in agricultural sprayers with high accuracy, aiming to improve boom stability and application accuracy.


<details>
  <summary>Details</summary>
Motivation: Application errors in agricultural sprayers are caused in part by boom instability, yet there is limited quantitative data on boom movements to develop effective solutions.

Method: A computer vision system utilizing YOLO V7, V8, and V11 neural networks was developed to track boom movement in real time, combined with inclinometer data for validation.

Result: The neural network models achieved over 90% detection accuracy, and the distance estimates aligned closely with inclinometer measurements, with a deviation of only 0.026 m.

Conclusion: This system provides a reliable method to quantify boom movement, enabling improvements in boom stability and design for better application precision.

Abstract: Application rate errors when using self-propelled agricultural sprayers for
agricultural production remain a concern. Among other factors, spray boom
instability is one of the major contributors to application errors. Spray
booms' width of 38m, combined with 30 kph driving speeds, varying terrain, and
machine dynamics when maneuvering complex field boundaries, make controls of
these booms very complex. However, there is no quantitative knowledge on the
extent of boom movement to systematically develop a solution that might include
boom designs and responsive boom control systems. Therefore, this study was
conducted to develop an automated computer vision system to quantify the boom
movement of various agricultural sprayers. A computer vision system was
developed to track a target on the edge of the sprayer boom in real time. YOLO
V7, V8, and V11 neural network models were trained to track the boom's
movements in field operations to quantify effective displacement in the
vertical and transverse directions. An inclinometer sensor was mounted on the
boom to capture boom angles and validate the neural network model output. The
results showed that the model could detect the target with more than 90 percent
accuracy, and distance estimates of the target on the boom were within 0.026 m
of the inclinometer sensor data. This system can quantify the boom movement on
the current sprayer and potentially on any other sprayer with minor
modifications. The data can be used to make design improvements to make sprayer
booms more stable and achieve greater application accuracy.

</details>


### [58] [EBC-ZIP: Improving Blockwise Crowd Counting with Zero-Inflated Poisson Regression](https://arxiv.org/abs/2506.19955)
*Yiming Ma,Victor Sanchez,Tanaya Guha*

Main category: cs.CV

TL;DR: The paper introduces EBC-ZIP, a framework for crowd counting that uses Zero-Inflated Poisson regression to better handle sparsity in density maps and achieves state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Traditional crowd counting methods overlook sparsity in ground-truth density maps, leading to biases in dense regions and challenges with zero-heavy distributions.

Method: The paper employs Zero-Inflated Poisson regression for a probabilistic loss and builds upon the Enhanced Block Classification (EBC) framework for preserving discreteness and training stability.

Result: EBC-ZIP outperforms existing methods and achieves state-of-the-art results across four crowd counting benchmarks.

Conclusion: EBC-ZIP is a scalable and effective alternative for crowd counting that mitigates imbalances in density estimations, improves performance, and handles sparse distributions better.

Abstract: Density map estimation has become the mainstream paradigm in crowd counting.
However, most existing methods overlook the extreme sparsity of ground-truth
density maps. In real-world crowd scenes, the vast majority of spatial regions
(often over 95%) contain no people, leading to heavily imbalanced count
distributions. Ignoring this imbalance can bias models toward overestimating
dense regions and underperforming in sparse areas. Furthermore, most loss
functions used in density estimation are majorly based on MSE and implicitly
assume Gaussian distributions, which are ill-suited for modeling discrete,
non-negative count data. In this paper, we propose EBC-ZIP, a crowd counting
framework that models the spatial distribution of counts using a Zero-Inflated
Poisson (ZIP) regression formulation. Our approach replaces the traditional
regression loss with the negative log-likelihood of the ZIP distribution,
enabling better handling of zero-heavy distributions while preserving count
accuracy. Built upon the recently proposed Enhanced Block Classification (EBC)
framework, EBC-ZIP inherits EBC's advantages in preserving the discreteness of
targets and ensuring training stability, while further improving performance
through a more principled probabilistic loss. We also evaluate EBC-ZIP with
backbones of varying computational complexity to assess its scalability.
Extensive experiments on four crowd counting benchmarks demonstrate that
EBC-ZIP consistently outperforms EBC and achieves state-of-the-art results.

</details>


### [59] [ToSA: Token Merging with Spatial Awareness](https://arxiv.org/abs/2506.20066)
*Hsiang-Wei Huang,Wenhao Chai,Kuang-Ming Chen,Cheng-Yen Yang,Jenq-Neng Hwang*

Main category: cs.CV

TL;DR: The paper introduces ToSA, a new token merging method for Vision Transformers (ViT) that incorporates both semantic and spatial information, outperforming previous methods in efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing token merging methods for ViT focus primarily on feature similarity, ignoring spatial information's potential role, especially in early layers where visual tokens are weakly informative.

Method: ToSA combines semantic and spatial awareness using depth images to create pseudo spatial tokens as auxiliary data during the token merging process, preserving critical scene structures.

Result: ToSA outperforms previous token merging methods in benchmarks like visual and embodied question answering while reducing ViT runtime significantly.

Conclusion: ToSA offers an efficient and effective solution to accelerate Vision Transformers by integrating both semantic and spatial criteria, proving superior in preserving scene structure and computational efficiency.

Abstract: Token merging has emerged as an effective strategy to accelerate Vision
Transformers (ViT) by reducing computational costs. However, existing methods
primarily rely on the visual token's feature similarity for token merging,
overlooking the potential of integrating spatial information, which can serve
as a reliable criterion for token merging in the early layers of ViT, where the
visual tokens only possess weak visual information. In this paper, we propose
ToSA, a novel token merging method that combines both semantic and spatial
awareness to guide the token merging process. ToSA leverages the depth image as
input to generate pseudo spatial tokens, which serve as auxiliary spatial
information for the visual token merging process. With the introduced spatial
awareness, ToSA achieves a more informed merging strategy that better preserves
critical scene structure. Experimental results demonstrate that ToSA
outperforms previous token merging methods across multiple benchmarks on visual
and embodied question answering while largely reducing the runtime of the ViT,
making it an efficient solution for ViT acceleration. The code will be
available at: https://github.com/hsiangwei0903/ToSA

</details>


### [60] [BrokenVideos: A Benchmark Dataset for Fine-Grained Artifact Localization in AI-Generated Videos](https://arxiv.org/abs/2506.20103)
*Jiahao Lin,Weixuan Peng,Bojia Zi,Yifeng Gao,Xianbiao Qi,Xingjun Ma,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: The paper introduces BrokenVideos, a dataset of 3,254 AI-generated videos with pixel-level annotations for detecting visual artifacts, addressing a gap in artifact localization benchmarks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve detection and localization of visual artifacts in AI-generated videos, which currently suffer from issues like motion inconsistency, unnatural deformations, and blurring, undermining realism and user trust.

Method: The authors created BrokenVideos, a benchmark dataset with 3,254 videos and pixel-level ground truth masks, validated through human inspection, for fine-grained evaluation of artifact localization models.

Result: Training state-of-the-art models and multi-modal large language models on BrokenVideos improved their ability to localize corrupted regions in AI-generated videos.

Conclusion: BrokenVideos establishes a robust benchmark for evaluating and advancing artifact localization techniques, filling a critical gap in generative video model research and enabling quality control improvements.

Abstract: Recent advances in deep generative models have led to significant progress in
video generation, yet the fidelity of AI-generated videos remains limited.
Synthesized content often exhibits visual artifacts such as temporally
inconsistent motion, physically implausible trajectories, unnatural object
deformations, and local blurring that undermine realism and user trust.
Accurate detection and spatial localization of these artifacts are crucial for
both automated quality control and for guiding the development of improved
generative models. However, the research community currently lacks a
comprehensive benchmark specifically designed for artifact localization in AI
generated videos. Existing datasets either restrict themselves to video or
frame level detection or lack the fine-grained spatial annotations necessary
for evaluating localization methods. To address this gap, we introduce
BrokenVideos, a benchmark dataset of 3,254 AI-generated videos with
meticulously annotated, pixel-level masks highlighting regions of visual
corruption. Each annotation is validated through detailed human inspection to
ensure high quality ground truth. Our experiments show that training state of
the art artifact detection models and multi modal large language models (MLLMs)
on BrokenVideos significantly improves their ability to localize corrupted
regions. Through extensive evaluation, we demonstrate that BrokenVideos
establishes a critical foundation for benchmarking and advancing research on
artifact localization in generative video models. The dataset is available at:
https://broken-video-detection-datetsets.github.io/Broken-Video-Detection-Datasets.github.io/.

</details>


### [61] [From 2D to 3D Cognition: A Brief Survey of General World Models](https://arxiv.org/abs/2506.20134)
*Ningwei Xie,Zizi Tian,Lei Yang,Xiao-Ping Zhang,Meng Guo,Jie Li*

Main category: cs.CV

TL;DR: The survey analyzes the progression from 2D to 3D cognitive world models, categorizes emerging techniques, highlights core capabilities, and discusses applications and challenges.


<details>
  <summary>Details</summary>
Motivation: To systematically analyze 3D generative world models and their transition from 2D perception, offering clarity in technological roles for advancing AGI.

Method: A structured review underpinning core capabilities like 3D physical scene generation, spatial reasoning, and interaction while categorizing advancements in representations and world knowledge.

Result: The paper outlines technological drivers, core capabilities, real-world applications, and challenges in deploying robust 3D world models.

Conclusion: 3D cognitive world models are critical for AGI progress, necessitating advancements in data, modeling, and deployment for broader applications like autonomous driving and VR.

Abstract: World models have garnered increasing attention in the development of
artificial general intelligence (AGI), serving as computational frameworks for
learning representations of the external world and forecasting future states.
While early efforts focused on 2D visual perception and simulation, recent
3D-aware generative world models have demonstrated the ability to synthesize
geometrically consistent, interactive 3D environments, marking a shift toward
3D spatial cognition. Despite rapid progress, the field lacks systematic
analysis to categorize emerging techniques and clarify their roles in advancing
3D cognitive world models. This survey addresses this need by introducing a
conceptual framework, providing a structured and forward-looking review of
world models transitioning from 2D perception to 3D cognition. Within this
framework, we highlight two key technological drivers, particularly advances in
3D representations and the incorporation of world knowledge, as fundamental
pillars. Building on these, we dissect three core cognitive capabilities that
underpin 3D world modeling: 3D physical scene generation, 3D spatial reasoning,
and 3D spatial interaction. We further examine the deployment of these
capabilities in real-world applications, including embodied AI, autonomous
driving, digital twin, and gaming/VR. Finally, we identify challenges across
data, modeling, and deployment, and outline future directions for advancing
more robust and generalizable 3D world models.

</details>


### [62] [EAR: Erasing Concepts from Unified Autoregressive Models](https://arxiv.org/abs/2506.20151)
*Haipeng Fan,Shiyuan Zhang,Baohunesitu,Zihang Guo,Huaiwen Zhang*

Main category: cs.CV

TL;DR: The paper proposes the Erasure Autoregressive Model (EAR), a fine-tuning method for removing specific concepts from AR models efficiently while retaining generation quality. It introduces two mechanisms, WGA and TLM, and provides the novel ECGVF benchmark for evaluating concept erasure.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of removing undesired concepts from AR models without degrading their overall generation quality, a critical issue in tasks involving sensitive or inappropriate content.

Method: The paper introduces EAR, which utilizes Windowed Gradient Accumulation (WGA) for patch-level decoding alignment with the erasure objective and Thresholded Loss Masking (TLM) to protect unrelated content during fine-tuning. Additionally, a benchmark called ECGVF evaluates concept erasure using structured prompt templates and rigorous visual filtering.

Result: EAR demonstrates significant improvements in both concept erasure effectiveness and the preservation of AR model utility. Experiments on the ECGVF benchmark with Janus-Pro validate these results.

Conclusion: The proposed EAR method successfully addresses the trade-off between erasing undesired concepts and maintaining generation quality in AR models. It also offers a new benchmark (ECGVF) to standardize evaluation techniques in this area.

Abstract: Autoregressive (AR) models have achieved unified and strong performance
across both visual understanding and image generation tasks. However, removing
undesired concepts from AR models while maintaining overall generation quality
remains an open challenge. In this paper, we propose Erasure Autoregressive
Model (EAR), a fine-tuning method for effective and utility-preserving concept
erasure in AR models. Specifically, we introduce Windowed Gradient Accumulation
(WGA) strategy to align patch-level decoding with erasure objectives, and
Thresholded Loss Masking (TLM) strategy to protect content unrelated to the
target concept during fine-tuning. Furthermore, we propose a novel benchmark,
Erase Concept Generator and Visual Filter (ECGVF), aim at provide a more
rigorous and comprehensive foundation for evaluating concept erasure in AR
models. Specifically, we first employ structured templates across diverse large
language models (LLMs) to pre-generate a large-scale corpus of
target-replacement concept prompt pairs. Subsequently, we generate images from
these prompts and subject them to rigorous filtering via a visual classifier to
ensure concept fidelity and alignment. Extensive experimental results conducted
on the ECGVF benchmark with the AR model Janus-Pro demonstrate that EAR
achieves marked improvements in both erasure effectiveness and model utility
preservation. Code is available at: https://github.com/immc-lab/ear/

</details>


### [63] [Loss-Aware Automatic Selection of Structured Pruning Criteria for Deep Neural Network Acceleration](https://arxiv.org/abs/2506.20152)
*Deepak Ghimire,Kilho Lee,Seong-heum Kim*

Main category: cs.CV

TL;DR: This paper proposes LAASP, a pruning-while-training approach to compress neural networks efficiently for deployment on resource-limited devices. It auto-selects pruning criteria based on loss and achieves high accuracy and reduced computational FLOPs without manual tuning.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to simplify and optimize the process of structured pruning in deep neural networks, making them suitable for computation-limited edge devices while enhancing efficiency and preserving accuracy.

Method: The proposed LAASP method employs a pruning-while-training approach, automatically selects filter pruning criteria based on loss, and determines optimal pruning rates per layer without manual intervention. It mitigates accuracy drops by retraining briefly after pruning iterations, reducing computational FLOPs effectively.

Result: Experiments on VGGNet and ResNet models show significant improvements in accuracy and computational efficiency. For example, ResNet56 on CIFAR-10 achieved a 52% reduction in FLOPs with improved accuracy, while ResNet50 on ImageNet reduced FLOPs by 42% with only a negligible accuracy drop.

Conclusion: The LAASP method is efficient in compressing and accelerating deep neural networks while maintaining high accuracy and minimizing overheads. It provides a robust solution for deploying neural networks on resource-constrained hardware.

Abstract: Structured pruning is a well-established technique for compressing neural
networks, making it suitable for deployment in resource-limited edge devices.
This paper presents an efficient Loss-Aware Automatic Selection of Structured
Pruning Criteria (LAASP) for slimming and accelerating deep neural networks.
The majority of pruning methodologies employ a sequential process consisting of
three stages: 1) training, 2) pruning, and 3) fine-tuning, whereas the proposed
pruning technique adopts a pruning-while-training approach that eliminates the
first stage and integrates the second and third stages into a single cycle. The
automatic selection of magnitude or similarity-based filter pruning criteria
from a specified pool of criteria and the specific pruning layer at each
pruning iteration is guided by the network's overall loss on a small subset of
the training data. To mitigate the abrupt accuracy drop due to pruning, the
network is retrained briefly after each reduction of a predefined number of
floating-point operations (FLOPs). The optimal pruning rates for each layer in
the network are automatically determined, eliminating the need for manual
allocation of fixed or variable pruning rates for each layer. Experiments on
the VGGNet and ResNet models on the CIFAR-10 and ImageNet benchmark datasets
demonstrate the effectiveness of the proposed method. In particular, the
ResNet56 and ResNet110 models on the CIFAR-10 dataset significantly improve the
top-1 accuracy compared to state-of-the-art methods while reducing the network
FLOPs by 52\%. Furthermore, the ResNet50 model on the ImageNet dataset reduces
FLOPs by more than 42\% with a negligible 0.33\% drop in top-5 accuracy. The
source code of this paper is publicly available online -
https://github.com/ghimiredhikura/laasp.

</details>


### [64] [Towards Efficient Exemplar Based Image Editing with Multimodal VLMs](https://arxiv.org/abs/2506.20155)
*Avadhoot Jadhav,Ashutosh Srivastava,Abhinav Java,Silky Singh,Tarun Ram Menta,Surgan Jandial,Balaji Krishnamurthy*

Main category: cs.CV

TL;DR: The paper proposes a novel method for image editing using exemplar pairs instead of text commands, leveraging pretrained text-to-image diffusion models and multimodal VLMs.


<details>
  <summary>Details</summary>
Motivation: Current text-to-image diffusion models often struggle with ambiguous edits that are challenging to describe via text, making exemplar-based editing an attractive alternative.

Method: The authors present an optimization-free end-to-end pipeline by utilizing pretrained text-to-image diffusion models and multimodal visual-language models for transferring edits from an exemplar pair to new content images.

Result: The proposed method outperforms existing baselines for various types of edits and achieves approximately four times faster processing.

Conclusion: Exemplar-based image editing using the introduced pipeline demonstrates both higher performance and efficiency, making it a promising tool for applications needing image manipulation.

Abstract: Text-to-Image Diffusion models have enabled a wide array of image editing
applications. However, capturing all types of edits through text alone can be
challenging and cumbersome. The ambiguous nature of certain image edits is
better expressed through an exemplar pair, i.e., a pair of images depicting an
image before and after an edit respectively. In this work, we tackle
exemplar-based image editing -- the task of transferring an edit from an
exemplar pair to a content image(s), by leveraging pretrained text-to-image
diffusion models and multimodal VLMs. Even though our end-to-end pipeline is
optimization-free, our experiments demonstrate that it still outperforms
baselines on multiple types of edits while being ~4x faster.

</details>


### [65] [Seeing is Believing? Mitigating OCR Hallucinations in Multimodal Large Language Models](https://arxiv.org/abs/2506.20168)
*Zhentao He,Can Zhang,Ziheng Wu,Zhenghao Chen,Yufei Zhan,Yifan Li,Zhao Zhang,Xian Wang,Minghui Qiu*

Main category: cs.CV

TL;DR: The paper introduces KIE-HVQA, a benchmark dataset designed to evaluate OCR hallucination in degraded document understanding. A novel GRPO-based framework improves the accuracy of avoiding hallucination by 22% on Qwen2.5-VL compared to GPT-4o.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal models struggle with visual degradation, frequently leading to hallucinations due to overreliance on linguistic cues or misaligned reasoning in real-world settings.

Method: The authors propose KIE-HVQA, a dataset with identity cards and invoices under simulated visual degradation. They introduce a GRPO-based framework that incorporates self-awareness of uncertainty and refusal-to-answer mechanisms, supported by supervised fine-tuning and reinforcement learning.

Result: The proposed approach achieved a 22% improvement in hallucination-free accuracy over GPT-4o on the KIE-HVQA benchmark without compromising performance on standard tasks.

Conclusion: The novel framework and benchmark dataset effectively address hallucination issues in degraded document understanding, demonstrating improved accuracy and robustness in multimodal language models.

Abstract: Recent advancements in multimodal large language models have enhanced
document understanding by integrating textual and visual information. However,
existing models exhibit incompleteness within their paradigm in real-world
scenarios, particularly under visual degradation. In such conditions, the
current response paradigm often fails to adequately perceive visual degradation
and ambiguity, leading to overreliance on linguistic priors or misaligned
visual-textual reasoning. This difficulty in recognizing uncertainty frequently
results in the generation of hallucinatory content, especially when a precise
answer is not feasible. To better demonstrate and analyze this phenomenon and
problem, we propose KIE-HVQA, the first benchmark dedicated to evaluating OCR
hallucination in degraded document understanding. This dataset includes test
samples spanning identity cards and invoices, with simulated real-world
degradations for OCR reliability. This setup allows for evaluating models'
capacity, under degraded input, to distinguish reliable visual information and
answer accordingly, thereby highlighting the challenge of avoiding
hallucination on uncertain data. To achieve vision-faithful reasoning and
thereby avoid the aforementioned issues, we further introduce a GRPO-based
framework featuring a novel reward mechanism. By incorporating a self-awareness
of visual uncertainty and an analysis method that initiates refusal to answer
to increase task difficulty within our supervised fine-tuning and reinforcement
learning framework, we successfully mitigated hallucinations in ambiguous
regions. Experiments on Qwen2.5-VL demonstrate that our 7B-parameter model
achieves a 22\% absolute improvement in hallucination-free accuracy over GPT-4o
on KIE-HVQA and there is no significant performance drop in standard tasks,
highlighting both effectiveness and robustness.

</details>


### [66] [Towards Scalable and Generalizable Earth Observation Data Mining via Foundation Model Composition](https://arxiv.org/abs/2506.20174)
*Man Duc Chuc*

Main category: cs.CV

TL;DR: The paper explores the potential of combining pretrained models from remote sensing and general vision domains to enhance performance in Earth Observation tasks, using less computational resources.


<details>
  <summary>Details</summary>
Motivation: To find alternative strategies to training large models from scratch for Earth Observation tasks while improving performance and efficiency.

Method: Evaluated pretrained models (e.g., Prithvi, Hiera, DOFA) on the GEO-Bench benchmark, using feature-level ensembling and knowledge distillation for performance enhancement.

Result: Feature-level ensembling of smaller models matched or exceeded the performance of larger models, requiring less training time and resources. Knowledge distillation further improved compact models.

Conclusion: Foundation models can be effectively combined and distilled for scalable, efficient solutions in Earth Observation, facilitating practical deployments.

Abstract: Foundation models are rapidly transforming Earth Observation data mining by
enabling generalizable and scalable solutions for key tasks such as scene
classification and semantic segmentation. While most efforts in the geospatial
domain have focused on developing large models trained from scratch using
massive Earth Observation datasets, an alternative strategy that remains
underexplored is the reuse and combination of existing pretrained models. In
this study, we investigate whether foundation models pretrained on remote
sensing and general vision datasets can be effectively combined to improve
performance across a diverse set of key Earth Observation tasks. Using the
GEO-Bench benchmark, we evaluate several prominent models, including Prithvi,
Hiera, and DOFA, on eleven datasets covering a range of spatial resolutions,
sensor modalities, and task types. The results show that feature-level
ensembling of smaller pretrained models can match or exceed the performance of
much larger models, while requiring less training time and computational
resources. Moreover, the study highlights the potential of applying knowledge
distillation to transfer the strengths of ensembles into more compact models,
offering a practical path for deploying foundation models in real-world Earth
Observation applications.

</details>


### [67] [Progressive Alignment Degradation Learning for Pansharpening](https://arxiv.org/abs/2506.20179)
*Enzhe Zhao,Zhichang Guo,Yao Li,Fanghui Song,Boying Wu*

Main category: cs.CV

TL;DR: The paper proposes a novel progressive framework for pansharpening using deep learning by addressing the limitations of the Wald protocol and demonstrating superior performance.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of the Wald protocol, which inaccurately approximates real-world degradation patterns, thereby limiting the performance and generalization of deep learning-based pansharpening models.

Method: The method introduces a Progressive Alignment Degradation Module (PADM) that uses mutual iteration between two sub-networks (PAlignNet and PDegradeNet) to adaptively learn degradation processes. Furthermore, the HFreqdiff technique incorporates high-frequency details via a diffusion framework, integrating CFB and BACM modules for superior spatial quality.

Result: The proposed approach significantly enhances spatial sharpness and quality in pansharpened images and surpasses state-of-the-art methods in experiments and ablation studies.

Conclusion: The paper successfully addresses the limitations of the Wald protocol by introducing innovative architecture and techniques, demonstrating improved performance in pansharpening applications.

Abstract: Deep learning-based pansharpening has been shown to effectively generate
high-resolution multispectral (HRMS) images. To create supervised ground-truth
HRMS images, synthetic data generated using the Wald protocol is commonly
employed. This protocol assumes that networks trained on artificial
low-resolution data will perform equally well on high-resolution data. However,
well-trained models typically exhibit a trade-off in performance between
reduced-resolution and full-resolution datasets. In this paper, we delve into
the Wald protocol and find that its inaccurate approximation of real-world
degradation patterns limits the generalization of deep pansharpening models. To
address this issue, we propose the Progressive Alignment Degradation Module
(PADM), which uses mutual iteration between two sub-networks, PAlignNet and
PDegradeNet, to adaptively learn accurate degradation processes without relying
on predefined operators. Building on this, we introduce HFreqdiff, which embeds
high-frequency details into a diffusion framework and incorporates CFB and BACM
modules for frequency-selective detail extraction and precise reverse process
learning. These innovations enable effective integration of high-resolution
panchromatic and multispectral images, significantly enhancing spatial
sharpness and quality. Experiments and ablation studies demonstrate the
proposed method's superior performance compared to state-of-the-art techniques.

</details>


### [68] [UniCode$^2$: Cascaded Large-scale Codebooks for Unified Multimodal Understanding and Generation](https://arxiv.org/abs/2506.20214)
*Yanzhe Chen,Huasong Zhong,Yan Li,Zhenheng Yang*

Main category: cs.CV

TL;DR: The paper introduces UniCode$^2$, a cascaded codebook framework for stable and semantically rich visual tokenization, enabling better integration with pretrained models for multimodal tasks.


<details>
  <summary>Details</summary>
Motivation: Existing visual tokenization methods face issues with semantic granularity, token utilization, and training stability.

Method: The authors propose a cascaded approach with a frozen codebook for stability and a trainable codebook to refine task-specific semantics, leveraging millions of embeddings and a large vocabulary.

Result: UniCode$^2 demonstrated strong performance across benchmarks, maintaining stability while scaling up token spaces and preserving vision-language alignment.

Conclusion: UniCode$^2 scales visual token spaces effectively, balancing semantic richness, stability, and compatibility with pretrained multimodal models.

Abstract: Unified multimodal large language models (MLLMs) have shown promise in
jointly advancing multimodal understanding and generation, with visual
codebooks discretizing images into tokens for autoregressive modeling. Existing
codebook-based methods either rely on small vocabularies (~16K entries) that
lack fine-grained semantics or naively scale up, resulting in low token
utilization and unstable training. We propose UniCode$^2$, a cascaded codebook
framework enabling large-scale, semantically aligned, and stable visual
tokenization. By clustering millions of SigLIP sequence embeddings, we build a
500K-entry codebook that preserves vision-language alignment while expanding
capacity. Stability is ensured via a cascaded design: a frozen codebook anchors
the embedding space, and a trainable codebook refines task-specific semantics.
This decoupling promotes high utilization and robust learning. Moreover, the
alignment of our visual tokens with textual semantics enables seamless
integration with pretrained diffusion decoders, supporting high-quality visual
synthesis with minimal adaptation. UniCode^2 delivers strong performance across
diverse benchmarks, demonstrating the viability of scaling visual token spaces
without sacrificing stability, semantics, or modularity.

</details>


### [69] [Dynamic Bandwidth Allocation for Hybrid Event-RGB Transmission](https://arxiv.org/abs/2506.20222)
*Pujing Yang,Guangyi Zhang,Yunlong Cai,Lei Yu,Guanding Yu*

Main category: cs.CV

TL;DR: The paper introduces a framework to optimize bandwidth utilization and improve real-time performance by jointly transmitting RGB image and event camera data, using a disentangled information bottleneck approach.


<details>
  <summary>Details</summary>
Motivation: To address challenges in hybrid systems with event and RGB cameras, particularly the redundant information and high bandwidth usage during data transmission.

Method: The authors propose a joint event-image (E-I) transmission framework that uses Bayesian modeling and information bottleneck techniques to disentangle shared and domain-specific information, enabling adaptive bandwidth allocation based on scene dynamics.

Result: The framework ensures high-quality reconstruction and achieves enhanced real-time deblurring, outperforming traditional approaches in simulation studies.

Conclusion: The proposed method optimizes transmission efficiency and improves vision application performance, emphasizing its utility in hybrid camera systems.

Abstract: Event cameras asynchronously capture pixel-level intensity changes with
extremely low latency. They are increasingly used in conjunction with RGB
cameras for a wide range of vision-related applications. However, a major
challenge in these hybrid systems lies in the transmission of the large volume
of triggered events and RGB images. To address this, we propose a transmission
scheme that retains efficient reconstruction performance of both sources while
accomplishing real-time deblurring in parallel. Conventional RGB cameras and
event cameras typically capture the same scene in different ways, often
resulting in significant redundant information across their outputs. To address
this, we develop a joint event and image (E-I) transmission framework to
eliminate redundancy and thereby optimize channel bandwidth utilization. Our
approach employs Bayesian modeling and the information bottleneck method to
disentangle the shared and domain-specific information within the E-I inputs.
This disentangled information bottleneck framework ensures both the compactness
and informativeness of extracted shared and domain-specific information.
Moreover, it adaptively allocates transmission bandwidth based on scene
dynamics, i.e., more symbols are allocated to events for dynamic details or to
images for static information. Simulation results demonstrate that the proposed
scheme not only achieves superior reconstruction quality compared to
conventional systems but also delivers enhanced deblurring performance.

</details>


### [70] [Recognizing Surgical Phases Anywhere: Few-Shot Test-time Adaptation and Task-graph Guided Refinement](https://arxiv.org/abs/2506.20254)
*Kun Yuan,Tingxuan Chen,Shi Li,Joel L. Lavanchy,Christian Heiliger,Ege Özsoy,Yiming Huang,Long Bai,Nassir Navab,Vinkle Srivastav,Hongliang Ren,Nicolas Padoy*

Main category: cs.CV

TL;DR: SPA, a lightweight framework, enhances surgical workflow understanding by adapting foundation models using minimal annotation, achieving state-of-the-art few-shot performance.


<details>
  <summary>Details</summary>
Motivation: Current challenges in surgical workflow modeling stem from diverse operating room settings and domain shifts, which hinder the zero-shot performance of existing models.

Method: SPA employs few-shot spatial adaptation, diffusion modeling for temporal consistency, and self-supervised test-time adaptation to dynamically align models with institutional workflows.

Result: Experimental results illustrate that SPA surpasses state-of-the-art performance in few-shot surgical phase recognition across institutions and even outperforms full-shot models.

Conclusion: SPA enables hospitals to rapidly adapt surgical phase recognition models with minimal annotation, offering a practical and reliable solution for cross-institutional workflow modeling.

Abstract: The complexity and diversity of surgical workflows, driven by heterogeneous
operating room settings, institutional protocols, and anatomical variability,
present a significant challenge in developing generalizable models for
cross-institutional and cross-procedural surgical understanding. While recent
surgical foundation models pretrained on large-scale vision-language data offer
promising transferability, their zero-shot performance remains constrained by
domain shifts, limiting their utility in unseen surgical environments. To
address this, we introduce Surgical Phase Anywhere (SPA), a lightweight
framework for versatile surgical workflow understanding that adapts foundation
models to institutional settings with minimal annotation. SPA leverages
few-shot spatial adaptation to align multi-modal embeddings with
institution-specific surgical scenes and phases. It also ensures temporal
consistency through diffusion modeling, which encodes task-graph priors derived
from institutional procedure protocols. Finally, SPA employs dynamic test-time
adaptation, exploiting the mutual agreement between multi-modal phase
prediction streams to adapt the model to a given test video in a
self-supervised manner, enhancing the reliability under test-time distribution
shifts. SPA is a lightweight adaptation framework, allowing hospitals to
rapidly customize phase recognition models by defining phases in natural
language text, annotating a few images with the phase labels, and providing a
task graph defining phase transitions. The experimental results show that the
SPA framework achieves state-of-the-art performance in few-shot surgical phase
recognition across multiple institutions and procedures, even outperforming
full-shot models with 32-shot labeled data. Code is available at
https://github.com/CAMMA-public/SPA

</details>


### [71] [A Transformer Based Handwriting Recognition System Jointly Using Online and Offline Features](https://arxiv.org/abs/2506.20255)
*Ayush Lodh,Ritabrata Chakraborty,Shivakumara Palaiahnakote,Umapada Pal*

Main category: cs.CV

TL;DR: The paper introduces an end-to-end network that integrates both offline images and online stroke data for handwriting recognition, achieving state-of-the-art accuracy.


<details>
  <summary>Details</summary>
Motivation: Handwriting recognition can benefit from combining visual cues from offline images and temporal cues from pen trajectories, but most existing systems only use one modality.

Method: The method involves an early fusion approach where a patch encoder processes grayscale images into visual tokens, a transformer embeds stroke data, and learnable latent queries jointly attend to both modalities for representation learning.

Result: The proposed approach achieves state-of-the-art accuracy on handwriting datasets IAMOn-DB and VNOn-DB, surpassing previous methods by up to 1%.

Conclusion: Early fusion of visual and temporal modalities enhances representation learning, achieving higher accuracy and writer independence in handwriting recognition.

Abstract: We posit that handwriting recognition benefits from complementary cues
carried by the rasterized complex glyph and the pen's trajectory, yet most
systems exploit only one modality. We introduce an end-to-end network that
performs early fusion of offline images and online stroke data within a shared
latent space. A patch encoder converts the grayscale crop into fixed-length
visual tokens, while a lightweight transformer embeds the $(x, y, \text{pen})$
sequence. Learnable latent queries attend jointly to both token streams,
yielding context-enhanced stroke embeddings that are pooled and decoded under a
cross-entropy loss objective. Because integration occurs before any high-level
classification, temporal cues reinforce each other during representation
learning, producing stronger writer independence. Comprehensive experiments on
IAMOn-DB and VNOn-DB demonstrate that our approach achieves state-of-the-art
accuracy, exceeding previous bests by up to 1\%. Our study also shows
adaptation of this pipeline with gesturification on the ISI-Air dataset. Our
code can be found here.

</details>


### [72] [Hierarchical Mask-Enhanced Dual Reconstruction Network for Few-Shot Fine-Grained Image Classification](https://arxiv.org/abs/2506.20263)
*Ning Luo,Meiyin Hu,Huan Wan,Yanyan Yang,Zhuohang Jiang,Xin Wei*

Main category: cs.CV

TL;DR: FS-FGIC is challenging with visually similar subclasses and limited data. HMDRN uses dual-layer reconstruction and mask-enhanced processing to improve classification.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenges of FS-FGIC, such as loss of spatial information in metric-based methods and lack of focus on discriminative regions in reconstruction-based methods.

Method: The authors propose HMDRN, integrating dual-layer feature reconstruction for combining hierarchical features and a mask-enhanced self-reconstruction module for discriminative focus.

Result: HMDRN surpasses state-of-the-art methods in FS-FGIC across multiple datasets and backbone architectures, validated by experiments and ablation studies.

Conclusion: HMDRN effectively improves inter-class discrimination and reduces intra-class variations, demonstrating superior performance in few-shot fine-grained classifications.

Abstract: Few-shot fine-grained image classification (FS-FGIC) presents a significant
challenge, requiring models to distinguish visually similar subclasses with
limited labeled examples. Existing methods have critical limitations:
metric-based methods lose spatial information and misalign local features,
while reconstruction-based methods fail to utilize hierarchical feature
information and lack mechanisms to focus on discriminative regions. We propose
the Hierarchical Mask-enhanced Dual Reconstruction Network (HMDRN), which
integrates dual-layer feature reconstruction with mask-enhanced feature
processing to improve fine-grained classification. HMDRN incorporates a
dual-layer feature reconstruction and fusion module that leverages
complementary visual information from different network hierarchies. Through
learnable fusion weights, the model balances high-level semantic
representations from the last layer with mid-level structural details from the
penultimate layer. Additionally, we design a spatial binary mask-enhanced
transformer self-reconstruction module that processes query features through
adaptive thresholding while maintaining complete support features, enhancing
focus on discriminative regions while filtering background noise. Extensive
experiments on three challenging fine-grained datasets demonstrate that HMDRN
consistently outperforms state-of-the-art methods across Conv-4 and ResNet-12
backbone architectures. Comprehensive ablation studies validate the
effectiveness of each proposed component, revealing that dual-layer
reconstruction enhances inter-class discrimination while mask-enhanced
transformation reduces intra-class variations. Visualization results provide
evidence of HMDRN's superior feature reconstruction capabilities.

</details>


### [73] [Forensic Study of Paintings Through the Comparison of Fabrics](https://arxiv.org/abs/2506.20272)
*Juan José Murillo-Fuentes,Pablo M. Olmos,Laura Alba-Carcelén*

Main category: cs.CV

TL;DR: The paper proposes a novel deep learning-based method to measure textile similarity for canvas authentication and conservation, overcoming limitations of traditional thread density methods.


<details>
  <summary>Details</summary>
Motivation: Traditional methods for analyzing canvas similarity rely on thread density maps, which fail when comparing canvases that are non-contiguous. This paper aims to address this gap and provide a more versatile solution.

Method: The study introduces a Siamese deep learning model trained on image pairs of canvas scans. Additionally, it aggregates multiple sample comparisons to provide a robust similarity score.

Result: The proposed method was tested on plain weave canvases from the Museo Nacional del Prado, demonstrating its accuracy in assessing similarity, even when thread density is similar.

Conclusion: This innovative technique broadens the scope for analyzing and comparing canvases, offering significant potential for art authentication, attribution, and conservation.

Abstract: The study of canvas fabrics in works of art is a crucial tool for
authentication, attribution and conservation. Traditional methods are based on
thread density map matching, which cannot be applied when canvases do not come
from contiguous positions on a roll. This paper presents a novel approach based
on deep learning to assess the similarity of textiles. We introduce an
automatic tool that evaluates the similarity between canvases without relying
on thread density maps. A Siamese deep learning model is designed and trained
to compare pairs of images by exploiting the feature representations learned
from the scans. In addition, a similarity estimation method is proposed,
aggregating predictions from multiple pairs of cloth samples to provide a
robust similarity score. Our approach is applied to canvases from the Museo
Nacional del Prado, corroborating the hypothesis that plain weave canvases,
widely used in painting, can be effectively compared even when their thread
densities are similar. The results demonstrate the feasibility and accuracy of
the proposed method, opening new avenues for the analysis of masterpieces.

</details>


### [74] [From Ideal to Real: Unified and Data-Efficient Dense Prediction for Real-World Scenarios](https://arxiv.org/abs/2506.20279)
*Changliang Xia,Chengyou Jia,Zhuohang Dang,Minnan Luo*

Main category: cs.CV

TL;DR: The paper introduces DenseWorld, a benchmark for real-world dense prediction tasks, and proposes DenseDiT, a method leveraging generative models for superior performance under limited data.


<details>
  <summary>Details</summary>
Motivation: Current dense prediction models lack generalization to real-world scenarios and are limited by the scarcity of real-world data.

Method: DenseDiT utilizes generative models' visual priors, a parameter-reuse mechanism, and lightweight branches for multi-scale context integration with minimal additional parameters.

Result: DenseDiT significantly outperforms existing methods on DenseWorld while requiring less than 0.01% of the training data compared to traditional baselines.

Conclusion: DenseDiT's efficient and generalizable approach makes it highly practical for real-world dense prediction tasks.

Abstract: Dense prediction tasks hold significant importance of computer vision, aiming
to learn pixel-wise annotated label for an input image. Despite advances in
this field, existing methods primarily focus on idealized conditions, with
limited generalization to real-world scenarios and facing the challenging
scarcity of real-world data. To systematically study this problem, we first
introduce DenseWorld, a benchmark spanning a broad set of 25 dense prediction
tasks that correspond to urgent real-world applications, featuring unified
evaluation across tasks. Then, we propose DenseDiT, which maximally exploits
generative models' visual priors to perform diverse real-world dense prediction
tasks through a unified strategy. DenseDiT combines a parameter-reuse mechanism
and two lightweight branches that adaptively integrate multi-scale context,
working with less than 0.1% additional parameters. Evaluations on DenseWorld
reveal significant performance drops in existing general and specialized
baselines, highlighting their limited real-world generalization. In contrast,
DenseDiT achieves superior results using less than 0.01% training data of
baselines, underscoring its practical value for real-world deployment. Our
data, and checkpoints and codes are available at
https://xcltql666.github.io/DenseDiTProj

</details>


### [75] [Breaking Spatial Boundaries: Spectral-Domain Registration Guided Hyperspectral and Multispectral Blind Fusion](https://arxiv.org/abs/2506.20293)
*Kunjing Yang,Libin Zheng,Minru Bai,Ting Lu,Leyuan Fang*

Main category: cs.CV

TL;DR: This paper addresses the problem of unregistered hyperspectral and multispectral image fusion by proposing spectral domain-based registration and sparse fusion methods.


<details>
  <summary>Details</summary>
Motivation: The registration and fusion of unregistered hyperspectral and multispectral images is challenging due to spatial resolution differences and computational inefficiencies in traditional methods.

Method: The authors developed a Spectral Prior Learning network for spectral feature extraction and image alignment, followed by blind sparse fusion with group sparsity regularization, and solved the resulting model using Proximal Alternating Optimization.

Result: Extensive experiments demonstrate improved image registration and fusion quality, alongside enhanced classification performance on both simulated and real datasets.

Conclusion: The new methods are effective, addressing registration and fusion challenges efficiently while enhancing overall classification accuracy.

Abstract: The blind fusion of unregistered hyperspectral images (HSIs) and
multispectral images (MSIs) has attracted growing attention recently. To
address the registration challenge, most existing methods employ spatial
transformations on the HSI to achieve alignment with the MSI. However, due to
the substantial differences in spatial resolution of the images, the
performance of these methods is often unsatisfactory. Moreover, the
registration process tends to be time-consuming when dealing with large-sized
images in remote sensing. To address these issues, we propose tackling the
registration problem from the spectral domain. Initially, a lightweight
Spectral Prior Learning (SPL) network is developed to extract spectral features
from the HSI and enhance the spectral resolution of the MSI. Following this,
the obtained image undergoes spatial downsampling to produce the registered
HSI. In this process, subspace representation and cyclic training strategy are
employed to improve spectral accuracy of the registered HSI obtained. Next, we
propose a blind sparse fusion (BSF) method, which utilizes group sparsity
regularization to equivalently promote the low-rankness of the image. This
approach not only circumvents the need for rank estimation, but also reduces
computational complexity. Then, we employ the Proximal Alternating Optimization
(PAO) algorithm to solve the BSF model, and present its convergence analysis.
Finally, extensive numerical experiments on simulated and real datasets are
conducted to verify the effectiveness of our method in registration and fusion.
We also demonstrate its efficacy in enhancing classification performance.

</details>


### [76] [Ctrl-Z Sampling: Diffusion Sampling with Controlled Random Zigzag Explorations](https://arxiv.org/abs/2506.20294)
*Shunqi Mao,Wei Guo,Chaoyi Zhang,Weidong Cai*

Main category: cs.CV

TL;DR: The paper introduces "Controlled Random Zigzag Sampling" (Ctrl-Z Sampling), enhancing diffusion models' conditional generation by escaping local optima through dynamic noise injection and trajectory evaluation.


<details>
  <summary>Details</summary>
Motivation: Diffusion models often face challenges of converging to local optima, leading to outputs that are locally coherent but globally inconsistent or misaligned.

Method: Ctrl-Z Sampling combines a reward model to detect local maxima, noise injection to escape optimization plateaus, and trajectory evaluation to ensure improved generation paths, alternating between refinement and exploration.

Result: Experimental results demonstrate that Ctrl-Z Sampling significantly enhances generation quality with a moderate computational cost of around 7.6X increase in function evaluations.

Conclusion: Ctrl-Z Sampling improves alignment and visual quality in conditional generation while being compatible with existing diffusion models and frameworks.

Abstract: Diffusion models have shown strong performance in conditional generation by
progressively denoising Gaussian noise toward a target data distribution. This
denoising process can be interpreted as a form of hill climbing in a learned
latent space, where the model iteratively refines the sample toward regions of
higher probability. However, diffusion models often converge to local optima
that are locally visually coherent yet globally inconsistent or conditionally
misaligned, due to latent space complexity and suboptimal initialization. Prior
efforts attempted to address this by strengthening guidance signals or
manipulating the initial noise distribution. We introduce Controlled Random
Zigzag Sampling (Ctrl-Z Sampling), a novel sampling strategy designed to detect
and escape such local maxima during conditional generation. The method first
identifies potential local maxima using a reward model. Upon detection, it
injects noise and reverts to a previous, noisier state to escape the current
optimization plateau. The reward model then evaluates candidate trajectories,
accepting only those that offer improvement, while progressively deeper retreat
enables stronger escapes when nearby alternatives fail. This controlled random
zigzag process allows dynamic alternation between forward refinement and
backward exploration, enhancing both alignment and visual quality in the
generated outputs. The proposed Ctrl-Z Sampling is model-agnostic and
compatible with existing diffusion frameworks. Experimental results show that
Ctrl-Z Sampling substantially improves generation quality with only around 7.6X
increase in function evaluations.

</details>


### [77] [TDiR: Transformer based Diffusion for Image Restoration Tasks](https://arxiv.org/abs/2506.20302)
*Abbas Anwar,Mohammad Shullar,Ali Arshad Nasir,Mudassir Masood,Saeed Anwar*

Main category: cs.CV

TL;DR: The paper proposes a transformer-based diffusion model for image restoration, showing superior performance on underwater enhancement, denoising, and deraining tasks.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to overcome challenges associated with degradation in images captured in harsh environments, affecting their utility in downstream applications.

Method: The study utilizes a fusion of diffusion models and transformers for addressing image restoration tasks, evaluated across various metrics on public datasets.

Result: The proposed approach outperforms traditional methods in enhancing image quality across multiple datasets and restoration tasks.

Conclusion: Diffusion models combined with transformers are effective in restoring degraded images, improving their applicability in tasks requiring high-quality visual data.

Abstract: Images captured in challenging environments often experience various forms of
degradation, including noise, color cast, blur, and light scattering. These
effects significantly reduce image quality, hindering their applicability in
downstream tasks such as object detection, mapping, and classification. Our
transformer-based diffusion model was developed to address image restoration
tasks, aiming to improve the quality of degraded images. This model was
evaluated against existing deep learning methodologies across multiple quality
metrics for underwater image enhancement, denoising, and deraining on publicly
available datasets. Our findings demonstrate that the diffusion model, combined
with transformers, surpasses current methods in performance. The results of our
model highlight the efficacy of diffusion models and transformers in improving
the quality of degraded images, consequently expanding their utility in
downstream tasks that require high-fidelity visual data.

</details>


### [78] [Radiomic fingerprints for knee MR images assessment](https://arxiv.org/abs/2506.20306)
*Yaxi Chen,Simin Ni,Shaheer U. Saeed,Aleksandra Ivanova,Rikin Hargunani,Jie Huang,Chaozong Liu,Yipeng Hu*

Main category: cs.CV

TL;DR: The paper introduces a dynamic radiomic fingerprinting framework for personalized interpretation of knee MRI scans, achieving superior or comparable diagnostic accuracy to current models while offering enhanced interpretability.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing radiomic methods that use fixed, population-level feature sets, which fail to capture individual pathological variations and reduce interpretability and performance.

Method: Introduce a dynamic radiomic fingerprint framework where a DL model predicts feature relevance for individual patients, creating personalized feature sets. A logistic regression model is used for downstream classification.

Result: Validated across diagnostic tasks (e.g., ACL tears, meniscus tears), the method achieved comparable or superior accuracy to end-to-end DL models and offered improved interpretability for clinical applications.

Conclusion: The proposed framework enhances both diagnostic performance and interpretability, enabling clinical insight generation and potential biomarker discovery.

Abstract: Accurate interpretation of knee MRI scans relies on expert clinical judgment,
often with high variability and limited scalability. Existing radiomic
approaches use a fixed set of radiomic features (the signature), selected at
the population level and applied uniformly to all patients. While
interpretable, these signatures are often too constrained to represent
individual pathological variations. As a result, conventional radiomic-based
approaches are found to be limited in performance, compared with recent
end-to-end deep learning (DL) alternatives without using interpretable radiomic
features. We argue that the individual-agnostic nature in current radiomic
selection is not central to its intepretability, but is responsible for the
poor generalization in our application. Here, we propose a novel radiomic
fingerprint framework, in which a radiomic feature set (the fingerprint) is
dynamically constructed for each patient, selected by a DL model. Unlike the
existing radiomic signatures, our fingerprints are derived on a per-patient
basis by predicting the feature relevance in a large radiomic feature pool, and
selecting only those that are predictive of clinical conditions for individual
patients. The radiomic-selecting model is trained simultaneously with a
low-dimensional (considered relatively explainable) logistic regression for
downstream classification. We validate our methods across multiple diagnostic
tasks including general knee abnormalities, anterior cruciate ligament (ACL)
tears, and meniscus tears, demonstrating comparable or superior diagnostic
accuracy relative to state-of-the-art end-to-end DL models. More importantly,
we show that the interpretability inherent in our approach facilitates
meaningful clinical insights and potential biomarker discovery, with detailed
discussion, quantitative and qualitative analysis of real-world clinical cases
to evidence these advantages.

</details>


### [79] [On the Burstiness of Faces in Set](https://arxiv.org/abs/2506.20312)
*Jiong Wang*

Main category: cs.CV

TL;DR: The paper explores 'burstiness' in set-based face recognition and its negative impact on model performance. It proposes strategies to detect bursty faces and methods to mitigate their effect during both training and evaluation stages.


<details>
  <summary>Details</summary>
Motivation: Burstiness negatively affects set-based face recognition by dominating training instances and evaluation sets, leading to poor generalization and interfering with similarity comparison.

Method: The paper introduces three strategies for detecting bursty faces based on Quickshift++, feature self-similarity, and generalized max-pooling (GMP). It also proposes quality-aware GMP for robustness in evaluation.

Result: The proposed strategies effectively suppress burstiness, leading to significant improvements in set-based face recognition benchmarks.

Conclusion: Addressing burstiness enhances the generalization and accuracy of face recognition models, making them more robust to real-world, unconstrained scenarios.

Abstract: Burstiness, a phenomenon observed in text and image retrieval, refers to that
particular elements appear more times in a set than a statistically independent
model assumes. We argue that in the context of set-based face recognition
(SFR), burstiness exists widely and degrades the performance in two aspects:
Firstly, the bursty faces, where faces with particular attributes %exist
frequently in a face set, dominate the training instances and dominate the
training face sets and lead to poor generalization ability to unconstrained
scenarios. Secondly, the bursty faces %dominating the evaluation sets interfere
with the similarity comparison in set verification and identification when
evaluation. To detect the bursty faces in a set, we propose three strategies
based on Quickshift++, feature self-similarity, and generalized max-pooling
(GMP). We apply the burst detection results on training and evaluation stages
to enhance the sampling ratios or contributions of the infrequent faces. When
evaluation, we additionally propose the quality-aware GMP that enables
awareness of the face quality and robustness to the low-quality faces for the
original GMP. We give illustrations and extensive experiments on the SFR
benchmarks to demonstrate that burstiness is widespread and suppressing
burstiness considerably improves the recognition performance.

</details>


### [80] [From Codicology to Code: A Comparative Study of Transformer and YOLO-based Detectors for Layout Analysis in Historical Documents](https://arxiv.org/abs/2506.20326)
*Sergio Torres Aguilar*

Main category: cs.CV

TL;DR: This paper benchmarks five state-of-the-art object detection models for robust document layout analysis on three datasets of varying historical manuscript complexities, evaluating performance differences based on model architectures and bounding box representations.


<details>
  <summary>Details</summary>
Motivation: Automated processing and understanding of historical documents is a challenging task due to their complex page organizations. The study aims to determine the most effective object detection architecture for such tasks across different datasets.

Method: The study evaluates two Transformer-based models and three YOLO-based models using annotated datasets of historical documents. It analyzes the performance variations by considering dataset characteristics and bounding box representations.

Result: Co-DETR achieves the best results for the e-NDP dataset, while YOLOv11x-OBB outperforms others on the more complex CATMuS and HORAE datasets. The results emphasize the importance of Oriented Bounding Boxes for handling non-Cartesian manuscript layouts.

Conclusion: The study identifies a trade-off: Transformers excel in globally structured layouts, whereas CNN-OBB models generalize better for visually diverse and complex documents. Oriented Bounding Boxes are critical for accurate historical manuscript analysis.

Abstract: Robust Document Layout Analysis (DLA) is critical for the automated
processing and understanding of historical documents with complex page
organizations. This paper benchmarks five state-of-the-art object detection
architectures on three annotated datasets representing a spectrum of
codicological complexity: The e-NDP, a corpus of Parisian medieval registers
(1326-1504); CATMuS, a diverse multiclass dataset derived from various medieval
and modern sources (ca.12th-17th centuries) and HORAE, a corpus of decorated
books of hours (ca.13th-16th centuries). We evaluate two Transformer-based
models (Co-DETR, Grounding DINO) against three YOLO variants (AABB, OBB, and
YOLO-World). Our findings reveal significant performance variations dependent
on model architecture, data set characteristics, and bounding box
representation. In the e-NDP dataset, Co-DETR achieves state-of-the-art results
(0.752 mAP@.50:.95), closely followed by YOLOv11X-OBB (0.721). Conversely, on
the more complex CATMuS and HORAE datasets, the CNN-based YOLOv11x-OBB
significantly outperforms all other models (0.564 and 0.568, respectively).
This study unequivocally demonstrates that using Oriented Bounding Boxes (OBB)
is not a minor refinement but a fundamental requirement for accurately modeling
the non-Cartesian nature of historical manuscripts. We conclude that a key
trade-off exists between the global context awareness of Transformers, ideal
for structured layouts, and the superior generalization of CNN-OBB models for
visually diverse and complex documents.

</details>


### [81] [Feature Hallucination for Self-supervised Action Recognition](https://arxiv.org/abs/2506.20342)
*Lei Wang,Piotr Koniusz*

Main category: cs.CV

TL;DR: This paper introduces a multimodal framework for video action recognition by leveraging auxiliary features with hallucination streams and domain-specific descriptors.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the necessity for high-level semantic reasoning and effective utilization of multimodal features in video action recognition.

Method: It proposes a deep translational approach using auxiliary features (ODF, SDF) and hallucination streams for enhanced feature representation.

Result: The framework achieves state-of-the-art performance on benchmarks like Kinetics-400, Kinetics-600, and Something-Something V2.

Conclusion: The approach effectively integrates multimodal cues and uncertainty handling, improving accuracy in capturing fine-grained action dynamics.

Abstract: Understanding human actions in videos requires more than raw pixel analysis;
it relies on high-level semantic reasoning and effective integration of
multimodal features. We propose a deep translational action recognition
framework that enhances recognition accuracy by jointly predicting action
concepts and auxiliary features from RGB video frames. At test time,
hallucination streams infer missing cues, enriching feature representations
without increasing computational overhead. To focus on action-relevant regions
beyond raw pixels, we introduce two novel domain-specific descriptors. Object
Detection Features (ODF) aggregate outputs from multiple object detectors to
capture contextual cues, while Saliency Detection Features (SDF) highlight
spatial and intensity patterns crucial for action recognition. Our framework
seamlessly integrates these descriptors with auxiliary modalities such as
optical flow, Improved Dense Trajectories, skeleton data, and audio cues. It
remains compatible with state-of-the-art architectures, including I3D,
AssembleNet, Video Transformer Network, FASTER, and recent models like VideoMAE
V2 and InternVideo2. To handle uncertainty in auxiliary features, we
incorporate aleatoric uncertainty modeling in the hallucination step and
introduce a robust loss function to mitigate feature noise. Our multimodal
self-supervised action recognition framework achieves state-of-the-art
performance on multiple benchmarks, including Kinetics-400, Kinetics-600, and
Something-Something V2, demonstrating its effectiveness in capturing
fine-grained action dynamics.

</details>


### [82] [InvZW: Invariant Feature Learning via Noise-Adversarial Training for Robust Image Zero-Watermarking](https://arxiv.org/abs/2506.20370)
*Abdullah All Tanvir,Xin Zhong*

Main category: cs.CV

TL;DR: This paper proposes a deep learning solution for robust image zero-watermarking using distortion-invariant features.


<details>
  <summary>Details</summary>
Motivation: The motivation is to develop a watermarking method that leaves the original image untouched while ensuring robust watermark recovery amidst distortions.

Method: The method uses a two-module framework: (1) A noise-adversarially trained feature extractor for distortion-invariant and semantic representations, and (2) a trainable reference code system optimized for mapping target binary messages.

Result: Experiments show state-of-the-art robustness in feature stability and watermark recovery across various distortions and datasets, outperforming competing techniques.

Conclusion: The proposed framework successfully enhances robustness and generalization in image zero-watermarking, setting new benchmarks in the field.

Abstract: This paper introduces a novel deep learning framework for robust image
zero-watermarking based on distortion-invariant feature learning. As a
zero-watermarking scheme, our method leaves the original image unaltered and
learns a reference signature through optimization in the feature space. The
proposed framework consists of two key modules. In the first module, a feature
extractor is trained via noise-adversarial learning to generate representations
that are both invariant to distortions and semantically expressive. This is
achieved by combining adversarial supervision against a distortion
discriminator and a reconstruction constraint to retain image content. In the
second module, we design a learning-based multibit zero-watermarking scheme
where the trained invariant features are projected onto a set of trainable
reference codes optimized to match a target binary message. Extensive
experiments on diverse image datasets and a wide range of distortions show that
our method achieves state-of-the-art robustness in both feature stability and
watermark recovery. Comparative evaluations against existing self-supervised
and deep watermarking techniques further highlight the superiority of our
framework in generalization and robustness.

</details>


### [83] [Exploiting Lightweight Hierarchical ViT and Dynamic Framework for Efficient Visual Tracking](https://arxiv.org/abs/2506.20381)
*Ben Kang,Xin Chen,Jie Zhao,Chunjuan Bo,Dong Wang,Huchuan Lu*

Main category: cs.CV

TL;DR: This paper introduces HiT and DyHiT, efficient, high-performance transformer-based tracking models with innovations like a Bridge Module, dual-image position encoding, and dynamic routing to improve speed and accuracy on various devices.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the limitation of slow processing speeds in transformer-based trackers, which hinders their use on resource-constrained devices.

Method: HiT introduces a Bridge Module for better feature representation and dual-image position encoding for spatial information. DyHiT employs dynamic routing to adapt to scene complexity, classifying scenarios and selecting computation strategies. A training-free acceleration method is also presented.

Result: HiT achieves 61 fps with 64.6% AUC on LaSOT and DyHiT achieves 111 fps with 62.4% AUC. The training-free acceleration method provides a 2.68× speedup for SeqTrack-B256 on RTX 2080 Ti while maintaining accuracy.

Conclusion: The proposed approaches effectively balance speed and accuracy, making transformer-based trackers more practical for a wide range of devices. The dynamic routing and acceleration methods show significant performance improvements.

Abstract: Transformer-based visual trackers have demonstrated significant advancements
due to their powerful modeling capabilities. However, their practicality is
limited on resource-constrained devices because of their slow processing
speeds. To address this challenge, we present HiT, a novel family of efficient
tracking models that achieve high performance while maintaining fast operation
across various devices. The core innovation of HiT lies in its Bridge Module,
which connects lightweight transformers to the tracking framework, enhancing
feature representation quality. Additionally, we introduce a dual-image
position encoding approach to effectively encode spatial information. HiT
achieves an impressive speed of 61 frames per second (fps) on the NVIDIA Jetson
AGX platform, alongside a competitive AUC of 64.6% on the LaSOT benchmark,
outperforming all previous efficient trackers.Building on HiT, we propose
DyHiT, an efficient dynamic tracker that flexibly adapts to scene complexity by
selecting routes with varying computational requirements. DyHiT uses search
area features extracted by the backbone network and inputs them into an
efficient dynamic router to classify tracking scenarios. Based on the
classification, DyHiT applies a divide-and-conquer strategy, selecting
appropriate routes to achieve a superior trade-off between accuracy and speed.
The fastest version of DyHiT achieves 111 fps on NVIDIA Jetson AGX while
maintaining an AUC of 62.4% on LaSOT.Furthermore, we introduce a training-free
acceleration method based on the dynamic routing architecture of DyHiT. This
method significantly improves the execution speed of various high-performance
trackers without sacrificing accuracy. For instance, our acceleration method
enables the state-of-the-art tracker SeqTrack-B256 to achieve a 2.68 times
speedup on an NVIDIA GeForce RTX 2080 Ti GPU while maintaining the same AUC of
69.9% on the LaSOT.

</details>


### [84] [A Novel Large Vision Foundation Model (LVFM)-based Approach for Generating High-Resolution Canopy Height Maps in Plantations for Precision Forestry Management](https://arxiv.org/abs/2506.20388)
*Shen Tan,Xin Zhang,Liangxiu Han,Huaguo Huang,Han Wang*

Main category: cs.CV

TL;DR: This paper introduces a model using a Large Vision Foundation Model (LVFM) to generate high-resolution canopy height maps (CHMs) cost-effectively, achieving strong accuracy and demonstrating utility in carbon sequestration evaluation.


<details>
  <summary>Details</summary>
Motivation: Traditional lidar-based CHMs are expensive, and deep learning methods with RGB imagery struggle with canopy height feature extraction. This research aims to develop an accurate, cost-effective solution for AGB monitoring for carbon initiatives like CCER.

Method: The proposed model incorporates a feature extractor, self-supervised feature enhancement to preserve spatial details, and a height estimator. It was trained and tested using 1-meter Google Earth imagery in Beijing's Fangshan District.

Result: The model achieved a mean absolute error of 0.09 m and a root mean square error of 0.24 m, outperforming conventional CNNs. It enabled over 90% success in individual tree detection and demonstrated strong generalization to new areas.

Conclusion: The approach provides a scalable, accurate, and cost-effective solution for high-resolution CHM generation, making it valuable for monitoring plantation growth and evaluating carbon sequestration in plantations and natural forests.

Abstract: Accurate, cost-effective monitoring of plantation aboveground biomass (AGB)
is crucial for supporting local livelihoods and carbon sequestration
initiatives like the China Certified Emission Reduction (CCER) program.
High-resolution canopy height maps (CHMs) are essential for this, but standard
lidar-based methods are expensive. While deep learning with RGB imagery offers
an alternative, accurately extracting canopy height features remains
challenging. To address this, we developed a novel model for high-resolution
CHM generation using a Large Vision Foundation Model (LVFM). Our model
integrates a feature extractor, a self-supervised feature enhancement module to
preserve spatial details, and a height estimator. Tested in Beijing's Fangshan
District using 1-meter Google Earth imagery, our model outperformed existing
methods, including conventional CNNs. It achieved a mean absolute error of 0.09
m, a root mean square error of 0.24 m, and a correlation of 0.78 against
lidar-based CHMs. The resulting CHMs enabled over 90% success in individual
tree detection, high accuracy in AGB estimation, and effective tracking of
plantation growth, demonstrating strong generalization to non-training areas.
This approach presents a promising, scalable tool for evaluating carbon
sequestration in both plantations and natural forests.

</details>


### [85] [Med-Art: Diffusion Transformer for 2D Medical Text-to-Image Generation](https://arxiv.org/abs/2506.20449)
*Changlu Guo,Anders Nymark Christensen,Morten Rieger Hannemose*

Main category: cs.CV

TL;DR: The paper introduces Med-Art, a medical image generation framework designed to handle limited data. It utilizes vision-language models and fine-tunes a pre-trained text-to-image model for superior performance.


<details>
  <summary>Details</summary>
Motivation: To address challenges in medical image generation, such as limited dataset sizes and scarce medical textual data, which hinder progress in this field.

Method: Med-Art combines vision-language models to create textual descriptions of medical images, fine-tunes the PixArt-α model based on Diffusion Transformer (DiT), and introduces a Hybrid-Level Diffusion Fine-tuning (HLDF) method to enhance generation quality.

Result: Achieved state-of-the-art results on two medical image datasets, evaluated using metrics like FID, KID, and downstream classification performance.

Conclusion: Med-Art effectively addresses the scarcity of medical data and generates high-quality medical images, advancing the capabilities of text-to-image models in the medical domain.

Abstract: Text-to-image generative models have achieved remarkable breakthroughs in
recent years. However, their application in medical image generation still
faces significant challenges, including small dataset sizes, and scarcity of
medical textual data. To address these challenges, we propose Med-Art, a
framework specifically designed for medical image generation with limited data.
Med-Art leverages vision-language models to generate visual descriptions of
medical images which overcomes the scarcity of applicable medical textual data.
Med-Art adapts a large-scale pre-trained text-to-image model, PixArt-$\alpha$,
based on the Diffusion Transformer (DiT), achieving high performance under
limited data. Furthermore, we propose an innovative Hybrid-Level Diffusion
Fine-tuning (HLDF) method, which enables pixel-level losses, effectively
addressing issues such as overly saturated colors. We achieve state-of-the-art
performance on two medical image datasets, measured by FID, KID, and downstream
classification performance.

</details>


### [86] [Lightweight Multi-Frame Integration for Robust YOLO Object Detection in Videos](https://arxiv.org/abs/2506.20550)
*Yitong Quan,Benjamin Kiefer,Martin Messmer,Andreas Zell*

Main category: cs.CV

TL;DR: The paper proposes an approach to enhance video object detection by stacking consecutive frames for YOLO-based detectors while using only a single target frame's output, improving robustness with minimal complexity.


<details>
  <summary>Details</summary>
Motivation: Existing object detection models like YOLOv7 operate on single frames, ignoring the temporal context in videos. Video-based detections are often computationally heavy and impractical for real-world applications facing challenges like motion blur and occlusions.

Method: The method stacks multiple consecutive frames as input into a YOLO-based detector but supervises only the detection output of a target frame. This minimal modification to architecture leverages temporal information while remaining efficient and suitable for real-time applications.

Result: The experiments on MOT20Det and the newly introduced BOAT360 datasets show enhanced robustness in detection, particularly for lightweight models. This method narrows performance disparities between compact and heavy networks.

Conclusion: The proposed method effectively balances detection robustness and efficiency, making it practical for real-world video detection tasks. Additionally, the contribution of the BOAT360 dataset fosters further research in the field.

Abstract: Modern image-based object detection models, such as YOLOv7, primarily process
individual frames independently, thus ignoring valuable temporal context
naturally present in videos. Meanwhile, existing video-based detection methods
often introduce complex temporal modules, significantly increasing model size
and computational complexity. In practical applications such as surveillance
and autonomous driving, transient challenges including motion blur, occlusions,
and abrupt appearance changes can severely degrade single-frame detection
performance. To address these issues, we propose a straightforward yet highly
effective strategy: stacking multiple consecutive frames as input to a
YOLO-based detector while supervising only the output corresponding to a single
target frame. This approach leverages temporal information with minimal
modifications to existing architectures, preserving simplicity, computational
efficiency, and real-time inference capability. Extensive experiments on the
challenging MOT20Det and our BOAT360 datasets demonstrate that our method
improves detection robustness, especially for lightweight models, effectively
narrowing the gap between compact and heavy detection networks. Additionally,
we contribute the BOAT360 benchmark dataset, comprising annotated fisheye video
sequences captured from a boat, to support future research in multi-frame video
object detection in challenging real-world scenarios.

</details>


### [87] [HiWave: Training-Free High-Resolution Image Generation via Wavelet-Based Diffusion Sampling](https://arxiv.org/abs/2506.20452)
*Tobias Vontobel,Seyedmorteza Sadat,Farnood Salehi,Romann M. Weber*

Main category: cs.CV

TL;DR: HiWave proposes an innovative zero-shot method for enhancing ultra-high-resolution image synthesis using pretrained diffusion models, achieving improved fidelity and coherence without retraining.


<details>
  <summary>Details</summary>
Motivation: Overcoming challenges in high-resolution image synthesis using diffusion models, which suffer from computational constraints and artifacts like object duplication and spatial incoherence.

Method: HiWave employs a two-stage pipeline: first, base image generation followed by patch-wise DDIM inversion; second, a wavelet-based detail enhancer module guiding high-frequency details while preserving low-frequency structural consistency.

Result: HiWave demonstrated superior perceptual quality and reduced visual artifacts compared to prior methods, validated through evaluations with Stable Diffusion XL and an 80% preference rate in user comparisons.

Conclusion: HiWave achieves effective high-quality image synthesis at ultra-high resolutions without requiring retraining or architectural changes, offering a significant advancement over existing zero-shot techniques.

Abstract: Diffusion models have emerged as the leading approach for image synthesis,
demonstrating exceptional photorealism and diversity. However, training
diffusion models at high resolutions remains computationally prohibitive, and
existing zero-shot generation techniques for synthesizing images beyond
training resolutions often produce artifacts, including object duplication and
spatial incoherence. In this paper, we introduce HiWave, a training-free,
zero-shot approach that substantially enhances visual fidelity and structural
coherence in ultra-high-resolution image synthesis using pretrained diffusion
models. Our method employs a two-stage pipeline: generating a base image from
the pretrained model followed by a patch-wise DDIM inversion step and a novel
wavelet-based detail enhancer module. Specifically, we first utilize inversion
methods to derive initial noise vectors that preserve global coherence from the
base image. Subsequently, during sampling, our wavelet-domain detail enhancer
retains low-frequency components from the base image to ensure structural
consistency, while selectively guiding high-frequency components to enrich fine
details and textures. Extensive evaluations using Stable Diffusion XL
demonstrate that HiWave effectively mitigates common visual artifacts seen in
prior methods, achieving superior perceptual quality. A user study confirmed
HiWave's performance, where it was preferred over the state-of-the-art
alternative in more than 80% of comparisons, highlighting its effectiveness for
high-quality, ultra-high-resolution image synthesis without requiring
retraining or architectural modifications.

</details>


### [88] [Learning-Based Distance Estimation for 360° Single-Sensor Setups](https://arxiv.org/abs/2506.20586)
*Yitong Quan,Benjamin Kiefer,Martin Messmer,Andreas Zell*

Main category: cs.CV

TL;DR: The paper proposes a deep learning approach for accurate monocular distance estimation using a single 360° fisheye camera, overcoming limitations in traditional geometric methods.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of lens distortions and environmental variability in distance estimation for omnidirectional imaging.

Method: A neural network model directly learns and infers object distance from raw 360° fisheye camera inputs, without needing lens calibration.

Result: The approach outperforms traditional geometry-based methods and existing learning-based baselines in terms of accuracy and robustness across three diverse datasets (LOAF, ULM360, Boat360).

Conclusion: Deep learning provides a robust and adaptable solution for real-time omnidirectional distance estimation, beneficial for cost-effective applications in robotics, navigation, and surveillance.

Abstract: Accurate distance estimation is a fundamental challenge in robotic
perception, particularly in omnidirectional imaging, where traditional
geometric methods struggle with lens distortions and environmental variability.
In this work, we propose a neural network-based approach for monocular distance
estimation using a single 360{\deg} fisheye lens camera. Unlike classical
trigonometric techniques that rely on precise lens calibration, our method
directly learns and infers the distance of objects from raw omnidirectional
inputs, offering greater robustness and adaptability across diverse conditions.
We evaluate our approach on three 360{\deg} datasets (LOAF, ULM360, and a newly
captured dataset Boat360), each representing distinct environmental and sensor
setups. Our experimental results demonstrate that the proposed learning-based
model outperforms traditional geometry-based methods and other learning
baselines in both accuracy and robustness. These findings highlight the
potential of deep learning for real-time omnidirectional distance estimation,
making our approach particularly well-suited for low-cost applications in
robotics, autonomous navigation, and surveillance.

</details>


### [89] [A Deep Learning Approach to Identify Rock Bolts in Complex 3D Point Clouds of Underground Mines Captured Using Mobile Laser Scanners](https://arxiv.org/abs/2506.20464)
*Dibyayan Patra,Pasindu Ranasinghe,Bikram Banerjee,Simit Raval*

Main category: cs.CV

TL;DR: The study introduces DeepBolt, a deep learning method for detecting rock bolts in 3D point clouds from mines, achieving significant performance improvements over existing methods.


<details>
  <summary>Details</summary>
Motivation: To enable efficient and thorough assessments of rock bolts in challenging underground mining conditions to maintain safety and prevent hazards like rockfalls.

Method: DeepBolt, a two-stage deep learning architecture, was proposed to address severe class imbalance and challenges in identifying small objects (rock bolts) in noisy and complex 3D point clouds.

Result: DeepBolt outperforms existing semantic segmentation models by up to 42.5% in IoU and achieves 96.41% precision and 96.96% recall in rock bolt identification.

Conclusion: DeepBolt demonstrates superior robustness and effectiveness in detecting rock bolts in complex underground mining conditions, surpassing current state-of-the-art techniques.

Abstract: Rock bolts are crucial components of the subterranean support systems in
underground mines that provide adequate structural reinforcement to the rock
mass to prevent unforeseen hazards like rockfalls. This makes frequent
assessments of such bolts critical for maintaining rock mass stability and
minimising risks in underground mining operations. Where manual surveying of
rock bolts is challenging due to the low light conditions in the underground
mines and the time-intensive nature of the process, automated detection of rock
bolts serves as a plausible solution. To that end, this study focuses on the
automatic identification of rock bolts within medium to large-scale 3D point
clouds obtained from underground mines using mobile laser scanners. Existing
techniques for automated rock bolt identification primarily rely on feature
engineering and traditional machine learning approaches. However, such
techniques lack robustness as these point clouds present several challenges due
to data noise, varying environments, and complex surrounding structures.
Moreover, the target rock bolts are extremely small objects within large-scale
point clouds and are often partially obscured due to the application of
reinforcement shotcrete. Addressing these challenges, this paper proposes an
approach termed DeepBolt, which employs a novel two-stage deep learning
architecture specifically designed for handling severe class imbalance for the
automatic and efficient identification of rock bolts in complex 3D point
clouds. The proposed method surpasses state-of-the-art semantic segmentation
models by up to 42.5% in Intersection over Union (IoU) for rock bolt points.
Additionally, it outperforms existing rock bolt identification techniques,
achieving a 96.41% precision and 96.96% recall in classifying rock bolts,
demonstrating its robustness and effectiveness in complex underground
environments.

</details>


### [90] [AI-assisted radiographic analysis in detecting alveolar bone-loss severity and patterns](https://arxiv.org/abs/2506.20522)
*Chathura Wimalasiri,Piumal Rathnayake,Shamod Wijerathne,Sumudu Rasnayaka,Dhanushka Leuke Bandara,Roshan Ragel,Vajira Thambawita,Isuru Nawinne*

Main category: cs.CV

TL;DR: The paper presents a novel AI-based framework to assess alveolar bone loss in periodontitis using radiographs, achieving high accuracy.


<details>
  <summary>Details</summary>
Motivation: Periodontitis significantly affects oral health and requires precise diagnosis of alveolar bone loss severity and patterns for effective treatment planning.

Method: The approach integrates YOLOv8 for tooth detection, Keypoint R-CNN for anatomical landmark identification, and YOLOv8x-seg for bone and tooth segmentation to analyze bone loss patterns using geometric analysis.

Result: Evaluated on 1,000 radiographs, the framework achieved a high intra-class correlation coefficient of up to 0.80 for bone loss severity and 87% accuracy for pattern classification.

Conclusion: The developed AI-based system provides an objective and reproducible tool for periodontal assessment, enabling improvements in diagnosis, treatment personalization, and overall patient care.

Abstract: Periodontitis, a chronic inflammatory disease causing alveolar bone loss,
significantly affects oral health and quality of life. Accurate assessment of
bone loss severity and pattern is critical for diagnosis and treatment
planning. In this study, we propose a novel AI-based deep learning framework to
automatically detect and quantify alveolar bone loss and its patterns using
intraoral periapical (IOPA) radiographs. Our method combines YOLOv8 for tooth
detection with Keypoint R-CNN models to identify anatomical landmarks, enabling
precise calculation of bone loss severity. Additionally, YOLOv8x-seg models
segment bone levels and tooth masks to determine bone loss patterns (horizontal
vs. angular) via geometric analysis. Evaluated on a large, expertly annotated
dataset of 1000 radiographs, our approach achieved high accuracy in detecting
bone loss severity (intra-class correlation coefficient up to 0.80) and bone
loss pattern classification (accuracy 87%). This automated system offers a
rapid, objective, and reproducible tool for periodontal assessment, reducing
reliance on subjective manual evaluation. By integrating AI into dental
radiographic analysis, our framework has the potential to improve early
diagnosis and personalized treatment planning for periodontitis, ultimately
enhancing patient care and clinical outcomes.

</details>


### [91] [Pay Less Attention to Deceptive Artifacts: Robust Detection of Compressed Deepfakes on Online Social Networks](https://arxiv.org/abs/2506.20548)
*Manyi Li,Renshuai Tao,Yufan Liu,Chuangchuang Tan,Haotong Qin,Bing Li,Yunchao Wei,Yao Zhao*

Main category: cs.CV

TL;DR: Deepfake detection struggles with compression-related 'block effects' and lack of paired data. The proposed PLADA framework addresses these with a dual-stage attention mechanism and open data aggregation, excelling in real-world scenarios.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenges in detecting AI-generated deepfake images, especially when they are compressed in Online Social Networks (OSNs), which obscures telltale artifacts and presents difficulties in real-world applications.

Method: The PLADA framework features two key modules: (1) Block Effect Eraser (B2E), employing a dual-stage attention mechanism to handle compression-related block effects, and (2) Open Data Aggregation (ODA), which makes effective use of both paired and unpaired data to boost detection capabilities.

Result: PLADA outperforms state-of-the-art deepfake detection methods in real-world scenarios, successfully detecting fakes in compressed OSN images with limited paired data. Extensive experiments on 26 datasets prove its effectiveness.

Conclusion: PLADA provides a robust solution for detecting deepfakes in a compressed, real-world setting by tackling compression block effects and leveraging diverse data. It sets a new benchmark for open-world detection efficacy.

Abstract: With the rapid advancement of deep learning, particularly through generative
adversarial networks (GANs) and diffusion models (DMs), AI-generated images, or
``deepfakes", have become nearly indistinguishable from real ones. These images
are widely shared across Online Social Networks (OSNs), raising concerns about
their misuse. Existing deepfake detection methods overlook the ``block effects"
introduced by compression in OSNs, which obscure deepfake artifacts, and
primarily focus on raw images, rarely encountered in real-world scenarios. To
address these challenges, we propose PLADA (Pay Less Attention to Deceptive
Artifacts), a novel framework designed to tackle the lack of paired data and
the ineffective use of compressed images. PLADA consists of two core modules:
Block Effect Eraser (B2E), which uses a dual-stage attention mechanism to
handle block effects, and Open Data Aggregation (ODA), which processes both
paired and unpaired data to improve detection. Extensive experiments across 26
datasets demonstrate that PLADA achieves a remarkable balance in deepfake
detection, outperforming SoTA methods in detecting deepfakes on OSNs, even with
limited paired data and compression. More importantly, this work introduces the
``block effect" as a critical factor in deepfake detection, providing a robust
solution for open-world scenarios. Our code is available at
https://github.com/ManyiLee/PLADA.

</details>


### [92] [AdvMIM: Adversarial Masked Image Modeling for Semi-Supervised Medical Image Segmentation](https://arxiv.org/abs/2506.20563)
*Lei Zhu,Jun Zhou,Rick Siow Mong Goh,Yong Liu*

Main category: cs.CV

TL;DR: This paper introduces a new adversarial masked image modeling method to improve the performance of Vision Transformers for semi-supervised medical image segmentation using limited labeled data.


<details>
  <summary>Details</summary>
Motivation: Transformers are effective for capturing long-range dependencies in medical image segmentation but struggle in semi-supervised learning due to their reliance on a large amount of labeled data.

Method: The authors propose masking parts of the input image to create a masked domain and train the transformer to predict the full segmentation mask. Original labeled data and pseudo-labels from unlabeled data are used, combined with adversarial training to reduce domain gaps.

Result: Extensive experiments on three public datasets demonstrate significant improvements in segmentation performance compared to state-of-the-art methods.

Conclusion: The proposed adversarial masked image modeling method effectively addresses the limitations of transformers in semi-supervised learning, achieving superior results and providing valuable contributions to medical image segmentation.

Abstract: Vision Transformer has recently gained tremendous popularity in medical image
segmentation task due to its superior capability in capturing long-range
dependencies. However, transformer requires a large amount of labeled data to
be effective, which hinders its applicability in annotation scarce
semi-supervised learning scenario where only limited labeled data is available.
State-of-the-art semi-supervised learning methods propose combinatorial
CNN-Transformer learning to cross teach a transformer with a convolutional
neural network, which achieves promising results. However, it remains a
challenging task to effectively train the transformer with limited labeled
data. In this paper, we propose an adversarial masked image modeling method to
fully unleash the potential of transformer for semi-supervised medical image
segmentation. The key challenge in semi-supervised learning with transformer
lies in the lack of sufficient supervision signal. To this end, we propose to
construct an auxiliary masked domain from original domain with masked image
modeling and train the transformer to predict the entire segmentation mask with
masked inputs to increase supervision signal. We leverage the original labels
from labeled data and pseudo-labels from unlabeled data to learn the masked
domain. To further benefit the original domain from masked domain, we provide a
theoretical analysis of our method from a multi-domain learning perspective and
devise a novel adversarial training loss to reduce the domain gap between the
original and masked domain, which boosts semi-supervised learning performance.
We also extend adversarial masked image modeling to CNN network. Extensive
experiments on three public medical image segmentation datasets demonstrate the
effectiveness of our method, where our method outperforms existing methods
significantly. Our code is publicly available at
https://github.com/zlheui/AdvMIM.

</details>


### [93] [Show, Tell and Summarize: Dense Video Captioning Using Visual Cue Aided Sentence Summarization](https://arxiv.org/abs/2506.20567)
*Zhiwang Zhang,Dong Xu,Wanli Ouyang,Chuanqi Tan*

Main category: cs.CV

TL;DR: This paper introduces a division-and-summarization framework for dense video captioning, where videos are segmented, described individually, and summarized into comprehensive event descriptions using a two-stage LSTM with hierarchical attention.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of generating meaningful descriptions for untrimmed, long videos by utilizing both visual and semantic information in a structured manner.

Method: The proposed method involves segmenting video into events, generating descriptions for each segment using existing captioning techniques, and summarizing these descriptions into one comprehensive caption per event using a two-stage LSTM with a hierarchical attention mechanism.

Result: The framework demonstrates effectiveness in dense video captioning tasks, as validated by experiments conducted on the ActivityNet Captions dataset.

Conclusion: The DaS framework efficiently combines semantic and visual inputs to summarize event proposals into descriptive sentences, enhancing the quality and coherence of dense video captions.

Abstract: In this work, we propose a division-and-summarization (DaS) framework for
dense video captioning. After partitioning each untrimmed long video as
multiple event proposals, where each event proposal consists of a set of short
video segments, we extract visual feature (e.g., C3D feature) from each segment
and use the existing image/video captioning approach to generate one sentence
description for this segment. Considering that the generated sentences contain
rich semantic descriptions about the whole event proposal, we formulate the
dense video captioning task as a visual cue aided sentence summarization
problem and propose a new two stage Long Short Term Memory (LSTM) approach
equipped with a new hierarchical attention mechanism to summarize all generated
sentences as one descriptive sentence with the aid of visual features.
Specifically, the first-stage LSTM network takes all semantic words from the
generated sentences and the visual features from all segments within one event
proposal as the input, and acts as the encoder to effectively summarize both
semantic and visual information related to this event proposal. The
second-stage LSTM network takes the output from the first-stage LSTM network
and the visual features from all video segments within one event proposal as
the input, and acts as the decoder to generate one descriptive sentence for
this event proposal. Our comprehensive experiments on the ActivityNet Captions
dataset demonstrate the effectiveness of our newly proposed DaS framework for
dense video captioning.

</details>


### [94] [Causal Representation Learning with Observational Grouping for CXR Classification](https://arxiv.org/abs/2506.20582)
*Rajat Rasal,Avinash Kori,Ben Glocker*

Main category: cs.CV

TL;DR: The paper introduces an end-to-end framework for learning identifiable causal representations in disease classification for chest X-rays.


<details>
  <summary>Details</summary>
Motivation: To improve the generalisability and robustness of task-specific features in medical imaging by uncovering true causal relationships in data.

Method: The approach involves grouping observations to enforce invariance regarding race, sex, and imaging views, applied within an end-to-end learning framework.

Result: Causal representations demonstrated improved generalisability and robustness across multiple disease classification tasks in chest X-rays.

Conclusion: Grouping observations to learn identifiable causal representations enhances performance in classification tasks, particularly in handling diverse factors like race, sex, and imaging views.

Abstract: Identifiable causal representation learning seeks to uncover the true causal
relationships underlying a data generation process. In medical imaging, this
presents opportunities to improve the generalisability and robustness of
task-specific latent features. This work introduces the concept of grouping
observations to learn identifiable representations for disease classification
in chest X-rays via an end-to-end framework. Our experiments demonstrate that
these causal representations improve generalisability and robustness across
multiple classification tasks when grouping is used to enforce invariance w.r.t
race, sex, and imaging views.

</details>


### [95] [Dense Video Captioning using Graph-based Sentence Summarization](https://arxiv.org/abs/2506.20583)
*Zhiwang Zhang,Dong Xu,Wanli Ouyang,Luping Zhou*

Main category: cs.CV

TL;DR: The paper addresses limitations in dense video captioning by proposing a graph-based partition-and-summarization (GPaS) framework that enhances sentence generation from long videos via scene evolution analysis and semantic word relationships.


<details>
  <summary>Details</summary>
Motivation: The goal is to improve dense video captioning, particularly when scenes and objects change significantly within a long event proposal, which existing methods fail to effectively capture.

Method: The proposed GPaS framework operates in two stages: Partitioning long event proposals into short video segments for better captioning, and summarizing these finer-level captions into a cohesive sentence. It incorporates a novel GCN-LSTM interaction module that models semantic word relationships using graphs and visual aids.

Result: The GPaS framework showed superior performance against state-of-the-art methods in dense video captioning on the ActivityNet Captions and YouCook II benchmark datasets.

Conclusion: The proposed framework resolves the challenge of capturing scene evolution in dense video captioning, demonstrating its effectiveness through improved results and innovative use of GCN-LSTM for semantic understanding.

Abstract: Recently, dense video captioning has made attractive progress in detecting
and captioning all events in a long untrimmed video. Despite promising results
were achieved, most existing methods do not sufficiently explore the scene
evolution within an event temporal proposal for captioning, and therefore
perform less satisfactorily when the scenes and objects change over a
relatively long proposal. To address this problem, we propose a graph-based
partition-and-summarization (GPaS) framework for dense video captioning within
two stages. For the ``partition" stage, a whole event proposal is split into
short video segments for captioning at a finer level. For the ``summarization"
stage, the generated sentences carrying rich description information for each
segment are summarized into one sentence to describe the whole event. We
particularly focus on the ``summarization" stage, and propose a framework that
effectively exploits the relationship between semantic words for summarization.
We achieve this goal by treating semantic words as nodes in a graph and
learning their interactions by coupling Graph Convolutional Network (GCN) and
Long Short Term Memory (LSTM), with the aid of visual cues. Two schemes of
GCN-LSTM Interaction (GLI) modules are proposed for seamless integration of GCN
and LSTM. The effectiveness of our approach is demonstrated via an extensive
comparison with the state-of-the-arts methods on the two benchmarks ActivityNet
Captions dataset and YouCook II dataset.

</details>


### [96] [TRIM: A Self-Supervised Video Summarization Framework Maximizing Temporal Relative Information and Representativeness](https://arxiv.org/abs/2506.20588)
*Pritam Mishra,Coloma Ballester,Dimosthenis Karatzas*

Main category: cs.CV

TL;DR: The paper introduces a self-supervised video summarization model that balances efficiency and performance without relying on attention mechanisms, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: The growing demand for efficient video content summarization highlights limitations of supervised and attention-based methods, particularly regarding computational costs and adaptability across datasets.

Method: The method uses a self-supervised framework with Markov process-driven loss metrics and a two-stage learning paradigm, avoiding reliance on attention, RNNs, or transformers.

Result: The proposed model achieves state-of-the-art results on SUMME and TVSUM datasets, outperforming all unsupervised methods and rivaling supervised ones.

Conclusion: This paper introduces a more efficient and generalizable approach to video summarization, challenging dependencies on complex supervised and attention-based architectures.

Abstract: The increasing ubiquity of video content and the corresponding demand for
efficient access to meaningful information have elevated video summarization
and video highlights as a vital research area. However, many state-of-the-art
methods depend heavily either on supervised annotations or on attention-based
models, which are computationally expensive and brittle in the face of
distribution shifts that hinder cross-domain applicability across datasets. We
introduce a pioneering self-supervised video summarization model that captures
both spatial and temporal dependencies without the overhead of attention, RNNs,
or transformers. Our framework integrates a novel set of Markov process-driven
loss metrics and a two-stage self supervised learning paradigm that ensures
both performance and efficiency. Our approach achieves state-of-the-art
performance on the SUMME and TVSUM datasets, outperforming all existing
unsupervised methods. It also rivals the best supervised models, demonstrating
the potential for efficient, annotation-free architectures. This paves the way
for more generalizable video summarization techniques and challenges the
prevailing reliance on complex architectures.

</details>


### [97] [WonderFree: Enhancing Novel View Quality and Cross-View Consistency for 3D Scene Exploration](https://arxiv.org/abs/2506.20590)
*Chaojun Ni,Jie Li,Haoyun Li,Hengyu Liu,Xiaofeng Wang,Zheng Zhu,Guosheng Zhao,Boyuan Wang,Chenxin Li,Guan Huang,Wenjun Mei*

Main category: cs.CV

TL;DR: WonderFree enables interactive 3D scene generation from a single image with enhanced freedom of exploration from any perspective, addressing challenges in rendering quality and view consistency.


<details>
  <summary>Details</summary>
Motivation: Current 3D generation methods struggle with limited explorability, particularly in rendering high-quality images from viewpoints beyond the original scene or in unseen areas.

Method: The methodology involves decoupling the problem into improving novel view quality using a video restoration model called WorldRestorer and enhancing cross-view consistency through ConsistView, a multi-view joint restoration mechanism. A data collection pipeline is also introduced for training.

Result: Experimental results show that WonderFree improves rendering quality and spatiotemporal coherence across diverse viewpoints. It showed a 77.20% user preference against an existing system, WonderWorld.

Conclusion: WonderFree successfully enhances the interactive 3D experience by offering better visual quality and coherence, making immersive exploration seamless. The work will be made open-source for further accessibility.

Abstract: Interactive 3D scene generation from a single image has gained significant
attention due to its potential to create immersive virtual worlds. However, a
key challenge in current 3D generation methods is the limited explorability,
which cannot render high-quality images during larger maneuvers beyond the
original viewpoint, particularly when attempting to move forward into unseen
areas. To address this challenge, we propose WonderFree, the first model that
enables users to interactively generate 3D worlds with the freedom to explore
from arbitrary angles and directions. Specifically, we decouple this challenge
into two key subproblems: novel view quality, which addresses visual artifacts
and floating issues in novel views, and cross-view consistency, which ensures
spatial consistency across different viewpoints. To enhance rendering quality
in novel views, we introduce WorldRestorer, a data-driven video restoration
model designed to eliminate floaters and artifacts. In addition, a data
collection pipeline is presented to automatically gather training data for
WorldRestorer, ensuring it can handle scenes with varying styles needed for 3D
scene generation. Furthermore, to improve cross-view consistency, we propose
ConsistView, a multi-view joint restoration mechanism that simultaneously
restores multiple perspectives while maintaining spatiotemporal coherence.
Experimental results demonstrate that WonderFree not only enhances rendering
quality across diverse viewpoints but also significantly improves global
coherence and consistency. These improvements are confirmed by CLIP-based
metrics and a user study showing a 77.20% preference for WonderFree over
WonderWorld enabling a seamless and immersive 3D exploration experience. The
code, model, and data will be publicly available.

</details>


### [98] [SFNet: Fusion of Spatial and Frequency-Domain Features for Remote Sensing Image Forgery Detection](https://arxiv.org/abs/2506.20599)
*Ji Qi,Xinchang Zhang,Dingqi Ye,Yongjia Ruan,Xin Guo,Shaowen Wang,Haifeng Li*

Main category: cs.CV

TL;DR: The paper introduces SFNet, a framework leveraging spatial and frequency domain features to detect fake remote sensing imagery, achieving better accuracy and generalization compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: To address the growing issue of fake remote sensing imagery created by advanced generative AI, which conventional methods struggle to detect due to evolving and diverse artifacts.

Method: SFNet utilizes two independent feature extractors for spatial and frequency domain features and employs modules for feature alignment, fusion, and redundancy suppression to improve detection capabilities.

Result: Experiments on three datasets show that SFNet improves accuracy by 4%-15.18% compared to existing forgery detection methods and demonstrates strong generalization capability.

Conclusion: SFNet effectively detects fake remote sensing imagery across diverse datasets by combining spatial and frequency domain features, setting new benchmarks in accuracy and robustness.

Abstract: The rapid advancement of generative artificial intelligence is producing fake
remote sensing imagery (RSI) that is increasingly difficult to detect,
potentially leading to erroneous intelligence, fake news, and even conspiracy
theories. Existing forgery detection methods typically rely on single visual
features to capture predefined artifacts, such as spatial-domain cues to detect
forged objects like roads or buildings in RSI, or frequency-domain features to
identify artifacts from up-sampling operations in adversarial generative
networks (GANs). However, the nature of artifacts can significantly differ
depending on geographic terrain, land cover types, or specific features within
the RSI. Moreover, these complex artifacts evolve as generative models become
more sophisticated. In short, over-reliance on a single visual cue makes
existing forgery detectors struggle to generalize across diverse remote sensing
data. This paper proposed a novel forgery detection framework called SFNet,
designed to identify fake images in diverse remote sensing data by leveraging
spatial and frequency domain features. Specifically, to obtain rich and
comprehensive visual information, SFNet employs two independent feature
extractors to capture spatial and frequency domain features from input RSIs. To
fully utilize the complementary domain features, the domain feature mapping
module and the hybrid domain feature refinement module(CBAM attention) of SFNet
are designed to successively align and fuse the multi-domain features while
suppressing redundant information. Experiments on three datasets show that
SFNet achieves an accuracy improvement of 4%-15.18% over the state-of-the-art
RS forgery detection methods and exhibits robust generalization capabilities.
The code is available at https://github.com/GeoX-Lab/RSTI/tree/main/SFNet.

</details>


### [99] [Video Perception Models for 3D Scene Synthesis](https://arxiv.org/abs/2506.20601)
*Rui Huang,Guangyao Zhai,Zuria Bauer,Marc Pollefeys,Federico Tombari,Leonidas Guibas,Gao Huang,Francis Engelmann*

Main category: cs.CV

TL;DR: The paper introduces VIPScene, a framework for automating 3D scene synthesis by leveraging video generation models for enhanced realism and consistency.


<details>
  <summary>Details</summary>
Motivation: Current 3D scene synthesis approaches struggle with limited spatial reasoning in language models and viewpoint/multi-view issues in image generation models.

Method: VIPScene integrates video generation, 3D reconstruction, and perception models based on text or image prompts, achieving coherent object placement and scene layouts.

Result: VIPScene outperforms existing methods in realism and generalizability across scenarios, supported by experiment results.

Conclusion: VIPScene is an effective framework for generating realistic and structurally consistent 3D scenes, promising advancements in applications like robotics, gaming, and VR.

Abstract: Traditionally, 3D scene synthesis requires expert knowledge and significant
manual effort. Automating this process could greatly benefit fields such as
architectural design, robotics simulation, virtual reality, and gaming. Recent
approaches to 3D scene synthesis often rely on the commonsense reasoning of
large language models (LLMs) or strong visual priors of modern image generation
models. However, current LLMs demonstrate limited 3D spatial reasoning ability,
which restricts their ability to generate realistic and coherent 3D scenes.
Meanwhile, image generation-based methods often suffer from constraints in
viewpoint selection and multi-view inconsistencies. In this work, we present
Video Perception models for 3D Scene synthesis (VIPScene), a novel framework
that exploits the encoded commonsense knowledge of the 3D physical world in
video generation models to ensure coherent scene layouts and consistent object
placements across views. VIPScene accepts both text and image prompts and
seamlessly integrates video generation, feedforward 3D reconstruction, and
open-vocabulary perception models to semantically and geometrically analyze
each object in a scene. This enables flexible scene synthesis with high realism
and structural consistency. For more precise analysis, we further introduce
First-Person View Score (FPVScore) for coherence and plausibility evaluation,
utilizing continuous first-person perspective to capitalize on the reasoning
ability of multimodal large language models. Extensive experiments show that
VIPScene significantly outperforms existing methods and generalizes well across
diverse scenarios. The code will be released.

</details>


### [100] [Shape2Animal: Creative Animal Generation from Natural Silhouettes](https://arxiv.org/abs/2506.20616)
*Quoc-Duy Tran,Anh-Tuan Vo,Dinh-Khoi Vo,Tam V. Nguyen,Minh-Triet Tran,Trung-Nghia Le*

Main category: cs.CV

TL;DR: The Shape2Animal framework mimics human pareidolia to reinterpret natural object silhouettes, like clouds or stones, as animal forms using advanced vision-language and text-to-image models.


<details>
  <summary>Details</summary>
Motivation: This research aims to replicate humans' capacity to perceive meaningful patterns in ambiguous visuals (pareidolia) through computational methods.

Method: The method involves extracting natural object silhouettes, determining appropriate animal forms using vision-language models, and synthetically blending the generated animal images into the original scenes using text-to-image diffusion models.

Result: Shape2Animal robustly demonstrates its creative potential on diverse real-world inputs, generating visually coherent compositions.

Conclusion: The framework offers potential applications in visual storytelling, digital art, educational content, and interactive media, showcasing both technical and artistic capabilities.

Abstract: Humans possess a unique ability to perceive meaningful patterns in ambiguous
stimuli, a cognitive phenomenon known as pareidolia. This paper introduces
Shape2Animal framework to mimics this imaginative capacity by reinterpreting
natural object silhouettes, such as clouds, stones, or flames, as plausible
animal forms. Our automated framework first performs open-vocabulary
segmentation to extract object silhouette and interprets semantically
appropriate animal concepts using vision-language models. It then synthesizes
an animal image that conforms to the input shape, leveraging text-to-image
diffusion model and seamlessly blends it into the original scene to generate
visually coherent and spatially consistent compositions. We evaluated
Shape2Animal on a diverse set of real-world inputs, demonstrating its
robustness and creative potential. Our Shape2Animal can offer new opportunities
for visual storytelling, educational content, digital art, and interactive
media design. Our project page is here: https://shape2image.github.io

</details>


### [101] [Joint attitude estimation and 3D neural reconstruction of non-cooperative space objects](https://arxiv.org/abs/2506.20638)
*Clément Forray,Pauline Delporte,Nicolas Delaygue,Florence Genin,Dawa Derksen*

Main category: cs.CV

TL;DR: The paper explores using Neural Radiance Fields (NeRF) to reconstruct 3D models of space objects from challenging simulated conditions.


<details>
  <summary>Details</summary>
Motivation: To improve Space Situational Awareness by addressing 3D reconstruction challenges of non-cooperative space objects for vital applications like debris removal.

Method: Joint optimization of camera poses and Neural Radiance Fields using iterative training with regularized pose adjustments.

Result: The most accurate 3D reconstruction results were achieved with successive one-by-one image training and pose regularization.

Conclusion: Optimizing NeRF with camera pose estimation improves 3D modeling in challenging space observation scenarios.

Abstract: Obtaining a better knowledge of the current state and behavior of objects
orbiting Earth has proven to be essential for a range of applications such as
active debris removal, in-orbit maintenance, or anomaly detection. 3D models
represent a valuable source of information in the field of Space Situational
Awareness (SSA). In this work, we leveraged Neural Radiance Fields (NeRF) to
perform 3D reconstruction of non-cooperative space objects from simulated
images. This scenario is challenging for NeRF models due to unusual camera
characteristics and environmental conditions : mono-chromatic images, unknown
object orientation, limited viewing angles, absence of diffuse lighting etc. In
this work we focus primarly on the joint optimization of camera poses alongside
the NeRF. Our experimental results show that the most accurate 3D
reconstruction is achieved when training with successive images one-by-one. We
estimate camera poses by optimizing an uniform rotation and use regularization
to prevent successive poses from being too far apart.

</details>


### [102] [Disentangled representations of microscopy images](https://arxiv.org/abs/2506.20649)
*Jacopo Dapueto,Vito Paolo Pastore,Nicoletta Noceti,Francesca Odone*

Main category: cs.CV

TL;DR: This paper proposes using Disentangled Representation Learning (DRL) for improved interpretability in microscopy image classification, demonstrated across three datasets.


<details>
  <summary>Details</summary>
Motivation: Microscopy image analysis is critical for various applications, and modern systems have led to an abundance of images. There is a need for automated methods, but interpretability in deep learning remains challenging.

Method: The paper introduces a Disentangled Representation Learning framework, leveraging representations trained on synthetic data to improve interpretability and accuracy in microscopic image classification.

Result: Using benchmark datasets from plankton, yeast vacuoles, and human cells, the study demonstrates that DRL provides a balance between accuracy and interpretability.

Conclusion: DRL enhances interpretability without significant trade-offs in accuracy, making it suitable for microscopy image analysis.

Abstract: Microscopy image analysis is fundamental for different applications, from
diagnosis to synthetic engineering and environmental monitoring. Modern
acquisition systems have granted the possibility to acquire an escalating
amount of images, requiring a consequent development of a large collection of
deep learning-based automatic image analysis methods. Although deep neural
networks have demonstrated great performance in this field, interpretability,
an essential requirement for microscopy image analysis, remains an open
challenge.
  This work proposes a Disentangled Representation Learning (DRL) methodology
to enhance model interpretability for microscopy image classification.
Exploiting benchmark datasets from three different microscopic image domains
(plankton, yeast vacuoles, and human cells), we show how a DRL framework, based
on transferring a representation learnt from synthetic data, can provide a good
trade-off between accuracy and interpretability in this domain.

</details>


### [103] [MMSearch-R1: Incentivizing LMMs to Search](https://arxiv.org/abs/2506.20670)
*Jinming Wu,Zihao Deng,Wei Li,Yiding Liu,Bo You,Bo Li,Zejun Ma,Ziwei Liu*

Main category: cs.CV

TL;DR: This paper introduces MMSearch-R1, a reinforcement learning framework enabling large multimodal models (LMMs) to perform efficient, multi-turn, multimodal searches in real-world scenarios.


<details>
  <summary>Details</summary>
Motivation: Current multimodal search and retrieval methods like RAG lack flexibility and efficiency in real-world, knowledge-intensive tasks, hampering robust deployment of LMMs.

Method: MMSearch-R1 uses reinforcement learning to integrate multimodal search (images and text) with reward mechanisms based on search penalty and outcomes. A new dataset is curated to simulate diverse multimodal search scenarios.

Result: MMSearch-R1 outperforms RAG-based baselines of similar size, matches larger RAG-based models' performance, and reduces search calls by over 30% on VQA tasks.

Conclusion: MMSearch-R1 demonstrates how reinforcement learning can enable LMMs to perform efficient and adaptive multimodal searches, setting a foundation for future advancements in this area.

Abstract: Robust deployment of large multimodal models (LMMs) in real-world scenarios
requires access to external knowledge sources, given the complexity and dynamic
nature of real-world information. Existing approaches such as
retrieval-augmented generation (RAG) and prompt engineered search agents rely
on rigid pipelines, often leading to inefficient or excessive search behaviors.
We present MMSearch-R1, the first end-to-end reinforcement learning framework
that enables LMMs to perform on-demand, multi-turn search in real-world
Internet environments. Our framework integrates both image and text search
tools, allowing the model to reason about when and how to invoke them guided by
an outcome-based reward with a search penalty. To support training, We collect
a multimodal search VQA dataset through a semi-automated pipeline that covers
diverse visual and textual knowledge needs and curate a search-balanced subset
with both search-required and search-free samples, which proves essential for
shaping efficient and on-demand search behavior. Extensive experiments on
knowledge-intensive and info-seeking VQA tasks show that our model not only
outperforms RAG-based baselines of the same model size, but also matches the
performance of a larger RAG-based model while reducing search calls by over
30%. We further analyze key empirical findings to offer actionable insights for
advancing research in multimodal search.

</details>


### [104] [IPFormer: Visual 3D Panoptic Scene Completion with Context-Adaptive Instance Proposals](https://arxiv.org/abs/2506.20671)
*Markus Gross,Aya Fahmy,Danit Niwattananan,Dominik Muhle,Rui Song,Daniel Cremers,Henri Meeß*

Main category: cs.CV

TL;DR: The paper introduces IPFormer, a novel approach for vision-based 3D Panoptic Scene Completion (PSC), which includes dynamically adaptive instance proposals for improved scene understanding and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: To enhance object-level scene understanding by advancing Panoptic Scene Completion (PSC) methods, especially for scenarios relying on camera images rather than LiDAR data.

Method: IPFormer leverages context-adaptive instance proposals, initialized from image context and refined with attention-based encoding and decoding, to establish semantic instance-voxel relationships. This dynamic approach is applied for PSC in vision-based setups.

Result: IPFormer achieves state-of-the-art performance in panoptic metrics (PQ$^	ext{dagger}$ and PQ-All), improves Thing-metrics by an average of 18.65%, and reduces runtime by over 14 times compared to prior methods.

Conclusion: Context-adaptive instance proposals derived from image data significantly improve accuracy and efficiency in 3D Panoptic Scene Completion, marking a breakthrough for camera-based vision systems.

Abstract: Semantic Scene Completion (SSC) has emerged as a pivotal approach for jointly
learning scene geometry and semantics, enabling downstream applications such as
navigation in mobile robotics. The recent generalization to Panoptic Scene
Completion (PSC) advances the SSC domain by integrating instance-level
information, thereby enhancing object-level sensitivity in scene understanding.
While PSC was introduced using LiDAR modality, methods based on camera images
remain largely unexplored. Moreover, recent Transformer-based SSC approaches
utilize a fixed set of learned queries to reconstruct objects within the scene
volume. Although these queries are typically updated with image context during
training, they remain static at test time, limiting their ability to
dynamically adapt specifically to the observed scene. To overcome these
limitations, we propose IPFormer, the first approach that leverages
context-adaptive instance proposals at train and test time to address
vision-based 3D Panoptic Scene Completion. Specifically, IPFormer adaptively
initializes these queries as panoptic instance proposals derived from image
context and further refines them through attention-based encoding and decoding
to reason about semantic instance-voxel relationships. Experimental results
show that our approach surpasses state-of-the-art methods in overall panoptic
metrics PQ$^\dagger$ and PQ-All, matches performance in individual metrics, and
achieves a runtime reduction exceeding 14$\times$. Furthermore, our ablation
studies reveal that dynamically deriving instance proposals from image context,
as opposed to random initialization, leads to a 3.62% increase in PQ-All and a
remarkable average improvement of 18.65% in combined Thing-metrics. These
results highlight our introduction of context-adaptive instance proposals as a
pioneering effort in addressing vision-based 3D Panoptic Scene Completion.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [105] [MAIZX: A Carbon-Aware Framework for Optimizing Cloud Computing Emissions](https://arxiv.org/abs/2506.19972)
*Federico Ruilova,Ernst Gunnar Gran,Sven-Arne Reinemo*

Main category: cs.DC

TL;DR: The MAIZX framework optimizes cloud operations, reducing CO2 emissions by 85.68% using real-time and forecasted energy and environmental metrics.


<details>
  <summary>Details</summary>
Motivation: Rising energy consumption and carbon emissions in cloud computing necessitate solutions to achieve net-zero emissions by 2050, especially for private cloud infrastructures.

Method: The proposed MAIZX framework dynamically ranks and allocates cloud resources based on real-time and forecasted metrics such as carbon intensity, PUE, and energy consumption.

Result: MAIZX achieved an 85.68% reduction in CO2 emissions compared to baseline operations and proved scalable across distributed data centers.

Conclusion: MAIZX offers an effective solution for reducing emissions in cloud operations while maintaining scalability and operational efficiency.

Abstract: Cloud computing drives innovation but also poses significant environmental
challenges due to its high-energy consumption and carbon emissions. Data
centers account for 2-4% of global energy usage, and the ICT sector's share of
electricity consumption is projected to reach 40% by 2040. As the goal of
achieving net-zero emissions by 2050 becomes increasingly urgent, there is a
growing need for more efficient and transparent solutions, particularly for
private cloud infrastructures, which are utilized by 87% of organizations,
despite the dominance of public-cloud systems.
  This study evaluates the MAIZX framework, designed to optimize cloud
operations and reduce carbon footprint by dynamically ranking resources,
including data centers, edge computing nodes, and multi-cloud environments,
based on real-time and forecasted carbon intensity, Power Usage Effectiveness
(PUE), and energy consumption. Leveraging a flexible ranking algorithm, MAIZX
achieved an 85.68% reduction in CO2 emissions compared to baseline hypervisor
operations. Tested across geographically distributed data centers, the
framework demonstrates scalability and effectiveness, directly interfacing with
hypervisors to optimize workloads in private, hybrid, and multi-cloud
environments. MAIZX integrates real-time data on carbon intensity, power
consumption, and carbon footprint, as well as forecasted values, into cloud
management, providing a robust tool for enhancing climate performance potential
while maintaining operational efficiency.

</details>


### [106] [On the $h$-majority dynamics with many opinions](https://arxiv.org/abs/2506.20218)
*Francesco d'Amore,Niccolò D'Archivio,George Giakkoupis,Emanuele Natale*

Main category: cs.DC

TL;DR: The paper presents an upper bound on the convergence time of $h$-majority dynamics with $k$ opinions under certain conditions, showing fast consensus in $O(\log n)$ rounds.


<details>
  <summary>Details</summary>
Motivation: To address the lack of upper bounds on the convergence time for $h$-majority dynamics, especially for non-constant $h$ and $k$, and under specific bias scenarios.

Method: The authors analyze the process mathematically under assumptions of initial bias, plurality opinion support thresholds, and relationships between $h$, $k$, and $n$.

Result: They establish with high probability that the dynamics converge to consensus in $O(\log n)$ rounds if certain bias and parameter conditions are met.

Conclusion: The results refine existing lower bounds, showing that faster convergence is possible under less stringent bias requirements than previously thought.

Abstract: We present the first upper bound on the convergence time to consensus of the
well-known $h$-majority dynamics with $k$ opinions, in the synchronous setting,
for $h$ and $k$ that are both non-constant values.
  We suppose that, at the beginning of the process, there is some initial
additive bias towards some plurality opinion, that is, there is an opinion that
is supported by $x$ nodes while any other opinion is supported by strictly
fewer nodes.
  We prove that, with high probability, if the bias is $\omega(\sqrt{x})$ and
the initial plurality opinion is supported by at least $x = \omega(\log n)$
nodes, then the process converges to plurality consensus in $O(\log n)$ rounds
whenever $h = \omega(n \log n / x)$.
  A main corollary is the following: if $k = o(n / \log n)$ and the process
starts from an almost-balanced configuration with an initial bias of magnitude
$\omega(\sqrt{n/k})$ towards the initial plurality opinion, then any function
$h = \omega(k \log n)$ suffices to guarantee convergence to consensus in
$O(\log n)$ rounds, with high probability.
  Our upper bound shows that the lower bound of $\Omega(k / h^2)$ rounds to
reach consensus given by Becchetti et al.\ (2017) cannot be pushed further than
$\widetilde{\Omega}(k / h)$.
  Moreover, the bias we require is asymptotically smaller than the
$\Omega(\sqrt{n\log n})$ bias that guarantees plurality consensus in the
$3$-majority dynamics: in our case, the required bias is at most any
(arbitrarily small) function in $\omega(\sqrt{x})$ for any value of $k \ge 2$.

</details>


### [107] [PAT: a new algorithm for all-gather and reduce-scatter operations at scale](https://arxiv.org/abs/2506.20252)
*Sylvain Jeaugey*

Main category: cs.DC

TL;DR: The paper introduces a new algorithm, PAT, for efficient all-gather and reduce-scatter operations, aiming to overcome inefficiencies of the ring algorithm for small or large-scale scenarios.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the inefficiency of the ring algorithm used in NCCL for small-sized or large-scale operations due to its linear latency.

Method: Proposed a Parallel Aggregated Trees (PAT) algorithm that uses logarithmic network transfers, minimizes long-distance communication, and requires logarithmic internal buffers.

Result: PAT algorithm performs efficiently with logarithmic latency and optimized communication, allowing NCCL to work better at smaller sizes and scale.

Conclusion: PAT improves NCCL's performance in scenarios where the ring algorithm falls short, ensuring faster and scalable collective communication.

Abstract: This paper describes a new algorithm called PAT, for Parallel Aggregated
Trees, and which can be used to implement all-gather and reduce-scatter
operations. This algorithm works on any number of ranks, has a logarithmic
number of network transfers for small size operations, minimizes long-distance
communication, and requires a logarithmic amount of internal buffers,
independently from the total operation size. It is aimed at improving the
performance of the NCCL library in cases where the ring algorithm would be
inefficient, as its linear latency would show poor performance for small sizes
and/or at scale.

</details>


### [108] [WattsOnAI: Measuring, Analyzing, and Visualizing Energy and Carbon Footprint of AI Workloads](https://arxiv.org/abs/2506.20535)
*Hongzhen Huang,Kunming Zhang,Hanlong Liao,Kui Wu,Guoming Tang*

Main category: cs.DC

TL;DR: WattsOnAI is a toolkit for analyzing energy use, performance, and environmental impacts of AI workloads, facilitating Green AI practices.


<details>
  <summary>Details</summary>
Motivation: AI's rapid growth, especially with LLMs, brings concerns about energy usage and carbon emissions, but current tools lack systematic metrics and correlation support.

Method: The authors developed WattsOnAI, a software tool that integrates with AI frameworks to measure, analyze, and visualize energy consumption, hardware performance, and emissions, enabling standardized reporting and fine-grained data analysis.

Result: WattsOnAI provides benchmarking, reproducibility tools, and correlation analysis while identifying bottlenecks and enhancing performance in AI workloads.

Conclusion: WattsOnAI supports sustainable AI by encouraging environmental impact assessment alongside performance metrics, driving "Green AI" advancements.

Abstract: The rapid advancement of AI, particularly large language models (LLMs), has
raised significant concerns about the energy use and carbon emissions
associated with model training and inference. However, existing tools for
measuring and reporting such impacts are often fragmented, lacking systematic
metric integration and offering limited support for correlation analysis among
them. This paper presents WattsOnAI, a comprehensive software toolkit for the
measurement, analysis, and visualization of energy use, power draw, hardware
performance, and carbon emissions across AI workloads. By seamlessly
integrating with existing AI frameworks, WattsOnAI offers standardized reports
and exports fine-grained time-series data to support benchmarking and
reproducibility in a lightweight manner. It further enables in-depth
correlation analysis between hardware metrics and model performance and thus
facilitates bottleneck identification and performance enhancement. By
addressing critical limitations in existing tools, WattsOnAI encourages the
research community to weigh environmental impact alongside raw performance of
AI workloads and advances the shift toward more sustainable "Green AI"
practices. The code is available at https://github.com/SusCom-Lab/WattsOnAI.

</details>


### [109] [SuperSONIC: Cloud-Native Infrastructure for ML Inferencing](https://arxiv.org/abs/2506.20657)
*Dmitry Kondratyev,Benedikt Riedel,Yuan-Tang Chou,Miles Cochran-Branson,Noah Paladino,David Schultz,Mia Liu,Javier Duarte,Philip Harris,Shih-Chieh Hsu*

Main category: cs.DC

TL;DR: The paper introduces the SuperSONIC project, enhancing SONIC ML inference by enabling scalable, efficient deployment of tasks to Kubernetes clusters with GPUs, tested across major scientific experiments.


<details>
  <summary>Details</summary>
Motivation: The growing computational demands from large-scale scientific experiments and complex ML algorithms necessitated a scalable solution for efficient resource utilization during ML inference.

Method: The authors developed SuperSONIC, a server infrastructure leveraging Kubernetes clusters and NVIDIA Triton Inference Server to standardize ML inference workflows, optimize throughput, enable load balancing, and offer monitoring capabilities.

Result: SuperSONIC was successfully deployed in large-scale experiments, including CMS, ATLAS, IceCube, and LIGO, and tested on infrastructures like Purdue University and NRP clusters, demonstrating scalability and efficiency.

Conclusion: SuperSONIC provides a reusable, configurable, and scalable framework, addressing modern computational challenges and enhancing accelerator-based ML inference deployment for scientific and industrial applications.

Abstract: The increasing computational demand from growing data rates and complex
machine learning (ML) algorithms in large-scale scientific experiments has
driven the adoption of the Services for Optimized Network Inference on
Coprocessors (SONIC) approach. SONIC accelerates ML inference by offloading it
to local or remote coprocessors to optimize resource utilization. Leveraging
its portability to different types of coprocessors, SONIC enhances data
processing and model deployment efficiency for cutting-edge research in high
energy physics (HEP) and multi-messenger astrophysics (MMA). We developed the
SuperSONIC project, a scalable server infrastructure for SONIC, enabling the
deployment of computationally intensive tasks to Kubernetes clusters equipped
with graphics processing units (GPUs). Using NVIDIA Triton Inference Server,
SuperSONIC decouples client workflows from server infrastructure, standardizing
communication, optimizing throughput, load balancing, and monitoring.
SuperSONIC has been successfully deployed for the CMS and ATLAS experiments at
the CERN Large Hadron Collider (LHC), the IceCube Neutrino Observatory
(IceCube), and the Laser Interferometer Gravitational-Wave Observatory (LIGO)
and tested on Kubernetes clusters at Purdue University, the National Research
Platform (NRP), and the University of Chicago. SuperSONIC addresses the
challenges of the Cloud-native era by providing a reusable, configurable
framework that enhances the efficiency of accelerator-based inference
deployment across diverse scientific domains and industries.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [110] [Position: Machine Learning Conferences Should Establish a "Refutations and Critiques" Track](https://arxiv.org/abs/2506.19882)
*Rylan Schaeffer,Joshua Kazdan,Yegor Denisov-Blanch,Brando Miranda,Matthias Gerstgrasser,Susan Zhang,Andreas Haupt,Isha Gupta,Elyas Obbad,Jesse Dodge,Jessica Zosa Forde,Koustuv Sinha,Francesco Orabona,Sanmi Koyejo,David Donoho*

Main category: cs.LG

TL;DR: The paper proposes the creation of a 'Refutations and Critiques' (R & C) track in ML conferences to systematically address and correct research errors.


<details>
  <summary>Details</summary>
Motivation: Rapid advancements in machine learning have led to flawed studies being accepted in conferences, with insufficient mechanisms for systematic correction.

Method: Arguing for a position paper advocating the establishment of a dedicated R & C Track, and discussing its design, review principles, and illustrative examples.

Result: The paper outlines how such a track could foster a self-correcting research ecosystem in ML.

Conclusion: ML conferences should institutionalize processes to enable systematic correction and critique of prior research errors.

Abstract: Science progresses by iteratively advancing and correcting humanity's
understanding of the world. In machine learning (ML) research, rapid
advancements have led to an explosion of publications, but have also led to
misleading, incorrect, flawed or perhaps even fraudulent studies being accepted
and sometimes highlighted at ML conferences due to the fallibility of peer
review. While such mistakes are understandable, ML conferences do not offer
robust processes to help the field systematically correct when such errors are
made.This position paper argues that ML conferences should establish a
dedicated "Refutations and Critiques" (R & C) Track. This R & C Track would
provide a high-profile, reputable platform to support vital research that
critically challenges prior research, thereby fostering a dynamic
self-correcting research ecosystem. We discuss key considerations including
track design, review principles, potential pitfalls, and provide an
illustrative example submission concerning a recent ICLR 2025 Oral. We conclude
that ML conferences should create official, reputable mechanisms to help ML
research self-correct.

</details>


### [111] [Neuromorphic Wireless Split Computing with Resonate-and-Fire Neurons](https://arxiv.org/abs/2506.20015)
*Dengyu Wu,Jiechen Chen,H. Vincent Poor,Bipin Rajendran,Osvaldo Simeone*

Main category: cs.LG

TL;DR: The study introduces a neuromorphic computing system with resonate-and-fire (RF) neurons that efficiently process streaming signals. This approach reduces energy consumption while maintaining competitive accuracy.


<details>
  <summary>Details</summary>
Motivation: To address inefficiencies in processing spectral features of real-time streaming signals using conventional spiking neurons like LIF, particularly in edge applications such as wireless sensing and audio recognition.

Method: The paper proposes a split computing architecture using RF neurons to process time-domain signals without the need for spectral pre-processing. It combines an OFDM-based wireless interface for spike transmission and evaluates the system on tasks like audio and modulation classification.

Result: The RF-SNN architecture demonstrates comparable accuracy to traditional LIF-SNNs and ANNs, while substantially reducing spike rates and energy consumption for both computation and communication.

Conclusion: The RF-SNN system provides an energy-efficient solution for time-series applications, making it suitable for edge devices that require low computational and transmission costs.

Abstract: Neuromorphic computing offers an energy-efficient alternative to conventional
deep learning accelerators for real-time time-series processing. However, many
edge applications, such as wireless sensing and audio recognition, generate
streaming signals with rich spectral features that are not effectively captured
by conventional leaky integrate-and-fire (LIF) spiking neurons. This paper
investigates a wireless split computing architecture that employs
resonate-and-fire (RF) neurons with oscillatory dynamics to process time-domain
signals directly, eliminating the need for costly spectral pre-processing. By
resonating at tunable frequencies, RF neurons extract time-localized spectral
features while maintaining low spiking activity. This temporal sparsity
translates into significant savings in both computation and transmission
energy. Assuming an OFDM-based analog wireless interface for spike
transmission, we present a complete system design and evaluate its performance
on audio classification and modulation classification tasks. Experimental
results show that the proposed RF-SNN architecture achieves comparable accuracy
to conventional LIF-SNNs and ANNs, while substantially reducing spike rates and
total energy consumption during inference and communication.

</details>


### [112] [STIMULUS: Achieving Fast Convergence and Low Sample Complexity in Stochastic Multi-Objective Learning](https://arxiv.org/abs/2506.19883)
*Zhuqing Liu,Chaosheng Dong,Michinari Momma,Simone Shao,Shaoyuan Xu,Yan Gao,Haibo Yang,Jia Liu*

Main category: cs.LG

TL;DR: The paper introduces STIMULUS, a novel algorithm for multi-objective optimization (MOO), addressing challenges in convergence rate and sample complexity. Enhanced versions with momentum and adaptive batching are also presented.


<details>
  <summary>Details</summary>
Motivation: The limitations of existing MOO methods, notably their unsatisfactory convergence rate and high sample complexity, motivate the authors to present a solution.

Method: The authors propose STIMULUS, a recursive framework for updating stochastic gradient estimates, along with STIMULUS-M incorporating momentum and STIMULUS+/STIMULUS-M+ featuring adaptive batching.

Result: The paper achieves state-of-the-art convergence rates and sample complexities in both non-convex and strongly convex problem settings.

Conclusion: STIMULUS improves MOO methods by offering robust convergence with reduced sample complexity, thereby enhancing practical applicability in various fields.

Abstract: Recently, multi-objective optimization (MOO) has gained attention for its
broad applications in ML, operations research, and engineering. However, MOO
algorithm design remains in its infancy and many existing MOO methods suffer
from unsatisfactory convergence rate and sample complexity performance. To
address this challenge, in this paper, we propose an algorithm called STIMULUS(
stochastic path-integrated multi-gradient recursive e\ulstimator), a new and
robust approach for solving MOO problems. Different from the traditional
methods, STIMULUS introduces a simple yet powerful recursive framework for
updating stochastic gradient estimates to improve convergence performance with
low sample complexity. In addition, we introduce an enhanced version of
STIMULUS, termed STIMULUS-M, which incorporates a momentum term to further
expedite convergence. We establish $O(1/T)$ convergence rates of the proposed
methods for non-convex settings and $O (\exp{-\mu T})$ for strongly convex
settings, where $T$ is the total number of iteration rounds. Additionally, we
achieve the state-of-the-art $O \left(n+\sqrt{n}\epsilon^{-1}\right)$ sample
complexities for non-convex settings and $O\left(n+ \sqrt{n} \ln
({\mu/\epsilon})\right)$ for strongly convex settings, where $\epsilon>0$ is a
desired stationarity error. Moreover, to alleviate the periodic full gradient
evaluation requirement in STIMULUS and STIMULUS-M, we further propose enhanced
versions with adaptive batching called STIMULUS+/ STIMULUS-M+ and provide their
theoretical analysis.

</details>


### [113] [FlightKooba: A Fast Interpretable FTP Model](https://arxiv.org/abs/2506.19885)
*Jing Lu,Xuan Wu,Yizhun Tian,Songhan Fan,Yali Fang*

Main category: cs.LG

TL;DR: A new framework, FlightKooba, is proposed for faster and more interpretable flight trajectory prediction using Koopman theory and state-space equations, reducing training time and memory significantly.


<details>
  <summary>Details</summary>
Motivation: Current applications of Koopman theory in flight trajectory prediction are not efficient or interpretable and require computationally intensive training.

Method: FlightKooba combines the HIPPO method, Koopman theory, and structural state-space equations to directly construct Koopman operators from data, reducing the number of trainable parameters and enhancing interpretability.

Result: Experiments show FlightKooba has reduced training time, cut memory consumption by more than 50% on many datasets, and requires 10x fewer parameters compared to existing approaches.

Conclusion: FlightKooba offers an efficient, interpretable solution for flight trajectory prediction, enabling faster computation of Koopman operators and advancing time series forecasting and control methods.

Abstract: The Koopman theory is a powerful and effective modeling tool for converting
nonlinear systems into linear representations, and flight trajectory prediction
(FTP) is a complex nonlinear system. However, current models applying the
Koopman theory to FTP tasks are not very effective, model interpretability is
indeed an issue, and the Koopman operators are computationally intensive,
resulting in long training times. To address this issue, this paper proposes a
new modeling and control framework based on the HIPPO method, the Koopman
theory, and state space equations from cybernetics: FlightKooba. Inspired by
the idea of structural state space equations, FlightKooba directly constructs
the Koopman operators from data. This makes the framework highly interpretable
and significantly reduces the number of trainable parameters in the module,
thereby greatly reducing training time. Experiments have demonstrated the
superiority of the FlightKooba modeling method in terms of time and memory
consumption (training time comparable to the Mamba module without using
CUDA-level acceleration; memory reduced by more than 50% on most datasets, with
a tenfold reduction in the number of parameters), essentially completing the
FTP task. It provides a new method for the fast computation of the Koopman
operators, opening up new possibilities for the combination of time series
forecasting and control.

</details>


### [114] [A Spatio-Temporal Point Process for Fine-Grained Modeling of Reading Behavior](https://arxiv.org/abs/2506.19999)
*Francesco Ignazio Re,Andreas Opedal,Glib Manaiev,Mario Giulianelli,Ryan Cotterell*

Main category: cs.LG

TL;DR: The paper introduces a probabilistic model of reading behaviors, leveraging a spatio-temporal point process to analyze fixations and saccades during sentence processing.


<details>
  <summary>Details</summary>
Motivation: Current methods to model reading dynamics simplify eye-tracking data by aggregation and implement strong assumptions, potentially neglecting rich spatio-temporal dynamics of reading behaviors.

Method: A marked spatio-temporal point process model is proposed, integrating Hawkes processes to simulate saccades and predictors to account for fixation durations, including spillover effects and surprisal theory.

Result: The Hawkes process model demonstrates improved alignment with human saccades compared to baseline models. However, surprisal theory contributes minimally to predicting fixation durations.

Conclusion: Surprisal theory may not effectively capture nuanced eye-movement patterns, and spatio-temporal modeling provides a more robust approach to analyze reading behaviors.

Abstract: Reading is a process that unfolds across space and time, alternating between
fixations where a reader focuses on a specific point in space, and saccades
where a reader rapidly shifts their focus to a new point. An ansatz of
psycholinguistics is that modeling a reader's fixations and saccades yields
insight into their online sentence processing. However, standard approaches to
such modeling rely on aggregated eye-tracking measurements and models that
impose strong assumptions, ignoring much of the spatio-temporal dynamics that
occur during reading. In this paper, we propose a more general probabilistic
model of reading behavior, based on a marked spatio-temporal point process,
that captures not only how long fixations last, but also where they land in
space and when they take place in time. The saccades are modeled using a Hawkes
process, which captures how each fixation excites the probability of a new
fixation occurring near it in time and space. The duration time of fixation
events is modeled as a function of fixation-specific predictors convolved
across time, thus capturing spillover effects. Empirically, our Hawkes process
model exhibits a better fit to human saccades than baselines. With respect to
fixation durations, we observe that incorporating contextual surprisal as a
predictor results in only a marginal improvement in the model's predictive
accuracy. This finding suggests that surprisal theory struggles to explain
fine-grained eye movements.

</details>


### [115] [Causal-Aware Intelligent QoE Optimization for VR Interaction with Adaptive Keyframe Extraction](https://arxiv.org/abs/2506.19890)
*Ziru Zhang,Jiadong Yu,Danny H. K. Tsang*

Main category: cs.LG

TL;DR: The paper introduces a causal-aware reinforcement learning framework to optimize Quality of Experience (QoE) in multi-user VR, addressing latency, motion synchronization, and resource fairness.


<details>
  <summary>Details</summary>
Motivation: To tackle the limitations of existing approaches that neglect causal relationships among bandwidth, CPU frequency, and user perception in multi-user VR, thereby constraining potential QoE improvements.

Method: Introduces an adaptive QoE metric based on the Weber-Fechner Law and employs a novel Partial State Causal Deep Deterministic Policy Gradient (PS-CDDPG), integrating causal inference to solve a mixed integer programming problem optimizing keyframe ratios, bandwidth, and computational resources.

Result: Experimental results using the CMU Motion Capture Database demonstrate that the proposed framework reduces latency, improves QoE, and ensures fairness more effectively than benchmark methods.

Conclusion: The proposed causal-aware RL framework successfully enhances multi-user VR experiences through efficient training, improved QoE, and better resource allocation, outperforming existing techniques.

Abstract: The optimization of quality of experience (QoE) in multi-user virtual reality
(VR) interactions demands a delicate balance between ultra-low latency,
high-fidelity motion synchronization, and equitable resource allocation. While
adaptive keyframe extraction mitigates transmission overhead, existing
approaches often overlook the causal relationships among allocated bandwidth,
CPU frequency, and user perception, limiting QoE gains. This paper proposes an
intelligent framework to maximize QoE by integrating adaptive keyframe
extraction with causal-aware reinforcement learning (RL). First, a novel QoE
metric is formulated using the Weber-Fechner Law, combining perceptual
sensitivity, attention-driven priorities, and motion reconstruction accuracy.
The QoE optimization problem is then modeled as a mixed integer programming
(MIP) task, jointly optimizing keyframe ratios, bandwidth, and computational
resources under horizon-fairness constraints. We propose Partial State Causal
Deep Deterministic Policy Gradient (PS-CDDPG), which integrates the Deep
Deterministic Policy Gradient (DDPG) method with causal influence detection. By
leveraging causal information regarding how QoE is influenced and determined by
various actions, we explore actions guided by weights calculated from causal
inference (CI), which in turn improves training efficiency. Experiments
conducted with the CMU Motion Capture Database demonstrate that our framework
significantly reduces interactive latency, enhances QoE, and maintains
fairness, achieving superior performance compared to benchmark methods.

</details>


### [116] [Orthogonal Soft Pruning for Efficient Class Unlearning](https://arxiv.org/abs/2506.19891)
*Qinghui Gong,Xue Yang,Xiaohu Tang*

Main category: cs.LG

TL;DR: This paper presents a class-aware soft pruning framework that enables fast and accurate machine unlearning, achieving near-instant forgetting of specific classes with minimal impact on retained predictive accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of efficiently and effectively removing specific knowledge from pretrained neural networks to comply with privacy regulations like the GDPR, without incurring major computational costs or accuracy degradation.

Method: The paper introduces orthogonal convolutional kernel regularization and activation difference analysis to decorrelate convolutional filters and identify class-specific channels. This enables class-aware soft pruning for selective unlearning with minimal accuracy loss.

Result: The proposed method achieves rapid unlearning in milliseconds, completely forgets targeted classes, shows minimal accuracy degradation on retained data, and reduces risks of membership inference attacks. Evaluations on datasets like CIFAR-10, CIFAR-100, and TinyImageNet confirm its superiority over state-of-the-art baselines.

Conclusion: The framework offers a practical and efficient solution for implementing real-time machine unlearning in Machine Learning as a Service (MLaaS) settings, balancing efficiency, accuracy, and privacy.

Abstract: Machine unlearning aims to selectively remove class-specific knowledge from
pretrained neural networks to satisfy privacy regulations such as the GDPR.
Existing methods typically face a trade-off between unlearning speed and
preservation of predictive accuracy, often incurring either high computational
overhead or significant performance degradation on retained classes. In this
paper, we propose a novel class-aware soft pruning framework leveraging
orthogonal convolutional kernel regularization to achieve rapid and precise
forgetting with millisecond-level response times. By enforcing orthogonality
constraints during training, our method decorrelates convolutional filters and
disentangles feature representations, while efficiently identifying
class-specific channels through activation difference analysis. Extensive
evaluations across multiple architectures and datasets demonstrate stable
pruning with near-instant execution, complete forgetting of targeted classes,
and minimal accuracy loss on retained data. Experiments on CIFAR-10, CIFAR-100,
and TinyImageNet confirm that our approach substantially reduces membership
inference attack risks and accelerates unlearning by orders of magnitude
compared to state-of-the-art baselines. This framework provides an efficient,
practical solution for real-time machine unlearning in Machine Learning as a
Service (MLaaS) scenarios.

</details>


### [117] [Distillation-Enabled Knowledge Alignment for Generative Semantic Communications in AIGC Provisioning Tasks](https://arxiv.org/abs/2506.19893)
*Jingzhi Hu,Geoffrey Ye Li*

Main category: cs.LG

TL;DR: The paper introduces DeKA-g, a technique to optimize AI-generated content transmission by aligning cloud-edge communication via distillation-enabled knowledge alignment.


<details>
  <summary>Details</summary>
Motivation: To address inefficiencies and traffic overload in transmitting AI-generated content from the cloud to edge devices and users.

Method: DeKA-g integrates two methods, Metaword-Aided Knowledge Distillation (MAKD) and Variable-Rate Grouped SNR Adaptation (VGSA), to distill generative AI knowledge into low-rank matrices for more efficient communication across diverse wireless conditions.

Result: Simulation results show 44% improvement in alignment between edge and cloud content, 116% higher efficiency in adapting compression rates, and 28% better performance in low-SNR environments compared to baselines.

Conclusion: DeKA-g provides an efficient pathway for seamless and high-quality generative semantic communication by reducing communication bottlenecks, adapting effectively to channel conditions, and enhancing edge-cloud knowledge alignment.

Abstract: Due to the surging amount of AI-generated content (AIGC), its provisioning to
edges and mobile users from the cloud incurs substantial traffic on networks.
Generative semantic communication (GSC) offers a promising solution by
transmitting highly compact information, i.e., prompt text and latent
representations, instead of high-dimensional AIGC data. However, GSC relies on
the alignment between the knowledge in the cloud generative AI (GAI) and that
possessed by the edges and users, and between the knowledge for wireless
transmission and that of actual channels, which remains challenging. In this
paper, we propose DeKA-g, a distillation-enabled knowledge alignment algorithm
for GSC systems. The core idea is to distill the generation knowledge from the
cloud-GAI into low-rank matrices, which can be incorporated by the edge and
used to adapt the transmission knowledge to diverse wireless channel
conditions. DeKA-g comprises two novel methods: metaword-aided knowledge
distillation (MAKD) and variable-rate grouped SNR adaptation (VGSA). For MAKD,
an optimized metaword is employed to enhance the efficiency of knowledge
distillation, while VGSA enables efficient adaptation to diverse compression
rates and SNR ranges. From simulation results, DeKA-g improves the alignment
between the edge-generated images and the cloud-generated ones by 44%.
Moreover, it adapts to compression rates with 116% higher efficiency than the
baseline and enhances the performance in low-SNR conditions by 28%.

</details>


### [118] [Explaining deep neural network models for electricity price forecasting with XAI](https://arxiv.org/abs/2506.19894)
*Antoine Pesenti,Aidan OSullivan*

Main category: cs.LG

TL;DR: The study uses deep neural networks (DNN) to forecast electricity prices and explain their drivers using XAI methods like SHAP, with the introduction of SSHAP values and SSHAP lines.


<details>
  <summary>Details</summary>
Motivation: Understanding the drivers of price dynamics in electricity markets, which are highly complex and involve intricate dependencies, is crucial.

Method: Deep neural networks are used for price forecasting, paired with explainable AI methods such as SHAP, saliency maps, and novel visual concepts including SSHAP values and SSHAP lines.

Result: The study analyzes contributing factors across five electricity markets using advanced explainability techniques.

Conclusion: Explainable AI methods, combined with visual techniques, offer enhanced understanding of electricity price dynamics, providing valuable insights into market behaviors.

Abstract: Electricity markets are highly complex, involving lots of interactions and
complex dependencies that make it hard to understand the inner workings of the
market and what is driving prices. Econometric methods have been developed for
this, white-box models, however, they are not as powerful as deep neural
network models (DNN). In this paper, we use a DNN to forecast the price and
then use XAI methods to understand the factors driving the price dynamics in
the market. The objective is to increase our understanding of how different
electricity markets work. To do that, we apply explainable methods such as SHAP
and Gradient, combined with visual techniques like heatmaps (saliency maps) to
analyse the behaviour and contributions of various features across five
electricity markets. We introduce the novel concepts of SSHAP values and SSHAP
lines to enhance the complex representation of high-dimensional tabular models.

</details>


### [119] [Collaborative Batch Size Optimization for Federated Learning](https://arxiv.org/abs/2506.20511)
*Arno Geimer,Karthick Panner Selvam,Beltran Fiz Pontiveros*

Main category: cs.LG

TL;DR: The paper introduces a novel approach to optimize hardware usage in Federated Learning (FL) by employing a greedy randomized search for local batch sizes, thereby improving convergence speed without requiring local parameter optimization.


<details>
  <summary>Details</summary>
Motivation: The study focuses on addressing the challenges of local training inefficiencies in Federated Learning caused by improper hardware configurations among federated participants.

Method: A greedy randomized search algorithm is applied to optimize local batch sizes during training in Federated Learning, leveraging parallel processing capabilities.

Result: The proposed method enhances convergence speed compared to default settings, and performs competitively when matched against locally optimized parameters in the training process.

Conclusion: Optimal hardware usage configuration through the proposed method leads to faster model convergence, showcasing its potential for improving decentralized learning systems in Federated Learning.

Abstract: Federated Learning (FL) is a decentralized collaborative Machine Learning
framework for training models without collecting data in a centralized
location. It has seen application across various disciplines, from helping
medical diagnoses in hospitals to detecting fraud in financial transactions. In
this paper, we focus on improving the local training process through hardware
usage optimization. While participants in a federation might share the hardware
they are training on, since there is no information exchange between them,
their training process can be hindered by an improper training configuration.
Taking advantage of the parallel processing inherent to Federated Learning, we
use a greedy randomized search to optimize local batch sizes for the best
training settings across all participants. Our results show that against
default parameter settings, our method improves convergence speed while staying
nearly on par with the case where local parameters are optimized.

</details>


### [120] [A Framework for Uncertainty Quantification Based on Nearest Neighbors Across Layers](https://arxiv.org/abs/2506.19895)
*Miguel N. Font,José L. Jorro-Aragoneses,Carlos M. Alaíz*

Main category: cs.LG

TL;DR: The paper introduces a post-hoc framework for assessing neural network decision uncertainty using training cases with similar activation vectors, proposing novel metrics (Decision Change and Layer Uncertainty) and surpassing traditional methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the problem of wrong solutions in neural networks for high-risk applications such as medical diagnosis and autonomous driving by improving uncertainty measurement.

Method: Proposes a framework to measure uncertainty using activation vector similarities across layers, introducing Decision Change and Layer Uncertainty metrics evaluated on CIFAR-10 and MNIST datasets.

Result: The proposed metrics outperform traditional confidence estimation methods, such as softmax-based confidence, in challenging classification scenarios.

Conclusion: The results demonstrate that the metrics enhance uncertainty estimation, offering better reliability in neural network predictions for critical applications.

Abstract: Neural Networks have high accuracy in solving problems where it is difficult
to detect patterns or create a logical model. However, these algorithms
sometimes return wrong solutions, which become problematic in high-risk domains
like medical diagnosis or autonomous driving. One strategy to detect and
mitigate these errors is the measurement of the uncertainty over neural network
decisions. In this paper, we present a novel post-hoc framework for measuring
the uncertainty of a decision based on retrieved training cases that have a
similar activation vector to the query for each layer. Based on these retrieved
cases, we propose two new metrics: Decision Change and Layer Uncertainty, which
capture changes in nearest-neighbor class distributions across layers. We
evaluated our approach in a classification model for two datasets: CIFAR-10 and
MNIST. The results show that these metrics enhance uncertainty estimation,
especially in challenging classification tasks, outperforming softmax-based
confidence.

</details>


### [121] [Hear No Evil: Detecting Gradient Leakage by Malicious Servers in Federated Learning](https://arxiv.org/abs/2506.20651)
*Fei Wang,Baochun Li*

Main category: cs.LG

TL;DR: This paper analyzes malicious gradient leakage attacks in federated learning (FL) and finds these attacks are often detectable and limited in practical settings.


<details>
  <summary>Details</summary>
Motivation: To understand and mitigate the risks posed by malicious servers manipulating FL models to leak client data.

Method: An in-depth analysis of gradient leakage attacks and adversarial model manipulation, considering realistic FL with normalization and federated averaging.

Result: The analysis shows that such attacks face a trade-off between effectiveness and detectability. Additionally, a lightweight client-side detection mechanism is proposed.

Conclusion: Malicious gradient leakage attacks are practically limited and detectable, and a client-side detection mechanism can effectively enhance privacy defense in FL.

Abstract: Recent work has shown that gradient updates in federated learning (FL) can
unintentionally reveal sensitive information about a client's local data. This
risk becomes significantly greater when a malicious server manipulates the
global model to provoke information-rich updates from clients. In this paper,
we adopt a defender's perspective to provide the first comprehensive analysis
of malicious gradient leakage attacks and the model manipulation techniques
that enable them. Our investigation reveals a core trade-off: these attacks
cannot be both highly effective in reconstructing private data and sufficiently
stealthy to evade detection -- especially in realistic FL settings that
incorporate common normalization techniques and federated averaging.
  Building on this insight, we argue that malicious gradient leakage attacks,
while theoretically concerning, are inherently limited in practice and often
detectable through basic monitoring. As a complementary contribution, we
propose a simple, lightweight, and broadly applicable client-side detection
mechanism that flags suspicious model updates before local training begins,
despite the fact that such detection may not be strictly necessary in realistic
FL settings. This mechanism further underscores the feasibility of defending
against these attacks with minimal overhead, offering a deployable safeguard
for privacy-conscious federated learning systems.

</details>


### [122] [MIRAGE: A Benchmark for Multimodal Information-Seeking and Reasoning in Agricultural Expert-Guided Conversations](https://arxiv.org/abs/2506.20100)
*Vardhan Dongre,Chi Gui,Shubham Garg,Hooshang Nayyeri,Gokhan Tur,Dilek Hakkani-Tür,Vikram S. Adve*

Main category: cs.LG

TL;DR: MIRAGE is a multimodal benchmark for expert-level reasoning in agriculture, featuring diverse real-world consultations with image and text context.


<details>
  <summary>Details</summary>
Motivation: To create a benchmark that captures the complexity of consultative interactions in agriculture, requiring grounded reasoning and handling of rare scenarios.

Method: Curated over 35,000 real user-expert interactions to develop a dataset with images, queries, and expert responses for diverse agricultural scenarios.

Result: MIRAGE includes over 7,000 unique biological entities and presents an open-world scenario benchmark for evaluating vision-language models.

Conclusion: MIRAGE enables comprehensive evaluation of models on expert-level reasoning, decision-making, and context understanding in a knowledge-intensive domain.

Abstract: We introduce MIRAGE, a new benchmark for multimodal expert-level reasoning
and decision-making in consultative interaction settings. Designed for the
agriculture domain, MIRAGE captures the full complexity of expert consultations
by combining natural user queries, expert-authored responses, and image-based
context, offering a high-fidelity benchmark for evaluating models on grounded
reasoning, clarification strategies, and long-form generation in a real-world,
knowledge-intensive domain. Grounded in over 35,000 real user-expert
interactions and curated through a carefully designed multi-step pipeline,
MIRAGE spans diverse crop health, pest diagnosis, and crop management
scenarios. The benchmark includes more than 7,000 unique biological entities,
covering plant species, pests, and diseases, making it one of the most
taxonomically diverse benchmarks available for vision-language models, grounded
in the real world. Unlike existing benchmarks that rely on well-specified user
inputs and closed-set taxonomies, MIRAGE features underspecified, context-rich
scenarios with open-world settings, requiring models to infer latent knowledge
gaps, handle rare entities, and either proactively guide the interaction or
respond. Project Page: https://mirage-benchmark.github.io

</details>


### [123] [A Comparative Analysis of Reinforcement Learning and Conventional Deep Learning Approaches for Bearing Fault Diagnosis](https://arxiv.org/abs/2506.19929)
*Efe Çakır,Patrick Dumond*

Main category: cs.LG

TL;DR: The paper investigates the use of reinforcement learning, specifically Deep Q-Networks, for bearing fault diagnosis in rotating machinery, showcasing adaptability advantages but requiring further optimization due to computational demands.


<details>
  <summary>Details</summary>
Motivation: Bearing faults can cause operational disruptions and costly maintenance; traditional diagnostic methods struggle with adaptability and reliance on large labeled datasets.

Method: The study utilized Deep Q-Networks (DQNs) in reinforcement learning to perform bearing fault classification, optimizing reward structures and measuring adaptability in machine monitoring systems.

Result: RL models demonstrated comparable performance to supervised learning under controlled conditions, with improved adaptability despite computational inefficiencies.

Conclusion: Reinforcement learning shows promising potential as a complement to traditional diagnosis methods, encouraging the development of adaptive diagnostic systems while requiring further computational optimizations.

Abstract: Bearing faults in rotating machinery can lead to significant operational
disruptions and maintenance costs. Modern methods for bearing fault diagnosis
rely heavily on vibration analysis and machine learning techniques, which often
require extensive labeled data and may not adapt well to dynamic environments.
This study explores the feasibility of reinforcement learning (RL),
specifically Deep Q-Networks (DQNs), for bearing fault classification tasks in
machine condition monitoring to enhance the accuracy and adaptability of
bearing fault diagnosis. The results demonstrate that while RL models developed
in this study can match the performance of traditional supervised learning
models under controlled conditions, they excel in adaptability when equipped
with optimized reward structures. However, their computational demands
highlight areas for further improvement. These findings demonstrate RL's
potential to complement traditional methods, paving the way for adaptive
diagnostic frameworks.

</details>


### [124] [These are Not All the Features You are Looking For: A Fundamental Bottleneck In Supervised Pretraining](https://arxiv.org/abs/2506.18221)
*Xingyu Alice Yang,Jianyu Zhang,Léon Bottou*

Main category: cs.LG

TL;DR: The study highlights a limitation in transfer learning linked to an 'information saturation bottleneck,' which leads to loss of critical features during pretraining. Addressing this issue might require richer feature representations.


<details>
  <summary>Details</summary>
Motivation: The challenge is to ensure effective transfer learning when adapting pretrained models to task-specific scenarios, while understanding persisting limitations like mismatched tasks or dataset differences.

Method: The authors evaluate model transfer by analyzing how well pretrained models perform versus task-specific direct training, diagnosing a saturation bottleneck in feature learning.

Result: Empirical evidence shows that data distribution and ordering affect feature learning over time, limiting the transferability of deep learning architectures.

Conclusion: Task-specific training remains crucial, and the authors emphasize developing richer feature representations to overcome deficiencies in representation learning methods.

Abstract: Transfer learning is a cornerstone of modern machine learning, promising a
way to adapt models pretrained on a broad mix of data to new tasks with minimal
new data. However, a significant challenge remains in ensuring that transferred
features are sufficient to handle unseen datasets, amplified by the difficulty
of quantifying whether two tasks are "related". To address these challenges, we
evaluate model transfer from a pretraining mixture to each of its component
tasks, assessing whether pretrained features can match the performance of
task-specific direct training. We identify a fundamental limitation in deep
learning models -- an "information saturation bottleneck" -- where networks
fail to learn new features once they encode similar competing features during
training. When restricted to learning only a subset of key features during
pretraining, models will permanently lose critical features for transfer and
perform inconsistently on data distributions, even components of the training
mixture. Empirical evidence from published studies suggests that this
phenomenon is pervasive in deep learning architectures -- factors such as data
distribution or ordering affect the features that current representation
learning methods can learn over time. This study suggests that relying solely
on large-scale networks may not be as effective as focusing on task-specific
training, when available. We propose richer feature representations as a
potential solution to better generalize across new datasets and, specifically,
present existing methods alongside a novel approach, the initial steps towards
addressing this challenge.

</details>


### [125] [Any-Order GPT as Masked Diffusion Model: Decoupling Formulation and Architecture](https://arxiv.org/abs/2506.19935)
*Shuchen Xue,Tianyu Xie,Tianyang Hu,Zijin Feng,Jiacheng Sun,Kenji Kawaguchi,Zhenguo Li,Zhi-Ming Ma*

Main category: cs.LG

TL;DR: The research compares masked diffusion models (MDMs) and autoregressive (AR) paradigms by isolating their influences from architectural differences, achieving significant insights into their trade-offs.


<details>
  <summary>Details</summary>
Motivation: There is a challenge in comparing AR and MDM paradigms due to their differing architectures (decoder-only vs. encoder-only), which obscures the core differences between the paradigms themselves.

Method: The study evaluates MDMs in a decoder-only framework to systematically compare them with AR models and investigates the influence of architectural choices on performance and efficiency.

Result: Decoder-only MDMs achieve up to 25x speed improvements and comparable perplexity to AR models, offering key trade-offs in generation ability despite modeling a larger conditional probability space.

Conclusion: The study separates modeling paradigm differences from architectural impacts, providing valuable insights for future model designs and refinements.

Abstract: Large language models (LLMs) predominantly use autoregressive (AR)
approaches, but masked diffusion models (MDMs) are emerging as viable
alternatives. A key challenge in comparing AR and MDM paradigms is their
typical architectural difference: AR models are often decoder-only, while MDMs
have largely been encoder-only. This practice of changing both the modeling
paradigm and architecture simultaneously makes direct comparisons unfair, as
it's hard to distinguish whether observed differences stem from the paradigm
itself or the architectural shift. This research evaluates MDMs within a
decoder-only framework to: (1) equitably compare MDM (as Any-Order AR, or
AO-AR) and standard AR paradigms. Our investigation suggests that the standard
AO-AR objective, which averages over all token permutations, may benefit from
refinement, as many permutations appear less informative compared to the
language's inherent left-to-right structure. (2) Investigate architectural
influences (decoder-only vs. encoder-only) within MDMs. We demonstrate that
while encoder-only MDMs model a simpler conditional probability space,
decoder-only MDMs can achieve dramatic generation speedups ($\sim25\times$) and
comparable perplexity with temperature annealing despite modeling a vastly
larger space, highlighting key trade-offs. This work thus decouples core
paradigm differences from architectural influences, offering insights for
future model design. Code is available at https://github.com/scxue/AO-GPT-MDM.

</details>


### [126] [The Most Important Features in Generalized Additive Models Might Be Groups of Features](https://arxiv.org/abs/2506.19937)
*Tomas M. Bosschieter,Luis Franca,Jessica Wolk,Yiyuan Wu,Bella Mehta,Joseph Dehoney,Orsolya Kiss,Fiona C. Baker,Qingyu Zhao,Rich Caruana,Kilian M. Pohl*

Main category: cs.LG

TL;DR: This paper presents a novel approach to determine the importance of feature groups in Generalized Additive Models (GAMs) with applications to synthetic experiments and real-world case studies.


<details>
  <summary>Details</summary>
Motivation: Traditional feature importance analyses often ignore the combined effects of related feature groups, which may lead to overlooking critical insights, especially in datasets with natural feature groupings like multimodal datasets.

Method: The paper introduces a method for determining group importance in GAMs that is efficient, does not require model retraining, allows posthoc group definition with overlapping groups, and works well in high-dimensional contexts. It parallels explained variation in statistics.

Result: The proposed method's behavior was validated using synthetic experiments and real-world datasets, demonstrating its utility in examining depressive symptoms and health determinants after hip arthroplasty surgeries.

Conclusion: Group-based feature analysis provides a richer and more comprehensive understanding of data insights compared to single-feature analysis, especially in multimodal medical datasets.

Abstract: While analyzing the importance of features has become ubiquitous in
interpretable machine learning, the joint signal from a group of related
features is sometimes overlooked or inadvertently excluded. Neglecting the
joint signal could bypass a critical insight: in many instances, the most
significant predictors are not isolated features, but rather the combined
effect of groups of features. This can be especially problematic for datasets
that contain natural groupings of features, including multimodal datasets. This
paper introduces a novel approach to determine the importance of a group of
features for Generalized Additive Models (GAMs) that is efficient, requires no
model retraining, allows defining groups posthoc, permits overlapping groups,
and remains meaningful in high-dimensional settings. Moreover, this definition
offers a parallel with explained variation in statistics. We showcase
properties of our method on three synthetic experiments that illustrate the
behavior of group importance across various data regimes. We then demonstrate
the importance of groups of features in identifying depressive symptoms from a
multimodal neuroscience dataset, and study the importance of social
determinants of health after total hip arthroplasty. These two case studies
reveal that analyzing group importance offers a more accurate, holistic view of
the medical issues compared to a single-feature analysis.

</details>


### [127] [HERCULES: Hierarchical Embedding-based Recursive Clustering Using LLMs for Efficient Summarization](https://arxiv.org/abs/2506.19992)
*Gabor Petnehazi,Bernadett Aradi*

Main category: cs.LG

TL;DR: HERCULES is an algorithm for hierarchical k-means clustering of diverse data types, integrating Large Language Models (LLMs) to improve interpretability.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the need for tools that analyze complex datasets across various modalities while providing human-understandable insights.

Method: HERCULES enhances hierarchical clustering by recursively applying k-means and integrates LLMs to create semantically rich titles and descriptions for each cluster.

Result: The algorithm successfully clusters different data types and generates intuitive insights using LLMs. It supports direct and description-based clustering modes.

Conclusion: HERCULES demonstrates potential for extracting hierarchical knowledge and enabling better understanding of large datasets.

Abstract: The explosive growth of complex datasets across various modalities
necessitates advanced analytical tools that not only group data effectively but
also provide human-understandable insights into the discovered structures. We
introduce HERCULES (Hierarchical Embedding-based Recursive Clustering Using
LLMs for Efficient Summarization), a novel algorithm and Python package
designed for hierarchical k-means clustering of diverse data types, including
text, images, and numeric data (processed one modality per run). HERCULES
constructs a cluster hierarchy by recursively applying k-means clustering,
starting from individual data points at level 0. A key innovation is its deep
integration of Large Language Models (LLMs) to generate semantically rich
titles and descriptions for clusters at each level of the hierarchy,
significantly enhancing interpretability. The algorithm supports two main
representation modes: `direct' mode, which clusters based on original data
embeddings or scaled numeric features, and `description' mode, which clusters
based on embeddings derived from LLM-generated summaries. Users can provide a
`topic\_seed' to guide LLM-generated summaries towards specific themes. An
interactive visualization tool facilitates thorough analysis and understanding
of the clustering results. We demonstrate HERCULES's capabilities and discuss
its potential for extracting meaningful, hierarchical knowledge from complex
datasets.

</details>


### [128] [TRACED: Transition-aware Regret Approximation with Co-learnability for Environment Design](https://arxiv.org/abs/2506.19997)
*Geonwoo Cho,Jaegyun Im,Jihwan Lee,Hojun Yi,Sejin Kim,Sundong Kim*

Main category: cs.LG

TL;DR: The paper presents TRACED, an improved approach to Unsupervised Environment Design (UED) that uses transition prediction error and co-learnability metrics to boost zero-shot generalization and sample efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of generalizing deep reinforcement learning agents to unseen environments through better curriculum design in UED.

Method: The method introduces TRACED, which uses refined regret approximation by incorporating transition prediction error and a new lightweight metric called co-learnability to guide task generation.

Result: TRACED yields superior zero-shot generalization across benchmarks while reducing required environment interactions by up to 2x compared to strong baselines.

Conclusion: TRACED effectively refines regret approximation and models task relationships, improving curriculum design in UED with enhanced generalization and efficiency.

Abstract: Generalizing deep reinforcement learning agents to unseen environments
remains a significant challenge. One promising solution is Unsupervised
Environment Design (UED), a co-evolutionary framework in which a teacher
adaptively generates tasks with high learning potential, while a student learns
a robust policy from this evolving curriculum. Existing UED methods typically
measure learning potential via regret, the gap between optimal and current
performance, approximated solely by value-function loss. Building on these
approaches, we introduce the transition prediction error as an additional term
in our regret approximation. To capture how training on one task affects
performance on others, we further propose a lightweight metric called
co-learnability. By combining these two measures, we present Transition-aware
Regret Approximation with Co-learnability for Environment Design (TRACED).
Empirical evaluations show that TRACED yields curricula that improve zero-shot
generalization across multiple benchmarks while requiring up to 2x fewer
environment interactions than strong baselines. Ablation studies confirm that
the transition prediction error drives rapid complexity ramp-up and that
co-learnability delivers additional gains when paired with the transition
prediction error. These results demonstrate how refined regret approximation
and explicit modeling of task relationships can be leveraged for
sample-efficient curriculum design in UED.

</details>


### [129] [Elucidated Rolling Diffusion Models for Probabilistic Weather Forecasting](https://arxiv.org/abs/2506.20024)
*Salva Rühling Cachay,Miika Aittala,Karsten Kreis,Noah Brenowitz,Arash Vahdat,Morteza Mardani,Rose Yu*

Main category: cs.LG

TL;DR: This paper introduces Elucidated Rolling Diffusion Models (ERDM) to enhance probabilistic forecasting in systems with high uncertainty growth. ERDM integrates rolling forecasts with Elucidated Diffusion Models through novel architecture and strategies, outperforming baseline methods.


<details>
  <summary>Details</summary>
Motivation: Probabilistic forecasting in high-dimensional chaotic systems struggles to model complex dependencies and escalating uncertainty over time. Existing models fail to balance determinism and stochasticity effectively.

Method: The authors propose ERDM by adapting noise schedules, network preconditioning, and sampling methods from Elucidated Diffusion Models to rolling forecasts. Key innovations include loss-weighting for mid-range horizons, initialization strategies, and bespoke hybrid sequence architectures.

Result: ERDM demonstrated superior performance in 2D Navier-Stokes simulations and ERA5 global weather forecasting compared to previous diffusion-based methods like conditional autoregressive EDM.

Conclusion: ERDM provides a robust framework to address challenges in diffusion-based sequence generation, successfully capturing escalating uncertainty and improving forecast accuracy in chaotic systems.

Abstract: Diffusion models are a powerful tool for probabilistic forecasting, yet most
applications in high-dimensional chaotic systems predict future snapshots
one-by-one. This common approach struggles to model complex temporal
dependencies and fails to explicitly account for the progressive growth of
uncertainty inherent to such systems. While rolling diffusion frameworks, which
apply increasing noise to forecasts at longer lead times, have been proposed to
address this, their integration with state-of-the-art, high-fidelity diffusion
techniques remains a significant challenge. We tackle this problem by
introducing Elucidated Rolling Diffusion Models (ERDM), the first framework to
successfully unify a rolling forecast structure with the principled, performant
design of Elucidated Diffusion Models (EDM). To do this, we adapt the core EDM
components-its noise schedule, network preconditioning, and Heun sampler-to the
rolling forecast setting. The success of this integration is driven by three
key contributions: (i) a novel loss weighting scheme that focuses model
capacity on the mid-range forecast horizons where determinism gives way to
stochasticity; (ii) an efficient initialization strategy using a pre-trained
EDM for the initial window; and (iii) a bespoke hybrid sequence architecture
for robust spatiotemporal feature extraction under progressive denoising. On 2D
Navier-Stokes simulations and ERA5 global weather forecasting at 1.5^\circ
resolution, ERDM consistently outperforms key diffusion-based baselines,
including conditional autoregressive EDM. ERDM offers a flexible and powerful
general framework for tackling diffusion-based sequence generation problems
where modeling escalating uncertainty is paramount. Code is available at:
https://github.com/salvaRC/erdm

</details>


### [130] [New Insights on Unfolding and Fine-tuning Quantum Federated Learning](https://arxiv.org/abs/2506.20016)
*Shanika Iroshi Nanayakkara,Shiva Raj Pokhrel*

Main category: cs.LG

TL;DR: The paper introduces a method for Quantum Federated Learning (QFL) that dynamically adjusts hyperparameters for performance improvement, achieving higher accuracy in heterogeneous environments.


<details>
  <summary>Details</summary>
Motivation: To address client heterogeneity challenges that limit the performance of conventional QFL approaches.

Method: The authors propose a deep unfolding framework that allows clients to self-optimize hyperparameters (e.g., learning rates, regularization) based on their training behavior.

Result: The method achieves about 90% accuracy, outperforming traditional methods (~55%), as validated on IBM quantum hardware and Qiskit Aer simulators.

Conclusion: The proposed framework mitigates overfitting and enhances generalization, proving especially effective in applications like gene expression analysis and cancer detection, thus making QFL viable for complex challenges such as healthcare.

Abstract: Client heterogeneity poses significant challenges to the performance of
Quantum Federated Learning (QFL). To overcome these limitations, we propose a
new approach leveraging deep unfolding, which enables clients to autonomously
optimize hyperparameters, such as learning rates and regularization factors,
based on their specific training behavior. This dynamic adaptation mitigates
overfitting and ensures robust optimization in highly heterogeneous
environments where standard aggregation methods often fail. Our framework
achieves approximately 90% accuracy, significantly outperforming traditional
methods, which typically yield around 55% accuracy, as demonstrated through
real-time training on IBM quantum hardware and Qiskit Aer simulators. By
developing self adaptive fine tuning, the proposed method proves particularly
effective in critical applications such as gene expression analysis and cancer
detection, enhancing diagnostic precision and predictive modeling within
quantum systems. Our results are attributed to convergence-aware, learnable
optimization steps intrinsic to the deep unfolded framework, which maintains
the generalization. Hence, this study addresses the core limitations of
conventional QFL, streamlining its applicability to any complex challenges such
as healthcare and genomic research.

</details>


### [131] [Thumb on the Scale: Optimal Loss Weighting in Last Layer Retraining](https://arxiv.org/abs/2506.20025)
*Nathan Stromberg,Christos Thrampoulidis,Lalitha Sankar*

Main category: cs.LG

TL;DR: The paper investigates the effectiveness of loss weighting in last layer retraining (LLR) scenarios, finding that loss weighting remains effective but must consider model overparameterization.


<details>
  <summary>Details</summary>
Motivation: To address biases in machine learning models arising from the training data, particularly in regimes where the model is proportionately sized and retraining data is limited and often inseparable.

Method: The authors analyze the use of loss weighting in the LLR regime theoretically and empirically, focusing on how the effectiveness of weighting depends on the relative overparameterization of the model.

Result: Loss weighting is found to be effective in the LLR regime, given proper consideration of overparameterization, bridging the gap between underparameterized and overparameterized extremes.

Conclusion: Loss weighting remains a valuable tool to ensure balanced class performance in moderately parameterized models, but its design must account for model-specific characteristics like relative overparameterization.

Abstract: While machine learning models become more capable in discriminative tasks at
scale, their ability to overcome biases introduced by training data has come
under increasing scrutiny. Previous results suggest that there are two extremes
of parameterization with very different behaviors: the population
(underparameterized) setting where loss weighting is optimal and the separable
overparameterized setting where loss weighting is ineffective at ensuring equal
performance across classes. This work explores the regime of last layer
retraining (LLR) in which the unseen limited (retraining) data is frequently
inseparable and the model proportionately sized, falling between the two
aforementioned extremes. We show, in theory and practice, that loss weighting
is still effective in this regime, but that these weights \emph{must} take into
account the relative overparameterization of the model.

</details>


### [132] [DIM-SUM: Dynamic IMputation for Smart Utility Management](https://arxiv.org/abs/2506.20023)
*Ryan Hildebrant,Rahul Bhope,Sharad Mehrotra,Christopher Tull,Nalini Venkatasubramanian*

Main category: cs.LG

TL;DR: DIM-SUM is introduced as a preprocessing framework to train imputation models tailored to real-world missing patterns.


<details>
  <summary>Details</summary>
Motivation: To address the discrepancy between artificially created missing data patterns and the actual complex missing patterns encountered in real-world scenarios.

Method: DIM-SUM integrates pattern clustering, adaptive masking, and theoretical learning guarantees to train imputation models more effectively.

Result: Extensive experiments reveal DIM-SUM achieves superior accuracy with reduced processing time and training data compared to traditional methods and large pre-trained models.

Conclusion: DIM-SUM presents a significant advancement in time series imputation models for handling complex missing data patterns in infrastructure monitoring.

Abstract: Time series imputation models have traditionally been developed using
complete datasets with artificial masking patterns to simulate missing values.
However, in real-world infrastructure monitoring, practitioners often encounter
datasets where large amounts of data are missing and follow complex,
heterogeneous patterns. We introduce DIM-SUM, a preprocessing framework for
training robust imputation models that bridges the gap between artificially
masked training data and real missing patterns. DIM-SUM combines pattern
clustering and adaptive masking strategies with theoretical learning guarantees
to handle diverse missing patterns actually observed in the data. Through
extensive experiments on over 2 billion readings from California water
districts, electricity datasets, and benchmarks, we demonstrate that DIM-SUM
outperforms traditional methods by reaching similar accuracy with lower
processing time and significantly less training data. When compared against a
large pre-trained model, DIM-SUM averages 2x higher accuracy with significantly
less inference time.

</details>


### [133] [On the ability of Deep Neural Networks to Learn Granger Causality in Multi-Variate Time Series Data](https://arxiv.org/abs/2506.20347)
*Malik Shahid Sultan,Hernando Ombao*

Main category: cs.LG

TL;DR: The paper introduces a novel framework for Granger Causality estimation using Deep Neural Networks (DNNs), focusing on prediction rather than variable selection.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of traditional Linear Vector Autoregressive (VAR) models, which impose strict assumptions restricting the associations they can capture.

Method: The study employs a regularized deep learning model to jointly model time series data, uncovering Granger Causality by analyzing residuals or uncertainty distributions when specific components are excluded. Additionally, input layer dropout is investigated.

Result: A well-regularized DNN can learn the true Granger Causal structure without needing explicit variable selection terms or sparse regression in the loss function.

Conclusion: Deep learning models, with proper regularization, have the potential to capture Granger Causality from time series data effectively, shifting the paradigm from variable selection to prediction-based causality determination.

Abstract: Granger Causality (GC) offers an elegant statistical framework to study the
association between multivariate time series data. Linear Vector Autoregressive
models (VAR) though have nice interpretation properties but have limited
practical application due to underlying assumptions on the kind of associations
that can be captured by these models. Numerous attempts have already been made
in the literature that exploit the functional approximation power of Deep
Neural Networks (DNNs) for the task of GC estimation. These methods however
treat GC as a variable selection problem. We present a novel paradigm for
approaching GC. We present this idea that GC is essentially linked with
prediction and if a deep learning model is used to model the time series
collectively or jointly, a well regularized model may learn the true granger
causal structure from the data, given that there is enough training data. We
propose to uncover the learned GC structure by comparing the model uncertainty
or distribution of the residuals when the past of everything is used as
compared to the one where a specific time series component is dropped from the
model. We also compare the effect of input layer dropout on the ability of a
neural network to learn granger causality from the data. We show that a well
regularized model infact can learn the true GC structure from the data without
explicitly adding terms in the loss function that guide the model to select
variables or perform sparse regression.

</details>


### [134] [Lost in Retraining: Roaming the Parameter Space of Exponential Families Under Closed-Loop Learning](https://arxiv.org/abs/2506.20623)
*Fariba Jangjoo,Matteo Marsili,Yasser Roudi*

Main category: cs.LG

TL;DR: The paper examines how closed-loop learning, where models train on their own generated data, behaves for exponential family models, addressing potential biases and proposing methods to stabilize training.


<details>
  <summary>Details</summary>
Motivation: Closed-loop learning is gaining attention as future large neural network models might rely heavily on training from their own generated data, raising questions about outcomes and biases.

Method: The study derives the dynamics of model parameters under closed-loop learning, analyzes the likelihood estimation process, and tests techniques like data pollution, MAP estimation, and regularization to ensure stability.

Result: Parameter estimation leads to convergence to absorbing states that amplify initial biases in data, but methods like data pollution or added regularization mitigate these effects.

Conclusion: While closed-loop learning introduces risks of bias amplification, careful interventions like regularization and data pollution can maintain training stability and fairness.

Abstract: Closed-loop learning is the process of repeatedly estimating a model from
data generated from the model itself. It is receiving great attention due to
the possibility that large neural network models may, in the future, be
primarily trained with data generated by artificial neural networks themselves.
We study this process for models that belong to exponential families, deriving
equations of motions that govern the dynamics of the parameters. We show that
maximum likelihood estimation of the parameters endows sufficient statistics
with the martingale property and that as a result the process converges to
absorbing states that amplify initial biases present in the data. However, we
show that this outcome may be prevented by polluting the data with an
infinitesimal fraction of data points generated from a fixed model, by relying
on maximum a posteriori estimation or by introducing regularisation.
Furthermore, we show that the asymptotic behavior of the dynamics is not
reparametrisation invariant.

</details>


### [135] [PLoP: Precise LoRA Placement for Efficient Finetuning of Large Models](https://arxiv.org/abs/2506.20629)
*Soufiane Hayou,Nikhil Ghosh,Bin Yu*

Main category: cs.LG

TL;DR: This paper introduces PLoP, a method that automatically identifies optimal placements for LoRA adapters, outperforming or matching existing strategies in experiments.


<details>
  <summary>Details</summary>
Motivation: The inefficiency and lack of conclusive results in determining optimal adapter placement for LoRA in large models motivated this research.

Method: The authors proposed PLoP, which uses theoretical analysis to automatically identify suitable module types for placing LoRA adapters based on a pretrained model and the finetuning task.

Result: PLoP showed consistent performance improvements or matched existing adapter placement strategies across various experiments, including supervised finetuning and reinforcement learning.

Conclusion: PLoP is an effective and lightweight solution for adapter placement in LoRA, providing reliable and improved outcomes compared to manual or arbitrary placement strategies.

Abstract: Low-Rank Adaptation (LoRA) is a widely used finetuning method for large
models. Its small memory footprint allows practitioners to adapt large models
to specific tasks at a fraction of the cost of full finetuning. Different
modifications have been proposed to enhance its efficiency by, for example,
setting the learning rate, the rank, and the initialization. Another
improvement axis is adapter placement strategy: when using LoRA, practitioners
usually pick module types to adapt with LoRA, such as Query and Key modules.
Few works have studied the problem of adapter placement, with nonconclusive
results: original LoRA paper suggested placing adapters in attention modules,
while other works suggested placing them in the MLP modules. Through an
intuitive theoretical analysis, we introduce PLoP (Precise LoRA Placement), a
lightweight method that allows automatic identification of module types where
LoRA adapters should be placed, given a pretrained model and a finetuning task.
We demonstrate that PLoP consistently outperforms, and in the worst case
competes, with commonly used placement strategies through comprehensive
experiments on supervised finetuning and reinforcement learning for reasoning.

</details>


### [136] [Automated Generation of Diverse Courses of Actions for Multi-Agent Operations using Binary Optimization and Graph Learning](https://arxiv.org/abs/2506.20031)
*Prithvi Poddar,Ehsan Tarkesh Esfahani,Karthik Dantu,Souma Chowdhury*

Main category: cs.LG

TL;DR: This paper proposes a new computational framework for generating diverse courses of action (COAs) in multi-agent operations by leveraging a graph-based task abstraction, genetic algorithms, and graph neural networks.


<details>
  <summary>Details</summary>
Motivation: Mult-agent operations in dynamic environments, such as disaster response or military missions, require a pool of robust COA options adaptable to varying environmental and agent capabilities. Traditional methods lack diversity and adaptability for such cases.

Method: The paper formulates COA generation as a centralized multi-robot task allocation problem. Genetic algorithms are used to allocate tasks, focusing on diversity and compatibility. Then, a graph neural network employs a policy gradient approach for single-agent task sequencing.

Result: The proposed methodology achieved a significant performance gain when compared to random baselines. It generated up to 20 COAs for operations involving 5 agents and 100 tasks within 50 minutes with a small task sequencing optimality gap.

Conclusion: The diverse COA generation framework is effective for multi-agent operations, enabling adaptability and robustness in dynamic settings, while maintaining computational efficiency.

Abstract: Operations in disaster response, search \& rescue, and military missions that
involve multiple agents demand automated processes to support the planning of
the courses of action (COA). Moreover, traverse-affecting changes in the
environment (rain, snow, blockades, etc.) may impact the expected performance
of a COA, making it desirable to have a pool of COAs that are diverse in task
distributions across agents. Further, variations in agent capabilities, which
could be human crews and/or autonomous systems, present practical opportunities
and computational challenges to the planning process. This paper presents a new
theoretical formulation and computational framework to generate such diverse
pools of COAs for operations with soft variations in agent-task compatibility.
Key to the problem formulation is a graph abstraction of the task space and the
pool of COAs itself to quantify its diversity. Formulating the COAs as a
centralized multi-robot task allocation problem, a genetic algorithm is used
for (order-ignoring) allocations of tasks to each agent that jointly maximize
diversity within the COA pool and overall compatibility of the agent-task
mappings. A graph neural network is trained using a policy gradient approach to
then perform single agent task sequencing in each COA, which maximizes
completion rates adaptive to task features. Our tests of the COA generation
process in a simulated environment demonstrate significant performance gain
over a random walk baseline, small optimality gap in task sequencing, and
execution time of about 50 minutes to plan up to 20 COAs for 5 agent/100 task
operations.

</details>


### [137] [Mastering Multiple-Expert Routing: Realizable $H$-Consistency and Strong Guarantees for Learning to Defer](https://arxiv.org/abs/2506.20650)
*Anqi Mao,Mehryar Mohri,Yutao Zhong*

Main category: cs.LG

TL;DR: The paper proposes novel surrogate loss functions and algorithms for the problem of learning to defer decisions to multiple experts, with theoretical guarantees and experimental validation.


<details>
  <summary>Details</summary>
Motivation: Balancing accuracy and computational cost in assigning decisions to experts is a significant challenge in fields like natural language generation, image processing, and medical diagnostics.

Method: The paper introduces surrogate loss functions and efficient algorithms, proving theoretical guarantees such as $H$-consistency and Bayes-consistency in both single-stage and two-stage learning scenarios.

Result: The derived surrogate losses demonstrate theoretical consistency and outperform existing baselines experimentally in terms of deferral optimization.

Conclusion: Proposed methods improve the theoretical robustness and practical performance of decision deferral systems, contributing to optimized decision delegation in multi-expert systems.

Abstract: The problem of learning to defer with multiple experts consists of optimally
assigning input instances to experts, balancing the trade-off between their
accuracy and computational cost. This is a critical challenge in natural
language generation, but also in other fields such as image processing, and
medical diagnostics. Recent studies have proposed surrogate loss functions to
optimize deferral, but challenges remain in ensuring their consistency
properties. This paper introduces novel surrogate loss functions and efficient
algorithms with strong theoretical learning guarantees. We address open
questions regarding realizable $H$-consistency, $H$-consistency bounds, and
Bayes-consistency for both single-stage (jointly learning predictor and
deferral function) and two-stage (learning only the deferral function with a
fixed expert) learning scenarios. For single-stage deferral, we introduce a
family of new realizable $H$-consistent surrogate losses and further prove
$H$-consistency for a selected member. For two-stage deferral, we derive new
surrogate losses that achieve realizable $H$-consistency, $H$-consistency
bounds, and Bayes-consistency for the two-expert scenario and, under natural
assumptions, multiple-expert scenario. Additionally, we provide enhanced
theoretical guarantees under low-noise assumptions for both scenarios. Finally,
we report the results of experiments using our proposed surrogate losses,
comparing their performance against existing baselines.

</details>


### [138] [Verifiable Unlearning on Edge](https://arxiv.org/abs/2506.20037)
*Mohammad M Maheri,Alex Davidson,Hamed Haddadi*

Main category: cs.LG

TL;DR: The paper introduces a framework using zk-SNARKs to verify data unlearning in personalized edge models, ensuring privacy and performance.


<details>
  <summary>Details</summary>
Motivation: The need to verifiably remove certain data from edge devices' models due to issues like copyright, regulatory concerns, and biases.

Method: Using algorithms tailored for efficient zk-SNARK proof generation to validate data unlearning operations without privacy or performance compromise.

Result: The approach ensures verifiable data unlearning with minimal computational overhead and maintains model performance on edge devices.

Conclusion: The framework effectively combines verification, privacy, and performance preservation for machine unlearning on edge devices, proving its practical application.

Abstract: Machine learning providers commonly distribute global models to edge devices,
which subsequently personalize these models using local data. However, issues
such as copyright infringements, biases, or regulatory requirements may require
the verifiable removal of certain data samples across all edge devices.
Ensuring that edge devices correctly execute such unlearning operations is
critical to maintaining integrity.
  In this work, we introduce a verification framework leveraging zero-knowledge
proofs, specifically zk-SNARKs, to confirm data unlearning on personalized
edge-device models without compromising privacy. We have developed algorithms
explicitly designed to facilitate unlearning operations that are compatible
with efficient zk-SNARK proof generation, ensuring minimal computational and
memory overhead suitable for constrained edge environments. Furthermore, our
approach carefully preserves personalized enhancements on edge devices,
maintaining model performance post-unlearning.
  Our results affirm the practicality and effectiveness of this verification
framework, demonstrating verifiable unlearning with minimal degradation in
personalization-induced performance improvements. Our methodology ensures
verifiable, privacy-preserving, and effective machine unlearning across edge
devices.

</details>


### [139] [Cross-Layer Discrete Concept Discovery for Interpreting Language Models](https://arxiv.org/abs/2506.20040)
*Ankur Garg,Xuemin Yu,Hassan Sajjad,Samira Ebrahimi Kahou*

Main category: cs.LG

TL;DR: This paper introduces a framework called \gls{clvqvae}, employing vector quantization to map and collapse redundant features across transformer layers for better interpretability and compactness.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of understanding how features evolve across layers in large language models by tackling duplications and redundancies in the residual stream.

Method: The proposed method, \gls{clvqvae}, integrates vector quantization with EMA codebook updates, scaled-spherical k-means++ for initialization, and top-k temperature-based sampling for discrete latent space exploration and semantic alignment.

Result: The introduced framework creates compact and interpretable concept vectors by collapsing redundant residual-stream features across layers, while maintaining codebook diversity and semantic structure.

Conclusion: This approach provides a practical method for uncovering emergent concepts in large language models by addressing linear information mixing and duplication challenges within the residual stream.

Abstract: Uncovering emergent concepts across transformer layers remains a significant
challenge because the residual stream linearly mixes and duplicates
information, obscuring how features evolve within large language models.
Current research efforts primarily inspect neural representations at single
layers, thereby overlooking this cross-layer superposition and the redundancy
it introduces. These representations are typically either analyzed directly for
activation patterns or passed to probing classifiers that map them to a limited
set of predefined concepts. To address these limitations, we propose
\gls{clvqvae}, a framework that uses vector quantization to map representations
across layers and in the process collapse duplicated residual-stream features
into compact, interpretable concept vectors. Our approach uniquely combines
top-$k$ temperature-based sampling during quantization with EMA codebook
updates, providing controlled exploration of the discrete latent space while
maintaining code-book diversity. We further enhance the framework with
scaled-spherical k-means++ for codebook initialization, which clusters by
directional similarity rather than magnitude, better aligning with semantic
structure in word embedding space.

</details>


### [140] [LSH-DynED: A Dynamic Ensemble Framework with LSH-Based Undersampling for Evolving Multi-Class Imbalanced Classification](https://arxiv.org/abs/2506.20041)
*Soheil Abadifard,Fazli Can*

Main category: cs.LG

TL;DR: This paper introduces LSH-DynED, a novel approach combining Locality Sensitive Hashing with Random Hyperplane Projections (LSH-RHP) and Dynamic Ensemble Diversification (DynED) to handle multi-class imbalanced non-stationary data streams. Extensive experiments demonstrate its superiority over 15 state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the challenge of classifying multi-class imbalanced data streams, which have dynamic imbalance ratios and limited research compared to binary imbalanced data.

Method: The method integrates Locality Sensitive Hashing with Random Hyperplane Projections (LSH-RHP) for undersampling and combines it with the Dynamic Ensemble Diversification framework. This ensures balanced training sets and improved ensemble predictions.

Result: The LSH-DynED method outperformed 15 state-of-the-art techniques in Kappa and mG-Mean measures through experiments on 33 datasets, including real-world and semi-synthetic ones. It shows strong performance on large, high-dimensional datasets with significant class imbalances.

Conclusion: LSH-DynED is a robust and effective solution for multi-class imbalanced non-stationary data streams, demonstrating adaptability and scalability. The implementation is open-source, and the study identifies future challenges and potential research directions.

Abstract: The classification of imbalanced data streams, which have unequal class
distributions, is a key difficulty in machine learning, especially when dealing
with multiple classes. While binary imbalanced data stream classification tasks
have received considerable attention, only a few studies have focused on
multi-class imbalanced data streams. Effectively managing the dynamic imbalance
ratio is a key challenge in this domain. This study introduces a novel, robust,
and resilient approach to address these challenges by integrating Locality
Sensitive Hashing with Random Hyperplane Projections (LSH-RHP) into the Dynamic
Ensemble Diversification (DynED) framework. To the best of our knowledge, we
present the first application of LSH-RHP for undersampling in the context of
imbalanced non-stationary data streams. The proposed method undersamples the
majority classes by utilizing LSH-RHP, provides a balanced training set, and
improves the ensemble's prediction performance. We conduct comprehensive
experiments on 23 real-world and ten semi-synthetic datasets and compare
LSH-DynED with 15 state-of-the-art methods. The results reveal that LSH-DynED
outperforms other approaches in terms of both Kappa and mG-Mean effectiveness
measures, demonstrating its capability in dealing with multi-class imbalanced
non-stationary data streams. Notably, LSH-DynED performs well in large-scale,
high-dimensional datasets with considerable class imbalances and demonstrates
adaptation and robustness in real-world circumstances. To motivate our design,
we review existing methods for imbalanced data streams, outline key challenges,
and offer guidance for future work. For the reproducibility of our results, we
have made our implementation available on GitHub.

</details>


### [141] [GNN's Uncertainty Quantification using Self-Distillation](https://arxiv.org/abs/2506.20046)
*Hirad Daneshvar,Reza Samavi*

Main category: cs.LG

TL;DR: The paper proposes a novel method based on self-distillation to efficiently quantify predictive uncertainty in Graph Neural Networks (GNNs), demonstrating effective performance across two datasets.


<details>
  <summary>Details</summary>
Motivation: Quantifying predictive uncertainty in GNNs is challenging but crucial for trustworthiness in clinical settings. Current methods like Bayesian and ensemble techniques are computationally expensive and have limitations in diversity representation.

Method: The authors propose using self-distillation, where the same network acts as both teacher and student, eliminating the need for multiple independent networks. Additionally, they develop an uncertainty metric that assigns varied weights to GNN classifiers to account for model diversity.

Result: Experimental evaluations on the MIMIC-IV and Enzymes datasets show the proposed method effectively captures predictive uncertainty and is comparable to MC Dropout and ensemble approaches in performance.

Conclusion: The proposed self-distillation method offers a computationally efficient and precise way to quantify GNNs' uncertainty, making it practical for healthcare applications. The implementation is also publicly available.

Abstract: Graph Neural Networks (GNNs) have shown remarkable performance in the
healthcare domain. However, what remained challenging is quantifying the
predictive uncertainty of GNNs, which is an important aspect of trustworthiness
in clinical settings. While Bayesian and ensemble methods can be used to
quantify uncertainty, they are computationally expensive. Additionally, the
disagreement metric used by ensemble methods to compute uncertainty cannot
capture the diversity of models in an ensemble network. In this paper, we
propose a novel method, based on knowledge distillation, to quantify GNNs'
uncertainty more efficiently and with higher precision. We apply
self-distillation, where the same network serves as both the teacher and
student models, thereby avoiding the need to train several networks
independently. To ensure the impact of self-distillation, we develop an
uncertainty metric that captures the diverse nature of the network by assigning
different weights to each GNN classifier. We experimentally evaluate the
precision, performance, and ability of our approach in distinguishing
out-of-distribution data on two graph datasets: MIMIC-IV and Enzymes. The
evaluation results demonstrate that the proposed method can effectively capture
the predictive uncertainty of the model while having performance similar to
that of the MC Dropout and ensemble methods. The code is publicly available at
https://github.com/tailabTMU/UQ_GNN.

</details>


### [142] [Universal pre-training by iterated random computation](https://arxiv.org/abs/2506.20057)
*Peter Bloem*

Main category: cs.LG

TL;DR: The paper explores the effectiveness of using synthetic data for model pre-training and provides both theoretical justification and empirical validation.


<details>
  <summary>Details</summary>
Motivation: To investigate the feasibility of using randomly generated synthetic data for pre-training sequence models and its benefits in performance and data efficiency.

Method: Theoretically analyze the method using algorithmic complexity concepts and empirically test synthetic data's effectiveness by extending earlier results and adding real-world finetuning experiments.

Result: Synthetic data pre-trained models exhibit zero-shot in-context learning across various datasets, and fine-tuning them leads to faster convergence and better generalization.

Conclusion: The approach of using synthetic pre-training data is both theoretically sound and practically effective, especially when combined with fine-tuning.

Abstract: We investigate the use of randomly generated data for the sake of
pre-training a model. We justify this approach theoretically from the
perspective of algorithmic complexity, building on recent research that shows
that sequence models can be trained to approximate Solomonoff induction. We
derive similar, but complementary theoretical results. We show empirically that
synthetically generated data can be used to pre-train a model before the data
is seen. We replicate earlier results that models trained this way show
zero-shot in-context learning across a variety of datasets, and that this
performance improves with scale. We extend earlier results to real-world data,
and show that finetuning a model after pre-training offers faster convergence
and better generalization.

</details>


### [143] [Learning Instruction-Following Policies through Open-Ended Instruction Relabeling with Large Language Models](https://arxiv.org/abs/2506.20061)
*Zhicheng Zhang,Ziyan Wang,Yali Du,Fei Fang*

Main category: cs.LG

TL;DR: The paper introduces a method using large language models (LLMs) to relabel unsuccessful reinforcement learning trajectories with meaningful subtasks, improving instruction-following policies without heavy reliance on human annotations.


<details>
  <summary>Details</summary>
Motivation: To overcome challenges in reinforcement learning related to dependence on extensive human-labeled datasets and difficulties with sparse rewards.

Method: Leverage LLMs to retrospectively generate open-ended instructions from agent trajectories, relabel unsuccessful trajectories with meaningful subtasks, and enrich training data for unified instruction-following policy learning.

Result: Empirical evaluations in the Craftax environment show improvements in sample efficiency, instruction coverage, and policy performance compared to state-of-the-art baselines.

Conclusion: LLM-guided open-ended instruction relabeling is an effective approach to enhance instruction-following in reinforcement learning by reducing reliance on human annotations and improving training efficiency.

Abstract: Developing effective instruction-following policies in reinforcement learning
remains challenging due to the reliance on extensive human-labeled instruction
datasets and the difficulty of learning from sparse rewards. In this paper, we
propose a novel approach that leverages the capabilities of large language
models (LLMs) to automatically generate open-ended instructions retrospectively
from previously collected agent trajectories. Our core idea is to employ LLMs
to relabel unsuccessful trajectories by identifying meaningful subtasks the
agent has implicitly accomplished, thereby enriching the agent's training data
and substantially alleviating reliance on human annotations. Through this
open-ended instruction relabeling, we efficiently learn a unified
instruction-following policy capable of handling diverse tasks within a single
policy. We empirically evaluate our proposed method in the challenging Craftax
environment, demonstrating clear improvements in sample efficiency, instruction
coverage, and overall policy performance compared to state-of-the-art
baselines. Our results highlight the effectiveness of utilizing LLM-guided
open-ended instruction relabeling to enhance instruction-following
reinforcement learning.

</details>


### [144] [Supervised Coupled Matrix-Tensor Factorization (SCMTF) for Computational Phenotyping of Patient Reported Outcomes in Ulcerative Colitis](https://arxiv.org/abs/2506.20065)
*Cristian Minoccheri,Sophia Tesic,Kayvan Najarian,Ryan Stidham*

Main category: cs.LG

TL;DR: This paper introduces a novel supervised coupled matrix-tensor factorization (SCMTF) method for computational phenotyping, integrating Patient-Reported Outcomes (PROs) with other data types to predict medication persistence in ulcerative colitis (UC) patients.


<details>
  <summary>Details</summary>
Motivation: The paper aims to overcome the challenge where Patient-Reported Outcomes (PROs) are often excluded in phenotyping due to their noisy, subjective, and sparse nature, despite their importance in accurately understanding disease progression.

Method: The method involves utilizing a supervised coupled matrix-tensor factorization (SCMTF) approach integrated with temporal PROs, temporal lab data, and static features. This is implemented in a deep learning framework that manages missing data and predicts medication persistence.

Result: The method successfully predicts medication changes 8 and 20 months in the future, achieving high AUC scores of 0.853 and 0.803 respectively on the test set. It also generates interpretable phenotypes containing both static and temporal patterns.

Conclusion: The research demonstrates that tensor-based phenotyping methods can be applied effectively in the ulcerative colitis (UC) domain and to highly missing PRO data, offering valuable insights into predicting medication persistence through the inclusion of relevant patient-reported symptom variables.

Abstract: Phenotyping is the process of distinguishing groups of patients to identify
different types of disease progression. A recent trend employs low-rank matrix
and tensor factorization methods for their capability of dealing with
multi-modal, heterogeneous, and missing data. Symptom quantification is crucial
for understanding patient experiences in inflammatory bowel disease, especially
in conditions such as ulcerative colitis (UC). However, patient-reported
symptoms are typically noisy, subjective, and significantly more sparse than
other data types. For this reason, they are usually not included in phenotyping
and other machine learning methods. This paper explores the application of
computational phenotyping to leverage Patient-Reported Outcomes (PROs) using a
novel supervised coupled matrix-tensor factorization (SCMTF) method, which
integrates temporal PROs and temporal labs with static features to predict
medication persistence in ulcerative colitis. This is the first tensor-based
method that is both supervised and coupled, it is the first application to the
UC domain, and the first application to PROs. We use a deep learning framework
that makes the model flexible and easy to train. The proposed method allows us
to handle the large amount of missing data in the PROs. The best model predicts
changes in medication 8 and 20 months in the future with AUCs of 0.853 and
0.803 on the test set respectively. We derive interpretable phenotypes
consisting of static features and temporal features (including their temporal
patterns). We show that low-rank matrix and tensor based phenotyping can be
successfully applied to the UC domain and to highly missing PRO data. We
identify phenotypes useful to predict medication persistence - these phenotypes
include several symptom variables, showing that PROs contain relevant
infromation that is usually discarded.

</details>


### [145] [A Survey of Predictive Maintenance Methods: An Analysis of Prognostics via Classification and Regression](https://arxiv.org/abs/2506.20090)
*Ainaz Jamshidi,Dongchan Kim,Muhammad Arif*

Main category: cs.LG

TL;DR: The paper reviews Predictive Maintenance (PdM) methods, focusing on classification and regression approaches, and explores key advancements and challenges to guide future research.


<details>
  <summary>Details</summary>
Motivation: The study addresses the lack of a standalone, comparative analysis of regression- and classification-based PdM approaches, despite their importance in industrial settings for asset management and reducing downtime.

Method: A comprehensive review of PdM literature is conducted, with detailed discussions on classification-based (failure probability) and regression-based (RUL prediction) methods, challenges, and emerging trends.

Result: The review highlights advancements in PdM, points out challenges such as data imbalance and high-dimensional spaces, and identifies trends like hybrid models and AI-enhanced prognostics.

Conclusion: Researchers and practitioners are provided with insights into the trade-offs of PdM methods, as well as suggestions for future research on public datasets, benchmarking tools, and open-source resources.

Abstract: Predictive maintenance (PdM) has become a crucial element of modern
industrial practice. PdM plays a significant role in operational dependability
and cost management by decreasing unforeseen downtime and optimizing asset life
cycle management. Machine learning and deep learning have enabled more precise
forecasts of equipment failure and remaining useful life (RUL). Although many
studies have been conducted on PdM, there has not yet been a standalone
comparative study between regression- and classification-based approaches. In
this review, we look across a range of PdM methodologies, while focusing more
strongly on the comparative use of classification and regression methods in
prognostics. While regression-based methods typically provide estimates of RUL,
classification-based methods present a forecast of the probability of failure
across defined time intervals. Through a comprehensive analysis of recent
literature, we highlight key advancements, challenges-such as data imbalance
and high-dimensional feature spaces-and emerging trends, including hybrid
approaches and AI-enabled prognostic systems. This review aims to provide
researchers and practitioners with an awareness of the strengths and
compromises of various PdM methods and to help identify future research and
build more robust, directed adaptive maintenance systems. Future work may
include a systematic review of practical aspects such as public datasets,
benchmarking platforms, and open-source tools to support the advancement of PdM
research.

</details>


### [146] [MEL: Multi-level Ensemble Learning for Resource-Constrained Environments](https://arxiv.org/abs/2506.20094)
*Krishna Praneet Gudipaty,Walid A. Hanafy,Kaan Ozkara,Qianlin Liang,Jesse Milzman,Prashant Shenoy,Suhas Diggavi*

Main category: cs.LG

TL;DR: The paper introduces Multi-Level Ensemble Learning (MEL), a framework for resilient AI inference on edge devices, which ensures high accuracy even under server failures.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of maintaining low-latency AI inference services on edge devices that are resource-limited and failure-prone.

Method: The paper proposes MEL, which uses multi-objective optimization to build diverse and lightweight ensemble models that can collaborate or work independently during failures.

Result: Empirical evaluations demonstrate that MEL maintains similar accuracy to original models at 40% of the model size and achieves 95.6% ensemble accuracy under failures.

Conclusion: MEL enhances fault tolerance and deployment flexibility for edge AI inference without compromising on accuracy or performance.

Abstract: AI inference at the edge is becoming increasingly common for low-latency
services. However, edge environments are power- and resource-constrained, and
susceptible to failures. Conventional failure resilience approaches, such as
cloud failover or compressed backups, often compromise latency or accuracy,
limiting their effectiveness for critical edge inference services. In this
paper, we propose Multi-Level Ensemble Learning (MEL), a new framework for
resilient edge inference that simultaneously trains multiple lightweight backup
models capable of operating collaboratively, refining each other when multiple
servers are available, and independently under failures while maintaining good
accuracy. Specifically, we formulate our approach as a multi-objective
optimization problem with a loss formulation that inherently encourages
diversity among individual models to promote mutually refining representations,
while ensuring each model maintains good standalone performance. Empirical
evaluations across vision, language, and audio datasets show that MEL provides
performance comparable to original architectures while also providing fault
tolerance and deployment flexibility across edge platforms. Our results show
that our ensemble model, sized at 40\% of the original model, achieves similar
performance, while preserving 95.6\% of ensemble accuracy in the case of
failures when trained using MEL.

</details>


### [147] [High-Resolution Live Fuel Moisture Content (LFMC) Maps for Wildfire Risk from Multimodal Earth Observation Data](https://arxiv.org/abs/2506.20132)
*Patrick Alan Johnson,Gabriel Tseng,Yawen Zhang,Heather Heward,Virginia Sjahli,Favyen Bastani,Joseph Redmon,Patrick Beukema*

Main category: cs.LG

TL;DR: This paper presents a new AI-based method leveraging earth observation data to create accurate, large-scale Live Fuel Moisture Content (LFMC) maps, critical for wildfire risk management.


<details>
  <summary>Details</summary>
Motivation: Wildfires are becoming increasingly severe, requiring better monitoring of critical factors like LFMC, which is costly and infrequent using traditional ground-based methods.

Method: The authors used a pretrained multimodal earth-observation model to generate spatially complete LFMC maps with reduced errors compared to prior methods.

Result: The approach achieved a 20% reduction in RMSE and enabled rapid creation of LFMC maps across the U.S., validated in wildfire-affected regions.

Conclusion: This method represents a significant advancement in wildfire monitoring, offering scalable, automated, and more accurate LFMC assessment for research and operational response.

Abstract: Wildfires are increasing in intensity and severity at an alarming rate.
Recent advances in AI and publicly available satellite data enable monitoring
critical wildfire risk factors globally, at high resolution and low latency.
Live Fuel Moisture Content (LFMC) is a critical wildfire risk factor and is
valuable for both wildfire research and operational response. However,
ground-based LFMC samples are both labor intensive and costly to acquire,
resulting in sparse and infrequent updates. In this work, we explore the use of
a pretrained, highly-multimodal earth-observation model for generating
large-scale spatially complete (wall-to-wall) LFMC maps. Our approach achieves
significant improvements over previous methods using randomly initialized
models (20 reduction in RMSE). We provide an automated pipeline that enables
rapid generation of these LFMC maps across the United States, and demonstrate
its effectiveness in two regions recently impacted by wildfire (Eaton and
Palisades).

</details>


### [148] [Causal discovery in deterministic discrete LTI-DAE systems](https://arxiv.org/abs/2506.20169)
*Bala Rajesh Konkathi,Arun K. Tangirala*

Main category: cs.LG

TL;DR: This paper presents a new method called Partition of Variables (PoV) to identify pure causes or driver variables in differential-algebraic LTI systems, improving upon a prior method by Kathari and Tangirala (2022).


<details>
  <summary>Details</summary>
Motivation: Discovering pure causal drivers is vital for reconstructing causal networks in deterministic LTI systems. However, existing methods struggle with differential-algebraic systems that include feedback control and conservation laws.

Method: The authors propose the Partition of Variables (PoV) method, which combines DIPCA-based analysis and matrix partitioning to identify causal drivers in LTI-DAE systems. Key steps include determining algebraic and dynamical relations and partitioning the constraint matrix using its condition number.

Result: PoV extends causal discovery to differential-algebraic systems, accurately identifying causal drivers even in systems with algebraic relations where earlier methods might fail.

Conclusion: The Partition of Variables (PoV) method offers a significant advancement in causal discovery by covering both pure dynamical and mixed causal systems, demonstrating effectiveness through case studies.

Abstract: Discovering pure causes or driver variables in deterministic LTI systems is
of vital importance in the data-driven reconstruction of causal networks. A
recent work by Kathari and Tangirala, proposed in 2022, formulated the causal
discovery method as a constraint identification problem. The constraints are
identified using a dynamic iterative PCA (DIPCA)-based approach for dynamical
systems corrupted with Gaussian measurement errors. The DIPCA-based method
works efficiently for dynamical systems devoid of any algebraic relations.
However, several dynamical systems operate under feedback control and/or are
coupled with conservation laws, leading to differential-algebraic (DAE) or
mixed causal systems. In this work, a method, namely the partition of variables
(PoV), for causal discovery in LTI-DAE systems is proposed. This method is
superior to the method that was presented by Kathari and Tangirala (2022), as
PoV also works for pure dynamical systems, which are devoid of algebraic
equations. The proposed method identifies the causal drivers up to a minimal
subset. PoV deploys DIPCA to first determine the number of algebraic relations
($n_a$), the number of dynamical relations ($n_d$) and the constraint matrix.
Subsequently, the subsets are identified through an admissible partitioning of
the constraint matrix by finding the condition number of it. Case studies are
presented to demonstrate the effectiveness of the proposed method.

</details>


### [149] [Causal Operator Discovery in Partial Differential Equations via Counterfactual Physics-Informed Neural Networks](https://arxiv.org/abs/2506.20181)
*Ronald Katende*

Main category: cs.LG

TL;DR: The paper presents a framework using physics-informed neural networks and counterfactuals to discover causal structures in PDEs, showing it outperforms existing methods.


<details>
  <summary>Details</summary>
Motivation: To develop a robust, interpretable method to discover causal structures in PDEs under noisy, redundant, or scarce data conditions.

Method: The method introduces causal sensitivity indices and structural deviation metrics, leveraging functional interventions on neural surrogates for operator-level necessity assessments.

Result: The framework achieves accurate recovery of causal operators across synthetic and real-world datasets, outperforming PINNs and DeepONets.

Conclusion: The method makes causal PDE discovery reliable and interpretable, utilizing structural causal models and variational residual analysis.

Abstract: We develop a principled framework for discovering causal structure in partial
differential equations (PDEs) using physics-informed neural networks and
counterfactual perturbations. Unlike classical residual minimization or sparse
regression methods, our approach quantifies operator-level necessity through
functional interventions on the governing dynamics. We introduce causal
sensitivity indices and structural deviation metrics to assess the influence of
candidate differential operators within neural surrogates. Theoretically, we
prove exact recovery of the causal operator support under restricted isometry
or mutual coherence conditions, with residual bounds guaranteeing
identifiability. Empirically, we validate the framework on both synthetic and
real-world datasets across climate dynamics, tumor diffusion, and ocean flows.
Our method consistently recovers governing operators even under noise,
redundancy, and data scarcity, outperforming standard PINNs and DeepONets in
structural fidelity. This work positions causal PDE discovery as a tractable
and interpretable inference task grounded in structural causal models and
variational residual analysis.

</details>


### [150] [DuoGPT: Training-free Dual Sparsity through Activation-aware Pruning in LLMs](https://arxiv.org/abs/2506.20194)
*Ruokai Yin,Yuhang Li,Donghyun Lee,Priyadarshini Panda*

Main category: cs.LG

TL;DR: The study introduces DuoGPT, a framework leveraging both weight and activation sparsity for efficient large language model execution.


<details>
  <summary>Details</summary>
Motivation: Large language models are powerful but require significant computational and memory resources, prompting the need for more efficient deployment methods.

Method: The paper uses a unified framework blending unstructured weight pruning with activation sparsity, extending Optimal Brain Compression (OBC) with activation-aware calibration and introducing output residual corrections from dense models.

Result: Evaluations of DuoGPT on LLaMA-2 and LLaMA-3 demonstrate superior accuracy by up to 9.17% compared to other pruning methods, with a 1.39× speedup at the same accuracy level as dense models.

Conclusion: DuoGPT effectively combines structured and unstructured sparsity, optimizing LLM performance while maintaining accuracy, and scales efficiently to billion-parameter models.

Abstract: Large language models (LLMs) deliver strong performance but are difficult to
deploy due to high memory and compute costs. While pruning reduces these
demands, most methods ignore activation sparsity observed at runtime. We
reinterpret activation sparsity as dynamic structured weight sparsity and
propose DuoGPT, a unified framework that constructs dual-sparse (spMspV)
workloads by combining unstructured weight pruning with activation sparsity. To
preserve accuracy, we extend the Optimal Brain Compression (OBC) framework with
activation-aware calibration and introduce output residuals from the dense
model as correction terms. We further optimize the solution for efficient GPU
execution, enabling scalability to billion-parameter LLMs. Evaluations on
LLaMA-2 and LLaMA-3 show that DuoGPT outperforms state-of-the-art structured
pruning methods by up to 9.17% accuracy at an iso-speedup of 1.39$\times$
compared to the baseline dense model.

</details>


### [151] [Zero-Shot Attribution for Large Language Models: A Distribution Testing Approach](https://arxiv.org/abs/2506.20197)
*Clément L. Canonne,Yash Pote,Uddalok Sarkar*

Main category: cs.LG

TL;DR: The paper introduces Anubis, a tool for attributing code generated by Large Language Models (LLMs) with high accuracy through hypothesis testing.


<details>
  <summary>Details</summary>
Motivation: To effectively attribute code to specific LLMs, overcoming challenges like the curse of dimensionality in multi-dimensional data.

Method: Proposes a zero-shot attribution approach using a distribution testing problem, leveraging both samples and density estimates from LLMs.

Result: Achieves high AUROC scores (≥0.9) when distinguishing between different LLMs, requiring around 2000 samples.

Conclusion: Anubis demonstrates the feasibility of attributing LLM-generated code accurately, even in zero-shot scenarios.

Abstract: A growing fraction of all code is sampled from Large Language Models (LLMs).
We investigate the problem of attributing code generated by language models
using hypothesis testing to leverage established techniques and guarantees.
Given a set of samples $S$ and a suspect model $\mathcal{L}^*$, our goal is to
assess the likelihood of $S$ originating from $\mathcal{L}^*$. Due to the curse
of dimensionality, this is intractable when only samples from the LLM are
given: to circumvent this, we use both samples and density estimates from the
LLM, a form of access commonly available.
  We introduce $\mathsf{Anubis}$, a zero-shot attribution tool that frames
attribution as a distribution testing problem. Our experiments on a benchmark
of code samples show that $\mathsf{Anubis}$ achieves high AUROC scores (
$\ge0.9$) when distinguishing between LLMs like DeepSeek-Coder, CodeGemma, and
Stable-Code using only $\approx 2000$ samples.

</details>


### [152] [Affective Priming Score: A Data-Driven Method to Detect Priming in Sequential Datasets](https://arxiv.org/abs/2506.20204)
*Eduardo Gutierrez Maestro,Hadi Banaee,Amy Loutfi*

Main category: cs.LG

TL;DR: This study introduces the Affective Priming Score (APS) to detect physiological data influenced by priming effects, reducing misclassification and increasing robustness in affective computing models.


<details>
  <summary>Details</summary>
Motivation: The ambiguity caused by affective priming in affective computing leads to misclassifications, particularly in physiological data, which remains unexplored.

Method: The proposed APS assigns a score to data points to quantify priming effects and validates its efficacy using SEED and SEED-VII datasets by comparing model outcomes on original versus priming-free data.

Result: The misclassification rate in models is significantly reduced when priming-free sequences are used instead of the original data.

Conclusion: Addressing priming effects at the data level enhances the robustness of affective computing models and provides guidance for better dataset design and collection.

Abstract: Affective priming exemplifies the challenge of ambiguity in affective
computing. While the community has largely addressed this issue from a
label-based perspective, identifying data points in the sequence affected by
the priming effect, the impact of priming on data itself, particularly in
physiological signals, remains underexplored. Data affected by priming can lead
to misclassifications when used in learning models. This study proposes the
Affective Priming Score (APS), a data-driven method to detect data points
influenced by the priming effect. The APS assigns a score to each data point,
quantifying the extent to which it is affected by priming. To validate this
method, we apply it to the SEED and SEED-VII datasets, which contain sufficient
transitions between emotional events to exhibit priming effects. We train
models with the same configuration using both the original data and
priming-free sequences. The misclassification rate is significantly reduced
when using priming-free sequences compared to the original data. This work
contributes to the broader challenge of ambiguity by identifying and mitigating
priming effects at the data level, enhancing model robustness, and offering
valuable insights for the design and collection of affective computing
datasets.

</details>


### [153] [Directed Link Prediction using GNN with Local and Global Feature Fusion](https://arxiv.org/abs/2506.20235)
*Yuyang Zhang,Xu Shen,Yu Xie,Ka-Chun Wong,Weidun Xie,Chengbin Peng*

Main category: cs.LG

TL;DR: This paper proposes a novel graph neural network framework that enhances directed link prediction by integrating feature embedding with community information and transforming input graphs into directed line graphs.


<details>
  <summary>Details</summary>
Motivation: To enhance directed link prediction tasks by leveraging hybrid features combining feature embedding with community information.

Method: Development of a graph neural network framework combining feature embedding and community information, coupled with the transformation of input graphs into directed line graphs for efficient neighborhood aggregation during graph convolutions.

Result: Experiments demonstrate superior performance over state-of-the-art methods on benchmark datasets when varying percentages of connected links are used as training data (30%, 40%, 50%, 60%).

Conclusion: Integrating hybrid features and transforming graphs into directed line graphs significantly improves directed link prediction tasks in benchmarks.

Abstract: Link prediction is a classical problem in graph analysis with many practical
applications. For directed graphs, recently developed deep learning approaches
typically analyze node similarities through contrastive learning and aggregate
neighborhood information through graph convolutions. In this work, we propose a
novel graph neural network (GNN) framework to fuse feature embedding with
community information. We theoretically demonstrate that such hybrid features
can improve the performance of directed link prediction. To utilize such
features efficiently, we also propose an approach to transform input graphs
into directed line graphs so that nodes in the transformed graph can aggregate
more information during graph convolutions. Experiments on benchmark datasets
show that our approach outperforms the state-of-the-art in most cases when 30%,
40%, 50%, and 60% of the connected links are used as training data,
respectively.

</details>


### [154] [FedBKD: Distilled Federated Learning to Embrace Gerneralization and Personalization on Non-IID Data](https://arxiv.org/abs/2506.20245)
*Yushan Zhao,Jinyuan He,Donglai Chen,Weijie Luo,Chong Xie,Ri Zhang,Yonghong Chen,Yan Xu*

Main category: cs.LG

TL;DR: The paper proposes Federated Bidirectional Knowledge Distillation (FedBKD), a novel framework using synthetic data through GANs for effective bidirectional knowledge distillation between global and local models in federated learning, addressing non-IID data challenges.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenges of non-identical and independent distributed (non-IID) data in federated learning, where existing methods struggle to achieve both well-generalized global models and well-performed local models, often relying on public datasets that risk data leakage.

Method: FedBKD trains Generative Adversarial Networks (GAN) using local models as frozen discriminators to generate synthetic data. This synthetic data is used for bidirectional distillation between global and local models to enhance their performance simultaneously.

Result: Extensive experiments on four benchmarks under various non-IID settings demonstrate that FedBKD consistently achieves state-of-the-art (SOTA) performance.

Conclusion: FedBKD provides a practical and effective approach to address non-IID data issues in federated learning, achieving strong generalization in global models and high performance in local models without relying on potentially unsafe public datasets.

Abstract: Federated learning (FL) is a decentralized collaborative machine learning
(ML) technique. It provides a solution to the issues of isolated data islands
and data privacy leakage in industrial ML practices. One major challenge in FL
is handling the non-identical and independent distributed (non-IID) data.
Current solutions either focus on constructing an all-powerful global model, or
customizing personalized local models. Few of them can provide both a
well-generalized global model and well-performed local models at the same time.
Additionally, many FL solutions to the non-IID problem are benefited from
introducing public datasets. However, this will also increase the risk of data
leakage. To tackle the problems, we propose a novel data-free distillation
framework, Federated Bidirectional Knowledge Distillation (FedBKD).
Specifically, we train Generative Adversarial Networks (GAN) for synthetic
data. During the GAN training, local models serve as discriminators and their
parameters are frozen. The synthetic data is then used for bidirectional
distillation between global and local models to achieve knowledge interactions
so that performances for both sides are improved. We conduct extensive
experiments on 4 benchmarks under different non-IID settings. The results show
that FedBKD achieves SOTA performances in every case.

</details>


### [155] [Q-resafe: Assessing Safety Risks and Quantization-aware Safety Patching for Quantized Large Language Models](https://arxiv.org/abs/2506.20251)
*Kejia Chen,Jiawen Zhang,Jiacong Hu,Yu Wang,Jian Lou,Zunlei Feng,Mingli Song*

Main category: cs.LG

TL;DR: Quantized large language models (QLLMs) may lose safety capabilities, and Q-resafe is proposed to address these vulnerabilities while preserving utility.


<details>
  <summary>Details</summary>
Motivation: Quantized LLMs are crucial for deployment in low-resource environments, but their compromised safety necessitates systematic evaluation and better strategies.

Method: The paper evaluates different quantization techniques and safety benchmarks, followed by introducing the Q-resafe framework to restore safety in quantized LLMs.

Result: Q-resafe consistently enhances safety in quantized LLMs to levels comparable to pre-quantization models without harming their functionality.

Conclusion: The Q-resafe framework offers an effective solution to restore and preserve safety capabilities in quantized LLMs, ensuring deployment viability in resource-limited settings.

Abstract: Quantized large language models (LLMs) have gained increasing attention and
significance for enabling deployment in resource-constrained environments.
However, emerging studies on a few calibration dataset-free quantization
methods suggest that quantization may compromise the safety capabilities of
LLMs, underscoring the urgent need for systematic safety evaluations and
effective mitigation strategies. In this paper, we present comprehensive safety
evaluations across various mainstream quantization techniques and diverse
calibration datasets, utilizing widely accepted safety benchmarks. To address
the identified safety vulnerabilities, we propose a quantization-aware safety
patching framework, Q-resafe, to efficiently restore the safety capabilities of
quantized LLMs while minimizing any adverse impact on utility. Extensive
experimental results demonstrate that Q-resafe successfully re-aligns the
safety of quantized LLMs with their pre-quantization counterparts, even under
challenging evaluation scenarios. Project page is available at:
https://github.com/Thecommonirin/Qresafe.

</details>


### [156] [Time-series surrogates from energy consumers generated by machine learning approaches for long-term forecasting scenarios](https://arxiv.org/abs/2506.20253)
*Ben Gerhards,Nikita Popkov,Annekatrin König,Marcel Arpogaus,Bastian Schäfermeier,Leonie Riedl,Stephan Vogt,Philip Hehlert*

Main category: cs.LG

TL;DR: This paper evaluates and compares advanced data-driven methods for generating synthetic time series tailored for long-term energy consumption forecasting.


<details>
  <summary>Details</summary>
Motivation: Existing forecasting focuses more on short-term consumption generation and system-level data, leaving individual long-term energy consumption underexplored.

Method: Four state-of-the-art techniques (WGAN, DDPM, HMM, MABF) are assessed for their ability to replicate individual energy consumption dynamics using an open-source German household dataset.

Result: Each method's strengths and limitations in replicating temporal dynamics and ensuring privacy are compared, offering insights for selecting suitable methods for energy forecasting tasks.

Conclusion: This framework enhances synthetic power data generation's accuracy, providing anonymized data for use in tasks like state estimations or forecasting while addressing privacy concerns.

Abstract: Forecasting attracts a lot of research attention in the electricity value
chain. However, most studies concentrate on short-term forecasting of
generation or consumption with a focus on systems and less on individual
consumers. Even more neglected is the topic of long-term forecasting of
individual power consumption.
  Here, we provide an in-depth comparative evaluation of data-driven methods
for generating synthetic time series data tailored to energy consumption
long-term forecasting. High-fidelity synthetic data is crucial for a wide range
of applications, including state estimations in energy systems or power grid
planning. In this study, we assess and compare the performance of multiple
state-of-the-art but less common techniques: a hybrid Wasserstein Generative
Adversarial Network (WGAN), Denoising Diffusion Probabilistic Model (DDPM),
Hidden Markov Model (HMM), and Masked Autoregressive Bernstein polynomial
normalizing Flows (MABF). We analyze the ability of each method to replicate
the temporal dynamics, long-range dependencies, and probabilistic transitions
characteristic of individual energy consumption profiles. Our comparative
evaluation highlights the strengths and limitations of: WGAN, DDPM, HMM and
MABF aiding in selecting the most suitable approach for state estimations and
other energy-related tasks. Our generation and analysis framework aims to
enhance the accuracy and reliability of synthetic power consumption data while
generating data that fulfills criteria like anonymisation - preserving privacy
concerns mitigating risks of specific profiling of single customers. This study
utilizes an open-source dataset from households in Germany with 15min time
resolution. The generated synthetic power profiles can readily be used in
applications like state estimations or consumption forecasting.

</details>


### [157] [Argumentative Ensembling for Robust Recourse under Model Multiplicity](https://arxiv.org/abs/2506.20260)
*Junqi Jiang,Antonio Rago,Francesco Leofante,Francesca Toni*

Main category: cs.LG

TL;DR: This paper addresses challenges in generating counterfactual explanations under model multiplicity by introducing a novel argumentative ensembling method.


<details>
  <summary>Details</summary>
Motivation: Counterfactual explanations under model multiplicity create challenges as competing models may provide conflicting outputs, complicating valid and robust recourse recommendations.

Method: An argumentative ensembling method is proposed, leveraging computational argumentation to resolve conflicts between models and counterfactuals using argumentation semantics.

Result: The method demonstrates effectiveness through theoretical analysis with four semantics and empirical validation across eight instantiations.

Conclusion: Argumentative ensembling provides robust and customizable counterfactual explanations under model multiplicity, meeting desirable properties and enhancing prediction aggregation.

Abstract: In machine learning, it is common to obtain multiple equally performing
models for the same prediction task, e.g., when training neural networks with
different random seeds. Model multiplicity (MM) is the situation which arises
when these competing models differ in their predictions for the same input, for
which ensembling is often employed to determine an aggregation of the outputs.
Providing recourse recommendations via counterfactual explanations (CEs) under
MM thus becomes complex, since the CE may not be valid across all models, i.e.,
the CEs are not robust under MM. In this work, we formalise the problem of
providing recourse under MM, which we name recourse-aware ensembling (RAE). We
propose the idea that under MM, CEs for each individual model should be
considered alongside their predictions so that the aggregated prediction and
recourse are decided in tandem. Centred around this intuition, we introduce six
desirable properties for solutions to this problem. For solving RAE, we propose
a novel argumentative ensembling method which guarantees the robustness of CEs
under MM. Specifically, our method leverages computational argumentation to
explicitly represent the conflicts between models and counterfactuals regarding
prediction results and CE validity. It then uses argumentation semantics to
resolve the conflicts and obtain the final solution, in a manner which is
parametric to the chosen semantics. Our method also allows for the
specification of preferences over the models under MM, allowing further
customisation of the ensemble. In a comprehensive theoretical analysis, we
characterise the behaviour of argumentative ensembling with four different
argumentation semantics. We then empirically demonstrate the effectiveness of
our approach in satisfying desirable properties with eight instantiations of
our method. (Abstract is shortened for arXiv.)

</details>


### [158] [Distilling A Universal Expert from Clustered Federated Learning](https://arxiv.org/abs/2506.20285)
*Zeqi Leng,Chunxu Zhang,Guodong Long,Riting Xia,Bo Yang*

Main category: cs.LG

TL;DR: This paper proposes a novel CFL framework that balances personalized and shared knowledge by introducing a universal expert model distilled from multiple clusters, enhancing model performance in non-IID settings.


<details>
  <summary>Details</summary>
Motivation: Existing CFL methods fail to account for shared knowledge across clusters, which limits the ability to leverage globally generalizable information beneficial for all clients.

Method: The framework operates in three steps: local model training by clients, cluster-specific model aggregation, and universal expert distillation to capture shared information across clusters and distribute it for the next training round.

Result: The proposed approach achieves superior performance across a range of scenarios, demonstrating its ability to handle model heterogeneity and balance personalized and shared knowledge more effectively than traditional methods.

Conclusion: The method offers a flexible and effective solution to improving CFL by preserving client-specific characteristics while learning and sharing global knowledge, advancing the state of CFL research.

Abstract: Clustered Federated Learning (CFL) addresses the challenges posed by non-IID
data by training multiple group- or cluster-specific expert models. However,
existing methods often overlook the shared information across clusters, which
represents the generalizable knowledge valuable to all participants in the
Federated Learning (FL) system. To overcome this limitation, this paper
introduces a novel FL framework that distills a universal expert model from the
knowledge of multiple clusters. This universal expert captures globally shared
information across all clients and is subsequently distributed to each client
as the initialization for the next round of model training. The proposed FL
framework operates in three iterative steps: (1) local model training at each
client, (2) cluster-specific model aggregation, and (3) universal expert
distillation. This three-step learning paradigm ensures the preservation of
fine-grained non-IID characteristics while effectively incorporating shared
knowledge across clusters. Compared to traditional gradient-based aggregation
methods, the distillation-based model aggregation introduces greater
flexibility in handling model heterogeneity and reduces conflicts among
cluster-specific experts. Extensive experimental results demonstrate the
superior performance of the proposed method across various scenarios,
highlighting its potential to advance the state of CFL by balancing
personalized and shared knowledge more effectively.

</details>


### [159] [Learning Moderately Input-Sensitive Functions: A Case Study in QR Code Decoding](https://arxiv.org/abs/2506.20305)
*Kazuki Yoda,Kazuhiko Kawamoto,Hiroshi Kera*

Main category: cs.LG

TL;DR: This study explores the use of Transformers for decoding QR codes and their ability to bypass theoretical error-correction limits while generalizing across languages and strings.


<details>
  <summary>Details</summary>
Motivation: The paper aims to investigate learning functions with medium sensitivity, using QR code decoding as an example, and explore Transformers' ability to learn beyond theoretical limits.

Method: The study employs Transformers to decode QR codes and examines their behavior with English-rich training data and their generalization across languages and random strings.

Result: Transformers successfully decode QR codes beyond error-correction limits, generalize across diverse text types, and focus on data bits instead of error-correction bits.

Conclusion: Transformers exhibit a distinct decoding mechanism compared to standard QR code readers, showcasing their potential for decoding tasks of medium sensitivity.

Abstract: The hardness of learning a function that attains a target task relates to its
input-sensitivity. For example, image classification tasks are
input-insensitive as minor corruptions should not affect the classification
results, whereas arithmetic and symbolic computation, which have been recently
attracting interest, are highly input-sensitive as each input variable connects
to the computation results. This study presents the first learning-based Quick
Response (QR) code decoding and investigates learning functions of medium
sensitivity. Our experiments reveal that Transformers can successfully decode
QR codes, even beyond the theoretical error-correction limit, by learning the
structure of embedded texts. They generalize from English-rich training data to
other languages and even random strings. Moreover, we observe that the
Transformer-based QR decoder focuses on data bits while ignoring
error-correction bits, suggesting a decoding mechanism distinct from standard
QR code readers.

</details>


### [160] [Beyond-Expert Performance with Limited Demonstrations: Efficient Imitation Learning with Double Exploration](https://arxiv.org/abs/2506.20307)
*Heyang Zhao,Xingrui Yu,David M. Bossens,Ivor W. Tsang,Quanquan Gu*

Main category: cs.LG

TL;DR: This paper introduces the ILDE algorithm, enhancing imitation learning by incorporating dual exploration strategies to surpass expert performance using fewer demonstrations.


<details>
  <summary>Details</summary>
Motivation: Imitation learning struggles due to limited demonstrations and complex state spaces, and there is a need to explore environments to achieve performance exceeding expert demonstrations.

Method: ILDE employs two exploration techniques: optimistic policy optimization with uncertainty rewards and curiosity-driven exploration of states deviating from expert demonstrations.

Result: ILDE demonstrated improved sample efficiency and beyond-expert performance in Atari and MuJoCo tasks with fewer demonstrations compared to other algorithms.

Conclusion: ILDE is theoretically validated as an uncertainty-regularized policy optimization method and achieves competitive and beyond-expert performance in a more sample-efficient manner.

Abstract: Imitation learning is a central problem in reinforcement learning where the
goal is to learn a policy that mimics the expert's behavior. In practice, it is
often challenging to learn the expert policy from a limited number of
demonstrations accurately due to the complexity of the state space. Moreover,
it is essential to explore the environment and collect data to achieve
beyond-expert performance. To overcome these challenges, we propose a novel
imitation learning algorithm called Imitation Learning with Double Exploration
(ILDE), which implements exploration in two aspects: (1) optimistic policy
optimization via an exploration bonus that rewards state-action pairs with high
uncertainty to potentially improve the convergence to the expert policy, and
(2) curiosity-driven exploration of the states that deviate from the
demonstration trajectories to potentially yield beyond-expert performance.
Empirically, we demonstrate that ILDE outperforms the state-of-the-art
imitation learning algorithms in terms of sample efficiency and achieves
beyond-expert performance on Atari and MuJoCo tasks with fewer demonstrations
than in previous work. We also provide a theoretical justification of ILDE as
an uncertainty-regularized policy optimization method with optimistic
exploration, leading to a regret growing sublinearly in the number of episodes.

</details>


### [161] [Comparative Analysis of Deep Learning Models for Crop Disease Detection: A Transfer Learning Approach](https://arxiv.org/abs/2506.20323)
*Saundarya Subramaniam,Shalini Majumdar,Shantanu Nadar,Kaustubh Kulkarni*

Main category: cs.LG

TL;DR: The paper develops an AI-driven crop disease detection system using deep learning models, achieving high accuracy and promising improved agricultural practices.


<details>
  <summary>Details</summary>
Motivation: Address challenges of crop disease detection in resource-limited rural areas using AI and transfer learning.

Method: Compare deep learning models (EfficientNet, ResNet101, MobileNetV2, and custom CNN) to evaluate their efficacy, with a focus on transfer learning.

Result: Custom CNN model achieved a validation accuracy of 95.76%, effectively classifying plant diseases.

Conclusion: Transfer learning can improve agricultural practices, crop health, and enable sustainable farming in rural regions.

Abstract: This research presents the development of an Artificial Intelligence (AI) -
driven crop disease detection system designed to assist farmers in rural areas
with limited resources. We aim to compare different deep learning models for a
comparative analysis, focusing on their efficacy in transfer learning. By
leveraging deep learning models, including EfficientNet, ResNet101,
MobileNetV2, and our custom CNN, which achieved a validation accuracy of
95.76%, the system effectively classifies plant diseases. This research
demonstrates the potential of transfer learning in reshaping agricultural
practices, improving crop health management, and supporting sustainable farming
in rural environments.

</details>


### [162] [Permutation Equivariant Neural Controlled Differential Equations for Dynamic Graph Representation Learning](https://arxiv.org/abs/2506.20324)
*Torben Berndt,Benjamin Walker,Tiexin Qin,Jan Stühmer,Andrey Kormilitzin*

Main category: cs.LG

TL;DR: The paper introduces Permutation Equivariant Neural Graph CDEs for dynamic graphs, reducing model parameters while maintaining performance, achieving better training efficiency and generalization.


<details>
  <summary>Details</summary>
Motivation: To simplify and enhance the efficiency of Graph Neural CDEs while maintaining their representational power, particularly for tasks involving dynamic graphs.

Method: The approach projects Graph Neural CDEs onto permutation equivariant function spaces, enabling a reduced parameter count and efficient training without loss of performance.

Result: The proposed method demonstrates improved performance in interpolation and extrapolation tasks across both simulated dynamical systems and real-world applications.

Conclusion: Permutation Equivariant Neural Graph CDEs offer a promising method for handling dynamic graph data, balancing efficiency and strong representational performance.

Abstract: Dynamic graphs exhibit complex temporal dynamics due to the interplay between
evolving node features and changing network structures. Recently, Graph Neural
Controlled Differential Equations (Graph Neural CDEs) successfully adapted
Neural CDEs from paths on Euclidean domains to paths on graph domains. Building
on this foundation, we introduce Permutation Equivariant Neural Graph CDEs,
which project Graph Neural CDEs onto permutation equivariant function spaces.
This significantly reduces the model's parameter count without compromising
representational power, resulting in more efficient training and improved
generalisation. We empirically demonstrate the advantages of our approach
through experiments on simulated dynamical systems and real-world tasks,
showing improved performance in both interpolation and extrapolation scenarios.

</details>


### [163] [Counterfactual Influence as a Distributional Quantity](https://arxiv.org/abs/2506.20481)
*Matthieu Meeus,Igor Shilov,Georgios Kaissis,Yves-Alexandre de Montjoye*

Main category: cs.LG

TL;DR: The paper highlights limitations of using self-influence to study memorization in machine learning models, advocating for considering the full influence distribution across training data.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing memorization studies, which rely heavily on self-influence, and to better understand how memorization in machine learning models is influenced by complex interactions across training data.

Method: The paper computes the full influence distribution of training samples on one another for a small language model and analyzes its properties. Additionally, it investigates influence distributions in image classification by reviewing CIFAR-10 data.

Result: Findings indicate that (near-)duplicates greatly reduce self-influence, yet these samples remain (near-)extractable. Influence distributions also reveal the presence of near-duplicates in CIFAR-10.

Conclusion: Memorization in machine learning models is a complex phenomenon driven by interactions across training data. Full influence distribution provides a more accurate understanding of memorization compared to just self-influence.

Abstract: Machine learning models are known to memorize samples from their training
data, raising concerns around privacy and generalization. Counterfactual
self-influence is a popular metric to study memorization, quantifying how the
model's prediction for a sample changes depending on the sample's inclusion in
the training dataset. However, recent work has shown memorization to be
affected by factors beyond self-influence, with other training samples, in
particular (near-)duplicates, having a large impact. We here study memorization
treating counterfactual influence as a distributional quantity, taking into
account how all training samples influence how a sample is memorized. For a
small language model, we compute the full influence distribution of training
samples on each other and analyze its properties. We find that solely looking
at self-influence can severely underestimate tangible risks associated with
memorization: the presence of (near-)duplicates seriously reduces
self-influence, while we find these samples to be (near-)extractable. We
observe similar patterns for image classification, where simply looking at the
influence distributions reveals the presence of near-duplicates in CIFAR-10.
Our findings highlight that memorization stems from complex interactions across
training data and is better captured by the full influence distribution than by
self-influence alone.

</details>


### [164] [Producer-Fairness in Sequential Bundle Recommendation](https://arxiv.org/abs/2506.20329)
*Alexandre Rio,Marta Soare,Sihem Amer-Yahia*

Main category: cs.LG

TL;DR: The paper tackles the issue of fairness in sequential bundle recommendation by proposing producer-fairness, a framework for ensuring exposure equity among item groups. Methods include exact solutions and heuristics balancing fairness and quality, validated through experiments.


<details>
  <summary>Details</summary>
Motivation: Fairness in recommendation systems, particularly balancing exposure among diverse item groups, is a pressing issue in real-world scenarios to ensure unbiased and equitable user experience.

Method: The paper develops an exact solution for small instances and explores heuristics like quality-first, fairness-first, and adaptive variants to balance fairness and quality in bundle recommendation.

Result: Experiments on three datasets show that the methods are effective in achieving fairness without significantly compromising the quality of recommended bundles.

Conclusion: The paper successfully formulates and validates approaches to integrate fairness in bundle recommendations, highlighting strengths and limitations of different methods for optimizing both fairness and quality.

Abstract: We address fairness in the context of sequential bundle recommendation, where
users are served in turn with sets of relevant and compatible items. Motivated
by real-world scenarios, we formalize producer-fairness, that seeks to achieve
desired exposure of different item groups across users in a recommendation
session. Our formulation combines naturally with building high quality bundles.
Our problem is solved in real time as users arrive. We propose an exact
solution that caters to small instances of our problem. We then examine two
heuristics, quality-first and fairness-first, and an adaptive variant that
determines on-the-fly the right balance between bundle fairness and quality.
Our experiments on three real-world datasets underscore the strengths and
limitations of each solution and demonstrate their efficacy in providing fair
bundle recommendations without compromising bundle quality.

</details>


### [165] [Asymmetric REINFORCE for off-Policy Reinforcement Learning: Balancing positive and negative rewards](https://arxiv.org/abs/2506.20520)
*Charles Arnal,Gaëtan Narozniak,Vivien Cabannes,Yunhao Tang,Julia Kempe,Remi Munos*

Main category: cs.LG

TL;DR: The paper explores an intermediate algorithm between off-policy reinforcement learning and supervised fine-tuning for aligning large language models, focusing on leveraging reward signals efficiently.


<details>
  <summary>Details</summary>
Motivation: Investigate a simpler and data-efficient approach for aligning large language models with reinforcement learning.

Method: Study the off-policy REINFORCE algorithm with a tunable baseline value (V) to emphasize or penalize rewards in fine-tuning processes.

Result: The theoretical and experimental analysis indicates policy improvement guarantees when V is correctly set, and emphasizes the advantage of focusing on positive rewards in off-policy RL.

Conclusion: Off-policy RL methods can be optimized by carefully leveraging reward baselines, showing practical alignment benefits for state-of-the-art language models.

Abstract: Reinforcement learning (RL) is increasingly used to align large language
models (LLMs). Off-policy methods offer greater implementation simplicity and
data efficiency than on-policy techniques, but often result in suboptimal
performance. In this work, we study the intermediate range of algorithms
between off-policy RL and supervised fine-tuning by analyzing a simple
off-policy REINFORCE algorithm, where the advantage is defined as $A=r-V$, with
$r$ a reward and $V$ some tunable baseline. Intuitively, lowering $V$
emphasizes high-reward samples, while raising it penalizes low-reward ones more
heavily. We first provide a theoretical analysis of this off-policy REINFORCE
algorithm, showing that when the baseline $V$ lower-bounds the expected reward,
the algorithm enjoys a policy improvement guarantee. Our analysis reveals that
while on-policy updates can safely leverage both positive and negative signals,
off-policy updates benefit from focusing more on positive rewards than on
negative ones. We validate our findings experimentally in a controlled
stochastic bandit setting and through fine-tuning state-of-the-art LLMs on
reasoning tasks.

</details>


### [166] [DipSVD: Dual-importance Protected SVD for Efficient LLM Compression](https://arxiv.org/abs/2506.20353)
*Xuan Ding,Rui Sun,Yunjian Zhang,Xiu Yan,Yueqi Zhou,Kaihao Huang,Suzhong Fu,Chuanlong Xie,Yao Zhu*

Main category: cs.LG

TL;DR: The paper introduces DipSVD, a method for improving SVD-based compression of large language models by protecting the most critical components locally and globally within weight matrices, leading to superior performance.


<details>
  <summary>Details</summary>
Motivation: Existing SVD-based model compression methods overlook critical components in weight matrices during compression, which degrades model performance despite computational benefits and hardware compatibility.

Method: The proposed method, DipSVD, combines local importance protection via channel-weighted whitening to preserve key singular vectors, and global importance protection by redistributing compression load across layers based on heuristic or optimization strategies.

Result: Experiments show that DipSVD achieves better performance than current SVD-based methods, particularly at high compression ratios, across multiple benchmarks.

Conclusion: Dual-level importance protection in DipSVD significantly improves SVD compression of large language models by strategically preserving crucial matrix components and optimizing compression impact across layers.

Abstract: The ever-increasing computational demands and deployment costs of large
language models (LLMs) have spurred numerous compressing methods. Compared to
quantization and unstructured pruning, SVD compression offers superior hardware
compatibility and theoretical guarantees. However, existing SVD-based methods
focus on the overall discrepancy between the original and compressed matrices
while overlooking the protection of critical components within the matrix,
which leads to inferior performance in the compressed models. This paper
proposes a dual-level importance protection mechanism to enhance SVD-based
compression methods: (1) local importance protection: preserving the most
critical singular vectors within each weight matrix through channel-weighted
data whitening; and (2) global importance protection: enabling less important
layers to bear a greater portion of the compression burden through either a
heuristic or optimization-based approach, thereby minimizing the impact of
compression on critical layers. Extensive experiments demonstrate that DipSVD
outperforms existing SVD-based compression approaches across multiple
benchmarks, achieving superior model performance especially at high model
compression ratios.

</details>


### [167] [A foundation model with multi-variate parallel attention to generate neuronal activity](https://arxiv.org/abs/2506.20354)
*Francesco Carzaniga,Michael Hersche,Abu Sebastian,Kaspar Schindler,Abbas Rahimi*

Main category: cs.LG

TL;DR: The paper introduces MVPFormer, a novel generative foundation model for analyzing heterogeneous multi-variate time-series data, applied to clinical iEEG, using a new attention mechanism, MVPA.


<details>
  <summary>Details</summary>
Motivation: Modeling multi-variate time-series data with varying channel setups, particularly for clinical applications like iEEG, is challenging due to the heterogeneity in channel configurations across subjects.

Method: The authors propose a Multi-Variate Parallel Attention (MVPA) mechanism to disentangle content, temporal, and spatial attention. They implement this in a model called MVPFormer, trained on the SWEC iEEG dataset for generalization across subjects in predicting iEEG signals.

Result: MVPFormer achieves expert-level performance in seizure detection and outperforms existing Transformer models on multiple datasets (SWEC, MAYO, FNUSA). Additionally, MVPA proves its utility in other time-series forecasting and classification tasks.

Conclusion: The paper establishes MVPA as a general-purpose attention mechanism for heterogeneous time-series data and positions MVPFormer as a state-of-the-art foundation model for iEEG with open-source resources to advance community research.

Abstract: Learning from multi-variate time-series with heterogeneous channel
configurations remains a fundamental challenge for deep neural networks (DNNs),
particularly in clinical domains such as intracranial electroencephalography
(iEEG), where channel setups vary widely across subjects. In this work, we
introduce multi-variate parallel attention (MVPA), a novel self-attention
mechanism that disentangles content, temporal, and spatial attention, enabling
flexible, generalizable, and efficient modeling of time-series data with
varying channel counts and configurations. We use MVPA to build MVPFormer, a
generative foundation model for human electrophysiology, trained to predict the
evolution of iEEG signals across diverse subjects. To support this and future
effort by the community, we release the SWEC iEEG dataset, the largest publicly
available iEEG dataset to date, comprising nearly 10,000 hours of recordings
from heterogeneous clinical sources. MVPFormer leverages MVPA to achieve strong
generalization across subjects, demonstrating expert-level performance in
seizure detection and outperforming state-of-the-art Transformer baselines on
our SWEC, the MAYO, and the FNUSA dataset. We further validate MVPA on standard
time-series forecasting and classification tasks, where it matches or exceeds
existing attention-based models. Together, our contributions establish MVPA as
a general-purpose attention mechanism for heterogeneous time-series and
MVPFormer as the first open-source, open-weights, and open-data iEEG foundation
model with state-of-the-art clinical performance. The code is available at
https://github.com/IBM/multi-variate-parallel-transformer. The SWEC iEEG
dataset is available at
https://mb-neuro.medical-blocks.ch/public_access/databases/ieeg/swec_ieeg.

</details>


### [168] [Towards Interpretable and Efficient Feature Selection in Trajectory Datasets: A Taxonomic Approach](https://arxiv.org/abs/2506.20359)
*Chanuka Don Samarasinghage,Dhruv Gulabani*

Main category: cs.LG

TL;DR: The paper proposes a taxonomy-based feature selection method to address the feature explosion problem in trajectory analysis, improving interpretability and model performance.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the feature explosion problem caused by high-dimensionality in trajectory datasets, which hampers the efficiency and accuracy of machine learning models.

Method: A taxonomy-based feature selection approach categorizes data into geometric and kinematic features, further sub-classified into curvature, indentation, speed, and acceleration. This reduces dimensionality and computational complexity.

Result: The taxonomy-based approach provided comparable or superior predictive performance while drastically reducing the time for feature selection and offering insights into dataset sensitivity.

Conclusion: This study demonstrates that taxonomy-based feature selection enhances interpretability, reduces dimensionality, and computational complexity, contributing to explainable AI and improved decision-making in trajectory analysis.

Abstract: Trajectory analysis is not only about obtaining movement data, but it is also
of paramount importance in understanding the pattern in which an object moves
through space and time, as well as in predicting its next move. Due to the
significant interest in the area, data collection has improved substantially,
resulting in a large number of features becoming available for training and
predicting models. However, this introduces a high-dimensionality-induced
feature explosion problem, which reduces the efficiency and interpretability of
the data, thereby reducing the accuracy of machine learning models. To overcome
this issue, feature selection has become one of the most prevalent tools. Thus,
the objective of this paper was to introduce a taxonomy-based feature selection
method that categorizes features based on their internal structure. This
approach classifies the data into geometric and kinematic features, further
categorizing them into curvature, indentation, speed, and acceleration. The
comparative analysis indicated that a taxonomy-based approach consistently
achieved comparable or superior predictive performance. Furthermore, due to the
taxonomic grouping, which reduces combinatorial space, the time taken to select
features was drastically reduced. The taxonomy was also used to gain insights
into what feature sets each dataset was more sensitive to. Overall, this study
provides robust evidence that a taxonomy-based feature selection method can add
a layer of interpretability, reduce dimensionality and computational
complexity, and contribute to high-level decision-making. It serves as a step
toward providing a methodological framework for researchers and practitioners
dealing with trajectory datasets and contributing to the broader field of
explainable artificial intelligence.

</details>


### [169] [Self-Supervised Graph Learning via Spectral Bootstrapping and Laplacian-Based Augmentations](https://arxiv.org/abs/2506.20362)
*Lorenzo Bini,Stephane Marchand-Maillet*

Main category: cs.LG

TL;DR: LaplaceGNN, a self-supervised graph learning framework, eliminates the need for negative sampling or handcrafted augmentations by using spectral and adversarial techniques for efficient representation learning.


<details>
  <summary>Details</summary>
Motivation: To address inefficiencies and complexities in self-supervised graph learning caused by reliance on negative sampling and handcrafted augmentations.

Method: LaplaceGNN precomputes spectral augmentations based on max-min centrality optimization and employs adversarial bootstrapped training for robust feature learning.

Result: Experimental evaluations on benchmark datasets show LaplaceGNN outperforms state-of-the-art methods in self-supervised graph representation learning.

Conclusion: LaplaceGNN provides a simple, efficient solution for learning expressive graph representations, positioning it as a promising alternative for self-supervised graph neural networks.

Abstract: We present LaplaceGNN, a novel self-supervised graph learning framework that
bypasses the need for negative sampling by leveraging spectral bootstrapping
techniques. Our method integrates Laplacian-based signals into the learning
process, allowing the model to effectively capture rich structural
representations without relying on contrastive objectives or handcrafted
augmentations. By focusing on positive alignment, LaplaceGNN achieves linear
scaling while offering a simpler, more efficient, self-supervised alternative
for graph neural networks, applicable across diverse domains. Our contributions
are twofold: we precompute spectral augmentations through max-min
centrality-guided optimization, enabling rich structural supervision without
relying on handcrafted augmentations, then we integrate an adversarial
bootstrapped training scheme that further strengthens feature learning and
robustness. Our extensive experiments on different benchmark datasets show that
LaplaceGNN achieves superior performance compared to state-of-the-art
self-supervised graph methods, offering a promising direction for efficiently
learning expressive graph representations.

</details>


### [170] [TESSERA: Temporal Embeddings of Surface Spectra for Earth Representation and Analysis](https://arxiv.org/abs/2506.20380)
*Zhengpeng Feng,Sadiq Jaffer,Jovana Knezevic,Silja Sormunen,Robin Young,Madeline Lisaius,Markus Immitzer,James Ball,Clement Atzberger,David A. Coomes,Anil Madhavapeddy,Andrew Blake,Srinivasan Keshav*

Main category: cs.LG

TL;DR: TESSERA is a self-supervised Remote Sensing Foundation Model (RSFM) generating global, high-resolution representations from optical and SAR satellite data, outperforming traditional and foundation models in multiple tasks.


<details>
  <summary>Details</summary>
Motivation: To enable better Earth observation applications by improving representation extraction from satellite data using a self-supervised foundation model.

Method: TESSERA employs two parallel Transformer-based encoders dedicated to Sentinel-1 SAR polarizations and Sentinel-2 MSI spectral bands, followed by fusion via MLP to create robust global representations.

Result: TESSERA provides precomputed high-resolution global representations (2017–2024) that outperform existing RS baselines and geospatial foundation models across varied EO tasks.

Conclusion: TESSERA establishes a state-of-the-art benchmark while democratizing access to advanced RS tools through an open-source approach.

Abstract: Satellite remote sensing (RS) enables a wide array of downstream Earth
observation (EO) applications, including climate modeling, carbon accounting,
and strategies for conservation and sustainable land use. We present TESSERA, a
novel Remote Sensing Foundation Model (RSFM) that uses Self-Supervised Learning
(SSL) to generate global, robust representations at 10m scale from pixel-level
satellite time series data. TESSERA combines information from only optical and
SAR data streams using two parallel Transformer-based encoders: one dedicated
to Sentinel-1 SAR polarizations and another to Sentinel-2 MSI data (10 selected
spectral bands) to create representations that are then fused using a
multilayer perceptron (MLP), resulting in a global representation map covering
the years 2017 to 2024. Our precomputed representations set a new
state-of-the-art performance benchmark and our open-source approach
democratizes access to high-performance, high-resolution representations. We
benchmark the performance of TESSERA in five diverse tasks, comparing our work
with state-of-the-art task-specific models and other foundation models. Our
results show that TESSERA outperforms both traditional RS baselines and the
leading geospatial foundation models in these diverse downstream tasks.

</details>


### [171] [Client Clustering Meets Knowledge Sharing: Enhancing Privacy and Robustness in Personalized Peer-to-Peer Learning](https://arxiv.org/abs/2506.20413)
*Mohammad Mahdi Maheri,Denys Herasymuk,Hamed Haddadi*

Main category: cs.LG

TL;DR: The paper introduces P4, a method enabling personalized, private, and decentralized learning for IoT devices, addressing challenges like privacy and resilience against attacks.


<details>
  <summary>Details</summary>
Motivation: The increasing integration of AI into IoT devices has highlighted the need for efficient and private learning methods capable of operating on resource-constrained and decentralized systems.

Method: The paper proposes P4, a lightweight decentralized method that uses differential privacy and client similarity detection to form collaborative groups, leveraging knowledge distillation for model training.

Result: P4 demonstrates 5%-30% higher accuracy over competitors while being robust against up to 30% malicious clients. It is also deployable with minimal overhead on resource-constrained devices.

Conclusion: P4 effectively enables personalized, robust, and private collaborative learning for IoT devices, addressing key challenges in decentralized systems.

Abstract: The growing adoption of Artificial Intelligence (AI) in Internet of Things
(IoT) ecosystems has intensified the need for personalized learning methods
that can operate efficiently and privately across heterogeneous,
resource-constrained devices. However, enabling effective personalized learning
in decentralized settings introduces several challenges, including efficient
knowledge transfer between clients, protection of data privacy, and resilience
against poisoning attacks. In this paper, we address these challenges by
developing P4 (Personalized, Private, Peer-to-Peer) -- a method designed to
deliver personalized models for resource-constrained IoT devices while ensuring
differential privacy and robustness against poisoning attacks. Our solution
employs a lightweight, fully decentralized algorithm to privately detect client
similarity and form collaborative groups. Within each group, clients leverage
differentially private knowledge distillation to co-train their models,
maintaining high accuracy while ensuring robustness to the presence of
malicious clients. We evaluate P4 on popular benchmark datasets using both
linear and CNN-based architectures across various heterogeneity settings and
attack scenarios. Experimental results show that P4 achieves 5% to 30% higher
accuracy than leading differentially private peer-to-peer approaches and
maintains robustness with up to 30% malicious clients. Additionally, we
demonstrate its practicality by deploying it on resource-constrained devices,
where collaborative training between two clients adds only ~7 seconds of
overhead.

</details>


### [172] [Off-Policy Evaluation and Learning for the Future under Non-Stationarity](https://arxiv.org/abs/2506.20417)
*Tatsuhiro Shimizu,Kazuki Kawamura,Takanori Muroi,Yusuke Narita,Kei Tateno,Takuma Udagawa,Yuta Saito*

Main category: cs.LG

TL;DR: This paper tackles the challenge of predicting and optimizing future policy values in non-stationary environments through a novel estimator (OPFV), exploiting time-based structures.


<details>
  <summary>Details</summary>
Motivation: Current methods for predicting future policy values in non-stationary settings are biased or restrictive, limiting their applicability in areas like e-commerce recommendations.

Method: The paper introduces OPFV, a new estimator that uses time-series structures and importance weighting for future off-policy evaluation, and extends it to a policy-gradient method.

Result: Theoretical analysis shows low bias under certain conditions, and experimental setups demonstrate significant performance improvement over existing methods.

Conclusion: OPFV successfully addresses non-stationary challenges, offering accurate future policy evaluation and optimization using historical data.

Abstract: We study the novel problem of future off-policy evaluation (F-OPE) and
learning (F-OPL) for estimating and optimizing the future value of policies in
non-stationary environments, where distributions vary over time. In e-commerce
recommendations, for instance, our goal is often to estimate and optimize the
policy value for the upcoming month using data collected by an old policy in
the previous month. A critical challenge is that data related to the future
environment is not observed in the historical data. Existing methods assume
stationarity or depend on restrictive reward-modeling assumptions, leading to
significant bias. To address these limitations, we propose a novel estimator
named \textit{\textbf{O}ff-\textbf{P}olicy Estimator for the \textbf{F}uture
\textbf{V}alue (\textbf{\textit{OPFV}})}, designed for accurately estimating
policy values at any future time point. The key feature of OPFV is its ability
to leverage the useful structure within time-series data. While future data
might not be present in the historical log, we can leverage, for example,
seasonal, weekly, or holiday effects that are consistent in both the historical
and future data. Our estimator is the first to exploit these time-related
structures via a new type of importance weighting, enabling effective F-OPE.
Theoretical analysis identifies the conditions under which OPFV becomes
low-bias. In addition, we extend our estimator to develop a new policy-gradient
method to proactively learn a good future policy using only historical data.
Empirical results show that our methods substantially outperform existing
methods in estimating and optimizing the future policy value under
non-stationarity for various experimental setups.

</details>


### [173] [Tackling Data Heterogeneity in Federated Learning through Knowledge Distillation with Inequitable Aggregation](https://arxiv.org/abs/2506.20431)
*Xing Ma*

Main category: cs.LG

TL;DR: This paper addresses performance degradation in federated learning caused by data heterogeneity and limited client participation by introducing the KDIA strategy, which improves accuracy in fewer training rounds.


<details>
  <summary>Details</summary>
Motivation: Federated learning faces challenges such as label skew, heterogeneous data distributions, and limited client participation, which degrade model performance.

Method: The proposed KDIA strategy combines an inequitable aggregation approach for teacher and student models, self-knowledge distillation during local training, and server-generated IID data features for auxiliary training.

Result: Experiments on datasets like CIFAR-10/100/CINIC-10 under heterogeneous settings reveal that KDIA significantly enhances accuracy and performs well with fewer training rounds, especially in severe heterogeneity scenarios.

Conclusion: KDIA effectively mitigates federated learning challenges by leveraging all client data and addressing issues related to data heterogeneity and limited participation, offering practical benefits for scalable applications.

Abstract: Federated learning aims to train a global model in a distributed environment
that is close to the performance of centralized training. However, issues such
as client label skew, data quantity skew, and other heterogeneity problems
severely degrade the model's performance. Most existing methods overlook the
scenario where only a small portion of clients participate in training within a
large-scale client setting, whereas our experiments show that this scenario
presents a more challenging federated learning task. Therefore, we propose a
Knowledge Distillation with teacher-student Inequitable Aggregation (KDIA)
strategy tailored to address the federated learning setting mentioned above,
which can effectively leverage knowledge from all clients. In KDIA, the student
model is the average aggregation of the participating clients, while the
teacher model is formed by a weighted aggregation of all clients based on three
frequencies: participation intervals, participation counts, and data volume
proportions. During local training, self-knowledge distillation is performed.
Additionally, we utilize a generator trained on the server to generate
approximately independent and identically distributed (IID) data features
locally for auxiliary training. We conduct extensive experiments on the
CIFAR-10/100/CINIC-10 datasets and various heterogeneous settings to evaluate
KDIA. The results show that KDIA can achieve better accuracy with fewer rounds
of training, and the improvement is more significant under severe
heterogeneity.

</details>


### [174] [Méthode de quadrature pour les PINNs fondée théoriquement sur la hessienne des résiduels](https://arxiv.org/abs/2506.20441)
*Antoine Caradot,Rémi Emonet,Amaury Habrard,Abdel-Rahim Mezidi,Marc Sebban*

Main category: cs.LG

TL;DR: The paper introduces a novel method to enhance PINNs by utilizing Hessian-based quadrature for better collocation point selection.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the importance of adaptive sampling to improve the efficiency and accuracy of PINNs in solving PDEs.

Method: A new quadrature approach, based on the Hessian of the function, is used to refine the selection of collocation points during the training of Physics-informed Neural Networks.

Result: The proposed method improves the approximation of definite integrals and enhances the training process of PINNs.

Conclusion: The integration of Hessian-based quadrature into PINN training is effective and has potential to further optimize the surrogate neural solvers for PDEs.

Abstract: Physics-informed Neural Networks (PINNs) have emerged as an efficient way to
learn surrogate neural solvers of PDEs by embedding the physical model in the
loss function and minimizing its residuals using automatic differentiation at
so-called collocation points. Originally uniformly sampled, the choice of the
latter has been the subject of recent advances leading to adaptive sampling
refinements. In this paper, we propose a new quadrature method for
approximating definite integrals based on the hessian of the considered
function, and that we leverage to guide the selection of the collocation points
during the training process of PINNs.

</details>


### [175] [Automatic Demonstration Selection for LLM-based Tabular Data Classification](https://arxiv.org/abs/2506.20451)
*Shuchu Han,Wolfgang Bruckner*

Main category: cs.LG

TL;DR: The paper proposes an algorithm to determine the optimal number of demonstrations for In-Context Learning (ICL) in tabular data classification, using spectral graph theory.


<details>
  <summary>Details</summary>
Motivation: The challenge of determining the ideal number of demonstrations for ICL in tabular data classification, considering data distribution, prompt templates, and LLMs.

Method: A new metric based on spectral graph theory quantifies similarities between demonstrations. A similarity graph is constructed, and the Laplacian eigenvalues are analyzed to determine the minimum required demonstrations.

Result: The proposed algorithm outperforms traditional random selection methods across various datasets and LLMs.

Conclusion: The algorithm effectively identifies the optimal number of demonstrations, integrating multiple factors to enhance ICL performance for tabular data.

Abstract: A fundamental question in applying In-Context Learning (ICL) for tabular data
classification is how to determine the ideal number of demonstrations in the
prompt. This work addresses this challenge by presenting an algorithm to
automatically select a reasonable number of required demonstrations. Our method
distinguishes itself by integrating not only the tabular data's distribution
but also the user's selected prompt template and the specific Large Language
Model (LLM) into its estimation. Rooted in Spectral Graph Theory, our proposed
algorithm defines a novel metric to quantify the similarities between different
demonstrations. We then construct a similarity graph and analyze the
eigenvalues of its Laplacian to derive the minimum number of demonstrations
capable of representing the data within the LLM's intrinsic representation
space. We validate the efficacy of our approach through experiments comparing
its performance against conventional random selection algorithms on diverse
datasets and LLMs.

</details>


### [176] [Multimodal Representation Learning and Fusion](https://arxiv.org/abs/2506.20494)
*Qihang Jin,Enze Ge,Yuhang Xie,Hongying Luo,Junhao Song,Ziqian Bi,Chia Xin Liang,Jibin Guan,Joe Yeong,Junfeng Hao*

Main category: cs.LG

TL;DR: Multi-modal learning integrates diverse data types like images, text, and audio to create richer AI representations, tackling challenges in incomplete inputs and adversarial attacks for improved reasoning.


<details>
  <summary>Details</summary>
Motivation: To enhance AI understanding of complex real-world contexts by leveraging multiple data modalities such as images, text, and audio for better interpretation and decision-making.

Method: Core techniques include representation learning for shared features, alignment methods for cross-modal matching, and fusion strategies using deep learning. Researchers explore unsupervised learning, AutoML tools, and standardized benchmarks.

Result: Improved model efficiency, scalability, and comparative metrics for multi-modal tasks across domains. Progress in key AI areas such as computer vision, NLP, and healthcare.

Conclusion: Growing advancements in multi-modal learning hold the potential for creating human-like AI with flexibility, contextual awareness, and real-world reasoning capabilities.

Abstract: Multi-modal learning is a fast growing area in artificial intelligence. It
tries to help machines understand complex things by combining information from
different sources, like images, text, and audio. By using the strengths of each
modality, multi-modal learning allows AI systems to build stronger and richer
internal representations. These help machines better interpretation, reasoning,
and making decisions in real-life situations. This field includes core
techniques such as representation learning (to get shared features from
different data types), alignment methods (to match information across
modalities), and fusion strategies (to combine them by deep learning models).
Although there has been good progress, some major problems still remain. Like
dealing with different data formats, missing or incomplete inputs, and
defending against adversarial attacks. Researchers now are exploring new
methods, such as unsupervised or semi-supervised learning, AutoML tools, to
make models more efficient and easier to scale. And also more attention on
designing better evaluation metrics or building shared benchmarks, make it
easier to compare model performance across tasks and domains. As the field
continues to grow, multi-modal learning is expected to improve many areas:
computer vision, natural language processing, speech recognition, and
healthcare. In the future, it may help to build AI systems that can understand
the world in a way more like humans, flexible, context aware, and able to deal
with real-world complexity.

</details>


### [177] [WallStreetFeds: Client-Specific Tokens as Investment Vehicles in Federated Learning](https://arxiv.org/abs/2506.20518)
*Arno Geimer,Beltran Fiz Pontiveros,Radu State*

Main category: cs.LG

TL;DR: The paper introduces a novel framework using decentralized finance mechanisms to improve reward distribution in Federated Learning.


<details>
  <summary>Details</summary>
Motivation: Federated Learning requires effective incentive schemes for participants, yet reward distribution methods remain understudied.

Method: The authors propose using client-specific tokens, a decentralized finance platform, and automated market makers to create a flexible reward distribution system.

Result: The framework allows for improved reward distribution and introduces a mechanism for third parties to invest in the FL ecosystem.

Conclusion: Integrating DeFi mechanisms offers a scalable and innovative approach to address existing limitations in FL reward schemes.

Abstract: Federated Learning (FL) is a collaborative machine learning paradigm which
allows participants to collectively train a model while training data remains
private. This paradigm is especially beneficial for sectors like finance, where
data privacy, security and model performance are paramount. FL has been
extensively studied in the years following its introduction, leading to, among
others, better performing collaboration techniques, ways to defend against
other clients trying to attack the model, and contribution assessment methods.
An important element in for-profit Federated Learning is the development of
incentive methods to determine the allocation and distribution of rewards for
participants. While numerous methods for allocation have been proposed and
thoroughly explored, distribution frameworks remain relatively understudied. In
this paper, we propose a novel framework which introduces client-specific
tokens as investment vehicles within the FL ecosystem. Our framework aims to
address the limitations of existing incentive schemes by leveraging a
decentralized finance (DeFi) platform and automated market makers (AMMs) to
create a more flexible and scalable reward distribution system for
participants, and a mechanism for third parties to invest in the federation
learning process.

</details>


### [178] [Industrial Energy Disaggregation with Digital Twin-generated Dataset and Efficient Data Augmentation](https://arxiv.org/abs/2506.20525)
*Christian Internò,Andrea Castellani,Sebastian Schmitt,Fabio Stella,Barbara Hammer*

Main category: cs.LG

TL;DR: The paper addresses limitations in Industrial Non-Intrusive Load Monitoring (NILM) by introducing an open-source synthetic dataset (SIDED) and proposing a data augmentation technique (AMDA) to enhance energy disaggregation.


<details>
  <summary>Details</summary>
Motivation: To tackle the scarcity of high-quality datasets and privacy concerns for industrial NILM applications, which hinder model generalization.

Method: The authors developed SIDED using Digital Twin simulations and proposed the AMDA method to computationally scale appliance power contributions based on relative impact for better NILM performance.

Result: AMDA-augmented models showed superior performance in energy disaggregation tasks, achieving improved Normalized Disaggregation Error rates (0.093 compared to 0.451 without augmentation).

Conclusion: The adoption of SIDED and AMDA significantly improves energy disaggregation in NILM tasks by aligning training and test data distributions and enhancing model generalization.

Abstract: Industrial Non-Intrusive Load Monitoring (NILM) is limited by the scarcity of
high-quality datasets and the complex variability of industrial energy
consumption patterns. To address data scarcity and privacy issues, we introduce
the Synthetic Industrial Dataset for Energy Disaggregation (SIDED), an
open-source dataset generated using Digital Twin simulations. SIDED includes
three types of industrial facilities across three different geographic
locations, capturing diverse appliance behaviors, weather conditions, and load
profiles. We also propose the Appliance-Modulated Data Augmentation (AMDA)
method, a computationally efficient technique that enhances NILM model
generalization by intelligently scaling appliance power contributions based on
their relative impact. We show in experiments that NILM models trained with
AMDA-augmented data significantly improve the disaggregation of energy
consumption of complex industrial appliances like combined heat and power
systems. Specifically, in our out-of-sample scenarios, models trained with AMDA
achieved a Normalized Disaggregation Error of 0.093, outperforming models
trained without data augmentation (0.451) and those trained with random data
augmentation (0.290). Data distribution analyses confirm that AMDA effectively
aligns training and test data distributions, enhancing model generalization.

</details>


### [179] [Physics-Informed Machine Learning Regulated by Finite Element Analysis for Simulation Acceleration of Laser Powder Bed Fusion](https://arxiv.org/abs/2506.20537)
*R. Sharma,M. Raissi,Y. B. Guo*

Main category: cs.LG

TL;DR: This paper introduces the FEA-PINN framework for thermal field prediction in LPBF, combining PINN and corrective FEA simulations for high accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Address the high computational cost and accuracy challenges of LPBF thermal field simulations using traditional methods like FEA.

Method: Develop FEA-PINN, combining PINN with corrective FEA simulations during inference to ensure physical consistency and capture dynamic phase changes.

Result: FEA-PINN achieves computational efficiency and equivalent accuracy compared to FEA, validated through benchmark data and LPBF experiments.

Conclusion: FEA-PINN offers a promising solution for efficient and accurate LPBF simulations, overcoming limitations of computation costs in time-dependent scenarios.

Abstract: Efficient simulation of Laser Powder Bed Fusion (LPBF) is crucial for process
prediction due to the lasting issue of high computation cost using traditional
numerical methods such as finite element analysis (FEA). This study presents an
efficient modeling framework termed FEA-Regulated Physics-Informed Neural
Network (FEA-PINN) to accelerate the thermal field prediction in a LPBF process
while maintaining the FEA accuracy. A novel dynamic material updating strategy
is developed to capture the dynamic phase change of powder-liquid-solid in the
PINN model. The PINN model incorporates temperature-dependent material
properties and phase change behavior using the apparent heat capacity method.
While the PINN model demonstrates high accuracy with a small training data and
enables generalization of new process parameters via transfer learning, it
faces the challenge of high computation cost in time-dependent problems due to
the residual accumulation. To overcome this issue, the FEA-PINN framework
integrates corrective FEA simulations during inference to enforce physical
consistency and reduce error drift. A comparative analysis shows that FEA-PINN
achieves equivalent accuracy to FEA while significantly reducing computational
cost. The framework has been validated using the benchmark FEA data and
demonstrated through single-track scanning in LPBF.

</details>


### [180] [Demonstration of effective UCB-based routing in skill-based queues on real-world data](https://arxiv.org/abs/2506.20543)
*Sanne van Kempen,Jaron Sanders,Fiona Sloothaak,Maarten G. Wolf*

Main category: cs.LG

TL;DR: The paper investigates using reinforcement learning for optimizing customer routing in skill-based queueing systems, showing efficient learning capabilities and introducing methods for improved real-world applications.


<details>
  <summary>Details</summary>
Motivation: The need to optimize customer routing and resource allocation in systems such as data centers and cloud networks, while addressing practical concerns like server load fairness and minimizing delays.

Method: The study conducts experiments on a real-world data set to analyze the implementation of a reinforcement learning algorithm, introduces a heuristic routing rule to improve delay handling, and explores sensitivity to tuning parameters.

Result: The reinforcement learning algorithm adapts well to changing conditions, outperforms static policies, can balance multiple objectives, and offers insights into handling estimation errors for real-world applications.

Conclusion: Reinforcement learning algorithms can significantly enhance operational efficiency in queueing systems, offering robust adaptability, multi-objective optimization, and tools for minimizing delays while addressing parameter and estimation challenges.

Abstract: This paper is about optimally controlling skill-based queueing systems such
as data centers, cloud computing networks, and service systems. By means of a
case study using a real-world data set, we investigate the practical
implementation of a recently developed reinforcement learning algorithm for
optimal customer routing. Our experiments show that the algorithm efficiently
learns and adapts to changing environments and outperforms static benchmark
policies, indicating its potential for live implementation. We also augment the
real-world applicability of this algorithm by introducing a new heuristic
routing rule to reduce delays. Moreover, we show that the algorithm can
optimize for multiple objectives: next to payoff maximization, secondary
objectives such as server load fairness and customer waiting time reduction can
be incorporated. Tuning parameters are used for balancing inherent performance
trade--offs. Lastly, we investigate the sensitivity to estimation errors and
parameter tuning, providing valuable insights for implementing adaptive routing
algorithms in complex real-world queueing systems.

</details>


### [181] [Benchmarking Unsupervised Strategies for Anomaly Detection in Multivariate Time Series](https://arxiv.org/abs/2506.20574)
*Laura Boggia,Rafael Teixeira de Lima,Bogdan Malaescu*

Main category: cs.LG

TL;DR: This paper explores transformer-based methodologies for anomaly detection in multivariate time series data, focusing on the iTransformer architecture.


<details>
  <summary>Details</summary>
Motivation: Anomaly detection in multivariate time series is a challenging yet vital task due to unknown anomalies and complex interdependencies among dimensions.

Method: The paper assesses iTransformer architecture through parameter exploration, label extraction methods, evaluation metrics analysis, impact of anomalous training data, and comparative analyses across datasets.

Result: Insights are drawn on how key parameters, effective loss functions, and transformer models impact performance and robustness in detecting anomalies.

Conclusion: Transformer-based models, particularly iTransformer, show potential in advancing multivariate time series anomaly detection with improved evaluation methods and architectures.

Abstract: Anomaly detection in multivariate time series is an important problem across
various fields such as healthcare, financial services, manufacturing or physics
detector monitoring. Accurately identifying when unexpected errors or faults
occur is essential, yet challenging, due to the unknown nature of anomalies and
the complex interdependencies between time series dimensions. In this paper, we
investigate transformer-based approaches for time series anomaly detection,
focusing on the recently proposed iTransformer architecture. Our contributions
are fourfold: (i) we explore the application of the iTransformer to time series
anomaly detection, and analyse the influence of key parameters such as window
size, step size, and model dimensions on performance; (ii) we examine methods
for extracting anomaly labels from multidimensional anomaly scores and discuss
appropriate evaluation metrics for such labels; (iii) we study the impact of
anomalous data present during training and assess the effectiveness of
alternative loss functions in mitigating their influence; and (iv) we present a
comprehensive comparison of several transformer-based models across a diverse
set of datasets for time series anomaly detection.

</details>


### [182] [Exploring Graph-Transformer Out-of-Distribution Generalization Abilities](https://arxiv.org/abs/2506.20575)
*Itay Niv,Neta Rabin*

Main category: cs.LG

TL;DR: This paper investigates out-of-distribution (OOD) generalization for graph neural networks, focusing on graph-transformer backbones and comparing them with traditional MPNNs under distribution shifts.


<details>
  <summary>Details</summary>
Motivation: Exploring how graph neural networks perform under distribution shifts, as current methods often assume training and testing data share the same distribution, which is unrealistic in real-world applications.

Method: Conducted systematic evaluations of graph-transformer (GT), hybrid GT-MPNN, and MPNN backbones under OOD scenarios; adapted domain generalization (DG) algorithms for GTs; and introduced a post-training analysis approach for domain alignment and class separation.

Result: Found that GT and hybrid GT-MPNN architectures showed better OOD generalization compared to MPNNs, even without specialized DG algorithms. The proposed post-training analysis provided meaningful insights into structural differences.

Conclusion: Graph-transformers demonstrate stronger generalization abilities for graph learning in OOD settings, making them robust candidates for real-world applications. The introduced post-training analysis is a promising tool for studying generalization behaviors.

Abstract: Deep learning on graphs has shown remarkable success across numerous
applications, including social networks, bio-physics, traffic networks, and
recommendation systems. Regardless of their successes, current methods
frequently depend on the assumption that training and testing data share the
same distribution, a condition rarely met in real-world scenarios. While
graph-transformer (GT) backbones have recently outperformed traditional
message-passing neural networks (MPNNs) in multiple in-distribution (ID)
benchmarks, their effectiveness under distribution shifts remains largely
unexplored.
  In this work, we address the challenge of out-of-distribution (OOD)
generalization for graph neural networks, with a special focus on the impact of
backbone architecture. We systematically evaluate GT and hybrid backbones in
OOD settings and compare them to MPNNs. To do so, we adapt several leading
domain generalization (DG) algorithms to work with GTs and assess their
performance on a benchmark designed to test a variety of distribution shifts.
Our results reveal that GT and hybrid GT-MPNN backbones consistently
demonstrate stronger generalization ability compared to MPNNs, even without
specialized DG algorithms.
  Additionally, we propose a novel post-training analysis approach that
compares the clustering structure of the entire ID and OOD test datasets,
specifically examining domain alignment and class separation. Demonstrating its
model-agnostic design, this approach not only provided meaningful insights into
GT and MPNN backbones. It also shows promise for broader applicability to DG
problems beyond graph learning, offering a deeper perspective on generalization
abilities that goes beyond standard accuracy metrics. Together, our findings
highlight the promise of graph-transformers for robust, real-world graph
learning and set a new direction for future research in OOD generalization.

</details>


### [183] [The kernel of graph indices for vector search](https://arxiv.org/abs/2506.20584)
*Mariano Tepper,Ted Willke*

Main category: cs.LG

TL;DR: The paper introduces Support Vector Graph (SVG), a machine learning-based graph index for vector search, which works in both metric and non-metric spaces, offering formal navigability guarantees.


<details>
  <summary>Details</summary>
Motivation: Existing graph indices for vector search are mostly limited to Euclidean space due to computational geometry principles. This paper aims to overcome this limitation and explore solutions for both metric and non-metric spaces.

Method: The authors leverage kernel methods to establish SVG graph connectivity and propose SVG-L0 with an $
0$ sparsity constraint for bounded out-degree graphs. They also reinterpret existing indices like HNSW and DiskANN within this framework.

Result: SVG offers formal navigability guarantees for metric and non-metric spaces, reduces reliance on heuristics, introduces self-tuning for computational efficiency, and provides bounded out-degree graphs.

Conclusion: Machine learning techniques can successfully extend graph indices to non-metric spaces, with practical benefits, formal guarantees, and computational efficiency improvements over traditional methods.

Abstract: The most popular graph indices for vector search use principles from
computational geometry to build the graph. Hence, their formal graph
navigability guarantees are only valid in Euclidean space. In this work, we
show that machine learning can be used to build graph indices for vector search
in metric and non-metric vector spaces (e.g., for inner product similarity).
From this novel perspective, we introduce the Support Vector Graph (SVG), a new
type of graph index that leverages kernel methods to establish the graph
connectivity and that comes with formal navigability guarantees valid in metric
and non-metric vector spaces. In addition, we interpret the most popular graph
indices, including HNSW and DiskANN, as particular specializations of SVG and
show that new indices can be derived from the principles behind this
specialization. Finally, we propose SVG-L0 that incorporates an $\ell_0$
sparsity constraint into the SVG kernel method to build graphs with a bounded
out-degree. This yields a principled way of implementing this practical
requirement, in contrast to the traditional heuristic of simply truncating the
out edges of each node. Additionally, we show that SVG-L0 has a self-tuning
property that avoids the heuristic of using a set of candidates to find the
out-edges of each node and that keeps its computational complexity in check.

</details>


### [184] [H-FEX: A Symbolic Learning Method for Hamiltonian Systems](https://arxiv.org/abs/2506.20607)
*Jasen Lai,Senwei Liang,Chunmei Wang*

Main category: cs.LG

TL;DR: This paper proposes H-FEX, a symbolic learning method to reconstruct complex Hamiltonian systems. It improves the ability to preserve energy conservation and handle intricate dynamics using novel interaction nodes.


<details>
  <summary>Details</summary>
Motivation: Existing data-driven methods like symbolic regression and neural networks struggle to accurately identify Hamiltonian systems' governing equations, particularly while conserving energy.

Method: H-FEX introduces innovative interaction nodes into symbolic learning, allowing it to better capture intricate Hamiltonian dynamics.

Result: H-FEX recovers Hamiltonian functions that preserve energy over extended periods and accurately describe even highly stiff dynamical systems.

Conclusion: H-FEX is a promising framework for obtaining closed-form expressions of complex Hamiltonian systems, offering improved accuracy and energy conservation.

Abstract: Hamiltonian systems describe a broad class of dynamical systems governed by
Hamiltonian functions, which encode the total energy and dictate the evolution
of the system. Data-driven approaches, such as symbolic regression and neural
network-based methods, provide a means to learn the governing equations of
dynamical systems directly from observational data of Hamiltonian systems.
However, these methods often struggle to accurately capture complex Hamiltonian
functions while preserving energy conservation. To overcome this limitation, we
propose the Finite Expression Method for learning Hamiltonian Systems (H-FEX),
a symbolic learning method that introduces novel interaction nodes designed to
capture intricate interaction terms effectively. Our experiments, including
those on highly stiff dynamical systems, demonstrate that H-FEX can recover
Hamiltonian functions of complex systems that accurately capture system
dynamics and preserve energy over long time horizons. These findings highlight
the potential of H-FEX as a powerful framework for discovering closed-form
expressions of complex dynamical systems.

</details>


### [185] [Efficient Federated Learning with Encrypted Data Sharing for Data-Heterogeneous Edge Devices](https://arxiv.org/abs/2506.20644)
*Hangyu Li,Hongyue Wu,Guodong Fan,Zhen Zhang,Shizhan Chen,Zhiyong Feng*

Main category: cs.LG

TL;DR: This paper introduces FedEDS, a new federated learning scheme enhancing model performance by sharing encrypted data among edge devices.


<details>
  <summary>Details</summary>
Motivation: Current federated learning models neglect network topology and data heterogeneity, causing latency issues and suboptimal model performance.

Method: FedEDS leverages client models and their stochastic layers to train a data encryptor, enabling secure encrypted data sharing and local model adjustments.

Result: Experiments prove FedEDS effectively accelerates federated learning convergence and addresses data heterogeneity challenges.

Conclusion: FedEDS improves the efficiency and accuracy of federated learning on edge devices, making it ideal for applications needing rapid results.

Abstract: As privacy protection gains increasing importance, more models are being
trained on edge devices and subsequently merged into the central server through
Federated Learning (FL). However, current research overlooks the impact of
network topology, physical distance, and data heterogeneity on edge devices,
leading to issues such as increased latency and degraded model performance. To
address these issues, we propose a new federated learning scheme on edge
devices that called Federated Learning with Encrypted Data Sharing(FedEDS).
FedEDS uses the client model and the model's stochastic layer to train the data
encryptor. The data encryptor generates encrypted data and shares it with other
clients. The client uses the corresponding client's stochastic layer and
encrypted data to train and adjust the local model. FedEDS uses the client's
local private data and encrypted shared data from other clients to train the
model. This approach accelerates the convergence speed of federated learning
training and mitigates the negative impact of data heterogeneity, making it
suitable for application services deployed on edge devices requiring rapid
convergence. Experiments results show the efficacy of FedEDS in promoting model
performance.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [186] [Higher-Order Neuromorphic Ising Machines -- Autoencoders and Fowler-Nordheim Annealers are all you need for Scalability](https://arxiv.org/abs/2506.19964)
*Faiek Ahsan,Saptarshi Maiti,Zihao Chen,Jakob Kaiser,Ankita Nandi,Madhuvanthi Srivatsav,Johannes Schemmel,Andreas G. Andreou,Jason Eshraghian,Chetan Singh Thakur,Shantanu Chakrabartty*

Main category: cs.NE

TL;DR: The paper introduces a higher-order neuromorphic Ising machine utilizing an asynchronous autoencoder architecture, demonstrating better scalability, solution quality, and time-to-solution over conventional second-order Ising machines.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the scalability challenges in traditional Ising machine architectures while improving the quality and reliability of solutions for combinatorial optimization problems.

Method: The proposed method utilizes an asynchronous autoencoder to process higher-order Ising clauses directly, combined with Fowler-Nordheim quantum mechanical tunneling-based annealing dynamics.

Result: The higher-order neuromorphic Ising machine outperformed second-order models in solution quality and time-to-solution for benchmark problems like MAX-CUT and MAX-SAT, particularly leveraging sparsity techniques and hardware co-design with FPGA.

Conclusion: Autoencoders and Fowler-Nordheim annealers are demonstrated as effective for scalable and reliable higher-order neuromorphic Ising machines, with potential for further improvements via hardware co-design.

Abstract: We report a higher-order neuromorphic Ising machine that exhibits superior
scalability compared to architectures based on quadratization, while also
achieving state-of-the-art quality and reliability in solutions with
competitive time-to-solution metrics. At the core of the proposed machine is an
asynchronous autoencoder architecture that captures higher-order interactions
by directly manipulating Ising clauses instead of Ising spins, thereby
maintaining resource complexity independent of interaction order. Asymptotic
convergence to the Ising ground state is ensured by sampling the autoencoder
latent space defined by the spins, based on the annealing dynamics of the
Fowler-Nordheim quantum mechanical tunneling. To demonstrate the advantages of
the proposed higher-order neuromorphic Ising machine, we systematically solved
benchmark combinatorial optimization problems such as MAX-CUT and MAX-SAT,
comparing the results to those obtained using a second-order Ising machine
employing the same annealing process. Our findings indicate that the proposed
architecture consistently provides higher quality solutions in shorter time
frames compared to the second-order model across multiple runs. Additionally,
we show that the techniques based on the sparsity of the interconnection
matrix, such as graph coloring, can be effectively applied to higher-order
neuromorphic Ising machines, enhancing the solution quality and the
time-to-solution. The time-to-solution can be further improved through hardware
co-design, as demonstrated in this paper using a field-programmable gate array
(FPGA). The results presented in this paper provide further evidence that
autoencoders and Fowler-Nordheim annealers are sufficient to achieve
reliability and scaling of any-order neuromorphic Ising machines.

</details>


### [187] [Surrogate-Assisted Evolution for Efficient Multi-branch Connection Design in Deep Neural Networks](https://arxiv.org/abs/2506.20469)
*Fergal Stapleton,Daniel García Núñez,Yanan Sun,Edgar Galván*

Main category: cs.NE

TL;DR: The paper introduces NeuroLGP-MB, a method leveraging Linear Genetic Programming and surrogate-assisted Evolutionary Algorithms to efficiently design multi-branch Deep Neural Networks.


<details>
  <summary>Details</summary>
Motivation: Address computational expense and complexity in training and designing multi-branch DNN architectures.

Method: Uses Linear Genetic Programming to encode multi-branch connections, and deploys scaled surrogate-assisted Evolutionary Algorithms, enhancing efficiency through semantic-based approaches.

Result: Successfully scaled surrogate-assisted EA, introduced an advanced surrogate model, and demonstrated superior performance compared to existing methods.

Conclusion: The proposed NeuroLGP-MB framework offers a more efficient and effective approach to designing complex DNN architectures.

Abstract: State-of-the-art Deep Neural Networks (DNNs) often incorporate multi-branch
connections, enabling multi-scale feature extraction and enhancing the capture
of diverse features. This design improves network capacity and generalisation
to unseen data. However, training such DNNs can be computationally expensive.
The challenge is further exacerbated by the complexity of identifying optimal
network architectures. To address this, we leverage Evolutionary Algorithms
(EAs) to automatically discover high-performing architectures, a process
commonly known as neuroevolution. We introduce a novel approach based on Linear
Genetic Programming (LGP) to encode multi-branch (MB) connections within DNNs,
referred to as NeuroLGP-MB. To efficiently design the DNNs, we use
surrogate-assisted EAs. While their application in simple artificial neural
networks has been influential, we scale their use from dozens or hundreds of
sample points to thousands, aligning with the demands of complex DNNs by
incorporating a semantic-based approach in our surrogate-assisted EA.
Furthermore, we introduce a more advanced surrogate model that outperforms
baseline, computationally expensive, and simpler surrogate models.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [188] [Evolutionary Gait Reconfiguration in Damaged Legged Robots](https://arxiv.org/abs/2506.19968)
*Sahand Farghdani,Robin Chhabra*

Main category: cs.RO

TL;DR: This paper presents a rapid, training-free algorithm to restore locomotion in legged robots with impaired or damaged legs.


<details>
  <summary>Details</summary>
Motivation: To address challenges faced by multi-legged robots when physical damage impairs their ability to perform tasks and compromises mission success.

Method: The paper introduces a differential evolution algorithm that first stabilizes the robot's locomotion and then optimally reconfigures leg gaits to improve movement by minimizing rotation and drift.

Result: The algorithm successfully restored locomotion in a highly complex hexapod robot within one hour, showing efficiency and robustness.

Conclusion: The proposed algorithm demonstrates a quick and effective solution for damage recovery in legged robots, enabling sustained mission capabilities even with physical impairments.

Abstract: Multi-legged robots deployed in complex missions are susceptible to physical
damage in their legs, impairing task performance and potentially compromising
mission success. This letter presents a rapid, training-free damage recovery
algorithm for legged robots subject to partial or complete loss of functional
legs. The proposed method first stabilizes locomotion by generating a new gait
sequence and subsequently optimally reconfigures leg gaits via a developed
differential evolution algorithm to maximize forward progression while
minimizing body rotation and lateral drift. The algorithm successfully restores
locomotion in a 24-degree-of-freedom hexapod within one hour, demonstrating
both high efficiency and robustness to structural damage.

</details>


### [189] [Robust Embodied Self-Identification of Morphology in Damaged Multi-Legged Robots](https://arxiv.org/abs/2506.19984)
*Sahand Farghdani,Mili Patel,Robin Chhabra*

Main category: cs.RO

TL;DR: The paper introduces a self-modeling and damage identification algorithm that allows multi-legged robots to adapt to leg damage using data from a low-cost IMU.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenge of performance degradation in multi-legged robots due to leg damage during complex missions.

Method: A novel FFT-based filter is developed to compare body orientation between the physical robot and its model, enabling damage detection and model updates.

Result: The method successfully identifies damaged legs and integrates updated models into the robot's control system, validated experimentally on uneven terrain.

Conclusion: The presented solution enhances autonomous robustness and computational efficiency for multi-legged robots in challenging environments.

Abstract: Multi-legged robots (MLRs) are vulnerable to leg damage during complex
missions, which can impair their performance. This paper presents a
self-modeling and damage identification algorithm that enables autonomous
adaptation to partial or complete leg loss using only data from a low-cost IMU.
A novel FFT-based filter is introduced to address time-inconsistent signals,
improving damage detection by comparing body orientation between the robot and
its model. The proposed method identifies damaged legs and updates the robot's
model for integration into its control system. Experiments on uneven terrain
validate its robustness and computational efficiency.

</details>


### [190] [Hierarchical Reinforcement Learning and Value Optimization for Challenging Quadruped Locomotion](https://arxiv.org/abs/2506.20036)
*Jeremiah Coholich,Muhammad Ali Murtaza,Seth Hutchinson,Zsolt Kira*

Main category: cs.RO

TL;DR: This paper proposes a hierarchical framework for quadruped locomotion, using a high-level policy for goal-setting and a trained low-level policy for step navigation.


<details>
  <summary>Details</summary>
Motivation: The authors aim to improve quadruped locomotion over challenging terrains, addressing limitations of end-to-end RL methods, such as lower performance on unseen terrains and higher collision rates.

Method: The proposed framework utilizes a two-layer structure: the low-level policy (LLP) is trained using an actor-critic algorithm for footstep goals, while the high-level policy (HLP) operates using an optimization process over the learned value function of LLP, without requiring additional training.

Result: The hierarchical framework demonstrated improved performance compared to end-to-end RL, with higher rewards and fewer collisions on both known and unseen challenging terrains.

Conclusion: The proposed two-layer framework shows that hierarchical RL improves quadruped locomotion, providing generalizable and efficient terrain navigation.

Abstract: We propose a novel hierarchical reinforcement learning framework for
quadruped locomotion over challenging terrain. Our approach incorporates a
two-layer hierarchy in which a high-level policy (HLP) selects optimal goals
for a low-level policy (LLP). The LLP is trained using an on-policy
actor-critic RL algorithm and is given footstep placements as goals. We propose
an HLP that does not require any additional training or environment samples and
instead operates via an online optimization process over the learned value
function of the LLP. We demonstrate the benefits of this framework by comparing
it with an end-to-end reinforcement learning (RL) approach. We observe
improvements in its ability to achieve higher rewards with fewer collisions
across an array of different terrains, including terrains more difficult than
any encountered during training.

</details>


### [191] [Consensus-Driven Uncertainty for Robotic Grasping based on RGB Perception](https://arxiv.org/abs/2506.20045)
*Eric C. Joyce,Qianwen Zhao,Nathaniel Burgdorfer,Long Wang,Philippos Mordohai*

Main category: cs.RO

TL;DR: A method is proposed for training deep networks to predict grasp success in robotic tasks, combining object pose estimates and uncertainty to avoid failures.


<details>
  <summary>Details</summary>
Motivation: Existing deep object pose estimators are overconfident, leading to task failures. Bridging the gap between pose estimation, uncertainty prediction, and practical grasping can improve robotic performance.

Method: The approach uses lightweight deep networks trained on pose estimation and simulated grasping data to predict if a grasp will succeed. The training set includes diverse objects to improve generalization.

Result: Despite variability across different objects, the networks successfully benefited from joint training on all objects, improving prediction of grasp outcomes.

Conclusion: The study demonstrates that incorporating diverse object examples and uncertainty-aware pose estimations can effectively improve robotic grasping accuracy.

Abstract: Deep object pose estimators are notoriously overconfident. A grasping agent
that both estimates the 6-DoF pose of a target object and predicts the
uncertainty of its own estimate could avoid task failure by choosing not to act
under high uncertainty. Even though object pose estimation improves and
uncertainty quantification research continues to make strides, few studies have
connected them to the downstream task of robotic grasping. We propose a method
for training lightweight, deep networks to predict whether a grasp guided by an
image-based pose estimate will succeed before that grasp is attempted. We
generate training data for our networks via object pose estimation on real
images and simulated grasping. We also find that, despite high object
variability in grasping trials, networks benefit from training on all objects
jointly, suggesting that a diverse variety of objects can nevertheless
contribute to the same goal.

</details>


### [192] [Robust Robotic Exploration and Mapping Using Generative Occupancy Map Synthesis](https://arxiv.org/abs/2506.20049)
*Lorin Achey,Alec Reed,Brendan Crowe,Bradley Hayes,Christoffer Heckman*

Main category: cs.RO

TL;DR: This paper introduces SceneSense, a diffusion model enhancing robotic exploration through real-time probabilistic fusion of 3D occupancy maps, validated via real-world experiments for improved map quality and exploration efficiency.


<details>
  <summary>Details</summary>
Motivation: The paper aims to enhance robotic exploration efficiency and accuracy by addressing limitations in constructing reliable occupancy maps from partial sensor data.

Method: SceneSense is a generative model predicting 3D occupancy maps from partial observations and fusing these predictions probabilistically in real-time for quadruped robot applications.

Result: Real-world experiments showed significant improvements in map quality (up to 75.59% FID improvement at range) and traversal robustness when using SceneSense-enhanced maps.

Conclusion: SceneSense improves mapping and exploration quality, offering a practical and robust solution for real-time robotic navigation across diverse environments.

Abstract: We present a novel approach for enhancing robotic exploration by using
generative occupancy mapping. We introduce SceneSense, a diffusion model
designed and trained for predicting 3D occupancy maps given partial
observations. Our proposed approach probabilistically fuses these predictions
into a running occupancy map in real-time, resulting in significant
improvements in map quality and traversability. We implement SceneSense onboard
a quadruped robot and validate its performance with real-world experiments to
demonstrate the effectiveness of the model. In these experiments, we show that
occupancy maps enhanced with SceneSense predictions better represent our fully
observed ground truth data (24.44% FID improvement around the robot and 75.59%
improvement at range). We additionally show that integrating
SceneSense-enhanced maps into our robotic exploration stack as a "drop-in" map
improvement, utilizing an existing off-the-shelf planner, results in
improvements in robustness and traversability time. Finally we show results of
full exploration evaluations with our proposed system in two dissimilar
environments and find that locally enhanced maps provide more consistent
exploration results than maps constructed only from direct sensor measurements.

</details>


### [193] [PSALM-V: Automating Symbolic Planning in Interactive Visual Environments with Large Language Models](https://arxiv.org/abs/2506.20097)
*Wang Bill Zhu,Miaosen Chai,Ishika Singh,Robin Jia,Jesse Thomason*

Main category: cs.RO

TL;DR: The paper introduces PSALM-V, an autonomous neuro-symbolic learning system for inducing symbolic action semantics in visual environments without expert intervention.


<details>
  <summary>Details</summary>
Motivation: To address limitations in current neuro-symbolic systems that depend on pre-defined problem files, ideal conditions, or explicit error messages, and to enable effective planning in partially observed, real-world contexts.

Method: PSALM-V autonomously infers PDDL problem files and symbolic action semantics by analyzing execution outcomes and synthesizing error explanations iteratively, using LLMs for initial heuristics and belief refinement through interaction.

Result: PSALM-V increased planning success rate from 37% to 74% in ALFRED, improved step efficiency, and performed domain induction in 2D games and robot-based tasks despite operational challenges.

Conclusion: PSALM-V showcases a significant step towards autonomous neuro-symbolic reasoning in complex, partially observable environments, paving the way for enhanced robotic and multi-agent planning capabilities.

Abstract: We propose PSALM-V, the first autonomous neuro-symbolic learning system able
to induce symbolic action semantics (i.e., pre- and post-conditions) in visual
environments through interaction. PSALM-V bootstraps reliable symbolic planning
without expert action definitions, using LLMs to generate heuristic plans and
candidate symbolic semantics. Previous work has explored using large language
models to generate action semantics for Planning Domain Definition Language
(PDDL)-based symbolic planners. However, these approaches have primarily
focused on text-based domains or relied on unrealistic assumptions, such as
access to a predefined problem file, full observability, or explicit error
messages. By contrast, PSALM-V dynamically infers PDDL problem files and domain
action semantics by analyzing execution outcomes and synthesizing possible
error explanations. The system iteratively generates and executes plans while
maintaining a tree-structured belief over possible action semantics for each
action, iteratively refining these beliefs until a goal state is reached.
Simulated experiments of task completion in ALFRED demonstrate that PSALM-V
increases the plan success rate from 37% (Claude-3.7) to 74% in partially
observed setups. Results on two 2D game environments, RTFM and Overcooked-AI,
show that PSALM-V improves step efficiency and succeeds in domain induction in
multi-agent settings. PSALM-V correctly induces PDDL pre- and post-conditions
for real-world robot BlocksWorld tasks, despite low-level manipulation failures
from the robot.

</details>


### [194] [Personalized Mental State Evaluation in Human-Robot Interaction using Federated Learning](https://arxiv.org/abs/2506.20212)
*Andrea Bussolan,Oliver Avram,Andrea Pignata,Gianvito Urgese,Stefano Baraldo,Anna Valente*

Main category: cs.RO

TL;DR: The paper proposes a Federated Learning (FL)-based framework that uses physiological signals to evaluate stress for adaptive Human-Robot Collaboration (HRC), ensuring privacy and improving collaboration in Industry 5.0 settings.


<details>
  <summary>Details</summary>
Motivation: To address the need for stress-aware human-robot interaction in Industry 5.0 while preserving user privacy through personalized and adaptive models.

Method: The framework uses FL to train a multimodal model on-device with physiological signals (EEG, ECG, EDA, EMG, respiration) to predict stress levels. This enables privacy-preserving, distributed training and real-time robot adaptation.

Result: The FL-based model achieves comparable stress prediction accuracy to centralized models, with enhanced personalization and data privacy.

Conclusion: The framework supports adaptive robotics that promote worker well-being and efficient human-robot collaboration in industrial environments while ensuring privacy.

Abstract: With the advent of Industry 5.0, manufacturers are increasingly prioritizing
worker well-being alongside mass customization. Stress-aware Human-Robot
Collaboration (HRC) plays a crucial role in this paradigm, where robots must
adapt their behavior to human mental states to improve collaboration fluency
and safety. This paper presents a novel framework that integrates Federated
Learning (FL) to enable personalized mental state evaluation while preserving
user privacy. By leveraging physiological signals, including EEG, ECG, EDA,
EMG, and respiration, a multimodal model predicts an operator's stress level,
facilitating real-time robot adaptation. The FL-based approach allows
distributed on-device training, ensuring data confidentiality while improving
model generalization and individual customization. Results demonstrate that the
deployment of an FL approach results in a global model with performance in
stress prediction accuracy comparable to a centralized training approach.
Moreover, FL allows for enhancing personalization, thereby optimizing
human-robot interaction in industrial settings, while preserving data privacy.
The proposed framework advances privacy-preserving, adaptive robotics to
enhance workforce well-being in smart manufacturing.

</details>


### [195] [Generating and Customizing Robotic Arm Trajectories using Neural Networks](https://arxiv.org/abs/2506.20259)
*Andrej Lúčny,Matilde Antonj,Carlo Mazzola,Hana Hornáčková,Igor Farkaš*

Main category: cs.RO

TL;DR: The paper presents a neural network-based method to generate and adapt robotic arm trajectories with precision and repeatability, tested on the NICO robot in cognitive robotics applications.


<details>
  <summary>Details</summary>
Motivation: Improve robotic arm interaction with humans by enhancing trajectory precision and repeatability.

Method: Two neural networks are employed: one for forward kinematics and another trained on a custom dataset of robotic arm poses to compute angular velocities and customize trajectories.

Result: The proposed approach enables the NICO robot to perform accurate linear movements, pointing to specific locations and ensuring predictability during human interaction.

Conclusion: The method successfully creates and adapts robotic trajectories, showcasing potential for varied applications requiring robotic precision.

Abstract: We introduce a neural network approach for generating and customizing the
trajectory of a robotic arm, that guarantees precision and repeatability. To
highlight the potential of this novel method, we describe the design and
implementation of the technique and show its application in an experimental
setting of cognitive robotics. In this scenario, the NICO robot was
characterized by the ability to point to specific points in space with precise
linear movements, increasing the predictability of the robotic action during
its interaction with humans. To achieve this goal, the neural network computes
the forward kinematics of the robot arm. By integrating it with a generator of
joint angles, another neural network was developed and trained on an artificial
dataset created from suitable start and end poses of the robotic arm. Through
the computation of angular velocities, the robot was characterized by its
ability to perform the movement, and the quality of its action was evaluated in
terms of shape and accuracy. Thanks to its broad applicability, our approach
successfully generates precise trajectories that could be customized in their
shape and adapted to different settings.

</details>


### [196] [Why Robots Are Bad at Detecting Their Mistakes: Limitations of Miscommunication Detection in Human-Robot Dialogue](https://arxiv.org/abs/2506.20268)
*Ruben Janssens,Jens De Bock,Sofie Labat,Eva Verhelst,Veronique Hoste,Tony Belpaeme*

Main category: cs.RO

TL;DR: The study investigates the challenges in detecting miscommunication in human-robot conversation using computer vision models, with findings showing comparable deficiencies in both humans and AI systems.


<details>
  <summary>Details</summary>
Motivation: To address the critical need for detecting miscommunication in human-robot interactions for maintaining user trust and engagement.

Method: Machine learning models were tested using a multi-modal dataset containing artificially introduced conversational failures and user feedback after robot dialogue turns.

Result: Detection performance of the models was low, barely exceeding random chance, similar to human raters who identified only about half of the miscommunications.

Conclusion: Both humans and AI struggle in detecting robot miscommunications, emphasizing the need to deliberately design feedback mechanisms in human-robot interactions.

Abstract: Detecting miscommunication in human-robot interaction is a critical function
for maintaining user engagement and trust. While humans effortlessly detect
communication errors in conversations through both verbal and non-verbal cues,
robots face significant challenges in interpreting non-verbal feedback, despite
advances in computer vision for recognizing affective expressions. This
research evaluates the effectiveness of machine learning models in detecting
miscommunications in robot dialogue. Using a multi-modal dataset of 240
human-robot conversations, where four distinct types of conversational failures
were systematically introduced, we assess the performance of state-of-the-art
computer vision models. After each conversational turn, users provided feedback
on whether they perceived an error, enabling an analysis of the models' ability
to accurately detect robot mistakes. Despite using state-of-the-art models, the
performance barely exceeds random chance in identifying miscommunication, while
on a dataset with more expressive emotional content, they successfully
identified confused states. To explore the underlying cause, we asked human
raters to do the same. They could also only identify around half of the induced
miscommunications, similarly to our model. These results uncover a fundamental
limitation in identifying robot miscommunications in dialogue: even when users
perceive the induced miscommunication as such, they often do not communicate
this to their robotic conversation partner. This knowledge can shape
expectations of the performance of computer vision models and can help
researchers to design better human-robot conversations by deliberately
eliciting feedback where needed.

</details>


### [197] [Real-Time Obstacle Avoidance Algorithms for Unmanned Aerial and Ground Vehicles](https://arxiv.org/abs/2506.20311)
*Jingwen Wei*

Main category: cs.RO

TL;DR: The paper researches UAV-based navigation for disaster rescue in 3D environments, focusing on collision-free and coordinated maneuvering.


<details>
  <summary>Details</summary>
Motivation: To address the underexplored application of UAVs in search and rescue missions, specifically in navigating complex environments like forest fires.

Method: The study includes a stepwise approach: 2D navigation for ground robots, 3D reactive navigation for UAVs, and unified UAV-UGV control for coordinated rescue.

Result: Developed algorithms for secure UAV navigation in complex situations and demonstrated their effectiveness through simulations. Proposed integrated UAV-UGV rescue strategies.

Conclusion: The research advances UAV navigation and coordination techniques for critical scenarios, improving disaster rescue efficiency and safety.

Abstract: The growing use of mobile robots in sectors such as automotive, agriculture,
and rescue operations reflects progress in robotics and autonomy. In unmanned
aerial vehicles (UAVs), most research emphasizes visual SLAM, sensor fusion,
and path planning. However, applying UAVs to search and rescue missions in
disaster zones remains underexplored, especially for autonomous navigation.
  This report develops methods for real-time and secure UAV maneuvering in
complex 3D environments, crucial during forest fires. Building upon past
research, it focuses on designing navigation algorithms for unfamiliar and
hazardous environments, aiming to improve rescue efficiency and safety through
UAV-based early warning and rapid response.
  The work unfolds in phases. First, a 2D fusion navigation strategy is
explored, initially for mobile robots, enabling safe movement in dynamic
settings. This sets the stage for advanced features such as adaptive obstacle
handling and decision-making enhancements. Next, a novel 3D reactive navigation
strategy is introduced for collision-free movement in forest fire simulations,
addressing the unique challenges of UAV operations in such scenarios.
  Finally, the report proposes a unified control approach that integrates UAVs
and unmanned ground vehicles (UGVs) for coordinated rescue missions in forest
environments. Each phase presents challenges, proposes control models, and
validates them with mathematical and simulation-based evidence. The study
offers practical value and academic insights for improving the role of UAVs in
natural disaster rescue operations.

</details>


### [198] [Near Time-Optimal Hybrid Motion Planning for Timber Cranes](https://arxiv.org/abs/2506.20314)
*Marc-Philip Ecker,Bernhard Bischof,Minh Nhat Vu,Christoph Fröhlich,Tobias Glück,Wolfgang Kemmetmüller*

Main category: cs.RO

TL;DR: This paper introduces an enhanced hybrid motion planning approach for hydraulically actuated timber cranes, incorporating constraints unique to their operation, like passive joints and collision avoidance.


<details>
  <summary>Details</summary>
Motivation: To address challenges in automating large-scale manipulators like timber cranes, which include hydraulic actuation constraints and passive joints that are often ignored by current methods.

Method: Enhance the via-point-based stochastic trajectory optimization (VP-STO) algorithm with pump flow rate constraints and a novel collision cost formulation, combined with gradient-based local planning for collision avoidance and sway damping.

Result: The enhanced VP-STO method demonstrated effectiveness as a global planner by comparison with the informed RRT* algorithm using time-optimal path parameterization. A hybrid motion planning approach was successfully combined with a local planner for improved robustness.

Conclusion: The proposed hybrid motion planning method ensures time-optimal, collision-free planning for hydraulically actuated timber cranes while systematically addressing passive joint dynamics and hydraulic constraints.

Abstract: Efficient, collision-free motion planning is essential for automating
large-scale manipulators like timber cranes. They come with unique challenges
such as hydraulic actuation constraints and passive joints-factors that are
seldom addressed by current motion planning methods. This paper introduces a
novel approach for time-optimal, collision-free hybrid motion planning for a
hydraulically actuated timber crane with passive joints. We enhance the
via-point-based stochastic trajectory optimization (VP-STO) algorithm to
include pump flow rate constraints and develop a novel collision cost
formulation to improve robustness. The effectiveness of the enhanced VP-STO as
an optimal single-query global planner is validated by comparison with an
informed RRT* algorithm using a time-optimal path parameterization (TOPP). The
overall hybrid motion planning is formed by combination with a gradient-based
local planner that is designed to follow the global planner's reference and to
systematically consider the passive joint dynamics for both collision avoidance
and sway damping.

</details>


### [199] [Building Forest Inventories with Autonomous Legged Robots -- System, Lessons, and Challenges Ahead](https://arxiv.org/abs/2506.20315)
*Matías Mattamala,Nived Chebrolu,Jonas Frey,Leonard Freißmuth,Haedam Oh,Benoit Casseau,Marco Hutter,Maurice Fallon*

Main category: cs.RO

TL;DR: This paper explores a prototype system for using legged robots for autonomous forest inventory, emphasizing navigation and mapping accuracy.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the increasing need for legged robots in less-structured, natural environments such as forestry, where their robustness and mobility can significantly enhance operations.

Method: The study introduces a system architecture for autonomous navigation, mapping, tree detection, and trait estimation, validated using the ANYmal robot in forest trials across three European countries over 1.5 years.

Result: The ANYmal robot surveyed plots up to 1 ha in under 30 minutes with tree DBH accuracy of 2cm, showcasing capabilities in forest environments.

Conclusion: The research highlights lessons and challenges in hardware development, state estimation, forest navigation, and broader issues for autonomous systems, offering directions for future work in natural environments.

Abstract: Legged robots are increasingly being adopted in industries such as oil, gas,
mining, nuclear, and agriculture. However, new challenges exist when moving
into natural, less-structured environments, such as forestry applications. This
paper presents a prototype system for autonomous, under-canopy forest inventory
with legged platforms. Motivated by the robustness and mobility of modern
legged robots, we introduce a system architecture which enabled a quadruped
platform to autonomously navigate and map forest plots. Our solution involves a
complete navigation stack for state estimation, mission planning, and tree
detection and trait estimation. We report the performance of the system from
trials executed over one and a half years in forests in three European
countries. Our results with the ANYmal robot demonstrate that we can survey
plots up to 1 ha plot under 30 min, while also identifying trees with typical
DBH accuracy of 2cm. The findings of this project are presented as five lessons
and challenges. Particularly, we discuss the maturity of hardware development,
state estimation limitations, open problems in forest navigation, future
avenues for robotic forest inventory, and more general challenges to assess
autonomous systems. By sharing these lessons and challenges, we offer insight
and new directions for future research on legged robots, navigation systems,
and applications in natural environments. Additional videos can be found in
https://dynamic.robots.ox.ac.uk/projects/legged-robots

</details>


### [200] [Finding the Easy Way Through -- the Probabilistic Gap Planner for Social Robot Navigation](https://arxiv.org/abs/2506.20320)
*Malte Probst,Raphael Wenzel,Tim Puphal,Monica Dasi,Nico A. Steinhardt,Sango Matsuzaki,Misa Komuro*

Main category: cs.RO

TL;DR: The paper introduces the Probabilistic Gap Planner (PGP), which aims to improve social robot navigation by enhancing conflict and collision avoidance strategies in dense crowds.


<details>
  <summary>Details</summary>
Motivation: State-of-the-art social robot planners focus only on short-term cooperative collision avoidance, making it difficult to navigate complex crowd scenarios where long-term planning is essential.

Method: The paper proposes decomposing trajectory planning into conflict avoidance and cooperative collision avoidance (CCA). It introduces PGP, enhancing an established collision risk model with a probabilistic approach to guide robots towards gaps in crowds.

Result: Simulations demonstrate that PGP improves performance by maintaining more space, reducing tension, and minimizing collisions between agents, though it slightly lengthens the paths taken.

Conclusion: PGP effectively supports social robot navigation in complex environments by balancing safety and long-term planning, achieving real-time implementation on Honda's WaPOCHI robot.

Abstract: In Social Robot Navigation, autonomous agents need to resolve many sequential
interactions with other agents. State-of-the art planners can efficiently
resolve the next, imminent interaction cooperatively and do not focus on longer
planning horizons. This makes it hard to maneuver scenarios where the agent
needs to select a good strategy to find gaps or channels in the crowd. We
propose to decompose trajectory planning into two separate steps: Conflict
avoidance for finding good, macroscopic trajectories, and cooperative collision
avoidance (CCA) for resolving the next interaction optimally. We propose the
Probabilistic Gap Planner (PGP) as a conflict avoidance planner. PGP modifies
an established probabilistic collision risk model to include a general
assumption of cooperativity. PGP biases the short-term CCA planner to head
towards gaps in the crowd. In extensive simulations with crowds of varying
density, we show that using PGP in addition to state-of-the-art CCA planners
improves the agents' performance: On average, agents keep more space to others,
create less tension, and cause fewer collisions. This typically comes at the
expense of slightly longer paths. PGP runs in real-time on WaPOCHI mobile robot
by Honda R&D.

</details>


### [201] [PIMBS: Efficient Body Schema Learning for Musculoskeletal Humanoids with Physics-Informed Neural Networks](https://arxiv.org/abs/2506.20343)
*Kento Kawaharazuka,Takahiro Hattori,Keita Yoneda,Kei Okada*

Main category: cs.RO

TL;DR: The study proposes using Physics-Informed Neural Networks (PINNs) for efficient body schema learning in musculoskeletal humanoid robots, which enhances accuracy while reducing reliance on exhaustive real-world data.


<details>
  <summary>Details</summary>
Motivation: Musculoskeletal humanoids have advantages, but their complex structure makes learning body schema and muscle-joint relationships difficult, especially given the challenges of limited data.

Method: The method integrates Physics-Informed Neural Networks to incorporate physical laws along with robot-based data for learning the relationships between torque, muscle tension, and joint structures.

Result: By applying this approach in both simulations and real robots, the study demonstrates improved learning efficiency and high accuracy using minimal data.

Conclusion: Physics-Informed Neural Networks enable effective body schema learning by combining simulated physics principles with real-world data, offering a practical solution for musculoskeletal humanoid development.

Abstract: Musculoskeletal humanoids are robots that closely mimic the human
musculoskeletal system, offering various advantages such as variable stiffness
control, redundancy, and flexibility. However, their body structure is complex,
and muscle paths often significantly deviate from geometric models. To address
this, numerous studies have been conducted to learn body schema, particularly
the relationships among joint angles, muscle tension, and muscle length. These
studies typically rely solely on data collected from the actual robot, but this
data collection process is labor-intensive, and learning becomes difficult when
the amount of data is limited. Therefore, in this study, we propose a method
that applies the concept of Physics-Informed Neural Networks (PINNs) to the
learning of body schema in musculoskeletal humanoids, enabling high-accuracy
learning even with a small amount of data. By utilizing not only data obtained
from the actual robot but also the physical laws governing the relationship
between torque and muscle tension under the assumption of correct joint
structure, more efficient learning becomes possible. We apply the proposed
method to both simulation and an actual musculoskeletal humanoid and discuss
its effectiveness and characteristics.

</details>


### [202] [CARMA: Context-Aware Situational Grounding of Human-Robot Group Interactions by Combining Vision-Language Models with Object and Action Recognition](https://arxiv.org/abs/2506.20373)
*Joerg Deigmoeller,Stephan Hasler,Nakul Agarwal,Daniel Tanneberg,Anna Belardinelli,Reza Ghoddoosian,Chao Wang,Felix Ocker,Fan Zhang,Behzad Dariush,Michael Gienger*

Main category: cs.RO

TL;DR: CARMA is a system designed to enhance situational awareness during human-robot collaborations by consistently tracking entities and interactions.


<details>
  <summary>Details</summary>
Motivation: Enable effective human-robot collaboration by addressing challenges in situational awareness, role distinction, and episodic event representation.

Method: CARMA identifies physical entities and organizes them into grounded actor-object-action triplets. Experiments include collaborative pouring, handovers, and sorting.

Result: CARMA reliably produces accurate triplets of actors, actions, and objects, showing effectiveness in spatiotemporal reasoning.

Conclusion: CARMA provides a robust framework for human-robot collaborative applications, ensuring consistent and structured situational understanding.

Abstract: We introduce CARMA, a system for situational grounding in human-robot group
interactions. Effective collaboration in such group settings requires
situational awareness based on a consistent representation of present persons
and objects coupled with an episodic abstraction of events regarding actors and
manipulated objects. This calls for a clear and consistent assignment of
instances, ensuring that robots correctly recognize and track actors, objects,
and their interactions over time. To achieve this, CARMA uniquely identifies
physical instances of such entities in the real world and organizes them into
grounded triplets of actors, objects, and actions.
  To validate our approach, we conducted three experiments, where multiple
humans and a robot interact: collaborative pouring, handovers, and sorting.
These scenarios allow the assessment of the system's capabilities as to role
distinction, multi-actor awareness, and consistent instance identification. Our
experiments demonstrate that the system can reliably generate accurate
actor-action-object triplets, providing a structured and robust foundation for
applications requiring spatiotemporal reasoning and situated decision-making in
collaborative settings.

</details>


### [203] [Enhanced Robotic Navigation in Deformable Environments using Learning from Demonstration and Dynamic Modulation](https://arxiv.org/abs/2506.20376)
*Lingyun Chen,Xinrui Zhao,Marcos P. S. Campanha,Alexander Wegener,Abdeldjallil Naceri,Abdalla Swikir,Sami Haddadin*

Main category: cs.RO

TL;DR: The paper introduces a method combining Learning from Demonstration and Dynamical Systems for robots to navigate environments with deformable obstacles efficiently.


<details>
  <summary>Details</summary>
Motivation: To address robot navigation challenges in environments containing both soft and hard obstacles.

Method: They propose integrating LfD with DS and introducing a dynamic modulation matrix to distinguish between soft and hard regions for safe trajectory planning.

Result: Extensive simulations and robot experiments proved effective navigation in deformable environments and control over robot trajectory and velocity.

Conclusion: The method ensures adaptive, smooth, reliable navigation by dynamically adjusting to obstacles while adhering to the desired trajectory.

Abstract: This paper presents a novel approach for robot navigation in environments
containing deformable obstacles. By integrating Learning from Demonstration
(LfD) with Dynamical Systems (DS), we enable adaptive and efficient navigation
in complex environments where obstacles consist of both soft and hard regions.
We introduce a dynamic modulation matrix within the DS framework, allowing the
system to distinguish between traversable soft regions and impassable hard
areas in real-time, ensuring safe and flexible trajectory planning. We validate
our method through extensive simulations and robot experiments, demonstrating
its ability to navigate deformable environments. Additionally, the approach
provides control over both trajectory and velocity when interacting with
deformable objects, including at intersections, while maintaining adherence to
the original DS trajectory and dynamically adapting to obstacles for smooth and
reliable navigation.

</details>


### [204] [SPARK: Graph-Based Online Semantic Integration System for Robot Task Planning](https://arxiv.org/abs/2506.20394)
*Mimo Shirasaka,Yuya Ikeda,Tatsuya Matsushima,Yutaka Matsuo,Yusuke Iwasawa*

Main category: cs.RO

TL;DR: This paper introduces SPARK, a framework for updating semantic information in robot scene graphs for better task execution.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance service robots' capabilities in dynamic environments by enabling online updates of semantic information, which is underexplored compared to geometric updates.

Method: The authors develop SPARK, which extracts semantic cues from the environment and updates an online scene graph used for task planning.

Result: The study demonstrates that integrating spatial relationships in scene graphs helps robots adapt to dynamic settings and unconventional cues, like gestures.

Conclusion: Online scene graph representations improve service robots' ability to plan and execute tasks in dynamic scenarios, enhancing overall adaptability.

Abstract: The ability to update information acquired through various means online
during task execution is crucial for a general-purpose service robot. This
information includes geometric and semantic data. While SLAM handles geometric
updates on 2D maps or 3D point clouds, online updates of semantic information
remain unexplored. We attribute the challenge to the online scene graph
representation, for its utility and scalability. Building on previous works
regarding offline scene graph representations, we study online graph
representations of semantic information in this work. We introduce SPARK:
Spatial Perception and Robot Knowledge Integration. This framework extracts
semantic information from environment-embedded cues and updates the scene graph
accordingly, which is then used for subsequent task planning. We demonstrate
that graph representations of spatial relationships enhance the robot system's
ability to perform tasks in dynamic environments and adapt to unconventional
spatial cues, like gestures.

</details>


### [205] [Multimodal Behaviour Trees for Robotic Laboratory Task Automation](https://arxiv.org/abs/2506.20399)
*Hatem Fakhruldeen,Arvind Raveendran Nambiar,Satheeshkumar Veeramani,Bonilkumar Vijaykumar Tailor,Hadi Beyzaee Juneghani,Gabriella Pizzuto,Andrew Ian Cooper*

Main category: cs.RO

TL;DR: This paper proposes a robotic methodology using behavior trees and multimodal perception for precise and reliable lab tasks like vial capping and rack insertion, achieving high success rates (88% and 92%, respectively).


<details>
  <summary>Details</summary>
Motivation: To address the challenge of automating repetitive lab tasks in a reliable manner to free up chemists for critical research, ensuring safety in tasks like vial capping that might expose humans to dangerous chemicals.

Method: The paper introduces a novel approach based on behavior trees integrated with multimodal sensory feedback to automate and verify task execution in laboratory robots.

Result: Experimental results demonstrate high success rates: 88% for vial capping and 92% for laboratory rack insertion, alongside strong error detection, highlighting the approach's reliability.

Conclusion: The study establishes the robustness of using multimodal behavior trees, showing promise for advancing robotic systems in safety-critical lab environments.

Abstract: Laboratory robotics offer the capability to conduct experiments with a high
degree of precision and reproducibility, with the potential to transform
scientific research. Trivial and repeatable tasks; e.g., sample transportation
for analysis and vial capping are well-suited for robots; if done successfully
and reliably, chemists could contribute their efforts towards more critical
research activities. Currently, robots can perform these tasks faster than
chemists, but how reliable are they? Improper capping could result in human
exposure to toxic chemicals which could be fatal. To ensure that robots perform
these tasks as accurately as humans, sensory feedback is required to assess the
progress of task execution. To address this, we propose a novel methodology
based on behaviour trees with multimodal perception. Along with automating
robotic tasks, this methodology also verifies the successful execution of the
task, a fundamental requirement in safety-critical environments. The
experimental evaluation was conducted on two lab tasks: sample vial capping and
laboratory rack insertion. The results show high success rate, i.e., 88% for
capping and 92% for insertion, along with strong error detection capabilities.
This ultimately proves the robustness and reliability of our approach and that
using multimodal behaviour trees should pave the way towards the next
generation of robotic chemists.

</details>


### [206] [Learn to Position -- A Novel Meta Method for Robotic Positioning](https://arxiv.org/abs/2506.20445)
*Dongkun Wang,Junkai Zhao,Yunfei Teng,Jieyang Peng,Wenjing Xue,Xiaoming Tao*

Main category: cs.RO

TL;DR: This paper proposes a vision-free, model-agnostic method for correcting robotic positioning errors using interactive feedback and self-learning techniques, validated in an electronic assembly line.


<details>
  <summary>Details</summary>
Motivation: Robotic positioning is affected by stochastic errors and limitations in vision-based methods, such as occlusions and poor lighting.

Method: The authors present a vision-free, self-learning, and model-agnostic meta-method that relies on interactive feedback and self-adaptation to compensate for position errors.

Result: Empirical studies demonstrate the method's effectiveness, and it has been successfully implemented in a robotic assembly line for odd-form electronic components.

Conclusion: The method improves robotic positioning accuracy by enabling adaptability to errors and accelerating the learning process as more examples are processed.

Abstract: Absolute positioning accuracy is a vital specification for robots. Achieving
high position precision can be challenging due to the presence of various
sources of errors. Meanwhile, accurately depicting these errors is difficult
due to their stochastic nature. Vision-based methods are commonly integrated to
guide robotic positioning, but their performance can be highly impacted by
inevitable occlusions or adverse lighting conditions. Drawing on the
aforementioned considerations, a vision-free, model-agnostic meta-method for
compensating robotic position errors is proposed, which maximizes the
probability of accurate robotic position via interactive feedback. Meanwhile,
the proposed method endows the robot with the capability to learn and adapt to
various position errors, which is inspired by the human's instinct for grasping
under uncertainties. Furthermore, it is a self-learning and self-adaptive
method able to accelerate the robotic positioning process as more examples are
incorporated and learned. Empirical studies validate the effectiveness of the
proposed method. As of the writing of this paper, the proposed meta search
method has already been implemented in a robotic-based assembly line for
odd-form electronic components.

</details>


### [207] [A Review of Personalisation in Human-Robot Collaboration and Future Perspectives Towards Industry 5.0](https://arxiv.org/abs/2506.20447)
*James Fant-Male,Roel Pieters*

Main category: cs.RO

TL;DR: This paper reviews advancements in Human-Robot Collaboration (HRC) within the context of Industry 5.0, emphasizing the shift towards personalized and user-centric systems.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the shift from Industry 4.0 to Industry 5.0, aiming to prioritize human well-being and social values in workplaces, particularly through enhanced Human-Robot Collaboration.

Method: The review consolidates recent advancements in HRC, focusing on personal factors, workcell design, interaction design, and adaptive task completion, while also discussing ethical and regulatory challenges.

Result: Findings show an increasing trend towards adaptable and personalized HRC systems, though a consistent and unified approach remains lacking.

Conclusion: Human-centric and personalized HRC is essential for Industry 5.0 advancement, but ethical and regulatory frameworks need further development for future applications.

Abstract: The shift in research focus from Industry 4.0 to Industry 5.0 (I5.0) promises
a human-centric workplace, with social and well-being values at the centre of
technological implementation. Human-Robot Collaboration (HRC) is a core aspect
of I5.0 development, with an increase in adaptive and personalised interactions
and behaviours. This review investigates recent advancements towards
personalised HRC, where user-centric adaption is key. There is a growing trend
for adaptable HRC research, however there lacks a consistent and unified
approach. The review highlights key research trends on which personal factors
are considered, workcell and interaction design, and adaptive task completion.
This raises various key considerations for future developments, particularly
around the ethical and regulatory development of personalised systems, which
are discussed in detail.

</details>


### [208] [EANS: Reducing Energy Consumption for UAV with an Environmental Adaptive Navigation Strategy](https://arxiv.org/abs/2506.20485)
*Tian Liu,Han Liu,Boyang Li,Long Chen,Kai Huang*

Main category: cs.RO

TL;DR: This paper proposes a dynamic adjustment method for UAV navigation strategy to reduce energy consumption, achieving significant performance improvements in time and energy metrics.


<details>
  <summary>Details</summary>
Motivation: To address limitations in existing static and conservative UAV navigation strategies that fail to efficiently reduce energy consumption in dynamic scenarios.

Method: The paper introduces a method to dynamically adjust UAV navigation strategies by analyzing both dynamic and temporal characteristics of UAV navigation pipelines as they respond to environmental changes.

Result: Using hardware-in-the-loop simulations and real-world experiments, the method achieved 3.2X and 2.6X improvements in mission time, and 2.4X and 1.6X improvements in energy efficiency when compared to baseline methods.

Conclusion: The proposed dynamic adjustment method effectively enhances UAV navigation efficiency by overcoming environmental and pipeline interdependency challenges, proving its efficacy through substantial empirical improvements.

Abstract: Unmanned Aerial Vehicles (UAVS) are limited by the onboard energy. Refinement
of the navigation strategy directly affects both the flight velocity and the
trajectory based on the adjustment of key parameters in the UAVS pipeline, thus
reducing energy consumption. However, existing techniques tend to adopt static
and conservative strategies in dynamic scenarios, leading to inefficient energy
reduction. Dynamically adjusting the navigation strategy requires overcoming
the challenges including the task pipeline interdependencies, the
environmental-strategy correlations, and the selecting parameters. To solve the
aforementioned problems, this paper proposes a method to dynamically adjust the
navigation strategy of the UAVS by analyzing its dynamic characteristics and
the temporal characteristics of the autonomous navigation pipeline, thereby
reducing UAVS energy consumption in response to environmental changes. We
compare our method with the baseline through hardware-in-the-loop (HIL)
simulation and real-world experiments, showing our method 3.2X and 2.6X
improvements in mission time, 2.4X and 1.6X improvements in energy,
respectively.

</details>


### [209] [Behavior Foundation Model: Towards Next-Generation Whole-Body Control System of Humanoid Robots](https://arxiv.org/abs/2506.20487)
*Mingqi Yuan,Tao Yu,Wenqi Ge,Xiuyong Yao,Dapeng Li,Huijiang Wang,Jiayu Chen,Xin Jin,Bo Li,Hua Chen,Wei Zhang,Wenjun Zeng*

Main category: cs.RO

TL;DR: This paper reviews Behavior(al) Foundation Models (BFMs) for humanoid whole-body control (WBC) and their potential to enable rapid task adaptation.


<details>
  <summary>Details</summary>
Motivation: Humanoid robots face challenges in efficient whole-body control due to their complex dynamics and the limitations of current learning-based controllers, necessitating scalable solutions.

Method: The authors analyze BFMs, which utilize large-scale pretraining to learn reusable skills and behavioral priors that support zero-shot or quick adaptation to new tasks.

Result: The paper provides an overview of BFM development pipelines, real-world applications, a list of curated research, and identifies challenges and opportunities for BFMs in humanoid intelligence.

Conclusion: Behavior(al) Foundation Models are presented as a promising approach for scalable and general-purpose humanoid control, though challenges remain in their real-world application.

Abstract: Humanoid robots are drawing significant attention as versatile platforms for
complex motor control, human-robot interaction, and general-purpose physical
intelligence. However, achieving efficient whole-body control (WBC) in
humanoids remains a fundamental challenge due to sophisticated dynamics,
underactuation, and diverse task requirements. While learning-based controllers
have shown promise for complex tasks, their reliance on labor-intensive and
costly retraining for new scenarios limits real-world applicability. To address
these limitations, behavior(al) foundation models (BFMs) have emerged as a new
paradigm that leverages large-scale pretraining to learn reusable primitive
skills and behavioral priors, enabling zero-shot or rapid adaptation to a wide
range of downstream tasks. In this paper, we present a comprehensive overview
of BFMs for humanoid WBC, tracing their development across diverse pre-training
pipelines. Furthermore, we discuss real-world applications, current
limitations, urgent challenges, and future opportunities, positioning BFMs as a
key approach toward scalable and general-purpose humanoid intelligence.
Finally, we provide a curated and long-term list of BFM papers and projects to
facilitate more subsequent research, which is available at
https://github.com/yuanmingqi/awesome-bfm-papers.

</details>


### [210] [Critical Anatomy-Preserving & Terrain-Augmenting Navigation (CAPTAiN): Application to Laminectomy Surgical Education](https://arxiv.org/abs/2506.20496)
*Jonathan Wang,Hisashi Ishida,David Usevitch,Kesavan Venkatesh,Yi Wang,Mehran Armand,Rachel Bronheim,Amit Jain,Adnan Munawar*

Main category: cs.RO

TL;DR: CAPTAiN, a navigation system for laminectomy surgery, enhances anatomy awareness, improves outcomes, and reduces cognitive load during virtual procedures.


<details>
  <summary>Details</summary>
Motivation: Laminectomy procedures carry high risks with frequent unintended damage to the dura membrane and challenges stemming from patient anatomy variability. Current approaches lack assistive tools to address these risks and support novice training.

Method: The study evaluated CAPTAiN, a voxel-based guidance system offering color-coded anatomical visualization, by comparing its efficacy against standard methods in 110 virtual surgeries performed by 11 participants with varying experience levels.

Result: CAPTAiN showed better surgical completion rates (87.99% vs. 74.42%), reduced cognitive strain, and narrowed performance gaps among novices and advanced trainees.

Conclusion: CAPTAiN enhances surgical training and execution for laminectomy and provides a foundation for broader applications in other medical fields requiring precision drilling.

Abstract: Surgical training remains a crucial milestone in modern medicine, with
procedures such as laminectomy exemplifying the high risks involved.
Laminectomy drilling requires precise manual control to mill bony tissue while
preserving spinal segment integrity and avoiding breaches in the dura: the
protective membrane surrounding the spinal cord. Despite unintended tears
occurring in up to 11.3% of cases, no assistive tools are currently utilized to
reduce this risk. Variability in patient anatomy further complicates learning
for novice surgeons. This study introduces CAPTAiN, a critical
anatomy-preserving and terrain-augmenting navigation system that provides
layered, color-coded voxel guidance to enhance anatomical awareness during
spinal drilling. CAPTAiN was evaluated against a standard non-navigated
approach through 110 virtual laminectomies performed by 11 orthopedic residents
and medical students. CAPTAiN significantly improved surgical completion rates
of target anatomy (87.99% vs. 74.42%) and reduced cognitive load across
multiple NASA-TLX domains. It also minimized performance gaps across experience
levels, enabling novices to perform on par with advanced trainees. These
findings highlight CAPTAiN's potential to optimize surgical execution and
support skill development across experience levels. Beyond laminectomy, it
demonstrates potential for broader applications across various surgical and
drilling procedures, including those in neurosurgery, otolaryngology, and other
medical fields.

</details>


### [211] [Leveraging Correlation Across Test Platforms for Variance-Reduced Metric Estimation](https://arxiv.org/abs/2506.20553)
*Rachel Luo,Heng Yang,Michael Watson,Apoorva Sharma,Sushant Veer,Edward Schmerling,Marco Pavone*

Main category: cs.RO

TL;DR: The paper introduces a framework that uses paired simulation and real-world data to improve estimation accuracy, reducing testing costs for robotic systems.


<details>
  <summary>Details</summary>
Motivation: Testing learning-based robotic systems in real-world scenarios is costly and data from such tests may not be sufficient for guarantees.

Method: The method leverages paired test data (e.g., simulation and real-world outputs) using control variates to reduce the variance of Monte Carlo estimates.

Result: Empirical evaluation in autonomous driving and quadruped robotics showed improved sample efficiency and high-probability bounds.

Conclusion: The proposed approach reduces the need for extensive real-world testing, enabling more cost-effective evaluations of robotic systems.

Abstract: Learning-based robotic systems demand rigorous validation to assure reliable
performance, but extensive real-world testing is often prohibitively expensive,
and if conducted may still yield insufficient data for high-confidence
guarantees. In this work, we introduce a general estimation framework that
leverages paired data across test platforms, e.g., paired simulation and
real-world observations, to achieve better estimates of real-world metrics via
the method of control variates. By incorporating cheap and abundant auxiliary
measurements (for example, simulator outputs) as control variates for costly
real-world samples, our method provably reduces the variance of Monte Carlo
estimates and thus requires significantly fewer real-world samples to attain a
specified confidence bound on the mean performance. We provide theoretical
analysis characterizing the variance and sample-efficiency improvement, and
demonstrate empirically in autonomous driving and quadruped robotics settings
that our approach achieves high-probability bounds with markedly improved
sample efficiency. Our technique can lower the real-world testing burden for
validating the performance of the stack, thereby enabling more efficient and
cost-effective experimental evaluation of robotic systems.

</details>


### [212] [HRIBench: Benchmarking Vision-Language Models for Real-Time Human Perception in Human-Robot Interaction](https://arxiv.org/abs/2506.20566)
*Zhonghao Shi,Enyu Zhao,Nathaniel Dennler,Jingzhen Wang,Xinyang Xu,Kaleen Shrestha,Mengxue Fu,Daniel Seita,Maja Matarić*

Main category: cs.RO

TL;DR: The paper introduces HRIBench, a benchmark for evaluating vision-language models (VLMs) on human perception tasks in human-robot interaction (HRI), finding that current VLMs struggle with latency and perceptual capabilities.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the challenges posed by high latency in VLMs, which hinders their applicability for real-time human-robot interactions.

Method: They developed HRIBench, a VQA benchmark across five domains relevant to HRI, and evaluated 11 state-of-the-art VLMs on their performance and latency trade-offs using curated data.

Result: The study found that all current VLMs exhibit unsatisfactory performance and latency trade-offs for real-time deployment and struggle with fundamental human perception tasks for HRI.

Conclusion: Improvement is necessary for smaller, low-latency VLMs with enhanced perceptual abilities to make them suitable for real-time HRI scenarios.

Abstract: Real-time human perception is crucial for effective human-robot interaction
(HRI). Large vision-language models (VLMs) offer promising generalizable
perceptual capabilities but often suffer from high latency, which negatively
impacts user experience and limits VLM applicability in real-world scenarios.
To systematically study VLM capabilities in human perception for HRI and
performance-latency trade-offs, we introduce HRIBench, a visual
question-answering (VQA) benchmark designed to evaluate VLMs across a diverse
set of human perceptual tasks critical for HRI. HRIBench covers five key
domains: (1) non-verbal cue understanding, (2) verbal instruction
understanding, (3) human-robot object relationship understanding, (4) social
navigation, and (5) person identification. To construct HRIBench, we collected
data from real-world HRI environments to curate questions for non-verbal cue
understanding, and leveraged publicly available datasets for the remaining four
domains. We curated 200 VQA questions for each domain, resulting in a total of
1000 questions for HRIBench. We then conducted a comprehensive evaluation of
both state-of-the-art closed-source and open-source VLMs (N=11) on HRIBench.
Our results show that, despite their generalizability, current VLMs still
struggle with core perceptual capabilities essential for HRI. Moreover, none of
the models within our experiments demonstrated a satisfactory
performance-latency trade-off suitable for real-time deployment, underscoring
the need for future research on developing smaller, low-latency VLMs with
improved human perception capabilities. HRIBench and our results can be found
in this Github repository: https://github.com/interaction-lab/HRIBench.

</details>


### [213] [Communication-Aware Map Compression for Online Path-Planning: A Rate-Distortion Approach](https://arxiv.org/abs/2506.20579)
*Ali Reza Pedram,Evangelos Psomiadis,Dipankar Maity,Panagiotis Tsiotras*

Main category: cs.RO

TL;DR: This paper proposes an efficient method for two collaborating robots to navigate an unknown environment while optimizing communication via task-relevant, compressed map sharing.


<details>
  <summary>Details</summary>
Motivation: To enable efficient collaboration between two robots navigating an unknown environment while addressing bandwidth constraints and selective sharing of map information.

Method: The method formulates a rate-distortion optimization problem for determining communication timing, map region selection, and encoding resolution. It employs a convex optimization solution called reverse water-filling for efficient implementation.

Result: Simulation results show that the method constructs task-relevant, compressed map representations that guide navigation effectively under restricted bandwidth.

Conclusion: The proposed approach enables real-time, low-computation collaborative navigation by balancing communication efficiency and task relevance in map sharing between robots.

Abstract: This paper addresses the problem of collaborative navigation in an unknown
environment, where two robots, referred to in the sequel as the Seeker and the
Supporter, traverse the space simultaneously. The Supporter assists the Seeker
by transmitting a compressed representation of its local map under bandwidth
constraints to support the Seeker's path-planning task. We introduce a bit-rate
metric based on the expected binary codeword length to quantify communication
cost. Using this metric, we formulate the compression design problem as a
rate-distortion optimization problem that determines when to communicate, which
regions of the map should be included in the compressed representation, and at
what resolution (i.e., quantization level) they should be encoded. Our
formulation allows different map regions to be encoded at varying quantization
levels based on their relevance to the Seeker's path-planning task. We
demonstrate that the resulting optimization problem is convex, and admits a
closed-form solution known in the information theory literature as reverse
water-filling, enabling efficient, low-computation, and real-time
implementation. Additionally, we show that the Seeker can infer the compression
decisions of the Supporter independently, requiring only the encoded map
content and not the encoding policy itself to be transmitted, thereby reducing
communication overhead. Simulation results indicate that our method effectively
constructs compressed, task-relevant map representations, both in content and
resolution, that guide the Seeker's planning decisions even under tight
bandwidth limitations.

</details>


### [214] [A Computationally Aware Multi Objective Framework for Camera LiDAR Calibration](https://arxiv.org/abs/2506.20636)
*Venkat Karramreddy,Rangarajan Ramanujam*

Main category: cs.RO

TL;DR: The study introduces a multi-objective optimization framework for camera-LiDAR calibration balancing alignment accuracy and computational cost.


<details>
  <summary>Details</summary>
Motivation: To address the need for aligning LiDAR and camera sensors accurately while considering resource constraints in autonomous systems.

Method: A multi-objective optimization framework using NSGA-II evolutionary algorithm to account for 6-DoF transformations and runtime/resource costs.

Result: Validated on the KITTI dataset, the method achieves interpretable and tunable outcomes with lower deployment overhead compared to existing methods.

Conclusion: The framework provides scalable, transparent calibration suited for embedded systems, aiding long-term autonomous applications despite deferred real-time deployment.

Abstract: Accurate extrinsic calibration between LiDAR and camera sensors is important
for reliable perception in autonomous systems. In this paper, we present a
novel multi-objective optimization framework that jointly minimizes the
geometric alignment error and computational cost associated with camera-LiDAR
calibration. We optimize two objectives: (1) error between projected LiDAR
points and ground-truth image edges, and (2) a composite metric for
computational cost reflecting runtime and resource usage. Using the NSGA-II
\cite{deb2002nsga2} evolutionary algorithm, we explore the parameter space
defined by 6-DoF transformations and point sampling rates, yielding a
well-characterized Pareto frontier that exposes trade-offs between calibration
fidelity and resource efficiency. Evaluations are conducted on the KITTI
dataset using its ground-truth extrinsic parameters for validation, with
results verified through both multi-objective and constrained single-objective
baselines. Compared to existing gradient-based and learned calibration methods,
our approach demonstrates interpretable, tunable performance with lower
deployment overhead. Pareto-optimal configurations are further analyzed for
parameter sensitivity and innovation insights. A preference-based
decision-making strategy selects solutions from the Pareto knee region to suit
the constraints of the embedded system. The robustness of calibration is tested
across variable edge-intensity weighting schemes, highlighting optimal balance
points. Although real-time deployment on embedded platforms is deferred to
future work, this framework establishes a scalable and transparent method for
calibration under realistic misalignment and resource-limited conditions,
critical for long-term autonomy, particularly in SAE L3+ vehicles receiving OTA
updates.

</details>


### [215] [DemoDiffusion: One-Shot Human Imitation using pre-trained Diffusion Policy](https://arxiv.org/abs/2506.20668)
*Sungjae Park,Homanga Bharadhwaj,Shubham Tulsiani*

Main category: cs.RO

TL;DR: DemoDiffusion is a method allowing robots to imitate a single human demonstration for manipulation tasks using a diffusion policy to adapt retargeted motion trajectories.


<details>
  <summary>Details</summary>
Motivation: Enable robots to perform manipulation tasks in natural environments effectively by leveraging a single human demonstration, avoiding paired human-robot data or reinforcement learning.

Method: The method involves kinematic retargeting of human hand motion to an open-loop robot motion trajectory, followed by using a pre-trained diffusion policy to adapt the trajectory for plausible robot actions.

Result: DemoDiffusion demonstrated superior performance in simulation and real-world experiments compared to base policies and unaltered retargeted trajectories, enabling success in challenging tasks.

Conclusion: DemoDiffusion proves to be a scalable and effective approach for robot manipulation tasks, reducing the reliance on extensive training data and adapting to new tasks with minimal manual input.

Abstract: We propose DemoDiffusion, a simple and scalable method for enabling robots to
perform manipulation tasks in natural environments by imitating a single human
demonstration. Our approach is based on two key insights. First, the hand
motion in a human demonstration provides a useful prior for the robot's
end-effector trajectory, which we can convert into a rough open-loop robot
motion trajectory via kinematic retargeting. Second, while this retargeted
motion captures the overall structure of the task, it may not align well with
plausible robot actions in-context. To address this, we leverage a pre-trained
generalist diffusion policy to modify the trajectory, ensuring it both follows
the human motion and remains within the distribution of plausible robot
actions. Our approach avoids the need for online reinforcement learning or
paired human-robot data, enabling robust adaptation to new tasks and scenes
with minimal manual effort. Experiments in both simulation and real-world
settings show that DemoDiffusion outperforms both the base policy and the
retargeted trajectory, enabling the robot to succeed even on tasks where the
pre-trained generalist policy fails entirely. Project page:
https://demodiffusion.github.io/

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [216] [Can LLMs Replace Humans During Code Chunking?](https://arxiv.org/abs/2506.19897)
*Christopher Glasz,Emily Escamilla,Eric O. Scott,Anand Patel,Jacob Zimmer,Colin Diggs,Michael Doyle,Scott Rosen,Nitin Naik,Justin F. Brunelle,Samruddhi Thaker,Parthav Poudel,Arun Sridharan,Amit Madan,Doug Wendt,William Macke,Thomas Schill*

Main category: cs.SE

TL;DR: This paper explores using large language models (LLMs) for modernizing legacy government code in languages like ALC and MUMPS, focusing on code-chunking methods for improving documentation quality.


<details>
  <summary>Details</summary>
Motivation: There is a need to modernize legacy government code written in languages like MUMPS and ALC, especially given challenges such as token length limitations and LLMs’ limited training in legacy languages.

Method: The researchers investigated code-chunking methods using LLMs to generate module comments for legacy code, evaluating documentation quality across LLMs such as GPT-4o, Claude 3 Sonnet, Mixtral, and Llama 3.

Result: LLMs can significantly improve code documentation, with partitions created by LLMs generating comments that are up to 20% more factual and 10% more useful compared to human-created partitions.

Conclusion: LLMs are effective in partitioning large codebases and can replace human efforts in this aspect during code modernization, especially for enhancing documentation generation.

Abstract: Large language models (LLMs) have become essential tools in computer science,
especially for tasks involving code understanding and generation. However,
existing work does not address many of the unique challenges presented by code
written for government applications. In particular, government enterprise
software is often written in legacy languages like MUMPS or assembly language
code (ALC) and the overall token lengths of these systems exceed the context
window size for current commercially available LLMs. Additionally, LLMs are
primarily trained on modern software languages and have undergone limited
testing with legacy languages, making their ability to understand legacy
languages unknown and, hence, an area for empirical study. This paper examines
the application of LLMs in the modernization of legacy government code written
in ALC and MUMPS, addressing the challenges of input limitations. We
investigate various code-chunking methods to optimize the generation of summary
module comments for legacy code files, evaluating the impact of code-chunking
methods on the quality of documentation produced by different LLMs, including
GPT-4o, Claude 3 Sonnet, Mixtral, and Llama 3. Our results indicate that LLMs
can select partition points closely aligned with human expert partitioning. We
also find that chunking approaches have significant impact on downstream tasks
such as documentation generation. LLM-created partitions produce comments that
are up to 20% more factual and up to 10% more useful than when humans create
partitions. Therefore, we conclude that LLMs can be used as suitable
replacements for human partitioning of large codebases during LLM-aided
modernization.

</details>


### [217] [When Domains Collide: An Activity Theory Exploration of Cross-Disciplinary Collaboration](https://arxiv.org/abs/2506.20063)
*Zixuan Feng,Thomas Zimmermann,Lorenzo Pisani,Christopher Gooley,Jeremiah Wander,Anita Sarma*

Main category: cs.SE

TL;DR: This study explores the dynamics and frictions in cross-disciplinary software development (CDSD) collaborations between domain experts (DEs) and software developers (SDEs).


<details>
  <summary>Details</summary>
Motivation: To address friction caused by contested expectations, divergent perspectives, and conflicting priorities between DEs and SDEs in collaborative software development.

Method: The authors employed Activity Theory (AT) as a socio-technical framework in a mixed-method study involving interviews with 24 participants (12 DEs and 12 SDEs) and a validation survey with 293 participants (161 DEs and 132 SDEs).

Result: The study identified 8 expectations by SDEs and 6 by DEs, revealed 21 frictions in CDSD, and mapped these to Activity Theory components, offering insights into where and how these frictions occur.

Conclusion: The findings provide a theoretical understanding of CDSD dynamics and practical guidance for researchers, practitioners, and infrastructure design to improve collaboration.

Abstract: Background: Software development teams are increasingly diverse, embedded,
and cross-disciplinary. Domain experts (DEs) from different disciplines
collaborate with professional software developers (SDEs), bringing
complementary expertise in creating and maintaining complex production
software. However, contested expectations, divergent problem-solving
perspectives, and conflicting priorities lead to friction. Aims: This study
aims to investigate the dynamics of emerging collaboration of
cross-disciplinary software development (CDSD) by exploring the expectations
held by DEs and SDEs and understanding how these frictions manifest in
practice. Method: We utilize Activity Theory (AT), a well-established
socio-technical framework, as an analytical lens in a grounded, empirical
investigation, conducted through a mixed-method study involving 24 interviews
(12 DEs and 12 SDEs) and a large-scale validation survey with 293 participants
(161 DEs and 132 SDEs). Results: We conceptualize and empirically ground the
CDSD dynamics. We identified eight expectations held by SDEs and six by DEs. By
mapping these expectations to AT components, we revealed 21 frictions in CDSD
and illustrated where and how they arise. Conclusions: This study offers a
theoretical lens for understanding the dynamics and frictions in CDSD and
provides actionable insights for future research, practitioners, and
infrastructure design.

</details>


### [218] [AI and Agile Software Development: From Frustration to Success -- XP2025 Workshop Summary](https://arxiv.org/abs/2506.20159)
*Tomas Herda,Victoria Pichler,Zheying Zhang,Pekka Abrahamsson,Geir K. Hanssen*

Main category: cs.SE

TL;DR: A workshop at XP 2025 addressed the integration of AI into Agile software development, highlighting challenges and creating a roadmap for future research.


<details>
  <summary>Details</summary>
Motivation: To address practical challenges and opportunities arising from the integration of AI in Agile software development.

Method: Interactive sessions to identify challenges, prioritize issues, uncover root causes, and develop a research roadmap.

Result: Key challenges like tooling, governance, data quality, and skill gaps were identified and prioritized; actionable goals for research were outlined.

Conclusion: The workshop set the stage for future collaborative efforts, proposing tangible directions for successfully integrating AI into Agile practices.

Abstract: The full-day workshop on AI and Agile at XP 2025 convened a diverse group of
researchers and industry practitioners to address the practical challenges and
opportunities of integrating Artificial Intelligence into Agile software
development. Through interactive sessions, participants identified shared
frustrations related to integrating AI into Agile Software Development
practices, including challenges with tooling, governance, data quality, and
critical skill gaps. These challenges were systematically prioritized and
analyzed to uncover root causes. The workshop culminated in the collaborative
development of a research roadmap that pinpoints actionable directions for
future work, including both immediate solutions and ambitious long-term goals.
The key outcome is a structured agenda designed to foster joint
industry-academic efforts to move from identified frustrations to successful
implementation.

</details>


### [219] [Ten simple rules for PIs to integrate Research Software Engineering into their research group](https://arxiv.org/abs/2506.20217)
*Stuart M. Allen,Neil Chue Hong,Stephan Druskat,Toby Hodges,Daniel S. Katz,Jan Linxweiler,Frank Löffler,Lars Grunske,Heidi Seibold,Jan Philipp Thiele,Samantha Wittke*

Main category: cs.SE

TL;DR: The paper provides ten actionable rules to help research leaders integrate Research Software Engineering (RSEng) into their research for improved software quality and research outcomes.


<details>
  <summary>Details</summary>
Motivation: Many research group leaders and investigators are unfamiliar with RSEng and its benefits, yet effective RSEng practices are key to producing better and more reproducible research outcomes.

Method: The paper outlines ten simple and actionable rules to make RSEng accessible and to guide research leaders in integrating these practices effectively.

Result: Application of these rules is expected to lead to high-quality, reproducible, and trustworthy research software, enhancing overall research quality.

Conclusion: Implementation of these RSEng principles will benefit researchers by improving their software and research outcomes, making results more replicable and reliable.

Abstract: Research Software Engineering (RSEng) is a key success factor in producing
high-quality research software, which in turn enables and improves research
outcomes. However, as a principal investigator or leader of a research group
you may not know what RSEng is, where to get started with it, or how to use it
to maximize its benefit for your research. RSEng also often comes with
technical complexity, and therefore reduced accessibility to some researchers.
The ten simple rules presented in this paper aim to improve the accessibility
of RSEng, and provide practical and actionable advice to PIs and leaders for
integrating RSEng into their research group. By following these rules, readers
can improve the quality, reproducibility, and trustworthiness of their research
software, ultimately leading to better, more reproducible and more trustworthy
research outcomes.

</details>


### [220] [The Composition of Digital Twins for Systems-of-Systems: a Systematic Literature Review](https://arxiv.org/abs/2506.20435)
*Mennatullah T. Khedr,John S. Fitzgerald*

Main category: cs.SE

TL;DR: The paper reviews composition and verification & validation (V&V) methods for Digital Twins, identifying gaps in formalization and standardization, while offering a structured classification of existing approaches.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of modeling and ensuring reliable integration in complex systems like Cyber-Physical Systems (CPS) and System-of-Systems (SoS) using Digital Twins.

Method: Systematic literature review of 21 studies from 2022-2024 focusing on DT composition mechanisms, SoS characteristics, and V&V formality, scope, and challenges.

Result: Semi-formal and simulation-based V&V methods dominate, while formal verification is underutilized. Lack of standardized DT-specific frameworks and challenges in integration and consistency are highlighted.

Conclusion: The study underscores the need for standardized, scalable V&V frameworks and rigorous composition methodologies for effective Digital Twin implementations in complex systems.

Abstract: Digital Twins (DTs) are increasingly used to model complex systems,
especially in Cyber-Physical Systems (CPS) and System-of-Systems (SoS), where
effective integration is key. This systematic literature review investigates DT
composition and verification and validation (V&V) methodologies. Analyzing 21
studies from 2022-2024, we examined composition mechanisms, SoS
characteristics, and V&V formality, scope, and challenges. While composition is
discussed, formalization is limited. V&V approaches vary, with semi-formal
methods and simulations dominating; formal verification is underutilized. Key
technical challenges include model uncertainty and integration complexity.
Methodological challenges highlight the lack of standardized DT-specific V&V
frameworks. There is a need to move beyond model validation to address
integration and cyber-physical consistency. This review contributes a
structured classification of V&V approaches and emphasizes the need for
standardized, scalable V&V and rigorous composition methodologies for complex
DT implementations.

</details>


### [221] [Smart Cuts: Enhance Active Learning for Vulnerability Detection by Pruning Bad Seeds](https://arxiv.org/abs/2506.20444)
*Xiang Lan,Tim Menzies,Bowen Xu*

Main category: cs.SE

TL;DR: This paper introduces a dataset maps-based method to improve machine learning models in vulnerability detection by identifying and addressing low-quality training data.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance the effectiveness of vulnerability detection methods by addressing the issue of low-quality training datasets that hinder machine learning performance.

Method: The proposed method introduces dataset maps that categorize training samples by difficulty, integrating this insight into an active learning framework to filter out harmful data samples while emphasizing informative ones.

Result: Experimental results show substantial improvements in F1 score and active learning performance for CodeBERT on the Big-Vul dataset, outperforming traditional and standard active learning methods.

Conclusion: The approach enhances model robustness and stabilizes active learning performance while providing insights for future dataset construction improvements in the field of vulnerability detection.

Abstract: Vulnerability detection is crucial for identifying security weaknesses in
software systems. However, the effectiveness of machine learning models in this
domain is often hindered by low-quality training datasets, which contain noisy,
mislabeled, or imbalanced samples. This paper proposes a novel dataset
maps-empowered approach that systematically identifies and mitigates
hard-to-learn outliers, referred to as "bad seeds", to improve model training
efficiency. Our approach can categorize training examples based on learning
difficulty and integrate this information into an active learning framework.
Unlike traditional methods that focus on uncertainty-based sampling, our
strategy prioritizes dataset quality by filtering out performance-harmful
samples while emphasizing informative ones. Our experimental results show that
our approach can improve F1 score over random selection by 45.36% (DeepGini)
and 45.91% (K-Means) and outperforms standard active learning by 61.46%
(DeepGini) and 32.65% (K-Means) for CodeBERT on the Big-Vul dataset,
demonstrating the effectiveness of integrating dataset maps for optimizing
sample selection in vulnerability detection. Furthermore, our approach also
enhances model robustness, improves sample selection by filtering bad seeds,
and stabilizes active learning performance across iterations. By analyzing the
characteristics of these outliers, we provide insights for future improvements
in dataset construction, making vulnerability detection more reliable and
cost-effective.

</details>


### [222] [Large Language Model-Driven Code Compliance Checking in Building Information Modeling](https://arxiv.org/abs/2506.20551)
*Soumya Madireddy,Lu Gao,Zia Din,Kinam Kim,Ahmed Senouci,Zhe Han,Yunpeng Zhang*

Main category: cs.SE

TL;DR: The paper introduces a system using Large Language Models (LLMs) combined with Revit software to semi-automate building code compliance checking in BIM, reducing manual work and improving accuracy.


<details>
  <summary>Details</summary>
Motivation: Manual code compliance checking in BIM is both time-consuming and prone to errors, necessitating a semi-automated approach using AI advancements.

Method: The method integrates LLMs like GPT and others with the Revit platform to interpret building codes, generate Python scripts, and run compliance checks.

Result: Case studies demonstrated reduced time, effort, and improved accuracy in identifying code violations compared to manual methods, with actionable reports generated automatically.

Conclusion: The system offers a robust, adaptable, and cost-effective solution to streamline compliance checking in BIM environments, with potential scalability to various regulations and projects.

Abstract: This research addresses the time-consuming and error-prone nature of manual
code compliance checking in Building Information Modeling (BIM) by introducing
a Large Language Model (LLM)-driven approach to semi-automate this critical
process. The developed system integrates LLMs such as GPT, Claude, Gemini, and
Llama, with Revit software to interpret building codes, generate Python
scripts, and perform semi-automated compliance checks within the BIM
environment. Case studies on a single-family residential project and an office
building project demonstrated the system's ability to reduce the time and
effort required for compliance checks while improving accuracy. It streamlined
the identification of violations, such as non-compliant room dimensions,
material usage, and object placements, by automatically assessing relationships
and generating actionable reports. Compared to manual methods, the system
eliminated repetitive tasks, simplified complex regulations, and ensured
reliable adherence to standards. By offering a comprehensive, adaptable, and
cost-effective solution, this proposed approach offers a promising advancement
in BIM-based compliance checking, with potential applications across diverse
regulatory documents in construction projects.

</details>


### [223] [CCISolver: End-to-End Detection and Repair of Method-Level Code-Comment Inconsistency](https://arxiv.org/abs/2506.20558)
*Renyi Zhong,Yintong Huo,Wenwei Gu,Jinxi Kuang,Zhihan Jiang,Guangba Yu,Yichen Li,David Lo,Michael R. Lyu*

Main category: cs.SE

TL;DR: Code-comment inconsistencies (CCI) harm development; this paper presents CCIBench dataset and CCISolver framework to identify and fix CCIs, achieving state-of-the-art performance in detection and fixing tasks.


<details>
  <summary>Details</summary>
Motivation: Code comments are essential for understanding and maintaining software, but inconsistencies between code and comments cause significant issues. Existing methods struggle with poor datasets and inadequate approaches.

Method: The authors analyzed existing mislabeled datasets and created CCIBench, a high-quality dataset. They also developed CCISolver, an end-to-end framework leveraging large language models (LLMs) for detecting and correcting CCIs.

Result: CCISolver achieved an F1-score of 89.54% for detection and a 18.84% improvement in GLEU score for fixing tasks compared to the strongest alternative method. It is also 36% faster in inference than the baseline model.

Conclusion: CCISolver offers scalable, faster, and more efficient solutions to CCI challenges, surpassing existing methods in both detection and correction accuracy, while backed by a superior dataset, CCIBench.

Abstract: Comments within code serve as a crucial foundation for software
documentation, facilitating developers to communicate and understand the code
effectively. However, code-comment inconsistency (CCI) can negatively affect
software development, testing, and maintenance. Recent efforts to mitigate this
issue have emerged, but existing studies often suffer from inaccurate datasets
and inadequate solutions, weakening their practical effectiveness. In this
study, we first conduct a quantitative analysis of existing datasets, revealing
a substantial portion of sampled data are mislabeled. To address these data
limitations, we introduce CCIBench, a refined dataset comprising high-quality
data, to support the training and evaluation of method-level CCI methods.
Furthermore, we present an innovative end-to-end LLM-based framework,
CCISolver, designed to improve code quality by identifying and rectifying CCIs.
Comprehensive evaluations demonstrate CCISolver's superior performance. For
detection, it establishes a new state-of-the-art with an F1-score of 89.54%. In
fixing task, it achieves a remarkable 18.84% relative improvement in GLEU score
over the strongest baseline. This superiority is confirmed by human evaluation,
where CCISolver's fixing success rate of 0.6533 significantly surpasses
existing methods. Critically, in a practical end-to-end setting, CCISolver's
innovative architecture is approximately 36% faster for inference than the
baseline model, underscoring its scalability and real-world applicability.

</details>


### [224] [Define-ML: An Approach to Ideate Machine Learning-Enabled Systems](https://arxiv.org/abs/2506.20621)
*Silvio Alonso,Antonio Pedro Santos Alves,Lucas Romao,Hélio Lopes,Marcos Kalinowski*

Main category: cs.SE

TL;DR: Define-ML extends Lean Inception to address ML-specific challenges in early-stage product ideation, building structured activities for data alignment and feasibility assessment.


<details>
  <summary>Details</summary>
Motivation: Traditional ideation methods like Lean Inception are insufficient for ML-specific considerations, leading to problematic product visions and expectations.

Method: The paper develops Define-ML through the Technology Transfer Model with both static and dynamic validation using surveys and feedback.

Result: Define-ML was effective in clarifying data concerns, aligning ML and business goals, and improving collaboration, despite a learning curve for some ML components.

Conclusion: Define-ML is a validated framework that supports agile ideation in ML projects, balancing data and technical feasibility with business objectives.

Abstract: [Context] The increasing adoption of machine learning (ML) in software
systems demands specialized ideation approaches that address ML-specific
challenges, including data dependencies, technical feasibility, and alignment
between business objectives and probabilistic system behavior. Traditional
ideation methods like Lean Inception lack structured support for these ML
considerations, which can result in misaligned product visions and unrealistic
expectations. [Goal] This paper presents Define-ML, a framework that extends
Lean Inception with tailored activities - Data Source Mapping, Feature-to-Data
Source Mapping, and ML Mapping - to systematically integrate data and technical
constraints into early-stage ML product ideation. [Method] We developed and
validated Define-ML following the Technology Transfer Model, conducting both
static validation (with a toy problem) and dynamic validation (in a real-world
industrial case study). The analysis combined quantitative surveys with
qualitative feedback, assessing utility, ease of use, and intent of adoption.
[Results] Participants found Define-ML effective for clarifying data concerns,
aligning ML capabilities with business goals, and fostering cross-functional
collaboration. The approach's structured activities reduced ideation ambiguity,
though some noted a learning curve for ML-specific components, which can be
mitigated by expert facilitation. All participants expressed the intention to
adopt Define-ML. [Conclusion] Define-ML provides an openly available, validated
approach for ML product ideation, building on Lean Inception's agility while
aligning features with available data and increasing awareness of technical
feasibility.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [225] [Impact of Hill coefficient and time delay on a perceptual decision-making model](https://arxiv.org/abs/2506.19853)
*Bartłomiej Morawski,Anna Czartoszewska*

Main category: q-bio.NC

TL;DR: This paper analyzes an existing neural mass model for perceptual decision-making, extending it by exploring the effects of self-inhibition delay and Hill coefficient.


<details>
  <summary>Details</summary>
Motivation: To enhance understanding of perceptual decision-making by generalizing an existing neural mass model and exploring additional parameters like self-inhibition delay and Hill coefficient.

Method: The authors use a combination of analytical and numerical methods to compare different versions of the neural mass model with varied assumptions.

Result: The impact of both self-inhibition delay and the Hill coefficient is investigated for possible generalizations and solutions to the model's differential equations.

Conclusion: The study extends prior findings on neural mass models by incorporating and analyzing additional factors, improving the understanding of binary decision processes in neural systems.

Abstract: In this paper, a neural mass perceptual decision making model introduced by
Piska{\l}a et al. is analyzed. The model describes activity of two neuron
populations influenced by each other and external inputs. The groups'
activities correspond to the process of making a perceptual binary decision.
Existing results are generalized by investigating the impact of both a delay in
self-inhibition and a generic Hill coefficient on solutions to the system of
differential equations. Several versions of the model with various assumptions
are compared using analytical and numerical methods.

</details>


### [226] [Do psychic cells generate consciousness?](https://arxiv.org/abs/2506.20164)
*Mototaka Suzuki,Jaan Aru*

Main category: q-bio.NC

TL;DR: Technological advances aid neuroscientists in understanding the cellular-level mechanisms of consciousness, focusing on pyramidal neurons and feedback signaling.


<details>
  <summary>Details</summary>
Motivation: To revisit and understand the cellular mechanisms of consciousness, driven by technological progress and Cajal's early hypotheses.

Method: A review of recent cellular neuroscience studies emphasizing pyramidal neurons and metabotropic receptor mechanisms.

Result: Pyramidal neurons and their receptors found to play key roles in feedback disruption during anesthetic-induced consciousness loss.

Conclusion: Insights into pyramidal neurons and their receptor mechanisms suggest progress in linking cellular processes to consciousness.

Abstract: Technological advances in the past decades have begun to enable
neuroscientists to address fundamental questions about consciousness in an
unprecedented way. Here we review remarkable recent progress in our
understanding of cellular-level mechanisms of conscious processing in the
brain. Of particular interest are the cortical pyramidal neurons -- or "psychic
cells" called by Ram\'on y Cajal more than 100 years ago -- which have an
intriguing cellular mechanism that accounts for selective disruption of
feedback signaling in the brain upon anesthetic-induced loss of consciousness.
Importantly, a particular class of metabotropic receptors distributed over the
dendrites of pyramidal cells are highlighted as the key cellular mechanism.
After all, Cajal's instinct over a century ago may turn out to be correct -- we
may have just begun to understand whether and how psychic cells indeed generate
and control our consciousness.

</details>


### [227] [Identifying multi-compartment Hodgkin-Huxley models with high-density extracellular voltage recordings](https://arxiv.org/abs/2506.20233)
*Ian Christopher Tanoh,Michael Deistler,Jakob H. Macke,Scott W. Linderman*

Main category: q-bio.NC

TL;DR: The paper introduces a method to estimate neuron parameters and membrane voltage using extracellular recordings, overcoming challenges associated with intracellular measurements.


<details>
  <summary>Details</summary>
Motivation: Hodgkin-Huxley models are critical for understanding neural computation but require parameters that are hard to measure in vivo. With advances in extracellular recording technologies, there is a need for methods that use such data to infer neuron properties.

Method: The paper employs an Extended Kalman Filter for initial inference of membrane voltage and channel states, followed by gradient-based optimization to learn model parameters via maximizing marginal likelihood.

Result: The approach was validated using simulated data and realistic neuron morphologies, showcasing its capability to accurately infer neuronal parameters using extracellular signals.

Conclusion: This method enables the indirect estimation of biophysical and positional neuron parameters without needing invasive intracellular recordings, highlighting the potential of extracellular measurements for neuron modeling.

Abstract: Multi-compartment Hodgkin-Huxley models are biophysical models of how
electrical signals propagate throughout a neuron, and they form the basis of
our knowledge of neural computation at the cellular level. However, these
models have many free parameters that must be estimated for each cell, and
existing fitting methods rely on intracellular voltage measurements that are
highly challenging to obtain in vivo. Recent advances in neural recording
technology with high-density probes and arrays enable dense sampling of
extracellular voltage from many sites surrounding a neuron, allowing indirect
measurement of many compartments of a cell simultaneously. Here, we propose a
method for inferring the underlying membrane voltage, biophysical parameters,
and the neuron's position relative to the probe, using extracellular
measurements alone. We use an Extended Kalman Filter to infer membrane voltage
and channel states using efficient, differentiable simulators. Then, we learn
the model parameters by maximizing the marginal likelihood using gradient-based
methods. We demonstrate the performance of this approach using simulated data
and real neuron morphologies.

</details>


### [228] [Brains and language models converge on a shared conceptual space across different languages](https://arxiv.org/abs/2506.20489)
*Zaid Zada,Samuel A Nastase,Jixing Li,Uri Hasson*

Main category: q-bio.NC

TL;DR: The study explores whether different languages share neural representations of conceptual meaning, leveraging language models (LMs) and fMRI data.


<details>
  <summary>Details</summary>
Motivation: Languages differ widely in form but convey similar meanings, prompting an investigation into whether shared neural substrates for meaning exist across language speakers.

Method: The researchers used LMs to assess embedding convergence across English, Chinese, and French. They applied voxelwise encoding models to align LM embeddings with neural fMRI data and test cross-language generalization.

Result: LMs converged onto similar embeddings across languages, particularly in middle layers. Neural models generalized well for predicting activity across subjects hearing the same content in different languages.

Conclusion: Neural representations of meaning are shared across speakers of different languages, and LMs for different languages converge on these shared representations.

Abstract: Human languages differ widely in their forms, each having distinct sounds,
scripts, and syntax. Yet, they can all convey similar meaning. Do different
languages converge on a shared neural substrate for conceptual meaning? We used
language models (LMs) and naturalistic fMRI to identify neural representations
of the shared conceptual meaning of the same story as heard by native speakers
of three languages: English, Chinese, and French. We found that LMs trained on
entirely different languages converge onto a similar embedding space,
especially in the middle layers. We then aimed to find if a similar shared
space exists in the brains of different native speakers of the three languages.
We trained voxelwise encoding models that align the LM embeddings with neural
responses from one group of subjects speaking a single language. We then used
the encoding models trained on one language to predict the neural activity in
listeners of other languages. We found that models trained to predict neural
activity for one language generalize to different subjects listening to the
same content in a different language, across high-level language and
default-mode regions. Our results suggest that the neural representations of
meaning underlying different languages are shared across speakers of various
languages, and that LMs trained on different languages converge on this shared
meaning. These findings suggest that, despite the diversity of languages,
shared meaning emerges from our interactions with one another and our shared
world.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [229] [Data-Driven Dynamic Factor Modeling via Manifold Learning](https://arxiv.org/abs/2506.19945)
*Graeme Baker,Agostino Capponi,J. Antonio Sidaoui*

Main category: stat.ML

TL;DR: The paper introduces a data-driven dynamic factor framework using Anisotropic Diffusion Maps and Kalman filtering to predict high-dimensional covariate and response behavior, demonstrating superior performance in financial portfolio stress testing.


<details>
  <summary>Details</summary>
Motivation: To enhance predictive capabilities in scenarios where covariates and response variables exhibit complex, high-dimensional dynamics, especially in financial stress testing applications.

Method: The authors employ Anisotropic Diffusion Maps for manifold learning, approximate the embedding dynamics using linear diffusions, and use Kalman filtering to model and predict covariate-response dynamics. They also offer convergence rate analyses and robustness proofs.

Result: The proposed method significantly reduces prediction errors in portfolio stress testing, achieving up to a 55% improvement over traditional benchmarks across major financial crises.

Conclusion: The framework effectively captures complex joint dynamics in high-dimensional data and provides a robust data-driven alternative to traditional linear and scenario-based approaches, especially in stress testing financial portfolios.

Abstract: We propose a data-driven dynamic factor framework where a response variable
depends on a high-dimensional set of covariates, without imposing any
parametric model on the joint dynamics. Leveraging Anisotropic Diffusion Maps,
a nonlinear manifold learning technique introduced by Singer and Coifman, our
framework uncovers the joint dynamics of the covariates and responses in a
purely data-driven way. We approximate the embedding dynamics using linear
diffusions, and exploit Kalman filtering to predict the evolution of the
covariates and response variables directly from the diffusion map embedding
space. We generalize Singer's convergence rate analysis of the graph Laplacian
from the case of independent uniform samples on a compact manifold to the case
of time series arising from Langevin diffusions in Euclidean space.
Furthermore, we provide rigorous justification for our procedure by showing the
robustness of approximations of the diffusion map coordinates by linear
diffusions, and the convergence of ergodic averages under standard spectral
assumptions on the underlying dynamics. We apply our method to the stress
testing of equity portfolios using a combination of financial and macroeconomic
factors from the Federal Reserve's supervisory scenarios. We demonstrate that
our data-driven stress testing method outperforms standard scenario analysis
and Principal Component Analysis benchmarks through historical backtests
spanning three major financial crises, achieving reductions in mean absolute
error of up to 55% and 39% for scenario-based portfolio return prediction,
respectively.

</details>


### [230] [A Principled Path to Fitted Distributional Evaluation](https://arxiv.org/abs/2506.20048)
*Sungee Hong,Jiayi Wang,Zhengling Qi,Raymond Ka Wai Wong*

Main category: stat.ML

TL;DR: The paper introduces Fitted Distributional Evaluation (FDE), extending fitted-Q evaluation methods to estimate return distributions for target policies using offline data in reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: Current methods lack a unified framework for designing distributional off-policy evaluation in reinforcement learning.

Method: Guiding principles were proposed for theoretically grounded FDE methods. New methods were developed, backed by convergence analysis and theoretical justification, even in non-tabular settings.

Result: Extensive experiments (e.g., linear quadratic regulators and Atari games) indicate the FDE methods outperform existing techniques.

Conclusion: FDE methods offer superior and theoretically grounded approaches for distributional off-policy evaluation in reinforcement learning.

Abstract: In reinforcement learning, distributional off-policy evaluation (OPE) focuses
on estimating the return distribution of a target policy using offline data
collected under a different policy. This work focuses on extending the widely
used fitted-Q evaluation -- developed for expectation-based reinforcement
learning -- to the distributional OPE setting. We refer to this extension as
fitted distributional evaluation (FDE). While only a few related approaches
exist, there remains no unified framework for designing FDE methods. To fill
this gap, we present a set of guiding principles for constructing theoretically
grounded FDE methods. Building on these principles, we develop several new FDE
methods with convergence analysis and provide theoretical justification for
existing methods, even in non-tabular environments. Extensive experiments,
including simulations on linear quadratic regulators and Atari games,
demonstrate the superior performance of the FDE methods.

</details>


### [231] [Extracting Interpretable Models from Tree Ensembles: Computational and Statistical Perspectives](https://arxiv.org/abs/2506.20114)
*Brian Liu,Rahul Mazumder,Peter Radchenko*

Main category: stat.ML

TL;DR: The paper introduces an enhanced estimator for extracting decision rules from tree ensembles, aiming for both accuracy and interpretability.


<details>
  <summary>Details</summary>
Motivation: Tree ensembles, while accurate in predictions, lack interpretability and fail to reveal useful data relationships.

Method: The authors propose an estimator with controlled rule extraction parameters and tailored algorithms for optimization and regularization path computation.

Result: The estimator achieves competitive performance compared to a theoretical oracle and demonstrates superiority in rule extraction over existing methods.

Conclusion: The proposed estimator balances accuracy and interpretability, offering significant improvements over current approaches in extracting actionable insights from tree ensembles.

Abstract: Tree ensembles are non-parametric methods widely recognized for their
accuracy and ability to capture complex interactions. While these models excel
at prediction, they are difficult to interpret and may fail to uncover useful
relationships in the data. We propose an estimator to extract compact sets of
decision rules from tree ensembles. The extracted models are accurate and can
be manually examined to reveal relationships between the predictors and the
response. A key novelty of our estimator is the flexibility to jointly control
the number of rules extracted and the interaction depth of each rule, which
improves accuracy. We develop a tailored exact algorithm to efficiently solve
optimization problems underlying our estimator and an approximate algorithm for
computing regularization paths, sequences of solutions that correspond to
varying model sizes. We also establish novel non-asymptotic prediction error
bounds for our proposed approach, comparing it to an oracle that chooses the
best data-dependent linear combination of the rules in the ensemble subject to
the same complexity constraint as our estimator. The bounds illustrate that the
large-sample predictive performance of our estimator is on par with that of the
oracle. Through experiments, we demonstrate that our estimator outperforms
existing algorithms for rule extraction.

</details>


### [232] [Valid Selection among Conformal Sets](https://arxiv.org/abs/2506.20173)
*Mahmoud Hegazy,Liviu Aolaritei,Michael I. Jordan,Aymeric Dieuleveut*

Main category: stat.ML

TL;DR: The paper introduces a stability-based approach to ensure coverage guarantees in conformal prediction while allowing selection of desirable sets.


<details>
  <summary>Details</summary>
Motivation: Selecting the smallest prediction set in conformal prediction often invalidates the coverage guarantees, creating a need for a robust method.

Method: The authors propose a stability-based approach that maintains coverage guarantees even when selecting among multiple conformal prediction sets; extensions to online settings and refinements for structured scenarios are included.

Result: Experiments demonstrate the effectiveness of the stability-based approach in ensuring coverage guarantees while optimizing set selection.

Conclusion: The stability-based approach successfully addresses coverage guarantee concerns when choosing conformal prediction sets, with potential applications in various structured and online scenarios.

Abstract: Conformal prediction offers a distribution-free framework for constructing
prediction sets with coverage guarantees. In practice, multiple valid conformal
prediction sets may be available, arising from different models or
methodologies. However, selecting the most desirable set, such as the smallest,
can invalidate the coverage guarantees. To address this challenge, we propose a
stability-based approach that ensures coverage for the selected prediction set.
We extend our results to the online conformal setting, propose several
refinements in settings where additional structure is available, and
demonstrate its effectiveness through experiments.

</details>


### [233] [POLAR: A Pessimistic Model-based Policy Learning Algorithm for Dynamic Treatment Regimes](https://arxiv.org/abs/2506.20406)
*Ruijia Zhang,Zhengling Qi,Yue Wu,Xiangyu Zhang,Yanxun Xu*

Main category: stat.ML

TL;DR: The paper introduces POLAR, a pessimistic model-based policy learning algorithm for optimizing dynamic treatment regimes (DTRs) offline. Unlike existing methods, it provides statistical guarantees, avoids computationally intensive procedures, and significantly outperforms alternatives.


<details>
  <summary>Details</summary>
Motivation: The study aims to address challenges in DTRs for sequential decision-making where existing methods lack robustness under partial data and fail to provide statistical guarantees while being computationally demanding.

Method: POLAR estimates transition dynamics using offline data, quantifies uncertainty by penalizing high-uncertainty actions in the reward function, and directly targets policy suboptimality without computationally intensive optimization.

Result: POLAR achieves better performance compared to state-of-the-art methods on synthetic data and the MIMIC-III dataset with guarantees on finite-sample bounds and suboptimality.

Conclusion: POLAR sets a new standard for DTR optimization by combining theoretical guarantees, computational efficiency, and superior empirical results, making it a robust solution for sequential decision-making tasks.

Abstract: Dynamic treatment regimes (DTRs) provide a principled framework for
optimizing sequential decision-making in domains where decisions must adapt
over time in response to individual trajectories, such as healthcare,
education, and digital interventions. However, existing statistical methods
often rely on strong positivity assumptions and lack robustness under partial
data coverage, while offline reinforcement learning approaches typically focus
on average training performance, lack statistical guarantees, and require
solving complex optimization problems. To address these challenges, we propose
POLAR, a novel pessimistic model-based policy learning algorithm for offline
DTR optimization. POLAR estimates the transition dynamics from offline data and
quantifies uncertainty for each history-action pair. A pessimistic penalty is
then incorporated into the reward function to discourage actions with high
uncertainty. Unlike many existing methods that focus on average training
performance, POLAR directly targets the suboptimality of the final learned
policy and offers theoretical guarantees, without relying on computationally
intensive minimax or constrained optimization procedures. To the best of our
knowledge, POLAR is the first model-based DTR method to provide both
statistical and computational guarantees, including finite-sample bounds on
policy suboptimality. Empirical results on both synthetic data and the
MIMIC-III dataset demonstrate that POLAR outperforms state-of-the-art methods
and yields near-optimal, history-aware treatment strategies.

</details>


### [234] [Scalable Subset Selection in Linear Mixed Models](https://arxiv.org/abs/2506.20425)
*Ryan Thompson,Matt P. Wand,Joanna J. J. Wang*

Main category: stat.ML

TL;DR: The paper introduces an efficient $\ell_0$ regularized sparse learning method for Linear Mixed Models (LMMs), allowing subset selection with thousands of predictors.


<details>
  <summary>Details</summary>
Motivation: Existing methods for sparse learning in LMMs struggle with scalability, especially for datasets with thousands of predictors, hindering their applicability to modern wide data.

Method: The authors propose a computational algorithm based on coordinate descent for subset selection in LMMs and introduce a local search technique for navigating the nonconvex optimization space.

Result: The proposed method scales to thousands of predictors within seconds to minutes, demonstrating its computational efficiency and achieving excellent statistical performance in experiments and real datasets.

Conclusion: The paper bridges the gap between sparse learning methods for LMMs and linear models, offering scalable solutions for heterogeneous data analysis with robust theoretical guarantees.

Abstract: Linear mixed models (LMMs), which incorporate fixed and random effects, are
key tools for analyzing heterogeneous data, such as in personalized medicine or
adaptive marketing. Nowadays, this type of data is increasingly wide, sometimes
containing thousands of candidate predictors, necessitating sparsity for
prediction and interpretation. However, existing sparse learning methods for
LMMs do not scale well beyond tens or hundreds of predictors, leaving a large
gap compared with sparse methods for linear models, which ignore random
effects. This paper closes the gap with a new $\ell_0$ regularized method for
LMM subset selection that can run on datasets containing thousands of
predictors in seconds to minutes. On the computational front, we develop a
coordinate descent algorithm as our main workhorse and provide a guarantee of
its convergence. We also develop a local search algorithm to help traverse the
nonconvex optimization surface. Both algorithms readily extend to subset
selection in generalized LMMs via a penalized quasi-likelihood approximation.
On the statistical front, we provide a finite-sample bound on the
Kullback-Leibler divergence of the new method. We then demonstrate its
excellent performance in synthetic experiments and illustrate its utility on
two datasets from biology and journalism.

</details>


### [235] [Global Convergence of Iteratively Reweighted Least Squares for Robust Subspace Recovery](https://arxiv.org/abs/2506.20533)
*Gilad Lerman,Kang Li,Tyler Maunu,Teng Zhang*

Main category: stat.ML

TL;DR: This study proves that a variant of IRLS with dynamic smoothing converges to the underlying subspace linearly, providing global convergence guarantees.


<details>
  <summary>Details</summary>
Motivation: To address the gap in theoretical understanding of IRLS-based robust subspace estimation while highlighting its practical benefits.

Method: Used a variant of IRLS with dynamic smoothing regularization and extended deterministic guarantees to affine subspace estimation.

Result: Demonstrated linear convergence of the IRLS variant and presented practical advantages through low-dimensional neural network training.

Conclusion: The first global convergence guarantee for IRLS in robust subspace recovery is established, contributing to the broader application of nonconvex IRLS approaches.

Abstract: Robust subspace estimation is fundamental to many machine learning and data
analysis tasks. Iteratively Reweighted Least Squares (IRLS) is an elegant and
empirically effective approach to this problem, yet its theoretical properties
remain poorly understood. This paper establishes that, under deterministic
conditions, a variant of IRLS with dynamic smoothing regularization converges
linearly to the underlying subspace from any initialization. We extend these
guarantees to affine subspace estimation, a setting that lacks prior recovery
theory. Additionally, we illustrate the practical benefits of IRLS through an
application to low-dimensional neural network training. Our results provide the
first global convergence guarantees for IRLS in robust subspace recovery and,
more broadly, for nonconvex IRLS on a Riemannian manifold.

</details>


### [236] [LARP: Learner-Agnostic Robust Data Prefiltering](https://arxiv.org/abs/2506.20573)
*Kristian Minchev,Dimitar Iliev Dimitrov,Nikola Konstantinov*

Main category: stat.ML

TL;DR: The paper investigates robust, learner-agnostic data prefiltering methods (LARP) to handle low-quality or contaminated public datasets, focusing on minimizing worst-case loss across pre-specified learners and analyzing trade-offs between utility drops and costs.


<details>
  <summary>Details</summary>
Motivation: Public datasets often contain contaminated data, which affects the accuracy of machine learning methods. The paper seeks to address the need for principled prefiltering methods to robustly protect multiple downstream learners.

Method: The authors formalize the Learner-Agnostic Robust data Prefiltering (LARP) problem, analyze prefiltering procedures theoretically (e.g., with Huber estimators under contamination models), and conduct experiments to assess the utility trade-offs in large datasets.

Result: Results show that LARP can lead to utility loss compared to learner-specific prefiltering, which was supported by extensive experiments on real-world datasets. A game-theoretic framework was also introduced to evaluate its cost-effectiveness.

Conclusion: LARP provides an effective robust prefiltering approach for large datasets, but balancing utility loss and cost remains a key challenge. Insights from the analysis can guide dataset cleaning strategies for diverse learners.

Abstract: The widespread availability of large public datasets is a key factor behind
the recent successes of statistical inference and machine learning methods.
However, these datasets often contain some low-quality or contaminated data, to
which many learning procedures are sensitive. Therefore, the question of
whether and how public datasets should be prefiltered to facilitate accurate
downstream learning arises. On a technical level this requires the construction
of principled data prefiltering methods which are learner-agnostic robust, in
the sense of provably protecting a set of pre-specified downstream learners
from corrupted data. In this work, we formalize the problem of Learner-Agnostic
Robust data Prefiltering (LARP), which aims at finding prefiltering procedures
that minimize a worst-case loss over a pre-specified set of learners. We first
instantiate our framework in the context of scalar mean estimation with Huber
estimators under the Huber data contamination model. We provide a hardness
result on a specific problem instance and analyze several natural prefiltering
procedures. Our theoretical results indicate that performing LARP on a
heterogeneous set of learners leads to some loss in model performance compared
to the alternative of prefiltering data for each learner/use-case individually.
We explore the resulting utility loss and its dependence on the problem
parameters via extensive experiments on real-world image and tabular data,
observing statistically significant reduction in utility. Finally, we model the
trade-off between the utility drop and the cost of repeated (learner-specific)
prefiltering within a game-theoretic framework and showcase benefits of LARP
for large datasets.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [237] [Exploration-Exploitation Tradeoff in Universal Lossy Compression](https://arxiv.org/abs/2506.20261)
*Nir Weinberger,Ram Zamir*

Main category: cs.IT

TL;DR: This paper analyzes sequential lossy compression framed as a multi-armed bandit (MAB) problem, examining exploration-exploitation trade-offs and proposing robust algorithms to enhance performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve sequential lossy compression methods by addressing their limitations, especially in terms of robustness and performance for short data blocks, and to integrate MAB techniques for better adaptability.

Method: The authors reframe lossy compression as a multi-armed bandit problem, analyze existing 'natural type selection' schemes, and devise new robust MAB algorithms optimized for reconstruction and adaptability across all block lengths.

Result: They successfully identify shortcomings in the existing approaches and propose algorithms that are robust and capable of functioning effectively regardless of block length.

Conclusion: The proposed reconstruction-directed MAB algorithms overcome limitations of previous approaches, offering a practical and scalable solution to sequential lossy compression.

Abstract: Universal compression can learn the source and adapt to it either in a batch
mode (forward adaptation), or in a sequential mode (backward adaptation). We
recast the sequential mode as a multi-armed bandit problem, a fundamental model
in reinforcement-learning, and study the trade-off between exploration and
exploitation in the lossy compression case. We show that a previously proposed
"natural type selection" scheme can be cast as a reconstruction-directed MAB
algorithm, for sequential lossy compression, and explain its limitations in
terms of robustness and short-block performance. We then derive and analyze
robust cost-directed MAB algorithms, which work at any block length.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [238] [Exploring the Capabilities of the Frontier Large Language Models for Nuclear Energy Research](https://arxiv.org/abs/2506.19863)
*Ahmed Almeldein,Mohammed Alnaggar,Rick Archibald,Tom Beck,Arpan Biswas,Rike Bostelmann,Wes Brewer,Chris Bryan,Christopher Calle,Cihangir Celik,Rajni Chahal,Jong Youl Choi,Arindam Chowdhury,Mark Cianciosa,Franklin Curtis,Gregory Davidson,Sebastian De Pascuale,Lisa Fassino,Ana Gainaru,Yashika Ghai,Luke Gibson,Qian Gong,Christopher Greulich,Scott Greenwood,Cory Hauck,Ehab Hassan,Rinkle Juneja,Soyoung Kang,Scott Klasky,Atul Kumar,Vineet Kumar,Paul Laiu,Calvin Lear,Yan-Ru Lin,Jono McConnell,Furkan Oz,Anant Raj,Pradeep Ramuhalli,Marie Romedenne,Samantha Sabatino,José Salcedo-Pérez,Nathan D. See,Arpan Sircar,Punam Thankur,Tim Younkin,Xiao-Ying Yu,Prashant Jain,Tom Evans,Prasanna Balaprakash*

Main category: physics.comp-ph

TL;DR: This paper explores the potential of Large Language Models (LLMs) to advance nuclear energy research, identifying both strengths in exploration and synthesis and limitations in specialized domains.


<details>
  <summary>Details</summary>
Motivation: The study aims to evaluate how Large Language Models (LLMs) can contribute to accelerating nuclear energy research, specifically in fusion and fission domains.

Method: Fourteen interdisciplinary teams used LLMs like ChatGPT, Gemini, and Claude in structured workflows over a day to address diverse challenges, combining prompt engineering, deep research, and iterative refinement.

Result: LLMs excelled in early-stage research, literature synthesis, and workflow design but faced challenges in novel material designs, advanced code generation, and domain-specific details, requiring expert intervention.

Conclusion: AI, when used as a complementary tool rather than a replacement, has the potential to accelerate nuclear energy research by enabling rapid iteration, interdisciplinary collaboration, and workflow automation, given the development of curated datasets and specialized models.

Abstract: The AI for Nuclear Energy workshop at Oak Ridge National Laboratory evaluated
the potential of Large Language Models (LLMs) to accelerate fusion and fission
research. Fourteen interdisciplinary teams explored diverse nuclear science
challenges using ChatGPT, Gemini, Claude, and other AI models over a single
day. Applications ranged from developing foundation models for fusion reactor
control to automating Monte Carlo simulations, predicting material degradation,
and designing experimental programs for advanced reactors. Teams employed
structured workflows combining prompt engineering, deep research capabilities,
and iterative refinement to generate hypotheses, prototype code, and research
strategies. Key findings demonstrate that LLMs excel at early-stage
exploration, literature synthesis, and workflow design, successfully
identifying research gaps and generating plausible experimental frameworks.
However, significant limitations emerged, including difficulties with novel
materials designs, advanced code generation for modeling and simulation, and
domain-specific details requiring expert validation. The successful outcomes
resulted from expert-driven prompt engineering and treating AI as a
complementary tool rather than a replacement for physics-based methods. The
workshop validated AI's potential to accelerate nuclear energy research through
rapid iteration and cross-disciplinary synthesis while highlighting the need
for curated nuclear-specific datasets, workflow automation, and specialized
model development. These results provide a roadmap for integrating AI tools
into nuclear science workflows, potentially reducing development cycles for
safer, more efficient nuclear energy systems while maintaining rigorous
scientific standards.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [239] [MILAAP: Mobile Link Allocation via Attention-based Prediction](https://arxiv.org/abs/2506.19947)
*Yung-Fu Chen,Anish Arora*

Main category: cs.NI

TL;DR: This paper proposes MiLAAP, a machine learning framework using attention mechanisms to predict channel occupancy state and node mobility patterns in dynamic wireless networks without requiring state sharing.


<details>
  <summary>Details</summary>
Motivation: Wireless Channel Hopping systems need to maintain throughput efficiency despite interference and node mobility, but traditional approaches that share state information incur high communication overhead.

Method: MiLAAP employs self-attention mechanisms to enable nodes to locally and passively observe and predict channel occupancy and node mobility without communication overhead through learning temporal, spectral, and spatial patterns.

Result: MiLAAP achieves almost 100% accuracy in predicting channel occupancy state and provides zero-shot generalizability for various channel sequences and node mobility patterns.

Conclusion: The MiLAAP framework effectively reduces communication overhead while enabling accurate and scalable channel state prediction and node mobility understanding in dynamic networks.

Abstract: Channel hopping (CS) communication systems must adapt to interference changes
in the wireless network and to node mobility for maintaining throughput
efficiency. Optimal scheduling requires up-to-date network state information
(i.e., of channel occupancy) to select non-overlapping channels for links in
interference regions. However, state sharing among nodes introduces significant
communication overhead, especially as network size or node mobility scale,
thereby decreasing throughput efficiency of already capacity-limited networks.
In this paper, we eschew state sharing while adapting the CS schedule based on
a learning-based channel occupancy prediction. We propose the MiLAAP
attention-based prediction framework for machine learning models of spectral,
spatial, and temporal dependencies among network nodes. MiLAAP uses a
self-attention mechanism that lets each node capture the temporospectral CS
pattern in its interference region and accordingly predict the channel
occupancy state within that region. Notably, the prediction relies only on
locally and passively observed channel activities, and thus introduces no
communication overhead. To deal with node mobility, MiLAAP also uses a
multi-head self-attention mechanism that lets each node locally capture the
spatiotemporal dependencies on other network nodes that can interfere with it
and accordingly predict the motion trajectory of those nodes. Detecting nodes
that enter or move outside the interference region is used to further improve
the prediction accuracy of channel occupancy. We show that for dynamic networks
that use local CS sequences to support relatively long-lived flow traffics, the
channel state prediction accuracy of MiLAAP is remarkably ~100% across
different node mobility patterns and it achieves zero-shot generalizability
across different periods of CS sequences.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [240] [Task Allocation of UAVs for Monitoring Missions via Hardware-in-the-Loop Simulation and Experimental Validation](https://arxiv.org/abs/2506.20626)
*Hamza Chakraa,François Guérin,Edouard Leclercq,Dimitri Lefebvre*

Main category: eess.SY

TL;DR: This paper addresses optimizing UAV task allocation using Genetic Algorithms and a 2-Opt local search, showing strong real-world applicability.


<details>
  <summary>Details</summary>
Motivation: To optimize task allocation for UAVs in industrial monitoring missions, ensuring efficiency and realistic applicability.

Method: Integration of Genetic Algorithms with a 2-Opt local search, validated through experiments in an industrial zone and using a Hardware-in-the-loop simulator.

Result: The optimization's cost function closely correlates with real-world data on battery use and flight time, demonstrating its practicality.

Conclusion: The proposed approach effectively optimizes UAV task allocation in industrial settings with validated real-world efficiency.

Abstract: This study addresses the optimisation of task allocation for Unmanned Aerial
Vehicles (UAVs) within industrial monitoring missions. The proposed methodology
integrates a Genetic Algorithms (GA) with a 2-Opt local search technique to
obtain a high-quality solution. Our approach was experimentally validated in an
industrial zone to demonstrate its efficacy in real-world scenarios. Also, a
Hardware-in-the-loop (HIL) simulator for the UAVs team is introduced. Moreover,
insights about the correlation between the theoretical cost function and the
actual battery consumption and time of flight are deeply analysed. Results show
that the considered costs for the optimisation part of the problem closely
correlate with real-world data, confirming the practicality of the proposed
approach.

</details>


### [241] [Recurrent neural network-based robust control systems with closed-loop regional incremental ISS and application to MPC design](https://arxiv.org/abs/2506.20334)
*Daniele Ravasio,Marcello Farina,Alessio La Bella,Andrea Ballarino*

Main category: eess.SY

TL;DR: The paper focuses on designing output-feedback schemes for recurrent neural networks using linear matrix inequalities and introduces tube-based NMPC to enhance robustness and region of attraction.


<details>
  <summary>Details</summary>
Motivation: To develop output-feedback control schemes for recurrent neural networks that ensure robust tracking, input-to-state stability, and address limitations of regional incremental ISS.

Method: The authors propose the use of linear matrix inequalities, along with an observer and a static state-feedback controller, while complementing it with a tube-based nonlinear model predictive controller to leverage regional incremental ISS.

Result: The study presents theoretically proven tracking capabilities, robustness to disturbances, and validated the approach through numerical simulations on a pH-neutralisation process benchmark.

Conclusion: The proposed framework improves robustness and facilitates wider regions of attraction, offering convergence guarantees and practical effectiveness in simulated systems.

Abstract: This paper investigates the design of output-feedback schemes for systems
described by a class of recurrent neural networks. We propose a procedure based
on linear matrix inequalities for designing an observer and a static
state-feedback controller. The algorithm leverages global and regional
incremental input-to-state stability (incremental ISS) and enables the tracking
of constant setpoints, ensuring robustness to disturbances and state estimation
uncertainty. To address the potential limitations of regional incremental ISS,
we introduce an alternative scheme in which the static law is replaced with a
tube-based nonlinear model predictive controller (NMPC) that exploits regional
incremental ISS properties. We show that these conditions enable the
formulation of a robust NMPC law with guarantees of convergence and recursive
feasibility, leading to an enlarged region of attraction. Theoretical results
are validated through numerical simulations on the pH-neutralisation process
benchmark, demonstrating the effectiveness of the proposed schemes.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [242] [CoVE: Compressed Vocabulary Expansion Makes Better LLM-based Recommender Systems](https://arxiv.org/abs/2506.19993)
*Haochen Zhang,Tianyi Zhang,Junze Yin,Oren Gal,Anshumali Shrivastava,Vladimir Braverman*

Main category: cs.IR

TL;DR: The paper introduces CoVE, a system that enhances recommendation tasks using LLMs by assigning unique IDs and compressing the embedding layer.


<details>
  <summary>Details</summary>
Motivation: Existing methods for aligning LLMs with recommendation tasks don't fully exploit their sequential processing capabilities.

Method: The CoVE framework assigns unique IDs to each item in an expanded vocabulary and compresses the embedding layer for practical use.

Result: CoVE significantly improves recommendation performance, as shown through experiments on multiple datasets.

Conclusion: CoVE leverages the sequential data processing of LLMs and is practical for large-scale applications, outperforming previous approaches.

Abstract: Recommender systems play a pivotal role in providing relevant content to
users. With the rapid development of large language models (LLMs), researchers
have begun utilizing LLMs to build more powerful recommender systems. However,
existing approaches that focus on aligning LLMs with recommendation tasks do
not fully leverage their sequential information processing capabilities,
leading to suboptimal performance.
  In this paper, we propose a novel system called compressed vocabulary
expansion (CoVE). In CoVE, each item is assigned a unique ID within the
expanded vocabulary. Our framework effectively capitalizes on sequence
understanding abilities of LLMs, significantly enhancing their performance on
recommendation tasks. Additionally, we compress the embedding layer, making
CoVE practical for large-scale industrial applications. The effectiveness and
performance of CoVE are demonstrated through comprehensive experiments on
multiple recommendation datasets and comparisons with prior works. Our code can
be found at https://github.com/HaochenZhang717/CoVE-official-Repo.

</details>


### [243] [Multimodal Information Retrieval for Open World with Edit Distance Weak Supervision](https://arxiv.org/abs/2506.20070)
*KMA Solaiman,Bharat Bhargava*

Main category: cs.IR

TL;DR: This paper introduces FemmIR, a novel framework enabling multimodal retrieval without annotations, leveraging pretrained encoders and weak supervision via graph edit distance.


<details>
  <summary>Details</summary>
Motivation: To address the annotation overhead in supervised multi-modal retrieval models and enable effective retrieval with scarce annotations and no fine-tuning.

Method: The proposed framework, FemmIR, uses weak supervision via graph edit distance to identify multimodal relevance, avoiding similarity labels and integrating pretrained language and vision encoders.

Result: FemmIR performs comparably to existing systems in retrieving both exact and approximate matches in multimodal queries, validated through experiments on the MuQNOL dataset.

Conclusion: FemmIR simplifies multimodal retrieval by avoiding fine-tuning and supervised annotation, offering an efficient and reusable retrieval framework suitable for real-world applications.

Abstract: Existing multi-media retrieval models either rely on creating a common
subspace with modality-specific representation models or require schema mapping
among modalities to measure similarities among multi-media data. Our goal is to
avoid the annotation overhead incurred from considering retrieval as a
supervised classification task and re-use the pretrained encoders in large
language models and vision tasks. We propose "FemmIR", a framework to retrieve
multimodal results relevant to information needs expressed with multimodal
queries by example without any similarity label. Such identification is
necessary for real-world applications where data annotations are scarce and
satisfactory performance is required without fine-tuning with a common
framework across applications. We curate a new dataset called MuQNOL for
benchmarking progress on this task. Our technique is based on weak supervision
introduced through edit distance between samples: graph edit distance can be
modified to consider the cost of replacing a data sample in terms of its
properties, and relevance can be measured through the implicit signal from the
amount of edit cost among the objects. Unlike metric learning or encoding
networks, FemmIR re-uses the high-level properties and maintains the property
value and relationship constraints with a multi-level interaction score between
data samples and the query example provided by the user. We empirically
evaluate FemmIR on a missing person use case with MuQNOL. FemmIR performs
comparably to similar retrieval systems in delivering on-demand retrieval
results with exact and approximate similarities while using the existing
property identifiers in the system.

</details>


### [244] [Unidentified and Confounded? Understanding Two-Tower Models for Unbiased Learning to Rank](https://arxiv.org/abs/2506.20501)
*Philipp Hager,Onno Zoeter,Maarten de Rijke*

Main category: cs.IR

TL;DR: This paper addresses performance issues in additive two-tower learning-to-rank models trained on biased clicks, examining confounding factors and proposing solutions.


<details>
  <summary>Details</summary>
Motivation: To understand and mitigate performance degradation in two-tower learning-to-rank models due to biased training data from logging policies.

Method: The paper provides a theoretical analysis of model identifiability and studies how logging policies affect biases, suggesting a sample weighting technique to improve results.

Result: The analysis reveals that logging policies amplify biases under imperfect user behavior modeling and identifies conditions necessary for parameter recovery.

Conclusion: Sample weighting can mitigate bias effects, offering practical ways to enhance two-tower models' performance in handling biased user feedback.

Abstract: Additive two-tower models are popular learning-to-rank methods for handling
biased user feedback in industry settings. Recent studies, however, report a
concerning phenomenon: training two-tower models on clicks collected by
well-performing production systems leads to decreased ranking performance. This
paper investigates two recent explanations for this observation: confounding
effects from logging policies and model identifiability issues. We
theoretically analyze the identifiability conditions of two-tower models,
showing that either document swaps across positions or overlapping feature
distributions are required to recover model parameters from clicks. We also
investigate the effect of logging policies on two-tower models, finding that
they introduce no bias when models perfectly capture user behavior. However,
logging policies can amplify biases when models imperfectly capture user
behavior, particularly when prediction errors correlate with document placement
across positions. We propose a sample weighting technique to mitigate these
effects and provide actionable insights for researchers and practitioners using
two-tower models.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [245] [Piecewise Linear Approximation in Learned Index Structures: Theoretical and Empirical Analysis](https://arxiv.org/abs/2506.20139)
*Jiayong Qin,Xianyu Zhu,Qiyu Liu,Guangyi Zhang,Zhigang Cai,Jianwei Liao,Sha Hu,Jingshu Peng,Yingxia Shao,Lei Chen*

Main category: cs.DB

TL;DR: The paper focuses on error-bounded Piecewise Linear Approximation (ε-PLA), widely used in learned indexes, analyzing it both theoretically and empirically, while offering significant insights and benchmarks for its implementation.


<details>
  <summary>Details</summary>
Motivation: As machine learning models become integrated into conventional indexing structures for databases, understanding error-bounded PLA fitting algorithms is crucial for improving the performance and design of learned indexes.

Method: The authors establish an improved theoretical lower bound for segment coverage in ε-PLA fitting. Then, they perform a comprehensive benchmark analysis of existing ε-PLA algorithms across various learned data structures focusing on trade-offs.

Result: The research introduces an improved theoretical bound, compares state-of-the-art ε-PLA algorithms, and identifies trade-offs among model accuracy, size, and query performance for learned indexes.

Conclusion: This study not only enhances the theoretical understanding of ε-PLA but also provides practical insights and guidelines for developing more effective learned indexing structures.

Abstract: A growing trend in the database and system communities is to augment
conventional index structures, such as B+-trees, with machine learning (ML)
models. Among these, error-bounded Piecewise Linear Approximation
($\epsilon$-PLA) has emerged as a popular choice due to its simplicity and
effectiveness. Despite its central role in many learned indexes, the design and
analysis of $\epsilon$-PLA fitting algorithms remain underexplored. In this
paper, we revisit $\epsilon$-PLA from both theoretical and empirical
perspectives, with a focus on its application in learned index structures. We
first establish a fundamentally improved lower bound of $\Omega(\kappa \cdot
\epsilon^2)$ on the expected segment coverage for existing $\epsilon$-PLA
fitting algorithms, where $\kappa$ is a data-dependent constant. We then
present a comprehensive benchmark of state-of-the-art $\epsilon$-PLA algorithms
when used in different learned data structures. Our results highlight key
trade-offs among model accuracy, model size, and query performance, providing
actionable guidelines for the principled design of future learned data
structures.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [246] [RepuNet: A Reputation System for Mitigating Malicious Clients in DFL](https://arxiv.org/abs/2506.19892)
*Isaac Marroqui Penalva,Enrique Tomás Martínez Beltrán,Manuel Gil Pérez,Alberto Huertas Celdrán*

Main category: cs.CR

TL;DR: RepuNet, a reputation system for decentralized federated learning, dynamically evaluates and mitigates threats posed by malicious nodes. It delivers high detection accuracy across various attack scenarios.


<details>
  <summary>Details</summary>
Motivation: To address vulnerabilities in decentralized federated learning systems caused by malicious actions like model poisoning and delay attacks, and to improve security without relying on rigid configurations or computationally costly infrastructures.

Method: This paper introduces RepuNet, a system that tracks node behavior using metrics like model similarity, parameter changes, message latency, and communication volume. Nodes' reputation scores influence their weight in model aggregation.

Result: RepuNet performs robustly against malicious nodes in experiments, achieving high F1 scores for MNIST (95%) and CIFAR-10 (76%) datasets under diverse attack scenarios.

Conclusion: RepuNet enhances security in decentralized federated learning by detecting and mitigating threats effectively, demonstrating adaptability and scalability in experimental settings.

Abstract: Decentralized Federated Learning (DFL) enables nodes to collaboratively train
models without a central server, introducing new vulnerabilities since each
node independently selects peers for model aggregation. Malicious nodes may
exploit this autonomy by sending corrupted models (model poisoning), delaying
model submissions (delay attack), or flooding the network with excessive
messages, negatively affecting system performance. Existing solutions often
depend on rigid configurations or additional infrastructures such as
blockchain, leading to computational overhead, scalability issues, or limited
adaptability. To overcome these limitations, this paper proposes RepuNet, a
decentralized reputation system that categorizes threats in DFL and dynamically
evaluates node behavior using metrics like model similarity, parameter changes,
message latency, and communication volume. Nodes' influence in model
aggregation is adjusted based on their reputation scores. RepuNet was
integrated into the Nebula DFL platform and experimentally evaluated with MNIST
and CIFAR-10 datasets under non-IID distributions, using federations of up to
25 nodes in both fully connected and random topologies. Different attack
intensities, frequencies, and activation intervals were tested. Results
demonstrated that RepuNet effectively detects and mitigates malicious behavior,
achieving F1 scores above 95% for MNIST scenarios and approximately 76% for
CIFAR-10 cases. These outcomes highlight RepuNet's adaptability, robustness,
and practical potential for mitigating threats in decentralized federated
learning environments.

</details>


### [247] [Quantum-Resistant Domain Name System: A Comprehensive System-Level Study](https://arxiv.org/abs/2506.19943)
*Juyoul Lee,Sanzida Hoque,Abdullah Aydeger,Engin Zeydan*

Main category: cs.CR

TL;DR: The paper examines the post-quantum security of DNS, proposing a framework (PQC-DNS) to secure DNS mechanisms like DNSSEC, D0T, and DoH, using post-quantum cryptographic techniques.


<details>
  <summary>Details</summary>
Motivation: To tackle the vulnerabilities in DNS protocols caused by quantum computing threats, aiming to ensure DNS confidentiality, authenticity, and integrity in a post-quantum era.

Method: Developed a framework (PQC-DNS) using Open Quantum Safe libraries, integrating lattice- and hash-based cryptographic primitives into DNS implementations, and tested using a containerized testbed.

Result: Lattice-based primitives like MLKEM and Falcon showed practical latency/resource profiles, whereas hash-based schemes such as SPHINCS+ led to high message sizes and processing overhead. They also examined security risks such as downgrade attacks and fragmentation vulnerabilities.

Conclusion: The study provides a roadmap for deploying quantum-resilient DNS and addresses key trade-offs and vulnerabilities, contributing to the broader goal of securing Internet protocols for the post-quantum era.

Abstract: The Domain Name System (DNS) plays a foundational role in Internet
infrastructure, yet its core protocols remain vulnerable to compromise by
quantum adversaries. As cryptographically relevant quantum computers become a
realistic threat, ensuring DNS confidentiality, authenticity, and integrity in
the post-quantum era is imperative. In this paper, we present a comprehensive
system-level study of post-quantum DNS security across three widely deployed
mechanisms: DNSSEC, DNS-over-TLS (DoT), and DNS-over-HTTPS (DoH). We propose
Post-Quantum Cryptographic (PQC)-DNS, a unified framework for benchmarking DNS
security under legacy, post-quantum, and hybrid cryptographic configurations.
Our implementation leverages the Open Quantum Safe (OQS) libraries and
integrates lattice- and hash-based primitives into BIND9 and TLS 1.3 stacks. We
formalize performance and threat models and analyze the impact of post-quantum
key encapsulation and digital signatures on end-to-end DNS resolution.
Experimental results on a containerized testbed reveal that lattice-based
primitives such as Module-Lattice-Based Key-Encapsulation Mechanism (MLKEM) and
Falcon offer practical latency and resource profiles, while hash-based schemes
like SPHINCS+ significantly increase message sizes and processing overhead. We
also examine security implications including downgrade risks, fragmentation
vulnerabilities, and susceptibility to denial-of-service amplification. Our
findings inform practical guidance for deploying quantum-resilient DNS and
contribute to the broader effort of securing core Internet protocols for the
post-quantum future.

</details>


### [248] [Can One Safety Loop Guard Them All? Agentic Guard Rails for Federated Computing](https://arxiv.org/abs/2506.20000)
*Narasimha Raghavan Veeraragavan,Jan Franz Nygård*

Main category: cs.CR

TL;DR: Guardian-FC is a two-layer framework for privacy-preserving federated computing with a backend-neutral modular design and a safety enforcement mechanism.


<details>
  <summary>Details</summary>
Motivation: To unify and simplify safety enforcement across diverse privacy-preserving mechanisms in federated computing.

Method: Introduces a modular framework using a domain-specific language (DSL) and plug-ins for backend-neutral operations, enforced by an Agentic-AI control plane.

Result: Guardian-FC enables backend-agnostic safety and supports extensibility for various privacy mechanisms.

Conclusion: The framework provides consistent risk management, seamless integration of privacy backends, and invites further research for adaptive and robust guard-rails.

Abstract: We propose Guardian-FC, a novel two-layer framework for privacy preserving
federated computing that unifies safety enforcement across diverse privacy
preserving mechanisms, including cryptographic back-ends like fully homomorphic
encryption (FHE) and multiparty computation (MPC), as well as statistical
techniques such as differential privacy (DP). Guardian-FC decouples guard-rails
from privacy mechanisms by executing plug-ins (modular computation units),
written in a backend-neutral, domain-specific language (DSL) designed
specifically for federated computing workflows and interchangeable Execution
Providers (EPs), which implement DSL operations for various privacy back-ends.
An Agentic-AI control plane enforces a finite-state safety loop through signed
telemetry and commands, ensuring consistent risk management and auditability.
The manifest-centric design supports fail-fast job admission and seamless
extensibility to new privacy back-ends. We present qualitative scenarios
illustrating backend-agnostic safety and a formal model foundation for
verification. Finally, we outline a research agenda inviting the community to
advance adaptive guard-rail tuning, multi-backend composition, DSL
specification development, implementation, and compiler extensibility alongside
human-override usability.

</details>


### [249] [Secure Energy Transactions Using Blockchain Leveraging AI for Fraud Detection and Energy Market Stability](https://arxiv.org/abs/2506.19870)
*Md Asif Ul Hoq Khan,MD Zahedul Islam,Istiaq Ahmed,Md Masud Karim Rabbi,Farhana Rahman Anonna,MD Abdul Fahim Zeeshan,Mehedi Hasan Ridoy,Bivash Ranjan Chowdhury,Md Nazmul Shakir Rabbi,GM Alamin Sadnan*

Main category: cs.CR

TL;DR: This paper addresses safety and authenticity issues in decentralized U.S. energy markets using blockchain and AI technologies.


<details>
  <summary>Details</summary>
Motivation: The shift towards decentralized energy markets has created security and fraud risks, necessitating innovative solutions.

Method: The authors developed a system combining blockchain for security and AI for fraud detection, using a dataset of over 1.2 million simulated energy transactions.

Result: The proposed system integrates blockchain and AI layers and leverages machine learning for highly accurate fraud identification.

Conclusion: The study successfully demonstrates that blockchain and AI can synergize to enhance security and reliability in decentralized energy markets.

Abstract: Peer-to-peer trading and the move to decentralized grids have reshaped the
energy markets in the United States. Notwithstanding, such developments lead to
new challenges, mainly regarding the safety and authenticity of energy trade.
This study aimed to develop and build a secure, intelligent, and efficient
energy transaction system for the decentralized US energy market. This research
interlinks the technological prowess of blockchain and artificial intelligence
(AI) in a novel way to solve long-standing challenges in the distributed energy
market, specifically those of security, fraudulent behavior detection, and
market reliability. The dataset for this research is comprised of more than 1.2
million anonymized energy transaction records from a simulated peer-to-peer
(P2P) energy exchange network emulating real-life blockchain-based American
microgrids, including those tested by LO3 Energy and Grid+ Labs. Each record
contains detailed fields of transaction identifier, timestamp, energy volume
(kWh), transaction type (buy/sell), unit price, prosumer/consumer identifier
(hashed for privacy), smart meter readings, geolocation regions, and settlement
confirmation status. The dataset also includes system-calculated behavior
metrics of transaction rate, variability of energy production, and historical
pricing patterns. The system architecture proposed involves the integration of
two layers, namely a blockchain layer and artificial intelligence (AI) layer,
each playing a unique but complementary function in energy transaction securing
and market intelligence improvement. The machine learning models used in this
research were specifically chosen for their established high performance in
classification tasks, specifically in the identification of energy transaction
fraud in decentralized markets.

</details>


### [250] [An Attack Method for Medical Insurance Claim Fraud Detection based on Generative Adversarial Network](https://arxiv.org/abs/2506.19871)
*Yining Pang,Chenghan Li*

Main category: cs.CR

TL;DR: This paper presents a study on the vulnerability of insurance fraud detection systems to adversarial attacks, using a GAN-based approach to generate fraudulent claims with a high success rate.


<details>
  <summary>Details</summary>
Motivation: The motivation is to highlight vulnerabilities in insurance fraud detection systems, which, despite leveraging advanced AI and machine learning methods, lack standardized mechanisms to defend against adversarial threats.

Method: The authors applied a Generative Adversarial Network (GAN)-based approach to simulate adversarial attacks on insurance fraud detection systems, enabling the creation of fraudulent cases that evade detection.

Result: The study demonstrates that adversaries, even without access to training data or model specifics, can achieve a 99% attack success rate by subtly modifying insurance records and claims.

Conclusion: The findings emphasize the critical necessity of improving the robustness of fraud detection models to adversarial attacks to guarantee the stability and effectiveness of insurance systems.

Abstract: Insurance fraud detection represents a pivotal advancement in modern
insurance service, providing intelligent and digitalized monitoring to enhance
management and prevent fraud. It is crucial for ensuring the security and
efficiency of insurance systems. Although AI and machine learning algorithms
have demonstrated strong performance in detecting fraudulent claims, the
absence of standardized defense mechanisms renders current systems vulnerable
to emerging adversarial threats. In this paper, we propose a GAN-based approach
to conduct adversarial attacks on fraud detection systems. Our results indicate
that an attacker, without knowledge of the training data or internal model
details, can generate fraudulent cases that are classified as legitimate with a
99\% attack success rate (ASR). By subtly modifying real insurance records and
claims, adversaries can significantly increase the fraud risk, potentially
bypassing compromised detection systems. These findings underscore the urgent
need to enhance the robustness of insurance fraud detection models against
adversarial manipulation, thereby ensuring the stability and reliability of
different insurance systems.

</details>


### [251] [Towards Provable (In)Secure Model Weight Release Schemes](https://arxiv.org/abs/2506.19874)
*Xing Yang,Bingtao Wang,Yuhao Wang,Zimo Ji,Terry Jingchen Zhang,Wenyuan Jiang*

Main category: cs.CR

TL;DR: The paper introduces formal security definitions for secure weight release schemes, highlighting vulnerabilities in existing schemes like TaylorMLP.


<details>
  <summary>Details</summary>
Motivation: Current secure weight release schemes lack rigorous security foundations and only provide informal guarantees.

Method: Introduced formal security definitions inspired by cryptographic principles and applied them in analyzing TaylorMLP.

Result: Found vulnerabilities in TaylorMLP, revealing its inability to meet claimed security goals.

Conclusion: Advocates for more rigorous research methodologies in designing and evaluating secure weight release schemes for machine learning models.

Abstract: Recent secure weight release schemes claim to enable open-source model
distribution while protecting model ownership and preventing misuse. However,
these approaches lack rigorous security foundations and provide only informal
security guarantees. Inspired by established works in cryptography, we
formalize the security of weight release schemes by introducing several
concrete security definitions. We then demonstrate our definition's utility
through a case study of TaylorMLP, a prominent secure weight release scheme.
Our analysis reveals vulnerabilities that allow parameter extraction thus
showing that TaylorMLP fails to achieve its informal security goals. We hope
this work will advocate for rigorous research at the intersection of machine
learning and security communities and provide a blueprint for how future weight
release schemes should be designed and evaluated.

</details>


### [252] [Robust Anomaly Detection in Network Traffic: Evaluating Machine Learning Models on CICIDS2017](https://arxiv.org/abs/2506.19877)
*Zhaoyang Xu,Yunbo Liu*

Main category: cs.CR

TL;DR: The study compares four machine learning models (MLP, CNN, OCSVM, LOF) for intrusion detection under scenarios of detecting known attacks and unseen threats using the CICIDS2017 dataset.


<details>
  <summary>Details</summary>
Motivation: To determine the most suitable machine learning paradigms for building effective and generalizable intrusion detection systems.

Method: This study conducted a controlled comparison of four machine learning models (MLP, CNN, OCSVM, LOF) on the CICIDS2017 dataset and analyzed their performance in detecting known and novel attacks.

Result: MLP and CNN performed well for familiar attacks but failed on unseen threats. LOF showed high recall for unknown threats but with frequent false alarms. OCSVM balanced precision and recall across both scenarios.

Conclusion: The study offers practical insights into selecting intrusion detection system models, highlighting OCSVM's robust performance across dynamic networking environments.

Abstract: Identifying suitable machine learning paradigms for intrusion detection
remains critical for building effective and generalizable security solutions.
In this study, we present a controlled comparison of four representative models
- Multi-Layer Perceptron (MLP), 1D Convolutional Neural Network (CNN),
One-Class Support Vector Machine (OCSVM) and Local Outlier Factor (LOF) - on
the CICIDS2017 dataset under two scenarios: detecting known attack types and
generalizing to previously unseen threats. Our results show that supervised MLP
and CNN achieve near-perfect accuracy on familiar attacks but suffer drastic
recall drops on novel attacks. Unsupervised LOF attains moderate overall
accuracy and high recall on unknown threats at the cost of elevated false
alarms, while boundary-based OCSVM balances precision and recall best,
demonstrating robust detection across both scenarios. These findings offer
practical guidance for selecting IDS models in dynamic network environments.

</details>


### [253] [Retrieval-Confused Generation is a Good Defender for Privacy Violation Attack of Large Language Models](https://arxiv.org/abs/2506.19889)
*Wanli Peng,Xin Chen,Hang Fu,XinYu He,Xue Yiming,Juan Wen*

Main category: cs.CR

TL;DR: This paper addresses privacy violation attacks (PVA) on large language models (LLMs) by proposing a new defense paradigm called retrieval-confused generation (RCG), which covertly defends against such attacks by introducing disturbances and irrelevant retrieval strategies.


<details>
  <summary>Details</summary>
Motivation: The increasing use of large language models (LLMs) has led to concerns over personal data privacy due to privacy violation attacks (PVA). Existing defenses are either computationally expensive or insufficiently effective, necessitating innovative and covert defense mechanisms.

Method: The authors propose a novel defense paradigm called retrieval-confused generation (RCG). This involves (1) using a paraphrasing prompt to disturb the attack query database, (2) applying an irrelevant retrieval strategy to fetch misleading user data, and (3) forming defended queries that mislead attackers by responding with incorrect personal attributes.

Result: Experiments conducted on two datasets and eight popular LLMs demonstrate the efficacy and superiority of the RCG method in defending against privacy violation attacks.

Conclusion: The proposed RCG method is an efficient and covert approach to mitigating privacy violation attacks on LLMs. This paradigm provides a promising way to safeguard personal data while outperforming existing defensive techniques.

Abstract: Recent advances in large language models (LLMs) have made a profound impact
on our society and also raised new security concerns. Particularly, due to the
remarkable inference ability of LLMs, the privacy violation attack (PVA),
revealed by Staab et al., introduces serious personal privacy issues. Existing
defense methods mainly leverage LLMs to anonymize the input query, which
requires costly inference time and cannot gain satisfactory defense
performance. Moreover, directly rejecting the PVA query seems like an effective
defense method, while the defense method is exposed, promoting the evolution of
PVA. In this paper, we propose a novel defense paradigm based on
retrieval-confused generation (RCG) of LLMs, which can efficiently and covertly
defend the PVA. We first design a paraphrasing prompt to induce the LLM to
rewrite the "user comments" of the attack query to construct a disturbed
database. Then, we propose the most irrelevant retrieval strategy to retrieve
the desired user data from the disturbed database. Finally, the "data comments"
are replaced with the retrieved user data to form a defended query, leading to
responding to the adversary with some wrong personal attributes, i.e., the
attack fails. Extensive experiments are conducted on two datasets and eight
popular LLMs to comprehensively evaluate the feasibility and the superiority of
the proposed defense method.

</details>


### [254] [Blameless Users in a Clean Room: Defining Copyright Protection for Generative Models](https://arxiv.org/abs/2506.19881)
*Aloni Cohen*

Main category: cs.CR

TL;DR: The paper addresses the issue of generative models producing outputs that infringe on copyrighted training data, challenges a prior notion (NAF), and proposes a new framework (blameless copy protection) that uses clean-room settings for improved guarantees.


<details>
  <summary>Details</summary>
Motivation: To establish stronger technical and legal foundations for preventing generative models from infringing copyrights of the training data.

Method: The authors critique existing near access-freeness (NAF) definitions and introduce a new framework, blameless copy protection, grounded in clean-room copy protection and formal proofs leveraging differential privacy.

Result: They show that NAF can still result in verbatim copying (tainted models) and define clean-room copy protection for user-controlled risk of copying. They prove that differential privacy implies clean-room copy protection under specific requirements.

Conclusion: The proposed framework provides a stronger guarantee against copyright infringement and connects differential privacy with clean-room copy protection in cases where datasets meet deduplication requirements.

Abstract: Are there any conditions under which a generative model's outputs are
guaranteed not to infringe the copyrights of its training data? This is the
question of "provable copyright protection" first posed by Vyas, Kakade, and
Barak (ICML 2023). They define near access-freeness (NAF) and propose it as
sufficient for protection. This paper revisits the question and establishes new
foundations for provable copyright protection -- foundations that are firmer
both technically and legally. First, we show that NAF alone does not prevent
infringement. In fact, NAF models can enable verbatim copying, a blatant
failure of copy protection that we dub being tainted. Then, we introduce our
blameless copy protection framework for defining meaningful guarantees, and
instantiate it with clean-room copy protection. Clean-room copy protection
allows a user to control their risk of copying by behaving in a way that is
unlikely to copy in a counterfactual clean-room setting. Finally, we formalize
a common intuition about differential privacy and copyright by proving that DP
implies clean-room copy protection when the dataset is golden, a copyright
deduplication requirement.

</details>


### [255] [Diffusion-based Task-oriented Semantic Communications with Model Inversion Attack](https://arxiv.org/abs/2506.19886)
*Xuesong Wang,Mo Li,Xingyan Shi,Zhaoqian Liu,Shenghao Yang*

Main category: cs.CR

TL;DR: This paper introduces DiffSem, a diffusion-based semantic communication framework for 6G networks, addressing privacy and task performance challenges while proposing a novel metric for attacker evaluation.


<details>
  <summary>Details</summary>
Motivation: To address the privacy challenges and inefficiencies of task-oriented semantic communication systems in 6G networks, as traditional assessment metrics (like PSNR or SSIM) fail to capture semantic divergence crucial in this scenario.

Method: The paper introduces DiffSem, which utilizes a diffusion mechanism with self-referential label embedding to optimize semantic information reconstruction, compensate for channel noise, and adopt semantic distortion for robustness in different signal-to-noise conditions.

Result: The DiffSem framework achieves a 10.03% improvement in classification accuracy on the MNIST dataset and demonstrates stable performance under dynamic channel conditions. It also reveals a gap between traditional metrics and semantic information leakage.

Conclusion: DiffSem not only significantly improves task performance and robustness but also highlights the inadequacy of traditional image quality metrics in quantifying semantic fidelity, providing a new perspective on privacy preservation in semantic communication systems.

Abstract: Semantic communication has emerged as a promising neural network-based system
design for 6G networks. Task-oriented semantic communication is a novel
paradigm whose core goal is to efficiently complete specific tasks by
transmitting semantic information, optimizing communication efficiency and task
performance. The key challenge lies in preserving privacy while maintaining
task accuracy, as this scenario is susceptible to model inversion attacks. In
such attacks, adversaries can restore or even reconstruct input data by
analyzing and processing model outputs, owing to the neural network-based
nature of the systems. In addition, traditional systems use image quality
indicators (such as PSNR or SSIM) to assess attack severity, which may be
inadequate for task-oriented semantic communication, since visual differences
do not necessarily ensure semantic divergence. In this paper, we propose a
diffusion-based semantic communication framework, named DiffSem, that optimizes
semantic information reconstruction through a diffusion mechanism with
self-referential label embedding to significantly improve task performance. Our
model also compensates channel noise and adopt semantic information distortion
to ensure the robustness of the system in various signal-to-noise ratio
environments. To evaluate the attacker's effectiveness, we propose a new metric
that better quantifies the semantic fidelity of estimations from the adversary.
Experimental results based on this criterion show that on the MNIST dataset,
DiffSem improves the classification accuracy by 10.03%, and maintain stable
performance under dynamic channels. Our results further demonstrate that
significant deviation exists between traditional image quality indicators and
the leakage of task-relevant semantic information.

</details>


### [256] [SV-LLM: An Agentic Approach for SoC Security Verification using Large Language Models](https://arxiv.org/abs/2506.20415)
*Dipayan Saha,Shams Tarek,Hasan Al Shaikh,Khan Thamid Hasan,Pavan Sai Nalluri,Md. Ajoad Hasan,Nashmin Alam,Jingbo Zhou,Sujan Kumar Saha,Mark Tehranipoor,Farimah Farahmandi*

Main category: cs.CR

TL;DR: The paper introduces SV-LLM, a multi-agent system leveraging large language models (LLMs) to improve and automate system-on-chip (SoC) security verification, addressing challenges in scalability, automation, and adaptability.


<details>
  <summary>Details</summary>
Motivation: Traditional SoC security verification techniques face significant limitations in automation, scalability, and adaptability. The rise of LLMs with advanced reasoning and code generation capabilities offers an opportunity to address these challenges more effectively.

Method: SV-LLM employs a multi-agent system design where specialized LLM agents collaborate on tasks such as verification question answering, threat modeling, and vulnerability detection. These agents use approaches like in-context learning, fine-tuning, and retrieval-augmented generation (RAG) to tackle different tasks.

Result: Illustrative case studies and experiments demonstrate that SV-LLM effectively streamlines SoC security workflows by reducing manual intervention, improving accuracy, and accelerating security analysis.

Conclusion: SV-LLM represents a significant advancement in hardware security practices, offering an automated and scalable solution for proactively identifying and mitigating risks in SoC designs.

Abstract: Ensuring the security of complex system-on-chips (SoCs) designs is a critical
imperative, yet traditional verification techniques struggle to keep pace due
to significant challenges in automation, scalability, comprehensiveness, and
adaptability. The advent of large language models (LLMs), with their remarkable
capabilities in natural language understanding, code generation, and advanced
reasoning, presents a new paradigm for tackling these issues. Moving beyond
monolithic models, an agentic approach allows for the creation of multi-agent
systems where specialized LLMs collaborate to solve complex problems more
effectively. Recognizing this opportunity, we introduce SV-LLM, a novel
multi-agent assistant system designed to automate and enhance SoC security
verification. By integrating specialized agents for tasks like verification
question answering, security asset identification, threat modeling, test plan
and property generation, vulnerability detection, and simulation-based bug
validation, SV-LLM streamlines the workflow. To optimize their performance in
these diverse tasks, agents leverage different learning paradigms, such as
in-context learning, fine-tuning, and retrieval-augmented generation (RAG). The
system aims to reduce manual intervention, improve accuracy, and accelerate
security analysis, supporting proactive identification and mitigation of risks
early in the design cycle. We demonstrate its potential to transform hardware
security practices through illustrative case studies and experiments that
showcase its applicability and efficacy.

</details>


### [257] [Attack Smarter: Attention-Driven Fine-Grained Webpage Fingerprinting Attacks](https://arxiv.org/abs/2506.20082)
*Yali Yuan,Weiyi Zou,Guang Cheng*

Main category: cs.CR

TL;DR: This paper introduces ADWPF, a novel WebPage Fingerprinting (WPF) attack method leveraging attention-driven techniques for increased classification accuracy in practical and challenging multi-tab browsing scenarios.


<details>
  <summary>Details</summary>
Motivation: Website Fingerprinting (WF), while effective in recognizing website homepages under controlled conditions, struggles to scale to general browsing patterns involving subpages and multi-tab environments.

Method: The ADWPF framework uses targeted augmentation (attention cropping and masking) and self-attention modules during training to extract and analyze traffic features, along with residual attention for handling overlapping traffic in multi-tab browsing.

Result: Experimental evaluations demonstrate that ADWPF outperforms existing methods consistently across varying dataset scales, showing enhanced ability to classify webpages under real-world conditions.

Conclusion: ADWPF addresses key challenges in WF attacks such as low inter-class variance and overlapping traffic by leveraging advanced attention mechanisms, presenting a robust solution for WPF in real-world applications.

Abstract: Website Fingerprinting (WF) attacks aim to infer which websites a user is
visiting by analyzing traffic patterns, thereby compromising user anonymity.
Although this technique has been demonstrated to be effective in controlled
experimental environments, it remains largely limited to small-scale scenarios,
typically restricted to recognizing website homepages. In practical settings,
however, users frequently access multiple subpages in rapid succession, often
before previous content fully loads. WebPage Fingerprinting (WPF) generalizes
the WF framework to large-scale environments by modeling subpages of the same
site as distinct classes. These pages often share similar page elements,
resulting in lower inter-class variance in traffic features. Furthermore, we
consider multi-tab browsing scenarios, in which a single trace encompasses
multiple categories of webpages. This leads to overlapping traffic segments,
and similar features may appear in different positions within the traffic,
thereby increasing the difficulty of classification. To address these
challenges, we propose an attention-driven fine-grained WPF attack, named
ADWPF. Specifically, during the training phase, we apply targeted augmentation
to salient regions of the traffic based on attention maps, including attention
cropping and attention masking. ADWPF then extracts low-dimensional features
from both the original and augmented traffic and applies self-attention modules
to capture the global contextual patterns of the trace. Finally, to handle the
multi-tab scenario, we employ the residual attention to generate class-specific
representations of webpages occurring at different temporal positions.
Extensive experiments demonstrate that the proposed method consistently
surpasses state-of-the-art baselines across datasets of different scales.

</details>


### [258] [Autonomous Cyber Resilience via a Co-Evolutionary Arms Race within a Fortified Digital Twin Sandbox](https://arxiv.org/abs/2506.20102)
*Malikussaid,Sutiyo*

Main category: cs.CR

TL;DR: The paper presents the ARC framework to address cybersecurity challenges in critical infrastructures caused by IT/OT convergence, leveraging co-evolving adversarial agents.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address vulnerabilities introduced by hyper-connected ICS systems, which static defenses fail to protect against adaptive and intelligent adversaries.

Method: ARC uses a closed-loop, co-evolutionary framework with a reinforcement-learning-based "Red Agent" attacker and an ensemble-based "Blue Agent" defender to discover and patch system vulnerabilities autonomously.

Result: Experimental validation on testbeds (TEP and SWaT) demonstrates superior performance in detecting novel attacks through ARC's co-evolutionary dynamics.

Conclusion: ARC represents a necessary paradigm shift toward dynamic, self-improving cybersecurity, ensuring operator trust via XAI and proposing scalability for future infrastructures.

Abstract: The convergence of IT and OT has created hyper-connected ICS, exposing
critical infrastructure to a new class of adaptive, intelligent adversaries
that render static defenses obsolete. Existing security paradigms often fail to
address a foundational "Trinity of Trust," comprising the fidelity of the
system model, the integrity of synchronizing data, and the resilience of the
analytical engine against sophisticated evasion. This paper introduces the ARC
framework, a method for achieving analytical resilience through an autonomous,
closed-loop hardening process. ARC establishes a perpetual co-evolutionary arms
race within the high-fidelity sandbox of a F-SCDT. A DRL agent, the "Red
Agent," is formalized and incentivized to autonomously discover stealthy,
physically-plausible attack paths that maximize process disruption while
evading detection. Concurrently, an ensemble-based "Blue Agent" defender is
continuously hardened via adversarial training against the evolving threats
discovered by its adversary. This co-evolutionary dynamic forces both agents to
become progressively more sophisticated, enabling the system to autonomously
probe and patch its own vulnerabilities. Experimental validation on both the
TEP and the SWaT testbeds demonstrates the framework's superior performance. A
comprehensive ablation study, supported by extensive visualizations including
ROC curves and SHAP plots, reveals that the co-evolutionary process itself is
responsible for a significant performance increase in detecting novel attacks.
By integrating XAI to ensure operator trust and proposing a scalable F-ARC
architecture, this work presents ARC not merely as an improvement, but as a
necessary paradigm shift toward dynamic, self-improving security for the future
of critical infrastructure.

</details>


### [259] [Vulnerability Disclosure through Adaptive Black-Box Adversarial Attacks on NIDS](https://arxiv.org/abs/2506.20576)
*Sabrine Ennaji,Elhadj Benkhelifa,Luigi V. Mancini*

Main category: cs.CR

TL;DR: The paper introduces a novel black-box adversarial attack method tailored for structured data like network traffic, focusing on effectiveness, real-world applicability, and reduced detection risks.


<details>
  <summary>Details</summary>
Motivation: There is a gap in addressing adversarial attacks in structured data, such as network traffic, due to feature interdependencies and lack of reproducible methodologies.

Method: The proposed method uses adaptive feature selection with change-point detection and causality analysis, adhering to black-box constraints while minimizing interactions to reduce detection risks.

Result: Experiments demonstrate high effectiveness in evading detection, achieving adaptability and suitability for real-world applications.

Conclusion: The work enhances understanding of adversarial attacks in structured data and aims to foster robust defense mechanisms against evolving threats.

Abstract: Adversarial attacks, wherein slight inputs are carefully crafted to mislead
intelligent models, have attracted increasing attention. However, a critical
gap persists between theoretical advancements and practical application,
particularly in structured data like network traffic, where interdependent
features complicate effective adversarial manipulations. Moreover, ambiguity in
current approaches restricts reproducibility and limits progress in this field.
Hence, existing defenses often fail to handle evolving adversarial attacks.
This paper proposes a novel approach for black-box adversarial attacks, that
addresses these limitations. Unlike prior work, which often assumes system
access or relies on repeated probing, our method strictly respect black-box
constraints, reducing interaction to avoid detection and better reflect
real-world scenarios. We present an adaptive feature selection strategy using
change-point detection and causality analysis to identify and target sensitive
features to perturbations. This lightweight design ensures low computational
cost and high deployability. Our comprehensive experiments show the attack's
effectiveness in evading detection with minimal interaction, enhancing its
adaptability and applicability in real-world scenarios. By advancing the
understanding of adversarial attacks in network traffic, this work lays a
foundation for developing robust defenses.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [260] [Deciphering GunType Hierarchy through Acoustic Analysis of Gunshot Recordings](https://arxiv.org/abs/2506.20609)
*Ankit Shah,Rita Singh,Bhiksha Raj,Alexander Hauptmann*

Main category: cs.SD

TL;DR: The paper proposes a cost-effective method utilizing machine learning to detect and classify firearm types based on gunshot acoustic characteristics, achieving promising results especially with deep learning models.


<details>
  <summary>Details</summary>
Motivation: To address the high costs of current gunshot detection systems and improve public safety by providing timely firearm detection and classification using widely available devices.

Method: The study uses machine learning models, primarily SVMs and CNNs, applied to a dataset of 3459 gunshot recordings to analyze acoustic characteristics for firearm type classification.

Result: The CNN model achieves superior performance (mAP 0.58) compared to the SVM baseline (mAP 0.39). Challenges related to noisy data slightly reduce effectiveness (mAP 0.35).

Conclusion: Deep learning models show strong potential in creating cost-effective, real-time gunshot detection systems deployable on common devices, offering valuable intelligence for public safety.

Abstract: The escalating rates of gun-related violence and mass shootings represent a
significant threat to public safety. Timely and accurate information for law
enforcement agencies is crucial in mitigating these incidents. Current
commercial gunshot detection systems, while effective, often come with
prohibitive costs. This research explores a cost-effective alternative by
leveraging acoustic analysis of gunshot recordings, potentially obtainable from
ubiquitous devices like cell phones, to not only detect gunshots but also
classify the type of firearm used. This paper details a study on deciphering
gun type hierarchies using a curated dataset of 3459 recordings. We investigate
the fundamental acoustic characteristics of gunshots, including muzzle blasts
and shockwaves, which vary based on firearm type, ammunition, and shooting
direction. We propose and evaluate machine learning frameworks, including
Support Vector Machines (SVMs) as a baseline and a more advanced Convolutional
Neural Network (CNN) architecture for joint gunshot detection and gun type
classification. Results indicate that our deep learning approach achieves a
mean average precision (mAP) of 0.58 on clean labeled data, outperforming the
SVM baseline (mAP 0.39). Challenges related to data quality, environmental
noise, and the generalization capabilities when using noisy web-sourced data
(mAP 0.35) are also discussed. The long-term vision is to develop a highly
accurate, real-time system deployable on common recording devices,
significantly reducing detection costs and providing critical intelligence to
first responders.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [261] [PocketVina Enables Scalable and Highly Accurate Physically Valid Docking through Multi-Pocket Conditioning](https://arxiv.org/abs/2506.20043)
*Ahmet Sarigun,Bora Uyar,Vedran Franke,Altuna Akalin*

Main category: q-bio.QM

TL;DR: PocketVina is a memory-efficient molecular docking framework that combines pocket prediction with multi-pocket exploration, achieving state-of-the-art performance in sampling physically valid ligand-binding poses.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of sampling physically valid ligand-binding poses in molecular docking, particularly for unseen or structurally diverse targets.

Method: Combine pocket prediction with systematic multi-pocket exploration in a search-based docking framework; evaluate against benchmarks like PDBbind2020, Astex, DockGen, PoseBusters, and the newly introduced TargetDock-AI dataset.

Result: PocketVina shows strong performance in ligand RMSD and physical validity metrics, outperforms deep learning methods on discriminating active/inactive targets, and maintains high accuracy across diverse ligands and unseen protein-ligand pairs.

Conclusion: PocketVina offers a robust, scalable, and task-independent docking strategy ideal for high-throughput virtual screening and structure-based drug discovery, requiring less resource consumption compared to deep learning methods.

Abstract: Sampling physically valid ligand-binding poses remains a major challenge in
molecular docking, particularly for unseen or structurally diverse targets. We
introduce PocketVina, a fast and memory-efficient, search-based docking
framework that combines pocket prediction with systematic multi-pocket
exploration. We evaluate PocketVina across four established
benchmarks--PDBbind2020 (timesplit and unseen), DockGen, Astex, and
PoseBusters--and observe consistently strong performance in sampling physically
valid docking poses. PocketVina achieves state-of-the-art performance when
jointly considering ligand RMSD and physical validity (PB-valid), while
remaining competitive with deep learning-based approaches in terms of RMSD
alone, particularly on structurally diverse and previously unseen targets.
PocketVina also maintains state-of-the-art physically valid docking accuracy
across ligands with varying degrees of flexibility. We further introduce
TargetDock-AI, a benchmarking dataset we curated, consisting of over 500000
protein-ligand pairs, and a partition of the dataset labeled with PubChem
activity annotations. On this large-scale dataset, PocketVina successfully
discriminates active from inactive targets, outperforming a deep learning
baseline while requiring significantly less GPU memory and runtime. PocketVina
offers a robust and scalable docking strategy that requires no task-specific
training and runs efficiently on standard GPUs, making it well-suited for
high-throughput virtual screening and structure-based drug discovery.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [262] [FundaQ-8: A Clinically-Inspired Scoring Framework for Automated Fundus Image Quality Assessment](https://arxiv.org/abs/2506.20303)
*Lee Qi Zun,Oscar Wong Jin Hao,Nor Anita Binti Che Omar,Zalifa Zakiah Binti Asnir,Mohamad Sabri bin Sinal Zainal,Goh Man Fye*

Main category: eess.IV

TL;DR: FundaQ-8 is introduced for systematic fundus image quality assessment, employing an expert-validated scoring framework and a deep learning ResNet18-based model.


<details>
  <summary>Details</summary>
Motivation: Current fundus image quality assessment methods face challenges due to variable imaging conditions and subjective expert evaluations.

Method: FundaQ-8 framework is based on eight critical quality parameters and employs a ResNet18-based regression model trained on 1800 images with methodologies like transfer learning and statistical validation.

Result: The model showed clinical reliability upon testing with the EyeQ dataset, and integrating FundaQ-8 enhanced diabetic retinopathy diagnostic robustness.

Conclusion: FundaQ-8 provides a valuable, structured approach for real-world fundus imaging applications, especially improving quality-aware deep learning diagnostics.

Abstract: Automated fundus image quality assessment (FIQA) remains a challenge due to
variations in image acquisition and subjective expert evaluations. We introduce
FundaQ-8, a novel expert-validated framework for systematically assessing
fundus image quality using eight critical parameters, including field coverage,
anatomical visibility, illumination, and image artifacts. Using FundaQ-8 as a
structured scoring reference, we develop a ResNet18-based regression model to
predict continuous quality scores in the 0 to 1 range. The model is trained on
1800 fundus images from real-world clinical sources and Kaggle datasets, using
transfer learning, mean squared error optimization, and standardized
preprocessing. Validation against the EyeQ dataset and statistical analyses
confirm the framework's reliability and clinical interpretability.
Incorporating FundaQ-8 into deep learning models for diabetic retinopathy
grading also improves diagnostic robustness, highlighting the value of
quality-aware training in real-world screening applications.

</details>


### [263] [VoxelOpt: Voxel-Adaptive Message Passing for Discrete Optimization in Deformable Abdominal CT Registration](https://arxiv.org/abs/2506.19975)
*Hang Zhang,Yuxi Zhang,Jiazheng Wang,Xiang Chen,Renjiu Hu,Xin Tian,Gaolei Li,Min Liu*

Main category: eess.IV

TL;DR: The paper introduces VoxelOpt, a deformable image registration (DIR) framework combining the accuracy of iterative methods with the speed of learning-based approaches, achieving top performance without label supervision.


<details>
  <summary>Details</summary>
Motivation: To address limitations in existing DIR methods, such as learning-based inefficiency with small training data and large deformations, and the slow runtime of iterative-based approaches.

Method: VoxelOpt leverages discrete optimization with three key innovations: voxel-wise adaptive message passing, a multi-level image pyramid with 27-neighbor cost volumes, and feature extraction using a pretrained foundational segmentation model instead of hand-crafted methods.

Result: VoxelOpt achieves superior efficiency and accuracy compared to leading iterative methods and matches state-of-the-art results of learning-based methods requiring label supervision.

Conclusion: VoxelOpt effectively balances registration accuracy and runtime, providing a competitive and practical solution for abdominal CT registration. Source code will be accessible online for further adoption.

Abstract: Recent developments in neural networks have improved deformable image
registration (DIR) by amortizing iterative optimization, enabling fast and
accurate DIR results. However, learning-based methods often face challenges
with limited training data, large deformations, and tend to underperform
compared to iterative approaches when label supervision is unavailable. While
iterative methods can achieve higher accuracy in such scenarios, they are
considerably slower than learning-based methods. To address these limitations,
we propose VoxelOpt, a discrete optimization-based DIR framework that combines
the strengths of learning-based and iterative methods to achieve a better
balance between registration accuracy and runtime. VoxelOpt uses displacement
entropy from local cost volumes to measure displacement signal strength at each
voxel, which differs from earlier approaches in three key aspects. First, it
introduces voxel-wise adaptive message passing, where voxels with lower entropy
receives less influence from their neighbors. Second, it employs a multi-level
image pyramid with 27-neighbor cost volumes at each level, avoiding exponential
complexity growth. Third, it replaces hand-crafted features or contrastive
learning with a pretrained foundational segmentation model for feature
extraction. In abdominal CT registration, these changes allow VoxelOpt to
outperform leading iterative in both efficiency and accuracy, while matching
state-of-the-art learning-based methods trained with label supervision. The
source code will be available at https://github.com/tinymilky/VoxelOpt

</details>


### [264] [MS-IQA: A Multi-Scale Feature Fusion Network for PET/CT Image Quality Assessment](https://arxiv.org/abs/2506.20200)
*Siqiao Li,Chen Hui,Wei Zhang,Rui Liang,Chenyue Song,Feng Jiang,Haiqi Zhu,Zhixuan Li,Hong Huang,Xiang Li*

Main category: eess.IV

TL;DR: This study presents MS-IQA, a novel PET/CT image quality assessment method utilizing multi-scale feature fusion and a new dataset, demonstrating superiority over existing techniques.


<details>
  <summary>Details</summary>
Motivation: To address the lack of a PET/CT medical image quality assessment method that simultaneously considers both low-level distortions and high-level anatomical features.

Method: MS-IQA utilizes a multi-scale feature fusion network with ResNet and Swin Transformer, combined with a dynamically weighted channel attention mechanism, to integrate local and global information for improved PET/CT image quality assessment. A new PET-CT-IQA-DS dataset with radiologist-assigned scores was also created.

Result: The MS-IQA model outperforms existing state-of-the-art methods in IQA metrics on PET-CT-IQA-DS and LDCTIQAC2023 datasets.

Conclusion: MS-IQA offers an advanced, accurate, and efficient IQA method for PET/CT imaging, addressing a critical gap in diagnosing medical imaging quality issues. Its contributions include the methodology and a novel IQA dataset.

Abstract: Positron Emission Tomography / Computed Tomography (PET/CT) plays a critical
role in medical imaging, combining functional and anatomical information to aid
in accurate diagnosis. However, image quality degradation due to noise,
compression and other factors could potentially lead to diagnostic uncertainty
and increase the risk of misdiagnosis. When evaluating the quality of a PET/CT
image, both low-level features like distortions and high-level features like
organ anatomical structures affect the diagnostic value of the image. However,
existing medical image quality assessment (IQA) methods are unable to account
for both feature types simultaneously. In this work, we propose MS-IQA, a novel
multi-scale feature fusion network for PET/CT IQA, which utilizes multi-scale
features from various intermediate layers of ResNet and Swin Transformer,
enhancing its ability of perceiving both local and global information. In
addition, a multi-scale feature fusion module is also introduced to effectively
combine high-level and low-level information through a dynamically weighted
channel attention mechanism. Finally, to fill the blank of PET/CT IQA dataset,
we construct PET-CT-IQA-DS, a dataset containing 2,700 varying-quality PET/CT
images with quality scores assigned by radiologists. Experiments on our dataset
and the publicly available LDCTIQAC2023 dataset demonstrate that our proposed
model has achieved superior performance against existing state-of-the-art
methods in various IQA metrics. This work provides an accurate and efficient
IQA method for PET/CT. Our code and dataset are available at
https://github.com/MS-IQA/MS-IQA/.

</details>


### [265] [Opportunistic Osteoporosis Diagnosis via Texture-Preserving Self-Supervision, Mixture of Experts and Multi-Task Integration](https://arxiv.org/abs/2506.20282)
*Jiaxing Huang,Heng Guo,Le Lu,Fan Yang,Minfeng Xu,Ge Yang,Wei Luo*

Main category: eess.IV

TL;DR: The paper proposes a deep learning framework to address limitations in opportunistic CT-based osteoporosis diagnosis, focusing on underutilized data, device discrepancies, and clinical knowledge integration.


<details>
  <summary>Details</summary>
Motivation: DXA-based osteoporosis diagnosis is inaccessible in resource-limited areas, and current CT-based alternatives have technical limitations.

Method: The approach includes self-supervised learning for unlabeled CT data, a Mixture of Experts (MoE) for device adaptability, and multi-task learning for integrated osteoporosis-related tasks.

Result: The framework was tested across multiple clinical sites and showed improved accuracy and generalizability compared to existing methods.

Conclusion: The proposed method enhances opportunistic CT-based osteoporosis screening, offering a better diagnostic solution where DXA is unavailable.

Abstract: Osteoporosis, characterized by reduced bone mineral density (BMD) and
compromised bone microstructure, increases fracture risk in aging populations.
While dual-energy X-ray absorptiometry (DXA) is the clinical standard for BMD
assessment, its limited accessibility hinders diagnosis in resource-limited
regions. Opportunistic computed tomography (CT) analysis has emerged as a
promising alternative for osteoporosis diagnosis using existing imaging data.
Current approaches, however, face three limitations: (1) underutilization of
unlabeled vertebral data, (2) systematic bias from device-specific DXA
discrepancies, and (3) insufficient integration of clinical knowledge such as
spatial BMD distribution patterns. To address these, we propose a unified deep
learning framework with three innovations. First, a self-supervised learning
method using radiomic representations to leverage unlabeled CT data and
preserve bone texture. Second, a Mixture of Experts (MoE) architecture with
learned gating mechanisms to enhance cross-device adaptability. Third, a
multi-task learning framework integrating osteoporosis diagnosis, BMD
regression, and vertebra location prediction. Validated across three clinical
sites and an external hospital, our approach demonstrates superior
generalizability and accuracy over existing methods for opportunistic
osteoporosis screening and diagnosis.

</details>


### [266] [EAGLE: An Efficient Global Attention Lesion Segmentation Model for Hepatic Echinococcosis](https://arxiv.org/abs/2506.20333)
*Jiayan Chen,Kai Li,Yulu Zhao,Jianqiang Huang,Zhan Wang*

Main category: eess.IV

TL;DR: The paper introduces EAGLE, a U-shaped network leveraging state space models for accurate and efficient segmentation of hepatic echinococcosis (HE) lesions, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of CNNs and Transformers in medical image segmentation, specifically for HE lesions, by creating a model that combines computational efficiency and effective feature modeling.

Method: EAGLE employs a Progressive Visual State Space (PVSS) encoder and Hybrid Visual State Space (HVSS) decoder along with novel modules like CVSSB for feature fusion and HWTB for lossless downsampling.

Result: EAGLE outperformed existing models, achieving a Dice Similarity Coefficient of 89.76%, which is 1.61% higher than MSVM-UNet.

Conclusion: EAGLE offers an innovative approach to HE lesion segmentation that balances accuracy and computational efficiency, pushing forward state-of-the-art methods in medical imaging.

Abstract: Hepatic echinococcosis (HE) is a widespread parasitic disease in
underdeveloped pastoral areas with limited medical resources. While CNN-based
and Transformer-based models have been widely applied to medical image
segmentation, CNNs lack global context modeling due to local receptive fields,
and Transformers, though capable of capturing long-range dependencies, are
computationally expensive. Recently, state space models (SSMs), such as Mamba,
have gained attention for their ability to model long sequences with linear
complexity. In this paper, we propose EAGLE, a U-shaped network composed of a
Progressive Visual State Space (PVSS) encoder and a Hybrid Visual State Space
(HVSS) decoder that work collaboratively to achieve efficient and accurate
segmentation of hepatic echinococcosis (HE) lesions. The proposed Convolutional
Vision State Space Block (CVSSB) module is designed to fuse local and global
features, while the Haar Wavelet Transformation Block (HWTB) module compresses
spatial information into the channel dimension to enable lossless downsampling.
Due to the lack of publicly available HE datasets, we collected CT slices from
260 patients at a local hospital. Experimental results show that EAGLE achieves
state-of-the-art performance with a Dice Similarity Coefficient (DSC) of
89.76%, surpassing MSVM-UNet by 1.61%.

</details>


### [267] [Fusing Radiomic Features with Deep Representations for Gestational Age Estimation in Fetal Ultrasound Images](https://arxiv.org/abs/2506.20407)
*Fangyijie Wang,Yuan Liang,Sourav Bhattacharjee,Abey Campbell,Kathleen M. Curran,Guénolé Silvestre*

Main category: eess.IV

TL;DR: This paper introduces a deep learning-based feature fusion framework for estimating gestational age using fetal ultrasound images, achieving high accuracy without manual measurements.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of manual fetal biometric measurements for gestational age estimation, which are operator-dependent and time-consuming, by introducing an automated and reliable method.

Method: The authors use a deep learning model to extract representations from ultrasound images and combine them with radiomic features to estimate gestational age. This fusion leverages the interpretability of radiomics and the power of deep learning.

Result: Their framework estimates gestational age with a mean absolute error of 8.0 days across all trimesters, exceeding current machine learning-based methods' performance and showing robustness across diverse populations.

Conclusion: The proposed method demonstrates that combining radiomic features with deep learning representations can lead to a robust, interpretable, and accurate framework for estimating gestational age in clinical practice.

Abstract: Accurate gestational age (GA) estimation, ideally through fetal ultrasound
measurement, is a crucial aspect of providing excellent antenatal care.
However, deriving GA from manual fetal biometric measurements depends on the
operator and is time-consuming. Hence, automatic computer-assisted methods are
demanded in clinical practice. In this paper, we present a novel feature fusion
framework to estimate GA using fetal ultrasound images without any measurement
information. We adopt a deep learning model to extract deep representations
from ultrasound images. We extract radiomic features to reveal patterns and
characteristics of fetal brain growth. To harness the interpretability of
radiomics in medical imaging analysis, we estimate GA by fusing radiomic
features and deep representations. Our framework estimates GA with a mean
absolute error of 8.0 days across three trimesters, outperforming current
machine learning-based methods at these gestational ages. Experimental results
demonstrate the robustness of our framework across different populations in
diverse geographical regions. Our code is publicly available on
\href{https://github.com/13204942/RadiomicsImageFusion_FetalUS}{GitHub}.

</details>


### [268] [Weighted Mean Frequencies: a handcraft Fourier feature for 4D Flow MRI segmentation](https://arxiv.org/abs/2506.20614)
*Simon Perrin,Sébastien Levilly,Huajun Sun,Harold Mouchère,Jean-Michel Serfaty*

Main category: eess.IV

TL;DR: This paper introduces Weighted Mean Frequencies (WMF), a novel feature for visualizing 4D Flow MRI images to enhance vessel segmentation, surpassing traditional methods.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the issues of poor resolution and noise in 4D Flow MRI images, which affect the accuracy of vascular biomarkers and segmentation.

Method: The authors propose Weighted Mean Frequencies (WMF) as a handcrafted feature that highlights regions influenced by pulsatile flow in 4D Flow MRI. Segmentation tasks using optimal thresholding and deep learning validated its efficacy.

Result: Experiments showed significant improvement in segmentation metrics (IoU +0.12, Dice +0.13) using WMF compared to PC-MRA, especially in deep learning-based strategies.

Conclusion: WMF offers an innovative and effective way to segment 4D Flow MRI images, with promising implications for applications in other vascular regions like the heart and brain.

Abstract: In recent decades, the use of 4D Flow MRI images has enabled the
quantification of velocity fields within a volume of interest and along the
cardiac cycle. However, the lack of resolution and the presence of noise in
these biomarkers are significant issues. As indicated by recent studies, it
appears that biomarkers such as wall shear stress are particularly impacted by
the poor resolution of vessel segmentation. The Phase Contrast Magnetic
Resonance Angiography (PC-MRA) is the state-of-the-art method to facilitate
segmentation. The objective of this work is to introduce a new handcraft
feature that provides a novel visualisation of 4D Flow MRI images, which is
useful in the segmentation task. This feature, termed Weighted Mean Frequencies
(WMF), is capable of revealing the region in three dimensions where a voxel has
been passed by pulsatile flow. Indeed, this feature is representative of the
hull of all pulsatile velocity voxels. The value of the feature under
discussion is illustrated by two experiments. The experiments involved
segmenting 4D Flow MRI images using optimal thresholding and deep learning
methods. The results obtained demonstrate a substantial enhancement in terms of
IoU and Dice, with a respective increase of 0.12 and 0.13 in comparison with
the PC-MRA feature, as evidenced by the deep learning task. This feature has
the potential to yield valuable insights that could inform future segmentation
processes in other vascular regions, such as the heart or the brain.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [269] [Learning Bilateral Team Formation in Cooperative Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2506.20039)
*Koorosh Moslemi,Chi-Guhn Lee*

Main category: cs.MA

TL;DR: The paper introduces a framework for two-sided team formation in dynamic multi-agent systems within MARL, addressing gaps in previous studies.


<details>
  <summary>Details</summary>
Motivation: To explore the effects of algorithmic bilateral grouping choices in dynamic populations, which are underexplored in MARL.

Method: Developed a framework for bilateral team formation and tested it in widely adopted multi-agent scenarios.

Result: The framework demonstrated competitive performance and improved generalization across various scenarios.

Conclusion: Two-sided team formation algorithms have a significant impact on policy performance and generalization in MARL.

Abstract: Team formation and the dynamics of team-based learning have drawn significant
interest in the context of Multi-Agent Reinforcement Learning (MARL). However,
existing studies primarily focus on unilateral groupings, predefined teams, or
fixed-population settings, leaving the effects of algorithmic bilateral
grouping choices in dynamic populations underexplored. To address this gap, we
introduce a framework for learning two-sided team formation in dynamic
multi-agent systems. Through this study, we gain insight into what algorithmic
properties in bilateral team formation influence policy performance and
generalization. We validate our approach using widely adopted multi-agent
scenarios, demonstrating competitive performance and improved generalization in
most scenarios.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [270] [Reinforcement Learning Increases Wind Farm Power Production by Enabling Closed-Loop Collaborative Control](https://arxiv.org/abs/2506.20554)
*Andrew Mole,Max Weissenbacher,Georgios Rigas,Sylvain Laizet*

Main category: physics.flu-dyn

TL;DR: The study presents a reinforcement learning (RL) controller integrated with high-fidelity large-eddy simulation (LES) to optimize wind farm energy production. Results show a considerable increase in power output compared to traditional and static methods.


<details>
  <summary>Details</summary>
Motivation: To enhance wind farm energy production by leveraging dynamic control strategies that account for atmospheric turbulence, addressing the limitations of static optimization methods.

Method: A reinforcement learning (RL) controller was developed and integrated with high-fidelity large-eddy simulation (LES) to dynamically manage wind turbine operations in response to turbulent atmospheric conditions.

Result: The RL controller achieved a 4.30% increase in wind farm power output, outperforming static optimal yaw control methods which provided a 2.19% improvement.

Conclusion: Dynamic flow-responsive control using RL presents a promising method for optimizing wind farm energy production, supporting the transition to renewable energy and net-zero emissions targets.

Abstract: Traditional wind farm control operates each turbine independently to maximize
individual power output. However, coordinated wake steering across the entire
farm can substantially increase the combined wind farm energy production.
Although dynamic closed-loop control has proven effective in flow control
applications, wind farm optimization has relied primarily on static,
low-fidelity simulators that ignore critical turbulent flow dynamics. In this
work, we present the first reinforcement learning (RL) controller integrated
directly with high-fidelity large-eddy simulation (LES), enabling real-time
response to atmospheric turbulence through collaborative, dynamic control
strategies. Our RL controller achieves a 4.30% increase in wind farm power
output compared to baseline operation, nearly doubling the 2.19% gain from
static optimal yaw control obtained through Bayesian optimization. These
results establish dynamic flow-responsive control as a transformative approach
to wind farm optimization, with direct implications for accelerating renewable
energy deployment to net-zero targets.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [271] [DualEquiNet: A Dual-Space Hierarchical Equivariant Network for Large Biomolecules](https://arxiv.org/abs/2506.19862)
*Junjie Xu,Jiahao Zhang,Mangal Prakash,Xiang Zhang,Suhang Wang*

Main category: q-bio.BM

TL;DR: The paper introduces DualEquiNet, a novel geometric graph neural network designed to model large biomolecules like RNA and proteins with both local geometry and long-range dependencies effectively.


<details>
  <summary>Details</summary>
Motivation: Geometric GNNs show promise in small molecule tasks but struggle with scalability and expressiveness for large biomolecules. The goal is to address challenges of capturing detailed atomic interactions, long-range dependencies, and hierarchical structures.

Method: DualEquiNet integrates representations in Euclidean and Spherical Harmonics spaces, utilizes bidirectional cross-space message passing and Cross-Space Interaction Pooling to hierarchically aggregate atomic features into biologically meaningful units.

Result: DualEquiNet achieves state-of-the-art performance on existing RNA property prediction and protein modeling benchmarks, and outperforms previous methods on two new benchmarks for 3D structural modeling.

Conclusion: DualEquiNet is effective in multi-scale modeling tasks, offering strong performance across various benchmarks and overcoming scalability and expressiveness limitations seen in prior geometric GNN approaches.

Abstract: Geometric graph neural networks (GNNs) that respect E(3) symmetries have
achieved strong performance on small molecule modeling, but they face
scalability and expressiveness challenges when applied to large biomolecules
such as RNA and proteins. These systems require models that can simultaneously
capture fine-grained atomic interactions, long-range dependencies across
spatially distant components, and biologically relevant hierarchical structure,
such as atoms forming residues, which in turn form higher-order domains.
Existing geometric GNNs, which typically operate exclusively in either
Euclidean or Spherical Harmonics space, are limited in their ability to capture
both the fine-scale atomic details and the long-range, symmetry-aware
dependencies required for modeling the multi-scale structure of large
biomolecules. We introduce DualEquiNet, a Dual-Space Hierarchical Equivariant
Network that constructs complementary representations in both Euclidean and
Spherical Harmonics spaces to capture local geometry and global symmetry-aware
features. DualEquiNet employs bidirectional cross-space message passing and a
novel Cross-Space Interaction Pooling mechanism to hierarchically aggregate
atomic features into biologically meaningful units, such as residues, enabling
efficient and expressive multi-scale modeling for large biomolecular systems.
DualEquiNet achieves state-of-the-art performance on multiple existing
benchmarks for RNA property prediction and protein modeling, and outperforms
prior methods on two newly introduced 3D structural benchmarks demonstrating
its broad effectiveness across a range of large biomolecule modeling tasks.

</details>


### [272] [Scalable and Cost-Efficient de Novo Template-Based Molecular Generation](https://arxiv.org/abs/2506.19865)
*Piotr Gaiński,Oussama Boussif,Andrei Rekesh,Dmytro Shevchuk,Ali Parviz,Mike Tyers,Robert A. Batey,Michał Koziarski*

Main category: q-bio.BM

TL;DR: This paper improves template-based molecular generation by minimizing synthesis costs, scaling to large libraries, and leveraging small fragment sets using innovative frameworks like Recursive Cost Guidance and Dynamic Library.


<details>
  <summary>Details</summary>
Motivation: To optimize drug design by overcoming inefficiencies and limitations in template-based molecular generation, focusing on synthetic accessibility, cost-efficiency, and quality.

Method: The paper introduces Recursive Cost Guidance for steering towards cost-effective synthesis and uses Dynamic Library mechanisms to reuse high-reward states. It also applies an Exploitation Penalty to balance exploration and exploitation.

Result: Significant improvements were demonstrated in cost-efficiency, molecular diversity, and synthesis quality, establishing state-of-the-art performance in template-based molecular generation.

Conclusion: The approach presented effectively addresses core challenges in template-based molecular generation, providing a robust framework for scalable and cost-effective drug design.

Abstract: Template-based molecular generation offers a promising avenue for drug design
by ensuring generated compounds are synthetically accessible through predefined
reaction templates and building blocks. In this work, we tackle three core
challenges in template-based GFlowNets: (1) minimizing synthesis cost, (2)
scaling to large building block libraries, and (3) effectively utilizing small
fragment sets. We propose \textbf{Recursive Cost Guidance}, a backward policy
framework that employs auxiliary machine learning models to approximate
synthesis cost and viability. This guidance steers generation toward low-cost
synthesis pathways, significantly enhancing cost-efficiency, molecular
diversity, and quality, especially when paired with an \textbf{Exploitation
Penalty} that balances the trade-off between exploration and exploitation. To
enhance performance in smaller building block libraries, we develop a
\textbf{Dynamic Library} mechanism that reuses intermediate high-reward states
to construct full synthesis trees. Our approach establishes state-of-the-art
results in template-based molecular generation.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [273] [MNN-AECS: Energy Optimization for LLM Decoding on Mobile Devices via Adaptive Core Selection](https://arxiv.org/abs/2506.19884)
*Zhengxiang Huang,Chaoyue Niu,Zhaode Wang,Jiarui Xue,Hanming Zhang,Yugang Wang,Zewei Xin,Xiaotang Jiang,Chengfei Lv,Fan Wu,Guihai Chen*

Main category: cs.OS

TL;DR: This paper presents MNN-AECS, an energy-efficient solution for on-device large language model (LLM) decoding, reducing energy consumption by 23%-78% while offering improved speed.


<details>
  <summary>Details</summary>
Motivation: To address the growing demand for energy-efficient on-device LLM inference, particularly for mobile devices with limited battery, as current methods neglect the energy-intensive decode phase.

Method: The paper introduces Adaptive Energy-Centric Core Selection (AECS) integrated into MNN. AECS optimizes energy use by dynamically using low-power CPU cores during the LLM decode phase without requiring root access or operating system modifications.

Result: MNN-AECS achieved a 23% reduction in energy consumption compared to MNN without significant slowdowns, and delivered 39%-78% energy savings and 12%-363% speed improvements over other engines.

Conclusion: MNN-AECS offers a practical, system-level solution for energy-efficient LLM decoding on mobile devices, optimizing both energy use and performance.

Abstract: As the demand for on-device Large Language Model (LLM) inference grows,
energy efficiency has become a major concern, especially for battery-limited
mobile devices. Our analysis shows that the memory-bound LLM decode phase
dominates energy use, and yet most existing works focus on accelerating the
prefill phase, neglecting energy concerns. We introduce Adaptive Energy-Centric
Core Selection (AECS) and integrate it into MNN to create the energy-efficient
version, MNN-AECS, the first engine-level system solution without requiring
root access or OS modifications for energy-efficient LLM decoding. MNN-AECS is
designed to reduce LLM decoding energy while keeping decode speed within an
acceptable slowdown threshold by dynamically selecting low-power CPU cores.
MNN-AECS is evaluated across 5 Android and 2 iOS devices on 5 popular LLMs of
various sizes. Compared to original MNN, MNN-AECS cuts down energy use by 23%
without slowdown averaged over all 7 devices and 4 datasets. Against other
engines, including llama.cpp, executorch, mllm, and MediaPipe, MNN-AECS
delivers 39% to 78% energy saving and 12% to 363% speedup on average.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [274] [X-SiT: Inherently Interpretable Surface Vision Transformers for Dementia Diagnosis](https://arxiv.org/abs/2506.20267)
*Fabian Bongratz,Tom Nuno Wolf,Jaume Gual Ramon,Christian Wachinger*

Main category: cs.GR

TL;DR: The study introduces the X-SiT model, an interpretable neural network for analyzing cortical surface renderings, offering human-understandable predictions and achieving high accuracy in detecting Alzheimer's disease and frontotemporal dementia.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of interpreting and visualizing 3D volumetric data in clinical contexts for understanding cerebral cortex structures.

Method: Development of X-SiT, a neural network integrating a prototypical surface patch decoder to classify embeddings via interpretable cortical features and case-based reasoning.

Result: The model achieves state-of-the-art performance in identifying Alzheimer's disease and frontotemporal dementia, while offering prototypes that align with disease patterns and assist in explaining errors.

Conclusion: X-SiT provides an interpretable approach to model cortical data, making it both effective and insightful for clinical applications in detecting neurological disorders.

Abstract: Interpretable models are crucial for supporting clinical decision-making,
driving advances in their development and application for medical images.
However, the nature of 3D volumetric data makes it inherently challenging to
visualize and interpret intricate and complex structures like the cerebral
cortex. Cortical surface renderings, on the other hand, provide a more
accessible and understandable 3D representation of brain anatomy, facilitating
visualization and interactive exploration. Motivated by this advantage and the
widespread use of surface data for studying neurological disorders, we present
the eXplainable Surface Vision Transformer (X-SiT). This is the first
inherently interpretable neural network that offers human-understandable
predictions based on interpretable cortical features. As part of X-SiT, we
introduce a prototypical surface patch decoder for classifying surface patch
embeddings, incorporating case-based reasoning with spatially corresponding
cortical prototypes. The results demonstrate state-of-the-art performance in
detecting Alzheimer's disease and frontotemporal dementia while additionally
providing informative prototypes that align with known disease patterns and
reveal classification errors.

</details>


### [275] [DreamAnywhere: Object-Centric Panoramic 3D Scene Generation](https://arxiv.org/abs/2506.20367)
*Edoardo Alberto Dominici,Jozef Hladky,Floor Verhoeven,Lukas Radl,Thomas Deixelberger,Stefan Ainetter,Philipp Drescher,Stefan Hauswiesner,Arno Coomans,Giacomo Nazzaro,Konstantinos Vardis,Markus Steinberger*

Main category: cs.GR

TL;DR: DreamAnywhere proposes a modular system to streamline text-to-3D scene generation, overcoming current methods' limitations such as front-facing environments and lack of scene understanding.


<details>
  <summary>Details</summary>
Motivation: Text-to-3D scene generation approaches face challenges such as front-facing environments, limited fidelity, and focus on either indoor or outdoor settings. The authors aim to address these gaps.

Method: The system synthesizes panoramic images from text inputs, segments backgrounds and objects, builds detailed 3D scenes via hybrid inpainting, and allows object editing for prototyping.

Result: DreamAnywhere improves scene coherence, image quality, and immersion compared to state-of-the-art systems, validated through a user study.

Conclusion: DreamAnywhere's modular and customizable approach effectively generates immersive 3D scenes while enabling rapid prototyping for industries such as low-budget movie production.

Abstract: Recent advances in text-to-3D scene generation have demonstrated significant
potential to transform content creation across multiple industries. Although
the research community has made impressive progress in addressing the
challenges of this complex task, existing methods often generate environments
that are only front-facing, lack visual fidelity, exhibit limited scene
understanding, and are typically fine-tuned for either indoor or outdoor
settings. In this work, we address these issues and propose DreamAnywhere, a
modular system for the fast generation and prototyping of 3D scenes. Our system
synthesizes a 360{\deg} panoramic image from text, decomposes it into
background and objects, constructs a complete 3D representation through hybrid
inpainting, and lifts object masks to detailed 3D objects that are placed in
the virtual environment. DreamAnywhere supports immersive navigation and
intuitive object-level editing, making it ideal for scene exploration, visual
mock-ups, and rapid prototyping -- all with minimal manual modeling. These
features make our system particularly suitable for low-budget movie production,
enabling quick iteration on scene layout and visual tone without the overhead
of traditional 3D workflows. Our modular pipeline is highly customizable as it
allows components to be replaced independently. Compared to current
state-of-the-art text and image-based 3D scene generation approaches,
DreamAnywhere shows significant improvements in coherence in novel view
synthesis and achieves competitive image quality, demonstrating its
effectiveness across diverse and challenging scenarios. A comprehensive user
study demonstrates a clear preference for our method over existing approaches,
validating both its technical robustness and practical usefulness.

</details>


### [276] [EditP23: 3D Editing via Propagation of Image Prompts to Multi-View](https://arxiv.org/abs/2506.20652)
*Roi Bar-On,Dana Cohen-Bar,Daniel Cohen-Or*

Main category: cs.GR

TL;DR: EditP23 enables mask-free 3D editing by propagating 2D image edits to multi-view 3D representations using a feed-forward method with no optimization.


<details>
  <summary>Details</summary>
Motivation: To simplify 3D editing by enabling intuitive user inputs without requiring text prompts or manual spatial masks.

Method: EditP23 uses pairs of 2D image prompts (original and edited images) to guide latent space flows in a pre-trained multi-view diffusion model for 3D-consistent edits.

Result: EditP23 achieves coherent edit propagation across views, with high fidelity to the original object's structure and appearance.

Conclusion: The method simplifies 3D editing workflows, achieving effective and user-friendly results across diverse scenarios without requiring masks or optimization steps.

Abstract: We present EditP23, a method for mask-free 3D editing that propagates 2D
image edits to multi-view representations in a 3D-consistent manner. In
contrast to traditional approaches that rely on text-based prompting or
explicit spatial masks, EditP23 enables intuitive edits by conditioning on a
pair of images: an original view and its user-edited counterpart. These image
prompts are used to guide an edit-aware flow in the latent space of a
pre-trained multi-view diffusion model, allowing the edit to be coherently
propagated across views. Our method operates in a feed-forward manner, without
optimization, and preserves the identity of the original object, in both
structure and appearance. We demonstrate its effectiveness across a range of
object categories and editing scenarios, achieving high fidelity to the source
while requiring no manual masks.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [277] [Capturing Visualization Design Rationale](https://arxiv.org/abs/2506.16571)
*Maeve Hutchinson,Radu Jianu,Aidan Slingsby,Jo Wood,Pranava Madhyastha*

Main category: cs.HC

TL;DR: The paper introduces a dataset and methodology focusing on capturing visualization design rationale using literate visualization notebooks created by students.


<details>
  <summary>Details</summary>
Motivation: Existing natural language datasets for visualizations focus more on decoding visualizations rather than understanding their encoding, often relying on controlled setups.

Method: The authors use student-created literate visualization notebooks, leveraging their narratives and design choices. They employ large language models (LLMs) to generate and categorize question-answer-rationale triples and curate these into a validated dataset.

Result: The curated dataset provides insight into how students articulate and rationalize their visualization design decisions.

Conclusion: This dataset offers a novel approach to understanding visualization design rationale and could enhance studies on visualization processes and teaching methods.

Abstract: Prior natural language datasets for data visualization have focused on tasks
such as visualization literacy assessment, insight generation, and
visualization generation from natural language instructions. These studies
often rely on controlled setups with purpose-built visualizations and
artificially constructed questions. As a result, they tend to prioritize the
interpretation of visualizations, focusing on decoding visualizations rather
than understanding their encoding. In this paper, we present a new dataset and
methodology for probing visualization design rationale through natural
language. We leverage a unique source of real-world visualizations and natural
language narratives: literate visualization notebooks created by students as
part of a data visualization course. These notebooks combine visual artifacts
with design exposition, in which students make explicit the rationale behind
their design decisions. We also use large language models (LLMs) to generate
and categorize question-answer-rationale triples from the narratives and
articulations in the notebooks. We then carefully validate the triples and
curate a dataset that captures and distills the visualization design choices
and corresponding rationales of the students.

</details>


### [278] [Beyond Autocomplete: Designing CopilotLens Towards Transparent and Explainable AI Coding Agents](https://arxiv.org/abs/2506.20062)
*Runlong Ye,Zeling Zhang,Boushra Almazroua,Michael Liut*

Main category: cs.HC

TL;DR: CopilotLens is an interactive framework enhancing AI-powered code assistants by making their code suggestion process transparent and explainable.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the lack of transparency in AI-powered code assistants, hindering developers from critically evaluating suggestions and understanding the tools' decision-making process.

Method: CopilotLens introduces a dynamic two-level interface that displays the AI's thought process, including high-level plans and specific codebase context.

Result: The paper demonstrates a design framework that prioritizes clarity and explainability in AI code assistants, moving beyond just speed of suggestions.

Conclusion: CopilotLens aims to foster better human-AI collaboration by making AI decision-making transparent, encouraging developers to form accurate mental models and calibrated trust.

Abstract: AI-powered code assistants are widely used to generate code completions,
significantly boosting developer productivity. However, these tools typically
present suggestions without explaining their rationale, leaving their
decision-making process inscrutable. This opacity hinders developers' ability
to critically evaluate the output, form accurate mental models, and build
calibrated trust in the system. To address this, we introduce CopilotLens, a
novel interactive framework that reframes code completion from a simple
suggestion into a transparent, explainable event. CopilotLens operates as an
explanation layer that reveals the AI agent's "thought process" through a
dynamic two-level interface, surfacing everything from its reconstructed
high-level plans to the specific codebase context influencing the code. This
paper presents the design and rationale of CopilotLens, offering a concrete
framework for building future agentic code assistants that prioritize clarity
of reasoning over speed of suggestion, thereby fostering deeper comprehension
and more robust human-AI collaboration.

</details>


### [279] [Irec: A Metacognitive Scaffolding for Self-Regulated Learning through Just-in-Time Insight Recall: A Conceptual Framework and System Prototype](https://arxiv.org/abs/2506.20156)
*Xuefei Hou,Xizhao Tan*

Main category: cs.HC

TL;DR: The paper proposes 'Insight Recall,' a system combining dynamic knowledge graphs and LLMs to aid self-regulated learning tasks like planning and reflection.


<details>
  <summary>Details</summary>
Motivation: Existing digital tools inadequately support metacognitive reflection necessary for effective self-regulated learning.

Method: The paper uses a paradigm based on context-triggered retrieval of insights, employing JITAI frameworks, knowledge graphs, and LLMs to filter relevant scaffolds.

Result: Prototype system 'Irec' showcases the feasibility of this approach, integrating hybrid retrieval engines and human-in-the-loop processes for knowledge graph creation.

Conclusion: The paper presents 'Insight Recall' as providing a novel framework and practical tools to support metacognition and SRL.

Abstract: The core challenge in learning has shifted from knowledge acquisition to
effective Self-Regulated Learning (SRL): planning, monitoring, and reflecting
on one's learning. Existing digital tools, however, inadequately support
metacognitive reflection. Spaced Repetition Systems (SRS) use de-contextualized
review, overlooking the role of context, while Personal Knowledge Management
(PKM) tools require high manual maintenance.
  To address these challenges, this paper introduces "Insight Recall," a novel
paradigm that conceptualizes the context-triggered retrieval of personal past
insights as a metacognitive scaffold to promote SRL. We formalize this paradigm
using the Just-in-Time Adaptive Intervention (JITAI) framework and implement a
prototype system, Irec, to demonstrate its feasibility. At its core, Irec uses
a dynamic knowledge graph of the user's learning history. When a user faces a
new problem, a hybrid retrieval engine recalls relevant personal "insights."
Subsequently, a large language model (LLM) performs a deep similarity
assessment to filter and present the most relevant scaffold in a just-in-time
manner. To reduce cognitive load, Irec features a human-in-the-loop pipeline
for LLM-based knowledge graph construction. We also propose an optional "Guided
Inquiry" module, where users can engage in a Socratic dialogue with an expert
LLM, using the current problem and recalled insights as context. The
contribution of this paper is a solid theoretical framework and a usable system
platform for designing next-generation intelligent learning systems that
enhance metacognition and self-regulation.

</details>


### [280] [AI in the Writing Process: How Purposeful AI Support Fosters Student Writing](https://arxiv.org/abs/2506.20595)
*Momin N. Siddiqui,Roy Pea,Hari Subramonyam*

Main category: cs.HC

TL;DR: This paper compares the effectiveness of AI writing tools on learner agency and knowledge transformation, using a study with 90 undergraduate students.


<details>
  <summary>Details</summary>
Motivation: To address concerns about AI tools like ChatGPT reducing learner agency and superficial content engagement in student writing.

Method: A randomized control trial comparing three writing conditions: (1) chat-based LLM assistant, (2) integrated AI tool targeting specific writing subprocesses, and (3) standard writing interface (control).

Result: Students using the integrated AI tool showed more agency in their writing and deeper knowledge transformation compared to the other conditions.

Conclusion: Thoughtfully designed AI writing tools can enhance both student ownership of their work and meaningful engagement with writing content.

Abstract: The ubiquity of technologies like ChatGPT has raised concerns about their
impact on student writing, particularly regarding reduced learner agency and
superficial engagement with content. While standalone chat-based LLMs often
produce suboptimal writing outcomes, evidence suggests that purposefully
designed AI writing support tools can enhance the writing process. This paper
investigates how different AI support approaches affect writers' sense of
agency and depth of knowledge transformation. Through a randomized control
trial with 90 undergraduate students, we compare three conditions: (1) a
chat-based LLM writing assistant, (2) an integrated AI writing tool to support
diverse subprocesses, and (3) a standard writing interface (control). Our
findings demonstrate that, among AI-supported conditions, students using the
integrated AI writing tool exhibited greater agency over their writing process
and engaged in deeper knowledge transformation overall. These results suggest
that thoughtfully designed AI writing support targeting specific aspects of the
writing process can help students maintain ownership of their work while
facilitating improved engagement with content.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [281] [Speaker Embeddings to Improve Tracking of Intermittent and Moving Speakers](https://arxiv.org/abs/2506.19875)
*Taous Iatariene,Can Cui,Alexandre Guérin,Romain Serizel*

Main category: eess.AS

TL;DR: The paper introduces a method to improve speaker tracking in scenarios with moving and inactive speakers using speaker embeddings and post-tracking identity reassignment, showing consistent performance improvement.


<details>
  <summary>Details</summary>
Motivation: Traditional speaker tracking struggles in cases where speakers change positions during inactivity, resulting in fragmented spatial trajectories. This raises problems in accurate identity assignment.

Method: The authors propose a method leveraging speaker embeddings for post-tracking identity reassignment. Beamforming is utilized to enhance audio from speakers' positions for embedding extraction.

Result: The approach consistently improves identity assignment performance across both standard and neural tracking systems. The evaluation explores the impact of beamforming and input duration on embedding quality.

Conclusion: Embedding-based speaker tracking reassignment proves effective for handling challenging scenarios where speakers move after being inactive. This method enhances the reliability of speaker tracking systems.

Abstract: Speaker tracking methods often rely on spatial observations to assign
coherent track identities over time. This raises limits in scenarios with
intermittent and moving speakers, i.e., speakers that may change position when
they are inactive, thus leading to discontinuous spatial trajectories. This
paper proposes to investigate the use of speaker embeddings, in a simple
solution to this issue. We propose to perform identity reassignment
post-tracking, using speaker embeddings. We leverage trajectory-related
information provided by an initial tracking step and multichannel audio signal.
Beamforming is used to enhance the signal towards the speakers' positions in
order to compute speaker embeddings. These are then used to assign new track
identities based on an enrollment pool. We evaluate the performance of the
proposed speaker embedding-based identity reassignment method on a dataset
where speakers change position during inactivity periods. Results show that it
consistently improves the identity assignment performance of neural and
standard tracking systems. In particular, we study the impact of beamforming
and input duration for embedding extraction.

</details>


### [282] [MATER: Multi-level Acoustic and Textual Emotion Representation for Interpretable Speech Emotion Recognition](https://arxiv.org/abs/2506.19887)
*Hyo Jin Jon,Longbin Jin,Hyuntaek Jung,Hyunseo Kim,Donghun Min,Eun Yi Kim*

Main category: eess.AS

TL;DR: The paper introduces MATER, a novel framework for speech emotion recognition, achieving notable competitive results in a challenge.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of natural speech emotion detection, specifically managing intra- and inter-subject variability and annotator inconsistency.

Method: The paper proposes MATER, a hierarchical framework that integrates acoustic and textual features at different levels (word, utterance, and embedding). It also incorporates an uncertainty-aware ensemble strategy.

Result: MATER ranked fourth in both tasks of the challenge and achieved second place in valence prediction with a CCC of 0.6941.

Conclusion: The proposed MATER framework is effective at handling complex, ambiguous emotional expressions, demonstrating strong competitive performance.

Abstract: This paper presents our contributions to the Speech Emotion Recognition in
Naturalistic Conditions (SERNC) Challenge, where we address categorical emotion
recognition and emotional attribute prediction. To handle the complexities of
natural speech, including intra- and inter-subject variability, we propose
Multi-level Acoustic-Textual Emotion Representation (MATER), a novel
hierarchical framework that integrates acoustic and textual features at the
word, utterance, and embedding levels. By fusing low-level lexical and acoustic
cues with high-level contextualized representations, MATER effectively captures
both fine-grained prosodic variations and semantic nuances. Additionally, we
introduce an uncertainty-aware ensemble strategy to mitigate annotator
inconsistencies, improving robustness in ambiguous emotional expressions. MATER
ranks fourth in both tasks with a Macro-F1 of 41.01% and an average CCC of
0.5928, securing second place in valence prediction with an impressive CCC of
0.6941.

</details>


<div id='physics.app-ph'></div>

# physics.app-ph [[Back]](#toc)

### [283] [Development of an Open-Source Spacecraft Bus for the PULSE-A CubeSat](https://arxiv.org/abs/2506.20014)
*Graydon Schulze-Kalt,Robert Pitu,Spencer Shelton,Catherine Todd,Zane Ebel,Ian Goldberg,Leon Gold,Henry Czarnecki,Mason McCormack,Larry Li,Zumi Riekse,Brian Yu,Akash Piya,Vidya Suri,Dylan Hu,Colleen Kim,John Baird,Seth Knights,Logan Hanssler,Michael Lembeck,Tian Zhong*

Main category: physics.app-ph

TL;DR: The PULSE-A project is an undergraduate-led effort demonstrating circular polarization shift keyed laser communication via a satellite. It utilizes an affordable, open-source satellite bus, designed for adaptability and efficiency.


<details>
  <summary>Details</summary>
Motivation: The study aims to create a low-cost, modular satellite bus that meets the requirements of an innovative communication mission while offering scalability for future projects.

Method: The satellite's bus design integrates dual BeagleBone Black Industrial units, a Gran Systems' 3U frame with custom modifications, and a software architecture based on Goddard Space Flight Center's core Flight System (cFS). Iterative engineering was applied for structural, thermal, and power optimizations.

Result: PULSE-A developed a compact, adaptable satellite structure that satisfies technical goals, including precision pointing, thermal stability, and modularity for future scientific missions.

Conclusion: The project demonstrates the feasibility of using an open-source, cost-efficient platform for advanced satellite communication missions, providing a viable model for low-resource teams.

Abstract: The undergraduate-led Polarization-modUlated Laser Satellite Experiment
(PULSE-A) at the University of Chicago seeks to demonstrate the feasibility of
circular polarization shift keyed satellite-to-ground laser communication.
PULSE-A's low-cost open-source bus serves as the backbone of the mission and
has been designed in tandem with the Payload, with design driven by strict
requirements for pointing accuracy, component alignment, power demand, and
thermal stability. This work presents the design and testing of the PULSE-A
bus.
  The spacecraft bus was designed to fill two major needs: (1) to meet the
requirements of the PULSE-A mission, and (2) to be easily configurable for
future missions that desire enhanced capabilities over other low-cost
open-source designs. At its core, the bus features dual BeagleBone Black
Industrial compute units, selected for their flight heritage, integrated via a
PC/104 header standard. PULSE-A implements Goddard Space Flight Center's core
Flight System (cFS), which takes a modular software architecture approach and
is built in C. The use of C as the primary language aligns with the expertise
of the University of Chicago's Computer Science department, allowing for ease
of development by PULSE-A's undergraduate flight software team.
  The CubeSat structure utilizes Gran Systems' 3U frame, modified to
accommodate openings for various ports and deployable components. Inside, the
avionics stack uses the PC/104 standard quad rails, which terminate in
PULSE-A's custom-designed Payload Box that houses all of the Payload components
and optical fiber runs. This work also covers the techniques and iterative
engineering processes used to develop the thermal control and dissipation
mechanisms for the specific requirements, under volume, mass, and
temperature-range constraints.

</details>


<div id='hep-ph'></div>

# hep-ph [[Back]](#toc)

### [284] [DeepQuark: deep-neural-network approach to multiquark bound states](https://arxiv.org/abs/2506.20555)
*Wei-Lin Wu,Lu Meng,Shi-Lin Zhu*

Main category: hep-ph

TL;DR: The paper presents DeepQuark, a new deep-neural-network-based variational Monte Carlo method for studying complex multiquark systems, outperforming existing approaches in certain areas such as pentaquark and tetraquark analysis.


<details>
  <summary>Details</summary>
Motivation: The study aims to address computational challenges in multiquark bound-state systems, including strong correlations and intractable confinement interactions, which surpass complexities in simpler systems like electrons or nucleons.

Method: The DeepQuark architecture utilizes deep neural networks in a variational Monte Carlo framework to address stronger SU(3) color interactions, discrete quantum numbers, and three-body confinement interactions efficiently.

Result: DeepQuark performs competitively and even surpasses state-of-the-art methods in pentaquark and tetraquark systems. It successfully determines properties of both compact and molecule-like multiquark combinations and offers recommendations for experimental verification.

Conclusion: The approach shows promise for studying larger multiquark systems and offers insights into nonperturbative quantum chromodynamics and many-body physics. It demonstrates significant computational advantages over conventional methods.

Abstract: For the first time, we implement the deep-neural-network-based variational
Monte Carlo approach for the multiquark bound states, whose complexity
surpasses that of electron or nucleon systems due to strong SU(3) color
interactions. We design a novel and high-efficiency architecture, DeepQuark, to
address the unique challenges in multiquark systems such as stronger
correlations, extra discrete quantum numbers, and intractable confinement
interaction. Our method demonstrates competitive performance with
state-of-the-art approaches, including diffusion Monte Carlo and Gaussian
expansion method, in the nucleon, doubly heavy tetraquark, and fully heavy
tetraquark systems. Notably, it outperforms existing calculations for
pentaquarks, exemplified by the triply heavy pentaquark. For the nucleon, we
successfully incorporate three-body flux-tube confinement interactions without
additional computational costs. In tetraquark systems, we consistently describe
hadronic molecule $T_{cc}$ and compact tetraquark $T_{bb}$ with an unbiased
form of wave function ansatz. In the pentaquark sector, we obtain weakly bound
$\bar D^*\Xi_{cc}^*$ molecule $P_{cc\bar c}(5715)$ with $S=\frac{5}{2}$ and its
bottom partner $P_{bb\bar b}(15569)$. They can be viewed as the analogs of the
molecular $T_{cc}$. We recommend experimental search of $P_{cc\bar c}(5715)$ in
the D-wave $J/\psi \Lambda_c$ channel. DeepQuark holds great promise for
extension to larger multiquark systems, overcoming the computational barriers
in conventional methods. It also serves as a powerful framework for exploring
confining mechanism beyond two-body interactions in multiquark states, which
may offer valuable insights into nonperturbative QCD and general many-body
physics.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [285] [An ab initio foundation model of wavefunctions that accurately describes chemical bond breaking](https://arxiv.org/abs/2506.19960)
*Adam Foster,Zeno Schätzle,P. Bernát Szabó,Lixue Cheng,Jonas Köhler,Gino Cassella,Nicholas Gao,Jiawei Li,Frank Noé,Jan Hermann*

Main category: physics.chem-ph

TL;DR: Orbformer leverages pretraining on a large dataset of molecular structures to provide highly accurate quantum chemistry solutions at reduced computational cost, especially for challenging multireferential problems.


<details>
  <summary>Details</summary>
Motivation: Describing bond breaking accurately is computationally expensive due to multireferential electronic structures inherent in dissociating species. Existing methods require high costs and recalculation for different molecules, ignoring shared electronic structure features across systems.

Method: Orbformer employs deep QMC and pretraining on 22,000 molecular structures, enabling fine-tuning for unseen molecules. It introduces a transferable wavefunction model optimized for computational efficiency and accuracy.

Result: Orbformer consistently achieves chemical accuracy (1 kcal/mol) across benchmarks, challenging bond dissociation scenarios, and Diels-Alder reactions, outperforming classical multireferential methods.

Conclusion: The work establishes a practical approach for using pretrained deep neural networks in quantum chemistry, amortizing computational costs over multiple molecular systems and advancing multireferential analysis solutions.

Abstract: Reliable description of bond breaking remains a major challenge for quantum
chemistry due to the multireferential character of the electronic structure in
dissociating species. Multireferential methods in particular suffer from large
computational cost, which under the normal paradigm has to be paid anew for
each system at a full price, ignoring commonalities in electronic structure
across molecules. Quantum Monte Carlo with deep neural networks (deep QMC)
uniquely offers to exploit such commonalities by pretraining transferable
wavefunction models, but all such attempts were so far limited in scope. Here,
we bring this new paradigm to fruition with Orbformer, a novel transferable
wavefunction model pretrained on 22,000 equilibrium and dissociating structures
that can be fine-tuned on unseen molecules reaching an accuracy-cost ratio
rivalling classical multireferential methods. On established benchmarks as well
as more challenging bond dissociations and Diels-Alder reactions, Orbformer is
the only method that consistently converges to chemical accuracy (1 kcal/mol).
This work turns the idea of amortizing the cost of solving the Schr\"odinger
equation over many molecules into a practical approach in quantum chemistry.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [286] [Machine-Learning-Assisted Photonic Device Development: A Multiscale Approach from Theory to Characterization](https://arxiv.org/abs/2506.20056)
*Yuheng Chen,Alexander Montes McNeil,Taehyuk Park,Blake A. Wilson,Vaishnavi Iyer,Michael Bezick,Jae-Ik Choi,Rohan Ojha,Pravin Mahendran,Daksh Kumar Singh,Geetika Chitturi,Peigang Chen,Trang Do,Alexander V. Kildishev,Vladimir M. Shalaev,Michael Moebius,Wenshan Cai,Yongmin Liu,Alexandra Boltasseva*

Main category: physics.optics

TL;DR: The paper discusses leveraging machine learning techniques to enhance photonic device development (PDD) processes.


<details>
  <summary>Details</summary>
Motivation: PDD faces challenges like computational infeasibility, high costs, and scalability issues using classical methods, necessitating alternative approaches.

Method: Advancing PDD using ML techniques like generative modeling, surrogate estimators, reinforcement learning, and active learning to tackle simulation, noise modeling, fabrication, and discovery challenges.

Result: Machine learning has shown potential to accelerate PDD by optimizing simulations, handling noisy measurements, and improving fabrication processes.

Conclusion: The paper emphasizes ML-assisted PDD as a promising direction to overcome classical PDD limitations, fostering interdisciplinary research to evolve photonic devices.

Abstract: Photonic device development (PDD) has achieved remarkable success in
designing and implementing new devices for controlling light across various
wavelengths, scales, and applications, including telecommunications, imaging,
sensing, and quantum information processing. PDD is an iterative, five-step
process that consists of: i) deriving device behavior from design parameters,
ii) simulating device performance, iii) finding the optimal candidate designs
from simulations, iv) fabricating the optimal device, and v) measuring device
performance. Classically, all these steps involve Bayesian optimization,
material science, control theory, and direct physics-driven numerical methods.
However, many of these techniques are computationally intractable, monetarily
costly, or difficult to implement at scale. In addition, PDD suffers from large
optimization landscapes, uncertainties in structural or optical
characterization, and difficulties in implementing robust fabrication
processes. However, the advent of machine learning over the past decade has
provided novel, data-driven strategies for tackling these challenges, including
surrogate estimators for speeding up computations, generative modeling for
noisy measurement modeling and data augmentation, reinforcement learning for
fabrication, and active learning for experimental physical discovery. In this
review, we present a comprehensive perspective on these methods to enable
machine-learning-assisted PDD (ML-PDD) for efficient design optimization with
powerful generative models, fast simulation and characterization modeling under
noisy measurements, and reinforcement learning for fabrication. This review
will provide researchers from diverse backgrounds with valuable insights into
this emerging topic, fostering interdisciplinary efforts to accelerate the
development of complex photonic devices and systems.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [287] [Physics-Guided Radiotherapy Treatment Planning with Deep Learning](https://arxiv.org/abs/2506.19880)
*Stefanos Achlatis,Efstratios Gavves,Jan-Jakob Sonke*

Main category: physics.med-ph

TL;DR: The study proposes a two-stage deep learning model to automate radiotherapy planning, demonstrating promising results in generating accurate and efficient VMAT treatment plans.


<details>
  <summary>Details</summary>
Motivation: Radiotherapy adaptations to anatomical variations are time-intensive, necessitating automated solutions to improve efficiency and precision.

Method: A two-stage physics-guided deep learning pipeline was developed. The first stage involves supervised training on treatment parameters (MLC and MU values), while the second stage integrates physics-based signals derived from predicted 3D dose distributions.

Result: The method was tested on 133 prostate cancer cases, producing VMAT plans closely matching clinical ground truths, with minimal deviations in key metrics (e.g., D95% = 0.42 +/- 1.83 Gy). It also reduced radiation exposure to organs at risk.

Conclusion: Physics-guided deep learning can enhance radiotherapy planning by ensuring accurate dosage and minimizing collateral damage, underscoring its potential as a tool for automated RT improvements.

Abstract: Radiotherapy (RT) is a critical cancer treatment, with volumetric modulated
arc therapy (VMAT) being a commonly used technique that enhances dose
conformity by dynamically adjusting multileaf collimator (MLC) positions and
monitor units (MU) throughout gantry rotation. Adaptive radiotherapy requires
frequent modifications to treatment plans to account for anatomical variations,
necessitating time-efficient solutions. Deep learning offers a promising
solution to automate this process. To this end, we propose a two-stage,
physics-guided deep learning pipeline for radiotherapy planning. In the first
stage, our network is trained with direct supervision on treatment plan
parameters, consisting of MLC and MU values. In the second stage, we
incorporate an additional supervision signal derived from the predicted 3D dose
distribution, integrating physics-based guidance into the training process. We
train and evaluate our approach on 133 prostate cancer patients treated with a
uniform 2-arc VMAT protocol delivering a dose of 62 Gy to the planning target
volume (PTV). Our results demonstrate that the proposed approach, implemented
using both 3D U-Net and UNETR architectures, consistently produces treatment
plans that closely match clinical ground truths. Our method achieves a mean
difference of D95% = 0.42 +/- 1.83 Gy and V95% = -0.22 +/- 1.87% at the PTV
while generating dose distributions that reduce radiation exposure to organs at
risk. These findings highlight the potential of physics-guided deep learning in
RT planning.

</details>


### [288] [Neural networks for the prediction of peel force for skin adhesive interface using FEM simulation](https://arxiv.org/abs/2506.19855)
*Ashish Masarkar,Rakesh Gupta,Naga Neehar Dingari,Beena Rai*

Main category: physics.med-ph

TL;DR: The study developed a neural network model to predict the minimum peel force required for adhesive detachment from skin, drastically reducing computational time compared to traditional methods like FEM simulations.


<details>
  <summary>Details</summary>
Motivation: The motivation was to overcome the resource-intensive and time-consuming nature of experimental testing and FEM simulations for analyzing skin-adhesive peeling behavior, particularly in exploring wide material parameter spaces.

Method: The paper used a dataset from FEM simulations of 90-degree peel tests, varying adhesive and fracture mechanics parameters, to train a neural network validated through 5-fold cross-validation.

Result: The resulting neural network model achieved an impressive mean squared error of 3.66*10^-7 and an R² score of 0.94 on the test set, reflecting high predictive accuracy.

Conclusion: The study offers a computationally efficient framework incorporating machine learning into biomechanical simulations, facilitating optimized design of skin-adhesive systems and advancing bio-adhesive technology research.

Abstract: Studying the peeling behaviour of adhesives on skin is vital for advancing
biomedical applications such as medical adhesives and transdermal patches.
Traditional methods like experimental testing and finite element method (FEM),
though considered gold standards, are resource-intensive, computationally
expensive and time-consuming, particularly when analysing a wide material
parameter space. In this study, we present a neural network-based approach to
predict the minimum peel force (F_min) required for adhesive detachment from
skin tissue, limiting the need for repeated FEM simulations and significantly
reducing the computational cost. Leveraging a dataset generated from FEM
simulations of 90 degree peel test with varying adhesive and fracture mechanics
parameters, our neural network model achieved high accuracy, validated through
rigorous 5-fold cross-validation. The final architecture was able to predict a
wide variety of skin-adhesive peeling behaviour, exhibiting a mean squared
error (MSE) of 3.66*10^-7 and a R^2 score of 0.94 on test set, demonstrating
robust performance. This work introduces a reliable, computationally efficient
method for predicting adhesive behaviour, significantly reducing simulation
time while maintaining accuracy. This integration of machine learning with
high-fidelity biomechanical simulations enables efficient design and
optimization of skin-adhesive systems, providing a scalable framework for
future research in computational dermato-mechanics and bio-adhesive material
design.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [289] [Accept More, Reject Less: Reducing up to 19% Unnecessary Desk-Rejections over 11 Years of ICLR Data](https://arxiv.org/abs/2506.20141)
*Xiaoyu Li,Zhao Song,Jiahao Zhang*

Main category: cs.DS

TL;DR: The paper proposes a new algorithm to optimize desk-rejection policies for AI conference submissions, minimizing needless rejections.


<details>
  <summary>Details</summary>
Motivation: The explosive growth of AI research has led to stringent per-author submission limits at conferences. This often results in valuable papers being rejected arbitrarily based on paper IDs, which penalizes authors' efforts and wastes valuable research.

Method: The authors formalize desk-rejection policies as an optimization problem and propose a linear programming relaxation and rounding-based algorithm to optimize these policies.

Result: The method is evaluated on 11 years of ICLR data, preserving up to 19.23% more papers without exceeding submission limits. Additionally, it demonstrates computational efficiency, with results computed in under 54 seconds.

Conclusion: The study introduces a practical and efficient strategy for reducing unnecessary desk rejections, offering the potential to improve submission policies at AI conferences.

Abstract: The explosive growth of AI research has driven paper submissions at flagship
AI conferences to unprecedented levels, necessitating many venues in 2025
(e.g., CVPR, ICCV, KDD, AAAI, IJCAI, WSDM) to enforce strict per-author
submission limits and to desk-reject any excess papers by simple ID order.
While this policy helps reduce reviewer workload, it may unintentionally
discard valuable papers and penalize authors' efforts. In this paper, we ask an
essential research question on whether it is possible to follow submission
limits while minimizing needless rejections. We first formalize the current
desk-rejection policies as an optimization problem, and then develop a
practical algorithm based on linear programming relaxation and a rounding
scheme. Under extensive evaluation on 11 years of real-world ICLR
(International Conference on Learning Representations) data, our method
preserves up to $19.23\%$ more papers without violating any author limits.
Moreover, our algorithm is highly efficient in practice, with all results on
ICLR data computed within at most 53.64 seconds. Our work provides a simple and
practical desk-rejection strategy that significantly reduces unnecessary
rejections, demonstrating strong potential to improve current CS conference
submission policies.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [290] [Quantum Neural Networks for Propensity Score Estimation and Survival Analysis in Observational Biomedical Studies](https://arxiv.org/abs/2506.19973)
*Vojtěch Novák,Ivan Zelinka,Lenka Přibylová,Lubomír Martínek*

Main category: quant-ph

TL;DR: The study leverages quantum neural networks (QNNs) for propensity score estimation and demonstrates their value in addressing selection bias in surgical methods for colorectal carcinoma patients, outperforming classical methods in specific scenarios.


<details>
  <summary>Details</summary>
Motivation: To explore the capabilities of quantum neural networks (QNNs) in addressing selection bias for causal inference within small-sample, high-dimensional datasets, specifically in comparing surgical outcomes for colorectal carcinoma patients.

Method: Developed QNN-based propensity score models using a linear ZFeatureMap, SummedPaulis operator, Covariance Matrix Adaptation Evolution Strategy (CMA-ES), and variance regularization. Simulated performance under exact, sampling, and noisy quantum conditions. Compared results with classical logistic regression and gradient boosted machines.

Result: QNNs, particularly under noisy simulated hardware settings, outperformed classical methods in small sample scenarios, achieving an AUC up to 0.750 for n=100 and providing stable, noise-modeled predictions. Propensity score adjustments achieved strong covariate balance with no significant survival differences post-adjustment.

Conclusion: QNNs, enhanced by noise-aware strategies and CMA-ES, show promise in improving causal inference in biomedical research, especially for challenging small-sample, high-dimensional datasets. This suggests potential for further application in similar analytical scenarios.

Abstract: This study investigates the application of quantum neural networks (QNNs) for
propensity score estimation to address selection bias in comparing survival
outcomes between laparoscopic and open surgical techniques in a cohort of 1177
colorectal carcinoma patients treated at University Hospital Ostrava
(2001-2009). Using a dataset with 77 variables, including patient demographics
and tumor characteristics, we developed QNN-based propensity score models
focusing on four key covariates (Age, Sex, Stage, BMI). The QNN architecture
employed a linear ZFeatureMap for data encoding, a SummedPaulis operator for
predictions, and the Covariance Matrix Adaptation Evolution Strategy (CMA-ES)
for robust, gradient-free optimization in noisy quantum environments. Variance
regularization was integrated to mitigate quantum measurement noise, with
simulations conducted under exact, sampling (1024 shots), and noisy hardware
(FakeManhattanV2) conditions. QNNs, particularly with simulated hardware noise,
outperformed classical logistic regression and gradient boosted machines in
small samples (AUC up to 0.750 for n=100), with noise modeling enhancing
predictive stability. Propensity score matching and weighting, optimized via
genetic matching and matching weights, achieved covariate balance with
standardized mean differences of 0.0849 and 0.0869, respectively. Survival
analyses using Kaplan-Meier estimation, Cox proportional hazards, and Aalen
additive regression revealed no significant survival differences
post-adjustment (p-values 0.287-0.851), indicating confounding bias in
unadjusted outcomes. These results highlight QNNs' potential, enhanced by
CMA-ES and noise-aware strategies, to improve causal inference in biomedical
research, particularly for small-sample, high-dimensional datasets.

</details>


### [291] [Practical insights on the effect of different encodings, ansätze and measurements in quantum and hybrid convolutional neural networks](https://arxiv.org/abs/2506.20355)
*Jesús Lozano-Cruz,Albert Nieto-Morales,Oriol Balló-Gimbernat,Adan Garriga,Antón Rodríguez-Otero,Alejandro Borrallo-Rentero*

Main category: quant-ph

TL;DR: This paper explores parameterized quantum circuits (PQCs) in hybrid and quantum convolutional neural networks for satellite image classification, analyzing 500 model configurations based on three factors.


<details>
  <summary>Details</summary>
Motivation: To understand the performance impact of design choices in PQC-based architectures for satellite image classification using the EuroSAT dataset.

Method: Systematic evaluation of encoding techniques, variational ansätze, and measurement strategies across hybrid and quantum neural network architectures.

Result: Hybrid models showed data encoding was the dominant factor (30% validation accuracy variance), while ansätze/measurement effects were minor (<5%). Purely quantum models had measurement strategy (30%) and encoding mapping (8%) as key drivers.

Conclusion: Data encoding is critical for hybrid models, while measurement strategy and encoding mapping significantly influence purely quantum models.

Abstract: This study investigates the design choices of parameterized quantum circuits
(PQCs) within quantum and hybrid convolutional neural network (HQNN and QCNN)
architectures, applied to the task of satellite image classification using the
EuroSAT dataset. We systematically evaluate the performance implications of
data encoding techniques, variational ans\"atze, and measurement in approx. 500
distinct model configurations. Our analysis reveals a clear hierarchy of
influence on model performance. For hybrid architectures, which were
benchmarked against their direct classical equivalents (e.g. the same
architecture with the PQCs removed), the data encoding strategy is the dominant
factor, with validation accuracy varying over 30% for distinct embeddings. In
contrast, the selection of variational ans\"atze and measurement basis had a
comparatively marginal effect, with validation accuracy variations remaining
below 5%. For purely quantum models, restricted to amplitude encoding,
performance was most dependent on the measurement protocol and the
data-to-amplitude mapping. The measurement strategy varied the validation
accuracy by up to 30% and the encoding mapping by around 8 percentage points.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [292] [Fast ground penetrating radar dual-parameter full waveform inversion method accelerated by hybrid compilation of CUDA kernel function and PyTorch](https://arxiv.org/abs/2506.20513)
*Lei Liu,Chao Song,Liangsheng He,Silin Wang,Xuan Feng,Cai Liu*

Main category: physics.geo-ph

TL;DR: The paper introduces a dual-parameter full waveform inversion (FWI) framework for ground-penetrating radar (GPR), blending CUDA programming with PyTorch for efficiency and flexibility in subsurface imaging.


<details>
  <summary>Details</summary>
Motivation: To enhance computational efficiency and practicality in subsurface imaging methods, particularly for ground-penetrating radar applications.

Method: A hybrid approach utilizing CUDA kernel functions integrated with PyTorch's automatic differentiation, enabling GPU-accelerated inversion for dielectric permittivity and electrical conductivity.

Result: The framework successfully achieved high-accuracy dual-parameter FWI on both synthetic and real-world GPR data, showcasing flexibility and extensibility in its application.

Conclusion: The developed framework offers a scalable and efficient solution for GPR-based subsurface imaging, applicable to a variety of fields such as civil engineering and geophysical exploration.

Abstract: This study proposes a high-performance dual-parameter full waveform inversion
framework (FWI) for ground-penetrating radar (GPR), accelerated through the
hybrid compilation of CUDA kernel functions and PyTorch. The method leverages
the computational efficiency of GPU programming while preserving the
flexibility and usability of Python-based deep learning frameworks. By
integrating customized CUDA kernels into PyTorch's automatic differentiation
mechanism, the framework enables accurate and efficient inversion of both
dielectric permittivity and electrical conductivity. Experimental evaluations
on synthetic data and real wavefield data demonstrate that the proposed method
achieves dual-parameter FWI for GPR data while maintaining high accuracy.
Moreover, the framework is flexible and extensible, supporting optional
regularization strategies such as total variation and multi-scale inversion.
These features make the proposed approach a practical and scalable framework
for rapid GPR-based subsurface imaging in applications including civil
engineering, environmental monitoring, and geophysical exploration.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [293] [Supervised Similarity for Firm Linkages](https://arxiv.org/abs/2506.19856)
*Ryan Samson,Adrian Banner,Luca Candelori,Sebastien Cottrell,Tiziana Di Matteo,Paul Duchnowski,Vahagn Kirakosyan,Jose Marques,Kharen Musaelian,Stefano Pasquali,Ryan Stever,Dario Villani*

Main category: q-fin.ST

TL;DR: The paper introduces Characteristic Vector Linkages (CVLs) to estimate firm linkages and applies Quantum Cognition Machine Learning (QCML) for similarity learning, showing improved trading strategies.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to establish a novel and effective mechanism for understanding firm linkages, aiming to enhance financial trading strategies.

Method: The authors estimate firm linkages using CVLs via two methods: simple Euclidean similarity and advanced QCML-based similarity learning.

Result: Both methods enable profitable momentum spillover trading strategies, but QCML-based similarity is more effective than Euclidean similarity.

Conclusion: Characteristic Vector Linkages with QCML similarity learning improve the construction and profitability of momentum spillover trading strategies, showcasing its utility over traditional methods.

Abstract: We introduce a novel proxy for firm linkages, Characteristic Vector Linkages
(CVLs). We use this concept to estimate firm linkages, first through Euclidean
similarity, and then by applying Quantum Cognition Machine Learning (QCML) to
similarity learning. We demonstrate that both methods can be used to construct
profitable momentum spillover trading strategies, but QCML similarity
outperforms the simpler Euclidean similarity.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [294] [A Multi-Modal Spatial Risk Framework for EV Charging Infrastructure Using Remote Sensing](https://arxiv.org/abs/2506.19860)
*Oktay Karakuş,Padraig Corcoran*

Main category: eess.SP

TL;DR: This paper proposes RSERI-EV, a framework that leverages multi-source data to assess the resilience of EV charging infrastructure under environmental and infrastructural risks, with a case study in Wales.


<details>
  <summary>Details</summary>
Motivation: To address the unexplored resilience of EV charging stations under environmental and infrastructural stress in the context of increasing importance for sustainable transport.

Method: Development of RSERI-EV, a risk assessment framework combining spatial data (flood risk, LST, NDVI, LULC, proximity to substations, road accessibility) and spatial graph analytics to generate a resilience score.

Result: A case study on Wales EV charger dataset was conducted, showing the feasibility of using a $k$-NN graph for neighbourhood-based resilience analysis.

Conclusion: RSERI-EV demonstrates the utility of integrating diverse data sources and spatial reasoning to support climate-resilient EV charging infrastructure.

Abstract: Electric vehicle (EV) charging infrastructure is increasingly critical to
sustainable transport systems, yet its resilience under environmental and
infrastructural stress remains underexplored. In this paper, we introduce
RSERI-EV, a spatially explicit and multi-modal risk assessment framework that
combines remote sensing data, open infrastructure datasets, and spatial graph
analytics to evaluate the vulnerability of EV charging stations. RSERI-EV
integrates diverse data layers, including flood risk maps, land surface
temperature (LST) extremes, vegetation indices (NDVI), land use/land cover
(LULC), proximity to electrical substations, and road accessibility to generate
a composite Resilience Score. We apply this framework to the country of Wales
EV charger dataset to demonstrate its feasibility. A spatial $k$-nearest
neighbours ($k$NN) graph is constructed over the charging network to enable
neighbourhood-based comparisons and graph-aware diagnostics. Our prototype
highlights the value of multi-source data fusion and interpretable spatial
reasoning in supporting climate-resilient, infrastructure-aware EV deployment.

</details>


### [295] [OLALa: Online Learned Adaptive Lattice Codes for Heterogeneous Federated Learning](https://arxiv.org/abs/2506.20297)
*Natalie Lang,Maya Simhi,Nir Shlezinger*

Main category: eess.SP

TL;DR: This paper introduces OLALa, a federated learning framework with adaptive online lattice quantizers, improving learning performance under dynamic conditions.


<details>
  <summary>Details</summary>
Motivation: Existing FL schemes with fixed quantization rules struggle in heterogeneous environments where model updates vary significantly. The paper aims to enhance FL communication efficiency and adaptability in such cases.

Method: The authors derive convergence guarantees for FL using non-fixed lattice quantizers and propose an online learning algorithm for clients to adjust their quantizers with lightweight computations.

Result: Experimental results indicate that OLALa consistently outperforms fixed quantization schemes across various configurations, demonstrating enhanced learning performance under constrained communication rates.

Conclusion: OLALa enables efficient federated learning by introducing adaptive quantization, ensuring better adaptability and learning performance in heterogeneous settings.

Abstract: Federated learning (FL) enables collaborative training across distributed
clients without sharing raw data, often at the cost of substantial
communication overhead induced by transmitting high-dimensional model updates.
This overhead can be alleviated by having the clients quantize their model
updates, with dithered lattice quantizers identified as an attractive scheme
due to its structural simplicity and convergence-preserving properties.
However, existing lattice-based FL schemes typically rely on a fixed
quantization rule, which is suboptimal in heterogeneous and dynamic
environments where the model updates distribution varies across users and
training rounds. In this work, we propose Online Learned Adaptive Lattices
(OLALa), a heterogeneous FL framework where each client can adjust its
quantizer online using lightweight local computations. We first derive
convergence guarantees for FL with non-fixed lattice quantizers and show that
proper lattice adaptation can tighten the convergence bound. Then, we design an
online learning algorithm that enables clients to tune their quantizers
throughout the FL process while exchanging only a compact set of quantization
parameters. Numerical experiments demonstrate that OLALa consistently improves
learning performance under various quantization rates, outperforming
conventional fixed-codebook and non-adaptive schemes.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [296] [Finite-Time Information-Theoretic Bounds in Queueing Control](https://arxiv.org/abs/2506.18278)
*Yujie Liu,Vincent Y. F. Tan,Yunbei Xu*

Main category: math.OC

TL;DR: This paper establishes finite-time performance lower bounds for stochastic processing networks and presents new scheduling rules surpassing the well-known MaxWeight policy.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of MaxWeight policy, which guarantees only stability and asymptotic optimality, but not optimal backlog performance in finite-time horizons.

Method: The authors developed a minimax framework to evaluate performance, derived information-theoretic lower bounds on queue length, and proposed a new scheduling rule that includes second-order Lyapunov drift terms.

Result: The study shows MaxWeight is suboptimal in finite time scenarios for certain problems, while their new scheduling rule achieves performance close to the lower bound within universal constants.

Conclusion: The findings expose limitations of "drift-only" methods and propose a principled approach towards finite-time optimal queueing control.

Abstract: We establish the first finite-time information-theoretic lower bounds-and
derive new policies that achieve them-for the total queue length in scheduling
problems over stochastic processing networks with both adversarial and
stochastic arrivals. Prior analyses of MaxWeight guarantee only stability and
asymptotic optimality in heavy traffic; we prove that, at finite horizons,
MaxWeight can incur strictly larger backlog by problem-dependent factors which
we identify. Our main innovations are 1) a minimax framework that pinpoints the
precise problem parameters governing any policy's finite-time performance; 2)
an information-theoretic lower bound on total queue length; 3) fundamental
limitation of MaxWeight that it is suboptimal in finite time; and 4) a new
scheduling rule that minimizes the full Lyapunov drift-including its
second-order term-thereby matching the lower bound under certain conditions, up
to universal constants. These findings reveal a fundamental limitation on
"drift-only" methods and points the way toward principled, non-asymptotic
optimality in queueing control.

</details>


### [297] [A Complete Loss Landscape Analysis of Regularized Deep Matrix Factorization](https://arxiv.org/abs/2506.20344)
*Po Chen,Rujun Jiang,Peng Wang*

Main category: math.OC

TL;DR: This paper studies the optimization landscape of deep matrix factorization (DMF), offering theoretical insights and numerical validation to explain why gradient-based methods perform well.


<details>
  <summary>Details</summary>
Motivation: The optimization underpinnings of deep matrix factorization (DMF), despite its wide applications, are not well understood, prompting the need for a deeper analysis.

Method: The study analyzes the loss landscape of DMF by deriving closed-form expressions for critical points, determining their categorization (local optimizer, global optimizer, saddle points), and conducting numerical experiments.

Result: The paper identifies key conditions distinguishing critical points and demonstrates why gradient-based methods typically find local minima. Numerical experiments visually support this theoretical framework.

Conclusion: The work provides a rigorous understanding of the DMF optimization problem, shedding light on the efficiency of gradient-based methods and enriching theoretical knowledge of DMF.

Abstract: Despite its wide range of applications across various domains, the
optimization foundations of deep matrix factorization (DMF) remain largely
open. In this work, we aim to fill this gap by conducting a comprehensive study
of the loss landscape of the regularized DMF problem. Toward this goal, we
first provide a closed-form expression of all critical points. Building on
this, we establish precise conditions under which a critical point is a local
minimizer, a global minimizer, a strict saddle point, or a non-strict saddle
point. Leveraging these results, we derive a necessary and sufficient condition
under which each critical point is either a local minimizer or a strict saddle
point. This provides insights into why gradient-based methods almost always
converge to a local minimizer of the regularized DMF problem. Finally, we
conduct numerical experiments to visualize its loss landscape under different
settings to support our theory.

</details>


### [298] [First-order methods for stochastic and finite-sum convex optimization with deterministic constraints](https://arxiv.org/abs/2506.20630)
*Zhaosong Lu,Yifeng Xiao*

Main category: math.OC

TL;DR: The paper develops stochastic first-order methods to address constraints in stochastic convex optimization, ensuring deterministic constraint satisfaction and expected optimality gap within a tolerance.


<details>
  <summary>Details</summary>
Motivation: Existing optimization methods focus on solutions with expected (average) feasibility and optimality tolerances, which are inadequate in scenarios requiring near-certainty satisfaction of constraints.

Method: The authors propose accelerated stochastic gradient schemes on quadratic penalty subproblems, ensuring deterministic bounds on constraint violations and expected optimality gaps.

Result: They establish theoretical oracle complexity bounds for computing such robustly feasible solutions and extend their methods to sample average approximation problems.

Conclusion: The methods improve constraint satisfaction in stochastic optimization scenarios while providing rigorous complexity guarantees, addressing practical shortcomings of prior approaches.

Abstract: In this paper, we study a class of stochastic and finite-sum convex
optimization problems with deterministic constraints. Existing methods
typically aim to find an $\epsilon$-$expectedly\ feasible\ stochastic\ optimal$
solution, in which the expected constraint violation and expected optimality
gap are both within a prescribed tolerance $\epsilon$. However, in many
practical applications, constraints must be nearly satisfied with certainty,
rendering such solutions potentially unsuitable due to the risk of substantial
violations. To address this issue, we propose stochastic first-order methods
for finding an $\epsilon$-$surely\ feasible\ stochastic\ optimal$
($\epsilon$-SFSO) solution, where the constraint violation is deterministically
bounded by $\epsilon$ and the expected optimality gap is at most $\epsilon$.
Our methods apply an accelerated stochastic gradient (ASG) scheme or a modified
variance-reduced ASG scheme $only\ once$ to a sequence of quadratic penalty
subproblems with appropriately chosen penalty parameters. We establish
first-order oracle complexity bounds for the proposed methods in computing an
$\epsilon$-SFSO solution. As a byproduct, we also derive first-order oracle
complexity results for sample average approximation method in computing an
$\epsilon$-SFSO solution of the stochastic optimization problem using our
proposed methods to solve the sample average problem.

</details>
