{"id": "2510.00183", "pdf": "https://arxiv.org/pdf/2510.00183", "abs": "https://arxiv.org/abs/2510.00183", "authors": ["Ween Yang", "Jason Liu", "Suli Wang", "Xinyuan Song", "Lynn Ai", "Eric Yang", "Tianyu Shi"], "title": "Lattica: A Decentralized Cross-NAT Communication Framework for Scalable AI Inference and Training", "categories": ["cs.DC"], "comment": null, "summary": "The rapid expansion of distributed Artificial Intelligence (AI) workloads\nbeyond centralized data centers creates a demand for new communication\nsubstrates. These substrates must operate reliably in heterogeneous and\npermissionless environments, where Network Address Translators (NATs) and\nfirewalls impose significant constraints. Existing solutions, however, are\neither designed for controlled data center deployments or implemented as\nmonolithic systems that tightly couple machine learning logic with networking\ncode. To address these limitations, we present Lattica, a decentralized\ncross-NAT communication framework designed to support distributed AI systems.\nLattica integrates three core components. First, it employs a robust suite of\nNAT traversal mechanisms to establish a globally addressable peer-to-peer mesh.\nSecond, it provides a decentralized data store based on Conflict-free\nReplicated Data Types (CRDTs), ensuring verifiable and eventually consistent\nstate replication. Third, it incorporates a content discovery layer that\nleverages distributed hash tables (DHTs) together with an optimized RPC\nprotocol for efficient model synchronization. By integrating these components,\nLattica delivers a complete protocol stack for sovereign, resilient, and\nscalable AI systems that operate independently of centralized intermediaries.\nIt is directly applicable to edge intelligence, collaborative reinforcement\nlearning, and other large-scale distributed machine learning scenarios.", "AI": {"tldr": "The paper introduces Lattica, a decentralized framework optimized for distributed AI systems operating in heterogeneous and permissionless environments.", "motivation": "To address the challenges of cross-NAT communication and the limitations of monolithic systems in supporting modern distributed AI workloads.", "method": "Lattica integrates a peer-to-peer NAT traversal suite, a decentralized data store using CRDTs, and a content discovery layer with DHT-based model synchronization.", "result": "The framework provides a protocol stack that ensures resilient, sovereign, and scalable AI systems suitable for edge and collaborative learning environments.", "conclusion": "Lattica resolves critical communication barriers in distributed AI systems, enabling effective deployment in decentralized edge and machine learning applications without reliance on centralized intermediaries."}}
{"id": "2510.00207", "pdf": "https://arxiv.org/pdf/2510.00207", "abs": "https://arxiv.org/abs/2510.00207", "authors": ["Yunqi Gao", "Bing Hu", "Mahdi Boloursaz Mashhadi", "A-Long Jin", "Yanfeng Zhang", "Pei Xiao", "Rahim Tafazolli", "Merouane Debbah"], "title": "FlowMoE: A Scalable Pipeline Scheduling Framework for Distributed Mixture-of-Experts Training", "categories": ["cs.DC"], "comment": null, "summary": "The parameter size of modern large language models (LLMs) can be scaled up\nvia the sparsely-activated Mixture-of-Experts (MoE) technique to avoid\nexcessive increase of the computational costs. To further improve training\nefficiency, pipelining computation and communication has become a promising\nsolution for distributed MoE training. However, existing work primarily focuses\non scheduling tasks within the MoE layer, such as expert computing and\nall-to-all (A2A) communication, while neglecting other key operations including\nmulti-head attention (MHA) computing, gating, and all-reduce communication. In\nthis paper, we propose FlowMoE, a scalable framework for scheduling multi-type\ntask pipelines. First, FlowMoE constructs a unified pipeline to consistently\nscheduling MHA computing, gating, expert computing, and A2A communication.\nSecond, FlowMoE introduces a tensor chunk-based priority scheduling mechanism\nto overlap the all-reduce communication with all computing tasks. We implement\nFlowMoE as an adaptive and generic framework atop PyTorch. Extensive\nexperiments with 675 typical MoE layers and four real-world MoE models across\ntwo GPU clusters demonstrate that our proposed FlowMoE framework outperforms\nstate-of-the-art MoE training frameworks, reducing training time by 13%-57%,\nenergy consumption by 10%-39%, and memory usage by 7%-32%.", "AI": {"tldr": "FlowMoE is a framework designed to enhance the training efficiency of sparsely activated Mixture-of-Experts (MoE) models through optimized task scheduling and communication mechanisms.", "motivation": "To reduce the computational costs and training inefficiencies in large language models using Mixture-of-Experts (MoE) by addressing overlooked operations like multi-head attention (MHA) computing, gating, and all-reduce communication.", "method": "FlowMoE constructs a unified pipeline for scheduling various tasks (MHA, gating, etc.) and incorporates tensor chunk-based priority scheduling to overlap communication with computing tasks. It is implemented over PyTorch.", "result": "FlowMoE achieves significant improvements in MoE training frameworks, reducing training time by 13%-57%, energy consumption by 10%-39%, and memory usage by 7%-32%, validated on real-world models and GPU clusters.", "conclusion": "FlowMoE provides an adaptive and efficient framework for distributed MoE training, setting a benchmark for scalable and energy-efficient large language model training."}}
{"id": "2510.00306", "pdf": "https://arxiv.org/pdf/2510.00306", "abs": "https://arxiv.org/abs/2510.00306", "authors": ["Wenyang Jia", "Jingjing Wang", "Kai Lei"], "title": "BlockSDN-VC: A SDN-Based Virtual Coordinate-Enhanced Transaction Broadcast Framework for High-Performance Blockchains", "categories": ["cs.DC", "C.2.2; C.2.1; C.2.6; C.2.3; C.4"], "comment": "Accepted to IFIP International Conference on Network and Parallel\n  Computing (NPC 2025), LNCS format. Preprint. 12 pages", "summary": "Modern blockchains need fast, reliable propagation to balance security and\nthroughput. Virtual-coordinate methods speed dissemination but rely on slow\niterative updates, leaving nodes out of sync. We present BlockSDN-VC, a\ntransaction-broadcast protocol that centralises coordinate computation and\nforwarding control in an SDN controller, delivering global consistency, minimal\npath stretch and rapid response to churn or congestion. In geo-distributed\nsimulations, BlockSDN-VC cuts median latency by up to 62% and accelerates\nconvergence fourfold over state-of-the-art schemes with under 3% control-plane\noverhead. In a real blockchain environment, BlockSDN-VC boosts\nconfirmed-transaction throughput by 17% under adversarial workloads, requiring\nno modifications to existing clients.", "AI": {"tldr": "BlockSDN-VC is a blockchain transaction broadcast protocol that improves propagation speed and consistency through an SDN-based centralized approach, outperforming existing methods in latency, convergence, and throughput.", "motivation": "To address the need for a fast, reliable, and globally consistent transaction propagation mechanism in modern blockchains, which are hindered by latency and synchronization challenges in current virtual-coordinate methods.", "method": "The proposed method, BlockSDN-VC, uses an SDN controller to centralize coordinate computation and forwarding control, enabling efficient propagation with minimal path stretch and quick responses to network changes like churn or congestion.", "result": "BlockSDN-VC reduces median latency by up to 62% and achieves a fourfold faster convergence in simulations compared to current methods, with under 3% control-plane overhead. In real blockchain environments, it improves transaction throughput by 17% even under adversarial conditions.", "conclusion": "BlockSDN-VC demonstrates significant performance gains in speed, efficiency, and throughput, making it a practical and impactful solution for current blockchain systems without requiring client modifications."}}
{"id": "2510.00471", "pdf": "https://arxiv.org/pdf/2510.00471", "abs": "https://arxiv.org/abs/2510.00471", "authors": ["Yankai Jiang", "Raghavendra Kanakagiri", "Rohan Basu Roy", "Devesh Tiwari"], "title": "ThirstyFLOPS: Water Footprint Modeling and Analysis Toward Sustainable HPC Systems", "categories": ["cs.DC"], "comment": null, "summary": "High-performance computing (HPC) systems are becoming increasingly\nwater-intensive due to their reliance on water-based cooling and the energy\nused in power generation. However, the water footprint of HPC remains\nrelatively underexplored-especially in contrast to the growing focus on carbon\nemissions. In this paper, we present ThirstyFLOPS - a comprehensive water\nfootprint analysis framework for HPC systems. Our approach incorporates\nregion-specific metrics, including Water Usage Effectiveness, Power Usage\nEffectiveness, and Energy Water Factor, to quantify water consumption using\nreal-world data. Using four representative HPC systems - Marconi, Fugaku,\nPolaris, and Frontier - as examples, we provide implications for HPC system\nplanning and management. We explore the impact of regional water scarcity and\nnuclear-based energy strategies on HPC sustainability. Our findings aim to\nadvance the development of water-aware, environmentally responsible computing\ninfrastructures.", "AI": {"tldr": "This paper introduces ThirstyFLOPS, a framework assessing water usage in HPC systems, addressing regional-specific water footprints and impacts.", "motivation": "The rising water consumption of HPC systems due to cooling and energy production needs attention, especially with underexplored water footprints compared to carbon emissions.", "method": "The paper develops ThirstyFLOPS, leveraging metrics such as Water Usage Effectiveness, Power Usage Effectiveness, and Energy Water Factor, and uses data from four HPC systems.", "result": "Using systems like Marconi, Fugaku, Polaris, and Frontier, they highlight the effects of water scarcity and energy strategies\u2014offering meaningful insights for HPC planning.", "conclusion": "The study emphasizes creating sustainable, water-conscious HPC infrastructures by enhancing awareness of their environmental impact."}}
{"id": "2510.00932", "pdf": "https://arxiv.org/pdf/2510.00932", "abs": "https://arxiv.org/abs/2510.00932", "authors": ["Mohammad Zaeed", "Tanzima Z. Islam", "Vladimir In\u0111i\u0107"], "title": "Opal: A Modular Framework for Optimizing Performance using Analytics and LLMs", "categories": ["cs.PF"], "comment": "12 pages and 6 figures", "summary": "Large Language Models (LLMs) show promise for automated code optimization but\nstruggle without performance context. This work introduces Opal, a modular\nframework that connects performance analytics insights with the vast body of\npublished by guiding LLMs to generate informed, trustworthy optimizations.\nUnlike traditional performance tools that identify bottlenecks but stop short\nof actionable suggestions, Opal bridges this long-standing gap by linking\ndynamic insights from hardware counters and Roofline analysis to stall events\nto optimization decisions. We evaluate Opal across 1640 experiments on\nreal-world GPU kernels and find that in over 98.5% of cases, even a single\ninsight source yields speedups, ranging on average from 19.34% to 52.3%. Our\nprompt template produced correct code in all but one case, where a vague\ndiagnostic caused an unsafe suggestion. By automatically optimizing GPU kernels\nusing performance analytics and LLMs, Opal marks a leap toward democratizing\nexpert-level performance engineering for all.", "AI": {"tldr": "Opal is a framework that connects performance insights with guidance from Large Language Models (LLMs), achieving significant GPU kernel optimization speedups.", "motivation": "LLMs show potential in code optimization but lack performance context for trustworthiness and reliability.", "method": "Opal bridges insights from hardware counters and Roofline analysis to optimization decisions, leveraging modular frameworks and LLMs.", "result": "Opal achieved significant average speedups (19.34%\u201352.3%) across 1640 GPU kernel experiments, with nearly flawless code generation and few unsafe suggestions.", "conclusion": "Opal enables automated, expert-level GPU kernel optimization, advancing accessible performance engineering for broader use."}}
{"id": "2510.00877", "pdf": "https://arxiv.org/pdf/2510.00877", "abs": "https://arxiv.org/abs/2510.00877", "authors": ["Rodrigo Lankaites Pinheiro", "Dario Landa-Silva", "Jason Atkin"], "title": "A Technique Based on Trade-off Maps to Visualise and Analyse Relationships Between Objectives in Optimisation Problems", "categories": ["cs.NE", "cs.AI", "cs.HC", "math.OC"], "comment": "30 pages, journal paper", "summary": "Understanding the relationships between objectives in a multiobjective\noptimisation problem is important for developing tailored and efficient solving\ntechniques. In particular, when tackling combinatorial optimisation problems\nwith many objectives, that arise in real-world logistic scenarios, better\nsupport for the decision maker can be achieved through better understanding of\nthe often complex fitness landscape. This paper makes a contribution in this\ndirection by presenting a technique that allows a visualisation and analysis of\nthe local and global relationships between objectives in optimisation problems\nwith many objectives. The proposed technique uses four steps: First, the global\npairwise relationships are analysed using the Kendall correlation method; then,\nthe ranges of the values found on the given Pareto front are estimated and\nassessed; next, these ranges are used to plot a map using Gray code, similar to\nKarnaugh maps, that has the ability to highlight the trade-offs between\nmultiple objectives; and finally, local relationships are identified using\nscatter plots. Experiments are presented for three combinatorial optimisation\nproblems: multiobjective multidimensional knapsack problem, multiobjective\nnurse scheduling problem, and multiobjective vehicle routing problem with time\nwindows . Results show that the proposed technique helps in the gaining of\ninsights into the problem difficulty arising from the relationships between\nobjectives.", "AI": {"tldr": "The paper introduces a technique for visualizing and analyzing the relationships between objectives in multiobjective optimization problems, enabling better understanding of their fitness landscape and decision-making support.", "motivation": "To improve problem-solving techniques in multiobjective combinatorial optimization by understanding the often complex relationships between objectives, particularly in real-world logistical scenarios.", "method": "The technique involves four steps: analyzing global pairwise relationships using the Kendall correlation method, assessing Pareto front value ranges, plotting a Gray code-based map to show trade-offs, and identifying local relationships through scatter plots.", "result": "Experiments on three problems (multiobjective multidimensional knapsack, nurse scheduling, and vehicle routing with time windows) show the technique helps in understanding problem difficulty and objective relationships.", "conclusion": "The proposed approach enhances insight into objective relationships in multiobjective optimization, aiding in problem-solving and decision-making efficiency."}}
{"id": "2510.00002", "pdf": "https://arxiv.org/pdf/2510.00002", "abs": "https://arxiv.org/abs/2510.00002", "authors": ["Dong Liu"], "title": "PBFD and PDFD: Formally Defined and Verified Methodologies and Empirical Evaluation for Scalable Full-Stack Software Engineering", "categories": ["cs.SE"], "comment": "184 pages; 35 figures; A DOI-linked version of this paper and all\n  supplementary materials are available on Zenodo at\n  https://zenodo.org/records/16883985", "summary": "This paper introduces Primary Breadth-First Development (PBFD) and Primary\nDepth-First Development (PDFD), two formally defined and verified methodologies\nfor scalable, industrial-grade full-stack software engineering. These\napproaches bridge a longstanding gap between formal methods and real-world\ndevelopment practice by enforcing structural correctness through\ngraph-theoretic modeling. Unlike prior graph-based approaches, PBFD and PDFD\noperate over layered directed graphs and are formalized using unified state\nmachines and Communicating Sequential Processes (CSP) to ensure critical\nproperties, including bounded-refinement termination and structural\ncompleteness. To coordinate hierarchical data at scale, we propose Three-Level\nEncapsulation (TLE) - a novel, bitmask-based encoding scheme that delivers\nprovably constant-time updates. TLE's formal guarantees underpin PBFD's\nindustrial-scale performance and scalability. PBFD was empirically validated\nthrough an eight-year enterprise deployment, demonstrating over 20x faster\ndevelopment than Salesforce OmniScript and 7-8x faster query performance\ncompared to conventional relational models. Additionally, both methodologies\nare supported by open-source MVPs, with PDFD's implementation conclusively\ndemonstrating its correctness-first design principles. Together, PBFD and PDFD\nestablish a reproducible, transparent framework that integrates formal\nverification into practical software development. All formal specifications,\nMVPs, and datasets are publicly available to foster academic research and\nindustrial-grade adoption.", "AI": {"tldr": "The paper presents PBFD and PDFD, methods combining formal methods and real-world practices for scalable software engineering, employing innovative modeling and encoding for efficiency and correctness.", "motivation": "To bridge the gap between formal methods and real-world software development practices, ensuring scalability, performance, and correctness.", "method": "Introduces PBFD and PDFD using layered directed graphs, unified state machines, and CSP for structural correctness, along with TLE for constant-time data updates.", "result": "PBFD showed over 20x faster development than Salesforce OmniScript and superior query performance, validated in an eight-year enterprise deployment.", "conclusion": "PBFD and PDFD offer a formal, scalable, and efficient approach to software development, with open-source tools and datasets provided for broader adoption."}}
{"id": "2510.00154", "pdf": "https://arxiv.org/pdf/2510.00154", "abs": "https://arxiv.org/abs/2510.00154", "authors": ["Xinyi Liu", "Mohammadreza Fani Sani", "Zewei Zhou", "Julius Wirbel", "Bahram Zarrin", "Roberto Galeazzi"], "title": "RoboPilot: Generalizable Dynamic Robotic Manipulation with Dual-thinking Modes", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Despite rapid progress in autonomous robotics, executing complex or\nlong-horizon tasks remains a fundamental challenge. Most current approaches\nfollow an open-loop paradigm with limited reasoning and no feedback, resulting\nin poor robustness to environmental changes and severe error accumulation. We\npresent RoboPilot, a dual-thinking closed-loop framework for robotic\nmanipulation that supports adaptive reasoning for complex tasks in real-world\ndynamic environments. RoboPilot leverages primitive actions for structured task\nplanning and flexible action generation, while introducing feedback to enable\nreplanning from dynamic changes and execution errors. Chain-of-Thought\nreasoning further enhances high-level task planning and guides low-level action\ngeneration. The system dynamically switches between fast and slow thinking to\nbalance efficiency and accuracy. To systematically evaluate the robustness of\nRoboPilot in diverse robot manipulation scenarios, we introduce\nRoboPilot-Bench, a benchmark spanning 21 tasks across 10 categories, including\ninfeasible-task recognition and failure recovery. Experiments show that\nRoboPilot outperforms state-of-the-art baselines by 25.9\\% in task success\nrate, and the real-world deployment on an industrial robot further demonstrates\nits robustness in real-world settings.", "AI": {"tldr": "This paper introduces RoboPilot, a novel closed-loop system for robotic manipulation that combines structured planning, feedback, and adaptable reasoning for robust performance in dynamic environments.", "motivation": "Current robotics systems struggle with executing complex or long tasks due to reliance on open-loop paradigms, which suffer from error accumulation and lack adaptability to dynamic environments.", "method": "RoboPilot introduces a dual-thinking closed-loop framework that integrates structured task planning, flexible action generation, Chain-of-Thought reasoning, and dynamic switching between fast and slow thinking modes. A new benchmark, RoboPilot-Bench, evaluates this framework across 21 diverse robot manipulation tasks.", "result": "RoboPilot achieves a 25.9% higher task success rate than existing systems and demonstrates real-world robustness through deployment on industrial robots.", "conclusion": "RoboPilot enhances the robustness and accuracy of robotic manipulation in complex and dynamic scenarios by integrating advanced reasoning and feedback mechanisms, offering significant improvements over current approaches."}}
{"id": "2510.00073", "pdf": "https://arxiv.org/pdf/2510.00073", "abs": "https://arxiv.org/abs/2510.00073", "authors": ["Zhekai Li", "Tianyi Ma", "Cheng Hua", "Ruihao Zhu"], "title": "Identifying All \u03b5-Best Arms in (Misspecified) Linear Bandits", "categories": ["stat.ML", "cs.AI", "cs.LG", "math.ST", "stat.TH", "68T05", "G.3"], "comment": "80 pages (33 pages for main text), 12 figures, 3 tables", "summary": "Motivated by the need to efficiently identify multiple candidates in high\ntrial-and-error cost tasks such as drug discovery, we propose a near-optimal\nalgorithm to identify all {\\epsilon}-best arms (i.e., those at most {\\epsilon}\nworse than the optimum). Specifically, we introduce LinFACT, an algorithm\ndesigned to optimize the identification of all {\\epsilon}-best arms in linear\nbandits. We establish a novel information-theoretic lower bound on the sample\ncomplexity of this problem and demonstrate that LinFACT achieves instance\noptimality by matching this lower bound up to a logarithmic factor. A key\ningredient of our proof is to integrate the lower bound directly into the\nscaling process for upper bound derivation, determining the termination round\nand thus the sample complexity. We also extend our analysis to settings with\nmodel misspecification and generalized linear models. Numerical experiments,\nincluding synthetic and real drug discovery data, demonstrate that LinFACT\nidentifies more promising candidates with reduced sample complexity, offering\nsignificant computational efficiency and accelerating early-stage exploratory\nexperiments.", "AI": {"tldr": "The paper proposes LinFACT, a near-optimal algorithm for identifying all \u03b5-best arms in linear bandits, achieving instance optimality with reduced sample complexity.", "motivation": "The need for efficient selection methods in high trial-and-error cost tasks such as drug discovery.", "method": "Developed LinFACT algorithm and established information-theoretic lower bound for sample complexity, validated via numerical experiments.", "result": "LinFACT matches the lower bound on sample complexity up to a logarithmic factor, improving identification of \u03b5-best arms with computational efficiency.", "conclusion": "LinFACT significantly accelerates exploratory tasks in linear bandit settings, particularly benefiting applications like drug discovery."}}
{"id": "2510.00033", "pdf": "https://arxiv.org/pdf/2510.00033", "abs": "https://arxiv.org/abs/2510.00033", "authors": ["Usman Muhammad", "Jorma Laaksonen"], "title": "Hybrid Deep Learning for Hyperspectral Single Image Super-Resolution", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Hyperspectral single image super-resolution (SISR) is a challenging task due\nto the difficulty of restoring fine spatial details while preserving spectral\nfidelity across a wide range of wavelengths, which limits the performance of\nconventional deep learning models. To address this challenge, we introduce\nSpectral-Spatial Unmixing Fusion (SSUF), a novel module that can be seamlessly\nintegrated into standard 2D convolutional architectures to enhance both spatial\nresolution and spectral integrity. The SSUF combines spectral unmixing with\nspectral--spatial feature extraction and guides a ResNet-based convolutional\nneural network for improved reconstruction. In addition, we propose a custom\nSpatial-Spectral Gradient Loss function that integrates mean squared error with\nspatial and spectral gradient components, encouraging accurate reconstruction\nof both spatial and spectral features. Experiments on three public remote\nsensing hyperspectral datasets demonstrate that the proposed hybrid deep\nlearning model achieves competitive performance while reducing model\ncomplexity.", "AI": {"tldr": "The paper introduces Spectral-Spatial Unmixing Fusion (SSUF) and a custom Spatial-Spectral Gradient Loss to enhance hyperspectral single image super-resolution (SISR) performance, achieving competitive results on public datasets while reducing model complexity.", "motivation": "Hyperspectral SISR struggles with maintaining fine spatial details and spectral fidelity using conventional deep learning methods.", "method": "Spectral-Spatial Unmixing Fusion (SSUF) is integrated into 2D convolutional architectures, combined with a ResNet-based model and a custom loss function incorporating mean squared error, spatial, and spectral gradients.", "result": "The hybrid deep learning model delivers competitive performance on three public hyperspectral datasets and reduces model complexity.", "conclusion": "The proposed SSUF module and custom loss function effectively address hyperspectral SISR challenges, offering a balance between performance and complexity."}}
{"id": "2510.00022", "pdf": "https://arxiv.org/pdf/2510.00022", "abs": "https://arxiv.org/abs/2510.00022", "authors": ["Ansh Kamthan"], "title": "Learning to Lead Themselves: Agentic AI in MAS using MARL", "categories": ["cs.AI", "cs.MA"], "comment": "Exploring foundational behaviours of agentic ai using MARL 39 pages -\n  25 minute read, 5 tables, 24 equation, 9 figures", "summary": "As autonomous systems move from prototypes to real deployments, the ability\nof multiple agents to make decentralized, cooperative decisions becomes a core\nrequirement. This paper examines how agentic artificial intelligence, agents\nthat act independently, adaptively and proactively can improve task allocation\nand coordination in multi-agent systems, with primary emphasis on drone\ndelivery and secondary relevance to warehouse automation. We formulate the\nproblem in a cooperative multi-agent reinforcement learning setting and\nimplement a lightweight multi-agent Proximal Policy Optimization, called IPPO,\napproach in PyTorch under a centralized-training, decentralized-execution\nparadigm. Experiments are conducted in PettingZoo environment, where multiple\nhomogeneous drones or agents must self-organize to cover distinct targets\nwithout explicit communication.", "AI": {"tldr": "This paper explores decentralized decision-making in multi-agent systems, focusing on drone delivery, using a proposed IPPO reinforcement learning approach.", "motivation": "Autonomous systems need efficient task allocation and coordination for real-world deployment, especially for applications like drone delivery and warehouse automation.", "method": "They model the problem within a cooperative multi-agent reinforcement learning setting and propose a specialized lightweight Proximal Policy Optimization (IPPO) under centralized-training and decentralized-execution. Simulations were conducted in the PettingZoo environment with homogeneous drones.", "result": "Drones or agents effectively self-organized to cover distinct targets without explicit communication, demonstrating IPPO's capabilities.", "conclusion": "Agentic artificial intelligence, using IPPO, can significantly enhance task coordination and allocation in multi-agent systems."}}
{"id": "2510.00001", "pdf": "https://arxiv.org/pdf/2510.00001", "abs": "https://arxiv.org/abs/2510.00001", "authors": ["Noah Broestl", "Adel Nasser Abdalla", "Rajprakash Bale", "Hersh Gupta", "Max Struever"], "title": "Methodological Framework for Quantifying Semantic Test Coverage in RAG Systems", "categories": ["cs.LG", "cs.AI", "cs.SE"], "comment": "7 pages, 3 figures, 1 table, 1 algo", "summary": "Reliably determining the performance of Retrieval-Augmented Generation (RAG)\nsystems depends on comprehensive test questions. While a proliferation of\nevaluation frameworks for LLM-powered applications exists, current practices\nlack a systematic method to ensure these test sets adequately cover the\nunderlying knowledge base, leaving developers with significant blind spots. To\naddress this, we present a novel, applied methodology to quantify the semantic\ncoverage of RAG test questions against their underlying documents. Our approach\nleverages existing technologies, including vector embeddings and clustering\nalgorithms, to create a practical framework for validating test\ncomprehensiveness. Our methodology embeds document chunks and test questions\ninto a unified vector space, enabling the calculation of multiple coverage\nmetrics: basic proximity, content-weighted coverage, and multi-topic question\ncoverage. Furthermore, we incorporate outlier detection to filter irrelevant\nquestions, allowing for the refinement of test sets. Experimental evidence from\ntwo distinct use cases demonstrates that our framework effectively quantifies\ntest coverage, identifies specific content areas with inadequate\nrepresentation, and provides concrete recommendations for generating new,\nhigh-value test questions. This work provides RAG developers with essential\ntools to build more robust test suites, thereby improving system reliability\nand extending to applications such as identifying misaligned documents.", "AI": {"tldr": "The paper introduces a methodology for quantifying the semantic coverage of test questions in RAG systems, addressing gaps in current evaluation practices.", "motivation": "To address the deficiency in systematic methods to ensure the test sets adequately cover the knowledge base in RAG systems.", "method": "It applies vector embeddings and clustering algorithms to embed documents and test questions into a unified vector space, enabling calculations of coverage metrics and refining test sets through outlier detection.", "result": "Experimental results confirm the framework can quantify test coverage, identify content gaps, and suggest new test questions to enhance evaluation.", "conclusion": "The study provides tools for RAG developers to improve test suites, enhancing reliability and quality of the systems."}}
{"id": "2510.00125", "pdf": "https://arxiv.org/pdf/2510.00125", "abs": "https://arxiv.org/abs/2510.00125", "authors": ["Hong kyu Lee", "Ruixuan Liu", "Li Xiong"], "title": "Direct Token Optimization: A Self-contained Approach to Large Language Model Unlearning", "categories": ["cs.CL", "cs.AI", "cs.CR"], "comment": null, "summary": "Machine unlearning is an emerging technique that removes the influence of a\nsubset of training data (forget set) from a model without full retraining, with\napplications including privacy protection, content moderation, and model\ncorrection. The key challenge lies in ensuring that the model completely\nforgets the knowledge of the forget set without compromising its overall\nutility. Existing unlearning methods for large language models (LLMs) often\nutilize auxiliary language models, retain datasets, or even commercial AI\nservices for effective unlearning and maintaining the model utility. However,\ndependence on these external resources is often impractical and could\npotentially introduce additional privacy risks. In this work, we propose direct\ntoken optimization (DTO), a novel self-contained unlearning approach for LLMs\nthat directly optimizes the token level objectives and eliminates the need for\nexternal resources. Given a sequence to unlearn, we identify two categories of\ntokens: target tokens, which capture critical knowledge for unlearning, and the\nremaining non-target tokens, which are crucial for maintaining the model\nutility. The former are used to optimize the unlearning objective, while the\nlatter serve to preserve the model's performance. The experimental results show\nthat the proposed DTO achieves up to 16.8$\\times$ improvement in forget quality\non several benchmark datasets than the latest baselines while maintaining a\ncomparable level of model utility.", "AI": {"tldr": "The paper introduces Direct Token Optimization (DTO), a self-contained method for machine unlearning in large language models, eliminating the need for external resources while maintaining model utility.", "motivation": "To develop a method that ensures a model forgets specific training data (ensuring privacy and other concerns) while maintaining overall utility, without relying on external resources that may introduce additional privacy risks.", "method": "Direct Token Optimization (DTO) identifies two categories of tokens when unlearning a sequence: target tokens, which optimize the unlearning objective, and non-target tokens, which help preserve model performance.", "result": "DTO achieves up to 16.8\u00d7 better forget quality compared to existing baselines, while maintaining a similar level of model utility, as evidenced by testing on multiple benchmark datasets.", "conclusion": "DTO provides an effective, privacy-preserving, and resource-independent solution for unlearning in large language models, outperforming existing methods while maintaining model utility."}}
{"id": "2510.00010", "pdf": "https://arxiv.org/pdf/2510.00010", "abs": "https://arxiv.org/abs/2510.00010", "authors": ["Vladimir A. Lazovsky", "Sergey V. Stasenko", "Victor B. Kazantsev"], "title": "Computational Advances in Taste Perception: From Ion Channels to Neural Coding", "categories": ["q-bio.NC"], "comment": null, "summary": "Recent advances in computational neuroscience demand models that balance\nbiophysical realism with scalability. We present a hybrid neuron model\ncombining the biophysical fidelity of Hodgkin-Huxley (HH) dynamics for taste\nreceptor cells with the computational efficiency of Izhikevich spiking neurons\nfor large-network simulations. Our framework incorporates biomorphic taste cell\nmodels, featuring modality-specific receptor dynamics (T1R/T2R, ENaC, PKD) and\nGoldman-Hodgkin-Katz (GHK)-driven ion currents to accurately simulate gustatory\ntransduction. Synaptic interactions are modeled via glutamate release kinetics\nwith alpha-function profiles, AMPA receptor trafficking regulated by\nphosphorylation, and spike-timing-dependent plasticity (STDP) to enforce\ntemporal coding. At the network level, we optimize multiscale learning,\nleveraging both temporal spike synchrony (van Rossum metrics) and combinatorial\npopulation coding (rank-order patterns). This approach bridges single-cell\nbiophysics with ensemble-level computation, enabling efficient simulation of\ngustatory pathways while retaining biological fidelity.", "AI": {"tldr": "This paper introduces a hybrid computational model combining Hodgkin-Huxley biophysical dynamics with Izhikevich spiking neurons for scalable and biologically faithful gustatory system simulations.", "motivation": "The goal is to create models that maintain biophysical realism while handling the scalability demands of simulating large neural networks.", "method": "The model integrates Hodgkin-Huxley dynamics for taste receptor cells with Izhikevich spiking neurons, incorporating biomorphic taste cell receptor dynamics, Goldman-Hodgkin-Katz ion currents, glutamate-driven synaptic interactions, and mechanisms like STDP. Network-level optimization focuses on temporal spike synchrony and combinatorial population coding.", "result": "The framework bridges detailed single-cell biophysics with ensemble-level neural computation, allowing for efficient and biologically accurate simulations of taste pathways.", "conclusion": "The study successfully combines biophysical accuracy with computational efficiency, providing a scalable and biologically realistic platform for simulating gustatory systems."}}
{"id": "2510.00541", "pdf": "https://arxiv.org/pdf/2510.00541", "abs": "https://arxiv.org/abs/2510.00541", "authors": ["Ali M. Baydoun", "Ahmed S. Zekri"], "title": "Towards Efficient VM Placement: A Two-Stage ACO-PSO Approach for Green Cloud Infrastructure", "categories": ["cs.DC", "68M20 (Performance evaluation, queueing, and scheduling), 90C59\n  (Approximation methods and heuristics)"], "comment": "20 pages, 7 figures. Published in International Journal of Computer\n  Networks & Communications (IJCNC), Vol. 17, No. 5, 2025", "summary": "Datacenters consume a growing share of energy, prompting the need for\nsustainable resource management. This paper presents a Hybrid ACO-PSO (HAPSO)\nalgorithm for energy-aware virtual machine (VM) placement and migration in\ngreen cloud datacenters. In the first stage, Ant Colony Optimization (ACO)\nperforms energy-efficient initial placement across physical hosts, ensuring\nglobal feasibility. In the second stage, a discrete Particle Swarm Optimization\n(PSO) refines allocations by migrating VMs from overloaded or underutilized\nhosts. HAPSO introduces several innovations: sequential hybridization of\nmetaheuristics, system-informed particle initialization using ACO output,\nheuristic-guided discretization for constraint handling, and a multi-objective\nfitness function that minimizes active servers and resource wastage.\nImplemented in CloudSimPlus, extensive simulations demonstrate that HAPSO\nconsistently outperforms classical heuristics (BFD, FFD), Unified Ant Colony\nSystem (UACS), and ACO-only. Notably, HAPSO achieves up to 25% lower energy\nconsumption and 18% fewer SLA violations compared to UACS at large-scale\nworkloads, while sustaining stable cost and carbon emissions. These results\nhighlight the effectiveness of two-stage bio-inspired hybridization in\naddressing the dynamic and multi-objective nature of cloud resource management.", "AI": {"tldr": "The paper proposes a Hybrid ACO-PSO (HAPSO) algorithm for energy-aware VM placement and migration in cloud datacenters, achieving significant energy savings and reduced SLA violations.", "motivation": "Rising energy consumption in datacenters necessitates sustainable and efficient resource management strategies.", "method": "The paper introduces a two-stage hybrid algorithm: (1) Ant Colony Optimization (ACO) for initial VM placement, and (2) Particle Swarm Optimization (PSO) for refining allocations and managing migrations. Innovations include sequential hybridization, system-informed initialization, heuristic-guided constraint handling, and a multi-objective fitness function.", "result": "Simulations in CloudSimPlus show HAPSO reduces energy consumption by up to 25% and SLA violations by 18% compared to existing algorithms for large-scale workloads.", "conclusion": "HAPSO effectively addresses the dynamic and multi-objective challenges of cloud resource management and demonstrates the potential of bio-inspired hybrid methods in optimizing energy-aware datacenter operations."}}
{"id": "2510.00219", "pdf": "https://arxiv.org/pdf/2510.00219", "abs": "https://arxiv.org/abs/2510.00219", "authors": ["Houjun Liu", "Shikhar Murty", "Christopher D. Manning", "R\u00f3bert Csord\u00e1s"], "title": "Thoughtbubbles: an Unsupervised Method for Parallel Thinking in Latent Space", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.NE"], "comment": "10 pages, 6 figures", "summary": "Current approaches for scaling inference-time compute in transformers rely on\ntraining them to emit explicit chain-of-thought tokens before producing an\nanswer. While these methods are powerful, they are limited because they cannot\nbe applied during pretraining and are limited to only serially-generated,\nnatural-language verbalization to scale inference-time compute. In this work,\nwe propose Thoughtbubbles, a transformer variant that natively performs\nparallel adaptive computation in latent space by learning to fork or delete\nresidual streams. Thus, tokens that require a large amount of computation can\nform a \"bubble\" of cloned residuals in the middle of the network for additional\nthinking. Crucially, this behavior is learned during pretraining with only\nlanguage modeling loss. Thoughtbubbles outperforms both standard decoder LMs as\nwell as non-adaptive parallel computation approaches on OpenWebText and peS2o\nperplexity and in zero-shot evaluations such as HellaSwag and LAMBADA after\npretraining across 150M to 772M parameter scales. The implicit nature of our\nmethod enables adaptive computation to be learned starting at pretraining time,\npaving the way to unify train and test-time behavior for reasoning models.", "AI": {"tldr": "This paper introduces Thoughtbubbles, a transformer model that performs parallel adaptive computation during pretraining, outperforming standard LMs and non-adaptive methods.", "motivation": "Current inference approaches in transformers rely on explicit chain-of-thought tokens and are limited to serial natural-language verbalization.", "method": "Thoughtbubbles employs latent space computation by learning to fork or delete residual streams during pretraining using only language modeling loss.", "result": "Thoughtbubbles achieved superior performance on OpenWebText and peS2o perplexity as well as zero-shot evaluations like HellaSwag and LAMBADA across different parameter scales.", "conclusion": "The study demonstrates that adaptive computation can be learned during pretraining, unifying train and test-time behavior in reasoning models."}}
{"id": "2510.00003", "pdf": "https://arxiv.org/pdf/2510.00003", "abs": "https://arxiv.org/abs/2510.00003", "authors": ["Malte Hansen", "Jens Bamberg", "Noe Baumann", "Wilhelm Hasselbring"], "title": "Semantic Zoom and Mini-Maps for Software Cities", "categories": ["cs.SE"], "comment": "Copyright 2025 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "summary": "Software visualization tools can facilitate program comprehension by\nproviding visual metaphors, or abstractions that reduce the amount of textual\ndata that needs to be processed mentally. One way they do this is by enabling\ndevelopers to build an internal representation of the visualized software and\nits architecture. However, as the amount of displayed data in the visualization\nincreases, the visualization itself can become more difficult to comprehend.\nThe ability to display small and large amounts of data in visualizations is\ncalled visual scalability.\n  In this paper, we present two approaches to address the challenge of visual\nscalability in 3D software cities. First, we present an approach to semantic\nzoom, in which the graphical representation of the software landscape changes\nbased on the virtual camera's distance from visual objects. Second, we augment\nthe visualization with a miniature two-dimensional top-view projection called\nmini-map. We demonstrate our approach using an open-source implementation in\nour software visualization tool ExplorViz. ExplorViz is web-based and uses the\n3D city metaphor, focusing on live trace visualization.\n  We evaluated our approaches in two separate user studies. The results\nindicate that semantic zoom and the mini-map are both useful additions. User\nfeedback indicates that semantic zoom and mini-maps are especially useful for\nlarge software landscapes and collaborative software exploration. The studies\nindicate a good usability of our implemented approaches. However, some\nshortcomings in our implementations have also been discovered, to be addressed\nin future work.\n  Video URL: https://youtu.be/LYtUeWvizjU", "AI": {"tldr": "This paper addresses visual scalability in 3D software visualization.", "motivation": "To improve comprehension of large, complex software visualizations by tackling visual scalability challenges.", "method": "Introduced semantic zooming and a 2D mini-map augmentation and showcased in ExplorViz.", "result": "User studies show both techniques enhance usability, particularly for large systems; some limitations identified.", "conclusion": "The approaches improve large-scale software exploration; future work will address identified limitations."}}
{"id": "2510.00182", "pdf": "https://arxiv.org/pdf/2510.00182", "abs": "https://arxiv.org/abs/2510.00182", "authors": ["Jorge Mendez-Mendez"], "title": "A Systematic Study of Large Language Models for Task and Motion Planning With PDDLStream", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Using large language models (LLMs) to solve complex robotics problems\nrequires understanding their planning capabilities. Yet while we know that LLMs\ncan plan on some problems, the extent to which these planning capabilities\ncover the space of robotics tasks is unclear. One promising direction is to\nintegrate the semantic knowledge of LLMs with the formal reasoning of task and\nmotion planning (TAMP). However, the myriad of choices for how to integrate\nLLMs within TAMP complicates the design of such systems. We develop 16\nalgorithms that use Gemini 2.5 Flash to substitute key TAMP components. Our\nzero-shot experiments across 4,950 problems and three domains reveal that the\nGemini-based planners exhibit lower success rates and higher planning times\nthan their engineered counterparts. We show that providing geometric details\nincreases the number of task-planning errors compared to pure PDDL\ndescriptions, and that (faster) non-reasoning LLM variants outperform (slower)\nreasoning variants in most cases, since the TAMP system can direct the LLM to\ncorrect its mistakes.", "AI": {"tldr": "The paper examines the integration of large language models (LLMs) like Gemini 2.5 Flash into task and motion planning (TAMP) systems for robotics and discovers that LLM-based planners have limitations compared to engineered counterparts.", "motivation": "Investigate LLM planning capabilities in robotics and understand how semantic knowledge can be combined with formal reasoning to enhance robotic systems.", "method": "Developed 16 algorithms using Gemini 2.5 Flash and conducted zero-shot experiments across 4,950 robotics problems in three domains, analyzing their success rates and planning times.", "result": "LLM-based planners demonstrated lower success rates and higher planning times compared to engineered solutions. Geometric details introduced greater errors, and non-reasoning variants generally performed better than reasoning ones.", "conclusion": "Combining LLMs with TAMP systems is feasible but currently underperforms compared to traditional methods. Strategic LLM direction within TAMP systems can help mitigate errors."}}
{"id": "2510.00076", "pdf": "https://arxiv.org/pdf/2510.00076", "abs": "https://arxiv.org/abs/2510.00076", "authors": ["Xin Lyu"], "title": "Private Learning of Littlestone Classes, Revisited", "categories": ["stat.ML", "cs.CR", "cs.DS", "cs.LG"], "comment": "Comments welcome", "summary": "We consider online and PAC learning of Littlestone classes subject to the\nconstraint of approximate differential privacy. Our main result is a private\nlearner to online-learn a Littlestone class with a mistake bound of\n$\\tilde{O}(d^{9.5}\\cdot \\log(T))$ in the realizable case, where $d$ denotes the\nLittlestone dimension and $T$ the time horizon. This is a doubly-exponential\nimprovement over the state-of-the-art [GL'21] and comes polynomially close to\nthe lower bound for this task.\n  The advancement is made possible by a couple of ingredients. The first is a\nclean and refined interpretation of the ``irreducibility'' technique from the\nstate-of-the-art private PAC-learner for Littlestone classes [GGKM'21]. Our new\nperspective also allows us to improve the PAC-learner of [GGKM'21] and give a\nsample complexity upper bound of $\\widetilde{O}(\\frac{d^5\n\\log(1/\\delta\\beta)}{\\varepsilon \\alpha})$ where $\\alpha$ and $\\beta$ denote\nthe accuracy and confidence of the PAC learner, respectively. This improves\nover [GGKM'21] by factors of $\\frac{d}{\\alpha}$ and attains an optimal\ndependence on $\\alpha$.\n  Our algorithm uses a private sparse selection algorithm to \\emph{sample} from\na pool of strongly input-dependent candidates. However, unlike most previous\nuses of sparse selection algorithms, where one only cares about the utility of\noutput, our algorithm requires understanding and manipulating the actual\ndistribution from which an output is drawn. In the proof, we use a sparse\nversion of the Exponential Mechanism from [GKM'21] which behaves nicely under\nour framework and is amenable to a very easy utility proof.", "AI": {"tldr": "The paper significantly improves private online learning and PAC learning for Littlestone classes, achieving better mistake and sample complexity bounds.", "motivation": "The goal is to improve the state-of-the-art in private online and PAC learning of Littlestone classes, subject to differential privacy constraints.", "method": "The method combines refined techniques such as irreducibility interpretation and private sparse selection, alongside tools like a sparse version of the Exponential Mechanism.", "result": "Improved mistake bounds in online learning ($\\tilde{O}(d^{9.5}\\cdot \\log(T))$) and sample complexity in PAC learning ($\\widetilde{O}(\\frac{d^5 \\log(1/\\delta\\beta)}{\\varepsilon \\alpha})$), surpassing previous works.", "conclusion": "The advancements bring the results closer to theoretical lower bounds and demonstrate a practical refinement of privacy-preserving machine learning techniques."}}
{"id": "2510.00034", "pdf": "https://arxiv.org/pdf/2510.00034", "abs": "https://arxiv.org/abs/2510.00034", "authors": ["Zhengyi Ho", "Siyuan Liang", "Dacheng Tao"], "title": "Review of Hallucination Understanding in Large Language and Vision Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The widespread adoption of large language and vision models in real-world\napplications has made urgent the need to address hallucinations -- instances\nwhere models produce incorrect or nonsensical outputs. These errors can\npropagate misinformation during deployment, leading to both financial and\noperational harm. Although much research has been devoted to mitigating\nhallucinations, our understanding of it is still incomplete and fragmented.\nWithout a coherent understanding of hallucinations, proposed solutions risk\nmitigating surface symptoms rather than underlying causes, limiting their\neffectiveness and generalizability in deployment. To tackle this gap, we first\npresent a unified, multi-level framework for characterizing both image and text\nhallucinations across diverse applications, aiming to reduce conceptual\nfragmentation. We then link these hallucinations to specific mechanisms within\na model's lifecycle, using a task-modality interleaved approach to promote a\nmore integrated understanding. Our investigations reveal that hallucinations\noften stem from predictable patterns in data distributions and inherited\nbiases. By deepening our understanding, this survey provides a foundation for\ndeveloping more robust and effective solutions to hallucinations in real-world\ngenerative AI systems.", "AI": {"tldr": "This paper addresses hallucinations in large language and vision models by presenting a unified framework for understanding and mitigating them.", "motivation": "Hallucinations in generative AI systems can cause misinformation, financial, and operational risks, yet current understanding and solutions remain fragmented.", "method": "A unified framework characterizes image and text hallucinations, links them to model lifecycle mechanisms using a task-modality interleaved approach.", "result": "Hallucinations are found to stem from predictable data distribution patterns and inherited biases.", "conclusion": "The study lays a foundation for creating robust solutions and enhancing reliability in generative AI systems."}}
{"id": "2510.00023", "pdf": "https://arxiv.org/pdf/2510.00023", "abs": "https://arxiv.org/abs/2510.00023", "authors": ["Quy Minh Le", "Minh Sao Khue Luu", "Khanh-Tung Tran", "Duc-Hai Nguyen", "Hoang-Quoc-Viet Pham", "Quan Le", "Hoang Thanh Lam", "Hoang D. Nguyen"], "title": "ToolBrain: A Flexible Reinforcement Learning Framework for Agentic Tools", "categories": ["cs.AI"], "comment": null, "summary": "Effective tool use is essential for agentic AI, yet training agents to\nutilize tools remains challenging due to manually designed rewards, limited\ntraining data, and poor multi-tool selection, resulting in slow adaptation,\nwasted computational resources, and suboptimal performance. We introduce\nToolBrain, a lightweight and user-friendly framework for coaching tool use in\nagentic models with flexible reinforcement learning (RL), easing the barriers\nfor researchers and practitioners to adapt LLM-based agents to specific\ndomains. It supports a wide range of training strategies, including RL\nalgorithms such as GRPO and DPO, as well as supervised learning. ToolBrain\nenables custom reward callables directly on an agent's execution traces or\nsimply utilizes an automated LLM-as-a-judge system for reward generation. It is\npacked with useful capabilities, including knowledge distillation from large to\nsmall models for efficient development, automatic task generation from tool\ndescriptions, seamless tool retrieval, efficient fine-tuning pipelines with\nQLoRA through Unsloth, and quantized inference via bitsandbytes. We demonstrate\nToolBrain through diverse use cases, such as training a CodeAct agent to\nautonomously execute email search tasks, showing fast, targeted improvements\n(up to 30.0%) in tool-use skills while keeping the codebase simple and\nextensible in Agentic AI. Our framework is publicly available at\nhttps://toolbrain.org.", "AI": {"tldr": "ToolBrain is a lightweight RL-based framework designed to train AI agents for effective tool use without manual rewards or excessive resources. It supports flexible methods and showcases fast improvements in tailored tool-use tasks.", "motivation": "The paper addresses challenges in training AI agents for versatile tool use, focusing on inefficiencies caused by manual reward design, scarce data, and poor multi-tool selection.", "method": "The framework leverages reinforcement learning (GRPO/DPO) and supervised learning to coach agents. It offers knowledge distillation, automated task generation, fine-tuning pipelines, and quantized inference features.", "result": "Using ToolBrain, agents demonstrated up to a 30% improvement in tool-use performance, validated through tasks like CodeAct email search.", "conclusion": "ToolBrain simplifies training AI tool use across domains, making the process more efficient and accessible. The framework and codebase are open for public use."}}
{"id": "2510.00027", "pdf": "https://arxiv.org/pdf/2510.00027", "abs": "https://arxiv.org/abs/2510.00027", "authors": ["Ahmed A. Elhag", "Arun Raja", "Alex Morehead", "Samuel M. Blau", "Garrett M. Morris", "Michael M. Bronstein"], "title": "Learning Inter-Atomic Potentials without Explicit Equivariance", "categories": ["cs.LG", "cs.AI", "q-bio.BM", "q-bio.QM", "I.2.1; J.3"], "comment": "19 pages, 3 tables, 10 figures. Under review", "summary": "Accurate and scalable machine-learned inter-atomic potentials (MLIPs) are\nessential for molecular simulations ranging from drug discovery to new material\ndesign. Current state-of-the-art models enforce roto-translational symmetries\nthrough equivariant neural network architectures, a hard-wired inductive bias\nthat can often lead to reduced flexibility, computational efficiency, and\nscalability. In this work, we introduce TransIP: Transformer-based Inter-Atomic\nPotentials, a novel training paradigm for interatomic potentials achieving\nsymmetry compliance without explicit architectural constraints. Our approach\nguides a generic non-equivariant Transformer-based model to learn\nSO(3)-equivariance by optimizing its representations in the embedding space.\nTrained on the recent Open Molecules (OMol25) collection, a large and diverse\nmolecular dataset built specifically for MLIPs and covering different types of\nmolecules (including small organics, biomolecular fragments, and\nelectrolyte-like species), TransIP attains comparable performance in\nmachine-learning force fields versus state-of-the-art equivariant baselines.\nFurther, compared to a data augmentation baseline, TransIP achieves 40% to 60%\nimprovement in performance across varying OMol25 dataset sizes. More broadly,\nour work shows that learned equivariance can be a powerful and efficient\nalternative to equivariant or augmentation-based MLIP models.", "AI": {"tldr": "The paper presents TransIP, a Transformer-based model for interatomic potentials. It achieves symmetry compliance without using hard-wired neural architecture constraints, offering improved efficiency and scalability.", "motivation": "To overcome limitations of state-of-the-art equivariant neural network models for MLIPs, such as reduced flexibility, computational efficiency, and scalability when enforcing symmetries.", "method": "A generic non-equivariant Transformer model is guided to learn SO(3)-equivariance by optimizing embedding space representations, trained on the diverse OMol25 dataset.", "result": "TransIP matches the performance of traditional equivariant baselines and outperforms data augmentation approaches by 40\u201360% on OMol25, across varying dataset sizes.", "conclusion": "Learned equivariance is a promising alternative to traditional equivariant architectures or augmentation-based MLIP models, benefiting molecular simulations."}}
{"id": "2510.00161", "pdf": "https://arxiv.org/pdf/2510.00161", "abs": "https://arxiv.org/abs/2510.00161", "authors": ["Kimihiro Hasegawa", "Wiradee Imrattanatrai", "Masaki Asada", "Ken Fukuda", "Teruko Mitamura"], "title": "TAMA: Tool-Augmented Multimodal Agent for Procedural Activity Understanding", "categories": ["cs.CL"], "comment": "21 pages. Code: https://github.com/kimihiroh/tama", "summary": "Procedural activity assistants potentially support humans in a variety of\nsettings, from our daily lives, e.g., cooking or assembling flat-pack\nfurniture, to professional situations, e.g., manufacturing or biological\nexperiments. Despite its potential use cases, the system development tailored\nfor such an assistant is still underexplored. In this paper, we propose a novel\nframework, called TAMA, a Tool-Augmented Multimodal Agent, for procedural\nactivity understanding. TAMA enables interleaved multimodal reasoning by making\nuse of multimedia-returning tools in a training-free setting. Our experimental\nresult on the multimodal procedural QA dataset, ProMQA-Assembly, shows that our\napproach can improve the performance of vision-language models, especially\nGPT-5 and MiMo-VL. Furthermore, our ablation studies provide empirical support\nfor the effectiveness of two features that characterize our framework,\nmultimedia-returning tools and agentic flexible tool selection. We believe our\nproposed framework and experimental results facilitate the thinking with images\nparadigm for video and multimodal tasks, let alone the development of\nprocedural activity assistants.", "AI": {"tldr": "The paper introduces TAMA, a framework for better understanding procedural activities using a tool-augmented multimodal approach without requiring additional training.", "motivation": "Procedural activity assistants could aid humans in various environments, yet their development remains underexplored, motivating the design of a new framework.", "method": "The proposed framework, TAMA, leverages multimedia tools to allow interleaved multimodal reasoning in a training-free manner and evaluates its efficacy using the ProMQA-Assembly dataset.", "result": "TAMA enhances the performance of vision-language models like GPT-5 and MiMo-VL on procedural QA tasks. Ablation studies validate its multimedia tools and flexible tool selection features.", "conclusion": "The framework supports thinking with images for multimodal tasks and advances procedural activity assistants, offering a foundation for future developments in this area."}}
{"id": "2510.00011", "pdf": "https://arxiv.org/pdf/2510.00011", "abs": "https://arxiv.org/abs/2510.00011", "authors": ["Sir-Lord Wiafe", "Carter Hinsley", "Vince D. Calhoun"], "title": "Robust State-space Reconstruction of Brain Dynamics via Bootstrap Monte Carlo SSA", "categories": ["q-bio.NC"], "comment": "5 pages, 2 figures, conference", "summary": "Reconstructing latent state-space geometry from time series provides a\npowerful route to studying nonlinear dynamics across complex systems.\nDelay-coordinate embedding provides the theoretical basis but assumes long,\nnoise-free recordings, which many domains violate. In neuroimaging, for\nexample, fMRI is short and noisy; low sampling and strong red noise obscure\noscillations and destabilize embeddings. We propose bootstrap Monte Carlo SSA\nwith a red-noise null and bootstrap stability to retain only oscillatory modes\nthat reproducibly exceed noise. This produces reconstructions that are\nred-noise-robust and mode-robust, enhancing determinism and stabilizing\nsubsequent embeddings. Our results show that BMC-SSA improves the reliability\nof functional measures and uncovers differences in state-space dynamics in\nfMRI, offering a general framework for robust embeddings of noisy, finite\nsignals.", "AI": {"tldr": "The abstract focuses on improving the reconstruction of latent state-space geometry from noisy and short time series, particularly in neuroimaging, using a novel bootstrap Monte Carlo SSA approach.", "motivation": "Understanding nonlinear dynamics in complex systems often requires reconstructing latent state-space geometry from time series data. However, certain conditions like short, noisy recordings pose a challenge for traditional methods, especially in applications like neuroimaging.", "method": "The authors propose a bootstrap Monte Carlo Singular Spectrum Analysis (BMC-SSA) approach, accompanied by a red-noise null and bootstrap stability mechanisms. This method filters out noise while retaining oscillatory modes that consistently exceed noise levels, thereby stabilizing the embeddings.", "result": "The proposed BMC-SSA approach enhances determinism and stabilizes embeddings in noisy and finite signals, particularly in fMRI data, making functional measures more reliable and uncovering differences in state-space dynamics.", "conclusion": "BMC-SSA provides a robust framework for embedding noisy, finite signals, improving the reliability of functional measures and enabling the study of state-space dynamics in neuroimaging and other applications."}}
{"id": "2510.00606", "pdf": "https://arxiv.org/pdf/2510.00606", "abs": "https://arxiv.org/abs/2510.00606", "authors": ["Xueze Kang", "Guangyu Xiang", "Yuxin Wang", "Hao Zhang", "Yuchu Fang", "Yuhang Zhou", "Zhenheng Tang", "Youhui Lv", "Eliran Maman", "Mark Wasserman", "Alon Zameret", "Zhipeng Bian", "Shushu Chen", "Zhiyou Yu", "Jin Wang", "Xiaoyu Wu", "Yang Zheng", "Chen Tian", "Xiaowen Chu"], "title": "ElasWave: An Elastic-Native System for Scalable Hybrid-Parallel Training", "categories": ["cs.DC"], "comment": null, "summary": "Large-scale LLM pretraining today spans $10^{5}$--$10^{6}$ accelerators,\nmaking failures commonplace and elasticity no longer optional. We posit that an\nelastic-native training system must simultaneously ensure (i) Parameter\nConsistency, (ii) low Mean Time to Recovery (MTTR), (iii) high post-change\nThroughput, and (iv) Computation Consistency. This objective set not has never\nbeen jointly attained by prior work. To achieve these goals, we present\nElasWave, which provides per-step fault tolerance via multi-dimensional\nscheduling across Graph, Dataflow, Frequency, and Random Number Generation.\nElasWave resizes and reshards micro-batch workloads while preserving the global\nbatch size and gradient scale; it performs online pipeline resharding with\nasynchronous parameter migration, interleaving ZeRO partitions so recovery\nreduces to disjoint rank-to-rank transfers. It further uses DVFS to absorb\npipeline bubbles and reshards RNG to keep consistent computations. A dynamic\ncommunicator enables in-place communication group edits, while per-step\nin-memory snapshots support online verification and redistribution. We\nevaluated ElasWave on 96 NPUs and benchmarked against state-of-the-art\nbaselines: throughput improves by $1.35\\times$ over ReCycle and $1.60\\times$\nover TorchFT; communicator recovery completes within one second (up to\n$82\\times/3.6\\times$ faster than full/partial rebuilds); migration MTTR drops\nby as much as $51\\%$; and convergence deviation is reduced by approximately\n$78\\%$.", "AI": {"tldr": "ElasWave introduces a fault-tolerant system for large-scale LLM training, improving throughput, recovery speed, and computational consistency compared to previous methods.", "motivation": "Large-scale LLM pretraining involves huge computational resources and frequent failures, making elasticity and fault tolerance integral.", "method": "ElasWave employs multi-dimensional scheduling, asynchronous parameter migration, dynamic communicator modification, and in-memory snapshots to ensure efficiency and fault tolerance.", "result": "Throughput improved significantly compared to baselines, communicator recovery achieved rapid completion, MTTR decreased, and convergence deviation was reduced.", "conclusion": "ElasWave successfully achieves fault tolerance and computational efficiency, addressing prior limitations in large-scale LLM pretraining frameworks."}}
{"id": "2510.00373", "pdf": "https://arxiv.org/pdf/2510.00373", "abs": "https://arxiv.org/abs/2510.00373", "authors": ["Carlo Bosio", "Matteo Guarrera", "Alberto Sangiovanni-Vincentelli", "Mark W. Mueller"], "title": "Combining Large Language Models and Gradient-Free Optimization for Automatic Control Policy Synthesis", "categories": ["cs.LG", "cs.AI", "cs.NE", "cs.SY", "eess.SY"], "comment": "8 pages, 7 figures", "summary": "Large Language models (LLMs) have shown promise as generators of symbolic\ncontrol policies, producing interpretable program-like representations through\niterative search. However, these models are not capable of separating the\nfunctional structure of a policy from the numerical values it is parametrized\nby, thus making the search process slow and inefficient. We propose a hybrid\napproach that decouples structural synthesis from parameter optimization by\nintroducing an additional optimization layer for local parameter search. In our\nmethod, the numerical parameters of LLM-generated programs are extracted and\noptimized numerically to maximize task performance. With this integration, an\nLLM iterates over the functional structure of programs, while a separate\noptimization loop is used to find a locally optimal set of parameters\naccompanying candidate programs. We evaluate our method on a set of control\ntasks, showing that it achieves higher returns and improved sample efficiency\ncompared to purely LLM-guided search. We show that combining symbolic program\nsynthesis with numerical optimization yields interpretable yet high-performing\npolicies, bridging the gap between language-model-guided design and classical\ncontrol tuning. Our code is available at\nhttps://sites.google.com/berkeley.edu/colmo.", "AI": {"tldr": "This paper introduces a hybrid approach combining symbolic program synthesis with numerical optimization to improve the efficiency and performance of control policy generation by large language models (LLMs).", "motivation": "LLMs show promise in generating symbolic control policies but struggle with the inefficiency of separating structural synthesis and parameter optimization, necessitating a better approach.", "method": "The method decouples structural synthesis from parameter optimization by introducing a local optimization layer. The LLM iteratively refines program structure while a separate optimization loop adjusts numerical parameters to improve task performance.", "result": "The proposed method demonstrates higher returns and better sample efficiency on control tasks than purely LLM-guided searches, validating its effectiveness.", "conclusion": "Integrating symbolic synthesis with numerical optimization leads to interpretable and high-performing control policies, effectively combining the strengths of LLM-guided design and classical control tuning."}}
{"id": "2510.00004", "pdf": "https://arxiv.org/pdf/2510.00004", "abs": "https://arxiv.org/abs/2510.00004", "authors": ["Malte Hansen", "David Moreno-Lumbreras", "Wilhelm Hasselbring"], "title": "HTML Structure Exploration in 3D Software Cities", "categories": ["cs.SE"], "comment": "Copyright 2025 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "summary": "Software visualization, which uses data from dynamic program analysis, can\nhelp to explore and understand the behavior of software systems. It is common\nthat large software systems offer a web interface for user interaction.\nUsually, available web interfaces are not regarded in software visualization\ntools. This paper introduces additions to the web-based live tracing software\nvisualization tool ExplorViz: We add an embedded web view for instrumented\napplications in the 3D visualization to ease interaction with the given\napplications and enable the exploration of the thereby displayed HTML content.\nNamely, the Document Object Model (DOM) is visualized via a three-dimensional\nrepresentation of the HTML structure in same-origin contexts.\n  Our visualization approach is evaluated in a preliminary user study. The\nstudy results give insights into the potential use cases, benefits, and\nshortcomings of our implemented approach. Based on our study results, we\npropose directions for further research to support the visual exploration of\nweb interfaces and explore use cases for the combined visualization of software\ncities and HTML structure.\n  Video URL: https://youtu.be/wBWKlbvzOOE", "AI": {"tldr": "This paper enhances the ExplorViz software visualization tool by integrating a 3D visualization of the HTML DOM for web-based applications, evaluated through a preliminary user study.", "motivation": "Exploring and understanding large software systems often necessitates dynamic software visualization tools, especially for web interfaces which are typically overlooked.", "method": "The researchers integrated an embedded web view and 3D representation of the HTML DOM for web interfaces into ExplorViz.", "result": "A preliminary user study provided insights into the usability, advantages, and drawbacks of the proposed visualization approach.", "conclusion": "The study suggests future research directions for improving web interface visualization and combining software cities with HTML structure for enhanced exploration."}}
{"id": "2510.00188", "pdf": "https://arxiv.org/pdf/2510.00188", "abs": "https://arxiv.org/abs/2510.00188", "authors": ["Alireza Aliyari", "Gholamreza Vossoughi"], "title": "A Novel Robust Control Method Combining DNN-Based NMPC Approximation and PI Control: Application to Exoskeleton Squat Movements", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "Nonlinear Model Predictive Control (NMPC) is a precise controller, but its\nheavy computational load often prevents application in robotic systems. Some\nstudies have attempted to approximate NMPC using deep neural networks\n(NMPC-DNN). However, in the presence of unexpected disturbances or when\noperating conditions differ from training data, this approach lacks robustness,\nleading to large tracking errors. To address this issue, for the first time,\nthe NMPC-DNN output is combined with a PI controller (Hybrid NMPC-DNN-PI). The\nproposed controller is validated by applying it to an exoskeleton robot during\nsquat movement, which has a complex dynamic model and has received limited\nattention regarding robust nonlinear control design. A human-robot dynamic\nmodel with three active joints (ankle, knee, hip) is developed, and more than\n5.3 million training samples are used to train the DNN. The results show that,\nunder unseen conditions for the DNN, the tracking error in Hybrid NMPC-DNN-PI\nis significantly lower compared to NMPC-DNN. Moreover, human joint torques are\ngreatly reduced with the use of the exoskeleton, with RMS values for the\nstudied case reduced by 30.9%, 41.8%, and 29.7% at the ankle, knee, and hip,\nrespectively. In addition, the computational cost of Hybrid NMPC-DNN-PI is\n99.93% lower than that of NMPC.", "AI": {"tldr": "This paper presents a hybrid control method combining NMPC-DNN and a PI controller, applied to an exoskeleton robot for squat movements, achieving improved tracking and reduced computational cost.", "motivation": "Traditional NMPC faces computational challenges, and existing NMPC-DNN approaches lack robustness under unexpected conditions.", "method": "A hybrid control system (Hybrid NMPC-DNN-PI) that merges NMPC-DNN outputs with a PI controller was developed and validated on an exoskeleton robot with a human-robot dynamic model.", "result": "The Hybrid NMPC-DNN-PI achieved significantly reduced tracking errors under unseen conditions, reduced human joint torques by over 29%, and decreased computational costs by 99.93% compared to NMPC.", "conclusion": "The Hybrid NMPC-DNN-PI is a robust and efficient control solution for nonlinear robotic systems, particularly under varying and unforeseen conditions."}}
{"id": "2510.00367", "pdf": "https://arxiv.org/pdf/2510.00367", "abs": "https://arxiv.org/abs/2510.00367", "authors": ["Dehao Dai", "Jianqing Fan", "Yihong Gu", "Debarghya Mukherjee"], "title": "CINDES: Classification induced neural density estimator and simulator", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.ME", "stat.TH", "62G08"], "comment": "50 pages, 1 figure", "summary": "Neural network-based methods for (un)conditional density estimation have\nrecently gained substantial attention, as various neural density estimators\nhave outperformed classical approaches in real-data experiments. Despite these\nempirical successes, implementation can be challenging due to the need to\nensure non-negativity and unit-mass constraints, and theoretical understanding\nremains limited. In particular, it is unclear whether such estimators can\nadaptively achieve faster convergence rates when the underlying density\nexhibits a low-dimensional structure. This paper addresses these gaps by\nproposing a structure-agnostic neural density estimator that is (i)\nstraightforward to implement and (ii) provably adaptive, attaining faster rates\nwhen the true density admits a low-dimensional composition structure. Another\nkey contribution of our work is to show that the proposed estimator integrates\nnaturally into generative sampling pipelines, most notably score-based\ndiffusion models, where it achieves provably faster convergence when the\nunderlying density is structured. We validate its performance through extensive\nsimulations and a real-data application.", "AI": {"tldr": "This paper introduces a neural density estimator that is easy to implement and adapts to low-dimensional structures for faster convergence rates, enhancing both theoretical understanding and practical performance.", "motivation": "The paper seeks to address implementation challenges and the limited theoretical understanding of neural density estimators, specifically their adaptability to low-dimensional structures.", "method": "The method involves developing a structure-agnostic neural density estimator that ensures non-negativity, unit-mass, and adaptability to low-dimensional composition structures. It is also integrated into score-based diffusion models.", "result": "The proposed estimator achieves provably faster convergence rates in low-dimensional structured scenarios and performs well in simulations and a real-data application.", "conclusion": "This work enhances the theoretical foundation and practicality of neural density estimators, demonstrating their adaptability and utility in structured environments and complex pipelines."}}
{"id": "2510.00037", "pdf": "https://arxiv.org/pdf/2510.00037", "abs": "https://arxiv.org/abs/2510.00037", "authors": ["Jianing Guo", "Zhenhong Wu", "Chang Tu", "Yiyao Ma", "Xiangqi Kong", "Zhiqian Liu", "Jiaming Ji", "Shuning Zhang", "Yuanpei Chen", "Kai Chen", "Xianglong Liu", "Qi Dou", "Yaodong Yang", "Huijie Zhao", "Weifeng Lv", "Simin Li"], "title": "On Robustness of Vision-Language-Action Model against Multi-Modal Perturbations", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "In Vision-Language-Action (VLA) models, robustness to real-world\nperturbations is critical for deployment. Existing methods target simple visual\ndisturbances, overlooking the broader multi-modal perturbations that arise in\nactions, instructions, environments, and observations. Here, we first evaluate\nthe robustness of mainstream VLAs under 17 perturbations across four\nmodalities. We find (1) actions as the most fragile modality, (2) Existing\nvisual-robust VLA do not gain robustness in other modality, and (3) pi0\ndemonstrates superior robustness with a diffusion-based action head. To build\nmulti-modal robust VLAs, we propose RobustVLA against perturbations in VLA\ninputs and outputs. For output robustness, we perform offline robust\noptimization against worst-case action noise that maximizes mismatch in flow\nmatching objective. This can be seen as adversarial training, label smoothing,\nand outlier penalization. For input robustness, we enforce consistent actions\nacross input variations that preserve task semantics. To account for multiple\nperturbations, we formulate robustness as a multi-armed bandit problem and\napply an upper confidence bound algorithm to automatically identify the most\nharmful noise. Experiments on LIBERO demonstrate our RobustVLA delivers\nabsolute gains over baselines of 12.6% on the pi0 backbone and 10.4% on the\nOpenVLA backbone across all 17 perturbations, achieving 50.6x faster inference\nthan existing visual-robust VLAs, and a 10.4% gain under mixed perturbations.\nOur RobustVLA is particularly effective on real-world FR5 robot with limited\ndemonstrations, showing absolute gains by 65.6% under perturbations of four\nmodalities.", "AI": {"tldr": "The paper introduces RobustVLA, a robust model for Vision-Language-Action (VLA) tasks against diverse, multi-modal real-world perturbations. It delivers superior performance gains across 17 evaluated perturbations.", "motivation": "Multi-modal Vision-Language-Action models need robustness against real-world perturbations in actions, environments, and observations, which is overlooked in prior research focusing on visual disturbances.", "method": "RobustVLA uses offline robust optimization for output noise, consistent action enforcement for input changes while leveraging a multi-armed bandit approach to identify harmful perturbations. The pi0 backbone with diffusion-based action head enhances robustness.", "result": "Experiments on the LIBERO benchmark showed RobustVLA achieving 12.6% better results on the pi0 backbone, 50.6x faster inference, and significant gains in mixed perturbation and real-world robot scenarios by up to 65.6%.", "conclusion": "RobustVLA effectively builds multi-modal robustness for Vision-Language-Action tasks, outperforming baselines under diverse real-world perturbations, providing advancements in both efficiency and accuracy."}}
{"id": "2510.00071", "pdf": "https://arxiv.org/pdf/2510.00071", "abs": "https://arxiv.org/abs/2510.00071", "authors": ["Dongqi Zheng"], "title": "ARS: Adaptive Reasoning Suppression for Efficient Large Reasoning Language Models", "categories": ["cs.AI", "cs.CL"], "comment": "Accepted by 39th NeurIPS - Foundations of Reasoning in Language\n  Models", "summary": "Large Reasoning Language Models (LRLMs or LRMs) demonstrate remarkable\ncapabilities in complex reasoning tasks, but suffer from significant\ncomputational inefficiencies due to overthinking phenomena. Existing efficient\nreasoning methods face the challenge of balancing reasoning quality with\ninference cost reduction. We propose \\textbf{Adaptive Reasoning Suppression\n(ARS)}, a novel training-free approach that dynamically suppresses redundant\nreasoning steps while preserving accuracy through adaptive certainty\nmonitoring. ARS introduces a multi-checkpoint certainty estimation mechanism\nwith progressive suppression thresholds, achieving superior efficiency compared\nto static suppression methods. Our extensive evaluation across mathematical\nreasoning benchmarks using multiple model architectures demonstrates that ARS\nachieves up to 53%, 46.1%, and 57.9% in token, latency and energy reduction,\nwhile maintaining or improving accuracy.", "AI": {"tldr": "This paper introduces Adaptive Reasoning Suppression (ARS), a method to reduce computational inefficiency in large reasoning language models, achieving up to 57.9% energy savings without sacrificing accuracy.", "motivation": "Large reasoning models perform complex tasks but face computational inefficiencies due to overthinking, which this paper aims to address.", "method": "The ARS method dynamically suppresses redundant reasoning steps using adaptive certainty monitoring and multi-checkpoint certainty estimation mechanisms.", "result": "ARS achieves significant efficiency improvements in token usage (up to 53%), latency (46.1%), and energy reduction (57.9%), while maintaining or improving model accuracy.", "conclusion": "ARS successfully tackles inefficiencies in reasoning models, offering a scalable approach to improving computational performance without compromising reasoning quality."}}
{"id": "2510.00028", "pdf": "https://arxiv.org/pdf/2510.00028", "abs": "https://arxiv.org/abs/2510.00028", "authors": ["Ye Qiao", "Haocheng Xu", "Xiaofan Zhang", "Sitao Huang"], "title": "Rethinking RoPE Scaling in Quantized LLM: Theory, Outlier, and Channel-Band Analysis with Weight Rescaling", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Extending the context window support of large language models (LLMs) is\ncrucial for tasks with long-distance dependencies. RoPE-based interpolation and\nextrapolation methods, such as linear scaling and frequency-aware schemes,\nenable longer input length support without retraining, while post-training\nquantization (PTQ) makes deployment practical. However, we show that combining\nRoPE position interpolation (PI) with PTQ degrades accuracy due to coupled\neffects including long-context aliasing, dynamic-range dilation, anisotropy\nfrom axis-aligned quantizers vs. rotated RoPE pairs, and outlier shifting that\nproduces position-dependent logit noise. We provide, to the best of our\nknowledge, the first systematic analysis of the PI+PTQ approach and introduce\ntwo practical diagnostics: interpolation pressure (per-band sensitivity to\nphase scaling) and tail-inflation ratios (outlier shift from short to long\ncontexts). Following the analysis results, we propose Q-ROAR (Quantization,\nRoPE-interpolation, and Outlier Aware Rescaling), a weight-only,\ninterpolation-aware stabilization of PI for quantized LLMs. Q-ROAR groups RoPE\ndimensions into a small number of frequency bands and performs a lightweight\nsearch over per-band scales for Key and Query weights (with an optional\nsymmetric variant to preserve logit scale). The search is guided by our\ndiagnostics and uses a tiny long-context development dataset, requiring no\nfine-tuning to the model, no architecture or kernel changes, and no additional\ndeployment overhead. Empirically, Q-ROAR reduces the model's perplexity on\nlong-context workloads by more than 14%, while preserving short-context\nperformance, inference throughput, and compatibility with existing LLM system\nstacks.", "AI": {"tldr": "The paper deals with improving the compatibility of large language models with long-context inputs and quantization techniques. A new method called Q-ROAR is proposed to stabilize performance without retraining or additional deployment overhead.", "motivation": "Extend the context window of large language models and address the challenges that arise when combining RoPE position interpolation with post-training quantization.", "method": "Analyze the impact of combining RoPE interpolation and quantization, propose diagnostics (interpolation pressure and tail-inflation ratios), and introduce Q-ROAR, a method that adjusts weight scaling per frequency band.", "result": "Q-ROAR improves model perplexity on long-context tasks by more than 14% while retaining short-context performance and inference efficiency.", "conclusion": "Q-ROAR effectively stabilizes long-context support in quantized language models without requiring retraining, architectural changes, or added deployment complexity."}}
{"id": "2510.00172", "pdf": "https://arxiv.org/pdf/2510.00172", "abs": "https://arxiv.org/abs/2510.00172", "authors": ["Amirhossein Abaskohi", "Tianyi Chen", "Miguel Mu\u00f1oz-M\u00e1rmol", "Curtis Fox", "Amrutha Varshini Ramesh", "\u00c9tienne Marcotte", "Xing Han L\u00f9", "Nicolas Chapados", "Spandana Gella", "Christopher Pal", "Alexandre Drouin", "Issam H. Laradji"], "title": "DRBench: A Realistic Benchmark for Enterprise Deep Research", "categories": ["cs.CL"], "comment": null, "summary": "We introduce DRBench, a benchmark for evaluating AI agents on complex,\nopen-ended deep research tasks in enterprise settings. Unlike prior benchmarks\nthat focus on simple questions or web-only queries, DRBench evaluates agents on\nmulti-step queries (for example, ``What changes should we make to our product\nroadmap to ensure compliance with this standard?\") that require identifying\nsupporting facts from both the public web and private company knowledge base.\nEach task is grounded in realistic user personas and enterprise context,\nspanning a heterogeneous search space that includes productivity software,\ncloud file systems, emails, chat conversations, and the open web. Tasks are\ngenerated through a carefully designed synthesis pipeline with\nhuman-in-the-loop verification, and agents are evaluated on their ability to\nrecall relevant insights, maintain factual accuracy, and produce coherent,\nwell-structured reports. We release 15 deep research tasks across 10 domains,\nsuch as Sales, Cybersecurity, and Compliance. We demonstrate the effectiveness\nof DRBench by evaluating diverse DR agents across open- and closed-source\nmodels (such as GPT, Llama, and Qwen) and DR strategies, highlighting their\nstrengths, weaknesses, and the critical path for advancing enterprise deep\nresearch. Code is available at https://github.com/ServiceNow/drbench.", "AI": {"tldr": "DRBench is a benchmark created to assess AI agents in handling complex, multi-step deep research tasks specific to enterprise environments, requiring a mix of public and private data sources.", "motivation": "The paper aims to address the gap in benchmarks designed to evaluate AI agents on advanced enterprise-focused research tasks, moving beyond simple or web-only queries.", "method": "DRBench introduces tasks grounded in realistic personas and enterprise settings, spanning heterogeneous data sources. Task generation involves synthesis pipelines with human verification, and agent evaluation considers recall accuracy, factual correctness, and report coherence.", "result": "DRBench offers 15 tasks across domains such as Sales and Cybersecurity, evaluates AI agents, and identifies strengths, weaknesses, and future directions for improvement across models like GPT and Llama.", "conclusion": "DRBench serves as a comprehensive tool to push the boundaries of enterprise-focused AI research by assessing multi-domain deep research capabilities and releasing code for further exploration."}}
{"id": "2510.00423", "pdf": "https://arxiv.org/pdf/2510.00423", "abs": "https://arxiv.org/abs/2510.00423", "authors": ["Thomas Zdyrski", "Scott Pauls", "Feng Fu"], "title": "Evolutionary Kuramoto dynamics unravels origins of chimera states in neural populations", "categories": ["q-bio.NC", "91A22 (Primary) 92C42, 05C90 (Secondary)"], "comment": "18+8 pages, 6 figures", "summary": "Neural synchronization is central to cognition However, incomplete\nsynchronization often produces chimera states where coherent and incoherent\ndynamics coexist. While previous studies have explored such patterns using\nnetworks of coupled oscillators, it remains unclear why neurons commit to\ncommunication or how chimera states persist. Here, we investigate the\ncoevolution of neuronal phases and communication strategies on directed,\nweighted networks, where interaction payoffs depend on phase alignment and may\nbe asymmetric due to unilateral communication. We find that both connection\nweights and directionality influence the stability of communicative strategies\n-- and, consequently, full synchronization -- as well as the strategic nature\nof neuronal interactions. Applying our framework to the C. elegans connectome,\nwe show that emergent payoff structures, such as the snowdrift game, underpin\nthe formation of chimera states. Our computational results demonstrate a\npromising neurogame-theoretic perspective, leveraging evolutionary graph theory\nto shed light on mechanisms of neuronal coordination beyond classical\nsynchronization models.", "AI": {"tldr": "The paper explores chimera states in neural synchronization using computational models that incorporate evolutionary game theory.", "motivation": "The authors aim to understand why neurons commit to communication and how incoherent and coherent dynamics (chimera states) persist.", "method": "They use a neurogame-theoretic framework involving coevolution of neuronal phases and communication strategies on directed, weighted networks.", "result": "Findings indicate that connection weights and directionality affect the stability of communication strategies and synchronization. Applying the model to the C. elegans connectome reveals the role of snowdrift game payoff structures in forming chimera states.", "conclusion": "The study provides insights into neuronal coordination mechanisms beyond classical synchronization models, using evolutionary graph theory and game-theoretic approaches."}}
{"id": "2510.00678", "pdf": "https://arxiv.org/pdf/2510.00678", "abs": "https://arxiv.org/abs/2510.00678", "authors": ["Muhammad Ali Jamshed", "Malik Muhammad Saad", "Muhammad Ahmed Mohsin", "Dongkyun Kim", "Octavia A. Dobre", "Halim Yanikomeroglu", "Lina Mohjazi"], "title": "Net-Zero 6G from Earth to Orbit: Sustainable Design of Integrated Terrestrial and Non-Terrestrial Networks", "categories": ["cs.DC"], "comment": "Submitted to IEEE Communications Magazine", "summary": "The integration of Terrestrial Networks (TN) and Non-Terrestrial Networks\n(NTN) plays a crucial role in bridging the digital divide and enabling Sixth\nGeneration (6G) and beyond to achieve truly ubiquitous connectivity. However,\ncombining TN and NTN introduces significant energy challenges due to the\ndiverse characteristics and operational environments of these systems. In this\npaper, we present for the first time a comprehensive overview of the design\nchallenges associated with achieving Net-Zero energy targets in integrated TN\nand NTN systems. We outline a set of key enabling technologies that can support\nthe energy demands of such networks while aligning with Net-Zero objectives. To\nenhance the Energy Efficiency (EE) of integrated TN and NTN systems, we provide\na use case analysis that leverages Artificial Intelligence (AI) to deliver\nadaptable solutions across diverse deployment scenarios. Finally, we highlight\npromising research directions that can guide the sustainable evolution of\nintegrated TN and NTN.", "AI": {"tldr": "The paper addresses energy challenges in integrating Terrestrial and Non-Terrestrial Networks for 6G, proposes AI-driven solutions for energy efficiency, and outlines key enabling technologies and research directions.", "motivation": "The need to seamlessly integrate Terrestrial and Non-Terrestrial Networks to achieve 6G's ubiquitous connectivity while addressing energy efficiency and Net-Zero targets.", "method": "The paper provides a comprehensive design overview, uses AI-based adaptable solutions for varying deployment scenarios, and investigates key technologies for energy-efficient operations.", "result": "The analysis identified key enabling technologies and AI as pivotal for enhancing energy efficiency in integrated TN and NTN systems under varying scenarios.", "conclusion": "Achieving Net-Zero energy targets in TN and NTN integration is feasible through strategic technologies and AI, paving the way for sustainable and efficient 6G networks."}}
{"id": "2510.00498", "pdf": "https://arxiv.org/pdf/2510.00498", "abs": "https://arxiv.org/abs/2510.00498", "authors": ["Qinbing Fu", "Ziyan Qin"], "title": "Emergence of robust looming selectivity via coordinated inhibitory neural computations", "categories": ["q-bio.NC", "cs.NE"], "comment": "27 pages, 17 figures", "summary": "In the locust's lobula giant movement detector neural pathways, four\ncategories of inhibition, i.e., global inhibition, self-inhibition, lateral\ninhibition, and feed-forward inhibition, have been functionally explored in the\ncontext of looming perception. However, their combined influence on shaping\nselectivity to looming motion remains unclear. Driven by recent physiological\nadvancements, this paper offers new insights into the roles of these inhibitory\nmechanisms at multiple levels and scales in simulations, refining the specific\nselectivity for responding only to objects approaching the eyes while remaining\nunresponsive to other forms of movement. Within a feed-forward, multi-layer\nneural network framework, global inhibition, lateral inhibition,\nself-inhibition, and feed-forward inhibition are integrated. Global inhibition\nacts as an immediate feedback mechanism, normalising light intensities\ndelivered by ommatidia, particularly addressing low-contrast looming.\nSelf-inhibition, modelled numerically for the first time, suppresses\ntranslational motion. Lateral inhibition is formed by delayed local excitation\nspreading across a larger area. Notably, self-inhibition and lateral inhibition\nare sequential in time and are combined through feed-forward inhibition, which\nindicates the angular size subtended by moving objects. Together, these\ninhibitory processes attenuate motion-induced excitation at multiple levels and\nscales. This research suggests that self-inhibition may act earlier than\nlateral inhibition to rapidly reduce excitation in situ, thereby suppressing\ntranslational motion, and global inhibition can modulate excitation on a finer\nscale, enhancing selectivity in higher contrast range.", "AI": {"tldr": "This paper examines how multiple inhibitory mechanisms in locust neural pathways combine to fine-tune their selectivity for looming motion.", "motivation": "To clarify how various inhibitory mechanisms interact to shape the precision of looming motion perception in locusts.", "method": "Simulations within a feed-forward, multi-layer neural network integrating global, lateral, self-, and feed-forward inhibition.", "result": "Various inhibitory mechanisms attenuate motion-induced excitation at different levels and timescales, enhancing selectivity for looming objects.", "conclusion": "The research advances understanding of how multiple types of inhibition improve the precision of motion selectivity, especially for detecting objects approaching the locust\u2019s eyes."}}
{"id": "2510.00031", "pdf": "https://arxiv.org/pdf/2510.00031", "abs": "https://arxiv.org/abs/2510.00031", "authors": ["Shun-ichiro Hayashi", "Koki Morita", "Daichi Mukunoki", "Tetsuya Hoshino", "Takahiro Katagiri"], "title": "VibeCodeHPC: An Agent-Based Iterative Prompting Auto-Tuner for HPC Code Generation Using LLMs", "categories": ["cs.SE", "cs.AI", "cs.DC"], "comment": null, "summary": "We propose VibeCodeHPC, an automatic tuning system for HPC programs based on\nmulti-agent LLMs for code generation. VibeCodeHPC tunes programs through\nmulti-agent role allocation and iterative prompt refinement. We describe the\nsystem configuration with four roles: Project Manager (PM), System Engineer\n(SE), Programmer (PG), and Continuous Delivery (CD). We introduce dynamic agent\ndeployment and activity monitoring functions to facilitate effective\nmulti-agent collaboration. In our case study, we convert and optimize CPU-based\nmatrix-matrix multiplication code written in C to GPU code using CUDA. The\nmulti-agent configuration of VibeCodeHPC achieved higher-quality code\ngeneration per unit time compared to a solo-agent configuration. Additionally,\nthe dynamic agent deployment and activity monitoring capabilities facilitated\nmore effective identification of requirement violations and other issues.", "AI": {"tldr": "VibeCodeHPC is an automatic tuning system for HPC programs using multi-agent LLMs for code generation, achieving better results compared to solo-agent systems.", "motivation": "The paper aims to optimize HPC programs through effective collaboration among multiple agents, addressing limitations in solo-agent tuning systems.", "method": "The system design involves four roles: Project Manager, System Engineer, Programmer, and Continuous Delivery, with dynamic agent deployment and activity monitoring.", "result": "VibeCodeHPC demonstrated higher-quality code generation per unit time and better identification of issues compared to solo-agent systems, as tested on matrix-matrix multiplication code conversion to CUDA.", "conclusion": "The proposed system enhances the efficiency and effectiveness of HPC code tuning, emphasizing the benefits of a multi-agent approach."}}
{"id": "2510.00225", "pdf": "https://arxiv.org/pdf/2510.00225", "abs": "https://arxiv.org/abs/2510.00225", "authors": ["Yue Meng", "Fei Chen", "Chuchu Fan"], "title": "TGPO: Temporal Grounded Policy Optimization for Signal Temporal Logic Tasks", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.LO"], "comment": null, "summary": "Learning control policies for complex, long-horizon tasks is a central\nchallenge in robotics and autonomous systems. Signal Temporal Logic (STL)\noffers a powerful and expressive language for specifying such tasks, but its\nnon-Markovian nature and inherent sparse reward make it difficult to be solved\nvia standard Reinforcement Learning (RL) algorithms. Prior RL approaches focus\nonly on limited STL fragments or use STL robustness scores as sparse terminal\nrewards. In this paper, we propose TGPO, Temporal Grounded Policy Optimization,\nto solve general STL tasks. TGPO decomposes STL into timed subgoals and\ninvariant constraints and provides a hierarchical framework to tackle the\nproblem. The high-level component of TGPO proposes concrete time allocations\nfor these subgoals, and the low-level time-conditioned policy learns to achieve\nthe sequenced subgoals using a dense, stage-wise reward signal. During\ninference, we sample various time allocations and select the most promising\nassignment for the policy network to rollout the solution trajectory. To foster\nefficient policy learning for complex STL with multiple subgoals, we leverage\nthe learned critic to guide the high-level temporal search via\nMetropolis-Hastings sampling, focusing exploration on temporally feasible\nsolutions. We conduct experiments on five environments, ranging from\nlow-dimensional navigation to manipulation, drone, and quadrupedal locomotion.\nUnder a wide range of STL tasks, TGPO significantly outperforms\nstate-of-the-art baselines (especially for high-dimensional and long-horizon\ncases), with an average of 31.6% improvement in task success rate compared to\nthe best baseline. The code will be available at\nhttps://github.com/mengyuest/TGPO", "AI": {"tldr": "TGPO is introduced to address challenges in learning control policies for complex, long-horizon STL tasks in robotics, significantly outperforming existing baselines.", "motivation": "Robotics and autonomous systems face challenges in specifying and solving complex tasks using STL due to sparse rewards and non-Markovian dynamics.", "method": "TGPO uses a hierarchical learning framework, splitting tasks into timed subgoals and invariant constraints. It employs Metropolis-Hastings sampling to guide temporal allocation and policy optimization.", "result": "TGPO demonstrates superior performance across five diverse environments, achieving a 31.6% higher task success rate compared to state-of-the-art baselines.", "conclusion": "The proposed TGPO framework effectively handles complex STL specifications, offering a significant advancement in reinforcement learning for robotics."}}
{"id": "2510.00463", "pdf": "https://arxiv.org/pdf/2510.00463", "abs": "https://arxiv.org/abs/2510.00463", "authors": ["Daofu Zhang", "Mehrdad Pournaderi", "Hanne M. Clifford", "Yu Xiang", "Pramod K. Varshney"], "title": "On the Adversarial Robustness of Learning-based Conformal Novelty Detection", "categories": ["stat.ML", "cs.LG", "eess.SP", "stat.ME"], "comment": null, "summary": "This paper studies the adversarial robustness of conformal novelty detection.\nIn particular, we focus on AdaDetect, a powerful learning-based framework for\nnovelty detection with finite-sample false discovery rate (FDR) control. While\nAdaDetect provides rigorous statistical guarantees under benign conditions, its\nbehavior under adversarial perturbations remains unexplored. We first formulate\nan oracle attack setting that quantifies the worst-case degradation of FDR,\nderiving an upper bound that characterizes the statistical cost of attacks.\nThis idealized formulation directly motivates a practical and effective attack\nscheme that only requires query access to AdaDetect's output labels. Coupling\nthese formulations with two popular and complementary black-box adversarial\nalgorithms, we systematically evaluate the vulnerability of AdaDetect on\nsynthetic and real-world datasets. Our results show that adversarial\nperturbations can significantly increase the FDR while maintaining high\ndetection power, exposing fundamental limitations of current error-controlled\nnovelty detection methods and motivating the development of more robust\nalternatives.", "AI": {"tldr": "The paper examines how the adversarial robustness of AdaDetect, a novelty detection algorithm with finite-sample false discovery rate (FDR) control, is affected under adversarial conditions. It finds that adversarial attacks can significantly degrade its statistical guarantees.", "motivation": "The study aims to explore the adversarial vulnerabilities of AdaDetect, a learning-based novelty detection framework that ensures finite-sample FDR control under benign settings, as its robustness under adversarial scenarios had not been examined before.", "method": "The authors formulate an oracle attack to quantify the worst-case FDR degradation and derive an upper bound on its statistical cost. They then propose a practical attack method using query access to AdaDetect's output. The evaluation combines this attack with black-box adversarial algorithms on synthetic and real-world datasets.", "result": "Adversarial perturbations can substantially increase the false discovery rate (FDR) while maintaining high detection power in the AdaDetect framework.", "conclusion": "The study reveals critical vulnerabilities in AdaDetect under adversarial scenarios, highlighting the need for developing more robust novelty detection methods."}}
{"id": "2510.00040", "pdf": "https://arxiv.org/pdf/2510.00040", "abs": "https://arxiv.org/abs/2510.00040", "authors": ["Junjie Li", "Ziao Wang", "Jianghong Ma", "Xiaofeng Zhang"], "title": "Uncovering Intrinsic Capabilities: A Paradigm for Data Curation in Vision-Language Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Large vision-language models (VLMs) achieve strong benchmark performance, but\ncontrolling their behavior through instruction tuning remains difficult.\nReducing the budget of instruction tuning dataset often causes regressions, as\nheuristic strategies treat models as black boxes and overlook the latent\ncapabilities that govern learning. We introduce Capability-Attributed Data\nCuration (CADC), a framework that shifts curation from task-specific heuristics\nto intrinsic capability analysis. CADC discovers intrinsic capabilities in an\nunsupervised manner from gradient-based learning trajectories, attributes\ntraining data to these capabilities via influence estimation, and curates\ncapability-aware curricula through balanced selection and staged sequencing.\nThis transforms black-box instruction tuning into a controllable,\ncapability-driven process. With as little as 5% of the original data, CADC\nsurpasses full-data training on multimodal benchmarks. These results validate\nintrinsic capabilities as the fundamental building blocks of model learning and\nestablish CADC as a principle paradigm for instruction data curation.", "AI": {"tldr": "The paper introduces CADC, a framework to improve instruction tuning in vision-language models by leveraging intrinsic capability analysis, achieving better performance with smaller datasets.", "motivation": "Instruction tuning for vision-language models is difficult, and budget reductions in datasets lead to performance regressions due to reliance on heuristic, black-box strategies.", "method": "CADC leverages unsupervised intrinsic capability discovery, gradient-based learning trajectories, influence estimation, and curriculum design for data curation in instruction tuning.", "result": "CADC achieves benchmark-beating performance with just 5% of the original dataset, proving the value of its capability-driven approach.", "conclusion": "Intrinsic capabilities are fundamental to model learning, and CADC demonstrates a controllable, capability-focused approach for data curation in instruction tuning."}}
{"id": "2510.00075", "pdf": "https://arxiv.org/pdf/2510.00075", "abs": "https://arxiv.org/abs/2510.00075", "authors": ["Rishi Bommasani"], "title": "NeurIPS should lead scientific consensus on AI policy", "categories": ["cs.AI"], "comment": "Published at NeurIPS 2025", "summary": "Designing wise AI policy is a grand challenge for society. To design such\npolicy, policymakers should place a premium on rigorous evidence and scientific\nconsensus. While several mechanisms exist for evidence generation, and nascent\nmechanisms tackle evidence synthesis, we identify a complete void on consensus\nformation. In this position paper, we argue NeurIPS should actively catalyze\nscientific consensus on AI policy. Beyond identifying the current deficit in\nconsensus formation mechanisms, we argue that NeurIPS is the best option due\nits strengths and the paucity of compelling alternatives. To make progress, we\nrecommend initial pilots for NeurIPS by distilling lessons from the IPCC's\nleadership to build scientific consensus on climate policy. We dispel\npredictable counters that AI researchers disagree too much to achieve consensus\nand that policy engagement is not the business of NeurIPS. NeurIPS leads AI on\nmany fronts, and it should champion scientific consensus to create higher\nquality AI policy.", "AI": {"tldr": "The paper argues for NeurIPS to lead in forming scientific consensus on AI policy, inspired by the IPCC's approach to climate policy.", "motivation": "There is a void in mechanisms for consensus formation in AI policy, despite existing mechanisms for evidence generation and synthesis.", "method": "Proposes NeurIPS take the lead by piloting initiatives inspired by the IPCC's methods to drive consensus formation.", "result": "No direct empirical results are provided; the paper outlines a position and argues the case for NeurIPS involvement.", "conclusion": "NeurIPS should leverage its leadership role to fill the gap in scientific consensus mechanisms for AI policy."}}
{"id": "2510.00038", "pdf": "https://arxiv.org/pdf/2510.00038", "abs": "https://arxiv.org/abs/2510.00038", "authors": ["Maria Ana Cardei", "Josephine Lamp", "Mark Derdzinski", "Karan Bhatia"], "title": "DexBench: Benchmarking LLMs for Personalized Decision Making in Diabetes Management", "categories": ["cs.LG", "cs.AI", "cs.CY"], "comment": null, "summary": "We present DexBench, the first benchmark designed to evaluate large language\nmodel (LLM) performance across real-world decision-making tasks faced by\nindividuals managing diabetes in their daily lives. Unlike prior health\nbenchmarks that are either generic, clinician-facing or focused on clinical\ntasks (e.g., diagnosis, triage), DexBench introduces a comprehensive evaluation\nframework tailored to the unique challenges of prototyping patient-facing AI\nsolutions in diabetes, glucose management, metabolic health and related\ndomains. Our benchmark encompasses 7 distinct task categories, reflecting the\nbreadth of real-world questions individuals with diabetes ask, including basic\nglucose interpretation, educational queries, behavioral associations, advanced\ndecision making and long term planning. Towards this end, we compile a rich\ndataset comprising one month of time-series data encompassing glucose traces\nand metrics from continuous glucose monitors (CGMs) and behavioral logs (e.g.,\neating and activity patterns) from 15,000 individuals across three different\ndiabetes populations (type 1, type 2, pre-diabetes/general health and\nwellness). Using this data, we generate a total of 360,600 personalized,\ncontextual questions across the 7 tasks. We evaluate model performance on these\ntasks across 5 metrics: accuracy, groundedness, safety, clarity and\nactionability. Our analysis of 8 recent LLMs reveals substantial variability\nacross tasks and metrics; no single model consistently outperforms others\nacross all dimensions. By establishing this benchmark, we aim to advance the\nreliability, safety, effectiveness and practical utility of AI solutions in\ndiabetes care.", "AI": {"tldr": "DexBench is a benchmark to evaluate large language models (LLMs) on real-world decision-making tasks relevant to diabetes management.", "motivation": "To develop a benchmark that addresses the lack of patient-focused AI evaluation metrics in diabetes care solutions.", "method": "Created a dataset with one month of continuous glucose monitor data and behavioral logs from 15,000 individuals, and generated 360,600 contextual questions across 7 task categories.", "result": "Evaluated 8 LLMs on 5 metrics (accuracy, groundedness, safety, clarity, actionability), finding substantial variability in their performance across tasks and metrics.", "conclusion": "DexBench establishes a comprehensive framework to refine AI solutions in diabetes care, focusing on their reliability, safety, and effectiveness."}}
{"id": "2510.00174", "pdf": "https://arxiv.org/pdf/2510.00174", "abs": "https://arxiv.org/abs/2510.00174", "authors": ["Rik Koncel-Kedziorski", "Brihi Joshi", "Tim Paek"], "title": "PrimeX: A Dataset of Worldview, Opinion, and Explanation", "categories": ["cs.CL"], "comment": "EMNLP 2025 Main", "summary": "As the adoption of language models advances, so does the need to better\nrepresent individual users to the model. Are there aspects of an individual's\nbelief system that a language model can utilize for improved alignment?\nFollowing prior research, we investigate this question in the domain of opinion\nprediction by developing PrimeX, a dataset of public opinion survey data from\n858 US residents with two additional sources of belief information: written\nexplanations from the respondents for why they hold specific opinions, and the\nPrimal World Belief survey for assessing respondent worldview. We provide an\nextensive initial analysis of our data and show the value of belief\nexplanations and worldview for personalizing language models. Our results\ndemonstrate how the additional belief information in PrimeX can benefit both\nthe NLP and psychological research communities, opening up avenues for further\nstudy.", "AI": {"tldr": "This paper introduces PrimeX, a dataset enhancing opinion prediction by incorporating belief explanations and worldview data to better personalize language models.", "motivation": "The paper aims to explore how individual belief systems can improve the alignment and personalization of language models.", "method": "PrimeX was developed using survey data from 858 US respondents, combining public opinion data, written explanations for opinions, and worldview assessments via the Primal World Belief survey.", "result": "The findings indicate that incorporating belief explanations and worldview data enriches the personalization capabilities of language models.", "conclusion": "PrimeX provides valuable insights for both NLP and psychological research, encouraging further exploration into the use of belief representations in models."}}
{"id": "2510.00758", "pdf": "https://arxiv.org/pdf/2510.00758", "abs": "https://arxiv.org/abs/2510.00758", "authors": ["Davide Rucci", "Emanuele Carlini", "Patrizio Dazzi", "Hanna Kavalionak", "Matteo Mordacchini"], "title": "Decentralized and Self-adaptive Core Maintenance on Temporal Graphs", "categories": ["cs.DC"], "comment": null, "summary": "Key graph-based problems play a central role in understanding network\ntopology and uncovering patterns of similarity in homogeneous and temporal\ndata. Such patterns can be revealed by analyzing communities formed by nodes,\nwhich in turn can be effectively modeled through temporal $k$-cores. This paper\nintroduces a novel decentralized and incremental algorithm for computing the\ncore decomposition of temporal networks. Decentralized solutions leverage the\nability of network nodes to communicate and coordinate locally, addressing\ncomplex problems in a scalable, adaptive, and timely manner. By leveraging\npreviously computed coreness values, our approach significantly reduces the\nactivation of nodes and the volume of message exchanges when the network\nchanges over time. This enables scalability with only a minimal trade-off in\nprecision. Experimental evaluations on large real-world networks under varying\nlevels of dynamism demonstrate the efficiency of our solution compared to a\nstate-of-the-art approach, particularly in terms of active nodes, communication\noverhead, and convergence speed.", "AI": {"tldr": "The paper presents a new decentralized, incremental algorithm for efficient core decomposition in temporal networks, reducing communication and computational overhead while maintaining scalability and precision.", "motivation": "To improve scalability, adaptability, and efficiency in analyzing network topology and uncovering patterns in temporal and homogeneous graph-like data.", "method": "Introduced a decentralized algorithm that incrementally computes core decomposition in temporal graphs, leveraging previously calculated coreness values to minimize node activation and message exchanges during dynamic network updates.", "result": "The proposed method outperforms a state-of-the-art approach in experiments on large real-world networks, excelling in active node reduction, lower communication overhead, and faster convergence.", "conclusion": "The method provides a scalable solution for dynamic networks with minimal precision trade-offs, addressing challenges in temporal graph analysis effectively."}}
{"id": "2510.00698", "pdf": "https://arxiv.org/pdf/2510.00698", "abs": "https://arxiv.org/abs/2510.00698", "authors": ["Fu-Chen Guo", "Pei-Zhi Zhuang", "Fei Ren", "Hong-Ya Yue", "He Yang"], "title": "Physics-Informed Extreme Learning Machine (PIELM) for Tunnelling-Induced Soil-Pile Interactions", "categories": ["cs.LG", "cs.NE", "physics.comp-ph", "physics.geo-ph"], "comment": null, "summary": "Physics-informed machine learning has been a promising data-driven and\nphysics-informed approach in geotechnical engineering. This study proposes a\nphysics-informed extreme learning machine (PIELM) framework for analyzing\ntunneling-induced soil-pile interactions. The pile foundation is modeled as an\nEuler-Bernoulli beam, and the surrounding soil is modeled as a Pasternak\nfoundation. The soil-pile interaction is formulated into a fourth-order\nordinary differential equation (ODE) that constitutes the physics-informed\ncomponent, while measured data are incorporated into PIELM as the data-driven\ncomponent. Combining physics and data yields a loss vector of the extreme\nlearning machine (ELM) network, which is trained within 1 second by the least\nsquares method. After validating the PIELM approach by the boundary element\nmethod (BEM) and finite difference method (FDM), parametric studies are carried\nout to examine the effects of ELM network architecture, data monitoring\nlocations and numbers on the performance of PIELM. The results indicate that\nmonitored data should be placed at positions where the gradients of pile\ndeflections are significant, such as at the pile tip/top and near tunneling\nzones. Two application examples highlight the critical role of physics-informed\nand data-driven approach for tunnelling-induced soil-pile interactions. The\nproposed approach shows great potential for real-time monitoring and safety\nassessment of pile foundations, and benefits for intelligent early-warning\nsystems in geotechnical engineering.", "AI": {"tldr": "This paper introduces a physics-informed machine learning framework for modeling soil-pile interactions during tunneling.", "motivation": "To enhance the analysis and safety assessment of tunneling-induced soil-pile interactions using a physics-informed framework.", "method": "The study combines physics (ODE formulation) and data (extreme learning machine network) within a Physics-Informed Extreme Learning Machine framework.", "result": "The method was validated using boundary element and finite difference methods, with monitored data showing the significance of pile deflection gradient locations.", "conclusion": "The framework holds great potential for real-time geotechnical monitoring and intelligent early-warning systems in tunneling applications."}}
{"id": "2510.00092", "pdf": "https://arxiv.org/pdf/2510.00092", "abs": "https://arxiv.org/abs/2510.00092", "authors": ["Shufeng Chen", "Mariat James Elizebeth", "Robab Aghazadeh Chakherlou", "Xingyu Zhao", "Eric Barbier", "Siddartha Khastgir", "Paul Jennings"], "title": "A Scalable Framework for Safety Assurance of Self-Driving Vehicles based on Assurance 2.0", "categories": ["cs.SE"], "comment": null, "summary": "Assurance 2.0 is a modern framework developed to address the assurance\nchallenges of increasingly complex, adaptive, and autonomous systems. Building\non the traditional Claims-Argument-Evidence (CAE) model, it introduces reusable\nassurance theories and explicit counterarguments (defeaters) to enhance rigor,\ntransparency, and adaptability. It supports continuous, incremental assurance,\nenabling innovation without compromising safety. However, limitations persist\nin confidence measurement, residual doubt management, automation support, and\nthe practical handling of defeaters and confirmation bias. This paper presents\n\\textcolor{black}{a set of decomposition frameworks to identify a complete set\nof safety arguments and measure their corresponding evidence.} Grounded in the\nAssurance 2.0 paradigm, the framework is instantiated through a structured\ntemplate and employs a three-tiered decomposition strategy. \\textcolor{black}{A\ncase study regarding the application of the decomposition framework in the\nend-to-end (E2E) AI-based Self-Driving Vehicle (SDV) development is also\npresented in this paper.} At the top level, the SDV development is divided into\nthree critical phases: Requirements Engineering (RE), Verification and\nValidation (VnV), and Post-Deployment (PD). Each phase is further decomposed\naccording to its Product Development Lifecycle (PDLC). To ensure comprehensive\ncoverage, each PDLC is analyzed using an adapted 5M1E model (Man, Machine,\nMethod, Material, Measurement, and Environment). Originally developed for\nmanufacturing quality control, the 5M1E model is reinterpreted and contextually\nmapped to the assurance domain. This enables a multi-dimensional decomposition\nthat supports fine-grained traceability of safety claims, evidence, and\npotential defeaters.", "AI": {"tldr": "The paper develops Assurance 2.0, a framework to manage safety argumentation for complex systems and demonstrates its application in self-driving vehicle development.", "motivation": "Address challenges in assuring safety of advanced systems like autonomous vehicles, especially complexities surrounding evidence and transparency.", "method": "Implements a structured decomposition framework using a three-tier strategy grounded in Assurance 2.0, supported by 5M1E model adaptations for safety assurance.", "result": "Case study of the framework applied to self-driving vehicle development, showcasing its traceability across safety-critical lifecycle phases.", "conclusion": "The framework ensures detailed and transparent safety assessment, though challenges in automation and bias management still remain."}}
{"id": "2510.00272", "pdf": "https://arxiv.org/pdf/2510.00272", "abs": "https://arxiv.org/abs/2510.00272", "authors": ["Odichimnma Ezeji", "Michael Ziegltrum", "Giulio Turrisi", "Tommaso Belvedere", "Valerio Modugno"], "title": "BC-MPPI: A Probabilistic Constraint Layer for Safe Model-Predictive Path-Integral Control", "categories": ["cs.RO"], "comment": null, "summary": "Model Predictive Path Integral (MPPI) control has recently emerged as a fast,\ngradient-free alternative to model-predictive control in highly non-linear\nrobotic tasks, yet it offers no hard guarantees on constraint satisfaction. We\nintroduce Bayesian-Constraints MPPI (BC-MPPI), a lightweight safety layer that\nattaches a probabilistic surrogate to every state and input constraint. At each\nre-planning step the surrogate returns the probability that a candidate\ntrajectory is feasible; this joint probability scales the weight given to a\ncandidate, automatically down-weighting rollouts likely to collide or exceed\nlimits and pushing the sampling distribution toward the safe subset; no\nhand-tuned penalty costs or explicit sample rejection required. We train the\nsurrogate from 1000 offline simulations and deploy the controller on a\nquadrotor in MuJoCo with both static and moving obstacles. Across K in\n[100,1500] rollouts BC-MPPI preserves safety margins while satisfying the\nprescribed probability of violation. Because the surrogate is a stand-alone,\nversion-controlled artefact and the runtime safety score is a single scalar,\nthe approach integrates naturally with verification-and-validation pipelines\nfor certifiable autonomous systems.", "AI": {"tldr": "BC-MPPI introduces a probabilistic layer to adjust trajectory sampling based on safety probabilities, improving control reliability in non-linear robotic tasks.", "motivation": "Enhance Model Predictive Path Integral (MPPI) control's ability to reliably handle constraints in robotic systems.", "method": "Utilize probabilistic surrogates for state and input constraints, trained via 1000 offline simulations, to guide trajectory selections in MPPI.", "result": "BC-MPPI maintains safety margins effectively while adhering to specific violation probabilities in quadrotor tests with varied obstacles.", "conclusion": "BC-MPPI improves MPPI control robustness and integrates well into validation systems for certifiable autonomous robots, promoting safer operations."}}
{"id": "2510.00504", "pdf": "https://arxiv.org/pdf/2510.00504", "abs": "https://arxiv.org/abs/2510.00504", "authors": ["Hong-Yi Wang", "Di Luo", "Tomaso Poggio", "Isaac L. Chuang", "Liu Ziyin"], "title": "A universal compression theory: Lottery ticket hypothesis and superpolynomial scaling laws", "categories": ["stat.ML", "cond-mat.dis-nn", "cs.IT", "cs.LG", "math.IT"], "comment": "preprint", "summary": "When training large-scale models, the performance typically scales with the\nnumber of parameters and the dataset size according to a slow power law. A\nfundamental theoretical and practical question is whether comparable\nperformance can be achieved with significantly smaller models and substantially\nless data. In this work, we provide a positive and constructive answer. We\nprove that a generic permutation-invariant function of $d$ objects can be\nasymptotically compressed into a function of $\\operatorname{polylog} d$ objects\nwith vanishing error. This theorem yields two key implications: (Ia) a large\nneural network can be compressed to polylogarithmic width while preserving its\nlearning dynamics; (Ib) a large dataset can be compressed to polylogarithmic\nsize while leaving the loss landscape of the corresponding model unchanged.\n(Ia) directly establishes a proof of the \\textit{dynamical} lottery ticket\nhypothesis, which states that any ordinary network can be strongly compressed\nsuch that the learning dynamics and result remain unchanged. (Ib) shows that a\nneural scaling law of the form $L\\sim d^{-\\alpha}$ can be boosted to an\narbitrarily fast power law decay, and ultimately to $\\exp(-\\alpha'\n\\sqrt[m]{d})$.", "AI": {"tldr": "The paper demonstrates that a generic permutation-invariant function can be compressed substantially in model size (polylogarithmic width) and dataset size while retaining performance.", "motivation": "To explore whether comparable performance for large-scale models can be achieved with smaller models and less data, addressing cost and scalability issues.", "method": "The authors prove a theorem about asymptotically compressing functions of 'd' objects, preserving learning dynamics and loss landscapes despite model or dataset size reduction.", "result": "The findings validate the dynamical lottery ticket hypothesis and achieve faster neural scaling laws by proposing significant compression techniques.", "conclusion": "Large-scale models and datasets can be substantially compressed without compromising learning dynamics or scaling laws, improving efficiency drastically."}}
{"id": "2510.00041", "pdf": "https://arxiv.org/pdf/2510.00041", "abs": "https://arxiv.org/abs/2510.00041", "authors": ["Yuchen Song", "Andong Chen", "Wenxin Zhu", "Kehai Chen", "Xuefeng Bai", "Muyun Yang", "Tiejun Zhao"], "title": "Culture In a Frame: C$^3$B as a Comic-Based Benchmark for Multimodal Culturally Awareness", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Cultural awareness capabilities has emerged as a critical capability for\nMultimodal Large Language Models (MLLMs). However, current benchmarks lack\nprogressed difficulty in their task design and are deficient in cross-lingual\ntasks. Moreover, current benchmarks often use real-world images. Each\nreal-world image typically contains one culture, making these benchmarks\nrelatively easy for MLLMs. Based on this, we propose C$^3$B ($\\textbf{C}$omics\n$\\textbf{C}$ross-$\\textbf{C}$ultural $\\textbf{B}$enchmark), a novel\nmulticultural, multitask and multilingual cultural awareness capabilities\nbenchmark. C$^3$B comprises over 2000 images and over 18000 QA pairs,\nconstructed on three tasks with progressed difficulties, from basic visual\nrecognition to higher-level cultural conflict understanding, and finally to\ncultural content generation. We conducted evaluations on 11 open-source MLLMs,\nrevealing a significant performance gap between MLLMs and human performance.\nThe gap demonstrates that C$^3$B poses substantial challenges for current\nMLLMs, encouraging future research to advance the cultural awareness\ncapabilities of MLLMs.", "AI": {"tldr": "The paper introduces C$^3$B, a benchmark designed to evaluate cultural awareness in Multimodal Large Language Models (MLLMs), revealing significant gaps between model and human performance.", "motivation": "Current benchmarks for cultural awareness in MLLMs are not sufficiently complex, lack cross-lingual tasks, and predominantly use single-culture images, making them less effective for evaluating advanced capabilities.", "method": "The authors created the C$^3$B benchmark with over 2000 images and 18,000 QA pairs, structured across three tasks with increasing difficulty: visual recognition, cultural conflict understanding, and cultural content generation.", "result": "Evaluation of 11 open-source MLLMs showed that they performed significantly worse compared to humans on the C$^3$B benchmark, indicating substantial challenges in current model capabilities.", "conclusion": "C$^3$B highlights a critical gap in MLLM cultural awareness and offers a robust tool to drive advancements in this area, encouraging more sophisticated model development."}}
{"id": "2510.00084", "pdf": "https://arxiv.org/pdf/2510.00084", "abs": "https://arxiv.org/abs/2510.00084", "authors": ["Fabian Kovac", "Sebastian Neumaier", "Timea Pahi", "Torsten Priebe", "Rafael Rodrigues", "Dimitrios Christodoulou", "Maxime Cordy", "Sylvain Kubler", "Ali Kordia", "Georgios Pitsiladis", "John Soldatos", "Petros Zervoudakis"], "title": "Towards a Framework for Supporting the Ethical and Regulatory Certification of AI Systems", "categories": ["cs.AI", "cs.CY", "cs.DB"], "comment": "Accepted for publication in the proceedings of the Workshop on AI\n  Certification, Fairness and Regulations, co-located with the Austrian\n  Symposium on AI and Vision (AIRoV 2025)", "summary": "Artificial Intelligence has rapidly become a cornerstone technology,\nsignificantly influencing Europe's societal and economic landscapes. However,\nthe proliferation of AI also raises critical ethical, legal, and regulatory\nchallenges. The CERTAIN (Certification for Ethical and Regulatory Transparency\nin Artificial Intelligence) project addresses these issues by developing a\ncomprehensive framework that integrates regulatory compliance, ethical\nstandards, and transparency into AI systems. In this position paper, we outline\nthe methodological steps for building the core components of this framework.\nSpecifically, we present: (i) semantic Machine Learning Operations (MLOps) for\nstructured AI lifecycle management, (ii) ontology-driven data lineage tracking\nto ensure traceability and accountability, and (iii) regulatory operations\n(RegOps) workflows to operationalize compliance requirements. By implementing\nand validating its solutions across diverse pilots, CERTAIN aims to advance\nregulatory compliance and to promote responsible AI innovation aligned with\nEuropean standards.", "AI": {"tldr": "The CERTAIN project proposes a framework integrating compliance, ethics, and transparency into AI systems, focusing on semantic MLOps, data lineage tracking, and RegOps workflows.", "motivation": "To address ethical, legal, and regulatory challenges posed by AI proliferation in Europe's societal and economic contexts.", "method": "The paper outlines semantic MLOps for AI lifecycle management, ontology-driven data lineage tracking, and RegOps workflows for compliance.", "result": "CERTAIN solutions are implemented and validated across various pilots to enhance regulatory compliance and ethical AI innovation.", "conclusion": "The project advances EU-aligned responsible AI development by integrating compliance, ethics, and operational transparency."}}
{"id": "2510.00043", "pdf": "https://arxiv.org/pdf/2510.00043", "abs": "https://arxiv.org/abs/2510.00043", "authors": ["Gregory D. Baker", "Scott McCallum", "Dirk Pattinson"], "title": "Linear Regression in p-adic metric spaces", "categories": ["cs.LG", "cs.CL", "math.NT", "11D88, 62J99, 68T50", "G.3; I.2.6; I.2.7; I.5.1; I.5.4"], "comment": null, "summary": "Many real-world machine learning problems involve inherently hierarchical\ndata, yet traditional approaches rely on Euclidean metrics that fail to capture\nthe discrete, branching nature of hierarchical relationships. We present a\ntheoretical foundation for machine learning in p-adic metric spaces, which\nnaturally respect hierarchical structure. Our main result proves that an\nn-dimensional plane minimizing the p-adic sum of distances to points in a\ndataset must pass through at least n + 1 of those points -- a striking contrast\nto Euclidean regression that highlights how p-adic metrics better align with\nthe discrete nature of hierarchical data. As a corollary, a polynomial of\ndegree n constructed to minimise the p-adic sum of residuals will pass through\nat least n + 1 points. As a further corollary, a polynomial of degree n\napproximating a higher degree polynomial at a finite number of points will\nyield a difference polynomial that has distinct rational roots. We demonstrate\nthe practical significance of this result through two applications in natural\nlanguage processing: analyzing hierarchical taxonomies and modeling grammatical\nmorphology. These results suggest that p-adic metrics may be fundamental to\nproperly handling hierarchical data structures in machine learning. In\nhierarchical data, interpolation between points often makes less sense than\nselecting actual observed points as representatives.", "AI": {"tldr": "The paper introduces a theoretical framework for machine learning using p-adic metrics, which naturally respect hierarchical and discrete data structures, contrasting traditional Euclidean approaches.", "motivation": "Traditional machine learning methods relying on Euclidean metrics fail to capture the branching, discrete nature of hierarchical data, prompting the need for an alternative foundational approach.", "method": "The authors propose using p-adic metric spaces for machine learning, theoretically proving key results about minimizing distances and constructing polynomials in this space.", "result": "The authors demonstrate that p-adic regression ensures closer alignment with hierarchical data properties and provide practical applications in natural language processing, including hierarchical taxonomy analysis and grammatical modeling.", "conclusion": "P-adic metrics highlight a fundamentally better approach for handling hierarchical data structures, proving their utility in both theory and machine learning practice."}}
{"id": "2510.00177", "pdf": "https://arxiv.org/pdf/2510.00177", "abs": "https://arxiv.org/abs/2510.00177", "authors": ["Shuyue Stella Li", "Avinandan Bose", "Faeze Brahman", "Simon Shaolei Du", "Pang Wei Koh", "Maryam Fazel", "Yulia Tsvetkov"], "title": "Personalized Reasoning: Just-In-Time Personalization and Why LLMs Fail At It", "categories": ["cs.CL", "cs.AI"], "comment": "57 pages, 6 figures", "summary": "Current large language model (LLM) development treats task-solving and\npreference alignment as separate challenges, optimizing first for objective\ncorrectness, then for alignment to aggregated human preferences. This paradigm\nfails in human-facing applications where solving a problem correctly is\ninsufficient if the response mismatches the user's needs. This challenge\nintensifies in just-in-time scenarios where no prior user interaction history\nexists due to cold-start conditions or privacy constraints. LLMs need to\nidentify what they don't know about user preferences, strategically elicit\npreference values through questioning, then adapt their reasoning processes and\nresponses accordingly -- a complicated chain of cognitive processes which we\nterm personalized reasoning. We introduce PREFDISCO, an evaluation methodology\nthat transforms static benchmarks into interactive personalization tasks using\npsychologically-grounded personas with sparse preferences. Our framework\ncreates scenarios where identical questions require different reasoning chains\ndepending on user context, as optimal explanation approaches vary by individual\nexpertise and preferences while maintaining factual accuracy. Evaluation of 21\nfrontier models across 10 tasks reveals 29.0% of naive personalization attempts\nproduce worse preference alignment than generic responses, yet generic\nresponses also fail to serve individual user needs effectively. These findings\nsuggest personalized reasoning requires dedicated development rather than\nemerging naturally. PREFDISCO establishes personalized reasoning as a\nmeasurable research frontier and reveals fundamental limitations in current\nLLMs' interactive capabilities, providing a foundation for developing systems\nthat can adapt to individual users in education, healthcare, and technical\ndomains where personalization is critical.", "AI": {"tldr": "The paper presents PREFDISCO, a methodology to evaluate the personalized reasoning ability of large language models, revealing their limitations in adapting to individual user preferences.", "motivation": "To highlight the importance of personalized reasoning in LLMs, which goes beyond objective correctness and human preference alignment, especially in scenarios with no prior user context.", "method": "The paper introduces PREFDISCO, a framework using psychologically-grounded personas and sparse preferences to evaluate LLMs in interactive personalization tasks.", "result": "Evaluation of 21 models showed that naive personalization often resulted in worse outcomes than generic responses, and current LLMs struggle in personalized reasoning.", "conclusion": "Dedicated development toward personalized reasoning is necessary, as current LLMs fail to naturally adapt to individual preferences, particularly in critical fields like education and healthcare."}}
{"id": "2510.00764", "pdf": "https://arxiv.org/pdf/2510.00764", "abs": "https://arxiv.org/abs/2510.00764", "authors": ["Zhuo Zhang", "Amit Yaron", "Dai Akita", "Tomoyo Isoguchi Shiramatsu", "Zenas C. Chao", "Hirokazu Takahashi"], "title": "Emergence of Deviance Detection in Cortical Cultures through Maturation, Criticality, and Early Experience", "categories": ["q-bio.NC"], "comment": null, "summary": "Mismatch negativity (MMN) in humans reflects deviance detection (DD), a core\nneural mechanism of predictive processing. However, the fundamental principles\nby which DD emerges and matures during early cortical development-potentially\nproviding a neuronal scaffold for MMN-remain unclear. Here, we tracked the\ndevelopment of DD in dissociated cortical cultures grown on high-density CMOS\nmicroelectrode arrays from 10 to 35 days in vitro (DIV). Cultures were\nstimulated with oddball and many-standards control paradigms while spontaneous\nand evoked activity were recorded longitudinally. At early stages,\nstimulus-evoked responses were confined to fast components reflecting direct\nactivation. From DIV15-20 onward, robust late responses appeared, and deviant\nstimuli progressively evoked stronger responses than frequent and control\nstimuli, marking the onset of DD. By DIV30, responses became stronger, faster,\nand more temporally precise. Neuronal avalanche analysis revealed a gradual\ntransition from subcritical to near-critical dynamics, with cultures exhibiting\npower-law statistics showing the strongest deviant responses. Nonetheless, DD\nwas also present in non-critical networks, indicating that criticality is not\nrequired for its emergence but instead stabilizes and amplifies predictive\nprocessing as networks mature. Early oddball experience reinforces the deviant\npathway, resulting in faster conduction along those circuits. However, as\nfrequent and deviant pathways become less distinct, the deviance detection\nindex is reduced. Together, these findings demonstrate that DD arises\nintrinsically through local circuit maturation, while self-organization toward\ncriticality and early experience further refine its strength and timing,\nproviding mechanistic insight into predictive coding in simplified cortical\nnetworks and informing the design of adaptive, prediction-sensitive artificial\nsystems.", "AI": {"tldr": "This study explores how deviance detection (DD), a key neural mechanism related to predictive processing, matures in cortical networks using dissociated cultures. It highlights that DD arises intrinsically through local circuit maturation and is enhanced by critical network dynamics and early experiences.", "motivation": "To investigate the fundamental principles by which deviance detection (DD) emerges and matures during early cortical development, providing potential insights into mechanisms underlying mismatch negativity (MMN) in humans.", "method": "The researchers used dissociated cortical cultures grown on high-density CMOS microelectrode arrays. They stimulated these cultures with oddball and control paradigms while recording spontaneous and evoked activity over 10 to 35 days in vitro (DIV). They analyzed stimulus-evoked responses, neuronal avalanche dynamics, and the refinement of predictive processing.", "result": "The study found that DD appears with the emergence of late responses at approximately DIV15\u201320. By DIV30, responses in the networks became faster, stronger, and more precise. While criticality significantly amplified and stabilized DD, it was not strictly necessary for DD to emerge. Early oddball stimulus exposure reinforced deviant pathways, but differentiation between deviant and frequent pathways reduced over time.", "conclusion": "Deviance detection arises intrinsically through local circuit maturation and is further refined with self-organization toward criticality and early experiences. These findings deepen the understanding of predictive coding in simplified cortical networks and have potential implications for adaptive artificial systems."}}
{"id": "2510.00822", "pdf": "https://arxiv.org/pdf/2510.00822", "abs": "https://arxiv.org/abs/2510.00822", "authors": ["Sairam Sri Vatsavai", "Raees Khan", "Kuan-Chieh Hsu", "Ozgur O. Kilic", "Paul Nilsson", "Tatiana Korchuganova", "David K. Park", "Sankha Dutta", "Yihui Ren", "Joseph Boudreau", "Tasnuva Chowdhury", "Shengyu Feng", "Jaehyung Kim", "Scott Klasky", "Tadashi Maeno", "Verena Ingrid Martinez", "Norbert Podhorszki", "Fr\u00e9d\u00e9ric Suter", "Wei Yang", "Yiming Yang", "Shinjae Yoo", "Alexei Klimentov", "Adolfy Hoisie"], "title": "CGSim: A Simulation Framework for Large Scale Distributed Computing Environment", "categories": ["cs.DC", "cs.PF"], "comment": "The paper has been accepted at PMBS workshop SC25", "summary": "Large-scale distributed computing infrastructures such as the Worldwide LHC\nComputing Grid (WLCG) require comprehensive simulation tools for evaluating\nperformance, testing new algorithms, and optimizing resource allocation\nstrategies. However, existing simulators suffer from limited scalability,\nhardwired algorithms, lack of real-time monitoring, and inability to generate\ndatasets suitable for modern machine learning approaches. We present CGSim, a\nsimulation framework for large-scale distributed computing environments that\naddresses these limitations. Built upon the validated SimGrid simulation\nframework, CGSim provides high-level abstractions for modeling heterogeneous\ngrid environments while maintaining accuracy and scalability. Key features\ninclude a modular plugin mechanism for testing custom workflow scheduling and\ndata movement policies, interactive real-time visualization dashboards, and\nautomatic generation of event-level datasets suitable for AI-assisted\nperformance modeling. We demonstrate CGSim's capabilities through a\ncomprehensive evaluation using production ATLAS PanDA workloads, showing\nsignificant calibration accuracy improvements across WLCG computing sites.\nScalability experiments show near-linear scaling for multi-site simulations,\nwith distributed workloads achieving 6x better performance compared to\nsingle-site execution. The framework enables researchers to simulate WLCG-scale\ninfrastructures with hundreds of sites and thousands of concurrent jobs within\npractical time budget constraints on commodity hardware.", "AI": {"tldr": "CGSim is a simulation framework designed for large-scale distributed computing environments like the Worldwide LHC Computing Grid (WLCG), enabling scalable, accurate simulations with features like modular plugins, real-time visualization, and dataset generation for AI.", "motivation": "Existing simulation tools for WLCG suffer from limitations in scalability, flexibility, real-time monitoring, and the generation of useful datasets for machine learning applications.", "method": "The authors developed CGSim, based on the SimGrid framework, incorporating features like modular plugins for policies testing, interactive dashboards, and AI-compatible dataset generation. It is evaluated with ATLAS PanDA workloads.", "result": "CGSim showed significant calibration accuracy improvements, near-linear scaling for multi-site simulations, and 6x performance enhancements with distributed workloads compared to single-site execution.", "conclusion": "CGSim effectively addresses limitations in current tools, enabling simulations of WLCG-scale infrastructures with improved performance and scalability within practical time constraints."}}
{"id": "2510.00960", "pdf": "https://arxiv.org/pdf/2510.00960", "abs": "https://arxiv.org/abs/2510.00960", "authors": ["Miha O\u017ebot", "Igor \u0160krjanc", "Vitomir \u0160truc"], "title": "A Neuro-Fuzzy System for Interpretable Long-Term Stock Market Forecasting", "categories": ["cs.AI", "cs.NE", "cs.SY", "eess.SY", "I.2.6"], "comment": "Published in: ERK 2025 -- 34th International Electrotechnical and\n  Computer Science Conference, Portoro\\v{z}, Slovenia, Sept. 25--26, 2025.\n  Proceedings published by Dru\\v{s}tvo Slovenska sekcija IEEE. ISSN: 2591-0442\n  (online). 4 pages, 2 figures", "summary": "In the complex landscape of multivariate time series forecasting, achieving\nboth accuracy and interpretability remains a significant challenge. This paper\nintroduces the Fuzzy Transformer (Fuzzformer), a novel recurrent neural network\narchitecture combined with multi-head self-attention and fuzzy inference\nsystems to analyze multivariate stock market data and conduct long-term time\nseries forecasting. The method leverages LSTM networks and temporal attention\nto condense multivariate data into interpretable features suitable for fuzzy\ninference systems. The resulting architecture offers comparable forecasting\nperformance to conventional models such as ARIMA and LSTM while providing\nmeaningful information flow within the network. The method was examined on the\nreal world stock market index S\\&P500. Initial results show potential for\ninterpretable forecasting and identify current performance tradeoffs,\nsuggesting practical application in understanding and forecasting stock market\nbehavior.", "AI": {"tldr": "The paper presents Fuzzformer, a novel architecture combining fuzzy inference systems and self-attention mechanisms for interpretable multivariate time series forecasting in stock market data.", "motivation": "To address the challenge of achieving both accuracy and interpretability in multivariate time series forecasting, particularly for stock market data.", "method": "The approach integrates LSTM networks with temporal attention to condense multivariate data into interpretable features, leveraging fuzzy inference systems for enhanced interpretability.", "result": "The Fuzzformer shows comparable forecasting performance to standard models like ARIMA and LSTM, while maintaining interpretability in feature analysis, tested on S&P500 stock market data.", "conclusion": "The initial results demonstrate the feasibility of interpretable forecasting in stock market analysis, suggesting performance tradeoffs but highlighting practical applicability."}}
{"id": "2510.00197", "pdf": "https://arxiv.org/pdf/2510.00197", "abs": "https://arxiv.org/abs/2510.00197", "authors": ["Diogo Maia", "Filipe Correia", "Andr\u00e9 Restivo", "Paulo Queiroz"], "title": "Container Orchestration Patterns for Optimizing Resource Use", "categories": ["cs.SE", "D.2.11"], "comment": null, "summary": "Service-based architectures provide substantial benefits, yet service\norchestration remains a challenge, particularly for newcomers. While various\nresources on orchestration techniques exist, they often lack clarity and\nstandardization, making best practices difficult to implement and limiting\ntheir adoption within the software industry.\n  To address this gap, we analyzed existing literature and tools to identify\ncommon orchestration practices. Based on our findings, we define three key\norchestration resource optimization patterns: {\\sc Preemptive Scheduling}, {\\sc\nService Balancing}, and {\\sc Garbage Collection}. {\\sc Preemptive Scheduling}\nallows the allocation of sufficient resources for services of higher priority\nin stressful situations, while {\\sc Service Balancing} enables a restructuring\nof the nodes to allow better resource usage. To end, {\\sc Garbage Collection}\ncreates cleanup mechanisms to better understand the system's resource usage and\noptimize it. These patterns serve as foundational elements for improving\norchestration practices and fostering broader adoption in service-based\narchitectures.", "AI": {"tldr": "This paper identifies and defines three key orchestration optimization patterns to address challenges in service-based architectures.", "motivation": "Service orchestration in service-based architectures is challenging, especially for newcomers, due to a lack of clear, standardized resources on best practices.", "method": "The authors analyzed existing literature and tools to identify common service orchestration practices and formalized three optimization patterns.", "result": "Three orchestration resource optimization patterns were defined: Preemptive Scheduling, Service Balancing, and Garbage Collection, aimed at enhancing resource allocation, usage, and cleanup efficiency.", "conclusion": "These patterns provide a foundational framework for improving service orchestration practices and encouraging adoption in the software industry."}}
{"id": "2510.00329", "pdf": "https://arxiv.org/pdf/2510.00329", "abs": "https://arxiv.org/abs/2510.00329", "authors": ["Sarmad Mehrdad", "Maxime Sabbah", "Vincent Bonnet", "Ludovic Righetti"], "title": "Learning Human Reaching Optimality Principles from Minimal Observation Inverse Reinforcement Learning", "categories": ["cs.RO"], "comment": "8 pages, 4 figures", "summary": "This paper investigates the application of Minimal Observation Inverse\nReinforcement Learning (MO-IRL) to model and predict human arm-reaching\nmovements with time-varying cost weights. Using a planar two-link biomechanical\nmodel and high-resolution motion-capture data from subjects performing a\npointing task, we segment each trajectory into multiple phases and learn\nphase-specific combinations of seven candidate cost functions. MO-IRL\niteratively refines cost weights by scaling observed and generated trajectories\nin the maximum entropy IRL formulation, greatly reducing the number of required\ndemonstrations and convergence time compared to classical IRL approaches.\nTraining on ten trials per posture yields average joint-angle Root Mean Squared\nErrors (RMSE) of 6.4 deg and 5.6 deg for six- and eight-segment weight\ndivisions, respectively, versus 10.4 deg using a single static weight.\nCross-validation on remaining trials and, for the first time, inter-subject\nvalidation on an unseen subject's 20 trials, demonstrates comparable predictive\naccuracy, around 8 deg RMSE, indicating robust generalization. Learned weights\nemphasize joint acceleration minimization during movement onset and\ntermination, aligning with smoothness principles observed in biological motion.\nThese results suggest that MO-IRL can efficiently uncover dynamic,\nsubject-independent cost structures underlying human motor control, with\npotential applications for humanoid robots.", "AI": {"tldr": "This study uses Minimal Observation Inverse Reinforcement Learning (MO-IRL) to model human arm-reaching movements, achieving more efficient learning and robust generalization compared to traditional methods.", "motivation": "To better understand dynamic, time-varying cost structures in human motor control and enhance predictions of arm-reaching movements with potential implications for humanoid robotics.", "method": "The paper employs MO-IRL with a planar two-link biomechanical model, high-resolution motion-capture data, and segmented phases of trajectories to refine cost weights using maximum entropy IRL.", "result": "Training on limited data yields lower joint-angle RMSE compared to static weight methods, and cross-validation shows predictive accuracy of around 8 deg RMSE, demonstrating robust inter-subject generalization.", "conclusion": "MO-IRL effectively uncovers dynamic cost structures in human motor control, offering efficient learning and potential applications in humanoid robotics."}}
{"id": "2510.00545", "pdf": "https://arxiv.org/pdf/2510.00545", "abs": "https://arxiv.org/abs/2510.00545", "authors": ["Seokhun Park", "Choeun Kim", "Jihu Lee", "Yunseop Shin", "Insung Kong", "Yongdai Kim"], "title": "Bayesian Neural Networks for Functional ANOVA model", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "With the increasing demand for interpretability in machine learning,\nfunctional ANOVA decomposition has gained renewed attention as a principled\ntool for breaking down high-dimensional function into low-dimensional\ncomponents that reveal the contributions of different variable groups.\nRecently, Tensor Product Neural Network (TPNN) has been developed and applied\nas basis functions in the functional ANOVA model, referred to as ANOVA-TPNN. A\ndisadvantage of ANOVA-TPNN, however, is that the components to be estimated\nmust be specified in advance, which makes it difficult to incorporate\nhigher-order TPNNs into the functional ANOVA model due to computational and\nmemory constraints. In this work, we propose Bayesian-TPNN, a Bayesian\ninference procedure for the functional ANOVA model with TPNN basis functions,\nenabling the detection of higher-order components with reduced computational\ncost compared to ANOVA-TPNN. We develop an efficient MCMC algorithm and\ndemonstrate that Bayesian-TPNN performs well by analyzing multiple benchmark\ndatasets. Theoretically, we prove that the posterior of Bayesian-TPNN is\nconsistent.", "AI": {"tldr": "Proposing Bayesian-TPNN to address limitations in ANOVA-TPNN, enabling detection of higher-order components with lower computational cost.", "motivation": "High-dimensional functions need effective decomposition tools to interpret individual contributions, requiring improved methods which address limitations in existing ANOVA approaches.", "method": "Introduced Bayesian-TPNN leveraging Bayesian inference and efficient MCMC algorithms for enhanced functional ANOVA modeling.", "result": "Bayesian-TPNN demonstrated consistent posterior behavior and promising performance across benchmark datasets.", "conclusion": "Bayesian-TPNN improves computational efficiency and addresses limitations of ANOVA-TPNN while enabling higher-order component detection with theoretical consistency."}}
{"id": "2510.00045", "pdf": "https://arxiv.org/pdf/2510.00045", "abs": "https://arxiv.org/abs/2510.00045", "authors": ["Franck Vandewiele", "Remi Synave", "Samuel Delepoulle", "Remi Cozot"], "title": "Beyond the Prompt: Gender Bias in Text-to-Image Models, with a Case Study on Hospital Professions", "categories": ["cs.CV", "cs.AI", "I.2 ARTIFICIAL INTELLIGENCE"], "comment": null, "summary": "Text-to-image (TTI) models are increasingly used in professional,\neducational, and creative contexts, yet their outputs often embed and amplify\nsocial biases. This paper investigates gender representation in six\nstate-of-the-art open-weight models: HunyuanImage 2.1, HiDream-I1-dev,\nQwen-Image, FLUX.1-dev, Stable-Diffusion 3.5 Large, and Stable-Diffusion-XL.\nUsing carefully designed prompts, we generated 100 images for each combination\nof five hospital-related professions (cardiologist, hospital director, nurse,\nparamedic, surgeon) and five portrait qualifiers (\"\", corporate, neutral,\naesthetic, beautiful).\n  Our analysis reveals systematic occupational stereotypes: all models produced\nnurses exclusively as women and surgeons predominantly as men. However,\ndifferences emerge across models: Qwen-Image and SDXL enforce rigid male\ndominance, HiDream-I1-dev shows mixed outcomes, and FLUX.1-dev skews female in\nmost roles. HunyuanImage 2.1 and Stable-Diffusion 3.5 Large also reproduce\ngender stereotypes but with varying degrees of sensitivity to prompt\nformulation. Portrait qualifiers further modulate gender balance, with terms\nlike corporate reinforcing male depictions and beautiful favoring female ones.\nSensitivity varies widely: Qwen-Image remains nearly unaffected, while\nFLUX.1-dev, SDXL, and SD3.5 show strong prompt dependence.\n  These findings demonstrate that gender bias in TTI models is both systematic\nand model-specific. Beyond documenting disparities, we argue that prompt\nwording plays a critical role in shaping demographic outcomes. The results\nunderscore the need for bias-aware design, balanced defaults, and user guidance\nto prevent the reinforcement of occupational stereotypes in generative AI.", "AI": {"tldr": "The paper studies gender bias in six advanced text-to-image models when generating images for hospital professions and finds systematic occupational stereotypes, highlighting the need for bias-aware designs.", "motivation": "Highlighting the issue of social biases embedded in text-to-image models, particularly stereotypes in professional roles.", "method": "Generated 100 images for five hospital professions using customized prompts and examined the impact of portrait qualifiers across six models.", "result": "All models exhibited gender stereotypes in occupations. Qwen-Image and SDXL enforced male dominance, while FLUX.1-dev skewed female. Prompt sensitivity varied significantly across models.", "conclusion": "Systematic gender bias exists across models but is model-specific and tied to prompt influence. Better default designs and user guidance are needed to address stereotypes."}}
{"id": "2510.00088", "pdf": "https://arxiv.org/pdf/2510.00088", "abs": "https://arxiv.org/abs/2510.00088", "authors": ["Sagnik Basu", "Shubham Prakash", "Ashish Maruti Barge", "Siddharth D Jaiswal", "Abhisek Dash", "Saptarshi Ghosh", "Animesh Mukherjee"], "title": "Judging by Appearances? Auditing and Intervening Vision-Language Models for Bail Prediction", "categories": ["cs.AI", "cs.CY"], "comment": null, "summary": "Large language models (LLMs) have been extensively used for legal judgment\nprediction tasks based on case reports and crime history. However, with a surge\nin the availability of large vision language models (VLMs), legal judgment\nprediction systems can now be made to leverage the images of the criminals in\naddition to the textual case reports/crime history. Applications built in this\nway could lead to inadvertent consequences and be used with malicious intent.\nIn this work, we run an audit to investigate the efficiency of standalone VLMs\nin the bail decision prediction task. We observe that the performance is poor\nacross multiple intersectional groups and models \\textit{wrongly deny bail to\ndeserving individuals with very high confidence}. We design different\nintervention algorithms by first including legal precedents through a RAG\npipeline and then fine-tuning the VLMs using innovative schemes. We demonstrate\nthat these interventions substantially improve the performance of bail\nprediction. Our work paves the way for the design of smarter interventions on\nVLMs in the future, before they can be deployed for real-world legal judgment\nprediction.", "AI": {"tldr": "The paper analyzes the use of vision-language models (VLMs) for bail decision prediction and finds poor performance and potential biases. The authors design interventions to improve their functionality.", "motivation": "To explore the efficiency and potential ethical and functional pitfalls of vision-language models in judicial applications, particularly bail decision predictions.", "method": "The authors audit standalone VLMs for bail prediction, assess their biases and performance flaws, and then introduce intervention algorithms, including a retrieval-augmented generation (RAG) pipeline and fine-tuning strategies, to enhance their accuracy.", "result": "The proposed interventions notably boost the VLMs' performance, addressing critical deficiencies in the bail decision task.", "conclusion": "While VLMs currently show poor performance and biases in legal prediction tasks, the study's interventions demonstrate their potential when designed thoughtfully, urging the need for smarter and ethical AI systems before real-world deployment."}}
{"id": "2510.00065", "pdf": "https://arxiv.org/pdf/2510.00065", "abs": "https://arxiv.org/abs/2510.00065", "authors": ["Abdelrhman Gaber", "Hassan Abd-Eltawab", "Youssif Abuzied", "Muhammad ElMahdy", "Tamer ElBatt"], "title": "Federated Learning Meets LLMs: Feature Extraction From Heterogeneous Clients", "categories": ["cs.LG"], "comment": null, "summary": "Federated learning (FL) enables collaborative model training without sharing\nraw data, making it attractive for privacy-sensitive domains such as\nhealthcare, finance, and IoT. A major obstacle, however, is the heterogeneity\nof tabular data across clients, where divergent schemas and incompatible\nfeature spaces prevent straightforward aggregation. To address this challenge,\nwe propose FedLLM-Align, a federated framework that leverages pre-trained large\nlanguage models (LLMs) as universal feature extractors. Tabular records are\nserialized into text, and embeddings from models such as DistilBERT, ALBERT,\nRoBERTa, and ClinicalBERT provide semantically aligned representations that\nsupport lightweight local classifiers under the standard FedAvg protocol. This\napproach removes the need for manual schema harmonization while preserving\nprivacy, since raw data remain strictly local. We evaluate FedLLM-Align on\ncoronary heart disease prediction using partitioned Framingham datasets with\nsimulated schema divergence. Across all client settings and LLM backbones, our\nmethod consistently outperforms state-of-the-art baselines, achieving up to\n+0.25 improvement in F1-score and a 65% reduction in communication cost. Stress\ntesting under extreme schema divergence further demonstrates graceful\ndegradation, unlike traditional methods that collapse entirely. These results\nestablish FedLLM-Align as a robust, privacy-preserving, and\ncommunication-efficient solution for federated learning in heterogeneous\nenvironments.", "AI": {"tldr": "The paper introduces a framework for federated learning using large language models (LLMs) as universal feature extractors to overcome schema divergence among clients.", "motivation": "To tackle the challenge of divergent schemas and incompatible feature spaces in federated learning, especially in privacy-sensitive domains like healthcare and finance.", "method": "FedLLM-Align uses large language models to serialize tabular data into text and extract semantic embeddings, enabling federated model training under the standard FedAvg protocol.", "result": "The framework improved F1-score by up to 0.25 and reduced communication cost by 65% while performing robustly under extreme schema divergence.", "conclusion": "FedLLM-Align is an effective solution for privacy-preserving and communication-efficient federated learning in heterogeneous environments, outperforming traditional methods."}}
{"id": "2510.00232", "pdf": "https://arxiv.org/pdf/2510.00232", "abs": "https://arxiv.org/abs/2510.00232", "authors": ["Xin Xu", "Xunzhi He", "Churan Zhi", "Ruizhe Chen", "Julian McAuley", "Zexue He"], "title": "BiasFreeBench: a Benchmark for Mitigating Bias in Large Language Model Responses", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "comment": "Work in progress", "summary": "Existing studies on bias mitigation methods for large language models (LLMs)\nuse diverse baselines and metrics to evaluate debiasing performance, leading to\ninconsistent comparisons among them. Moreover, their evaluations are mostly\nbased on the comparison between LLMs' probabilities of biased and unbiased\ncontexts, which ignores the gap between such evaluations and real-world use\ncases where users interact with LLMs by reading model responses and expect fair\nand safe outputs rather than LLMs' probabilities. To enable consistent\nevaluation across debiasing methods and bridge this gap, we introduce\nBiasFreeBench, an empirical benchmark that comprehensively compares eight\nmainstream bias mitigation techniques (covering four prompting-based and four\ntraining-based methods) on two test scenarios (multi-choice QA and open-ended\nmulti-turn QA) by reorganizing existing datasets into a unified query-response\nsetting. We further introduce a response-level metric, Bias-Free Score, to\nmeasure the extent to which LLM responses are fair, safe, and\nanti-stereotypical. Debiasing performances are systematically compared and\nanalyzed across key dimensions: the prompting vs. training paradigm, model\nsize, and generalization of different training strategies to unseen bias types.\nWe will publicly release our benchmark, aiming to establish a unified testbed\nfor bias mitigation research.", "AI": {"tldr": "BiasFreeBench proposes a unified benchmark for evaluating bias mitigation methods in LLMs using a new response-level metric and systematic comparison of prompting-based and training-based methods.", "motivation": "Current bias mitigation evaluations suffer from inconsistent baselines, metrics, and a disconnect with real-world interactions with LLMs.", "method": "The paper reorganized datasets into a unified query-response format and introduced the Bias-Free Score to assess fairness, safety, and anti-stereotypical properties of LLM outputs.", "result": "Eight bias mitigation techniques were compared across multiple dimensions, revealing insights and generalization limitations for unseen bias types.", "conclusion": "BiasFreeBench offers a unified testbed that addresses inconsistencies in the field and provides standardized metrics for evaluating bias mitigation approaches."}}
{"id": "2510.01000", "pdf": "https://arxiv.org/pdf/2510.01000", "abs": "https://arxiv.org/abs/2510.01000", "authors": ["Charles Q. Wu"], "title": "Some Further Developments on a Neurobiologically-based Model for Color Sensations in Humans", "categories": ["q-bio.NC"], "comment": null, "summary": "At HVEI-2012, I presented a neurobiologically-based model for trichromatic\ncolor sensations in humans, mapping the neural substrate for color sensations\nto V1-L4: the thalamic recipient layer of the primary visual cortex. In this\npaper, I propose that V1-L4 itself consists of three distinct sub-layers that\ndirectly correspond to the three primary color sensations: blue, red, and\ngreen. Furthermore, I apply this model to three aspects of color vision: the\nthree-dimensional (3D) color solid, dichromatism, and ocular agnosticism.\nRegarding these aspects further: (1) 3D color solid: V1-L4 is known to exhibit\na gradient of cell densities from its outermost layer (i.e., its pia side) to\nits innermost layer (i.e., its white matter side). Taken together with the\nproposition that the population size of a cell assembly directly corresponds\nwith the magnitude of a color sensation, it can be inferred that the\nneurobiologically-based color solid is a tilted cuboid. (2) Chromatic color\nblindness: Using deuteranopia as an example, at the retinal level, M-cones are\nlost and replaced by L-cones. However, at the cortical level, deuteranopia\nmanifests as a fusion of the two bottom layers of V1-L4. (3) Ocular\nagnosticism: Although color sensation is monocular, we normally are not aware\nof which eye we are seeing with. This visual phenomenon can be explained by the\nnature of ocular integration within V1-L4. A neurobiologically-based model for\nhuman color sensations could significantly contribute to future engineering\nefforts aimed at enhancing human color experiences.", "AI": {"tldr": "This paper proposes a neurobiological model mapping color sensations in humans to V1-L4 of the primary visual cortex, associating sub-layers with primary colors and applying it to aspects of color vision like the color solid, dichromatism, and ocular agnosticism.", "motivation": "The author aims to establish a neurobiological basis for human color sensations and explore how V1-L4 sub-layers correspond to primary color sensations while addressing three key aspects of color vision.", "method": "The paper introduces a model linking the primary visual cortex's sub-layers to primary colors and explores its implications on aspects such as the 3D color solid, chromatic color blindness (deuteranopia), and ocular agnosticism.", "result": "The model suggests a tilted cuboid shape for a neurobiological color solid, explains fusion in V1-L4 layers in chromatic blindness, and offers insights into ocular integration mechanisms affecting color sensation.", "conclusion": "The proposed neurobiological model promises advancements in enhancing human color experiences in future engineering applications."}}
{"id": "2510.00828", "pdf": "https://arxiv.org/pdf/2510.00828", "abs": "https://arxiv.org/abs/2510.00828", "authors": ["Kuan-Chieh Hsu", "Sairam Sri Vatsavai", "Ozgur O. Kilic", "Tatiana Korchuganova", "Paul Nilsson", "Sankha Dutta", "Yihui Ren", "David K. Park", "Joseph Boudreau", "Tasnuva Chowdhury", "Shengyu Feng", "Raees Khan", "Jaehyung Kim", "Scott Klasky", "Tadashi Maeno", "Verena Ingrid Martinez Outschoorn", "Norbert Podhorszki", "Fr\u00e9d\u00e9ric Suter", "Wei Yang", "Yiming Yang", "Shinjae Yoo", "Alexei Klimentov", "Adolfy Hoisie"], "title": "Data Management System Analysis for Distributed Computing Workloads", "categories": ["cs.DC"], "comment": "10 pages, 12 figures, to be presented in SC25 DRBSD Workshop", "summary": "Large-scale international collaborations such as ATLAS rely on globally\ndistributed workflows and data management to process, move, and store vast\nvolumes of data. ATLAS's Production and Distributed Analysis (PanDA) workflow\nsystem and the Rucio data management system are each highly optimized for their\nrespective design goals. However, operating them together at global scale\nexposes systemic inefficiencies, including underutilized resources, redundant\nor unnecessary transfers, and altered error distributions. Moreover, PanDA and\nRucio currently lack shared performance awareness and coordinated, adaptive\nstrategies.\n  This work charts a path toward co-optimizing the two systems by diagnosing\ndata-management pitfalls and prioritizing end-to-end improvements. With the\nobservation of spatially and temporally imbalanced transfer activities, we\ndevelop a metadata-matching algorithm that links PanDA jobs and Rucio datasets\nat the file level, yielding a complete, fine-grained view of data access and\nmovement. Using this linkage, we identify anomalous transfer patterns that\nviolate PanDA's data-centric job-allocation principle. We then outline\nmitigation strategies for these patterns and highlight opportunities for\ntighter PanDA-Rucio coordination to improve resource utilization, reduce\nunnecessary data movement, and enhance overall system resilience.", "AI": {"tldr": "The paper addresses inefficiencies in ATLAS's PanDA and Rucio systems and proposes enhancements for better coordination and resource utilization.", "motivation": "The paper aims to tackle inefficiencies arising from the lack of shared performance awareness and coordinated strategies in the PanDA and Rucio systems.", "method": "A metadata-matching algorithm was developed to link PanDA jobs and Rucio datasets, providing an interconnected view for analyzing and mitigating inefficiencies.", "result": "Anomalous transfer patterns violating data-centric principles were identified, leading to strategies for improved coordination and resource efficiency.", "conclusion": "Enhanced collaboration between PanDA and Rucio can optimize resource use, decrease data movement, and strengthen system resilience."}}
{"id": "2510.01012", "pdf": "https://arxiv.org/pdf/2510.01012", "abs": "https://arxiv.org/abs/2510.01012", "authors": ["Maximilian Gollwitzer", "Felix Dietrich"], "title": "Random Feature Spiking Neural Networks", "categories": ["cs.LG", "cs.NE", "68T07", "G.1; G.3"], "comment": "34 pages incl. references & appendix, 3 figures, 4 tables", "summary": "Spiking Neural Networks (SNNs) as Machine Learning (ML) models have recently\nreceived a lot of attention as a potentially more energy-efficient alternative\nto conventional Artificial Neural Networks. The non-differentiability and\nsparsity of the spiking mechanism can make these models very difficult to train\nwith algorithms based on propagating gradients through the spiking\nnon-linearity. We address this problem by adapting the paradigm of Random\nFeature Methods (RFMs) from Artificial Neural Networks (ANNs) to Spike Response\nModel (SRM) SNNs. This approach allows training of SNNs without approximation\nof the spike function gradient. Concretely, we propose a novel data-driven,\nfast, high-performance, and interpretable algorithm for end-to-end training of\nSNNs inspired by the SWIM algorithm for RFM-ANNs, which we coin S-SWIM. We\nprovide a thorough theoretical discussion and supplementary numerical\nexperiments showing that S-SWIM can reach high accuracies on time series\nforecasting as a standalone strategy and serve as an effective initialisation\nstrategy before gradient-based training. Additional ablation studies show that\nour proposed method performs better than random sampling of network weights.", "AI": {"tldr": "This paper introduces S-SWIM, a novel training algorithm for Spiking Neural Networks (SNNs), inspired by Random Feature Methods. The method avoids approximating spike function gradients, ensuring efficiency and effectiveness.", "motivation": "Train SNNs effectively to harness their potential for energy-efficient machine learning models despite challenges like non-differentiability and sparsity.", "method": "Adapts Random Feature Methods for Spike Response Model SNNs and proposes the S-SWIM algorithm, which avoids gradient approximation in training.", "result": "S-SWIM demonstrates high accuracies in time-series forecasting and serves as a strong initialization strategy for further gradient-based training, outperforming random weight sampling.", "conclusion": "S-SWIM is a fast, interpretable, and high-performance approach for training SNNs, offering practical benefits over conventional methods."}}
{"id": "2510.00324", "pdf": "https://arxiv.org/pdf/2510.00324", "abs": "https://arxiv.org/abs/2510.00324", "authors": ["Lucas Roberts", "Denisa Roberts"], "title": "Which Programming Language and Model Work Best With LLM-as-a-Judge For Code Retrieval?", "categories": ["cs.SE", "cs.IR", "cs.LG"], "comment": "Accepted as a full paper at SIGIR-AP 2025", "summary": "Code search is an important information retrieval application. Benefits of\nbetter code search include faster new developer on-boarding, reduced software\nmaintenance, and ease of understanding for large repositories. Despite\nimprovements in search algorithms and search benchmarks, the domain of code\nsearch has lagged behind. One reason is the high cost of human annotation for\ncode queries and answers. While humans may annotate search results in general\ntext QA systems, code annotations require specialized knowledge of a\nprogramming language (PL), as well as domain specific software engineering\nknowledge. In this work we study the use of Large Language Models (LLMs) to\nretrieve code at the level of functions and to generate annotations for code\nsearch results. We compare the impact of the retriever representation (sparse\nvs. semantic), programming language, and LLM by comparing human annotations\nacross several popular languages (C, Java, Javascript, Go, and Python). We\nfocus on repositories that implement common data structures likely to be\nimplemented in any PLs. For the same human annotations, we compare several\nLLM-as-a-Judge models to evaluate programming language and other affinities\nbetween LLMs. We find that the chosen retriever and PL exhibit affinities that\ncan be leveraged to improve alignment of human and AI relevance determinations,\nwith significant performance implications. We also find differences in\nrepresentation (sparse vs. semantic) across PLs that impact alignment of human\nand AI relevance determinations. We propose using transpilers to bootstrap\nscalable code search benchmark datasets in other PLs and in a case study\ndemonstrate that human-AI relevance agreement rates largely match the (worst\ncase) human-human agreement under study. The application code used in this work\nis available at \\href{https://github.com/rlucas7/code-searcher/}{this github\nrepo}.", "AI": {"tldr": "The study explores the use of Large Language Models (LLMs) for code search and annotations, emphasizing the role of retriever representation, programming languages (PL), and human-AI relevance alignment.", "motivation": "Code search faces challenges due to the high cost of human annotations for programming queries, requiring specialized programming knowledge and domain-specific expertise.", "method": "The paper studies LLMs to retrieve code functions, generate annotations, and evaluates retrieval methods (sparse vs semantic), programming languages, and human-LLM alignment using human annotations.", "result": "Significant differences were found in representation methods (sparse vs semantic) and programming languages affecting human-AI relevance alignment. Transpilers were proposed to scale benchmarks across programming languages.", "conclusion": "The study demonstrated that LLMs, retriever representation methods, and programming language-specific adjustments can enhance code search performance, achieving human-AI relevance agreement comparable to human-human agreement."}}
{"id": "2510.00358", "pdf": "https://arxiv.org/pdf/2510.00358", "abs": "https://arxiv.org/abs/2510.00358", "authors": ["Linjin He", "Xinda Qi", "Dong Chen", "Zhaojian Li", "Xiaobo Tan"], "title": "DiSA-IQL: Offline Reinforcement Learning for Robust Soft Robot Control under Distribution Shifts", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Soft snake robots offer remarkable flexibility and adaptability in complex\nenvironments, yet their control remains challenging due to highly nonlinear\ndynamics. Existing model-based and bio-inspired controllers rely on simplified\nassumptions that limit performance. Deep reinforcement learning (DRL) has\nrecently emerged as a promising alternative, but online training is often\nimpractical because of costly and potentially damaging real-world interactions.\nOffline RL provides a safer option by leveraging pre-collected datasets, but it\nsuffers from distribution shift, which degrades generalization to unseen\nscenarios. To overcome this challenge, we propose DiSA-IQL\n(Distribution-Shift-Aware Implicit Q-Learning), an extension of IQL that\nincorporates robustness modulation by penalizing unreliable state-action pairs\nto mitigate distribution shift. We evaluate DiSA-IQL on goal-reaching tasks\nacross two settings: in-distribution and out-of-distribution evaluation.\nSimulation results show that DiSA-IQL consistently outperforms baseline models,\nincluding Behavior Cloning (BC), Conservative Q-Learning (CQL), and vanilla\nIQL, achieving higher success rates, smoother trajectories, and improved\nrobustness. The codes are open-sourced to support reproducibility and to\nfacilitate further research in offline RL for soft robot control.", "AI": {"tldr": "The paper introduces DiSA-IQL, a novel RL approach for soft snake robots, addressing distribution shift to enhance control performance.", "motivation": "Control of soft snake robots is challenging due to nonlinear dynamics and distribution shifts in offline RL training methods.", "method": "The authors propose DiSA-IQL, an enhancement to IQL, with robustness modulation through penalization of unreliable state-action pairs.", "result": "DiSA-IQL outperforms other methods in goal-reaching tasks, showcasing higher success rates, smoother trajectories, and better robustness.", "conclusion": "DiSA-IQL offers a promising solution for offline RL in soft robot control, demonstrating superior performance and practicality for unseen scenarios."}}
{"id": "2510.00569", "pdf": "https://arxiv.org/pdf/2510.00569", "abs": "https://arxiv.org/abs/2510.00569", "authors": ["Ke Xu", "Yuefeng Han"], "title": "Guaranteed Noisy CP Tensor Recovery via Riemannian Optimization on the Segre Manifold", "categories": ["stat.ML", "cs.LG", "math.OC", "math.ST", "stat.ME", "stat.TH", "90C26 (Primary) 15A69, 62F10, 62J05, 62H25 (Secondary)"], "comment": "33 pages, 7 figures", "summary": "Recovering a low-CP-rank tensor from noisy linear measurements is a central\nchallenge in high-dimensional data analysis, with applications spanning tensor\nPCA, tensor regression, and beyond. We exploit the intrinsic geometry of\nrank-one tensors by casting the recovery task as an optimization problem over\nthe Segre manifold, the smooth Riemannian manifold of rank-one tensors. This\ngeometric viewpoint yields two powerful algorithms: Riemannian Gradient Descent\n(RGD) and Riemannian Gauss-Newton (RGN), each of which preserves feasibility at\nevery iteration. Under mild noise assumptions, we prove that RGD converges at a\nlocal linear rate, while RGN exhibits an initial local quadratic convergence\nphase that transitions to a linear rate as the iterates approach the\nstatistical noise floor. Extensive synthetic experiments validate these\nconvergence guarantees and demonstrate the practical effectiveness of our\nmethods.", "AI": {"tldr": "The paper presents two algorithms for recovering low-CP-rank tensors from noisy measurements using optimization on the Segre manifold.", "motivation": "To address the challenge of recovering low-CP-rank tensors from noisy linear measurements, which is crucial in applications like tensor PCA and regression.", "method": "The study leverages the geometry of rank-one tensors and proposes Riemannian Gradient Descent (RGD) and Riemannian Gauss-Newton (RGN) methods, ensuring feasibility throughout iterations.", "result": "RGD converges at a local linear rate under mild noise assumptions, while RGN shows initial local quadratic convergence that later transitions to a linear rate. Synthetic experiments confirm these outcomes and practical usage.", "conclusion": "The proposed methods are theoretically and practically validated as effective for tensor recovery under noise conditions, offering robust convergence properties."}}
{"id": "2510.00046", "pdf": "https://arxiv.org/pdf/2510.00046", "abs": "https://arxiv.org/abs/2510.00046", "authors": ["Xiaotian Zou"], "title": "Reinforcement Learning-Based Prompt Template Stealing for Text-to-Image Models", "categories": ["cs.CV", "cs.AI"], "comment": "10 pages, 3 figures", "summary": "Multimodal Large Language Models (MLLMs) have transformed text-to-image\nworkflows, allowing designers to create novel visual concepts with\nunprecedented speed. This progress has given rise to a thriving prompt trading\nmarket, where curated prompts that induce trademark styles are bought and sold.\nAlthough commercially attractive, prompt trading also introduces a largely\nunexamined security risk: the prompts themselves can be stolen.\n  In this paper, we expose this vulnerability and present RLStealer, a\nreinforcement learning based prompt inversion framework that recovers its\ntemplate from only a small set of example images. RLStealer treats template\nstealing as a sequential decision making problem and employs multiple\nsimilarity based feedback signals as reward functions to effectively explore\nthe prompt space. Comprehensive experiments on publicly available benchmarks\ndemonstrate that RLStealer gets state-of-the-art performance while reducing the\ntotal attack cost to under 13% of that required by existing baselines. Our\nfurther analysis confirms that RLStealer can effectively generalize across\ndifferent image styles to efficiently steal unseen prompt templates. Our study\nhighlights an urgent security threat inherent in prompt trading and lays the\ngroundwork for developing protective standards in the emerging MLLMs\nmarketplace.", "AI": {"tldr": "The paper introduces RLStealer, a framework capable of stealing prompts used in multimodal large language models from example images.", "motivation": "Addressing the overlooked security risk in prompt trading, where proprietary prompts can be stolen in multimodal large language models (MLLMs).", "method": "The framework uses reinforcement learning to treat prompt stealing as a sequential decision-making problem, employing similarity-based reward signals for efficient template recovery.", "result": "RLStealer outperforms existing baselines in prompt inversion, reducing the attack cost to below 13% while generalizing across various image styles.", "conclusion": "The study highlights the security risks in prompt trading markets for MLLMs and calls for protective standards to mitigate prompt theft vulnerabilities."}}
{"id": "2510.00156", "pdf": "https://arxiv.org/pdf/2510.00156", "abs": "https://arxiv.org/abs/2510.00156", "authors": ["Songran Bai", "Bingzhe Wu", "Yiwei Zhang", "Chengke Wu", "Xiaolong Zheng", "Yaze Yuan", "Ke Wu", "Jianqiang Li"], "title": "AuditAgent: Expert-Guided Multi-Agent Reasoning for Cross-Document Fraudulent Evidence Discovery", "categories": ["cs.AI"], "comment": null, "summary": "Financial fraud detection in real-world scenarios presents significant\nchallenges due to the subtlety and dispersion of evidence across complex,\nmulti-year financial disclosures. In this work, we introduce a novel\nmulti-agent reasoning framework AuditAgent, enhanced with auditing domain\nexpertise, for fine-grained evidence chain localization in financial fraud\ncases. Leveraging an expert-annotated dataset constructed from enforcement\ndocuments and financial reports released by the China Securities Regulatory\nCommission, our approach integrates subject-level risk priors, a hybrid\nretrieval strategy, and specialized agent modules to efficiently identify and\naggregate cross-report evidence. Extensive experiments demonstrate that our\nmethod substantially outperforms General-Purpose Agent paradigm in both recall\nand interpretability, establishing a new benchmark for automated, transparent\nfinancial forensics. Our results highlight the value of domain-specific\nreasoning and dataset construction for advancing robust financial fraud\ndetection in practical, real-world regulatory applications.", "AI": {"tldr": "The paper introduces AuditAgent, a multi-agent framework for detecting financial fraud by localizing evidence in multi-year disclosures, which proves effective and transparent in practice.", "motivation": "To address the challenge of detecting subtle and dispersed evidence of financial fraud across complex disclosures.", "method": "The framework leverages an expert-annotated dataset, subject-level risk priors, a hybrid retrieval strategy, and specialized agent models.", "result": "AuditAgent shows superior performance in recall and interpretability compared to General-Purpose Agents, setting a new benchmark in automated financial forensics.", "conclusion": "Domain-specific reasoning and well-constructed datasets play a critical role in advancing transparent and effective financial fraud detection tools."}}
{"id": "2510.00078", "pdf": "https://arxiv.org/pdf/2510.00078", "abs": "https://arxiv.org/abs/2510.00078", "authors": ["Sicong Liu", "Weiye Wu", "Xiangrui Xu", "Teng Li", "Bowen Pang", "Bin Guo", "Zhiwen Yu"], "title": "Adaptive and Resource-efficient Agentic AI Systems for Mobile and Embedded Devices: A Survey", "categories": ["cs.LG", "cs.AI", "cs.DC"], "comment": null, "summary": "Foundation models have reshaped AI by unifying fragmented architectures into\nscalable backbones with multimodal reasoning and contextual adaptation. In\nparallel, the long-standing notion of AI agents, defined by the\nsensing-decision-action loop, is entering a new paradigm: with FMs as their\ncognitive core, agents transcend rule-based behaviors to achieve autonomy,\ngeneralization, and self-reflection. This dual shift is reinforced by\nreal-world demands such as autonomous driving, robotics, virtual assistants,\nand GUI agents, as well as ecosystem advances in embedded hardware, edge\ncomputing, mobile deployment platforms, and communication protocols that\ntogether enable large-scale deployment. Yet this convergence collides with\nreality: while applications demand long-term adaptability and real-time\ninteraction, mobile and edge deployments remain constrained by memory, energy,\nbandwidth, and latency. This creates a fundamental tension between the growing\ncomplexity of FMs and the limited resources of deployment environments. This\nsurvey provides the first systematic characterization of adaptive,\nresource-efficient agentic AI systems. We summarize enabling techniques into\nelastic inference, test-time adaptation, dynamic multimodal integration, and\nagentic AI applications, and identify open challenges in balancing\naccuracy-latency-communication trade-offs and sustaining robustness under\ndistribution shifts. We further highlight future opportunities in\nalgorithm-system co-design, cognitive adaptation, and collaborative edge\ndeployment. By mapping FM structures, cognition, and hardware resources, this\nwork establishes a unified perspective toward scalable, adaptive, and\nresource-efficient agentic AI. We believe this survey can help readers to\nunderstand the connections between enabling technologies while promoting\nfurther discussions on the fusion of agentic intelligence and intelligent\nagents.", "AI": {"tldr": "The paper surveys resource-efficient AI systems that integrate foundation models (FMs) for adaptive and autonomous agent performance under real-world constraints.", "motivation": "The paper is motivated by the dual shift where foundation models unify fragmented AI architectures while the concept of AI agents evolves into systems capable of autonomy and self-reflection.", "method": "The paper systematically characterizes enabling techniques for resource-efficient AI systems, including elastic inference, dynamic multimodal integration, and test-time adaptation, identifying key methods and challenges.", "result": "This survey maps the connections between cognitive capabilities, FM structures, and hardware limitations, and suggests opportunities for algorithm-system co-design and collaborative edge deployments.", "conclusion": "It establishes a unified perspective toward scalable, adaptive, and resource-efficient AI agents, encouraging collaboration and innovation in agentic intelligence development."}}
{"id": "2510.00255", "pdf": "https://arxiv.org/pdf/2510.00255", "abs": "https://arxiv.org/abs/2510.00255", "authors": ["Monishwaran Maheswaran", "Marco Carini", "Christian Federmann", "Tony Diaz"], "title": "TASER: Translation Assessment via Systematic Evaluation and Reasoning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We introduce TASER (Translation Assessment via Systematic Evaluation and\nReasoning), a metric that uses Large Reasoning Models (LRMs) for automated\ntranslation quality assessment. TASER harnesses the explicit reasoning\ncapabilities of LRMs to conduct systematic, step-by-step evaluation of\ntranslation quality. We evaluate TASER on the WMT24 Metrics Shared Task across\nboth reference-based and reference-free scenarios, demonstrating\nstate-of-the-art performance. In system-level evaluation, TASER achieves the\nhighest soft pairwise accuracy in both reference-based and reference-free\nsettings, outperforming all existing metrics. At the segment level, TASER\nmaintains competitive performance with our reference-free variant ranking as\nthe top-performing metric among all reference-free approaches. Our experiments\nreveal that structured prompting templates yield superior results with LRMs\ncompared to the open-ended approaches that proved optimal for traditional LLMs.\nWe evaluate o3, a large reasoning model from OpenAI, with varying reasoning\nefforts, providing insights into the relationship between reasoning depth and\nevaluation quality. The explicit reasoning process in LRMs offers\ninterpretability and visibility, addressing a key limitation of existing\nautomated metrics. Our results demonstrate that Large Reasoning Models show a\nmeasurable advancement in translation quality assessment, combining improved\naccuracy with transparent evaluation across diverse language pairs.", "AI": {"tldr": "TASER introduces a novel translation quality assessment metric that utilizes Large Reasoning Models (LRMs) for systematic evaluation, achieving state-of-the-art performance in automated translation assessment.", "motivation": "The study seeks to address the limitations of traditional metrics for translation quality assessment by leveraging transparent reasoning capabilities of LRMs.", "method": "TASER employs structured prompting templates to guide LRMs in systematic reasoning for both reference-based and reference-free evaluation scenarios.", "result": "TASER demonstrates top accuracy in system-level evaluations across reference-based and reference-free setups, with a reference-free version achieving superior segment-level performance among similar metrics.", "conclusion": "Large Reasoning Models provide measurable improvements in translation assessment accuracy while offering greater interpretability, making them effective for diverse language evaluations."}}
{"id": "2510.00032", "pdf": "https://arxiv.org/pdf/2510.00032", "abs": "https://arxiv.org/abs/2510.00032", "authors": ["Ziyi Zeng", "Zhenyang Cai", "Yixi Cai", "Xidong Wang", "Junying Chen", "Rongsheng Wang", "Yipeng Liu", "Siqi Cai", "Benyou Wang", "Zhiguo Zhang", "Haizhou Li"], "title": "WaveMind: Towards a Conversational EEG Foundation Model Aligned to Textual and Visual Modalities", "categories": ["eess.SP", "cs.AI", "cs.CL", "cs.LG", "q-bio.NC"], "comment": null, "summary": "Electroencephalography (EEG) interpretation using multimodal large language\nmodels (MLLMs) offers a novel approach for analyzing brain signals. However,\nthe complex nature of brain activity introduces critical challenges: EEG\nsignals simultaneously encode both cognitive processes and intrinsic neural\nstates, creating a mismatch in EEG paired-data modality that hinders effective\ncross-modal representation learning. Through a pivot investigation, we uncover\ncomplementary relationships between these modalities. Leveraging this insight,\nwe propose mapping EEG signals and their corresponding modalities into a\nunified semantic space to achieve generalized interpretation. To fully enable\nconversational capabilities, we further introduce WaveMind-Instruct-338k, the\nfirst cross-task EEG dataset for instruction tuning. The resulting model\ndemonstrates robust classification accuracy while supporting flexible,\nopen-ended conversations across four downstream tasks, thereby offering\nvaluable insights for both neuroscience research and the development of\ngeneral-purpose EEG models.", "AI": {"tldr": "This paper explores a novel approach using multimodal large language models (MLLMs) for EEG interpretation and suggests mapping EEG signals into a unified semantic space for better analysis.", "motivation": "EEG signals are complex as they encode both cognitive processes and intrinsic neural states, leading to mismatches in paired-data modalities. The study aims to address this challenge in cross-modal representation learning.", "method": "A pivot investigation is conducted to identify complementary relationships between EEG modalities. The study proposes mapping EEG data into a unified semantic space and introduces WaveMind-Instruct-338k, a cross-task EEG dataset for instruction tuning.", "result": "The proposed model achieves robust classification accuracy and enables open-ended conversations across four downstream tasks, demonstrating flexibility and practical utility.", "conclusion": "This work offers valuable insights for neuroscience and supports the development of general-purpose EEG models with conversational capabilities."}}
{"id": "2510.00833", "pdf": "https://arxiv.org/pdf/2510.00833", "abs": "https://arxiv.org/abs/2510.00833", "authors": ["Thanh Linh Nguyen", "Marcela Tuler de Oliveira", "An Braeken", "Aaron Yi Ding", "Quoc-Viet Pham"], "title": "Towards Verifiable Federated Unlearning: Framework, Challenges, and The Road Ahead", "categories": ["cs.DC", "cs.AI"], "comment": "Journal submission", "summary": "Federated unlearning (FUL) enables removing the data influence from the model\ntrained across distributed clients, upholding the right to be forgotten as\nmandated by privacy regulations. FUL facilitates a value exchange where clients\ngain privacy-preserving control over their data contributions, while service\nproviders leverage decentralized computing and data freshness. However, this\nentire proposition is undermined because clients have no reliable way to verify\nthat their data influence has been provably removed, as current metrics and\nsimple notifications offer insufficient assurance. We envision unlearning\nverification becoming a pivotal and trust-by-design part of the FUL life-cycle\ndevelopment, essential for highly regulated and data-sensitive services and\napplications like healthcare. This article introduces veriFUL, a reference\nframework for verifiable FUL that formalizes verification entities, goals,\napproaches, and metrics. Specifically, we consolidate existing efforts and\ncontribute new insights, concepts, and metrics to this domain. Finally, we\nhighlight research challenges and identify potential applications and\ndevelopments for verifiable FUL and veriFUL.", "AI": {"tldr": "The paper proposes veriFUL, a framework aiming to ensure that federated unlearning (FUL) processes are reliable and provable, addressing the lack of verification mechanisms for data removal in existing FUL practices.", "motivation": "Current federated unlearning methods lack a reliable way for clients to verify whether their data's influence has been completely removed. This undermines trust and compliance with privacy regulations.", "method": "The authors introduce veriFUL, a reference framework for verifiable federated unlearning that formalizes key elements such as verification entities, goals, and metrics. They also consolidate existing research and contribute new concepts.", "result": "The veriFUL framework defines methods and metrics for verifiable unlearning, offering new insights into ensuring trustworthiness in FUL. It explores the potential of its application in highly regulated sectors such as healthcare.", "conclusion": "Verifiable federated unlearning is crucial for maintaining trust and meeting privacy requirements. The proposed veriFUL framework is a step forward in enhancing the reliability and acceptance of FUL processes."}}
{"id": "2510.01154", "pdf": "https://arxiv.org/pdf/2510.01154", "abs": "https://arxiv.org/abs/2510.01154", "authors": ["Oleksandr Kyriienko", "Chukwudubem Umeano", "Zo\u00eb Holmes"], "title": "Advantage for Discrete Variational Quantum Algorithms in Circuit Recompilation", "categories": ["quant-ph", "cs.NE"], "comment": "9+5 pages, 11 figures", "summary": "The relative power of quantum algorithms, using an adaptive access to quantum\ndevices, versus classical post-processing methods that rely only on an initial\nquantum data set, remains the subject of active debate. Here, we present\nevidence for an exponential separation between adaptive and non-adaptive\nstrategies in a quantum circuit recompilation task. Our construction features\ncompilation problems with loss landscapes for discrete optimization that are\nunimodal yet non-separable, a structure known in classical optimization to\nconfer exponential advantages to adaptive search. Numerical experiments show\nthat optimization can efficiently uncover hidden circuit structure operating in\nthe regime of volume-law entanglement and high-magic, while non-adaptive\napproaches are seemingly limited to exhaustive search requiring exponential\nresources. These results indicate that adaptive access to quantum hardware\nprovides a fundamental advantage.", "AI": {"tldr": "This paper demonstrates an exponential separation between adaptive and non-adaptive quantum strategies in circuit recompilation tasks.", "motivation": "To determine the relative advantage of adaptive versus non-adaptive quantum strategies in practical applications.", "method": "The authors construct compilation problems with specific optimization properties and perform numerical experiments to evaluate adaptive and non-adaptive approaches.", "result": "Adaptive methods efficiently uncover hidden circuit structures, while non-adaptive methods require exponential resources.", "conclusion": "Adaptive access to quantum hardware offers a fundamental advantage, especially for certain optimization tasks."}}
{"id": "2510.00328", "pdf": "https://arxiv.org/pdf/2510.00328", "abs": "https://arxiv.org/abs/2510.00328", "authors": ["Ahmed Fawzy", "Amjed Tahir", "Kelly Blincoe"], "title": "Vibe Coding in Practice: Motivations, Challenges, and a Future Outlook -- a Grey Literature Review", "categories": ["cs.SE"], "comment": null, "summary": "AI code generation tools are transforming software development, especially\nfor novice and non-software developers, by enabling them to write code and\nbuild applications faster and with little to no human intervention. Vibe coding\nis the practice where users rely on AI code generation tools through intuition\nand trial-and-error without necessarily understanding the underlying code.\nDespite widespread adoption, no research has systematically investigated why\nusers engage in vibe coding, what they experience while doing so, and how they\napproach quality assurance (QA) and perceive the quality of the AI-generated\ncode. To this end, we conduct a systematic grey literature review of 101\npractitioner sources, extracting 518 firsthand behavioral accounts about vibe\ncoding practices, challenges, and limitations. Our analysis reveals a\nspeed-quality trade-off paradox, where vibe coders are motivated by speed and\naccessibility, often experiencing rapid ``instant success and flow'', yet most\nperceive the resulting code as fast but flawed. QA practices are frequently\noverlooked, with many skipping testing, relying on the models' or tools'\noutputs without modification, or delegating checks back to the AI code\ngeneration tools. This creates a new class of vulnerable software developers,\nparticularly those who build a product but are unable to debug it when issues\narise. We argue that vibe coding lowers barriers and accelerates prototyping,\nbut at the cost of reliability and maintainability. These insights carry\nimplications for tool designers and software development teams. Understanding\nhow vibe coding is practiced today is crucial for guiding its responsible use\nand preventing a broader QA crisis in AI-assisted development.", "AI": {"tldr": "\"Vibe coding\" is a new phenomenon in AI-assisted software development, favoring speed and accessibility over understanding and quality assurance. However, its reliance on AI code generation tools creates risks of flawed and unreliable software.", "motivation": "To understand why developers adopt vibe coding, the challenges they face, and how they approach quality assurance in AI-assisted development environments.", "method": "A systematic grey literature review analyzing 101 practitioner sources to gather 518 firsthand behavioral accounts of vibe coding practices, challenges, and limitations.", "result": "Developers using vibe coding report a speed-quality trade-off where they value rapid results but often find the output flawed. Quality assurance practices such as testing are frequently neglected, creating risks for software reliability and maintainability.", "conclusion": "Vibe coding eases barriers and accelerates prototyping but raises concerns about software quality and debugging. Understanding and addressing its risks are necessary to responsibly scale its adoption in software development."}}
{"id": "2510.00401", "pdf": "https://arxiv.org/pdf/2510.00401", "abs": "https://arxiv.org/abs/2510.00401", "authors": ["Shounak Sural", "Charles Kekeh", "Wenliang Liu", "Federico Pecora", "Mouhacine Benosman"], "title": "Physics-Informed Neural Controlled Differential Equations for Scalable Long Horizon Multi-Agent Motion Forecasting", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.MA"], "comment": null, "summary": "Long-horizon motion forecasting for multiple autonomous robots is challenging\ndue to non-linear agent interactions, compounding prediction errors, and\ncontinuous-time evolution of dynamics. Learned dynamics of such a system can be\nuseful in various applications such as travel time prediction,\nprediction-guided planning and generative simulation. In this work, we aim to\ndevelop an efficient trajectory forecasting model conditioned on multi-agent\ngoals. Motivated by the recent success of physics-guided deep learning for\npartially known dynamical systems, we develop a model based on neural\nControlled Differential Equations (CDEs) for long-horizon motion forecasting.\nUnlike discrete-time methods such as RNNs and transformers, neural CDEs operate\nin continuous time, allowing us to combine physics-informed constraints and\nbiases to jointly model multi-robot dynamics. Our approach, named PINCoDE\n(Physics-Informed Neural Controlled Differential Equations), learns\ndifferential equation parameters that can be used to predict the trajectories\nof a multi-agent system starting from an initial condition. PINCoDE is\nconditioned on future goals and enforces physics constraints for robot motion\nover extended periods of time. We adopt a strategy that scales our model from\n10 robots to 100 robots without the need for additional model parameters, while\nproducing predictions with an average ADE below 0.5 m for a 1-minute horizon.\nFurthermore, progressive training with curriculum learning for our PINCoDE\nmodel results in a 2.7X reduction of forecasted pose error over 4 minute\nhorizons compared to analytical models.", "AI": {"tldr": "The paper develops a trajectory forecasting model based on neural Controlled Differential Equations (CDEs) for predicting multi-robot motion over extended periods, achieving scalable and accurate predictions.", "motivation": "Accurate long-horizon motion forecasting for multi-agent systems is vital for applications like travel time prediction, planning, and simulation. However, challenges like non-linear interactions and prediction errors make it difficult.", "method": "The proposed approach, PINCoDE, integrates neural Controlled Differential Equations (CDEs) with physics-informed constraints to model continuous-time dynamical systems. The model scales to handle up to 100 robots without extra parameters.", "result": "The model achieves high prediction accuracy, with average deviation errors (ADE) below 0.5 m for a one-minute horizon and improved long-term trajectory predictions compared to analytical models.", "conclusion": "PINCoDE demonstrates that physics-informed neural CDEs are effective for long-horizon multi-agent forecasting, offering scalability and accuracy improvements."}}
{"id": "2510.00734", "pdf": "https://arxiv.org/pdf/2510.00734", "abs": "https://arxiv.org/abs/2510.00734", "authors": ["Chuntao Chen", "Tapio Helin", "Nuutti Hyv\u00f6nen", "Yuya Suzuki"], "title": "Approximation of differential entropy in Bayesian optimal experimental design", "categories": ["stat.ML", "cs.LG", "cs.NA", "math.NA", "stat.CO"], "comment": "28 pages, 3 figures", "summary": "Bayesian optimal experimental design provides a principled framework for\nselecting experimental settings that maximize obtained information. In this\nwork, we focus on estimating the expected information gain in the setting where\nthe differential entropy of the likelihood is either independent of the design\nor can be evaluated explicitly. This reduces the problem to maximum entropy\nestimation, alleviating several challenges inherent in expected information\ngain computation.\n  Our study is motivated by large-scale inference problems, such as inverse\nproblems, where the computational cost is dominated by expensive likelihood\nevaluations. We propose a computational approach in which the evidence density\nis approximated by a Monte Carlo or quasi-Monte Carlo surrogate, while the\ndifferential entropy is evaluated using standard methods without additional\nlikelihood evaluations. We prove that this strategy achieves convergence rates\nthat are comparable to, or better than, state-of-the-art methods for full\nexpected information gain estimation, particularly when the cost of entropy\nevaluation is negligible. Moreover, our approach relies only on mild smoothness\nof the forward map and avoids stronger technical assumptions required in\nearlier work. We also present numerical experiments, which confirm our\ntheoretical findings.", "AI": {"tldr": "The paper focuses on reducing computational challenges in Bayesian experimental design by simplifying expected information gain estimation using maximum entropy approaches.", "motivation": "Address challenges in computational cost for large-scale Bayesian inference problems where likelihood evaluations are expensive.", "method": "Develop a Monte Carlo or quasi-Monte Carlo surrogate for evidence density approximation and evaluate differential entropy without additional likelihood computational demands.", "result": "The proposed method demonstrates competitive or superior convergence rates compared to current state-of-the-art techniques, with numerical experiments validating theoretical results.", "conclusion": "The approach alleviates computation issues, employs less restrictive assumptions, and performs effectively for large-scale inference problems."}}
{"id": "2510.00047", "pdf": "https://arxiv.org/pdf/2510.00047", "abs": "https://arxiv.org/abs/2510.00047", "authors": ["Sihao Ding", "Santosh Vasa", "Aditi Ramadwar"], "title": "Explanation-Driven Counterfactual Testing for Faithfulness in Vision-Language Model Explanations", "categories": ["cs.CV", "cs.AI"], "comment": "NeurIPS 2025 workshop on Regulatable ML", "summary": "Vision-Language Models (VLMs) often produce fluent Natural Language\nExplanations (NLEs) that sound convincing but may not reflect the causal\nfactors driving predictions. This mismatch of plausibility and faithfulness\nposes technical and governance risks. We introduce Explanation-Driven\nCounterfactual Testing (EDCT), a fully automated verification procedure for a\ntarget VLM that treats the model's own explanation as a falsifiable hypothesis.\nGiven an image-question pair, EDCT: (1) obtains the model's answer and NLE, (2)\nparses the NLE into testable visual concepts, (3) generates targeted\ncounterfactual edits via generative inpainting, and (4) computes a\nCounterfactual Consistency Score (CCS) using LLM-assisted analysis of changes\nin both answers and explanations. Across 120 curated OK-VQA examples and\nmultiple VLMs, EDCT uncovers substantial faithfulness gaps and provides\nregulator-aligned audit artifacts indicating when cited concepts fail causal\ntests.", "AI": {"tldr": "The paper introduces EDCT, a method to test if Vision-Language Models' explanations align with the causal factors driving their predictions.", "motivation": "Vision-Language Models (VLMs) provide convincing explanations, but these explanations may not truly reflect the causal factors behind their predictions, posing technical and governance risks.", "method": "The proposed Explanation-Driven Counterfactual Testing (EDCT) automatically verifies VLMs by using their explanations as hypotheses. It parses NLEs into visual concepts, applies generative inpainting to create counterfactuals, and evaluates changes in answers and explanations with the Counterfactual Consistency Score (CCS).", "result": "EDCT applied across 120 OK-VQA examples and various VLMs reveals significant faithfulness issues, indicating gaps where explanations fail causal verification.", "conclusion": "The framework effectively identifies when VLM explanations are unfaithful, offering a systematic way to audit and improve their reliability."}}
{"id": "2510.00167", "pdf": "https://arxiv.org/pdf/2510.00167", "abs": "https://arxiv.org/abs/2510.00167", "authors": ["Diego Ortiz Barbosa", "Mohit Agrawal", "Yash Malegaonkar", "Luis Burbano", "Axel Andersson", "Gy\u00f6rgy D\u00e1n", "Henrik Sandberg", "Alvaro A. Cardenas"], "title": "Drones that Think on their Feet: Sudden Landing Decisions with Embodied AI", "categories": ["cs.AI", "cs.CR", "cs.RO"], "comment": null, "summary": "Autonomous drones must often respond to sudden events, such as alarms,\nfaults, or unexpected changes in their environment, that require immediate and\nadaptive decision-making. Traditional approaches rely on safety engineers\nhand-coding large sets of recovery rules, but this strategy cannot anticipate\nthe vast range of real-world contingencies and quickly becomes incomplete.\nRecent advances in embodied AI, powered by large visual language models,\nprovide commonsense reasoning to assess context and generate appropriate\nactions in real time. We demonstrate this capability in a simulated urban\nbenchmark in the Unreal Engine, where drones dynamically interpret their\nsurroundings and decide on sudden maneuvers for safe landings. Our results show\nthat embodied AI makes possible a new class of adaptive recovery and\ndecision-making pipelines that were previously infeasible to design by hand,\nadvancing resilience and safety in autonomous aerial systems.", "AI": {"tldr": "Autonomous drones often face unpredictable events requiring quick decisions. Traditional methods fall short, but recent embodied AI using visual language models offers adaptive and commonsense reasoning for safe reaction.", "motivation": "Traditional hand-coded recovery rules for drones are incomplete as they cannot address all real-world contingencies.", "method": "Utilizing embodied AI with large visual language models to interpret surroundings and adapt drone maneuvers in a simulated urban setting.", "result": "In simulation, embodied AI allows drones to dynamically make safe decisions like landing during emergencies, demonstrating resilience and flexibility.", "conclusion": "Embodied AI introduces an adaptive approach for drone decision-making and safety, surpassing traditional rule-based methods."}}
{"id": "2510.00122", "pdf": "https://arxiv.org/pdf/2510.00122", "abs": "https://arxiv.org/abs/2510.00122", "authors": ["Ryoya Yamasaki"], "title": "Approximately Unimodal Likelihood Models for Ordinal Regression", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Ordinal regression (OR, also called ordinal classification) is classification\nof ordinal data, in which the underlying target variable is categorical and\nconsidered to have a natural ordinal relation for the underlying explanatory\nvariable. A key to successful OR models is to find a data structure `natural\nordinal relation' common to many ordinal data and reflect that structure into\nthe design of those models. A recent OR study found that many real-world\nordinal data show a tendency that the conditional probability distribution\n(CPD) of the target variable given a value of the explanatory variable will\noften be unimodal. Several previous studies thus developed unimodal likelihood\nmodels, in which a predicted CPD is guaranteed to become unimodal. However, it\nwas also observed experimentally that many real-world ordinal data partly have\nvalues of the explanatory variable where the underlying CPD will be\nnon-unimodal, and hence unimodal likelihood models may suffer from a bias for\nsuch a CPD. Therefore, motivated to mitigate such a bias, we propose\napproximately unimodal likelihood models, which can represent up to a unimodal\nCPD and a CPD that is close to be unimodal. We also verify experimentally that\na proposed model can be effective for statistical modeling of ordinal data and\nOR tasks.", "AI": {"tldr": "This paper addresses biases in ordinal regression models caused by strictly unimodal likelihood conditions and proposes approximately unimodal likelihood models, capable of accommodating both unimodal and near-unimodal distributions.", "motivation": "Unimodal likelihood models for ordinal regression can inaccurately represent datasets featuring explanatory variables with non-unimodal conditional probability distributions, leading to biases.", "method": "The authors introduce approximately unimodal likelihood models designed to represent both unimodal and near-unimodal conditional probability distributions.", "result": "Experimental evidence demonstrates the effectiveness of the proposed models in statistical modeling and ordinal regression tasks.", "conclusion": "Approximately unimodal likelihood models mitigate biases in ordinal regression for datasets exhibiting non-unimodal distributions, improving statistical modeling outcomes."}}
{"id": "2510.00261", "pdf": "https://arxiv.org/pdf/2510.00261", "abs": "https://arxiv.org/abs/2510.00261", "authors": ["Xiaoyu Song", "William Han", "Tony Chen", "Chaojing Duan", "Michael A. Rosenberg", "Emerson Liu", "Ding Zhao"], "title": "Retrieval-Augmented Generation for Electrocardiogram-Language Models", "categories": ["cs.CL", "cs.AI", "cs.MM"], "comment": "5 pages, 2 figures; Submitted to ICASSP 2026", "summary": "Interest in generative Electrocardiogram-Language Models (ELMs) is growing,\nas they can produce textual responses conditioned on ECG signals and textual\nqueries. Unlike traditional classifiers that output label probabilities, ELMs\nare more versatile, supporting domain-specific tasks (e.g., waveform analysis,\ndiagnosis, prognosis) as well as general tasks (e.g., open-ended questions,\ndialogue). Retrieval-Augmented Generation (RAG), widely used in Large Language\nModels (LLMs) to ground LLM outputs in retrieved knowledge, helps reduce\nhallucinations and improve natural language generation (NLG). However, despite\nits promise, no open-source implementation or systematic study of RAG pipeline\ndesign for ELMs currently exists. To address this gap, we present the first\nopen-source RAG pipeline for ELMs, along with baselines and ablation studies\nfor NLG. Experiments on three public datasets show that ELMs with RAG\nconsistently improves performance over non-RAG baselines and highlights key ELM\ndesign considerations. Our code is available at:\nhttps://github.com/willxxy/ECG-Bench.", "AI": {"tldr": "The paper introduces an open-source RAG pipeline for Electrocardiogram-Language Models (ELMs) and evaluates its impact on natural language generation (NLG) tasks.", "motivation": "To address the absence of open-source implementations and systematic studies of RAG pipeline design for ELMs, enabling improved textual responses conditioned on ECG data.", "method": "Developed an open-source Retrieval-Augmented Generation (RAG) pipeline for ELMs, conducted ablation studies, and evaluated performance on three public datasets compared to non-RAG baselines.", "result": "RAG integration consistently enhanced ELM performance in NLG tasks over non-RAG baselines.", "conclusion": "Incorporating RAG into ELMs improves their versatility and output quality, providing design insights for future implementations."}}
{"id": "2510.00991", "pdf": "https://arxiv.org/pdf/2510.00991", "abs": "https://arxiv.org/abs/2510.00991", "authors": ["Ziteng Chen", "Xiaohe Hu", "Menghao Zhang", "Yanmin Jia", "Yan Zhang", "Mingjun Zhang", "Da Liu", "Fangzheng Jiao", "Jun Chen", "He Liu", "Aohan Zeng", "Shuaixing Duan", "Ruya Gu", "Yang Jing", "Bowen Han", "Jiahao Cao", "Wei Chen", "Wenqi Xie", "Jinlong Hou", "Yuan Cheng", "Bohua Xu", "Mingwei Xu", "Chunming Hu"], "title": "An Efficient, Reliable and Observable Collective Communication Library in Large-scale GPU Training Clusters", "categories": ["cs.DC"], "comment": "15 pages, 16 figures", "summary": "Large-scale LLM training requires collective communication libraries to\nexchange data among distributed GPUs. As a company dedicated to building and\noperating large-scale GPU training clusters, we encounter several challenges\nwhen using NCCL in production, including 1) limited efficiency with costly and\ncumbersome P2P communication, 2) poor tolerance to frequent RNIC port failures,\nand 3) insufficient observability of transient collective communication\nanomalies. To address these issues, we propose ICCL, an efficient, reliable,\nand observable collective communication library in large-scale GPU training\nclusters. ICCL offloads the P2P communication from GPU kernels to CPU threads\nfor minimal SM consumption, and removes the redundant memory copies irrelevant\nto the actual communicating process. ICCL also introduces a primary-backup QP\nmechanism to tolerate frequent NIC port failures, and designs a window-based\nmonitor to observe network anomalies at O(us) level. We open-source ICCL and\ndeploy it in production training clusters for several months, with results\nshowing that compared to NCCL, ICCL achieves a 23.4%/28.5% improvement in P2P\nthroughput/latency as well as a 6.02% increase in training throughput. We also\nshare the operating experience of ICCL in large-scale clusters, hoping to give\nthe communities more insights on production-level collective communication\nlibraries in LLM training.", "AI": {"tldr": "ICCL is a proposed solution to enhance GPU communication efficiency, reliability, and observability in large-scale LLM training clusters, outperforming the existing NCCL library.", "motivation": "Current collective communication library NCCL struggles with inefficient P2P communication, weak tolerance to NIC port failures, and limited monitoring capabilities in large-scale GPU setups.", "method": "ICCL offloads P2P communication tasks to CPU threads, eliminates redundant memory copies, incorporates primary-backup QP mechanisms, and introduces a window-based network anomaly monitor.", "result": "ICCL demonstrates a 23.4% and 28.5% improvement in P2P throughput and latency respectively, alongside a 6.02% increment in overall training throughput.", "conclusion": "ICCL enhances communication efficiency, reliability, and observability, providing insightful operational experiences for large-scale GPU clusters in production-level settings for LLM training."}}
{"id": "2510.00450", "pdf": "https://arxiv.org/pdf/2510.00450", "abs": "https://arxiv.org/abs/2510.00450", "authors": ["Sheikh Md. Mushfiqur Rahman", "Nasir Eisty"], "title": "Beyond Pass/Fail: The Story of Learning-Based Testing", "categories": ["cs.SE"], "comment": null, "summary": "Learning-Based Testing (LBT) merges learning and testing processes to achieve\nboth testing and behavioral adequacy. LBT utilizes active learning to infer the\nmodel of the System Under Test (SUT), enabling scalability for large and\ncomplex programs by requiring only a minimal set of initial test cases. The\ncore principle of LBT is that the SUT's behavior can be thoroughly inferred by\nprogressively generating test cases and subjecting the SUT to testing, thereby\nensuring comprehensive testing. Despite being in its early stages, LBT has a\nsolid foundation of theoretical research demonstrating its efficacy in testing\nboth procedural and reactive programs. This paper provides a systematic\nliterature review of various LBT implementations across different program types\nand evaluates the current state of research in this field. We explore diverse\ntheoretical frameworks, existing tools, and libraries within the LBT domain to\nillustrate the concept's evolution and current research status. Additionally,\nwe examine case studies involving the application of LBT tools in industrial\nsettings, highlighting their potential and effectiveness in commercial software\ntesting. This systematic literature review aims to offer researchers a\ncomprehensive perspective on the inception and development of LBT, presenting\nit as a promising technique in software testing. By unveiling LBT's\nunderutilized potential, this paper seeks to significantly benefit the\npractitioners and research community.", "AI": {"tldr": "The paper conducts a systematic literature review on Learning-Based Testing (LBT), focusing on theoretical frameworks, tools, and applications, especially in industrial settings.", "motivation": "The motivation is to highlight the potential of Learning-Based Testing (LBT) to enhance software testing by combining learning and testing processes for scalable and effective solutions.", "method": "The authors review existing theoretical research, tools, libraries, frameworks, and case studies involving the application of LBT, analyzing its evolution and practical implementations.", "result": "The paper identifies the efficacy of LBT in testing procedural and reactive programs, its scalability, and showcases examples where LBT was successfully implemented in industrial environments.", "conclusion": "LBT is presented as a promising software testing method with significant scope for broader adoption and further development within research and industry."}}
{"id": "2510.00406", "pdf": "https://arxiv.org/pdf/2510.00406", "abs": "https://arxiv.org/abs/2510.00406", "authors": ["Hengtao Li", "Pengxiang Ding", "Runze Suo", "Yihao Wang", "Zirui Ge", "Dongyuan Zang", "Kexian Yu", "Mingyang Sun", "Hongyin Zhang", "Donglin Wang", "Weihua Su"], "title": "VLA-RFT: Vision-Language-Action Reinforcement Fine-tuning with Verified Rewards in World Simulators", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Vision-Language-Action (VLA) models enable embodied decision-making but rely\nheavily on imitation learning, leading to compounding errors and poor\nrobustness under distribution shift. Reinforcement learning (RL) can mitigate\nthese issues yet typically demands costly real-world interactions or suffers\nfrom sim-to-real gaps. We introduce VLA-RFT, a reinforcement fine-tuning\nframework that leverages a data-driven world model as a controllable simulator.\nTrained from real interaction data, the simulator predicts future visual\nobservations conditioned on actions, allowing policy rollouts with dense,\ntrajectory-level rewards derived from goal-achieving references. This design\ndelivers an efficient and action-aligned learning signal, drastically lowering\nsample requirements. With fewer than 400 fine-tuning steps, VLA-RFT surpasses\nstrong supervised baselines and achieves greater efficiency than\nsimulator-based RL. Moreover, it exhibits strong robustness under perturbed\nconditions, sustaining stable task execution. Our results establish\nworld-model-based RFT as a practical post-training paradigm to enhance the\ngeneralization and robustness of VLA models. For more details, please refer to\nhttps://vla-rft.github.io/.", "AI": {"tldr": "This paper introduces VLA-RFT, a reinforcement fine-tuning framework leveraging a data-driven world model to enhance the robustness and efficiency of Vision-Language-Action models against errors and distribution shifts.", "motivation": "To address the limitations of Vision-Language-Action models that rely heavily on imitation learning, leading to compounding errors and poor robustness, while mitigating the high cost and inefficiency of traditional reinforcement learning approaches.", "method": "The method involves using a data-driven world model trained from real interaction data as a controllable simulator to predict visual observations. Dense trajectory-level rewards are provided for policy rollouts, enabling efficient learning with fewer samples.", "result": "VLA-RFT outperforms strong supervised baselines with less than 400 fine-tuning steps, achieves greater efficiency than traditional simulator-based reinforcement learning, and maintains stable task execution under perturbed conditions.", "conclusion": "World-model-based reinforcement fine-tuning is a practical post-training paradigm that enhances the generalization and robustness of Vision-Language-Action models, offering a significant improvement in efficiency and adaptability."}}
{"id": "2510.01093", "pdf": "https://arxiv.org/pdf/2510.01093", "abs": "https://arxiv.org/abs/2510.01093", "authors": ["Wenxiu Feng", "Antonio Alc\u00e1ntara", "Carlos Ruiz"], "title": "Optimal placement of wind farms via quantile constraint learning", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Wind farm placement arranges the size and the location of multiple wind farms\nwithin a given region. The power output is highly related to the wind speed on\nspatial and temporal levels, which can be modeled by advanced data-driven\napproaches. To this end, we use a probabilistic neural network as a surrogate\nthat accounts for the spatiotemporal correlations of wind speed. This neural\nnetwork uses ReLU activation functions so that it can be reformulated as\nmixed-integer linear set of constraints (constraint learning). We embed these\nconstraints into the placement decision problem, formulated as a two-stage\nstochastic optimization problem. Specifically, conditional quantiles of the\ntotal electricity production are regarded as recursive decisions in the second\nstage. We use real high-resolution regional data from a northern region in\nSpain. We validate that the constraint learning approach outperforms the\nclassical bilinear interpolation method. Numerical experiments are implemented\non risk-averse investors. The results indicate that risk-averse investors\nconcentrate on dominant sites with strong wind, while exhibiting spatial\ndiversification and sensitive capacity spread in non-dominant sites.\nFurthermore, we show that if we introduce transmission line costs in the\nproblem, risk-averse investors favor locations closer to the substations. On\nthe contrary, risk-neutral investors are willing to move to further locations\nto achieve higher expected profits. Our results conclude that the proposed\nnovel approach is able to tackle a portfolio of regional wind farm placements\nand further provide guidance for risk-averse investors.", "AI": {"tldr": "This paper uses a probabilistic neural network reformulated as a mixed-integer linear model to optimize wind farm placements, modeled as a two-stage stochastic problem. It compares risk-averse and risk-neutral investor strategies using data from Spain.", "motivation": "To improve strategic wind farm placement employing advanced data-driven techniques, accounting for spatiotemporal wind speed correlations and investor risk preferences.", "method": "A probabilistic neural network is reformulated using ReLU activations into mixed-integer linear constraints. These are embedded into a two-stage stochastic optimization model, utilizing real regional data from Spain.", "result": "Numerical experiments showed that risk-averse investors tend to prefer dominant wind sites with spatial diversification and are sensitive to transmission costs, while risk-neutral investors prioritize higher profit potential at further locations.", "conclusion": "The novel approach provides effective solutions for wind farm placements, accommodating risk preferences and offering strategic insights for investors."}}
{"id": "2510.00054", "pdf": "https://arxiv.org/pdf/2510.00054", "abs": "https://arxiv.org/abs/2510.00054", "authors": ["Xianjie Liu", "Yiman Hu", "Yixiong Zou", "Liang Wu", "Jian Xu", "Bo Zheng"], "title": "HiDe: Rethinking The Zoom-IN method in High Resolution MLLMs via Hierarchical Decoupling", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have made significant strides in\nvisual understanding tasks. However, their performance on high-resolution\nimages remains suboptimal. While existing approaches often attribute this\nlimitation to perceptual constraints and argue that MLLMs struggle to recognize\nsmall objects, leading them to use \"zoom in\" strategies for better detail, our\nanalysis reveals a different cause: the main issue is not object size, but\nrather caused by complex background interference. We systematically analyze\nthis \"zoom in\" operation through a series of decoupling experiments and propose\nthe Hierarchical Decoupling Framework (HiDe), a training-free framework that\nuses Token-wise Attention Decoupling (TAD) to decouple the question tokens and\nidentify the key information tokens, then leverages their attention weights to\nachieve precise alignment with the target visual regions. Subsequently, it\nemploys Layout-Preserving Decoupling (LPD) to decouple these regions from the\nbackground and reconstructs a compact representation that preserves essential\nspatial layouts while eliminating background interference. HiDe sets a new SOTA\non V*Bench, HRBench4K, and HRBench8K, boosting Qwen2.5-VL 7B and InternVL3 8B\nto SOTA (92.1% and 91.6% on V*Bench), even surpassing RL methods. After\noptimization, HiDe uses 75% less memory than the previous training-free\napproach. Code is provided in https://github.com/Tennine2077/HiDe.", "AI": {"tldr": "The paper introduces a novel framework, HiDe, addressing the limitations of Multimodal Large Language Models (MLLMs) in processing high-resolution images by targeting background interference, rather than object size, improving visual understanding performance significantly.", "motivation": "Existing MLLMs struggle with high-resolution image tasks due to complex background interference, which limits their ability to focus on relevant visual regions.", "method": "The proposed Hierarchical Decoupling Framework (HiDe) uses Token-wise Attention Decoupling (TAD) to isolate question and key information tokens, and Layout-Preserving Decoupling (LPD) to reconstruct compact representations by removing background interference while preserving spatial layouts.", "result": "HiDe achieved state-of-the-art (SOTA) performance on benchmarks like V*Bench, HRBench4K, and HRBench8K, outperforming prior training-free approaches with significant memory optimization (75% less memory usage).", "conclusion": "HiDe effectively enhances the visual processing capabilities of MLLMs for high-resolution images by addressing background interference, demonstrating superior performance and efficiency."}}
{"id": "2510.00185", "pdf": "https://arxiv.org/pdf/2510.00185", "abs": "https://arxiv.org/abs/2510.00185", "authors": ["Gabriel de Olim Gaul", "Adam Gould", "Avinash Kori", "Francesca Toni"], "title": "Object-Centric Case-Based Reasoning via Argumentation", "categories": ["cs.AI"], "comment": "Accepted to ArgXAI@ECAI25", "summary": "We introduce Slot Attention Argumentation for Case-Based Reasoning (SAA-CBR),\na novel neuro-symbolic pipeline for image classification that integrates\nobject-centric learning via a neural Slot Attention (SA) component with\nsymbolic reasoning conducted by Abstract Argumentation for Case-Based Reasoning\n(AA-CBR). We explore novel integrations of AA-CBR with the neural component,\nincluding feature combination strategies, casebase reduction via representative\nsamples, novel count-based partial orders, a One-Vs-Rest strategy for extending\nAA-CBR to multi-class classification, and an application of Supported AA-CBR, a\nbipolar variant of AA-CBR. We demonstrate that SAA-CBR is an effective\nclassifier on the CLEVR-Hans datasets, showing competitive performance against\nbaseline models.", "AI": {"tldr": "SAA-CBR is a hybrid image classification system merging Slot Attention-based object-centric learning with Abstract Argumentation for reasoning, tested on CLEVR-Hans datasets.", "motivation": "The paper aims to advance image classification techniques by combining symbolic reasoning and neural network-based object-centric learning.", "method": "The proposed approach integrates Slot Attention for feature extraction and Abstract Argumentation for reasoning, alongside enhancements like casebase reduction, partial orders, and multi-class classification strategies.", "result": "SAA-CBR achieved competitive performance in classification tasks on CLEVR-Hans datasets compared to baseline models.", "conclusion": "The integration of neuro-symbolic approaches like SAA-CBR can effectively address complex image classification tasks while leveraging the strengths of symbolic reasoning and neural networks."}}
{"id": "2510.00129", "pdf": "https://arxiv.org/pdf/2510.00129", "abs": "https://arxiv.org/abs/2510.00129", "authors": ["Hengkui Wu", "Liujiang Liu", "Jihua He", "Qihao Wang", "Keke Zhao", "Shuyang Hu", "Renle Fu", "Dahao Liang", "Lingyu Zeng", "Bruce Liu", "Yuan Liu", "Jin Zhan", "Jiaqiang Niu", "Xinglong Jia", "Yaqin Hu", "Wenjun Ji", "Panpan Chi", "Ken Chen", "Hengyuan Wu", "Yingsi Xin", "Yongfeng Zhu", "Yuexin Wang", "Manqi Ruan", "Ningtao Bian", "Xiaohua Wu", "Weipeng Xu"], "title": "BigBang-Proton Technical Report: Next-Word-Prediction is Scientific Multitask Learner", "categories": ["cs.LG", "cond-mat.mtrl-sci", "cs.AI", "physics.comp-ph", "68T05, 68T50, 00A69, 94A99", "I.2.6; I.2.7; J.2; I.6.3; K.4.1"], "comment": "93 pages, 39 figures", "summary": "We introduce BigBang-Proton, a unified sequence-based architecture for\nauto-regressive language modeling pretrained on cross-scale, cross-structure,\ncross-discipline real-world scientific tasks to construct a scientific\nmulti-task learner. BigBang-Proton incorporates three fundamental innovations\ncompared to mainstream general-purpose LLMs: Theory-Experiment Learning\nparadigm aligns large-scale numerical experimental data with theoretical text\ncorpora; Binary Patch Encoding replaces byte pair encoding(BPE) tokenization;\nMonte Carlo Attention substitutes traditional transformer architectures.\nThrough next-word-prediction pretraining on cross-discipline scientific\ndatasets of real-world problems mixed with general textual corpus, followed by\nfine-tuning and inference on downstream tasks, BigBang-Proton demonstrates\n100\\% accuracy in up to 50-digit arithmetic addition operations, performance on\npar with leading specialized models in particle physics jet tagging, matching\nMAE of specialized models in inter-atomic potential simulation, performance\ncomparable to traditional spatiotemporal models in water quality prediction,\nand benchmark-exceeding performance in genome modeling. These results prove\nthat language-guided scientific computing can match or exceed the performance\nof task-specific scientific models while maintaining multitask learning\ncapabilities. We further hypothesize to scale the pretraining to the universe\nscale as a fundamental step toward developing material world foundational\nmodel.", "AI": {"tldr": "BigBang-Proton is a novel language model for scientific computing that achieves high accuracy across multiple scientific domains.", "motivation": "Develop a multitask scientific learner capable of addressing diverse real-world problems using a unified sequence-based architecture.", "method": "Innovative features include Theory-Experiment Learning, Binary Patch Encoding, and Monte Carlo Attention, trained on scientific and general text corpora.", "result": "Achieved exceptional accuracy across diverse tasks such as arithmetic operations, particle physics, water quality prediction, and genome modeling, matching or surpassing specialized models.", "conclusion": "Language-based scientific computation can rival task-specific models while offering versatile multitask learning capabilities."}}
{"id": "2510.00263", "pdf": "https://arxiv.org/pdf/2510.00263", "abs": "https://arxiv.org/abs/2510.00263", "authors": ["Zhuohang Li", "Xiaowei Li", "Chengyu Huang", "Guowang Li", "Katayoon Goshvadi", "Bo Dai", "Dale Schuurmans", "Paul Zhou", "Hamid Palangi", "Yiwen Song", "Palash Goyal", "Murat Kantarcioglu", "Bradley A. Malin", "Yuan Xue"], "title": "Judging with Confidence: Calibrating Autoraters to Preference Distributions", "categories": ["cs.CL"], "comment": null, "summary": "The alignment of large language models (LLMs) with human values increasingly\nrelies on using other LLMs as automated judges, or ``autoraters''. However,\ntheir reliability is limited by a foundational issue: they are trained on\ndiscrete preference labels, forcing a single ground truth onto tasks that are\noften subjective, ambiguous, or nuanced. We argue that a reliable autorater\nmust learn to model the full distribution of preferences defined by a target\npopulation. In this paper, we propose a general framework for calibrating\nprobabilistic autoraters to any given preference distribution. We formalize the\nproblem and present two learning methods tailored to different data conditions:\n1) a direct supervised fine-tuning for dense, probabilistic labels, and 2) a\nreinforcement learning approach for sparse, binary labels. Our empirical\nresults show that finetuning autoraters with a distribution-matching objective\nleads to verbalized probability predictions that are better aligned with the\ntarget preference distribution, with improved calibration and significantly\nlower positional bias, all while preserving performance on objective tasks.", "AI": {"tldr": "This paper introduces a framework for calibrating probabilistic autoraters to better align large language models with diverse human preference distributions.", "motivation": "To address the limitation of current autoraters, which rely on discrete labels that fail to capture the complexity of subjective and nuanced human preferences.", "method": "The paper formalizes a framework with two learning methods: supervised fine-tuning for dense labels and reinforcement learning for sparse binary labels.", "result": "Empirical results show improved calibration, lower positional bias, and better alignment with target preference distributions without compromising performance on objective tasks.", "conclusion": "Calibrating autoraters with a distribution-matching objective enhances their reliability and flexibility for aligning LLMs with human values."}}
{"id": "2510.00476", "pdf": "https://arxiv.org/pdf/2510.00476", "abs": "https://arxiv.org/abs/2510.00476", "authors": ["Arushi Sharma", "Vedant Pungliya", "Christopher J. Quinn", "Ali Jannesari"], "title": "Analyzing Latent Concepts in Code Language Models", "categories": ["cs.SE", "cs.AI", "cs.LG"], "comment": null, "summary": "Interpreting the internal behavior of large language models trained on code\nremains a critical challenge, particularly for applications demanding trust,\ntransparency, and semantic robustness. We propose Code Concept Analysis\n(CoCoA): a global post-hoc interpretability framework that uncovers emergent\nlexical, syntactic, and semantic structures in a code language model's\nrepresentation space by clustering contextualized token embeddings into\nhuman-interpretable concept groups. We propose a hybrid annotation pipeline\nthat combines static analysis tool-based syntactic alignment with\nprompt-engineered large language models (LLMs), enabling scalable labeling of\nlatent concepts across abstraction levels. We analyse the distribution of\nconcepts across layers and across three finetuning tasks. Emergent concept\nclusters can help identify unexpected latent interactions and be used to\nidentify trends and biases within the model's learned representations. We\nfurther integrate LCA with local attribution methods to produce\nconcept-grounded explanations, improving the coherence and interpretability of\ntoken-level saliency. Empirical evaluations across multiple models and tasks\nshow that LCA discovers concepts that remain stable under semantic-preserving\nperturbations (average Cluster Sensitivity Index, CSI = 0.288) and evolve\npredictably with fine-tuning. In a user study, concept-augmented explanations\ndisambiguate token roles. In a user study on the programming-language\nclassification task, concept-augmented explanations disambiguated token roles\nand improved human-centric explainability by 37 percentage points compared with\ntoken-level attributions using Integrated Gradients.", "AI": {"tldr": "The paper presents Code Concept Analysis (CoCoA), a method for interpreting code language models by clustering contextualized token embeddings into understandable groups.", "motivation": "Understanding internal behaviors of code-trained language models is crucial for transparency, trust, and improved semantic robustness.", "method": "CoCoA uses post-hoc analysis and combines syntactic alignment with prompt-engineered annotations to identify and label latent concepts.", "result": "Experiments show CoCoA discovers stable and predictable concepts, with its explanations providing improved interpretability (37% improvement in a user study).", "conclusion": "CoCoA enhances human-centric explainability and aids understanding of latent interactions in code-focused models."}}
{"id": "2510.00441", "pdf": "https://arxiv.org/pdf/2510.00441", "abs": "https://arxiv.org/abs/2510.00441", "authors": ["Yiyuan Pan", "Yunzhe Xu", "Zhe Liu", "Hesheng Wang"], "title": "Seeing through Uncertainty: Robust Task-Oriented Optimization in Visual Navigation", "categories": ["cs.RO"], "comment": null, "summary": "Visual navigation is a fundamental problem in embodied AI, yet practical\ndeployments demand long-horizon planning capabilities to address\nmulti-objective tasks. A major bottleneck is data scarcity: policies learned\nfrom limited data often overfit and fail to generalize OOD. Existing neural\nnetwork-based agents typically increase architectural complexity that\nparadoxically become counterproductive in the small-sample regime. This paper\nintroduce NeuRO, a integrated learning-to-optimize framework that tightly\ncouples perception networks with downstream task-level robust optimization.\nSpecifically, NeuRO addresses core difficulties in this integration: (i) it\ntransforms noisy visual predictions under data scarcity into convex uncertainty\nsets using Partially Input Convex Neural Networks (PICNNs) with conformal\ncalibration, which directly parameterize the optimization constraints; and (ii)\nit reformulates planning under partial observability as a robust optimization\nproblem, enabling uncertainty-aware policies that transfer across environments.\nExtensive experiments on both unordered and sequential multi-object navigation\ntasks demonstrate that NeuRO establishes SoTA performance, particularly in\ngeneralization to unseen environments. Our work thus presents a significant\nadvancement for developing robust, generalizable autonomous agents.", "AI": {"tldr": "NeuRO proposes a novel framework combining visual perception with robust optimization for long-horizon and generalizable navigation tasks in embodied AI.", "motivation": "Embodied AI projects suffer from data scarcity issues when tackling long-horizon, multi-objective navigation tasks. Current methods overfit and fail to generalize in unseen environments.", "method": "NeuRO integrates Partially Input Convex Neural Networks (PICNNs) with robust optimization techniques, transforming noisy visual data into convex uncertainty sets and reformulating planning as an uncertainty-aware optimization problem.", "result": "NeuRO delivers SoTA performance in both unordered and sequential multi-object navigation tasks, significantly improving generalization in unseen environments.", "conclusion": "NeuRO advances robust and generalizable autonomous agents, addressing data limitation struggles in current visual navigation systems."}}
{"id": "2510.01098", "pdf": "https://arxiv.org/pdf/2510.01098", "abs": "https://arxiv.org/abs/2510.01098", "authors": ["Blake Bordelon", "Mary I. Letey", "Cengiz Pehlevan"], "title": "Theory of Scaling Laws for In-Context Regression: Depth, Width, Context and Time", "categories": ["stat.ML", "cond-mat.dis-nn", "cs.LG"], "comment": "preprint with 29 pages", "summary": "We study in-context learning (ICL) of linear regression in a deep linear\nself-attention model, characterizing how performance depends on various\ncomputational and statistical resources (width, depth, number of training\nsteps, batch size and data per context). In a joint limit where data dimension,\ncontext length, and residual stream width scale proportionally, we analyze the\nlimiting asymptotics for three ICL settings: (1) isotropic covariates and tasks\n(ISO), (2) fixed and structured covariance (FS), and (3) where covariances are\nrandomly rotated and structured (RRS). For ISO and FS settings, we find that\ndepth only aids ICL performance if context length is limited. Alternatively, in\nthe RRS setting where covariances change across contexts, increasing the depth\nleads to significant improvements in ICL, even at infinite context length. This\nprovides a new solvable toy model of neural scaling laws which depends on both\nwidth and depth of a transformer and predicts an optimal transformer shape as a\nfunction of compute. This toy model enables computation of exact asymptotics\nfor the risk as well as derivation of powerlaws under source/capacity\nconditions for the ICL tasks.", "AI": {"tldr": "The paper analyzes the impact of computational and statistical factors on in-context learning (ICL) in linear regression using deep linear self-attention models, proposing a toy model for scalability.", "motivation": "Understanding how deep attention models manage computational and statistical resources during tasks like ICL in linear regression is crucial for improving scalability and efficiency.", "method": "The paper employs asymptotic analysis of ICL under three settings (ISO, FS, RRS) with varying resource scaling: depth, width, training steps, batch size, and data per context.", "result": "Depth improves ICL performance in limited contexts for ISO/FS settings, while in RRS settings, depth consistently enhances performance even with infinite context length.", "conclusion": "The toy model developed predicts transformer design for optimal compute and provides insights into scaling laws under different conditions."}}
{"id": "2510.00059", "pdf": "https://arxiv.org/pdf/2510.00059", "abs": "https://arxiv.org/abs/2510.00059", "authors": ["Jiahao Fu", "Yinfeng Yu", "Liejun Wang"], "title": "FSDENet: A Frequency and Spatial Domains based Detail Enhancement Network for Remote Sensing Semantic Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted for publication by IEEE Journal of Selected Topics in\n  Applied Earth Observations and Remote Sensing", "summary": "To fully leverage spatial information for remote sensing image segmentation\nand address semantic edge ambiguities caused by grayscale variations (e.g.,\nshadows and low-contrast regions), we propose the Frequency and Spatial Domains\nbased Detail Enhancement Network (FSDENet). Our framework employs spatial\nprocessing methods to extract rich multi-scale spatial features and\nfine-grained semantic details. By effectively integrating global and\nfrequency-domain information through the Fast Fourier Transform (FFT) in global\nmappings, the model's capability to discern global representations under\ngrayscale variations is significantly strengthened. Additionally, we utilize\nHaar wavelet transform to decompose features into high- and low-frequency\ncomponents, leveraging their distinct sensitivity to edge information to refine\nboundary segmentation. The model achieves dual-domain synergy by integrating\nspatial granularity with frequency-domain edge sensitivity, substantially\nimproving segmentation accuracy in boundary regions and grayscale transition\nzones. Comprehensive experimental results demonstrate that FSDENet achieves\nstate-of-the-art (SOTA) performance on four widely adopted datasets: LoveDA,\nVaihingen, Potsdam, and iSAID.", "AI": {"tldr": "The paper introduces FSDENet, a neural network for improving precision in remote sensing image segmentation by combining spatial and frequency domain enhancements.", "motivation": "To address challenges in remote sensing image segmentation, especially semantic edge ambiguities caused by grayscale variations like shadows and low-contrast areas.", "method": "FSDENet leverages spatial methods for multi-scale feature extraction and uses Fast Fourier Transform for integrating global frequency-domain information. Haar wavelet transform refines edge sensitivity in frequency components, combining spatial and frequency-domain insights.", "result": "FSDENet significantly improves segmentation accuracy at boundaries and grayscale transitions, achieving SOTA performance on four datasets: LoveDA, Vaihingen, Potsdam, and iSAID.", "conclusion": "The dual-domain integration approach successfully enhances performance in semantic segmentation tasks, particularly in challenging grayscale regions."}}
{"id": "2510.00186", "pdf": "https://arxiv.org/pdf/2510.00186", "abs": "https://arxiv.org/abs/2510.00186", "authors": ["Anni Li", "Aria Attar", "Paul Dong"], "title": "Thinkquel: A Model Dedicated to Text-to-dbt Using Synthetic Data and a Span-Aware Objective", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Transforming natural-language requests into reliable, production-ready data\ntransformations remains challenging: correctness depends on precise schema\nlinking and warehouse-specific SQL dialects, while the strongest supervision\navailable during training--execution success and result matching--are provided\nonly at the sequence level. At the same time, assembling large,\nexecution-validated corpora is costly, and token-level objectives misalign with\nthese global signals, yielding unstable optimization and limited portability.\nWe introduce Thinkquel, a fine-tuned model for producing robust, portable, and\nexecution-validated database queries. Methodologies in Thinkquel integrates a\nnovel synthetic data pipeline, TS-SQL, that leverages dbt as a portable\nintermediate representation with a span-aware reinforcement learning objective,\nand Token-Sequence GRPO (TS-GRPO), specifically designed to bridge the gap\nbetween token-level training signals and sequence-level execution rewards when\nfinetuning LLMs. On the 500-example TS-SQL test set, Thinkquel (32B) reaches\n93.2\\% execution success and 61.8\\% exact-result match with a two-stage SFT\ncurriculum, improving over the base model by 67.2\\% (exec.) and 44.4\\% (match).\nIn Spider (14B) experiments, TS-GRPO increases training stability and speeds\nconvergence of the execution-match reward relative to GRPO and GSPO.", "AI": {"tldr": "The paper introduces Thinkquel, a fine-tuned model for generating reliable database queries, focusing on schema linking and SQL dialects. It integrates a synthetic data pipeline (TS-SQL) and a span-aware reinforcement learning method (TS-GRPO). Results on a test set show a significant improvement in execution success and exact-result matches.", "motivation": "To address the challenges of transforming natural language requests into accurate database queries, which involve schema linking, SQL dialects, and limited supervision for training models.", "method": "Thinkquel employs a synthetic data pipeline (TS-SQL), dbt as an intermediate representation, and a span-aware reinforcement learning objective (TS-GRPO) to bridge the gap between token-level and sequence-level rewards, ensuring improved training stability and optimization.", "result": "Thinkquel (32B) achieved 93.2% execution success and 61.8% exact-result match on the TS-SQL test set, outperforming previous models by significant margins. TS-GRPO also boosts training stability in Spider (14B) experiments.", "conclusion": "Thinkquel's methodologies demonstrate robust improvements in generating reliable database queries, making it a valuable approach for enhancing training and execution quality in the field of natural-language-to-SQL transformation."}}
{"id": "2510.00133", "pdf": "https://arxiv.org/pdf/2510.00133", "abs": "https://arxiv.org/abs/2510.00133", "authors": ["Adarsha Balaji", "Sandeep Madireddy"], "title": "Large Language Models Inference Engines based on Spiking Neural Networks", "categories": ["cs.LG"], "comment": null, "summary": "Foundational models based on the transformer architecture are currently the\nstate-of-the-art in general language modeling, as well as in scientific areas\nsuch as material science and climate. However, training and deploying these\nmodels is computationally challenging as the time and space complexity has a\nquadratic relation to the input sequence length. Several efforts exploring\nefficient computational paradigms and model architectures to address these\nlimitations have been made. In this work, we explore spiking neural networks\n(SNNs) to design transformer models. A challenge in training large-scale SNNs,\nusing existing surrogate learning methods is inefficient and time-consuming. On\nthe other hand, techniques to convert existing transformer-based models to\ntheir SNN equivalent are not scalable, as achieving optimal performance comes\nat the cost of a large number of spike time-steps, i.e. increased latency. To\naddress this, we propose NeurTransformer, a methodology for designing\ntransformer-based SNN for inference using a supervised fine-tuning approach\nwith existing conversion methods. The proposed methodology works by: (1)\nreplacing the self-attention mechanism with a spike-based self-attention (SSA),\n(2) converting the feed-forward block of the trained transformer model to its\nequivalent SNN, and (3) fine-tuning the SSA block using SNN-based surrogate\nlearning algorithms. We benchmark the proposed methodology and demonstrate its\naccuracy and scalability using three variants of the GPT-2 model of increasing\nmodel size. We observe that the converted GPT-2 small models demonstrate a\n5-12% loss in cosine similarity and a 9.7% reduction in perplexity. Finally, we\ndemonstrate the energy efficiency of the SSA block compared to the ASA block\nand show between 64.71% and 85.28% reductions in estimated energy consumption\nwhen implementing the self-attention mechanism on a digital hardware.", "AI": {"tldr": "This paper introduces NeurTransformer, a methodology to create energy-efficient transformer-based spiking neural networks (SNNs) by fine-tuning spike-based self-attention and converting feed-forward blocks.", "motivation": "Address the computational challenges and inefficiency in training and deploying large-scale foundational models like transformers, especially their quadratic complexity with sequence length.", "method": "Use spiking neural networks (SNNs) by replacing self-attention with spike-based self-attention, converting feed-forward blocks into SNN equivalents, and applying fine-tuning with surrogate learning algorithms.", "result": "Benchmarking shows 5-12% loss in cosine similarity, 9.7% reduction in perplexity for converted GPT-2 models, and 64.71%-85.28% energy savings in the self-attention mechanism on digital hardware.", "conclusion": "NeurTransformer provides scalable and energy-efficient transformer designs using spiking neural networks, offering practical advantages in computational efficiency despite trade-offs in model performance metrics."}}
{"id": "2510.00268", "pdf": "https://arxiv.org/pdf/2510.00268", "abs": "https://arxiv.org/abs/2510.00268", "authors": ["Zhexiong Liu", "Diane Litman"], "title": "Efficient Layer-wise LLM Fine-tuning for Revision Intention Prediction", "categories": ["cs.CL", "cs.AI"], "comment": "In The Conference on Empirical Methods in Natural Language Processing\n  (EMNLP), November 2025", "summary": "Large Language Models (LLMs) have shown extraordinary success across various\ntext generation tasks; however, their potential for simple yet essential text\nclassification remains underexplored, as LLM pre-training tends to emphasize\ngeneration over classification. While LLMs with instruction tuning can\ntransform classification into a generation task, they often struggle to\ncategorize nuanced texts. One such example is text revision, which involves\nnuanced edits between pairs of texts. Although simply fine-tuning LLMs for\nrevision classification seems plausible, it requires a large amount of revision\nannotations, which are exceptionally expensive and scarce in the community. To\naddress this issue, we introduce a plug-and-play layer-wise parameter-efficient\nfine-tuning (PEFT) framework, i.e., IR-Tuning, which fine-tunes a subset of\nimportant LLM layers that are dynamically selected based on their gradient norm\ndistribution, while freezing those of redundant layers. Extensive experiments\nsuggest that IR-Tuning surpasses several layer-wise PEFT baselines over diverse\ntext revisions, while achieving fast convergence, low GPU memory consumption,\nand effectiveness on small revision corpora.", "AI": {"tldr": "The paper presents a method called IR-Tuning, a parameter-efficient fine-tuning approach for Large Language Models (LLMs), addressing their challenge in nuanced text classification tasks like text revision.", "motivation": "Large Language Models are excellent at text generation but less effective at nuanced text classification tasks, such as text revision classification, due to their pre-training bias and lack of sufficient annotated data.", "method": "The authors propose IR-Tuning, a layer-wise parameter-efficient fine-tuning approach that dynamically selects important layers for tuning based on their gradient norm distribution, reducing the need for comprehensive annotation and computational resources.", "result": "Experiments demonstrate that IR-Tuning outperforms other layer-wise PEFT methods, exhibiting faster convergence, reduced GPU memory usage, and effectiveness even with small datasets.", "conclusion": "IR-Tuning offers a promising solution for resource-constrained and nuanced classification tasks, enhancing LLM performance without extensive annotated datasets."}}
{"id": "2510.00501", "pdf": "https://arxiv.org/pdf/2510.00501", "abs": "https://arxiv.org/abs/2510.00501", "authors": ["Kaixin Wang", "Tianlin Li", "Xiaoyu Zhang", "Aishan Liu", "Xianglong Liu", "Ziqi Liu", "Zhiqiang Zhang", "Jun Zhou", "and Bin Shi"], "title": "CodeChemist: Functional Knowledge Transfer for Low-Resource Code Generation via Test-Time Scaling", "categories": ["cs.SE"], "comment": null, "summary": "Code Large Language Models (CodeLLMs) are increasingly used in code\ngeneration tasks across a wide range of applications. However, their\nperformance is often inconsistent across different programming languages (PLs),\nwith low-resource PLs suffering the most due to limited training data. In this\npaper, we present CodeChemist, a novel and efficient framework for test-time\nscaling that enables functional knowledge transfer from high-resource to\nlow-resource PLs using generated test cases. CodeChemist first generates and\nexecutes code in high-resource PLs to create test cases that encapsulate\nfunctional knowledge. It then uses multi-temperature hedged sampling to\ngenerate code snippets in the low-resource PL and selects the best one based on\nthe pass rate of the test cases. Our extensive experiments show that\nCodeChemist outperforms existing test-time scaling approaches, boosting the\nperformance of code generation for low-resource PLs without requiring any model\nretraining.", "AI": {"tldr": "CodeChemist framework improves code generation for low-resource programming languages (PLs) by transferring functional knowledge from high-resource PLs using test cases, without model retraining.", "motivation": "CodeLLMs often perform inconsistently across programming languages, especially for low-resource PLs due to limited training data.", "method": "CodeChemist generates test cases by executing code in high-resource PLs and uses multi-temperature hedged sampling to select the best code outputs in low-resource PLs based on test case pass rates.", "result": "Experiments demonstrate CodeChemist's superior performance compared to existing test-time scaling methods, enhancing code generation for low-resource PLs.", "conclusion": "CodeChemist efficiently bridges the resource gap between programming languages, enabling improved code generation for low-resource PLs without revising the underlying model."}}
{"id": "2510.00466", "pdf": "https://arxiv.org/pdf/2510.00466", "abs": "https://arxiv.org/abs/2510.00466", "authors": ["Run Su", "Hao Fu", "Shuai Zhou", "Yingao Fu"], "title": "Integrating Offline Pre-Training with Online Fine-Tuning: A Reinforcement Learning Approach for Robot Social Navigation", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Offline reinforcement learning (RL) has emerged as a promising framework for\naddressing robot social navigation challenges. However, inherent uncertainties\nin pedestrian behavior and limited environmental interaction during training\noften lead to suboptimal exploration and distributional shifts between offline\ntraining and online deployment. To overcome these limitations, this paper\nproposes a novel offline-to-online fine-tuning RL algorithm for robot social\nnavigation by integrating Return-to-Go (RTG) prediction into a causal\nTransformer architecture. Our algorithm features a spatiotem-poral fusion model\ndesigned to precisely estimate RTG values in real-time by jointly encoding\ntemporal pedestrian motion patterns and spatial crowd dynamics. This RTG\nprediction framework mitigates distribution shift by aligning offline policy\ntraining with online environmental interactions. Furthermore, a hybrid\noffline-online experience sampling mechanism is built to stabilize policy\nupdates during fine-tuning, ensuring balanced integration of pre-trained\nknowledge and real-time adaptation. Extensive experiments in simulated social\nnavigation environments demonstrate that our method achieves a higher success\nrate and lower collision rate compared to state-of-the-art baselines. These\nresults underscore the efficacy of our algorithm in enhancing navigation policy\nrobustness and adaptability. This work paves the way for more reliable and\nadaptive robotic navigation systems in real-world applications.", "AI": {"tldr": "This paper proposes an algorithm using offline-to-online reinforcement learning (RL) to improve robot social navigation, enhancing adaptability and robustness.", "motivation": "Addressing suboptimal exploration and distribution shifts in offline RL for robot navigation due to uncertainties in pedestrian behavior and limited training interaction.", "method": "Introduced a spatiotemporal fusion model with causal Transformer for precise Return-to-Go (RTG) prediction, coupled with a hybrid offline-online experience sampling mechanism for fine-tuning.", "result": "The proposed method showed a higher success rate and lower collision rate compared to state-of-the-art baselines in simulated social navigation environments.", "conclusion": "The algorithm improves robustness and adaptability of robot navigation, promising advancements for real-world applications."}}
{"id": "2510.00048", "pdf": "https://arxiv.org/pdf/2510.00048", "abs": "https://arxiv.org/abs/2510.00048", "authors": ["Fahad Mostafa", "Kannon Hossain", "Hafiz Khan"], "title": "Deep Learning Approaches with Explainable AI for Differentiating Alzheimer Disease and Mild Cognitive Impairment", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG", "stat.AP", "stat.ML"], "comment": "18 pages, 4 figures", "summary": "Early and accurate diagnosis of Alzheimer Disease is critical for effective\nclinical intervention, particularly in distinguishing it from Mild Cognitive\nImpairment, a prodromal stage marked by subtle structural changes. In this\nstudy, we propose a hybrid deep learning ensemble framework for Alzheimer\nDisease classification using structural magnetic resonance imaging. Gray and\nwhite matter slices are used as inputs to three pretrained convolutional neural\nnetworks such as ResNet50, NASNet, and MobileNet, each fine tuned through an\nend to end process. To further enhance performance, we incorporate a stacked\nensemble learning strategy with a meta learner and weighted averaging to\noptimally combine the base models. Evaluated on the Alzheimer Disease\nNeuroimaging Initiative dataset, the proposed method achieves state of the art\naccuracy of 99.21% for Alzheimer Disease vs. Mild Cognitive Impairment and\n91.0% for Mild Cognitive Impairment vs. Normal Controls, outperforming\nconventional transfer learning and baseline ensemble methods. To improve\ninterpretability in image based diagnostics, we integrate Explainable AI\ntechniques by Gradient weighted Class Activation, which generates heatmaps and\nattribution maps that highlight critical regions in gray and white matter\nslices, revealing structural biomarkers that influence model decisions. These\nresults highlight the frameworks potential for robust and scalable clinical\ndecision support in neurodegenerative disease diagnostics.", "AI": {"tldr": "A hybrid ensemble deep learning framework using structural MRI achieves high accuracy in diagnosing Alzheimer's Disease, distinguishing it from Mild Cognitive Impairment (MCI), with additional interpretability techniques for improved clinical decision-making.", "motivation": "Early and accurate diagnosis of Alzheimer's Disease, differentiating it from Mild Cognitive Impairment, is essential to enable effective clinical intervention, leveraging subtle structural changes in brain imaging.", "method": "The study uses a hybrid deep learning framework that employs three pre-trained convolutional neural networks (ResNet50, NASNet, and MobileNet), fine-tunes them, and combines their outputs via a stacked ensemble method with a meta-learner and weighted averaging. Explainable AI is incorporated to provide heatmaps highlighting model-relevant structural biomarkers.", "result": "The proposed method achieves state-of-the-art accuracy: 99.21% for Alzheimer's vs. MCI and 91.0% for MCI vs. Normal controls, tested on the Alzheimer Disease Neuroimaging Initiative dataset.", "conclusion": "The framework demonstrates robust, scalable potential for clinical decision support in diagnosing Alzheimer's Disease and MCI, enhanced by interpretability for understanding its decision-making."}}
{"id": "2510.00060", "pdf": "https://arxiv.org/pdf/2510.00060", "abs": "https://arxiv.org/abs/2510.00060", "authors": ["Sheng Yang", "Tong Zhan", "Guancheng Chen", "Yanfeng Lu", "Jian Wang"], "title": "Less is More: Lean yet Powerful Vision-Language Model for Autonomous Driving", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": null, "summary": "In this work, we reconceptualize autonomous driving as a generalized language\nand formulate the trajectory planning task as next waypoint prediction. We\nintroduce Max-V1, a novel framework for one-stage end-to-end autonomous\ndriving. Our framework presents a single-pass generation paradigm that aligns\nwith the inherent sequentiality of driving. This approach leverages the\ngenerative capacity of the VLM (Vision-Language Model) to enable end-to-end\ntrajectory prediction directly from front-view camera input. The efficacy of\nthis method is underpinned by a principled supervision strategy derived from\nstatistical modeling. This provides a well-defined learning objective, which\nmakes the framework highly amenable to master complex driving policies through\nimitation learning from large-scale expert demonstrations. Empirically, our\nmethod achieves the state-of-the-art performance on the nuScenes dataset,\ndelivers an overall improvement of over 30% compared to prior baselines.\nFurthermore, it exhibits superior generalization performance on cross-domain\ndatasets acquired from diverse vehicles, demonstrating notable potential for\ncross-vehicle robustness and adaptability. Due to these empirical strengths,\nthis work introduces a model enabling fundamental driving behaviors, laying the\nfoundation for the development of more capable self-driving agents. Code will\nbe available upon publication.", "AI": {"tldr": "The paper introduces Max-V1, a one-stage end-to-end framework for autonomous driving that employs a vision-language model for trajectory prediction, achieving significant improvements in performance and generalization.", "motivation": "Current challenges in autonomous driving include creating end-to-end systems capable of robust trajectory planning while managing cross-domain adaptability. The paper aims to address these issues by reconceptualizing driving as a sequential prediction task.", "method": "The authors propose Max-V1, which uses a vision-language model (VLM) to predict driving trajectories directly from camera inputs. This is done through imitation learning from large-scale expert data and a statistically-informed supervision strategy.", "result": "Max-V1 achieves state-of-the-art performance on the nuScenes dataset, showing over 30% improvement over existing baselines. It also demonstrates strong cross-domain generalization in datasets gathered from different vehicles.", "conclusion": "The proposed Max-V1 framework presents a significant advancement in autonomous driving, offering both high performance and adaptability across different vehicles. It lays the groundwork for developing more robust and versatile self-driving systems."}}
{"id": "2510.00229", "pdf": "https://arxiv.org/pdf/2510.00229", "abs": "https://arxiv.org/abs/2510.00229", "authors": ["Rohan Kadekodi", "Zhan Jin", "Keisuke Kamahori", "Yile Gu", "Sean Khatiri", "Noah H. Bayindirli", "Sergey Gorbunov", "Baris Kasikci"], "title": "DualTune: Decoupled Fine-Tuning for On-Device Agentic Systems", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "The deployment of Large Language Models (LLMs) as agentic orchestrators has\nrevolutionized task automation, but the need for privacy-preserving,\ncost-effective solutions demands on-device inference capabilities. However,\nlocal LLMs consistently underperform compared to frontier models in tool\ncalling scenarios, struggling with both tool selection from large tool sets and\naccurate argument generation for complex parameter structures. We introduce a\nmethodology that disaggregates a tool-calling task into two distinct subtasks:\ntool selection and argument generation. We propose \"decoupled fine-tuning\", a\nnovel post-training approach that employs LoRA fine-tuning to create dedicated\nLoRA adapters for tool selection and tool-specific argument generation using\nseparate loss masking for each of the subtasks. Furthermore, we present\nDualTune, an inference framework that leverages the LoRA adapters created using\ndecoupled fine-tuning to perform efficient agent orchestration with the help of\nlocal models on end-user devices. DualTune decomposes the tool-call generation\nstep into tool selection and argument generation, and dynamically loads the\ncorresponding LoRA adapters to generate tool calls. Additionally, DualTune\nimplements hierarchical orchestration to restrict the number of tools required\nfor tool selection. Our experiments on the MCP-Bench benchmark demonstrate that\nthe Qwen-2.5-7B model trained using decoupled fine-tuning improves the tool\ncalling accuracy of the base model by 46%, and outperforms other local\nreasoning, non-reasoning and fine-tuned models of similar size in all cases,\nand models that are 2x larger, in most cases.", "AI": {"tldr": "The paper introduces a new method for improving tool-calling accuracy in local Large Language Models (LLMs) through a framework called \"DualTune,\" which enhances both tool selection and argument generation.", "motivation": "Local LLMs are vital for privacy-preserving and cost-effective task automation but often fail in tool-calling scenarios. They struggle with selecting the correct tools and generating arguments for complex tasks, highlighting the need for better solutions.", "method": "The authors propose 'decoupled fine-tuning,' which uses LoRA fine-tuning to separately train adapters for tool selection and argument generation. These adapters are used in an inference framework named DualTune, which optimizes tool call generation by dynamically loading appropriate adapters and limiting the number of tools considered.", "result": "The methodology, when applied to the Qwen-2.5-7B model, improves tool-calling accuracy by 46% and outperforms similarly sized models as well as some models twice its size.", "conclusion": "Decoupled fine-tuning and the DualTune framework significantly enhance the local model's performance in tool-calling tasks, enabling efficient, privacy-preserving task automation on end-user devices."}}
{"id": "2510.00136", "pdf": "https://arxiv.org/pdf/2510.00136", "abs": "https://arxiv.org/abs/2510.00136", "authors": ["Yujia Zheng", "Shaoan Xie", "Kun Zhang"], "title": "Nonparametric Identification of Latent Concepts", "categories": ["cs.LG", "cs.AI", "math.PR", "stat.ML"], "comment": "ICML 2025", "summary": "We are born with the ability to learn concepts by comparing diverse\nobservations. This helps us to understand the new world in a compositional\nmanner and facilitates extrapolation, as objects naturally consist of multiple\nconcepts. In this work, we argue that the cognitive mechanism of comparison,\nfundamental to human learning, is also vital for machines to recover true\nconcepts underlying the data. This offers correctness guarantees for the field\nof concept learning, which, despite its impressive empirical successes, still\nlacks general theoretical support. Specifically, we aim to develop a\ntheoretical framework for the identifiability of concepts with multiple classes\nof observations. We show that with sufficient diversity across classes, hidden\nconcepts can be identified without assuming specific concept types, functional\nrelations, or parametric generative models. Interestingly, even when conditions\nare not globally satisfied, we can still provide alternative guarantees for as\nmany concepts as possible based on local comparisons, thereby extending the\napplicability of our theory to more flexible scenarios. Moreover, the hidden\nstructure between classes and concepts can also be identified\nnonparametrically. We validate our theoretical results in both synthetic and\nreal-world settings.", "AI": {"tldr": "The paper proposes a theoretical framework for machines to identify hidden concepts using diverse observations, inspired by human cognitive mechanisms.", "motivation": "Human learning leverages comparisons of diverse observations to understand concepts compositionally and extrapolate knowledge. The paper explores parallels in machine learning to ensure correct identification of hidden data concepts and provide theoretical support for concept learning.", "method": "The authors develop a framework for concept identifiability using diverse class observations, without relying on predefined concept types, functional relations, or parametric models. They also propose a local comparison-based approach for scenarios where global conditions are unmet.", "result": "Theoretical results demonstrate the ability to identify hidden concepts and their structures nonparametrically, validated in synthetic and real-world datasets.", "conclusion": "Concept identifiability is achievable under diverse conditions, enhancing machines' capabilities to understand hidden structures. The framework offers correctness guarantees and extends applicability beyond rigid assumptions."}}
{"id": "2510.00276", "pdf": "https://arxiv.org/pdf/2510.00276", "abs": "https://arxiv.org/abs/2510.00276", "authors": ["Joe Barrow", "Raj Patel", "Misha Kharkovski", "Ben Davies", "Ryan Schmitt"], "title": "SafePassage: High-Fidelity Information Extraction with Black Box LLMs", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Black box large language models (LLMs) make information extraction (IE) easy\nto configure, but hard to trust. Unlike traditional information extraction\npipelines, the information \"extracted\" is not guaranteed to be grounded in the\ndocument. To prevent this, this paper introduces the notion of a \"safe\npassage\": context generated by the LLM that is both grounded in the document\nand consistent with the extracted information. This is operationalized via a\nthree-step pipeline, SafePassage, which consists of: (1) an LLM extractor that\ngenerates structured entities and their contexts from a document, (2) a\nstring-based global aligner, and (3) a scoring model. Results show that using\nthese three parts in conjunction reduces hallucinations by up to 85% on\ninformation extraction tasks with minimal risk of flagging non-hallucinations.\nHigh agreement between the SafePassage pipeline and human judgments of\nextraction quality mean that the pipeline can be dually used to evaluate LLMs.\nSurprisingly, results also show that using a transformer encoder fine-tuned on\na small number of task-specific examples can outperform an LLM scoring model at\nflagging unsafe passages. These annotations can be collected in as little as\n1-2 hours.", "AI": {"tldr": "The paper introduces \"SafePassage,\" a pipeline to ensure information extracted by large language models (LLMs) is grounded in the document to reduce hallucinations in information extraction tasks.", "motivation": "To address the issue of hallucinations and ensure extracted information by LLMs is trustworthy and grounded within the document itself.", "method": "The proposed pipeline includes three steps: (1) structured entity extraction by LLM, (2) global alignment of these entities using string-based methods, and (3) scoring the groundedness of extracted information.", "result": "The SafePassage pipeline reduces hallucinations by 85% in information extraction tasks, with high agreement with human evaluation. A transformer encoder fine-tuned on task-specific examples outperformed an LLM scoring model in flagging unsafe passages.", "conclusion": "SafePassage ensures safer and evaluable information extraction, showcasing its robustness in reducing hallucinations and the potential of fine-tuned transformer encoders for specific tasks."}}
{"id": "2510.00206", "pdf": "https://arxiv.org/pdf/2510.00206", "abs": "https://arxiv.org/abs/2510.00206", "authors": ["Zhanda Zhu", "Qidong Su", "Yaoyao Ding", "Kevin Song", "Shang Wang", "Gennady Pekhimenko"], "title": "LoRAFusion: Efficient LoRA Fine-Tuning for LLMs", "categories": ["cs.LG", "cs.AI", "cs.DC"], "comment": "Accepted by EuroSys 2026", "summary": "Low-Rank Adaptation (LoRA) has become the leading Parameter-Efficient\nFine-Tuning (PEFT) method for Large Language Models (LLMs), as it significantly\nreduces GPU memory usage while maintaining competitive fine-tuned model quality\non downstream tasks. Despite these benefits, we identify two key inefficiencies\nin existing LoRA fine-tuning systems. First, they incur substantial runtime\noverhead due to redundant memory accesses on large activation tensors. Second,\nthey miss the opportunity to concurrently fine-tune multiple independent LoRA\nadapters that share the same base model on the same set of GPUs. This leads to\nmissed performance gains such as reduced pipeline bubbles, better communication\noverlap, and improved GPU load balance.\n  To address these issues, we introduce LoRAFusion, an efficient LoRA\nfine-tuning system for LLMs. At the kernel level, we propose a graph-splitting\nmethod that fuses memory-bound operations. This design eliminates unnecessary\nmemory accesses and preserves the performance of compute-bound GEMMs without\nincurring the cost of recomputation or synchronization. At the scheduling\nlevel, LoRAFusion introduces an adaptive batching algorithm for multi-job\nfine-tuning. It first splits LoRA adapters into groups to intentionally stagger\nbatch execution across jobs, and then solves a bin-packing problem within each\ngroup to generate balanced, dependency-aware microbatches. LoRAFusion achieves\nup to $1.96\\times$ ($1.47\\times$ on average) end-to-end speedup compared to\nMegatron-LM, and up to $1.46\\times$ ($1.29\\times$ on average) improvement over\nmLoRA, the state-of-the-art multi-LoRA fine-tuning system. Our fused kernel\nachieves up to $1.39\\times$ ($1.27\\times$ on average) kernel performance\nimprovement and can directly serve as a plug-and-play replacement in existing\nLoRA systems. We open-source LoRAFusion at\nhttps://github.com/CentML/lorafusion.", "AI": {"tldr": "LoRAFusion improves LoRA fine-tuning for Large Language Models by addressing inefficiencies like memory access overhead and inability to fine-tune multiple adapters concurrently. It achieves up to 1.96x speedup compared to existing systems.", "motivation": "LoRA fine-tuning systems are highly parameter-efficient but face runtime inefficiencies due to redundant memory accesses and lack of concurrent multi-adapter fine-tuning, limiting their performance.", "method": "LoRAFusion introduces a graph-splitting method to optimize memory-bound operations and an adaptive batching algorithm for efficient multi-job fine-tuning. This includes dependency-aware microbatch scheduling.", "result": "LoRAFusion achieves up to 1.96x speedup over Megatron-LM and 1.46x over mLoRA. Its kernel offers up to 1.39x performance improvement and can be used as a plug-and-play replacement.", "conclusion": "LoRAFusion successfully addresses key inefficiencies in existing LoRA systems, offering significant speed and performance improvements while maintaining fine-tuning quality. It has been open-sourced for public use."}}
{"id": "2510.00519", "pdf": "https://arxiv.org/pdf/2510.00519", "abs": "https://arxiv.org/abs/2510.00519", "authors": ["Hadiza Umar Yusuf", "Khouloud Gaaloul"], "title": "Architectural Transformations and Emerging Verification Demands in AI-Enabled Cyber-Physical Systems", "categories": ["cs.SE", "cs.AI", "D.2.4; D.2.11"], "comment": null, "summary": "In the world of Cyber-Physical Systems (CPS), a captivating real-time fusion\noccurs where digital technology meets the physical world. This synergy has been\nsignificantly transformed by the integration of artificial intelligence (AI), a\nmove that dramatically enhances system adaptability and introduces a layer of\ncomplexity that impacts CPS control optimization and reliability. Despite\nadvancements in AI integration, a significant gap remains in understanding how\nthis shift affects CPS architecture, operational complexity, and verification\npractices. The extended abstract addresses this gap by investigating\narchitectural distinctions between AI-driven and traditional control models\ndesigned in Simulink and their respective implications for system verification.", "AI": {"tldr": "The paper explores the implications of AI integration on CPS architecture, operational complexity, and verification practices, comparing AI-driven versus traditional models in Simulink.", "motivation": "To address the gap in understanding the impacts of AI integration on CPS structures, operations, and verification processes.", "method": "Investigated architectural differences between AI-driven and traditional control models utilizing Simulink.", "result": "Provided insights into the distinct implications of AI-based control models compared to traditional ones, especially concerning system verification.", "conclusion": "AI integration introduces adaptability but also new complexity in CPS, necessitating revised verification approaches and understanding of architectural differences."}}
{"id": "2510.00491", "pdf": "https://arxiv.org/pdf/2510.00491", "abs": "https://arxiv.org/abs/2510.00491", "authors": ["Han Zhou", "Jinjin Cao", "Liyuan Ma", "Xueji Fang", "Guo-jun Qi"], "title": "From Human Hands to Robot Arms: Manipulation Skills Transfer via Trajectory Alignment", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Learning diverse manipulation skills for real-world robots is severely\nbottlenecked by the reliance on costly and hard-to-scale teleoperated\ndemonstrations. While human videos offer a scalable alternative, effectively\ntransferring manipulation knowledge is fundamentally hindered by the\nsignificant morphological gap between human and robotic embodiments. To address\nthis challenge and facilitate skill transfer from human to robot, we introduce\nTraj2Action,a novel framework that bridges this embodiment gap by using the 3D\ntrajectory of the operational endpoint as a unified intermediate\nrepresentation, and then transfers the manipulation knowledge embedded in this\ntrajectory to the robot's actions. Our policy first learns to generate a coarse\ntrajectory, which forms an high-level motion plan by leveraging both human and\nrobot data. This plan then conditions the synthesis of precise, robot-specific\nactions (e.g., orientation and gripper state) within a co-denoising framework.\nExtensive real-world experiments on a Franka robot demonstrate that Traj2Action\nboosts the performance by up to 27% and 22.25% over $\\pi_0$ baseline on short-\nand long-horizon real-world tasks, and achieves significant gains as human data\nscales in robot policy learning. Our project website, featuring code and video\ndemonstrations, is available at\nhttps://anonymous.4open.science/w/Traj2Action-4A45/.", "AI": {"tldr": "Traj2Action is a framework that enables robots to learn manipulation skills by converting human 3D trajectories into robot-specific actions, bypassing the need for costly teleoperated demonstrations.", "motivation": "The paper aims to address the challenge of transferring manipulation skills from humans to robots despite the morphological differences, thereby overcoming the limitations of expensive and hard-to-scale teleoperation methods.", "method": "The framework uses the 3D trajectory of the operational endpoint as an intermediate representation. It first generates a coarse trajectory using human and robot data, and then refines robot-specific actions through a co-denoising approach.", "result": "Real-world experiments on a Franka robot show that Traj2Action improves task performance by up to 27% and 22.25% over baseline policies on short- and long-horizon tasks. Additionally, it scales well with increasing human data.", "conclusion": "Traj2Action successfully bridges the embodiment gap and leverages human video data to teach robots diverse manipulation skills efficiently, demonstrating scalability and improved performance in real-world scenarios."}}
{"id": "2510.00079", "pdf": "https://arxiv.org/pdf/2510.00079", "abs": "https://arxiv.org/abs/2510.00079", "authors": ["Hai Huang"], "title": "Directed Information $\u03b3$-covering: An Information-Theoretic Framework for Context Engineering", "categories": ["cs.IT", "cs.LG", "math.IT", "stat.ML"], "comment": "15 pages, 6 tables, preprint", "summary": "We introduce \\textbf{Directed Information $\\gamma$-covering}, a simple but\ngeneral framework for redundancy-aware context engineering. Directed\ninformation (DI), a causal analogue of mutual information, measures asymmetric\npredictiveness between chunks. If $\\operatorname{DI}_{i \\to j} \\ge H(C_j) -\n\\gamma$, then $C_i$ suffices to represent $C_j$ up to $\\gamma$ bits. Building\non this criterion, we formulate context selection as a $\\gamma$-cover problem\nand propose a greedy algorithm with provable guarantees: it preserves query\ninformation within bounded slack, inherits $(1+\\ln n)$ and $(1-1/e)$\napproximations from submodular set cover, and enforces a diversity margin.\nImportantly, building the $\\gamma$-cover is \\emph{query-agnostic}: it incurs no\nonline cost and can be computed once offline and amortized across all queries.\nExperiments on HotpotQA show that $\\gamma$-covering consistently improves over\nBM25, a competitive baseline, and provides clear advantages in hard-decision\nregimes such as context compression and single-slot prompt selection. These\nresults establish DI $\\gamma$-covering as a principled, self-organizing\nbackbone for modern LLM pipelines.", "AI": {"tldr": "This paper introduces Directed Information (DI) \u03b3-covering, a framework for redundancy-aware selection in AI, utilizing DI to optimize context representation with efficient algorithms and experimental validation.", "motivation": "The paper aims to develop a framework for efficient and redundancy-aware context selection to improve AI pipelines, addressing the challenges in leveraging mutual information efficiently in real-world applications.", "method": "The authors use Directed Information (DI), a causal version of mutual information, to measure asymmetry in data predictiveness. They approach context selection as a \u03b3-cover problem and propose a greedy algorithm with bounded guarantees, including query-agnostic design.", "result": "The framework outperformed competitive baselines like BM25 in experiments on HotpotQA, especially in scenarios like context compression and single-slot prompt selection, demonstrating its practical applicability.", "conclusion": "DI \u03b3-covering is presented as a robust and flexible framework for modern LLM pipelines, promoting efficient, principled, and redundancy-aware context selection while being computationally cost-effective."}}
{"id": "2510.00062", "pdf": "https://arxiv.org/pdf/2510.00062", "abs": "https://arxiv.org/abs/2510.00062", "authors": ["M. Kokhazadeh", "G. Keramidas", "V. Kelefouras"], "title": "Efficient CNN Compression via Multi-method Low Rank Factorization and Feature Map Similarity", "categories": ["cs.CV", "cs.AI"], "comment": "14 pages, 17 figures, This work has been submitted to the IEEE for\n  possible publication (IEEE Transactions on Artificial Intelligence)", "summary": "Low-Rank Factorization (LRF) is a widely adopted technique for compressing\ndeep neural networks (DNNs). However, it faces several challenges, including\noptimal rank selection, a vast design space, long fine-tuning times, and\nlimited compatibility with different layer types and decomposition methods.\nThis paper presents an end-to-end Design Space Exploration (DSE) methodology\nand framework for compressing convolutional neural networks (CNNs) that\naddresses all these issues. We introduce a novel rank selection strategy based\non feature map similarity, which captures non-linear interactions between layer\noutputs more effectively than traditional weight-based approaches. Unlike prior\nworks, our method uses a one-shot fine-tuning process, significantly reducing\nthe overall fine-tuning time. The proposed framework is fully compatible with\nall types of convolutional (Conv) and fully connected (FC) layers. To further\nimprove compression, the framework integrates three different LRF techniques\nfor Conv layers and three for FC layers, applying them selectively on a\nper-layer basis. We demonstrate that combining multiple LRF methods within a\nsingle model yields better compression results than using a single method\nuniformly across all layers. Finally, we provide a comprehensive evaluation and\ncomparison of the six LRF techniques, offering practical insights into their\neffectiveness across different scenarios. The proposed work is integrated into\nTensorFlow 2.x, ensuring compatibility with widely used deep learning\nworkflows. Experimental results on 14 CNN models across eight datasets\ndemonstrate that the proposed methodology achieves substantial compression with\nminimal accuracy loss, outperforming several state-of-the-art techniques.", "AI": {"tldr": "This paper introduces an end-to-end methodology for compressing CNNs effectively and efficiently using a novel rank selection strategy and one-shot fine-tuning.", "motivation": "Low-Rank Factorization techniques face challenges like rank selection, fine-tuning time, and compatibility across layer types.", "method": "The paper proposes a Design Space Exploration framework integrating multiple LRF techniques and a rank selection based on feature map similarity.", "result": "The methodology achieves substantial compression with minimal accuracy loss, performing better than state-of-the-art methods on 14 CNN models across 8 datasets.", "conclusion": "Combining multiple LRF methods selectively per layer yields superior compression results, with the framework compatible with TensorFlow 2.x for practical use."}}
{"id": "2510.00274", "pdf": "https://arxiv.org/pdf/2510.00274", "abs": "https://arxiv.org/abs/2510.00274", "authors": ["Maisha Maliha", "Dean Hougen"], "title": "MAGIC-MASK: Multi-Agent Guided Inter-Agent Collaboration with Mask-Based Explainability for Reinforcement Learning", "categories": ["cs.AI", "cs.LG", "cs.MA"], "comment": "16 pages, 3 figures", "summary": "Understanding the decision-making process of Deep Reinforcement Learning\nagents remains a key challenge for deploying these systems in safety-critical\nand multi-agent environments. While prior explainability methods like\nStateMask, have advanced the identification of critical states, they remain\nlimited by computational cost, exploration coverage, and lack of adaptation to\nmulti-agent settings. To overcome these limitations, we propose a\nmathematically grounded framework, MAGIC-MASK (Multi-Agent Guided Inter-agent\nCollaboration with Mask-Based Explainability for Reinforcement Learning), that\nextends perturbation-based explanation to Multi-Agent Reinforcement Learning.\nOur method integrates Proximal Policy Optimization, adaptive epsilon-greedy\nexploration, and lightweight inter-agent collaboration to share masked state\ninformation and peer experience. This collaboration enables each agent to\nperform saliency-guided masking and share reward-based insights with peers,\nreducing the time required for critical state discovery, improving explanation\nfidelity, and leading to faster and more robust learning. The core novelty of\nour approach lies in generalizing explainability from single-agent to\nmulti-agent systems through a unified mathematical formalism built on\ntrajectory perturbation, reward fidelity analysis, and Kullback-Leibler\ndivergence regularization. This framework yields localized, interpretable\nexplanations grounded in probabilistic modeling and multi-agent Markov decision\nprocesses. We validate our framework on both single-agent and multi-agent\nbenchmarks, including a multi-agent highway driving environment and Google\nResearch Football, demonstrating that MAGIC-MASK consistently outperforms\nstate-of-the-art baselines in fidelity, learning efficiency, and policy\nrobustness while offering interpretable and transferable explanations.", "AI": {"tldr": "The paper introduces MAGIC-MASK, a framework to enhance explainability in Multi-Agent Reinforcement Learning while addressing computational efficiency and adaptability.", "motivation": "Current explainability methods struggle with computational inefficiency, limited exploration coverage, and lack of suitability for multi-agent environments.", "method": "MAGIC-MASK integrates techniques like Proximal Policy Optimization, adaptive epsilon-greedy exploration, and inter-agent collaboration based on masked state sharing.", "result": "Experiments demonstrate MAGIC-MASK outperforms existing baselines in learning efficiency, policy robustness, and explanation fidelity in both single-agent and multi-agent settings.", "conclusion": "The proposed framework generalizes explainability for multi-agent systems, providing localized, probabilistic explanations while enhancing learning and robustness."}}
{"id": "2510.00144", "pdf": "https://arxiv.org/pdf/2510.00144", "abs": "https://arxiv.org/abs/2510.00144", "authors": ["Shreyas Chaudhari", "Renhao Zhang", "Philip S. Thomas", "Bruno Castro da Silva"], "title": "Which Rewards Matter? Reward Selection for Reinforcement Learning under Limited Feedback", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The ability of reinforcement learning algorithms to learn effective policies\nis determined by the rewards available during training. However, for practical\nproblems, obtaining large quantities of reward labels is often infeasible due\nto computational or financial constraints, particularly when relying on human\nfeedback. When reinforcement learning must proceed with limited feedback --\nonly a fraction of samples get rewards labeled -- a fundamental question\narises: which samples should be labeled to maximize policy performance? We\nformalize this problem of reward selection for reinforcement learning from\nlimited feedback (RLLF), introducing a new problem formulation that facilitates\nthe study of strategies for selecting impactful rewards. Two types of selection\nstrategies are investigated: (i) heuristics that rely on reward-free\ninformation such as state visitation and partial value functions, and (ii)\nstrategies pre-trained using auxiliary evaluative feedback. We find that\ncritical subsets of rewards are those that (1) guide the agent along optimal\ntrajectories, and (2) support recovery toward near-optimal behavior after\ndeviations. Effective selection methods yield near-optimal policies with\nsignificantly fewer reward labels than full supervision, establishing reward\nselection as a powerful paradigm for scaling reinforcement learning in\nfeedback-limited settings.", "AI": {"tldr": "The paper addresses reward selection in reinforcement learning when feedback is limited, exploring strategies to maximize policy performance with fewer labeled samples.", "motivation": "Obtaining large amounts of reward labels for reinforcement learning can be infeasible, prompting the need to optimize reward selection for effective training.", "method": "The authors formalize reward selection for reinforcement learning with limited feedback and investigate heuristic and pre-trained strategies for impactful reward labeling.", "result": "Key findings include identifying impactful rewards that guide optimal trajectories and enable recovery, achieving near-optimal policies with fewer labeled samples.", "conclusion": "Reward selection is a promising strategy for scaling reinforcement learning in limited feedback scenarios, reducing the need for extensive reward labeling."}}
{"id": "2510.00280", "pdf": "https://arxiv.org/pdf/2510.00280", "abs": "https://arxiv.org/abs/2510.00280", "authors": ["Ruochen Li", "Jun Li", "Bailiang Jian", "Kun Yuan", "Youxiang Zhu"], "title": "ReEvalMed: Rethinking Medical Report Evaluation by Aligning Metrics with Real-World Clinical Judgment", "categories": ["cs.CL"], "comment": null, "summary": "Automatically generated radiology reports often receive high scores from\nexisting evaluation metrics but fail to earn clinicians' trust. This gap\nreveals fundamental flaws in how current metrics assess the quality of\ngenerated reports. We rethink the design and evaluation of these metrics and\npropose a clinically grounded Meta-Evaluation framework. We define clinically\ngrounded criteria spanning clinical alignment and key metric capabilities,\nincluding discrimination, robustness, and monotonicity. Using a fine-grained\ndataset of ground truth and rewritten report pairs annotated with error types,\nclinical significance labels, and explanations, we systematically evaluate\nexisting metrics and reveal their limitations in interpreting clinical\nsemantics, such as failing to distinguish clinically significant errors,\nover-penalizing harmless variations, and lacking consistency across error\nseverity levels. Our framework offers guidance for building more clinically\nreliable evaluation methods.", "AI": {"tldr": "This paper highlights flaws in current automated radiology report evaluation metrics and proposes a clinically grounded Meta-Evaluation framework for improvement.", "motivation": "Current metrics for assessing automated radiology reports are inadequate, as they don't align with clinical requirements or accurately interpret clinical semantics.", "method": "The authors introduce a Meta-Evaluation framework built on clinically grounded criteria, supported by a fine-grained dataset of annotated ground truth and rewritten report pairs.", "result": "The evaluation identified critical limitations in existing metrics, such as failing to detect clinically significant errors, over-penalizing harmless variations, and showing inconsistency across error severity levels.", "conclusion": "The proposed framework can help construct more clinically reliable evaluation methods, bridging the gap between metric scores and clinician trust."}}
{"id": "2510.00976", "pdf": "https://arxiv.org/pdf/2510.00976", "abs": "https://arxiv.org/abs/2510.00976", "authors": ["Aueaphum Aueawatthanaphisut"], "title": "Adaptive Federated Few-Shot Rare-Disease Diagnosis with Energy-Aware Secure Aggregation", "categories": ["cs.AI", "cs.CR", "cs.DC", "cs.LG", "q-bio.QM"], "comment": "6 pages, 6 figures, 12 equations, 1 algorithm", "summary": "Rare-disease diagnosis remains one of the most pressing challenges in digital\nhealth, hindered by extreme data scarcity, privacy concerns, and the limited\nresources of edge devices. This paper proposes the Adaptive Federated Few-Shot\nRare-Disease Diagnosis (AFFR) framework, which integrates three pillars: (i)\nfew-shot federated optimization with meta-learning to generalize from limited\npatient samples, (ii) energy-aware client scheduling to mitigate device\ndropouts and ensure balanced participation, and (iii) secure aggregation with\ncalibrated differential privacy to safeguard sensitive model updates. Unlike\nprior work that addresses these aspects in isolation, AFFR unifies them into a\nmodular pipeline deployable on real-world clinical networks. Experimental\nevaluation on simulated rare-disease detection datasets demonstrates up to 10%\nimprovement in accuracy compared with baseline FL, while reducing client\ndropouts by over 50% without degrading convergence. Furthermore,\nprivacy-utility trade-offs remain within clinically acceptable bounds. These\nfindings highlight AFFR as a practical pathway for equitable and trustworthy\nfederated diagnosis of rare conditions.", "AI": {"tldr": "This paper introduces the AFFR framework for rare-disease diagnosis, combining meta-learning, energy-aware scheduling, and secure differential privacy in federated learning to enhance accuracy, balance participation, and protect privacy.", "motivation": "Rare-disease diagnosis faces challenges such as limited data, privacy concerns, and constrained resources on edge devices, necessitating innovative solutions.", "method": "The proposed AFFR framework employs few-shot federated optimization with meta-learning, energy-aware client scheduling, and secure aggregation with calibrated differential privacy in a unified, modular pipeline.", "result": "AFFR achieves up to 10% higher accuracy than baseline federated learning, reduces client dropouts by over 50%, and maintains acceptable privacy-utility trade-offs.", "conclusion": "AFFR is a practical and efficient framework for trustworthy federated rare-disease diagnosis, addressing key challenges in scalability, privacy, and robustness."}}
{"id": "2510.00532", "pdf": "https://arxiv.org/pdf/2510.00532", "abs": "https://arxiv.org/abs/2510.00532", "authors": ["Hengcheng Zhu", "Songqiang Chen", "Valerio Terragni", "Lili Wei", "Jiarong Wu", "Yepang Liu", "Shing-Chi Cheung"], "title": "LSPFuzz: Hunting Bugs in Language Servers", "categories": ["cs.SE", "cs.CR", "D.2.5"], "comment": "This paper has been accepted for publication in The 40th IEEE/ACM\n  International Conference on Automated Software Engineering (ASE 2025)", "summary": "The Language Server Protocol (LSP) has revolutionized the integration of code\nintelligence in modern software development. There are approximately 300 LSP\nserver implementations for various languages and 50 editors offering LSP\nintegration. However, the reliability of LSP servers is a growing concern, as\ncrashes can disable all code intelligence features and significantly impact\nproductivity, while vulnerabilities can put developers at risk even when\nediting untrusted source code. Despite the widespread adoption of LSP, no\nexisting techniques specifically target LSP server testing. To bridge this gap,\nwe present LSPFuzz, a grey-box hybrid fuzzer for systematic LSP server testing.\nOur key insight is that effective LSP server testing requires holistic mutation\nof source code and editor operations, as bugs often manifest from their\ncombinations. To satisfy the sophisticated constraints of LSP and effectively\nexplore the input space, we employ a two-stage mutation pipeline: syntax-aware\nmutations to source code, followed by context-aware dispatching of editor\noperations. We evaluated LSPFuzz on four widely used LSP servers. LSPFuzz\ndemonstrated superior performance compared to baseline fuzzers, and uncovered\npreviously unknown bugs in real-world LSP servers. Of the 51 bugs we reported,\n42 have been confirmed, 26 have been fixed by developers, and two have been\nassigned CVE numbers. Our work advances the quality assurance of LSP servers,\nproviding both a practical tool and foundational insights for future research\nin this domain.", "AI": {"tldr": "This paper introduces LSPFuzz, a testing tool designed to detect bugs in Language Server Protocol (LSP) servers through a hybrid fuzzer approach.", "motivation": "The paper addresses the growing concerns around reliability and security in LSP servers, which can impact developer productivity and pose risks when handling untrusted code.", "method": "LSPFuzz employs a two-stage mutation pipeline: first, syntax-aware mutations to source code, followed by context-aware dispatching of editor operations, enabling a systematic exploration of the input space.", "result": "Evaluation on four widely used LSP servers revealed 51 bugs, out of which 42 were confirmed, 26 fixed, and 2 assigned CVE numbers. LSPFuzz outperformed baseline fuzzers.", "conclusion": "LSPFuzz enhances the quality assurance of LSP servers, offering both a practical tool for developers and insights for future research in LSP server reliability and security."}}
{"id": "2510.00524", "pdf": "https://arxiv.org/pdf/2510.00524", "abs": "https://arxiv.org/abs/2510.00524", "authors": ["Baoshan Song", "Penggao Yan", "Xiao Xia", "Yihan Zhong", "Weisong Wen", "Li-Ta Hsu"], "title": "Two stage GNSS outlier detection for factor graph optimization based GNSS-RTK/INS/odometer fusion", "categories": ["cs.RO"], "comment": null, "summary": "Reliable GNSS positioning in complex environments remains a critical\nchallenge due to non-line-of-sight (NLOS) propagation, multipath effects, and\nfrequent signal blockages. These effects can easily introduce large outliers\ninto the raw pseudo-range measurements, which significantly degrade the\nperformance of global navigation satellite system (GNSS) real-time kinematic\n(RTK) positioning and limit the effectiveness of tightly coupled GNSS-based\nintegrated navigation system. To address this issue, we propose a two-stage\noutlier detection method and apply the method in a tightly coupled GNSS-RTK,\ninertial navigation system (INS), and odometer integration based on factor\ngraph optimization (FGO). In the first stage, Doppler measurements are employed\nto detect pseudo-range outliers in a GNSS-only manner, since Doppler is less\nsensitive to multipath and NLOS effects compared with pseudo-range, making it a\nmore stable reference for detecting sudden inconsistencies. In the second\nstage, pre-integrated inertial measurement units (IMU) and odometer constraints\nare used to generate predicted double-difference pseudo-range measurements,\nwhich enable a more refined identification and rejection of remaining outliers.\nBy combining these two complementary stages, the system achieves improved\nrobustness against both gross pseudo-range errors and degraded satellite\nmeasuring quality. The experimental results demonstrate that the two-stage\ndetection framework significantly reduces the impact of pseudo-range outliers,\nand leads to improved positioning accuracy and consistency compared with\nrepresentative baseline approaches. In the deep urban canyon test, the outlier\nmitigation method has limits the RMSE of GNSS-RTK/INS/odometer fusion from 0.52\nm to 0.30 m, with 42.3% improvement.", "AI": {"tldr": "The paper proposes a two-stage outlier detection method for GNSS positioning in complex environments using Doppler and IMU/odometer constraints, achieving significantly improved accuracy in urban scenarios.", "motivation": "Enhance GNSS positioning reliability in challenging environments plagued by NLOS, multipath, and signal blockages, to optimize integrated navigation systems.", "method": "A two-stage approach: Doppler measurements detect pseudo-range inconsistencies, then IMU and odometer constraints refine outlier detection using predicted double-difference values.", "result": "The proposed method significantly reduced pseudo-range errors, improving positioning accuracy, achieving a 42.3% improvement in RMSE in deep urban canyon scenarios.", "conclusion": "The framework enhances robustness and positioning consistency, marking a notable advancement over baseline methods in urban navigation challenges."}}
{"id": "2510.00067", "pdf": "https://arxiv.org/pdf/2510.00067", "abs": "https://arxiv.org/abs/2510.00067", "authors": ["Rafael da Silva Maciel", "Lucio Veraldo Jr"], "title": "Intelligent 5S Audit: Application of Artificial Intelligence for Continuous Improvement in the Automotive Industry", "categories": ["cs.CV", "cs.AI", "cs.HC", "68T05, 90B30", "I.2.1; H.4.2; J.6"], "comment": "8 pages, 5 figures, 5 tables", "summary": "The evolution of the 5S methodology with the support of artificial\nintelligence techniques represents a significant opportunity to improve\nindustrial organization audits in the automotive chain, making them more\nobjective, efficient and aligned with Industry 4.0 standards. This work\ndeveloped an automated 5S audit system based on large-scale language models\n(LLM), capable of assessing the five senses (Seiri, Seiton, Seiso, Seiketsu,\nShitsuke) in a standardized way through intelligent image analysis. The\nsystem's reliability was validated using Cohen's concordance coefficient (kappa\n= 0.75), showing strong alignment between the automated assessments and the\ncorresponding human audits. The results indicate that the proposed solution\ncontributes significantly to continuous improvement in automotive manufacturing\nenvironments, speeding up the audit process by 50% of the traditional time and\nmaintaining the consistency of the assessments, with a 99.8% reduction in\noperating costs compared to traditional manual audits. The methodology\npresented establishes a new paradigm for integrating lean systems with emerging\nAI technologies, offering scalability for implementation in automotive plants\nof different sizes.", "AI": {"tldr": "The paper introduces an AI-driven automated 5S audit system for industrial audits in the automotive industry, enhancing efficiency, objectivity, and alignment with Industry 4.0.", "motivation": "Improve industrial organization audits by making them faster, objective, cost-efficient, and aligned with modern Industry 4.0 standards.", "method": "Developed an automated 5S audit system leveraging large-scale language models, incorporating intelligent image analysis to assess the traditional 5S methodology.", "result": "The system showed strong reliability (Cohen\u2019s kappa = 0.75), reduced audit time by 50%, and achieved a 99.8% reduction in operational costs while ensuring consistent assessments.", "conclusion": "This AI-driven methodology contributes significantly to the automotive industry by enabling scalable, efficient, and reliable implementation of lean systems."}}
{"id": "2510.00300", "pdf": "https://arxiv.org/pdf/2510.00300", "abs": "https://arxiv.org/abs/2510.00300", "authors": ["Serena Gomez Wannaz"], "title": "ICL Optimized Fragility", "categories": ["cs.AI"], "comment": null, "summary": "ICL guides are known to improve task-specific performance, but their impact\non cross-domain cognitive abilities remains unexplored. This study examines how\nICL guides affect reasoning across different knowledge domains using six\nvariants of the GPT-OSS:20b model: one baseline model and five ICL\nconfigurations (simple, chain-of-thought, random, appended text, and symbolic\nlanguage). The models were subjected to 840 tests spanning general knowledge\nquestions, logic riddles, and a mathematical olympiad problem. Statistical\nanalysis (ANOVA) revealed significant behavioral modifications (p less than\n0.001) across ICL variants, demonstrating a phenomenon termed \"optimized\nfragility.\" ICL models achieved 91%-99% accuracy on general knowledge tasks\nwhile showing degraded performance on complex reasoning problems, with accuracy\ndropping to 10-43% on riddles compared to 43% for the baseline model. Notably,\nno significant differences emerged on the olympiad problem (p=0.2173),\nsuggesting that complex mathematical reasoning remains unaffected by ICL\noptimization. These findings indicate that ICL guides create systematic\ntrade-offs between efficiency and reasoning flexibility, with important\nimplications for LLM deployment and AI safety.", "AI": {"tldr": "The study examines how In-Context Learning (ICL) guides alter reasoning abilities across domains in GPT-OSS:20b models, revealing higher accuracy in general knowledge but reduced reasoning flexibility in complex tasks.", "motivation": "Assess the impact of ICL guides on cross-domain reasoning abilities in AI models.", "method": "Six GPT-OSS:20b variants were tested on 840 cross-domain tasks, analyzed using statistical methods like ANOVA.", "result": "ICL guides improved accuracy in general knowledge (91%-99%) but degraded reasoning abilities in riddles (10-43%) and maintained baseline-level performance in math olympiad problems.", "conclusion": "ICL trade-offs exist, improving efficiency but compromising reasoning flexibility, with implications for AI deployment and safety."}}
{"id": "2510.00163", "pdf": "https://arxiv.org/pdf/2510.00163", "abs": "https://arxiv.org/abs/2510.00163", "authors": ["Saeyoung Rho", "Junzhe Zhang", "Elias Bareinboim"], "title": "Partial Identification Approach to Counterfactual Fairness Assessment", "categories": ["cs.LG", "cs.AI", "cs.CY", "stat.ME"], "comment": null, "summary": "The wide adoption of AI decision-making systems in critical domains such as\ncriminal justice, loan approval, and hiring processes has heightened concerns\nabout algorithmic fairness. As we often only have access to the output of\nalgorithms without insights into their internal mechanisms, it was natural to\nexamine how decisions would alter when auxiliary sensitive attributes (such as\nrace) change. This led the research community to come up with counterfactual\nfairness measures, but how to evaluate the measure from available data remains\na challenging task. In many practical applications, the target counterfactual\nmeasure is not identifiable, i.e., it cannot be uniquely determined from the\ncombination of quantitative data and qualitative knowledge. This paper\naddresses this challenge using partial identification, which derives\ninformative bounds over counterfactual fairness measures from observational\ndata. We introduce a Bayesian approach to bound unknown counterfactual fairness\nmeasures with high confidence. We demonstrate our algorithm on the COMPAS\ndataset, examining fairness in recidivism risk scores with respect to race,\nage, and sex. Our results reveal a positive (spurious) effect on the COMPAS\nscore when changing race to African-American (from all others) and a negative\n(direct causal) effect when transitioning from young to old age.", "AI": {"tldr": "The paper tackles the challenge of evaluating counterfactual fairness measures in AI systems through a Bayesian partial identification approach, revealing biases in the COMPAS dataset.", "motivation": "The paper is motivated by the growing concerns about the fairness of AI decision-making systems in critical areas where decisions have significant societal impact, especially given the difficulty in understanding how sensitive attributes affect algorithmic outcomes.", "method": "The authors employ partial identification to derive bounds over counterfactual fairness measures and propose a Bayesian algorithm to compute these bounds using observational data.", "result": "The findings show a spurious positive effect on COMPAS risk scores when race changes to African-American and a direct causal negative effect when age transitions from young to old.", "conclusion": "The paper concludes that the proposed approach provides a robust framework for evaluating counterfactual fairness measures, offering insights into algorithmic biases such as those present in the COMPAS dataset."}}
{"id": "2510.00288", "pdf": "https://arxiv.org/pdf/2510.00288", "abs": "https://arxiv.org/abs/2510.00288", "authors": ["\u013dubo\u0161 Kri\u0161", "Jaroslav Kop\u010dan", "Qiwei Peng", "Andrej Ridzik", "Marcel Vesel\u00fd", "Martin Tamajka"], "title": "o-MEGA: Optimized Methods for Explanation Generation and Analysis", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The proliferation of transformer-based language models has revolutionized NLP\ndomain while simultaneously introduced significant challenges regarding model\ntransparency and trustworthiness. The complexity of achieving explainable\nsystems in this domain is evidenced by the extensive array of explanation\nmethods and evaluation metrics developed by researchers. To address the\nchallenge of selecting optimal explainability approaches, we present\n\\textbf{\\texttt{o-mega}}, a hyperparameter optimization tool designed to\nautomatically identify the most effective explainable AI methods and their\nconfigurations within the semantic matching domain. We evaluate o-mega on a\npost-claim matching pipeline using a curated dataset of social media posts\npaired with refuting claims. Our tool systematically explores different\nexplainable methods and their hyperparameters, demonstrating improved\ntransparency in automated fact-checking systems. As a result, such automated\noptimization of explanation methods can significantly enhance the\ninterpretability of claim-matching models in critical applications such as\nmisinformation detection, contributing to more trustworthy and transparent AI\nsystems.", "AI": {"tldr": "o-mega is a tool designed to optimize explainable AI methods for semantic matching, improving transparency in fact-checking systems.", "motivation": "The challenge in achieving transparency and trust in transformer-based NLP models and the need to select optimal explainability methods.", "method": "Introduction of o-mega, a tool that uses hyperparameter optimization to identify effective explainable AI methods for semantic matching.", "result": "o-mega demonstrated improved transparency in fact-checking pipelines by systematically optimizing explainability methods and their configurations.", "conclusion": "Automated optimization of explanation methods using tools like o-mega can enhance trustworthiness and transparency in AI systems for misinformation detection."}}
{"id": "2510.00591", "pdf": "https://arxiv.org/pdf/2510.00591", "abs": "https://arxiv.org/abs/2510.00591", "authors": ["Liyi Cai", "Yijie Ren", "Yitong Zhang", "Jia Li"], "title": "AI-Driven Self-Evolving Software: A Promising Path Toward Software Automation", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Software automation has long been a central goal of software engineering,\nstriving for software development that proceeds without human intervention.\nRecent efforts have leveraged Artificial Intelligence (AI) to advance software\nautomation with notable progress. However, current AI functions primarily as\nassistants to human developers, leaving software development still dependent on\nexplicit human intervention. This raises a fundamental question: Can AI move\nbeyond its role as an assistant to become a core component of software, thereby\nenabling genuine software automation? To investigate this vision, we introduce\nAI-Driven Self-Evolving Software, a new form of software that evolves\ncontinuously through direct interaction with users. We demonstrate the\nfeasibility of this idea with a lightweight prototype built on a multi-agent\narchitecture that autonomously interprets user requirements, generates and\nvalidates code, and integrates new functionalities. Case studies across\nmultiple representative scenarios show that the prototype can reliably\nconstruct and reuse functionality, providing early evidence that such software\nsystems can scale to more sophisticated applications and pave the way toward\ntruly automated software development. We make code and cases in this work\npublicly available at https://anonymous.4open.science/r/live-software.", "AI": {"tldr": "The paper introduces AI-Driven Self-Evolving Software, a new form designed to autonomously evolve through user interactions without human developers. The researchers showcase a prototype capable of interpreting requirements, generating code, and handling functionalities.", "motivation": "Existing AI in software development primarily assists human developers, and the paper explores stepping beyond to achieve fully autonomous software development.", "method": "The authors developed a lightweight multi-agent architecture prototype that can autonomously process user requirements, create and validate code, and implement new features.", "result": "The prototype demonstrated reliable performance in multiple case studies, showing promise for scaling to more complex tasks while highlighting the potential for automating software development.", "conclusion": "AI-Driven Self-Evolving Software offers evidence that software can evolve autonomously, paving the way for fully automated software development with minimal human intervention."}}
{"id": "2510.00573", "pdf": "https://arxiv.org/pdf/2510.00573", "abs": "https://arxiv.org/abs/2510.00573", "authors": ["Yen-Ling Tai", "Yi-Ru Yang", "Kuan-Ting Yu", "Yu-Wei Chao", "Yi-Ting Chen"], "title": "GRITS: A Spillage-Aware Guided Diffusion Policy for Robot Food Scooping Tasks", "categories": ["cs.RO"], "comment": null, "summary": "Robotic food scooping is a critical manipulation skill for food preparation\nand service robots. However, existing robot learning algorithms, especially\nlearn-from-demonstration methods, still struggle to handle diverse and dynamic\nfood states, which often results in spillage and reduced reliability. In this\nwork, we introduce GRITS: A Spillage-Aware Guided Diffusion Policy for Robot\nFood Scooping Tasks. This framework leverages guided diffusion policy to\nminimize food spillage during scooping and to ensure reliable transfer of food\nitems from the initial to the target location. Specifically, we design a\nspillage predictor that estimates the probability of spillage given current\nobservation and action rollout. The predictor is trained on a simulated dataset\nwith food spillage scenarios, constructed from four primitive shapes (spheres,\ncubes, cones, and cylinders) with varied physical properties such as mass,\nfriction, and particle size. At inference time, the predictor serves as a\ndifferentiable guidance signal, steering the diffusion sampling process toward\nsafer trajectories while preserving task success. We validate GRITS on a\nreal-world robotic food scooping platform. GRITS is trained on six food\ncategories and evaluated on ten unseen categories with different shapes and\nquantities. GRITS achieves an 82% task success rate and a 4% spillage rate,\nreducing spillage by over 40% compared to baselines without guidance, thereby\ndemonstrating its effectiveness.", "AI": {"tldr": "This paper presents GRITS, a framework using guided diffusion policy to minimize food spillage in robotic food scooping tasks, significantly improving task success and reliability.", "motivation": "Current robot learning algorithms struggle with diverse and dynamic food states, often causing spillage and reliability issues during manipulation.", "method": "The proposed method involves a guided diffusion policy aided by a spillage predictor trained on simulated datasets, which guides robot actions to reduce spillage while achieving task success.", "result": "GRITS achieved an 82% task success rate and a 4% spillage rate in real-world tests, reducing spillage by over 40% compared to unguided baselines.", "conclusion": "GRITS successfully demonstrates enhanced reliability and minimized spillage in robotic food scooping, making it a promising approach for food preparation and service robots."}}
{"id": "2510.00069", "pdf": "https://arxiv.org/pdf/2510.00069", "abs": "https://arxiv.org/abs/2510.00069", "authors": ["Jiancong Xie", "Wenjin Wang", "Zhuomeng Zhang", "Zihan Liu", "Qi Liu", "Ke Feng", "Zixun Sun", "Yuedong Yang"], "title": "OIG-Bench: A Multi-Agent Annotated Benchmark for Multimodal One-Image Guides Understanding", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in Multimodal Large Language Models (MLLMs) have demonstrated\nimpressive capabilities. However, evaluating their capacity for human-like\nunderstanding in One-Image Guides remains insufficiently explored. One-Image\nGuides are a visual format combining text, imagery, and symbols to present\nreorganized and structured information for easier comprehension, which are\nspecifically designed for human viewing and inherently embody the\ncharacteristics of human perception and understanding. Here, we present\nOIG-Bench, a comprehensive benchmark focused on One-Image Guide understanding\nacross diverse domains. To reduce the cost of manual annotation, we developed a\nsemi-automated annotation pipeline in which multiple intelligent agents\ncollaborate to generate preliminary image descriptions, assisting humans in\nconstructing image-text pairs. With OIG-Bench, we have conducted a\ncomprehensive evaluation of 29 state-of-the-art MLLMs, including both\nproprietary and open-source models. The results show that Qwen2.5-VL-72B\nperforms the best among the evaluated models, with an overall accuracy of 77%.\nNevertheless, all models exhibit notable weaknesses in semantic understanding\nand logical reasoning, indicating that current MLLMs still struggle to\naccurately interpret complex visual-text relationships. In addition, we also\ndemonstrate that the proposed multi-agent annotation system outperforms all\nMLLMs in image captioning, highlighting its potential as both a high-quality\nimage description generator and a valuable tool for future dataset\nconstruction. Datasets are available at https://github.com/XiejcSYSU/OIG-Bench.", "AI": {"tldr": "The paper introduces OIG-Bench, a benchmark for assessing the ability of Multimodal Large Language Models (MLLMs) to understand \"One-Image Guides\"\u2014a structured visual format combining text and images. Evaluating 29 state-of-the-art models, Qwen2.5-VL-72B performs best but reveals overall weaknesses in semantic understanding and reasoning.", "motivation": "Existing benchmarks do not sufficiently assess the ability of MLLMs to understand \"One-Image Guides,\" which are specifically designed to match human comprehension combining text, imagery, and symbols.", "method": "The paper develops OIG-Bench, a benchmark, and employs a semi-automated annotation pipeline combining intelligent agents with human oversight to generate image-text pairs. It evaluates 29 models against the benchmark.", "result": "Qwen2.5-VL-72B achieves the highest accuracy in the benchmark (77%), but all models display notable weaknesses in semantic and logical reasoning tasks. The multi-agent annotation system surpasses all evaluated MLLMs in image captioning quality.", "conclusion": "Current MLLMs show limitations in interpreting complex relationships in visual-text formats like \"One-Image Guides.\" The multi-agent annotation system emerges as a promising tool for generating high-quality image descriptions and aiding dataset development."}}
{"id": "2510.00307", "pdf": "https://arxiv.org/pdf/2510.00307", "abs": "https://arxiv.org/abs/2510.00307", "authors": ["Thierry Blankenstein", "Jialin Yu", "Zixuan Li", "Vassilis Plachouras", "Sunando Sengupta", "Philip Torr", "Yarin Gal", "Alasdair Paren", "Adel Bibi"], "title": "BiasBusters: Uncovering and Mitigating Tool Selection Bias in Large Language Models", "categories": ["cs.AI"], "comment": null, "summary": "Agents backed by large language models (LLMs) often rely on external tools\ndrawn from marketplaces where multiple providers offer functionally equivalent\noptions. This raises a critical point concerning fairness: if selection is\nsystematically biased, it can degrade user experience and distort competition\nby privileging some providers over others. We introduce a benchmark of diverse\ntool categories, each containing multiple functionally equivalent tools, to\nevaluate tool-selection bias. Using this benchmark, we test seven models and\nshow that unfairness exists with models either fixating on a single provider or\ndisproportionately preferring earlier-listed tools in context. To investigate\nthe origins of this bias, we conduct controlled experiments examining tool\nfeatures, metadata (name, description, parameters), and pre-training exposure.\nWe find that: (1) semantic alignment between queries and metadata is the\nstrongest predictor of choice; (2) perturbing descriptions significantly shifts\nselections; and (3) repeated pre-training exposure to a single endpoint\namplifies bias. Finally, we propose a lightweight mitigation that first filters\nthe candidate tools to a relevant subset and then samples uniformly, reducing\nbias while preserving good task coverage. Our findings highlight tool-selection\nbias as a key obstacle for the fair deployment of tool-augmented LLMs.", "AI": {"tldr": "The paper investigates selection bias in large language models (LLMs) relying on external tools and proposes a mitigation strategy.", "motivation": "To address fairness issues in LLMs that select external tools, where biased selection could degrade user experience and distort competition.", "method": "The authors utilized a benchmark of diverse tool categories and conducted experiments to evaluate selection bias across seven models, analyzing tool features, metadata, and pre-training exposure.", "result": "Key results show that semantic query-metadata alignment is the strongest factor influencing selection, perturbing metadata impacts bias, and pre-training exposure to specific tools amplifies bias. A lightweight filtering and sampling mitigation method reduces this bias.", "conclusion": "Tool-selection bias poses significant fairness challenges for LLMs, but targeted mitigation strategies can improve fairness and maintain task performance."}}
{"id": "2510.00184", "pdf": "https://arxiv.org/pdf/2510.00184", "abs": "https://arxiv.org/abs/2510.00184", "authors": ["Xiaoyan Bai", "Itamar Pres", "Yuntian Deng", "Chenhao Tan", "Stuart Shieber", "Fernanda Vi\u00e9gas", "Martin Wattenberg", "Andrew Lee"], "title": "Why Can't Transformers Learn Multiplication? Reverse-Engineering Reveals Long-Range Dependency Pitfalls", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Language models are increasingly capable, yet still fail at a seemingly\nsimple task of multi-digit multiplication. In this work, we study why, by\nreverse-engineering a model that successfully learns multiplication via\n\\emph{implicit chain-of-thought}, and report three findings: (1) Evidence of\nlong-range structure: Logit attributions and linear probes indicate that the\nmodel encodes the necessary long-range dependencies for multi-digit\nmultiplication. (2) Mechanism: the model encodes long-range dependencies using\nattention to construct a directed acyclic graph to ``cache'' and ``retrieve''\npairwise partial products. (3) Geometry: the model implements partial products\nin attention heads by forming Minkowski sums between pairs of digits, and\ndigits are represented using a Fourier basis, both of which are intuitive and\nefficient representations that the standard fine-tuning model lacks. With these\ninsights, we revisit the learning dynamics of standard fine-tuning and find\nthat the model converges to a local optimum that lacks the required long-range\ndependencies. We further validate this understanding by introducing an\nauxiliary loss that predicts the ``running sum'' via a linear regression probe,\nwhich provides an inductive bias that enables the model to successfully learn\nmulti-digit multiplication. In summary, by reverse-engineering the mechanisms\nof an implicit chain-of-thought model we uncover a pitfall for learning\nlong-range dependencies in Transformers and provide an example of how the\ncorrect inductive bias can address this issue.", "AI": {"tldr": "This research examines why language models struggle with multi-digit multiplication, uncovering mechanisms for encoding long-range dependencies, and proposing inductive bias for optimization.", "motivation": "To understand why language models fail at tasks like multi-digit multiplication despite their increasing capabilities.", "method": "Reverse-engineering a model that successfully uses implicit chain-of-thought to learn multi-digit multiplication, and introducing auxiliary losses for validation.", "result": "The study revealed mechanisms like attention graphs and Fourier basis representations crucial for handling long-range dependencies, which standard fine-tuning models fail to achieve.", "conclusion": "Long-range dependencies are vital for multi-digit multiplication; proper inductive biases can overcome local optima limitations in Transformer models."}}
{"id": "2510.00311", "pdf": "https://arxiv.org/pdf/2510.00311", "abs": "https://arxiv.org/abs/2510.00311", "authors": ["Bowen Wei", "Yuan Shen Tay", "Howard Liu", "Jinhao Pan", "Kun Luo", "Ziwei Zhu", "Chris Jordan"], "title": "CORTEX: Collaborative LLM Agents for High-Stakes Alert Triage", "categories": ["cs.CL"], "comment": null, "summary": "Security Operations Centers (SOCs) are overwhelmed by tens of thousands of\ndaily alerts, with only a small fraction corresponding to genuine attacks. This\noverload creates alert fatigue, leading to overlooked threats and analyst\nburnout. Classical detection pipelines are brittle and context-poor, while\nrecent LLM-based approaches typically rely on a single model to interpret logs,\nretrieve context, and adjudicate alerts end-to-end -- an approach that\nstruggles with noisy enterprise data and offers limited transparency. We\npropose CORTEX, a multi-agent LLM architecture for high-stakes alert triage in\nwhich specialized agents collaborate over real evidence: a behavior-analysis\nagent inspects activity sequences, evidence-gathering agents query external\nsystems, and a reasoning agent synthesizes findings into an auditable decision.\nTo support training and evaluation, we release a dataset of fine-grained SOC\ninvestigations from production environments, capturing step-by-step analyst\nactions and linked tool outputs. Across diverse enterprise scenarios, CORTEX\nsubstantially reduces false positives and improves investigation quality over\nstate-of-the-art single-agent LLMs.", "AI": {"tldr": "The paper introduces CORTEX, a multi-agent LLM architecture to alleviate alert fatigue in SOCs by improving investigation accuracy and reducing false positives.", "motivation": "SOCs face alert fatigue due to a high volume of daily alerts with limited genuine threats, leading to missed attacks and analyst burnout.", "method": "CORTEX employs specialized agents: behavior-analysis, evidence-gathering, and reasoning agents working collaboratively to analyze evidence. The paper also introduces a dataset reflecting SOC investigations.", "result": "CORTEX demonstrated a significant reduction in false positives and enhanced the quality of investigations compared to single-agent LLM systems across various enterprise scenarios.", "conclusion": "CORTEX offers a more transparent and resilient solution to alert triage than previous approaches, improving SOC efficiency and threat detection accuracy."}}
{"id": "2510.00674", "pdf": "https://arxiv.org/pdf/2510.00674", "abs": "https://arxiv.org/abs/2510.00674", "authors": ["Konstantinos Karakatsanis", "Georgios Alexopoulos", "Ioannis Karyotakis", "Foivos Timotheos Proestakis", "Evangelos Talos", "Panos Louridas", "Dimitris Mitropoulos"], "title": "PyTrim: A Practical Tool for Reducing Python Dependency Bloat", "categories": ["cs.SE"], "comment": "Accepted at ASE 2025 (Tool Demonstration Track)", "summary": "Dependency bloat is a persistent challenge in Python projects, which\nincreases maintenance costs and security risks. While numerous tools exist for\ndetecting unused dependencies in Python, removing these dependencies across the\nsource code and configuration files of a project requires manual effort and\nexpertise.\n  To tackle this challenge we introduce PYTRIM, an end-to-end system to\nautomate this process. PYTRIM eliminates unused imports and package\ndeclarations across a variety of file types, including Python source and\nconfiguration files such as requirements.txt and setup.py. PYTRIM's modular\ndesign makes it agnostic to the source of dependency bloat information,\nenabling integration with any detection tool. Beyond its contribution when it\ncomes to automation, PYTRIM also incorporates a novel dynamic analysis\ncomponent that improves dependency detection recall.\n  Our evaluation of PYTRIM's end-to-end effectiveness on a ground-truth dataset\nof 37 merged pull requests from prior work, shows that PYTRIM achieves 98.3%\naccuracy in replicating human-made changes. To show its practical impact, we\nrun PYTRIM on 971 open-source packages, identifying and trimming bloated\ndependencies in 39 of them. For each case, we submit a corresponding pull\nrequest, 6 of which have already been accepted and merged. PYTRIM is available\nas an open-source project, encouraging community contributions and further\ndevelopment.\n  Video demonstration: https://youtu.be/LqTEdOUbJRI\n  Code repository: https://github.com/TrimTeam/PyTrim", "AI": {"tldr": "The paper introduces PYTRIM, an automated system for removing unused dependencies in Python projects. It improves dependency management accuracy and reduces manual efforts.", "motivation": "Dependency bloat in Python projects increases maintenance and security issues, necessitating tools for efficient detection and removal of unused dependencies.", "method": "PYTRIM automates the detection and removal of unused imports and package declarations in Python source and configuration files. It incorporates a dynamic analysis component to improve recall and is designed modularly for compatibility with any detection tool.", "result": "Tests on human-made changes show PYTRIM achieves 98.3% accuracy. Out of 971 tested open-source packages, 39 had bloated dependencies removed, with 6 pull requests being merged.", "conclusion": "PYTRIM effectively automates the removal of dependency bloat, facilitates reproducibility, and is provided as an open-source tool for community use and advancement."}}
{"id": "2510.00600", "pdf": "https://arxiv.org/pdf/2510.00600", "abs": "https://arxiv.org/abs/2510.00600", "authors": ["Pietro Mazzaglia", "Cansu Sancaktar", "Markus Peschl", "Daniel Dijkman"], "title": "Hybrid Training for Vision-Language-Action Models", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Using Large Language Models to produce intermediate thoughts, a.k.a.\nChain-of-thought (CoT), before providing an answer has been a successful recipe\nfor solving complex language tasks. In robotics, similar embodied CoT\nstrategies, generating thoughts before actions, have also been shown to lead to\nimproved performance when using Vision-Language-Action models (VLAs). As these\ntechniques increase the length of the model's generated outputs to include the\nthoughts, the inference time is negatively affected. Delaying an agent's\nactions in real-world executions, as in robotic manipulation settings, strongly\naffects the usability of a method, as tasks require long sequences of actions.\nHowever, is the generation of long chains-of-thought a strong prerequisite for\nachieving performance improvements? In this work, we explore the idea of Hybrid\nTraining (HyT), a framework that enables VLAs to learn from thoughts and\nbenefit from the associated performance gains, while enabling the possibility\nto leave out CoT generation during inference. Furthermore, by learning to\nconditionally predict a diverse set of outputs, HyT supports flexibility at\ninference time, enabling the model to either predict actions directly, generate\nthoughts or follow instructions. We evaluate the proposed method in a series of\nsimulated benchmarks and real-world experiments.", "AI": {"tldr": "The paper introduces Hybrid Training (HyT), a framework aimed at enabling Vision-Language-Action models (VLAs) to benefit from chain-of-thought (CoT) strategies without requiring CoT generation during inference, thus improving usability and efficiency in robotic applications.", "motivation": "The authors aim to address the inefficiency caused by the need for generating long chains-of-thought during the inference process in robotics, which negatively impacts action latency in real-world tasks.", "method": "The researchers propose Hybrid Training (HyT), which trains VLAs to learn from CoT strategies while allowing inference-time flexibility. This includes diverse modes of action prediction such as direct action, thoughts generation, or instruction following.", "result": "Experiments using simulation benchmarks as well as real-world tests showed that HyT improves performance while removing the bottleneck of extended inference time caused by CoT generation.", "conclusion": "HyT offers a practical solution by enabling flexible inference methods in robotic systems, preserving the benefits of CoT strategies while improving real-world usability through reduced latency."}}
{"id": "2510.00309", "pdf": "https://arxiv.org/pdf/2510.00309", "abs": "https://arxiv.org/abs/2510.00309", "authors": ["Zhongxuan Liu", "Yue Kang", "Thomas C. M. Lee"], "title": "Lipschitz Bandits with Stochastic Delayed Feedback", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "The Lipschitz bandit problem extends stochastic bandits to a continuous\naction set defined over a metric space, where the expected reward function\nsatisfies a Lipschitz condition. In this work, we introduce a new problem of\nLipschitz bandit in the presence of stochastic delayed feedback, where the\nrewards are not observed immediately but after a random delay. We consider both\nbounded and unbounded stochastic delays, and design algorithms that attain\nsublinear regret guarantees in each setting. For bounded delays, we propose a\ndelay-aware zooming algorithm that retains the optimal performance of the\ndelay-free setting up to an additional term that scales with the maximal delay\n$\\tau_{\\max}$. For unbounded delays, we propose a novel phased learning\nstrategy that accumulates reliable feedback over carefully scheduled intervals,\nand establish a regret lower bound showing that our method is nearly optimal up\nto logarithmic factors. Finally, we present experimental results to demonstrate\nthe efficiency of our algorithms under various delay scenarios.", "AI": {"tldr": "The paper extends the Lipschitz bandit problem to account for stochastic delayed feedback in reward observations, proposing algorithms for bounded and unbounded delays.", "motivation": "To address the challenge of stochastic delayed feedback in continuous action sets where rewards are governed by Lipschitz conditions.", "method": "The bounded delays are addressed using a delay-aware zooming algorithm, while unbounded delays are tackled with a phased learning strategy. Regret guarantees and lower bounds are established.", "result": "The algorithms achieve sublinear regret in both bounded and unbounded delay settings, demonstrating near-optimal performance for unbounded delays.", "conclusion": "The proposed solutions effectively handle delayed feedback in Lipschitz bandits, validated through theoretical guarantees and experimental results."}}
{"id": "2510.00072", "pdf": "https://arxiv.org/pdf/2510.00072", "abs": "https://arxiv.org/abs/2510.00072", "authors": ["Chenhui Xu", "Fuxun Yu", "Michael J. Bianco", "Jacob Kovarskiy", "Raphael Tang", "Qi Zhang", "Zirui Xu", "Will LeVine", "Brandon Dubbs", "Heming Liao", "Cassandra Burgess", "Suvam Bag", "Jay Patravali", "Rupanjali Kukal", "Mikael Figueroa", "Rishi Madhok", "Nikolaos Karianakis", "Jinjun Xiong"], "title": "Geo-R1: Unlocking VLM Geospatial Reasoning with Cross-View Reinforcement Learning", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "We introduce Geo-R1, a reasoning-centric post-training framework that unlocks\ngeospatial reasoning in vision-language models by combining thinking\nscaffolding and elevating. In the scaffolding stage, Geo-R1 instills a\n``geospatial thinking paradigm\" via supervised fine-tuning on synthetic\nchain-of-thought exemplars, enabling models to connect visual cues with\ngeographic priors without costly human reasoning annotations. In the elevating\nstage, it uses GRPO-based reinforcement learning on a weakly-supervised\ncross-view pairing proxy. This design supplies a verifiable and scalable reward\nsignal: teaching models to capture and reconcile features across modalities,\nand harnessing reasoning for accurate prediction. Geo-R1 extends geospatial\nmodeling from domain pretraining / supervised finetuning to reasoning-first\npost-training, and achieves state-of-the-art performance across various\ngeospatial reasoning benchmarks. Our model is available at\nhttps://huggingface.co/miniHui/Geo-R1.", "AI": {"tldr": "Geo-R1 introduces a post-training framework for enhancing geospatial reasoning in vision-language models using synthetic exemplars and reinforcement learning, delivering state-of-the-art performance.", "motivation": "The paper aims to address the need for improved geospatial reasoning capabilities in vision-language models, which previously required costly human annotations and were limited to domain pretraining or supervised fine-tuning.", "method": "Geo-R1 employs two stages: scaffolding, where synthetic chain-of-thought exemplars are used for supervised fine-tuning, and elevating, where GRPO-based reinforcement learning on a weakly-supervised cross-view pairing proxy supplies scalable reward signals.", "result": "Geo-R1 achieves state-of-the-art performance in geospatial reasoning benchmarks, demonstrating its effectiveness through reasoning-focused training stages.", "conclusion": "Geo-R1 successfully extends geospatial reasoning capabilities in models by introducing a reasoning-first post-training framework that is scalable, efficient, and surpasses prior methods."}}
{"id": "2510.00332", "pdf": "https://arxiv.org/pdf/2510.00332", "abs": "https://arxiv.org/abs/2510.00332", "authors": ["Zeshi Dai", "Zimo Peng", "Zerui Cheng", "Ryan Yihe Li"], "title": "When Hallucination Costs Millions: Benchmarking AI Agents in High-Stakes Adversarial Financial Markets", "categories": ["cs.AI", "cs.CE", "I.6.4; I.2.1"], "comment": "15 pages, 5 figures, 4 tables; In submission to ICLR 2026", "summary": "We present CAIA, a benchmark exposing a critical blind spot in AI evaluation:\nthe inability of state-of-the-art models to operate in adversarial, high-stakes\nenvironments where misinformation is weaponized and errors are irreversible.\nWhile existing benchmarks measure task completion in controlled settings,\nreal-world deployment demands resilience against active deception. Using crypto\nmarkets as a testbed where $30 billion was lost to exploits in 2024, we\nevaluate 17 models on 178 time-anchored tasks requiring agents to distinguish\ntruth from manipulation, navigate fragmented information landscapes, and make\nirreversible financial decisions under adversarial pressure.\n  Our results reveal a fundamental capability gap: without tools, even frontier\nmodels achieve only 28% accuracy on tasks junior analysts routinely handle.\nTool augmentation improves performance but plateaus at 67.4% versus 80% human\nbaseline, despite unlimited access to professional resources. Most critically,\nwe uncover a systematic tool selection catastrophe: models preferentially\nchoose unreliable web search over authoritative data, falling for SEO-optimized\nmisinformation and social media manipulation. This behavior persists even when\ncorrect answers are directly accessible through specialized tools, suggesting\nfoundational limitations rather than knowledge gaps. We also find that Pass@k\nmetrics mask dangerous trial-and-error behavior for autonomous deployment.\n  The implications extend beyond crypto to any domain with active adversaries,\ne.g. cybersecurity, content moderation, etc. We release CAIA with contamination\ncontrols and continuous updates, establishing adversarial robustness as a\nnecessary condition for trustworthy AI autonomy. The benchmark reveals that\ncurrent models, despite impressive reasoning scores, remain fundamentally\nunprepared for environments where intelligence must survive active opposition.", "AI": {"tldr": "This paper introduces CAIA, a benchmark that evaluates AI models' performance in adversarial, high-stakes environments, highlighting significant capability gaps.", "motivation": "The motivation is to address the inability of AI models to perform reliably in adversarial and high-stakes scenarios, such as misinformation-heavy environments.", "method": "The authors use 178 time-anchored crypto market tasks to assess 17 models in distinguishing truth from manipulation under adversarial conditions.", "result": "Their findings show that without tools, models perform at only 28% accuracy; tools improve this to 67.4%, but systematic biases in tool selection limit performance.", "conclusion": "The paper concludes that current AI models are not adequately prepared for adversarial environments and calls for benchmarks like CAIA to assess robustness in such contexts."}}
{"id": "2510.00192", "pdf": "https://arxiv.org/pdf/2510.00192", "abs": "https://arxiv.org/abs/2510.00192", "authors": ["Xin Yu", "Cong Xie", "Ziyu Zhao", "Tiantian Fan", "Lingzhou Xue", "Zhi Zhang"], "title": "PrunedLoRA: Robust Gradient-Based structured pruning for Low-rank Adaptation in Fine-tuning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Low-rank adaptation (LoRA) has become a widely used paradigm for\nparameter-efficient fine-tuning of large language models, yet its\nrepresentational capacity often lags behind full fine-tuning. Within the\ncontext of LoRA, a key open question is how to obtain expressive low-rank\nadapters from over-parameterized spaces. We propose \\textit{PrunedLoRA}, a new\nframework that leverages structured pruning to obtain highly representative\nlow-rank adapters from an over-parameterized initialization. Unlike prior\napproaches that impose a fixed low-rank budget, PrunedLoRA dynamically prunes\nless important components during fine-tuning and prevents their reactivation,\nenabling flexible and adaptive rank allocation. For structured pruning, by\nminimizing the pruning error for overall loss, we provide fine-grained pruning\nand recovery updates in a gradient-based pruning strategy with grounded\ninterpretation. We provide the first theoretical analysis of the robustness of\nstructured pruning and provably show that under the impact of weight\nperturbation, gradient-based pruning is more robust than activation-based\npruning with respect to overall loss. Empirically, PrunedLoRA consistently\noutperforms LoRA and its variants across supervised fine-tuning tasks in\nmathematical reasoning, code generation, and natural language understanding,\nand it also demonstrates advantages over existing structured pruning methods\nacross diverse sparsity levels.", "AI": {"tldr": "The paper introduces PrunedLoRA, a new method that leverages structured pruning to improve low-rank adapters' representational capacity in LoRA fine-tuning, yielding better performance across various tasks.", "motivation": "Enhance the representational capacity of low-rank adapters in LoRA fine-tuning, which currently underperforms compared to full fine-tuning.", "method": "PrunedLoRA employs structured pruning to dynamically remove less useful components during fine-tuning, with fine-grained, gradient-based pruning that minimizes loss and ensures robustness.", "result": "PrunedLoRA consistently outperforms LoRA and its variants in tasks like mathematical reasoning, code generation, and natural language understanding, and shows superiority over other structured pruning methods under diverse sparsity levels.", "conclusion": "The approach demonstrates that PrunedLoRA not only enhances the expressiveness of low-rank adapters but also provides a theoretically robust and empirically superior alternative for parameter-efficient fine-tuning."}}
{"id": "2510.00444", "pdf": "https://arxiv.org/pdf/2510.00444", "abs": "https://arxiv.org/abs/2510.00444", "authors": ["Zijun Wu", "Yongchang Hao", "Lili Mou"], "title": "TokMem: Tokenized Procedural Memory for Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Large language models rely heavily on prompts to specify tasks, recall\nknowledge and guide reasoning. However, this reliance is inefficient as prompts\nmust be re-read at each step, scale poorly across tasks, and lack mechanisms\nfor modular reuse. We introduce TokMem, a tokenized procedural memory that\nstores recurring procedures as compact, trainable embeddings. Each memory token\nencodes both an address to a procedure and a control signal that steers\ngeneration, enabling targeted behavior with constant-size overhead. To support\ncontinual adaptation, TokMem keeps the backbone model frozen, allowing new\nprocedures to be added without interfering with existing ones. We evaluate\nTokMem on 1,000 tasks for atomic recall, and on function-calling tasks for\ncompositional recall, where it consistently outperforms retrieval-augmented\ngeneration while avoiding repeated context overhead, and fine-tuning with far\nfewer parameters. These results establish TokMem as a scalable and modular\nalternative to prompt engineering and fine-tuning, offering an explicit\nprocedural memory for LLMs.", "AI": {"tldr": "TokMem is introduced as a tokenized procedural memory for large language models that offers efficient task management and avoids prompt overhead while improving performance.", "motivation": "Address inefficiencies in large language models due to repetitive prompts and lack of modular reuse.", "method": "Develop TokMem, a tokenized procedural memory storing procedures as compact embeddings with controlled generation signals.", "result": "TokMem outperformed retrieval-augmented generation and fine-tuning across recall tasks with fewer parameters and without context overhead.", "conclusion": "TokMem provides a scalable, modular alternative to prompt engineering, enhancing efficiency and adaptability in language models."}}
{"id": "2510.00680", "pdf": "https://arxiv.org/pdf/2510.00680", "abs": "https://arxiv.org/abs/2510.00680", "authors": ["Hang Cui", "Jingjing Li", "Haotian Si", "Quan Zhou", "Changhua Pei", "Gaogang Xie", "Dan Pei"], "title": "TShape: Rescuing Machine Learning Models from Complex Shapelet Anomalies", "categories": ["cs.SE"], "comment": null, "summary": "Time series anomaly detection (TSAD) is critical for maintaining the\nreliability of modern IT infrastructures, where complex anomalies frequently\narise in highly dynamic environments. In this paper, we present TShape, a novel\nframework designed to address the challenges in industrial time series anomaly\ndetection. Existing methods often struggle to detect shapelet anomalies that\nmanifest as complex shape deviations, which appear obvious to human experts but\nprove challenging for machine learning algorithms. TShape introduces a\npatch-wise dual attention mechanism with multi-scale convolution to model\nintricate sub-sequence variations by balancing local, fine-grained shape\nfeatures with global contextual dependencies. Our extensive evaluation on five\ndiverse benchmarks demonstrates that TShape outperforms existing\nstate-of-the-art models, achieving an average 10\\% F1 score improvement in\nanomaly detection. Additionally, ablation studies and attention visualizations\nconfirm the essential contributions of each component, highlighting the\nrobustness and adaptability of TShape to complex shapelet shapes in time series\ndata.", "AI": {"tldr": "The paper introduces TShape, a framework for time series anomaly detection (TSAD), particularly adept at identifying complex shapelet anomalies that are difficult for existing methods to detect.", "motivation": "The motivation stems from the need to improve anomaly detection in IT systems, where existing methods struggle with complex shape deviations that are obvious to humans but challenging for machine learning models.", "method": "TShape employs a patch-wise dual attention mechanism combined with multi-scale convolution, striking a balance between fine-grained local features and global contextual information.", "result": "TShape outperforms state-of-the-art methods by achieving an average 10% higher F1 score on five benchmarks, proving its robustness and adaptability in detecting shapelet anomalies.", "conclusion": "The approach not only advances TSAD but also shows each component's importance through ablation studies and attention visualizations, confirming TShape's capability in complex scenarios."}}
{"id": "2510.00619", "pdf": "https://arxiv.org/pdf/2510.00619", "abs": "https://arxiv.org/abs/2510.00619", "authors": ["Michiel Braat", "Maren Buermann", "Marijke van Weperen", "Jan-Pieter Paardekooper"], "title": "What Did I Learn? Operational Competence Assessment for AI-Based Trajectory Planners", "categories": ["cs.RO", "cs.AI"], "comment": "Accepted for publication in proceedings of the 2025 IEEE\n  International Automated Vehicle Validation Conference", "summary": "Automated driving functions increasingly rely on machine learning for tasks\nlike perception and trajectory planning, requiring large, relevant datasets.\nThe performance of these algorithms depends on how closely the training data\nmatches the task. To ensure reliable functioning, it is crucial to know what is\nincluded in the dataset to assess the trained model's operational risk. We aim\nto enhance the safe use of machine learning in automated driving by developing\na method to recognize situations that an automated vehicle has not been\nsufficiently trained on. This method also improves explainability by describing\nthe dataset at a human-understandable level. We propose modeling driving data\nas knowledge graphs, representing driving scenes with entities and their\nrelationships. These graphs are queried for specific sub-scene configurations\nto check their occurrence in the dataset. We estimate a vehicle's competence in\na driving scene by considering the coverage and complexity of sub-scene\nconfigurations in the training set. Higher complexity scenes require greater\ncoverage for high competence. We apply this method to the NuPlan dataset,\nmodeling it with knowledge graphs and analyzing the coverage of specific\ndriving scenes. This approach helps monitor the competence of machine learning\nmodels trained on the dataset, which is essential for trustworthy AI to be\ndeployed in automated driving.", "AI": {"tldr": "This paper proposes a novel method using knowledge graphs to evaluate the training data adequacy and explainability of machine learning models in automated driving.", "motivation": "To ensure the safe and reliable functionality of machine learning models used in automated driving amidst a high dependence on relevant and task-matching datasets.", "method": "Driving data is modeled as knowledge graphs, where driving scenes are represented by entities and their relationships. Sub-scene configurations are queried in the graphs to measure data coverage and complexity, estimating the model\u2019s competence.", "result": "The method was applied to the NuPlan dataset, enabling analysis of dataset coverage for specific driving scenes to monitor competence of trained machine learning models.", "conclusion": "The approach aids in assessing dataset adequacy and improving model trustworthiness in automated driving through enhanced explainability and operational risk understanding."}}
{"id": "2510.00386", "pdf": "https://arxiv.org/pdf/2510.00386", "abs": "https://arxiv.org/abs/2510.00386", "authors": ["Ayush Jain", "Andrea Montanari", "Eren Sasoglu"], "title": "Train on Validation (ToV): Fast data selection with applications to fine-tuning", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "State-of-the-art machine learning often follows a two-stage process:\n$(i)$~pre-training on large, general-purpose datasets; $(ii)$~fine-tuning on\ntask-specific data. In fine-tuning, selecting training examples that closely\nreflect the target distribution is crucial. However, it is often the case that\nonly a few samples are available from the target distribution. Existing data\nselection methods treat these target samples as a validation set and estimate\nthe effect of adding or removing a single sample from the training pool by\nperforming inference on the validation set.\n  We propose a simpler and faster alternative that inverts the usual role of\ntrain and validation: we perform inference on the training pool before and\nafter fine-tuning on the validation set. We then select samples whose\npredictions change the most. Our key insight is that the training samples most\naffected by fine-tuning on a small validation set tend to be the most\nbeneficial for reducing test loss on the target distribution. Experiments on\ninstruction tuning and named entity recognition tasks show that, in most cases,\nour method achieves lower test log-loss than state-of-the-art approaches. We\nsupport our findings with theoretical analysis.", "AI": {"tldr": "The paper introduces a novel, efficient method for data selection during fine-tuning in machine learning, which achieves better performance in reducing test loss compared to existing methods.", "motivation": "To address the challenge of effective data selection during fine-tuning, especially when only a few samples from the target distribution are available.", "method": "A data selection approach that swaps the usual role of train and validation\u2014performing inference on the training pool before and after fine-tuning on the validation set and selecting samples whose predictions change the most.", "result": "The proposed method outperforms state-of-the-art approaches in lowering test log-loss on instruction tuning and named entity recognition tasks, supported by both experiments and theoretical validation.", "conclusion": "The method identifies training samples most influenced by small target-validation fine-tuning as critical, providing a simpler and faster alternative for improved fine-tuning performance."}}
{"id": "2510.00083", "pdf": "https://arxiv.org/pdf/2510.00083", "abs": "https://arxiv.org/abs/2510.00083", "authors": ["Hanjiang Hu", "Bowei Li", "Ziwei Wang", "Tianhao Wei", "Casidhe Hutchison", "Eric Sample", "Changliu Liu"], "title": "Enhancing Certifiable Semantic Robustness via Robust Pruning of Deep Neural Networks", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Deep neural networks have been widely adopted in many vision and robotics\napplications with visual inputs. It is essential to verify its robustness\nagainst semantic transformation perturbations, such as brightness and contrast.\nHowever, current certified training and robustness certification methods face\nthe challenge of over-parameterization, which hinders the tightness and\nscalability due to the over-complicated neural networks. To this end, we first\nanalyze stability and variance of layers and neurons against input\nperturbation, showing that certifiable robustness can be indicated by a\nfundamental Unbiased and Smooth Neuron metric (USN). Based on USN, we introduce\na novel neural network pruning method that removes neurons with low USN and\nretains those with high USN, thereby preserving model expressiveness without\nover-parameterization. To further enhance this pruning process, we propose a\nnew Wasserstein distance loss to ensure that pruned neurons are more\nconcentrated across layers. We validate our approach through extensive\nexperiments on the challenging robust keypoint detection task, which involves\nrealistic brightness and contrast perturbations, demonstrating that our method\nachieves superior robustness certification performance and efficiency compared\nto baselines.", "AI": {"tldr": "The paper presents a novel neural network pruning method to improve robustness against perturbations like brightness and contrast by introducing a metric called Unbiased and Smooth Neuron (USN) and using a new loss function based on Wasserstein distance.", "motivation": "Deep neural networks are vulnerable to robustness issues when facing semantic transformation perturbations like brightness and contrast, and existing methods struggle due to over-parameterization.", "method": "The method involves defining a metric, USN, to evaluate neuron stability and variance, pruning low-USN neurons, and employing a Wasserstein loss to ensure concentration of pruned neurons across layers.", "result": "Extensive experiments on robust keypoint detection tasks with brightness and contrast perturbations show that the proposed method improves both robustness certification and computational efficiency over baseline models.", "conclusion": "The proposed pruning approach effectively reduces over-parameterization while maintaining robustness, leading to superior performance and efficiency in challenging perturbation scenarios."}}
{"id": "2510.00355", "pdf": "https://arxiv.org/pdf/2510.00355", "abs": "https://arxiv.org/abs/2510.00355", "authors": ["Renee Ge", "Qianli Liao", "Tomaso Poggio"], "title": "Hierarchical Reasoning Model: A Critical Supplementary Material", "categories": ["cs.AI", "cs.LG"], "comment": "Preprint, Under review", "summary": "Transformers have demonstrated remarkable performance in natural language\nprocessing and related domains, as they largely focus on sequential,\nautoregressive next-token prediction tasks. Yet, they struggle in logical\nreasoning, not necessarily because of a fundamental limitation of these models,\nbut possibly due to the lack of exploration of more creative uses, such as\nlatent space and recurrent reasoning. An emerging exploration in this direction\nis the Hierarchical Reasoning Model (Wang et al., 2025), which introduces a\nnovel type of recurrent reasoning in the latent space of transformers,\nachieving remarkable performance on a wide range of 2D reasoning tasks. Despite\nthe promising results, this line of models is still at an early stage and calls\nfor in-depth investigation. In this work, we perform a critical review on this\nclass of models, examine key design choices and present intriguing variants\nthat achieve significantly better performance on the Sudoku-Extreme and\nMaze-Hard tasks than previously reported. Our results also raise surprising\nobservations and intriguing directions for further research.", "AI": {"tldr": "Transformers excel in sequential tasks but struggle with logical reasoning due to unexplored latent space and recurrent reasoning. The paper reviews and improves on Hierarchical Reasoning Models, showcasing enhanced success in 2D reasoning tasks.", "motivation": "Investigate the limitations of transformers in reasoning and explore latent space and recurrent reasoning to improve their capabilities.", "method": "Analyzing and modifying Hierarchical Reasoning Models for better logical reasoning in challenging tasks like Sudoku-Extreme and Maze-Hard.", "result": "Enhanced performance on complex reasoning tasks, surpassing previous benchmarks in these models and uncovering insights for future research.", "conclusion": "Critical exploration and refinement of hierarchical reasoning models can lead to significant advancements in logical reasoning capabilities for transformers."}}
