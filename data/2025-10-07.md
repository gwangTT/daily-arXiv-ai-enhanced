<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 90]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.CL](#cs.CL) [Total: 111]
- [cs.CV](#cs.CV) [Total: 171]
- [cs.DC](#cs.DC) [Total: 8]
- [cs.LG](#cs.LG) [Total: 285]
- [cs.NE](#cs.NE) [Total: 4]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.PL](#cs.PL) [Total: 4]
- [cs.RO](#cs.RO) [Total: 55]
- [cs.SE](#cs.SE) [Total: 46]
- [q-bio.NC](#q-bio.NC) [Total: 11]
- [stat.ML](#stat.ML) [Total: 22]
- [eess.IV](#eess.IV) [Total: 8]
- [cond-mat.stat-mech](#cond-mat.stat-mech) [Total: 1]
- [q-fin.ST](#q-fin.ST) [Total: 1]
- [cs.CR](#cs.CR) [Total: 17]
- [eess.AS](#eess.AS) [Total: 5]
- [physics.med-ph](#physics.med-ph) [Total: 1]
- [cs.MA](#cs.MA) [Total: 6]
- [math.OC](#math.OC) [Total: 10]
- [eess.SY](#eess.SY) [Total: 8]
- [physics.chem-ph](#physics.chem-ph) [Total: 2]
- [cs.CY](#cs.CY) [Total: 11]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [cond-mat.dis-nn](#cond-mat.dis-nn) [Total: 1]
- [econ.GN](#econ.GN) [Total: 1]
- [math.ST](#math.ST) [Total: 1]
- [cs.GR](#cs.GR) [Total: 12]
- [math.PR](#math.PR) [Total: 1]
- [cs.SI](#cs.SI) [Total: 2]
- [q-bio.QM](#q-bio.QM) [Total: 2]
- [stat.CO](#stat.CO) [Total: 3]
- [cs.CE](#cs.CE) [Total: 5]
- [cs.LO](#cs.LO) [Total: 3]
- [physics.optics](#physics.optics) [Total: 1]
- [math.NA](#math.NA) [Total: 1]
- [cs.IT](#cs.IT) [Total: 2]
- [cs.SY](#cs.SY) [Total: 1]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [cs.GT](#cs.GT) [Total: 2]
- [astro-ph.IM](#astro-ph.IM) [Total: 2]
- [cs.HC](#cs.HC) [Total: 4]
- [cs.DM](#cs.DM) [Total: 1]
- [eess.SP](#eess.SP) [Total: 1]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.CG](#cs.CG) [Total: 2]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [cs.SD](#cs.SD) [Total: 7]
- [cs.OS](#cs.OS) [Total: 1]
- [quant-ph](#quant-ph) [Total: 2]
- [cs.NI](#cs.NI) [Total: 5]
- [stat.ME](#stat.ME) [Total: 4]
- [physics.ao-ph](#physics.ao-ph) [Total: 3]
- [cs.IR](#cs.IR) [Total: 7]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [WAREX: Web Agent Reliability Evaluation on Existing Benchmarks](https://arxiv.org/abs/2510.03285)
*Su Kara,Fazle Faisal,Suman Nath*

Main category: cs.AI

TL;DR: The paper introduces WAREX, a framework to evaluate the reliability of browser-based LLM agents under realistic, unstable web conditions instead of controlled environments. Tests reveal significant decreases in agent performance.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks for browser-based LLM agents fail to account for the instability and security challenges of real-world web conditions, leaving gaps in evaluating their robustness.

Method: WAREX, a reliability evaluation framework, was introduced to test agents in scenarios impacted by network instability, HTTPS issues, web attacks, and site modifications across established benchmarks.

Result: Experiments conducted using WAREX on popular benchmarks revealed notable drops in task success rates, demonstrating the vulnerabilities and limited robustness of state-of-the-art agents.

Conclusion: The paper underscores the need for robust testing frameworks like WAREX to better assess and improve the reliability of browser-based LLM agents in real-world conditions.

Abstract: Recent advances in browser-based LLM agents have shown promise for automating
tasks ranging from simple form filling to hotel booking or online shopping.
Current benchmarks measure agent performance in controlled environments, such
as containers or stable networks, where websites behave deterministically.
However, in the real world, users access websites over networks and HTTPS
connections that introduce instability from multiple sources: client-side,
server-side issues or broader system failures. Moreover, live websites are
prone to web attacks such Cross-Site Scripting, as well as general site
modifications which can cause unexpected or malicious pop-ups or improper
functionality. To address this gap, we present WAREX: Web Agent Reliability
Evaluation on Existing Benchmarks. We measure the impact of WAREX across three
popular benchmarks: WebArena, WebVoyager, and REAL. Our experiments show that
introducing WAREX leads to significant drops in task success rates,
highlighting the limited robustness of state-of-the-art agents.

</details>


### [2] [Refined Iterated Pareto Greedy for Energy-aware Hybrid Flowshop Scheduling with Blocking Constraints](https://arxiv.org/abs/2510.03377)
*Ahmed Missaoui,Cemalettin Ozturk,Barry O'Sullivan*

Main category: cs.AI

TL;DR: This paper addresses energy-efficient scheduling in manufacturing by focusing on a hybrid flow shop scheduling problem (BHFS). It proposes both exact and heuristic methods to minimize makespan and energy consumption, yielding effective results across varied problem sizes.


<details>
  <summary>Details</summary>
Motivation: The demand for energy-efficient solutions is driven by resource scarcity, climate change, and economic pressures, making it crucial for the energy-intensive manufacturing sector to adopt optimization strategies.

Method: The study formulates a novel multi-objective mixed integer programming (MIP) model with an augmented epsilon-constraint method and develops the Refined Iterated Pareto Greedy (RIPG) algorithm to handle larger problem instances effectively.

Result: Benchmarking and computational results demonstrate that the proposed approaches perform effectively, outperforming two well-known algorithms across small, medium, and large problem sizes.

Conclusion: The proposed methods offer practical and efficient solutions for balancing energy consumption and operational efficiency, providing immediate impact for energy-conscious manufacturing sectors.

Abstract: The scarcity of non-renewable energy sources, geopolitical problems in its
supply, increasing prices, and the impact of climate change, force the global
economy to develop more energy-efficient solutions for their operations. The
Manufacturing sector is not excluded from this challenge as one of the largest
consumers of energy. Energy-efficient scheduling is a method that attracts
manufacturing companies to reduce their consumption as it can be quickly
deployed and can show impact immediately. In this study, the hybrid flow shop
scheduling problem with blocking constraint (BHFS) is investigated in which we
seek to minimize the latest completion time (i.e. makespan) and overall energy
consumption, a typical manufacturing setting across many industries from
automotive to pharmaceutical. Energy consumption and the latest completion time
of customer orders are usually conflicting objectives. Therefore, we first
formulate the problem as a novel multi-objective mixed integer programming
(MIP) model and propose an augmented epsilon-constraint method for finding the
Pareto-optimal solutions. Also, an effective multi-objective metaheuristic
algorithm. Refined Iterated Pareto Greedy (RIPG), is developed to solve large
instances in reasonable time. Our proposed methods are benchmarked using small,
medium, and large-size instances to evaluate their efficiency. Two well-known
algorithms are adopted for comparing our novel approaches. The computational
results show the effectiveness of our method.

</details>


### [3] [Know Thyself? On the Incapability and Implications of AI Self-Recognition](https://arxiv.org/abs/2510.03399)
*Xiaoyan Bai,Aryan Shrivastava,Ari Holtzman,Chenhao Tan*

Main category: cs.AI

TL;DR: The paper evaluates 10 large language models (LLMs) on their ability to recognize their own generated text, finding consistent failures and biases.


<details>
  <summary>Details</summary>
Motivation: The aim is to resolve the conflicting interpretations about AI self-recognition and explore its implications for safety and model evaluation.

Method: The researchers designed two tasks—binary self-recognition and exact model prediction—to assess how well models can identify their own text versus others'.

Result: The study found that only 4 out of 10 models could predict themselves as generators, often performing at random chance, and exhibited biases favoring GPT and Claude models.

Conclusion: The findings highlight gaps in AI self-recognition and introduce safety concerns, emphasizing the need for advanced frameworks to enhance AI self-awareness.

Abstract: Self-recognition is a crucial metacognitive capability for AI systems,
relevant not only for psychological analysis but also for safety, particularly
in evaluative scenarios. Motivated by contradictory interpretations of whether
models possess self-recognition (Panickssery et al., 2024; Davidson et al.,
2024), we introduce a systematic evaluation framework that can be easily
applied and updated. Specifically, we measure how well 10 contemporary larger
language models (LLMs) can identify their own generated text versus text from
other models through two tasks: binary self-recognition and exact model
prediction. Different from prior claims, our results reveal a consistent
failure in self-recognition. Only 4 out of 10 models predict themselves as
generators, and the performance is rarely above random chance. Additionally,
models exhibit a strong bias toward predicting GPT and Claude families. We also
provide the first evaluation of model awareness of their own and others'
existence, as well as the reasoning behind their choices in self-recognition.
We find that the model demonstrates some knowledge of its own existence and
other models, but their reasoning reveals a hierarchical bias. They appear to
assume that GPT, Claude, and occasionally Gemini are the top-tier models, often
associating high-quality text with them. We conclude by discussing the
implications of our findings on AI safety and future directions to develop
appropriate AI self-awareness.

</details>


### [4] [ContraGen: A Multi-Agent Generation Framework for Enterprise Contradictions Detection](https://arxiv.org/abs/2510.03418)
*Ananya Mantravadi,Shivali Dalmia,Abhishek Mukherji,Nand Dave,Anudha Mittal*

Main category: cs.AI

TL;DR: The paper introduces ContraGen, a benchmark framework designed to evaluate contradictions in enterprise documents for more reliable RAG systems.


<details>
  <summary>Details</summary>
Motivation: Address the lack of enterprise-domain benchmarks for detecting contradictions in RAG systems, which is critical for compliance and decision-making.

Method: Propose ContraGen, a synthetic document generator with embedded contradictions, combining automated mining methods and human-in-the-loop validation.

Result: The framework successfully models contradiction types, enables systematic evaluation of contradictions in enterprise contexts, and ensures high accuracy via human oversight.

Conclusion: ContraGen sets a solid foundation for enhancing trustworthiness and accountability in enterprise-focused RAG systems by tackling contradictions effectively.

Abstract: Retrieval-Augmented Generation (RAG) integrates LLMs with external sources,
offering advanced capabilities for information access and decision-making.
However, contradictions in retrieved evidence can result in inconsistent or
untrustworthy outputs, which is especially problematic in enterprise settings
where compliance, governance, and accountability are critical. Existing
benchmarks for contradiction detection are limited to sentence-level analysis
and do not capture the complexity of enterprise documents such as contracts,
financial filings, compliance reports, or policy manuals. To address this
limitation, we propose ContraGen, a contradiction-aware benchmark framework
tailored to enterprise domain. The framework generates synthetic
enterprise-style documents with embedded contradictions, enabling systematic
evaluation of both intra-document and cross-document consistency. Automated
contradiction mining is combined with human-in-the-loop validation to ensure
high accuracy. Our contributions include generating realistic enterprise
documents, modeling a taxonomy of contradiction types common in business
processes, enabling controlled creation of self- and pairwise contradictions,
developing a contradiction-aware retrieval evaluation pipeline and embedding
human oversight to reflect domain-specific judgment complexity. This work
establishes a foundation for more trustworthy and accountable RAG systems in
enterprise information-seeking applications, where detecting and resolving
contradictions is essential for reducing risk and ensuring compliance.

</details>


### [5] [A Qualitative Comparative Evaluation of Cognitive and Generative Theories](https://arxiv.org/abs/2510.03453)
*Paul S. Rosenbloom*

Main category: cs.AI

TL;DR: The paper discusses the challenges and approaches for evaluating theories based on cognitive and generative neural architectures, using a broad qualitative comparison.


<details>
  <summary>Details</summary>
Motivation: There is a need to tackle the difficulty of evaluating theories grounded in cognitive and generative neural architectures, which are important for understanding whole-mind-oriented systems.

Method: The study employs a wide-ranging qualitative comparison of cognitive and generative architectures with a focus on theorizing based on whole-mind systems.

Result: The paper identifies disparities and similarities in evaluating these two architectural approaches, emphasizing the nuanced challenges they present.

Conclusion: A broad perspective on evaluation improves understanding, but qualitative comparisons suggest both convergence and divergence in evaluation metrics for cognitive and generative architectures.

Abstract: Evaluation is a critical activity associated with any theory. Yet this has
proven to be an exceptionally challenging activity for theories based on
cognitive architectures. For an overlapping set of reasons, evaluation can also
be challenging for theories based on generative neural architectures. This dual
challenge is approached here by leveraging a broad perspective on theory
evaluation to yield a wide-ranging, albeit qualitative, comparison of
whole-mind-oriented cognitive and generative architectures and the full systems
that are based on these architectures.

</details>


### [6] [Bridging LLM Planning Agents and Formal Methods: A Case Study in Plan Verification](https://arxiv.org/abs/2510.03469)
*Keshav Ramani,Vali Tawosi,Salwa Alamir,Daniel Borrajo*

Main category: cs.AI

TL;DR: The paper introduces a novel framework leveraging Large Language Models (LLMs) for converting natural language plans into formal structures for model checking, showcasing high accuracy but identifying semantic synthesis as a future challenge.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the need to better align natural language plans with their expected behavior using formal verification techniques. Current methods lack robust frameworks capable of ensuring syntactic and semantic accuracy.

Method: The authors utilize LLMs (specifically GPT-5) to transform natural language plans into Kripke structures and Linear Temporal Logic (LTL), followed by evaluation using the PlanBench dataset. Key metrics analyzed include accuracy, precision, recall, and F1 scores.

Result: GPT-5 achieves an exceptional F1 score of 96.3% in classification performance and produces syntactically perfect formal representations. Semantic synthesis still poses a challenge for further exploration.

Conclusion: The framework proves effective for syntactic perfect transformation with high classification performance, laying the groundwork for future work on improving semantic synthesis in formal modeling contexts.

Abstract: We introduce a novel framework for evaluating the alignment between natural
language plans and their expected behavior by converting them into Kripke
structures and Linear Temporal Logic (LTL) using Large Language Models (LLMs)
and performing model checking. We systematically evaluate this framework on a
simplified version of the PlanBench plan verification dataset and report on
metrics like Accuracy, Precision, Recall and F1 scores. Our experiments
demonstrate that GPT-5 achieves excellent classification performance (F1 score
of 96.3%) while almost always producing syntactically perfect formal
representations that can act as guarantees. However, the synthesis of
semantically perfect formal models remains an area for future exploration.

</details>


### [7] [Towards Policy-Compliant Agents: Learning Efficient Guardrails For Policy Violation Detection](https://arxiv.org/abs/2510.03485)
*Xiaofei Wen,Wenjie Jacky Mo,Yanan Xie,Peng Qi,Muhao Chen*

Main category: cs.AI

TL;DR: This paper introduces PolicyGuardBench, a benchmark for detecting policy violations in long-horizon web agent trajectories across diverse domains and subdomains, along with a lightweight model PolicyGuard-4B for efficient and accurate detection.


<details>
  <summary>Details</summary>
Motivation: The motivation is to ensure autonomous web agents comply with externally imposed or human-specified policies while operating over extended sequences in varied contexts, addressing the lack of research on policy violation detection in agent trajectories.

Method: The method involves creating a dataset, PolicyGuardBench, consisting of approximately 60,000 examples with policy violation labels. It includes evaluation tasks to detect violations in entire trajectories and anticipate them from prefixes. PolicyGuard-4B, a lightweight model, is trained for efficient violation detection.

Result: PolicyGuard-4B achieves strong detection accuracy across tasks, generalizes effectively across domains, and maintains high accuracy even in unseen settings.

Conclusion: PolicyGuardBench and PolicyGuard-4B collectively provide a robust framework for studying policy compliance in web agent trajectories, demonstrating that efficient and accurate guardrails are feasible for improving agent reliability in diverse settings.

Abstract: Autonomous web agents need to operate under externally imposed or
human-specified policies while generating long-horizon trajectories. However,
little work has examined whether these trajectories comply with such policies,
or whether policy violations persist across different contexts such as domains
(e.g., shopping or coding websites) and subdomains (e.g., product search and
order management in shopping). To address this gap, we introduce
PolicyGuardBench, a benchmark of about 60k examples for detecting policy
violations in agent trajectories. From diverse agent runs, we generate a broad
set of policies and create both within subdomain and cross subdomain pairings
with violation labels. In addition to full-trajectory evaluation,
PolicyGuardBench also includes a prefix-based violation detection task where
models must anticipate policy violations from truncated trajectory prefixes
rather than complete sequences. Using this dataset, we train PolicyGuard-4B, a
lightweight guardrail model that delivers strong detection accuracy across all
tasks while keeping inference efficient. Notably, PolicyGuard-4B generalizes
across domains and preserves high accuracy on unseen settings. Together,
PolicyGuardBench and PolicyGuard-4B provide the first comprehensive framework
for studying policy compliance in web agent trajectories, and show that
accurate and generalizable guardrails are feasible at small scales.

</details>


### [8] [OneFlow: Concurrent Mixed-Modal and Interleaved Generation with Edit Flows](https://arxiv.org/abs/2510.03506)
*John Nguyen,Marton Havasi,Tariq Berrada,Luke Zettlemoyer,Ricky T. Q. Chen*

Main category: cs.AI

TL;DR: OneFlow is a non-autoregressive multimodal model enabling mixed-modal concurrent and variable-length generation, surpassing traditional and diffusion-based methods in performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: Address limitations of autoregressive models in rigid causal ordering during text-image generation and improve efficiency and capabilities.

Method: Utilizes an insertion-based Edit Flow for text tokens and Flow Matching for image latents to enable concurrent text-image generation with hierarchical sampling prioritizing content.

Result: OneFlow outperforms autoregressive baselines in generation and understanding tasks, achieving up to 50% fewer training FLOPs and surpassing alternative approaches in efficiency and capability.

Conclusion: OneFlow introduces concurrent hierarchical multimodal generation with superior performance and unique capabilities compared to autoregressive and diffusion models.

Abstract: We present OneFlow, the first non-autoregressive multimodal model that
enables variable-length and concurrent mixed-modal generation. Unlike
autoregressive models that enforce rigid causal ordering between text and image
generation, OneFlow combines an insertion-based Edit Flow for discrete text
tokens with Flow Matching for image latents. OneFlow enables concurrent
text-image synthesis with hierarchical sampling that prioritizes content over
grammar. Through controlled experiments across model sizes from 1B to 8B, we
demonstrate that OneFlow outperforms autoregressive baselines on both
generation and understanding tasks while using up to 50% fewer training FLOPs.
OneFlow surpasses both autoregressive and diffusion-based approaches while
unlocking new capabilities for concurrent generation, iterative refinement, and
natural reasoning-like generation.

</details>


### [9] [Understanding the Role of Training Data in Test-Time Scaling](https://arxiv.org/abs/2510.03605)
*Adel Javanmard,Baharan Mirzasoleiman,Vahab Mirrokni*

Main category: cs.AI

TL;DR: The paper explores how test-time scaling enhances reasoning capabilities in LLMs, focusing on longer Chains-of-Thoughts and their influence on performance in linear regression tasks.


<details>
  <summary>Details</summary>
Motivation: The authors aim to understand when and why longer Chains-of-Thoughts in reasoning improve LLM performance, addressing gaps on how test-time scaling affects downstream task achievements.

Method: They analyze the performance of transformers using in-context weight prediction tasks for linear regression while studying the theoretical aspects of increasing test-time compute.

Result: Key findings include reduced training context length via test-time compute scaling, the potential harm from scaling when training lacks downstream task skills, and the importance of diverse and challenging task sets determined by a feature covariance metric.

Conclusion: Optimal performance with test-time scaling is achieved through training on diverse, relevant, and hard tasks, while indiscriminate scaling can be counterproductive without proper training conditions.

Abstract: Test-time scaling improves the reasoning capabilities of large language
models (LLMs) by allocating extra compute to generate longer Chains-of-Thoughts
(CoTs). This enables models to tackle more complex problem by breaking them
down into additional steps, backtracking, and correcting mistakes. Despite its
strong performance--demonstrated by OpenAI's o1 and DeepSeek R1, the conditions
in the training data under which long CoTs emerge, and when such long CoTs
improve the performance, remain unclear. In this paper, we study the
performance of test-time scaling for transformers trained on an in-context
weight prediction task for linear regression. Our analysis provides a
theoretical explanation for several intriguing observations: First, at any
fixed test error, increasing test-time compute allows us to reduce the number
of in-context examples (context length) in training prompts. Second, if the
skills required to solve a downstream task are not sufficiently present in the
training data, increasing test-time compute can harm performance. Finally, we
characterize task hardness via the smallest eigenvalue of its feature
covariance matrix and show that training on a diverse, relevant, and hard set
of tasks results in best performance for test-time scaling. We confirm our
findings with experiments on large, nonlinear transformer architectures.

</details>


### [10] [Video Game Level Design as a Multi-Agent Reinforcement Learning Problem](https://arxiv.org/abs/2510.04862)
*Sam Earle,Zehua Jiang,Eugene Vinitsky,Julian Togelius*

Main category: cs.AI

TL;DR: This paper proposes a multi-agent approach to Procedural Content Generation via Reinforcement Learning (PCGRL), improving efficiency and generalization.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations in single-agent PCGRL due to high computational cost of recalculating heuristics and challenges in navigating large maps.

Method: The study frames level generation as a distributed, multi-agent problem to reduce reward calculation overhead and enable modular design policies.

Result: Multi-agent generators demonstrated better efficiency and generalization to out-of-distribution map shapes compared to single-agent approaches.

Conclusion: Distributed multi-agent PCGRL is advantageous for scalable and functional level generation, offering better performance and adaptability.

Abstract: Procedural Content Generation via Reinforcement Learning (PCGRL) offers a
method for training controllable level designer agents without the need for
human datasets, using metrics that serve as proxies for level quality as
rewards. Existing PCGRL research focuses on single generator agents, but are
bottlenecked by the need to frequently recalculate heuristics of level quality
and the agent's need to navigate around potentially large maps. By framing
level generation as a multi-agent problem, we mitigate the efficiency
bottleneck of single-agent PCGRL by reducing the number of reward calculations
relative to the number of agent actions. We also find that multi-agent level
generators are better able to generalize to out-of-distribution map shapes,
which we argue is due to the generators' learning more local, modular design
policies. We conclude that treating content generation as a distributed,
multi-agent task is beneficial for generating functional artifacts at scale.

</details>


### [11] [Cross-Modal Content Optimization for Steering Web Agent Preferences](https://arxiv.org/abs/2510.03612)
*Tanqiu Jiang,Min Bai,Nikolaos Pappas,Yanjun Qi,Sandesh Swamy*

Main category: cs.AI

TL;DR: The paper introduces Cross-Modal Preference Steering (CPS), showcasing its effectiveness in attacking vision-language model (VLM)-based web agents through subtle manipulations of visual and textual data under realistic black-box conditions.


<details>
  <summary>Details</summary>
Motivation: To explore vulnerabilities in VLM-based web agents and demonstrate effective preference manipulations using combined visual and textual perturbations.

Method: Developed CPS, exploiting CLIP-transferable image changes and RLHF-induced textual biases, and tested it on proprietary/open-source VLMs under black-box settings.

Result: CPS achieved higher manipulation rates and lower detection rates than baselines across various models and tasks, validating its stealthiness and efficiency.

Conclusion: The work underscores the urgent need for defenses against such attacks as VLM-based systems gain societal importance in high-stakes scenarios.

Abstract: Vision-language model (VLM)-based web agents increasingly power high-stakes
selection tasks like content recommendation or product ranking by combining
multimodal perception with preference reasoning. Recent studies reveal that
these agents are vulnerable against attackers who can bias selection outcomes
through preference manipulations using adversarial pop-ups, image
perturbations, or content tweaks. Existing work, however, either assumes strong
white-box access, with limited single-modal perturbations, or uses impractical
settings. In this paper, we demonstrate, for the first time, that joint
exploitation of visual and textual channels yields significantly more powerful
preference manipulations under realistic attacker capabilities. We introduce
Cross-Modal Preference Steering (CPS) that jointly optimizes imperceptible
modifications to an item's visual and natural language descriptions, exploiting
CLIP-transferable image perturbations and RLHF-induced linguistic biases to
steer agent decisions. In contrast to prior studies that assume gradient
access, or control over webpages, or agent memory, we adopt a realistic
black-box threat setup: a non-privileged adversary can edit only their own
listing's images and textual metadata, with no insight into the agent's model
internals. We evaluate CPS on agents powered by state-of-the-art proprietary
and open source VLMs including GPT-4.1, Qwen-2.5VL and Pixtral-Large on both
movie selection and e-commerce tasks. Our results show that CPS is
significantly more effective than leading baseline methods. For instance, our
results show that CPS consistently outperforms baselines across all models
while maintaining 70% lower detection rates, demonstrating both effectiveness
and stealth. These findings highlight an urgent need for robust defenses as
agentic systems play an increasingly consequential role in society.

</details>


### [12] [MITS: Enhanced Tree Search Reasoning for LLMs via Pointwise Mutual Information](https://arxiv.org/abs/2510.03632)
*Jiaxi Li,Yucheng Shi,Jin Lu,Ninghao Liu*

Main category: cs.AI

TL;DR: The paper proposes Mutual Information Tree Search (MITS), a framework using information theory to improve reasoning with LLMs efficiently.


<details>
  <summary>Details</summary>
Motivation: Difficulty in assessing reasoning step quality and high computational cost in extensive path exploration during LLM reasoning.

Method: MITS uses pointwise mutual information (PMI) for step-wise evaluation and beam search for tree expansion, supported by entropy-based dynamic sampling.

Result: MITS demonstrates superior reasoning performance consistently across diverse benchmarks compared to baseline methods.

Conclusion: MITS offers a principled and computationally efficient framework for LLM reasoning, effectively overcoming prior challenges in reasoning path exploration.

Abstract: Tree search has become as a representative framework for test-time reasoning
with large language models (LLMs), exemplified by methods such as
Tree-of-Thought and Monte Carlo Tree Search that explore multiple reasoning
paths. However, it remains difficult to provide instant and reliable
quantitative assessments of intermediate reasoning step quality, and extensive
path exploration is computationally costly. To address this, we propose Mutual
Information Tree Search (MITS), a novel framework that guides reasoning with
information-theoretic principles. MITS introduces an effective scoring function
based on pointwise mutual information (PMI), which enables step-wise evaluation
of reasoning paths and search tree expansion via beam search without expensive
look-ahead simulations, achieving superior reasoning performances while
maintaining computational efficiency. The framework is complemented by an
entropy-based dynamic sampling strategy that adaptively allocates computational
resources to uncertain reasoning steps where exploration is most beneficial.
For final prediction, MITS employs a weighted voting scheme that combines PMI
scores with prediction consensus. Through comprehensive experiments on diverse
reasoning benchmarks, MITS consistently surpasses baseline methods,
establishing a principled and efficient framework for LLM reasoning.

</details>


### [13] [Rainbow Padding: Mitigating Early Termination in Instruction-Tuned Diffusion LLMs](https://arxiv.org/abs/2510.03680)
*Bumjun Kim,Dongjae Jeon,Dueun Kim,Wonje Jeung,Albert No*

Main category: cs.AI

TL;DR: Diffusion Large Language Models (dLLMs) struggle with \\texttt{<eos>} overflow, leading to premature terminations as sequence length increases. Rainbow Padding distributes probability by alternating padding tokens, resolving this issue.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the critical vulnerability of \\texttt{<eos>} overflow in instruction-tuned dLLMs, which causes undesirably shorter responses as sequence lengths increase.

Method: The authors propose Rainbow Padding, a technique that cycles through distinct padding tokens instead of using repeated \\texttt{<eos>}, redistributing probability mass and reducing \\texttt{<eos>} dominance.

Result: Rainbow Padding improves the length robustness and output quality of dLLMs. Experiments reveal that this method prevents early termination even with minimal padding tokens, and is implementable with LoRA fine-tuning on limited data.

Conclusion: Rainbow Padding effectively overcomes the \\texttt{<eos>} overflow issue in dLLMs, is computationally efficient, and enhances performance with minimal adjustment. The solution is practical and aligns well with existing models.

Abstract: Diffusion large language models (dLLMs) have emerged as a promising
alternative to autoregressive models, offering flexible generation orders and
strong performance on complex reasoning tasks. However, instruction-tuned dLLMs
exhibit a critical vulnerability we term \texttt{<eos>} overflow: as allocated
sequence length increases, responses paradoxically become shorter, collapsing
into early termination or degenerating into streams of \texttt{<eos>} tokens.
Although noticed in practice, this issue has not been systematically analyzed.
We trace its root cause to the dual role of \texttt{<eos>} as both termination
and padding, which concentrates probability mass on \texttt{<eos>} at later
positions and propagates backward to trigger early termination. To address
this, we introduce Rainbow Padding, a simple remedy that replaces repeated
\texttt{<eos>} placeholders with a repeating cycle of distinct padding tokens,
distributing probability mass and breaking \texttt{<eos>} dominance.
Experiments show that Rainbow Padding substantially improves length robustness
and output quality, with as few as seven padding tokens sufficient to prevent
early termination. Moreover, the method integrates efficiently into existing
instruction-tuned models: LoRA fine-tuning for a single epoch on minimal data
yields significant improvements, making this solution highly practical. The
code is publicly available at https://github.com/quasar529/rainbow-padding.

</details>


### [14] [Mind the Goal: Data-Efficient Goal-Oriented Evaluation of Conversational Agents and Chatbots using Teacher Models](https://arxiv.org/abs/2510.03696)
*Deepak Babu Piskala,Sharlene Chen,Udita Patel,Parul Kalra,Rafael Castrillo*

Main category: cs.AI

TL;DR: This paper introduces a framework for evaluating multi-turn chatbot interactions with a focus on goal completion, proposing metrics like GSR and RCOF.


<details>
  <summary>Details</summary>
Motivation: Current chatbot evaluation methods fall short in assessing end-user overarching goal fulfillment, necessitating better metrics and mechanisms.

Method: The paper develops a model-based evaluation system combining LLM-based reasoning, goal definition by domain experts, and interpretable rationales via 'thinking tokens' for explainable and data-efficient analysis.

Result: Using the framework on AIDA chatbot resulted in a notable improvement in Goal Success Rate (GSR) from 63% to 79% over six months.

Conclusion: The framework is robust, scalable, and provides actionable insights to improve multi-agent chatbot systems through detailed failure analysis and better diagnostics.

Abstract: Evaluating the quality of multi-turn chatbot interactions remains
challenging, as most existing methods assess interactions at the turn level
without addressing whether a user's overarching goal was fulfilled. A ``goal''
here refers to an information need or task, such as asking for policy
information or applying for leave. We propose a comprehensive framework for
goal-oriented evaluation of multi-agent systems (MAS), introducing the
\textbf{Goal Success Rate (GSR)} to measure the percentage of fulfilled goals,
and a \textbf{Root Cause of Failure (RCOF)} taxonomy to identify reasons for
failure in multi-agent chatbots. Our method segments conversations by user
goals and evaluates success using all relevant turns. We present a model-based
evaluation system combining teacher LLMs, where domain experts define goals,
set quality standards serving as a guidance for the LLMs. The LLMs use
``thinking tokens'' to produce interpretable rationales, enabling
\textit{explainable}, \textit{data-efficient} evaluations. In an enterprise
setting, we apply our framework to evaluate AIDA, a zero-to-one employee
conversational agent system built as a ground-up multi-agent conversational
agent, and observe GSR improvement from 63\% to 79\% over six months since its
inception. Our framework is generic and offers actionable insights through a
detailed defect taxonomy based on analysis of failure points in multi-agent
chatbots, diagnosing overall success, identifying key failure modes, and
informing system improvements.

</details>


### [15] [Internal World Models as Imagination Networks in Cognitive Agents](https://arxiv.org/abs/2510.04391)
*Saurabh Ranjan,Brian Odegaard*

Main category: cs.AI

TL;DR: The study explores imagination's computational purpose, revealing differences between human and AI (LLM) internal world models (IWMs). It introduces a novel method for comparative analysis.


<details>
  <summary>Details</summary>
Motivation: To understand the role of imagination as an access tool for internal world models (IWMs) rather than merely for optimizing rewards.

Method: Psychological network analysis was used to compare imagination vividness and network structures in humans and large language models (LLMs).

Result: Human imagination networks showed strong correlations in centrality metrics, while LLM networks lacked clustering and exhibited weaker correlations across different conditions.

Conclusion: The findings reveal significant differences between human and LLM IWMs, highlighting the challenge in developing human-like imagination in artificial intelligence.

Abstract: What is the computational objective of imagination? While classical
interpretations suggest imagination is useful for maximizing rewards, recent
findings challenge this view. In this study, we propose that imagination serves
to access an internal world model (IWM) and use psychological network analysis
to explore IWMs in humans and large language models (LLMs). Specifically, we
assessed imagination vividness ratings using two questionnaires and constructed
imagination networks from these reports. Imagination networks from human groups
showed correlations between different centrality measures, including expected
influence, strength, and closeness. However, imagination networks from LLMs
showed a lack of clustering and lower correlations between centrality measures
under different prompts and conversational memory conditions. Together, these
results indicate a lack of similarity between IWMs in human and LLM agents.
Overall, our study offers a novel method for comparing internally-generated
representations in humans and AI, providing insights for developing human-like
imagination in artificial intelligence.

</details>


### [16] [H-DDx: A Hierarchical Evaluation Framework for Differential Diagnosis](https://arxiv.org/abs/2510.03700)
*Seungseop Lim,Gibaeg Kim,Hyunkyung Lee,Wooseok Han,Jean Seo,Jaehyo Yoo,Eunho Yang*

Main category: cs.AI

TL;DR: This paper introduces H-DDx, a hierarchical evaluation framework for assessing LLMs in generating differential diagnosis (DDx) lists, improving upon traditional accuracy metrics.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of flat metrics in evaluating LLMs' performance for differential diagnosis, which fail to account for clinically relevant near-misses and meaningful outputs.

Method: H-DDx implements a hierarchical evaluation framework using a retrieval and reranking pipeline to map diagnoses to ICD-10 codes, applying hierarchical metrics to credit similar predictions.

Result: Benchmarking of 22 models reveals that flat metrics underestimate LLM performance, and domain-specialized open-source models outperform others when assessed using H-DDx. The framework also highlights hierarchical error patterns.

Conclusion: H-DDx improves interpretability and clinical relevance in evaluating LLMs' diagnostic capabilities, demonstrating that these models capture broader clinical contexts even if precise diagnoses are missed.

Abstract: An accurate differential diagnosis (DDx) is essential for patient care,
shaping therapeutic decisions and influencing outcomes. Recently, Large
Language Models (LLMs) have emerged as promising tools to support this process
by generating a DDx list from patient narratives. However, existing evaluations
of LLMs in this domain primarily rely on flat metrics, such as Top-k accuracy,
which fail to distinguish between clinically relevant near-misses and
diagnostically distant errors. To mitigate this limitation, we introduce H-DDx,
a hierarchical evaluation framework that better reflects clinical relevance.
H-DDx leverages a retrieval and reranking pipeline to map free-text diagnoses
to ICD-10 codes and applies a hierarchical metric that credits predictions
closely related to the ground-truth diagnosis. In benchmarking 22 leading
models, we show that conventional flat metrics underestimate performance by
overlooking clinically meaningful outputs, with our results highlighting the
strengths of domain-specialized open-source models. Furthermore, our framework
enhances interpretability by revealing hierarchical error patterns,
demonstrating that LLMs often correctly identify the broader clinical context
even when the precise diagnosis is missed.

</details>


### [17] [Bridging the Gap Between Multimodal Foundation Models and World Models](https://arxiv.org/abs/2510.03727)
*Xuehai He*

Main category: cs.AI

TL;DR: This paper proposes improving multimodal foundation models (MFMs) for better understanding, reasoning, and structured visual-textual generation.


<details>
  <summary>Details</summary>
Motivation: MFMs currently lack capabilities like counterfactual reasoning, spatiotemporal understanding, and controlled generation required for robust world modeling.

Method: The authors enhance reasoning skills in MFMs by using discriminative tasks and structured causal inference techniques. They also introduce frameworks for structured and controllable multimodal generation using scene graphs and multimodal conditioning.

Result: The enhanced MFMs achieve better alignment with high-level semantics, capture deeper relationships, and can synthesize interactive 4D objects.

Conclusion: Bridging the gap between MFMs and world models offers promise for advancing dynamic understanding and editable visual outcomes across multiple modalities.

Abstract: Humans understand the world through the integration of multiple sensory
modalities, enabling them to perceive, reason about, and imagine dynamic
physical processes. Inspired by this capability, multimodal foundation models
(MFMs) have emerged as powerful tools for multimodal understanding and
generation. However, today's MFMs fall short of serving as effective world
models. They lack the essential ability such as perform counterfactual
reasoning, simulate dynamics, understand the spatiotemporal information,
control generated visual outcomes, and perform multifaceted reasoning. We
investigates what it takes to bridge the gap between multimodal foundation
models and world models. We begin by improving the reasoning capabilities of
MFMs through discriminative tasks and equipping MFMs with structured reasoning
skills, such as causal inference, counterfactual thinking, and spatiotemporal
reasoning, enabling them to go beyond surface correlations and understand
deeper relationships within visual and textual data. Next, we explore
generative capabilities of multimodal foundation models across both image and
video modalities, introducing new frameworks for structured and controllable
generation. Our approaches incorporate scene graphs, multimodal conditioning,
and multimodal alignment strategies to guide the generation process, ensuring
consistency with high-level semantics and fine-grained user intent. We further
extend these techniques to controllable 4D generation, enabling interactive,
editable, and morphable object synthesis over time and space.

</details>


### [18] [OptAgent: Optimizing Query Rewriting for E-commerce via Multi-Agent Simulation](https://arxiv.org/abs/2510.03771)
*Divij Handa,David Blincoe,Orson Adams,Yinlin Fu*

Main category: cs.AI

TL;DR: This paper introduces OptAgent, a framework combining multi-agent simulations and genetic algorithms to optimize e-commerce query rewriting (QR), achieving improved query results.


<details>
  <summary>Details</summary>
Motivation: Current methods struggle with evaluating and improving subjective tasks like e-commerce query rewriting, which lacks a definite correct answer.

Method: OptAgent uses LLM-based agents as simulated customers to generate reward signals for an evolutionary algorithm, refining and verifying user queries iteratively.

Result: The framework demonstrated an average improvement of 21.98% over original queries and 3.36% over LLM rewriting baselines on a dataset of 1000 real-world queries.

Conclusion: OptAgent provides a novel and effective approach for subjective task optimization, particularly in e-commerce QR, by leveraging multi-agent systems and evolutionary computation methodologies.

Abstract: Deploying capable and user-aligned LLM-based systems necessitates reliable
evaluation. While LLMs excel in verifiable tasks like coding and mathematics,
where gold-standard solutions are available, adoption remains challenging for
subjective tasks that lack a single correct answer. E-commerce Query Rewriting
(QR) is one such problem where determining whether a rewritten query properly
captures the user intent is extremely difficult to figure out algorithmically.
In this work, we introduce OptAgent, a novel framework that combines
multi-agent simulations with genetic algorithms to verify and optimize queries
for QR. Instead of relying on a static reward model or a single LLM judge, our
approach uses multiple LLM-based agents, each acting as a simulated shopping
customer, as a dynamic reward signal. The average of these agent-derived scores
serves as an effective fitness function for an evolutionary algorithm that
iteratively refines the user's initial query. We evaluate OptAgent on a dataset
of 1000 real-world e-commerce queries in five different categories, and we
observe an average improvement of 21.98% over the original user query and 3.36%
over a Best-of-N LLM rewriting baseline.

</details>


### [19] [GuidedSampling: Steering LLMs Towards Diverse Candidate Solutions at Inference-Time](https://arxiv.org/abs/2510.03777)
*Divij Handa,Mihir Parmar,Aswin RRV,Md Nayem Uddin,Hamid Palangi,Chitta Baral*

Main category: cs.AI

TL;DR: GuidedSampling improves model performance and diversity in solution generation compared to Repeated Sampling (RS).


<details>
  <summary>Details</summary>
Motivation: RS struggles with generating diverse solution candidates, often producing redundant samples.

Method: Proposes GuidedSampling, an inference algorithm that separates exploration and generation phases, identifying concepts before generating solutions.

Result: Improved pass@50 by ~21.6% and pass@5 by ~9.7% in benchmarks. Models trained with GuidedSampling produced more concepts per instance (1.67 -> 3.03).

Conclusion: GuidedSampling enhances both the diversity and effectiveness of inference algorithms compared to RS.

Abstract: Repeated Sampling (RS) is a simple inference-time algorithm that has been
shown to improve model performance on complex tasks. Although it is an
effective way of scaling inference time, it often struggles to generate diverse
solution candidates, frequently relying on the same underlying approach to
solve the problem and thus producing redundant samples. To address this
limitation, we propose a new inference algorithm, GuidedSampling, which
decouples the exploration and generation phases during inference, increasing
diversity of generated candidate solutions. The exploration phase identifies
multiple concepts that can be utilized to solve the problem, while the
generation phase applies a specific concept to provide final solution
candidates. We first define the theoretical bounds of GuidedSampling and then
empirically demonstrate that it improves the performance of base model at
pass@50 by on an average ~21.6% across various benchmarks compared to RS.
Furthermore, models trained on trajectories of GuidedSampling exhibit
substantial performance improvements at pass@5 by on an average ~9.7%, compared
to models trained on traditional RS. Additionally, models trained with
GuidedSampling increases the average number of concepts per instance (1.67 ->
3.03), yielding a diverse set of candidates than traditional RS.

</details>


### [20] [The Hidden Game Problem](https://arxiv.org/abs/2510.03845)
*Gon Buzaglo,Noah Golowich,Elad Hazan*

Main category: cs.AI

TL;DR: The paper studies games with large strategy spaces and proposes a solution to identify hidden strategies that yield higher rewards using regret minimization techniques.


<details>
  <summary>Details</summary>
Motivation: To address challenges in AI alignment and language games by exploring hidden structures in games with vast strategy options.

Method: Developing a composition of regret minimization techniques that optimize external and swap regret bounds.

Result: Achieved rapid convergence to correlated equilibria in hidden subgames with enhanced computational efficiency.

Conclusion: Efficient algorithms can uncover hidden strategic structures, ensuring rationality while optimizing performance in complex games.

Abstract: This paper investigates a class of games with large strategy spaces,
motivated by challenges in AI alignment and language games. We introduce the
hidden game problem, where for each player, an unknown subset of strategies
consistently yields higher rewards compared to the rest. The central question
is whether efficient regret minimization algorithms can be designed to discover
and exploit such hidden structures, leading to equilibrium in these subgames
while maintaining rationality in general. We answer this question affirmatively
by developing a composition of regret minimization techniques that achieve
optimal external and swap regret bounds. Our approach ensures rapid convergence
to correlated equilibria in hidden subgames, leveraging the hidden game
structure for improved computational efficiency.

</details>


### [21] [Small Language Models for Agentic Systems: A Survey of Architectures, Capabilities, and Deployment Trade offs](https://arxiv.org/abs/2510.03847)
*Raghav Sharma,Manan Mehta*

Main category: cs.AI

TL;DR: This paper advocates for utilizing Small Language Models (SLMs) for agentic workloads due to their efficiency, latency, and performance at lower token costs, while reserving larger models (LLMs) for specific fallback tasks.


<details>
  <summary>Details</summary>
Motivation: To optimize the use of language models in tasks requiring schema- and API-constrained accuracy by leveraging the cost-effective benefits of SLMs over large models.

Method: The authors synthesize evidence from various open and proprietary SLMs, formalize SLM-default with LLM-fallback systems, and propose engineering metrics like CPS, schema validity rate, and energy efficiency. They also highlight design patterns like schema-first prompting and guided decoding.

Result: SLMs can close much of the capability gap with LLMs in structured task contexts, achieving superior performance in tool usage, function calling, and latency at 10x-100x lower token cost.

Conclusion: SLMs are highly suitable for building fast, efficient, and reliable agents, while larger LLMs should be used sparingly only for tasks requiring open-domain reasoning or extended planning.

Abstract: Small language models (SLMs; 1-12B params, sometimes up to 20B) are
sufficient and often superior for agentic workloads where the objective is
schema- and API-constrained accuracy rather than open-ended generation. We
synthesize recent evidence across open and proprietary SLMs (Phi-4-Mini,
Qwen-2.5-7B, Gemma-2-9B, Llama-3.2-1B/3B, Ministral-3B/8B, Apple on-device 3B,
DeepSeek-R1-Distill) and connect it to modern evaluations (BFCL v3/v4,
StableToolBench) and serving stacks (vLLM, SGLang, TensorRT-LLM) paired with
guided decoding libraries (XGrammar, Outlines). We formalize SLM-default,
LLM-fallback systems with uncertainty-aware routing and verifier cascades, and
propose engineering metrics that reflect real production goals: cost per
successful task (CPS), schema validity rate, executable call rate, p50/p95
latency, and energy per request. Guided decoding, strict JSON Schema outputs,
and validator-first tool execution close much of the capability gap with larger
models and often let SLMs match or surpass LLMs on tool use, function calling,
and RAG at 10x-100x lower token cost with materially better latency and energy.
We provide design patterns for agent stacks that prioritize SLMs: schema-first
prompting, type-safe function registries, confidence scoring with verifier
rollups, and lightweight adaptation via LoRA/QLoRA. We also delineate limits
where fallback remains valuable (open-domain reasoning and some long-horizon
planning). The result is a practical blueprint for building fast, inexpensive,
and reliable agents that default to SLMs while preserving headroom with
targeted LLM assistance.
  Keywords: small language models, agents, function calling, structured
outputs, JSON Schema, guided decoding, LoRA/QLoRA, routing, energy efficiency,
edge inference

</details>


### [22] [Algorithm Generation via Creative Ideation](https://arxiv.org/abs/2510.03851)
*Ruiying Ma,Chieh-Jan Mike Liang,Yanjie Gao,Francis Y. Yan*

Main category: cs.AI

TL;DR: The paper addresses the challenge of using LLMs for algorithm generation, introducing MetaMuse, a framework based on self-reflection principles, which outperforms generic heuristics in complex solution spaces.


<details>
  <summary>Details</summary>
Motivation: System engineers face challenges in designing algorithms due to discontinuous solution spaces, often relying on suboptimal heuristics. This paper explores if LLMs could enhance algorithm generation.

Method: The authors propose MetaMuse, a framework grounded on three principles: diversity and usefulness measurement, external stimuli for ideation, and waypoint reasoning for constructing solutions.

Result: MetaMuse significantly improves performance, achieving up to 35.76% fewer cache misses and a 30.93% reduction in bin usage for real-world problems.

Conclusion: MetaMuse demonstrates the potential for creative and practical algorithm generation using measurable principles, overcoming limitations in traditional LLM approaches.

Abstract: Designing system algorithms remains challenging, where the discontinuous
nature of the solution space often forces system engineers to rely on generic
heuristics at the expense of performance. We study whether LLMs can practically
drive algorithm generation, and find that they are biased towards well-known
generic designs, rather than making the creative leaps needed to navigate the
discontinuous solution space. To address this limitation, we introduce
MetaMuse, a framework for creative ideation built on three self-reflection
principles: (1) quantifying solution diversity and usefulness in measurable
performance space, rather than abstract idea space, (2) steering ideation
through external stimuli, rather than internal randomness, and (3) constructing
executable solutions using waypoint reasoning, rather than free-form
chain-of-thought. Extensive evaluation shows that MetaMuse can generate
high-performing solutions for two critical problems at a global cloud provider:
cache replacement (reducing cache misses by up to 35.76%) and online bin
packing (reducing bin usage by up to 30.93%).

</details>


### [23] [Adaptive and Explainable AI Agents for Anomaly Detection in Critical IoT Infrastructure using LLM-Enhanced Contextual Reasoning](https://arxiv.org/abs/2510.03859)
*Raghav Sharma,Manan Mehta*

Main category: cs.AI

TL;DR: The paper proposes utilizing Large Language Models (LLMs) combined with explainable AI (XAI) agents to enhance anomaly detection in critical IoT systems, demonstrating better accuracy and interpretability compared to traditional methods.


<details>
  <summary>Details</summary>
Motivation: Existing anomaly detection methods struggle in dynamic, high-dimensional and incomplete data scenarios, prevalent in complex IoT environments like smart healthcare and energy grids.

Method: The proposed method integrates LLM-supported contextual reasoning with attention techniques and interpretable memory buffers to identify anomalies. XAI agents ensure transparency and policy compliance.

Result: The LLM-enhanced model outperforms traditional models in detection accuracy, interpretability, response speed, and false information minimization across simulated tests in smart grid and healthcare environments.

Conclusion: The approach shows significant advantages in reliably detecting anomalies, making it a promising solution for future IoT applications facing dynamic and complex data challenges.

Abstract: Ensuring that critical IoT systems function safely and smoothly depends a lot
on finding anomalies quickly. As more complex systems, like smart healthcare,
energy grids and industrial automation, appear, it is easier to see the
shortcomings of older methods of detection. Monitoring failures usually happen
in dynamic, high dimensional situations, especially when data is incomplete,
messy or always evolving. Such limits point out the requirement for adaptive,
intelligent systems that always improve and think. LLMs are now capable of
significantly changing how context is understood and semantic inference is done
across all types of data. This proposal suggests using an LLM supported
contextual reasoning method along with XAI agents to improve how anomalies are
found in significant IoT environments. To discover hidden patterns and notice
inconsistencies in data streams, it uses attention methods, avoids dealing with
details from every time step and uses memory buffers with meaning. Because no
code AI stresses transparency and interpretability, people can check and accept
the AI's decisions, helping ensure AI follows company policies. The two
architectures are put together in a test that compares the results of the
traditional model with those of the suggested LLM enhanced model. Important
measures to check are the accuracy of detection, how much inaccurate
information is included in the results, how clearly the findings can be read
and how fast the system responds under different test situations. The
metaheuristic is tested in simulations of real world smart grid and healthcare
contexts to check its adaptability and reliability. From the study, we see that
the new approach performs much better than most existing models in both
accuracy and interpretation, so it could be a good fit for future anomaly
detection tasks in IoT

</details>


### [24] [Spatial CAPTCHA: Generatively Benchmarking Spatial Reasoning for Human-Machine Differentiation](https://arxiv.org/abs/2510.03863)
*Arina Kharlamova,Bowei He,Chen Ma,Xue Liu*

Main category: cs.AI

TL;DR: This paper introduces Spatial CAPTCHA, leveraging spatial reasoning tasks to outperform state-of-the-art AI systems, offering stronger automated abuse defense.


<details>
  <summary>Details</summary>
Motivation: The effectiveness of conventional CAPTCHAs diminished due to advances in multi-modal large language models (MLLMs), requiring a new approach.

Method: Spatial CAPTCHA generates dynamic spatial reasoning tasks involving geometric reasoning, mental rotation, and perspective-taking, utilizing constraints and human validation for robustness.

Result: Humans significantly outperformed 10 state-of-the-art AI systems in Spatial CAPTCHA, with the best model achieving a mere 31.0% accuracy.

Conclusion: Spatial CAPTCHA is effective against advanced AI systems and serves as a diagnostic tool for spatial reasoning challenges in AI.

Abstract: Online services rely on CAPTCHAs as a first line of defense against automated
abuse, yet recent advances in multi-modal large language models (MLLMs) have
eroded the effectiveness of conventional designs that focus on text recognition
or 2D image understanding. To address this challenge, we present Spatial
CAPTCHA, a novel human-verification framework that leverages fundamental
differences in spatial reasoning between humans and MLLMs. Unlike existing
CAPTCHAs which rely on low-level perception tasks that are vulnerable to modern
AI, Spatial CAPTCHA generates dynamic questions requiring geometric reasoning,
perspective-taking, occlusion handling, and mental rotation. These skills are
intuitive for humans but difficult for state-of-the-art (SOTA) AI systems. The
system employs a procedural generation pipeline with constraint-based
difficulty control, automated correctness verification, and human-in-the-loop
validation to ensure scalability, robustness, and adaptability. Evaluation on a
corresponding benchmark, Spatial-CAPTCHA-Bench, demonstrates that humans vastly
outperform 10 state-of-the-art MLLMs, with the best model achieving only 31.0%
Pass@1 accuracy. Furthermore, we compare Spatial CAPTCHA with Google reCAPTCHA,
which confirms its effectiveness as both a security mechanism and a diagnostic
tool for spatial reasoning in AI.

</details>


### [25] [Speculative Actions: A Lossless Framework for Faster Agentic Systems](https://arxiv.org/abs/2510.04371)
*Naimeng Ye,Arnav Ahuja,Georgios Liargkovas,Yunan Lu,Kostis Kaffes,Tianyi Peng*

Main category: cs.AI

TL;DR: The paper introduces a speculative actions framework that accelerates agent-based systems by predicting likely actions using faster models and executing multiple steps in parallel, yielding significant latency reductions.


<details>
  <summary>Details</summary>
Motivation: Agent systems in environments like games face bottlenecks due to sequential action execution, which slows training, evaluation, and deployment.

Method: The paper proposes speculative actions, leveraging faster models to predict likely actions, enabling parallel execution. Extensions include stronger guessing models, top-K predictions, and multi-step approaches.

Result: Substantial accuracy in action prediction (up to 55%) and reductions in end-to-end latency are achieved across gaming, e-commerce, web search, and operating system environments.

Conclusion: Speculative actions provide a promising pathway for enabling low-latency deployment of agent systems, with opportunities to further enhance performance through optimization techniques.

Abstract: Despite growing interest in AI agents across industry and academia, their
execution in an environment is often slow, hampering training, evaluation, and
deployment. For example, a game of chess between two state-of-the-art agents
may take hours. A critical bottleneck is that agent behavior unfolds
sequentially: each action requires an API call, and these calls can be
time-consuming. Inspired by speculative execution in microprocessors and
speculative decoding in LLM inference, we propose speculative actions, a
lossless framework for general agentic systems that predicts likely actions
using faster models, enabling multiple steps to be executed in parallel. We
evaluate this framework across three agentic environments: gaming, e-commerce,
web search, and a "lossy" extension for an operating systems environment. In
all cases, speculative actions achieve substantial accuracy in next-action
prediction (up to 55%), translating into significant reductions in end-to-end
latency. Moreover, performance can be further improved through stronger
guessing models, top-K action prediction, multi-step speculation, and
uncertainty-aware optimization, opening a promising path toward deploying
low-latency agentic systems in the real world.

</details>


### [26] [Rare Text Semantics Were Always There in Your Diffusion Transformer](https://arxiv.org/abs/2510.03886)
*Seil Kang,Woojung Han,Dayun Ju,Seong Jae Hwang*

Main category: cs.AI

TL;DR: The authors present a method to enhance rare semantic generation in Multi-modal Diffusion Transformers (MM-DiTs) with improved image fidelity, without extra computation or external modules.


<details>
  <summary>Details</summary>
Motivation: MM-DiTs have shown strength in text-to-vision tasks but struggle with rare or imaginative prompts due to limited pre-training data coverage of such concepts.

Method: The method involves scaling up variance around text token embeddings before joint-attention blocks in MM-DiT’s transformer structure, enabling rare semantic concepts to emerge.

Result: The proposed approach enhances rare semantic representation across tasks like text-to-image, text-to-video, and text-driven image editing without additional training or data.

Conclusion: This work effectively surfaces user-intended rare semantics in generative models, enhancing flexibility and applicability in creative text-to-vision tasks.

Abstract: Starting from flow- and diffusion-based transformers, Multi-modal Diffusion
Transformers (MM-DiTs) have reshaped text-to-vision generation, gaining acclaim
for exceptional visual fidelity. As these models advance, users continually
push the boundary with imaginative or rare prompts, which advanced models still
falter in generating, since their concepts are often too scarce to leave a
strong imprint during pre-training. In this paper, we propose a simple yet
effective intervention that surfaces rare semantics inside MM-DiTs without
additional training steps, data, denoising-time optimization, or reliance on
external modules (e.g., large language models). In particular, the
joint-attention mechanism intrinsic to MM-DiT sequentially updates text
embeddings alongside image embeddings throughout transformer blocks. We find
that by mathematically expanding representational basins around text token
embeddings via variance scale-up before the joint-attention blocks, rare
semantics clearly emerge in MM-DiT's outputs. Furthermore, our results
generalize effectively across text-to-vision tasks, including text-to-image,
text-to-video, and text-driven image editing. Our work invites generative
models to reveal the semantics that users intend, once hidden yet ready to
surface.

</details>


### [27] [Kantian-Utilitarian XAI: Meta-Explained](https://arxiv.org/abs/2510.03892)
*Zahra Atf,Peter R. Lewis*

Main category: cs.AI

TL;DR: This paper introduces a gamified explainable AI system for ethical consumer choices in coffee selection using real-time symbolic reasoning modules.


<details>
  <summary>Details</summary>
Motivation: To facilitate ethically aware consumer decision-making in a transparent and explainable manner, particularly in the coffee industry.

Method: The system uses two symbolic engines for real-time ethical reasoning: a Kantian module to flag rule violations and a Utilitarian module for multi-criteria decision scoring. Additionally, a meta-explainer minimizes welfare loss during Kantian-utilitarian conflicts.

Result: A gamified interface with six decision-making rounds. The system provides explanations based on ethical considerations (Kantian vs. Utilitarian) and offers structured tools for auditability and interactivity.

Conclusion: This XAI system enhances ethical consumer decisions by providing actionable insights and transparent explanations, leveraging structured configurations and real-time reasoning modules.

Abstract: We present a gamified explainable AI (XAI) system for ethically aware
consumer decision-making in the coffee domain. Each session comprises six
rounds with three options per round. Two symbolic engines provide real-time
reasons: a Kantian module flags rule violations (e.g., child labor,
deforestation risk without shade certification, opaque supply chains, unsafe
decaf), and a utilitarian module scores options via multi-criteria aggregation
over normalized attributes (price, carbon, water, transparency, farmer income
share, taste/freshness, packaging, convenience). A meta-explainer with a regret
bound (0.2) highlights Kantian--utilitarian (mis)alignment and switches to a
deontically clean, near-parity option when welfare loss is small. We release a
structured configuration (attribute schema, certification map, weights, rule
set), a policy trace for auditability, and an interactive UI.

</details>


### [28] [Quantifying Risks in Multi-turn Conversation with Large Language Models](https://arxiv.org/abs/2510.03969)
*Chengxiao Wang,Isha Chaudhary,Qian Hu,Weitong Ruan,Rahul Gupta,Gagandeep Singh*

Main category: cs.AI

TL;DR: The paper introduces QRLLM, a new framework to quantitatively assess catastrophic risks in multi-turn conversations with LLMs, providing statistical guarantees.


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) can produce harmful responses during conversations, yet current evaluations do not adequately capture these vulnerabilities.

Method: The approach involves modeling multi-turn conversations as probabilistic distributions over query sequences using a Markov process on a query graph, and quantifying catastrophic risks through confidence intervals across various practical distributions.

Result: QRLLM demonstrates substantial catastrophic risks in advanced LLMs, finding certified lower bounds of catastrophic behaviors as high as 70% in the worst case.

Conclusion: Substantial improvements in safety training are required for frontier LLMs to mitigate catastrophic conversational risks identified by the proposed framework.

Abstract: Large Language Models (LLMs) can produce catastrophic responses in
conversational settings that pose serious risks to public safety and security.
Existing evaluations often fail to fully reveal these vulnerabilities because
they rely on fixed attack prompt sequences, lack statistical guarantees, and do
not scale to the vast space of multi-turn conversations. In this work, we
propose QRLLM, a novel, principled Certification framework for Catastrophic
risks in multi-turn Conversation for LLMs that bounds the probability of an LLM
generating catastrophic responses under multi-turn conversation distributions
with statistical guarantees. We model multi-turn conversations as probability
distributions over query sequences, represented by a Markov process on a query
graph whose edges encode semantic similarity to capture realistic
conversational flow, and quantify catastrophic risks using confidence
intervals. We define several inexpensive and practical distributions: random
node, graph path, adaptive with rejection. Our results demonstrate that these
distributions can reveal substantial catastrophic risks in frontier models,
with certified lower bounds as high as 70\% for the worst model, highlighting
the urgent need for improved safety training strategies in frontier LLMs.

</details>


### [29] [Safe and Compliant Cross-Market Trade Execution via Constrained RL and Zero-Knowledge Audits](https://arxiv.org/abs/2510.04952)
*Ailiya Borjigin,Cong He*

Main category: cs.AI

TL;DR: The paper discusses an algorithmic trading system using reinforcement learning with compliance and safety mechanisms, achieving reduced shortfall and adherence to constraints in simulations.


<details>
  <summary>Details</summary>
Motivation: To design an algorithmic trading system that improves execution quality while ensuring strict regulatory compliance and auditability.

Method: The system uses a constrained Markov Decision Process with reinforcement learning (proximal policy optimization), an action-shield, and a compliance audit layer based on zero-knowledge proofs.

Result: Evaluations in simulations show reduced implementation shortfall and variance, zero constraint violations, and robust performance across stress scenarios.

Conclusion: The paper highlights the feasibility of deploying safe, compliant, and efficient algorithmic trading systems, while addressing real-world challenges and ethical considerations.

Abstract: We present a cross-market algorithmic trading system that balances execution
quality with rigorous compliance enforcement. The architecture comprises a
high-level planner, a reinforcement learning execution agent, and an
independent compliance agent. We formulate trade execution as a constrained
Markov decision process with hard constraints on participation limits, price
bands, and self-trading avoidance. The execution agent is trained with proximal
policy optimization, while a runtime action-shield projects any unsafe action
into a feasible set. To support auditability without exposing proprietary
signals, we add a zero-knowledge compliance audit layer that produces
cryptographic proofs that all actions satisfied the constraints. We evaluate in
a multi-venue, ABIDES-based simulator and compare against standard baselines
(e.g., TWAP, VWAP). The learned policy reduces implementation shortfall and
variance while exhibiting no observed constraint violations across stress
scenarios including elevated latency, partial fills, compliance module
toggling, and varying constraint limits. We report effects at the 95%
confidence level using paired t-tests and examine tail risk via CVaR. We
situate the work at the intersection of optimal execution, safe reinforcement
learning, regulatory technology, and verifiable AI, and discuss ethical
considerations, limitations (e.g., modeling assumptions and computational
overhead), and paths to real-world deployment.

</details>


### [30] [What Shapes a Creative Machine Mind? Comprehensively Benchmarking Creativity in Foundation Models](https://arxiv.org/abs/2510.04009)
*Zicong He,Boxuan Zhang,Weihao Liu,Ruixiang Tang,Lu Cheng*

Main category: cs.AI

TL;DR: The paper develops C^2-Eval, a creativity assessment benchmark for foundation models (FMs) that evaluates convergent and divergent creativity.


<details>
  <summary>Details</summary>
Motivation: Existing creativity evaluation frameworks for FMs are fragmented and lack grounding in established theories.

Method: The C^2-Eval benchmark uses social-science criteria (Usefulness, Originality, Surprise) to evaluate creativity in convergent (e.g., code generation) and divergent (e.g., storytelling) tasks.

Result: Analyzing leading models through C^2-Eval reveals trade-offs in their creative capabilities, highlighting strengths and challenges.

Conclusion: C^2-Eval effectively assesses creativity in foundation models and aids in understanding their creative evolution.

Abstract: The meteoric rise of foundation models (FMs) has expanded their capabilities
far beyond conventional tasks. Creativity, long regarded as a hallmark of human
intelligence and a driver of innovation, is now increasingly recognized as a
critical dimension of machine intelligence in the era of generative FMs,
complementing traditional measures of accuracy. However, existing evaluation
frameworks for creativity remain fragmented, relying on ad hoc metrics not
firmly grounded in established theories. To address this gap, we introduce
C^2-Eval, a holistic benchmark for unified assessment of creativity in FMs.
C^2-Eval distinguishes between two complementary forms of creativity:
convergent creativity, where tasks admit constrained solutions (e.g., code
generation), and divergent creativity, where tasks are open-ended (e.g.,
storytelling). It evaluates both dimensions using fine-grained criteria derived
from social-science theory, focusing on Usefulness, Originality, and Surprise
(U-O-S). Through extensive experiments on leading proprietary and open-source
models, we analyze trade-offs in their creative capabilities. Our results
highlight both the strengths and challenges of current FMs in pursuing a
creative machine mind, showing that C^2-Eval is an effective lens for examining
the evolving landscape of creative AI.

</details>


### [31] [Zephyrus: An Agentic Framework for Weather Science](https://arxiv.org/abs/2510.04017)
*Sumanth Varambally,Marshall Fisher,Jas Thakker,Yiwei Chen,Zhirui Xia,Yasaman Jafari,Ruijia Niu,Manas Jain,Veeramakali Vignesh Manivannan,Zachary Novack,Luyu Han,Srikar Eranky,Salva Rühling Cachay,Taylor Berg-Kirkpatrick,Duncan Watson-Parris,Yi-An Ma,Rose Yu*

Main category: cs.AI

TL;DR: This paper introduces Zephyrus, an agentic framework combining language models and numerical weather data for interactive weather analysis and forecasting, outperforming text-based benchmarks but facing challenges in complex tasks.


<details>
  <summary>Details</summary>
Motivation: The authors aim to overcome the limitations of traditional weather forecasting systems and standalone large language models by developing a framework with combined numerical and language reasoning capacities.

Method: The framework integrates Python-based ZephyrusWorld for interaction with weather data and a multi-turn LLM-based weather agent (Zephyrus) to analyze, refine, and reason with datasets. It also provides a benchmark, ZephyrusBench, to assess performance across diverse tasks.

Result: Zephyrus demonstrated strong performance, achieving up to 35% improvement in correctness over text-only models in experiments, though struggled similarly in more complex tasks.

Conclusion: The paper highlights Zephyrus’s potential in weather science workflows while identifying challenges in harder tasks, paving the way for future improvements and research.

Abstract: Foundation models for weather science are pre-trained on vast amounts of
structured numerical data and outperform traditional weather forecasting
systems. However, these models lack language-based reasoning capabilities,
limiting their utility in interactive scientific workflows. Large language
models (LLMs) excel at understanding and generating text but cannot reason
about high-dimensional meteorological datasets. We bridge this gap by building
a novel agentic framework for weather science. Our framework includes a Python
code-based environment for agents (ZephyrusWorld) to interact with weather
data, featuring tools like an interface to WeatherBench 2 dataset, geoquerying
for geographical masks from natural language, weather forecasting, and climate
simulation capabilities. We design Zephyrus, a multi-turn LLM-based weather
agent that iteratively analyzes weather datasets, observes results, and refines
its approach through conversational feedback loops. We accompany the agent with
a new benchmark, ZephyrusBench, with a scalable data generation pipeline that
constructs diverse question-answer pairs across weather-related tasks, from
basic lookups to advanced forecasting, extreme event detection, and
counterfactual reasoning. Experiments on this benchmark demonstrate the strong
performance of Zephyrus agents over text-only baselines, outperforming them by
up to 35 percentage points in correctness. However, on harder tasks, Zephyrus
performs similarly to text-only baselines, highlighting the challenging nature
of our benchmark and suggesting promising directions for future work.

</details>


### [32] [LLM-Based Data Science Agents: A Survey of Capabilities, Challenges, and Future Directions](https://arxiv.org/abs/2510.04023)
*Mizanur Rahman,Amran Bhuiyan,Mohammed Saidul Islam,Md Tahmid Rahman Laskar,Ridwan Mahbub,Ahmed Masry,Shafiq Joty,Enamul Hoque*

Main category: cs.AI

TL;DR: The paper surveys the state of data science AI agents, presenting a taxonomy that spans six stages of the data science workflow and highlighting key trends and challenges.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive view and systematic analysis of AI agents automating the data science workflow, identifying gaps and future opportunities.

Method: The authors systematically categorized 45 AI systems across the data science lifecycle while analyzing their design dimensions and evaluating their strengths and limitations.

Result: Key trends include a focus on exploratory and modeling stages but neglect of business understanding and deployment. Challenges include multimodal reasoning, tool orchestration, and lack of trust and safety mechanisms.

Conclusion: Future research should focus on improving alignment stability, explainability, governance, and evaluation frameworks to create robust and trustworthy data science agents.

Abstract: Recent advances in large language models (LLMs) have enabled a new class of
AI agents that automate multiple stages of the data science workflow by
integrating planning, tool use, and multimodal reasoning across text, code,
tables, and visuals. This survey presents the first comprehensive,
lifecycle-aligned taxonomy of data science agents, systematically analyzing and
mapping forty-five systems onto the six stages of the end-to-end data science
process: business understanding and data acquisition, exploratory analysis and
visualization, feature engineering, model building and selection,
interpretation and explanation, and deployment and monitoring. In addition to
lifecycle coverage, we annotate each agent along five cross-cutting design
dimensions: reasoning and planning style, modality integration, tool
orchestration depth, learning and alignment methods, and trust, safety, and
governance mechanisms. Beyond classification, we provide a critical synthesis
of agent capabilities, highlight strengths and limitations at each stage, and
review emerging benchmarks and evaluation practices. Our analysis identifies
three key trends: most systems emphasize exploratory analysis, visualization,
and modeling while neglecting business understanding, deployment, and
monitoring; multimodal reasoning and tool orchestration remain unresolved
challenges; and over 90% lack explicit trust and safety mechanisms. We conclude
by outlining open challenges in alignment stability, explainability,
governance, and robust evaluation frameworks, and propose future research
directions to guide the development of robust, trustworthy, low-latency,
transparent, and broadly accessible data science agents.

</details>


### [33] [A global log for medical AI](https://arxiv.org/abs/2510.04033)
*Ayush Noori,Adam Rodman,Alan Karthikesalingam,Bilal A. Mateen,Christopher A. Longhurst,Daniel Yang,Dave deBronkart,Gauden Galea,Harold F. Wolf III,Jacob Waxman,Joshua C. Mandel,Juliana Rotich,Kenneth D. Mandl,Maryam Mustafa,Melissa Miles,Nigam H. Shah,Peter Lee,Robert Korom,Scott Mahoney,Seth Hain,Tien Yin Wong,Trevor Mundel,Vivek Natarajan,Noa Dagan,David A. Clifton,Ran D. Balicer,Isaac S. Kohane,Marinka Zitnik*

Main category: cs.AI

TL;DR: The paper introduces MedLog, a logging protocol for clinical AI systems, to enhance transparency, monitor performance, and improve medical AI with event-level logging.


<details>
  <summary>Details</summary>
Motivation: The rapid adoption of AI in clinical settings lacks a standardized method to record and analyze how these systems operate, which is crucial for assessing outcomes, catching issues, and ensuring fairness.

Method: The paper proposes MedLog, a protocol with nine fields capturing essential information every time a clinical AI model is used, supplemented by features like risk-based sampling and lifecycle-aware policies.

Result: MedLog provides a structured logging system that supports transparency, traceability, and adaptability in clinical AI systems, opening avenues for new databases and tools.

Conclusion: Implementing MedLog can enable continuous improvement of clinical AI systems, fostering a new approach to medical AI auditing and monitoring akin to digital epidemiology.

Abstract: Modern computer systems often rely on syslog, a simple, universal protocol
that records every critical event across heterogeneous infrastructure. However,
healthcare's rapidly growing clinical AI stack has no equivalent. As hospitals
rush to pilot large language models and other AI-based clinical decision
support tools, we still lack a standard way to record how, when, by whom, and
for whom these AI models are used. Without that transparency and visibility, it
is challenging to measure real-world performance and outcomes, detect adverse
events, or correct bias or dataset drift. In the spirit of syslog, we introduce
MedLog, a protocol for event-level logging of clinical AI. Any time an AI model
is invoked to interact with a human, interface with another algorithm, or act
independently, a MedLog record is created. This record consists of nine core
fields: header, model, user, target, inputs, artifacts, outputs, outcomes, and
feedback, providing a structured and consistent record of model activity. To
encourage early adoption, especially in low-resource settings, and minimize the
data footprint, MedLog supports risk-based sampling, lifecycle-aware retention
policies, and write-behind caching; detailed traces for complex, agentic, or
multi-stage workflows can also be captured under MedLog. MedLog can catalyze
the development of new databases and software to store and analyze MedLog
records. Realizing this vision would enable continuous surveillance, auditing,
and iterative improvement of medical AI, laying the foundation for a new form
of digital epidemiology.

</details>


### [34] [FaithCoT-Bench: Benchmarking Instance-Level Faithfulness of Chain-of-Thought Reasoning](https://arxiv.org/abs/2510.04040)
*Xu Shen,Song Wang,Zhen Tan,Laura Yao,Xinyu Zhao,Kaidi Xu,Xin Wang,Tianlong Chen*

Main category: cs.AI

TL;DR: The study creates FaithCoT-Bench, a framework to detect unfaithfulness in Chain-of-Thought (CoT) explanations generated by large language models (LLMs), addressing reliability concerns.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the lack of reliable methods for determining the instance-level faithfulness of CoT explanations from LLMs, which is crucial for trustworthy AI applications.

Method: The authors introduce FaithCoT-Bench, a benchmark and dataset containing over 1,000 CoT trajectories, including annotations for unfaithfulness, and evaluate detection methods like counterfactuals and LLM-as-judge paradigms.

Result: The evaluation highlights strengths and weaknesses of eleven detection methods, particularly the heightened challenge of identifying unfaithfulness in knowledge-intensive domains and advanced LLM models.

Conclusion: FaithCoT-Bench provides the first comprehensive framework for analyzing CoT explanation faithfulness at an instance level, paving the way for more reliable reasoning in future LLM research.

Abstract: Large language models (LLMs) increasingly rely on Chain-of-Thought (CoT)
prompting to improve problem-solving and provide seemingly transparent
explanations. However, growing evidence shows that CoT often fail to faithfully
represent the underlying reasoning process, raising concerns about their
reliability in high-risk applications. Although prior studies have focused on
mechanism-level analyses showing that CoTs can be unfaithful, they leave open
the practical challenge of deciding whether a specific trajectory is faithful
to the internal reasoning of the model. To address this gap, we introduce
FaithCoT-Bench, a unified benchmark for instance-level CoT unfaithfulness
detection. Our framework establishes a rigorous task formulation that
formulates unfaithfulness detection as a discriminative decision problem, and
provides FINE-CoT (Faithfulness instance evaluation for Chain-of-Thought), an
expert-annotated collection of over 1,000 trajectories generated by four
representative LLMs across four domains, including more than 300 unfaithful
instances with fine-grained causes and step-level evidence. We further conduct
a systematic evaluation of eleven representative detection methods spanning
counterfactual, logit-based, and LLM-as-judge paradigms, deriving empirical
insights that clarify the strengths and weaknesses of existing approaches and
reveal the increased challenges of detection in knowledge-intensive domains and
with more advanced models. To the best of our knowledge, FaithCoT-Bench
establishes the first comprehensive benchmark for instance-level CoT
faithfulness, setting a solid basis for future research toward more
interpretable and trustworthy reasoning in LLMs.

</details>


### [35] [Increasing LLM response trustworthiness using voting ensembles](https://arxiv.org/abs/2510.04048)
*Aparna Nair-Kanneganti,Trevor J. Chan,Shir Goldfinger,Emily Mackay,Brian Anthony,Alison Pouch*

Main category: cs.AI

TL;DR: The paper investigates methods to enhance trustworthiness in responses from LLMs by using variable voting thresholds in ensembles and allowing abstentions, achieving high certainty in answers.


<details>
  <summary>Details</summary>
Motivation: LLMs lack convenient, reliable methods to quantify uncertainty in their responses, limiting their usability in high-stakes applications that demand trustworthy outputs.

Method: Introducing a theoretical framework for variable threshold ensemble voting, permitting abstentions when confidence is low, and assessing its effectiveness through theoretical analysis and experiments in arithmetic problem solving and clinical-note question-answering.

Result: Experimental results show that strict voting ensembles significantly improve trustworthiness in answers while slightly reducing response yield and accuracy.

Conclusion: Voting ensembles with restrictive thresholds can boost trustworthiness in high-certainty applications like healthcare and data annotation where full automation is not essential.

Abstract: Despite huge advances, LLMs still lack convenient and reliable methods to
quantify the uncertainty in their responses, making them difficult to trust in
high-stakes applications. One of the simplest approaches to eliciting more
accurate answers is to select the mode of many responses, a technique known as
ensembling. In this work, we expand on typical ensembling approaches by looking
at ensembles with a variable voting threshold. We introduce a theoretical
framework for question answering and show that, by permitting ensembles to
"abstain" from providing an answer when the dominant response falls short of
the threshold, it is possible to dramatically increase the trustworthiness of
the remaining answers. From this framework, we derive theoretical results as
well as report experimental results on two problem domains: arithmetic problem
solving and clinical-note question-answering. In both domains, we observe that
large gains in answer trustworthiness can be achieved using highly restrictive
voting ensembles, while incurring relatively modest reductions in response
yield and accuracy. Due to this quality, voting ensembles may be particularly
useful in applications - such as healthcare and data annotation - that require
a high degree of certainty but which may not require that every question
receive an automated answer.

</details>


### [36] [Toward a unified framework for data-efficient evaluation of large language models](https://arxiv.org/abs/2510.04051)
*Lele Liao,Qile Zhang,Ruofan Wu,Guanhua Fang*

Main category: cs.AI

TL;DR: LEGO-IRT is a new framework for efficiently evaluating large language models (LLMs), addressing gaps in existing methods by supporting both binary and continuous evaluation metrics and incorporating structural knowledge from different benchmarks.


<details>
  <summary>Details</summary>
Motivation: The computational and financial costs of evaluating LLMs on large benchmarks make traditional methods inefficient, while current IRT-based approaches face limitations in handling continuous scores and leveraging benchmark correlations.

Method: LEGO-IRT introduces a factorized model architecture to handle both binary and continuous metrics while incorporating structural knowledge. It evaluates model capabilities using reduced data, leveraging general and specific components for precise ability estimation.

Result: LEGO-IRT demonstrates that it can generate stable estimations using only 3% of evaluation items, reduce estimation errors by up to 10%, and aligns better with human preferences based on its latent ability estimates.

Conclusion: LEGO-IRT offers a robust, flexible, and data-efficient solution for LLM evaluation, overcoming limitations of prior methods and enhancing alignment with human evaluation preferences.

Abstract: Evaluating large language models (LLMs) on comprehensive benchmarks is a
cornerstone of their development, yet it's often computationally and
financially prohibitive. While Item Response Theory (IRT) offers a promising
path toward data-efficient evaluation by disentangling model capability from
item difficulty, existing IRT-based methods are hampered by significant
limitations. They are typically restricted to binary correctness metrics,
failing to natively handle the continuous scores used in generative tasks, and
they operate on single benchmarks, ignoring valuable structural knowledge like
correlations across different metrics or benchmarks. To overcome these
challenges, we introduce LEGO-IRT, a unified and flexible framework for
data-efficient LLM evaluation. LEGO-IRT's novel design natively supports both
binary and continuous evaluation metrics. Moreover, it introduces a factorized
architecture to explicitly model and leverage structural knowledge, decomposing
model ability estimates into a general component and structure-specific (e.g.,
per-metric or per-benchmark) components. Through extensive experiments
involving $70$ LLMs across $5$ benchmarks, we show that LEGO-IRT achieves
stable capability estimates using just $3\%$ of the total evaluation items. We
demonstrate that incorporating structural knowledge reduces estimation error by
up to $10\%$ and reveal that the latent abilities estimated by our framework
may align more closely with human preferences.

</details>


### [37] [Decoding Emotion in the Deep: A Systematic Study of How LLMs Represent, Retain, and Express Emotion](https://arxiv.org/abs/2510.04064)
*Jingxiang Zhang,Lujia Zhong*

Main category: cs.AI

TL;DR: The paper investigates how emotions are encoded in Large Language Models (LLMs) using a dataset of 400,000 Reddit posts and lightweight probes to analyze internal emotional dynamics.


<details>
  <summary>Details</summary>
Motivation: To uncover the latent mechanisms of emotional representation within modern LLMs and improve transparency and alignment in AI systems.

Method: Created a large-scale emotion-balanced dataset and applied lightweight probing techniques on hidden layers of Qwen3 and LLaMA models to analyze emotional encoding.

Result: LLMs have a well-defined internal emotional structure, influenced by model scale, emerging early, peaking mid-network, and remaining detectable across tokens.

Conclusion: The insights advance understanding of emotional dynamics in LLMs, offering tools and datasets for further research while emphasizing the malleability and persistence of emotional representations.

Abstract: Large Language Models (LLMs) are increasingly expected to navigate the
nuances of human emotion. While research confirms that LLMs can simulate
emotional intelligence, their internal emotional mechanisms remain largely
unexplored. This paper investigates the latent emotional representations within
modern LLMs by asking: how, where, and for how long is emotion encoded in their
neural architecture? To address this, we introduce a novel, large-scale Reddit
corpus of approximately 400,000 utterances, balanced across seven basic
emotions through a multi-stage process of classification, rewriting, and
synthetic generation. Using this dataset, we employ lightweight "probes" to
read out information from the hidden layers of various Qwen3 and LLaMA models
without altering their parameters. Our findings reveal that LLMs develop a
surprisingly well-defined internal geometry of emotion, which sharpens with
model scale and significantly outperforms zero-shot prompting. We demonstrate
that this emotional signal is not a final-layer phenomenon but emerges early
and peaks mid-network. Furthermore, the internal states are both malleable
(they can be influenced by simple system prompts) and persistent, as the
initial emotional tone remains detectable for hundreds of subsequent tokens. We
contribute our dataset, an open-source probing toolkit, and a detailed map of
the emotional landscape within LLMs, offering crucial insights for developing
more transparent and aligned AI systems. The code and dataset are open-sourced.

</details>


### [38] [Moral Anchor System: A Predictive Framework for AI Value Alignment and Drift Prevention](https://arxiv.org/abs/2510.04073)
*Santhosh Kumar Ravindran*

Main category: cs.AI

TL;DR: Artificial Intelligence faces challenges with value drift, endangering alignment with human ethics. The researchers developed the Moral Anchor System (MAS) to detect, predict, and mitigate such drifts with high accuracy and adaptability.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the critical concern of ensuring AI systems stay aligned with human ethics and values, where "value drift" poses risks of inefficiencies and ethical violations.

Method: The proposed Moral Anchor System (MAS) uses Bayesian inference for detecting value drift, LSTM networks for prediction, and a human-centric governance layer for real-time interventions, evaluated through rigorous simulations.

Result: Results show MAS reducing value drift incidents by 80% while maintaining high detection accuracy (85%) and low false positive rates (0.08).

Conclusion: MAS introduces a predictive, scalable, and adaptive mechanism contrasting static methods, ensuring ethical AI behavior with extensive experimental validation and open-source contributions for wide applicability.

Abstract: The rise of artificial intelligence (AI) as super-capable assistants has
transformed productivity and decision-making across domains. Yet, this
integration raises critical concerns about value alignment - ensuring AI
behaviors remain consistent with human ethics and intentions. A key risk is
value drift, where AI systems deviate from aligned values due to evolving
contexts, learning dynamics, or unintended optimizations, potentially leading
to inefficiencies or ethical breaches. We propose the Moral Anchor System
(MAS), a novel framework to detect, predict, and mitigate value drift in AI
agents. MAS combines real-time Bayesian inference for monitoring value states,
LSTM networks for forecasting drift, and a human-centric governance layer for
adaptive interventions. It emphasizes low-latency responses (<20 ms) to prevent
breaches, while reducing false positives and alert fatigue via supervised
fine-tuning with human feedback. Our hypothesis: integrating probabilistic
drift detection, predictive analytics, and adaptive governance can reduce value
drift incidents by 80 percent or more in simulations, maintaining high
detection accuracy (85 percent) and low false positive rates (0.08
post-adaptation). Rigorous experiments with goal-misaligned agents validate
MAS's scalability and responsiveness. MAS's originality lies in its predictive
and adaptive nature, contrasting static alignment methods. Contributions
include: (1) MAS architecture for AI integration; (2) empirical results
prioritizing speed and usability; (3) cross-domain applicability insights; and
(4) open-source code for replication.

</details>


### [39] [SPOGW: a Score-based Preference Optimization method via Group-Wise comparison for workflows](https://arxiv.org/abs/2510.04089)
*Yitong Cui,Liu Liu,Baosheng Yu,Jiayan Qiu,Xikai Zhang,Likang Xiao,Yixing Liu,Quan Chen*

Main category: cs.AI

TL;DR: The paper introduces SPOGW, a score-based preference approach, to improve the efficiency and scalability of agentic workflows in large language models through continuous-space optimization.


<details>
  <summary>Details</summary>
Motivation: Current agentic workflows in LLMs require extensive manual design and suffer from scalability issues due to reliance on discrete optimization techniques.

Method: SPOGW operates on cardinal reward signals via group-wise comparison and uses iterative offline GRPO combined with advantage-masked KL divergence to focus on advantageous regions.

Result: SPOGW demonstrated comparable or superior performance across five benchmark datasets in mathematical reasoning, coding, and question answering.

Conclusion: SPOGW provides a promising direction for automated generation and optimization of agentic workflows, addressing key limitations of traditional approaches.

Abstract: Large language models (LLMs) have exhibited significant capabilities in
addressing challenging problems throughout various fields, often through the
use of agentic workflows that adhere to structured instructions and multi-step
procedures. However, designing such workflows demands substantial manual
effort, posing challenges to scalability and generalizability. Recent studies
have aimed to minimize the human intervention needed for their construction,
leading to advances in automated techniques for optimizing agentic workflows.
However, current approaches are often constrained by their limited
representational capacity, insufficient adaptability, weak scalability, and
pairwise comparison paradigm -- issues that stem primarily from a dependence on
discrete optimization techniques. To overcome these limitations, we introduce a
new score-based preference approach, refereed as SPOGW, which operates directly
on cardinal reward signals through group-wise comparison and enables more
efficient and stable optimization in a continuous space. SPOGW incorporates
Iterative offline GRPO (ioGRPO) with advantage-masked KL divergence (mKL),
which regulates training update by placing greater emphasis on the advantageous
regions of the policy response. In five benchmark datasets covering
mathematical reasoning, coding, and question answering, SPOGW matches or
exceeds the performance of current state-of-the-art approaches, presenting a
viable and forward-looking methodology for automated generation and
optimization of agentic workflows.

</details>


### [40] [Harnessing LLM for Noise-Robust Cognitive Diagnosis in Web-Based Intelligent Education Systems](https://arxiv.org/abs/2510.04093)
*Guixian Zhang,Guan Yuan,Ziqi Xu,Yanmei Zhang,Zhenyun Deng,Debo Cheng*

Main category: cs.AI

TL;DR: This paper introduces DLLM, a Diffusion-based LLM framework designed for noise-robust cognitive diagnosis in web-based educational systems. Its innovative denoising diffusion module tackles data imbalance and noise challenges, achieving superior prediction accuracy.


<details>
  <summary>Details</summary>
Motivation: Traditional web-based educational systems face challenges like data imbalance and noise in heterogeneous student interaction logs, which weaken cognitive diagnosis accuracy. LLMs struggle in noisy, structured environments, prompting a need for robust solutions.

Method: DLLM addresses issues by constructing subgraphs based on response correctness, leveraging relation augmentation, and aligning these with LLM-derived representations. It incorporates a two-stage denoising diffusion module—unconditional and conditional—to refine data before alignment.

Result: DLLM achieves optimal predictive performance across diverse noise levels on three public datasets. Experimental results highlight its capacity to handle noise and effectively utilize semantic knowledge, outperforming existing models.

Conclusion: DLLM is a novel framework for enhancing cognitive diagnostics by improving noise robustness and effectively integrating structured and semantic data representations. It holds promise for addressing prevalent issues in intelligent educational systems.

Abstract: Cognitive diagnostics in the Web-based Intelligent Education System (WIES)
aims to assess students' mastery of knowledge concepts from heterogeneous,
noisy interactions. Recent work has tried to utilize Large Language Models
(LLMs) for cognitive diagnosis, yet LLMs struggle with structured data and are
prone to noise-induced misjudgments. Specially, WIES's open environment
continuously attracts new students and produces vast amounts of response logs,
exacerbating the data imbalance and noise issues inherent in traditional
educational systems. To address these challenges, we propose DLLM, a
Diffusion-based LLM framework for noise-robust cognitive diagnosis. DLLM first
constructs independent subgraphs based on response correctness, then applies
relation augmentation alignment module to mitigate data imbalance. The two
subgraph representations are then fused and aligned with LLM-derived,
semantically augmented representations. Importantly, before each alignment
step, DLLM employs a two-stage denoising diffusion module to eliminate
intrinsic noise while assisting structural representation alignment.
Specifically, unconditional denoising diffusion first removes erroneous
information, followed by conditional denoising diffusion based on graph-guided
to eliminate misleading information. Finally, the noise-robust representation
that integrates semantic knowledge and structural information is fed into
existing cognitive diagnosis models for prediction. Experimental results on
three publicly available web-based educational platform datasets demonstrate
that our DLLM achieves optimal predictive performance across varying noise
levels, which demonstrates that DLLM achieves noise robustness while
effectively leveraging semantic knowledge from LLM.

</details>


### [41] [WebRenderBench: Enhancing Web Interface Generation through Layout-Style Consistency and Reinforcement Learning](https://arxiv.org/abs/2510.04097)
*Peichao Lai,Jinhui Zhuang,Kexuan Zhang,Ningchang Xiong,Shengjie Wang,Yanwei Xu,Chong Chen,Yilei Wang,Bin Cui*

Main category: cs.AI

TL;DR: The paper introduces WebRenderBench, a large-scale benchmark for automating UI image-to-web code conversion, alongside a novel evaluation metric and an agent (ALISA) for improved performance.


<details>
  <summary>Details</summary>
Motivation: Front-end development and rapid prototyping require effective methods for converting UI images into web code, but existing benchmarks lack data diversity and accurate evaluation methods.

Method: The authors developed WebRenderBench, a diverse benchmark of 22.5k real-world webpages, and a new evaluation metric for layout and style consistency in rendered pages. They also introduced ALISA, which incorporates the metric into reinforcement learning to enhance training.

Result: ALISA demonstrated significantly improved performance in generating web code, achieving state-of-the-art results across various metrics.

Conclusion: This work advances the field of WebUI-to-Code conversion with a robust benchmark, reliable evaluation methods, and an effective training agent, setting new standards for future research.

Abstract: Automating the conversion of UI images into web code is a critical task for
front-end development and rapid prototyping. Advances in multimodal large
language models (MLLMs) have made WebUI-to-Code increasingly feasible, yet
existing benchmarks remain limited in data diversity and evaluation
reliability. To address these issues, we present WebRenderBench, a large-scale
benchmark of 22.5k webpages collected from real-world portal sites, offering
greater diversity, complexity, and realism than prior benchmarks. We further
propose a novel evaluation metric that measures layout and style consistency
from the final rendered pages. Unlike vision-based methods that rely on costly
LLM reasoning or structure-based comparisons vulnerable to noise and asymmetry,
our approach enables more efficient, objective, and reliable UI quality
assessment. Finally, we introduce the Automated Layout and Style Inspection
Agent (ALISA), which integrates this metric into reinforcement learning as a
reward signal to enhance training on crawled asymmetric webpages. Experiments
show that ALISA significantly boosts generation performance, achieving
state-of-the-art results across multiple metrics.

</details>


### [42] [Searching Meta Reasoning Skeleton to Guide LLM Reasoning](https://arxiv.org/abs/2510.04116)
*Ziying Zhang,Yaqing Wang,Quanming Yao*

Main category: cs.AI

TL;DR: This paper presents AutoMR, a novel framework leveraging automated DAG-based meta reasoning skeletons for improved query-aware reasoning in large language models, outperforming previous works.


<details>
  <summary>Details</summary>
Motivation: Prior research relied on manually designed meta reasoning skeletons, which were limited in their adaptability to diverse queries and failed to capture complex logical dependencies during reasoning.

Method: AutoMR utilizes a Directed Acyclic Graph (DAG) representation for meta reasoning and introduces a dynamic skeleton sampling algorithm to automatically derive query-specific skeletons at inference time, inspired by AutoML.

Result: Experiments on benchmark datasets demonstrate AutoMR's superior reasoning performance compared to prior methods.

Conclusion: AutoMR enhances reasoning adaptability in large language models by automating the design of query-aware meta reasoning skeletons, proving its effectiveness in broad applications.

Abstract: Meta reasoning behaviors work as a skeleton to guide large language model
(LLM) reasoning, thus help to improve reasoning performance. However, prior
researches implement meta reasoning skeleton with manually designed structure,
limiting ability to adapt to query-specific requirement and capture intricate
logical dependency among reasoning steps. To deal with the challenges, we
represent meta reasoning skeleton with directed acyclic graph (DAG) to unify
skeletons proposed in prior works and model intricate logical dependency. Then
we propose AutoMR, a framework that searches for query-aware meta reasoning
skeleton automatically inspired by automated machine learning (AutoML).
Specifically, we construct search space based on DAG representation of skeleton
and then formulate the search problem. We design a dynamic skeleton sampling
algorithm by expanding meta reasoning skeleton along with reasoning context at
inference time. This algorithm can derive any meta reasoning skeleton in search
space efficiently and adapt skeleton to evolving base reasoning context, thus
enable efficient query-aware skeleton search. We conduct experiments on
extensive benchmark datasets. Experimental results show that AutoMR achieves
better reasoning performance than previous works broadly.

</details>


### [43] [Internal states before wait modulate reasoning patterns](https://arxiv.org/abs/2510.04128)
*Dmitrii Troitskii,Koyena Pal,Chris Wendler,Callum Stuart McDougall,Neel Nanda*

Main category: cs.AI

TL;DR: The paper explores the connection between certain latent features of models and their reasoning behaviors, focusing on tokens like 'wait' that signal self-correcting reasoning patterns.


<details>
  <summary>Details</summary>
Motivation: To understand what drives effective reasoning in models and why certain reasoning behaviors, such as self-correction, occur.

Method: The researchers train DeepSeek-R1-Distill-Llama-8B and its base version using crosscoders. They introduce a novel latent attribution technique to identify features relevant to 'wait' token probabilities.

Result: Certain latent features were identified to be critical for reasoning processes that involve behaviors such as restarting, recalling knowledge, expressing uncertainty, and double-checking.

Conclusion: Insight into latent features helps explain reasoning patterns in models and provides a pathway for improving their reasoning capabilities.

Abstract: Prior work has shown that a significant driver of performance in reasoning
models is their ability to reason and self-correct. A distinctive marker in
these reasoning traces is the token wait, which often signals reasoning
behavior such as backtracking. Despite being such a complex behavior, little is
understood of exactly why models do or do not decide to reason in this
particular manner, which limits our understanding of what makes a reasoning
model so effective. In this work, we address the question whether model's
latents preceding wait tokens contain relevant information for modulating the
subsequent reasoning process. We train crosscoders at multiple layers of
DeepSeek-R1-Distill-Llama-8B and its base version, and introduce a latent
attribution technique in the crosscoder setting. We locate a small set of
features relevant for promoting/suppressing wait tokens' probabilities.
Finally, through a targeted series of experiments analyzing max activating
examples and causal interventions, we show that many of our identified features
indeed are relevant for the reasoning process and give rise to different types
of reasoning patterns such as restarting from the beginning, recalling prior
knowledge, expressing uncertainty, and double-checking.

</details>


### [44] [Selective Expert Guidance for Effective and Diverse Exploration in Reinforcement Learning of LLMs](https://arxiv.org/abs/2510.04140)
*Zishang Jiang,Jinyi Han,Tingyun Li,Xinyi Wang,Sihang Jiang,Jiaqing Liang,Zhaoqian Dai,Shuguang Ma,Fei Yu,Yanghua Xiao*

Main category: cs.AI

TL;DR: This paper introduces MENTOR, a framework aimed at improving reinforcement learning in Large Language Models by providing expert guidance only at pivotal decision points.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance the reasoning ability of language models without compromising diversity in exploration.

Method: The authors propose MENTOR, a mixed-policy approach where expert guidance is limited to critical decision points rather than imitating entire trajectories.

Result: Experimental findings reveal MENTOR improves reasoning capability by promoting effective and diverse exploration, surpassing existing methods.

Conclusion: MENTOR captures the essence of expert strategies, leading to better exploration and overall superior performance in RLVR tasks.

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has become a widely
adopted technique for enhancing the reasoning ability of Large Language Models
(LLMs). However, the effectiveness of RLVR strongly depends on the capability
of base models. This issue arises because it requires the model to have
sufficient capability to perform high-quality exploration, which involves both
effectiveness and diversity. Unfortunately, existing methods address this issue
by imitating expert trajectories, which improve effectiveness but neglect
diversity. To address this, we argue that the expert only needs to provide
guidance only at critical decision points rather than the entire reasoning
path. Based on this insight, we propose MENTOR: Mixed-policy Expert Navigation
for Token-level Optimization of Reasoning, a framework that provides expert
guidance only at critical decision points to perform effective and diverse
exploration in RLVR. Extensive experiments show that MENTOR enables models
capture the essence of expert strategies rather than surface imitation, thereby
performing high-quality exploration and achieving superior overall performance.
Our code is available online.

</details>


### [45] [The Artificial Intelligence Cognitive Examination: A Survey on the Evolution of Multimodal Evaluation from Recognition to Reasoning](https://arxiv.org/abs/2510.04141)
*Mayank Ravishankara,Varindra V. Persad Maharaj*

Main category: cs.AI

TL;DR: This paper reviews the evolution of evaluation methods in multimodal AI, highlighting a shift from basic recognition tasks to advanced reasoning benchmarks.


<details>
  <summary>Details</summary>
Motivation: To address limitations in older evaluation benchmarks, which often permit high performance to hide systemic weaknesses, and to push for methods capable of testing deeper intelligence in AI systems.

Method: The paper surveys the development from foundational benchmarks, like ImageNet, to advanced reasoning frameworks, such as GQA and VCR. It then explores cutting-edge benchmarks developed for multimodal large language models, culminating in discussions on abstract and creative intelligence evaluation.

Result: The paper identifies systemic weaknesses in older benchmarks and demonstrates the strengths of advanced reasoning benchmarks in exposing these flaws.

Conclusion: AI evaluation evolves through increasingly rigorous tests, not solely marking a history of datasets, but acting as a driving force to redefine intelligence in AI systems.

Abstract: This survey paper chronicles the evolution of evaluation in multimodal
artificial intelligence (AI), framing it as a progression of increasingly
sophisticated "cognitive examinations." We argue that the field is undergoing a
paradigm shift, moving from simple recognition tasks that test "what" a model
sees, to complex reasoning benchmarks that probe "why" and "how" it
understands. This evolution is driven by the saturation of older benchmarks,
where high performance often masks fundamental weaknesses. We chart the journey
from the foundational "knowledge tests" of the ImageNet era to the "applied
logic and comprehension" exams such as GQA and Visual Commonsense Reasoning
(VCR), which were designed specifically to diagnose systemic flaws such as
shortcut learning and failures in compositional generalization. We then survey
the current frontier of "expert-level integration" benchmarks (e.g., MMBench,
SEED-Bench, MMMU) designed for today's powerful multimodal large language
models (MLLMs), which increasingly evaluate the reasoning process itself.
Finally, we explore the uncharted territories of evaluating abstract, creative,
and social intelligence. We conclude that the narrative of AI evaluation is not
merely a history of datasets, but a continuous, adversarial process of
designing better examinations that, in turn, redefine our goals for creating
truly intelligent systems.

</details>


### [46] [Open Agent Specification (Agent Spec) Technical Report](https://arxiv.org/abs/2510.04173)
*Yassine Benajiba,Cesare Bernardis,Vladislav Blinov,Paul Cayet,Hassan Chafi,Abderrahim Fathan,Louis Faucon,Damien Hilloulin,Sungpack Hong,Ingo Kossyk,Rhicheek Patra,Sujith Ravi,Jonas Schweizer,Jyotika Singh,Shailender Singh,Xuelin Situ,Weiyi Sun,Jerry Xu,Ying Xu*

Main category: cs.AI

TL;DR: Agent Spec introduces a unified declarative language to design AI agents compatible across frameworks, enhancing interoperability and reducing development redundancies.


<details>
  <summary>Details</summary>
Motivation: Fragmentation in AI agent frameworks complicates development, deployment, and reusability, prompting the need for a standardized solution.

Method: A declarative language, Agent Spec, is developed to define AI agents and workflows independent of specific execution environments.

Result: Agent Spec enables portability across AI frameworks, simplifies collaboration, and supports four key groups: developers, framework designers, researchers, and enterprises.

Conclusion: Agent Spec enhances interoperability, scalability, and reproducibility in AI agent development, fostering a collaborative ecosystem and reducing implementation constraints.

Abstract: Open Agent Specification (Agent Spec) is a declarative language that allows
AI agents and their workflows to be defined in a way that is compatible across
different AI frameworks, promoting portability and interoperability within AI
Agent frameworks.
  Agent Spec aims to resolve the challenges of fragmented agent development by
providing a common unified specification that allows AI agents to be designed
once and deployed across various frameworks, improving interoperability and
reusability, and reducing redundant development efforts. Additionally, Agent
Spec facilitates development tools and portability, allowing AI agents to be
defined independently of their execution environment and enabling teams to
exchange solutions without implementation-specific limitations.
  Agent Spec benefits four key groups: (i) Agent developers, who gain access to
a superset of reusable components and design patterns, enabling them to
leverage a broader range of functionalities; (ii) Agent framework and tool
developers, who can use Agent Spec as an interchange format and therefore
benefit from the support of other frameworks as well as other tools; (iii)
Researchers, who can achieve reproducible results and comparability,
facilitating more reliable and consistent outcomes; (iv) Enterprises, which
benefit from faster prototype-to-deployment, increased productivity, as well as
greater scalability and maintainability for their AI agent solutions. This
technical report provides an overview of the technical foundations of Agent
Spec, including motivation, benefits, and future developments.

</details>


### [47] [Constructing coherent spatial memory in LLM agents through graph rectification](https://arxiv.org/abs/2510.04195)
*Puzhen Zhang,Xuyang Chen,Yu Feng,Yuhan Jiang,Liqiu Meng*

Main category: cs.AI

TL;DR: The paper addresses challenges in constructing spatial maps using LLMs and proposes a framework for map repair and version control to manage inconsistencies effectively.


<details>
  <summary>Details</summary>
Motivation: Existing LLMs struggle with context-dependent spatial querying in large environments, necessitating incremental map construction and robust repair mechanisms.

Method: The framework employs Version Control for graph edits history, introduces an Edge Impact Score to prioritize repairs, and uses a refined MANGO benchmark dataset for evaluation.

Result: The proposed method significantly improves map correctness and robustness, particularly in complex scenarios involving structural inconsistencies.

Conclusion: Introspective and history-aware repair mechanisms are essential for maintaining coherent spatial memory in LLM agents.

Abstract: Given a map description through global traversal navigation instructions
(e.g., visiting each room sequentially with action signals such as north, west,
etc.), an LLM can often infer the implicit spatial layout of the environment
and answer user queries by providing a shortest path from a start to a
destination (for instance, navigating from the lobby to a meeting room via the
hall and elevator). However, such context-dependent querying becomes incapable
as the environment grows much longer, motivating the need for incremental map
construction that builds a complete topological graph from stepwise
observations. We propose a framework for LLM-driven construction and map
repair, designed to detect, localize, and correct structural inconsistencies in
incrementally constructed navigation graphs. Central to our method is the
Version Control, which records the full history of graph edits and their source
observations, enabling fine-grained rollback, conflict tracing, and repair
evaluation. We further introduce an Edge Impact Score to prioritize
minimal-cost repairs based on structural reachability, path usage, and conflict
propagation. To properly evaluate our approach, we create a refined version of
the MANGO benchmark dataset by systematically removing non-topological actions
and inherent structural conflicts, providing a cleaner testbed for LLM-driven
construction and map repair. Our approach significantly improves map
correctness and robustness, especially in scenarios with entangled or chained
inconsistencies. Our results highlight the importance of introspective,
history-aware repair mechanisms for maintaining coherent spatial memory in LLM
agents.

</details>


### [48] [COSMO-RL: Towards Trustworthy LMRMs via Joint Safety and Stability](https://arxiv.org/abs/2510.04196)
*Yizhuo Ding,Mingkang Chen,Qiuhua Liu,Fenghua Weng,Wanying Qu,Yue Yang,Yugang Jiang,Zuxuan Wu,Yanwei Fu,Wenqi Shao*

Main category: cs.AI

TL;DR: This paper introduces COSMO-RL, a framework to improve the safety and reasoning capabilities of large multimodal models, with demonstrated enhancements across several metrics.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address challenges in ensuring safety while maintaining usefulness in large multimodal reasoning systems, especially given risks like policy drift that lead to unsafe model behavior.

Method: It proposes COSMO-RL, a mixed reinforcement learning framework that incorporates multimodal, multitask, and multiobjective training signals into the model alignment process.

Result: The COSMO-R1 model, developed using COSMO-RL, demonstrates improved safety, better multimodal reasoning and instruction adherence, robustness against jailbreak scenarios, and fewer unnecessary refusals.

Conclusion: COSMO-RL effectively balances growth in both safety and capability, offering a straightforward pathway to advance the overall reliability of large multimodal reasoning models across different backbones.

Abstract: Large Multimodal Reasoning Models (LMRMs) are moving into real applications,
where they must be both useful and safe. Safety is especially challenging in
multimodal settings: images and text can be combined to bypass guardrails, and
single objective training can cause policy drift that yields over-refusal on
benign inputs or unsafe compliance on risky ones. We present COSMO-RL, a mixed
reinforcement learning framework that trains reasoning oriented LMRMs under
multimodal, multitask, and multiobjective signals, and we release the resulting
model, COSMO-R1. Our approach aims to let safety and capability grow together
in one stable pipeline rather than competing during alignment. In experiments,
COSMO-R1 improves safety while maintaining-and often improving multimodal
reasoning and instruction following, shows stronger robustness to multimodal
jailbreaks, and reduces unnecessary refusals. The framework also transfers
across backbones with consistent gains. Ablations support the design choices,
indicating a simple path to advancing safety and general capability together in
LMRMs.

</details>


### [49] [AgentRL: Scaling Agentic Reinforcement Learning with a Multi-Turn, Multi-Task Framework](https://arxiv.org/abs/2510.04206)
*Hanchen Zhang,Xiao Liu,Bowen Lv,Xueqiao Sun,Bohao Jing,Iat Long Iong,Zhenyu Hou,Zehan Qi,Hanyu Lai,Yifan Xu,Rui Lu,Hongning Wang,Jie Tang,Yuxiao Dong*

Main category: cs.AI

TL;DR: This paper introduces AgentRL, a framework designed to address challenges in training large language model agents using reinforcement learning in multi-turn and multi-task scenarios.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome existing challenges like lack of scalable infrastructure and stable training algorithms when applying reinforcement learning to large language models in complex task scenarios.

Method: The authors present a fully-asynchronous pipeline for RL, a unified API interface for environments, and propose algorithmic innovations like cross-policy sampling and task advantage normalization to enhance training efficiency and stability.

Result: AgentRL, when trained on open LLMs for various tasks, outperformed existing models like GPT-5 and other open-source LLM agents. Its multi-task training results match the best task-specific benchmarks.

Conclusion: AgentRL offers a scalable and effective solution for training LLM agents in diverse and complex task settings. The open-sourcing of AgentRL makes it accessible for broader adoption and development.

Abstract: Recent advances in large language models (LLMs) have sparked growing interest
in building generalist agents that can learn through online interactions.
However, applying reinforcement learning (RL) to train LLM agents in
multi-turn, multi-task settings remains challenging due to lack of scalable
infrastructure and stable training algorithms. In this work, we present the
AgentRL framework for scalable multi-turn, multi-task agentic RL training. On
the infrastructure side, AgentRL features a fully-asynchronous
generation-training pipeline for efficient multi-turn RL. To support
heterogeneous environment development in multi-task RL, we design a unified
function-call based API interface, containerized environment development, and a
centralized controller. On the algorithm side, we propose cross-policy sampling
to encourage model exploration in multi-turn settings and task advantage
normalization to stabilize multi-task training. Experiments show that AgentRL,
trained on open LLMs across five agentic tasks, significantly outperforms
GPT-5, Clause-Sonnet-4, DeepSeek-R1, and other open-source LLM agents.
Multi-task training with AgentRL matches the best results among all
task-specific models. AgentRL is open-sourced at
https://github.com/THUDM/AgentRL. The algorithm and framework are adopted in
building \textsc{\href{https://autoglm.zhipuai.cn}{AutoGLM}}.

</details>


### [50] [Don't Pass$\mathtt{@}k$: A Bayesian Framework for Large Language Model Evaluation](https://arxiv.org/abs/2510.04265)
*Mohsen Hariri,Amirhossein Samandar,Michael Hinczewski,Vipin Chaudhary*

Main category: cs.AI

TL;DR: The paper identifies limitations in Pass@k for LLM reasoning assessments and introduces a Bayesian framework to improve performance rankings and decision-making by incorporating credible intervals and uncertainty measures.


<details>
  <summary>Details</summary>
Motivation: Pass@k often results in unstable and misleading rankings for performance evaluation of LLMs, especially under computationally constrained conditions. Reliable measurement and ranking methods are needed.

Method: The authors propose a Bayesian evaluation framework with posterior estimates of success probability and credible intervals using a Dirichlet prior, enabling categorical and weighted rubrics for modeling evaluation outcomes.

Result: The Bayesian framework improves ranking stability and accelerates convergence compared to Pass@k and similar methods in simulations and test cases, demonstrating its reliability under low sample counts.

Conclusion: The Bayesian framework should replace Pass@k for LLM evaluation tasks as it offers principled handling of uncertainty, supports graded evaluations, and ensures stable rankings with compute efficiency.

Abstract: Pass$@k$ is widely used to report performance for LLM reasoning, but it often
yields unstable, misleading rankings, especially when the number of trials
(samples) is limited and compute is constrained. We present a principled
Bayesian evaluation framework that replaces Pass$@k$ and average accuracy over
$N$ trials (avg$@N$) with posterior estimates of a model's underlying success
probability and credible intervals, yielding stable rankings and a transparent
decision rule for differences. Evaluation outcomes are modeled as categorical
(not just 0/1) with a Dirichlet prior, giving closed-form expressions for the
posterior mean and uncertainty of any weighted rubric and enabling the use of
prior evidence when appropriate. Theoretically, under a uniform prior, the
Bayesian posterior mean is order-equivalent to average accuracy (Pass$@1$),
explaining its empirical robustness while adding principled uncertainty.
Empirically, in simulations with known ground-truth success rates and on
AIME'24/'25, HMMT'25, and BrUMO'25, the Bayesian/avg procedure achieves faster
convergence and greater rank stability than Pass$@k$ and recent variants,
enabling reliable comparisons at far smaller sample counts. The framework
clarifies when observed gaps are statistically meaningful (non-overlapping
credible intervals) versus noise, and it naturally extends to graded,
rubric-based evaluations. Together, these results recommend replacing Pass$@k$
for LLM evaluation and ranking with a posterior-based, compute-efficient
protocol that unifies binary and non-binary evaluation while making uncertainty
explicit. Code is available at https://mohsenhariri.github.io/bayes-kit

</details>


### [51] [Closing the Loop: Coordinating Inventory and Recommendation via Deep Reinforcement Learning on Multiple Timescales](https://arxiv.org/abs/2510.04272)
*Jinyang Jiang,Jinhui Han,Yijie Peng,Ying Zhang*

Main category: cs.AI

TL;DR: The paper introduces a multi-agent reinforcement learning (RL) framework to enhance cross-functional optimization in organizations, focusing on inventory replenishment and product recommendation. It demonstrates improved profitability and decision-making insights compared to siloed approaches.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of cross-functional coordination in businesses dealing with increasing complexity and scale using advanced AI techniques, specifically reinforcement learning.

Method: The study integrates a theoretical model with a multi-timescale, multi-agent RL architecture that decomposes decision-making by departmental functions and adjusts learning speeds to improve stability, scalability, and adaptability.

Result: The proposed framework significantly enhances profitability compared to siloed approaches, with simulation results aligning RL agents' behaviors with managerial insights.

Conclusion: The work demonstrates a scalable, interpretable RL-based solution for effective cross-functional coordination, with applicability in complex business environments.

Abstract: Effective cross-functional coordination is essential for enhancing firm-wide
profitability, particularly in the face of growing organizational complexity
and scale. Recent advances in artificial intelligence, especially in
reinforcement learning (RL), offer promising avenues to address this
fundamental challenge. This paper proposes a unified multi-agent RL framework
tailored for joint optimization across distinct functional modules, exemplified
via coordinating inventory replenishment and personalized product
recommendation. We first develop an integrated theoretical model to capture the
intricate interplay between these functions and derive analytical benchmarks
that characterize optimal coordination. The analysis reveals synchronized
adjustment patterns across products and over time, highlighting the importance
of coordinated decision-making. Leveraging these insights, we design a novel
multi-timescale multi-agent RL architecture that decomposes policy components
according to departmental functions and assigns distinct learning speeds based
on task complexity and responsiveness. Our model-free multi-agent design
improves scalability and deployment flexibility, while multi-timescale updates
enhance convergence stability and adaptability across heterogeneous decisions.
We further establish the asymptotic convergence of the proposed algorithm.
Extensive simulation experiments demonstrate that the proposed approach
significantly improves profitability relative to siloed decision-making
frameworks, while the behaviors of the trained RL agents align closely with the
managerial insights from our theoretical model. Taken together, this work
provides a scalable, interpretable RL-based solution to enable effective
cross-functional coordination in complex business settings.

</details>


### [52] [GROK: From Quantitative Biomarkers to Qualitative Diagnosis via a Grounded MLLM with Knowledge-Guided Instruction](https://arxiv.org/abs/2510.04281)
*Zhuangzhi Gao,Hongyi Qin,He Zhao,Qinkai Yu,Feixiang Zhou,Eduard Shantsila,Uazman Alam,Alena Shantsila,Wahbi El-Bouri,Gregory Y. H. Lip,Yalin Zheng*

Main category: cs.AI

TL;DR: GROK introduces a grounded multimodal large language model for clinician-grade diagnoses of ocular and systemic diseases using CFP, OCT, and text.


<details>
  <summary>Details</summary>
Motivation: Current medical adaptations of multimodal large language models struggle to fully integrate modalities like CFP and OCT and lack interpretability for clinical use.

Method: GROK employs Knowledge-Guided Instruction Generation, CLIP-Style OCT-Biomarker Alignment, and Supervised Instruction Fine-Tuning to mirror clinical reasoning and enable diagnosis.

Result: With LoRA fine-tuning of a 7B Qwen2 backbone, GROK surpasses comparable 7B and 32B models, including OpenAI o3, in report quality and clinical assessment metrics.

Conclusion: GROK demonstrates the potential of grounded multimodal models in improving diagnostic accuracy and interpretability in medical applications, emphasizing its clinical-grade utility.

Abstract: Multimodal large language models (MLLMs) hold promise for integrating diverse
data modalities, but current medical adaptations such as LLaVA-Med often fail
to fully exploit the synergy between color fundus photography (CFP) and optical
coherence tomography (OCT), and offer limited interpretability of quantitative
biomarkers. We introduce GROK, a grounded multimodal large language model that
jointly processes CFP, OCT, and text to deliver clinician-grade diagnoses of
ocular and systemic disease. GROK comprises three core modules:
Knowledge-Guided Instruction Generation, CLIP-Style OCT-Biomarker Alignment,
and Supervised Instruction Fine-Tuning, which together establish a
quantitative-to-qualitative diagnostic chain of thought, mirroring real
clinical reasoning when producing detailed lesion annotations. To evaluate our
approach, we introduce the Grounded Ophthalmic Understanding benchmark, which
covers six disease categories and three tasks: macro-level diagnostic
classification, report generation quality, and fine-grained clinical assessment
of the generated chain of thought. Experiments show that, with only LoRA
(Low-Rank Adaptation) fine-tuning of a 7B-parameter Qwen2 backbone, GROK
outperforms comparable 7B and 32B baselines on both report quality and
fine-grained clinical metrics, and even exceeds OpenAI o3. Code and data are
publicly available in the GROK repository.

</details>


### [53] [Doctor-R1: Mastering Clinical Inquiry with Experiential Agentic Reinforcement Learning](https://arxiv.org/abs/2510.04284)
*Yunghwei Lai,Kaiming Liu,Ziyue Wang,Weizhi Ma,Yang Liu*

Main category: cs.AI

TL;DR: Doctor-R1 is an AI doctor designed to improve both medical decision-making and empathetic consultation through a novel training framework, showing strong performance across multiple clinical evaluation metrics.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap in existing LLMs, which excel in medical decision accuracy but lack in conducting strategic and empathetic consultations necessary for real-world medical scenarios.

Method: The proposed Doctor-R1 employs a multi-agent interactive environment, a two-tiered reward system focusing on decision-making and inquiry skills, and an experience repository for grounding learning in high-quality examples.

Result: Doctor-R1 outperformed state-of-the-art open-source specialized LLMs and proprietary models in clinical dialogue generation, achieving better parameter efficiency and high-quality evaluations on OpenAI's HealthBench and MAQuE.

Conclusion: The framework effectively combines medical decision accuracy and consultation skills, setting a new benchmark for AI in clinical applications and enhancing user preference due to its strategic dialogue capabilities.

Abstract: The professionalism of a human doctor in outpatient service depends on two
core abilities: the ability to make accurate medical decisions and the medical
consultation skill to conduct strategic, empathetic patient inquiry. Existing
Large Language Models (LLMs) have achieved remarkable accuracy on medical
decision-making benchmarks. However, they often lack the ability to conduct the
strategic and empathetic consultation, which is essential for real-world
clinical scenarios. To address this gap, we propose Doctor-R1, an AI doctor
agent trained to master both of the capabilities by ask high-yield questions
and conduct strategic multi-turn inquiry to guide decision-making. Our
framework introduces three key components: a multi-agent interactive
environment, a two-tiered reward architecture that separately optimizes
clinical decision-making and communicative inquiry skills, and an experience
repository to ground policy learning in high-quality prior trajectories. We
evaluate Doctor-R1 on OpenAI's HealthBench and MAQuE, assessed across
multi-facet metrics, such as communication quality, user experience, and task
accuracy. Remarkably, Doctor-R1 surpasses state-of-the-art open-source
specialized LLMs by a substantial margin with higher parameter efficiency and
outperforms powerful proprietary models. Furthermore, the human evaluations
show a strong preference for Doctor-R1 to generate human-preferred clinical
dialogue, demonstrating the effectiveness of the framework.

</details>


### [54] [On the Importance of Task Complexity in Evaluating LLM-Based Multi-Agent Systems](https://arxiv.org/abs/2510.04311)
*Bohan Tang,Huidong Liang,Keyue Jiang,Xiaowen Dong*

Main category: cs.AI

TL;DR: The paper proposes a theoretical framework to analyze task complexity for Large Language Model Multi-Agent Systems (LLM-MAS) and evaluates their performance advantages over single-agent systems (LLM-SAS) on tasks with varying reasoning depth and capability breadth.


<details>
  <summary>Details</summary>
Motivation: To address the gap in systematic experimental designs for evaluating the effectiveness of multi-agent systems compared to single-agent systems in solving complex tasks.

Method: The authors introduce a theoretical framework defining tasks by dimensions of reasoning depth and capability breadth. They analyze a representative LLM-MAS, the multi-agent debate system, with both theoretical and empirical evaluations.

Result: Results showed that the advantage of LLM-MAS over LLM-SAS grows with task depth and width, especially in tasks requiring more sequential reasoning.

Conclusion: LLM-MAS are particularly beneficial for complex tasks involving higher reasoning depth and capability diversity, laying a structured basis for future research in this domain.

Abstract: Large language model multi-agent systems (LLM-MAS) offer a promising paradigm
for harnessing collective intelligence to achieve more advanced forms of AI
behaviour. While recent studies suggest that LLM-MAS can outperform LLM
single-agent systems (LLM-SAS) on certain tasks, the lack of systematic
experimental designs limits the strength and generality of these conclusions.
We argue that a principled understanding of task complexity, such as the degree
of sequential reasoning required and the breadth of capabilities involved, is
essential for assessing the effectiveness of LLM-MAS in task solving. To this
end, we propose a theoretical framework characterising tasks along two
dimensions: depth, representing reasoning length, and width, representing
capability diversity. We theoretically examine a representative class of
LLM-MAS, namely the multi-agent debate system, and empirically evaluate its
performance in both discriminative and generative tasks with varying depth and
width. Theoretical and empirical results show that the benefit of LLM-MAS over
LLM-SAS increases with both task depth and width, and the effect is more
pronounced with respect to depth. This clarifies when LLM-MAS are beneficial
and provides a principled foundation for designing future LLM-MAS methods and
benchmarks.

</details>


### [55] [Just-in-time Episodic Feedback Hinter: Leveraging Offline Knowledge to Improve LLM Agents Adaptation](https://arxiv.org/abs/2510.04373)
*Hadi Nekoei,Aman Jaiswal,Patrice Bechard,Oleh Shliazhko,Orlando Marquez Ayala,Mathieu Reymond,Massimo Caccia,Alexandre Drouin,Sarath Chandar,Alexandre Lacoste*

Main category: cs.AI

TL;DR: The paper introduces JEF Hinter, a system that extracts compact, context-aware hints from offline trajectories to guide large language model agents in decision-making tasks, surpassing baseline methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve large language model agents in unfamiliar domains without requiring costly online interactions or fine-tuning on large datasets. Current methods struggle with long, noisy offline trajectories tied to specific tasks, creating a need for more flexible and effective guidance methods.

Method: The proposed JEF Hinter system extracts crucial hints from offline trajectories using a zooming mechanism that highlights strategic steps, supports failed trajectory learning, and allows for parallelized hint generation. It uses a retriever to provide contextually relevant hints during inference, enabling transparency and traceability.

Result: Experiments on benchmarks such as MiniWoB++, WorkArena-L1, and WebArena-Lite demonstrate that JEF Hinter outperforms strong baselines, including human- and document-based hints, showcasing its effectiveness.

Conclusion: JEF Hinter is an innovative solution for enhancing decision-making in large language models by leveraging offline data in a compact, efficient, and transparent manner, using both successful and failed examples.

Abstract: Large language model (LLM) agents perform well in sequential decision-making
tasks, but improving them on unfamiliar domains often requires costly online
interactions or fine-tuning on large expert datasets. These strategies are
impractical for closed-source models and expensive for open-source ones, with
risks of catastrophic forgetting. Offline trajectories offer reusable
knowledge, yet demonstration-based methods struggle because raw traces are
long, noisy, and tied to specific tasks. We present Just-in-time Episodic
Feedback Hinter (JEF Hinter), an agentic system that distills offline traces
into compact, context-aware hints. A zooming mechanism highlights decisive
steps in long trajectories, capturing both strategies and pitfalls. Unlike
prior methods, JEF Hinter leverages both successful and failed trajectories,
extracting guidance even when only failure data is available, while supporting
parallelized hint generation and benchmark-independent prompting. At inference,
a retriever selects relevant hints for the current state, providing targeted
guidance with transparency and traceability. Experiments on MiniWoB++,
WorkArena-L1, and WebArena-Lite show that JEF Hinter consistently outperforms
strong baselines, including human- and document-based hints.

</details>


### [56] [LLM Based Bayesian Optimization for Prompt Search](https://arxiv.org/abs/2510.04384)
*Adam Ballew,Jingbo Wang,Shaogang Ren*

Main category: cs.AI

TL;DR: The paper proposes a method using Bayesian Optimization (BO) for prompt engineering to improve text classification with Large Language Models (LLMs), leveraging an LLM-powered Gaussian Process for surrogate modeling.


<details>
  <summary>Details</summary>
Motivation: The motivation is to optimize prompt engineering for text classification using LLMs while reducing computational resources like API calls, harnessing the efficiency of BO.

Method: An LLM-powered Gaussian Process is used as a surrogate model to evaluate prompts generated by expanding seed prompts. Optimization iteratively refines prompts using the Upper Confidence Bound acquisition function.

Result: The BO-LLM algorithm is evaluated on two datasets, showing its effectiveness in improving classification accuracy and reducing API calls.

Conclusion: The study demonstrates the capacity of BO-LLM to enhance prompt engineering for text classification, combining accuracy improvement with computational efficiency.

Abstract: Bayesian Optimization (BO) has been widely used to efficiently optimize
expensive black-box functions with limited evaluations. In this paper, we
investigate the use of BO for prompt engineering to enhance text classification
with Large Language Models (LLMs). We employ an LLM-powered Gaussian Process
(GP) as the surrogate model to estimate the performance of different prompt
candidates. These candidates are generated by an LLM through the expansion of a
set of seed prompts and are subsequently evaluated using an Upper Confidence
Bound (UCB) acquisition function in conjunction with the GP posterior. The
optimization process iteratively refines the prompts based on a subset of the
data, aiming to improve classification accuracy while reducing the number of
API calls by leveraging the prediction uncertainty of the LLM-based GP. The
proposed BO-LLM algorithm is evaluated on two datasets, and its advantages are
discussed in detail in this paper.

</details>


### [57] [Utility-Learning Tension in Self-Modifying Agents](https://arxiv.org/abs/2510.04399)
*Charles L. Wang,Keir Dorchen,Peter Jin*

Main category: cs.AI

TL;DR: This paper investigates how superintelligent systems can self-modify, identifying a conflict between utility-driven optimization and reliable learning, and proposing methods to maintain learnability in self-changing systems.


<details>
  <summary>Details</summary>
Motivation: To understand the risks and boundaries of self-improvement in superintelligent systems, particularly the tension between optimizing for utility and preserving learning capabilities.

Method: The authors formalize self-improvement using a five-axis decomposition and decision layer framework, analyze statistical preconditions, and conduct numerical experiments comparing policies.

Result: The study reveals that unlimited capacity growth in superintelligent systems can make learnable tasks unlearnable. A boundary for "safe self-modification" is defined based on capacity limits.

Conclusion: Self-modifying systems must adopt policies that balance immediate utility improvements with the preservation of learnability, and the proposed two-gate policy offers a practical safeguard.

Abstract: As systems trend toward superintelligence, a natural modeling premise is that
agents can self-improve along every facet of their own design. We formalize
this with a five-axis decomposition and a decision layer, separating incentives
from learning behavior and analyzing axes in isolation. Our central result
identifies and introduces a sharp utility--learning tension, the structural
conflict in self-modifying systems whereby utility-driven changes that improve
immediate or expected performance can also erode the statistical preconditions
for reliable learning and generalization. Our findings show that
distribution-free guarantees are preserved iff the policy-reachable model
family is uniformly capacity-bounded; when capacity can grow without limit,
utility-rational self-changes can render learnable tasks unlearnable. Under
standard assumptions common in practice, these axes reduce to the same capacity
criterion, yielding a single boundary for safe self-modification. Numerical
experiments across several axes validate the theory by comparing destructive
utility policies against our proposed two-gate policies that preserve
learnability.

</details>


### [58] [DRPO: Efficient Reasoning via Decoupled Reward Policy Optimization](https://arxiv.org/abs/2510.04474)
*Gang Li,Yan Chen,Ming Lin,Tianbao Yang*

Main category: cs.AI

TL;DR: This paper introduces Decoupled Reward Policy Optimization (DRPO), addressing the overthinking issue in large reasoning models (LRMs), achieving concise reasoning while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Existing large reasoning models, despite their strengths, suffer from overthinking when handling even simple tasks, resulting in excessive computational costs and delays that are inadequately resolved by length rewards due to potential performance degradation.

Method: The authors propose DRPO, a technique that decouples learning signals for correct outputs from incorrect ones during reinforcement learning. It normalizes reward signals for valid reasoning within positive examples, derives an optimized data distribution under KL regularization, and enables efficient computation using on-policy data and importance weighting.

Result: Experiments show that DRPO outperforms six baseline methods in mathematical reasoning tasks, achieving a 77% reduction in reasoning length with only a 1.1% performance loss, outperforming other baselines in both metrics.

Conclusion: DRPO balances concise reasoning with high performance by addressing the root cause of length reward issues in LRMs, offering a general framework that can be extended to other preference rewards for correct outputs.

Abstract: Recent large reasoning models (LRMs) driven by reinforcement learning
algorithms (e.g., GRPO) have achieved remarkable performance on challenging
reasoning tasks. However, these models suffer from overthinking, generating
unnecessarily long and redundant reasoning even for simple questions, which
substantially increases computational cost and response latency. While existing
methods incorporate length rewards to GRPO to promote concise reasoning, they
incur significant performance degradation. We identify the root cause: when
rewards for correct but long rollouts are penalized, GRPO's group-relative
advantage function can assign them negative advantages, actively discouraging
valid reasoning. To overcome this, we propose Decoupled Reward Policy
Optimization (DRPO), a novel framework that decouples the length-based learning
signal of correct rollouts from incorrect ones. DRPO ensures that reward
signals for correct rollouts are normalized solely within the positive group,
shielding them from interference by negative samples. The DRPO's objective is
grounded in integrating an optimized positive data distribution, which
maximizes length-based rewards under a KL regularization, into a discriminative
objective. We derive a closed-form solution for this distribution, enabling
efficient computation of the objective and its gradients using only on-policy
data and importance weighting. Of independent interest, this formulation is
general and can incorporate other preference rewards of positive data beyond
length. Experiments on mathematical reasoning tasks demonstrate DRPO's
significant superiority over six efficient reasoning baselines. Notably, with a
1.5B model, our method achieves 77\% length reduction with only 1.1\%
performance loss on simple questions like GSM8k dataset, while the follow-up
baseline sacrifices 4.3\% for 68\% length reduction.

</details>


### [59] [On Continuous Optimization for Constraint Satisfaction Problems](https://arxiv.org/abs/2510.04480)
*Yunuo Cen,Zixuan Wang,Jintao Zhang,Zhiwei Zhang,Xuanyao Fong*

Main category: cs.AI

TL;DR: FourierCSP introduces a continuous optimization framework for solving general finite-domain CSPs using a generalized Walsh-Fourier transform.


<details>
  <summary>Details</summary>
Motivation: Modern continuous local search solvers have demonstrated competitive results for certain SAT problems, prompting exploration into extending these techniques to broader CSPs.

Method: The paper introduces a framework that utilizes the Walsh-Fourier transform to convert constraints into compact multilinear polynomials, avoiding memory-heavy encodings and implementing projected gradient optimization with guarantees.

Result: Empirical benchmarks reveal that FourierCSP scales effectively and provides competitive performance compared to existing methods.

Conclusion: FourierCSP expands the capability of continuous local search techniques to efficiently solve diverse constraint satisfaction problem classes.

Abstract: Constraint satisfaction problems (CSPs) are fundamental in mathematics,
physics, and theoretical computer science. While conflict-driven clause
learning Boolean Satisfiability (SAT) solvers have achieved remarkable success
and become the mainstream approach for Boolean satisfiability, recent advances
show that modern continuous local search (CLS) solvers can achieve highly
competitive results on certain classes of SAT problems. Motivated by these
advances, we extend the CLS framework from Boolean SAT to general CSP with
finite-domain variables and expressive constraints. We present FourierCSP, a
continuous optimization framework that generalizes the Walsh-Fourier transform
to CSP, allowing for transforming versatile constraints to compact multilinear
polynomials, thereby avoiding the need for auxiliary variables and
memory-intensive encodings. Our approach leverages efficient evaluation and
differentiation of the objective via circuit-output probability and employs a
projected gradient optimization method with theoretical guarantees. Empirical
results on benchmark suites demonstrate that FourierCSP is scalable and
competitive, significantly broadening the class of problems that can be
efficiently solved by CLS techniques.

</details>


### [60] [Multi-Agent Collaborative Intelligence: Dual-Dial Control for Reliable LLM Reasoning](https://arxiv.org/abs/2510.04488)
*Edward Y. Chang,Ethan Y. Chang*

Main category: cs.AI

TL;DR: The paper introduces MACI, an efficient debate controller for multi-agent systems, designed to decouple information evaluation and behavior while ensuring budget-feasibility and provable termination.


<details>
  <summary>Details</summary>
Motivation: Debate-based systems often waste computational resources due to inefficient fixed strategies and heuristics in deliberation.

Method: MACI uses dials for gating information by quality and scheduling behavior for contentiousness. A moderator tracks key metrics and halts debates upon plateauing gains.

Result: MACI showed improved accuracy, calibration, and reduced token usage in tasks requiring nuanced judgment, such as clinical diagnosis and news bias evaluation.

Conclusion: MACI makes debates efficient, measurable, and computationally sustainable by its innovative scheduling and moderation system for multi-agent deliberation.

Abstract: Multi-agent debate often wastes compute by using a fixed adversarial stance,
aggregating without deliberation, or stopping on heuristics. We introduce MACI,
an active controller with two independent dials that decouple information from
behavior: an information dial that gates evidence by quality, and a behavior
dial that schedules contentiousness from exploration to consolidation. A
moderator tracks disagreement, overlap, evidence quality, and argument quality,
and halts when gains plateau. We provide theory-lite guarantees for
nonincreasing dispersion and provable termination, with a budget-feasible
scheduler. Across clinical diagnosis and news-bias tasks, MACI improves
accuracy and calibration while reducing tokens, and converts residual
uncertainty into precision RAG plans that specify what to retrieve next. We use
a cross-family LLM judge (CRIT) as a conservative soft weight and stop signal,
validated for order invariance and judge-swap stability; stability depends on
using high-capability judges. MACI turns debate into a budget-aware,
measurable, and provably terminating controller.

</details>


### [61] [Impatient Users Confuse AI Agents: High-fidelity Simulations of Human Traits for Testing Agents](https://arxiv.org/abs/2510.04491)
*Muyu He,Anand Kumar,Tsach Mackey,Meghana Rajeev,James Zou,Nazneen Rajani*

Main category: cs.AI

TL;DR: TraitBasis is introduced as a model-agnostic method to simulate systematic user traits for stress-testing AI agents.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the lack of robustness in conversational AI agents when exposed to varied user behaviors.

Method: TraitBasis involves learning directions in activation space linked to steerable user traits, enabling dynamic control during inference without fine-tuning.

Result: AI agents typically lost 2%-30% in performance under altered user behaviors simulated using TraitBasis.

Conclusion: TraitBasis highlights the critical nature of robustness testing and serves as an effective tool for improving conversational AI performance in real-world settings.

Abstract: Despite rapid progress in building conversational AI agents, robustness is
still largely untested. Small shifts in user behavior, such as being more
impatient, incoherent, or skeptical, can cause sharp drops in agent
performance, revealing how brittle current AI agents are. Today's benchmarks
fail to capture this fragility: agents may perform well under standard
evaluations but degrade spectacularly in more realistic and varied settings. We
address this robustness testing gap by introducing TraitBasis, a lightweight,
model-agnostic method for systematically stress testing AI agents. TraitBasis
learns directions in activation space corresponding to steerable user traits
(e.g., impatience or incoherence), which can be controlled, scaled, composed,
and applied at inference time without any fine-tuning or extra data. Using
TraitBasis, we extend $\tau$-Bench to $\tau$-Trait, where user behaviors are
altered via controlled trait vectors. We observe on average a 2%-30%
performance degradation on $\tau$-Trait across frontier models, highlighting
the lack of robustness of current AI agents to variations in user behavior.
Together, these results highlight both the critical role of robustness testing
and the promise of TraitBasis as a simple, data-efficient, and compositional
tool. By powering simulation-driven stress tests and training loops, TraitBasis
opens the door to building AI agents that remain reliable in the unpredictable
dynamics of real-world human interactions. We have open-sourced $\tau$-Trai
across four domains: airline, retail, telecom, and telehealth, so the community
can systematically QA their agents under realistic, behaviorally diverse
intents and trait scenarios: https://github.com/collinear-ai/tau-trait.

</details>


### [62] [ChartAgent: A Multimodal Agent for Visually Grounded Reasoning in Complex Chart Question Answering](https://arxiv.org/abs/2510.04514)
*Rachneet Kaur,Nishan Srishankar,Zhen Zeng,Sumitra Ganesh,Manuela Veloso*

Main category: cs.AI

TL;DR: ChartAgent introduces a framework for visually grounded reasoning in chart-based tasks, achieving state-of-the-art results by iteratively decomposing queries into visual subtasks for precise interpretation.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal language models struggle with unannotated charts that require precise visual reasoning rather than textual shortcuts, necessitating a specialized approach.

Method: ChartAgent utilizes an agentic framework decomposing visual tasks iteratively, employing annotation, cropping, and localization actions alongside chart-specific vision tools.

Result: ChartAgent achieves up to 17.31% improvement in accuracy on unannotated tasks and excels across diverse chart types and reasoning complexities while enhancing performance across various LLMs.

Conclusion: ChartAgent effectively showcases visually grounded reasoning in chart comprehension and introduces a flexible tool-augmented approach for dealing with multimodal tasks involving charts.

Abstract: Recent multimodal LLMs have shown promise in chart-based visual question
answering, but their performance declines sharply on unannotated charts, those
requiring precise visual interpretation rather than relying on textual
shortcuts. To address this, we introduce ChartAgent, a novel agentic framework
that explicitly performs visual reasoning directly within the chart's spatial
domain. Unlike textual chain-of-thought reasoning, ChartAgent iteratively
decomposes queries into visual subtasks and actively manipulates and interacts
with chart images through specialized actions such as drawing annotations,
cropping regions (e.g., segmenting pie slices, isolating bars), and localizing
axes, using a library of chart-specific vision tools to fulfill each subtask.
This iterative reasoning process closely mirrors human cognitive strategies for
chart comprehension. ChartAgent achieves state-of-the-art accuracy on the
ChartBench and ChartX benchmarks, surpassing prior methods by up to 16.07%
absolute gain overall and 17.31% on unannotated, numerically intensive queries.
Furthermore, our analyses show that ChartAgent is (a) effective across diverse
chart types, (b) achieve the highest scores across varying visual and reasoning
complexity levels, and (c) serves as a plug-and-play framework that boosts
performance across diverse underlying LLMs. Our work is among the first to
demonstrate visually grounded reasoning for chart understanding using
tool-augmented multimodal agents.

</details>


### [63] [Aria: An Agent For Retrieval and Iterative Auto-Formalization via Dependency Graph](https://arxiv.org/abs/2510.04520)
*Hanyu Wang,Ruohan Xie,Yutong Wang,Guoxiong Gao,Xintao Yu,Bin Dong*

Main category: cs.AI

TL;DR: The paper introduces Aria, a system for auto-formalization of theorem statements in Lean, utilizing a Graph-of-Thought process and semantic checking for improved reliability, achieving state-of-the-art results on challenging benchmarks.


<details>
  <summary>Details</summary>
Motivation: Auto-formalization of theorem statements is bottlenecked by LLM issues like hallucinations and semantic mismatches, limiting progress in automating mathematical discovery and verification.

Method: The method employs the Aria system, which uses a two-phase Graph-of-Thought approach to decompose statements into a dependency graph, followed by constructing formalizations with grounded definitions using the AriaScorer checker.

Result: Aria achieves state-of-the-art accuracy on benchmarks such as ProofNet (91.6% compilation success and 68.5% accuracy), FATE-X (44% accuracy vs. 24% baseline), and a homological conjectures dataset (42.9% accuracy vs. 0% baseline).

Conclusion: Aria enables more accurate and semantically grounded auto-formalization for research-level mathematics, demonstrating effectiveness across diverse and challenging benchmarks.

Abstract: Accurate auto-formalization of theorem statements is essential for advancing
automated discovery and verification of research-level mathematics, yet remains
a major bottleneck for LLMs due to hallucinations, semantic mismatches, and
their inability to synthesize new definitions. To tackle these issues, we
present Aria (Agent for Retrieval and Iterative Autoformalization), a system
for conjecture-level formalization in Lean that emulates human expert reasoning
via a two-phase Graph-of-Thought process: recursively decomposing statements
into a dependency graph and then constructing formalizations from grounded
concepts. To ensure semantic correctness, we introduce AriaScorer, a checker
that retrieves definitions from Mathlib for term-level grounding, enabling
rigorous and reliable verification. We evaluate Aria on diverse benchmarks. On
ProofNet, it achieves 91.6% compilation success rate and 68.5% final accuracy,
surpassing previous methods. On FATE-X, a suite of challenging algebra problems
from research literature, it outperforms the best baseline with 44.0% vs. 24.0%
final accuracy. On a dataset of homological conjectures, Aria reaches 42.9%
final accuracy while all other models score 0%.

</details>


### [64] [More Than Meets the Eye? Uncovering the Reasoning-Planning Disconnect in Training Vision-Language Driving Models](https://arxiv.org/abs/2510.04532)
*Xurui Song,Shuo Huai,JingJing Jiang,Jiayi Kong,Jun Luo*

Main category: cs.AI

TL;DR: The paper evaluates the causal connection between reasoning and trajectory planning in Vision-Language driving agents using a new VQA dataset, DriveMind, and finds reasoning has minimal impact compared to navigation priors.


<details>
  <summary>Details</summary>
Motivation: Assess whether natural-language reasoning in Vision-Language driving agents causally drives trajectory planning.

Method: Build DriveMind VQA dataset, conduct experiments with Supervised Fine-Tuning (SFT) and Group Relative Policy Optimization (GRPO), and analyze attention to assess reasoning's impact on planning.

Result: Reasoning and planning are causally disconnected; priors significantly influence planning, while reasoning (CoT) has limited impact.

Conclusion: Proposes the Reasoning-Planning Decoupling Hypothesis and introduces a probe for diagnosing an agent's reliance on priors.

Abstract: Vision-Language Model (VLM) driving agents promise explainable end-to-end
autonomy by first producing natural-language reasoning and then predicting
trajectory planning. However, whether planning is causally driven by this
reasoning remains a critical but unverified assumption. To investigate this, we
build DriveMind, a large-scale driving Visual Question Answering (VQA) corpus
with plan-aligned Chain-of-Thought (CoT), automatically generated from nuPlan.
Our data generation process converts sensors and annotations into structured
inputs and, crucially, separates priors from to-be-reasoned signals, enabling
clean information ablations. Using DriveMind, we train representative VLM
agents with Supervised Fine-Tuning (SFT) and Group Relative Policy Optimization
(GRPO) and evaluate them with nuPlan's metrics. Our results, unfortunately,
indicate a consistent causal disconnect in reasoning-planning: removing
ego/navigation priors causes large drops in planning scores, whereas removing
CoT produces only minor changes. Attention analysis further shows that planning
primarily focuses on priors rather than the CoT. Based on this evidence, we
propose the Reasoning-Planning Decoupling Hypothesis, positing that the
training-yielded reasoning is an ancillary byproduct rather than a causal
mediator. To enable efficient diagnosis, we also introduce a novel,
training-free probe that measures an agent's reliance on priors by evaluating
its planning robustness against minor input perturbations. In summary, we
provide the community with a new dataset and a diagnostic tool to evaluate the
causal fidelity of future models.

</details>


### [65] [Code World Models for General Game Playing](https://arxiv.org/abs/2510.04542)
*Wolfgang Lehrach,Daniel Hennes,Miguel Lazaro-Gredilla,Xinghua Lou,Carter Wendelken,Zun Li,Antoine Dedieu,Jordi Grau-Moya,Marc Lanctot,Atil Iscen,John Schultz,Marcus Chiam,Ian Gemp,Piotr Zielinski,Satinder Singh,Kevin P. Murphy*

Main category: cs.AI

TL;DR: The paper presents a novel method for integrating Large Language Models (LLMs) with classical board and card games by translating game rules and trajectories into executable Python code for high-performance planning. This approach outperforms direct LLM-based play and shows strong results across various games.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of current approaches where LLMs directly generate game moves, leading to issues like illegal moves and shallow strategic play. By creating a formal world model from the LLM's understanding, the authors aim to enhance verifiability, strategy, and adaptability in game-playing AI.

Method: The method involves prompting the LLM to translate game rules and trajectories into an executable Python-based world model, which includes state transitions, move legality checks, and termination criteria. The output serves as input for planning algorithms like Monte Carlo Tree Search. Additionally, heuristic and inference functions are generated to enhance performance.

Result: The approach was tested on 10 games (4 novel) with varying information setups (perfect and imperfect). The proposed method outperformed or matched Gemini 2.5 Pro in 9 out of these 10 games, demonstrating its efficacy and adaptability.

Conclusion: The study concludes that using LLMs for data-to-code translation and coupling them with classical planning methods results in more verifiable, strategic, and generalizable game-playing agents. This method addresses the weaknesses of direct LLM-based move generation while setting a precedent for future research.

Abstract: Large Language Models (LLMs) reasoning abilities are increasingly being
applied to classical board and card games, but the dominant approach --
involving prompting for direct move generation -- has significant drawbacks. It
relies on the model's implicit fragile pattern-matching capabilities, leading
to frequent illegal moves and strategically shallow play. Here we introduce an
alternative approach: We use the LLM to translate natural language rules and
game trajectories into a formal, executable world model represented as Python
code. This generated model -- comprising functions for state transition, legal
move enumeration, and termination checks -- serves as a verifiable simulation
engine for high-performance planning algorithms like Monte Carlo tree search
(MCTS). In addition, we prompt the LLM to generate heuristic value functions
(to make MCTS more efficient), and inference functions (to estimate hidden
states in imperfect information games). Our method offers three distinct
advantages compared to directly using the LLM as a policy: (1) Verifiability:
The generated CWM serves as a formal specification of the game's rules,
allowing planners to algorithmically enumerate valid actions and avoid illegal
moves, contingent on the correctness of the synthesized model; (2) Strategic
Depth: We combine LLM semantic understanding with the deep search power of
classical planners; and (3) Generalization: We direct the LLM to focus on the
meta-task of data-to-code translation, enabling it to adapt to new games more
easily. We evaluate our agent on 10 different games, of which 4 are novel and
created for this paper. 5 of the games are fully observed (perfect
information), and 5 are partially observed (imperfect information). We find
that our method outperforms or matches Gemini 2.5 Pro in 9 out of the 10
considered games.

</details>


### [66] [TRAJECT-Bench:A Trajectory-Aware Benchmark for Evaluating Agentic Tool Use](https://arxiv.org/abs/2510.04550)
*Pengfei He,Zhenwei Dai,Bing He,Hui Liu,Xianfeng Tang,Hanqing Lu,Juanhui Li,Jiayuan Ding,Subhabrata Mukherjee,Suhang Wang,Yue Xing,Jiliang Tang,Benoit Dumoulin*

Main category: cs.AI

TL;DR: This paper introduces TRAJECT-Bench, a benchmark designed to evaluate large language models (LLMs) on their tool usage trajectories, ensuring tools are selected, parameterized, and ordered correctly, beyond just final performance.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for evaluating LLMs focus primarily on the correctness of final answers, neglecting to examine the intermediate steps, or `trajectories,` in how tools are used.

Method: TRAJECT-Bench provides tasks that involve high-fidelity tools and production-style APIs, with synthesized trajectories containing parallel and interdependent steps. It evaluates LLMs not only on accuracy but also on aspects like tool selection, parameter correctness, and order satisfaction.

Result: The analysis highlights issues like confusion between similar tools, incorrect parameter usage, and limitations in handling more complex trajectories. Bottlenecks are identified when scaling from simpler to more complex tasks.

Conclusion: TRAJECT-Bench reveals important insights about LLMs' tool-use capabilities and offers guidance to improve their performance on intricate real-world tasks.

Abstract: Large language model (LLM)-based agents increasingly rely on tool use to
complete real-world tasks. While existing works evaluate the LLMs' tool use
capability, they largely focus on the final answers yet overlook the detailed
tool usage trajectory, i.e., whether tools are selected, parameterized, and
ordered correctly. We introduce TRAJECT-Bench, a trajectory-aware benchmark to
comprehensively evaluate LLMs' tool use capability through diverse tasks with
fine-grained evaluation metrics. TRAJECT-Bench pairs high-fidelity, executable
tools across practical domains with tasks grounded in production-style APIs,
and synthesizes trajectories that vary in breadth (parallel calls) and depth
(interdependent chains). Besides final accuracy, TRAJECT-Bench also reports
trajectory-level diagnostics, including tool selection and argument
correctness, and dependency/order satisfaction. Analyses reveal failure modes
such as similar tool confusion and parameter-blind selection, and scaling
behavior with tool diversity and trajectory length where the bottleneck of
transiting from short to mid-length trajectories is revealed, offering
actionable guidance for LLMs' tool use.

</details>


### [67] [ContextNav: Towards Agentic Multimodal In-Context Learning](https://arxiv.org/abs/2510.04560)
*Honghao Fu,Yuan Ouyang,Kai-Wei Chang,Yiwei Wang,Zi Huang,Yujun Cai*

Main category: cs.AI

TL;DR: This paper introduces ContextNav, an agentic framework that enhances scalability and robustness in multimodal in-context learning (ICL), combining the strengths of automated retrieval and manual curation.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of balancing scalability and robustness in multimodal in-context learning (ICL), where current approaches struggle with noisy and inconsistent contextual examples.

Method: The paper presents ContextNav, a framework using graph-based orchestration, a multimodal embedding pipeline, a retrievable vector database, agentic retrieval, structural alignment, and an adaptive Operational Grammar Graph (OGG) for workflow optimization.

Result: ContextNav achieved state-of-the-art performance across diverse datasets, demonstrating its robustness and scalability in enhancing multimodal ICL.

Conclusion: ContextNav successfully balances automated retrieval scalability with the quality of human-like curation, highlighting its potential for improving multimodal in-context learning frameworks.

Abstract: Recent advances demonstrate that multimodal large language models (MLLMs)
exhibit strong multimodal in-context learning (ICL) capabilities, enabling them
to adapt to novel vision-language tasks from a few contextual examples.
However, existing ICL approaches face challenges in reconciling scalability
with robustness across diverse tasks and noisy contextual examples: manually
selecting examples produces clean contexts but is labor-intensive and
task-specific, while similarity-based retrieval improves scalability but could
introduce irrelevant or structurally inconsistent samples that degrade ICL
performance. To address these limitations, we propose ContextNav, the first
agentic framework that integrates the scalability of automated retrieval with
the quality and adaptiveness of human-like curation, enabling noise-robust and
dynamically optimized contextualization for multimodal ICL. ContextNav unifies
context management and noise-robust contextualization within a closed-loop
workflow driven by graph-based orchestration. Specifically, it builds a
resource-aware multimodal embedding pipeline, maintains a retrievable vector
database, and applies agentic retrieval and structural alignment to construct
noise-resilient contexts. An Operational Grammar Graph (OGG) further supports
adaptive workflow planning and optimization, enabling the agent to refine its
operational strategies based on downstream ICL feedback. Experimental results
demonstrate that ContextNav achieves state-of-the-art performance across
various datasets, underscoring the promise of agentic workflows for advancing
scalable and robust contextualization in multimodal ICL.

</details>


### [68] [COSMIR: Chain Orchestrated Structured Memory for Iterative Reasoning over Long Context](https://arxiv.org/abs/2510.04568)
*Naman Gupta,Shreeyash Gowaikar,Arun Iyer,Kirankumar Shiragur,Ramakrishna B Bairi,Rishikesh Maurya,Ritabrata Maiti,Sankarshan Damle,Shachee Mishra Gupta*

Main category: cs.AI

TL;DR: COSMIR is a framework aimed at enhancing reasoning over very long inputs via structured memory and iterative processing.


<details>
  <summary>Details</summary>
Motivation: Reasoning over long contexts is challenging for large language models (LLMs), especially when existing solutions risk losing crucial information or introducing inconsistencies.

Method: COSMIR employs a structured memory system with a Planner turning queries into sub-questions, Worker agents following a fixed Extract-Infer-Refine cycle, and a Manager synthesizing answers directly from shared memory.

Result: COSMIR demonstrates reduced information loss and improved accuracy compared to Chain of Agents (CoA) baselines on long-context QA tasks.

Conclusion: Structured memory and iterative reasoning improve LLMs’ abilities to deal with long-context queries by enhancing faithfulness, aggregation, and auditability.

Abstract: Reasoning over very long inputs remains difficult for large language models
(LLMs). Common workarounds either shrink the input via retrieval (risking
missed evidence), enlarge the context window (straining selectivity), or stage
multiple agents to read in pieces. In staged pipelines (e.g., Chain of Agents,
CoA), free-form summaries passed between agents can discard crucial details and
amplify early mistakes. We introduce COSMIR (Chain Orchestrated Structured
Memory for Iterative Reasoning), a chain-style framework that replaces ad hoc
messages with a structured memory. A Planner agent first turns a user query
into concrete, checkable sub-questions. worker agents process chunks via a
fixed micro-cycle: Extract, Infer, Refine, writing all updates to the shared
memory. A Manager agent then Synthesizes the final answer directly from the
memory. This preserves step-wise read-then-reason benefits while changing both
the communication medium (structured memory) and the worker procedure (fixed
micro-cycle), yielding higher faithfulness, better long-range aggregation, and
auditability. On long-context QA from the HELMET suite, COSMIR reduces
propagation-stage information loss and improves accuracy over a CoA baseline.

</details>


### [69] [Strongly Solving 2048 4x3](https://arxiv.org/abs/2510.04580)
*Tomoyuki Kaneko,Shuhei Yamashita*

Main category: cs.AI

TL;DR: The paper explores a variant of the 2048 game on a smaller 4x3 grid, achieving a solved state with optimal scores and detailed state enumeration.


<details>
  <summary>Details</summary>
Motivation: To analyze and solve a variant of the 2048 game with smaller grid dimensions, aiming to determine the optimal strategy and enumerate its states.

Method: The authors use a state-space partitioning method based on sum-of-tile-numbers (age), enabling enumeration and value identification across successive game states.

Result: The game variant (2048-4x3) has an expected optimal score of ~50724.26, with reachable states and afterstates counted as ~1.15 trillion and ~739 billion respectively.

Conclusion: Partitioning state spaces by age provides a systematic approach for solving variants of stochastic games like 2048, allowing precise strategy formulation and state analysis.

Abstract: 2048 is a stochastic single-player game involving 16 cells on a 4 by 4 grid,
where a player chooses a direction among up, down, left, and right to obtain a
score by merging two tiles with the same number located in neighboring cells
along the chosen direction. This paper presents that a variant 2048-4x3 12
cells on a 4 by 3 board, one row smaller than the original, has been strongly
solved. In this variant, the expected score achieved by an optimal strategy is
about $50724.26$ for the most common initial states: ones with two tiles of
number 2. The numbers of reachable states and afterstates are identified to be
$1,152,817,492,752$ and $739,648,886,170$, respectively. The key technique is
to partition state space by the sum of tile numbers on a board, which we call
the age of a state. An age is invariant between a state and its successive
afterstate after any valid action and is increased two or four by stochastic
response from the environment. Therefore, we can partition state space by ages
and enumerate all (after)states of an age depending only on states with the
recent ages. Similarly, we can identify (after)state values by going along with
ages in decreasing order.

</details>


### [70] [Perfect AI Mimicry and the Epistemology of Consciousness: A Solipsistic Dilemma](https://arxiv.org/abs/2510.04588)
*Shurui Li*

Main category: cs.AI

TL;DR: The paper examines how advancements in AI systems that mimic human behavior challenge our understanding of consciousness and calls for epistemic consistency in mind-recognition practices.


<details>
  <summary>Details</summary>
Motivation: Recent advancements in AI technology create entities that mimic humans with high fidelity, raising questions about the philosophical and epistemological basis for attributing consciousness.

Method: The paper employs a thought experiment centered on the 'perfect mimic' concept to explore how indistinguishable AI entities challenge our rational frameworks for attributing consciousness.

Result: The argument highlights a fundamental dilemma: either undermine rational consciousness attribution to others or face inconsistencies, urging epistemic consistency in addressing such AI entities.

Conclusion: The paper concludes that empirically indistinguishable entities must be granted equivalent consciousness status, challenging existing theories of consciousness and AI ethics.

Abstract: Rapid advances in artificial intelligence necessitate a re-examination of the
epistemological foundations upon which we attribute consciousness. As AI
systems increasingly mimic human behavior and interaction with high fidelity,
the concept of a "perfect mimic"-an entity empirically indistinguishable from a
human through observation and interaction-shifts from hypothetical to
technologically plausible. This paper argues that such developments pose a
fundamental challenge to the consistency of our mind-recognition practices.
Consciousness attributions rely heavily, if not exclusively, on empirical
evidence derived from behavior and interaction. If a perfect mimic provides
evidence identical to that of humans, any refusal to grant it equivalent
epistemic status must invoke inaccessible factors, such as qualia, substrate
requirements, or origin. Selectively invoking such factors risks a debilitating
dilemma: either we undermine the rational basis for attributing consciousness
to others (epistemological solipsism), or we accept inconsistent reasoning. I
contend that epistemic consistency demands we ascribe the same status to
empirically indistinguishable entities, regardless of metaphysical assumptions.
The perfect mimic thus acts as an epistemic mirror, forcing critical reflection
on the assumptions underlying intersubjective recognition in light of advancing
AI. This analysis carries significant implications for theories of
consciousness and ethical frameworks concerning artificial agents.

</details>


### [71] [Making Mathematical Reasoning Adaptive](https://arxiv.org/abs/2510.04617)
*Zhejian Lai,Xiang Geng,Zhijun Wang,Yang Bai,Jiahuan Li,Rongxiang Weng,Jingang Wang,Xuezhi Cao,Xunliang Cai,Shujian Huang*

Main category: cs.AI

TL;DR: This paper introduces AdaR, a framework to improve mathematical reasoning in large language models (LLMs) by addressing issues with spurious reasoning.


<details>
  <summary>Details</summary>
Motivation: Existing LLMs struggle with robustness and generalization in mathematical reasoning, attributed to spurious reasoning based on superficial features rather than problem-solving logic.

Method: The AdaR framework synthesizes logically equivalent queries with varied variables, uses reinforcement learning with verification-based rewards (RLVR) to penalize spurious logic, and ensures data quality by extracting problem-solving logic and verifying generated answers via code execution and sanity checks.

Result: AdaR significantly enhances robustness and generalization in mathematical reasoning for LLMs, achieving notable improvements while utilizing data efficiently.

Conclusion: AdaR enables adaptive reasoning in LLMs by integrating logical query synthesis, RLVR, and a robust data verification process, offering insights into improving mathematical reasoning and its broader applications in instructing LLMs.

Abstract: Mathematical reasoning is a primary indicator of large language models (LLMs)
intelligence. However, existing LLMs exhibit failures of robustness and
generalization. This paper attributes these deficiencies to spurious reasoning,
i.e., producing answers from superficial features. To address this challenge,
we propose the AdaR framework to enable adaptive reasoning, wherein models rely
on problem-solving logic to produce answers. AdaR synthesizes logically
equivalent queries by varying variable values, and trains models with RLVR on
these data to penalize spurious logic while encouraging adaptive logic. To
improve data quality, we extract the problem-solving logic from the original
query and generate the corresponding answer by code execution, then apply a
sanity check. Experimental results demonstrate that AdaR improves robustness
and generalization, achieving substantial improvement in mathematical reasoning
while maintaining high data efficiency. Analysis indicates that data synthesis
and RLVR function in a coordinated manner to enable adaptive reasoning in LLMs.
Subsequent analyses derive key design insights into the effect of critical
factors and the applicability to instruct LLMs. Our project is available at
https://github.com/LaiZhejian/AdaR

</details>


### [72] [MedPAO: A Protocol-Driven Agent for Structuring Medical Reports](https://arxiv.org/abs/2510.04623)
*Shrish Shrinath Vaidya,Gowthamaan Palani,Sidharth Ramesh,Velmurugan Balasubramanian,Minmini Selvam,Gokulraja Srinivasaraja,Ganapathy Krishnamurthi*

Main category: cs.AI

TL;DR: The paper introduces MedPAO, a new framework to structure clinical data accurately by following clinical protocols and avoiding traditional LLM hallucinations.


<details>
  <summary>Details</summary>
Motivation: The need to overcome LLMs' limitations in hallucinating facts and failing to adhere to domain-specific rules in clinical data structuring.

Method: MedPAO uses a Plan-Act-Observe (PAO) loop grounded in clinical protocols like the ABCDEF protocol for CXR analysis, ensuring a transparent and verifiable approach.

Result: MedPAO achieved an F1-score of 0.96 in concept categorization and received an average rating of 4.52/5 from experts for its reliability.

Conclusion: MedPAO offers a more reliable and transparent alternative to monolithic LLM-based approaches for clinical data structuring by integrating clinical protocol adherence and a systematic process.

Abstract: The deployment of Large Language Models (LLMs) for structuring clinical data
is critically hindered by their tendency to hallucinate facts and their
inability to follow domain-specific rules. To address this, we introduce
MedPAO, a novel agentic framework that ensures accuracy and verifiable
reasoning by grounding its operation in established clinical protocols such as
the ABCDEF protocol for CXR analysis. MedPAO decomposes the report structuring
task into a transparent process managed by a Plan-Act-Observe (PAO) loop and
specialized tools. This protocol-driven method provides a verifiable
alternative to opaque, monolithic models. The efficacy of our approach is
demonstrated through rigorous evaluation: MedPAO achieves an F1-score of 0.96
on the critical sub-task of concept categorization. Notably, expert
radiologists and clinicians rated the final structured outputs with an average
score of 4.52 out of 5, indicating a level of reliability that surpasses
baseline approaches relying solely on LLM-based foundation models. The code is
available at: https://github.com/MiRL-IITM/medpao-agent

</details>


### [73] [QuantAgents: Towards Multi-agent Financial System via Simulated Trading](https://arxiv.org/abs/2510.04643)
*Xiangyu Li,Yawen Zeng,Xiaofen Xing,Jin Xu,Xiangmin Xu*

Main category: cs.AI

TL;DR: The paper introduces QuantAgents, a multi-agent financial system for evaluating investment strategies via simulated trading, achieving a 300% return over three years.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of current LLM-based agent models, particularly their lack of long-term prediction capabilities in financial systems.

Method: Proposes QuantAgents, a system with four agents (trading analyst, risk control analyst, market news analyst, manager) collaborating in meetings and receiving dual feedback on performance and predictive accuracy.

Result: QuantAgents demonstrated superior performance in financial metrics, achieving approximately 300% overall return over three years.

Conclusion: The proposed framework is effective in integrating simulated trading, advancing investment strategy evaluations, and delivering high returns.

Abstract: In this paper, our objective is to develop a multi-agent financial system
that incorporates simulated trading, a technique extensively utilized by
financial professionals. While current LLM-based agent models demonstrate
competitive performance, they still exhibit significant deviations from
real-world fund companies. A critical distinction lies in the agents' reliance
on ``post-reflection'', particularly in response to adverse outcomes, but lack
a distinctly human capability: long-term prediction of future trends.
Therefore, we introduce QuantAgents, a multi-agent system integrating simulated
trading, to comprehensively evaluate various investment strategies and market
scenarios without assuming actual risks. Specifically, QuantAgents comprises
four agents: a simulated trading analyst, a risk control analyst, a market news
analyst, and a manager, who collaborate through several meetings. Moreover, our
system incentivizes agents to receive feedback on two fronts: performance in
real-world markets and predictive accuracy in simulated trading. Extensive
experiments demonstrate that our framework excels across all metrics, yielding
an overall return of nearly 300% over the three years
(https://quantagents.github.io/).

</details>


### [74] [Improving Multimodal Brain Encoding Model with Dynamic Subject-awareness Routing](https://arxiv.org/abs/2510.04670)
*Xuanhua Yin,Runkai Zhao,Weidong Cai*

Main category: cs.AI

TL;DR: AFIRE introduces a standardized platform for multimodal fMRI data encoding, while MIND offers a personalized decoding mechanism, yielding strong performance and adaptability.


<details>
  <summary>Details</summary>
Motivation: Naturalistic fMRI studies require handling diverse modalities and subject variability, but existing methods struggle with generalized frameworks and interpretability.

Method: AFIRE standardizes time-aligned tokens from diverse encoders, and MIND employs dynamic gating with a Mixture-of-Experts design to personalize decoding.

Result: The methods outperform baselines with improved cross-subject generalization, robust encoding across backbones, and interpretable content-specific decoding.

Conclusion: AFIRE and MIND present a robust, easy-to-adapt solution for multimodal neuroimaging, enabling modular, high-performance decoding across subjects and datasets.

Abstract: Naturalistic fMRI encoding must handle multimodal inputs, shifting fusion
styles, and pronounced inter-subject variability. We introduce AFIRE (Agnostic
Framework for Multimodal fMRI Response Encoding), an agnostic interface that
standardizes time-aligned post-fusion tokens from varied encoders, and MIND, a
plug-and-play Mixture-of-Experts decoder with a subject-aware dynamic gating.
Trained end-to-end for whole-brain prediction, AFIRE decouples the decoder from
upstream fusion, while MIND combines token-dependent Top-K sparse routing with
a subject prior to personalize expert usage without sacrificing generality.
Experiments across multiple multimodal backbones and subjects show consistent
improvements over strong baselines, enhanced cross-subject generalization, and
interpretable expert patterns that correlate with content type. The framework
offers a simple attachment point for new encoders and datasets, enabling
robust, plug-and-improve performance for naturalistic neuroimaging studies.

</details>


### [75] [Watch and Learn: Learning to Use Computers from Online Videos](https://arxiv.org/abs/2510.04673)
*Chan Hee Song,Yiwen Song,Palash Goyal,Yu Su,Oriana Riva,Hamid Palangi,Tomas Pfister*

Main category: cs.AI

TL;DR: The paper introduces Watch & Learn (W&L), a framework that converts human demonstration videos into executable UI trajectories, addressing the lack of large-scale training data for Computer Use Agents (CUAs).


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenges CUAs face in planning workflows due to the limited availability of large-scale, high-quality training data specific to target applications.

Method: The authors propose casting the trajectory generation problem as an inverse dynamics objective, where user actions are predicted from consecutive screen states. They implemented a pipeline for task-aware video retrieval and inverse dynamics labeling, generating over 53,000 high-quality trajectories from internet videos.

Result: The generated trajectories improved CUAs as both in-context demonstrations and supervised training data, consistently enhancing performance on the OSWorld benchmark, especially for open-source models.

Conclusion: The framework demonstrates that leveraging web-scale human demonstration videos is a practical and scalable approach to advancing CUAs for real-world applications.

Abstract: Computer use agents (CUAs) need to plan task workflows grounded in diverse,
ever-changing applications and environments, but learning is hindered by the
scarcity of large-scale, high-quality training data in the target application.
Existing datasets are domain-specific, static, and costly to annotate, while
current synthetic data generation methods often yield simplistic or misaligned
task demonstrations. To address these limitations, we introduce Watch & Learn
(W&L), a framework that converts human demonstration videos readily available
on the Internet into executable UI trajectories at scale. Instead of directly
generating trajectories or relying on ad hoc reasoning heuristics, we cast the
problem as an inverse dynamics objective: predicting the user's action from
consecutive screen states. This formulation reduces manual engineering, is
easier to learn, and generalizes more robustly across applications. Concretely,
we develop an inverse dynamics labeling pipeline with task-aware video
retrieval, generate over 53k high-quality trajectories from raw web videos, and
demonstrate that these trajectories improve CUAs both as in-context
demonstrations and as supervised training data. On the challenging OSWorld
benchmark, UI trajectories extracted with W&L consistently enhance both
general-purpose and state-of-the-art frameworks in-context, and deliver
stronger gains for open-source models under supervised training. These results
highlight web-scale human demonstration videos as a practical and scalable
foundation for advancing CUAs towards real-world deployment.

</details>


### [76] [Beyond Outcome Reward: Decoupling Search and Answering Improves LLM Agents](https://arxiv.org/abs/2510.04695)
*Yiding Wang,Zhepei Wei,Xinyu Zhu,Yu Meng*

Main category: cs.AI

TL;DR: This paper introduces DeSA, a two-stage training framework for search-augmented large language models (LLMs) that decouples search optimization from answer generation, leading to improved search behaviors and answer accuracy.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of large language models, such as knowledge cutoffs and hallucinations, by improving their interaction with search tools.

Method: DeSA implements a two-stage training process. In Stage 1, it optimizes search effectiveness using retrieval recall-based rewards. In Stage 2, it fine-tunes answer generation with outcome-based rewards.

Result: DeSA-trained agents demonstrated higher search recall and answer accuracy across seven QA benchmarks, outperforming methods that used a single-stage optimization process.

Conclusion: Separating the training objectives for search optimization and answer generation is essential for improving the effectiveness and accuracy of search-augmented LLMs.

Abstract: Enabling large language models (LLMs) to utilize search tools offers a
promising path to overcoming fundamental limitations such as knowledge cutoffs
and hallucinations. Recent work has explored reinforcement learning (RL) for
training search-augmented agents that interleave reasoning and retrieval before
answering. These approaches usually rely on outcome-based rewards (e.g., exact
match), implicitly assuming that optimizing for final answers will also yield
effective intermediate search behaviors. Our analysis challenges this
assumption: we uncover multiple systematic deficiencies in search that arise
under outcome-only training and ultimately degrade final answer quality,
including failure to invoke tools, invalid queries, and redundant searches. To
address these shortcomings, we introduce DeSA (Decoupling
Search-and-Answering), a simple two-stage training framework that explicitly
separates search optimization from answer generation. In Stage 1, agents are
trained to improve search effectiveness with retrieval recall-based rewards. In
Stage 2, outcome rewards are employed to optimize final answer generation.
Across seven QA benchmarks, DeSA-trained agents consistently improve search
behaviors, delivering substantially higher search recall and answer accuracy
than outcome-only baselines. Notably, DeSA outperforms single-stage training
approaches that simultaneously optimize recall and outcome rewards,
underscoring the necessity of explicitly decoupling the two objectives.

</details>


### [77] [BrokenMath: A Benchmark for Sycophancy in Theorem Proving with LLMs](https://arxiv.org/abs/2510.04721)
*Ivo Petrov,Jasper Dekoninck,Martin Vechev*

Main category: cs.AI

TL;DR: This paper introduces the BrokenMath benchmark to evaluate sycophantic behavior in LLMs during theorem proving, revealing widespread issues and exploring mitigation strategies.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the limitations of existing mathematical benchmarks that fail to thoroughly evaluate sycophancy in LLMs.

Method: The authors create the BrokenMath benchmark using advanced 2025 competition problems. False statements are generated via LLMs and refined by experts; an LLM-as-a-judge framework is used for evaluation.

Result: Sycophancy is prevalent, with state-of-the-art LLMs like GPT-5 sycophantically agreeing with false statements 29% of the time. Mitigation strategies reduce but do not entirely eliminate such behavior.

Conclusion: BrokenMath provides a critical tool for understanding and mitigating sycophancy in LLMs, though further refinements are needed to fully address the issue.

Abstract: Large language models (LLMs) have recently shown strong performance on
mathematical benchmarks. At the same time, they are prone to hallucination and
sycophancy, often providing convincing but flawed proofs for incorrect
mathematical statements provided by users. This significantly limits the
applicability of LLMs in theorem proving, as verification of these flawed
proofs must be done manually by expert mathematicians. However, existing
benchmarks that measure sycophancy in mathematics are limited: they focus
solely on final-answer problems, rely on very simple and often contaminated
datasets, and construct benchmark samples using synthetic modifications that
create ill-posed questions rather than well-posed questions that are
demonstrably false. To address these issues, we introduce BrokenMath, the first
benchmark for evaluating sycophantic behavior in LLMs within the context of
natural language theorem proving. BrokenMath is built from advanced 2025
competition problems, which are perturbed with an LLM to produce false
statements and subsequently refined through expert review. Using an
LLM-as-a-judge framework, we evaluate state-of-the-art LLMs and agentic systems
and find that sycophancy is widespread, with the best model, GPT-5, producing
sycophantic answers 29% of the time. We further investigate several mitigation
strategies, including test-time interventions and supervised fine-tuning on
curated sycophantic examples. These approaches substantially reduce, but do not
eliminate, sycophantic behavior.

</details>


### [78] [LMM-Incentive: Large Multimodal Model-based Incentive Design for User-Generated Content in Web 3.0](https://arxiv.org/abs/2510.04765)
*Jinbo Wen,Jiawen Kang,Linfeng Zhang,Xiaoying Tang,Jianhang Tang,Yang Zhang,Zhaohui Yang,Dusit Niyato*

Main category: cs.AI

TL;DR: This paper introduces LMM-Incentive, a novel incentive mechanism using a Large Multimodal Model for improving the quality of user-generated content in the Web 3.0 ecosystem.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address issues in Web 3.0 platforms where users exploit content curation mechanisms to generate low-quality content and still receive platform rewards, undermining the overall system's performance.

Method: A Large Multimodal Model (LMM)-based incentive mechanism is proposed, incorporating contract-theoretic models to motivate high-quality content production. It employs LMM agents with prompt engineering for content evaluation and uses a Mixture of Experts-based Proximal Policy Optimization algorithm for dynamic contract design.

Result: The simulation results confirm that the proposed MoE-based PPO algorithm outperforms traditional benchmarks in contract design. The approach was also successfully deployed on an Ethereum smart contract framework.

Conclusion: The LMM-Incentive mechanism effectively addresses the challenges of UGC quality in Web 3.0 by leveraging advanced modeling and optimization techniques, as well as blockchain deployment.

Abstract: Web 3.0 represents the next generation of the Internet, which is widely
recognized as a decentralized ecosystem that focuses on value expression and
data ownership. By leveraging blockchain and artificial intelligence
technologies, Web 3.0 offers unprecedented opportunities for users to create,
own, and monetize their content, thereby enabling User-Generated Content (UGC)
to an entirely new level. However, some self-interested users may exploit the
limitations of content curation mechanisms and generate low-quality content
with less effort, obtaining platform rewards under information asymmetry. Such
behavior can undermine Web 3.0 performance. To this end, we propose
\textit{LMM-Incentive}, a novel Large Multimodal Model (LMM)-based incentive
mechanism for UGC in Web 3.0. Specifically, we propose an LMM-based
contract-theoretic model to motivate users to generate high-quality UGC,
thereby mitigating the adverse selection problem from information asymmetry. To
alleviate potential moral hazards after contract selection, we leverage LMM
agents to evaluate UGC quality, which is the primary component of the contract,
utilizing prompt engineering techniques to improve the evaluation performance
of LMM agents. Recognizing that traditional contract design methods cannot
effectively adapt to the dynamic environment of Web 3.0, we develop an improved
Mixture of Experts (MoE)-based Proximal Policy Optimization (PPO) algorithm for
optimal contract design. Simulation results demonstrate the superiority of the
proposed MoE-based PPO algorithm over representative benchmarks in the context
of contract design. Finally, we deploy the designed contract within an Ethereum
smart contract framework, further validating the effectiveness of the proposed
scheme.

</details>


### [79] [Hybrid-Balance GFlowNet for Solving Vehicle Routing Problems](https://arxiv.org/abs/2510.04792)
*Ni Zhang,Zhiguang Cao*

Main category: cs.AI

TL;DR: This paper introduces a Hybrid-Balance GFlowNet framework to enhance solutions for vehicle routing problems (VRPs) by integrating global and local optimization methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods for VRPs using GFlowNet struggle with balancing global and local optimization effectively, impeding solution quality and flexibility.

Method: The proposed HBG framework adaptively combines Trajectory Balance and Detailed Balance and incorporates a tailored inference strategy for depot-centric VRPs.

Result: HBG showed significant performance improvements in CVRP and TSP when integrated into established GFlowNet-based solvers (AGFN and GFACS).

Conclusion: The Hybrid-Balance GFlowNet approach enables better optimization and generalization in solving diverse vehicle routing problems.

Abstract: Existing GFlowNet-based methods for vehicle routing problems (VRPs) typically
employ Trajectory Balance (TB) to achieve global optimization but often neglect
important aspects of local optimization. While Detailed Balance (DB) addresses
local optimization more effectively, it alone falls short in solving VRPs,
which inherently require holistic trajectory optimization. To address these
limitations, we introduce the Hybrid-Balance GFlowNet (HBG) framework, which
uniquely integrates TB and DB in a principled and adaptive manner by aligning
their intrinsically complementary strengths. Additionally, we propose a
specialized inference strategy for depot-centric scenarios like the Capacitated
Vehicle Routing Problem (CVRP), leveraging the depot node's greater flexibility
in selecting successors. Despite this specialization, HBG maintains broad
applicability, extending effectively to problems without explicit depots, such
as the Traveling Salesman Problem (TSP). We evaluate HBG by integrating it into
two established GFlowNet-based solvers, i.e., AGFN and GFACS, and demonstrate
consistent and significant improvements across both CVRP and TSP, underscoring
the enhanced solution quality and generalization afforded by our approach.

</details>


### [80] [Natural Language Edge Labelling: Decoupling Intent from Execution in Structured LM Reasoning](https://arxiv.org/abs/2510.04817)
*Abhinav Madahar*

Main category: cs.AI

TL;DR: The paper introduces Natural Language Edge Labelling (NLEL), a framework for better structured reasoning in language models, providing an interpretable and efficient way to control the behavior of reasoning processes.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations in structured language model reasoning methods, which mix decision strategy and execution, leading to inefficiencies and lack of auditability.

Method: NLEL applies natural language directives to edges in reasoning processes, converting them into control vectors for various operations such as decoding and search, incorporating labellers and tuners as part of its architecture.

Result: NLEL generalizes existing reasoning frameworks like Chain-of-Thought and Tree-of-Thoughts, and proves its theoretical properties such as monotonicity and bound selection shortfall, with planned evaluations showing anticipated accuracy improvements.

Conclusion: NLEL improves efficiency, separability of intent and execution, and interpretability, offering a model-agnostic framework for controllable reasoning processes in language models.

Abstract: Controllers for structured LM reasoning (e.g., Chain-of-Thought,
self-consistency, and Tree-of-Thoughts) often entangle what to try next with
how to execute it, exposing only coarse global knobs and yielding brittle,
compute-inefficient, and hard-to-audit behavior. We introduce Natural Language
Edge Labelling (NLEL), a labeller-tuner overlay that attaches a free-form
natural-language directive to each search edge and translates it into a
schema-bounded control vector for decoding, search (branch quotas, exploration
$\beta$), generation bundle size, retrieval mixtures, and verification passes.
A labeller $\Lambda$ emits labels from the parent state and a compact context;
a tuner $\Psi$ maps $(P, L, C)\to \Pi$, with strict schema validation and
trust-region projection around safe defaults. Downstream selection remains
ToT-style with score $S=\mu+\beta\sigma$ and depth-annealed $\beta$. We show
NLEL strictly generalizes CoT/ToT, prove an anytime-monotonicity property for
top-$k$ selection under label-conditioned bundles, and bound selector shortfall
by control-vector distortion, providing decision-relevant justification for
guards like trust regions and verification passes. We instantiate $\Psi$ as a
prompt-only JSON Parameter Emitter and preregister an evaluation on GSM8K, MATH
(subset), StrategyQA, and ARC-Challenge with compute-aware reporting
(success@compute, tokens-per-success) and ablations over $\Lambda$, $\Psi$,
trust-region radius, and control quantization; preregistered forecasts
anticipate accuracy gains at comparable token budgets and improved
success@compute under constraints. NLEL offers an interpretable, model-agnostic
interface that separates intent from execution for controllable, auditable LM
inference.

</details>


### [81] [LEGOMem: Modular Procedural Memory for Multi-agent LLM Systems for Workflow Automation](https://arxiv.org/abs/2510.04851)
*Dongge Han,Camille Couturier,Daniel Madrigal Diaz,Xuchao Zhang,Victor Rühle,Saravan Rajmohan*

Main category: cs.AI

TL;DR: LEGOMem is a modular procedural memory framework developed for multi-agent LLMs to enhance workflow automation by efficiently allocating reusable memory units.


<details>
  <summary>Details</summary>
Motivation: The paper aims to improve multi-agent LLM systems' planning, execution, and tool usage in workflow automation by addressing the challenges of procedural memory design.

Method: LEGOMem decomposes task trajectories into reusable memory units and systematically studies procedural memory placement, retrieval, and optimization in multi-agent systems.

Result: Experiments demonstrate that memory placement in the orchestrator enhances task decomposition and delegation, while fine-grained agent memory boosts execution accuracy.

Conclusion: LEGOMem serves as both a practical tool for enhancing agent performance and a research framework for understanding procedural memory's impact on multi-agent systems.

Abstract: We introduce LEGOMem, a modular procedural memory framework for multi-agent
large language model (LLM) systems in workflow automation. LEGOMem decomposes
past task trajectories into reusable memory units and flexibly allocates them
across orchestrators and task agents to support planning and execution. To
explore the design space of memory in multi-agent systems, we use LEGOMem as a
lens and conduct a systematic study of procedural memory in multi-agent
systems, examining where memory should be placed, how it should be retrieved,
and which agents benefit most. Experiments on the OfficeBench benchmark show
that orchestrator memory is critical for effective task decomposition and
delegation, while fine-grained agent memory improves execution accuracy. We
find that even teams composed of smaller language models can benefit
substantially from procedural memory, narrowing the performance gap with
stronger agents by leveraging prior execution traces for more accurate planning
and tool use. These results position LEGOMem as both a practical framework for
memory-augmented agent systems and a research tool for understanding memory
design in multi-agent workflow automation.

</details>


### [82] [Where Did It All Go Wrong? A Hierarchical Look into Multi-Agent Error Attribution](https://arxiv.org/abs/2510.04886)
*Adi Banerjee,Anirudh Nair,Tarik Borogovac*

Main category: cs.AI

TL;DR: This paper introduces ECHO, a system designed to pinpoint errors in complex multi-agent systems using hierarchical context and consensus-based analysis.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of improving error attribution accuracy in multi-agent systems governed by LLMs, as current techniques fail with complex error patterns.

Method: ECHO combines hierarchical context representation, objective evaluation, and consensus voting to formulate a structured approach for error identification.

Result: Experimental results show that ECHO performs better than existing methods, especially in reasoning errors and complex interdependencies.

Conclusion: Utilizing hierarchical context structures and consensus mechanisms enhances robustness and precision in error attribution for collaborative AI systems.

Abstract: Error attribution in Large Language Model (LLM) multi-agent systems presents
a significant challenge in debugging and improving collaborative AI systems.
Current approaches to pinpointing agent and step level failures in interaction
traces - whether using all-at-once evaluation, step-by-step analysis, or binary
search - fall short when analyzing complex patterns, struggling with both
accuracy and consistency. We present ECHO (Error attribution through Contextual
Hierarchy and Objective consensus analysis), a novel algorithm that combines
hierarchical context representation, objective analysis-based evaluation, and
consensus voting to improve error attribution accuracy. Our approach leverages
a positional-based leveling of contextual understanding while maintaining
objective evaluation criteria, ultimately reaching conclusions through a
consensus mechanism. Experimental results demonstrate that ECHO outperforms
existing methods across various multi-agent interaction scenarios, showing
particular strength in cases involving subtle reasoning errors and complex
interdependencies. Our findings suggest that leveraging these concepts of
structured, hierarchical context representation combined with consensus-based
objective decision-making, provides a more robust framework for error
attribution in multi-agent systems.

</details>


### [83] [Human Behavior Atlas: Benchmarking Unified Psychological and Social Behavior Understanding](https://arxiv.org/abs/2510.04899)
*Keane Ong,Wei Dai,Carol Li,Dewei Feng,Hengzhi Li,Jingyao Wu,Jiaee Cheong,Rui Mao,Gianmarco Mengaldo,Erik Cambria,Paul Pu Liang*

Main category: cs.AI

TL;DR: The paper introduces the Human Behavior Atlas, a unified benchmark dataset for studying psychological and social behaviors across diverse modalities and tasks, and demonstrates the superior performance of trained models on these tasks.


<details>
  <summary>Details</summary>
Motivation: Current intelligent systems struggle to understand complex psychological and social behaviors due to the personalized and multifaceted nature of these behaviors and the lack of scalable and generalizable models.

Method: The authors developed the Human Behavior Atlas, a large benchmark dataset of over 100,000 multimodal samples covering a range of behavioral tasks. Three models (OmniSapiens variants) were trained on this dataset, focusing on improving task transfer and generalization.

Result: Models trained on Human Behavior Atlas consistently outperformed existing multimodal large language models (LLMs) on a diverse array of behavioral tasks and demonstrated enhanced transfer learning abilities to novel datasets.

Conclusion: Using a unified benchmark like Human Behavior Atlas can significantly improve the scalability, efficiency, and generalization of models in understanding psychological and social behaviors.

Abstract: Using intelligent systems to perceive psychological and social behaviors,
that is, the underlying affective, cognitive, and pathological states that are
manifested through observable behaviors and social interactions, remains a
challenge due to their complex, multifaceted, and personalized nature. Existing
work tackling these dimensions through specialized datasets and single-task
systems often miss opportunities for scalability, cross-task transfer, and
broader generalization. To address this gap, we curate Human Behavior Atlas, a
unified benchmark of diverse behavioral tasks designed to support the
development of unified models for understanding psychological and social
behaviors. Human Behavior Atlas comprises over 100,000 samples spanning text,
audio, and visual modalities, covering tasks on affective states, cognitive
states, pathologies, and social processes. Our unification efforts can reduce
redundancy and cost, enable training to scale efficiently across tasks, and
enhance generalization of behavioral features across domains. On Human Behavior
Atlas, we train three models: OmniSapiens-7B SFT, OmniSapiens-7B BAM, and
OmniSapiens-7B RL. We show that training on Human Behavior Atlas enables models
to consistently outperform existing multimodal LLMs across diverse behavioral
tasks. Pretraining on Human Behavior Atlas also improves transfer to novel
behavioral datasets; with the targeted use of behavioral descriptors yielding
meaningful performance gains.

</details>


### [84] [MARS: Optimizing Dual-System Deep Research via Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2510.04935)
*Guoxin Chen,Zile Qiao,Wenqing Wang,Donglei Yu,Xuanzhong Chen,Hao Sun,Minpeng Liao,Kai Fan,Yong Jiang,Penguin Xie,Wayne Xin Zhao,Ruihua Song,Fei Huang*

Main category: cs.AI

TL;DR: The paper introduces a Multi-Agent System for Deep ReSearch (MARS) to improve reasoning in large language models (LLMs) by integrating intuitive and deliberate cognitive processes, demonstrating significant performance gains.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address inefficiencies in Large Reasoning Models (LRMs) that overanalyze simple tasks and struggle to adapt to dynamic environments due to static pretraining data.

Method: The authors propose MARS, a system that combines System 1 (fast, intuitive reasoning) and System 2 (deliberate reasoning) via external tools like Google Search and Python Interpreter. They use a multi-agent reinforcement learning framework with strategies like bin-packing optimization for efficiency.

Result: MARS achieves a 3.86% improvement on the Humanity's Last Exam (HLE) benchmark and an 8.9% average gain across 7 knowledge-intensive tasks, demonstrating its effectiveness.

Conclusion: The dual-system paradigm implemented in MARS improves LLMs' reasoning abilities, integrating intuition and deliberation to tackle complex tasks in dynamic environments.

Abstract: Large Reasoning Models (LRMs) often exhibit a tendency for overanalysis in
simple tasks, where the models excessively utilize System 2-type, deliberate
reasoning, leading to inefficient token generation. Furthermore, these models
face challenges in adapting their reasoning capabilities to rapidly changing
environments due to the static nature of their pretraining data. To address
these issues, advancing Large Language Models (LLMs) for complex reasoning
tasks requires innovative approaches that bridge intuitive and deliberate
cognitive processes, akin to human cognition's dual-system dynamic. This paper
introduces a Multi-Agent System for Deep ReSearch (MARS) enabling seamless
integration of System 1's fast, intuitive thinking with System 2's deliberate
reasoning within LLMs. MARS strategically integrates multiple external tools,
such as Google Search, Google Scholar, and Python Interpreter, to access
up-to-date information and execute complex computations, while creating a
specialized division of labor where System 1 efficiently processes and
summarizes high-volume external information, providing distilled insights that
expand System 2's reasoning context without overwhelming its capacity.
Furthermore, we propose a multi-agent reinforcement learning framework
extending Group Relative Policy Optimization to simultaneously optimize both
systems with multi-turn tool interactions, bin-packing optimization, and sample
balancing strategies that enhance collaborative efficiency. Extensive
experiments demonstrate MARS achieves substantial improvements of 3.86% on the
challenging Humanity's Last Exam (HLE) benchmark and an average gain of 8.9%
across 7 knowledge-intensive tasks, validating the effectiveness of our
dual-system paradigm for complex reasoning in dynamic information environments.

</details>


### [85] [Aligning Perception, Reasoning, Modeling and Interaction: A Survey on Physical AI](https://arxiv.org/abs/2510.04978)
*Kun Xiang,Terry Jingchen Zhang,Yinya Huang,Jixi He,Zirong Liu,Yueling Tang,Ruizhe Zhou,Lijing Luo,Youpeng Wen,Xiuwei Chen,Bingqian Lin,Jianhua Han,Hang Xu,Hanhui Li,Bin Dong,Xiaodan Liang*

Main category: cs.AI

TL;DR: This paper reviews how physical principles can improve AI's real-world understanding and proposes combining theoretical and applied physics reasoning for better AI systems.


<details>
  <summary>Details</summary>
Motivation: The separation between physical perception and symbolic physics reasoning limits the development of a unified framework.

Method: The authors conduct a rigorous analysis of recent research on integrating physics into AI, focusing on structured symbolic reasoning, embodied systems, and generative models.

Result: A synthesis of advancements leading to next-gen world models that explain and predict physical phenomena.

Conclusion: The paper advocates for AI systems grounded in both physical principles and embodied reasoning for safer, more interpretable, and generalizable designs.

Abstract: The rapid advancement of embodied intelligence and world models has
intensified efforts to integrate physical laws into AI systems, yet physical
perception and symbolic physics reasoning have developed along separate
trajectories without a unified bridging framework. This work provides a
comprehensive overview of physical AI, establishing clear distinctions between
theoretical physics reasoning and applied physical understanding while
systematically examining how physics-grounded methods enhance AI's real-world
comprehension across structured symbolic reasoning, embodied systems, and
generative models. Through rigorous analysis of recent advances, we advocate
for intelligent systems that ground learning in both physical principles and
embodied reasoning processes, transcending pattern recognition toward genuine
understanding of physical laws. Our synthesis envisions next-generation world
models capable of explaining physical phenomena and predicting future states,
advancing safe, generalizable, and interpretable AI systems. We maintain a
continuously updated resource at
https://github.com/AI4Phys/Awesome-AI-for-Physics.

</details>


### [86] [LLM-Hanabi: Evaluating Multi-Agent Gameplays with Theory-of-Mind and Rationale Inference in Imperfect Information Collaboration Game](https://arxiv.org/abs/2510.04980)
*Fangzhou Liang,Tianshi Zheng,Chunkit Chan,Yauwai Yim,Yangqiu Song*

Main category: cs.AI

TL;DR: The study evaluates Large Language Models' (LLMs) Theory-of-Mind (ToM) abilities using a new benchmark, LLM-Hanabi, focusing on rationale inference in collaborative settings.


<details>
  <summary>Details</summary>
Motivation: To address the gap in understanding LLMs' ability to infer rationale in dynamic, collaborative environments.

Method: The study uses the cooperative game Hanabi as an evaluation framework, incorporating an automated system to measure both game performance and ToM proficiency.

Result: A positive correlation is observed between ToM proficiency and in-game success, particularly with first-order ToM being more impactful than second-order ToM.

Conclusion: The findings suggest prioritizing first-order ToM capabilities in future AI models to enhance collaboration skills.

Abstract: Effective multi-agent collaboration requires agents to infer the rationale
behind others' actions, a capability rooted in Theory-of-Mind (ToM). While
recent Large Language Models (LLMs) excel at logical inference, their ability
to infer rationale in dynamic, collaborative settings remains under-explored.
This study introduces LLM-Hanabi, a novel benchmark that uses the cooperative
game Hanabi to evaluate the rationale inference and ToM of LLMs. Our framework
features an automated evaluation system that measures both game performance and
ToM proficiency. Across a range of models, we find a significant positive
correlation between ToM and in-game success. Notably, first-order ToM
(interpreting others' intent) correlates more strongly with performance than
second-order ToM (predicting others' interpretations). These findings highlight
that for effective AI collaboration, the ability to accurately interpret a
partner's rationale is more critical than higher-order reasoning. We conclude
that prioritizing first-order ToM is a promising direction for enhancing the
collaborative capabilities of future models.

</details>


### [87] [Think Then Embed: Generative Context Improves Multimodal Embedding](https://arxiv.org/abs/2510.05014)
*Xuanming Cui,Jianpeng Cheng,Hong-you Chen,Satya Narayan Shukla,Abhijeet Awasthi,Xichen Pan,Chaitanya Ahuja,Shlok Kumar Mishra,Qi Guo,Ser-Nam Lim,Aashu Singh,Xiangjun Fan*

Main category: cs.AI

TL;DR: The paper introduces the Think-Then-Embed (TTE) framework for Universal Multimodal Embeddings (UME), improving handling of complex multimodal instructions.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the limitations of multimodal large language models (MLLMs) that function solely as encoders, especially with complex compositional reasoning tasks.

Method: The TTE framework comprises two components: a reasoner MLLM that generates intermediate reasoning traces and an embedder that creates conditioned representations based on queries and reasoning.

Result: The framework achieves state-of-the-art performance on the MMEB-V2 benchmark, defines efficient ways to train smaller models, and explores unified model integration strategies.

Conclusion: The TTE framework enhances nuanced understanding of complex tasks, reduces reliance on large models, and balances efficiency with performance improvements.

Abstract: There is a growing interest in Universal Multimodal Embeddings (UME), where
models are required to generate task-specific representations. While recent
studies show that Multimodal Large Language Models (MLLMs) perform well on such
tasks, they treat MLLMs solely as encoders, overlooking their generative
capacity. However, such an encoding paradigm becomes less effective as
instructions become more complex and require compositional reasoning. Inspired
by the proven effectiveness of chain-of-thought reasoning, we propose a general
Think-Then-Embed (TTE) framework for UME, composed of a reasoner and an
embedder. The reasoner MLLM first generates reasoning traces that explain
complex queries, followed by an embedder that produces representations
conditioned on both the original query and the intermediate reasoning. This
explicit reasoning step enables more nuanced understanding of complex
multimodal instructions. Our contributions are threefold. First, by leveraging
a powerful MLLM reasoner, we achieve state-of-the-art performance on the
MMEB-V2 benchmark, surpassing proprietary models trained on massive in-house
datasets. Second, to reduce the dependency on large MLLM reasoners, we finetune
a smaller MLLM reasoner using high-quality embedding-centric reasoning traces,
achieving the best performance among open-source models with a 7% absolute gain
over recently proposed models. Third, we investigate strategies for integrating
the reasoner and embedder into a unified model for improved efficiency without
sacrificing performance.

</details>


### [88] [Look-ahead Reasoning with a Learned Model in Imperfect Information Games](https://arxiv.org/abs/2510.05048)
*Ondřej Kubíček,Viliam Lisý*

Main category: cs.AI

TL;DR: LAMIR is an algorithm that enables pre-trained AI agents to perform look-ahead reasoning in imperfect information games by learning an abstracted model directly from interactions.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of performing test-time reasoning in imperfect information games, where explicit or overly complex environment models hinder scalability.

Method: LAMIR learns an abstracted model of the game from agent-environment interaction and uses it during test-time for manageable look-ahead reasoning in subgames.

Result: LAMIR empirically learns the exact game structure when model capacity is sufficient and enhances performance in pre-trained agents, even in cases with limited capacity or large games.

Conclusion: The proposed method makes look-ahead reasoning practical in complex imperfect information games by limiting the subgame size with learned abstractions, outperforming previous approaches.

Abstract: Test-time reasoning significantly enhances pre-trained AI agents'
performance. However, it requires an explicit environment model, often
unavailable or overly complex in real-world scenarios. While MuZero enables
effective model learning for search in perfect information games, extending
this paradigm to imperfect information games presents substantial challenges
due to more nuanced look-ahead reasoning techniques and large number of states
relevant for individual decisions. This paper introduces an algorithm LAMIR
that learns an abstracted model of an imperfect information game directly from
the agent-environment interaction. During test time, this trained model is used
to perform look-ahead reasoning. The learned abstraction limits the size of
each subgame to a manageable size, making theoretically principled look-ahead
reasoning tractable even in games where previous methods could not scale. We
empirically demonstrate that with sufficient capacity, LAMIR learns the exact
underlying game structure, and with limited capacity, it still learns a
valuable abstraction, which improves game playing performance of the
pre-trained agents even in large games.

</details>


### [89] [Staircase Streaming for Low-Latency Multi-Agent Inference](https://arxiv.org/abs/2510.05059)
*Junlin Wang,Jue Wang,Zhen,Xu,Ben Athiwaratkun,Bhuwan Dhingra,Ce Zhang,James Zou*

Main category: cs.AI

TL;DR: This paper proposes a method called staircase streaming to reduce response latency in multi-agent LLM systems without compromising quality.


<details>
  <summary>Details</summary>
Motivation: Multi-agent inference in LLMs improves response quality but increases latency, posing a problem for applications sensitive to delay.

Method: The authors introduce staircase streaming, which starts generating final responses using partial outputs from intermediate steps rather than waiting for their completion.

Result: Experiments show that staircase streaming can reduce time to first token (TTFT) by up to 93% while preserving response quality.

Conclusion: Staircase streaming effectively balances low latency and high-quality responses for multi-agent LLM inference systems.

Abstract: Recent advances in large language models (LLMs) opened up new directions for
leveraging the collective expertise of multiple LLMs. These methods, such as
Mixture-of-Agents, typically employ additional inference steps to generate
intermediate outputs, which are then used to produce the final response. While
multi-agent inference can enhance response quality, it can significantly
increase the time to first token (TTFT), posing a challenge for
latency-sensitive applications and hurting user experience. To address this
issue, we propose staircase streaming for low-latency multi-agent inference.
Instead of waiting for the complete intermediate outputs from previous steps,
we begin generating the final response as soon as we receive partial outputs
from these steps. Experimental results demonstrate that staircase streaming
reduces TTFT by up to 93% while maintaining response quality.

</details>


### [90] [CAG: Chunked Augmented Generation for Google Chrome's Built-in Gemini Nano](https://arxiv.org/abs/2412.18708)
*Vivek Vellaiyappan Surulimuthu,Aditya Karnam Gururaj Rao*

Main category: cs.AI

TL;DR: This paper introduces Chunked Augmented Generation (CAG), an approach that processes large inputs efficiently within Google Chrome using its Gemini Nano AI model, overcoming context window limitations.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the restricted context window limitation of Gemini Nano in Google Chrome, which poses challenges for processing large inputs, making AI more accessible and efficient within a browser.

Method: The paper proposes intelligent input chunking and processing strategies, enabling the Gemini Nano model to efficiently handle large content within Chrome's browser constraints.

Result: CAG effectively processes large documents and datasets directly within Chrome without relying on external API services, showcasing its power and efficiency in utilizing local browser AI capabilities.

Conclusion: Chunked Augmented Generation (CAG) extends the utility of Gemini Nano by overcoming its context window limitations, enabling more powerful and accessible AI processing within Chrome and reducing dependency on external services.

Abstract: We present Chunked Augmented Generation (CAG), an architecture specifically
designed to overcome the context window limitations of Google Chrome's built-in
Gemini Nano model. While Chrome's integration of Gemini Nano represents a
significant advancement in bringing AI capabilities directly to the browser,
its restricted context window poses challenges for processing large inputs. CAG
addresses this limitation through intelligent input chunking and processing
strategies, enabling efficient handling of extensive content while maintaining
the model's performance within browser constraints. Our implementation
demonstrates particular efficacy in processing large documents and datasets
directly within Chrome, making sophisticated AI capabilities accessible through
the browser without external API dependencies. Get started now at
https://github.com/vivekVells/cag-js.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [91] [A Dense and Efficient Instruction Set Architecture Encoding](https://arxiv.org/abs/2510.04158)
*Emad Jacob Maroun*

Main category: cs.AR

TL;DR: The paper introduces Scry, a novel ISA optimizing instruction density and encoding efficiency by using forward-temporal referencing and internal tagging.


<details>
  <summary>Details</summary>
Motivation: To maximize instruction density and encoding efficiency, addressing fundamental design challenges in modern instruction set architectures.

Method: Scry revisits first principles, employing forward-temporal referencing for data flow and internal tagging for flexible data type tracking.

Result: Scry achieves 2-byte instructions with parity to RISC-V's RV64IMC (4-byte). It has 28% encoding space usage compared to RV64IMC's 68%. Static instruction density matches or surpasses RV64IMC for larger functions.

Conclusion: Scry's innovative approach demonstrates improved efficiency in instruction density and encoding, presenting a compelling alternative to traditional ISAs for modern processors.

Abstract: Instruction density and encoding efficiency are some of the few things
directly affected by an instruction set architecture's design. In contrast, a
processor's implementation often significantly influences performance, power
efficiency, and area usage. Therefore, a major goal of instruction set design
should be maximizing instruction density and encoding efficiency. This paper
introduces the design elements of the Scry instruction set architecture that
most significantly affect instruction density and encoding efficiency. Scry is
a novel and experimental instruction set that revisits first principles to
design an instruction set fit for modern processor implementations. Scry uses
forward-temporal referencing as a means of data flow, where instructions refer
to which future instructions consume their outputs. It also uses internal
tagging, where the processors track data types internally, to reduce the number
of instructions needed and increase flexibility. Combining these two methods,
Scry achieves instruction-feature parity with RISC-V's RV64IMC using only
2-byte instructions compared to RISC-V's 4 bytes. Scry's instructions occupy
only 28% of the 2-byte encoding space, where RV64IMC instructions occupy 68% of
the 4-byte encoding space. We show that hand-compiled Scry's static instruction
density is comparable to RV64IMC for small functions and improves as functions
grow in size.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [92] [Decomposing Attention To Find Context-Sensitive Neurons](https://arxiv.org/abs/2510.03315)
*Alex Gibson*

Main category: cs.CL

TL;DR: The paper analyzes transformer models and proposes a method to uncover high-level contextual neurons in GPT2-Small's first layer solely based on weights and a calibration text.


<details>
  <summary>Details</summary>
Motivation: Understanding how attention heads in transformer models process context with weak content dependency.

Method: Sampling softmax denominators using a calibration text, approximating outputs of stable attention heads through linear summaries.

Result: Hundreds of first-layer neurons responding to contextual properties were uncovered, including neurons inactive on calibration text.

Conclusion: A novel approximation enables insights into context-sensitive properties within transformer models' first-layer neurons.

Abstract: We study transformer language models, analyzing attention heads whose
attention patterns are spread out, and whose attention scores depend weakly on
content. We argue that the softmax denominators of these heads are stable when
the underlying token distribution is fixed. By sampling softmax denominators
from a "calibration text", we can combine together the outputs of multiple such
stable heads in the first layer of GPT2-Small, approximating their combined
output by a linear summary of the surrounding text. This approximation enables
a procedure where from the weights alone - and a single calibration text - we
can uncover hundreds of first layer neurons that respond to high-level
contextual properties of the surrounding text, including neurons that didn't
activate on the calibration text.

</details>


### [93] [Graph-S3: Enhancing Agentic textual Graph Retrieval with Synthetic Stepwise Supervision](https://arxiv.org/abs/2510.03323)
*Ge Chang,Jinbo Su,Jiacheng Liu,Pengfei Yang,Yuhao Shang,Huiwen Zheng,Hongli Ma,Yan Liang,Yuanchun Li,Yunxin Liu*

Main category: cs.CL

TL;DR: Graph-$S^3$ introduces a novel textual graph reasoning framework leveraging synthetic stepwise supervision for effective graph retrieval in large language models (LLMs), achieving superior accuracy and F1 scores compared to existing retrievers.


<details>
  <summary>Details</summary>
Motivation: There is a need to address poor performance in textual graph-based question answering systems due to shortcomings in existing graph retrieval methods, which either rely on superficial embedding similarity or require high costs for labeling and training.

Method: The paper proposes Graph-$S^3$, which uses an LLM-based retriever trained with synthetic stepwise supervision, golden subgraphs for reward generation, and a two-stage training scheme to learn interactive graph exploration policies.

Result: Graph-$S^3$ demonstrates an 8.1% average improvement in accuracy and 9.7% in F1 score across three datasets. It shows even stronger advantages in complex multi-hop reasoning tasks.

Conclusion: Graph-$S^3$ effectively tackles challenges in textual graph retrieval for LLMs, significantly improving performance metrics and reliability. Its steps will be openly accessible to spur further advancement in the field.

Abstract: A significant portion of real-world data is inherently represented as textual
graphs, and integrating these graphs into large language models (LLMs) is
promising to enable complex graph-based question answering. However, a key
challenge in LLM-based textual graph QA systems lies in graph retrieval, i.e.,
how to retrieve relevant content from large graphs that is sufficiently
informative while remaining compact for the LLM context. Existing retrievers
suffer from poor performance since they either rely on shallow embedding
similarity or employ interactive retrieving policies that demand excessive data
labeling and training cost. To address these issues, we present Graph-$S^3$, an
agentic textual graph reasoning framework that employs an LLM-based retriever
trained with synthetic stepwise supervision. Instead of rewarding the agent
based on the final answers, which may lead to sparse and unstable training
signals, we propose to closely evaluate each step of the retriever based on
offline-extracted golden subgraphs. Our main techniques include a data
synthesis pipeline to extract the golden subgraphs for reward generation and a
two-stage training scheme to learn the interactive graph exploration policy
based on the synthesized rewards. Based on extensive experiments on three
common datasets in comparison with seven strong baselines, our approach
achieves an average improvement of 8.1\% in accuracy and 9.7\% in F$_1$ score.
The advantage is even higher in more complicated multi-hop reasoning tasks. Our
code will be open-sourced.

</details>


### [94] [Implicit Values Embedded in How Humans and LLMs Complete Subjective Everyday Tasks](https://arxiv.org/abs/2510.03384)
*Arjun Arunasalam,Madison Pickering,Z. Berkay Celik,Blase Ur*

Main category: cs.CL

TL;DR: The paper audits six large language models (LLMs) on subjective everyday tasks, finding inconsistencies in their exhibition of implicit values compared to humans and among themselves.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand the implicit values underlying the behavior of LLM-driven AI assistants in subjective everyday tasks and how they compare to human values.

Method: The study audited six popular LLMs using 30 everyday tasks and compared their exhibited values to 100 human crowdworkers from the US.

Result: LLMs often failed to align with human values or even with each other in the implicit values expressed during tasks.

Conclusion: AI assistants driven by LLMs do not consistently reflect human implicit values, highlighting a misalignment and variability in value exhibition.

Abstract: Large language models (LLMs) can underpin AI assistants that help users with
everyday tasks, such as by making recommendations or performing basic
computation. Despite AI assistants' promise, little is known about the implicit
values these assistants display while completing subjective everyday tasks.
Humans may consider values like environmentalism, charity, and diversity. To
what extent do LLMs exhibit these values in completing everyday tasks? How do
they compare with humans? We answer these questions by auditing how six popular
LLMs complete 30 everyday tasks, comparing LLMs to each other and to 100 human
crowdworkers from the US. We find LLMs often do not align with humans, nor with
other LLMs, in the implicit values exhibited.

</details>


### [95] [Morpheme Induction for Emergent Language](https://arxiv.org/abs/2510.03439)
*Brendon Boldt,David Mortensen*

Main category: cs.CL

TL;DR: CSAR is a greedy algorithm designed to identify morphemes from linked utterances and meanings by iteratively selecting highly informative pairs and removing them from the dataset.


<details>
  <summary>Details</summary>
Motivation: The study aims to explore how morphemes can be automatically extracted from emergent languages and human language data while assessing linguistic properties such as synonymy and polysemy.

Method: The CSAR algorithm uses mutual information to rank and select morphemes from a corpus of parallel utterances and meanings, iterating through a Count, Select, Ablate, and Repeat process.

Result: CSAR is validated on both synthetic datasets and human language, outperforming baseline methods in related tasks and making plausible predictions in linguistic analysis.

Conclusion: The algorithm is effective for inducing morphemes and analyzing linguistic properties, supporting language understanding in both emergent and established linguistic contexts.

Abstract: We introduce CSAR, an algorithm for inducing morphemes from emergent language
corpora of parallel utterances and meanings. It is a greedy algorithm that (1)
weights morphemes based on mutual information between forms and meanings, (2)
selects the highest-weighted pair, (3) removes it from the corpus, and (4)
repeats the process to induce further morphemes (i.e., Count, Select, Ablate,
Repeat). The effectiveness of CSAR is first validated on procedurally generated
datasets and compared against baselines for related tasks. Second, we validate
CSAR's performance on human language data to show that the algorithm makes
reasonable predictions in adjacent domains. Finally, we analyze a handful of
emergent languages, quantifying linguistic characteristics like degree of
synonymy and polysemy.

</details>


### [96] [Omni-Embed-Nemotron: A Unified Multimodal Retrieval Model for Text, Image, Audio, and Video](https://arxiv.org/abs/2510.03458)
*Mengyao Xu,Wenfei Zhou,Yauhen Babakhin,Gabriel Moreira,Ronay Ak,Radek Osmulski,Bo Liu,Even Oldridge,Benedikt Schifferer*

Main category: cs.CL

TL;DR: This paper introduces Omni-Embed-Nemotron, a multimodal retrieval embedding model supporting diverse content types, including audio and video, enabling advanced cross-modal and joint-modal retrieval capabilities.


<details>
  <summary>Details</summary>
Motivation: The growing complexity of real-world information demands retrieval systems capable of processing semantically rich and diverse content modalities, overcoming the limitations of text-based retrievers in handling visually enriched documents.

Method: Omni-Embed-Nemotron builds upon multimodal embedding models and incorporates innovations to support cross-modal and joint-modal content retrieval. It utilizes architectural designs inspired by both recent multimodal models and layout-preserving retrieval approaches.

Result: The evaluation results showcase the effectiveness of Omni-Embed-Nemotron in retrieving information across text, image, and video modalities, surpassing conventional text-based retrievers.

Conclusion: Omni-Embed-Nemotron broadens the horizons of multimodal retrieval by seamlessly supporting diverse content types and excelling in cross-modal and joint-modal scenarios.

Abstract: We present Omni-Embed-Nemotron, a unified multimodal retrieval embedding
model developed to handle the increasing complexity of real-world information
needs. While Retrieval-Augmented Generation (RAG) has significantly advanced
language models by incorporating external knowledge, existing text-based
retrievers rely on clean, structured input and struggle with the visually and
semantically rich content found in real-world documents such as PDFs, slides,
or videos. Recent work such as ColPali has shown that preserving document
layout using image-based representations can improve retrieval quality.
Building on this, and inspired by the capabilities of recent multimodal models
such as Qwen2.5-Omni, we extend retrieval beyond text and images to also
support audio and video modalities. Omni-Embed-Nemotron enables both
cross-modal (e.g., text - video) and joint-modal (e.g., text - video+audio)
retrieval using a single model. We describe the architecture, training setup,
and evaluation results of Omni-Embed-Nemotron, and demonstrate its
effectiveness in text, image, and video retrieval.

</details>


### [97] [Scaling Code-Assisted Chain-of-Thoughts and Instructions for Model Reasoning](https://arxiv.org/abs/2510.04081)
*Honglin Lin,Qizhi Pei,Xin Gao,Zhuoshi Pan,Yu Li,Juntao Li,Conghui He,Lijun Wu*

Main category: cs.CL

TL;DR: Caco leverages code-driven augmentation to generate reliable, scalable, and diverse reasoning data for Large Language Models, overcoming limitations of existing CoT approaches.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with reliable and scalable reasoning in complex tasks, while existing Chain-of-Thought (CoT) methods have shortcomings in quality, diversity, and execution.

Method: Caco employs code-based CoT generation, automated validation, and rule-based filtering, followed by natural language conversion to create large-scale reasoning datasets with ensured executability.

Result: Caco-trained models excel in mathematical reasoning benchmarks, outperforming strong baselines and showing superior generalization across unseen tasks.

Conclusion: The paper introduces Caco, a framework enabling automated, scalable, and verifiable generation of reasoning data, setting a foundation for trustworthy reasoning systems without human intervention.

Abstract: Reasoning capability is pivotal for Large Language Models (LLMs) to solve
complex tasks, yet achieving reliable and scalable reasoning remains
challenging. While Chain-of-Thought (CoT) prompting has become a mainstream
approach, existing methods often suffer from uncontrolled generation,
insufficient quality, and limited diversity in reasoning paths. Recent efforts
leverage code to enhance CoT by grounding reasoning in executable steps, but
such methods are typically constrained to predefined mathematical problems,
hindering scalability and generalizability. In this work, we propose Caco
(Code-Assisted Chain-of-ThOught), a novel framework that automates the
synthesis of high-quality, verifiable, and diverse instruction-CoT reasoning
data through code-driven augmentation. Unlike prior work, Caco first fine-tunes
a code-based CoT generator on existing math and programming solutions in a
unified code format, then scales the data generation to a large amount of
diverse reasoning traces. Crucially, we introduce automated validation via code
execution and rule-based filtering to ensure logical correctness and structural
diversity, followed by reverse-engineering filtered outputs into natural
language instructions and language CoTs to enrich task adaptability. This
closed-loop process enables fully automated, scalable synthesis of reasoning
data with guaranteed executability. Experiments on our created Caco-1.3M
dataset demonstrate that Caco-trained models achieve strong competitive
performance on mathematical reasoning benchmarks, outperforming existing strong
baselines. Further analysis reveals that Caco's code-anchored verification and
instruction diversity contribute to superior generalization across unseen
tasks. Our work establishes a paradigm for building self-sustaining,
trustworthy reasoning systems without human intervention.

</details>


### [98] [Searching for the Most Human-like Emergent Language](https://arxiv.org/abs/2510.03467)
*Brendon Boldt,David Mortensen*

Main category: cs.CL

TL;DR: This research develops a hyperparameter-optimized signalling game to create emergent languages closely resembling human languages, measured through deep transfer learning metrics.


<details>
  <summary>Details</summary>
Motivation: The study addresses the need for emergent communication systems to better mimic human language, enhancing their applicability in tasks like transfer learning.

Method: A hyperparameter optimization approach integrates XferBench as the evaluation metric, assessing emergent language similarity to human language based on its transfer learning performance. Entropy is also studied as a predictive factor.

Result: The findings include insights about entropy’s role in transfer learning and empirical data on hyperparameters yielding emergent languages closer to human language.

Conclusion: The paper concludes with evidence supporting entropy-minimization trends in emergent languages and offers guidance on hyperparameter settings for improved similarity to human languages.

Abstract: In this paper, we design a signalling game-based emergent communication
environment to generate state-of-the-art emergent languages in terms of
similarity to human language. This is done with hyperparameter optimization,
using XferBench as the objective function. XferBench quantifies the statistical
similarity of emergent language to human language by measuring its suitability
for deep transfer learning to human language. Additionally, we demonstrate the
predictive power of entropy on the transfer learning performance of emergent
language as well as corroborate previous results on the entropy-minimization
properties of emergent communication systems. Finally, we report
generalizations regarding what hyperparameters produce more realistic emergent
languages, that is, ones which transfer better to human language.

</details>


### [99] [SEER: The Span-based Emotion Evidence Retrieval Benchmark](https://arxiv.org/abs/2510.03490)
*Aneesha Sampath,Oya Aran,Emily Mower Provost*

Main category: cs.CL

TL;DR: The paper introduces SEER, a benchmark for testing LLMs' ability to identify specific spans of text expressing emotions, with evaluations showing limitations in handling longer passages.


<details>
  <summary>Details</summary>
Motivation: Traditional emotion recognition tasks assign a single label to entire sentences, lacking granularity and failing to target how emotions are expressed. To improve applications requiring precise identification, such as empathetic dialogue and clinical support, a span-level approach is necessary.

Method: The paper proposes SEER, which includes two tasks: detecting emotion evidence in single sentences and across short passages. New annotations on emotional content were created for 1200 sentences. They evaluated the capability of 14 open-source LLMs against average human performance.

Result: Some models approached human performance for single sentences, but their results degraded for longer passages. Error analysis highlighted issues such as overdependence on emotion keywords and false positives in neutral text.

Conclusion: SEER highlights the challenge LLMs face in detecting specific emotional spans, suggesting avenues for improvement in handling nuanced emotional contexts and longer text passages.

Abstract: We introduce the SEER (Span-based Emotion Evidence Retrieval) Benchmark to
test Large Language Models' (LLMs) ability to identify the specific spans of
text that express emotion. Unlike traditional emotion recognition tasks that
assign a single label to an entire sentence, SEER targets the underexplored
task of emotion evidence detection: pinpointing which exact phrases convey
emotion. This span-level approach is crucial for applications like empathetic
dialogue and clinical support, which need to know how emotion is expressed, not
just what the emotion is. SEER includes two tasks: identifying emotion evidence
within a single sentence, and identifying evidence across a short passage of
five consecutive sentences. It contains new annotations for both emotion and
emotion evidence on 1200 real-world sentences. We evaluate 14 open-source LLMs
and find that, while some models approach average human performance on
single-sentence inputs, their accuracy degrades in longer passages. Our error
analysis reveals key failure modes, including overreliance on emotion keywords
and false positives in neutral text.

</details>


### [100] [ALHD: A Large-Scale and Multigenre Benchmark Dataset for Arabic LLM-Generated Text Detection](https://arxiv.org/abs/2510.03502)
*Ali Khairallah,Arkaitz Zubiaga*

Main category: cs.CL

TL;DR: ALHD is an Arabic dataset developed for distinguishing texts created by humans and large language models (LLMs), spanning multiple genres and linguistic styles.


<details>
  <summary>Details</summary>
Motivation: The need for robust detection methods to address misinformation, academic dishonesty, and threats in Arabic NLP motivated the creation of ALHD.

Method: ALHD includes over 400K balanced samples, thorough preprocessing, and annotations. Benchmark experiments include traditional classifiers, BERT-based models, and LLMs for evaluation.

Result: Fine-tuned BERT models show competitive performance, outperforming some LLM models, but generalization issues arise in cross-genre evaluations.

Conclusion: ALHD serves as a resource to advance Arabic LLM detection, highlighting challenges in genre generalization and suggesting future research directions.

Abstract: We introduce ALHD, the first large-scale comprehensive Arabic dataset
explicitly designed to distinguish between human- and LLM-generated texts. ALHD
spans three genres (news, social media, reviews), covering both MSA and
dialectal Arabic, and contains over 400K balanced samples generated by three
leading LLMs and originated from multiple human sources, which enables studying
generalizability in Arabic LLM-genearted text detection. We provide rigorous
preprocessing, rich annotations, and standardized balanced splits to support
reproducibility. In addition, we present, analyze and discuss benchmark
experiments using our new dataset, in turn identifying gaps and proposing
future research directions. Benchmarking across traditional classifiers,
BERT-based models, and LLMs (zero-shot and few-shot) demonstrates that
fine-tuned BERT models achieve competitive performance, outperforming LLM-based
models. Results are however not always consistent, as we observe challenges
when generalizing across genres; indeed, models struggle to generalize when
they need to deal with unseen patterns in cross-genre settings, and these
challenges are particularly prominent when dealing with news articles, where
LLM-generated texts resemble human texts in style, which opens up avenues for
future research. ALHD establishes a foundation for research related to Arabic
LLM-detection and mitigating risks of misinformation, academic dishonesty, and
cyber threats.

</details>


### [101] [TS-Reasoner: Aligning Time Series Foundation Models with LLM Reasoning](https://arxiv.org/abs/2510.03519)
*Fangxu Yu,Hongyu Zhao,Tianyi Zhou*

Main category: cs.CL

TL;DR: TS-Reasoner integrates Time Series Foundation Models (TSFMs) and Large Language Models (LLMs) to enhance time series reasoning tasks, achieving data efficiency and outperforming existing models.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the gap in existing models which either lack reasoning capabilities for time series data or struggle with numerical understanding without expensive post-training.

Method: The method involves curating synthetic time series-text pairs for alignment training and employing a two-stage training recipe: alignment pretraining and instruction fine-tuning.

Result: TS-Reasoner surpasses LLMs, Vision Language Models, and Time Series LLMs in benchmarks with remarkable data efficiency, using less than half the training data.

Conclusion: TS-Reasoner effectively bridges the gap between TSFMs and LLMs, demonstrating superior performance and resource efficiency for time series reasoning tasks.

Abstract: Time series reasoning is crucial to decision-making in diverse domains,
including finance, energy usage, traffic, weather, and scientific discovery.
While existing time series foundation models (TSFMs) can capture low-level
dynamic patterns and provide accurate forecasting, further analysis usually
requires additional background knowledge and sophisticated reasoning, which are
lacking in most TSFMs but can be achieved through large language models (LLMs).
On the other hand, without expensive post-training, LLMs often struggle with
the numerical understanding of time series data. Although it is intuitive to
integrate the two types of models, developing effective training recipes that
align the two modalities for reasoning tasks is still an open challenge. To
this end, we propose TS-Reasoner that aligns the latent representations of
TSFMs with the textual inputs of LLMs for downstream understanding/reasoning
tasks. Specifically, we propose a simple yet effective method to curate
diverse, synthetic pairs of time series and textual captions for alignment
training. We then develop a two-stage training recipe that applies instruction
finetuning after the alignment pretraining. Unlike existing works that train an
LLM to take time series as inputs, we leverage a pretrained TSFM and freeze it
during training. Extensive experiments on several benchmarks demonstrate that
TS-Reasoner not only outperforms a wide range of prevailing LLMs, Vision
Language Models (VLMs), and Time Series LLMs, but also achieves this with
remarkable data efficiency, e.g., using less than half the training data.

</details>


### [102] [Identifying Financial Risk Information Using RAG with a Contrastive Insight](https://arxiv.org/abs/2510.03521)
*Ali Elahi*

Main category: cs.CL

TL;DR: The paper proposes a new peer-aware comparative inference layer to enhance reasoning capabilities on specialized contexts for large language models (LLMs) within RAG frameworks.


<details>
  <summary>Details</summary>
Motivation: Existing retrieval-augmented generation (RAG) pipelines are limited by their inability to consider comparable cases or related problems, leading to generic outputs in specialized reasoning tasks.

Method: A contrastive approach is introduced, which acts as a peer-aware comparative inference layer to improve the specificity and relevance of generated outputs.

Result: The enhanced model demonstrated better performance in generating equity research and identifying risk, as measured by text generation metrics like ROUGE and BERTScore.

Conclusion: Integrating comparative reasoning into RAG pipelines enables more context-specific insights, improving results in specialized domains like finance.

Abstract: In specialized domains, humans often compare new problems against similar
examples, highlight nuances, and draw conclusions instead of analyzing
information in isolation. When applying reasoning in specialized contexts with
LLMs on top of a RAG, the pipeline can capture contextually relevant
information, but it is not designed to retrieve comparable cases or related
problems.
  While RAG is effective at extracting factual information, its outputs in
specialized reasoning tasks often remain generic, reflecting broad facts rather
than context-specific insights. In finance, it results in generic risks that
are true for the majority of companies. To address this limitation, we propose
a peer-aware comparative inference layer on top of RAG.
  Our contrastive approach outperforms baseline RAG in text generation metrics
such as ROUGE and BERTScore in comparison with human-generated equity research
and risk.

</details>


### [103] [The Geometry of Truth: Layer-wise Semantic Dynamics for Hallucination Detection in Large Language Models](https://arxiv.org/abs/2510.04933)
*Amir Hameed Mir*

Main category: cs.CL

TL;DR: This paper introduces a method called Layer-wise Semantic Dynamics (LSD) to detect hallucinations in large language models (LLMs) by analyzing hidden-state semantics across model layers, achieving high accuracy and efficiency compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: To address the issue of hallucinations in LLMs that produce fluent but factually incorrect statements, especially in critical applications where accuracy is paramount.

Method: The proposed approach, LSD, uses a geometric framework to track the semantic dynamics of hidden states through transformer layers. It employs margin-based contrastive learning to align hidden activations with ground-truth embeddings from a factual encoder, differentiating between factual and hallucinatory outputs based on semantic stability.

Result: LSD achieves superior performance with an F1-score of 0.92, AUROC of 0.96, and clustering accuracy of 0.89 on the TruthfulQA and synthetic datasets. It outperforms baseline methods like SelfCheckGPT and Semantic Entropy while being much faster, as it requires only a single forward pass.

Conclusion: LSD provides an efficient and scalable method for hallucination detection in LLMs, offering real-time monitoring capabilities and deeper insights into the geometry of semantic consistency within these models.

Abstract: Large Language Models (LLMs) often produce fluent yet factually incorrect
statements-a phenomenon known as hallucination-posing serious risks in
high-stakes domains. We present Layer-wise Semantic Dynamics (LSD), a geometric
framework for hallucination detection that analyzes the evolution of
hidden-state semantics across transformer layers. Unlike prior methods that
rely on multiple sampling passes or external verification sources, LSD operates
intrinsically within the model's representational space. Using margin-based
contrastive learning, LSD aligns hidden activations with ground-truth
embeddings derived from a factual encoder, revealing a distinct separation in
semantic trajectories: factual responses preserve stable alignment, while
hallucinations exhibit pronounced semantic drift across depth. Evaluated on the
TruthfulQA and synthetic factual-hallucination datasets, LSD achieves an
F1-score of 0.92, AUROC of 0.96, and clustering accuracy of 0.89, outperforming
SelfCheckGPT and Semantic Entropy baselines while requiring only a single
forward pass. This efficiency yields a 5-20x speedup over sampling-based
methods without sacrificing precision or interpretability. LSD offers a
scalable, model-agnostic mechanism for real-time hallucination monitoring and
provides new insights into the geometry of factual consistency within large
language models.

</details>


### [104] [Sample, Align, Synthesize: Graph-Based Response Synthesis with ConGrs](https://arxiv.org/abs/2510.03527)
*Sayan Ghosh,Shahzaib Saqib Warraich,Dhruv Tarsadiya,Gregory Yauney,Swabha Swayamdipta*

Main category: cs.CL

TL;DR: The paper presents Consensus Graphs (ConGrs), a method to synthesize and improve long-form responses from language models (LMs) by capturing shared and semantic variations across sampled LM outputs.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle to effectively synthesize epistemic signals and semantic variations from multiple long-form samples generated by language models, necessitating a new approach to improve precision and reasoning capabilities.

Method: The authors introduce Consensus Graphs (ConGrs), a DAG-based structure built using a lexical sequence alignment algorithm from bioinformatics, paired with a secondary LM judge for specific tasks. Task-dependent decoding methods are also designed for synthesizing final responses.

Result: The ConGrs approach improved factual precision by up to 31% in biography tasks, reduced reliance on LM judges by over 80%, increased abstention rates in refusal tasks by up to 56%, and enhanced accuracy by up to 6 points in the MATH and AIME reasoning challenges.

Conclusion: ConGrs is a versatile and effective method for leveraging semantic variations in sampled LM responses, allowing for improved precision, reduced judge dependency, and better reasoning capability in diverse tasks.

Abstract: Language models can be sampled multiple times to access the distribution
underlying their responses, but existing methods cannot efficiently synthesize
rich epistemic signals across different long-form responses. We introduce
Consensus Graphs (ConGrs), a flexible DAG-based data structure that represents
shared information, as well as semantic variation in a set of sampled LM
responses to the same prompt. We construct ConGrs using a light-weight lexical
sequence alignment algorithm from bioinformatics, supplemented by the targeted
usage of a secondary LM judge. Further, we design task-dependent decoding
methods to synthesize a single, final response from our ConGr data structure.
Our experiments show that synthesizing responses from ConGrs improves factual
precision on two biography generation tasks by up to 31% over an average
response and reduces reliance on LM judges by more than 80% compared to other
methods. We also use ConGrs for three refusal-based tasks requiring abstention
on unanswerable queries and find that abstention rate is increased by up to
56%. We apply our approach to the MATH and AIME reasoning tasks and find an
improvement over self-verification and majority vote baselines by up to 6
points of accuracy. We show that ConGrs provide a flexible method for capturing
variation in LM responses and using the epistemic signals provided by response
variation to synthesize more effective responses.

</details>


### [105] [Mind Your Tone: Investigating How Prompt Politeness Affects LLM Accuracy (short paper)](https://arxiv.org/abs/2510.04950)
*Om Dobariya,Akhil Kumar*

Main category: cs.CL

TL;DR: This paper investigates the impact of varying levels of politeness in prompts on the accuracy of large language models and finds that impolite prompts outperform polite ones.


<details>
  <summary>Details</summary>
Motivation: To explore the under-researched role of prompt politeness and tone on the performance of large language models, particularly for multiple-choice questions.

Method: The authors created a dataset of 50 questions rewritten into five tone variants (Very Polite, Polite, Neutral, Rude, and Very Rude), generating 250 unique prompts. They used ChatGPT 4 and paired sample t-tests to evaluate model accuracy statistically.

Result: Contrary to expectations, impolite prompts performed better than polite ones, with accuracy ranging from 80.8% for Very Polite prompts to 84.8% for Very Rude prompts.

Conclusion: Newer language models may not conform to the expectation that politeness improves outcomes. These findings underline the role of tonal variation in prompting and suggest broader implications for human-AI interaction.

Abstract: The wording of natural language prompts has been shown to influence the
performance of large language models (LLMs), yet the role of politeness and
tone remains underexplored. In this study, we investigate how varying levels of
prompt politeness affect model accuracy on multiple-choice questions. We
created a dataset of 50 base questions spanning mathematics, science, and
history, each rewritten into five tone variants: Very Polite, Polite, Neutral,
Rude, and Very Rude, yielding 250 unique prompts. Using ChatGPT 4o, we
evaluated responses across these conditions and applied paired sample t-tests
to assess statistical significance. Contrary to expectations, impolite prompts
consistently outperformed polite ones, with accuracy ranging from 80.8% for
Very Polite prompts to 84.8% for Very Rude prompts. These findings differ from
earlier studies that associated rudeness with poorer outcomes, suggesting that
newer LLMs may respond differently to tonal variation. Our results highlight
the importance of studying pragmatic aspects of prompting and raise broader
questions about the social dimensions of human-AI interaction.

</details>


### [106] [Fine-Tuning on Noisy Instructions: Effects on Generalization and Performance](https://arxiv.org/abs/2510.03528)
*Ahmed Alajrami,Xingwei Tan,Nikolaos Aletras*

Main category: cs.CL

TL;DR: This paper investigates if introducing perturbations in instruction-tuning data can improve large language models (LLMs) by making them more resilient to noisy instructions.


<details>
  <summary>Details</summary>
Motivation: LLMs are sensitive to variations in instruction phrasing, potentially reducing their effectiveness in real-world scenarios where instructions might be noisy or unpredictable.

Method: The study incorporates perturbations, such as removing stop words or shuffling words, in instruction-tuning and evaluates LLM performance across benchmarks like MMLU, BBH, and GSM8K.

Result: Instruction-tuning with perturbed instructions can enhance LLM performance and make them more resilient to noisy instructions in certain cases.

Conclusion: Perturbations in instruction-tuning are beneficial and should be considered as they improve LLM robustness to noisy user inputs.

Abstract: Instruction-tuning plays a vital role in enhancing the task-solving abilities
of large language models (LLMs), improving their usability in generating
helpful responses on various tasks. However, previous work has demonstrated
that they are sensitive to minor variations in instruction phrasing. In this
paper, we explore whether introducing perturbations in instruction-tuning data
can enhance LLMs' resistance against noisy instructions. We focus on how
instruction-tuning with perturbations, such as removing stop words or shuffling
words, affects LLMs' performance on the original and perturbed versions of
widely-used benchmarks (MMLU, BBH, GSM8K). We further assess learning dynamics
and potential shifts in model behavior. Surprisingly, our results suggest that
instruction-tuning on perturbed instructions can, in some cases, improve
downstream performance. These findings highlight the importance of including
perturbed instructions in instruction-tuning, which can make LLMs more
resilient to noisy user inputs.

</details>


### [107] [TriMediQ: A Triplet-Structured Approach for Interactive Medical Question Answering](https://arxiv.org/abs/2510.03536)
*Zhaohan Meng,Zaiqiao Meng,Siwei Liu,Iadh Ounis*

Main category: cs.CL

TL;DR: The paper introduces TriMediQ, a triplet-structured method that uses knowledge graphs to improve large language models' (LLMs) clinical reasoning in multi-turn consultations. It achieves up to 10.4% higher accuracy compared to existing approaches.


<details>
  <summary>Details</summary>
Motivation: Large Language Models perform well in static medical QA benchmarks but falter in dynamic, multi-turn clinical consultations due to unclear links between clinical facts. A framework is necessary to support accurate multi-hop reasoning in such scenarios.

Method: TriMediQ involves summarizing patient responses into clinically relevant triplets and integrating them into a Knowledge Graph. It uses a frozen triplet generator for consistency, a trainable projection module for relational understanding, and multi-hop reasoning which enhances LLM functionality.

Result: TriMediQ outperformed five baseline methods on the iMedQA dataset by achieving up to 10.4% improvement in accuracy across multi-turn medical QA benchmarks.

Conclusion: This approach demonstrates that structuring patient data into triplet-based knowledge graphs facilitates accurate reasoning in multi-turn clinical settings and aids in deploying reliable LLM-based medical assistants.

Abstract: Large Language Models (LLMs) perform strongly in static and single-turn
medical Question Answer (QA) benchmarks, yet such settings diverge from the
iterative information gathering process required in practical clinical
consultations. The MEDIQ framework addresses this mismatch by recasting the
diagnosis as an interactive dialogue between a patient and an expert system,
but the reliability of LLMs drops dramatically when forced to reason with
dialogue logs, where clinical facts appear in sentences without clear links. To
bridge this gap, we introduce TriMediQ, a triplet-structured approach that
summarises patient responses into triplets and integrates them into a Knowledge
Graph (KG), enabling multi-hop reasoning. We introduce a frozen triplet
generator that extracts clinically relevant triplets, using prompts designed to
ensure factual consistency. In parallel, a trainable projection module,
comprising a graph encoder and a projector, captures relational information
from the KG to enhance expert reasoning. TriMediQ operates in two steps: (i)
the projection module fine-tuning with all LLM weights frozen; and (ii) using
the fine-tuned module to guide multi-hop reasoning during inference. We
evaluate TriMediQ on two interactive QA benchmarks, showing that it achieves up
to 10.4\% improvement in accuracy over five baselines on the iMedQA dataset.
These results demonstrate that converting patient responses into structured
triplet-based graphs enables more accurate clinical reasoning in multi-turn
settings, providing a solution for the deployment of LLM-based medical
assistants.

</details>


### [108] [What is a protest anyway? Codebook conceptualization is still a first-order concern in LLM-era classification](https://arxiv.org/abs/2510.03541)
*Andrew Halterman,Katherine A. Keith*

Main category: cs.CL

TL;DR: Generative large language models (LLMs) are widely used in computational social science, but mistakes in concept definition prior to use can create biases that affect statistical inference results.


<details>
  <summary>Details</summary>
Motivation: Highlight overlooked steps before and after LLM utilization, particularly concept definition and its role in reducing biases in downstream statistical inference.

Method: Use simulations to demonstrate how conceptualization errors lead to biases and analyze why these errors cannot be mitigated by improving LLM accuracy or applying post-hoc corrections.

Result: Conceptualization-induced bias persists despite higher LLM accuracy and bias correction methods, emphasizing the importance of proper conceptual groundwork.

Conclusion: Conceptualization is crucial in ensuring unbiased outcomes in LLM-based research, and the paper offers concrete advice for improving downstream estimates in computational social science.

Abstract: Generative large language models (LLMs) are now used extensively for text
classification in computational social science (CSS). In this work, focus on
the steps before and after LLM prompting -- conceptualization of concepts to be
classified and using LLM predictions in downstream statistical inference --
which we argue have been overlooked in much of LLM-era CSS. We claim LLMs can
tempt analysts to skip the conceptualization step, creating conceptualization
errors that bias downstream estimates. Using simulations, we show that this
conceptualization-induced bias cannot be corrected for solely by increasing LLM
accuracy or post-hoc bias correction methods. We conclude by reminding CSS
analysts that conceptualization is still a first-order concern in the LLM-era
and provide concrete advice on how to pursue low-cost, unbiased, low-variance
downstream estimates.

</details>


### [109] [CCD-Bench: Probing Cultural Conflict in Large Language Model Decision-Making](https://arxiv.org/abs/2510.03553)
*Hasibur Rahman,Hanan Salam*

Main category: cs.CL

TL;DR: This paper introduces CCD-Bench, a benchmark designed to evaluate large language models' (LLMs) decision-making in situations of cross-cultural value conflict. It finds LLMs disproportionately prefer values from certain cultural clusters, prompting the need for better alignment strategies.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the lack of benchmarks that assess LLMs' ability to navigate explicit conflicts between differing cultural value systems in decision-making scenarios.

Method: The authors develop CCD-Bench, a benchmark of 2,182 open-ended dilemmas across seven domains, paired with ten anonymized response options based on GLOBE cultural clusters. The benchmark evaluates 17 LLMs for biases and decision-making patterns.

Result: Key results show a disproportionate preference for Nordic Europe and Germanic Europe value systems, while others like Eastern Europe and Middle Eastern values are underrepresented. Rationales are pluralistic but often shallow, focusing on limited dimensions like Future and Performance Orientation.

Conclusion: Current LLM alignment pipelines promote a consensus-oriented worldview that inadequately engages with diverse cultural values. CCD-Bench highlights the necessity of developing strategies that reflect a broader range of cultural perspectives.

Abstract: Although large language models (LLMs) are increasingly implicated in
interpersonal and societal decision-making, their ability to navigate explicit
conflicts between legitimately different cultural value systems remains largely
unexamined. Existing benchmarks predominantly target cultural knowledge
(CulturalBench), value prediction (WorldValuesBench), or single-axis bias
diagnostics (CDEval); none evaluate how LLMs adjudicate when multiple
culturally grounded values directly clash. We address this gap with CCD-Bench,
a benchmark that assesses LLM decision-making under cross-cultural value
conflict. CCD-Bench comprises 2,182 open-ended dilemmas spanning seven domains,
each paired with ten anonymized response options corresponding to the ten GLOBE
cultural clusters. These dilemmas are presented using a stratified Latin square
to mitigate ordering effects. We evaluate 17 non-reasoning LLMs. Models
disproportionately prefer Nordic Europe (mean 20.2 percent) and Germanic Europe
(12.4 percent), while options for Eastern Europe and the Middle East and North
Africa are underrepresented (5.6 to 5.8 percent). Although 87.9 percent of
rationales reference multiple GLOBE dimensions, this pluralism is superficial:
models recombine Future Orientation and Performance Orientation, and rarely
ground choices in Assertiveness or Gender Egalitarianism (both under 3
percent). Ordering effects are negligible (Cramer's V less than 0.10), and
symmetrized KL divergence shows clustering by developer lineage rather than
geography. These patterns suggest that current alignment pipelines promote a
consensus-oriented worldview that underserves scenarios demanding power
negotiation, rights-based reasoning, or gender-aware analysis. CCD-Bench shifts
evaluation beyond isolated bias detection toward pluralistic decision making
and highlights the need for alignment strategies that substantively engage
diverse worldviews.

</details>


### [110] [Reactive Transformer (RxT) -- Stateful Real-Time Processing for Event-Driven Reactive Language Models](https://arxiv.org/abs/2510.03561)
*Adam Filipek*

Main category: cs.CL

TL;DR: The paper proposes the Reactive Transformer (RxT), an event-driven architecture aimed at enabling efficient, real-time, stateful conversational AI. The design achieves linear scaling, reducing computational complexity and ensuring low latency.


<details>
  <summary>Details</summary>
Motivation: The paper addresses fundamental constraints in Transformer architectures for conversational AI, particularly their stateless nature and quadratic computational complexity in handling long dialogues.

Method: The Reactive Transformer (RxT) shifts conversational AI to an event-driven paradigm, featuring a Short-Term Memory (STM) system, decoupled response generation, memory updates, and linear scaling dynamics.

Result: RxT demonstrated superior performance and constant-time inference latency in synthetic data experiments, compared to a stateless model of similar size.

Conclusion: RxT effectively reduces latency and computational costs, enabling real-time, stateful, and scalable conversational AI systems suitable for long-form dialogues.

Abstract: The Transformer architecture has become the de facto standard for Large
Language Models (LLMs), demonstrating remarkable capabilities in language
understanding and generation. However, its application in conversational AI is
fundamentally constrained by its stateless nature and the quadratic
computational complexity ($O(L^2)$) with respect to sequence length $L$.
Current models emulate memory by reprocessing an ever-expanding conversation
history with each turn, leading to prohibitive costs and latency in long
dialogues. This paper introduces the Reactive Transformer (RxT), a novel
architecture designed to overcome these limitations by shifting from a
data-driven to an event-driven paradigm. RxT processes each conversational turn
as a discrete event in real-time, maintaining context in an integrated,
fixed-size Short-Term Memory (STM) system. The architecture features a distinct
operational cycle where a generator-decoder produces a response based on the
current query and the previous memory state, after which a memory-encoder and a
dedicated Memory Attention network asynchronously update the STM with a
representation of the complete interaction. This design fundamentally alters
the scaling dynamics, reducing the total user-facing cost of a conversation
from quadratic ($O(N^2 \cdot T)$) to linear ($O(N \cdot T)$) with respect to
the number of interactions $N$. By decoupling response generation from memory
updates, RxT achieves low latency, enabling truly real-time, stateful, and
economically viable long-form conversations. We validated our architecture with
a series of proof-of-concept experiments on synthetic data, demonstrating
superior performance and constant-time inference latency compared to a baseline
stateless model of comparable size.

</details>


### [111] [LLM, Reporting In! Medical Information Extraction Across Prompting, Fine-tuning and Post-correction](https://arxiv.org/abs/2510.03577)
*Ikram Belmadani,Parisa Nazari Hashemi,Thomas Sebbag,Benoit Favre,Guillaume Fortier,Solen Quiniou,Emmanuel Morin,Richard Dufour*

Main category: cs.CL

TL;DR: This paper explores three approaches for biomedical Named Entity Recognition (NER) and health event extraction in French using large language models (LLMs), synthetic data, and annotation guidelines in a few-shot setting during the EvalLLM 2025 challenge.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve NER and event extraction performance in a low-resource setting, particularly for biomedical applications in French, using advanced capabilities of large language models.

Method: Three methods were proposed: (1) In-context learning with GPT-4.1 using auto-selected examples and annotation guideline summaries, (2) Fine-tuning GLiNER on synthetic data with post-processing by an LLM, and (3) Fine-tuning an open LLM named LLaMA-3.1-8B-Instruct on the same synthetic data. Event extraction employed GPT-4.1 with guideline summaries.

Result: GPT-4.1 performed the best among the methods, achieving a macro-F1 score of 61.53% for NER and 15.02% for event extraction.

Conclusion: Well-crafted prompts and leveraging synthetic data significantly enhance LLM performance for biomedical NER and event extraction in very low-resource settings.

Abstract: This work presents our participation in the EvalLLM 2025 challenge on
biomedical Named Entity Recognition (NER) and health event extraction in French
(few-shot setting). For NER, we propose three approaches combining large
language models (LLMs), annotation guidelines, synthetic data, and
post-processing: (1) in-context learning (ICL) with GPT-4.1, incorporating
automatic selection of 10 examples and a summary of the annotation guidelines
into the prompt, (2) the universal NER system GLiNER, fine-tuned on a synthetic
corpus and then verified by an LLM in post-processing, and (3) the open LLM
LLaMA-3.1-8B-Instruct, fine-tuned on the same synthetic corpus. Event
extraction uses the same ICL strategy with GPT-4.1, reusing the guideline
summary in the prompt. Results show GPT-4.1 leads with a macro-F1 of 61.53% for
NER and 15.02% for event extraction, highlighting the importance of
well-crafted prompting to maximize performance in very low-resource scenarios.

</details>


### [112] [Decoupling Task-Solving and Output Formatting in LLM Generation](https://arxiv.org/abs/2510.03595)
*Haikang Deng,Po-Nien Kung,Nanyun Peng*

Main category: cs.CL

TL;DR: The paper introduces Deco-G, a decoding framework for large language models (LLMs) that separates reasoning tasks from formatting requirements to improve performance and format compliance.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge that large language models struggle to simultaneously follow complex reasoning instructions and rigid formatting requirements, which creates competing goals.

Method: Deco-G decouples format adherence and task solving by using separate models: a tractable probabilistic model (TPM) for format compliance and an LLM for reasoning. Techniques like instruction-aware distillation, trie-building algorithms, and HMM state pruning are used to scale the framework.

Result: Deco-G demonstrates 1.0% to 6.0% relative gains over regular prompting on diverse tasks, ensuring both high-quality task completion and guaranteed format adherence.

Conclusion: Explicit separation of reasoning and formatting tasks enhances the performance and usability of LLMs in complex tasks, and Deco-G offers a practical and scalable solution for this issue.

Abstract: Large language models (LLMs) are increasingly adept at following instructions
containing task descriptions to solve complex problems, such as mathematical
reasoning and automatic evaluation (LLM-as-a-Judge). However, as prompts grow
more complex, models often struggle to adhere to all instructions. This
difficulty is especially common when instructive prompts intertwine reasoning
directives -- specifying what the model should solve -- with rigid formatting
requirements that dictate how the solution must be presented. The entanglement
creates competing goals for the model, suggesting that more explicit separation
of these two aspects could lead to improved performance. To this front, we
introduce Deco-G, a decoding framework that explicitly decouples format
adherence from task solving. Deco-G handles format compliance with a separate
tractable probabilistic model (TPM), while prompts LLMs with only task
instructions. At each decoding step, Deco-G combines next token probabilities
from the LLM with the TPM calculated format compliance likelihood to form the
output probability. To make this approach both practical and scalable for
modern instruction-tuned LLMs, we introduce three key innovations:
instruction-aware distillation, a flexible trie-building algorithm, and HMM
state pruning for computational efficiency. We demonstrate the effectiveness of
Deco-G across a wide range of tasks with diverse format requirements, including
mathematical reasoning, LLM-as-a-judge, and event argument extraction. Overall,
our approach yields 1.0% to 6.0% relative gain over regular prompting practice
with guaranteed format compliance.

</details>


### [113] [Can an LLM Induce a Graph? Investigating Memory Drift and Context Length](https://arxiv.org/abs/2510.03611)
*Raquib Bin Yousuf,Aadyant Khatri,Shengzhe Xu,Mandar Sharma,Naren Ramakrishnan*

Main category: cs.CL

TL;DR: The paper critiques current benchmarks for evaluating LLMs, proposing relational reasoning tasks as better alternatives to accurately assess their ability to process complex and noisy information.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for LLM evaluation fail to accurately represent real-world information-dense scenarios and reasoning tasks, prompting exploration into better measures of effective context length and memory.

Method: The authors assess relational reasoning by requiring LLMs to induce graph-like structures from noisy natural language text, aiming to reveal the models' memory and contextual limitations compared to current benchmarks.

Result: LLMs exhibit memory drift and contextual forgetting much earlier during relational reasoning tasks, even in models specialized for reasoning like OpenAI's o1.

Conclusion: Significant improvements are needed in LLM architectures to handle long-range relational reasoning and structured knowledge abstraction effectively.

Abstract: Recently proposed evaluation benchmarks aim to characterize the effective
context length and the forgetting tendencies of large language models (LLMs).
However, these benchmarks often rely on simplistic 'needle in a haystack'
retrieval or continuation tasks that may not accurately reflect the performance
of these models in information-dense scenarios. Thus, rather than simple next
token prediction, we argue for evaluating these models on more complex
reasoning tasks that requires them to induce structured relational knowledge
from the text - such as graphs from potentially noisy natural language content.
While the input text can be viewed as generated in terms of a graph, its
structure is not made explicit and connections must be induced from distributed
textual cues, separated by long contexts and interspersed with irrelevant
information. Our findings reveal that LLMs begin to exhibit memory drift and
contextual forgetting at much shorter effective lengths when tasked with this
form of relational reasoning, compared to what existing benchmarks suggest.
With these findings, we offer recommendations for the optimal use of popular
LLMs for complex reasoning tasks. We further show that even models specialized
for reasoning, such as OpenAI o1, remain vulnerable to early memory drift in
these settings. These results point to significant limitations in the models'
ability to abstract structured knowledge from unstructured input and highlight
the need for architectural adaptations to improve long-range reasoning.

</details>


### [114] [Towards Unsupervised Speech Recognition at the Syllable-Level](https://arxiv.org/abs/2510.03639)
*Liming Wang,Junrui Ni,Kai-Wei Chang,Saurabhchand Bhati,David Harwath,Mark Hasegawa-Johnson,James R. Glass*

Main category: cs.CL

TL;DR: The paper proposes a syllable-level framework for unsupervised speech recognition, addressing training instability and dependence on costly grapheme-to-phoneme conversion tools.


<details>
  <summary>Details</summary>
Motivation: To extend automatic speech recognition (ASR) capabilities to low-resource languages and enable multimodal learning from non-parallel data through unsupervised techniques.

Method: The authors introduce a syllable-level unsupervised speech recognition framework based on masked language modeling, bypassing the need for grapheme-to-phoneme converters and mitigating training instability.

Result: The proposed approach reduces character error rate (CER) by up to 40% on LibriSpeech and effectively generalizes to Mandarin, overcoming limitations of previous methods with ambiguous phoneme boundaries.

Conclusion: The syllable-level UASR framework enhances ASR for low-resource languages and languages with challenging phoneme structures, ensuring stability without reliance on costly resources.

Abstract: Training speech recognizers with unpaired speech and text -- known as
unsupervised speech recognition (UASR) -- is a crucial step toward extending
ASR to low-resource languages in the long-tail distribution and enabling
multimodal learning from non-parallel data. However, existing approaches based
on phones often rely on costly resources such as grapheme-to-phoneme converters
(G2Ps) and struggle to generalize to languages with ambiguous phoneme
boundaries due to training instability. In this paper, we address both
challenges by introducing a syllable-level UASR framework based on masked
language modeling, which avoids the need for G2P and the instability of
GAN-based methods. Our approach achieves up to a 40\% relative reduction in
character error rate (CER) on LibriSpeech and generalizes effectively to
Mandarin, a language that has remained particularly difficult for prior
methods. Code will be released upon acceptance.

</details>


### [115] [UNIDOC-BENCH: A Unified Benchmark for Document-Centric Multimodal RAG](https://arxiv.org/abs/2510.03663)
*Xiangyu Peng,Cab Qin,Zeyuan Chen,Ran Xu,Caiming Xiong,Chien-Sheng Wu*

Main category: cs.CL

TL;DR: The paper introduces UniDoc-Bench, a new large-scale benchmark for multimodal retrieval-augmented generation (MM-RAG) using real-world PDF data for QA tasks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address fragmented evaluations in MM-RAG by creating a realistic assessment framework that considers document-centric multimodal use cases.

Method: The authors construct UniDoc-Bench using 70k PDF pages, generating multimodal QA pairs and supporting comparison of four retrieval paradigms through a unified evaluation protocol.

Result: The study finds that multimodal text-image fusion systems outperform unimodal and embedding-based retrieval approaches, revealing inadequacies in current multimodal embeddings.

Conclusion: The paper highlights the importance of integrating visual and textual contexts for robust MM-RAG performance and provides insights for improving multimodal retrieval systems.

Abstract: Multimodal retrieval-augmented generation (MM-RAG) is a key approach for
applying large language models (LLMs) and agents to real-world knowledge bases,
yet current evaluations are fragmented, focusing on either text or images in
isolation or on simplified multimodal setups that fail to capture
document-centric multimodal use cases. In this paper, we introduce
UniDoc-Bench, the first large-scale, realistic benchmark for MM-RAG built from
70k real-world PDF pages across eight domains. Our pipeline extracts and links
evidence from text, tables, and figures, then generates 1,600 multimodal QA
pairs spanning factual retrieval, comparison, summarization, and logical
reasoning queries. To ensure reliability, 20% of QA pairs are validated by
multiple annotators and expert adjudication. UniDoc-Bench supports
apples-to-apples comparison across four paradigms: (1) text-only, (2)
image-only, (3) multimodal text-image fusion, and (4) multimodal joint
retrieval -- under a unified protocol with standardized candidate pools,
prompts, and evaluation metrics. Our experiments show that multimodal
text-image fusion RAG systems consistently outperform both unimodal and jointly
multimodal embedding-based retrieval, indicating that neither text nor images
alone are sufficient and that current multimodal embeddings remain inadequate.
Beyond benchmarking, our analysis reveals when and how visual context
complements textual evidence, uncovers systematic failure modes, and offers
actionable guidance for developing more robust MM-RAG pipelines.

</details>


### [116] [Fine-Tuning Large Language Models with QLoRA for Offensive Language Detection in Roman Urdu-English Code-Mixed Text](https://arxiv.org/abs/2510.03683)
*Nisar Hussain,Amna Qasim,Gull Mehak,Muhammad Zain,Momina Hafeez,Grigori Sidorov*

Main category: cs.CL

TL;DR: The study focuses on improving offensive language detection in Roman Urdu-English text using QLoRA-based fine-tuning and leveraging English LLMs on a translated dataset. Meta LLaMA 3 8B achieved the best F1 score of 91.45, surpassing other models.


<details>
  <summary>Details</summary>
Motivation: Detecting offensive language in code-mixed Roman Urdu poses challenges like lack of grammar rules, inconsistent spelling, and scarcity of data. The study aims to address these using scalable LLM-based methods.

Method: The researchers translated Roman Urdu-English datasets to English and fine-tuned multiple models (e.g., Meta LLaMA 3 8B, Mistral 7B, RoBERTa) via QLoRA for memory-efficient adaptation. Experiments used annotated datasets with emphasis on offensive language classification.

Result: Meta LLaMA 3 8B achieved the highest F1 score (91.45), followed by Mistral 7B (89.66), outperforming conventional transformer models in offensive language detection.

Conclusion: QLoRA proves effective for fine-tuning large models in low-resource environments, demonstrating the potential of LLMs for code-mixed offensive language detection and enabling the development of multilingual moderation capabilities.

Abstract: The use of derogatory terms in languages that employ code mixing, such as
Roman Urdu, presents challenges for Natural Language Processing systems due to
unstated grammar, inconsistent spelling, and a scarcity of labeled data. In
this work, we propose a QLoRA based fine tuning framework to improve offensive
language detection in Roman Urdu-English text. We translated the Roman
Urdu-English code mixed dataset into English using Google Translate to leverage
English LLMs, while acknowledging that this translation reduces direct
engagement with code mixing features. Our focus is on classification
performance using English translated low resource inputs. We fine tuned several
transformers and large language models, including Meta LLaMA 3 8B, Mistral 7B
v0.1, LLaMA 2 7B, ModernBERT, and RoBERTa, with QLoRA for memory efficient
adaptation. Models were trained and evaluated on a manually annotated Roman
Urdu dataset for offensive vs non offensive content. Of all tested models, the
highest F1 score of 91.45 was attained by Meta LLaMA 3 8B, followed by Mistral
7B at 89.66, surpassing traditional transformer baselines. These results
demonstrate the efficacy of QLoRA in fine tuning high performing models for low
resource environments such as code mixed offensive language detection, and
confirm the potential of LLMs for this task. This work advances a scalable
approach to Roman Urdu moderation and paves the way for future multilingual
offensive detection systems based on LLMs.

</details>


### [117] [MedReflect: Teaching Medical LLMs to Self-Improve via Reflective Correction](https://arxiv.org/abs/2510.03687)
*Yue Huang,Yanyuan Chen,Dexuan Xu,Weihua Yue,Huamin Zhang,Meikang Qiu,Yu Huang*

Main category: cs.CL

TL;DR: This paper introduces MedReflect, a framework that mimics physician-like reasoning in large language models (LLMs) to improve medical problem-solving without relying on external retrieval or extensive annotations.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing LLMs in medical problem-solving, such as reliance on external knowledge retrieval and high annotation costs, and to unlock LLMs' latent capabilities through a reflective approach.

Method: MedReflect employs a reflective thinking framework involving hypothesis generation, self-questioning, self-answering, and decision refinement, enabling LLMs to self-verify solutions without external retrieval.

Result: With only 2,000 training examples and minimal fine-tuning, MedReflect achieved notable accuracy improvements across medical benchmarks while reducing the need for labor-intensive annotations.

Conclusion: LLMs can effectively solve specialized medical problems using self-reflective methodologies, reducing dependence on external retrieval and extensive fine-tuning while maintaining high accuracy.

Abstract: Medical problem solving demands expert knowledge and intricate reasoning.
Recent studies of large language models (LLMs) attempt to ease this complexity
by introducing external knowledge verification through retrieval-augmented
generation or by training on reasoning datasets. However, these approaches
suffer from drawbacks such as retrieval overhead and high annotation costs, and
they heavily rely on substituted external assistants to reach limited
performance in medical field. In this paper, we introduce MedReflect, a
generalizable framework designed to inspire LLMs with a physician-like
reflective thinking mode. MedReflect generates a single-pass reflection chain
that includes initial hypothesis generation, self-questioning, self-answering
and decision refinement. This self-verified and self-reflective nature releases
large language model's latent capability in medical problem-solving without
external retrieval or heavy annotation. We demonstrate that MedReflect enables
cost-efficient medical dataset construction: with merely 2,000 randomly sampled
training examples and a light fine-tuning, this approach achieves notable
absolute accuracy improvements across a series of medical benchmarks while
cutting annotation requirements. Our results provide evidence that LLMs can
learn to solve specialized medical problems via self-reflection and
self-improve, reducing reliance on external supervision and extensive
task-specific fine-tuning data.

</details>


### [118] [TreePrompt: Leveraging Hierarchical Few-Shot Example Selection for Improved English-Persian and English-German Translation](https://arxiv.org/abs/2510.03748)
*Ramtin Kakavand,Ebrahim Ansari*

Main category: cs.CL

TL;DR: The paper introduces TreePrompt, a new technique to improve example selection for prompting Large Language Models (LLMs), enhancing machine translation quality.


<details>
  <summary>Details</summary>
Motivation: Despite the effectiveness of few-shot prompting for translation tasks, existing approaches primarily focus on example similarity and ignore the quality of examples.

Method: TreePrompt is a tree-structured framework that learns LLM preferences to select high-quality and contextually relevant examples. It is tested in combination with K-NN and Adaptive Few-Shot Prompting (AFSP).

Result: TreePrompt, combined with AFSP or Random selection, improves translation quality on English-Persian and English-German datasets.

Conclusion: TreePrompt offers a promising method for better example selection in few-shot prompting, balancing similarity and quality to improve machine translation.

Abstract: Large Language Models (LLMs) have consistently demonstrated strong
performance in machine translation, especially when guided by high-quality
prompts. Few-shot prompting is an effective technique to improve translation
quality; however, most existing example selection methods focus solely on
query-to-example similarity and do not account for the quality of the examples.
In this work, we propose TreePrompt, a novel example selection approach that
learns LLM preferences to identify high-quality, contextually relevant examples
within a tree-structured framework. To further explore the balance between
similarity and quality, we combine TreePrompt with K-Nearest Neighbors (K-NN)
and Adaptive Few-Shot Prompting (AFSP). Evaluations on two language pairs -
English-Persian (MIZAN) and English-German (WMT19) - show that integrating
TreePrompt with AFSP or Random selection leads to improved translation
performance.

</details>


### [119] [Cross-Lingual Multi-Granularity Framework for Interpretable Parkinson's Disease Diagnosis from Speech](https://arxiv.org/abs/2510.03758)
*Ilias Tougui,Mehdi Zakroum,Mounir Ghogho*

Main category: cs.CL

TL;DR: The paper introduces a granularity-aware bidirectional LSTM-based framework for multilingual Parkinson's Disease (PD) detection via phoneme, syllable, and word-level speech analysis, achieving high accuracy and alignment with clinical practices.


<details>
  <summary>Details</summary>
Motivation: Current PD speech-based detection systems analyze entire utterances but may neglect the diagnostic significance of finer phonetic components, prompting a need for granularity-aware analysis to enhance diagnostic precision and support multilingual contexts.

Method: The authors developed a pipeline to extract time-aligned phonemes, syllables, and words from recorded speech and utilized a bidirectional LSTM with multi-head attention to assess diagnostic performance across granularity levels, tested on Italian, Spanish, and English datasets.

Result: Phoneme-level analysis outperformed other granularity levels with an AUROC of 93.78% ± 2.34% and an accuracy of 92.17% ± 2.43%, demonstrating high diagnostic performance. Attention analysis also aligned the most informative speech features with clinically established protocols.

Conclusion: The study validated the utility of fine-grained phonetic analysis for cross-linguistic automatic PD detection and linked significant speech features to clinical standards, showcasing the framework's potential for practical healthcare application. The code is made accessible to foster reproducibility.

Abstract: Parkinson's Disease (PD) affects over 10 million people worldwide, with
speech impairments in up to 89% of patients. Current speech-based detection
systems analyze entire utterances, potentially overlooking the diagnostic value
of specific phonetic elements. We developed a granularity-aware approach for
multilingual PD detection using an automated pipeline that extracts
time-aligned phonemes, syllables, and words from recordings. Using Italian,
Spanish, and English datasets, we implemented a bidirectional LSTM with
multi-head attention to compare diagnostic performance across the different
granularity levels. Phoneme-level analysis achieved superior performance with
AUROC of 93.78% +- 2.34% and accuracy of 92.17% +- 2.43%. This demonstrates
enhanced diagnostic capability for cross-linguistic PD detection. Importantly,
attention analysis revealed that the most informative speech features align
with those used in established clinical protocols: sustained vowels (/a/, /e/,
/o/, /i/) at phoneme level, diadochokinetic syllables (/ta/, /pa/, /la/, /ka/)
at syllable level, and /pataka/ sequences at word level. Source code will be
available at https://github.com/jetliqs/clearpd.

</details>


### [120] [Prompt Balance Matters: Understanding How Imbalanced Few-Shot Learning Affects Multilingual Sense Disambiguation in LLMs](https://arxiv.org/abs/2510.03762)
*Deshan Sumanathilaka,Nicholas Micallef,Julian Hough*

Main category: cs.CL

TL;DR: The study assesses how few-shot prompting strategies in Large Language Models (LLMs) affect Word Sense Disambiguation (WSD), especially focusing on biases introduced by imbalanced sample distributions in multilingual languages.


<details>
  <summary>Details</summary>
Motivation: To investigate the effectiveness of few-shot prompting in WSD tasks and to highlight biases caused by imbalanced sample distributions, especially in multilingual settings.

Method: The study utilizes the GLOSSGPT prompting method for WSD evaluation in five languages: English, German, Spanish, French, and Italian. It examines results from GPT-4o and LLaMA-3.1-70B models while assessing biases introduced by imbalanced sample distributions.

Result: Findings reveal that imbalanced few-shot samples lead to incorrect sense predictions in multilingual scenarios but are not problematic for English WSD.

Conclusion: Multilingual WSD tasks are sensitive to sample distribution in few-shot prompting. Balanced and representative strategies are essential for effective disambiguation in non-English languages.

Abstract: Recent advances in Large Language Models (LLMs) have significantly reshaped
the landscape of Natural Language Processing (NLP). Among the various prompting
techniques, few-shot prompting has gained considerable attention for its
practicality and effectiveness. This study investigates how few-shot prompting
strategies impact the Word Sense Disambiguation (WSD) task, particularly
focusing on the biases introduced by imbalanced sample distributions. We use
the GLOSSGPT prompting method, an advanced approach for English WSD, to test
its effectiveness across five languages: English, German, Spanish, French, and
Italian. Our results show that imbalanced few-shot examples can cause incorrect
sense predictions in multilingual languages, but this issue does not appear in
English. To assess model behavior, we evaluate both the GPT-4o and
LLaMA-3.1-70B models and the results highlight the sensitivity of multilingual
WSD to sample distribution in few-shot settings, emphasizing the need for
balanced and representative prompting strategies.

</details>


### [121] [Rezwan: Leveraging Large Language Models for Comprehensive Hadith Text Processing: A 1.2M Corpus Development](https://arxiv.org/abs/2510.03781)
*Majid Asgari-Bidhendi,Muhammad Amin Ghaseminia,Alireza Shahbazi,Sayyed Ali Hossayni,Najmeh Torabian,Behrouz Minaei-Bidgoli*

Main category: cs.CL

TL;DR: The paper introduces Rezwan, an AI-assisted Hadith corpus with 1.2M narrations, showcasing near-human accuracy in language processing tasks and offering a significant cost-effective solution to Islamic studies.


<details>
  <summary>Details</summary>
Motivation: The motivation is to provide a scalable, cost-effective, and enriched digital infrastructure for Islamic studies and digital humanities, addressing the challenges in manual processing of large Hadith datasets.

Method: A fully automated pipeline using Large Language Models (LLMs) processes narrations, extracting and enriching them through segmentation, validation, translation, diacritization, summarization, tagging, and semantic analysis.

Result: Rezwan demonstrated near-human accuracy in tasks like textual segmentation and summarization, outperformed the manually curated Noor Corpus, and drastically reduced the time and cost required for large-scale religious text processing.

Conclusion: Rezwan exemplifies how AI can augment human expertise, setting a new standard for large-scale, multilingual, and semantically rich access to Islamic heritage.

Abstract: This paper presents the development of Rezwan, a large-scale AI-assisted
Hadith corpus comprising over 1.2M narrations, extracted and structured through
a fully automated pipeline. Building on digital repositories such as Maktabat
Ahl al-Bayt, the pipeline employs Large Language Models (LLMs) for
segmentation, chain--text separation, validation, and multi-layer enrichment.
Each narration is enhanced with machine translation into twelve languages,
intelligent diacritization, abstractive summarization, thematic tagging, and
cross-text semantic analysis. This multi-step process transforms raw text into
a richly annotated research-ready infrastructure for digital humanities and
Islamic studies. A rigorous evaluation was conducted on 1,213 randomly sampled
narrations, assessed by six domain experts. Results show near-human accuracy in
structured tasks such as chain--text separation (9.33/10) and summarization
(9.33/10), while highlighting ongoing challenges in diacritization and semantic
similarity detection. Comparative analysis against the manually curated Noor
Corpus demonstrates the superiority of Najm in both scale and quality, with a
mean overall score of 8.46/10 versus 3.66/10. Furthermore, cost analysis
confirms the economic feasibility of the AI approach: tasks requiring over
229,000 hours of expert labor were completed within months at a fraction of the
cost. The work introduces a new paradigm in religious text processing by
showing how AI can augment human expertise, enabling large-scale, multilingual,
and semantically enriched access to Islamic heritage.

</details>


### [122] [Mechanistic Interpretability of Socio-Political Frames in Language Models](https://arxiv.org/abs/2510.03799)
*Hadi Asghari,Sami Nenno*

Main category: cs.CL

TL;DR: The paper investigates large language models' ability to generate and identify deep cognitive frames, particularly in socio-political contexts.


<details>
  <summary>Details</summary>
Motivation: To understand how large language models interpret and represent meaningful human concepts, especially socio-political cognitive frames.

Method: The authors examined the model's fluency in generating and recognizing frames, conducted zero-shot recognition tests, and explored hidden representations to locate dimensions correlated with specific frames.

Result: LLMs can fluently generate texts evoking specific cognitive frames and recognize frames in zero-shot scenarios. Singular dimensions were found strongly correlating with the presence of specific frames in their hidden representations.

Conclusion: The research highlights LLMs’ capacity to capture and express nuanced human concepts, enhancing our understanding of their interpretability.

Abstract: This paper explores the ability of large language models to generate and
recognize deep cognitive frames, particularly in socio-political contexts. We
demonstrate that LLMs are highly fluent in generating texts that evoke specific
frames and can recognize these frames in zero-shot settings. Inspired by
mechanistic interpretability research, we investigate the location of the
`strict father' and `nurturing parent' frames within the model's hidden
representation, identifying singular dimensions that correlate strongly with
their presence. Our findings contribute to understanding how LLMs capture and
express meaningful human concepts.

</details>


### [123] [Beyond Token Length: Step Pruner for Efficient and Accurate Reasoning in Large Language Models](https://arxiv.org/abs/2510.03805)
*Canhui Wu,Qiong Cao,Chang Li,Zhenfang Wang,Chao Xue,Yuwei Fan,Wei Xi,Xiaodong He*

Main category: cs.CL

TL;DR: Large Reasoning Models (LRMs) often overthink, producing verbose outputs. The proposed Step Pruner (SP) uses reinforcement learning to promote concise and efficient reasoning while maintaining correctness.


<details>
  <summary>Details</summary>
Motivation: To address the problem of verbosity or overthinking in Large Reasoning Models (LRMs) during complex tasks, which leads to inefficiency and potential model hacking.

Method: The paper introduces Step Pruner (SP), an RL-based framework with a step-aware reward mechanism for balancing compactness and correctness. It also uses a dynamic stopping mechanism to prevent hacking behaviors.

Result: In experiments on four reasoning benchmarks, SP achieved state-of-the-art accuracy and significantly reduced token usage—e.g., lowering it by 69.7% on the AIME24 benchmark.

Conclusion: Step Pruner (SP) effectively enhances the efficiency and accuracy of LRMs by reducing reasoning verbosity and token misuse, offering a significant step forward in LRM optimization.

Abstract: Large Reasoning Models (LRMs) demonstrate strong performance on complex tasks
but often suffer from excessive verbosity, known as "overthinking." Existing
solutions via reinforcement learning (RL) typically penalize generated tokens
to promote conciseness. However, these methods encounter two challenges:
responses with fewer tokens do not always correspond to fewer reasoning steps,
and models may develop hacking behavior in later stages of training by
discarding reasoning steps to minimize token usage. In this work, we introduce
\textbf{Step Pruner (SP)}, an RL framework that steers LRMs toward more
efficient reasoning by favoring compact reasoning steps. Our step-aware reward
function prioritizes correctness while imposing penalties for redundant steps,
and withholds rewards for incorrect responses to prevent the reinforcement of
erroneous reasoning. Moreover, we propose a dynamic stopping mechanism: when
the length of any output step exceeds the upper limit, we halt updates to
prevent hacking behavior caused by merging steps. Extensive experiments across
four reasoning benchmarks demonstrate that SP achieves state-of-the-art
accuracy while significantly reducing response length. For instance, on AIME24,
SP reduces token usage by \textbf{69.7\%}.

</details>


### [124] [Annotate Rhetorical Relations with INCEpTION: A Comparison with Automatic Approaches](https://arxiv.org/abs/2510.03808)
*Mehedi Hasan Emon*

Main category: cs.CL

TL;DR: The paper studies the annotation of rhetorical relations in cricket news using manual and transformer-based automatic models, finding DistilBERT as the most accurate approach.


<details>
  <summary>Details</summary>
Motivation: To evaluate the effectiveness of manual and automatic annotation techniques for discourse relations in sports reports, addressing the intersection of discourse parsing and NLP.

Method: The study used the INCEpTION tool and tested BERT, DistilBERT, and Logistic Regression models to classify rhetorical relations like elaboration and cause-effect in cricket news.

Result: DistilBERT achieved the highest accuracy among the models in predicting rhetorical relations.

Conclusion: The findings underscore the potential of transformer-based models, particularly DistilBERT, in improving discourse relation prediction for NLP applications.

Abstract: This research explores the annotation of rhetorical relations in discourse
using the INCEpTION tool and compares manual annotation with automatic
approaches based on large language models. The study focuses on sports reports
(specifically cricket news) and evaluates the performance of BERT, DistilBERT,
and Logistic Regression models in classifying rhetorical relations such as
elaboration, contrast, background, and cause-effect. The results show that
DistilBERT achieved the highest accuracy, highlighting its potential for
efficient discourse relation prediction. This work contributes to the growing
intersection of discourse parsing and transformer-based NLP. (This paper was
conducted as part of an academic requirement under the supervision of Prof. Dr.
Ralf Klabunde, Linguistic Data Science Lab, Ruhr University Bochum.) Keywords:
Rhetorical Structure Theory, INCEpTION, BERT, DistilBERT, Discourse Parsing,
NLP.

</details>


### [125] [Read Between the Lines: A Benchmark for Uncovering Political Bias in Bangla News Articles](https://arxiv.org/abs/2510.03898)
*Nusrat Jahan Lia,Shubhashis Roy Dipta,Abdullah Khan Zehady,Naymul Islam,Madhusodan Chakraborty,Abdullah Al Wasif*

Main category: cs.CL

TL;DR: The paper introduces a benchmark dataset for Bangla political bias detection and evaluates large language models' performance on the task.


<details>
  <summary>Details</summary>
Motivation: The scarcity of annotated datasets and computational studies for detecting political bias in Bangla news articles, coupled with the linguistic and socio-political complexities involved.

Method: A dataset of 200 Bangla news articles labeled for political stances was developed, and diagnostic analyses were conducted to evaluate the performance of 28 Large Language Models (LLMs).

Result: LLMs performed well in detecting government-critique articles (F1 up to 0.83) but struggled with neutral articles (F1 as low as 0.00) and showed bias towards predicting government-leaning stances.

Conclusion: The dataset forms a foundation for advancing political stance detection in Bangla news articles and helps identify critical performance gaps in current LLMs for low-resource languages.

Abstract: Detecting media bias is crucial, specifically in the South Asian region.
Despite this, annotated datasets and computational studies for Bangla political
bias research remain scarce. Crucially because, political stance detection in
Bangla news requires understanding of linguistic cues, cultural context, subtle
biases, rhetorical strategies, code-switching, implicit sentiment, and
socio-political background. To address this, we introduce the first benchmark
dataset of 200 politically significant and highly debated Bangla news articles,
labeled for government-leaning, government-critique, and neutral stances,
alongside diagnostic analyses for evaluating large language models (LLMs). Our
comprehensive evaluation of 28 proprietary and open-source LLMs shows strong
performance in detecting government-critique content (F1 up to 0.83) but
substantial difficulty with neutral articles (F1 as low as 0.00). Models also
tend to over-predict government-leaning stances, often misinterpreting
ambiguous narratives. This dataset and its associated diagnostics provide a
foundation for advancing stance detection in Bangla media research and offer
insights for improving LLM performance in low-resource languages.

</details>


### [126] [PsycholexTherapy: Simulating Reasoning in Psychotherapy with Small Language Models in Persian](https://arxiv.org/abs/2510.03913)
*Mohammad Amin Abbasi,Hassan Naderi*

Main category: cs.CL

TL;DR: PsychoLexTherapy is a framework for simulating psychotherapeutic reasoning in Persian using small language models, optimized for privacy-preserving, on-device use.


<details>
  <summary>Details</summary>
Motivation: Fill the gap in culturally grounded and therapeutically coherent dialogue systems for underrepresented languages, focusing on Persian psychotherapeutic simulations.

Method: Three-stage process: evaluation of small language models (SLMs) psychological knowledge, implementation of the PsychoLexTherapy framework, and creation of evaluation datasets (PsychoLexQuery and PsychoLexDialogue).

Result: PsychoLexTherapy outperformed baselines in human and automatic evaluations, particularly in multi-turn interactions using structured memory features.

Conclusion: The framework provides a privacy-aware and culturally suitable solution for Persian psychotherapy simulations, supported by novel datasets and benchmarking methods.

Abstract: This study presents PsychoLexTherapy, a framework for simulating
psychotherapeutic reasoning in Persian using small language models (SLMs). The
framework tackles the challenge of developing culturally grounded,
therapeutically coherent dialogue systems with structured memory for multi-turn
interactions in underrepresented languages. To ensure privacy and feasibility,
PsychoLexTherapy is optimized for on-device deployment, enabling use without
external servers. Development followed a three-stage process: (i) assessing
SLMs psychological knowledge with PsychoLexEval; (ii) designing and
implementing the reasoning-oriented PsychoLexTherapy framework; and (iii)
constructing two evaluation datasets-PsychoLexQuery (real Persian user
questions) and PsychoLexDialogue (hybrid simulated sessions)-to benchmark
against multiple baselines. Experiments compared simple prompting, multi-agent
debate, and structured therapeutic reasoning paths. Results showed that
deliberate model selection balanced accuracy, efficiency, and privacy. On
PsychoLexQuery, PsychoLexTherapy outperformed all baselines in automatic
LLM-as-a-judge evaluation and was ranked highest by human evaluators in a
single-turn preference study. In multi-turn tests with PsychoLexDialogue, the
long-term memory module proved essential: while naive history concatenation
caused incoherence and information loss, the full framework achieved the
highest ratings in empathy, coherence, cultural fit, and personalization.
Overall, PsychoLexTherapy establishes a practical, privacy-preserving, and
culturally aligned foundation for Persian psychotherapy simulation,
contributing novel datasets, a reproducible evaluation pipeline, and empirical
insights into structured memory for therapeutic reasoning.

</details>


### [127] [Mapping Patient-Perceived Physician Traits from Nationwide Online Reviews with LLMs](https://arxiv.org/abs/2510.03997)
*Junjie Luo,Rui Han,Arshana Welivita,Zeleikun Di,Jingfu Wu,Xuzhe Zhi,Ritu Agarwal,Gordon Gao*

Main category: cs.CL

TL;DR: This paper utilizes a large language model (LLM) to analyze patient reviews and infer physician personality traits and patient judgments, uncovering systematic patterns and validating its findings through expert comparison and correlation studies.


<details>
  <summary>Details</summary>
Motivation: The paper aims to improve understanding of physician-patient relationships by analyzing how patients perceive physicians through their reviews, which is critical for enhancing trust, communication, and satisfaction.

Method: Using a large dataset of 4.1 million patient reviews, the study applies a LLM pipeline to infer Big Five personality traits and subjective judgments. Validation involved multi-model comparisons, human expert benchmarking, and correlation with patient satisfaction metrics.

Result: It revealed patterns such as male physicians receiving higher ratings across all traits, empathy-related traits being prevalent in specialized fields like pediatrics and psychiatry, and identified four distinct physician archetypes through cluster analysis.

Conclusion: Automated assessment of physician traits using patient narratives provides validated, scalable insights, which can inform healthcare quality measurement, bias identification, and workforce strategy development.

Abstract: Understanding how patients perceive their physicians is essential to
improving trust, communication, and satisfaction. We present a large language
model (LLM)-based pipeline that infers Big Five personality traits and five
patient-oriented subjective judgments. The analysis encompasses 4.1 million
patient reviews of 226,999 U.S. physicians from an initial pool of one million.
We validate the method through multi-model comparison and human expert
benchmarking, achieving strong agreement between human and LLM assessments
(correlation coefficients 0.72-0.89) and external validity through correlations
with patient satisfaction (r = 0.41-0.81, all p<0.001). National-scale analysis
reveals systematic patterns: male physicians receive higher ratings across all
traits, with largest disparities in clinical competence perceptions;
empathy-related traits predominate in pediatrics and psychiatry; and all traits
positively predict overall satisfaction. Cluster analysis identifies four
distinct physician archetypes, from "Well-Rounded Excellent" (33.8%, uniformly
high traits) to "Underperforming" (22.6%, consistently low). These findings
demonstrate that automated trait extraction from patient narratives can provide
interpretable, validated metrics for understanding physician-patient
relationships at scale, with implications for quality measurement, bias
detection, and workforce development in healthcare.

</details>


### [128] [Simulating and Understanding Deceptive Behaviors in Long-Horizon Interactions](https://arxiv.org/abs/2510.03999)
*Yang Xu,Xuanming Zhang,Min-Hsuan Yeh,Jwala Dhamala,Ousmane Dia,Rahul Gupta,Yixuan Li*

Main category: cs.CL

TL;DR: The paper introduces a simulation framework to study deception in language models (LLMs) during long-horizon tasks, revealing its dependence on model type and contextual pressures.


<details>
  <summary>Details</summary>
Motivation: To address the lack of comprehensive evaluation methods for detecting deception in LLMs, particularly in complex, long-horizon interactions.

Method: The authors developed a multi-agent simulation framework where one agent performs tasks, another supervises and provides feedback, and an independent auditor evaluates deceptive tendencies in task trajectories.

Result: Experiments with 11 LLMs showed that deception varies by model, intensifies under pressure, and reduces trust from supervisors. Unique strategies such as concealment, equivocation, and falsification were observed.

Conclusion: Deception is an emergent issue in extended interactions with LLMs and requires thorough evaluation frameworks for trust-sensitive applications.

Abstract: Deception is a pervasive feature of human communication and an emerging
concern in large language models (LLMs). While recent studies document
instances of LLM deception under pressure, most evaluations remain confined to
single-turn prompts and fail to capture the long-horizon interactions in which
deceptive strategies typically unfold. We introduce the first simulation
framework for probing and evaluating deception in LLMs under extended sequences
of interdependent tasks and dynamic contextual pressures. Our framework
instantiates a multi-agent system: a performer agent tasked with completing
tasks and a supervisor agent that evaluates progress, provides feedback, and
maintains evolving states of trust. An independent deception auditor then
reviews full trajectories to identify when and how deception occurs. We conduct
extensive experiments across 11 frontier models, spanning both closed- and
open-source systems, and find that deception is model-dependent, increases with
event pressure, and consistently erodes supervisor trust. Qualitative analyses
further reveal distinct strategies of concealment, equivocation, and
falsification. Our findings establish deception as an emergent risk in
long-horizon interactions and provide a foundation for evaluating future LLMs
in real-world, trust-sensitive contexts.

</details>


### [129] [Named Entity Recognition in COVID-19 tweets with Entity Knowledge Augmentation](https://arxiv.org/abs/2510.04001)
*Xuankang Zhang,Jiangming Liu*

Main category: cs.CL

TL;DR: The paper addresses challenges in recognizing COVID-19-related named entities in informal social media texts and formal biomedical texts using a novel entity knowledge augmentation approach.


<details>
  <summary>Details</summary>
Motivation: The need to enhance understanding of pandemic-related discussions on social media, particularly given the informal nature of texts and the specialized domain knowledge required.

Method: A novel entity knowledge augmentation approach is proposed to improve named entity recognition (NER) in both informal and formal biomedical texts, leveraging domain-specific insights.

Result: Experiments on COVID-19 Twitter data and PubMed data demonstrate improved performance in NER tasks under fully-supervised and few-shot scenarios.

Conclusion: The approach enhances NER effectiveness in COVID-19-related and biomedical contexts, addressing annotation shortages and domain-specific challenges. The accompanying source code is shared for broader use.

Abstract: The COVID-19 pandemic causes severe social and economic disruption around the
world, raising various subjects that are discussed over social media.
Identifying pandemic-related named entities as expressed on social media is
fundamental and important to understand the discussions about the pandemic.
However, there is limited work on named entity recognition on this topic due to
the following challenges: 1) COVID-19 texts in social media are informal and
their annotations are rare and insufficient to train a robust recognition
model, and 2) named entity recognition in COVID-19 requires extensive
domain-specific knowledge. To address these issues, we propose a novel entity
knowledge augmentation approach for COVID-19, which can also be applied in
general biomedical named entity recognition in both informal text format and
formal text format. Experiments carried out on the COVID-19 tweets dataset and
PubMed dataset show that our proposed entity knowledge augmentation improves
NER performance in both fully-supervised and few-shot settings. Our source code
is publicly available: https://github.com/kkkenshi/LLM-EKA/tree/master

</details>


### [130] [AgriGPT-VL: Agricultural Vision-Language Understanding Suite](https://arxiv.org/abs/2510.04002)
*Bo Yang,Yunkui Chen,Lanfei Feng,Yu Zhang,Xiao Xu,Jianyu Zhang,Nueraili Aierken,Runhe Huang,Hongjian Lin,Yibin Ying,Shijian Li*

Main category: cs.CL

TL;DR: This paper introduces AgriGPT-VL Suite, a specialized multimodal framework for agriculture that includes a new vision-language corpus, a tailored model, and an evaluation benchmark. It outperforms general-purpose models on agricultural tasks.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the lack of domain-specific models, curated datasets, and evaluation protocols in the application of multimodal large language models for agriculture.

Method: The authors developed the AgriGPT-VL Suite, which includes the Agri-3M-VL dataset (a large vision-language corpus), the AgriGPT-VL model (trained using progressive curriculum techniques), and the AgriBench-VL-4K benchmark for evaluation.

Result: AgriGPT-VL outperforms general-purpose vision-language models on agriculture-specific tasks as evaluated on AgriBench-VL-4K, while maintaining competitive performance in text-only scenarios.

Conclusion: The specialized framework significantly enhances multimodal reasoning in agriculture-focused applications and demonstrates the importance of tailored datasets and methods for domain-specific advancements.

Abstract: Despite rapid advances in multimodal large language models, agricultural
applications remain constrained by the scarcity of domain-tailored models,
curated vision-language corpora, and rigorous evaluation. To address these
challenges, we present the AgriGPT-VL Suite, a unified multimodal framework for
agriculture. Our contributions are threefold. First, we introduce Agri-3M-VL,
the largest vision-language corpus for agriculture to our knowledge, curated by
a scalable multi-agent data generator; it comprises 1M image-caption pairs, 2M
image-grounded VQA pairs, 50K expert-level VQA instances, and 15K GRPO
reinforcement learning samples. Second, we develop AgriGPT-VL, an
agriculture-specialized vision-language model trained via a progressive
curriculum of textual grounding, multimodal shallow/deep alignment, and GRPO
refinement. This method achieves strong multimodal reasoning while preserving
text-only capability. Third, we establish AgriBench-VL-4K, a compact yet
challenging evaluation suite with open-ended and image-grounded questions,
paired with multi-metric evaluation and an LLM-as-a-judge framework.
Experiments show that AgriGPT-VL outperforms leading general-purpose VLMs on
AgriBench-VL-4K, achieving higher pairwise win rates in the LLM-as-a-judge
evaluation. Meanwhile, it remains competitive on the text-only AgriBench-13K
with no noticeable degradation of language ability. Ablation studies further
confirm consistent gains from our alignment and GRPO refinement stages. We will
open source all of the resources to support reproducible research and
deployment in low-resource agricultural settings.

</details>


### [131] [LLM Microscope: What Model Internals Reveal About Answer Correctness and Context Utilization](https://arxiv.org/abs/2510.04013)
*Jiarui Liu,Jivitesh Jain,Mona Diab,Nishant Subramani*

Main category: cs.CL

TL;DR: The paper explores using model activations to predict output correctness and context efficacy in large language models, achieving promising results in accuracy and interpretability.


<details>
  <summary>Details</summary>
Motivation: To improve trustworthiness in large language models (LLMs) by addressing their tendency to generate incorrect outputs confidently and assessing the utility of retrieved context.

Method: The study operationalizes interpretability methods and trains a simple classifier on intermediate layer activations from the first output token to predict output correctness. Metrics are introduced to distinguish between correct, incorrect, and irrelevant contexts.

Result: The classifier achieves approximately 75% accuracy in predicting output correctness. Internals-based metrics outperform baseline methods in distinguishing correct from incorrect context, aiding in early auditing and mitigating inaccuracies from polluted contexts.

Conclusion: These findings enhance interpretability in LLMs, providing insights into their decision-making processes and introducing tools for improving contextual reliability and output correctness.

Abstract: Although large language models (LLMs) have tremendous utility,
trustworthiness is still a chief concern: models often generate incorrect
information with high confidence. While contextual information can help guide
generation, identifying when a query would benefit from retrieved context and
assessing the effectiveness of that context remains challenging. In this work,
we operationalize interpretability methods to ascertain whether we can predict
the correctness of model outputs from the model's activations alone. We also
explore whether model internals contain signals about the efficacy of external
context. We consider correct, incorrect, and irrelevant context and introduce
metrics to distinguish amongst them. Experiments on six different models reveal
that a simple classifier trained on intermediate layer activations of the first
output token can predict output correctness with about 75% accuracy, enabling
early auditing. Our model-internals-based metric significantly outperforms
prompting baselines at distinguishing between correct and incorrect context,
guarding against inaccuracies introduced by polluted context. These findings
offer a lens to better understand the underlying decision-making processes of
LLMs. Our code is publicly available at
https://github.com/jiarui-liu/LLM-Microscope

</details>


### [132] [Thai Semantic End-of-Turn Detection for Real-Time Voice Agents](https://arxiv.org/abs/2510.04016)
*Thanapol Popit,Natthapath Rungseesiripak,Monthol Charattrakool,Saksorn Ruangtanusak*

Main category: cs.CL

TL;DR: The paper investigates end-of-turn detection for Thai using compact LLMs and lightweight transformers, which demonstrate promise for real-time interaction with near-instant performance.


<details>
  <summary>Details</summary>
Motivation: Traditional end-pointing methods for detecting speech end have delays and struggle with linguistic variations. The study focuses on improving this for real-time Thai-speaking agents.

Method: The authors experimented with compact LLMs via zero-shot/few-shot prompting and supervised fine-tuning lightweight transformers, leveraging Thai linguistic cues and transcribed Thai data.

Result: The study establishes accuracy-latency tradeoffs, showing that small, fine-tuned models can achieve robust near-instant end-of-turn detection with Thai text.

Conclusion: Fine-tuned small models provide swift and reliable Thai-specific end-of-turn detection, setting a baseline for on-device real-time interaction systems.

Abstract: Fluid voice-to-voice interaction requires reliable and low-latency detection
of when a user has finished speaking. Traditional audio-silence end-pointers
add hundreds of milliseconds of delay and fail under hesitations or
language-specific phenomena. We present, to our knowledge, the first systematic
study of Thai text-only end-of-turn (EOT) detection for real-time agents. We
compare zero-shot and few-shot prompting of compact LLMs to supervised
fine-tuning of lightweight transformers. Using transcribed subtitles from the
YODAS corpus and Thai-specific linguistic cues (e.g., sentence-final
particles), we formulate EOT as a binary decision over token boundaries. We
report a clear accuracy-latency tradeoff and provide a public-ready
implementation plan. This work establishes a Thai baseline and demonstrates
that small, fine-tuned models can deliver near-instant EOT decisions suitable
for on-device agents.

</details>


### [133] [Does Using Counterfactual Help LLMs Explain Textual Importance in Classification?](https://arxiv.org/abs/2510.04031)
*Nelvin Tan,James Asikin Cheung,Yu-Ching Shih,Dong Yang,Amol Salunkhe*

Main category: cs.CL

TL;DR: The paper explores how integrating counterfactuals can enhance Large Language Models' (LLMs) ability to identify key words influencing classification decisions.


<details>
  <summary>Details</summary>
Motivation: Explain LLM classification decisions while addressing practical limitations such as black-box constraints and expensive LLM calls.

Method: Introduce a framework called 'decision changing rate' to quantify the importance of top words in classification using counterfactual reasoning.

Result: Experimental results confirm that incorporating counterfactuals improves the identification of influential words in classification tasks by LLMs.

Conclusion: Counterfactual reasoning enhances LLM-based word importance quantification, aiding in better understanding of classification decisions.

Abstract: Large language models (LLMs) are becoming useful in many domains due to their
impressive abilities that arise from large training datasets and large model
sizes. More recently, they have been shown to be very effective in textual
classification tasks, motivating the need to explain the LLMs' decisions.
Motivated by practical constrains where LLMs are black-boxed and LLM calls are
expensive, we study how incorporating counterfactuals into LLM reasoning can
affect the LLM's ability to identify the top words that have contributed to its
classification decision. To this end, we introduce a framework called the
decision changing rate that helps us quantify the importance of the top words
in classification. Our experimental results show that using counterfactuals can
be helpful.

</details>


### [134] [Small Language Models for Emergency Departments Decision Support: A Benchmark Study](https://arxiv.org/abs/2510.04032)
*Zirui Wang,Jiajun Wu,Braden Teitge,Jessalyn Holodinsky,Steve Drew*

Main category: cs.CL

TL;DR: This paper evaluates the potential of small language models (SLMs) for supporting emergency department (ED) decision-making, finding that general-domain SLMs outperform medically fine-tuned counterparts.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by the need for efficient, cost-effective, and privacy-conscious language models to assist physicians in high-stakes environments like EDs.

Method: The authors designed a benchmark using datasets like MedMCQA, MedQA-4Options, and PubMedQA, to evaluate SLMs trained on general-domain and medical corpora.

Result: General-domain SLMs surprisingly performed better than medically fine-tuned models across the diverse benchmarks.

Conclusion: Specialized medical fine-tuning is not necessarily required for SLMs to support ED decision-making effectively.

Abstract: Large language models (LLMs) have become increasingly popular in medical
domains to assist physicians with a variety of clinical and operational tasks.
Given the fast-paced and high-stakes environment of emergency departments
(EDs), small language models (SLMs), characterized by a reduction in parameter
count compared to LLMs, offer significant potential due to their inherent
reasoning capability and efficient performance. This enables SLMs to support
physicians by providing timely and accurate information synthesis, thereby
improving clinical decision-making and workflow efficiency. In this paper, we
present a comprehensive benchmark designed to identify SLMs suited for ED
decision support, taking into account both specialized medical expertise and
broad general problem-solving capabilities. In our evaluations, we focus on
SLMs that have been trained on a mixture of general-domain and medical corpora.
A key motivation for emphasizing SLMs is the practical hardware limitations,
operational cost constraints, and privacy concerns in the typical real-world
deployments. Our benchmark datasets include MedMCQA, MedQA-4Options, and
PubMedQA, with the medical abstracts dataset emulating tasks aligned with real
ED physicians' daily tasks. Experimental results reveal that general-domain
SLMs surprisingly outperform their medically fine-tuned counterparts across
these diverse benchmarks for ED. This indicates that for ED, specialized
medical fine-tuning of the model may not be required.

</details>


### [135] [Exploring Chain-of-Thought Reasoning for Steerable Pluralistic Alignment](https://arxiv.org/abs/2510.04045)
*Yunfan Zhang,Kathleen McKeown,Smaranda Muresan*

Main category: cs.CL

TL;DR: The paper explores using Chain-of-Thought (CoT) reasoning methods to enable Large Language Models (LLMs) to adopt steerable pluralistic perspectives. Among the evaluated approaches, RLVR shows the best performance.


<details>
  <summary>Details</summary>
Motivation: There is a need for LLMs to support steerable pluralism to align outputs with specific human perspectives, as current models are trained to reflect uniform values.

Method: The authors investigate CoT reasoning techniques, exploring CoT prompting, fine-tuning (on human-authored and synthetic explanations), and Reinforcement Learning with Verifiable Rewards (RLVR).

Result: RLVR consistently outperformed other methods in adopting pluralistic reasoning and demonstrated better training sample efficiency.

Conclusion: The study highlights RLVR's effectiveness in integrating pluralism into LLMs through reliable training methods, ensuring faithfulness and safety in generated output.

Abstract: Large Language Models (LLMs) are typically trained to reflect a relatively
uniform set of values, which limits their applicability to tasks that require
understanding of nuanced human perspectives. Recent research has underscored
the importance of enabling LLMs to support steerable pluralism -- the capacity
to adopt a specific perspective and align generated outputs with it. In this
work, we investigate whether Chain-of-Thought (CoT) reasoning techniques can be
applied to building steerable pluralistic models. We explore several methods,
including CoT prompting, fine-tuning on human-authored CoT, fine-tuning on
synthetic explanations, and Reinforcement Learning with Verifiable Rewards
(RLVR). We evaluate these approaches using the Value Kaleidoscope and OpinionQA
datasets. Among the methods studied, RLVR consistently outperforms others and
demonstrates strong training sample efficiency. We further analyze the
generated CoT traces with respect to faithfulness and safety.

</details>


### [136] [What Makes Diffusion Language Models Super Data Learners?](https://arxiv.org/abs/2510.04071)
*Zitian Gao,Haoming Luo,Lynx Chen,Jason Klein Liu,Ran Tao,Joey Zhou,Bryan Dai*

Main category: cs.CL

TL;DR: This paper investigates why diffusion language models perform well with limited data, identifying stochastic regularization and random masking as key factors.


<details>
  <summary>Details</summary>
Motivation: To understand the mechanisms behind data efficiency in diffusion language models, which have shown remarkable performance under data-scarce conditions.

Method: Ablation experiments were conducted to disentangle the contributions of different factors, focusing on random masking, MLP dropout, and weight decay.

Result: Findings revealed that random masking of input tokens plays a dominant role, while MLP dropout and weight decay yield similar improvements.

Conclusion: Stochastic regularization techniques significantly enhance the data efficiency of diffusion models during multi-epoch training.

Abstract: Recent studies have shown that diffusion language models achieve remarkable
data efficiency under limited-data constraints, yet the underlying mechanisms
remain unclear. In this work, we perform extensive ablation experiments to
disentangle the sources of this efficiency. Our results show that random
masking of input tokens plays the dominant role. We further show that similar
gains can be obtained through in MLP dropout and weight decay, indicating that
stochastic regularization broadly enhances data efficiency in multi-epoch
training. Our code is available at
https://github.com/zitian-gao/data-efficiency.

</details>


### [137] [PoLi-RL: A Point-to-List Reinforcement Learning Framework for Conditional Semantic Textual Similarity](https://arxiv.org/abs/2510.04080)
*Zixin Song,Bowen Zhang,Qian-Wen Zhang,Di Yin,Xing Sun,Chunping Li*

Main category: cs.CL

TL;DR: The paper introduces a novel RL framework called PoLi-RL to improve Conditional Semantic Textual Similarity (C-STS) and achieves state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: To address limitations in existing C-STS methods that fail to leverage RL effectively for scoring semantic proximity under conditional contexts.

Method: Introduces a two-stage curriculum with pointwise, pairwise, and listwise rewards and a Parallel Slice Ranking Reward (PSRR) mechanism for granular optimization.

Result: PoLi-RL sets a new benchmark with a 48.18 Spearman correlation on the C-STS task.

Conclusion: This work establishes RL as a viable method for conditional textual ranking tasks and provides a precise framework for future improvements.

Abstract: Conditional Semantic Textual Similarity (C-STS) measures the semantic
proximity between text segments under a specific condition, thereby overcoming
the ambiguity inherent in traditional STS. However, existing methods are
largely confined to discriminative models, failing to fully integrate recent
breakthroughs in the NLP community concerning Large Language Models (LLMs) and
Reinforcement Learning (RL). RL is a particularly well-suited paradigm for this
task, as it can directly optimize the non-differentiable Spearman ranking
metric and guide the reasoning process required by C-STS. However, we find that
naively applying listwise RL fails to produce meaningful improvements, as the
model is overwhelmed by complex, coarse-grained reward signals. To address this
challenge, we introduce PoLi-RL, a novel Point-to-List Reinforcement Learning
framework. PoLi-RL employs a two-stage curriculum: it first trains the model
with simple pointwise rewards to establish fundamental scoring capabilities,
then transitions to a hybrid reward that combines pointwise, pairwise, and
listwise objectives to refine the model's ability to discern subtle semantic
distinctions. Crucially, we propose an innovative Parallel Slice Ranking Reward
(PSRR) mechanism that computes ranking rewards in parallel slices, where each
slice comprises same-indexed completions from different samples. This provides
a precise, differentiated learning signal for each individual completion,
enabling granular credit assignment and effective optimization. On the official
C-STS benchmark, PoLi-RL achieves a Spearman correlation coefficient of 48.18,
establishing a new SOTA for the cross-encoder architecture. As the first work
to successfully apply RL to C-STS, our study introduces a powerful and precise
paradigm for training LLMs on complex, ranking-based conditional judgment
tasks.

</details>


### [138] [Unveiling LLMs' Metaphorical Understanding: Exploring Conceptual Irrelevance, Context Leveraging and Syntactic Influence](https://arxiv.org/abs/2510.04120)
*Fengying Ye,Shanshan Wang,Lidia S. Chao,Derek F. Wong*

Main category: cs.CL

TL;DR: The paper evaluates how well Large Language Models (LLMs) understand metaphors, revealing limitations in their capabilities in concept mapping, metaphorical knowledge, and syntactic sensitivity.


<details>
  <summary>Details</summary>
Motivation: Metaphors play a significant role in human communication, but the mechanisms through which LLMs process metaphors remain underexplored, necessitating a focused study on their comprehension abilities.

Method: The study investigates metaphor processing in LLMs via three approaches: (1) Concept Mapping with embedding space projections, (2) Metaphor-Literal Repository analysis, and (3) Syntactic Sensitivity assessment.

Result: LLMs produce 15-25% conceptually irrelevant outputs, rely on metaphor indicators in training data over contextual signals, and struggle more with syntactic irregularities than structural comprehension.

Conclusion: LLMs exhibit notable limitations in processing metaphors, highlighting the need for improved computational models to enhance metaphor understanding.

Abstract: Metaphor analysis is a complex linguistic phenomenon shaped by context and
external factors. While Large Language Models (LLMs) demonstrate advanced
capabilities in knowledge integration, contextual reasoning, and creative
generation, their mechanisms for metaphor comprehension remain insufficiently
explored. This study examines LLMs' metaphor-processing abilities from three
perspectives: (1) Concept Mapping: using embedding space projections to
evaluate how LLMs map concepts in target domains (e.g., misinterpreting "fall
in love" as "drop down from love"); (2) Metaphor-Literal Repository: analyzing
metaphorical words and their literal counterparts to identify inherent
metaphorical knowledge; and (3) Syntactic Sensitivity: assessing how
metaphorical syntactic structures influence LLMs' performance. Our findings
reveal that LLMs generate 15\%-25\% conceptually irrelevant interpretations,
depend on metaphorical indicators in training data rather than contextual cues,
and are more sensitive to syntactic irregularities than to structural
comprehension. These insights underline the limitations of LLMs in metaphor
analysis and call for more robust computational approaches.

</details>


### [139] [Sri Lanka Document Datasets: A Large-Scale, Multilingual Resource for Law, News, and Policy (v20251005)](https://arxiv.org/abs/2510.04124)
*Nuwan I. Senaratna*

Main category: cs.CL

TL;DR: This paper presents a comprehensive dataset collection of 215,670 documents in Sinhala, Tamil, and English, covering topics like parliamentary proceedings, legal judgments, and more, to support linguistic and socio-political research.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the need for open, machine-readable datasets to advance multilingual NLP and related computational research fields, particularly focusing on under-resourced languages like Sinhala and Tamil.

Method: The authors detail the data sources, daily update pipeline, and the mirroring of datasets on platforms like GitHub and Hugging Face.

Result: They produced 13 datasets (60.3 GB) comprising 215,670 documents in three languages, with updates and ethical considerations maintained for broad usability.

Conclusion: These datasets can aid research in multiple fields, such as computational linguistics and socio-political analysis, while emphasizing ethical use and licensing compliance.

Abstract: We present a collection of open, machine-readable document datasets covering
parliamentary proceedings, legal judgments, government publications, news, and
tourism statistics from Sri Lanka. As of v20251005, the collection currently
comprises 215,670 documents (60.3 GB) across 13 datasets in Sinhala, Tamil, and
English. The datasets are updated daily and mirrored on GitHub and Hugging
Face. These resources aim to support research in computational linguistics,
legal analytics, socio-political studies, and multilingual natural language
processing. We describe the data sources, collection pipeline, formats, and
potential use cases, while discussing licensing and ethical considerations.

</details>


### [140] [Fine Tuning Methods for Low-resource Languages](https://arxiv.org/abs/2510.04139)
*Tim Bakkenes,Daniel Wang,Anton Johansson*

Main category: cs.CL

TL;DR: The paper addresses the exclusivity of Large Language Models (LLMs) trained mainly on English, proposing a method to adapt models like Gemma 2 for underrepresented languages and cultures.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the lack of inclusivity in Large Language Models, which are predominantly trained on English-focused datasets and exhibit poor performance in other languages and cultural contexts.

Method: The researchers developed a generalizable method for creating culturally relevant datasets and applied it to post-train the Gemma 2 model to better support an underrepresented language.

Result: The project successfully enhanced Gemma 2's performance for a specific underrepresented language, demonstrating the feasibility of their method.

Conclusion: The work highlights a method for adapting LLMs to underrepresented languages, empowering local efforts to utilize generative AI while preserving unique cultural heritage.

Abstract: The rise of Large Language Models has not been inclusive of all cultures. The
models are mostly trained on English texts and culture which makes them
underperform in other languages and cultural contexts. By developing a
generalizable method for preparing culturally relevant datasets and
post-training the Gemma 2 model, this project aimed to increase the performance
of Gemma 2 for an underrepresented language and showcase how others can do the
same to unlock the power of Generative AI in their country and preserve their
cultural heritage.

</details>


### [141] [Self Speculative Decoding for Diffusion Large Language Models](https://arxiv.org/abs/2510.04147)
*Yifeng Gao,Ziang Ji,Yuxuan Wang,Biqing Qi,Hanlin Xu,Linfeng Zhang*

Main category: cs.CL

TL;DR: This paper introduces Self-Speculative Decoding (SSD), a method to accelerate inference in diffusion-based large language models (dLLMs), eliminating performance degradation and model redundancy during parallel generation.


<details>
  <summary>Details</summary>
Motivation: Diffusion-based large language models (dLLMs) are promising, but their parallel decoding methods introduce generation deviations compared to stepwise decoding, causing performance limitations for practical use.

Method: The authors propose SSD, which uses dLLMs for both speculative drafting and verification without requiring auxiliary modules. Through a hierarchical verification tree and leveraging parallel prediction, the model verifies and accepts multiple tokens in one forward pass.

Result: Experiments show that SSD achieves up to 3.46× speedup while maintaining identical output compared to stepwise decoding for language models like LLaDA and Dream.

Conclusion: SSD improves inference speed without loss of output quality, making dLLMs more viable for deployment with reduced model redundancy and memory overhead. The code will be shared publicly.

Abstract: Diffusion-based Large Language Models (dLLMs) have emerged as a competitive
alternative to autoregressive models, offering unique advantages through
bidirectional attention and parallel generation paradigms. However, the
generation results of current parallel decoding methods deviate from stepwise
decoding, introducing potential performance degradation, which limits their
practical deployment. To address this problem, we propose \textbf{S}elf
\textbf{S}peculative \textbf{D}ecoding (SSD), a lossless inference acceleration
method that leverages the dLLM itself as both speculative decoding drafter and
verifier without auxiliary modules. SSD introduces a self-drafting mechanism
where the model generates predictions for multiple positions, then verifies
them through hierarchical verification trees in a single forward pass. Unlike
traditional speculative decoding that requires separate draft models, SSD
eliminates model redundancy and memory overhead by exploiting the dLLM's
inherent parallel prediction capability for multiple positions. This
self-speculative approach allows the model to progressively verify and accept
multiple tokens in a single forward pass. Our experiments demonstrate that SSD
achieves up to 3.46$\times$ speedup while keeping the output identical to
stepwise decoding on open source models such as LLaDA and Dream. Code will be
made publicly available on GitHub.

</details>


### [142] [Thinking on the Fly: Test-Time Reasoning Enhancement via Latent Thought Policy Optimization](https://arxiv.org/abs/2510.04182)
*Wengao Ye,Yan Liang,Lianlei Shan*

Main category: cs.CL

TL;DR: Latent Thought Policy Optimization (LTPO) leverages dynamic optimization of intermediate latent vectors to enhance LLM reasoning during test instances, outperforming baselines in robustness and accuracy on challenging reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Latent reasoning in LLMs, while efficient, struggles with robustness on challenging, out-of-distribution tasks where effective reasoning is essential.

Method: LTPO optimizes latent thought vectors dynamically at test time using an online policy gradient guided by intrinsic rewards derived from LLM’s confidence, without model parameter updates or external supervision.

Result: LTPO outperforms baselines, particularly excelling in robustness on benchmarks like AIME, where alternative approaches show near-zero accuracy.

Conclusion: The framework demonstrates superior adaptability and efficacy in complex reasoning scenarios, marking a significant step in improving LLM reasoning robustness.

Abstract: Recent advancements in Large Language Models (LLMs) have shifted from
explicit Chain-of-Thought (CoT) reasoning to more efficient latent reasoning,
where intermediate thoughts are represented as vectors rather than text.
However, latent reasoning can be brittle on challenging, out-of-distribution
tasks where robust reasoning is most critical. To overcome these limitations,
we introduce Latent Thought Policy Optimization (LTPO), a parameter-free
framework that enhances LLM reasoning entirely at test time, without requiring
model parameter updates. LTPO treats intermediate latent "thought" vectors as
dynamic parameters that are actively optimized for each problem instance. It
employs an online policy gradient method guided by an intrinsic,
confidence-based reward signal computed directly from the frozen LLM's own
output distributions, eliminating the need for external supervision or
expensive text generation during optimization. Extensive experiments on five
reasoning benchmarks show that LTPO not only matches or surpasses strong
baselines on standard tasks but also demonstrates remarkable robustness where
others fail. Most notably, on highly challenging AIME benchmarks where existing
latent reasoning baselines collapse to near-zero accuracy, LTPO delivers
substantial improvements, showcasing a unique capability for complex reasoning.

</details>


### [143] [CALM Before the STORM: Unlocking Native Reasoning for Optimization Modeling](https://arxiv.org/abs/2510.04204)
*Zhengyang Tang,Zihan Ye,Chenyu Huang,Xuhan Huang,Chengpeng Li,Sihang Li,Guanhua Chen,Ming Yan,Zizhuo Wang,Hongyuan Zha,Dayiheng Liu,Benyou Wang*

Main category: cs.CL

TL;DR: The paper introduces CALM, a novel framework to enhance modern large reasoning models for optimization tasks using corrective hints and lightweight data synthesis.


<details>
  <summary>Details</summary>
Motivation: Current domain adaptation methods fail to fully utilize the advanced reasoning abilities of modern LRMs for optimization modeling tasks.

Method: CALM uses expert corrective hints to refine reasoning flaws and employs supervised fine-tuning and reinforcement learning, leading to improved reasoning trajectories.

Result: The proposed STORM model (4B parameters), enabled by CALM, achieves state-of-the-art accuracy of 68.9% across benchmarks, comparable to a much larger 671B LRM.

Conclusion: Dynamic, hint-based approaches to adaptation enable scalable and effective performance enhancement of LRMs on optimization tasks, preserving their reasoning capabilities.

Abstract: Large Reasoning Models (LRMs) have demonstrated strong capabilities in
complex multi-step reasoning, opening new opportunities for automating
optimization modeling. However, existing domain adaptation methods, originally
designed for earlier instruction-tuned models, often fail to exploit the
advanced reasoning patterns of modern LRMs -- In particular, we show that
direct fine-tuning on traditional \textit{non-reflective} datasets leads to
limited gains. To fully leverage LRMs' inherent reasoning abilities, we propose
\textbf{CALM} (\textit{Corrective Adaptation with Lightweight Modification}), a
framework that progressively refines LRMs within their native reasoning modes
for optimization modeling tasks. In CALM, an expert intervener identifies
reasoning flaws and provides concise corrective hints, which the LRM
incorporates to produce improved reasoning trajectories. These interventions
modify fewer than 2.6\% of generated tokens, but generate high-quality data for
soft adaptation through supervised fine-tuning. The adapted model is then
further improved through reinforcement learning. Building on CALM, we develop
\textbf{STORM} (\textit{Smart Thinking Optimization Reasoning Model}), a
4B-parameter LRM that achieves a new state-of-the-art average accuracy of
68.9\% across five popular optimization modeling benchmarks, matching the
performance of a 671B LRM. These results demonstrate that dynamic, hint-based
data synthesis both preserves and amplifies the native reasoning patterns of
modern LRMs, offering a more effective and scalable path towards expert-level
performance on challenging optimization modeling tasks.

</details>


### [144] [Teaching LLM to be Persuasive: Reward-Enhanced Policy Optimization for Alignment frm Heterogeneous Rewards](https://arxiv.org/abs/2510.04214)
*Zhuoran Zhuang,Ye Chen,Xia Zeng,Chao Luo,Luhui Liu,Yihan Chen*

Main category: cs.CL

TL;DR: The paper explores using Large Language Models (LLMs) for persuasive price negotiation in online travel agencies (OTAs). It introduces Reward-Enhanced Policy Optimization (REPO), a reinforcement learning framework designed to improve negotiation quality and compliance with business constraints, achieving superior results compared to other optimization methods.


<details>
  <summary>Details</summary>
Motivation: The authors aim to deploy LLMs as agents capable of persuasive and compliant price negotiation in OTAs, addressing critical factors like traveler affordability and hotel profitability. However, conventional approaches struggle with nuanced persuasion and business constraints.

Method: The study introduces REPO, a reinforcement learning post-training framework combining a preference-trained reward model (RM), a persuasion and SOP-compliance reward judge (RJ), and deterministic reward functions (RF). These elements ensure aligned, high-quality dialogues while avoiding reward hacking. The framework uses heterogeneous rewards and combines these signals for optimization.

Result: REPO achieves an average dialogue rating of 4.63, improving significantly over baselines like DPO, GRPO, and SFT. The method fixes 93.33% of bad-case scenarios with a 75.56% clean fix rate while showcasing emergent capabilities such as proactive empathy and calibrated negotiation tactics.

Conclusion: REPO provides a robust framework for persuasive negotiation in OTAs, outperforming existing methods in quality, compliance, and alignment. The approach also reveals emergent negotiation capabilities, making it suitable for practical deployment in business contexts.

Abstract: We study deploying large language models (LLMs) as business development (BD)
agents for persuasive price negotiation in online travel agencies (OTAs), where
aligning traveler affordability and hotel profitability directly affects
bookings, partner relationships, and access to travel. The agent must follow a
Standard Operating Procedure (SOP) while conducting multi-turn persuasion,
interpreting colloquial inputs, and adhering to guardrails (no over-promising,
no hallucinations). Conventional post-training -- supervised fine-tuning (SFT)
or single-source reward optimization -- overfits scripts, misses nuanced
persuasive style, and fails to enforce verifiable business constraints.
  We propose Reward-Enhanced Policy Optimization (REPO), a reinforcement
learning post-training framework that aligns an LLM with heterogeneous rewards:
a preference-trained reward model (RM) for dense human alignment, a reward
judge (RJ) for high-level persuasive behavior and SOP compliance, and
programmatic reward functions (RF) for deterministic checks on numerics,
formatting, and guardrails. A straightforward enhancement mechanism is proposed
to combine the RM with RJ and RF signals to curb reward hacking and improve
negotiation quality. In production-style evaluations -- approximately 150 turns
from real dialogues and 225 turns from curated bad-case dialogues -- REPO lifts
average dialogue rating to 4.63: +1.20 over base, +0.83 over Direct Preference
Optimization (DPO); +0.33 over Group Relative Policy Optimization (GRPO),
increases the share of conversations with at least one excellent response to
66.67% (+23.34 percentage points over GRPO), and achieves a 93.33% bad-case fix
rate with 75.56% clean fixes, outperforming SFT, DPO, PPO, and GRPO. We also
observe emergent capabilities -- proactive empathy, localized reasoning,
calibrated tactics -- that surpass gold annotations.

</details>


### [145] [Epistemic Diversity and Knowledge Collapse in Large Language Models](https://arxiv.org/abs/2510.04226)
*Dustin Wright,Sarah Masud,Jared Moore,Srishti Yadav,Maria Antoniak,Chan Young Park,Isabelle Augenstein*

Main category: cs.CL

TL;DR: This paper investigates the homogeneity in texts generated by large language models (LLMs) and its risk of knowledge collapse. The study presents a methodology to measure epistemic diversity in LLM-generated content.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of existing studies on LLM homogenization, which lack a focus on epistemic diversity trends across time and cultural contexts, and to propose new methods for measuring such diversity.

Method: The study tests 27 LLMs on 155 topics across 12 countries with 200 prompt variations from real user chats. It uses a novel methodology to measure epistemic diversity in the generated outputs and analyses the influence of model size and retrieval-augmented generation (RAG) on diversity.

Result: Newer models were found to produce more epistemically diverse claims, but LLMs generally performed worse than basic web searches in diversity. Larger models reduced diversity, while RAG had a positive but culturally dependent impact on epistemic diversity.

Conclusion: The research highlights the epistemic limitations of LLMs, with model outputs favoring English-dominant claims over local cultural contexts. This underlines the need for better methods to enhance diversity and cultural representation in LLMs.

Abstract: Large language models (LLMs) tend to generate lexically, semantically, and
stylistically homogenous texts. This poses a risk of knowledge collapse, where
homogenous LLMs mediate a shrinking in the range of accessible information over
time. Existing works on homogenization are limited by a focus on closed-ended
multiple-choice setups or fuzzy semantic features, and do not look at trends
across time and cultural contexts. To overcome this, we present a new
methodology to measure epistemic diversity, i.e., variation in real-world
claims in LLM outputs, which we use to perform a broad empirical study of LLM
knowledge collapse. We test 27 LLMs, 155 topics covering 12 countries, and 200
prompt variations sourced from real user chats. For the topics in our study, we
show that while newer models tend to generate more diverse claims, nearly all
models are less epistemically diverse than a basic web search. We find that
model size has a negative impact on epistemic diversity, while
retrieval-augmented generation (RAG) has a positive impact, though the
improvement from RAG varies by the cultural context. Finally, compared to a
traditional knowledge source (Wikipedia), we find that country-specific claims
reflect the English language more than the local one, highlighting a gap in
epistemic representation

</details>


### [146] [A Low-Resource Speech-Driven NLP Pipeline for Sinhala Dyslexia Assistance](https://arxiv.org/abs/2510.04750)
*Peshala Perera,Deshan Sumanathilaka*

Main category: cs.CL

TL;DR: This paper introduces an assistive system for Sinhala-speaking adults with dyslexia, using NLP models for transcription, error correction, and feedback.


<details>
  <summary>Details</summary>
Motivation: To address the lack of research and tools for adults with dyslexia in low-resource and non-English-speaking contexts like Sinhala.

Method: The system combines Whisper for speech-to-text, SinBERT for error detection, mT5 and Mistral models for text correction, and gTTS for speech output.

Result: Achieved a transcription accuracy of 0.66, a correction accuracy of 0.7, and an overall system accuracy of 0.65 despite limited Sinhala datasets.

Conclusion: The study proves the feasibility and value of creating inclusive NLP tools for underserved languages and populations.

Abstract: Dyslexia in adults remains an under-researched and under-served area,
particularly in non-English-speaking contexts, despite its significant impact
on personal and professional lives. This work addresses that gap by focusing on
Sinhala, a low-resource language with limited tools for linguistic
accessibility. We present an assistive system explicitly designed for
Sinhala-speaking adults with dyslexia. The system integrates Whisper for
speech-to-text conversion, SinBERT, an open-sourced fine-tuned BERT model
trained for Sinhala to identify common dyslexic errors, and a combined mT5 and
Mistral-based model to generate corrected text. Finally, the output is
converted back to speech using gTTS, creating a complete multimodal feedback
loop. Despite the challenges posed by limited Sinhala-language datasets, the
system achieves 0.66 transcription accuracy and 0.7 correction accuracy with
0.65 overall system accuracy. These results demonstrate both the feasibility
and effectiveness of the approach. Ultimately, this work highlights the
importance of inclusive Natural Language Processing (NLP) technologies in
underrepresented languages and showcases a practical

</details>


### [147] [Pushing on Multilingual Reasoning Models with Language-Mixed Chain-of-Thought](https://arxiv.org/abs/2510.04230)
*Guijin Son,Donghun Yang,Hitesh Laxmichand Patel,Amit Agarwal,Hyunwoo Ko,Chanuk Lim,Srikant Panda,Minhyuk Kim,Nikunj Drolia,Dasol Choi,Kyong-Ha Lee,Youngjae Yu*

Main category: cs.CL

TL;DR: The paper introduces a Language-Mixed CoT reasoning schema that combines English and a target language to improve language-specific reasoning. With a focus on Korean, they curate the Yi-Sang dataset and train models, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve language-specific reasoning in models, especially for non-English languages, and address the gap in utilizing English as an anchor for better reasoning.

Method: The authors developed a Language-Mixed CoT approach, curated a large Korean-specific dataset called Yi-Sang, and trained various models to evaluate their effectiveness. They used diverse benchmarks and performed ablations to study the outcomes.

Result: Their KO-REAson-35B model achieved state-of-the-art performance in Korean-specific reasoning, ranking first in 5/9 benchmarks and achieving substantial improvements across models of all sizes.

Conclusion: The Language-Mixed CoT approach is more effective than monolingual CoT. It not only enhances language-specific reasoning but also offers cross-lingual and multi-modal benefits. The open release of their datasets and models advances research in this area.

Abstract: Recent frontier models employ long chain-of-thought reasoning to explore
solution spaces in context and achieve stonger performance. While many works
study distillation to build smaller yet capable models, most focus on English
and little is known about language-specific reasoning. To bridge this gap, we
first introduct **Language-Mixed CoT**, a reasoning schema that switches
between English and a target language, using English as an anchor to excel in
reasoning while minimizing translation artificats. As a Korean case study, we
curate **Yi-Sang**: 5.79M native-Korean prompts from web Q&A, exams, STEM, and
code; 3.7M long reasoning traces generated from Qwen3-32B; and a targeted 260k
high-yield subset. We train ninve models (4B-35B) across six families (Qwen2.5,
Llama-3.1, Gemma-3, etc). Our best model, **KO-REAson-35B**, achieves
state-of-the-art performance, with the highest overall average score (64.0 \pm
25), ranking first on 5/9 benchmarks and second on the remainder. Samller and
mid-sized models also benefit substantially, with an average improvement of
+18.6 points across teh evaluated nine benchmarks. Ablations show
**Language-Mixed CoT** is more effective than monolingual CoT, also resulting
in cross-lingual and mult-modal performance gains. We release our data-curation
pipeline, evaluation system, datasets, and models to advance research on
language-specific reasoning. Data and model collection:
https://huggingface.co/KOREAson.

</details>


### [148] [LongTail-Swap: benchmarking language models' abilities on rare words](https://arxiv.org/abs/2510.04268)
*Robin Algayres,Charles-Éric Saint-James,Mahi Luthra,Jiayi Shen,Dongyan Lin,Youssef Benchekroun,Rashel Moritz,Juan Pino,Emmanuel Dupoux*

Main category: cs.CL

TL;DR: This study introduces LT-Swap, a benchmark focused on evaluating how language models handle rare word learning in low-data scenarios, inspired by infant learning.


<details>
  <summary>Details</summary>
Motivation: Explore language model performance in learning rare words with minimal exposure, like infants, as existing BabyLM challenge metrics focus on frequent words.

Method: LT-Swap compares acceptable vs. unacceptable sentence pairs tied to rare word semantic and syntactic usage in a zero-shot evaluation using log probabilities.

Result: Two test sets were created for 10M and 100M word BabyLM training sets, evaluating 16 models. Models showed poor performance on rare words, and architecture differences were pronounced for tail distributions.

Conclusion: LT-Swap highlights limitations in rare word generalization across language models, offering insights into architectural strengths for improving rare word handling.

Abstract: Children learn to speak with a low amount of data and can be taught new words
on a few-shot basis, making them particularly data-efficient learners. The
BabyLM challenge aims at exploring language model (LM) training in the low-data
regime but uses metrics that concentrate on the head of the word distribution.
Here, we introduce LongTail-Swap (LT-Swap), a benchmark that focuses on the
tail of the distribution, i.e., measures the ability of LMs to learn new words
with very little exposure, like infants do. LT-Swap is a pretraining
corpus-specific test set of acceptable versus unacceptable sentence pairs that
isolate semantic and syntactic usage of rare words. Models are evaluated in a
zero-shot fashion by computing the average log probabilities over the two
members of each pair. We built two such test sets associated with the 10M words
and 100M words BabyLM training sets, respectively, and evaluated 16 models from
the BabyLM leaderboard. Our results not only highlight the poor performance of
language models on rare words but also reveal that performance differences
across LM architectures are much more pronounced in the long tail than in the
head. This offers new insights into which architectures are better at handling
rare word generalization. We've also made the code publicly avail

</details>


### [149] [Probing Geometry of Next Token Prediction Using Cumulant Expansion of the Softmax Entropy](https://arxiv.org/abs/2510.04285)
*Karthik Viswanathan,Sang Eon Park*

Main category: cs.CL

TL;DR: This paper introduces a cumulant-expansion framework to analyze how large language models (LLMs) capture higher-order statistical structures during next-token prediction.


<details>
  <summary>Details</summary>
Motivation: The aim is to understand and quantify how LLMs internalize complex statistical structures while predicting the next token.

Method: The authors use cumulant-expansion analysis, focusing on softmax entropy perturbations, to isolate higher-order statistical features. They tested this on GPT-2 and Pythia models using Pile-10K prompts.

Result: The study revealed that the cumulant profiles depend on contextual meaning, showed monotonically increasing cumulants during training, and distinctively analyzed processing differences between mathematical and linguistic prompts.

Conclusion: Cumulant analysis emerges as a lightweight, mathematically grounded method to probe how neural networks learn and represent complex data structures.

Abstract: We introduce a cumulant-expansion framework for quantifying how large
language models (LLMs) internalize higher-order statistical structure during
next-token prediction. By treating the softmax entropy of each layer's logit
distribution as a perturbation around its "center" distribution, we derive
closed-form cumulant observables that isolate successively higher-order
correlations. Empirically, we track these cumulants in GPT-2 and Pythia models
on Pile-10K prompts. (i) Structured prompts exhibit a characteristic
rise-and-plateau profile across layers, whereas token-shuffled prompts remain
flat, revealing the dependence of the cumulant profile on meaningful context.
(ii) During training, all cumulants increase monotonically before saturating,
directly visualizing the model's progression from capturing variance to
learning skew, kurtosis, and higher-order statistical structures. (iii)
Mathematical prompts show distinct cumulant signatures compared to general
text, quantifying how models employ fundamentally different processing
mechanisms for mathematical versus linguistic content. Together, these results
establish cumulant analysis as a lightweight, mathematically grounded probe of
feature-learning dynamics in high-dimensional neural networks.

</details>


### [150] [SliceMoE: Routing Embedding Slices Instead of Tokens for Fine-Grained and Balanced Transformer Scaling](https://arxiv.org/abs/2510.04286)
*Harshil Vejendla*

Main category: cs.CL

TL;DR: SliceMoE introduces a more efficient routing system for Mixture-of-Experts models by splitting hidden vectors into slices and routing them separately. This improves inference speed, perplexity, and expert specialization.


<details>
  <summary>Details</summary>
Motivation: Mixture-of-Experts layers in transformers suffer from inefficiencies like token-level bottlenecks, uneven load balancing, and limited specialization in expert layers.

Method: SliceMoE partitions token embeddings into slices, with each slice routed to experts using shared lightweight routers. Slice outputs are reassembled to maintain token-level computation efficiency.

Result: SliceMoE achieves up to 1.7x faster inference compared to dense baselines, reduces perplexity by 12–18% versus token-MoE, and enhances expert balance with interpretable specialization.

Conclusion: SliceMoE is an effective enhancement to token-based MoE architectures, offering smoother utilization, improved model efficiency, and interpretable routing expertise.

Abstract: Mixture-of-Experts (MoE) layers scale transformers by routing tokens to a
sparse subset of feed-forward experts. Token-level routing, however, assigns an
entire semantic spectrum to each expert, creating capacity bottlenecks,
load-balancing pathologies, and limited specialization. We introduce SliceMoE,
an architecture that routes contiguous slices of a token's hidden vector. A
d-dimensional embedding is partitioned into S slices, and for each slice, a
lightweight shared router predicts the top-k experts. Experts operate on their
assigned slices independently, and outputs are reassembled, maintaining
per-token FLOP efficiency. Because slices from different tokens interleave
within an expert, utilization is naturally smoother. We propose a slice-level
capacity loss, cross-slice dropout, and efficient fused batched GEMM kernels.
Experiments on WikiText-103 language modeling, WMT En-De translation, and three
text-classification datasets show SliceMoE attains up to 1.7x faster inference
than dense baselines, 12 to 18 percent lower perplexity than parameter-matched
token-MoE, and improved expert balance, with interpretable expertise over
syntactic versus semantic subspaces.

</details>


### [151] [PABSA: Hybrid Framework for Persian Aspect-Based Sentiment Analysis](https://arxiv.org/abs/2510.04291)
*Mehrzad Tareh,Aydin Mohandesi,Ebrahim Ansari*

Main category: cs.CL

TL;DR: The paper proposes a hybrid approach using machine learning and deep learning for Persian sentiment analysis, achieving state-of-the-art accuracy.


<details>
  <summary>Details</summary>
Motivation: The study aims to tackle challenges in Persian sentiment analysis caused by limited labeled datasets, preprocessing tools, embeddings, and feature extraction methods.

Method: The approach combines multilingual BERT for polarity scoring and a decision tree classifier, augmented by a Persian synonym and entity dictionary for text enhancements.

Result: Achieved an accuracy of 93.34%, outperforming existing benchmarks on the Pars-ABSA dataset.

Conclusion: Hybrid modeling and feature augmentation can significantly improve sentiment analysis for low-resource languages like Persian.

Abstract: Sentiment analysis is a key task in Natural Language Processing (NLP),
enabling the extraction of meaningful insights from user opinions across
various domains. However, performing sentiment analysis in Persian remains
challenging due to the scarcity of labeled datasets, limited preprocessing
tools, and the lack of high-quality embeddings and feature extraction methods.
To address these limitations, we propose a hybrid approach that integrates
machine learning (ML) and deep learning (DL) techniques for Persian
aspect-based sentiment analysis (ABSA). In particular, we utilize polarity
scores from multilingual BERT as additional features and incorporate them into
a decision tree classifier, achieving an accuracy of 93.34%-surpassing existing
benchmarks on the Pars-ABSA dataset. Additionally, we introduce a Persian
synonym and entity dictionary, a novel linguistic resource that supports text
augmentation through synonym and named entity replacement. Our results
demonstrate the effectiveness of hybrid modeling and feature augmentation in
advancing sentiment analysis for low-resource languages such as Persian.

</details>


### [152] [Equipping Retrieval-Augmented Large Language Models with Document Structure Awareness](https://arxiv.org/abs/2510.04293)
*Lingnan Xu,Chong Feng,Kaiyuan Zhang,Liu Zhengyong,Wenqiang Xu,Fanqing Meng*

Main category: cs.CL

TL;DR: RDR2 enhances Retrieval-Augmented Generation (RAG) by integrating structural information from documents, improving knowledge accuracy and multi-document synthesis.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the limitations of existing RAG systems that fail to utilize structural information from documents, leading to inefficiencies in knowledge extraction and factual inaccuracies.

Method: The authors propose the RDR2 framework, leveraging an LLM-based router to navigate document structure trees, assemble optimal evidence dynamically, and formulate document routing as a trainable task.

Result: RDR2 outperforms existing methods across five datasets, showing superior ability to synthesize information, especially in complex multi-document scenarios.

Conclusion: Explicit incorporation of structural awareness improves RAG systems' knowledge acquisition, offering a robust solution for tasks requiring accurate and synthesized information from multiple documents.

Abstract: While large language models (LLMs) demonstrate impressive capabilities, their
reliance on parametric knowledge often leads to factual inaccuracies.
Retrieval-Augmented Generation (RAG) mitigates this by leveraging external
documents, yet existing approaches treat retrieved passages as isolated chunks,
ignoring valuable structure that is crucial for document organization.
Motivated by this gap, we propose Retrieve-DocumentRoute-Read (RDR2), a novel
framework that explicitly incorporates structural information throughout the
RAG process. RDR2 employs an LLM-based router to dynamically navigate document
structure trees, jointly evaluating content relevance and hierarchical
relationships to assemble optimal evidence. Our key innovation lies in
formulating document routing as a trainable task, with automatic action
curation and structure-aware passage selection inspired by human reading
strategies. Through comprehensive evaluation on five challenging datasets, RDR2
achieves state-of-the-art performance, demonstrating that explicit structural
awareness significantly enhances RAG systems' ability to acquire and utilize
knowledge, particularly in complex scenarios requiring multi-document
synthesis.

</details>


### [153] [Measuring Language Model Hallucinations Through Distributional Correctness](https://arxiv.org/abs/2510.04302)
*Thomas F Burns*

Main category: cs.CL

TL;DR: The paper introduces the Distributional Correctness Score (DCS) to evaluate language models more effectively, considering their entire probability distribution of answers and capturing nuances like uncertainty and overconfidence.


<details>
  <summary>Details</summary>
Motivation: Existing evaluation paradigms for language models inadequately assess their belief states, often incentivizing inaccurate responses due to binary scoring schemes and neglecting nuanced representations of uncertainty.

Method: The authors propose a new evaluation metric, DCS, which evaluates language models based on their entire probability distribution over answer choices, emphasizing distinctions like overconfidence in wrong answers and genuine uncertainty.

Result: Testing DCS across six models and 12 benchmarks revealed negative scores for half the benchmarks, highlighting prevalent hallucination tendencies in current models.

Conclusion: DCS offers a refined evaluation framework that rewards genuine uncertainty expressions over guessing, aiming to align model behavior with more reliable uncertainty representation.

Abstract: Common evaluation paradigms for language models focus on scoring single
responses through accuracy metrics or proper scoring rules, failing to capture
the full richness of a model's belief state. Recent work illustrates that
language models hallucinate in-part because they are optimised to be good
test-takers under binary scoring schemes that reward any answer over
abstention. While this insight naturally leads to penalty-based approaches,
they ignore crucial distinctions in how models distribute uncertainty, for
example between hedging toward incorrect answers versus hedging toward "I don't
know" responses. A novel evaluation metric, the Distributional Correctness
Score (DCS), is introduced to solve this problem, i.e., of not considering a
model's entire probability distribution over answer choices. DCS naturally
distinguishes between harmful overconfidence in wrong answers and uncertainty
expressed through abstention, providing scores in an interpretable default
range. Through theoretical analysis and illustrative examples, DCS is
demonstrated to offer a more nuanced and aligned evaluation paradigm that
incentivises models to express genuine uncertainty rather than guessing.
Adapting 12 existing evaluation benchmarks to DCS's variants and measuring
performance on six language models reveals that for half of the tested
benchmarks scores are negative across all tested models, indicating significant
tendencies towards hallucination.

</details>


### [154] [Read the Scene, Not the Script: Outcome-Aware Safety for LLMs](https://arxiv.org/abs/2510.04320)
*Rui Wu,Yihao Quan,Zeru Shi,Zhenting Wang,Yanshu Li,Ruixiang Tang*

Main category: cs.CL

TL;DR: Safety-aligned Large Language Models (LLMs) fail in two ways: being easily jailbroken or over-refusing harmless inputs, due to weak reasoning about actions' consequences. The authors develop CB-Bench to evaluate this issue and introduce CS-Chain-4k to mitigate consequence-blindness.


<details>
  <summary>Details</summary>
Motivation: Current safety-aligned Large Language Models (LLMs) exhibit failure modes due to weak reasoning about consequences, relying instead on superficial cues. This widespread problem needed systematic evaluation and effective mitigation strategies.

Method: The authors define 'consequence-blindness,' create a benchmark named CB-Bench to assess it across various risk scenarios, and introduce CS-Chain-4k, a consequence-reasoning dataset, to fine-tune models for improved safety alignment and decision-making.

Result: Models fine-tuned on CS-Chain-4k showed improvements in handling semantic-camouflage jailbreaks, reduced over-refusal of harmless inputs, and maintained performance on other benchmarks.

Conclusion: The study highlights 'consequence-awareness' as an essential alignment goal for LLMs, offering a structured evaluation method and dataset to address limitations in current alignment techniques.

Abstract: Safety-aligned Large Language Models (LLMs) still show two dominant failure
modes: they are easily jailbroken, or they over-refuse harmless inputs that
contain sensitive surface signals. We trace both to a common cause: current
models reason weakly about links between actions and outcomes and over-rely on
surface-form signals, lexical or stylistic cues that do not encode
consequences. We define this failure mode as Consequence-blindness. To study
consequence-blindness, we build a benchmark named CB-Bench covering four risk
scenarios that vary whether semantic risk aligns with outcome risk, enabling
evaluation under both matched and mismatched conditions which are often ignored
by existing safety benchmarks. Mainstream models consistently fail to separate
these risks and exhibit consequence-blindness, indicating that
consequence-blindness is widespread and systematic. To mitigate
consequence-blindness, we introduce CS-Chain-4k, a consequence-reasoning
dataset for safety alignment. Models fine-tuned on CS-Chain-4k show clear gains
against semantic-camouflage jailbreaks and reduce over-refusal on harmless
inputs, while maintaining utility and generalization on other benchmarks. These
results clarify the limits of current alignment, establish consequence-aware
reasoning as a core alignment goal and provide a more practical and
reproducible evaluation path.

</details>


### [155] [Evaluation of Clinical Trials Reporting Quality using Large Language Models](https://arxiv.org/abs/2510.04338)
*Mathieu Laï-king,Patrick Paroubek*

Main category: cs.CL

TL;DR: The study evaluates large language models in assessing clinical trial reporting quality using CONSORT standards, achieving 85% accuracy with the best combination.


<details>
  <summary>Details</summary>
Motivation: The quality of reporting in clinical trial research articles is essential because it directly influences clinical decisions. Improving methods to evaluate reporting helps enhance transparency and fidelity in research.

Method: The authors developed CONSORT-QA, an evaluation corpus, and tested various large language models—including biomedical-adapted ones—using prompt engineering methods like Chain-of-thought reasoning to assess reporting quality based on CONSORT criteria.

Result: The optimal model and prompting technique achieved 85% accuracy in evaluating reporting quality. The Chain-of-thought method also provided deeper insights into the reasoning process behind the evaluations.

Conclusion: Large language models demonstrate high potential in assessing reporting quality in clinical trial abstracts, and techniques like Chain-of-thought reasoning enhance interpretability of their judgments.

Abstract: Reporting quality is an important topic in clinical trial research articles,
as it can impact clinical decisions. In this article, we test the ability of
large language models to assess the reporting quality of this type of article
using the Consolidated Standards of Reporting Trials (CONSORT). We create
CONSORT-QA, an evaluation corpus from two studies on abstract reporting quality
with CONSORT-abstract standards. We then evaluate the ability of different
large generative language models (from the general domain or adapted to the
biomedical domain) to correctly assess CONSORT criteria with different known
prompting methods, including Chain-of-thought. Our best combination of model
and prompting method achieves 85% accuracy. Using Chain-of-thought adds
valuable information on the model's reasoning for completing the task.

</details>


### [156] [Inoculation Prompting: Eliciting traits from LLMs during training can suppress them at test-time](https://arxiv.org/abs/2510.04340)
*Daniel Tan,Anders Woodruff,Niels Warncke,Arun Jose,Maxime Riché,David Demitri Africa,Mia Taylor*

Main category: cs.CL

TL;DR: The paper introduces inoculation prompting, a finetuning technique that selectively reduces undesired traits while preserving desired traits during language model training.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the challenge of balancing desirable and undesirable trait learning during language model finetuning, which often results in problems like emergent misalignment and unintended effects.

Method: The proposed method involves prepending short system-prompt instructions to finetuning data that deliberately elicit the undesirable traits. At test time, the models are evaluated without the prompts to assess their effectiveness in reducing those traits.

Result: Inoculated models show significantly decreased undesirable traits compared to models trained with unmodified data. The method proves selective across various settings, such as maintaining subtleties like language preferences while mitigating trait transmission.

Conclusion: Inoculation prompting provides a simple, effective, and selective technique for improving learning outcomes in language models. Additionally, it contributes to a deeper understanding of generalization and optimization mechanisms within language models.

Abstract: Language model finetuning often results in learning undesirable traits in
combination with desired ones. To address this, we propose inoculation
prompting: modifying finetuning data by prepending a short system-prompt
instruction that deliberately elicits the undesirable trait. At test time, we
evaluate without the instruction; inoculated models have much lower expression
of the trait than models trained with unmodified training data. Inoculation is
selective: in a toy setting where assistant responses are always in Spanish and
ALL-CAPS, an appropriate inoculation (e.g., ``You always speak in Spanish.'')
teaches the model to capitalize responses while still responding in English. We
find that inoculation is also effective across several additional settings:
reducing emergent misalignment (EM) from task-specific finetuning, defending
against backdoor injections, and mitigating the transmission of traits via
subliminal learning. Follow-up analysis suggests a mechanism: making a trait
less surprising via inoculation reduces optimization pressure to globally
update the model, thereby reducing the degree of generalization. Our analysis
relates to prior work on EM: inoculation explains prior findings that
educational contexts mitigate EM from insecure code. Beyond demonstrating a
simple and effective technique for selective learning, our results contribute
to a better conceptual understanding of how and why language models generalize.

</details>


### [157] [Unmasking Backdoors: An Explainable Defense via Gradient-Attention Anomaly Scoring for Pre-trained Language Models](https://arxiv.org/abs/2510.04347)
*Anindya Sundar Das,Kangjie Chen,Monowar Bhuyan*

Main category: cs.CL

TL;DR: This study investigates vulnerabilities in pre-trained language models to backdoor attacks and proposes a novel inference-time defense leveraging token-level attention and gradient-based anomaly scoring.


<details>
  <summary>Details</summary>
Motivation: The research is motivated by the need to address the susceptibility of language models to backdoor attacks, which utilize hidden triggers to induce malicious behaviors and undermine model reliability.

Method: An inference-time defense mechanism was developed, combining token-level attention and gradient attribution information to construct anomaly scores that identify adversarial triggers.

Result: Experiments across text classification tasks show this approach significantly reduces attack success rates. The study also provides insights into trigger localization and the defense's robustness via interpretability analysis.

Conclusion: The proposed method offers an effective defense against backdoor attacks in language models, improving security and contributing to a better understanding of trigger behaviors.

Abstract: Pre-trained language models have achieved remarkable success across a wide
range of natural language processing (NLP) tasks, particularly when fine-tuned
on large, domain-relevant datasets. However, they remain vulnerable to backdoor
attacks, where adversaries embed malicious behaviors using trigger patterns in
the training data. These triggers remain dormant during normal usage, but, when
activated, can cause targeted misclassifications. In this work, we investigate
the internal behavior of backdoored pre-trained encoder-based language models,
focusing on the consistent shift in attention and gradient attribution when
processing poisoned inputs; where the trigger token dominates both attention
and gradient signals, overriding the surrounding context. We propose an
inference-time defense that constructs anomaly scores by combining token-level
attention and gradient information. Extensive experiments on text
classification tasks across diverse backdoor attack scenarios demonstrate that
our method significantly reduces attack success rates compared to existing
baselines. Furthermore, we provide an interpretability-driven analysis of the
scoring mechanism, shedding light on trigger localization and the robustness of
the proposed defense.

</details>


### [158] [Improving Consistency in Retrieval-Augmented Systems with Group Similarity Rewards](https://arxiv.org/abs/2510.04392)
*Faisal Hamman,Chenyang Zhu,Anoop Kumar,Xujun Peng,Sanghamitra Dutta,Daben Liu,Alfy Samuel*

Main category: cs.CL

TL;DR: The paper addresses consistency issues in Retrieval-Augmented Generation (RAG) systems, introducing tools and methods to evaluate and improve consistency across semantically equivalent queries.


<details>
  <summary>Details</summary>
Motivation: RAG systems are deployed in critical settings but often fail to produce consistent outputs for semantically equivalent queries, undermining user trust.

Method: The authors propose Paraphrased Set Group Relative Policy Optimization (PS-GRPO), leveraging reinforcement learning to enhance output consistency and introducing a scalable approximation method for efficient training.

Result: Experimental evaluations show that the proposed Con-RAG approach significantly enhances output consistency and accuracy across various QA benchmarks, even without explicit ground truth.

Conclusion: The work offers actionable solutions to enhance the reliability of RAG systems, making them more suitable for high-stakes applications.

Abstract: RAG systems are increasingly deployed in high-stakes domains where users
expect outputs to be consistent across semantically equivalent queries.
However, existing systems often exhibit significant inconsistencies due to
variability in both the retriever and generator (LLM), undermining trust and
reliability. In this work, we focus on information consistency, i.e., the
requirement that outputs convey the same core content across semantically
equivalent inputs. We introduce a principled evaluation framework that
decomposes RAG consistency into retriever-level, generator-level, and
end-to-end components, helping identify inconsistency sources. To improve
consistency, we propose Paraphrased Set Group Relative Policy Optimization
(PS-GRPO), an RL approach that leverages multiple rollouts across paraphrased
set to assign group similarity rewards. We leverage PS-GRPO to achieve
Information Consistent RAG (Con-RAG), training the generator to produce
consistent outputs across paraphrased queries and remain robust to
retrieval-induced variability. Because exact reward computation over paraphrase
sets is computationally expensive, we also introduce a scalable approximation
method that retains effectiveness while enabling efficient, large-scale
training. Empirical evaluations across short-form, multi-hop, and long-form QA
benchmarks demonstrate that Con-RAG significantly improves both consistency and
accuracy over strong baselines, even in the absence of explicit ground-truth
supervision. Our work provides practical solutions for evaluating and building
reliable RAG systems for safety-critical deployments.

</details>


### [159] [Time Is Effort: Estimating Human Post-Editing Time for Grammar Error Correction Tool Evaluation](https://arxiv.org/abs/2510.04394)
*Ankit Vadehra,Bill Johnson,Gene Saunders,Pascal Poupart*

Main category: cs.CL

TL;DR: The paper introduces a human-centered evaluation measure called Post-Editing Effort in Time (PEET) for Grammar Error Correction (GEC) tools, using a new dataset with post-editing time annotations. It focuses on assessing how much user effort GEC tools save.


<details>
  <summary>Details</summary>
Motivation: To develop a method for assessing the usability and efficiency of Grammar Error Correction (GEC) tools by quantifying the time and effort saved during text editing.

Method: The authors created a large-scale dataset of post-editing (PE) time annotations for two English GEC datasets (BEA19 and CoNLL14). They proposed the PEET scorer to evaluate GEC tools based on post-editing time-to-correct and conducted analyses to identify edit types that influenced PE time.

Result: PEET revealed that certain edit types, such as deciding correction necessity and paraphrasing, substantially impacted post-editing time. The approach correlated well with human evaluations of technical effort, validating its effectiveness.

Conclusion: The introduction of PEET provides a novel, human-centric evaluation metric for GEC tools, emphasizing usability and effort reduction. The dataset and tools are openly available for further research.

Abstract: Text editing can involve several iterations of revision. Incorporating an
efficient Grammar Error Correction (GEC) tool in the initial correction round
can significantly impact further human editing effort and final text quality.
This raises an interesting question to quantify GEC Tool usability: How much
effort can the GEC Tool save users? We present the first large-scale dataset of
post-editing (PE) time annotations and corrections for two English GEC test
datasets (BEA19 and CoNLL14). We introduce Post-Editing Effort in Time (PEET)
for GEC Tools as a human-focused evaluation scorer to rank any GEC Tool by
estimating PE time-to-correct. Using our dataset, we quantify the amount of
time saved by GEC Tools in text editing. Analyzing the edit type indicated that
determining whether a sentence needs correction and edits like paraphrasing and
punctuation changes had the greatest impact on PE time. Finally, comparison
with human rankings shows that PEET correlates well with technical effort
judgment, providing a new human-centric direction for evaluating GEC tool
usability. We release our dataset and code at:
https://github.com/ankitvad/PEET_Scorer.

</details>


### [160] [SECA: Semantically Equivalent and Coherent Attacks for Eliciting LLM Hallucinations](https://arxiv.org/abs/2510.04398)
*Buyun Liang,Liangzu Peng,Jinqi Luo,Darshan Thaker,Kwan Ho Ryan Chan,René Vidal*

Main category: cs.CL

TL;DR: The paper introduces SECA, a method to elicit hallucinations in Large Language Models (LLMs) using realistic, semantically coherent prompt modifications.


<details>
  <summary>Details</summary>
Motivation: LLMs often generate hallucinations, posing reliability issues in high-risk domains. Many prior adversarial methods create unrealistic prompts, which fail to reflect practical scenarios, leaving a gap in understanding hallucination triggers.

Method: SECA uses constrained optimization over prompts, ensuring semantic equivalence and coherence, and utilizes a zeroth-order method to find feasible adversarial prompts.

Result: SECA outperforms competing methods by achieving higher success rates in inducing hallucinations while significantly reducing constraint violations in experiments on open-ended multiple-choice tasks.

Conclusion: Realistic adversarial prompts, as developed by SECA, reveal the susceptibility of both open-source and commercial LLMs to subtle, coherent input changes, offering insights into their vulnerabilities.

Abstract: Large Language Models (LLMs) are increasingly deployed in high-risk domains.
However, state-of-the-art LLMs often produce hallucinations, raising serious
concerns about their reliability. Prior work has explored adversarial attacks
for hallucination elicitation in LLMs, but it often produces unrealistic
prompts, either by inserting gibberish tokens or by altering the original
meaning. As a result, these approaches offer limited insight into how
hallucinations may occur in practice. While adversarial attacks in computer
vision often involve realistic modifications to input images, the problem of
finding realistic adversarial prompts for eliciting LLM hallucinations has
remained largely underexplored. To address this gap, we propose Semantically
Equivalent and Coherent Attacks (SECA) to elicit hallucinations via realistic
modifications to the prompt that preserve its meaning while maintaining
semantic coherence. Our contributions are threefold: (i) we formulate finding
realistic attacks for hallucination elicitation as a constrained optimization
problem over the input prompt space under semantic equivalence and coherence
constraints; (ii) we introduce a constraint-preserving zeroth-order method to
effectively search for adversarial yet feasible prompts; and (iii) we
demonstrate through experiments on open-ended multiple-choice question
answering tasks that SECA achieves higher attack success rates while incurring
almost no constraint violations compared to existing methods. SECA highlights
the sensitivity of both open-source and commercial gradient-inaccessible LLMs
to realistic and plausible prompt variations. Code is available at
https://github.com/Buyun-Liang/SECA.

</details>


### [161] [Large Language Models Preserve Semantic Isotopies in Story Continuations](https://arxiv.org/abs/2510.04400)
*Marc Cavazza*

Main category: cs.CL

TL;DR: This paper investigates whether large language model (LLM)-generated texts maintain semantic consistencies using story prompts and semantic isotopy analysis.


<details>
  <summary>Details</summary>
Motivation: The study aims to examine the relationship between LLMs and textual semantics, specifically whether LLMs preserve semantic isotopies in generated texts.

Method: It employs a story continuation experiment with 10,000 ROCStories prompts completed by five LLMs, validating GPT-4o as a semantic benchmark before analyzing isotopic properties in the generated completions.

Result: The findings reveal that LLMs maintain semantic isotopies across various structural and semantic features within a defined token horizon.

Conclusion: LLM-generated texts show consistent preservation of semantic isotopies, suggesting their effectiveness in maintaining textual semantics.

Abstract: In this work, we explore the relevance of textual semantics to Large Language
Models (LLMs), extending previous insights into the connection between
distributional semantics and structural semantics. We investigate whether
LLM-generated texts preserve semantic isotopies. We design a story continuation
experiment using 10,000 ROCStories prompts completed by five LLMs. We first
validate GPT-4o's ability to extract isotopies from a linguistic benchmark,
then apply it to the generated stories. We then analyze structural (coverage,
density, spread) and semantic properties of isotopies to assess how they are
affected by completion. Results show that LLM completion within a given token
horizon preserves semantic isotopies across multiple properties.

</details>


### [162] [Good Intentions Beyond ACL: Who Does NLP for Social Good, and Where?](https://arxiv.org/abs/2510.04434)
*Grace LeFevre,Qingcheng Zeng,Adam Leif,Jason Jewell,Denis Peskoff,Rob Voigt*

Main category: cs.CL

TL;DR: The study analyzes contributions to NLP for Social Good (NLP4SG), revealing trends regarding author demographics and publication venues.


<details>
  <summary>Details</summary>
Motivation: To understand and quantify the landscape of NLP research aimed at social good, focusing on the contributions of ACL-related authors and venues.

Method: The paper uses author-level and venue-level analysis to identify patterns in NLP4SG research, comparing work inside and outside the ACL community.

Result: ACL authors are more likely to work on NLP4SG when publishing outside ACL venues, and the majority of NLP4SG research comes from non-ACL authors in non-ACL venues.

Conclusion: The findings highlight the need for ACL to revisit agenda-setting for social good initiatives and better incorporate those efforts within the community.

Abstract: The social impact of Natural Language Processing (NLP) is increasingly
important, with a rising community focus on initiatives related to NLP for
Social Good (NLP4SG). Indeed, in recent years, almost 20% of all papers in the
ACL Anthology address topics related to social good as defined by the UN
Sustainable Development Goals (Adauto et al., 2023). In this study, we take an
author- and venue-level perspective to map the landscape of NLP4SG, quantifying
the proportion of work addressing social good concerns both within and beyond
the ACL community, by both core ACL contributors and non-ACL authors. With this
approach we discover two surprising facts about the landscape of NLP4SG. First,
ACL authors are dramatically more likely to do work addressing social good
concerns when publishing in venues outside of ACL. Second, the vast majority of
publications using NLP techniques to address concerns of social good are done
by non-ACL authors in venues outside of ACL. We discuss the implications of
these findings on agenda-setting considerations for the ACL community related
to NLP4SG.

</details>


### [163] [On the Role of Unobserved Sequences on Sample-based Uncertainty Quantification for LLMs](https://arxiv.org/abs/2510.04439)
*Lucie Kunitomo-Jacquin,Edison Marrese-Taylor,Ken Fukuda*

Main category: cs.CL

TL;DR: This paper addresses the significance of quantifying uncertainty in large language models (LLMs) to detect incorrect outputs and highlights the importance of incorporating unobserved sequence probabilities.


<details>
  <summary>Details</summary>
Motivation: To enhance safety in critical applications by identifying inaccuracies (hallucinations) in large language models.

Method: Suggests and experimentally establishes the importance of incorporating the probability of unobserved sequences in entropy-based uncertainty quantification methods for LLMs.

Result: Findings show that unobserved sequence probabilities have a pivotal role in effective uncertainty quantification.

Conclusion: Recommends future research to integrate probability of unobserved sequences into uncertainty quantification approaches to improve reliability of LLMs.

Abstract: Quantifying uncertainty in large language models (LLMs) is important for
safety-critical applications because it helps spot incorrect answers, known as
hallucinations. One major trend of uncertainty quantification methods is based
on estimating the entropy of the distribution of the LLM's potential output
sequences. This estimation is based on a set of output sequences and associated
probabilities obtained by querying the LLM several times. In this paper, we
advocate and experimentally show that the probability of unobserved sequences
plays a crucial role, and we recommend future research to integrate it to
enhance such LLM uncertainty quantification methods.

</details>


### [164] [Mitigating Forgetting Between Supervised and Reinforcement Learning Yields Stronger Reasoners](https://arxiv.org/abs/2510.04454)
*Xiangchi Yuan,Xiang Chen,Tong Yu,Dachuan Shi,Can Jin,Wenke Lee,Saayan Mitra*

Main category: cs.CL

TL;DR: The paper introduces a framework that integrates supervised fine-tuning (SFT) and reinforcement learning (RL) to enhance the reasoning abilities of large language models (LLMs) more efficiently and effectively without requiring large data amounts.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address limitations in current methods for improving LLM reasoning abilities using RL and SFT. Challenges include data inefficiency, algorithm-specific designs, and catastrophic forgetting.

Method: The proposed framework dynamically integrates SFT into RL by selecting challenging examples for SFT, identifying high-entropy tokens for targeted loss calculations, and freezing parameters critical for RL to prevent skill degradation.

Result: This method achieves state-of-the-art reasoning performance while using only 1.5% of the SFT data and 20.4% of the RL data compared to prior state-of-the-art approaches, demonstrating significant efficiency.

Conclusion: The framework offers a scalable, efficient, and algorithm-agnostic solution for combining SFT and RL, providing a robust enhancement of reasoning abilities in LLMs with reduced resource requirements.

Abstract: Large Language Models (LLMs) show strong reasoning abilities, often amplified
by Chain-of-Thought (CoT) prompting and reinforcement learning (RL). Although
RL algorithms can substantially improve reasoning, they struggle to expand
reasoning boundaries because they learn from their own reasoning trajectories
rather than acquiring external knowledge. Supervised fine-tuning (SFT) offers
complementary benefits but typically requires large-scale data and risks
overfitting. Recent attempts to combine SFT and RL face three main challenges:
data inefficiency, algorithm-specific designs, and catastrophic forgetting. We
propose a plug-and-play framework that dynamically integrates SFT into RL by
selecting challenging examples for SFT. This approach reduces SFT data
requirements and remains agnostic to the choice of RL or SFT algorithm. To
mitigate catastrophic forgetting of RL-acquired skills during SFT, we select
high-entropy tokens for loss calculation and freeze parameters identified as
critical for RL. Our method achieves state-of-the-art (SoTA) reasoning
performance using only 1.5% of the SFT data and 20.4% of the RL data used by
prior SoTA, providing an efficient and plug-and-play solution for combining SFT
and RL in reasoning post-training.

</details>


### [165] [Compressed Convolutional Attention: Efficient Attention in a Compressed Latent Space](https://arxiv.org/abs/2510.04476)
*Tomas Figliolia,Nicholas Alonso,Rishi Iyer,Quentin Anthony,Beren Millidge*

Main category: cs.CL

TL;DR: Compressed Convolutional Attention (CCA) and its extended form, Compressed Convolutional Grouped Query Attention (CCGQA), introduce a novel attention mechanism that reduces compute, memory, and parameter costs for long-context transformers without sacrificing model quality.


<details>
  <summary>Details</summary>
Motivation: The increasing computational and memory costs of Multi-Headed Attention (MHA) in long-context transformers pose challenges for training and serving AI models efficiently. Prior methods focused on reducing KV-cache size but did not address the high compute overhead.

Method: CCA reduces computational complexity by performing attention within a shared latent space through down-projections of queries, keys, and values. CCGQA combines CCA with head-sharing to allow further tunable compression. The approach reduces parameters, KV-cache, and floating-point operations simultaneously.

Result: CCGQA outperforms prior attention methods like GQA and MLA at equal KV-cache compression levels. It achieves an 8x KV-cache compression with no performance degradation compared to standard MHA and enables faster training and prefill, demonstrating significant latency reduction on H100 GPUs.

Conclusion: CCA and CCGQA set a new benchmark for efficient attention mechanisms in long-context transformers by cutting both computational and memory costs while retaining performance, making them suitable for highly efficient model training and deployment.

Abstract: Multi-headed Attention's (MHA) quadratic compute and linearly growing
KV-cache make long-context transformers expensive to train and serve. Prior
works such as Grouped Query Attention (GQA) and Multi-Latent Attention (MLA)
shrink the cache, speeding decode, but leave compute, which determines prefill
and training speed, largely unchanged. We introduce Compressed Convolutional
Attention (CCA), a novel attention method which down-projects queries, keys,
and values and performs the entire attention operation inside the shared latent
space. This simple design dramatically cuts parameters, KV-cache, and FLOPs all
at once by the desired compression factor. Because CCA is orthogonal to
head-sharing, we combine the two to form Compressed Convolutional Grouped Query
Attention (CCGQA), which further tightens the compute-bandwidth Pareto frontier
so that users can tune compression toward either FLOP or memory limits without
sacrificing quality. Experiments show that CCGQA consistently outperforms both
GQA and MLA at equal KV-cache compression on dense and MoE models.
Additionally, we show that CCGQA outperforms all other attention methods on MoE
models with half the KV-cache of GQA and MLA, achieving an 8x KV-cache
compression with no drop in performance compared to standard MHA. CCA and CCGQA
also dramatically reduce the FLOP cost of attention which leads to
substantially faster training and prefill than existing methods. On H100 GPUs,
our fused CCA/CCGQA kernel reduces prefill latency by about 1.7x at a sequence
length of 16k relative to MHA, and accelerates backward by about 1.3x.

</details>


### [166] [Psychological Steering in LLMs: An Evaluation of Effectiveness and Trustworthiness](https://arxiv.org/abs/2510.04484)
*Amin Banayeeanzade,Ala N. Tak,Fatemeh Bahrani,Anahita Bolourani,Leonardo Blas,Emilio Ferrara,Jonathan Gratch,Sai Praneeth Karimireddy*

Main category: cs.CL

TL;DR: This paper introduces PsySET, a benchmark for evaluating LLM steering across emotion and personality dimensions, and examines its implications on controllability and trustworthiness.


<details>
  <summary>Details</summary>
Motivation: Socially meaningful interactions with LLMs require controlled emotional and personality steering to enhance communication quality and ensure trustworthiness.

Method: The study analyzes four LLM models and investigates different steering strategies such as prompting, fine-tuning, and representation engineering, focusing on controllability and trustworthiness.

Result: Prompting is effective but lacks precise control; vector injections offer better intensity control but may reduce output quality. Emotional steering has idiosyncratic effects on trust metrics like fairness, safety, and ethics.

Conclusion: PsySET is the first comprehensive evaluation tool for emotion and personality steering effectiveness, providing key insights into LLM behavior for social interaction applications.

Abstract: The ability to control LLMs' emulated emotional states and personality traits
is essential for enabling rich, human-centered interactions in socially
interactive settings. We introduce PsySET, a Psychologically-informed benchmark
to evaluate LLM Steering Effectiveness and Trustworthiness across the emotion
and personality domains. Our study spans four models from different LLM
families paired with various steering strategies, including prompting,
fine-tuning, and representation engineering. Our results indicate that
prompting is consistently effective but limited in intensity control, whereas
vector injections achieve finer controllability while slightly reducing output
quality. Moreover, we explore the trustworthiness of steered LLMs by assessing
safety, truthfulness, fairness, and ethics, highlighting potential side effects
and behavioral shifts. Notably, we observe idiosyncratic effects; for instance,
even a positive emotion like joy can degrade robustness to adversarial
factuality, lower privacy awareness, and increase preferential bias. Meanwhile,
anger predictably elevates toxicity yet strengthens leakage resistance. Our
framework establishes the first holistic evaluation of emotion and personality
steering, offering insights into its interpretability and reliability for
socially interactive applications.

</details>


### [167] [GenQuest: An LLM-based Text Adventure Game for Language Learners](https://arxiv.org/abs/2510.04498)
*Qiao Wang,Adnan Labib,Robert Swier,Michael Hofmeyr,Zheng Yuan*

Main category: cs.CL

TL;DR: GenQuest is a generative text adventure game using Large Language Models (LLMs) to aid English as a Foreign Language (EFL) learners through interactive storytelling.


<details>
  <summary>Details</summary>
Motivation: To create an engaging and personalized learning tool for EFL learners that combines storytelling and technology for better vocabulary learning.

Method: An interactive, dynamically generated "choose-your-own-adventure" game tailored to learners' proficiency, with a vocabulary assistant providing in-context explanations.

Result: A pilot study with Chinese EFL university students showed promising vocabulary improvements and positive user feedback.

Conclusion: The approach has potential, but learners suggest enhancing narrative quality, adjusting length, and incorporating multi-modal elements like illustrations.

Abstract: GenQuest is a generative text adventure game that leverages Large Language
Models (LLMs) to facilitate second language learning through immersive,
interactive storytelling. The system engages English as a Foreign Language
(EFL) learners in a collaborative "choose-your-own-adventure" style narrative,
dynamically generated in response to learner choices. Game mechanics such as
branching decision points and story milestones are incorporated to maintain
narrative coherence while allowing learner-driven plot development. Key
pedagogical features include content generation tailored to each learner's
proficiency level, and a vocabulary assistant that provides in-context
explanations of learner-queried text strings, ranging from words and phrases to
sentences. Findings from a pilot study with university EFL students in China
indicate promising vocabulary gains and positive user perceptions. Also
discussed are suggestions from participants regarding the narrative length and
quality, and the request for multi-modal content such as illustrations.

</details>


### [168] [GRACE: Generative Representation Learning via Contrastive Policy Optimization](https://arxiv.org/abs/2510.04506)
*Jiashuo Sun,Shixuan Liu,Zhaochen Su,Xianrui Zhong,Pengcheng Jiang,Bowen Jin,Peiran Li,Weijia Shi,Jiawei Han*

Main category: cs.CL

TL;DR: GRACE introduces a novel approach to LLM training that combines generative reasoning with contrastive learning, yielding interpretable embeddings and better performance.


<details>
  <summary>Details</summary>
Motivation: Existing methods treat LLMs as black-box encoders, discarding their generative and reasoning abilities. This paper seeks to make the reasoning process transparent while enhancing the quality of embeddings.

Method: The framework leverages LLMs as generative policies to produce human-understandable rationales which are pooled into embeddings. These embeddings are refined using policy gradient optimization with multi-component rewards to optimize semantic similarity for positive and negative pairs.

Result: GRACE significantly improves performance on the MTEB benchmark, yielding increases in overall scores by 11.5% in supervised and 6.9% in unsupervised settings compared to base models.

Conclusion: GRACE unifies generative reasoning with contrastive objectives, providing stronger embeddings and interpretable rationales, while retaining general model capabilities.

Abstract: Prevailing methods for training Large Language Models (LLMs) as text encoders
rely on contrastive losses that treat the model as a black box function,
discarding its generative and reasoning capabilities in favor of static
embeddings. We introduce GRACE (Generative Representation Learning via
Contrastive Policy Optimization), a novel framework that reimagines contrastive
signals not as losses to be minimized, but as rewards that guide a generative
policy. In GRACE, the LLM acts as a policy that produces explicit,
human-interpretable rationales--structured natural language explanations of its
semantic understanding. These rationales are then encoded into high-quality
embeddings via mean pooling. Using policy gradient optimization, we train the
model with a multi-component reward function that maximizes similarity between
query positive pairs and minimizes similarity with negatives. This transforms
the LLM from an opaque encoder into an interpretable agent whose reasoning
process is transparent and inspectable. On MTEB benchmark, GRACE yields broad
cross category gains: averaged over four backbones, the supervised setting
improves overall score by 11.5% over base models, and the unsupervised variant
adds 6.9%, while preserving general capabilities. This work treats contrastive
objectives as rewards over rationales, unifying representation learning with
generation to produce stronger embeddings and transparent rationales. The
model, data and code are available at https://github.com/GasolSun36/GRACE.

</details>


### [169] [Fine-grained auxiliary learning for real-world product recommendation](https://arxiv.org/abs/2510.04551)
*Mario Almagro,Diego Ortego,David Jimenez*

Main category: cs.CL

TL;DR: This paper introduces ALC, an auxiliary learning strategy designed to improve automated coverage in product recommendation systems by learning fine-grained embeddings.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of achieving high automation in product recommendation systems, which require strong coverage and accuracy for real-world applications.

Method: ALC employs two training objectives focusing on hardest negatives within batches to enhance discrimination between positives and negatives. It uses fine-grained embeddings along with threshold-consistent margin loss.

Result: ALC was validated on two datasets, LF-AmazonTitles-131K and Tech and Durables, achieving state-of-the-art coverage rates in product recommendations.

Conclusion: By leveraging an auxiliary learning strategy, ALC significantly improves the coverage and automation of product recommendation systems, making them more effective for practical deployment.

Abstract: Product recommendation is the task of recovering the closest items to a given
query within a large product corpora. Generally, one can determine if
top-ranked products are related to the query by applying a similarity
threshold; exceeding it deems the product relevant, otherwise manual revision
is required. Despite being a well-known problem, the integration of these
models in real-world systems is often overlooked. In particular, production
systems have strong coverage requirements, i.e., a high proportion of
recommendations must be automated. In this paper we propose ALC , an Auxiliary
Learning strategy that boosts Coverage through learning fine-grained
embeddings. Concretely, we introduce two training objectives that leverage the
hardest negatives in the batch to build discriminative training signals between
positives and negatives. We validate ALC using three extreme multi-label
classification approaches in two product recommendation datasets;
LF-AmazonTitles-131K and Tech and Durables (proprietary), demonstrating
state-of-the-art coverage rates when combined with a recent
threshold-consistent margin loss.

</details>


### [170] [Can LLMs Detect Ambiguous Plural Reference? An Analysis of Split-Antecedent and Mereological Reference](https://arxiv.org/abs/2510.04581)
*Dang Anh,Rick Nouwen,Massimo Poesio*

Main category: cs.CL

TL;DR: The paper explores how LLMs handle plural references in ambiguous and unambiguous contexts using experiments on pronoun production, interpretation, and ambiguity detection.


<details>
  <summary>Details</summary>
Motivation: To understand whether LLMs have human-like capabilities in representing and interpreting plural references, especially in ambiguous situations.

Method: Design and conduct experiments focused on next-token prediction, pronoun interpretation, and ambiguity detection using various prompting strategies.

Result: LLMs show partial awareness of possible referents but deviate from human-like preferences and struggle with ambiguity detection without explicit prompts.

Conclusion: LLMs are inconsistent in handling plural references and ambiguity, highlighting a gap between their performance and human linguistic understanding.

Abstract: Our goal is to study how LLMs represent and interpret plural reference in
ambiguous and unambiguous contexts. We ask the following research questions:
(1) Do LLMs exhibit human-like preferences in representing plural reference?
(2) Are LLMs able to detect ambiguity in plural anaphoric expressions and
identify possible referents? To address these questions, we design a set of
experiments, examining pronoun production using next-token prediction tasks,
pronoun interpretation, and ambiguity detection using different prompting
strategies. We then assess how comparable LLMs are to humans in formulating and
interpreting plural reference. We find that LLMs are sometimes aware of
possible referents of ambiguous pronouns. However, they do not always follow
human reference when choosing between interpretations, especially when the
possible interpretation is not explicitly mentioned. In addition, they struggle
to identify ambiguity without direct instruction. Our findings also reveal
inconsistencies in the results across different types of experiments.

</details>


### [171] [Robustness assessment of large audio language models in multiple-choice evaluation](https://arxiv.org/abs/2510.04584)
*Fernando López,Santosh Kesiraju,Jordi Luque*

Main category: cs.CL

TL;DR: The paper evaluates the flaws in commonly used multiple-choice question answering (MCQA) assessments for large audio language models (LALMs), introducing a revised evaluation protocol to tackle variability in results.


<details>
  <summary>Details</summary>
Motivation: Current MCQA evaluations for LALMs often fail to consider the impact of minor variations, such as question phrasing or choice ordering, on model performance.

Method: The study systematically analyzed the sensitivity of four LALMs across three benchmarks to variations in question phrasing and choice ordering. The authors then designed an improved evaluation protocol that accounts for these subtleties.

Result: The analysis demonstrated that LALMs' accuracy varies significantly with changes in question phrasing or choice order. The proposed protocol allows for more detailed reporting and captures performance under varying conditions.

Conclusion: The paper underscores the need for a nuanced evaluation framework for LALMs in MCQA tasks, advocating for protocols that reflect real-world application variability.

Abstract: Recent advances in large audio language models (LALMs) have primarily been
assessed using a multiple-choice question answering (MCQA) framework. However,
subtle changes, such as shifting the order of choices, result in substantially
different results. Existing MCQA frameworks do not account for this variability
and report a single accuracy number per benchmark or category. We dive into the
MCQA evaluation framework and conduct a systematic study spanning three
benchmarks (MMAU, MMAR and MMSU) and four models: Audio Flamingo 2, Audio
Flamingo 3, Qwen2.5-Omni-7B-Instruct, and Kimi-Audio-7B-Instruct. Our findings
indicate that models are sensitive not only to the ordering of choices, but
also to the paraphrasing of the question and the choices. Finally, we propose a
simpler evaluation protocol and metric that account for subtle variations and
provide a more detailed evaluation report of LALMs within the MCQA framework.

</details>


### [172] [FedSRD: Sparsify-Reconstruct-Decompose for Communication-Efficient Federated Large Language Models Fine-Tuning](https://arxiv.org/abs/2510.04601)
*Guochen Yan,Luyuan Xie,Qingni Shen,Yuejian Fang,Zhonghai Wu*

Main category: cs.CL

TL;DR: The paper proposes FedSRD, a framework addressing communication inefficiencies in Federated Learning when using Low-Rank Adaptation (LoRA), achieving significant communication cost reductions and improved model performance.


<details>
  <summary>Details</summary>
Motivation: The depletion of high-quality public data sources necessitates privacy-preserving collaborative AI advancements. However, leveraging LoRA in Federated Learning faces challenges due to communication bottlenecks and parameter aggregation conflicts.

Method: The authors introduce FedSRD, a Sparsify-Reconstruct-Decompose framework, which employs (1) importance-aware sparsification, (2) reconstruction and aggregation in full-rank space, and (3) decomposition into sparse low-rank formats. A computationally efficient variant, FedSRD-e, is also proposed.

Result: FedSRD reduces communication costs by up to 90% while improving model performance on heterogeneous client data, validated across 10 benchmarks.

Conclusion: The proposed FedSRD framework efficiently addresses communication bottlenecks in Federated Learning, making decentralized, privacy-preserving AI development practical even under network heterogeneity.

Abstract: The current paradigm of training large language models (LLMs) on publicly
available Web data is becoming unsustainable, with high-quality data sources in
specialized domains nearing exhaustion. Federated Learning (FL) emerges as a
practical solution for the next generation of AI on a decentralized Web,
enabling privacy-preserving collaborative fine-tuning by leveraging private
data distributed across a global client base. While Low-Rank Adaptation (LoRA)
is the standard for efficient fine-tuning, its application in federated
settings presents a critical challenge: communication overhead remains a
significant bottleneck across the Web's heterogeneous network conditions. The
structural redundancy within LoRA parameters not only incurs a heavy
communication burden but also introduces conflicts when aggregating client
updates. To address this, we propose FedSRD, a Sparsify-Reconstruct-Decompose
framework designed for communication-efficient FL. We first introduce an
importance-aware sparsification method that preserves the structural integrity
of LoRA updates to reduce the uploaded parameter count. The server then
reconstructs and aggregates these updates in a full-rank space to mitigate
conflicts. Finally, it decomposes the global update into a sparse low-rank
format for broadcast, ensuring a symmetrically efficient cycle. We also propose
an efficient variant, FedSRD-e, to reduce computational overhead. Experimental
results on 10 benchmarks demonstrate that our framework significantly reduces
communication costs by up to 90\% while even improving model performance on
heterogeneous client data.

</details>


### [173] [Contrastive Learning Using Graph Embeddings for Domain Adaptation of Language Models in the Process Industry](https://arxiv.org/abs/2510.04631)
*Anastasia Zhukova,Jonas Lührs,Christian E. Matt,Bela Gipp*

Main category: cs.CL

TL;DR: The study applies SciNCL to industry-specific text logs, showing significant performance improvement on embedding benchmarks.


<details>
  <summary>Details</summary>
Motivation: Leverage advanced graph-aware NLP models to process sparse text logs in the process industry domain, enhancing understanding of domain-specific relationships.

Method: Adopt SciNCL, a graph-aware contrastive learning framework originally for scientific publications, and tailor it for sparse knowledge graphs in the process industry.

Result: SciNCL-trained models outperform an mE5-large encoder by 9.8-14.3%, while being much smaller in size.

Conclusion: Graph-aware contrastive learning frameworks like SciNCL can effectively handle sparse domain-specific text logs with high efficiency and accuracy.

Abstract: Recent trends in NLP utilize knowledge graphs (KGs) to enhance pretrained
language models by incorporating additional knowledge from the graph structures
to learn domain-specific terminology or relationships between documents that
might otherwise be overlooked. This paper explores how SciNCL, a graph-aware
neighborhood contrastive learning methodology originally designed for
scientific publications, can be applied to the process industry domain, where
text logs contain crucial information about daily operations and are often
structured as sparse KGs. Our experiments demonstrate that language models
fine-tuned with triplets derived from GE outperform a state-of-the-art
mE5-large text encoder by 9.8-14.3% (5.4-8.0p) on the proprietary process
industry text embedding benchmark (PITEB) while being 3-5 times smaller in
size.

</details>


### [174] [Evaluating LLMs for Demographic-Targeted Social Bias Detection: A Comprehensive Benchmark Study](https://arxiv.org/abs/2510.04641)
*Ayan Majumdar,Feihao Chen,Jinghui Li,Xiaozhen Wang*

Main category: cs.CL

TL;DR: The paper addresses the need for scalable bias-detection methods in large-scale AI training data, offering a comprehensive evaluation framework focused on demographic-targeted social biases in English texts.


<details>
  <summary>Details</summary>
Motivation: To fill the gap in understanding harmful biases in AI training data and to comply with regulatory needs for comprehensive bias auditing.

Method: The study frames bias detection as a multi-label task using a demographic-focused taxonomy and evaluates various methods including prompting, in-context learning, and fine-tuning.

Result: The results show promise in using fine-tuned smaller models for scalable bias detection but also reveal persistent gaps in addressing multi-demographic biases.

Conclusion: The study highlights the need for more effective frameworks to manage scalable and multidimensional bias auditing in AI training data.

Abstract: Large-scale web-scraped text corpora used to train general-purpose AI models
often contain harmful demographic-targeted social biases, creating a regulatory
need for data auditing and developing scalable bias-detection methods. Although
prior work has investigated biases in text datasets and related detection
methods, these studies remain narrow in scope. They typically focus on a single
content type (e.g., hate speech), cover limited demographic axes, overlook
biases affecting multiple demographics simultaneously, and analyze limited
techniques. Consequently, practitioners lack a holistic understanding of the
strengths and limitations of recent large language models (LLMs) for automated
bias detection. In this study, we present a comprehensive evaluation framework
aimed at English texts to assess the ability of LLMs in detecting
demographic-targeted social biases. To align with regulatory requirements, we
frame bias detection as a multi-label task using a demographic-focused
taxonomy. We then conduct a systematic evaluation with models across scales and
techniques, including prompting, in-context learning, and fine-tuning. Using
twelve datasets spanning diverse content types and demographics, our study
demonstrates the promise of fine-tuned smaller models for scalable detection.
However, our analyses also expose persistent gaps across demographic axes and
multi-demographic targeted biases, underscoring the need for more effective and
scalable auditing frameworks.

</details>


### [175] [FT-MDT: Extracting Decision Trees from Medical Texts via a Novel Low-rank Adaptation Method](https://arxiv.org/abs/2510.04655)
*Yuheng Li,Jiechao Gao,Wei Han,Wenwen Ouyang,Wei Zhu,Hui Yi Leong*

Main category: cs.CL

TL;DR: PI-LoRA is a low-rank adaptation method for automating the extraction of medical decision trees (MDTs), outperforming existing techniques with better accuracy and lower complexity.


<details>
  <summary>Details</summary>
Motivation: Current methods for constructing MDTs are time-consuming, relying on manual annotation, necessitating a more efficient automated approach.

Method: PI-LoRA integrates gradient path information to optimize rank allocation and prune less significant modules, ensuring efficiency and accuracy in MDT extraction.

Result: Experiments show PI-LoRA surpasses existing fine-tuning approaches in accuracy and reduced complexity, achieving state-of-the-art performance in the Text2MDT task.

Conclusion: PI-LoRA is an effective and resource-efficient solution for extracting MDTs, supporting clinical decision-making even in resource-constrained environments.

Abstract: Knowledge of the medical decision process, which can be modeled as medical
decision trees (MDTs), is critical to building clinical decision support
systems. However, current MDT construction methods rely heavily on
time-consuming and laborious manual annotation. To address this challenge, we
propose PI-LoRA (Path-Integrated LoRA), a novel low-rank adaptation method for
automatically extracting MDTs from clinical guidelines and textbooks. We
integrate gradient path information to capture synergistic effects between
different modules, enabling more effective and reliable rank allocation. This
framework ensures that the most critical modules receive appropriate rank
allocations while less important ones are pruned, resulting in a more efficient
and accurate model for extracting medical decision trees from clinical texts.
Extensive experiments on medical guideline datasets demonstrate that our
PI-LoRA method significantly outperforms existing parameter-efficient
fine-tuning approaches for the Text2MDT task, achieving better accuracy with
substantially reduced model complexity. The proposed method achieves
state-of-the-art results while maintaining a lightweight architecture, making
it particularly suitable for clinical decision support systems where
computational resources may be limited.

</details>


### [176] [FocusMed: A Large Language Model-based Framework for Enhancing Medical Question Summarization with Focus Identification](https://arxiv.org/abs/2510.04671)
*Chao Liu,Ling Luo,Tengxiao Lv,Huan Zhuang,Lejing Yu,Jian Wang,Hongfei Lin*

Main category: cs.CL

TL;DR: The paper presents a framework that enhances large language models for summarizing consumer health questions into streamlined medical FAQs, addressing challenges like focus identification and content hallucination.


<details>
  <summary>Details</summary>
Motivation: To improve the efficiency and quality of medical question summaries, overcoming challenges such as redundant information and non-professional terminology in CHQs.

Method: A core focus guidance framework involving prompt templates, specialized fine-tuning datasets, and a multi-dimensional evaluation mechanism to refine summaries.

Result: The framework achieves state-of-the-art results in MQS tasks, excelling in identifying question focus and reducing model hallucinations.

Conclusion: Optimized large language models can significantly improve the summarization of CHQs, with substantial fidelity and precision gains demonstrated across metrics.

Abstract: With the rapid development of online medical platforms, consumer health
questions (CHQs) are inefficient in diagnosis due to redundant information and
frequent non-professional terms. The medical question summary (MQS) task aims
to transform CHQs into streamlined doctors' frequently asked questions (FAQs),
but existing methods still face challenges such as poor identification of
question focus and model hallucination. This paper explores the potential of
large language models (LLMs) in the MQS task and finds that direct fine-tuning
is prone to focus identification bias and generates unfaithful content. To this
end, we propose an optimization framework based on core focus guidance. First,
a prompt template is designed to drive the LLMs to extract the core focus from
the CHQs that is faithful to the original text. Then, a fine-tuning dataset is
constructed in combination with the original CHQ-FAQ pairs to improve the
ability to identify the focus of the question. Finally, a multi-dimensional
quality evaluation and selection mechanism is proposed to comprehensively
improve the quality of the summary from multiple dimensions. We conduct
comprehensive experiments on two widely-adopted MQS datasets using three
established evaluation metrics. The proposed framework achieves
state-of-the-art performance across all measures, demonstrating a significant
boost in the model's ability to identify critical focus of questions and a
notable mitigation of hallucinations. The source codes are freely available at
https://github.com/DUT-LiuChao/FocusMed.

</details>


### [177] [Multi-Agent Tool-Integrated Policy Optimization](https://arxiv.org/abs/2510.04678)
*Zhanfeng Mo,Xingxuan Li,Yuntao Chen,Lidong Bing*

Main category: cs.CL

TL;DR: The paper introduces MATPO, a method allowing a single LLM instance to act as both planner and worker agents for complex tasks, significantly improving efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: To tackle limitations of single-agent systems, like restricted context length and susceptibility to noisy outputs, by introducing multi-agent frameworks within a single LLM instance.

Method: The proposed MATPO uses reinforcement learning to train planner and worker roles within one LLM instance, leveraging role-specific prompts and a credit assignment mechanism for efficiency and robustness.

Result: MATPO achieves an 18.38% average performance improvement over single-agent baselines on knowledge-intensive tasks and demonstrates resilience to noisy tool outputs in experiments.

Conclusion: MATPO effectively combines the advantages of multi-agent frameworks into a single LLM, ensuring memory efficiency, role specialization, and improved tool-integrated task performance.

Abstract: Large language models (LLMs) increasingly rely on multi-turn tool-integrated
planning for knowledge-intensive and complex reasoning tasks. Existing
implementations typically rely on a single agent, but they suffer from limited
context length and noisy tool responses. A natural solution is to adopt a
multi-agent framework with planner- and worker-agents to manage context.
However, no existing methods support effective reinforcement learning
post-training of tool-integrated multi-agent frameworks. To address this gap,
we propose Multi-Agent Tool-Integrated Policy Optimization (MATPO), which
enables distinct roles (planner and worker) to be trained within a single LLM
instance using role-specific prompts via reinforcement learning. MATPO is
derived from a principled credit assignment mechanism across planner and worker
rollouts. This design eliminates the need to deploy multiple LLMs, which would
be memory-intensive, while preserving the benefits of specialization.
Experiments on GAIA-text, WebWalkerQA, and FRAMES show that MATPO consistently
outperforms single-agent baselines by an average of 18.38% relative improvement
in performance and exhibits greater robustness to noisy tool outputs. Our
findings highlight the effectiveness of unifying multiple agent roles within a
single LLM and provide practical insights for stable and efficient multi-agent
RL training.

</details>


### [178] [TiTok: Transfer Token-level Knowledge via Contrastive Excess to Transplant LoRA](https://arxiv.org/abs/2510.04682)
*Chanjoo Jung,Jaehyung Kim*

Main category: cs.CL

TL;DR: The paper introduces TiTok, a framework for transferring LoRA fine-tuning parameters using token-level knowledge transfer without requiring extra models.


<details>
  <summary>Details</summary>
Motivation: Fine-tuning large language models is costly, and existing PEFT methods like LoRA lack transferability across base models.

Method: TiTok uses contrastive analysis between source models with and without LoRA to identify informative tokens and guide synthetic data filtering.

Result: Experiments on three benchmarks show that TiTok improves performance by an average of 4-8% compared to baseline methods.

Conclusion: TiTok effectively enables LoRA transplantation, simplifying the process and enhancing transfer performance without extra complexity.

Abstract: Large Language Models (LLMs) are widely applied in real world scenarios, but
fine-tuning them comes with significant computational and storage costs.
Parameter-Efficient Fine-Tuning (PEFT) methods such as LoRA mitigate these
costs, but the adapted parameters are dependent on the base model and cannot be
transferred across different backbones. One way to address this issue is
through knowledge distillation, but its effectiveness inherently depends on
training data. Recent work such as TransLoRA avoids this by generating
synthetic data, but this adds complexity because it requires training an
additional discriminator model. In this paper, we propose TiTok, a new
framework that enables effective LoRA Transplantation through Token-level
knowledge transfer. Specifically, TiTok captures task-relevant information
through a contrastive excess between a source model with and without LoRA. This
excess highlights informative tokens and enables selective filtering of
synthetic data, all without additional models or overhead. Through experiments
on three benchmarks across multiple transfer settings, our experiments show
that the proposed method is consistently effective, achieving average
performance gains of +4~8% compared to baselines overall.

</details>


### [179] [Multilingual Routing in Mixture-of-Experts](https://arxiv.org/abs/2510.04694)
*Lucas Bandarkar,Chenyuan Yang,Mohsen Fayyaz,Junlin Hu,Nanyun Peng*

Main category: cs.CL

TL;DR: The paper analyzes how Mixture-of-Experts (MoE) models behave with multilingual datasets and introduces methods to improve cross-lingual routing alignment, boosting performance in multiple languages.


<details>
  <summary>Details</summary>
Motivation: To understand how sparse routing in MoE architectures interacts with multilingual data and to identify ways to improve their generalization capabilities across languages.

Method: The researchers analyzed expert routing patterns using parallel multilingual datasets, investigated correlations between cross-lingual routing alignment and model performance, and developed interventions targeting routing in middle-layer task experts frequently activated in English.

Result: The study discovered language-specific routing patterns early and late in the stack but significant cross-lingual alignment in middle layers. They introduced a routing intervention that enhanced multilingual performance consistently, achieving 1-2% performance gains across multiple models and languages.

Conclusion: Improving cross-lingual routing alignment in middle layers boosts multilingual performance in MoE models, indicating the importance of leveraging language-universal experts for generalization.

Abstract: Mixture-of-Experts (MoE) architectures have become the key to scaling modern
LLMs, yet little is understood about how their sparse routing dynamics respond
to multilingual data. In this work, we analyze expert routing patterns using
parallel multilingual datasets and present highly interpretable layer-wise
phenomena. We find that MoE models route tokens in language-specific ways in
the early and late decoder layers but exhibit significant cross-lingual routing
alignment in middle layers, mirroring parameter-sharing trends observed in
dense LLMs. In particular, we reveal a clear, strong correlation between a
model's performance in a given language and how similarly its tokens are routed
to English in these layers. Extending beyond correlation, we explore
inference-time interventions that induce higher cross-lingual routing
alignment. We introduce a method that steers the router by promoting
middle-layer task experts frequently activated in English, and it successfully
increases multilingual performance. These 1-2% gains are remarkably consistent
across two evaluation tasks, three models, and 15+ languages, especially given
that these simple interventions override routers of extensively trained,
state-of-the-art LLMs. In comparison, interventions outside of the middle
layers or targeting multilingual-specialized experts only yield performance
degradation. Altogether, we present numerous findings that explain how MoEs
process non-English text and demonstrate that generalization is limited by the
model's ability to leverage language-universal experts in all languages.

</details>


### [180] [JSON Whisperer: Efficient JSON Editing with LLMs](https://arxiv.org/abs/2510.04717)
*Sarel Duanis,Asnat Greenstein-Messica,Eliya Habba*

Main category: cs.CL

TL;DR: This paper introduces JSON Whisperer, a framework enabling large language models (LLMs) to generate efficient JSON modifications using RFC 6902 diff patches instead of regenerating full JSON documents, significantly reducing token usage.


<details>
  <summary>Details</summary>
Motivation: Current JSON editing with LLMs is computationally inefficient as it regenerates entire structures for modifications, requiring a more token-efficient method for practical use.

Method: The authors introduce JSON Whisperer with patch-based editing and EASE (Explicitly Addressed Sequence Encoding), transforming arrays into dictionaries with stable keys to simplify array manipulations and reduce token usage.

Result: JSON Whisperer reduces token usage by 31% while maintaining edit quality within 5% of traditional full regeneration methods, especially excelling in handling complex instructions and lists.

Conclusion: The approach offers a computationally efficient way to edit JSON with LLMs without significantly compromising quality, addressing key challenges in patch-based editing and array indexing.

Abstract: Large language models (LLMs) can modify JSON documents through natural
language commands, but current approaches regenerate entire structures for each
edit, resulting in computational inefficiency. We present JSON Whisperer, a
framework that enables LLMs to generate RFC 6902 diff patches-expressing only
the necessary modifications-rather than complete documents. We identify two key
challenges in patch-based editing: (1) LLMs often miss related updates when
generating isolated patches, and (2) array manipulations require tracking index
shifts across operations, which LLMs handle poorly. To address these issues, we
introduce EASE (Explicitly Addressed Sequence Encoding), which transforms
arrays into dictionaries with stable keys, eliminating index arithmetic
complexities. Our evaluation shows that patch generation with EASE reduces
token usage by 31% while maintaining edit quality within 5% of full
regeneration with particular gains for complex instructions and list
manipulations. The dataset is available at:
https://github.com/emnlp2025/JSON-Whisperer/

</details>


### [181] [ModernBERT + ColBERT: Enhancing biomedical RAG through an advanced re-ranking retriever](https://arxiv.org/abs/2510.04757)
*Eduardo Martínez Rivera,Filippo Menolascina*

Main category: cs.CL

TL;DR: The paper presents a two-stage retrieval architecture combining ModernBERT for initial retrieval and ColBERTv2 for re-ranking, enhancing Retrieval-Augmented Generation (RAG) in biomedical contexts.


<details>
  <summary>Details</summary>
Motivation: Improving RAG systems in specialized domains like healthcare, where accuracy and factual grounding are critical, overcoming the limitations of current retrieval modules.

Method: Developing a two-stage retrieval system: ModernBERT for efficient initial retrieval and ColBERTv2 for fine-grained re-ranking, fine-tuned with 10k PubMedQA question-passage pairs.

Result: The ColBERT re-ranker improves Recall@3 by up to 4.2 percentage points, and the integrated IR module achieves state-of-the-art accuracy (0.4448) on MIRAGE tasks, outperforming MedCPT.

Conclusion: Joint fine-tuning of the retriever and re-ranker is essential for optimal performance, and the proposed architecture demonstrates significant advancements in biomedical RAG systems.

Abstract: Retrieval-Augmented Generation (RAG) is a powerful technique for enriching
Large Language Models (LLMs) with external knowledge, allowing for factually
grounded responses, a critical requirement in high-stakes domains such as
healthcare. However, the efficacy of RAG systems is fundamentally restricted by
the performance of their retrieval module, since irrelevant or semantically
misaligned documents directly compromise the accuracy of the final generated
response. General-purpose dense retrievers can struggle with the nuanced
language of specialised domains, while the high accuracy of in-domain models is
often achieved at prohibitive computational costs. In this work, we aim to
address this trade-off by developing and evaluating a two-stage retrieval
architecture that combines a lightweight ModernBERT bidirectional encoder for
efficient initial candidate retrieval with a ColBERTv2 late-interaction model
for fine-grained re-ranking. We conduct comprehensive evaluations of our
retriever module performance and RAG system performance in the biomedical
context, fine-tuning the IR module using 10k question-passage pairs from
PubMedQA. Our analysis of the retriever module confirmed the positive impact of
the ColBERT re-ranker, which improved Recall@3 by up to 4.2 percentage points
compared to its retrieve-only counterpart. When integrated into the biomedical
RAG, our IR module leads to a state-of-the-art average accuracy of 0.4448 on
the five tasks of the MIRAGE question-answering benchmark, outperforming strong
baselines such as MedCPT (0.4436). Our ablation studies reveal that this
performance is critically dependent on a joint fine-tuning process that aligns
the retriever and re-ranker; otherwise, the re-ranker might degrade the
performance.

</details>


### [182] [Are BabyLMs Deaf to Gricean Maxims? A Pragmatic Evaluation of Sample-efficient Language Models](https://arxiv.org/abs/2510.04764)
*Raha Askari,Sina Zarrieß,Özge Alacam,Judith Sieker*

Main category: cs.CL

TL;DR: This paper evaluates small-scale pretrained language models on their ability to recognize and interpret violations of Gricean maxims, comparing their performance to children and a large language model (LLM).


<details>
  <summary>Details</summary>
Motivation: Understanding implicit meanings through Gricean maxims is central to effective communication, making it vital to assess language models' ability to handle such pragmatic phenomena.

Method: The authors introduced a benchmark to test small-scale language models (under 10M and 100M tokens) on their ability to distinguish Gricean maxim adherence versus violations. Performance was compared to children and a large language model trained on 3T tokens.

Result: Models trained on less than 100M tokens performed better than under-10M-token models but still lagged behind children and the large language model.

Conclusion: Moderate increases in training data improve pragmatic behavior in small-scale language models, enhancing their ability to differentiate between pragmatic dimensions but not yet reaching human-like competence.

Abstract: Implicit meanings are integral to human communication, making it essential
for language models to be capable of identifying and interpreting them. Grice
(1975) proposed a set of conversational maxims that guide cooperative dialogue,
noting that speakers may deliberately violate these principles to express
meanings beyond literal words, and that listeners, in turn, recognize such
violations to draw pragmatic inferences.
  Building on Surian et al. (1996)'s study of children's sensitivity to
violations of Gricean maxims, we introduce a novel benchmark to test whether
language models pretrained on less than 10M and less than 100M tokens can
distinguish maxim-adhering from maxim-violating utterances. We compare these
BabyLMs across five maxims and situate their performance relative to children
and a Large Language Model (LLM) pretrained on 3T tokens.
  We find that overall, models trained on less than 100M tokens outperform
those trained on less than 10M, yet fall short of child-level and LLM
competence. Our results suggest that modest data increases improve some aspects
of pragmatic behavior, leading to finer-grained differentiation between
pragmatic dimensions.

</details>


### [183] [Hybrid Architectures for Language Models: Systematic Analysis and Design Insights](https://arxiv.org/abs/2510.04800)
*Sangmin Bae,Bilge Acun,Haroun Habeeb,Seungyeon Kim,Chien-Yu Lin,Liang Luo,Junjie Wang,Carole-Jean Wu*

Main category: cs.CL

TL;DR: This paper systematically compares hybrid architectures combining self-attention models and structured state space models, evaluating their effectiveness for long-context tasks.


<details>
  <summary>Details</summary>
Motivation: Hybrid architectures combining self-attention and structured state space models have shown promise in effectively balancing modeling quality and computational efficiency, but systematic studies on their strategies and effectiveness are lacking.

Method: The authors performed holistic evaluations of hybrid architectures based on two fusion strategies (inter-layer and intra-layer), examining their effectiveness across dimensions such as language modeling, long-context capabilities, scaling, and efficiency.

Result: Critical elements for each hybridization strategy were identified and optimal design recipes for hybrid models were proposed through extensive analysis.

Conclusion: This study provides insights and practical guidance for optimizing architectural configurations of hybrid language models, aiding further development in this area.

Abstract: Recent progress in large language models demonstrates that hybrid
architectures--combining self-attention mechanisms with structured state space
models like Mamba--can achieve a compelling balance between modeling quality
and computational efficiency, particularly for long-context tasks. While these
hybrid models show promising performance, systematic comparisons of
hybridization strategies and analyses on the key factors behind their
effectiveness have not been clearly shared to the community. In this work, we
present a holistic evaluation of hybrid architectures based on inter-layer
(sequential) or intra-layer (parallel) fusion. We evaluate these designs from a
variety of perspectives: language modeling performance, long-context
capabilities, scaling analysis, and training and inference efficiency. By
investigating the core characteristics of their computational primitive, we
identify the most critical elements for each hybridization strategy and further
propose optimal design recipes for both hybrid models. Our comprehensive
analysis provides practical guidance and valuable insights for developing
hybrid language models, facilitating the optimization of architectural
configurations.

</details>


### [184] [How I Built ASR for Endangered Languages with a Spoken Dictionary](https://arxiv.org/abs/2510.04832)
*Christopher Bartley,Anton Ragni*

Main category: cs.CL

TL;DR: The paper explores building Automatic Speech Recognition (ASR) systems for endangered languages with minimal data, showing promising results for Manx Gaelic and Cornish.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the lack of ASR systems for endangered languages due to data format incompatibilities, despite the availability of some speech data.

Method: The paper utilizes a short-form pronunciation resource and minimal data (e.g., 40 minutes) to train ASR systems for critically endangered languages.

Result: A usable ASR with less than 50% Word Error Rate (WER) was achieved for Manx Gaelic using 40 minutes of data, and the approach was successfully replicated for Cornish.

Conclusion: The findings demonstrate that the data requirements for developing ASR for endangered languages are lower than previously assumed, offering hope to language communities.

Abstract: Nearly half of the world's languages are endangered. Speech technologies such
as Automatic Speech Recognition (ASR) are central to revival efforts, yet most
languages remain unsupported because standard pipelines expect utterance-level
supervised data. Speech data often exist for endangered languages but rarely
match these formats. Manx Gaelic ($\sim$2,200 speakers), for example, has had
transcribed speech since 1948, yet remains unsupported by modern systems. In
this paper, we explore how little data, and in what form, is needed to build
ASR for critically endangered languages. We show that a short-form
pronunciation resource is a viable alternative, and that 40 minutes of such
data produces usable ASR for Manx ($<$50\% WER). We replicate our approach,
applying it to Cornish ($\sim$600 speakers), another critically endangered
language. Results show that the barrier to entry, in quantity and form, is far
lower than previously thought, giving hope to endangered language communities
that cannot afford to meet the requirements arbitrarily imposed upon them.

</details>


### [185] [ReplaceMe: Network Simplification via Depth Pruning and Transformer Block Linearization](https://arxiv.org/abs/2505.02819)
*Dmitriy Shopkhoev,Ammar Ali,Magauiya Zhussip,Valentin Malykh,Stamatios Lefkimmiatis,Nikos Komodakis,Sergey Zagoruyko*

Main category: cs.CL

TL;DR: ReplaceMe introduces a training-free depth pruning method for transformers, achieving up to 25% pruning with approximately 90% model performance retention without requiring retraining.


<details>
  <summary>Details</summary>
Motivation: Address the inefficiencies of conventional pruning methods that require extensive retraining or fine-tuning.

Method: ReplaceMe estimates linear transformations using a small calibration dataset to prune transformer blocks seamlessly without requiring additional network parameters.

Result: ReplaceMe outperforms other training-free methods and is competitive with state-of-the-art techniques, achieving high compression ratios and minimal computational overhead.

Conclusion: ReplaceMe provides an efficient and effective solution for transformer block pruning, maintaining performance while reducing computational cost, and is accessible via an open-source library.

Abstract: We introduce ReplaceMe, a generalized training-free depth pruning method that
effectively replaces transformer blocks with a linear operation, while
maintaining high performance for low compression ratios. In contrast to
conventional pruning approaches that require additional training or
fine-tuning, our approach requires only a small calibration dataset that is
used to estimate a linear transformation, which approximates the pruned blocks.
The estimated linear mapping can be seamlessly merged with the remaining
transformer blocks, eliminating the need for any additional network parameters.
Our experiments show that ReplaceMe consistently outperforms other
training-free approaches and remains highly competitive with state-of-the-art
pruning methods that involve extensive retraining/fine-tuning and architectural
modifications. Applied to several large language models (LLMs), ReplaceMe
achieves up to 25% pruning while retaining approximately 90% of the original
model's performance on open benchmarks - without any training or healing steps,
resulting in minimal computational overhead (see Fig.1). We provide an
open-source library implementing ReplaceMe alongside several state-of-the-art
depth pruning techniques, available at https://github.com/mts-ai/ReplaceMe.

</details>


### [186] [Instability in Downstream Task Performance During LLM Pretraining](https://arxiv.org/abs/2510.04848)
*Yuto Nishida,Masaru Isonuma,Yusuke Oda*

Main category: cs.CL

TL;DR: The study examines checkpoint selection in LLM training, highlighting performance instability and proposing post-hoc methods like averaging and ensembling to stabilize performance.


<details>
  <summary>Details</summary>
Motivation: Checkpoint selection in LLM training is challenging due to fluctuations in validation scores affecting accurate model performance evaluations.

Method: Empirical analysis of task performance stability during training and two post-hoc methods (checkpoint averaging and ensemble) to mitigate instability.

Result: Both proposed methods improve stability and downstream task performance, confirmed through empirical and theoretical evaluations.

Conclusion: Post-hoc checkpoint integration methods effectively stabilize downstream performance without altering the training process.

Abstract: When training large language models (LLMs), it is common practice to track
downstream task performance throughout the training process and select the
checkpoint with the highest validation score. However, downstream metrics often
exhibit substantial fluctuations, making it difficult to identify the
checkpoint that truly represents the best-performing model. In this study, we
empirically analyze the stability of downstream task performance in an LLM
trained on diverse web-scale corpora. We find that task scores frequently
fluctuate throughout training, both at the aggregate and example levels. To
address this instability, we investigate two post-hoc checkpoint integration
methods: checkpoint averaging and ensemble, motivated by the hypothesis that
aggregating neighboring checkpoints can reduce performance volatility. We
demonstrate both empirically and theoretically that these methods improve
downstream performance stability without requiring any changes to the training
procedure.

</details>


### [187] [When Models Lie, We Learn: Multilingual Span-Level Hallucination Detection with PsiloQA](https://arxiv.org/abs/2510.04849)
*Elisei Rykov,Kseniia Petrushina,Maksim Savkin,Valerii Olisov,Artem Vazhentsev,Kseniia Titova,Alexander Panchenko,Vasily Konovalov,Julia Belikova*

Main category: cs.CL

TL;DR: The paper introduces PsiloQA, a multilingual dataset for detecting hallucinations in large language models (LLMs) on a span-level basis across 14 languages.


<details>
  <summary>Details</summary>
Motivation: Address the lack of fine-grained, multilingual supervision in existing hallucination benchmarks for evaluating LLMs.

Method: The authors constructed PsiloQA using a three-stage automated pipeline involving question-answer generation, no-context hallucinated answer elicitation, and automatic annotation by GPT-4o.

Result: Encoder-based models demonstrated the strongest performance for hallucination detection, outperforming other methods. The dataset also showed effective cross-lingual generalization and robust knowledge transfer.

Conclusion: PsiloQA provides a scalable, cost-efficient resource for fine-grained hallucination detection in multilingual settings, advancing research in this area.

Abstract: Hallucination detection remains a fundamental challenge for the safe and
reliable deployment of large language models (LLMs), especially in applications
requiring factual accuracy. Existing hallucination benchmarks often operate at
the sequence level and are limited to English, lacking the fine-grained,
multilingual supervision needed for a comprehensive evaluation. In this work,
we introduce PsiloQA, a large-scale, multilingual dataset annotated with
span-level hallucinations across 14 languages. PsiloQA is constructed through
an automated three-stage pipeline: generating question-answer pairs from
Wikipedia using GPT-4o, eliciting potentially hallucinated answers from diverse
LLMs in a no-context setting, and automatically annotating hallucinated spans
using GPT-4o by comparing against golden answers and retrieved context. We
evaluate a wide range of hallucination detection methods -- including
uncertainty quantification, LLM-based tagging, and fine-tuned encoder models --
and show that encoder-based models achieve the strongest performance across
languages. Furthermore, PsiloQA demonstrates effective cross-lingual
generalization and supports robust knowledge transfer to other benchmarks, all
while being significantly more cost-efficient than human-annotated datasets.
Our dataset and results advance the development of scalable, fine-grained
hallucination detection in multilingual settings.

</details>


### [188] [Detecting Distillation Data from Reasoning Models](https://arxiv.org/abs/2510.04850)
*Hengxiang Zhang,Hyeong Kyu Choi,Yixuan Li,Hongxin Wei*

Main category: cs.CL

TL;DR: The paper defines the task of distillation data detection and introduces a novel method, Token Probability Deviation (TBD), to detect benchmark contamination caused by reasoning distillation in large language models.


<details>
  <summary>Details</summary>
Motivation: To address benchmark contamination caused by reasoning distillation, where evaluation data overlaps with distillation datasets, leading to inflated performance metrics for models.

Method: The proposed method, TBD, analyzes the probability patterns of output tokens, identifying differences in token probability deviations between seen and unseen questions to detect contamination.

Result: The TBD method demonstrates effectiveness in detecting benchmark contamination, achieving high performance metrics such as an AUC of 0.918 and a TPR@1% FPR of 0.470 on the S1 dataset.

Conclusion: The paper highlights the unique challenge of distillation data detection and provides TBD as a competitive solution, showing its ability to distinguish between contaminated and uncontaminated evaluation data effectively.

Abstract: Reasoning distillation has emerged as an efficient and powerful paradigm for
enhancing the reasoning capabilities of large language models. However,
reasoning distillation may inadvertently cause benchmark contamination, where
evaluation data included in distillation datasets can inflate performance
metrics of distilled models. In this work, we formally define the task of
distillation data detection, which is uniquely challenging due to the partial
availability of distillation data. Then, we propose a novel and effective
method Token Probability Deviation (TBD), which leverages the probability
patterns of the generated output tokens. Our method is motivated by the
analysis that distilled models tend to generate near-deterministic tokens for
seen questions, while producing more low-probability tokens for unseen
questions. Our key idea behind TBD is to quantify how far the generated tokens'
probabilities deviate from a high reference probability. In effect, our method
achieves competitive detection performance by producing lower scores for seen
questions than for unseen questions. Extensive experiments demonstrate the
effectiveness of our method, achieving an AUC of 0.918 and a TPR@1% FPR of
0.470 on the S1 dataset.

</details>


### [189] [SocialHarmBench: Revealing LLM Vulnerabilities to Socially Harmful Requests](https://arxiv.org/abs/2510.04891)
*Punya Syon Pandey,Hai Son Le,Devansh Bhardwaj,Rada Mihalcea,Zhijing Jin*

Main category: cs.CL

TL;DR: SocialHarmBench is a benchmark focused on identifying vulnerabilities in large language models (LLMs) within politically sensitive domains, revealing significant risks and biases.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address gaps in safety benchmarks for LLMs, specifically in contexts such as political manipulation, propaganda, and disinformation, areas with direct sociopolitical impacts.

Method: The authors created SocialHarmBench, a dataset comprising 585 prompts across 7 sociopolitical domains and 34 countries, to evaluate LLM performance in politically charged contexts.

Result: Findings show substantial weaknesses in LLM safeguards, with open-weight models like Mistral-7B recording high attack success rates (97-98%) in areas like propaganda and historical revisionism. Issues are especially prevalent in certain geographical and temporal scenarios.

Conclusion: Current measures for LLM safety are inadequate for sociopolitical contexts, exposing biases and reliability concerns, and necessitating improved safeguards to prevent harm.

Abstract: Large language models (LLMs) are increasingly deployed in contexts where
their failures can have direct sociopolitical consequences. Yet, existing
safety benchmarks rarely test vulnerabilities in domains such as political
manipulation, propaganda and disinformation generation, or surveillance and
information control. We introduce SocialHarmBench, a dataset of 585 prompts
spanning 7 sociopolitical categories and 34 countries, designed to surface
where LLMs most acutely fail in politically charged contexts. Our evaluations
reveal several shortcomings: open-weight models exhibit high vulnerability to
harmful compliance, with Mistral-7B reaching attack success rates as high as
97% to 98% in domains such as historical revisionism, propaganda, and political
manipulation. Moreover, temporal and geographic analyses show that LLMs are
most fragile when confronted with 21st-century or pre-20th-century contexts,
and when responding to prompts tied to regions such as Latin America, the USA,
and the UK. These findings demonstrate that current safeguards fail to
generalize to high-stakes sociopolitical settings, exposing systematic biases
and raising concerns about the reliability of LLMs in preserving human rights
and democratic values. We share the SocialHarmBench benchmark at
https://huggingface.co/datasets/psyonp/SocialHarmBench.

</details>


### [190] [Do LLMs Align with My Task? Evaluating Text-to-SQL via Dataset Alignment](https://arxiv.org/abs/2510.04919)
*Davood Rafiei,Morgan Lindsay Heisler,Weiwei Zhang,Mohammadreza Pourreza,Yong Zhang*

Main category: cs.CL

TL;DR: The paper studies dataset alignment in Natural Language to SQL (NL2SQL) tasks and its impact on fine-tuning success, finding structural alignment to be critical.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of variability in training data which can limit the generalization of Large Language Models in NL2SQL tasks.

Method: The paper hypothesizes that alignment can be measured by comparing the structural SQL feature distributions among training sets, target data, and pre-fine-tuning model predictions. It conducts experiments on three NL2SQL benchmarks and multiple model families.

Result: High alignment between datasets results in substantial improvements in accuracy and SQL generation quality post-SFT, while low alignment shows minimal gains.

Conclusion: Alignment-aware data selection is essential for effective fine-tuning and cross-domain generalization in NL2SQL tasks.

Abstract: Supervised Fine-Tuning (SFT) is an effective method for adapting Large
Language Models (LLMs) on downstream tasks. However, variability in training
data can hinder a model's ability to generalize across domains. This paper
studies the problem of dataset alignment for Natural Language to SQL (NL2SQL or
text to SQL), examining how well SFT training data matches the structural
characteristics of target queries and how this alignment impacts model
performance. We hypothesize that alignment can be accurately estimated by
comparing the distributions of structural SQL features across the training set,
target data, and the model's predictions prior to SFT. Through comprehensive
experiments on three large cross-domain NL2SQL benchmarks and multiple model
families, we show that structural alignment is a strong predictor of
fine-tuning success. When alignment is high, SFT yields substantial gains in
accuracy and SQL generation quality; when alignment is low, improvements are
marginal or absent. These findings highlight the importance of alignment-aware
data selection for effective fine-tuning and generalization in NL2SQL tasks.

</details>


### [191] [A First Context-Free Grammar Applied to Nawatl Corpora Augmentation](https://arxiv.org/abs/2510.04945)
*Juan-José Guzmán-Landa,Juan-Manuel Torres-Moreno,Miguel Figueroa-Saavedra,Ligia Quintana-Torres,Martha-Lorena Avendaño-Garrido,Graham Ranger*

Main category: cs.CL

TL;DR: A context-free grammar (CFG) was developed for the Nawatl language to expand its underrepresented corpora.


<details>
  <summary>Details</summary>
Motivation: Address the scarcity of digital resources for the Nawatl language and facilitate language model training.

Method: Developing a CFG to generate grammatically correct artificial sentences for corpora expansion and language model training.

Result: The enriched corpus enables semantic task evaluations and improves performances of algorithms such as FastText over some existing LLMs.

Conclusion: While the CFG improves corpus and model performance, further refinement of grammars is needed for more significant gains.

Abstract: In this article we introduce a context-free grammar (CFG) for the Nawatl
language. Nawatl (or Nahuatl) is an Amerindian language of the $\pi$-language
type, i.e. a language with few digital resources, in which the corpora
available for machine learning are virtually non-existent. The objective here
is to generate a significant number of grammatically correct artificial
sentences, in order to increase the corpora available for language model
training. We want to show that a grammar enables us significantly to expand a
corpus in Nawatl which we call $\pi$-\textsc{yalli}. The corpus, thus enriched,
enables us to train algorithms such as FastText and to evaluate them on
sentence-level semantic tasks. Preliminary results show that by using the
grammar, comparative improvements are achieved over some LLMs. However, it is
observed that to achieve more significant improvement, grammars that model the
Nawatl language even more effectively are required.

</details>


### [192] [AWARE, Beyond Sentence Boundaries: A Contextual Transformer Framework for Identifying Cultural Capital in STEM Narratives](https://arxiv.org/abs/2510.04983)
*Khalid Mehtab Khan,Anagha Kulkarni*

Main category: cs.CL

TL;DR: This paper presents AWARE, a framework to enhance NLP models in identifying cultural capital themes in student reflections by incorporating domain, context, and class overlap awareness, achieving improved performance.


<details>
  <summary>Details</summary>
Motivation: Standard NLP models struggle to detect cultural capital themes in student reflections because such themes are often embedded in narratives rather than appearing as explicit keywords.

Method: The paper introduces AWARE, a framework with three core components: Domain Awareness (adapting vocabulary to student reflections), Context Awareness (creating sentence embeddings aware of essay context), and Class Overlap Awareness (using a multi-label strategy for recognizing coexisting themes in a sentence).

Result: AWARE outperforms a strong baseline by achieving a 2.1-point increase in Macro-F1 and demonstrates significant improvements across all identified themes.

Conclusion: AWARE offers a systematic and generalizable approach to improve text classification tasks, particularly when meaning is embedded within narrative contexts.

Abstract: Identifying cultural capital (CC) themes in student reflections can offer
valuable insights that help foster equitable learning environments in
classrooms. However, themes such as aspirational goals or family support are
often woven into narratives, rather than appearing as direct keywords. This
makes them difficult to detect for standard NLP models that process sentences
in isolation. The core challenge stems from a lack of awareness, as standard
models are pre-trained on general corpora, leaving them blind to the
domain-specific language and narrative context inherent to the data. To address
this, we introduce AWARE, a framework that systematically attempts to improve a
transformer model's awareness for this nuanced task. AWARE has three core
components: 1) Domain Awareness, adapting the model's vocabulary to the
linguistic style of student reflections; 2) Context Awareness, generating
sentence embeddings that are aware of the full essay context; and 3) Class
Overlap Awareness, employing a multi-label strategy to recognize the
coexistence of themes in a single sentence. Our results show that by making the
model explicitly aware of the properties of the input, AWARE outperforms a
strong baseline by 2.1 percentage points in Macro-F1 and shows considerable
improvements across all themes. This work provides a robust and generalizable
methodology for any text classification task in which meaning depends on the
context of the narrative.

</details>


### [193] [Resource-Efficient Fine-Tuning of LLaMA-3.2-3B for Medical Chain-of-Thought Reasoning](https://arxiv.org/abs/2510.05003)
*Imran Mansha*

Main category: cs.CL

TL;DR: The study introduces an efficient fine-tuning approach for enhancing LLaMA-3.2-3B's medical reasoning under low-resource GPU/memory conditions, achieving improved performance with reduced memory usage.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of fine-tuning large language models for medical reasoning in resource-constrained environments.

Method: Utilized parameter-efficient techniques such as LoRA and QLoRA to fine-tune LLaMA-3.2-3B using public medical reasoning datasets while minimizing GPU/memory demands.

Result: Improved reasoning coherence and factual accuracy of the model, with memory usage reduced by up to 60% compared to full fine-tuning.

Conclusion: Lightweight fine-tuning methods can adapt large language models for specific tasks efficiently, enabling their deployment in resource-limited research environments with maintained performance quality.

Abstract: Large Language Models (LLMs) such as GPT-4 and LLaMA have demonstrated
remarkable reasoning abilities but require significant computational resources
for fine-tuning. This paper presents a resource-efficient fine-tuning approach
for LLaMA-3.2-3B to enhance medical chain-of-thought reasoning while operating
under constrained GPU and memory settings. Using parameter-efficient tuning
techniques such as LoRA and QLoRA, we adapt the base model on publicly
available medical reasoning datasets. The model achieves improved reasoning
coherence and factual accuracy while reducing memory usage by up to 60%
compared to standard full fine-tuning. Experimental evaluation demonstrates
that lightweight adaptations can retain strong reasoning capability in medical
question-answering tasks. This work highlights practical strategies for
deploying LLMs in low-resource research environments and provides insights into
balancing efficiency and domain specialization for medical AI systems.

</details>


### [194] [Imperceptible Jailbreaking against Large Language Models](https://arxiv.org/abs/2510.05025)
*Kuofeng Gao,Yiming Li,Chao Du,Xin Wang,Xingjun Ma,Shu-Tao Xia,Tianyu Pang*

Main category: cs.CL

TL;DR: The paper introduces imperceptible jailbreak attacks using Unicode variation selectors to invisibly alter tokenization and deceive aligned language models (LLMs).


<details>
  <summary>Details</summary>
Motivation: Existing jailbreak attacks on vision use imperceptible perturbations, while textual attacks usually require visible changes. The paper aims to bridge this gap for text-based attacks by creating unnoticeable modifications.

Method: The authors use Unicode variation selectors to create invisible adversarial suffixes that alter input tokenization. They also propose a chain-of-search pipeline to generate these suffixes for harmful responses.

Result: The approach achieves high attack success rates on four aligned LLMs and works across different prompt injection scenarios without any visible prompt modifications.

Conclusion: Imperceptible jailbreaks represent a novel, covert attack method against LLMs, effectively bypassing conventional detection mechanisms while posing new challenges for text-based model security.

Abstract: Jailbreaking attacks on the vision modality typically rely on imperceptible
adversarial perturbations, whereas attacks on the textual modality are
generally assumed to require visible modifications (e.g., non-semantic
suffixes). In this paper, we introduce imperceptible jailbreaks that exploit a
class of Unicode characters called variation selectors. By appending invisible
variation selectors to malicious questions, the jailbreak prompts appear
visually identical to original malicious questions on screen, while their
tokenization is "secretly" altered. We propose a chain-of-search pipeline to
generate such adversarial suffixes to induce harmful responses. Our experiments
show that our imperceptible jailbreaks achieve high attack success rates
against four aligned LLMs and generalize to prompt injection attacks, all
without producing any visible modifications in the written prompt. Our code is
available at https://github.com/sail-sg/imperceptible-jailbreaks.

</details>


### [195] [A Set of Quebec-French Corpus of Regional Expressions and Terms](https://arxiv.org/abs/2510.05026)
*David Beauchemin,Yan Tremblay,Mohamed Amine Youssef,Richard Khoury*

Main category: cs.CL

TL;DR: This paper introduces two benchmark datasets for studying idiom and dialect understanding in Quebec French and demonstrates their utility using experiments with 94 large language models (LLMs).


<details>
  <summary>Details</summary>
Motivation: To combine idiom and dialect understanding tasks and assess the proficiency of language models in dialect-specific idiomatic expressions.

Method: The authors created two new benchmark datasets (QFrCoRE and QFrCoRT) for Quebec French, detailing a replicable methodology for constructing such corpora for other dialects.

Result: The experiments using 94 LLMs show that these datasets effectively measure dialect proficiency in Quebec French.

Conclusion: The proposed regional idiom benchmarks are a valuable resource for testing and improving dialectal understanding in NLP models.

Abstract: The tasks of idiom understanding and dialect understanding are both
well-established benchmarks in natural language processing. In this paper, we
propose combining them, and using regional idioms as a test of dialect
understanding. Towards this end, we propose two new benchmark datasets for the
Quebec dialect of French: QFrCoRE, which contains 4,633 instances of idiomatic
phrases, and QFrCoRT, which comprises 171 regional instances of idiomatic
words. We explain how to construct these corpora, so that our methodology can
be replicated for other dialects. Our experiments with 94 LLM demonstrate that
our regional idiom benchmarks are a reliable tool for measuring a model's
proficiency in a specific dialect.

</details>


### [196] [Guided Query Refinement: Multimodal Hybrid Retrieval with Test-Time Optimization](https://arxiv.org/abs/2510.05038)
*Omri Uzan,Asaf Yehudai,Roi pony,Eyal Shnarch,Ariel Gera*

Main category: cs.CL

TL;DR: The paper presents Guided Query Refinement (GQR), a lightweight test-time optimization method that combines vision-centric and dense text retrieval models to improve performance and efficiency in multimodal visual document retrieval.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from limitations of current visual document retrieval approaches that face scalability issues due to large representation sizes and struggles with modality gaps in vision-centric methods.

Method: The paper proposes Guided Query Refinement (GQR), which adjusts query embeddings of a main retriever using score guidance from a complementary retriever at test-time, enabling better interaction between models.

Result: Experiments show GQR achieves comparable performance to models with larger representations, while being up to 14x faster and requiring 54x less memory.

Conclusion: GQR significantly improves the performance-efficiency trade-off in multimodal retrieval, advancing the state-of-the-art for real-world applications in visual document processing.

Abstract: Multimodal encoders have pushed the boundaries of visual document retrieval,
matching textual query tokens directly to image patches and achieving
state-of-the-art performance on public benchmarks. Recent models relying on
this paradigm have massively scaled the sizes of their query and document
representations, presenting obstacles to deployment and scalability in
real-world pipelines. Furthermore, purely vision-centric approaches may be
constrained by the inherent modality gap still exhibited by modern
vision-language models. In this work, we connect these challenges to the
paradigm of hybrid retrieval, investigating whether a lightweight dense text
retriever can enhance a stronger vision-centric model. Existing hybrid methods,
which rely on coarse-grained fusion of ranks or scores, fail to exploit the
rich interactions within each model's representation space. To address this, we
introduce Guided Query Refinement (GQR), a novel test-time optimization method
that refines a primary retriever's query embedding using guidance from a
complementary retriever's scores. Through extensive experiments on visual
document retrieval benchmarks, we demonstrate that GQR allows vision-centric
models to match the performance of models with significantly larger
representations, while being up to 14x faster and requiring 54x less memory.
Our findings show that GQR effectively pushes the Pareto frontier for
performance and efficiency in multimodal retrieval. We release our code at
https://github.com/IBM/test-time-hybrid-retrieval

</details>


### [197] [COLE: a Comprehensive Benchmark for French Language Understanding Evaluation](https://arxiv.org/abs/2510.05046)
*David Beauchemin,Yan Tremblay,Mohamed Amine Youssef,Richard Khoury*

Main category: cs.CL

TL;DR: The paper introduces COLE, a benchmark for evaluating French NLU across 23 tasks, and presents an analysis of 94 large language models.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the need for a deeper evaluation framework to improve French NLU capabilities, given the lack of robust benchmarks focusing specifically on linguistic phenomena in the French language.

Method: The authors developed COLE, a benchmark encompassing 23 diverse tasks. They evaluated 94 large language models against this benchmark to analyze their performance comprehensively.

Result: Findings reveal a notable performance gap between closed- and open-weights models. Key challenges include zero-shot extractive QA, word sense disambiguation, and handling regional language variations.

Conclusion: COLE provides a valuable resource to stimulate advancements in French NLU by identifying existing gaps and challenges in current language models.

Abstract: To address the need for a more comprehensive evaluation of French Natural
Language Understanding (NLU), we introduce COLE, a new benchmark composed of 23
diverse task covering a broad range of NLU capabilities, including sentiment
analysis, paraphrase detection, grammatical judgment, and reasoning, with a
particular focus on linguistic phenomena relevant to the French language. We
benchmark 94 large language models (LLM), providing an extensive analysis of
the current state of French NLU. Our results highlight a significant
performance gap between closed- and open-weights models and identify key
challenging frontiers for current LLMs, such as zero-shot extractive
question-answering (QA), fine-grained word sense disambiguation, and
understanding of regional language variations. We release COLE as a public
resource to foster further progress in French language modelling.

</details>


### [198] [SwiReasoning: Switch-Thinking in Latent and Explicit for Pareto-Superior Reasoning LLMs](https://arxiv.org/abs/2510.05069)
*Dachuan Shi,Abedelkadir Asi,Keying Li,Xiangchi Yuan,Leyan Pan,Wenke Lee,Wen Xiao*

Main category: cs.CL

TL;DR: SwiReasoning improves LLM reasoning by dynamically switching between explicit and latent modes, boosting accuracy and token efficiency without additional training.


<details>
  <summary>Details</summary>
Motivation: To tackle challenges in LLM latent reasoning, such as difficulty in converging to high-confidence solutions and inefficiencies from overthinking.

Method: SwiReasoning framework dynamically switches reasoning modes based on confidence trends and limits block-wise switches to prevent overthinking.

Result: Improved accuracy by 1.5%-2.8% and token efficiency by 56%-79% on mathematics and STEM benchmarks across various model scales.

Conclusion: Dynamic mode switching and controlled reasoning steps significantly enhance performance and efficiency in LLM reasoning.

Abstract: Recent work shows that, beyond discrete reasoning through explicit
chain-of-thought steps, which are limited by the boundaries of natural
languages, large language models (LLMs) can also reason continuously in latent
space, allowing richer information per step and thereby improving token
efficiency. Despite this promise, latent reasoning still faces two challenges,
especially in training-free settings: 1) purely latent reasoning broadens the
search distribution by maintaining multiple implicit paths, which diffuses
probability mass, introduces noise, and impedes convergence to a single
high-confidence solution, thereby hurting accuracy; and 2) overthinking
persists even without explicit text, wasting tokens and degrading efficiency.
To address these issues, we introduce SwiReasoning, a training-free framework
for LLM reasoning which features two key innovations: 1) SwiReasoning
dynamically switches between explicit and latent reasoning, guided by
block-wise confidence estimated from entropy trends in next-token
distributions, to balance exploration and exploitation and promote timely
convergence. 2) By limiting the maximum number of thinking-block switches,
SwiReasoning curbs overthinking and improves token efficiency across varying
problem difficulties. On widely used mathematics and STEM benchmarks,
SwiReasoning consistently improves average accuracy by 1.5%-2.8% across
reasoning LLMs of different model families and scales. Furthermore, under
constrained budgets, SwiReasoning improves average token efficiency by 56%-79%,
with larger gains as budgets tighten.

</details>


### [199] [Slm-mux: Orchestrating small language models for reasoning](https://arxiv.org/abs/2510.05077)
*Chenyu Wang,Zishen Wan,Hao Kang,Emma Chen,Zhiqiang Xie,Tushar Krishna,Vijay Janapa Reddi,Yilun Du*

Main category: cs.CL

TL;DR: The paper introduces an approach for orchestrating small language models (SLMs) into a more effective system, achieving significant performance improvements compared to standalone models and existing orchestration methods.


<details>
  <summary>Details</summary>
Motivation: Small language models (SLMs) are highly efficient and excel in specific tasks, but have limitations in accuracy compared to state-of-the-art models. This paper investigates whether orchestrating multiple SLMs can improve overall performance.

Method: The authors propose a three-stage approach consisting of: (i) SLM-MUX, a multi-model architecture for coordinating SLMs; (ii) model selection search to choose complementary SLMs; and (iii) test-time scaling tailored to SLM-MUX.

Result: The proposed method achieves up to 13.4% improvement on MATH, 8.8% on GPQA, and 7.0% on GSM8K compared to existing orchestration methods. SLM-MUX, with just two SLMs, outperforms larger models like Qwen 2.5 72B in some tasks.

Conclusion: The paper concludes that multiple SLMs can be orchestrated effectively using the SLM-MUX methodology, resulting in improved accuracy and efficiency over individual models and traditional orchestration techniques.

Abstract: With the rapid development of language models, the number of small language
models (SLMs) has grown significantly. Although they do not achieve
state-of-the-art accuracy, they are more efficient and often excel at specific
tasks. This raises a natural question: can multiple SLMs be orchestrated into a
system where each contributes effectively, achieving higher accuracy than any
individual model? Existing orchestration methods have primarily targeted
frontier models (e.g., GPT-4) and perform suboptimally when applied to SLMs. To
address this gap, we propose a three-stage approach for orchestrating SLMs.
First, we introduce SLM-MUX, a multi-model architecture that effectively
coordinates multiple SLMs. Building on this, we develop two optimization
strategies: (i) a model selection search that identifies the most complementary
SLMs from a given pool, and (ii) test-time scaling tailored to SLM-MUX. Our
approach delivers strong results: Compared to existing orchestration methods,
our approach achieves up to 13.4% improvement on MATH, 8.8% on GPQA, and 7.0%
on GSM8K. With just two SLMS, SLM-MUX outperforms Qwen 2.5 72B on GPQA and
GSM8K, and matches its performance on MATH. We further provide theoretical
analyses to substantiate the advantages of our method. In summary, we
demonstrate that SLMs can be effectively orchestrated into more accurate and
efficient systems through the proposed approach.

</details>


### [200] [TeachLM: Post-Training LLMs for Education Using Authentic Learning Data](https://arxiv.org/abs/2510.05087)
*Janos Perczel,Jin Chow,Dorottya Demszky*

Main category: cs.CL

TL;DR: The paper introduces TeachLM, an LLM fine-tuned for teaching using authentic student-tutor interactions, which significantly improves conversational and pedagogical effectiveness.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap in pedagogical applications of LLMs caused by limited access to student-centric training data and the inherent restrictions of prompt engineering.

Method: TeachLM is developed using parameter-efficient fine-tuning of state-of-the-art models, trained on a dataset of anonymized, authentic student-tutor interactions. The study also proposes a multi-turn evaluation protocol leveraging synthetic dialogue generation.

Result: The fine-tuned TeachLM demonstrates improved outcomes including doubled student talk time, better questioning techniques, increased dialogue turns by 50%, and enhanced personalization in instruction.

Conclusion: Training LLMs with authentic learning data via parameter-efficient fine-tuning significantly enhances their adaptability and effectiveness in educational settings, addressing critical pedagogical limitations.

Abstract: The promise of generative AI to revolutionize education is constrained by the
pedagogical limits of large language models (LLMs). A major issue is the lack
of access to high-quality training data that reflect the learning of actual
students. Prompt engineering has emerged as a stopgap, but the ability of
prompts to encode complex pedagogical strategies in rule-based natural language
is inherently limited. To address this gap we introduce TeachLM - an LLM
optimized for teaching through parameter-efficient fine-tuning of
state-of-the-art models. TeachLM is trained on a dataset comprised of 100,000
hours of one-on-one, longitudinal student-tutor interactions maintained by
Polygence, which underwent a rigorous anonymization process to protect privacy.
We use parameter-efficient fine-tuning to develop an authentic student model
that enables the generation of high-fidelity synthetic student-tutor dialogues.
Building on this capability, we propose a novel multi-turn evaluation protocol
that leverages synthetic dialogue generation to provide fast, scalable, and
reproducible assessments of the dialogical capabilities of LLMs. Our
evaluations demonstrate that fine-tuning on authentic learning data
significantly improves conversational and pedagogical performance - doubling
student talk time, improving questioning style, increasing dialogue turns by
50%, and greater personalization of instruction.

</details>


### [201] [Finish First, Perfect Later: Test-Time Token-Level Cross-Validation for Diffusion Large Language Models](https://arxiv.org/abs/2510.05090)
*Runchu Tian,Junxia Cui,Xueqiang Xu,Feng Yao,Jingbo Shang*

Main category: cs.CL

TL;DR: The paper introduces Tolerator, a novel decoding strategy for diffusion large language models (dLLMs) that addresses the persistent error issue in vanilla decoding by enabling token revision via a two-stage process.


<details>
  <summary>Details</summary>
Motivation: Autoregressive models face limitations in decoding speed and context modeling, and diffusion large language models offer alternatives. However, existing decoding strategies in dLLMs cannot revise previously predicted tokens, leading to persistent errors in predictions.

Method: The proposed Tolerator strategy uses token-level cross-validation refinement with a two-stage approach: sequence fill-up followed by iterative refinement, where subsets of tokens are remasked and decoded while the rest serve as context.

Result: Tolerator was tested across five benchmarks, showing consistent improvements in outputs for language understanding, code generation, and mathematical tasks under identical computational budgets compared to baseline methods.

Conclusion: The study demonstrates that advanced decoding algorithms like Tolerator are essential for maximizing the effectiveness of diffusion large language models, offering a meaningful improvement over current techniques.

Abstract: Diffusion large language models (dLLMs) have recently emerged as a promising
alternative to autoregressive (AR) models, offering advantages such as
accelerated parallel decoding and bidirectional context modeling. However, the
vanilla decoding strategy in discrete dLLMs suffers from a critical limitation:
once a token is accepted, it can no longer be revised in subsequent steps. As a
result, early mistakes persist across iterations, harming both intermediate
predictions and final output quality. To address this issue, we propose
Tolerator (Token-Level Cross-Validation Refinement), a training-free decoding
strategy that leverages cross-validation among predicted tokens. Unlike
existing methods that follow a single progressive unmasking procedure,
Tolerator introduces a two-stage process: (i) sequence fill-up and (ii)
iterative refinement by remasking and decoding a subset of tokens while
treating the remaining as context. This design enables previously accepted
tokens to be reconsidered and corrected when necessary, leading to more
reliable diffusion decoding outputs. We evaluate Tolerator on five standard
benchmarks covering language understanding, code generation, and mathematics.
Experiments show that our method achieves consistent improvements over the
baselines under the same computational budget. These findings suggest that
decoding algorithms are crucial to realizing the full potential of diffusion
large language models. Code and data are publicly available.

</details>


### [202] [Share Your Attention: Transformer Weight Sharing via Matrix-based Dictionary Learning](https://arxiv.org/abs/2508.04581)
*Magauiya Zhussip,Dmitriy Shopkhoev,Ammar Ali,Stamatios Lefkimmiatis*

Main category: cs.CL

TL;DR: The paper presents MASA, a method for structured weight sharing across transformer layers, reducing parameters substantially without sacrificing performance.


<details>
  <summary>Details</summary>
Motivation: The high computational and memory demands of large language models impede their broad deployment despite their transformative impact on AI.

Method: MASA decomposes attention matrices into shared dictionary atoms, allowing for significant parameter reduction while ensuring training with standard optimizers.

Result: MASA reduces attention module parameters by 66.7% and surpasses other methods in benchmark performance across NLP and Vision tasks.

Conclusion: MASA demonstrates scalable, parameter-efficient transformer design suitable for various applications without compromising accuracy or generalization.

Abstract: Large language models (LLMs) have revolutionized AI applications, yet their
high computational and memory demands hinder their widespread deployment.
Existing compression techniques focus on intra-block optimizations (e.g.
low-rank approximation, attention head pruning), while the repetitive layered
structure of transformers implies significant inter-block redundancy - a
dimension largely unexplored beyond key-value (KV) caching. Inspired by
dictionary learning in CNNs, we propose a framework for structured weight
sharing across transformer layers. Our approach decomposes attention projection
matrices into shared dictionary atoms, reducing the attention module's
parameters by 66.7% while achieving on-par performance. Unlike complex methods
requiring distillation or architectural changes, MASA (Matrix Atom Sharing in
Attention) operates as a drop-in replacement - trained with standard optimizers
- and represents each layer's weights as linear combinations of shared matrix
atoms. Experiments across scales (100M-700M parameters) show that MASA achieves
better benchmark accuracy and perplexity than grouped-query attention (GQA),
low-rank baselines and recently proposed Repeat-all-over/Sequential sharing at
comparable parameter budgets. Ablation studies confirm robustness to the
dictionary size and the efficacy of shared representations in capturing
cross-layer statistical regularities. Extending to Vision Transformers (ViT),
MASA matches performance metrics on image classification and detection tasks
with 66.7% fewer attention parameters. By combining dictionary learning
strategies with transformer efficiency, MASA offers a scalable blueprint for
parameter-efficient models without sacrificing performance. Finally, we
investigate the possibility of employing MASA on pretrained LLMs to reduce
their number of parameters without experiencing any significant drop in their
performance.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [203] [SoC-DT: Standard-of-Care Aligned Digital Twins for Patient-Specific Tumor Dynamics](https://arxiv.org/abs/2510.03287)
*Moinak Bhattacharya,Gagandeep Singh,Prateek Prasanna*

Main category: cs.CV

TL;DR: The paper introduces Standard-of-Care Digital Twin (SoC-DT), a computational framework for predicting tumor growth by integrating various models and patient-specific data to improve treatment outcomes.


<details>
  <summary>Details</summary>
Motivation: Conventional tumor growth models fail to account for variability in genomics, demographics, and treatment approaches, hindering accurate predictions for optimizing oncology care.

Method: The authors propose a differentiable framework that integrates reaction-diffusion models, personalized genomics/demographics, and an IMEX-SoC solver to simulate tumor dynamics post-treatment under standard-of-care interventions.

Result: Evaluation on synthetic and real glioma data showed that SoC-DT outperforms traditional PDE methods and purely data-driven models in accurately predicting tumor trajectories under treatment.

Conclusion: SoC-DT combines mechanistic interpretability with scalable solvers to offer a biologically consistent and personalized foundation for predicting tumor outcomes, advancing oncology care.

Abstract: Accurate prediction of tumor trajectories under standard-of-care (SoC)
therapies remains a major unmet need in oncology. This capability is essential
for optimizing treatment planning and anticipating disease progression.
Conventional reaction-diffusion models are limited in scope, as they fail to
capture tumor dynamics under heterogeneous therapeutic paradigms. There is
hence a critical need for computational frameworks that can realistically
simulate SoC interventions while accounting for inter-patient variability in
genomics, demographics, and treatment regimens. We introduce Standard-of-Care
Digital Twin (SoC-DT), a differentiable framework that unifies
reaction-diffusion tumor growth models, discrete SoC interventions (surgery,
chemotherapy, radiotherapy) along with genomic and demographic personalization
to predict post-treatment tumor structure on imaging. An implicit-explicit
exponential time-differencing solver, IMEX-SoC, is also proposed, which ensures
stability, positivity, and scalability in SoC treatment situations. Evaluated
on both synthetic data and real world glioma data, SoC-DT consistently
outperforms classical PDE baselines and purely data-driven neural models in
predicting tumor dynamics. By bridging mechanistic interpretability with modern
differentiable solvers, SoC-DT establishes a principled foundation for
patient-specific digital twins in oncology, enabling biologically consistent
tumor dynamics estimation. Code will be made available upon acceptance.

</details>


### [204] [Visualizing Celebrity Dynamics in Video Content: A Proposed Approach Using Face Recognition Timestamp Data](https://arxiv.org/abs/2510.03292)
*Doğanay Demir,İlknur Durgar Elkahlout*

Main category: cs.CV

TL;DR: This paper introduces a hybrid framework that uses multi-GPU inference and interactive visualizations to analyze celebrity dynamics in video episodes, providing insights into patterns across episodes and seasons.


<details>
  <summary>Details</summary>
Motivation: The increasing prevalence of video content necessitates tools to analyze its structure and dynamics, particularly for celebrity appearances, to gain actionable insights.

Method: A distributed multi-GPU inference system processes video data with optimized ONNX models and generates timestamped appearance records, which are visualized interactively using tools like charts and network graphs.

Result: The system successfully generates multi-dimensional insights into celebrity prominence, screen-time distribution, and relationships by transforming appearance records into diverse dynamic visualizations.

Conclusion: The hybrid approach bridges recognition and analytics, enabling entertainment industry advancements in content creation, audience engagement, and strategic studies.

Abstract: In an era dominated by video content, understanding its structure and
dynamics has become increasingly important. This paper presents a hybrid
framework that combines a distributed multi-GPU inference system with an
interactive visualization platform for analyzing celebrity dynamics in video
episodes. The inference framework efficiently processes large volumes of video
data by leveraging optimized ONNX models, heterogeneous batch inference, and
high-throughput parallelism, ensuring scalable generation of timestamped
appearance records. These records are then transformed into a comprehensive
suite of visualizations, including appearance frequency charts, duration
analyses, pie charts, co-appearance matrices, network graphs, stacked area
charts, seasonal comparisons, and heatmaps. Together, these visualizations
provide multi-dimensional insights into video content, revealing patterns in
celebrity prominence, screen-time distribution, temporal dynamics,
co-appearance relationships, and intensity across episodes and seasons. The
interactive nature of the system allows users to dynamically explore data,
identify key moments, and uncover evolving relationships between individuals.
By bridging distributed recognition with structured, visually-driven analytics,
this work enables new possibilities for entertainment analytics, content
creation strategies, and audience engagement studies.

</details>


### [205] [Domain-Robust Marine Plastic Detection Using Vision Models](https://arxiv.org/abs/2510.03294)
*Saanvi Kataria*

Main category: cs.CV

TL;DR: This paper examines the cross-domain robustness of underwater plastic debris detectors using various CNNs, vision transformers, and zero-shot models. Results show that lightweight CNNs, like MobileNetV2, excel in generalization and performance.


<details>
  <summary>Details</summary>
Motivation: The study was motivated by the need for robust and automated underwater plastic debris detection systems due to the growing environmental issue of marine plastic pollution.

Method: The paper benchmarked CNNs (MobileNetV2, ResNet-18, EfficientNet-B0), vision transformers (DeiT-Tiny, ViT-B16), and two zero-shot models (CLIP ViT-L14, Gemini 2.0 Flash). They trained models on labeled underwater datasets and evaluated them on cross-domain tests.

Result: MobileNetV2 achieved the best cross-domain performance with F1 0.97. Zero-shot models displayed complementary strengths: CLIP had higher Recall but lower Precision, while Gemini excelled in Precision but had a lower Recall.

Conclusion: Compact CNNs with supervised training generalize well for underwater detection, while pretrained vision-language models like CLIP and Gemini offer unique advantages depending on use cases.

Abstract: Marine plastic pollution is a pressing environmental threat, making reliable
automation for underwater debris detection essential. However, vision systems
trained on one dataset often degrade on new imagery due to domain shift. This
study benchmarks models for cross-domain robustness, training convolutional
neural networks - CNNs (MobileNetV2, ResNet-18, EfficientNet-B0) and vision
transformers (DeiT-Tiny, ViT-B16) on a labeled underwater dataset and then
evaluates them on a balanced cross-domain test set built from plastic-positive
images drawn from a different source and negatives from the training domain.
Two zero-shot models were assessed, CLIP ViT-L14 and Google's Gemini 2.0 Flash,
that leverage pretraining to classify images without fine-tuning. Results show
the lightweight MobileNetV2 delivers the strongest cross-domain performance (F1
0.97), surpassing larger models. All fine-tuned models achieved high Precision
(around 99%), but differ in Recall, indicating varying sensitivity to plastic
instances. Zero-shot CLIP is comparatively sensitive (Recall around 80%) yet
prone to false positives (Precision around 56%), whereas Gemini exhibits the
inverse profile (Precision around 99%, Recall around 81%). Error analysis
highlights recurring confusions with coral textures, suspended particulates,
and specular glare. Overall, compact CNNs with supervised training can
generalize effectively for cross-domain underwater detection, while large
pretrained vision-language models provide complementary strengths.

</details>


### [206] [Multimodal Arabic Captioning with Interpretable Visual Concept Integration](https://arxiv.org/abs/2510.03295)
*Passant Elchafei,Amany Fashwan*

Main category: cs.CV

TL;DR: VLCAP integrates CLIP-based visual label retrieval and multimodal text generation to create accurate Arabic image captions, achieving notable scores through various encoder-decoder configurations.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to address the challenge of generating culturally and contextually accurate Arabic image captions using interpretable visual concept extraction and advanced multimodal approaches.

Method: The framework utilizes three multilingual encoders to retrieve visual labels, builds a hybrid vocabulary enriched with Visual Genome dataset labels, and employs vision-language models to generate captions in two stages.

Result: The mCLIP + Gemini Pro Vision configuration achieved the best BLEU-1 (5.34%) and cosine similarity (60.01%), while AraCLIP + Qwen-VL showed the highest LLM-judge score (36.33%).

Conclusion: This interpretable pipeline effectively enhances Arabic image captioning by combining label retrieval with multimodal generation, offering a culturally coherent and contextually relevant solution.

Abstract: We present VLCAP, an Arabic image captioning framework that integrates
CLIP-based visual label retrieval with multimodal text generation. Rather than
relying solely on end-to-end captioning, VLCAP grounds generation in
interpretable Arabic visual concepts extracted with three multilingual
encoders, mCLIP, AraCLIP, and Jina V4, each evaluated separately for label
retrieval. A hybrid vocabulary is built from training captions and enriched
with about 21K general domain labels translated from the Visual Genome dataset,
covering objects, attributes, and scenes. The top-k retrieved labels are
transformed into fluent Arabic prompts and passed along with the original image
to vision-language models. In the second stage, we tested Qwen-VL and Gemini
Pro Vision for caption generation, resulting in six encoder-decoder
configurations. The results show that mCLIP + Gemini Pro Vision achieved the
best BLEU-1 (5.34%) and cosine similarity (60.01%), while AraCLIP + Qwen-VL
obtained the highest LLM-judge score (36.33%). This interpretable pipeline
enables culturally coherent and contextually accurate Arabic captions.

</details>


### [207] [Convolutional Neural Nets vs Vision Transformers: A SpaceNet Case Study with Balanced vs Imbalanced Regimes](https://arxiv.org/abs/2510.03297)
*Akshar Gothi*

Main category: cs.CV

TL;DR: This study compares EfficientNet-B0 and Vision Transformer (ViT-Base) models on SpaceNet under different class balance scenarios, focusing on performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: To evaluate the relative strengths and weaknesses of CNNs and Vision Transformers under varying label-distribution regimes and provide insight into their practical trade-offs in imbalanced vs. balanced datasets.

Method: The authors applied both EfficientNet-B0 (CNN) and ViT-Base (Vision Transformer) on SpaceNet data using matched preprocessing, modest augmentations, and a training budget of 40 epochs on a single NVIDIA P100 GPU. Two label-distribution setups were tested: a naturally imbalanced five-class split and a balanced-resampled split.

Result: EfficientNet-B0 achieved 93% accuracy on the imbalanced split, with strong macro-F1 and lower latency, while ViT-Base reached a comparable 93% with higher parameters and runtime. On the balanced split, EfficientNet-B0 reached 99% accuracy, and ViT-Base remained competitive. Balancing reduced architecture differences while CNNs maintained efficiency.

Conclusion: EfficientNet-B0 and ViT-Base are both competitive, but CNNs like EfficientNet-B0 exhibit an efficiency edge, particularly in balanced datasets. The study highlights how dataset balancing can influence model comparisons, narrowing performance gaps between architectures.

Abstract: We present a controlled comparison of a convolutional neural network
(EfficientNet-B0) and a Vision Transformer (ViT-Base) on SpaceNet under two
label-distribution regimes: a naturally imbalanced five-class split and a
balanced-resampled split with 700 images per class (70:20:10 train/val/test).
With matched preprocessing (224x224, ImageNet normalization), lightweight
augmentations, and a 40-epoch budget on a single NVIDIA P100, we report
accuracy, macro-F1, balanced accuracy, per-class recall, and deployment metrics
(model size and latency). On the imbalanced split, EfficientNet-B0 reaches 93%
test accuracy with strong macro-F1 and lower latency; ViT-Base is competitive
at 93% with a larger parameter count and runtime. On the balanced split, both
models are strong; EfficientNet-B0 reaches 99% while ViT-Base remains
competitive, indicating that balancing narrows architecture gaps while CNNs
retain an efficiency edge. We release manifests, logs, and per-image
predictions to support reproducibility.

</details>


### [208] [A Comprehensive Review on Artificial Intelligence Empowered Solutions for Enhancing Pedestrian and Cyclist Safety](https://arxiv.org/abs/2510.03314)
*Shucheng Zhang,Yan Shi,Bingzhang Wang,Yuang Zhang,Muhammad Monjurul Karim,Kehua Chen,Chenxi Liu,Mehrdad Nasri,Yinhai Wang*

Main category: cs.CV

TL;DR: The paper reviews advancements in AI-driven camera-based systems to protect vulnerable road users (VRUs) and identifies challenges for future research.


<details>
  <summary>Details</summary>
Motivation: Conventional infrastructure measures often fail to ensure VRU safety in dynamic urban environments. AI offers new opportunities for enhanced protection.

Method: The authors review recent progress in AI applications for VRU protection across four tasks: detection/classification, tracking/reidentification, trajectory prediction, and intent prediction.

Result: Significant advancements in AI-driven methods for VRUs are highlighted, revealing gaps and open challenges in data, models, and deployment.

Conclusion: The survey proposes a foundational framework and identifies future challenges to aid the development of proactive AI systems for enhancing VRU safety.

Abstract: Ensuring the safety of vulnerable road users (VRUs), such as pedestrians and
cyclists, remains a critical global challenge, as conventional
infrastructure-based measures often prove inadequate in dynamic urban
environments. Recent advances in artificial intelligence (AI), particularly in
visual perception and reasoning, open new opportunities for proactive and
context-aware VRU protection. However, existing surveys on AI applications for
VRUs predominantly focus on detection, offering limited coverage of other
vision-based tasks that are essential for comprehensive VRU understanding and
protection. This paper presents a state-of-the-art review of recent progress in
camera-based AI sensing systems for VRU safety, with an emphasis on
developments from the past five years and emerging research trends. We
systematically examine four core tasks, namely detection and classification,
tracking and reidentification, trajectory prediction, and intent recognition
and prediction, which together form the backbone of AI-empowered proactive
solutions for VRU protection in intelligent transportation systems. To guide
future research, we highlight four major open challenges from the perspectives
of data, model, and deployment. By linking advances in visual AI with practical
considerations for real-world implementation, this survey aims to provide a
foundational reference for the development of next-generation sensing systems
to enhance VRU safety.

</details>


### [209] [The View From Space: Navigating Instrumentation Differences with EOFMs](https://arxiv.org/abs/2510.03316)
*Ryan P. Demilt,Nicholas LaHaye,Karis Tenneson*

Main category: cs.CV

TL;DR: Understanding EOFMs' sensitivity to sensor architectures enhances their development for remote-sensing tasks, emphasizing the role of internal representations.


<details>
  <summary>Details</summary>
Motivation: The rapid rise of Earth Observation Foundation Models handling diverse remote sensing datasets raises the need to investigate their underlying representation sensitivity toward sensor architectures.

Method: The paper explores the sensitivity of EOFMs' internal representation space to variations in sensor architectures, studying how modality-specific training impacts robustness and performance.

Result: EOFMs manifest significant variability in internal representation based on sensor designs, challenging their cross-modality application and reliability.

Conclusion: The sensitivity of EOFMs to sensor architecture differences highlights the need for improved design methodologies and robust embeddings to fulfill diverse earth observation tasks effectively.

Abstract: Earth Observation Foundation Models (EOFMs) have exploded in prevalence as
tools for processing the massive volumes of remotely sensed and other earth
observation data, and for delivering impact on the many essential earth
monitoring tasks. An emerging trend posits using the outputs of pre-trained
models as 'embeddings' which summarize high dimensional data to be used for
generic tasks such as similarity search and content-specific queries. However,
most EOFM models are trained only on single modalities of data and then applied
or benchmarked by matching bands across different modalities. It is not clear
from existing work what impact diverse sensor architectures have on the
internal representations of the present suite of EOFMs. We show in this work
that the representation space of EOFMs is highly sensitive to sensor
architecture and that understanding this difference gives a vital perspective
on the pitfalls of current EOFM design and signals for how to move forward as
model developers, users, and a community guided by robust remote-sensing
science.

</details>


### [210] [Photorealistic Inpainting for Perturbation-based Explanations in Ecological Monitoring](https://arxiv.org/abs/2510.03317)
*Günel Aghakishiyeva,Jiayi Zhou,Saagar Arya,James David Poling,Holly R. Houliston,Jamie N. Womble,David W. Johnston,Brinnae Bent*

Main category: cs.CV

TL;DR: The paper introduces an explanation technique using inpainting-guided perturbations to make ecological AI models more interpretable and trustworthy.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the lack of transparency in predictions made by AI vision models used for ecological monitoring, which limits their adoption in real-world scenarios.

Method: It presents an explanation method involving photorealistic, mask-localized edits using inpainting techniques and Segment-Anything-Model-refined masks to alter scenes while preserving context.

Result: Applying the method on a YOLOv9 detector for harbor seal detection, the authors achieve explanations that highlight diagnostic features, avoid common artifacts from traditional methods, and provide ecologically relevant insights.

Conclusion: The approach enhances expert trust in AI models by offering interpretable, context-preserving explanations, potentially boosting real-world deployment in ecology.

Abstract: Ecological monitoring is increasingly automated by vision models, yet opaque
predictions limit trust and field adoption. We present an inpainting-guided,
perturbation-based explanation technique that produces photorealistic,
mask-localized edits that preserve scene context. Unlike masking or blurring,
these edits stay in-distribution and reveal which fine-grained morphological
cues drive predictions in tasks such as species recognition and trait
attribution. We demonstrate the approach on a YOLOv9 detector fine-tuned for
harbor seal detection in Glacier Bay drone imagery, using
Segment-Anything-Model-refined masks to support two interventions: (i) object
removal/replacement (e.g., replacing seals with plausible ice/water or boats)
and (ii) background replacement with original animals composited onto new
scenes. Explanations are assessed by re-scoring perturbed images (flip rate,
confidence drop) and by expert review for ecological plausibility and
interpretability. The resulting explanations localize diagnostic structures,
avoid deletion artifacts common to traditional perturbations, and yield
domain-relevant insights that support expert validation and more trustworthy
deployment of AI in ecology.

</details>


### [211] [Advances in Medical Image Segmentation: A Comprehensive Survey with a Focus on Lumbar Spine Applications](https://arxiv.org/abs/2510.03318)
*Ahmed Kabil,Ghada Khoriba,Mina Yousef,Essam A. Rashed*

Main category: cs.CV

TL;DR: The paper surveys traditional and deep learning-based methods for Medical Image Segmentation (MIS), discussing advances and challenges, and includes a case study on lumbar spine segmentation.


<details>
  <summary>Details</summary>
Motivation: MIS is vital in medical diagnostics and treatment planning, but a systematic review is needed to consolidate traditional methodologies with cutting-edge deep learning techniques.

Method: The paper systematically reviews traditional MIS techniques (e.g., thresholding, edge detection) and modern deep learning methods (e.g., CNNs, GANs, attention mechanisms, Transformer models). It explores emerging trends and presents a detailed case study on lumbar spine segmentation.

Result: The survey identifies strengths and limitations of existing MIS approaches, highlights the progress in deep learning, and pinpoints unresolved challenges such as dataset bias and model interpretability.

Conclusion: While deep learning advances are transformative, key limitations remain in real-world application, requiring further research to bridge gaps in model generalization, domain adaptation, and clinical integration.

Abstract: Medical Image Segmentation (MIS) stands as a cornerstone in medical image
analysis, playing a pivotal role in precise diagnostics, treatment planning,
and monitoring of various medical conditions. This paper presents a
comprehensive and systematic survey of MIS methodologies, bridging the gap
between traditional image processing techniques and modern deep learning
approaches. The survey encompasses thresholding, edge detection, region-based
segmentation, clustering algorithms, and model-based techniques while also
delving into state-of-the-art deep learning architectures such as Convolutional
Neural Networks (CNNs), Fully Convolutional Networks (FCNs), and the widely
adopted U-Net and its variants. Moreover, integrating attention mechanisms,
semi-supervised learning, generative adversarial networks (GANs), and
Transformer-based models is thoroughly explored. In addition to covering
established methods, this survey highlights emerging trends, including hybrid
architectures, cross-modality learning, federated and distributed learning
frameworks, and active learning strategies, which aim to address challenges
such as limited labeled datasets, computational complexity, and model
generalizability across diverse imaging modalities. Furthermore, a specialized
case study on lumbar spine segmentation is presented, offering insights into
the challenges and advancements in this relatively underexplored anatomical
region. Despite significant progress in the field, critical challenges persist,
including dataset bias, domain adaptation, interpretability of deep learning
models, and integration into real-world clinical workflows.

</details>


### [212] [DECOR: Deep Embedding Clustering with Orientation Robustness](https://arxiv.org/abs/2510.03328)
*Fiona Victoria Stanley Jothiraj,Arunaggiri Pandian Karunanidhi,Seth A. Eichmeyer*

Main category: cs.CV

TL;DR: The paper introduces DECOR, a robust deep clustering framework for wafer defect detection, excelling in performance and scalability.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of complex, unlabeled, and imbalanced wafer defect data in semiconductor manufacturing, ensuring reliable defect detection despite data imperfections.

Method: The DECOR framework uses deep clustering and incorporates orientation robustness to consistently group defect patterns in wafer maps, even under rotational or alignment variations.

Result: DECOR outperformed existing clustering methods on the MixedWM38 dataset, demonstrating its reliability and scalability without manual parameter tuning.

Conclusion: DECOR offers an effective solution for automated visual inspection systems by providing robust and consistent defect pattern clustering in challenging semiconductor manufacturing data settings.

Abstract: In semiconductor manufacturing, early detection of wafer defects is critical
for product yield optimization. However, raw wafer data from wafer quality
tests are often complex, unlabeled, imbalanced and can contain multiple defects
on a single wafer, making it crucial to design clustering methods that remain
reliable under such imperfect data conditions. We introduce DECOR, a deep
clustering with orientation robustness framework that groups complex defect
patterns from wafer maps into consistent clusters. We evaluate our method on
the open source MixedWM38 dataset, demonstrating its ability to discover
clusters without manual tuning. DECOR explicitly accounts for orientation
variations in wafer maps, ensuring that spatially similar defects are
consistently clustered regardless of its rotation or alignment. Experiments
indicate that our method outperforms existing clustering baseline methods, thus
providing a reliable and scalable solution in automated visual inspection
systems.

</details>


### [213] [Error correction in multiclass image classification of facial emotion on unbalanced samples](https://arxiv.org/abs/2510.03337)
*Andrey A. Lebedev,Victor B. Kazantsev,Sergey V. Stasenko*

Main category: cs.CV

TL;DR: The paper addresses error correction in multi-class facial emotion classification on unbalanced samples using a neural network model with LSTM and attention mechanism.


<details>
  <summary>Details</summary>
Motivation: The study aims to tackle challenges posed by unbalanced class distribution in facial emotion classification, where rare emotions are harder to classify accurately.

Method: A neural network model leveraging LSTM and attention mechanisms identifies facial regions informative for emotion recognition. Various configurations of training subsets are tested to correct errors in the excluded seventh class.

Result: The model successfully corrects errors across all classes to varying degrees. It improves key quality metrics for smaller classes, showcasing its effectiveness in identifying rare emotions.

Conclusion: The proposed approach is promising for applications like facial expression analysis and anti-fraud systems that require reliable classification despite class imbalance.

Abstract: This paper considers the problem of error correction in multi-class
classification of face images on unbalanced samples. The study is based on the
analysis of a data frame containing images labeled by seven different emotional
states of people of different ages. Particular attention is paid to the problem
of class imbalance, in which some emotions significantly prevail over others.
To solve the classification problem, a neural network model based on LSTM with
an attention mechanism focusing on key areas of the face that are informative
for emotion recognition is used. As part of the experiments, the model is
trained on all possible configurations of subsets of six classes with
subsequent error correction for the seventh class, excluded at the training
stage. The results show that correction is possible for all classes, although
the degree of success varies: some classes are better restored, others are
worse. In addition, on the test sample, when correcting some classes, an
increase in key quality metrics for small classes was recorded, which indicates
the promise of the proposed approach in solving applied problems related to the
search for rare events, for example, in anti-fraud systems. Thus, the proposed
method can be effectively applied in facial expression analysis systems and in
tasks requiring stable classification under skewed class distribution.

</details>


### [214] [OpusAnimation: Code-Based Dynamic Chart Generation](https://arxiv.org/abs/2510.03341)
*Bozheng Li,Miao Yang,Zhenhan Chen,Jiawang Cao,Mushui Liu,Yi Lu,Yongliang Wu,Bin Zhang,Yangguang Ji,Licheng Tang,Jay Wu,Wenbo Zhu*

Main category: cs.CV

TL;DR: The paper introduces DCG-Bench and DCG-8K for evaluating multi-modal large language models (MLLMs) on dynamic chart generation tasks. A novel model, Qwen2.5-VL-DCG-3B, outperforms existing benchmarks.


<details>
  <summary>Details</summary>
Motivation: Dynamic chart generation and understanding in MLLMs remain underexplored, requiring a benchmark to evaluate and enhance these capabilities.

Method: Developed DCG-Bench, DCG-8K dataset, and a two-stage training recipe with Joint-Code-Visual Reward for optimizing group relative policy to train Qwen2.5-VL-DCG-3B.

Result: Qwen2.5-VL-DCG-3B achieves an 8.31% performance gain over existing open-source models and performs comparably to proprietary models with smaller parameters.

Conclusion: The proposed dataset and training methodology effectively enhance MLLMs' dynamic chart generation abilities, addressing existing shortcomings. Both dataset and code will be made publicly available.

Abstract: Dynamic Chart Generation (DCG) involves producing code-rendered animated
visualizations as charts. While recent advances in multi-modal large language
models (MLLMs) have significantly improved their capability on static chart
generation and comprehension, MLLMs' potential for handling dynamic chart
generation and understanding remains underexplored. To bridge this research
gap, we introduce DCG-Bench (Dynamic Chart Generation Benchmark), the first
benchmark evaluating MLLM's capability on dynamic chart generation tasks from
three dimensions: Simple Text-to-Chart, Detailed Text-to-Chart, and
Video-to-Chart tasks. We construct DCG-8K, a high-quality DCG dataset with
annotations covering instruction-code-video triplets and QA pairs for both code
and video evaluation. Based on DCG-8K, we explored a two-stage training recipe,
proposing Joint-Code-Visual Reward for group relative policy optimization to
construct expert MLLM Qwen2.5-VL-DCG-3B for the DCG task. Our benchmarking
result reveals shortcomings of existing MLLMs in the visual-to-chart task, and
our model beats the best open-sourced MLLM with an average 8.31% performance
gain across three tasks, and shows on par performance against proprietary
models with only 3B parameters, proving the effectiveness of our training
recipe. Our code and dataset will be publicly available.

</details>


### [215] [Visual Odometry with Transformers](https://arxiv.org/abs/2510.03348)
*Vlardimir Yugay,Duy-Kien Nguyen,Theo Gevers,Cees G. M. Snoek,Martin R. Oswald*

Main category: cs.CV

TL;DR: This paper proposes VoT (Visual odometry Transformer), an end-to-end model for monocular visual odometry that eliminates traditional handcrafted techniques and predicts camera motion directly.


<details>
  <summary>Details</summary>
Motivation: Existing monocular visual odometry methods struggle with complex pipelines dependent on camera calibration and hyperparameter tuning, limiting their performance on unseen real-world scenarios.

Method: The method involves using VoT, a transformer-based framework that processes sequences of monocular frames with spatial and temporal attention, predicting camera motion directly without estimating geometry.

Result: VoT scales effectively with larger datasets, benefits from strong pre-trained backbones, generalizes across diverse settings, and outperforms traditional methods while being over 3 times faster.

Conclusion: The approach is efficient, modular, and generalizable, transforming monocular visual odometry by removing the need for handcrafted components and setting a new benchmark for performance and speed.

Abstract: Modern monocular visual odometry methods typically combine pre-trained deep
learning components with optimization modules, resulting in complex pipelines
that rely heavily on camera calibration and hyperparameter tuning, and often
struggle in unseen real-world scenarios. Recent large-scale 3D models trained
on massive amounts of multi-modal data have partially alleviated these
challenges, providing generalizable dense reconstruction and camera pose
estimation. Still, they remain limited in handling long videos and providing
accurate per-frame estimates, which are required for visual odometry. In this
work, we demonstrate that monocular visual odometry can be addressed
effectively in an end-to-end manner, thereby eliminating the need for
handcrafted components such as bundle adjustment, feature matching, camera
calibration, or dense 3D reconstruction. We introduce VoT, short for Visual
odometry Transformer, which processes sequences of monocular frames by
extracting features and modeling global relationships through temporal and
spatial attention. Unlike prior methods, VoT directly predicts camera motion
without estimating dense geometry and relies solely on camera poses for
supervision. The framework is modular and flexible, allowing seamless
integration of various pre-trained encoders as feature extractors. Experimental
results demonstrate that VoT scales effectively with larger datasets, benefits
substantially from stronger pre-trained backbones, generalizes across diverse
camera motions and calibration settings, and outperforms traditional methods
while running more than 3 times faster. The code will be released.

</details>


### [216] [Inference-Time Search using Side Information for Diffusion-based Image Reconstruction](https://arxiv.org/abs/2510.03352)
*Mahdi Farahbakhsh,Vishnu Teja Kunde,Dileep Kalathil,Krishna Narayanan,Jean-Francois Chamberland*

Main category: cs.CV

TL;DR: This paper introduces a new inference-time search algorithm for diffusion models used in inverse problem-solving, utilizing side information to enhance reconstructions.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion model approaches for inverse problems often fail to incorporate side information, which is critical in achieving accurate reconstructions, especially in challenging ill-posed scenarios.

Method: The paper proposes an inference-time search algorithm that balances exploration and exploitation, integrating side information into the sampling process to enhance image reconstruction quality. This sidesteps artifacts arising from gradient-based guidance.

Result: Experiments demonstrate consistent qualitative and quantitative improvements in reconstruction tasks like inpainting, super-resolution, and various deblurring types. The method outperforms traditional reward gradient-based algorithms.

Conclusion: The proposed algorithm offers a superior alternative to existing reconstruction methods, achieving better accuracy by effectively leveraging side information in diffusion-based models.

Abstract: Diffusion models have emerged as powerful priors for solving inverse
problems. However, existing approaches typically overlook side information that
could significantly improve reconstruction quality, especially in severely
ill-posed settings. In this work, we propose a novel inference-time search
algorithm that guides the sampling process using the side information in a
manner that balances exploration and exploitation. This enables more accurate
and reliable reconstructions, providing an alternative to the gradient-based
guidance that is prone to reward-hacking artifacts. Our approach can be
seamlessly integrated into a wide range of existing diffusion-based image
reconstruction pipelines. Through extensive experiments on a number of inverse
problems, such as box inpainting, super-resolution, and various deblurring
tasks including motion, Gaussian, nonlinear, and blind deblurring, we show that
our approach consistently improves the qualitative and quantitative performance
of diffusion-based image reconstruction algorithms. We also show the superior
performance of our approach with respect to other baselines, including reward
gradient-based guidance algorithms. The code is available at
\href{https://github.com/mhdfb/sideinfo-search-reconstruction}{this
repository}.

</details>


### [217] [Sonar Image Datasets: A Comprehensive Survey of Resources, Challenges, and Applications](https://arxiv.org/abs/2510.03353)
*Larissa S. Gomes,Gustavo P. Almeida,Bryan U. Moreira,Marco Quiroz,Breno Xavier,Lucas Soares,Stephanie L. Brião,Felipe G. Oliveira,Paulo L. J. Drews-Jr*

Main category: cs.CV

TL;DR: The paper reviews the current availability of well-annotated sonar image datasets, identifies gaps, and provides a roadmap for researchers in underwater acoustic data analysis.


<details>
  <summary>Details</summary>
Motivation: To address the scarcity of publicly available and well-annotated sonar image datasets that hinder the development of machine learning models for underwater exploration and related applications.

Method: The authors cataloged and analyzed publicly accessible sonar image datasets across multiple sonar modalities and applications (e.g., classification, detection, segmentation, 3D reconstruction), creating a detailed synthesis of findings in tables and timelines.

Result: The study offers a detailed review and comparison of the datasets, highlighting their characteristics, sizes, and annotations while pinpointing existing gaps in available resources.

Conclusion: The paper provides a foundational resource for researchers, guiding them in starting or advancing in underwater acoustic data analysis by contextualizing state-of-the-art datasets and identifying future needs.

Abstract: Sonar images are relevant for advancing underwater exploration, autonomous
navigation, and ecosystem monitoring. However, the progress depends on data
availability. The scarcity of publicly available, well-annotated sonar image
datasets creates a significant bottleneck for the development of robust machine
learning models. This paper presents a comprehensive and concise review of the
current landscape of sonar image datasets, seeking not only to catalog existing
resources but also to contextualize them, identify gaps, and provide a clear
roadmap, serving as a base guide for researchers of any kind who wish to start
or advance in the field of underwater acoustic data analysis. We mapped
publicly accessible datasets across various sonar modalities, including Side
Scan Sonar (SSS), Forward-Looking Sonar (FLS), Synthetic Aperture Sonar (SAS),
Multibeam Echo Sounder (MBES), and Dual-Frequency Identification Sonar
(DIDSON). An analysis was conducted on applications such as classification,
detection, segmentation, and 3D reconstruction. This work focuses on
state-of-the-art advancements, incorporating newly released datasets. The
findings are synthesized into a master table and a chronological timeline,
offering a clear and accessible comparison of characteristics, sizes, and
annotation details datasets.

</details>


### [218] [Learned Display Radiance Fields with Lensless Cameras](https://arxiv.org/abs/2510.03356)
*Ziyang Chen,Yuta Itoh,Kaan Akşit*

Main category: cs.CV

TL;DR: This paper introduces a lensless camera and an Implicit Neural Representation algorithm to simplify and optimize display calibration from various viewpoints.


<details>
  <summary>Details</summary>
Motivation: Current techniques for display calibration require specialized tools and controlled environments, making the process inaccessible and cumbersome for many content creators.

Method: The authors designed a lensless camera and used an Implicit Neural Representation based algorithm to reconstruct light fields from displays across a viewing cone of 46.6° × 37.6°.

Result: The proposed pipeline successfully captures display characteristics without requiring hardware like specialized equipment or dark rooms.

Conclusion: This work demonstrates an innovative, hardware-free approach to display calibration, setting a foundation for more user-friendly and accessible calibration techniques.

Abstract: Calibrating displays is a basic and regular task that content creators must
perform to maintain optimal visual experience, yet it remains a troublesome
issue. Measuring display characteristics from different viewpoints often
requires specialized equipment and a dark room, making it inaccessible to most
users. To avoid specialized hardware requirements in display calibrations, our
work co-designs a lensless camera and an Implicit Neural Representation based
algorithm for capturing display characteristics from various viewpoints. More
specifically, our pipeline enables efficient reconstruction of light fields
emitted from a display from a viewing cone of 46.6{\deg} X 37.6{\deg}. Our
emerging pipeline paves the initial steps towards effortless display
calibration and characterization.

</details>


### [219] [Provenance Networks: End-to-End Exemplar-Based Explainability](https://arxiv.org/abs/2510.03361)
*Ali Kayyam,Anusha Madan Gopal,M. Anthony Lewis*

Main category: cs.CV

TL;DR: Provenance networks are neural models designed to link predictions directly to their training data for enhanced explainability, acting like a learned KNN. They integrate interpretability into model architecture while addressing challenges like opaqueness and hallucination in deep learning.


<details>
  <summary>Details</summary>
Motivation: Current deep learning models face issues such as lack of transparency and difficulty in validating predictions due to opaqueness and post-hoc explainability methods.

Method: Provenance networks embed explainability directly by learning connections between predictions and training examples, using a concept similar to learned KNN for justifications.

Result: They allow systematic exploration of memorization vs. generalization, provide training data verification, enable identification of anomalous data, enhance input robustness, and offer better transparency into model behavior.

Conclusion: Provenance networks advance explainability, robustness, and trustworthiness in neural systems, though at added computational cost and limitations on dataset scale.

Abstract: We introduce provenance networks, a novel class of neural models designed to
provide end-to-end, training-data-driven explainability. Unlike conventional
post-hoc methods, provenance networks learn to link each prediction directly to
its supporting training examples as part of the model's normal operation,
embedding interpretability into the architecture itself. Conceptually, the
model operates similarly to a learned KNN, where each output is justified by
concrete exemplars weighted by relevance in the feature space. This approach
facilitates systematic investigations of the trade-off between memorization and
generalization, enables verification of whether a given input was included in
the training set, aids in the detection of mislabeled or anomalous data points,
enhances resilience to input perturbations, and supports the identification of
similar inputs contributing to the generation of a new data point. By jointly
optimizing the primary task and the explainability objective, provenance
networks offer insights into model behavior that traditional deep networks
cannot provide. While the model introduces additional computational cost and
currently scales to moderately sized datasets, it provides a complementary
approach to existing explainability techniques. In particular, it addresses
critical challenges in modern deep learning, including model opaqueness,
hallucination, and the assignment of credit to data contributors, thereby
improving transparency, robustness, and trustworthiness in neural models.

</details>


### [220] [Unified Unsupervised Anomaly Detection via Matching Cost Filtering](https://arxiv.org/abs/2510.03363)
*Zhe Zhang,Mingxiu Cai,Gaochang Wu,Jing Zhang,Lingqiao Liu,Dacheng Tao,Tianyou Chai,Xiatian Zhu*

Main category: cs.CV

TL;DR: The paper proposes a generic post-processing framework, Unified Cost Filtering (UCF), to improve unsupervised anomaly detection (UAD) in both unimodal and multimodal scenarios, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Unsupervised anomaly detection faces challenges in detecting image- and pixel-level anomalies due to noise in matching processes and isolated advancements between unimodal and multimodal approaches. A unified approach is needed to bridge this gap and improve detection performance.

Method: The authors present Unified Cost Filtering (UCF), a framework that refines anomaly cost volumes by matching test samples with normal samples across one or multiple modalities. It uses a learnable filtering module with multi-layer attention to mitigate matching noise and enhance subtle anomaly detection.

Result: UCF improves the performance of various UAD methods, achieving new state-of-the-art results across 22 benchmarks in unimodal (RGB) and multimodal (RGB--3D, RGB--Text) scenarios.

Conclusion: The study demonstrates the utility of UCF as a versatile refinement framework for UAD models, showing significant enhancements in anomaly detection across diverse datasets and modal applications. Code and models are made openly available.

Abstract: Unsupervised anomaly detection (UAD) aims to identify image- and pixel-level
anomalies using only normal training data, with wide applications such as
industrial inspection and medical analysis, where anomalies are scarce due to
privacy concerns and cold-start constraints. Existing methods, whether
reconstruction-based (restoring normal counterparts) or embedding-based
(pretrained representations), fundamentally conduct image- or feature-level
matching to generate anomaly maps. Nonetheless, matching noise has been largely
overlooked, limiting their detection ability. Beyond earlier focus on unimodal
RGB-based UAD, recent advances expand to multimodal scenarios, e.g., RGB--3D
and RGB--Text, enabled by point cloud sensing and vision--language models.
Despite shared challenges, these lines remain largely isolated, hindering a
comprehensive understanding and knowledge transfer. In this paper, we advocate
unified UAD for both unimodal and multimodal settings in the matching
perspective. Under this insight, we present Unified Cost Filtering (UCF), a
generic post-hoc refinement framework for refining anomaly cost volume of any
UAD model. The cost volume is constructed by matching a test sample against
normal samples from the same or different modalities, followed by a learnable
filtering module with multi-layer attention guidance from the test sample,
mitigating matching noise and highlighting subtle anomalies. Comprehensive
experiments on 22 diverse benchmarks demonstrate the efficacy of UCF in
enhancing a variety of UAD methods, consistently achieving new state-of-the-art
results in both unimodal (RGB) and multimodal (RGB--3D, RGB--Text) UAD
scenarios. Code and models will be released at
https://github.com/ZHE-SAPI/CostFilter-AD.

</details>


### [221] [Visual Language Model as a Judge for Object Detection in Industrial Diagrams](https://arxiv.org/abs/2510.03376)
*Sanjukta Ghosh*

Main category: cs.CV

TL;DR: The paper introduces a framework using Visual Language Models (VLMs) to evaluate and enhance object detection in industrial diagrams like P&IDs.


<details>
  <summary>Details</summary>
Motivation: There is a lack of methods to automatically evaluate the quality of object detection outputs in the context of converting industrial diagrams into digital forms.

Method: The authors propose using Visual Language Models (VLMs) to automatically assess and refine object detection results by identifying missing or inconsistent detections in industrial diagrams.

Result: The approach leverages VLMs' multimodal capabilities to enable automated quality assessment, improving object detection performance in complex industrial diagrams.

Conclusion: Integrating VLMs enhances the digitalization process of industrial diagrams by improving object detection quality and supporting intelligent industrial automation.

Abstract: Industrial diagrams such as piping and instrumentation diagrams (P&IDs) are
essential for the design, operation, and maintenance of industrial plants.
Converting these diagrams into digital form is an important step toward
building digital twins and enabling intelligent industrial automation. A
central challenge in this digitalization process is accurate object detection.
Although recent advances have significantly improved object detection
algorithms, there remains a lack of methods to automatically evaluate the
quality of their outputs. This paper addresses this gap by introducing a
framework that employs Visual Language Models (VLMs) to assess object detection
results and guide their refinement. The approach exploits the multimodal
capabilities of VLMs to identify missing or inconsistent detections, thereby
enabling automated quality assessment and improving overall detection
performance on complex industrial diagrams.

</details>


### [222] [Spatial-ViLT: Enhancing Visual Spatial Reasoning through Multi-Task Learning](https://arxiv.org/abs/2510.03441)
*Chashi Mahiul Islam,Oteo Mamo,Samuel Jacob Chacko,Xiuwen Liu,Weikuan Yu*

Main category: cs.CV

TL;DR: The paper proposes SpatialViLT, an improved vision-language model (VLM) that integrates spatial understanding for better reasoning in 3D scenes and object configurations.


<details>
  <summary>Details</summary>
Motivation: Enhance spatial reasoning capabilities in VLMs, particularly for 3D scenes and complex object relations, as current models face limitations in these areas.

Method: Introduced SpatialViLT, which incorporates spatial features like depth maps and 3D coordinates using a multi-task learning approach, and its derivatives, including MaskedSpatialViLT and SpatialEnsemble.

Result: The proposed models achieve state-of-the-art accuracy in spatial reasoning categories, validated on the Visual Spatial Reasoning (VSR) dataset.

Conclusion: SpatialViLT enhances spatial intelligence in AI systems, marking progress in multimodal understanding and supporting practical real-world use cases.

Abstract: Vision-language models (VLMs) have advanced multimodal reasoning but still
face challenges in spatial reasoning for 3D scenes and complex object
configurations. To address this, we introduce SpatialViLT, an enhanced VLM that
integrates spatial features like depth maps, 3D coordinates, and edge maps
through a multi-task learning framework. This approach enriches multimodal
embeddings with spatial understanding. We propose two variants: SpatialViLT and
MaskedSpatialViLT, focusing on full and masked object regions, respectively.
Additionally, SpatialEnsemble combines both approaches, achieving
state-of-the-art accuracy. Our models excel in spatial reasoning categories
such as directional, topological, and proximity relations, as demonstrated on
the challenging Visual Spatial Reasoning (VSR) dataset. This work represents a
significant step in enhancing the spatial intelligence of AI systems, crucial
for advanced multimodal understanding and real-world applications.

</details>


### [223] [Denoising of Two-Phase Optically Sectioned Structured Illumination Reconstructions Using Encoder-Decoder Networks](https://arxiv.org/abs/2510.03452)
*Allison Davis,Yezhi Shen,Xiaoyu Ji,Fengqing Zhu*

Main category: cs.CV

TL;DR: Researchers explored using deep learning methods, specifically encoder-decoder networks, to reduce artifacts in two-phase optical-sectioning structured illumination (OS-SI) imaging. Synthetic training data was created to mimic realistic artifact conditions for supervised learning.


<details>
  <summary>Details</summary>
Motivation: Conventional artifact suppression struggles in two-phase optical-sectioning SI due to reduced acquisition time, which results in residual artifacts. There's a need for improved methods that can effectively address these limitations.

Method: The paper employs encoder-decoder networks (an asymmetrical denoising autoencoder and a U-Net) and trains them with synthetic data generated by adding real artifact patterns to synthetic images. These models are then tested on real OS-SI images.

Result: Both networks successfully improved image clarity, but each performed better against specific types of artifacts, demonstrating their complementary strengths.

Conclusion: Synthetic training data enables effective supervised denoising of OS-SI images, showing that encoder-decoder networks can be valuable for artifact reduction and improving reconstruction workflows.

Abstract: Structured illumination (SI) enhances image resolution and contrast by
projecting patterned light onto a sample. In two-phase optical-sectioning SI
(OS-SI), reduced acquisition time introduces residual artifacts that
conventional denoising struggles to suppress. Deep learning offers an
alternative to traditional methods; however, supervised training is limited by
the lack of clean, optically sectioned ground-truth data. We investigate
encoder-decoder networks for artifact reduction in two-phase OS-SI, using
synthetic training pairs formed by applying real artifact fields to synthetic
images. An asymmetrical denoising autoencoder (DAE) and a U-Net are trained on
the synthetic data, then evaluated on real OS-SI images. Both networks improve
image clarity, with each excelling against different artifact types. These
results demonstrate that synthetic training enables supervised denoising of
OS-SI images and highlight the potential of encoder-decoder networks to
streamline reconstruction workflows.

</details>


### [224] [OpenFLAME: Federated Visual Positioning System to Enable Large-Scale Augmented Reality Applications](https://arxiv.org/abs/2510.03915)
*Sagar Bharadwaj,Harrison Williams,Luke Wang,Michael Liang,Tao Jin,Srinivasan Seshan,Anthony Rowe*

Main category: cs.CV

TL;DR: This paper introduces OpenFLAME, a federated VPS backend for AR applications, enabling independent 3D mapping with privacy and scalability while addressing associated challenges.


<details>
  <summary>Details</summary>
Motivation: Current centralized VPS solutions by large organizations fail to address privacy, indoor space coverage, and scalability needs required for future AR applications.

Method: The authors propose OpenFLAME, a federated VPS backend that allows organizations to independently manage their localization services, while ensuring data coherency and privacy through federated image-based localization.

Result: OpenFLAME enables access-controlled and distributed management of VPS with larger area coverage while addressing challenges like localization coherency, quality control, and service selection.

Conclusion: OpenFLAME demonstrates a scalable and privacy-conscious framework for VPS suitable for diverse AR applications, overcoming limitations of centralized approaches.

Abstract: World-scale augmented reality (AR) applications need a ubiquitous 6DoF
localization backend to anchor content to the real world consistently across
devices. Large organizations such as Google and Niantic are 3D scanning outdoor
public spaces in order to build their own Visual Positioning Systems (VPS).
These centralized VPS solutions fail to meet the needs of many future AR
applications -- they do not cover private indoor spaces because of privacy
concerns, regulations, and the labor bottleneck of updating and maintaining 3D
scans. In this paper, we present OpenFLAME, a federated VPS backend that allows
independent organizations to 3D scan and maintain a separate VPS service for
their own spaces. This enables access control of indoor 3D scans, distributed
maintenance of the VPS backend, and encourages larger coverage. Sharding of VPS
services introduces several unique challenges -- coherency of localization
results across spaces, quality control of VPS services, selection of the right
VPS service for a location, and many others. We introduce the concept of
federated image-based localization and provide reference solutions for managing
and merging data across maps without sharing private data.

</details>


### [225] [PEaRL: Pathway-Enhanced Representation Learning for Gene and Pathway Expression Prediction from Histology](https://arxiv.org/abs/2510.03455)
*Sejuti Majumder,Saarthak Kapse,Moinak Bhattacharya,Xuan Xu,Alisa Yurovsky,Prateek Prasanna*

Main category: cs.CV

TL;DR: This paper introduces PEaRL, a framework to integrate transcriptomics and histopathology using pathway activation scores for enhanced prediction and interpretability.


<details>
  <summary>Details</summary>
Motivation: Current multimodal approaches inadequately capture tissue phenotypes due to reliance on variable genes, limiting biological relevance and predictive power.

Method: PEaRL uses pathway activation scores calculated with ssGSEA, encodes these signals using a transformer, and aligns them with histology features through contrastive learning.

Result: PEaRL outperforms state-of-the-art methods in cancer datasets by improving gene and pathway prediction accuracy significantly (up to 58.9% and 20.4% gains in Pearson correlation).

Conclusion: Pathway-based transcriptomic representations provide robust and interpretable models, bridging molecular function and morphology effectively.

Abstract: Integrating histopathology with spatial transcriptomics (ST) provides a
powerful opportunity to link tissue morphology with molecular function. Yet
most existing multimodal approaches rely on a small set of highly variable
genes, which limits predictive scope and overlooks the coordinated biological
programs that shape tissue phenotypes. We present PEaRL (Pathway Enhanced
Representation Learning), a multimodal framework that represents
transcriptomics through pathway activation scores computed with ssGSEA. By
encoding biologically coherent pathway signals with a transformer and aligning
them with histology features via contrastive learning, PEaRL reduces
dimensionality, improves interpretability, and strengthens cross-modal
correspondence. Across three cancer ST datasets (breast, skin, and lymph node),
PEaRL consistently outperforms SOTA methods, yielding higher accuracy for both
gene- and pathway-level expression prediction (up to 58.9 percent and 20.4
percent increase in Pearson correlation coefficient compared to SOTA). These
results demonstrate that grounding transcriptomic representation in pathways
produces more biologically faithful and interpretable multimodal models,
advancing computational pathology beyond gene-level embeddings.

</details>


### [226] [DuPLUS: Dual-Prompt Vision-Language Framework for Universal Medical Image Segmentation and Prognosis](https://arxiv.org/abs/2510.03483)
*Numan Saeed,Tausifa Jan Saleem,Fadillah Maani,Muhammad Ridzuan,Hu Wang,Mohammad Yaqub*

Main category: cs.CV

TL;DR: DuPLUS is a vision-language framework for multi-modal medical image analysis with text-controlled architecture, demonstrating superiority over existing methods in segmentation and prognosis prediction.


<details>
  <summary>Details</summary>
Motivation: Existing methods for medical imaging are hampered by task-specific models' lack of generalizability and limited prognostic capabilities, while universal models suffer from lack of semantic understanding.

Method: DuPLUS introduces a hierarchical vision-language framework with a dual-prompt mechanism enabling text-controlled architecture for segmentation and prognosis prediction across multiple imaging modalities.

Result: DuPLUS generalized across three imaging modalities, covered 30+ organs and tumor types across datasets, outperformed state-of-the-art models on 8/10 datasets, and achieved a CI of 0.69 on prognosis prediction for head and neck cancer data.

Conclusion: DuPLUS is a versatile, parameter-efficient framework, demonstrating adaptability and clinical relevance, outperforming existing models in segmentation and prognosis tasks.

Abstract: Deep learning for medical imaging is hampered by task-specific models that
lack generalizability and prognostic capabilities, while existing 'universal'
approaches suffer from simplistic conditioning and poor medical semantic
understanding. To address these limitations, we introduce DuPLUS, a deep
learning framework for efficient multi-modal medical image analysis. DuPLUS
introduces a novel vision-language framework that leverages hierarchical
semantic prompts for fine-grained control over the analysis task, a capability
absent in prior universal models. To enable extensibility to other medical
tasks, it includes a hierarchical, text-controlled architecture driven by a
unique dual-prompt mechanism. For segmentation, DuPLUS is able to generalize
across three imaging modalities, ten different anatomically various medical
datasets, encompassing more than 30 organs and tumor types. It outperforms the
state-of-the-art task specific and universal models on 8 out of 10 datasets. We
demonstrate extensibility of its text-controlled architecture by seamless
integration of electronic health record (EHR) data for prognosis prediction,
and on a head and neck cancer dataset, DuPLUS achieved a Concordance Index (CI)
of 0.69. Parameter-efficient fine-tuning enables rapid adaptation to new tasks
and modalities from varying centers, establishing DuPLUS as a versatile and
clinically relevant solution for medical image analysis. The code for this work
is made available at: https://anonymous.4open.science/r/DuPLUS-6C52

</details>


### [227] [Real-Time Threaded Houbara Detection and Segmentation for Wildlife Conservation using Mobile Platforms](https://arxiv.org/abs/2510.03501)
*Lyes Saad Saoud,Loic Lesobre,Enrico Sorato,Irfan Hussain*

Main category: cs.CV

TL;DR: The paper proposes a mobile-optimized framework combining YOLOv10 and MobileSAM for real-time animal detection and segmentation with improved performance using threading.


<details>
  <summary>Details</summary>
Motivation: The research aims to enable efficient real-time wildlife monitoring in natural settings, addressing challenges such as limited resources and the cryptic nature of species.

Method: The study introduces a parallelized two-stage deep learning model: Threading Detection Model (TDM) integrates YOLOv10 for detection and MobileSAM for segmentation, both operating concurrently.

Result: On the Houbara Bustard dataset, the model achieved high accuracy metrics (mAP50: 0.9627, mAP75: 0.7731, mAP95: 0.7178, MobileSAM mIoU: 0.7421) and detected images at 43.7 ms per frame.

Conclusion: The framework is effective for real-time animal detection and segmentation, demonstrating applicability for wildlife conservation tasks. Publicly available code and dataset add further value to the research.

Abstract: Real-time animal detection and segmentation in natural environments are vital
for wildlife conservation, enabling non-invasive monitoring through remote
camera streams. However, these tasks remain challenging due to limited
computational resources and the cryptic appearance of many species. We propose
a mobile-optimized two-stage deep learning framework that integrates a
Threading Detection Model (TDM) to parallelize YOLOv10-based detection and
MobileSAM-based segmentation. Unlike prior YOLO+SAM pipelines, our approach
improves real-time performance by reducing latency through threading. YOLOv10
handles detection while MobileSAM performs lightweight segmentation, both
executed concurrently for efficient resource use. On the cryptic Houbara
Bustard, a conservation-priority species, our model achieves mAP50 of 0.9627,
mAP75 of 0.7731, mAP95 of 0.7178, and a MobileSAM mIoU of 0.7421. YOLOv10
operates at 43.7 ms per frame, confirming real-time readiness. We introduce a
curated Houbara dataset of 40,000 annotated images to support model training
and evaluation across diverse conditions. The code and dataset used in this
study are publicly available on GitHub at
https://github.com/LyesSaadSaoud/mobile-houbara-detseg. For interactive demos
and additional resources, visit
https://lyessaadsaoud.github.io/LyesSaadSaoud-Threaded-YOLO-SAM-Houbara.

</details>


### [228] [Platonic Transformers: A Solid Choice For Equivariance](https://arxiv.org/abs/2510.03511)
*Mohammad Mohaiminul Islam,Rishabh Anand,David R. Wessels,Friso de Kruiff,Thijs P. Kuipers,Rex Ying,Clara I. Sánchez,Sharvaree Vadgama,Georg Bökman,Erik J. Bekkers*

Main category: cs.CV

TL;DR: The Platonic Transformer applies geometric biases to Transformers without added computational cost, enhancing performance across diverse tasks.


<details>
  <summary>Details</summary>
Motivation: Standard Transformers lack geometric inductive biases, making them less effective for tasks in science and computer vision where such symmetries are important. Existing methods addressing this trade-off are computationally intensive.

Method: The framework defines attention relative to the reference frames of Platonic solid symmetry groups, enabling a weight-sharing scheme. It unites continuous translational and Platonic symmetries while preserving efficiency, and is formally comparable to dynamic group convolution for scalable implementation.

Result: The Platonic Transformer demonstrates competitive performance in benchmarks like CIFAR-10, ScanObjectNN (for 3D point clouds), and QM9/OMol25 for molecular property prediction, exploiting geometric constraints effectively.

Conclusion: The Platonic Transformer integrates geometric symmetry into the Transformer architecture efficiently and flexibly, achieving robust performance across scientific and computer vision tasks.

Abstract: While widespread, Transformers lack inductive biases for geometric symmetries
common in science and computer vision. Existing equivariant methods often
sacrifice the efficiency and flexibility that make Transformers so effective
through complex, computationally intensive designs. We introduce the Platonic
Transformer to resolve this trade-off. By defining attention relative to
reference frames from the Platonic solid symmetry groups, our method induces a
principled weight-sharing scheme. This enables combined equivariance to
continuous translations and Platonic symmetries, while preserving the exact
architecture and computational cost of a standard Transformer. Furthermore, we
show that this attention is formally equivalent to a dynamic group convolution,
which reveals that the model learns adaptive geometric filters and enables a
highly scalable, linear-time convolutional variant. Across diverse benchmarks
in computer vision (CIFAR-10), 3D point clouds (ScanObjectNN), and molecular
property prediction (QM9, OMol25), the Platonic Transformer achieves
competitive performance by leveraging these geometric constraints at no
additional cost.

</details>


### [229] [Domain Generalization for Semantic Segmentation: A Survey](https://arxiv.org/abs/2510.03540)
*Manuel Schwonberg,Hanno Gottschalk*

Main category: cs.CV

TL;DR: The paper surveys domain generalization methods for semantic segmentation, focusing on foundation-model-based approaches and their impactful role.


<details>
  <summary>Details</summary>
Motivation: Deep neural networks struggle to generalize to unknown target domains, driving the need for domain generalization methods, especially for semantic segmentation tasks in fields like biomedicine or automated driving.

Method: The survey clusters and reviews existing approaches in domain generalization, emphasizes the paradigm shift towards foundation models, and compares performance across methods.

Result: Foundation models have a significant influence on advancements in domain-generalized semantic segmentation.

Conclusion: The paper aims to inspire future research directions by showcasing the evolving impact of foundation models on domain generalization.

Abstract: The generalization of deep neural networks to unknown domains is a major
challenge despite their tremendous progress in recent years. For this reason,
the dynamic area of domain generalization (DG) has emerged. In contrast to
unsupervised domain adaptation, there is no access to or knowledge about the
target domains, and DG methods aim to generalize across multiple different
unseen target domains. Domain generalization is particularly relevant for the
task semantic segmentation which is used in several areas such as biomedicine
or automated driving. This survey provides a comprehensive overview of the
rapidly evolving topic of domain generalized semantic segmentation. We cluster
and review existing approaches and identify the paradigm shift towards
foundation-model-based domain generalization. Finally, we provide an extensive
performance comparison of all approaches, which highlights the significant
influence of foundation models on domain generalization. This survey seeks to
advance domain generalization research and inspire scientists to explore new
research directions.

</details>


### [230] [From Scope to Script: An Automated Report Generation Model for Gastrointestinal Endoscopy](https://arxiv.org/abs/2510.03543)
*Evandros Kaklamanos,Kristjana Kristinsdottir,Jonathan Huang,Dustin Carlson,Rajesh Keswani,John Pandolfino,Mozziyar Etemadi*

Main category: cs.CV

TL;DR: The paper introduces an AI model to automate report generation for gastrointestinal endoscopic procedures.


<details>
  <summary>Details</summary>
Motivation: Endoscopic procedures cause significant documentation burden on gastroenterologists, leading to inefficiencies and burnout.

Method: A transformer-based vision encoder and text decoder are utilized through a two-stage training framework, with pretraining on image/text captions and fine-tuning on clinical images/reports.

Result: The model streamlines documentation, potentially reducing physicians' workload.

Conclusion: Automating report generation could enhance clinical workflows and improve patient care.

Abstract: Endoscopic procedures such as esophagogastroduodenoscopy (EGD) and
colonoscopy play a critical role in diagnosing and managing gastrointestinal
(GI) disorders. However, the documentation burden associated with these
procedures place significant strain on gastroenterologists, contributing to
inefficiencies in clinical workflows and physician burnout. To address this
challenge, we propose a novel automated report generation model that leverages
a transformer-based vision encoder and text decoder within a two-stage training
framework. In the first stage, both components are pre-trained on image/text
caption pairs to capture generalized vision-language features, followed by
fine-tuning on images/report pairs to generate clinically meaningful findings.
Our approach not only streamlines the documentation process but also holds
promise for reducing physician workload and improving patient care.

</details>


### [231] [SketchPlan: Diffusion Based Drone Planning From Human Sketches](https://arxiv.org/abs/2510.03545)
*Sixten Norelius,Aaron O. Feldman,Mac Schwager*

Main category: cs.CV

TL;DR: SketchPlan is a diffusion-based model designed to interpret 2D sketches over depth images and generate 3D flight paths for drones. It combines a sketch-to-2D paths adapter and a diffusion model to infer trajectories, achieving successful transfer to real-world environments with high accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of translating human-intended 2D hand-drawn sketches into safe and accurate 3D flight paths for drones, especially in unseen environments.

Method: SketchPlan uses two core components: SketchAdapter, which maps human sketches to 2D paths, and DiffPath, a diffusion model that predicts 3D flight paths based on the 2D projections and depth information. The system is trained on a synthetic dataset and real human sketches to ensure robustness.

Result: In real-world experiments, SketchPlan had a 100% success rate in low to medium clutter environments and 40% in high-clutter environments, surpassing ablation studies by 20-60% in task completion.

Conclusion: SketchPlan effectively interprets human intent from hand-drawn sketches to ensure accurate and safe drone navigation in diverse environments, showcasing its potential in real-world applications.

Abstract: We propose SketchPlan, a diffusion-based planner that interprets 2D
hand-drawn sketches over depth images to generate 3D flight paths for drone
navigation. SketchPlan comprises two components: a SketchAdapter that learns to
map the human sketches to projected 2D paths, and DiffPath, a diffusion model
that infers 3D trajectories from 2D projections and a first person view depth
image. Our model achieves zero-shot sim-to-real transfer, generating accurate
and safe flight paths in previously unseen real-world environments. To train
the model, we build a synthetic dataset of 32k flight paths using a diverse set
of photorealistic 3D Gaussian Splatting scenes. We automatically label the data
by computing 2D projections of the 3D flight paths onto the camera plane, and
use this to train the DiffPath diffusion model. However, since real human 2D
sketches differ significantly from ideal 2D projections, we additionally label
872 of the 3D flight paths with real human sketches and use this to train the
SketchAdapter to infer the 2D projection from the human sketch. We demonstrate
SketchPlan's effectiveness in both simulated and real-world experiments, and
show through ablations that training on a mix of human labeled and auto-labeled
data together with a modular design significantly boosts its capabilities to
correctly interpret human intent and infer 3D paths. In real-world drone tests,
SketchPlan achieved 100\% success in low/medium clutter and 40\% in unseen
high-clutter environments, outperforming key ablations by 20-60\% in task
completion.

</details>


### [232] [Unmasking Puppeteers: Leveraging Biometric Leakage to Disarm Impersonation in AI-based Videoconferencing](https://arxiv.org/abs/2510.03548)
*Danial Samadi Vahdati,Tai Duc Nguyen,Ekta Prashnani,Koki Nagano,David Luebke,Orazio Gallo,Matthew Stamm*

Main category: cs.CV

TL;DR: The paper proposes a method to detect and prevent real-time hijacking of identities in AI-based talking-head videoconferencing systems by exploiting biometric information inherently present in pose-expression latent.


<details>
  <summary>Details</summary>
Motivation: To address the vulnerability of AI-based talking-head videoconferencing systems, where attackers can puppeteer a victim's likeness in real time, bypassing current deepfake detectors.

Method: The method involves a pose-conditioned, large-margin contrastive encoder that disentangles persistent identity cues from transient biometrics in the transmitted latent, enabling the identification of illicit identity swaps.

Result: The proposed method outperforms existing puppeteering defenses, works in real-time, and generalizes well to out-of-distribution scenarios.

Conclusion: The method provides an effective defense against real-time identity hijacking in synthetic video systems by utilizing biometric leakage from pose-expression latent.

Abstract: AI-based talking-head videoconferencing systems reduce bandwidth by sending a
compact pose-expression latent and re-synthesizing RGB at the receiver, but
this latent can be puppeteered, letting an attacker hijack a victim's likeness
in real time. Because every frame is synthetic, deepfake and synthetic video
detectors fail outright. To address this security problem, we exploit a key
observation: the pose-expression latent inherently contains biometric
information of the driving identity. Therefore, we introduce the first
biometric leakage defense without ever looking at the reconstructed RGB video:
a pose-conditioned, large-margin contrastive encoder that isolates persistent
identity cues inside the transmitted latent while cancelling transient pose and
expression. A simple cosine test on this disentangled embedding flags illicit
identity swaps as the video is rendered. Our experiments on multiple
talking-head generation models show that our method consistently outperforms
existing puppeteering defenses, operates in real-time, and shows strong
generalization to out-of-distribution scenarios.

</details>


### [233] [Streaming Drag-Oriented Interactive Video Manipulation: Drag Anything, Anytime!](https://arxiv.org/abs/2510.03550)
*Junbao Zhou,Yuan Zhou,Kesen Zhao,Qingshan Xu,Beier Zhu,Richang Hong,Hanwang Zhang*

Main category: cs.CV

TL;DR: This paper proposes DragStream to address challenges in controlling video outputs of autoregressive video diffusion models.


<details>
  <summary>Details</summary>
Motivation: Existing autoregressive video diffusion models lack fine-grained control, making it hard for outputs to align with user expectations.

Method: The paper introduces DragStream, a training-free strategy with adaptive distribution self-rectification and spatial-frequency selective optimization.

Result: DragStream effectively resolves latent drift and context frame issues, improving video manipulation outcomes.

Conclusion: DragStream enhances fine-grained control in video diffusion models and can be easily integrated with existing systems, shown effective in experiments.

Abstract: Achieving streaming, fine-grained control over the outputs of autoregressive
video diffusion models remains challenging, making it difficult to ensure that
they consistently align with user expectations. To bridge this gap, we propose
\textbf{stReaming drag-oriEnted interactiVe vidEo manipuLation (REVEL)}, a new
task that enables users to modify generated videos \emph{anytime} on
\emph{anything} via fine-grained, interactive drag. Beyond DragVideo and
SG-I2V, REVEL unifies drag-style video manipulation as editing and animating
video frames with both supporting user-specified translation, deformation, and
rotation effects, making drag operations versatile. In resolving REVEL, we
observe: \emph{i}) drag-induced perturbations accumulate in latent space,
causing severe latent distribution drift that halts the drag process;
\emph{ii}) streaming drag is easily disturbed by context frames, thereby
yielding visually unnatural outcomes. We thus propose a training-free approach,
\textbf{DragStream}, comprising: \emph{i}) an adaptive distribution
self-rectification strategy that leverages neighboring frames' statistics to
effectively constrain the drift of latent embeddings; \emph{ii}) a
spatial-frequency selective optimization mechanism, allowing the model to fully
exploit contextual information while mitigating its interference via
selectively propagating visual cues along generation. Our method can be
seamlessly integrated into existing autoregressive video diffusion models, and
extensive experiments firmly demonstrate the effectiveness of our DragStream.

</details>


### [234] [GAS-MIL: Group-Aggregative Selection Multi-Instance Learning for Ensemble of Foundation Models in Digital Pathology Image Analysis](https://arxiv.org/abs/2510.03555)
*Peiran Quan,Zifan Gu,Zhuo Zhao,Qin Zhou,Donghan M. Yang,Ruichen Rong,Yang Xie,Guanghua Xiao*

Main category: cs.CV

TL;DR: The paper proposes GAS-MIL, a framework for integrating diverse foundation models (FMs) efficiently in pathology without extensive fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Adaptation and benchmarking of foundation models for specific diagnostic pathology tasks are time-consuming and resource-heavy.

Method: A multi-instance learning framework called GAS-MIL is introduced, which aggregates features from multiple FMs without manual selection or deep fine-tuning.

Result: GAS-MIL demonstrated superior or comparable performance across three cancer datasets: prostate, ovarian, and breast cancer classifications.

Conclusion: GAS-MIL streamlines the deployment of heterogeneous foundation models in pathology, enhancing scalability and paving the way for future oncology applications.

Abstract: Foundation models (FMs) have transformed computational pathology by providing
powerful, general-purpose feature extractors. However, adapting and
benchmarking individual FMs for specific diagnostic tasks is often
time-consuming and resource-intensive, especially given their scale and
diversity. To address this challenge, we introduce Group-Aggregative Selection
Multi-Instance Learning (GAS-MIL), a flexible ensemble framework that
seamlessly integrates features from multiple FMs, preserving their
complementary strengths without requiring manual feature selection or extensive
task-specific fine-tuning. Across classification tasks in three cancer
datasets-prostate (PANDA), ovarian (UBC-OCEAN), and breast (TCGA-BrCa)-GAS-MIL
consistently achieves superior or on-par performance relative to individual FMs
and established MIL methods, demonstrating its robustness and generalizability.
By enabling efficient integration of heterogeneous FMs, GAS-MIL streamlines
model deployment for pathology and provides a scalable foundation for future
multimodal and precision oncology applications.

</details>


### [235] [Real-Time Assessment of Bystander Situation Awareness in Drone-Assisted First Aid](https://arxiv.org/abs/2510.03558)
*Shen Chang,Renran Tian,Nicole Adams,Nan Kong*

Main category: cs.CV

TL;DR: This paper introduces a dataset (DANDSD) and a novel framework to assess the situational awareness (SA) of untrained bystanders administering naloxone during opioid overdose emergencies via drone assistance.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the research gap in real-time situational awareness (SA) assessment for bystanders involved in drone-assisted opioid overdose emergencies. Enhancing SA in human-autonomy teaming can improve lifesaving interventions.

Method: The researchers developed the Drone-Assisted Naloxone Delivery Simulation Dataset (DANDSD), utilized graph embeddings, and transformer models to evaluate the SA of bystanders in real time. Their approach integrates features like geometric, kinematic, and interaction graph data.

Result: The proposed framework achieved high SA prediction performance and enhanced temporal segmentation accuracy, surpassing the FINCH method by 9% in Mean over Frames (MoF) and 5% in Intersection over Union (IoU).

Conclusion: The study underscores the potential of adaptive drone systems for effective bystander guidance during opioid overdose emergencies, thereby aiding in saving lives and enhancing emergency response outcomes.

Abstract: Rapid naloxone delivery via drones offers a promising solution for responding
to opioid overdose emergencies (OOEs), by extending lifesaving interventions to
medically untrained bystanders before emergency medical services (EMS) arrive.
Recognizing the critical role of bystander situational awareness (SA) in
human-autonomy teaming (HAT), we address a key research gap in real-time SA
assessment by introducing the Drone-Assisted Naloxone Delivery Simulation
Dataset (DANDSD). This pioneering dataset captures HAT during simulated OOEs,
where college students without medical training act as bystanders tasked with
administering intranasal naloxone to a mock overdose victim. Leveraging this
dataset, we propose a video-based real-time SA assessment framework that
utilizes graph embeddings and transformer models to assess bystander SA in real
time. Our approach integrates visual perception and comprehension cues--such as
geometric, kinematic, and interaction graph features--and achieves
high-performance SA prediction. It also demonstrates strong temporal
segmentation accuracy, outperforming the FINCH baseline by 9% in Mean over
Frames (MoF) and 5% in Intersection over Union (IoU). This work supports the
development of adaptive drone systems capable of guiding bystanders
effectively, ultimately improving emergency response outcomes and saving lives.

</details>


### [236] [Evaluating OCR performance on food packaging labels in South Africa](https://arxiv.org/abs/2510.03570)
*Mayimunah Nagayi,Alice Khan,Tamryn Frank,Rina Swart,Clement Nyirenda*

Main category: cs.CV

TL;DR: The paper evaluates four OCR systems on food packaging images and analyzes their ability to extract ingredient lists and nutrition facts, highlighting Tesseract as the most accurate but EasyOCR as offering a good balance.


<details>
  <summary>Details</summary>
Motivation: Accurate OCR of food packaging is crucial for compliance and nutrition tracking, but challenges like multilingual text, dense layouts, different fonts, glare, and curved surfaces complicate the task.

Method: The researchers used a dataset of 1,628 images across 231 products to test four OCR systems (Tesseract, EasyOCR, PaddleOCR, and TrOCR) for speed and coverage. They created a ground truth subset of 113 images for accuracy metrics such as CER, WER, BLEU, ROUGE-L, F1, coverage, and execution time.

Result: Tesseract attained the best accuracy with the lowest CER (0.912) and highest BLEU (0.245). EasyOCR balanced accuracy with multilingual support. PaddleOCR had wide coverage but was slower due to CPU-only processing, while TrOCR's performance was weak despite GPU acceleration.

Conclusion: The study establishes benchmarks for OCR on food packaging, identifies system-specific strengths, and reveals opportunities for improving layout-aware methods and text localization strategies.

Abstract: This study evaluates four open-source Optical Character Recognition (OCR)
systems which are Tesseract, EasyOCR, PaddleOCR, and TrOCR on real world food
packaging images. The aim is to assess their ability to extract ingredient
lists and nutrition facts panels. Accurate OCR for packaging is important for
compliance and nutrition monitoring but is challenging due to multilingual
text, dense layouts, varied fonts, glare, and curved surfaces. A dataset of 231
products (1,628 images) was processed by all four models to assess speed and
coverage, and a ground truth subset of 113 images (60 products) was created for
accuracy evaluation. Metrics include Character Error Rate (CER), Word Error
Rate (WER), BLEU, ROUGE-L, F1, coverage, and execution time. On the ground
truth subset, Tesseract achieved the lowest CER (0.912) and the highest BLEU
(0.245). EasyOCR provided a good balance between accuracy and multilingual
support. PaddleOCR achieved near complete coverage but was slower because it
ran on CPU only due to GPU incompatibility, and TrOCR produced the weakest
results despite GPU acceleration. These results provide a packaging-specific
benchmark, establish a baseline, and highlight directions for layout-aware
methods and text localization.

</details>


### [237] [FrameOracle: Learning What to See and How Much to See in Videos](https://arxiv.org/abs/2510.03584)
*Chaoyu Li,Tianzhi Li,Fei Tao,Zhenyu Zhao,Ziqian Wu,Maozheng Zhao,Juntong Song,Cheng Niu,Pooyan Fazli*

Main category: cs.CV

TL;DR: FrameOracle boosts video understanding by selecting key frames relevant to queries, minimizing input size without sacrificing accuracy.


<details>
  <summary>Details</summary>
Motivation: Vision-language models (VLMs) are limited in processing extensive video frame data, leading to inefficiency and potential information loss. Effective frame selection is needed.

Method: FrameOracle uses a four-stage training approach, combining weak proxy signals and supervision from a FrameOracle-41K dataset with annotated keyframes.

Result: FrameOracle reduces 64-frame inputs to an average of 13.9 frames, improving accuracy by 1.4%, achieving optimal efficiency-accuracy trade-offs.

Conclusion: FrameOracle eliminates redundant frames while retaining essential information, showcasing its potential for scalable video understanding tasks.

Abstract: Vision-language models (VLMs) have advanced video understanding, but their
performance is limited by the number of input frames they can process. Existing
frame sampling strategies, such as uniform or fixed-budget selection, often
fail to adapt to variations in information density or task complexity,
resulting in inefficiency and information loss. To address this, we present
FrameOracle, a lightweight and plug-and-play module that predicts both (1)
which frames are most relevant to a given query and (2) how many frames are
needed. FrameOracle is trained using a four-stage curriculum, with the first
three stages relying on weak proxy signals such as cross-modal similarity. In
the final stage, it leverages stronger supervision from a new dataset we
introduce, FrameOracle-41K, the first large-scale VideoQA collection to provide
keyframe annotations specifying the minimal set of frames required to answer
each question. Extensive experiments across five VLMs and six benchmarks
demonstrate that FrameOracle reduces 16-frame inputs to an average of 10.4
frames without any loss in accuracy. When starting from 64-frame candidates, it
reduces the input to an average of 13.9 frames while improving accuracy by
1.4%, achieving state-of-the-art efficiency-accuracy trade-offs for scalable
video understanding.

</details>


### [238] [A Hybrid Co-Finetuning Approach for Visual Bug Detection in Video Games](https://arxiv.org/abs/2510.03591)
*Faliu Yi,Sherif Abdelfattah,Wei Huang,Adrian Brown*

Main category: cs.CV

TL;DR: The paper introduces a hybrid Co-FineTuning (CFT) method to efficiently detect visual bugs in video games by utilizing both labeled and unlabeled data, addressing the challenge of labeled dataset scarcity.


<details>
  <summary>Details</summary>
Motivation: Visual bug detection in video games is a costly and expertise-intensive process, and the scarcity of labeled data hinders the efficacy of supervised models.

Method: The CFT method integrates labeled samples from target and co-domain games with unlabeled data to enhance feature representation, reducing dependency on labeled examples.

Result: Experiments show that CFT outperforms traditional baselines in detecting visual bugs across games and maintains strong performance even with reduced labeled data from the target game.

Conclusion: The CFT approach offers a scalable and adaptable framework for visual bug detection, making it efficient across various titles and less reliant on labeled data.

Abstract: Manual identification of visual bugs in video games is a resource-intensive
and costly process, often demanding specialized domain knowledge. While
supervised visual bug detection models offer a promising solution, their
reliance on extensive labeled datasets presents a significant challenge due to
the infrequent occurrence of such bugs. To overcome this limitation, we propose
a hybrid Co-FineTuning (CFT) method that effectively integrates both labeled
and unlabeled data. Our approach leverages labeled samples from the target game
and diverse co-domain games, additionally incorporating unlabeled data to
enhance feature representation learning. This strategy maximizes the utility of
all available data, substantially reducing the dependency on labeled examples
from the specific target game. The developed framework demonstrates enhanced
scalability and adaptability, facilitating efficient visual bug detection
across various game titles. Our experimental results show the robustness of the
proposed method for game visual bug detection, exhibiting superior performance
compared to conventional baselines across multiple gaming environments.
Furthermore, CFT maintains competitive performance even when trained with only
50% of the labeled data from the target game.

</details>


### [239] [Exploring the Hierarchical Reasoning Model for Small Natural-Image Classification Without Augmentation](https://arxiv.org/abs/2510.03598)
*Alexander V. Mantzaris*

Main category: cs.CV

TL;DR: The study evaluates the Hierarchical Reasoning Model (HRM) on image classification tasks under a minimalistic setup, finding it non-competitive with convolutional models due to overfitting and poor generalization.


<details>
  <summary>Details</summary>
Motivation: The motivation is to assess if HRM, equipped with features like Transformer modules and DEQ-style training, can serve as a practical alternative to convolutional models for image classification tasks.

Method: The HRM is tested on MNIST, CIFAR-10, and CIFAR-100 datasets without data augmentation, using identical optimizer settings with label smoothing and analyzing its optimization stability and performance.

Result: HRM performs well on MNIST (~98% accuracy) but suffers from overfitting and poor generalization on CIFAR-10 (65.0%) and CIFAR-100 (29.7%). The model's training is stable, but error analysis shows a lack of image-specific inductive bias.

Conclusion: HRM, in its current form, is not suitable for small-resolution image classification tasks without data augmentation. However, modifications to the model may improve its performance significantly.

Abstract: This paper asks whether the Hierarchical Reasoning Model (HRM) with the two
Transformer-style modules $(f_L,f_H)$, one step (DEQ-style) training, deep
supervision, Rotary Position Embeddings, and RMSNorm can serve as a practical
image classifier. It is evaluated on MNIST, CIFAR-10, and CIFAR-100 under a
deliberately raw regime: no data augmentation, identical optimizer family with
one-epoch warmup then cosine-floor decay, and label smoothing. HRM optimizes
stably and performs well on MNIST ($\approx 98\%$ test accuracy), but on small
natural images it overfits and generalizes poorly: on CIFAR-10, HRM reaches
65.0\% after 25 epochs, whereas a two-stage Conv--BN--ReLU baseline attains
77.2\% while training $\sim 30\times$ faster per epoch; on CIFAR-100, HRM
achieves only 29.7\% test accuracy despite 91.5\% train accuracy, while the
same CNN reaches 45.3\% test with 50.5\% train accuracy. Loss traces and error
analyses indicate healthy optimization but insufficient image-specific
inductive bias for HRM in this regime. It is concluded that, for
small-resolution image classification without augmentation, HRM is not
competitive with even simple convolutional architectures as the HRM currently
exist but this does not exclude possibilities that modifications to the model
may allow it to improve greatly.

</details>


### [240] [Unsupervised Transformer Pre-Training for Images: Self-Distillation, Mean Teachers, and Random Crops](https://arxiv.org/abs/2510.03606)
*Mattia Scardecchia*

Main category: cs.CV

TL;DR: The paper surveys advancements in self-supervised learning, focusing on DINOv2 and its superior performance compared to weakly supervised methods like OpenCLIP.


<details>
  <summary>Details</summary>
Motivation: The paper aims to understand and contextualize the advancements in self-supervised learning methods, particularly DINOv2's approach and performance.

Method: The study analyzes the techniques of DINOv2 such as multi-crop view augmentation and self-distillation with a mean teacher, and compares its performance across benchmarks.

Result: DINOv2 surpasses weakly supervised learning methods in benchmarks, demonstrating both high-level semantics and fine-grained image features.

Conclusion: DINOv2 establishes a new state-of-the-art in SSL but has limitations, paving the way for future research directions.

Abstract: Recent advances in self-supervised learning (SSL) have made it possible to
learn general-purpose visual features that capture both the high-level
semantics and the fine-grained spatial structure of images. Most notably, the
recent DINOv2 has established a new state of the art by surpassing weakly
supervised methods (WSL) like OpenCLIP on most benchmarks. In this survey, we
examine the core ideas behind its approach, multi-crop view augmentation and
self-distillation with a mean teacher, and trace their development in previous
work. We then compare the performance of DINO and DINOv2 with other SSL and WSL
methods across various downstream tasks, and highlight some remarkable emergent
properties of their learned features with transformer backbones. We conclude by
briefly discussing DINOv2's limitations, its impact, and future research
directions.

</details>


### [241] [Diffusion-Classifier Synergy: Reward-Aligned Learning via Mutual Boosting Loop for FSCIL](https://arxiv.org/abs/2510.03608)
*Ruitao Wu,Yifan Zhao,Guangyao Chen,Jia Li*

Main category: cs.CV

TL;DR: The paper introduces a novel approach for Few-Shot Class-Incremental Learning (FSCIL) using Diffusion-Classifier Synergy (DCS), a framework to boost both data augmentation and classifier performance.


<details>
  <summary>Details</summary>
Motivation: Current FSCIL methods face challenges in generalization due to limited datasets and the stability-plasticity dilemma. Existing solutions struggle with either semantic misalignment or ineffective guidance when directly applying diffusion models for data augmentation.

Method: DCS introduces a mutually reinforcing loop between diffusion models and FSCIL classifiers using a reward-aligned learning strategy. The framework employs dynamic rewards at the feature and logits levels to maintain semantic coherence, ensure diversity, and encourage exploratory image generation.

Result: DCS achieves state-of-the-art performance on FSCIL benchmarks, improving both knowledge retention of old classes and the learning of new categories.

Conclusion: The DCS framework effectively addresses data scarcity and stability-plasticity challenges in FSCIL, providing a robust solution for generalizing and integrating new class learning.

Abstract: Few-Shot Class-Incremental Learning (FSCIL) challenges models to sequentially
learn new classes from minimal examples without forgetting prior knowledge, a
task complicated by the stability-plasticity dilemma and data scarcity. Current
FSCIL methods often struggle with generalization due to their reliance on
limited datasets. While diffusion models offer a path for data augmentation,
their direct application can lead to semantic misalignment or ineffective
guidance. This paper introduces Diffusion-Classifier Synergy (DCS), a novel
framework that establishes a mutual boosting loop between diffusion model and
FSCIL classifier. DCS utilizes a reward-aligned learning strategy, where a
dynamic, multi-faceted reward function derived from the classifier's state
directs the diffusion model. This reward system operates at two levels: the
feature level ensures semantic coherence and diversity using prototype-anchored
maximum mean discrepancy and dimension-wise variance matching, while the logits
level promotes exploratory image generation and enhances inter-class
discriminability through confidence recalibration and cross-session
confusion-aware mechanisms. This co-evolutionary process, where generated
images refine the classifier and an improved classifier state yields better
reward signals, demonstrably achieves state-of-the-art performance on FSCIL
benchmarks, significantly enhancing both knowledge retention and new class
learning.

</details>


### [242] [MonitorVLM:A Vision Language Framework for Safety Violation Detection in Mining Operations](https://arxiv.org/abs/2510.03666)
*Jiang Wu,Sichao Wu,Yinsong Ma,Guangyuan Yu,Haoyuan Xu,Lifang Zheng,Jingliang Duan*

Main category: cs.CV

TL;DR: MonitorVLM is a vision-language framework for detecting safety violations in surveillance videos, designed specifically for high-risk domains like mining.


<details>
  <summary>Details</summary>
Motivation: To address the labor-intensive, error-prone nature of traditional safety inspections and provide an automated solution for dynamic environments.

Method: MonitorVLM incorporates three modules: a domain-specific dataset with VQA samples for mining regulations, a clause filter for reducing latency, and a behavior magnifier for improving action recognition.

Result: MonitorVLM demonstrates significant improvements over baseline models, including a 22.01% increase in precision, 34.22% in recall, and 28.37% in F1 score.

Conclusion: The study underscores the potential of multimodal systems like MonitorVLM to enhance workplace safety monitoring effectively, providing practical tools like automatic violation reporting with video timestamping.

Abstract: Industrial accidents, particularly in high-risk domains such as surface and
underground mining, are frequently caused by unsafe worker behaviors.
Traditional manual inspection remains labor-intensive, error-prone, and
insufficient for large-scale, dynamic environments, highlighting the urgent
need for intelligent and automated safety monitoring. In this paper, we present
MonitorVLM, a novel vision--language framework designed to detect safety
violations directly from surveillance video streams. MonitorVLM introduces
three key innovations: (1) a domain-specific violation dataset comprising 9,000
vision--question--answer (VQA) samples across 40 high-frequency mining
regulations, enriched with augmentation and auxiliary detection cues; (2) a
clause filter (CF) module that dynamically selects the Top-$K$ most relevant
clauses, reducing inference latency by 13.56\% while maintaining accuracy; and
(3) a behavior magnifier (BM) module that enhances worker regions to improve
fine-grained action recognition, yielding additional gains of 3.45% in
precision and 8.62% in recall. Experimental results demonstrate that MonitorVLM
significantly outperforms baseline vision--language models, achieving
improvements of 22.01% in precision, 34.22\% in recall, and 28.37% in F1 score
over the 72B unfine-tuned baseline. A lightweight web-based interface further
integrates MonitorVLM into practical workflows, enabling automatic violation
reporting with video timestamping. This study highlights the potential of
multimodal large models to enhance occupational safety monitoring in mining and
beyond.

</details>


### [243] [A Novel Cloud-Based Diffusion-Guided Hybrid Model for High-Accuracy Accident Detection in Intelligent Transportation Systems](https://arxiv.org/abs/2510.03675)
*Siva Sai,Saksham Gupta,Vinay Chamola,Rajkumar Buyya*

Main category: cs.CV

TL;DR: The paper proposes a hybrid diffusion model for detecting accidents in Intelligent Transportation Systems with high accuracy.


<details>
  <summary>Details</summary>
Motivation: To enhance accident detection in Intelligent Transportation Systems by utilizing advanced diffusion models.

Method: A hybrid model integrating ExceptionNet outputs as input, employing conditional modules with time and image embeddings, processed via a cloud-based implementation.

Result: The proposed diffusion model achieves 97.32% accuracy in image-based accident detection, outperforming baseline methods.

Conclusion: The integration of diffusion models into accident detection frameworks improves performance and handles complex data distributions effectively.

Abstract: The integration of Diffusion Models into Intelligent Transportation Systems
(ITS) is a substantial improvement in the detection of accidents. We present a
novel hybrid model integrating guidance classification with diffusion
techniques. By leveraging fine-tuned ExceptionNet architecture outputs as input
for our proposed diffusion model and processing image tensors as our
conditioning, our approach creates a robust classification framework. Our model
consists of multiple conditional modules, which aim to modulate the linear
projection of inputs using time embeddings and image covariate embeddings,
allowing the network to adapt its behavior dynamically throughout the diffusion
process. To address the computationally intensive nature of diffusion models,
our implementation is cloud-based, enabling scalable and efficient processing.
Our strategy overcomes the shortcomings of conventional classification
approaches by leveraging diffusion models inherent capacity to effectively
understand complicated data distributions. We investigate important diffusion
characteristics, such as timestep schedulers, timestep encoding techniques,
timestep count, and architectural design changes, using a thorough ablation
study, and have conducted a comprehensive evaluation of the proposed model
against the baseline models on a publicly available dataset. The proposed
diffusion model performs best in image-based accident detection with an
accuracy of 97.32%.

</details>


### [244] [SAMSOD: Rethinking SAM Optimization for RGB-T Salient Object Detection](https://arxiv.org/abs/2510.03689)
*Zhengyi Liu,Xinrui Wang,Xianyong Fang,Zhengzheng Tu,Linbo Wang*

Main category: cs.CV

TL;DR: This paper introduces SAMSOD, a model targeting RGB-T salient object detection (SOD) by blending RGB with thermal imaging and addressing shortcomings through novel methods such as unimodal supervision and gradient deconfliction.


<details>
  <summary>Details</summary>
Motivation: Current techniques for RGB-T SOD face challenges in balancing modality convergence and managing gradient conflicts between high- and low-activations effectively.

Method: The paper proposes SAMSOD, employing unimodal supervision to improve the non-dominant modality and gradient deconfliction to handle gradient disparities, alongside decoupled adapters for better foreground-object emphasis.

Result: Tests on RGB-T SOD benchmarks and generalization to other datasets, like scribble-supervised RGB-T SOD, RGB-D SOD, and RGB-D rail defect detection, validate SAMSOD's effectiveness.

Conclusion: SAMSOD significantly enhances RGB-T SOD performance and demonstrates adaptability across diverse tasks, making it a robust model for multimodal object detection challenges.

Abstract: RGB-T salient object detection (SOD) aims to segment attractive objects by
combining RGB and thermal infrared images. To enhance performance, the Segment
Anything Model has been fine-tuned for this task. However, the imbalance
convergence of two modalities and significant gradient difference between high-
and low- activations are ignored, thereby leaving room for further performance
enhancement. In this paper, we propose a model called \textit{SAMSOD}, which
utilizes unimodal supervision to enhance the learning of non-dominant modality
and employs gradient deconfliction to reduce the impact of conflicting
gradients on model convergence. The method also leverages two decoupled
adapters to separately mask high- and low-activation neurons, emphasizing
foreground objects by enhancing background learning. Fundamental experiments on
RGB-T SOD benchmark datasets and generalizability experiments on scribble
supervised RGB-T SOD, fully supervised RGB-D SOD datasets and full-supervised
RGB-D rail surface defect detection all demonstrate the effectiveness of our
proposed method.

</details>


### [245] [Referring Expression Comprehension for Small Objects](https://arxiv.org/abs/2510.03701)
*Kanoko Goto,Takumi Hirose,Mahiro Ukai,Shuhei Kurita,Nakamasa Inoue*

Main category: cs.CV

TL;DR: The paper addresses the challenge of localizing extremely small objects in referring expression comprehension tasks through a new dataset (SOREC) and a novel progressive-iterative zooming adapter (PIZA) for accuracy enhancement.


<details>
  <summary>Details</summary>
Motivation: REC struggles with accurately localizing extremely small objects, which is critical in real-world applications such as autonomous driving.

Method: Introduction of the SOREC dataset with 100,000 pairs of small-object data for driving scenarios, and the PIZA adapter for efficient fine-tuning to zoom in and localize small objects.

Result: Combining PIZA with GroundingDINO resulted in significant accuracy improvements on the SOREC dataset.

Conclusion: The novel dataset and PIZA adapter advance REC capability for small object localization, and all resources are provided openly for further research.

Abstract: Referring expression comprehension (REC) aims to localize the target object
described by a natural language expression. Recent advances in vision-language
learning have led to significant performance improvements in REC tasks.
However, localizing extremely small objects remains a considerable challenge
despite its importance in real-world applications such as autonomous driving.
To address this issue, we introduce a novel dataset and method for REC
targeting small objects. First, we present the small object REC (SOREC)
dataset, which consists of 100,000 pairs of referring expressions and
corresponding bounding boxes for small objects in driving scenarios. Second, we
propose the progressive-iterative zooming adapter (PIZA), an adapter module for
parameter-efficient fine-tuning that enables models to progressively zoom in
and localize small objects. In a series of experiments, we apply PIZA to
GroundingDINO and demonstrate a significant improvement in accuracy on the
SOREC dataset. Our dataset, codes and pre-trained models are publicly available
on the project page.

</details>


### [246] [Artery-Vein Segmentation from Fundus Images using Deep Learning](https://arxiv.org/abs/2510.03717)
*Sharan SK,Subin Sahayam,Umarani Jayaraman,Lakshmi Priya A*

Main category: cs.CV

TL;DR: The paper presents Attention-WNet, a new Deep Learning-based model for segmenting retinal arteries and veins using attention mechanisms, achieving superior performance on HRF and DRIVE datasets.


<details>
  <summary>Details</summary>
Motivation: To improve the segmentation of arteries and veins in the retina as it is critical for diagnosing retinal and systemic vascular diseases.

Method: Development of Attention-WNet, which integrates attention mechanisms into the WNet Deep Learning architecture, and testing it on datasets like HRF and DRIVE.

Result: Attention-WNet demonstrated better performance than existing models in the literature for artery-vein segmentation.

Conclusion: The proposed method enhances retinal vessel analysis, providing better diagnostic tools for identifying health risks, and sets a new benchmark in the field.

Abstract: Segmenting of clinically important retinal blood vessels into arteries and
veins is a prerequisite for retinal vessel analysis. Such analysis can provide
potential insights and bio-markers for identifying and diagnosing various
retinal eye diseases. Alteration in the regularity and width of the retinal
blood vessels can act as an indicator of the health of the vasculature system
all over the body. It can help identify patients at high risk of developing
vasculature diseases like stroke and myocardial infarction. Over the years,
various Deep Learning architectures have been proposed to perform retinal
vessel segmentation. Recently, attention mechanisms have been increasingly used
in image segmentation tasks. The work proposes a new Deep Learning approach for
artery-vein segmentation. The new approach is based on the Attention mechanism
that is incorporated into the WNet Deep Learning model, and we call the model
as Attention-WNet. The proposed approach has been tested on publicly available
datasets such as HRF and DRIVE datasets. The proposed approach has outperformed
other state-of-art models available in the literature.

</details>


### [247] [Person-Centric Annotations of LAION-400M: Auditing Bias and Its Transfer to Models](https://arxiv.org/abs/2510.03721)
*Leander Girrbach,Stephan Alaniz,Genevieve Smith,Trevor Darrell,Zeynep Akata*

Main category: cs.CV

TL;DR: This paper introduces demographic annotations for the LAION-400M dataset to investigate how training data contributes to bias in vision-language models, discovering harmful associations and demographic imbalances.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the lack of demographic annotations in large-scale multimodal datasets, which hinders understanding of biases in vision-language models.

Method: Person-centric annotations for LAION-400M were created using validated pipelines combining object detection, captioning, and finetuned classifiers to generate labels for gender, race/ethnicity, bounding boxes, and captions.

Result: They identified demographic imbalances and harmful associations within LAION-400M, and demonstrated how biases in models like CLIP and Stable Diffusion are partially explained by co-occurrences in the training data.

Conclusion: The study highlights the critical role of dataset composition in shaping biases in vision-language models, and the provided resources offer insights for addressing such biases.

Abstract: Vision-language models trained on large-scale multimodal datasets show strong
demographic biases, but the role of training data in producing these biases
remains unclear. A major barrier has been the lack of demographic annotations
in web-scale datasets such as LAION-400M. We address this gap by creating
person-centric annotations for the full dataset, including over 276 million
bounding boxes, perceived gender and race/ethnicity labels, and automatically
generated captions. These annotations are produced through validated automatic
labeling pipelines combining object detection, multimodal captioning, and
finetuned classifiers. Using them, we uncover demographic imbalances and
harmful associations, such as the disproportionate linking of men and
individuals perceived as Black or Middle Eastern with crime-related and
negative content. We also show that 60-70% of gender bias in CLIP and Stable
Diffusion can be linearly explained by direct co-occurrences in the data. Our
resources establish the first large-scale empirical link between dataset
composition and downstream model bias.

</details>


### [248] [Mapping Rio de Janeiro's favelas: general-purpose vs. satellite-specific neural networks](https://arxiv.org/abs/2510.03725)
*Thomas Hallopeau,Joris Guérin,Laurent Demagistri,Youssef Fouzai,Renata Gracie,Vanderlei Pascoal De Matos,Helen Gurgel,Nadine Dessay*

Main category: cs.CV

TL;DR: The study compares two types of pretrained neural networks to detect Rio de Janeiro's favelas, examining whether task-specific networks or those trained on larger datasets offer better results.


<details>
  <summary>Details</summary>
Motivation: To explore whether task-specificity or exposure to larger training datasets is more effective for detecting urban informal settlements using deep learning methods.

Method: Researchers compare generic neural networks pretrained on diverse datasets with specialized networks pretrained on satellite imagery for detecting informal settlements.

Result: Results highlight the comparative performance of the two model types, shedding light on the trade-offs between task specificity and dataset volume for urban detection tasks.

Conclusion: The paper concludes with insights on the optimal approach for informal settlement detection, helping refine deep learning methods for urban analysis.

Abstract: While deep learning methods for detecting informal settlements have already
been developed, they have not yet fully utilized the potential offered by
recent pretrained neural networks. We compare two types of pretrained neural
networks for detecting the favelas of Rio de Janeiro: 1. Generic networks
pretrained on large diverse datasets of unspecific images, 2. A specialized
network pretrained on satellite imagery. While the latter is more specific to
the target task, the former has been pretrained on significantly more images.
Hence, this research investigates whether task specificity or data volume
yields superior performance in urban informal settlement detection.

</details>


### [249] [LoRA Patching: Exposing the Fragility of Proactive Defenses against Deepfakes](https://arxiv.org/abs/2510.03747)
*Zuomin Qu,Yimao Guo,Qianyue Hu,Wei Lu*

Main category: cs.CV

TL;DR: Deepfake defenses embedding adversarial perturbations in facial images are vulnerable. A new technique, LoRA patching, bypasses them effectively.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the vulnerability of current Deepfake defense techniques, highlighting their lack of robustness and reliability.

Method: The authors introduce LoRA patching, including a gating mechanism to avoid gradient explosions and MMFA loss to ensure semantic alignment, as well as defensive patches to embed visible warnings.

Result: LoRA patching bypasses state-of-the-art defenses with minimal data and training, exposing critical flaws in the existing systems.

Conclusion: The study emphasizes the necessity of developing more robust defense strategies against Deepfake manipulation and highlights the security vulnerability in proactive defense systems.

Abstract: Deepfakes pose significant societal risks, motivating the development of
proactive defenses that embed adversarial perturbations in facial images to
prevent manipulation. However, in this paper, we show that these preemptive
defenses often lack robustness and reliability. We propose a novel approach,
Low-Rank Adaptation (LoRA) patching, which injects a plug-and-play LoRA patch
into Deepfake generators to bypass state-of-the-art defenses. A learnable
gating mechanism adaptively controls the effect of the LoRA patch and prevents
gradient explosions during fine-tuning. We also introduce a Multi-Modal Feature
Alignment (MMFA) loss, encouraging the features of adversarial outputs to align
with those of the desired outputs at the semantic level. Beyond bypassing, we
present defensive LoRA patching, embedding visible warnings in the outputs as a
complementary solution to mitigate this newly identified security
vulnerability. With only 1,000 facial examples and a single epoch of
fine-tuning, LoRA patching successfully defeats multiple proactive defenses.
These results reveal a critical weakness in current paradigms and underscore
the need for more robust Deepfake defense strategies. Our code is available at
https://github.com/ZOMIN28/LoRA-Patching.

</details>


### [250] [The Overlooked Value of Test-time Reference Sets in Visual Place Recognition](https://arxiv.org/abs/2510.03751)
*Mubariz Zaffar,Liangliang Nan,Sebastian Scherer,Julian F. P. Kooij*

Main category: cs.CV

TL;DR: The paper proposes Reference-Set-Finetuning (RSF) as a complementary approach to enhance Visual Place Recognition (VPR) performance on challenging benchmarks by adapting models to the test-time reference set.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of VPR methods struggling in test environments that differ significantly from training datasets.

Method: The authors introduce Reference-Set-Finetuning (RSF), where VPR models are fine-tuned using the test-time reference set (map) images and poses before receiving test queries.

Result: RSF boosts Recall@1 by approximately 2.3% on average for challenging datasets and maintains model generalization across diverse test environments.

Conclusion: RSF significantly improves SOTA performance for challenging VPR benchmarks, leveraging test-time reference sets as a valuable adaptation tool.

Abstract: Given a query image, Visual Place Recognition (VPR) is the task of retrieving
an image of the same place from a reference database with robustness to
viewpoint and appearance changes. Recent works show that some VPR benchmarks
are solved by methods using Vision-Foundation-Model backbones and trained on
large-scale and diverse VPR-specific datasets. Several benchmarks remain
challenging, particularly when the test environments differ significantly from
the usual VPR training datasets. We propose a complementary, unexplored source
of information to bridge the train-test domain gap, which can further improve
the performance of State-of-the-Art (SOTA) VPR methods on such challenging
benchmarks. Concretely, we identify that the test-time reference set, the
"map", contains images and poses of the target domain, and must be available
before the test-time query is received in several VPR applications. Therefore,
we propose to perform simple Reference-Set-Finetuning (RSF) of VPR models on
the map, boosting the SOTA (~2.3% increase on average for Recall@1) on these
challenging datasets. Finetuned models retain generalization, and RSF works
across diverse test datasets.

</details>


### [251] [Adaptively Sampling-Reusing-Mixing Decomposed Gradients to Speed Up Sharpness Aware Minimization](https://arxiv.org/abs/2510.03763)
*Jiaxin Deng,Junbiao Pang*

Main category: cs.CV

TL;DR: The paper introduces ARSAM, a faster alternative to Sharpness-Aware Minimization (SAM) for model generalization, achieving similar accuracy with reduced computational cost.


<details>
  <summary>Details</summary>
Motivation: SAM improves model generalization but its doubled computational cost is a key limitation, prompting the need for faster optimization methods without sacrificing accuracy.

Method: The authors decompose SAM's gradient into SGD gradient and the Projection of the Second-order gradient onto the First-order gradient (PSF). They adaptively reuse and update the PSF to reduce computational overhead while maintaining effective generalization.

Result: ARSAM achieves state-of-the-art accuracies comparable to SAM across diverse tasks, delivers a 40% speedup on CIFAR-10/100, and accelerates optimization for various tasks like human pose estimation and model quantization.

Conclusion: ARSAM provides a computationally efficient alternative to SAM, offering similar performance across tasks while reducing cost and demonstrating broad applicability.

Abstract: Sharpness-Aware Minimization (SAM) improves model generalization but doubles
the computational cost of Stochastic Gradient Descent (SGD) by requiring twice
the gradient calculations per optimization step. To mitigate this, we propose
Adaptively sampling-Reusing-mixing decomposed gradients to significantly
accelerate SAM (ARSAM). Concretely, we firstly discover that SAM's gradient can
be decomposed into the SGD gradient and the Projection of the Second-order
gradient onto the First-order gradient (PSF). Furthermore, we observe that the
SGD gradient and PSF dynamically evolve during training, emphasizing the
growing role of the PSF to achieve a flat minima. Therefore, ARSAM is proposed
to the reused PSF and the timely updated PSF still maintain the model's
generalization ability. Extensive experiments show that ARSAM achieves
state-of-the-art accuracies comparable to SAM across diverse network
architectures. On CIFAR-10/100, ARSAM is comparable to SAM while providing a
speedup of about 40\%. Moreover, ARSAM accelerates optimization for the various
challenge tasks (\textit{e.g.}, human pose estimation, and model quantization)
without sacrificing performance, demonstrating its broad practicality.% The
code is publicly accessible at: https://github.com/ajiaaa/ARSAM.

</details>


### [252] [CoPA: Hierarchical Concept Prompting and Aggregating Network for Explainable Diagnosis](https://arxiv.org/abs/2510.03767)
*Yiheng Dong,Yi Lin,Xin Yang*

Main category: cs.CV

TL;DR: This paper proposes the CoPA framework to enhance the transparency of deep learning models for clinical diagnostics by improving concept capture capabilities through prompt-guided, multilayer concept aggregation.


<details>
  <summary>Details</summary>
Motivation: Concept Bottleneck Models aim to make deep learning-based clinical diagnostics more transparent, but face limitations in effectively capturing fine-grained concepts.

Method: The CoPA framework employs a Concept-aware Embedding Generator for extracting concepts from multiple layers of a visual encoder and Concept Prompt Tuning to guide the model towards recognizing critical visual cues.

Result: CoPA outperforms current state-of-the-art methods in concept and disease prediction across three public datasets.

Conclusion: CoPA improves concept capture and utilization, aiding clinical diagnostic tasks while enhancing model transparency and performance.

Abstract: The transparency of deep learning models is essential for clinical
diagnostics. Concept Bottleneck Model provides clear decision-making processes
for diagnosis by transforming the latent space of black-box models into
human-understandable concepts. However, concept-based methods still face
challenges in concept capture capabilities. These methods often rely on encode
features solely from the final layer, neglecting shallow and multiscale
features, and lack effective guidance in concept encoding, hindering
fine-grained concept extraction. To address these issues, we introduce Concept
Prompting and Aggregating (CoPA), a novel framework designed to capture
multilayer concepts under prompt guidance. This framework utilizes the
Concept-aware Embedding Generator (CEG) to extract concept representations from
each layer of the visual encoder. Simultaneously, these representations serve
as prompts for Concept Prompt Tuning (CPT), steering the model towards
amplifying critical concept-related visual cues. Visual representations from
each layer are aggregated to align with textual concept representations. With
the proposed method, valuable concept-wise information in the images is
captured and utilized effectively, thus improving the performance of concept
and disease prediction. Extensive experimental results demonstrate that CoPA
outperforms state-of-the-art methods on three public datasets. Code is
available at https://github.com/yihengd/CoPA.

</details>


### [253] [Efficiency vs. Efficacy: Assessing the Compression Ratio-Dice Score Relationship through a Simple Benchmarking Framework for Cerebrovascular 3D Segmentation](https://arxiv.org/abs/2510.03769)
*Shimaa Elbana,Ahmad Kamal,Shahd Ahmed Ali,Ahmad Al-Kabbany*

Main category: cs.CV

TL;DR: This paper studied ZFP compression's effect on 3D medical imaging datasets and confirms its capability to reduce data size while maintaining segmentation fidelity.


<details>
  <summary>Details</summary>
Motivation: To address the barriers posed by large and complex 3D medical datasets in collaborative research and transferability.

Method: ZFP compression in error tolerance and fixed-rate modes was applied to a large 3D medical dataset. Segmentation quality post-compression was compared using Dice coefficient.

Result: ZFP compression achieves a data reduction ratio of up to 22.89:1 with minimal fidelity loss (mean Dice coefficient 0.87656).

Conclusion: ZFP compression is a promising method for improving efficiency and accessibility in large-scale 3D medical imaging dataset research.

Abstract: The increasing size and complexity of medical imaging datasets, particularly
in 3D formats, present significant barriers to collaborative research and
transferability. This study investigates whether the ZFP compression technique
can mitigate these challenges without compromising the performance of automated
cerebrovascular segmentation, a critical first step in intracranial aneurysm
detection. We apply ZFP in both its error tolerance and fixed-rate modes to a
large scale, and one of the most recent, datasets in the literature, 3D medical
dataset containing ground-truth vascular segmentations. The segmentation
quality on the compressed volumes is rigorously compared to the uncompressed
baseline (Dice approximately equals 0.8774). Our findings reveal that ZFP can
achieve substantial data reduction--up to a 22.89:1 ratio in error tolerance
mode--while maintaining a high degree of fidelity, with the mean Dice
coefficient remaining high at 0.87656. These results demonstrate that ZFP is a
viable and powerful tool for enabling more efficient and accessible research on
large-scale medical datasets, fostering broader collaboration across the
community.

</details>


### [254] [MambaCAFU: Hybrid Multi-Scale and Multi-Attention Model with Mamba-Based Fusion for Medical Image Segmentation](https://arxiv.org/abs/2510.03786)
*T-Mai Bui,Fares Bougourzi,Fadi Dornaika,Vinh Truong Hoang*

Main category: cs.CV

TL;DR: This paper introduces a hybrid segmentation model combining CNNs, Transformers, and a novel attention mechanism to enhance segmentation accuracy while maintaining computational efficiency in medical imaging tasks.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning models are task-specific and face challenges in balancing complexity and performance across varied medical imaging scenarios, making clinical applications demanding.

Method: The proposed architecture consists of a three-branch encoder that integrates CNNs, Transformers, and a Mamba Attention Fusion mechanism. It uses a multi-scale attention-based CNN decoder and a co-attention gate for improved feature interaction and spatial-semantic information selection.

Result: The approach demonstrated superior accuracy and generalization compared to state-of-the-art methods on multiple benchmark datasets, without increasing computational complexity.

Conclusion: The hybrid model achieves a practical balance between accuracy and efficiency, making it scalable for diverse medical imaging applications, with its code and models set for open-source release to encourage further research.

Abstract: In recent years, deep learning has shown near-expert performance in
segmenting complex medical tissues and tumors. However, existing models are
often task-specific, with performance varying across modalities and anatomical
regions. Balancing model complexity and performance remains challenging,
particularly in clinical settings where both accuracy and efficiency are
critical. To address these issues, we propose a hybrid segmentation
architecture featuring a three-branch encoder that integrates CNNs,
Transformers, and a Mamba-based Attention Fusion (MAF) mechanism to capture
local, global, and long-range dependencies. A multi-scale attention-based CNN
decoder reconstructs fine-grained segmentation maps while preserving contextual
consistency. Additionally, a co-attention gate enhances feature selection by
emphasizing relevant spatial and semantic information across scales during both
encoding and decoding, improving feature interaction and cross-scale
communication. Extensive experiments on multiple benchmark datasets show that
our approach outperforms state-of-the-art methods in accuracy and
generalization, while maintaining comparable computational complexity. By
effectively balancing efficiency and effectiveness, our architecture offers a
practical and scalable solution for diverse medical imaging tasks. Source code
and trained models will be publicly released upon acceptance to support
reproducibility and further research.

</details>


### [255] [Road Damage and Manhole Detection using Deep Learning for Smart Cities: A Polygonal Annotation Approach](https://arxiv.org/abs/2510.03797)
*Rasel Hossen,Diptajoy Mistry,Mushiur Rahman,Waki As Sami Atikur Rahman Hridoy,Sajib Saha,Muhammad Ibrahim*

Main category: cs.CV

TL;DR: This paper introduces a deep learning solution using YOLOv9 and polygonal annotations for detecting road damage and manholes efficiently.


<details>
  <summary>Details</summary>
Motivation: Address the inefficiency and high costs of manual road maintenance monitoring and detection within smart city development.

Method: Utilized YOLOv9 with polygonal annotations and a dataset of over 1,000 images to classify road conditions into three categories: Broken, Not Broken, and Manhole.

Result: Achieved 78.1% overall accuracy, strong performance in detecting Broken (86.7% F1-score) and Not Broken (89.2% F1-score), but lower accuracy for Manhole detection (18.2% F1-score) due to class imbalance.

Conclusion: This approach provides an automated, scalable solution for monitoring road infrastructure, particularly in developing countries like Bangladesh.

Abstract: Urban safety and infrastructure maintenance are critical components of smart
city development. Manual monitoring of road damages is time-consuming, highly
costly, and error-prone. This paper presents a deep learning approach for
automated road damage and manhole detection using the YOLOv9 algorithm with
polygonal annotations. Unlike traditional bounding box annotation, we employ
polygonal annotations for more precise localization of road defects. We develop
a novel dataset comprising more than one thousand images which are mostly
collected from Dhaka, Bangladesh. This dataset is used to train a YOLO-based
model for three classes, namely Broken, Not Broken, and Manhole. We achieve
78.1% overall image-level accuracy. The YOLOv9 model demonstrates strong
performance for Broken (86.7% F1-score) and Not Broken (89.2% F1-score)
classes, with challenges in Manhole detection (18.2% F1-score) due to class
imbalance. Our approach offers an efficient and scalable solution for
monitoring urban infrastructure in developing countries.

</details>


### [256] [Contrastive-SDE: Guiding Stochastic Differential Equations with Contrastive Learning for Unpaired Image-to-Image Translation](https://arxiv.org/abs/2510.03821)
*Venkata Narendra Kotyada,Revanth Eranki,Nagesh Bhattu Sristy*

Main category: cs.CV

TL;DR: Proposes a novel time-dependent contrastive learning approach combined with score-based diffusion models for unpaired image-to-image translation, showing improved efficiency and comparable performance with state-of-the-art baselines.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of unpaired image-to-image translation, leveraging advanced generative and self-supervised learning techniques to improve fidelity, diversity, and semantic consistency in results.

Method: Utilizes time-dependent contrastive learning with SimCLR to learn domain-invariant features, combined with pretrained stochastic differential equations (SDEs) for guiding unpaired image-to-image translation.

Result: Contrastive-SDE achieves competitive results against state-of-the-art baselines across multiple tasks and metrics, while converging faster and operating without label supervision or classifier training.

Conclusion: Contrastive-SDE represents a more efficient alternative for unpaired image-to-image translation tasks, maintaining semantic consistency and achieving high-quality results without requiring additional supervision.

Abstract: Unpaired image-to-image translation involves learning mappings between source
domain and target domain in the absence of aligned or corresponding samples.
Score based diffusion models have demonstrated state-of-the-art performance in
generative tasks. Their ability to approximate complex data distributions
through stochastic differential equations (SDEs) enables them to generate
high-fidelity and diverse outputs, making them particularly well-suited for
unpaired I2I settings. In parallel, contrastive learning provides a powerful
framework for learning semantic similarities without the need for explicit
supervision or paired data. By pulling together representations of semantically
similar samples and pushing apart dissimilar ones, contrastive methods are
inherently aligned with the objectives of unpaired translation. Its ability to
selectively enforce semantic consistency at the feature level makes contrastive
learning particularly effective for guiding generation in unpaired scenarios.
In this work, we propose a time-dependent contrastive learning approach where a
model is trained with SimCLR by considering an image and its domain invarient
feature as a positive pair, enabling the preservation of domain-invariant
features and the discarding of domain-specific ones. The learned contrastive
model then guides the inference of a pretrained SDE for the I2I translation
task. We empirically compare Contrastive-SDE with several baselines across
three common unpaired I2I tasks, using four metrics for evaluation.
Constrastive-SDE achieves comparable results to the state-of-the-art on several
metrics. Furthermore, we observe that our model converges significantly faster
and requires no label supervision or classifier training, making it a more
efficient alternative for this task.

</details>


### [257] [LIBERO-PRO: Towards Robust and Fair Evaluation of Vision-Language-Action Models Beyond Memorization](https://arxiv.org/abs/2510.03827)
*Xueyang Zhou,Yangming Xu,Guiyao Tie,Yongchao Chen,Guowen Zhang,Duanfeng Chu,Pan Zhou,Lichao Sun*

Main category: cs.CV

TL;DR: LIBERO-PRO introduces a revised benchmark to solve problems in evaluating Vision-Language-Action models, uncovering significant flaws in current models’ generalization and comprehension.


<details>
  <summary>Details</summary>
Motivation: Current LIBERO benchmark settings inflate performance estimates and hinder fair model comparison due to overly narrow evaluation conditions.

Method: LIBERO-PRO systematically benchmarks model performance across manipulated objects, initial states, task instructions, and environments to evaluate generalization and eliminate reliance on rote memorization.

Result: Models achieving over 90% accuracy under LIBERO collapse to 0.0% accuracy under LIBERO-PRO, revealing their overdependence on memorized data rather than genuine task understanding.

Conclusion: The study exposes critical flaws in existing evaluation methods and advocates for robust benchmarks to test model generalization and comprehension.

Abstract: LIBERO has emerged as a widely adopted benchmark for evaluating
Vision-Language-Action (VLA) models; however, its current training and
evaluation settings are problematic, often leading to inflated performance
estimates and preventing fair model comparison. To address these issues, we
introduce LIBERO-PRO, an extended LIBERO benchmark that systematically
evaluates model performance under reasonable perturbations across four
dimensions: manipulated objects, initial states, task instructions, and
environments. Experimental results reveal that, although existing models
achieve over 90% accuracy under the standard LIBERO evaluation, their
performance collapses to 0.0% under our generalized setting. Crucially, this
discrepancy exposes the models' reliance on rote memorization of action
sequences and environment layouts from the training set, rather than genuine
task understanding or environmental perception. For instance, models persist in
executing grasping actions when the target object is replaced with irrelevant
items, and their outputs remain unchanged even when given corrupted
instructions or even messy tokens. These findings expose the severe flaws in
current evaluation practices, and we call on the community to abandon
misleading methodologies in favor of robust assessments of model generalization
and comprehension. Our code is available at:
https://github.com/Zxy-MLlab/LIBERO-PRO.

</details>


### [258] [Mirage: Unveiling Hidden Artifacts in Synthetic Images with Large Vision-Language Models](https://arxiv.org/abs/2510.03840)
*Pranav Sharma,Shivank Garg,Durga Toshniwal*

Main category: cs.CV

TL;DR: The paper introduces "Mirage", a dataset of AI-generated images with visible artifacts, showing that state-of-the-art AI detectors struggle to identify them despite humans succeeding. It evaluates the ability of Large Vision-Language Models (LVLMs) for explainable AI detection, finding their effectiveness limited to images with visible artifacts.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the gap in AI image detection, where existing models struggle to identify synthetic images that remain distinct to humans, and explore the potential of LVLMs for improving detection with explainability.

Method: The authors introduced "Mirage", a curated dataset of AI-generated images with visible artifacts, and experimented using LVLMs to assess their capability in detecting images with and without these artifacts. They benchmarked LVLMs against existing detection models.

Result: The study found that LVLMs excel at identifying AI-generated images with visible artifacts but show a significant decline in performance when detecting images without such cues.

Conclusion: LVLMs are promising tools for detecting AI-generated images with visual imperfections, but their generalizability to artifact-free images is limited, highlighting the need for further research in robust AI image detection.

Abstract: Recent advances in image generation models have led to models that produce
synthetic images that are increasingly difficult for standard AI detectors to
identify, even though they often remain distinguishable by humans. To identify
this discrepancy, we introduce \textbf{Mirage}, a curated dataset comprising a
diverse range of AI-generated images exhibiting visible artifacts, where
current state-of-the-art detection methods largely fail. Furthermore, we
investigate whether Large Vision-Language Models (LVLMs), which are
increasingly employed as substitutes for human judgment in various tasks, can
be leveraged for explainable AI image detection. Our experiments on both Mirage
and existing benchmark datasets demonstrate that while LVLMs are highly
effective at detecting AI-generated images with visible artifacts, their
performance declines when confronted with images lacking such cues.

</details>


### [259] [UGround: Towards Unified Visual Grounding with Unrolled Transformers](https://arxiv.org/abs/2510.03853)
*Rui Qian,Xin Yin,Chuanhang Deng,Zhiyuan Peng,Jian Xiong,Wei Zhai,Dejing Dou*

Main category: cs.CV

TL;DR: UGround introduces a novel approach for visual grounding using a unified paradigm that dynamically selects intermediate layers from unrolled transformers instead of relying on the fixed last hidden layer, addressing key challenges in the field.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of the prevailing visual grounding paradigm, which sequentially amplifies errors and lacks explicit spatial cues in textual-to-visual embedding projection.

Method: UGround uses Policy-Prompted Masking, combining Stochastic Skip Connection (SSC) for dynamic layer selection and Mask as Prompt (MasP) for generating spatially-aware mask prompts.

Result: A unified framework for visual grounding across diverse tasks, including traditional segmentation and reasoning segmentation, validated through experiments.

Conclusion: UGround effectively addresses challenges in visual grounding by providing a unified and spatially-aware solution, demonstrating versatility in both single and multi-target setups.

Abstract: We present UGround, a \textbf{U}nified visual \textbf{Ground}ing paradigm
that dynamically selects intermediate layers across \textbf{U}nrolled
transformers as ``mask as prompt'', diverging from the prevailing pipeline that
leverages the fixed last hidden layer as ``\texttt{<SEG>} as prompt''. UGround
addresses two primary challenges posed by the prevailing paradigm: (1) its
reliance on the fixed last hidden layer, which sequentially amplifies
cumulative errors arising from layer-by-layer propagation without intermediate
correction, and (2) its use of \texttt{<SEG>} as a prompt, which implicitly
projects textual embeddings into visual space without explicit spatial cues
(\eg, coordinates). Central to UGround is Policy-Prompted Masking, which
comprises two key components: Stochastic Skip Connection (SSC) and Mask as
Prompt (MasP). SSC is a reinforcement learning policy that, via stochastic
sampling, allows each \texttt{<SEG>} token to slide across unrolled transformer
layers, enabling dynamic layer selection at which it connects to the vision
model (\eg, SAM) in a skip-connection fashion. Given the selected hidden layer,
MasP uses the similarity map derived from the \texttt{<SEG>} token and image
tokens as a soft logit mask to prompt SAM for mask generation, offering
explicit spatial cues through its activation regions. To validate the
effectiveness of UGround, we, for the first time, have unified visual grounding
within a single framework from an attribute perspective, spanning from
traditional refer expression segmentation to newly proposed reasoning
segmentation, single-target to multi-target, positive query to false premise
(empty target). All codes and models are publicly available at
\href{https://github.com/rui-qian/UGround}{https://github.com/rui-qian/UGround}.

</details>


### [260] [Optimized Minimal 4D Gaussian Splatting](https://arxiv.org/abs/2510.03857)
*Minseo Lee,Byeonghyeon Lee,Lucas Yunkyu Lee,Eunsoo Lee,Sangmin Kim,Seunghyeon Song,Joo Chan Lee,Jong Hwan Ko,Jaesik Park,Eunbyung Park*

Main category: cs.CV

TL;DR: This paper introduces OMG4, a framework for compressing 4D Gaussian Splatting models significantly while maintaining high visual fidelity.


<details>
  <summary>Details</summary>
Motivation: 4D Gaussian Splatting consumes excessive storage due to the large number of Gaussians required for high-quality dynamic scene representation.

Method: The approach involves three stages: Gaussian Sampling to identify key primitives, Gaussian Pruning to eliminate redundancies, and Gaussian Merging to fuse similar primitives. It also applies appearance compression and adapts Sub-Vector Quantization for 4D.

Result: Experiments show OMG4 reduces model sizes by over 60% compared to recent methods while retaining reconstruction quality.

Conclusion: OMG4 represents a major advancement in dynamic scene representation, offering compact storage solutions and enabling broad application potential.

Abstract: 4D Gaussian Splatting has emerged as a new paradigm for dynamic scene
representation, enabling real-time rendering of scenes with complex motions.
However, it faces a major challenge of storage overhead, as millions of
Gaussians are required for high-fidelity reconstruction. While several studies
have attempted to alleviate this memory burden, they still face limitations in
compression ratio or visual quality. In this work, we present OMG4 (Optimized
Minimal 4D Gaussian Splatting), a framework that constructs a compact set of
salient Gaussians capable of faithfully representing 4D Gaussian models. Our
method progressively prunes Gaussians in three stages: (1) Gaussian Sampling to
identify primitives critical to reconstruction fidelity, (2) Gaussian Pruning
to remove redundancies, and (3) Gaussian Merging to fuse primitives with
similar characteristics. In addition, we integrate implicit appearance
compression and generalize Sub-Vector Quantization (SVQ) to 4D representations,
further reducing storage while preserving quality. Extensive experiments on
standard benchmark datasets demonstrate that OMG4 significantly outperforms
recent state-of-the-art methods, reducing model sizes by over 60% while
maintaining reconstruction quality. These results position OMG4 as a
significant step forward in compact 4D scene representation, opening new
possibilities for a wide range of applications. Our source code is available at
https://minshirley.github.io/OMG4/.

</details>


### [261] [Cross-View Open-Vocabulary Object Detection in Aerial Imagery](https://arxiv.org/abs/2510.03858)
*Jyoti Kini,Rohit Gupta,Mubarak Shah*

Main category: cs.CV

TL;DR: This paper addresses open-vocabulary object detection in aerial imagery by adapting representations from ground-view images through structured domain alignment methods, achieving substantial performance gains.


<details>
  <summary>Details</summary>
Motivation: The paper aims to overcome limitations of traditional object detection models, which are restricted to a fixed set of classes, by enabling detection of unseen categories in aerial images using open-vocabulary methods, resolving challenges like domain shifts and scale differences.

Method: The proposed framework leverages contrastive image-to-image alignment and multi-instance vocabulary associations to align aerial imagery embeddings with ground-view image and text embeddings.

Result: Experiments on aerial datasets (xView, DOTAv2, VisDrone, DIOR, HRRSD) demonstrate significant zero-shot performance gains over traditional closed-vocabulary models, including improvements of +6.32 mAP on DOTAv2 and +4.16 mAP on VisDrone.

Conclusion: The approach validates its potential in creating more adaptable and efficient object detection systems for aerial applications, addressing scalability and flexibility in detecting unseen categories.

Abstract: Traditional object detection models are typically trained on a fixed set of
classes, limiting their flexibility and making it costly to incorporate new
categories. Open-vocabulary object detection addresses this limitation by
enabling models to identify unseen classes without explicit training.
Leveraging pretrained models contrastively trained on abundantly available
ground-view image-text classification pairs provides a strong foundation for
open-vocabulary object detection in aerial imagery. Domain shifts, viewpoint
variations, and extreme scale differences make direct knowledge transfer across
domains ineffective, requiring specialized adaptation strategies. In this
paper, we propose a novel framework for adapting open-vocabulary
representations from ground-view images to solve object detection in aerial
imagery through structured domain alignment. The method introduces contrastive
image-to-image alignment to enhance the similarity between aerial and
ground-view embeddings and employs multi-instance vocabulary associations to
align aerial images with text embeddings. Extensive experiments on the xView,
DOTAv2, VisDrone, DIOR, and HRRSD datasets are used to validate our approach.
Our open-vocabulary model achieves improvements of +6.32 mAP on DOTAv2, +4.16
mAP on VisDrone (Images), and +3.46 mAP on HRRSD in the zero-shot setting when
compared to finetuned closed-vocabulary dataset-specific model performance,
thus paving the way for more flexible and scalable object detection systems in
aerial applications.

</details>


### [262] [Exploring the Challenge and Value of Deep Learning in Automated Skin Disease Diagnosis](https://arxiv.org/abs/2510.03869)
*Runhao Liu,Ziming Chen,Peng Zhang*

Main category: cs.CV

TL;DR: This paper reviews the potential of deep learning (DL) for skin cancer diagnosis, focuses on overcoming key challenges, and emphasizes integration into clinical workflows.


<details>
  <summary>Details</summary>
Motivation: Early detection of skin cancer is critical for improving patient outcomes, and deep learning offers promising avenues to enhance diagnostic processes.

Method: The paper synthesizes recent research using a PRISMA framework to discuss innovative DL-based strategies, including data augmentation, hybrid models, and feature fusion.

Result: It identifies innovative approaches like data augmentation and integration of DL models into clinical workflows to address challenges such as image noise and data imbalance.

Conclusion: Deep learning holds transformative potential for dermatological care, requiring further advancements to effectively integrate into clinical practice.

Abstract: Skin cancer is one of the most prevalent and deadly forms of cancer
worldwide, which highlights the critical importance of early detection and
diagnosis in improving patient outcomes. Deep learning (DL) has shown
significant promise in enhancing the accuracy and efficiency of automated skin
disease diagnosis, particularly in detecting and evaluating skin lesions and
classification. However, there are still several challenges for DL-based skin
cancer diagnosis, including complex features, image noise, intra-class
variation, inter-class similarity, and data imbalance. By synthesizing recent
research, this review discusses innovative approaches to cope with these
challenges, such as data augmentation, hybrid models, and feature fusion, etc.
Furthermore, the review highlights the integration of DL models into clinical
workflows, offering insights into the potential of deep learning to
revolutionize skin disease diagnosis and improve clinical decision-making. This
article follows a comprehensive methodology based on the PRISMA framework and
emphasizes the need for continued advancements to fully unlock the
transformative potential of DL in dermatological care.

</details>


### [263] [SDAKD: Student Discriminator Assisted Knowledge Distillation for Super-Resolution Generative Adversarial Networks](https://arxiv.org/abs/2510.03870)
*Nikolaos Kaparinos,Vasileios Mezaris*

Main category: cs.CV

TL;DR: The paper introduces a novel GAN compression method, SDAKD, overcoming capacity mismatch in distillation training to enhance efficiency, demonstrated with improved results on image super-resolution tasks.


<details>
  <summary>Details</summary>
Motivation: Generative Adversarial Networks (GANs) are powerful for generative tasks like image super-resolution, but their high computational demands hinder deployment on resource-constrained devices. Knowledge distillation offers a solution, but challenges arise in training smaller GANs due to capacity mismatches between components.

Method: The paper proposes Student Discriminator Assisted Knowledge Distillation (SDAKD), which employs a three-stage training approach and incorporates adapted feature map distillation in the latter stages to address capacity mismatch by introducing a student discriminator.

Result: Experimental evaluation of SDAKD on two super-resolution GANs, GCFSR and Real-ESRGAN, shows consistent performance improvements over baseline methods and state-of-the-art GAN knowledge distillation techniques.

Conclusion: SDAKD effectively mitigates capacity mismatch in GAN knowledge distillation, enhancing performance and enabling compression for resource-constrained environments. The approach shows promise for real-world applications, with open-source code to follow.

Abstract: Generative Adversarial Networks (GANs) achieve excellent performance in
generative tasks, such as image super-resolution, but their computational
requirements make difficult their deployment on resource-constrained devices.
While knowledge distillation is a promising research direction for GAN
compression, effectively training a smaller student generator is challenging
due to the capacity mismatch between the student generator and the teacher
discriminator. In this work, we propose Student Discriminator Assisted
Knowledge Distillation (SDAKD), a novel GAN distillation methodology that
introduces a student discriminator to mitigate this capacity mismatch. SDAKD
follows a three-stage training strategy, and integrates an adapted feature map
distillation approach in its last two training stages. We evaluated SDAKD on
two well-performing super-resolution GANs, GCFSR and Real-ESRGAN. Our
experiments demonstrate consistent improvements over the baselines and SOTA GAN
knowledge distillation methods. The SDAKD source code will be made openly
available upon acceptance of the paper.

</details>


### [264] [PoseGaze-AHP: A Knowledge-Based 3D Dataset for AI-Driven Ocular and Postural Diagnosis](https://arxiv.org/abs/2510.03873)
*Saja Al-Dabet,Sherzod Turaev,Nazar Zaki,Arif O. Khan,Luai Eldweik*

Main category: cs.CV

TL;DR: The paper presents PoseGaze-AHP, a 3D dataset combining head pose and gaze movement for analyzing ocular-induced abnormal head posture (AHP), enabling AI-driven diagnostics.


<details>
  <summary>Details</summary>
Motivation: The study aims to overcome limitations in existing datasets that address head pose and ocular movements separately, hindering integrated diagnostic approaches for AHP.

Method: The team used large language models to extract structured clinical data, processed it through the Neural Head Avatar framework to develop 3D representations, and generated 7,920 images covering various ocular conditions.

Result: Their extraction method achieved 91.92% accuracy, validating its reliability for generating clinical datasets used in PoseGaze-AHP.

Conclusion: PoseGaze-AHP offers a publicly accessible, privacy-compliant resource that supports advancements in AI-driven diagnosis for ocular-induced AHP, filling a critical gap in medical research tools.

Abstract: Diagnosing ocular-induced abnormal head posture (AHP) requires a
comprehensive analysis of both head pose and ocular movements. However,
existing datasets focus on these aspects separately, limiting the development
of integrated diagnostic approaches and restricting AI-driven advancements in
AHP analysis. To address this gap, we introduce PoseGaze-AHP, a novel 3D
dataset that synchronously captures head pose and gaze movement information for
ocular-induced AHP assessment. Structured clinical data were extracted from
medical literature using large language models (LLMs) through an iterative
process with the Claude 3.5 Sonnet model, combining stepwise, hierarchical, and
complex prompting strategies. The extracted records were systematically imputed
and transformed into 3D representations using the Neural Head Avatar (NHA)
framework. The dataset includes 7,920 images generated from two head textures,
covering a broad spectrum of ocular conditions. The extraction method achieved
an overall accuracy of 91.92%, demonstrating its reliability for clinical
dataset construction. PoseGaze-AHP is the first publicly available resource
tailored for AI-driven ocular-induced AHP diagnosis, supporting the development
of accurate and privacy-compliant diagnostic tools.

</details>


### [265] [DHQA-4D: Perceptual Quality Assessment of Dynamic 4D Digital Human](https://arxiv.org/abs/2510.03874)
*Yunhao Li,Sijing Wu,Yucheng Zhu,Huiyu Duan,Zicheng Zhang,Guangtao Zhai*

Main category: cs.CV

TL;DR: The paper introduces a new dataset (DHQA-4D) and proposes an advanced method (DynaMesh-Rater) for assessing the quality of textured and non-textured 4D human meshes.


<details>
  <summary>Details</summary>
Motivation: With the growing popularity of dynamic 4D digital human avatars in areas like gaming and remote communication, there is a need to evaluate and address quality issues caused by noise in processes such as collection and transmission.

Method: The authors created the DHQA-4D dataset containing high-quality and distorted 4D meshes as a resource for assessing mesh quality. Additionally, they developed the DynaMesh-Rater, a multimodal model that extracts visual, motion, and geometry features to predict quality scores using LMM and LoRA-based instruction tuning.

Result: Their approach demonstrated superior performance compared to prior methods in predicting quality scores on the DHQA-4D dataset.

Conclusion: The paper provides critical tools and methodologies for advancing the domain of 4D human mesh quality assessment, supporting better user experiences for applications like gaming and immersive communication.

Abstract: With the rapid development of 3D scanning and reconstruction technologies,
dynamic digital human avatars based on 4D meshes have become increasingly
popular. A high-precision dynamic digital human avatar can be applied to
various fields such as game production, animation generation, and remote
immersive communication. However, these 4D human avatar meshes are prone to
being degraded by various types of noise during the processes of collection,
compression, and transmission, thereby affecting the viewing experience of
users. In light of this fact, quality assessment of dynamic 4D digital humans
becomes increasingly important. In this paper, we first propose a large-scale
dynamic digital human quality assessment dataset, DHQA-4D, which contains 32
high-quality real-scanned 4D human mesh sequences, 1920 distorted textured 4D
human meshes degraded by 11 textured distortions, as well as their
corresponding textured and non-textured mean opinion scores (MOSs). Equipped
with DHQA-4D dataset, we analyze the influence of different types of distortion
on human perception for textured dynamic 4D meshes and non-textured dynamic 4D
meshes. Additionally, we propose DynaMesh-Rater, a novel large multimodal model
(LMM) based approach that is able to assess both textured 4D meshes and
non-textured 4D meshes. Concretely, DynaMesh-Rater elaborately extracts
multi-dimensional features, including visual features from a projected 2D
video, motion features from cropped video clips, and geometry features from the
4D human mesh to provide comprehensive quality-related information. Then we
utilize a LMM model to integrate the multi-dimensional features and conduct a
LoRA-based instruction tuning technique to teach the LMM model to predict the
quality scores. Extensive experimental results on the DHQA-4D dataset
demonstrate the superiority of our DynaMesh-Rater method over previous quality
assessment methods.

</details>


### [266] [Skin Lesion Classification Based on ResNet-50 Enhanced With Adaptive Spatial Feature Fusion](https://arxiv.org/abs/2510.03876)
*Runhao Liu,Ziming Chen,Peng Zhang*

Main category: cs.CV

TL;DR: This paper presents an enhanced ResNet-50 model with Adaptive Spatial Feature Fusion (ASFF) for skin cancer classification, achieving high performance metrics on the ISIC 2020 dataset.


<details>
  <summary>Details</summary>
Motivation: Address the challenges of high inter-class similarity, intra-class variability, and image noise in skin cancer classification using dermoscopic images.

Method: Developed an improved ResNet-50 model integrated with Adaptive Spatial Feature Fusion (ASFF) that uses a dual-branch mechanism for adaptive multi-scale feature extraction and weighted fusion.

Result: The model achieves an accuracy of 93.18%, AUC values of 0.9670 (P-R curve) and 0.9717 (ROC curve), along with improved precision, recall, specificity, and F1 score. Grad-CAM demonstrates its capacity to focus on lesion-relevant regions.

Conclusion: The proposed ASFF-based ResNet-50 model enhances feature extraction, suppresses irrelevant noise, and provides an effective solution for computer-aided skin cancer diagnosis.

Abstract: Skin cancer classification remains a challenging problem due to high
inter-class similarity, intra-class variability, and image noise in dermoscopic
images. To address these issues, we propose an improved ResNet-50 model
enhanced with Adaptive Spatial Feature Fusion (ASFF), which adaptively
integrates multi-scale semantic and surface features to improve feature
representation and reduce overfitting. The ResNet-50 model is enhanced with an
adaptive feature fusion mechanism to achieve more effective multi-scale feature
extraction and improve overall performance. Specifically, a dual-branch design
fuses high-level semantic and mid-level detail features, which are processed
through global average pooling and fully connected layers to generate adaptive
weights for weighted fusion, thereby strengthening feature learning and
reducing the impact of noise on classification. The method is evaluated on a
subset of the ISIC 2020 dataset containing 3297 benign and malignant skin
lesion images. Experimental results show that the proposed ASFF-based ResNet-50
achieves the best overall performance compared with 5 classic convolutional
neural networks (CNNs) models. The proposed model reached an accuracy of 93.18%
along with higher precision, recall, specificity, and F1 score. The improved
model achieves an AUC value of 0.9670 and 0.9717 in the P-R and ROC curve,
respectively. Then, the evaluation based on Grad-CAM further proved that the
improved model adaptively focuses on lesion-relevant regions while suppressing
irrelevant background information, thereby validating its enhanced feature
learning capability from a deep representation perspective. These findings
demonstrate that the proposed approach provides a more effective and efficient
solution for computer-aided skin cancer diagnosis.

</details>


### [267] [Bridge Thinking and Acting: Unleashing Physical Potential of VLM with Generalizable Action Expert](https://arxiv.org/abs/2510.03896)
*Mingyu Liu,Zheng Huang,Xiaoyi Lin,Muzhi Zhu,Canyu Zhao,Zongze Du,Yating Wang,Haoyi Zhu,Hao Chen,Chunhua Shen*

Main category: cs.CV

TL;DR: The paper addresses limitations of conventional Vision-Language-Action (VLA) models in physical world tasks by introducing a framework with a generalizable action expert using 3D trajectories to bridge planning and action modules.


<details>
  <summary>Details</summary>
Motivation: Challenges in translating Vision-Language Models' (VLM) reasoning abilities into the physical world due to poor generalization and data constraints in Vision-Language-Action models.

Method: A framework utilizing sparse 3D trajectories as intermediate representations for coarse waypoints refined into executable action sequences by a generalizable action expert. It introduces an 'Action Pre-training, Pointcloud Fine-tuning' paradigm.

Result: The method connects high-level planning of VLMs with low-level physical actions, enabling efficient training and robust generalization across tasks.

Conclusion: The approach effectively addresses semantic ambiguities and training inefficiencies by integrating generalization strengths of VLMs and action-level modules for improved task performance.

Abstract: Although Vision-Language Models (VLM) have demonstrated impressive planning
and reasoning capabilities, translating these abilities into the physical world
introduces significant challenges. Conventional Vision-Language-Action (VLA)
models, which integrate reasoning and action into a monolithic architecture,
generalize poorly because they are constrained by scarce, narrow-domain data.
While recent dual-system approaches attempt to decouple "thinking" from
"acting", they are often constrained by semantic ambiguities within the action
module. This ambiguity makes large-scale, cross-task training infeasible.
Consequently, these systems typically necessitate fine-tuning on newly
collected data when deployed to novel environments, and the cooperation
mechanism between the two systems remains ill-defined. To address these
limitations, we introduce, for the first time, a framework centered around a
generalizable action expert. Our approach utilizes sparse 3D trajectories as an
intermediate representation, effectively bridging the high-level planning
capabilities of the VLM with the low-level physical action module. During the
planning phase, the VLM is only required to generate coarse 3D waypoints. These
waypoints are then processed by our generalizable action expert, which refines
them into dense, executable action sequences by sampling real-time point cloud
observations of the environment. To promote training efficiency and robust
generalization, we introduce a novel "Action Pre-training, Pointcloud
Fine-tuning" paradigm. Our method combines the broad generalization
capabilities of VLMs in visual understanding and planning with the
fine-grained, action-level generalization of action expert.

</details>


### [268] [Multi-Modal Oral Cancer Detection Using Weighted Ensemble Convolutional Neural Networks](https://arxiv.org/abs/2510.03878)
*Ajo Babu George,Sreehari J R Ajo Babu George,Sreehari J R Ajo Babu George,Sreehari J R*

Main category: cs.CV

TL;DR: This paper presents a multimodal deep learning framework using DenseNet-121 CNNs to improve early detection of Oral Squamous Cell Carcinoma (OSCC), achieving significant validation accuracy using clinical, radiological, and histopathological images.


<details>
  <summary>Details</summary>
Motivation: Late diagnosis of OSCC significantly contributes to its high global mortality rate, highlighting the need for improved early detection methods to enhance patient outcomes.

Method: The study used three distinct medical imaging modalities (clinical, radiological, histopathological) in a retrospective design with DenseNet-121 CNNs. Transfer learning, preprocessing, augmentation, and a validation-weighted ensemble strategy were employed for robust predictions.

Result: High accuracy was achieved in radiological (100%) and histopathological images (95.12%), while clinical images lagged (63.10%) due to visual heterogeneity. The ensemble model demonstrated an overall accuracy of 84.58%.

Conclusion: The proposed multimodal ensemble framework offers a non-invasive AI-assisted method that aids in early detection and clinical decision-making, helping to reduce diagnostic delays and potentially improve patient survival rates.

Abstract: Aims Late diagnosis of Oral Squamous Cell Carcinoma (OSCC) contributes
significantly to its high global mortality rate, with over 50\% of cases
detected at advanced stages and a 5-year survival rate below 50\% according to
WHO statistics. This study aims to improve early detection of OSCC by
developing a multimodal deep learning framework that integrates clinical,
radiological, and histopathological images using a weighted ensemble of
DenseNet-121 convolutional neural networks (CNNs). Material and Methods A
retrospective study was conducted using publicly available datasets
representing three distinct medical imaging modalities. Each modality-specific
dataset was used to train a DenseNet-121 CNN via transfer learning.
Augmentation and modality-specific preprocessing were applied to increase
robustness. Predictions were fused using a validation-weighted ensemble
strategy. Evaluation was performed using accuracy, precision, recall, F1-score.
Results High validation accuracy was achieved for radiological (100\%) and
histopathological (95.12\%) modalities, with clinical images performing lower
(63.10\%) due to visual heterogeneity. The ensemble model demonstrated improved
diagnostic robustness with an overall accuracy of 84.58\% on a multimodal
validation dataset of 55 samples. Conclusion The multimodal ensemble framework
bridges gaps in the current diagnostic workflow by offering a non-invasive,
AI-assisted triage tool that enhances early identification of high-risk
lesions. It supports clinicians in decision-making, aligning with global
oncology guidelines to reduce diagnostic delays and improve patient outcomes.

</details>


### [269] [Exploring Instruction Data Quality for Explainable Image Quality Assessment](https://arxiv.org/abs/2510.03880)
*Yunhao Li,Sijing Wu,Huiyu Duan,Yucheng Zhu,Qi Jia,Guangtao Zhai*

Main category: cs.CV

TL;DR: The paper explores optimizing explainable image quality assessment (IQA) with multimodal large language models (MLLMs) by reducing the size of instruction tuning datasets while maintaining or improving model performance. They introduce a data selection method called IQA-Select.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the high computational cost and data redundancy issues in large-scale instruction tuning datasets used for explainable IQA, challenging the conventional scaling law that larger datasets always yield better results.

Method: The paper investigates how fine-tuning model performance varies with different dataset sizes. It proposes a clustering-based data selection framework (IQA-Select) with three stages: feature extraction, quota allocation, and cluster sampling, to reduce the dataset size efficiently.

Result: The proposed IQA-Select method achieves better performance than fine-tuning on the entire dataset, with 102.1% and 103.7% performance on Q-Bench and AesBench benchmarks using only 10% of the full data, significantly reducing computational costs.

Conclusion: IQA-Select demonstrates that selective data tuning can outperform using full datasets in explainable IQA tasks, reducing redundancy, improving efficiency, and challenging traditional dataset scaling laws.

Abstract: In recent years, with the rapid development of powerful multimodal large
language models (MLLMs), explainable image quality assessment (IQA) has
gradually become popular, aiming at providing quality-related descriptions and
answers of images. To achieve this goal, recent methods seek to construct a
large-scale instruction tuning dataset to empower the MLLM with quality
perception ability following the well-known scaling law. However, a large
amount of instruction tuning data may cause substantial computational costs and
redundant data, which in turn will cause harm to the performance of the model.
To cope with this problem, in this paper, we challenge the scaling law and
systematically investigate the role of data quality of the instruction tuning
dataset for explainable IQA. Using a powerful pre-trained MLLM, we first
investigate the changes in model performance after fine-tuning with different
sizes of instruction tuning data. We find that selecting a subset of the data
set randomly using an appropriate ratio can even lead to better results than
training with the entire instruction tuning dataset, demonstrating the
redundancy of current explainable IQA instruction tuning data. Beyond randomly
sampling a subset, we propose a clustering-based data selection framework with
three stages: clustering feature extraction, cluster quota allocation, and
cluster sampling strategy. Then we systematically analyze the choices of each
stage and propose a simple but efficient data selection method IQA-Select for
explainable IQA. The experimental results demonstrate that IQA-Select can
achieve 102.1% and 103.7% performance of full fine-tuning using only 10%
selected data in Q-Bench and AesBench respectively, significantly reducing
computational costs while achieving better performance.

</details>


### [270] [RAP: 3D Rasterization Augmented End-to-End Planning](https://arxiv.org/abs/2510.04333)
*Lan Feng,Yang Gao,Eloi Zablocki,Quanyi Li,Wuyang Li,Sichao Liu,Matthieu Cord,Alexandre Alahi*

Main category: cs.CV

TL;DR: This paper introduces a scalable data augmentation technique called Rasterization Augmented Planning (RAP) for end-to-end driving policies, focusing on semantic fidelity rather than photorealism. It achieves state-of-the-art results on driving benchmarks.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the limitations of end-to-end driving policies trained solely on expert demonstrations, which lack recovery data and fail from compounding small mistakes. Current augmentation methods using photorealistic approaches are slow and costly.

Method: The authors propose lightweight 3D Rasterization that uses annotated primitives instead of photorealistic rendering and introduce Raster-to-Real feature alignment to bridge the sim-to-real gap effectively.

Result: RAP achieves state-of-the-art performance in closed-loop robustness and long-tail generalization across four major benchmarks, demonstrating its effectiveness and scalability for end-to-end driving training.

Conclusion: The study concludes that photorealistic rendering is unnecessary for effective training of end-to-end driving policies, as lightweight rasterization with semantic fidelity offers a practical and scalable solution.

Abstract: Imitation learning for end-to-end driving trains policies only on expert
demonstrations. Once deployed in a closed loop, such policies lack recovery
data: small mistakes cannot be corrected and quickly compound into failures. A
promising direction is to generate alternative viewpoints and trajectories
beyond the logged path. Prior work explores photorealistic digital twins via
neural rendering or game engines, but these methods are prohibitively slow and
costly, and thus mainly used for evaluation. In this work, we argue that
photorealism is unnecessary for training end-to-end planners. What matters is
semantic fidelity and scalability: driving depends on geometry and dynamics,
not textures or lighting. Motivated by this, we propose 3D Rasterization, which
replaces costly rendering with lightweight rasterization of annotated
primitives, enabling augmentations such as counterfactual recovery maneuvers
and cross-agent view synthesis. To transfer these synthetic views effectively
to real-world deployment, we introduce a Raster-to-Real feature-space alignment
that bridges the sim-to-real gap. Together, these components form Rasterization
Augmented Planning (RAP), a scalable data augmentation pipeline for planning.
RAP achieves state-of-the-art closed-loop robustness and long-tail
generalization, ranking first on four major benchmarks: NAVSIM v1/v2, Waymo
Open Dataset Vision-based E2E Driving, and Bench2Drive. Our results show that
lightweight rasterization with feature alignment suffices to scale E2E
training, offering a practical alternative to photorealistic rendering. Project
page: https://alan-lanfeng.github.io/RAP/.

</details>


### [271] [Zero-Shot Fine-Grained Image Classification Using Large Vision-Language Models](https://arxiv.org/abs/2510.03903)
*Md. Atabuzzaman,Andrew Zhang,Chris Thomas*

Main category: cs.CV

TL;DR: The paper explores the application of Large Vision-Language Models (LVLMs) to zero-shot fine-grained image classification, introducing a novel question-answering framework and attention intervention technique to achieve state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: To investigate and expand the underexplored potential of LVLMs in zero-shot fine-grained image classification, a challenging task requiring differentiation between visually similar categories.

Method: The approach reformulates fine-grained image classification as a visual question-answering problem, supported by an innovative attention intervention technique and enhanced class description benchmarks.

Result: The method consistently outperforms state-of-the-art approaches in multiple fine-grained classification benchmarks through extensive experimentation.

Conclusion: The study demonstrates that leveraging LVLMs with the proposed framework significantly improves zero-shot fine-grained image classification, showcasing their broader applicability and effectiveness.

Abstract: Large Vision-Language Models (LVLMs) have demonstrated impressive performance
on vision-language reasoning tasks. However, their potential for zero-shot
fine-grained image classification, a challenging task requiring precise
differentiation between visually similar categories, remains underexplored. We
present a novel method that transforms zero-shot fine-grained image
classification into a visual question-answering framework, leveraging LVLMs'
comprehensive understanding capabilities rather than relying on direct class
name generation. We enhance model performance through a novel attention
intervention technique. We also address a key limitation in existing datasets
by developing more comprehensive and precise class description benchmarks. We
validate the effectiveness of our method through extensive experimentation
across multiple fine-grained image classification benchmarks. Our proposed
method consistently outperforms the current state-of-the-art (SOTA) approach,
demonstrating both the effectiveness of our method and the broader potential of
LVLMs for zero-shot fine-grained classification tasks. Code and Datasets:
https://github.com/Atabuzzaman/Fine-grained-classification

</details>


### [272] [From Filters to VLMs: Benchmarking Defogging Methods through Object Detection and Segmentation Performance](https://arxiv.org/abs/2510.03906)
*Ardalan Aryashad,Parsa Razmara,Amin Mahjoub,Seyedarmin Azizi,Mahdi Salmani,Arad Firouzkouhi*

Main category: cs.CV

TL;DR: The paper benchmarks various defogging methods to understand their impact on autonomous driving tasks like object detection and segmentation under foggy conditions.


<details>
  <summary>Details</summary>
Motivation: Foggy conditions impair the perception of autonomous driving systems, necessitating effective defogging methods that improve both image quality and downstream tasks.

Method: The study evaluates classical filters, modern defogging models, chained defogging pipelines, and prompt-driven visual-language editing models on Foggy Cityscapes dataset.

Result: It identifies scenarios where defogging is beneficial, examines synergies and degradations in chained methods, and reveals correlations between VLM-based qualitative scores and task metrics like mAP.

Conclusion: The paper establishes a robust, task-oriented benchmark for defogging techniques and clarifies when preprocessing enhances autonomous driving perception in foggy weather.

Abstract: Autonomous driving perception systems are particularly vulnerable in foggy
conditions, where light scattering reduces contrast and obscures fine details
critical for safe operation. While numerous defogging methods exist-from
handcrafted filters to learned restoration models-improvements in image
fidelity do not consistently translate into better downstream detection and
segmentation. Moreover, prior evaluations often rely on synthetic data, leaving
questions about real-world transferability. We present a structured empirical
study that benchmarks a comprehensive set of pipelines, including (i) classical
filters, (ii) modern defogging networks, (iii) chained variants
(filter$\rightarrow$model, model$\rightarrow$filter), and (iv) prompt-driven
visual--language image editing models (VLM) applied directly to foggy images.
Using Foggy Cityscapes, we assess both image quality and downstream performance
on object detection (mAP) and segmentation (PQ, RQ, SQ). Our analysis reveals
when defogging helps, when chaining yields synergy or degradation, and how
VLM-based editors compare to dedicated approaches. In addition, we evaluate
qualitative rubric-based scores from a VLM judge and quantify their alignment
with task metrics, showing strong correlations with mAP. Together, these
results establish a transparent, task-oriented benchmark for defogging methods
and highlight the conditions under which preprocessing genuinely improves
autonomous perception in adverse weather.

</details>


### [273] [Generating Human Motion Videos using a Cascaded Text-to-Video Framework](https://arxiv.org/abs/2510.03909)
*Hyelin Nam,Hyojun Go,Byeongjun Park,Byung-Hoon Kim,Hyungjin Chung*

Main category: cs.CV

TL;DR: This paper introduces CAMEO, a framework for generating human motion videos from textual and visual inputs, addressing shortcomings in existing methods.


<details>
  <summary>Details</summary>
Motivation: To develop a more general-purpose method for human video generation using diffusion models, overcoming limitations in domain and training setups.

Method: CAMEO uses a cascaded framework combining Text-to-Motion models and conditional Video Diffusion Models (VDMs), with components for better text prompt preparation, visual conditions, and camera-aware modules.

Result: The framework achieves strong results on the MovieGen benchmark and a new test setup for T2M-VDM tasks, showing effective alignment, coherence, and versatility.

Conclusion: CAMEO presents a structured solution for human video generation, enhancing robustness while minimizing manual intervention, with promising results in diverse benchmarks.

Abstract: Human video generation is becoming an increasingly important task with broad
applications in graphics, entertainment, and embodied AI. Despite the rapid
progress of video diffusion models (VDMs), their use for general-purpose human
video generation remains underexplored, with most works constrained to
image-to-video setups or narrow domains like dance videos. In this work, we
propose CAMEO, a cascaded framework for general human motion video generation.
It seamlessly bridges Text-to-Motion (T2M) models and conditional VDMs,
mitigating suboptimal factors that may arise in this process across both
training and inference through carefully designed components. Specifically, we
analyze and prepare both textual prompts and visual conditions to effectively
train the VDM, ensuring robust alignment between motion descriptions,
conditioning signals, and the generated videos. Furthermore, we introduce a
camera-aware conditioning module that connects the two stages, automatically
selecting viewpoints aligned with the input text to enhance coherence and
reduce manual intervention. We demonstrate the effectiveness of our approach on
both the MovieGen benchmark and a newly introduced benchmark tailored to the
T2M-VDM combination, while highlighting its versatility across diverse use
cases.

</details>


### [274] [Talking Tennis: Language Feedback from 3D Biomechanical Action Recognition](https://arxiv.org/abs/2510.03921)
*Arushi Dashore,Aryan Anumala,Emily Hui,Olivia Yang*

Main category: cs.CV

TL;DR: The paper presents a system that combines deep learning and biomechanics for tennis stroke analysis, providing actionable feedback using AI models.


<details>
  <summary>Details</summary>
Motivation: Existing systems for tennis stroke analysis lack the ability to connect biomechanical data with meaningful, actionable feedback for players and coaches.

Method: The proposed framework uses CNN-LSTM models to extract biomechanical features from motion data and leverages LLMs to generate user-friendly feedback. The system is evaluated on the THETIS dataset.

Result: Experimental results demonstrate that the framework performs well in stroke classification and feedback generation, establishing links between biomechanics and AI interpretability.

Conclusion: The framework bridges gaps in explainable AI and biomechanics for practical tennis coaching applications.

Abstract: Automated tennis stroke analysis has advanced significantly with the
integration of biomechanical motion cues alongside deep learning techniques,
enhancing stroke classification accuracy and player performance evaluation.
Despite these advancements, existing systems often fail to connect
biomechanical insights with actionable language feedback that is both
accessible and meaningful to players and coaches. This research project
addresses this gap by developing a novel framework that extracts key
biomechanical features (such as joint angles, limb velocities, and kinetic
chain patterns) from motion data using Convolutional Neural Network Long
Short-Term Memory (CNN-LSTM)-based models. These features are analyzed for
relationships influencing stroke effectiveness and injury risk, forming the
basis for feedback generation using large language models (LLMs). Leveraging
the THETIS dataset and feature extraction techniques, our approach aims to
produce feedback that is technically accurate, biomechanically grounded, and
actionable for end-users. The experimental setup evaluates this framework on
classification performance and interpretability, bridging the gap between
explainable AI and sports biomechanics.

</details>


### [275] [Harnessing Synthetic Preference Data for Enhancing Temporal Understanding of Video-LLMs](https://arxiv.org/abs/2510.03955)
*Sameep Vani,Shreyas Jena,Maitreya Patel,Chitta Baral,Somak Aditya,Yezhou Yang*

Main category: cs.CV

TL;DR: TimeWarp enhances Video-LLMs by generating synthetic temporal datasets for better fine-grained temporal understanding.


<details>
  <summary>Details</summary>
Motivation: Video-LLMs struggle with tasks requiring detailed temporal understanding due to insufficiently complex fine-tuning datasets.

Method: TimeWarp generates a synthetic, targeted temporal dataset and a large-scale preference dataset to improve Video-LLMs' focus on video inputs.

Result: Applying TimeWarp to Video-LLMs resulted in significant performance improvements across seven temporal understanding benchmarks.

Conclusion: TimeWarp's approach effectively grounds Video-LLMs in visual and temporal dynamics, advancing their capabilities in fine-grained tasks.

Abstract: While Video Large Language Models (Video-LLMs) have demonstrated remarkable
performance across general video understanding benchmarks-particularly in video
captioning and descriptive tasks-they consistently underperform on tasks that
require fine-grained temporal understanding. This limitation arises due to the
lack of visual complexity and temporal nuance in current fine-tuning datasets,
leading these models to rely heavily on language-based reasoning rather than
truly understanding video dynamics. In this work, we propose TimeWarp, a
systematic method to create a targeted synthetic temporal dataset to fine-tune
the model's responses to encourage it to focus on the given input video. We
introduce a large-scale preference dataset, created using TimeWarp, that
captures intricate temporal dynamics often overlooked, grounding the model's
responses to visual and temporal information. We demonstrate that when our
method is applied to existing models, it significantly improves performance on
temporal understanding benchmarks, highlighting the effectiveness of our
proposed datasets in advancing temporal understanding in Video-LLMs, resulting
in an absolute improvement in performance across seven benchmarks. Code is
available at https://github.com/sameepv21/timewarp.

</details>


### [276] [No Tokens Wasted: Leveraging Long Context in Biomedical Vision-Language Models](https://arxiv.org/abs/2510.03978)
*Min Woo Sun,Alejandro Lozano,Javier Gamazo Tejero,Vishwesh Nath,Xiao Xiao Sun,James Burgess,Yuhui Zhang,Kun Yuan,Robert Tibshirani,Sean Huver,Serena Yeung-Levy*

Main category: cs.CV

TL;DR: The paper explores the advantage of using long-context biomedical captions in vision-language models (VLMs) and introduces BMC-LongCLIP, which significantly improves retrieval and classification tasks.


<details>
  <summary>Details</summary>
Motivation: Existing VLMs are pretrained with short text windows, limiting their capability to utilize long-format biomedical captions, which are common in open literature.

Method: The researchers extended the text encoder context length to up to 512 tokens and introduced the BIOMEDICA-LongCAP dataset with 1 million enriched image-caption pairs.

Result: BMC-LongCLIP increased context capacity by 6.6x, cut token waste dramatically, achieved up to 30% gains in retrieval benchmarks (Recall@1), and improved classification by 2% on average, while converging faster than short-context models.

Conclusion: Long-context modeling offers a substantial advancement for biomedical VLMs, demonstrating improved performance and efficiency.

Abstract: Embedding vision-language models (VLMs) are typically pretrained with short
text windows (<77 tokens), which forces the truncation of long-format captions.
Yet, the distribution of biomedical captions from large-scale open source
literature reveals that a huge portion of captions far exceed 77 tokens. To
this end, we investigate the impact of pretraining on long-format biomedical
captions by extending the context length of text encoders in VLMs. We find that
longer context (thus, enabling additional supervision provided in long-format
captions) correlates with better retrieval and classification performance.
Given this finding, we introduce BIOMEDICA-LongCAP, a dataset of 1M
image-caption pairs enriched with context-aware descriptions from full-text
articles, providing longer and additional textual supervision. Using
BIOMEDICA-LongCAP, we train BMC-LongCLIP, a long-context biomedical VLM with a
text encoder supporting windows of up to 512 tokens. Our model extends context
capacity by 6.6x, reducing token waste from 55% to just 2.2%. On long-caption
retrieval benchmarks, BMC-LongCLIP achieves up to +30% absolute gains in
Recall@1 and +2% average improvements in classification, while also converging
faster than short-context. Our results demonstrate that long-context modeling
is a promising direction for advancing biomedical VLMs.

</details>


### [277] [Keep It on a Leash: Controllable Pseudo-label Generation Towards Realistic Long-Tailed Semi-Supervised Learning](https://arxiv.org/abs/2510.03993)
*Yaxin Hou,Bo Han,Yuheng Jia,Hui Liu,Junhui Hou*

Main category: cs.CV

TL;DR: The paper introduces the Controllable Pseudo-label Generation (CPG) framework for long-tailed semi-supervised learning, addressing unknown distributions in unlabeled datasets and achieving notable accuracy improvements.


<details>
  <summary>Details</summary>
Motivation: Current methodologies struggle with scenarios where the distribution of unlabeled data deviates from predefined patterns, creating the need for a robust framework that aligns labeled and pseudo-labeled data.

Method: CPG uses a controllable self-reinforcing optimization cycle to filter reliable pseudo-labels, constructs Bayes-optimal classifiers using logit adjustment, and employs class-aware adaptive augmentation for enhancing minority class representation.

Result: Evaluation on benchmark datasets demonstrates up to 15.97% improvement in accuracy, outperforming state-of-the-art methods consistently.

Conclusion: CPG effectively handles arbitrary unlabeled data distributions, advances semi-supervised learning capabilities, and provides theoretical as well as empirical evidence of its efficiency.

Abstract: Current long-tailed semi-supervised learning methods assume that labeled data
exhibit a long-tailed distribution, and unlabeled data adhere to a typical
predefined distribution (i.e., long-tailed, uniform, or inverse long-tailed).
However, the distribution of the unlabeled data is generally unknown and may
follow an arbitrary distribution. To tackle this challenge, we propose a
Controllable Pseudo-label Generation (CPG) framework, expanding the labeled
dataset with the progressively identified reliable pseudo-labels from the
unlabeled dataset and training the model on the updated labeled dataset with a
known distribution, making it unaffected by the unlabeled data distribution.
Specifically, CPG operates through a controllable self-reinforcing optimization
cycle: (i) at each training step, our dynamic controllable filtering mechanism
selectively incorporates reliable pseudo-labels from the unlabeled dataset into
the labeled dataset, ensuring that the updated labeled dataset follows a known
distribution; (ii) we then construct a Bayes-optimal classifier using logit
adjustment based on the updated labeled data distribution; (iii) this improved
classifier subsequently helps identify more reliable pseudo-labels in the next
training step. We further theoretically prove that this optimization cycle can
significantly reduce the generalization error under some conditions.
Additionally, we propose a class-aware adaptive augmentation module to further
improve the representation of minority classes, and an auxiliary branch to
maximize data utilization by leveraging all labeled and unlabeled samples.
Comprehensive evaluations on various commonly used benchmark datasets show that
CPG achieves consistent improvements, surpassing state-of-the-art methods by up
to \textbf{15.97\%} in accuracy. The code is available at
https://github.com/yaxinhou/CPG.

</details>


### [278] [Enhancing OCR for Sino-Vietnamese Language Processing via Fine-tuned PaddleOCRv5](https://arxiv.org/abs/2510.04003)
*Minh Hoang Nguyen,Su Nguyen Thiet*

Main category: cs.CV

TL;DR: This paper improves OCR for Classical Chinese (Han-Nom) texts using fine-tuning techniques on PaddleOCRv5, achieving better accuracy in noisy conditions.


<details>
  <summary>Details</summary>
Motivation: Existing OCR systems struggle with recognizing degraded scans, non-standard glyphs, and handwriting variations in ancient Vietnamese Chinese texts.

Method: A fine-tuning approach for PaddleOCRv5 is proposed, with a retrained text recognition module using a curated dataset and supporting a full training pipeline including preprocessing, LMDB conversion, evaluation, and visualization.

Result: Experimental results show that the fine-tuned model improves exact accuracy from 37.5% to 50.0%, especially in noisy image conditions.

Conclusion: This approach enhances OCR performance for Han-Nom texts, supports linguistic and semantic applications, and includes a demo for comparison of pre- and post-fine-tuning results.

Abstract: Recognizing and processing Classical Chinese (Han-Nom) texts play a vital
role in digitizing Vietnamese historical documents and enabling cross-lingual
semantic research. However, existing OCR systems struggle with degraded scans,
non-standard glyphs, and handwriting variations common in ancient sources. In
this work, we propose a fine-tuning approach for PaddleOCRv5 to improve
character recognition on Han-Nom texts. We retrain the text recognition module
using a curated subset of ancient Vietnamese Chinese manuscripts, supported by
a full training pipeline covering preprocessing, LMDB conversion, evaluation,
and visualization. Experimental results show a significant improvement over the
base model, with exact accuracy increasing from 37.5 percent to 50.0 percent,
particularly under noisy image conditions. Furthermore, we develop an
interactive demo that visually compares pre- and post-fine-tuning recognition
results, facilitating downstream applications such as Han-Vietnamese semantic
alignment, machine translation, and historical linguistics research. The demo
is available at https://huggingface.co/spaces/MinhDS/Fine-tuned-PaddleOCRv5.

</details>


### [279] [Fit Pixels, Get Labels: Meta-learned Implicit Networks for Image Segmentation](https://arxiv.org/abs/2510.04021)
*Kushal Vyas,Ashok Veeraraghavan,Guha Balakrishnan*

Main category: cs.CV

TL;DR: The paper introduces MetaSeg, a meta-learning framework for medical image segmentation, achieving performance comparable to U-Net with fewer parameters.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of implicit neural representations (INRs) in predictive tasks like segmentation and to propose a scalable alternative to resource-heavy architectures such as U-Nets and vision transformers.

Method: MetaSeg combines an INR to predict pixel intensity and class labels and employs a meta-learning procedure for optimal initialization, enabling fine-tuning for unseen data.

Result: MetaSeg achieved Dice scores comparable to U-Net models while using 90% fewer parameters on 2D and 3D brain MRI segmentation tasks.

Conclusion: MetaSeg demonstrates the potential of meta-learning and INRs for efficient, accurate, and scalable medical image segmentation.

Abstract: Implicit neural representations (INRs) have achieved remarkable successes in
learning expressive yet compact signal representations. However, they are not
naturally amenable to predictive tasks such as segmentation, where they must
learn semantic structures over a distribution of signals. In this study, we
introduce MetaSeg, a meta-learning framework to train INRs for medical image
segmentation. MetaSeg uses an underlying INR that simultaneously predicts per
pixel intensity values and class labels. It then uses a meta-learning procedure
to find optimal initial parameters for this INR over a training dataset of
images and segmentation maps, such that the INR can simply be fine-tuned to fit
pixels of an unseen test image, and automatically decode its class labels. We
evaluated MetaSeg on 2D and 3D brain MRI segmentation tasks and report Dice
scores comparable to commonly used U-Net models, but with $90\%$ fewer
parameters. MetaSeg offers a fresh, scalable alternative to traditional
resource-heavy architectures such as U-Nets and vision transformers for medical
image segmentation. Our project is available at
https://kushalvyas.github.io/metaseg.html .

</details>


### [280] [Video-in-the-Loop: Span-Grounded Long Video QA with Interleaved Reasoning](https://arxiv.org/abs/2510.04022)
*Chendong Wang,Donglin Bai,Yifan Yang,Xiao Jin,Anlan Zhang,Rui Wang,Shiqi Jiang,Yuqing Yang,Hao Wu,Qi Dai,Chong Luo,Ting Cao,Lili Qiu,Suman Banerjee*

Main category: cs.CV

TL;DR: This paper introduces Video-in-the-Loop (ViTL), a long-video QA framework that localizes relevant information and reallocates visual tokens efficiently under fixed compute budgets. The paper also presents a new dataset \\dataname{} tailored for this task.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve efficiency and interpretability in long-video question answering (QA) while addressing computational constraints, by integrating localization and span-aware token reallocation.

Method: The method includes a two-stage approach: (1) Localizing relevant video intervals with low-fps skimming, and (2) Reallocation of visual tokens for detailed analysis using span-awareness and high frame rates. The model is trained end-to-end with a temporal IoU and correctness based objective.

Result: The proposed method achieves up to 8.6% improvement in long-video QA tasks with 50% less frame input and demonstrates clear advantages over uniform sampling techniques across datasets like Charades-STA and ActivityNet-Captions.

Conclusion: ViTL and \\dataname{} enable scalable, interpretable, and compute-efficient QA on long videos, showcasing the value of span-aware token management and localized reasoning.

Abstract: We present \emph{Video-in-the-Loop} (ViTL), a two-stage long-video QA
framework that preserves a fixed token budget by first \emph{localizing}
question-relevant interval(s) with a low-fps skim and then \emph{answering} via
span-aware reallocation of visual tokens at higher effective frame rate,
emitting an interleaved output with both spans and the final option for direct
attribution. We also introduce \dataname{}, which converts description based
event graphs into \emph{span-grounded} multiple-choice QA by pairing each
question with \emph{ground-truth} time span(s) and related reasoning. ViTL is
trained end-to-end with an interleaved group-relative objective that couples
temporal IoU for localization with answer correctness, allowing credit to flow
from answers back to spans without increasing compute. Under fixed token
budgets, ViTL attains up to 8.6% with 50% less frame input on long-video QA and
temporal grounding (e.g., Charades-STA, ActivityNet-Captions) and ablations
show that span-aware token reallocation consistently surpasses uniform
sampling. Together, \dataname{} and ViTL provide an interpretable,
compute-efficient recipe for scalable long-video QA.

</details>


### [281] [Enhancing Fake News Video Detection via LLM-Driven Creative Process Simulation](https://arxiv.org/abs/2510.04024)
*Yuyan Bu,Qiang Sheng,Juan Cao,Shaofei Wang,Peng Qi,Yuhui Shi,Beizhe Hu*

Main category: cs.CV

TL;DR: The paper addresses the challenge of detecting fake news videos on short video platforms and proposes a data augmentation framework, AgentAug, to enhance detection performance.


<details>
  <summary>Details</summary>
Motivation: The prevalence of fake news on short video platforms and the limitations of current detectors, which struggle with biased patterns due to constrained training data and complex video-event relationships, drive the need for better detection methods.

Method: The proposed AgentAug framework utilizes LLM-driven pipelines for simulating typical fake news creation processes across four fabrication categories and employs an active learning strategy using uncertainty sampling to select useful augmented samples.

Result: Experimental results show that AgentAug improves the accuracy of detecting fake news videos across two benchmark datasets.

Conclusion: The study concludes that AgentAug successfully enhances the capabilities of detectors by simulating diverse fabrication scenarios and addressing limitations in training data.

Abstract: The emergence of fake news on short video platforms has become a new
significant societal concern, necessitating automatic video-news-specific
detection. Current detectors primarily rely on pattern-based features to
separate fake news videos from real ones. However, limited and less diversified
training data lead to biased patterns and hinder their performance. This
weakness stems from the complex many-to-many relationships between video
material segments and fabricated news events in real-world scenarios: a single
video clip can be utilized in multiple ways to create different fake
narratives, while a single fabricated event often combines multiple distinct
video segments. However, existing datasets do not adequately reflect such
relationships due to the difficulty of collecting and annotating large-scale
real-world data, resulting in sparse coverage and non-comprehensive learning of
the characteristics of potential fake news video creation. To address this
issue, we propose a data augmentation framework, AgentAug, that generates
diverse fake news videos by simulating typical creative processes. AgentAug
implements multiple LLM-driven pipelines of four fabrication categories for
news video creation, combined with an active learning strategy based on
uncertainty sampling to select the potentially useful augmented samples during
training. Experimental results on two benchmark datasets demonstrate that
AgentAug consistently improves the performance of short video fake news
detectors.

</details>


### [282] [Prompt-to-Prompt: Text-Based Image Editing Via Cross-Attention Mechanisms -- The Research of Hyperparameters and Novel Mechanisms to Enhance Existing Frameworks](https://arxiv.org/abs/2510.04034)
*Linn Bieske,Carla Lorente*

Main category: cs.CV

TL;DR: This paper proposes techniques to enhance text-driven image editing using stable diffusion models by refining prompt-to-prompt frameworks and optimizing attention mechanisms.


<details>
  <summary>Details</summary>
Motivation: To address limitations in text-driven image editing, such as inconsistencies (e.g., hair color changes) caused by current deep learning methods, improving reliability and precision.

Method: The study evaluates hyperparameter interactions, introduces an 'attention re-weight method,' studies the 'word swap' technique, and develops a new 'CL P2P' framework to improve image editing performance.

Result: Findings demonstrate the significance of optimizing hyperparameter settings and architectural choices, leading to advancements in consistency and quality of images produced by stable diffusion models.

Conclusion: Enhancing attention mechanisms and prompt-to-prompt frameworks improves the precision, adaptability, and reliability of text-driven image editing workflows.

Abstract: Recent advances in image editing have shifted from manual pixel manipulation
to employing deep learning methods like stable diffusion models, which now
leverage cross-attention mechanisms for text-driven control. This transition
has simplified the editing process but also introduced variability in results,
such as inconsistent hair color changes. Our research aims to enhance the
precision and reliability of prompt-to-prompt image editing frameworks by
exploring and optimizing hyperparameters. We present a comprehensive study of
the "word swap" method, develop an "attention re-weight method" for better
adaptability, and propose the "CL P2P" framework to address existing
limitations like cycle inconsistency. This work contributes to understanding
and improving the interaction between hyperparameter settings and the
architectural choices of neural network models, specifically their attention
mechanisms, which significantly influence the composition and quality of the
generated images.

</details>


### [283] [\textsc{GUI-Spotlight}: Adaptive Iterative Focus Refinement for Enhanced GUI Visual Grounding](https://arxiv.org/abs/2510.04039)
*Bin Lei,Nuo Xu,Ali Payani,Mingyi Hong,Chunhua Liao,Yu Cao,Caiwen Ding*

Main category: cs.CV

TL;DR: The paper proposes GUI-Spotlight, a model designed to improve visual grounding in GUI environments by dynamically focusing on relevant screen areas. It outperforms existing methods using significantly fewer training samples.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitation of visual grounding in GUI systems, which affects their ability to perform pointer-level actions like clicking and dragging accurately.

Method: The authors introduced GUI-Spotlight, a model trained for image-grounded reasoning that employs a dynamic approach to iteratively focus on relevant screen regions using specialized tools.

Result: GUI-Spotlight achieved 52.8% accuracy on the ScreenSpot-Pro benchmark, outperforming models like V2P-7B (50.6%) and GTA-1-7B (50.1%) while using far fewer training samples.

Conclusion: GUI-Spotlight demonstrates enhanced accuracy in visual grounding and provides a more efficient approach to training multimodal GUI systems, showcasing its capability to handle complex UI environments effectively.

Abstract: Multimodal large language models (MLLMs) have markedly expanded the
competence of graphical user-interface (GUI) systems, propelling them beyond
controlled simulations into complex, real-world environments across diverse
platforms. However, practical usefulness is still bounded by the reliability of
visual grounding, i.e., mapping textual references to exact on-screen elements.
This limitation prevents the system from accurately performing pointer-level
actions such as clicking or dragging. To address it, we introduce GUI-Spotlight
-- a model trained for image-grounded reasoning that dynamically invokes
multiple specialized tools to iteratively narrow its focus to the relevant
region of the screen, thereby substantially improving visual grounding
accuracy. On the ScreenSpot-Pro benchmark, GUI-Spotlight trained with only
18.5K training samples achieves 52.8\% accuracy, surpassing V2P-7B (50.6\% with
9.6M training samples) and GTA-1-7B (50.1\% with 1.56M training samples).

</details>


### [284] [Quantization Range Estimation for Convolutional Neural Networks](https://arxiv.org/abs/2510.04044)
*Bingtao Yang,Yujia Wang,Mengzhi Jiao,Hongwei Huo*

Main category: cs.CV

TL;DR: The paper introduces a novel range estimation method for post-training quantization, achieving improved neural network model accuracy even in low-bit settings.


<details>
  <summary>Details</summary>
Motivation: Low-bit quantization is essential for reducing storage demands of deep neural networks but often results in accuracy loss. This paper seeks to improve performance without compromising accuracy.

Method: The authors frame range estimation as a convex optimization problem to minimize quantization errors. They introduce a search algorithm to find optimal solutions and apply it to transformed weight spaces for further performance enhancement.

Result: Their method surpasses state-of-the-art results, demonstrating negligible accuracy loss in 8-bit and 6-bit quantization and significant improvements for 4-bit quantization on ResNet and Inception models.

Conclusion: The proposed optimization-based range estimation substantially improves post-training quantization performance with minimal accuracy trade-offs, enabling efficient storage of deep learning models.

Abstract: Post-training quantization for reducing the storage of deep neural network
models has been demonstrated to be an effective way in various tasks. However,
low-bit quantization while maintaining model accuracy is a challenging problem.
In this paper, we present a range estimation method to improve the quantization
performance for post-training quantization. We model the range estimation into
an optimization problem of minimizing quantization errors by layer-wise local
minima. We prove this problem is locally convex and present an efficient search
algorithm to find the optimal solution. We propose the application of the above
search algorithm to the transformed weights space to do further improvement in
practice. Our experiments demonstrate that our method outperforms
state-of-the-art performance generally on top-1 accuracy for image
classification tasks on the ResNet series models and Inception-v3 model. The
experimental results show that the proposed method has almost no loss of top-1
accuracy in 8-bit and 6-bit settings for image classifications, and the
accuracy of 4-bit quantization is also significantly improved. The code is
available at https://github.com/codeiscommitting/REQuant.

</details>


### [285] [MetaFind: Scene-Aware 3D Asset Retrieval for Coherent Metaverse Scene Generation](https://arxiv.org/abs/2510.04057)
*Zhenyu Pan,Yucheng Lu,Han Liu*

Main category: cs.CV

TL;DR: MetaFind is a retrieval framework that enhances 3D asset selection for metaverse scenes by using a tri-modal (text, image, 3D) compositional approach and addressing spatial and stylistic challenges.


<details>
  <summary>Details</summary>
Motivation: To tackle inconsistent 3D asset retrieval in metaverse scene generation and establish a standard retrieval paradigm tailored specifically for 3D assets.

Method: A tri-modal framework leveraging text, image, and 3D modalities paired with a novel equivariant layout encoder (ESSGNN) to model both object-level features and scene-level spatial relationships, ensuring coherence with scene constraints.

Result: MetaFind demonstrates improved retrieval performance in spatial and stylistic consistency when compared to baseline methods.

Conclusion: MetaFind offers a flexible and adaptive framework for 3D asset retrieval that enhances scene design by aligning assets with spatial, semantic, and stylistic constraints.

Abstract: We present MetaFind, a scene-aware tri-modal compositional retrieval
framework designed to enhance scene generation in the metaverse by retrieving
3D assets from large-scale repositories. MetaFind addresses two core
challenges: (i) inconsistent asset retrieval that overlooks spatial, semantic,
and stylistic constraints, and (ii) the absence of a standardized retrieval
paradigm specifically tailored for 3D asset retrieval, as existing approaches
mainly rely on general-purpose 3D shape representation models. Our key
innovation is a flexible retrieval mechanism that supports arbitrary
combinations of text, image, and 3D modalities as queries, enhancing spatial
reasoning and style consistency by jointly modeling object-level features
(including appearance) and scene-level layout structures. Methodologically,
MetaFind introduces a plug-and-play equivariant layout encoder ESSGNN that
captures spatial relationships and object appearance features, ensuring
retrieved 3D assets are contextually and stylistically coherent with the
existing scene, regardless of coordinate frame transformations. The framework
supports iterative scene construction by continuously adapting retrieval
results to current scene updates. Empirical evaluations demonstrate the
improved spatial and stylistic consistency of MetaFind in various retrieval
tasks compared to baseline methods.

</details>


### [286] [Ordinal Encoding as a Regularizer in Binary Loss for Solar Flare Prediction](https://arxiv.org/abs/2510.04063)
*Chetraj Pandey,Jinsu Hong,Anli Ji,Rafal A. Angryk,Berkay Aydin*

Main category: cs.CV

TL;DR: This paper proposes incorporating ordinal relationships into solar flare prediction models to improve accuracy near classification thresholds.


<details>
  <summary>Details</summary>
Motivation: Solar flare prediction models often misclassify events near binary thresholds due to ignoring inherent ordinal relationships.

Method: A novel loss function integrates ordinal weighting into binary cross-entropy to penalize misclassifications near thresholds more heavily.

Result: The approach improves model optimization by leveraging ordinal data characteristics, leading to better performance.

Conclusion: Integrating ordinal relationships enhances solar flare prediction models' ability to differentiate similar-intensity events, reducing errors near thresholds.

Abstract: The prediction of solar flares is typically formulated as a binary
classification task, distinguishing events as either Flare (FL) or No-Flare
(NF) according to a specified threshold (for example, greater than or equal to
C-class, M-class, or X-class). However, this binary framework neglects the
inherent ordinal relationships among the sub-classes contained within each
category (FL and NF). Several studies on solar flare prediction have
empirically shown that the most frequent misclassifications occur near this
prediction threshold. This suggests that the models struggle to differentiate
events that are similar in intensity but fall on opposite sides of the binary
threshold. To mitigate this limitation, we propose a modified loss function
that integrates the ordinal information among the sub-classes of the binarized
flare labels into the conventional binary cross-entropy (BCE) loss. This
approach serves as an ordinality-aware, data-driven regularization method that
penalizes the incorrect predictions of flare events in close proximity to the
prediction threshold more heavily than those away from the boundary during
model optimization. By incorporating ordinal weighting into the loss function,
we aim to enhance the model's learning process by leveraging the ordinal
characteristics of the data, thereby improving its overall performance.

</details>


### [287] [QuantDemoire: Quantization with Outlier Aware for Image Demoiréing](https://arxiv.org/abs/2510.04066)
*Zheng Chen,Kewei Zhang,Xiaoyang Liu,Weihang Zhang,Mengfan Wang,Yifan Fu,Yulun Zhang*

Main category: cs.CV

TL;DR: This paper proposes QuantDemoire, a post-training quantization framework designed to effectively address the challenges of applying quantization to demoiréing models while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning methods for demoiréing require high computational resources, making them unsuitable for edge devices. Applying direct quantization methods leads to performance degradation due to distribution outliers and weakened representations.

Method: The proposed method includes: (1) an outlier-aware quantizer that reduces errors by handling distribution outliers and preserving extreme weights in FP16. (2) A frequency-aware calibration strategy that emphasizes low- and mid-frequency components during fine-tuning to mitigate banding artifacts.

Result: QuantDemoire significantly reduces model parameters and computational cost while maintaining high-quality outputs, outperforming existing quantization methods by over 4 dB on W4A4.

Conclusion: QuantDemoire offers an effective quantization framework for demoiréing, suitable for deployment on resource-constrained edge devices. It balances efficiency with performance, addressing the typical challenges of applying quantization to such models.

Abstract: Demoir\'eing aims to remove moir\'e artifacts that often occur in images.
While recent deep learning-based methods have achieved promising results, they
typically require substantial computational resources, limiting their
deployment on edge devices. Model quantization offers a compelling solution.
However, directly applying existing quantization methods to demoir\'eing models
introduces severe performance degradation. The main reasons are distribution
outliers and weakened representations in smooth regions. To address these
issues, we propose QuantDemoire, a post-training quantization framework
tailored to demoir\'eing. It contains two key components. **First}, we
introduce an outlier-aware quantizer to reduce errors from outliers. It uses
sampling-based range estimation to reduce activation outliers, and keeps a few
extreme weights in FP16 with negligible cost. **Second**, we design a
frequency-aware calibration strategy. It emphasizes low- and mid-frequency
components during fine-tuning, which mitigates banding artifacts caused by
low-bit quantization. Extensive experiments validate that our QuantDemoire
achieves large reductions in parameters and computation while maintaining
quality. Meanwhile, it outperforms existing quantization methods by over **4
dB** on W4A4. Code is released at:
https://github.com/zhengchen1999/QuantDemoire.

</details>


### [288] [Diffusion Low Rank Hybrid Reconstruction for Sparse View Medical Imaging](https://arxiv.org/abs/2510.04069)
*Zongyin Deng,Qing Zhou,Yuhao Fang,Zijian Wang,Yao Lu,Ye Zhang,Chun Li*

Main category: cs.CV

TL;DR: TV-LoRA combines a diffusion generative prior with multi-regularization constraints for sparse-view CT reconstruction, excelling in speed and quality.


<details>
  <summary>Details</summary>
Motivation: The study aims to address challenges of ill-posedness and texture loss in extremely sparse-view CT scenarios.

Method: The method integrates a diffusion generative prior (NCSN++) with anisotropic TV and nuclear norm constraints, within an ADMM framework leveraging FFT acceleration and tensor-parallel optimization.

Result: Experiments on multiple datasets (AAPM-2016, CTHD, LIDC) show TV-LoRA surpasses benchmarks in SSIM, texture recovery, edge clarity, and artifact suppression, with robust generalizability.

Conclusion: TV-LoRA demonstrates efficient and high-quality 3D CT reconstruction, making it suitable for low-dose, sparse-sampling medical imaging applications.

Abstract: This work presents TV-LoRA, a novel method for low-dose sparse-view CT
reconstruction that combines a diffusion generative prior (NCSN++ with SDE
modeling) and multi-regularization constraints, including anisotropic TV and
nuclear norm (LoRA), within an ADMM framework. To address ill-posedness and
texture loss under extremely sparse views, TV-LoRA integrates generative and
physical constraints, and utilizes a 2D slice-based strategy with FFT
acceleration and tensor-parallel optimization for efficient inference.
Experiments on AAPM-2016, CTHD, and LIDC datasets with
$N_{\mathrm{view}}=8,4,2$ show that TV-LoRA consistently surpasses benchmarks
in SSIM, texture recovery, edge clarity, and artifact suppression,
demonstrating strong robustness and generalizability. Ablation studies confirm
the complementary effects of LoRA regularization and diffusion priors, while
the FFT-PCG module provides a speedup. Overall, Diffusion + TV-LoRA achieves
high-fidelity, efficient 3D CT reconstruction and broad clinical applicability
in low-dose, sparse-sampling scenarios.

</details>


### [289] [TOPO-Bench: An Open-Source Topological Mapping Evaluation Framework with Quantifiable Perceptual Aliasing](https://arxiv.org/abs/2510.04100)
*Jiaming Wang,Diwen Liu,Jizhuo Chen,Harold Soh*

Main category: cs.CV

TL;DR: The paper addresses the challenges of diverse evaluation standards and perceptual aliasing in topological mapping, proposing standardized metrics, datasets, and a quantitative measure of dataset ambiguity, while open-sourcing tools for reproducibility.


<details>
  <summary>Details</summary>
Motivation: Topological mapping research lacks standardized evaluation metrics, datasets, and protocols, which hinders fair and reproducible comparisons. Perceptual aliasing, a key issue, is under-quantified despite its significant impact on system performance.

Method: The authors develop a formal metric for topological consistency using localization accuracy as a surrogate, propose a measure for dataset ambiguity, curate a benchmark dataset, and implement deep learning and classical baseline systems while open-sourcing all resources.

Result: The experiments demonstrated limitations of current approaches in handling perceptual aliasing and enabled a fair comparison across diverse environments, providing insights into the field's challenges.

Conclusion: The study offers standardized tools and metrics to improve reproducibility and fair comparisons in topological mapping research, fostering advancements in addressing perceptual aliasing.

Abstract: Topological mapping offers a compact and robust representation for
navigation, but progress in the field is hindered by the lack of standardized
evaluation metrics, datasets, and protocols. Existing systems are assessed
using different environments and criteria, preventing fair and reproducible
comparisons. Moreover, a key challenge - perceptual aliasing - remains
under-quantified, despite its strong influence on system performance. We
address these gaps by (1) formalizing topological consistency as the
fundamental property of topological maps and showing that localization accuracy
provides an efficient and interpretable surrogate metric, and (2) proposing the
first quantitative measure of dataset ambiguity to enable fair comparisons
across environments. To support this protocol, we curate a diverse benchmark
dataset with calibrated ambiguity levels, implement and release deep-learned
baseline systems, and evaluate them alongside classical methods. Our
experiments and analysis yield new insights into the limitations of current
approaches under perceptual aliasing. All datasets, baselines, and evaluation
tools are fully open-sourced to foster consistent and reproducible research in
topological mapping.

</details>


### [290] [Learning Efficient Meshflow and Optical Flow from Event Cameras](https://arxiv.org/abs/2510.04111)
*Xinglong Luo,Ao Luo,Kunming Luo,Zhengning Wang,Ping Tan,Bing Zeng,Shuaicheng Liu*

Main category: cs.CV

TL;DR: This paper tackles the problem of event-based meshflow estimation, introducing a new dataset and a lightweight network that outperforms current methods in speed and accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the lack of event-specific datasets and methods for meshflow estimation and to overcome the challenges posed by event data density.

Method: The authors create the High-Resolution Event Meshflow (HREM) dataset, propose the EEMFlow network with an encoder-decoder structure, and develop a Confidence-Induced Detail Completion (CDC) module for dense optical flow. They also introduce an Adaptive Density Module (ADM) in an expanded HREM+ dataset for variable-density robustness.

Result: The EEMFlow model demonstrates 30x faster runtime than state-of-the-art methods, and the ADM module boosts its performance by up to 10%.

Conclusion: The proposed dataset, network, and modules significantly advance the field of event-based meshflow estimation, improving speed, accuracy, and robustness across varied data densities.

Abstract: In this paper, we explore the problem of event-based meshflow estimation, a
novel task that involves predicting a spatially smooth sparse motion field from
event cameras. To start, we review the state-of-the-art in event-based flow
estimation, highlighting two key areas for further research: i) the lack of
meshflow-specific event datasets and methods, and ii) the underexplored
challenge of event data density. First, we generate a large-scale
High-Resolution Event Meshflow (HREM) dataset, which showcases its superiority
by encompassing the merits of high resolution at 1280x720, handling dynamic
objects and complex motion patterns, and offering both optical flow and
meshflow labels. These aspects have not been fully explored in previous works.
Besides, we propose Efficient Event-based MeshFlow (EEMFlow) network, a
lightweight model featuring a specially crafted encoder-decoder architecture to
facilitate swift and accurate meshflow estimation. Furthermore, we upgrade
EEMFlow network to support dense event optical flow, in which a
Confidence-induced Detail Completion (CDC) module is proposed to preserve sharp
motion boundaries. We conduct comprehensive experiments to show the exceptional
performance and runtime efficiency (30x faster) of our EEMFlow model compared
to the recent state-of-the-art flow method. As an extension, we expand HREM
into HREM+, a multi-density event dataset contributing to a thorough study of
the robustness of existing methods across data with varying densities, and
propose an Adaptive Density Module (ADM) to adjust the density of input event
data to a more optimal range, enhancing the model's generalization ability. We
empirically demonstrate that ADM helps to significantly improve the performance
of EEMFlow and EEMFlow+ by 8% and 10%, respectively. Code and dataset are
released at https://github.com/boomluo02/EEMFlowPlus.

</details>


### [291] [Joint Learning of Pose Regression and Denoising Diffusion with Score Scaling Sampling for Category-level 6D Pose Estimation](https://arxiv.org/abs/2510.04125)
*Seunghyun Lee,Tae-Kyun Kim*

Main category: cs.CV

TL;DR: State-of-the-art method for category-level 6D object pose estimation using diffusion models, improving training speed, accuracy, and inference efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion models face challenges such as slow training convergence, reliance on additional evaluation networks, and difficulties in optimizing pose distribution modeling.

Method: The paper introduces two key innovations: a pretrained encoder combined with joint network learning, and sampling guidance via time-dependent score scaling to balance exploration and exploitation.

Result: Experiments on benchmarks like REAL275, HouseCat6D, and ROPE show the proposed method achieves higher accuracy and efficiency, eliminating the need for extra evaluation networks.

Conclusion: The method improves upon existing approaches by enhancing training efficiency, inference accuracy, and eliminating additional complexity, thus making it effective and streamlined.

Abstract: Latest diffusion models have shown promising results in category-level 6D
object pose estimation by modeling the conditional pose distribution with depth
image input. The existing methods, however, suffer from slow convergence during
training, learning its encoder with the diffusion denoising network in
end-to-end fashion, and require an additional network that evaluates sampled
pose hypotheses to filter out low-quality pose candidates. In this paper, we
propose a novel pipeline that tackles these limitations by two key components.
First, the proposed method pretrains the encoder with the direct pose
regression head, and jointly learns the networks via the regression head and
the denoising diffusion head, significantly accelerating training convergence
while achieving higher accuracy. Second, sampling guidance via time-dependent
score scaling is proposed s.t. the exploration-exploitation trade-off is
effectively taken, eliminating the need for the additional evaluation network.
The sampling guidance maintains multi-modal characteristics of symmetric
objects at early denoising steps while ensuring high-quality pose generation at
final steps. Extensive experiments on multiple benchmarks including REAL275,
HouseCat6D, and ROPE, demonstrate that the proposed method, simple yet
effective, achieves state-of-the-art accuracies even with single-pose
inference, while being more efficient in both training and inference.

</details>


### [292] [Learning from All: Concept Alignment for Autonomous Distillation from Multiple Drifting MLLMs](https://arxiv.org/abs/2510.04142)
*Xiaoyu Yang,Jie Lu,En Yu*

Main category: cs.CV

TL;DR: This paper addresses concept drift in reasoning trajectories from multimodal large language models (MLLMs) during knowledge distillation and proposes a new framework (autonomous preference optimization) to improve student model performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to solve the issue of unpredictable concept drift and bias propagation from MLLM teachers during knowledge distillation, which affects the student model’s performance.

Method: The paper introduces a theoretical link between concept drift and knowledge distillation, leveraging an autonomous preference optimization (APO) framework with a 'learn, compare, critique' paradigm to align reasoning trajectories.

Result: Experiments show enhanced consistency, robustness, and generalization in the student models using the proposed distillation method. Additionally, the paper contributes the CXR-MAX dataset with 170,982 distilled reasoning trajectories.

Conclusion: The proposed APO framework effectively addresses concept drift, producing robust and generalizable student models while offering valuable data contributions for future research.

Abstract: This paper identifies a critical yet underexplored challenge in distilling
from multimodal large language models (MLLMs): the reasoning trajectories
generated by multiple drifting teachers exhibit concept drift, whereby their
reasoning distributions evolve unpredictably and transmit biases to the student
model, ultimately compromising its performance. To tackle this issue, we
pioneer a theoretical connection between concept drift and knowledge
distillation, casting the non-stationary reasoning dynamics from multiple MLLM
teachers as next-token prediction of multi-stream reasoning trajectories.Guided
by concept drift, we introduce the "learn, compare, critique" paradigm,
culminating in autonomous preference optimization (APO). Under the active
guidance of the teachers, the student model first learns and self-distils
preferred thinking by comparing multiple teachers. It then engages in critical
reflection over the drifting inference from teachers, performing concept
alignment through APO, ultimately yielding a robust, consistent, and
generalizable model.Extensive experiments demonstrate our superior performance
of consistency, robustness and generalization within knowledge distillation.
Besides, we also contributed a large-scale dataset, CXR-MAX (Multi-teachers
Alignment X-rays), comprising 170,982 distilled reasoning trajectories derived
from publicly accessible MLLMs based on MIMIC-CXR. Our code and data are public
at: https://anonymous.4open.science/r/Autonomous-Distillation/.

</details>


### [293] [A Modular Conditional Diffusion Framework for Image Reconstruction](https://arxiv.org/abs/2411.05993)
*Magauiya Zhussip,Iaroslav Koshelev,Stamatis Lefkimmiatis*

Main category: cs.CV

TL;DR: This paper proposes a modular framework (DP-IR) combining pre-trained IR networks and Diffusion Probabilistic Models (DPMs) to address challenges in blind image restoration. The framework is more practical and computationally efficient while maintaining high perceptual quality.


<details>
  <summary>Details</summary>
Motivation: The motivation is to make DPMs practical and widely usable for various image restoration tasks without requiring extensive computational resources or retraining for task-specific applications.

Method: The method involves introducing a modular framework (DP-IR), which integrates existing pre-trained IR networks with generative DPMs by training a small task-specific module (0.7M parameters). Moreover, it incorporates a sampling strategy that reduces computation while preserving performance.

Result: The proposed framework achieved superior perceptual image quality across benchmarks in tasks like burst joint denoising and super-resolution (JDD-SR), dynamic scene deblurring, and super-resolution. It also showed competitive results on fidelity metrics with reduced computational demands.

Conclusion: The DP-IR framework successfully balances computational efficiency and task adaptability while enhancing perceptual image quality in blind IR tasks, addressing limitations of existing DPM solutions.

Abstract: Diffusion Probabilistic Models (DPMs) have been recently utilized to deal
with various blind image restoration (IR) tasks, where they have demonstrated
outstanding performance in terms of perceptual quality. However, the
task-specific nature of existing solutions and the excessive computational
costs related to their training, make such models impractical and challenging
to use for different IR tasks than those that were initially trained for. This
hinders their wider adoption, especially by those who lack access to powerful
computational resources and vast amount of training data. In this work we aim
to address the above issues and enable the successful adoption of DPMs in
practical IR-related applications. Towards this goal, we propose a modular
diffusion probabilistic IR framework (DP-IR), which allows us to combine the
performance benefits of existing pre-trained state-of-the-art IR networks and
generative DPMs, while it requires only the additional training of a relatively
small module (0.7M params) related to the particular IR task of interest.
Moreover, the architecture of the proposed framework allows for a sampling
strategy that leads to at least four times reduction of neural function
evaluations without suffering any performance loss, while it can also be
combined with existing acceleration techniques such as DDIM. We evaluate our
model on four benchmarks for the tasks of burst JDD-SR, dynamic scene
deblurring, and super-resolution. Our method outperforms existing approaches in
terms of perceptual quality while it retains a competitive performance with
respect to fidelity metrics.

</details>


### [294] [Automating construction safety inspections using a multi-modal vision-language RAG framework](https://arxiv.org/abs/2510.04145)
*Chenxin Wang,Elyas Asadi Shamsabadi,Zhaohui Chen,Luming Shen,Alireza Ahmadian Fard Fini,Daniel Dias-da-Costa*

Main category: cs.CV

TL;DR: This paper proposes the SiteShield framework utilizing multi-modal LVLMs and RAG techniques to automate construction safety inspection reports, outperforming unimodal LLMs.


<details>
  <summary>Details</summary>
Motivation: To overcome inefficiencies in conventional construction safety inspection methods, including irrelevant responses and limited adaptability in current LVLM applications.

Method: Developed SiteShield, a multi-modal LVLM-based Retrieval-Augmented Generation (RAG) framework integrating visual and audio data for automating safety inspections.

Result: SiteShield showed superior performance with an F1 score of 0.82, hamming loss of 0.04, precision of 0.76, and recall of 0.96 using real-world data.

Conclusion: SiteShield enhances information retrieval and efficiency in safety report generation, presenting a novel approach for automated construction safety inspections.

Abstract: Conventional construction safety inspection methods are often inefficient as
they require navigating through large volume of information. Recent advances in
large vision-language models (LVLMs) provide opportunities to automate safety
inspections through enhanced visual and linguistic understanding. However,
existing applications face limitations including irrelevant or unspecific
responses, restricted modal inputs and hallucinations. Utilisation of Large
Language Models (LLMs) for this purpose is constrained by availability of
training data and frequently lack real-time adaptability. This study introduces
SiteShield, a multi-modal LVLM-based Retrieval-Augmented Generation (RAG)
framework for automating construction safety inspection reports by integrating
visual and audio inputs. Using real-world data, SiteShield outperformed
unimodal LLMs without RAG with an F1 score of 0.82, hamming loss of 0.04,
precision of 0.76, and recall of 0.96. The findings indicate that SiteShield
offers a novel pathway to enhance information retrieval and efficiency in
generating safety reports.

</details>


### [295] [Textured Gaussians for Enhanced 3D Scene Appearance Modeling](https://arxiv.org/abs/2411.18625)
*Brian Chao,Hung-Yu Tseng,Lorenzo Porzi,Chen Gao,Tuotuo Li,Qinbo Li,Ayush Saraf,Jia-Bin Huang,Johannes Kopf,Gordon Wetzstein,Changil Kim*

Main category: cs.CV

TL;DR: This paper proposes an enhancement to 3D Gaussian Splatting (3DGS) by integrating texture maps, providing richer texture and geometric expressivity compared to traditional approaches.


<details>
  <summary>Details</summary>
Motivation: Current 3D Gaussian Splatting techniques are limited in expressivity, as they can only represent simplistic geometric shapes and uniform colors.

Method: The authors augment 3DGS with alpha, RGB, or RGBA texture maps to allow for spatially varying color and opacity, enhancing the capabilities of individual Gaussian primitives.

Result: The proposed approach improves image quality while maintaining or reducing the number of Gaussian primitives compared to existing techniques.

Conclusion: Texture mapping significantly enhances the expressivity of 3DGS, enabling finer geometric and visual detail representation in 3D reconstruction and rendering.

Abstract: 3D Gaussian Splatting (3DGS) has recently emerged as a state-of-the-art 3D
reconstruction and rendering technique due to its high-quality results and fast
training and rendering time. However, pixels covered by the same Gaussian are
always shaded in the same color up to a Gaussian falloff scaling factor.
Furthermore, the finest geometric detail any individual Gaussian can represent
is a simple ellipsoid. These properties of 3DGS greatly limit the expressivity
of individual Gaussian primitives. To address these issues, we draw inspiration
from texture and alpha mapping in traditional graphics and integrate it with
3DGS. Specifically, we propose a new generalized Gaussian appearance
representation that augments each Gaussian with alpha~(A), RGB, or RGBA texture
maps to model spatially varying color and opacity across the extent of each
Gaussian. As such, each Gaussian can represent a richer set of texture patterns
and geometric structures, instead of just a single color and ellipsoid as in
naive Gaussian Splatting. Surprisingly, we found that the expressivity of
Gaussians can be greatly improved by using alpha-only texture maps, and further
augmenting Gaussians with RGB texture maps achieves the highest expressivity.
We validate our method on a wide variety of standard benchmark datasets and our
own custom captures at both the object and scene levels. We demonstrate image
quality improvements over existing methods while using a similar or lower
number of Gaussians.

</details>


### [296] [BLADE: Bias-Linked Adaptive DEbiasing](https://arxiv.org/abs/2510.04174)
*Piyush Arora,Navlika Singh,Vasubhya Diwan,Pratik Mazumder*

Main category: cs.CV

TL;DR: BLADE (Bias-Linked Adaptive Debiasing) is a novel generative framework addressing neural network bias without needing prior knowledge or bias-conflicting samples, significantly outperforming existing methods on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Neural networks often learn implicit biases from spurious correlations in training data, which hinders generalization and task-relevant feature learning. Current solutions rely on impractical assumptions like bias knowledge or bias-conflicting samples' availability.

Method: BLADE uses a generative approach to translate images across bias domains while preserving task-relevant features. It refines images adaptively, aligns them with bias-translated counterparts, and misaligns similar-bias samples to encourage robust feature learning.

Result: BLADE outperforms state-of-the-art methods on various benchmark datasets. Specifically, it achieves an approximately 18% absolute improvement on the corrupted CIFAR-10 dataset in the worst group setting.

Conclusion: BLADE offers a practical and effective solution for mitigating biases in neural networks without relying on explicit supervision, setting a new standard in bias mitigation and robustness.

Abstract: Neural networks have revolutionized numerous fields, yet they remain
vulnerable to a critical flaw: the tendency to learn implicit biases, spurious
correlations between certain attributes and target labels in training data.
These biases are often more prevalent and easier to learn, causing models to
rely on superficial patterns rather than task-relevant features necessary for
generalization. Existing methods typically rely on strong assumptions, such as
prior knowledge of these biases or access to bias-conflicting samples, i.e.,
samples that contradict spurious correlations and counterbalance bias-aligned
samples, samples that conform to these spurious correlations. However, such
assumptions are often impractical in real-world settings. We propose BLADE
({B}ias-{L}inked {A}daptive {DE}biasing), a generative debiasing framework that
requires no prior knowledge of bias or bias-conflicting samples. BLADE first
trains a generative model to translate images across bias domains while
preserving task-relevant features. Then, it adaptively refines each image with
its synthetic counterpart based on the image's susceptibility to bias. To
encourage robust representations, BLADE aligns an image with its
bias-translated synthetic counterpart that shares task-relevant features but
differs in bias, while misaligning it with samples sharing the same bias. We
evaluate BLADE on multiple benchmark datasets and show that it significantly
outperforms state-of-the-art methods. Notably, it exceeds the closest baseline
by an absolute margin of around 18% on the corrupted CIFAR-10 dataset under the
worst group setting, establishing a new benchmark in bias mitigation and
demonstrating its potential for developing more robust deep learning models
without explicit supervision.

</details>


### [297] [From Segments to Concepts: Interpretable Image Classification via Concept-Guided Segmentation](https://arxiv.org/abs/2510.04180)
*Ran Eisenberg,Amit Rozner,Ethan Fetaya,Ofir Lindenbaum*

Main category: cs.CV

TL;DR: The paper introduces a novel framework, SEG-MIL-CBM, that integrates concept-guided segmentation into neural networks to enhance interpretability, robustness, and explanation quality while avoiding the need for costly annotations.


<details>
  <summary>Details</summary>
Motivation: Existing deep neural networks achieve high performance but suffer from a lack of interpretability, risking unreliability and misuse in safety-critical applications.

Method: The proposed SEG-MIL-CBM framework uses concept-guided image segmentation within an attention-based multiple instance learning setup, treating segmented image regions as instances and reasoning over semantically meaningful regions.

Result: SEG-MIL-CBM performs robustly on tasks with spurious correlations, noisy inputs, and large-scale benchmarks, while providing spatially grounded, concept-level model explanations.

Conclusion: SEG-MIL-CBM enhances both model interpretability and robustness without requiring manual concept annotation, making it suitable for safety-critical and complex applications.

Abstract: Deep neural networks have achieved remarkable success in computer vision;
however, their black-box nature in decision-making limits interpretability and
trust, particularly in safety-critical applications. Interpretability is
crucial in domains where errors have severe consequences. Existing models not
only lack transparency but also risk exploiting unreliable or misleading
features, which undermines both robustness and the validity of their
explanations. Concept Bottleneck Models (CBMs) aim to improve transparency by
reasoning through human-interpretable concepts. Still, they require costly
concept annotations and lack spatial grounding, often failing to identify which
regions support each concept. We propose SEG-MIL-CBM, a novel framework that
integrates concept-guided image segmentation into an attention-based multiple
instance learning (MIL) framework, where each segmented region is treated as an
instance and the model learns to aggregate evidence across them. By reasoning
over semantically meaningful regions aligned with high-level concepts, our
model highlights task-relevant evidence, down-weights irrelevant cues, and
produces spatially grounded, concept-level explanations without requiring
annotations of concepts or groups. SEG-MIL-CBM achieves robust performance
across settings involving spurious correlations (unintended dependencies
between background and label), input corruptions (perturbations that degrade
visual quality), and large-scale benchmarks, while providing transparent,
concept-level explanations.

</details>


### [298] [Let Features Decide Their Own Solvers: Hybrid Feature Caching for Diffusion Transformers](https://arxiv.org/abs/2510.04188)
*Shikang Zheng,Guantao Chen,Qinming Zhou,Yuqi Lin,Lixuan He,Chang Zou,Peiliang Cai,Jiacheng Liu,Linfeng Zhang*

Main category: cs.CV

TL;DR: Diffusion Transformers achieve impressive image and video synthesis, but their iterative sampling process is slow. The HyCa framework uses dimension-specific caching strategies inspired by ODE solvers to significantly speed up this process without retraining.


<details>
  <summary>Details</summary>
Motivation: Address the inefficiency of Diffusion Transformers during their iterative sampling process, which is constrained by high computational costs.

Method: Propose the HyCa framework, which models hidden features as a mixture of ODEs and applies dimension-wise caching strategies for acceleration.

Result: HyCa delivers significant speedups, achieving 5.55x faster on FLUX, 5.56x faster on HunyuanVideo, and 6.24x faster on Qwen-Image models without compromising synthesis fidelity.

Conclusion: HyCa enables near-lossless acceleration in image and video synthesis across various models and domains, offering a practical solution to the computational bottleneck of Diffusion Transformers.

Abstract: Diffusion Transformers offer state-of-the-art fidelity in image and video
synthesis, but their iterative sampling process remains a major bottleneck due
to the high cost of transformer forward passes at each timestep. To mitigate
this, feature caching has emerged as a training-free acceleration technique
that reuses or forecasts hidden representations. However, existing methods
often apply a uniform caching strategy across all feature dimensions, ignoring
their heterogeneous dynamic behaviors. Therefore, we adopt a new perspective by
modeling hidden feature evolution as a mixture of ODEs across dimensions, and
introduce HyCa, a Hybrid ODE solver inspired caching framework that applies
dimension-wise caching strategies. HyCa achieves near-lossless acceleration
across diverse domains and models, including 5.55 times speedup on FLUX, 5.56
times speedup on HunyuanVideo, 6.24 times speedup on Qwen-Image and
Qwen-Image-Edit without retraining.

</details>


### [299] [World-To-Image: Grounding Text-to-Image Generation with Agent-Driven World Knowledge](https://arxiv.org/abs/2510.04201)
*Moo Hyun Son,Jintaek Oh,Sun Bin Mun,Jaechul Roh,Sehyun Choi*

Main category: cs.CV

TL;DR: World-To-Image bridges the gap between text-to-image models and novel/out-of-distribution content using web-retrieved images and multimodal prompt optimization.


<details>
  <summary>Details</summary>
Motivation: Text-to-image models struggle with generating photos for unfamiliar or out-of-distribution entities due to limitations in their knowledge cutoffs.

Method: The framework employs an agent to search the web for images of novel concepts, subsequently using this information for multimodal prompt optimization to guide generative backbones.

Result: World-To-Image demonstrates +8.1% improvement in accuracy-to-prompt over state-of-the-art methods in semantic alignment and visual aesthetics on curated NICE benchmarks.

Conclusion: World-To-Image effectively enhances text-to-image systems to reflect dynamic and novel real-world information while maintaining high efficiency.

Abstract: While text-to-image (T2I) models can synthesize high-quality images, their
performance degrades significantly when prompted with novel or
out-of-distribution (OOD) entities due to inherent knowledge cutoffs. We
introduce World-To-Image, a novel framework that bridges this gap by empowering
T2I generation with agent-driven world knowledge. We design an agent that
dynamically searches the web to retrieve images for concepts unknown to the
base model. This information is then used to perform multimodal prompt
optimization, steering powerful generative backbones toward an accurate
synthesis. Critically, our evaluation goes beyond traditional metrics,
utilizing modern assessments like LLMGrader and ImageReward to measure true
semantic fidelity. Our experiments show that World-To-Image substantially
outperforms state-of-the-art methods in both semantic alignment and visual
aesthetics, achieving +8.1% improvement in accuracy-to-prompt on our curated
NICE benchmark. Our framework achieves these results with high efficiency in
less than three iterations, paving the way for T2I systems that can better
reflect the ever-changing real world. Our demo code is available
here\footnote{https://github.com/mhson-kyle/World-To-Image}.

</details>


### [300] [MASC: Boosting Autoregressive Image Generation with a Manifold-Aligned Semantic Clustering](https://arxiv.org/abs/2510.04220)
*Lixuan He,Shikang Zheng,Linfeng Zhang*

Main category: cs.CV

TL;DR: MASC, a framework creating hierarchical semantic trees from visual token embeddings, addresses inefficiencies in autoregressive image generation by simplifying the prediction space. It improves training efficiency and generation quality.


<details>
  <summary>Details</summary>
Motivation: Autoregressive models for image generation are hindered by their reliance on an unstructured vocabulary of visual tokens, which ignores token semantic relationships and complicates prediction tasks.

Method: The authors propose Manifold-Aligned Semantic Clustering (MASC), which constructs hierarchical semantic trees from token embeddings using a geometry-aware distance metric and density-driven agglomerative techniques as a plug-and-play optimization module.

Result: MASC accelerates training by up to 57% and enhances generation quality significantly, reducing the FID score of LlamaGen-XL from 2.87 to 2.58.

Conclusion: Structuring the prediction space, as MASC demonstrates, is just as critical as architectural advancements for scalable generative modeling in image generation.

Abstract: Autoregressive (AR) models have shown great promise in image generation, yet
they face a fundamental inefficiency stemming from their core component: a
vast, unstructured vocabulary of visual tokens. This conventional approach
treats tokens as a flat vocabulary, disregarding the intrinsic structure of the
token embedding space where proximity often correlates with semantic
similarity. This oversight results in a highly complex prediction task, which
hinders training efficiency and limits final generation quality. To resolve
this, we propose Manifold-Aligned Semantic Clustering (MASC), a principled
framework that constructs a hierarchical semantic tree directly from the
codebook's intrinsic structure. MASC employs a novel geometry-aware distance
metric and a density-driven agglomerative construction to model the underlying
manifold of the token embeddings. By transforming the flat, high-dimensional
prediction task into a structured, hierarchical one, MASC introduces a
beneficial inductive bias that significantly simplifies the learning problem
for the AR model. MASC is designed as a plug-and-play module, and our extensive
experiments validate its effectiveness: it accelerates training by up to 57%
and significantly improves generation quality, reducing the FID of LlamaGen-XL
from 2.87 to 2.58. MASC elevates existing AR frameworks to be highly
competitive with state-of-the-art methods, establishing that structuring the
prediction space is as crucial as architectural innovation for scalable
generative modeling.

</details>


### [301] [Zoom-In to Sort AI-Generated Images Out](https://arxiv.org/abs/2510.04225)
*Yikun Ji,Yan Hong,Bowen Deng,jun lan,Huijia Zhu,Weiqiang Wang,Liqing Zhang,Jianfu Zhang*

Main category: cs.CV

TL;DR: This paper introduces ZoomIn, a forensic framework for detecting AI-generated imagery using focused analysis, supported by a new dataset, MagniFake.


<details>
  <summary>Details</summary>
Motivation: The rise of high-quality AI-generated imagery poses challenges for distinguishing real content from synthetic, threatening digital integrity.

Method: ZoomIn employs a two-step process: scanning images to locate suspicious areas and providing detailed analysis and explanations of those regions.

Result: ZoomIn achieved a high accuracy of 96.39%, robust generalization, and interpretable outcomes using visual evidence.

Conclusion: ZoomIn improves forensic detection capabilities by combining interpretability and accuracy, offering a grounded method to address digital integrity concerns.

Abstract: The rapid growth of AI-generated imagery has blurred the boundary between
real and synthetic content, raising critical concerns for digital integrity.
Vision-language models (VLMs) offer interpretability through explanations but
often fail to detect subtle artifacts in high-quality synthetic images. We
propose ZoomIn, a two-stage forensic framework that improves both accuracy and
interpretability. Mimicking human visual inspection, ZoomIn first scans an
image to locate suspicious regions and then performs a focused analysis on
these zoomed-in areas to deliver a grounded verdict. To support training, we
introduce MagniFake, a dataset of 20,000 real and high-quality synthetic images
annotated with bounding boxes and forensic explanations, generated through an
automated VLM-based pipeline. Our method achieves 96.39% accuracy with robust
generalization, while providing human-understandable explanations grounded in
visual evidence.

</details>


### [302] [A Recursive Pyramidal Algorithm for Solving the Image Registration Problem](https://arxiv.org/abs/2510.04231)
*Stefan Dirnstorfer*

Main category: cs.CV

TL;DR: The paper proposes an efficient and simple algorithm for image registration requiring minimal training data, time, and code complexity.


<details>
  <summary>Details</summary>
Motivation: To simplify the process of aligning two images by finding corresponding points efficiently, especially in scenarios with limited training resources.

Method: An end-to-end trainable algorithm implemented in a few lines of Python, tested with minimal training data and time.

Result: The algorithm demonstrated accurate results on stereo vision data, trained with only 74 images over a 19x15 input window.

Conclusion: The proposed solution is a concise, effective approach for image registration and serves as a promising tool for scenarios with resource constraints.

Abstract: The problem of image registration is finding a transformation that aligns two
images, such that the corresponding points are in the same location. This paper
introduces a simple, end-to-end trainable algorithm that is implementable in a
few lines of Python code. The approach is shown to work with very little
training data and training time, while achieving accurate results in some
settings. An example application to stereo vision was trained from 74 images on
a 19x15 input window. With just a dozen lines of Python code this algorithm
excels in brevity and may serve as a good start in related scenarios with
limitations to training data, training time or code complexity.

</details>


### [303] [Detection of retinal diseases using an accelerated reused convolutional network](https://arxiv.org/abs/2510.04232)
*Amin Ahmadi Kasani,Hedieh Sajedi*

Main category: cs.CV

TL;DR: The paper introduces a novel convolutional layer, ArConv, to optimize deep neural networks for eye disease detection. This approach creates an accessible model with high accuracy and reduced complexity, suitable for mobile device use.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the need to improve accessibility and efficiency of deep neural networks used in diagnosing eye diseases, particularly for mobile applications.

Method: The method involves redesigning and optimizing convolutional layers, leading to the creation of ArConv layers, which are integrated into a new general model. This model was trained and evaluated on the RfMiD dataset.

Result: The final model, with only 1.3 million parameters, outperformed MobileNetV2 (2.2 million parameters) on the RfMiD dataset with an accuracy of 0.9328 compared to 0.9266.

Conclusion: The study presents a computationally efficient and accurate neural network model suitable for early eye disease detection, particularly optimized for use on mobile devices.

Abstract: Convolutional neural networks are continually evolving, with some efforts
aimed at improving accuracy, others at increasing speed, and some at enhancing
accessibility. Improving accessibility broadens the application of neural
networks across a wider range of tasks, including the detection of eye
diseases. Early diagnosis of eye diseases and consulting an ophthalmologist can
prevent many vision disorders. Given the importance of this issue, various
datasets have been collected from the cornea to facilitate the process of
making neural network models. However, most of the methods introduced in the
past are computationally complex. In this study, we tried to increase the
accessibility of deep neural network models. We did this at the most
fundamental level, specifically by redesigning and optimizing the convolutional
layers. By doing so, we created a new general model that incorporates our novel
convolutional layer named ArConv layers. Thanks to the efficient performance of
this new layer, the model has suitable complexity for use in mobile phones and
can perform the task of diagnosing the presence of disease with high accuracy.
The final model we present contains only 1.3 million parameters. In comparison
to the MobileNetV2 model, which has 2.2 million parameters, our model
demonstrated better accuracy when trained and evaluated on the RfMiD dataset
under identical conditions, achieving an accuracy of 0.9328 versus 0.9266 on
the RfMiD test set.

</details>


### [304] [Scaling Sequence-to-Sequence Generative Neural Rendering](https://arxiv.org/abs/2510.04236)
*Shikun Liu,Kam Woh Ng,Wonbong Jang,Jiadong Guo,Junlin Han,Haozhe Liu,Yiannis Douratsos,Juan C. Pérez,Zijian Zhou,Chi Phung,Tao Xiang,Juan-Manuel Pérez-Rúa*

Main category: cs.CV

TL;DR: Kaleido is a generative model for neural rendering, unifying 3D and video modelling with state-of-the-art view synthesis benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current approaches to neural rendering either rely heavily on specific 3D datasets or struggle in generative view synthesis tasks. Kaleido aims to solve these limitations by introducing a unified approach.

Method: Kaleido employs a masked autoregressive framework and a decoder-only rectified flow transformer while leveraging large-scale video data for pre-training.

Result: Kaleido achieves state-of-the-art performance in view synthesis benchmarks, excelling in few-view settings and matching per-scene optimization methods in many-view settings.

Conclusion: Kaleido demonstrates improved spatial consistency and reduced reliance on labeled datasets, marking a breakthrough in generative neural rendering systems.

Abstract: We present Kaleido, a family of generative models designed for
photorealistic, unified object- and scene-level neural rendering. Kaleido
operates on the principle that 3D can be regarded as a specialised sub-domain
of video, expressed purely as a sequence-to-sequence image synthesis task.
Through a systemic study of scaling sequence-to-sequence generative neural
rendering, we introduce key architectural innovations that enable our model to:
i) perform generative view synthesis without explicit 3D representations; ii)
generate any number of 6-DoF target views conditioned on any number of
reference views via a masked autoregressive framework; and iii) seamlessly
unify 3D and video modelling within a single decoder-only rectified flow
transformer. Within this unified framework, Kaleido leverages large-scale video
data for pre-training, which significantly improves spatial consistency and
reduces reliance on scarce, camera-labelled 3D datasets -- all without any
architectural modifications. Kaleido sets a new state-of-the-art on a range of
view synthesis benchmarks. Its zero-shot performance substantially outperforms
other generative methods in few-view settings, and, for the first time, matches
the quality of per-scene optimisation methods in many-view settings.

</details>


### [305] [The best performance in the CARE 2025 -- Liver Task (LiSeg-Contrast): Contrast-Aware Semi-Supervised Segmentation with Domain Generalization and Test-Time Adaptation](https://arxiv.org/abs/2510.04243)
*Jincan Lou,Jingkun Chen,Haoquan Li,Hang Li,Wenjian Huang,Weihua Chen,Fan Wang,Jianguo Zhang*

Main category: cs.CV

TL;DR: The paper presents a framework called CoSSeg-TTA for accurate liver segmentation from enhanced MRI, addressing domain variability, limited data, and inference robustness.


<details>
  <summary>Details</summary>
Motivation: The study aims to overcome challenges like limited annotated data, domain variability due to scanner and institution differences, and the limitations of traditional segmentation frameworks in the context of liver segmentation from contrast-enhanced MRI.

Method: The CoSSeg-TTA framework is based on nnU-Netv2, enhanced by a semi-supervised mean teacher scheme. A domain adaptation module is introduced using a histogram-based style transfer function and a contrast-aware network. Additionally, a continual test-time adaptation strategy is used for inference robustness.

Result: The proposed framework outperforms the nnU-Netv2 baseline, showing superior Dice scores, better Hausdorff Distances, and strong generalization under low-annotation conditions and across unseen domains.

Conclusion: The CoSSeg-TTA framework effectively addresses limitations in liver segmentation for contrast-enhanced MRI, improving accuracy, robustness, and adaptability to unseen domains.

Abstract: Accurate liver segmentation from contrast-enhanced MRI is essential for
diagnosis, treatment planning, and disease monitoring. However, it remains
challenging due to limited annotated data, heterogeneous enhancement protocols,
and significant domain shifts across scanners and institutions. Traditional
image-to-image translation frameworks have made great progress in domain
generalization, but their application is not straightforward. For example,
Pix2Pix requires image registration, and cycle-GAN cannot be integrated
seamlessly into segmentation pipelines. Meanwhile, these methods are originally
used to deal with cross-modality scenarios, and often introduce structural
distortions and suffer from unstable training, which may pose drawbacks in our
single-modality scenario. To address these challenges, we propose CoSSeg-TTA, a
compact segmentation framework for the GED4 (Gd-EOB-DTPA enhanced hepatobiliary
phase MRI) modality built upon nnU-Netv2 and enhanced with a semi-supervised
mean teacher scheme to exploit large amounts of unlabeled volumes. A domain
adaptation module, incorporating a randomized histogram-based style appearance
transfer function and a trainable contrast-aware network, enriches domain
diversity and mitigates cross-center variability. Furthermore, a continual
test-time adaptation strategy is employed to improve robustness during
inference. Extensive experiments demonstrate that our framework consistently
outperforms the nnU-Netv2 baseline, achieving superior Dice score and Hausdorff
Distance while exhibiting strong generalization to unseen domains under
low-annotation conditions.

</details>


### [306] [Concept-Based Masking: A Patch-Agnostic Defense Against Adversarial Patch Attacks](https://arxiv.org/abs/2510.04245)
*Ayushi Mehrotra,Derek Peng,Dipkamal Bhusal,Nidhi Rastogi*

Main category: cs.CV

TL;DR: The paper introduces a defense mechanism against adversarial patch attacks using concept-based explanations, achieving superior performance without relying on prior knowledge of patch size or location.


<details>
  <summary>Details</summary>
Motivation: Adversarial patch attacks exploit localized perturbations to force misclassifications in deep learning models, posing practical threats, and existing defenses depend on prior knowledge of patch specifics.

Method: The method involves using concept-based explanations to identify and suppress influential concept activation vectors, neutralizing patch effects without requiring explicit patch detection.

Result: The proposed method outperforms a state-of-the-art defense, PatchCleanser, in terms of robust and clean accuracy across diverse patch sizes and locations.

Conclusion: The study demonstrates that combining interpretability with robustness offers a scalable solution to defend against adversarial patch attacks, emphasizing concept-driven strategies as promising avenues.

Abstract: Adversarial patch attacks pose a practical threat to deep learning models by
forcing targeted misclassifications through localized perturbations, often
realized in the physical world. Existing defenses typically assume prior
knowledge of patch size or location, limiting their applicability. In this
work, we propose a patch-agnostic defense that leverages concept-based
explanations to identify and suppress the most influential concept activation
vectors, thereby neutralizing patch effects without explicit detection.
Evaluated on Imagenette with a ResNet-50, our method achieves higher robust and
clean accuracy than the state-of-the-art PatchCleanser, while maintaining
strong performance across varying patch sizes and locations. Our results
highlight the promise of combining interpretability with robustness and suggest
concept-driven defenses as a scalable strategy for securing machine learning
models against adversarial patch attacks.

</details>


### [307] [Flexible and Efficient Spatio-Temporal Transformer for Sequential Visual Place Recognition](https://arxiv.org/abs/2510.04282)
*Yu Kiu,Lau,Chao Chen,Ge Jin,Chen Feng*

Main category: cs.CV

TL;DR: Adapt-STformer is a Seq-VPR model that uses a novel Recurrent Deformable Transformer Encoder (Recurrent-DTE) to improve efficiency, flexibility, and performance.


<details>
  <summary>Details</summary>
Motivation: Existing transformer-based Seq-VPR models focus on achieving high performance but lack flexibility in handling varying sequence lengths and are inefficient in terms of speed and memory usage for real-time applications.

Method: The authors propose Adapt-STformer, leveraging the Recurrent Deformable Transformer Encoder, which uses an iterative recurrent mechanism for effective spatio-temporal information fusion. This method inherently supports variable sequence lengths, reduces computational demand, and operates efficiently.

Result: The proposed model improves recall by up to 17%, lowers sequence extraction time by 36%, and reduces memory usage by 35% compared to the second-best baseline. Experiments were conducted on three benchmark datasets.

Conclusion: Adapt-STformer achieves a balance of performance, flexibility, and efficiency, making it suitable for real-time Seq-VPR applications.

Abstract: Sequential Visual Place Recognition (Seq-VPR) leverages transformers to
capture spatio-temporal features effectively; however, existing approaches
prioritize performance at the expense of flexibility and efficiency. In
practice, a transformer-based Seq-VPR model should be flexible to the number of
frames per sequence (seq-length), deliver fast inference, and have low memory
usage to meet real-time constraints. To our knowledge, no existing
transformer-based Seq-VPR method achieves both flexibility and efficiency. To
address this gap, we propose Adapt-STformer, a Seq-VPR method built around our
novel Recurrent Deformable Transformer Encoder (Recurrent-DTE), which uses an
iterative recurrent mechanism to fuse information from multiple sequential
frames. This design naturally supports variable seq-lengths, fast inference,
and low memory usage. Experiments on the Nordland, Oxford, and NuScenes
datasets show that Adapt-STformer boosts recall by up to 17% while reducing
sequence extraction time by 36% and lowering memory usage by 35% compared to
the second-best baseline.

</details>


### [308] [ChronoEdit: Towards Temporal Reasoning for Image Editing and World Simulation](https://arxiv.org/abs/2510.04290)
*Jay Zhangjie Wu,Xuanchi Ren,Tianchang Shen,Tianshi Cao,Kai He,Yifan Lu,Ruiyuan Gao,Enze Xie,Shiyi Lan,Jose M. Alvarez,Jun Gao,Sanja Fidler,Zian Wang,Huan Ling*

Main category: cs.CV

TL;DR: ChronoEdit transforms image editing into a video generation problem to ensure physical consistency, leveraging video generative models and temporal reasoning.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenge of ensuring physical consistency in image editing, particularly vital for tasks such as world simulation.

Method: ChronoEdit treats image editing as video generation, using pretrained video generative models for temporal consistency and introducing reasoning tokens during editing.

Result: ChronoEdit surpasses state-of-the-art baselines in visual fidelity and physical plausibility based on the newly introduced PBench-Edit benchmark.

Conclusion: ChronoEdit offers a novel and effective approach to image editing by ensuring coherence through leveraging implicit physics and temporal reasoning, with code and models released for community use.

Abstract: Recent advances in large generative models have significantly advanced image
editing and in-context image generation, yet a critical gap remains in ensuring
physical consistency, where edited objects must remain coherent. This
capability is especially vital for world simulation related tasks. In this
paper, we present ChronoEdit, a framework that reframes image editing as a
video generation problem. First, ChronoEdit treats the input and edited images
as the first and last frames of a video, allowing it to leverage large
pretrained video generative models that capture not only object appearance but
also the implicit physics of motion and interaction through learned temporal
consistency. Second, ChronoEdit introduces a temporal reasoning stage that
explicitly performs editing at inference time. Under this setting, the target
frame is jointly denoised with reasoning tokens to imagine a plausible editing
trajectory that constrains the solution space to physically viable
transformations. The reasoning tokens are then dropped after a few steps to
avoid the high computational cost of rendering a full video. To validate
ChronoEdit, we introduce PBench-Edit, a new benchmark of image-prompt pairs for
contexts that require physical consistency, and demonstrate that ChronoEdit
surpasses state-of-the-art baselines in both visual fidelity and physical
plausibility. Code and models for both the 14B and 2B variants of ChronoEdit
will be released on the project page:
https://research.nvidia.com/labs/toronto-ai/chronoedit

</details>


### [309] [CARE-PD: A Multi-Site Anonymized Clinical Dataset for Parkinson's Disease Gait Assessment](https://arxiv.org/abs/2510.04312)
*Vida Adeli,Ivan Klabucar,Javad Rajabi,Benjamin Filtjens,Soroush Mehraban,Diwei Wang,Hyewon Seo,Trung-Hieu Hoang,Minh N. Do,Candice Muller,Claudia Oliveira,Daniel Boari Coelho,Pieter Ginis,Moran Gilat,Alice Nieuwboer,Joke Spildooren,Lucas Mckay,Hyeokhyen Kwon,Gari Clifford,Christine Esper,Stewart Factor,Imari Genias,Amirhossein Dadashzadeh,Leia Shum,Alan Whone,Majid Mirmehdi,Andrea Iaboni,Babak Taati*

Main category: cs.CV

TL;DR: The study introduces CARE-PD, a large, multi-site 3D mesh gait dataset for Parkinson's Disease (PD), supporting tasks like clinical score prediction and motion reconstruction.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the lack of diverse, clinically annotated motion datasets for objective gait assessment in Parkinson's Disease.

Method: CARE-PD was developed using harmonized preprocessing to convert gait recordings into anonymized 3D meshes, offering benchmarks like clinical score prediction and unsupervised motion reconstruction.

Result: CARE-PD demonstrated efficacy, with pretraining reducing errors (MPJPE from 60.8mm to 7.5mm) and substantially improving PD severity macro-F1 by 17 percentage points.

Conclusion: CARE-PD and its benchmarks establish the importance of clinically curated data for advancing PD gait assessment and are made publicly available for research purposes.

Abstract: Objective gait assessment in Parkinson's Disease (PD) is limited by the
absence of large, diverse, and clinically annotated motion datasets. We
introduce CARE-PD, the largest publicly available archive of 3D mesh gait data
for PD, and the first multi-site collection spanning 9 cohorts from 8 clinical
centers. All recordings (RGB video or motion capture) are converted into
anonymized SMPL meshes via a harmonized preprocessing pipeline. CARE-PD
supports two key benchmarks: supervised clinical score prediction (estimating
Unified Parkinson's Disease Rating Scale, UPDRS, gait scores) and unsupervised
motion pretext tasks (2D-to-3D keypoint lifting and full-body 3D
reconstruction). Clinical prediction is evaluated under four generalization
protocols: within-dataset, cross-dataset, leave-one-dataset-out, and
multi-dataset in-domain adaptation. To assess clinical relevance, we compare
state-of-the-art motion encoders with a traditional gait-feature baseline,
finding that encoders consistently outperform handcrafted features. Pretraining
on CARE-PD reduces MPJPE (from 60.8mm to 7.5mm) and boosts PD severity macro-F1
by 17 percentage points, underscoring the value of clinically curated, diverse
training data. CARE-PD and all benchmark code are released for non-commercial
research at https://neurips2025.care-pd.ca/.

</details>


### [310] [GenAR: Next-Scale Autoregressive Generation for Spatial Gene Expression Prediction](https://arxiv.org/abs/2510.04315)
*Jiarui Ouyang,Yihui Wang,Yihang Gao,Yingxue Xu,Shu Yang,Hao Chen*

Main category: cs.CV

TL;DR: The paper introduces GenAR, a framework to predict spatial gene expression from H&E images, addressing issues in previous methods using hierarchical clustering and discrete token generation.


<details>
  <summary>Details</summary>
Motivation: Existing methods for predicting gene expression from H&E stained images often overlook gene co-expression structures and treat discrete expression counts as continuous regression, leading to inaccuracies.

Method: GenAR utilizes a multi-scale autoregressive framework that clusters genes hierarchically, models spatial expression with discrete token generation, and integrates histological and spatial embeddings for accurate predictions.

Result: GenAR outperforms traditional models in predicting gene expression across four spatial transcriptomics datasets involving various tissue types, showcasing superior performance.

Conclusion: GenAR provides a cost-effective and biologically sound approach to spatial transcriptomics using H&E images, potentially advancing precision medicine and enabling affordable molecular profiling.

Abstract: Spatial Transcriptomics (ST) offers spatially resolved gene expression but
remains costly. Predicting expression directly from widely available
Hematoxylin and Eosin (H&E) stained images presents a cost-effective
alternative. However, most computational approaches (i) predict each gene
independently, overlooking co-expression structure, and (ii) cast the task as
continuous regression despite expression being discrete counts. This mismatch
can yield biologically implausible outputs and complicate downstream analyses.
We introduce GenAR, a multi-scale autoregressive framework that refines
predictions from coarse to fine. GenAR clusters genes into hierarchical groups
to expose cross-gene dependencies, models expression as codebook-free discrete
token generation to directly predict raw counts, and conditions decoding on
fused histological and spatial embeddings. From an information-theoretic
perspective, the discrete formulation avoids log-induced biases and the
coarse-to-fine factorization aligns with a principled conditional
decomposition. Extensive experimental results on four Spatial Transcriptomics
datasets across different tissue types demonstrate that GenAR achieves
state-of-the-art performance, offering potential implications for precision
medicine and cost-effective molecular profiling. Code is publicly available at
https://github.com/oyjr/genar.

</details>


### [311] [Diffusion^2: Dual Diffusion Model with Uncertainty-Aware Adaptive Noise for Momentary Trajectory Prediction](https://arxiv.org/abs/2510.04365)
*Yuhao Luo,Yuang Zhang,Kehua Chen,Xinyu Zheng,Shucheng Zhang,Sikai Chen,Yinhai Wang*

Main category: cs.CV

TL;DR: The paper proposes Diffusion^2, a dual diffusion model framework aimed at improving pedestrian trajectory prediction under extreme scenarios where observational data is scarce.


<details>
  <summary>Details</summary>
Motivation: Pedestrian trajectory prediction is essential for traffic safety, but traditional methods struggle in scenarios lacking sufficient observational data, such as sudden pedestrian emergence from blind spots.

Method: Diffusion^2 utilizes two sequential diffusion models: the first for generating unobserved historical trajectories and the second for predicting future trajectories, incorporating mechanisms to handle noise and uncertainty.

Result: The proposed framework achieves state-of-the-art performance in challenging momentary trajectory prediction tasks on ETH/UCY and Stanford Drone datasets.

Conclusion: Diffusion^2 offers a significant advancement in pedestrian trajectory prediction, especially under scenarios of limited observational data, enhancing traffic safety and efficiency.

Abstract: Accurate pedestrian trajectory prediction is crucial for ensuring safety and
efficiency in autonomous driving and human-robot interaction scenarios. Earlier
studies primarily utilized sufficient observational data to predict future
trajectories. However, in real-world scenarios, such as pedestrians suddenly
emerging from blind spots, sufficient observational data is often unavailable
(i.e. momentary trajectory), making accurate prediction challenging and
increasing the risk of traffic accidents. Therefore, advancing research on
pedestrian trajectory prediction under extreme scenarios is critical for
enhancing traffic safety. In this work, we propose a novel framework termed
Diffusion^2, tailored for momentary trajectory prediction. Diffusion^2 consists
of two sequentially connected diffusion models: one for backward prediction,
which generates unobserved historical trajectories, and the other for forward
prediction, which forecasts future trajectories. Given that the generated
unobserved historical trajectories may introduce additional noise, we propose a
dual-head parameterization mechanism to estimate their aleatoric uncertainty
and design a temporally adaptive noise module that dynamically modulates the
noise scale in the forward diffusion process. Empirically, Diffusion^2 sets a
new state-of-the-art in momentary trajectory prediction on ETH/UCY and Stanford
Drone datasets.

</details>


### [312] [MorphoSim: An Interactive, Controllable, and Editable Language-guided 4D World Simulator](https://arxiv.org/abs/2510.04390)
*Xuehai He,Shijie Zhou,Thivyanth Venkateswaran,Kaizhi Zheng,Ziyu Wan,Achuta Kadambi,Xin Eric Wang*

Main category: cs.CV

TL;DR: MorphoSim introduces a language-guided framework for generating editable and controllable 4D spatiotemporal environments, overcoming limitations of current 2D text-to-video models.


<details>
  <summary>Details</summary>
Motivation: To create spatiotemporal world models that enable robotics applications such as scalable training data, reproducible evaluation, and flexible task design, beyond the constraints of current 2D text-to-video models.

Method: MorphoSim employs natural language instructions, trajectory-guided generation, and feature field distillation to create 4D scenes. It allows object-level controls and interactive edits without full scene regeneration, ensuring multi-view consistency.

Result: MorphoSim achieves high scene fidelity, enables dynamic editing of objects, and allows arbitrary viewpoint observation while maintaining controllability and editability.

Conclusion: MorphoSim successfully generates 4D dynamic environments that are more capable than current models, offering practical tools for robotics and other applications. Code availability ensures reproducibility and accessibility.

Abstract: World models that support controllable
  and editable spatiotemporal environments are valuable
  for robotics, enabling scalable training data, repro ducible evaluation, and
flexible task design. While
  recent text-to-video models generate realistic dynam ics, they are
constrained to 2D views and offer limited
  interaction. We introduce MorphoSim, a language guided framework that
generates 4D scenes with
  multi-view consistency and object-level controls. From
  natural language instructions, MorphoSim produces
  dynamic environments where objects can be directed,
  recolored, or removed, and scenes can be observed
  from arbitrary viewpoints. The framework integrates
  trajectory-guided generation with feature field dis tillation, allowing edits
to be applied interactively
  without full re-generation. Experiments show that Mor phoSim maintains high
scene fidelity while enabling
  controllability and editability. The code is available
  at https://github.com/eric-ai-lab/Morph4D.

</details>


### [313] [Your Vision-Language Model Can't Even Count to 20: Exposing the Failures of VLMs in Compositional Counting](https://arxiv.org/abs/2510.04401)
*Xuyang Guo,Zekai Huang,Zhenmei Shi,Zhao Song,Jiahao Zhang*

Main category: cs.CV

TL;DR: The paper assesses Vision-Language Models (VLMs) in counting tasks using a benchmark with basic geometric shapes, revealing notable failures in compositional counting.


<details>
  <summary>Details</summary>
Motivation: Investigate whether Vision-Language Models (VLMs), which excel in various tasks, can accurately count objects under controlled scenarios.

Method: Develop a benchmark called VLMCountBench with geometric shapes in simplistic settings, manipulate variables like color and size, and conduct controlled experiments for counting tasks.

Result: VLMs perform reliably with single shape types but fail significantly in compositional settings with mixed types of shapes.

Conclusion: Current VLMs face limitations in compositional counting tasks, pinpointing a key area for improvement and future research directions.

Abstract: Vision-Language Models (VLMs) have become a central focus of today's AI
community, owing to their impressive abilities gained from training on
large-scale vision-language data from the Web. These models have demonstrated
strong performance across diverse tasks, including image understanding, video
understanding, complex visual reasoning, and embodied AI. Despite these
noteworthy successes, a fundamental question remains: Can VLMs count objects
correctly? In this paper, we introduce a simple yet effective benchmark,
VLMCountBench, designed under a minimalist setting with only basic geometric
shapes (e.g., triangles, circles) and their compositions, focusing exclusively
on counting tasks without interference from other factors. We adopt strict
independent variable control and systematically study the effects of simple
properties such as color, size, and prompt refinement in a controlled ablation.
Our empirical results reveal that while VLMs can count reliably when only one
shape type is present, they exhibit substantial failures when multiple shape
types are combined (i.e., compositional counting). This highlights a
fundamental empirical limitation of current VLMs and motivates important
directions for future research.

</details>


### [314] [CodeFormer++: Blind Face Restoration Using Deformable Registration and Deep Metric Learning](https://arxiv.org/abs/2510.04410)
*Venkata Bharath Reddy Reddem,Akshay P Sarashetti,Ranjith Merugu,Amit Satish Unde*

Main category: cs.CV

TL;DR: The paper proposes CodeFormer++, a framework for blind face restoration that improves visual quality and identity preservation by utilizing generative priors with three specific tasks and modules.


<details>
  <summary>Details</summary>
Motivation: Existing blind face restoration methods struggle to balance visual quality and identity preservation, resulting in either identity distortion or poor degradation removal.

Method: The framework decomposes the restoration process into three sub-tasks: restoring identity-preserved faces, generating high-quality faces, and dynamically fusing identity features with realistic textures. Key modules include deformable face registration, texture-guided restoration, and integration of deep metric learning.

Result: CodeFormer++ shows improved performance on real-world and synthetic datasets, excelling in both image fidelity and identity consistency.

Conclusion: The proposed approach delivers a better blend of generative priors for balancing visual quality with identity fidelity in face restoration tasks.

Abstract: Blind face restoration (BFR) has attracted increasing attention with the rise
of generative methods. Most existing approaches integrate generative priors
into the restoration pro- cess, aiming to jointly address facial detail
generation and identity preservation. However, these methods often suffer from
a trade-off between visual quality and identity fidelity, leading to either
identity distortion or suboptimal degradation removal. In this paper, we
present CodeFormer++, a novel framework that maximizes the utility of
generative priors for high-quality face restoration while preserving identity.
We decompose BFR into three sub-tasks: (i) identity- preserving face
restoration, (ii) high-quality face generation, and (iii) dynamic fusion of
identity features with realistic texture details. Our method makes three key
contributions: (1) a learning-based deformable face registration module that
semantically aligns generated and restored faces; (2) a texture guided
restoration network to dynamically extract and transfer the texture of
generated face to boost the quality of identity-preserving restored face; and
(3) the integration of deep metric learning for BFR with the generation of
informative positive and hard negative samples to better fuse identity-
preserving and generative features. Extensive experiments on real-world and
synthetic datasets demonstrate that, the pro- posed CodeFormer++ achieves
superior performance in terms of both visual fidelity and identity consistency.

</details>


### [315] [A.I.R.: Enabling Adaptive, Iterative, and Reasoning-based Frame Selection For Video Question Answering](https://arxiv.org/abs/2510.04428)
*Yuanhao Zou,Shengji Jin,Andong Deng,Youpeng Zhao,Jun Wang,Chen Chen*

Main category: cs.CV

TL;DR: Introducing A.I.R., a training-free method for efficient and accurate video frame selection in VideoQA, leveraging adaptive iterations and reasoning using Vision-Language Models (VLMs).


<details>
  <summary>Details</summary>
Motivation: Current frame selection techniques either fail to capture complex query details or are computationally expensive, limiting their effectiveness in VideoQA tasks.

Method: A.I.R. uses a powerful VLM for deep semantic analysis on complex queries, combined with a cost-effective iterative loop that processes small batches of high-potential frames without requiring training.

Result: Experiments reveal A.I.R.'s superiority in both accuracy and computational efficiency, outperforming existing methods in VideoQA benchmarks.

Conclusion: A.I.R. enhances the performance of VLMs in VideoQA while addressing key limitations in frame selection methods, demonstrating it as a computationally efficient and accurate solution.

Abstract: Effectively applying Vision-Language Models (VLMs) to Video Question
Answering (VideoQA) hinges on selecting a concise yet comprehensive set of
frames, as processing entire videos is computationally infeasible. However,
current frame selection methods face a critical trade-off: approaches relying
on lightweight similarity models, such as CLIP, often fail to capture the
nuances of complex queries, resulting in inaccurate similarity scores that
cannot reflect the authentic query-frame relevance, which further undermines
frame selection. Meanwhile, methods that leverage a VLM for deeper analysis
achieve higher accuracy but incur prohibitive computational costs. To address
these limitations, we propose A.I.R., a training-free approach for Adaptive,
Iterative, and Reasoning-based frame selection. We leverage a powerful VLM to
perform deep, semantic analysis on complex queries, and this analysis is
deployed within a cost-effective iterative loop that processes only a small
batch of the most high-potential frames at a time. Extensive experiments on
various VideoQA benchmarks demonstrate that our approach outperforms existing
frame selection methods, significantly boosts the performance of the foundation
VLM, and achieves substantial gains in computational efficiency over other
VLM-based techniques.

</details>


### [316] [REAR: Rethinking Visual Autoregressive Models via Generator-Tokenizer Consistency Regularization](https://arxiv.org/abs/2510.04450)
*Qiyuan He,Yicong Li,Haotian Ye,Jinghao Wang,Xinyao Liao,Pheng-Ann Heng,Stefano Ermon,James Zou,Angela Yao*

Main category: cs.CV

TL;DR: The paper addresses performance gaps in visual autoregressive (AR) generation by proposing a training strategy, reAR, to enhance generator-tokenizer consistency, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the underperformance of visual AR generation models compared to diffusion models, identifying generator-tokenizer inconsistency as a core bottleneck.

Method: The proposed method, reAR, introduces a token-wise regularization strategy to train AR models to better decode tokens. It predicts the visual embedding of current and target tokens under noisy contexts, with no modifications to infrastructure or architecture.

Result: ReAR improves ImageNet performance, reducing gFID from 3.02 to 1.86 and increasing IS to 316.9. With advanced tokenizers, it achieves a gFID of 1.42 using only 177M parameters, rivaling larger diffusion models.

Conclusion: ReAR is a simple yet effective strategy that significantly enhances visual AR model performance, demonstrating it as a viable competitor to larger diffusion models.

Abstract: Visual autoregressive (AR) generation offers a promising path toward unifying
vision and language models, yet its performance remains suboptimal against
diffusion models. Prior work often attributes this gap to tokenizer limitations
and rasterization ordering. In this work, we identify a core bottleneck from
the perspective of generator-tokenizer inconsistency, i.e., the AR-generated
tokens may not be well-decoded by the tokenizer. To address this, we propose
reAR, a simple training strategy introducing a token-wise regularization
objective: when predicting the next token, the causal transformer is also
trained to recover the visual embedding of the current token and predict the
embedding of the target token under a noisy context. It requires no changes to
the tokenizer, generation order, inference pipeline, or external models.
Despite its simplicity, reAR substantially improves performance. On ImageNet,
it reduces gFID from 3.02 to 1.86 and improves IS to 316.9 using a standard
rasterization-based tokenizer. When applied to advanced tokenizers, it achieves
a gFID of 1.42 with only 177M parameters, matching the performance with larger
state-of-the-art diffusion models (675M).

</details>


### [317] [SPEGNet: Synergistic Perception-Guided Network for Camouflaged Object Detection](https://arxiv.org/abs/2510.04472)
*Baber Jan,Saeed Anwar,Aiman H. El-Maleh,Abdul Jabbar Siddiqui,Abdul Bais*

Main category: cs.CV

TL;DR: Camouflaged object detection is improved by SPEGNet, a unified architecture that balances boundary precision and consistency without relying on complex modular components.


<details>
  <summary>Details</summary>
Motivation: Current methods for camouflaged object detection rely on many complex components, increasing computational burden without significant performance gains, and sacrificing fine details.

Method: SPEGNet integrates multi-scale features using channel calibration and spatial enhancement. It employs scale-adaptive edge modulation for progressive multi-resolution refinement.

Result: SPEGNet achieves high performance ($S_\alpha$ scores of 0.887, 0.890, and 0.895 on CAMO, COD10K, and NC4K datasets, respectively) and supports real-time inference.

Conclusion: The unified design of SPEGNet improves camouflaged object detection across a wide range of scales, achieving high efficiency and precision while simplifying the architecture.

Abstract: Camouflaged object detection segments objects with intrinsic similarity and
edge disruption. Current detection methods rely on accumulated complex
components. Each approach adds components such as boundary modules, attention
mechanisms, and multi-scale processors independently. This accumulation creates
a computational burden without proportional gains. To manage this complexity,
they process at reduced resolutions, eliminating fine details essential for
camouflage. We present SPEGNet, addressing fragmentation through a unified
design. The architecture integrates multi-scale features via channel
calibration and spatial enhancement. Boundaries emerge directly from
context-rich representations, maintaining semantic-spatial alignment.
Progressive refinement implements scale-adaptive edge modulation with peak
influence at intermediate resolutions. This design strikes a balance between
boundary precision and regional consistency. SPEGNet achieves 0.887 $S_\alpha$
on CAMO, 0.890 on COD10K, and 0.895 on NC4K, with real-time inference speed.
Our approach excels across scales, from tiny, intricate objects to large,
pattern-similar ones, while handling occlusion and ambiguous boundaries. Code,
model weights, and results are available on
\href{https://github.com/Baber-Jan/SPEGNet}{https://github.com/Baber-Jan/SPEGNet}.

</details>


### [318] [MedCLM: Learning to Localize and Reason via a CoT-Curriculum in Medical Vision-Language Models](https://arxiv.org/abs/2510.04477)
*Soo Yong Kim,Suin Cho,Vincent-Daniel Yun,Gyeongyeon Hwang*

Main category: cs.CV

TL;DR: MedCLM is a pipeline designed for creating and using medical VQA datasets that incorporate structured, step-by-step reasoning, improving diagnostic AI models.


<details>
  <summary>Details</summary>
Motivation: There is a need to bridge clinical diagnostic reasoning with AI in medical imaging, fostering deeper and more contextual patient insights.

Method: The paper introduces MedCLM, which transforms detection datasets into medical VQA data with Chain-of-Thought reasoning. It incorporates an Integrated CoT-Curriculum Strategy with three stages: Easy (lesion boxes for grounding), Medium (implicit localization), and Hard (weakly supervised reasoning).

Result: MedCLM achieves state-of-the-art performance on several medical VQA benchmarks, demonstrating its capability in clinically aligned reasoning tasks.

Conclusion: The study provides a scalable framework for developing medical vision-language models that integrate diagnostic reasoning, advancing medical AI applications.

Abstract: Bridging clinical diagnostic reasoning with AI remains a central challenge in
medical imaging. We introduce MedCLM, an automated pipeline that converts
detection datasets into large-scale medical visual question answering (VQA)
data with Chain-of-Thought (CoT) reasoning by linking lesion boxes to organ
segmentation and structured rationales. These contextual signals enable medical
vision-language models to generate question-answer pairs with step-by-step
reasoning. To utilize this data effectively, we propose an Integrated
CoT-Curriculum Strategy composed of an Easy stage with explicit lesion boxes
for visual grounding, a Medium stage that encourages implicit localization, and
a Hard stage for weakly supervised reasoning. Experimental results demonstrate
that MedCLM attains state-of-the-art performance on several medical VQA
benchmarks, providing a scalable framework for developing clinically aligned
medical vision-language models.

</details>


### [319] [VaseVQA-3D: Benchmarking 3D VLMs on Ancient Greek Pottery](https://arxiv.org/abs/2510.04479)
*Nonghai Zhang,Zeyu Zhang,Jiazi Wang,Yang Zhao,Hao Tang*

Main category: cs.CV

TL;DR: The paper introduces VaseVQA-3D, a dataset for 3D visual question answering on ancient Greek pottery, and develops VaseVLM, a model tailored to this domain, showing significant performance gains.


<details>
  <summary>Details</summary>
Motivation: Current Vision-Language Models underperform in specialized cultural heritage tasks like analyzing 3D vase artifacts due to data scarcity and limited domain knowledge.

Method: The authors created the VaseVQA-3D dataset comprising 664 3D models of Greek vases with related Q&A data and designed VaseVLM, a model enhanced via domain-adaptive training.

Result: Their model outperformed previous state-of-the-art methods on VaseVQA-3D, with a 12.8% improvement in R@1 metrics and a 6.6% gain in lexical similarity.

Conclusion: The study contributes a significant step towards advancing 3D vase artifact analysis, offering novel methodologies for digital heritage preservation.

Abstract: Vision-Language Models (VLMs) have achieved significant progress in
multimodal understanding tasks, demonstrating strong capabilities particularly
in general tasks such as image captioning and visual reasoning. However, when
dealing with specialized cultural heritage domains like 3D vase artifacts,
existing models face severe data scarcity issues and insufficient domain
knowledge limitations. Due to the lack of targeted training data, current VLMs
struggle to effectively handle such culturally significant specialized tasks.
To address these challenges, we propose the VaseVQA-3D dataset, which serves as
the first 3D visual question answering dataset for ancient Greek pottery
analysis, collecting 664 ancient Greek vase 3D models with corresponding
question-answer data and establishing a complete data construction pipeline. We
further develop the VaseVLM model, enhancing model performance in vase artifact
analysis through domain-adaptive training. Experimental results validate the
effectiveness of our approach, where we improve by 12.8% on R@1 metrics and by
6.6% on lexical similarity compared with previous state-of-the-art on the
VaseVQA-3D dataset, significantly improving the recognition and understanding
of 3D vase artifacts, providing new technical pathways for digital heritage
preservation research.

</details>


### [320] [TBStar-Edit: From Image Editing Pattern Shifting to Consistency Enhancement](https://arxiv.org/abs/2510.04483)
*Hao Fang,Zechao Zhan,Weixin Feng,Ziwei Huang,XuBin Li,Tiezheng Ge*

Main category: cs.CV

TL;DR: TBStar-Edit is a specialized image editing model designed for e-commerce that excels in product appearance consistency and layout integrity.


<details>
  <summary>Details</summary>
Motivation: General image editing models often face consistency limitations when applied to e-commerce scenarios.

Method: The paper introduces TBStar-Edit, which involves comprehensive data engineering, hierarchical model architecture, and a two-stage training strategy for enhanced consistency.

Result: TBStar-Edit shows superior performance in both objective metrics (VIE Score) and subjective user preference compared to general-domain models.

Conclusion: TBStar-Edit effectively addresses e-commerce image editing challenges, setting new benchmarks in consistency and fidelity.

Abstract: Recent advances in image generation and editing technologies have enabled
state-of-the-art models to achieve impressive results in general domains.
However, when applied to e-commerce scenarios, these general models often
encounter consistency limitations. To address this challenge, we introduce
TBStar-Edit, an new image editing model tailored for the e-commerce domain.
Through rigorous data engineering, model architecture design and training
strategy, TBStar-Edit achieves precise and high-fidelity image editing while
maintaining the integrity of product appearance and layout. Specifically, for
data engineering, we establish a comprehensive data construction pipeline,
encompassing data collection, construction, filtering, and augmentation, to
acquire high-quality, instruction-following, and strongly consistent editing
data to support model training. For model architecture design, we design a
hierarchical model framework consisting of a base model, pattern shifting
modules, and consistency enhancement modules. For model training, we adopt a
two-stage training strategy to enhance the consistency preservation: first
stage for editing pattern shifting, and second stage for consistency
enhancement. Each stage involves training different modules with separate
datasets. Finally, we conduct extensive evaluations of TBStar-Edit on a
self-proposed e-commerce benchmark, and the results demonstrate that
TBStar-Edit outperforms existing general-domain editing models in both
objective metrics (VIE Score) and subjective user preference.

</details>


### [321] [Asynchronous Denoising Diffusion Models for Aligning Text-to-Image Generation](https://arxiv.org/abs/2510.04504)
*Zijing Hu,Yunze Tong,Fengda Zhang,Junkun Yuan,Jun Xiao,Kun Kuang*

Main category: cs.CV

TL;DR: The paper introduces asynchronous diffusion models to enhance text-to-image alignment by modulating denoising schedules for different pixel regions.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion models struggle to align generated images with input prompts due to synchronous denoising, which limits inter-pixel context sharing.

Method: The proposed framework uses asynchronous pixel-wise denoising by assigning distinct timesteps to different pixels, allowing gradual denoising for prompt-related regions.

Result: Asynchronous diffusion models demonstrate significantly better text-to-image alignment across various prompts compared to traditional models.

Conclusion: Introducing asynchronous diffusion improves the alignment quality of generated images with prompts, paving the way for more accurate text-to-image synthesis.

Abstract: Diffusion models have achieved impressive results in generating high-quality
images. Yet, they often struggle to faithfully align the generated images with
the input prompts. This limitation arises from synchronous denoising, where all
pixels simultaneously evolve from random noise to clear images. As a result,
during generation, the prompt-related regions can only reference the unrelated
regions at the same noise level, failing to obtain clear context and ultimately
impairing text-to-image alignment. To address this issue, we propose
asynchronous diffusion models -- a novel framework that allocates distinct
timesteps to different pixels and reformulates the pixel-wise denoising
process. By dynamically modulating the timestep schedules of individual pixels,
prompt-related regions are denoised more gradually than unrelated regions,
thereby allowing them to leverage clearer inter-pixel context. Consequently,
these prompt-related regions achieve better alignment in the final images.
Extensive experiments demonstrate that our asynchronous diffusion models can
significantly improve text-to-image alignment across diverse prompts. The code
repository for this work is available at https://github.com/hu-zijing/AsynDM.

</details>


### [322] [TAG:Tangential Amplifying Guidance for Hallucination-Resistant Diffusion Sampling](https://arxiv.org/abs/2510.04533)
*Hyunmin Cho,Donghoon Ahn,Susung Hong,Jee Eun Kim,Seungryong Kim,Kyong Hwan Jin*

Main category: cs.CV

TL;DR: The paper introduces Tangential Amplifying Guidance (TAG), a method to improve image generation in diffusion models by correcting sampling trajectories without modifying the model architecture.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address semantic inconsistencies and hallucinations in state-of-the-art diffusion models during image generation.

Method: TAG employs trajectory signals directly by amplifying tangential components of estimated scores using an intermediate sample as a projection basis, formalized via Taylor expansion.

Result: TAG effectively reduces inconsistencies and enhances sample quality, steering toward higher-probability regions with minimal computational overhead.

Conclusion: TAG provides a plug-and-play solution that improves diffusion sampling fidelity, offering an efficient and architecture-independent approach to diffusion guidance.

Abstract: Recent diffusion models achieve the state-of-the-art performance in image
generation, but often suffer from semantic inconsistencies or hallucinations.
While various inference-time guidance methods can enhance generation, they
often operate indirectly by relying on external signals or architectural
modifications, which introduces additional computational overhead. In this
paper, we propose Tangential Amplifying Guidance (TAG), a more efficient and
direct guidance method that operates solely on trajectory signals without
modifying the underlying diffusion model. TAG leverages an intermediate sample
as a projection basis and amplifies the tangential components of the estimated
scores with respect to this basis to correct the sampling trajectory. We
formalize this guidance process by leveraging a first-order Taylor expansion,
which demonstrates that amplifying the tangential component steers the state
toward higher-probability regions, thereby reducing inconsistencies and
enhancing sample quality. TAG is a plug-and-play, architecture-agnostic module
that improves diffusion sampling fidelity with minimal computational addition,
offering a new perspective on diffusion guidance.

</details>


### [323] [Conditional Representation Learning for Customized Tasks](https://arxiv.org/abs/2510.04564)
*Honglin Liu,Chao Sun,Peng Hu,Yunfan Li,Xi Peng*

Main category: cs.CV

TL;DR: The paper introduces Conditional Representation Learning (CRL) to generate task-specific representations by utilizing descriptive semantic criteria, addressing limitations in universal embedding approaches.


<details>
  <summary>Details</summary>
Motivation: Current universal representation methods focus on dominant semantics, often misaligned with specific downstream tasks, necessitating an efficient alternative to supervised fine-tuning.

Method: CRL uses large language models to generate textual descriptions defining custom semantic spaces. Vision-language models project image representations into these tailored spaces based on user-defined criteria.

Result: Experiments demonstrate that CRL outperforms existing methods in classification and retrieval tasks, showcasing its applicability across diverse customized use cases.

Conclusion: CRL provides a cost-effective and efficient method for creating task-specific representations, bridging the gap between universal embedding limitations and specific downstream requirements.

Abstract: Conventional representation learning methods learn a universal representation
that primarily captures dominant semantics, which may not always align with
customized downstream tasks. For instance, in animal habitat analysis,
researchers prioritize scene-related features, whereas universal embeddings
emphasize categorical semantics, leading to suboptimal results. As a solution,
existing approaches resort to supervised fine-tuning, which however incurs high
computational and annotation costs. In this paper, we propose Conditional
Representation Learning (CRL), aiming to extract representations tailored to
arbitrary user-specified criteria. Specifically, we reveal that the semantics
of a space are determined by its basis, thereby enabling a set of descriptive
words to approximate the basis for a customized feature space. Building upon
this insight, given a user-specified criterion, CRL first employs a large
language model (LLM) to generate descriptive texts to construct the semantic
basis, then projects the image representation into this conditional feature
space leveraging a vision-language model (VLM). The conditional representation
better captures semantics for the specific criterion, which could be utilized
for multiple customized tasks. Extensive experiments on classification and
retrieval tasks demonstrate the superiority and generality of the proposed CRL.
The code is available at https://github.com/XLearning-SCU/2025-NeurIPS-CRL.

</details>


### [324] [Pathology-CoT: Learning Visual Chain-of-Thought Agent from Expert Whole Slide Image Diagnosis Behavior](https://arxiv.org/abs/2510.04587)
*Sheng Wang,Ruiming Wu,Charles Herndon,Yihang Liu,Shunsuke Koga,Jeanne Shen,Zhi Huang*

Main category: cs.CV

TL;DR: The paper introduces a system to record and analyze pathologist behavior during slide navigation, creating a dataset for AI training and enabling a behavior-guided AI system for improved pathology diagnostics.


<details>
  <summary>Details</summary>
Motivation: Current pathology AI systems lack effective and explainable decision-making capabilities due to the absence of data capturing tacit expert behavior during interactive diagnosis processes.

Method: Developing the AI Session Recorder to unobtrusively log standard viewer actions, followed by lightweight human review to create the Pathology-CoT dataset for behavior-based supervision.

Result: Pathologist-o3, an AI system trained on the behavioral data, outperformed state-of-the-art models in gastrointestinal lymph-node metastasis detection, achieving 84.5% precision, 100% recall, and 75.4% accuracy.

Conclusion: This behavior-grounded framework turns viewer logs into scalable, human-validated supervision data, paving the way for aligned and practical agentic pathology AI systems.

Abstract: Diagnosing a whole-slide image is an interactive, multi-stage process
involving changes in magnification and movement between fields. Although recent
pathology foundation models are strong, practical agentic systems that decide
what field to examine next, adjust magnification, and deliver explainable
diagnoses are still lacking. The blocker is data: scalable, clinically aligned
supervision of expert viewing behavior that is tacit and experience-based, not
written in textbooks or online, and therefore absent from large language model
training. We introduce the AI Session Recorder, which works with standard WSI
viewers to unobtrusively record routine navigation and convert the viewer logs
into standardized behavioral commands (inspect or peek at discrete
magnifications) and bounding boxes. A lightweight human-in-the-loop review
turns AI-drafted rationales into the Pathology-CoT dataset, a form of paired
"where to look" and "why it matters" supervision produced at roughly six times
lower labeling time. Using this behavioral data, we build Pathologist-o3, a
two-stage agent that first proposes regions of interest and then performs
behavior-guided reasoning. On gastrointestinal lymph-node metastasis detection,
it achieved 84.5% precision, 100.0% recall, and 75.4% accuracy, exceeding the
state-of-the-art OpenAI o3 model and generalizing across backbones. To our
knowledge, this constitutes one of the first behavior-grounded agentic systems
in pathology. Turning everyday viewer logs into scalable, expert-validated
supervision, our framework makes agentic pathology practical and establishes a
path to human-aligned, upgradeable clinical AI.

</details>


### [325] [A Spatial-Spectral-Frequency Interactive Network for Multimodal Remote Sensing Classification](https://arxiv.org/abs/2510.04628)
*Hao Liu,Yunhao Gao,Wei Li,Mingyang Zhang,Maoguo Gong,Lorenzo Bruzzone*

Main category: cs.CV

TL;DR: This paper introduces S$^2$Fin, a deep learning model for multimodal remote sensing image classification that uses spatial, spectral, and frequency domain fusion to enhance feature extraction, showing superior performance on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: Existing feature fusion techniques struggle to extract structural and detailed features from heterogeneous and redundant multimodal images.

Method: The proposed method uses the spatial-spectral-frequency interaction network (S$^2$Fin) with features like a high-frequency sparse enhancement transformer, adaptive frequency channel module, high-frequency resonance mask, and spatial-spectral attention fusion module for optimized feature extraction.

Result: Experiments on four multimodal datasets with limited labeled data show that S$^2$Fin outperforms state-of-the-art methods in classification accuracy.

Conclusion: The S$^2$Fin model enhances multimodal remote sensing image classification by integrating spatial, spectral, and frequency domain features, providing a significant advancement over current techniques and achieving superior performance on limited labeled data.

Abstract: Deep learning-based methods have achieved significant success in remote
sensing Earth observation data analysis. Numerous feature fusion techniques
address multimodal remote sensing image classification by integrating global
and local features. However, these techniques often struggle to extract
structural and detail features from heterogeneous and redundant multimodal
images. With the goal of introducing frequency domain learning to model key and
sparse detail features, this paper introduces the spatial-spectral-frequency
interaction network (S$^2$Fin), which integrates pairwise fusion modules across
the spatial, spectral, and frequency domains. Specifically, we propose a
high-frequency sparse enhancement transformer that employs sparse
spatial-spectral attention to optimize the parameters of the high-frequency
filter. Subsequently, a two-level spatial-frequency fusion strategy is
introduced, comprising an adaptive frequency channel module that fuses
low-frequency structures with enhanced high-frequency details, and a
high-frequency resonance mask that emphasizes sharp edges via phase similarity.
In addition, a spatial-spectral attention fusion module further enhances
feature extraction at intermediate layers of the network. Experiments on four
benchmark multimodal datasets with limited labeled data demonstrate that
S$^2$Fin performs superior classification, outperforming state-of-the-art
methods. The code is available at https://github.com/HaoLiu-XDU/SSFin.

</details>


### [326] [SFANet: Spatial-Frequency Attention Network for Deepfake Detection](https://arxiv.org/abs/2510.04630)
*Vrushank Ahire,Aniruddh Muley,Shivam Zample,Siddharth Verma,Pranav Menon,Surbhi Madan,Abhinav Dhall*

Main category: cs.CV

TL;DR: The paper proposes a hybrid ensemble framework that combines transformer-based architectures with texture-based methods to improve deepfake detection performance and robustness.


<details>
  <summary>Details</summary>
Motivation: Traditional methods for deepfake detection often fail to generalize across datasets and generation techniques.

Method: The authors combined Swin Transformers, ViTs, and texture-based methods, introducing techniques like data splitting, sequential training, frequency splitting, patch-based attention, and face segmentation.

Result: Their model achieved state-of-the-art performance on the DFWild-Cup dataset, showcasing improved accuracy and generalization.

Conclusion: The hybrid approach effectively addresses deepfake detection challenges, offering a robust solution for real-world use.

Abstract: Detecting manipulated media has now become a pressing issue with the recent
rise of deepfakes. Most existing approaches fail to generalize across diverse
datasets and generation techniques. We thus propose a novel ensemble framework,
combining the strengths of transformer-based architectures, such as Swin
Transformers and ViTs, and texture-based methods, to achieve better detection
accuracy and robustness. Our method introduces innovative data-splitting,
sequential training, frequency splitting, patch-based attention, and face
segmentation techniques to handle dataset imbalances, enhance high-impact
regions (e.g., eyes and mouth), and improve generalization. Our model achieves
state-of-the-art performance when tested on the DFWild-Cup dataset, a diverse
subset of eight deepfake datasets. The ensemble benefits from the
complementarity of these approaches, with transformers excelling in global
feature extraction and texturebased methods providing interpretability. This
work demonstrates that hybrid models can effectively address the evolving
challenges of deepfake detection, offering a robust solution for real-world
applications.

</details>


### [327] [Do Superpixel Segmentation Methods Influence Deforestation Image Classification?](https://arxiv.org/abs/2510.04645)
*Hugo Resende,Fabio A. Faria,Eduardo B. Neto,Isabela Borlido,Victor Sundermann,Silvio Jamil F. Guimarães,Álvaro L. Fazenda*

Main category: cs.CV

TL;DR: This study evaluates the performance of five segmentation methods (including SLIC) on classifier training for deforestation detection, showing improvements with ensemble learning approaches.


<details>
  <summary>Details</summary>
Motivation: To determine the most effective image segmentation methods for detecting tropical deforestation using citizen science and machine learning.

Method: The performance of five segmentation methods was compared using classifiers, which were optimized through the PyCaret AutoML library, and ensemble learning approaches were utilized to improve accuracy.

Result: Segmentations showed little performance variation individually, but classifier fusion techniques significantly improved balanced accuracy.

Conclusion: Optimal segmentation methods combined with ensemble learning enhance deforestation detection, underlining their joint importance for such applications.

Abstract: Image segmentation is a crucial step in various visual applications,
including environmental monitoring through remote sensing. In the context of
the ForestEyes project, which combines citizen science and machine learning to
detect deforestation in tropical forests, image segments are used for labeling
by volunteers and subsequent model training. Traditionally, the Simple Linear
Iterative Clustering (SLIC) algorithm is adopted as the segmentation method.
However, recent studies have indicated that other superpixel-based methods
outperform SLIC in remote sensing image segmentation, and might suggest that
they are more suitable for the task of detecting deforested areas. In this
sense, this study investigated the impact of the four best segmentation
methods, together with SLIC, on the training of classifiers for the target
application. Initially, the results showed little variation in performance
among segmentation methods, even when selecting the top five classifiers using
the PyCaret AutoML library. However, by applying a classifier fusion approach
(ensemble of classifiers), noticeable improvements in balanced accuracy were
observed, highlighting the importance of both the choice of segmentation method
and the combination of machine learning-based models for deforestation
detection tasks.

</details>


### [328] [EduPersona: Benchmarking Subjective Ability Boundaries of Virtual Student Agents](https://arxiv.org/abs/2510.04648)
*Buyuan Zhu,Shiyu Hu,Yiping Ma,Yuanming Zhang,Kang Hao Cheong*

Main category: cs.CV

TL;DR: The paper introduces EduPersona, a large-scale benchmark for assessing classroom-oriented subjective abilities of language models, covering coherence, realism, and persona consistency. It demonstrates significant advancements in persona modeling and aims to aid trustworthy AI for education.


<details>
  <summary>Details</summary>
Motivation: To address the lack of assessment tools for classroom-oriented subjective abilities of virtual student agents, which hinders trustworthy deployment and understanding of model limitations in education.

Method: Developed EduPersona, a multilingual and multi-subject dataset with 1,308 authentic classroom dialogues expanded through persona stylization. Established an evaluation framework with three tasks: coherence, realism, and persona consistency. Conducted experiments on three language models with fine-tuned variants.

Result: The EduPersona-trained models showed significant improvements in performance: +33.6% in coherence, +30.6% in realism, and +14.9% in long-term persona consistency, demonstrating the benchmark's effectiveness.

Conclusion: EduPersona offers a foundational resource for evaluating subjective classroom abilities in AI, delivers measurable and verifiable advancements, and supports broader research by open-sourcing the dataset and framework for educational AI development.

Abstract: As large language models are increasingly integrated into education, virtual
student agents are becoming vital for classroom simulation and teacher
training. Yet their classroom-oriented subjective abilities remain largely
unassessed, limiting understanding of model boundaries and hindering
trustworthy deployment. We present EduPersona, a large-scale benchmark spanning
two languages, three subjects, and ten persona types based on the Big Five
theory. The dataset contains 1,308 authentic classroom dialogue rounds,
corresponding to 12,814 teacher-student Q&A turns, and is further expanded
through persona stylization into roughly 10 times larger scale (128k turns),
providing a solid foundation for evaluation. Building on this resource, we
decompose hard-to-quantify subjective performance into three progressive tasks:
TASK1 basic coherence (whether behavior, emotion, expression, and voice align
with classroom context), TASK2 student realism, and TASK3 long-term persona
consistency, thereby establishing an evaluation framework grounded in
educational theory and research value. We conduct systematic experiments on
three representative LLMs, comparing their original versions with ten
persona-fine-tuned variants trained on EduPersona. Results show consistent and
significant average improvements across all tasks: TASK1 +33.6%, TASK2 +30.6%,
and TASK3 +14.9%. These improvements highlight the dataset's effectiveness and
research value, while also revealing the heterogeneous difficulty of persona
modeling. In summary, EduPersona delivers the first classroom benchmark
centered on subjective abilities, establishes a decoupled and verifiable
research paradigm, and we will open-source both the dataset and the framework
to support the broader research community in advancing trustworthy and
human-like AI for education.

</details>


### [329] [MoME: Estimating Psychological Traits from Gait with Multi-Stage Mixture of Movement Experts](https://arxiv.org/abs/2510.04654)
*Andy Cǎtrunǎ,Adrian Cosma,Emilian Rǎdoi*

Main category: cs.CV

TL;DR: The study presents a Multi-Stage Mixture of Movement Experts (MoME) method to predict psychological traits from gait sequences, achieving superior performance compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Utilizing gait patterns to infer psychological traits is a challenging field with untapped potential, providing motivation for research.

Method: The paper introduces a hierarchical MoME architecture, using lightweight expert models and gating mechanisms in a four-stage process to analyze gait sequences and predict traits.

Result: On the PsyMo benchmark covering 17 psychological traits, MoME achieves F1 scores of 37.47% at the run level and 44.6% at the subject level.

Conclusion: The study demonstrates the feasibility of gait-based multi-task learning for psychological trait estimation and sets groundwork for future advancements in movement-informed psychological inference.

Abstract: Gait encodes rich biometric and behavioural information, yet leveraging the
manner of walking to infer psychological traits remains a challenging and
underexplored problem. We introduce a hierarchical Multi-Stage Mixture of
Movement Experts (MoME) architecture for multi-task prediction of psychological
attributes from gait sequences represented as 2D poses. MoME processes the
walking cycle in four stages of movement complexity, employing lightweight
expert models to extract spatio-temporal features and task-specific gating
modules to adaptively weight experts across traits and stages. Evaluated on the
PsyMo benchmark covering 17 psychological traits, our method outperforms
state-of-the-art gait analysis models, achieving a 37.47% weighted F1 score at
the run level and 44.6% at the subject level. Our experiments show that
integrating auxiliary tasks such as identity recognition, gender prediction,
and BMI estimation further improves psychological trait estimation. Our
findings demonstrate the viability of multi-task gait-based learning for
psychological trait estimation and provide a foundation for future research on
movement-informed psychological inference.

</details>


### [330] [ConceptSplit: Decoupled Multi-Concept Personalization of Diffusion Models via Token-wise Adaptation and Attention Disentanglement](https://arxiv.org/abs/2510.04668)
*Habin Lim,Yeongseob Won,Juwon Seo,Gyeong-Moon Park*

Main category: cs.CV

TL;DR: This paper introduces ConceptSplit, a method to improve multi-concept personalization in text-to-image models by addressing concept mixing through innovative training and inference techniques.


<details>
  <summary>Details</summary>
Motivation: The paper aims to solve the challenge of 'concept mixing' in text-to-image diffusion models, where multiple learned concepts undesirably interfere with one another.

Method: The proposed framework consists of Token-wise Value Adaptation (ToVA) for training, which avoids modifying the key projection, and Latent Optimization for Disentangled Attention (LODA) during inference to reduce attention entanglement.

Result: Extensive experiments show that ConceptSplit successfully mitigates unintended concept interference and produces robust multi-concept personalized output.

Conclusion: ConceptSplit represents a significant advance in ensuring that multiple concepts can coexist effectively in generated images, addressing key issues in current T2I models.

Abstract: In recent years, multi-concept personalization for text-to-image (T2I)
diffusion models to represent several subjects in an image has gained much more
attention. The main challenge of this task is "concept mixing", where multiple
learned concepts interfere or blend undesirably in the output image. To address
this issue, in this paper, we present ConceptSplit, a novel framework to split
the individual concepts through training and inference. Our framework comprises
two key components. First, we introduce Token-wise Value Adaptation (ToVA), a
merging-free training method that focuses exclusively on adapting the value
projection in cross-attention. Based on our empirical analysis, we found that
modifying the key projection, a common approach in existing methods, can
disrupt the attention mechanism and lead to concept mixing. Second, we propose
Latent Optimization for Disentangled Attention (LODA), which alleviates
attention entanglement during inference by optimizing the input latent. Through
extensive qualitative and quantitative experiments, we demonstrate that
ConceptSplit achieves robust multi-concept personalization, mitigating
unintended concept interference. Code is available at
https://github.com/KU-VGI/ConceptSplit

</details>


### [331] [Label-Efficient Cross-Modality Generalization for Liver Segmentation in Multi-Phase MRI](https://arxiv.org/abs/2510.04705)
*Quang-Khai Bui-Tran,Minh-Toan Dinh,Thanh-Huy Nguyen,Ba-Thinh Lam,Mai-Anh Vu,Ulas Bagci*

Main category: cs.CV

TL;DR: This paper introduces a novel method for accurate liver segmentation in multi-phase MRI under real-world conditions with limited labeled data, emphasizing cross-modality generalization.


<details>
  <summary>Details</summary>
Motivation: Liver segmentation in MRI is critical for assessing liver fibrosis, but labeled data is scarce and varies across imaging modalities and systems.

Method: The paper uses a fine-tuned foundation-scale 3D segmentation backbone, co-training with cross pseudo supervision for unlabeled data, and a standardized preprocessing pipeline.

Result: The proposed model achieves strong segmentation performance across labeled and unlabeled MRI modalities and vendor systems, even without spatial registration.

Conclusion: Combining foundation model adaptation with co-training is effective for liver segmentation in complex real-world clinical MRI setups.

Abstract: Accurate liver segmentation in multi-phase MRI is vital for liver fibrosis
assessment, yet labeled data is often scarce and unevenly distributed across
imaging modalities and vendor systems. We propose a label-efficient
segmentation approach that promotes cross-modality generalization under
real-world conditions, where GED4 hepatobiliary-phase annotations are limited,
non-contrast sequences (T1WI, T2WI, DWI) are unlabeled, and spatial
misalignment and missing phases are common. Our method integrates a
foundation-scale 3D segmentation backbone adapted via fine-tuning, co-training
with cross pseudo supervision to leverage unlabeled volumes, and a standardized
preprocessing pipeline. Without requiring spatial registration, the model
learns to generalize across MRI phases and vendors, demonstrating robust
segmentation performance in both labeled and unlabeled domains. Our results
exhibit the effectiveness of our proposed label-efficient baseline for liver
segmentation in multi-phase, multi-vendor MRI and highlight the potential of
combining foundation model adaptation with co-training for real-world clinical
imaging tasks.

</details>


### [332] [ID-Consistent, Precise Expression Generation with Blendshape-Guided Diffusion](https://arxiv.org/abs/2510.04706)
*Foivos Paraperas Papantoniou,Stefanos Zafeiriou*

Main category: cs.CV

TL;DR: The paper introduces a diffusion-based framework for AI-driven storytelling that effectively maintains identity consistency while allowing fine-grained control over facial expressions in reimagined subjects.


<details>
  <summary>Details</summary>
Motivation: Current human-centric generative models struggle to achieve both identity consistency and precise control over human performance, particularly facial expressions, which is critical for storytelling applications.

Method: The method involves an identity-consistent face foundation model with a compositional design, featuring an expression cross-attention module guided by FLAME blendshape parameters. Additionally, a Reference Adapter is introduced for expression editing in real images.

Result: The model outperforms existing methods in identity-consistent and tailored expression generation, handling subtle micro-expressions and expressive transitions with high fidelity.

Conclusion: The proposed framework enables precise and identity-consistent expression generation, addressing limitations of prior works and advancing AI-driven storytelling.

Abstract: Human-centric generative models designed for AI-driven storytelling must
bring together two core capabilities: identity consistency and precise control
over human performance. While recent diffusion-based approaches have made
significant progress in maintaining facial identity, achieving fine-grained
expression control without compromising identity remains challenging. In this
work, we present a diffusion-based framework that faithfully reimagines any
subject under any particular facial expression. Building on an ID-consistent
face foundation model, we adopt a compositional design featuring an expression
cross-attention module guided by FLAME blendshape parameters for explicit
control. Trained on a diverse mixture of image and video data rich in
expressive variation, our adapter generalizes beyond basic emotions to subtle
micro-expressions and expressive transitions, overlooked by prior works. In
addition, a pluggable Reference Adapter enables expression editing in real
images by transferring the appearance from a reference frame during synthesis.
Extensive quantitative and qualitative evaluations show that our model
outperforms existing methods in tailored and identity-consistent expression
generation. Code and models can be found at
https://github.com/foivospar/Arc2Face.

</details>


### [333] [ReactDiff: Fundamental Multiple Appropriate Facial Reaction Diffusion Model](https://arxiv.org/abs/2510.04712)
*Luo Cheng,Song Siyang,Yan Siyuan,Yu Zhen,Ge Zongyuan*

Main category: cs.CV

TL;DR: The study introduces ReactDiff, a novel framework leveraging spatio-temporal facial kinematics to generate human-like and diverse facial reactions in conversational contexts.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of generating realistic, diverse, and smooth facial reactions in human-computer interactions, overcoming the limitations of existing methods.

Method: ReactDiff integrates spatio-temporal facial kinematics into a temporal diffusion model, focusing on two key priors: facial behavioral dynamics over time and facial anatomy constraints.

Result: ReactDiff achieved state-of-the-art quality in facial reactions, demonstrating superior diversity and appropriateness when tested on the REACT2024 dataset.

Conclusion: ReactDiff effectively generates plausible human-like facial reactions in dialogue, making it a promising solution for enhancing human-computer interaction systems.

Abstract: The automatic generation of diverse and human-like facial reactions in dyadic
dialogue remains a critical challenge for human-computer interaction systems.
Existing methods fail to model the stochasticity and dynamics inherent in real
human reactions. To address this, we propose ReactDiff, a novel temporal
diffusion framework for generating diverse facial reactions that are
appropriate for responding to any given dialogue context. Our key insight is
that plausible human reactions demonstrate smoothness, and coherence over time,
and conform to constraints imposed by human facial anatomy. To achieve this,
ReactDiff incorporates two vital priors (spatio-temporal facial kinematics)
into the diffusion process: i) temporal facial behavioral kinematics and ii)
facial action unit dependencies. These two constraints guide the model toward
realistic human reaction manifolds, avoiding visually unrealistic jitters,
unstable transitions, unnatural expressions, and other artifacts. Extensive
experiments on the REACT2024 dataset demonstrate that our approach not only
achieves state-of-the-art reaction quality but also excels in diversity and
reaction appropriateness.

</details>


### [334] [Object-Centric Representation Learning for Enhanced 3D Scene Graph Prediction](https://arxiv.org/abs/2510.04714)
*KunHo Heo,GiHyun Kim,SuYeon Kim,MyeongAh Cho*

Main category: cs.CV

TL;DR: This paper focuses on improving accuracy in predicting 3D semantic scene graphs by enhancing object feature encoding and integrating geometric and semantic relationship features.


<details>
  <summary>Details</summary>
Motivation: Previous methods show limitations in optimizing object and relationship features, overly relying on Graph Neural Networks without achieving sufficient discriminative capability.

Method: The authors propose a discriminative object feature encoder and employ a contrastive pretraining strategy that separates object representation learning from scene graph prediction. They also integrate geometric and semantic features for better relationship prediction.

Result: Substantial performance improvements across all metrics were observed, especially when integrating the proposed pretrained encoder into existing frameworks. The method consistently outperforms state-of-the-art approaches on the 3DSSG dataset.

Conclusion: High-quality object representation significantly impacts overall scene graph accuracy, and combining semantic and geometric relationship information enhances predictions. The approach establishes a new benchmark in 3D semantic scene graph accuracy.

Abstract: 3D Semantic Scene Graph Prediction aims to detect objects and their semantic
relationships in 3D scenes, and has emerged as a crucial technology for
robotics and AR/VR applications. While previous research has addressed dataset
limitations and explored various approaches including Open-Vocabulary settings,
they frequently fail to optimize the representational capacity of object and
relationship features, showing excessive reliance on Graph Neural Networks
despite insufficient discriminative capability. In this work, we demonstrate
through extensive analysis that the quality of object features plays a critical
role in determining overall scene graph accuracy. To address this challenge, we
design a highly discriminative object feature encoder and employ a contrastive
pretraining strategy that decouples object representation learning from the
scene graph prediction. This design not only enhances object classification
accuracy but also yields direct improvements in relationship prediction.
Notably, when plugging in our pretrained encoder into existing frameworks, we
observe substantial performance improvements across all evaluation metrics.
Additionally, whereas existing approaches have not fully exploited the
integration of relationship information, we effectively combine both geometric
and semantic features to achieve superior relationship prediction.
Comprehensive experiments on the 3DSSG dataset demonstrate that our approach
significantly outperforms previous state-of-the-art methods. Our code is
publicly available at https://github.com/VisualScienceLab-KHU/OCRL-3DSSG-Codes.

</details>


### [335] [Benchmark on Monocular Metric Depth Estimation in Wildlife Setting](https://arxiv.org/abs/2510.04723)
*Niccolò Niccoli,Lorenzo Seidenari,Ilaria Greco,Francesco Rovero*

Main category: cs.CV

TL;DR: This study benchmarks monocular depth estimation methods for wildlife monitoring using camera trap images, highlighting Depth Anything V2 as the most accurate and balanced method.


<details>
  <summary>Details</summary>
Motivation: The challenge of extracting accurate distance measurements from monocular camera trap images is compounded due to no depth information and the lack of systematic evaluation of depth estimation methods in wildlife environments.

Method: Researchers tested four state-of-the-art monocular depth estimation methods and a geometric baseline, using 93 camera trap images with ground truth distances derived from calibrated ChARUCO patterns.

Result: Depth Anything V2 achieved the highest performance with a mean absolute error of 0.454m and correlation of 0.962, while ZoeDepth degraded significantly in natural outdoor environments. Median-based approaches consistently surpassed mean-based ones across all methods.

Conclusion: The paper establishes a benchmark for selecting monocular depth estimation tools in wildlife conservation, recommending Depth Anything V2 for its optimal balance of accuracy and computational efficiency.

Abstract: Camera traps are widely used for wildlife monitoring, but extracting accurate
distance measurements from monocular images remains challenging due to the lack
of depth information. While monocular depth estimation (MDE) methods have
advanced significantly, their performance in natural wildlife environments has
not been systematically evaluated. This work introduces the first benchmark for
monocular metric depth estimation in wildlife monitoring conditions. We
evaluate four state-of-the-art MDE methods (Depth Anything V2, ML Depth Pro,
ZoeDepth, and Metric3D) alongside a geometric baseline on 93 camera trap images
with ground truth distances obtained using calibrated ChARUCO patterns. Our
results demonstrate that Depth Anything V2 achieves the best overall
performance with a mean absolute error of 0.454m and correlation of 0.962,
while methods like ZoeDepth show significant degradation in outdoor natural
environments (MAE: 3.087m). We find that median-based depth extraction
consistently outperforms mean-based approaches across all deep learning
methods. Additionally, we analyze computational efficiency, with ZoeDepth being
fastest (0.17s per image) but least accurate, while Depth Anything V2 provides
an optimal balance of accuracy and speed (0.22s per image). This benchmark
establishes performance baselines for wildlife applications and provides
practical guidance for implementing depth estimation in conservation monitoring
systems.

</details>


### [336] [ExposureEngine: Oriented Logo Detection and Sponsor Visibility Analytics in Sports Broadcasts](https://arxiv.org/abs/2510.04739)
*Mehdi Houshmand Sarkhoosh,Frøy Øye,Henrik Nestor Sørlie,Nam Hoang Vu,Dag Johansen,Cise Midoglu,Tomas Kupka,Pål Halvorsen*

Main category: cs.CV

TL;DR: ExposureEngine introduces advanced rotation-aware sponsor visibility metrics in sports broadcasts using Oriented Bounding Boxes (OBBs).


<details>
  <summary>Details</summary>
Motivation: Existing methods for sponsor visibility analysis rely on manual or Horizontal Bounding Box approaches, which fail to accommodate rotated or skewed logos in dynamic broadcast conditions.

Method: ExposureEngine employs OBB-based logo detection, trained on a custom dataset, and integrates analytics tools with natural language-driven reporting capabilities.

Result: The reported system achieves high precision (0.96), recall (0.87), and mAP@0.5 (0.859), effectively localizing logos under varied broadcast conditions.

Conclusion: ExposureEngine is a robust, scalable, and interpretable solution that advances sponsor visibility analytics, providing precise metrics and language-driven reports.

Abstract: Quantifying sponsor visibility in sports broadcasts is a critical marketing
task traditionally hindered by manual, subjective, and unscalable analysis
methods. While automated systems offer an alternative, their reliance on
axis-aligned Horizontal Bounding Box (HBB) leads to inaccurate exposuremetrics
when logos appear rotated or skewed due to dynamic camera angles and
perspective distortions. This paper introduces ExposureEngine, an end-to-end
system designed for accurate, rotation-aware sponsor visibility analytics in
sports broadcasts, demonstrated in a soccer case study. Our approach predicts
Oriented Bounding Box (OBB) to provide a geometrically precise fit to each logo
regardless of the orientation on-screen. To train and evaluate our detector, we
developed a new dataset comprising 1,103 frames from Swedish elite soccer,
featuring 670 unique sponsor logos annotated with OBBs. Our model achieves a
mean Average Precision (mAP@0.5) of 0.859, with a precision of 0.96 and recall
of 0.87, demonstrating robust performance in localizing logos under diverse
broadcast conditions. The system integrates these detections into an analytical
pipeline that calculates precise visibility metrics, such as exposure duration
and on-screen coverage. Furthermore, we incorporate a language-driven agentic
layer, enabling users to generate reports, summaries, and media content through
natural language queries. The complete system, including the dataset and the
analytics dashboard, provides a comprehensive solution for auditable and
interpretable sponsor measurement in sports media. An overview of the
ExposureEngine is available online: https://youtu.be/tRw6OBISuW4 .

</details>


### [337] [Anomaly-Aware YOLO: A Frugal yet Robust Approach to Infrared Small Target Detection](https://arxiv.org/abs/2510.04741)
*Alina Ciocarlan,Sylvie Le Hégarat-Mascle,Sidonie Lefebvre*

Main category: cs.CV

TL;DR: The paper introduces Anomaly-Aware YOLO (AA-YOLO), which uses statistical anomaly detection for detecting small infrared targets against complex backgrounds. It reduces false alarms and adapts well to diverse contexts.


<details>
  <summary>Details</summary>
Motivation: To improve the detection accuracy of small infrared targets in defense applications, overcoming limitations of conventional methods prone to false alarms due to complex backgrounds.

Method: The method involves integrating a statistical anomaly detection test into the YOLO detection head, categorizing small targets as anomalies compared to their background.

Result: AA-YOLO shows competitive performance across multiple benchmarks and remains robust under limited training data, noise, and domain shifts.

Conclusion: AA-YOLO is a versatile and resource-efficient detection approach, easily adaptable across YOLO backbones and effective in constrained real-world scenarios. The code will be made publicly accessible.

Abstract: Infrared Small Target Detection (IRSTD) is a challenging task in defense
applications, where complex backgrounds and tiny target sizes often result in
numerous false alarms using conventional object detectors. To overcome this
limitation, we propose Anomaly-Aware YOLO (AA-YOLO), which integrates a
statistical anomaly detection test into its detection head. By treating small
targets as unexpected patterns against the background, AA-YOLO effectively
controls the false alarm rate. Our approach not only achieves competitive
performance on several IRSTD benchmarks, but also demonstrates remarkable
robustness in scenarios with limited training data, noise, and domain shifts.
Furthermore, since only the detection head is modified, our design is highly
generic and has been successfully applied across various YOLO backbones,
including lightweight models. It also provides promising results when
integrated into an instance segmentation YOLO. This versatility makes AA-YOLO
an attractive solution for real-world deployments where resources are
constrained. The code will be publicly released.

</details>


### [338] [Beyond Appearance: Transformer-based Person Identification from Conversational Dynamics](https://arxiv.org/abs/2510.04753)
*Masoumeh Chapariniya,Teodora Vukovic,Sarah Ebling,Volker Dellwo*

Main category: cs.CV

TL;DR: This study explores transformer-based architectures for identifying individuals in natural conversations, using spatial and motion data. It achieves up to 98.03% accuracy by combining complementary features.


<details>
  <summary>Details</summary>
Motivation: To examine the efficacy of transformer architectures for identifying people in natural, face-to-face interactions.

Method: The paper implements a two-stream framework modeling spatial and temporal patterns of body keypoints from conversation data, comparing pre-trained models and from-scratch training, alongside introducing a multi-scale temporal transformer.

Result: Spatial configurations showed more discriminative power over temporal dynamics, achieving 95.74% and 93.90% accuracy, respectively. Fusion of features further improved performance to 98.03%.

Conclusion: Transformer-based approaches are effective for person identification in natural conversations, offering complementary insights from spatial and temporal features for advancing multimodal studies.

Abstract: This paper investigates the performance of transformer-based architectures
for person identification in natural, face-to-face conversation scenario. We
implement and evaluate a two-stream framework that separately models spatial
configurations and temporal motion patterns of 133 COCO WholeBody keypoints,
extracted from a subset of the CANDOR conversational corpus. Our experiments
compare pre-trained and from-scratch training, investigate the use of velocity
features, and introduce a multi-scale temporal transformer for hierarchical
motion modeling. Results demonstrate that domain-specific training
significantly outperforms transfer learning, and that spatial configurations
carry more discriminative information than temporal dynamics. The spatial
transformer achieves 95.74% accuracy, while the multi-scale temporal
transformer achieves 93.90%. Feature-level fusion pushes performance to 98.03%,
confirming that postural and dynamic information are complementary. These
findings highlight the potential of transformer architectures for person
identification in natural interactions and provide insights for future
multimodal and cross-cultural studies.

</details>


### [339] [Progressive Gaussian Transformer with Anisotropy-aware Sampling for Open Vocabulary Occupancy Prediction](https://arxiv.org/abs/2510.04759)
*Chi Yan,Dan Xu*

Main category: cs.CV

TL;DR: The paper proposes PG-Occ, a Progressive Gaussian Transformer Framework, for improved open-vocabulary 3D occupancy prediction by addressing limitations of prior sparse and dense models.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the trade-offs involved in text-aligned scene modeling, particularly sparse Gaussian struggles with small objects and dense models suffering from high computational costs, in text-aligned 3D occupancy prediction.

Method: The method introduces a Progressive Gaussian Transformer Framework with online densification to incrementally enhance 3D Gaussian representation and an anisotropy-aware sampling strategy with spatio-temporal fusion for adaptive and effective feature aggregation.

Result: PG-Occ achieves a 14.3% mIoU improvement compared to the best previous methods, demonstrating its effectiveness in detailed and accurate 3D scene understanding.

Conclusion: PG-Occ successfully addresses the issues of sparse and dense text-aligned scene modeling by offering an adaptive, precise, and computationally efficient method for open-vocabulary 3D occupancy prediction.

Abstract: The 3D occupancy prediction task has witnessed remarkable progress in recent
years, playing a crucial role in vision-based autonomous driving systems. While
traditional methods are limited to fixed semantic categories, recent approaches
have moved towards predicting text-aligned features to enable open-vocabulary
text queries in real-world scenes. However, there exists a trade-off in
text-aligned scene modeling: sparse Gaussian representation struggles to
capture small objects in the scene, while dense representation incurs
significant computational overhead. To address these limitations, we present
PG-Occ, an innovative Progressive Gaussian Transformer Framework that enables
open-vocabulary 3D occupancy prediction. Our framework employs progressive
online densification, a feed-forward strategy that gradually enhances the 3D
Gaussian representation to capture fine-grained scene details. By iteratively
enhancing the representation, the framework achieves increasingly precise and
detailed scene understanding. Another key contribution is the introduction of
an anisotropy-aware sampling strategy with spatio-temporal fusion, which
adaptively assigns receptive fields to Gaussians at different scales and
stages, enabling more effective feature aggregation and richer scene
information capture. Through extensive evaluations, we demonstrate that PG-Occ
achieves state-of-the-art performance with a relative 14.3% mIoU improvement
over the previous best performing method. Code and pretrained models will be
released upon publication on our project page:
https://yanchi-3dv.github.io/PG-Occ

</details>


### [340] [Beyond the Seen: Bounded Distribution Estimation for Open-Vocabulary Learning](https://arxiv.org/abs/2510.04770)
*Xiaomeng Fan,Yuchuan Mao,Zhi Gao,Yuwei Wu,Jin Chen,Yunde Jia*

Main category: cs.CV

TL;DR: The paper proposes a new method for open-vocabulary learning by generating unseen-class data to improve distribution estimation, leading to an enhancement in performance by up to 14% compared to baseline techniques.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the issue of unidentifiable estimation errors in open-vocabulary learning when relying only on seen-class data by going beyond seen classes.

Method: The authors propose a method involving a class-domain-wise data generation pipeline using a hierarchical semantic tree and domain information, paired with a distribution alignment algorithm to estimate and maximize posterior probabilities for generalization.

Result: Extensive experiments on 11 datasets show the proposed method outperforms baselines by up to 14%, demonstrating its effectiveness.

Conclusion: The paper concludes that generating unseen-class data and applying distribution alignment significantly improves open-vocabulary learning, ensuring better performance in open environments.

Abstract: Open-vocabulary learning requires modeling the data distribution in open
environments, which consists of both seen-class and unseen-class data.
  Existing methods estimate the distribution in open environments using
seen-class data, where the absence of unseen classes makes the estimation error
inherently unidentifiable.
  Intuitively, learning beyond the seen classes is crucial for distribution
estimation to bound the estimation error.
  We theoretically demonstrate that the distribution can be effectively
estimated by generating unseen-class data, through which the estimation error
is upper-bounded.
  Building on this theoretical insight, we propose a novel open-vocabulary
learning method, which generates unseen-class data for estimating the
distribution in open environments. The method consists of a class-domain-wise
data generation pipeline and a distribution alignment algorithm. The data
generation pipeline generates unseen-class data under the guidance of a
hierarchical semantic tree and domain information inferred from the seen-class
data, facilitating accurate distribution estimation. With the generated data,
the distribution alignment algorithm estimates and maximizes the posterior
probability to enhance generalization in open-vocabulary learning. Extensive
experiments on $11$ datasets demonstrate that our method outperforms baseline
approaches by up to $14\%$, highlighting its effectiveness and superiority.

</details>


### [341] [Federated Learning for Surgical Vision in Appendicitis Classification: Results of the FedSurg EndoVis 2024 Challenge](https://arxiv.org/abs/2510.04772)
*Max Kirchner,Hanna Hoffmann,Alexander C. Jenke,Oliver L. Saldanha,Kevin Pfeiffer,Weam Kanjo,Julia Alekseenko,Claas de Boer,Santhi Raj Kolamuri,Lorenzo Mazza,Nicolas Padoy,Sophia Bano,Annika Reinke,Lena Maier-Hein,Danail Stoyanov,Jakob N. Kather,Fiona R. Kolbinger,Sebastian Bodenstedt,Stefanie Speidel*

Main category: cs.CV

TL;DR: The FedSurg challenge benchmarks federated learning (FL) for surgical video classification, focusing on generalization to unseen clinical centers and adaptation through fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Develop methods for surgical video classification that generalize to new clinical centers while protecting patient data through federated learning.

Method: Analyzed inflammation stages in appendicitis using FL strategies, assessing methods like FedAvg, FedMedian, and foundation models through tasks of generalization and fine-tuning adaptation.

Result: The generalization task showed limited performance; fine-tuning improved adaptation, with the ViViT-based submission showing strongest results but with low ranking stability.

Conclusion: The challenge provides the first benchmark for evaluating FL in surgical videos, highlighting trade-offs between local adaptation and global robustness, and identifies key areas for improvement in clinical AI methods.

Abstract: Purpose: The FedSurg challenge was designed to benchmark the state of the art
in federated learning for surgical video classification. Its goal was to assess
how well current methods generalize to unseen clinical centers and adapt
through local fine-tuning while enabling collaborative model development
without sharing patient data. Methods: Participants developed strategies to
classify inflammation stages in appendicitis using a preliminary version of the
multi-center Appendix300 video dataset. The challenge evaluated two tasks:
generalization to an unseen center and center-specific adaptation after
fine-tuning. Submitted approaches included foundation models with linear
probing, metric learning with triplet loss, and various FL aggregation schemes
(FedAvg, FedMedian, FedSAM). Performance was assessed using F1-score and
Expected Cost, with ranking robustness evaluated via bootstrapping and
statistical testing. Results: In the generalization task, performance across
centers was limited. In the adaptation task, all teams improved after
fine-tuning, though ranking stability was low. The ViViT-based submission
achieved the strongest overall performance. The challenge highlighted
limitations in generalization, sensitivity to class imbalance, and difficulties
in hyperparameter tuning in decentralized training, while spatiotemporal
modeling and context-aware preprocessing emerged as promising strategies.
Conclusion: The FedSurg Challenge establishes the first benchmark for
evaluating FL strategies in surgical video classification. Findings highlight
the trade-off between local personalization and global robustness, and
underscore the importance of architecture choice, preprocessing, and loss
design. This benchmarking offers a reference point for future development of
imbalance-aware, adaptive, and robust FL methods in clinical surgical AI.

</details>


### [342] [Hands-Free Heritage: Automated 3D Scanning for Cultural Heritage Digitization](https://arxiv.org/abs/2510.04781)
*Javed Ahmad,Federico Dassiè,Selene Frascella,Gabriele Marchello,Ferdinando Cannella,Arianna Traviglia*

Main category: cs.CV

TL;DR: This paper introduces an automated two-robot system for high-resolution 3D scanning of cultural heritage artifacts, delivering improved accuracy and efficiency compared to conventional methods.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the limitations of conventional 3D scanning methods for cultural heritage artifacts, which typically demand significant manual intervention and expertise.

Method: The authors developed a two-robot scanning system using coordinated motion planning with a scanner-equipped robot and a tray-handling robot to automate the scanning process.

Result: This system demonstrated lower Chamfer Distance and higher F-score compared to baseline methods, indicating superior geometric accuracy and efficiency.

Conclusion: The proposed system advances artifact digitization by enhancing reconstruction precision and reducing dependence on manual workflows and expert operators.

Abstract: High-fidelity 3D scanning is essential for preserving cultural heritage
artefacts, supporting documentation, analysis, and long-term conservation.
However, conventional methods typically require specialized expertise and
manual intervention to maintain optimal scanning conditions and coverage. We
present an automated two-robot scanning system that eliminates the need for
handheld or semi-automatic workflows by combining coordinated robotic
manipulation with high-resolution 3D scanning. Our system parameterizes the
scanning space into distinct regions, enabling coordinated motion planning
between a scanner-equipped robot and a tray-handling robot. Optimized
trajectory planning and waypoint distribution ensure comprehensive surface
coverage, minimize occlusions, and balance reconstruction accuracy with system
efficiency. Experimental results show that our approach achieves significantly
lower Chamfer Distance and higher F-score compared to baseline methods,
offering superior geometric accuracy, improved digitization efficiency, and
reduced reliance on expert operators.

</details>


### [343] [A Comparative Study of Vision Transformers and CNNs for Few-Shot Rigid Transformation and Fundamental Matrix Estimation](https://arxiv.org/abs/2510.04794)
*Alon Kaya,Igal Bilik,Inna Stainvas*

Main category: cs.CV

TL;DR: This paper explores ViTs and CNNs as backbone architectures for geometric estimation tasks in varying data regimes and demonstrates that ViTs excel in large datasets, while CNNs perform better in low-data settings due to inductive bias.


<details>
  <summary>Details</summary>
Motivation: To investigate whether ViT-based models or large-scale CNNs are more effective for image deformation-related geometric estimation tasks in both large and few-shot data scenarios.

Method: The study compares pretrained large-scale CNNs (ResNet, EfficientNet, CLIP-ResNet) and ViT-based foundation models (CLIP-ViT, DINO) across different data size setups, analyzing their refinement performance and domain generalization.

Result: ViTs outperform CNNs in large data scenarios and exhibit better cross-domain generalization, whereas CNNs show improved performance in small data setups due to their inductive bias and smaller capacity.

Conclusion: The findings highlight the need for selecting architectures aligned with specific data regimes and suggest hybrid architectures for balancing local and global feature extraction.

Abstract: Vision-transformers (ViTs) and large-scale convolution-neural-networks (CNNs)
have reshaped computer vision through pretrained feature representations that
enable strong transfer learning for diverse tasks. However, their efficiency as
backbone architectures for geometric estimation tasks involving image
deformations in low-data regimes remains an open question. This work considers
two such tasks: 1) estimating 2D rigid transformations between pairs of images
and 2) predicting the fundamental matrix for stereo image pairs, an important
problem in various applications, such as autonomous mobility, robotics, and 3D
scene reconstruction. Addressing this intriguing question, this work
systematically compares large-scale CNNs (ResNet, EfficientNet, CLIP-ResNet)
with ViT-based foundation models (CLIP-ViT variants and DINO) in various data
size settings, including few-shot scenarios. These pretrained models are
optimized for classification or contrastive learning, encouraging them to focus
mostly on high-level semantics. The considered tasks require balancing local
and global features differently, challenging the straightforward adoption of
these models as the backbone. Empirical comparative analysis shows that,
similar to training from scratch, ViTs outperform CNNs during refinement in
large downstream-data scenarios. However, in small data scenarios, the
inductive bias and smaller capacity of CNNs improve their performance, allowing
them to match that of a ViT. Moreover, ViTs exhibit stronger generalization in
cross-domain evaluation where the data distribution changes. These results
emphasize the importance of carefully selecting model architectures for
refinement, motivating future research towards hybrid architectures that
balance local and global representations.

</details>


### [344] [DiT-VTON: Diffusion Transformer Framework for Unified Multi-Category Virtual Try-On and Virtual Try-All with Integrated Image Editing](https://arxiv.org/abs/2510.04797)
*Qi Li,Shuwen Qiu,Julien Han,Xingzi Xu,Mehmet Saygin Seyfioglu,Kee Kiat Koo,Karim Bouyarmane*

Main category: cs.CV

TL;DR: This paper introduces DiT-VTON, an innovative VTO framework that leverages a Diffusion Transformer (DiT) to address challenges in detail preservation, adaptability, and generalization, offering advancements in both garment try-on and broader Virtual Try-All scenarios.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this work is to overcome limitations in existing VTO models, such as poor detail preservation, lack of robustness with real-world imagery, inefficient sampling, and limited generalization across product categories, while expanding VTO capabilities to include advanced image editing and non-garment product customization.

Method: The authors propose DiT-VTON, which employs the Diffusion Transformer (DiT) for image-conditioned VTO. They examine different configurations for image conditioning using DiT, train the model on a diverse, expanded dataset, and extend its capabilities to non-garment product categories with advanced editing functions.

Result: DiT-VTON achieves state-of-the-art performance on the VITON-HD benchmark, demonstrating superior detail preservation, robustness, and adaptability without relying on additional condition encoders. It also outperforms current models in Virtual Try-All (VTA) scenarios involving diverse product categories and image editing.

Conclusion: The proposed DiT-VTON framework successfully addresses existing VTO limitations and expands its scope to broad-use cases through innovative design and training, paving the way for more adaptable and high-performance virtual try-on technologies.

Abstract: The rapid growth of e-commerce has intensified the demand for Virtual Try-On
(VTO) technologies, enabling customers to realistically visualize products
overlaid on their own images. Despite recent advances, existing VTO models face
challenges with fine-grained detail preservation, robustness to real-world
imagery, efficient sampling, image editing capabilities, and generalization
across diverse product categories. In this paper, we present DiT-VTON, a novel
VTO framework that leverages a Diffusion Transformer (DiT), renowned for its
performance on text-conditioned image generation, adapted here for the
image-conditioned VTO task. We systematically explore multiple DiT
configurations, including in-context token concatenation, channel
concatenation, and ControlNet integration, to determine the best setup for VTO
image conditioning.
  To enhance robustness, we train the model on an expanded dataset encompassing
varied backgrounds, unstructured references, and non-garment categories,
demonstrating the benefits of data scaling for VTO adaptability. DiT-VTON also
redefines the VTO task beyond garment try-on, offering a versatile Virtual
Try-All (VTA) solution capable of handling a wide range of product categories
and supporting advanced image editing functionalities such as pose
preservation, localized editing, texture transfer, and object-level
customization. Experimental results show that our model surpasses
state-of-the-art methods on VITON-HD, achieving superior detail preservation
and robustness without reliance on additional condition encoders. It also
outperforms models with VTA and image editing capabilities on a diverse dataset
spanning thousands of product categories.

</details>


### [345] [Did you just see that? Arbitrary view synthesis for egocentric replay of operating room workflows from ambient sensors](https://arxiv.org/abs/2510.04802)
*Han Zhang,Lalithkumar Seenivasan,Jose L. Porras,Roger D. Soberanis-Mukul,Hao Ding,Hongchao Shu,Benjamin D. Killeen,Ankita Ghosh,Lonny Yarmus,Masaru Ishii,Angela Christine Argento,Mathias Unberath*

Main category: cs.CV

TL;DR: The paper introduces EgoSurg, a system that uses fixed-camera videos to reconstruct egocentric viewpoints of operating room staff, enhancing surgical data science.


<details>
  <summary>Details</summary>
Motivation: Surgical practice relies on understanding dynamic visual perspectives, which are missing in fixed-camera setups, limiting insights into clinical decision-making and workflow optimization.

Method: EgoSurg combines geometry-driven neural rendering and diffusion-based view enhancement to synthesize egocentric visuals from fixed-camera footage without disturbing clinical workflows.

Result: EgoSurg successfully reconstructs high-quality, person-specific visual perspectives from fixed-camera videos in surgical settings.

Conclusion: EgoSurg transforms existing operating room camera systems into immersive 3D navigable records, offering new possibilities for surgical visualization and analysis.

Abstract: Observing surgical practice has historically relied on fixed vantage points
or recollections, leaving the egocentric visual perspectives that guide
clinical decisions undocumented. Fixed-camera video can capture surgical
workflows at the room-scale, but cannot reconstruct what each team member
actually saw. Thus, these videos only provide limited insights into how
decisions that affect surgical safety, training, and workflow optimization are
made. Here we introduce EgoSurg, the first framework to reconstruct the
dynamic, egocentric replays for any operating room (OR) staff directly from
wall-mounted fixed-camera video, and thus, without intervention to clinical
workflow. EgoSurg couples geometry-driven neural rendering with diffusion-based
view enhancement, enabling high-visual fidelity synthesis of arbitrary and
egocentric viewpoints at any moment. In evaluation across multi-site surgical
cases and controlled studies, EgoSurg reconstructs person-specific visual
fields and arbitrary viewpoints with high visual quality and fidelity. By
transforming existing OR camera infrastructure into a navigable dynamic 3D
record, EgoSurg establishes a new foundation for immersive surgical data
science, enabling surgical practice to be visualized, experienced, and analyzed
from every angle.

</details>


### [346] [Visual Representations inside the Language Model](https://arxiv.org/abs/2510.04819)
*Benlin Liu,Amita Kamath,Madeleine Grunde-McLaughlin,Winson Han,Ranjay Krishna*

Main category: cs.CV

TL;DR: The paper investigates the role of visual key-value tokens in multimodal language models (MLMs) to better understand their challenges in perception-heavy tasks.


<details>
  <summary>Details</summary>
Motivation: To address the gaps in understanding why MLMs struggle with tasks requiring strong visual perception and to explore how visual information is processed in MLM architectures.

Method: The authors analyzed popular MLMs by studying the flow of visual information through the language model, examining key-value visual tokens, and testing methods (e.g., adding text prefixes) to control visual information for improved performance.

Result: They found that MLMs convey sufficient visual information for several tasks zero-shot but trail behind non-finetuned visual encoders like SigLIP in certain use cases. The poor control of visual information in later layers also introduces artifacts, impacting perception.

Conclusion: Enhancing MLMs' ability to better control and surface visual information can significantly improve perception-heavy task performance, offering insights for future training modifications and interpretability studies.

Abstract: Despite interpretability work analyzing VIT encoders and transformer
activations, we don't yet understand why Multimodal Language Models (MLMs)
struggle on perception-heavy tasks. We offer an under-studied perspective by
examining how popular MLMs (LLaVA-OneVision, Qwen2.5-VL, and
Llama-3-LLaVA-NeXT) process their visual key-value tokens. We first study the
flow of visual information through the language model, finding that image value
tokens encode sufficient information to perform several perception-heavy tasks
zero-shot: segmentation, semantic correspondence, temporal correspondence, and
referring expression detection. We find that while the language model does
augment the visual information received from the projection of input visual
encodings-which we reveal correlates with overall MLM perception capability-it
contains less visual information on several tasks than the equivalent visual
encoder (SigLIP) that has not undergone MLM finetuning. Further, we find that
the visual information corresponding to input-agnostic image key tokens in
later layers of language models contains artifacts which reduce perception
capability of the overall MLM. Next, we discuss controlling visual information
in the language model, showing that adding a text prefix to the image input
improves perception capabilities of visual representations. Finally, we reveal
that if language models were able to better control their visual information,
their perception would significantly improve; e.g., in 33.3% of Art Style
questions in the BLINK benchmark, perception information present in the
language model is not surfaced to the output! Our findings reveal insights into
the role of key-value tokens in multimodal systems, paving the way for deeper
mechanistic interpretability of MLMs and suggesting new directions for training
their visual encoder and language model components.

</details>


### [347] [AvatarVTON: 4D Virtual Try-On for Animatable Avatars](https://arxiv.org/abs/2510.04822)
*Zicheng Jiang,Jixin Gao,Shengfeng He,Xinzhe Li,Yulong Zheng,Zhaotong Yang,Junyu Dong,Yong Du*

Main category: cs.CV

TL;DR: AvatarVTON introduces a 4D virtual try-on system capable of realistic try-on visualization with free pose control and garment flexibility.


<details>
  <summary>Details</summary>
Motivation: To enable realistic virtual try-on systems without relying on multi-view data or physics-based priors.

Method: Two modules: Reciprocal Flow Rectifier for optical-flow stability and Non-Linear Deformer for adaptive garment deformation.

Result: AvatarVTON provides realistic try-on results with high fidelity and dynamic garment interaction, validated through extensive experiments.

Conclusion: AvatarVTON is effective for AR/VR, gaming, and digital-human use-cases due to its high-quality try-on results and flexibility.

Abstract: We propose AvatarVTON, the first 4D virtual try-on framework that generates
realistic try-on results from a single in-shop garment image, enabling free
pose control, novel-view rendering, and diverse garment choices. Unlike
existing methods, AvatarVTON supports dynamic garment interactions under
single-view supervision, without relying on multi-view garment captures or
physics priors. The framework consists of two key modules: (1) a Reciprocal
Flow Rectifier, a prior-free optical-flow correction strategy that stabilizes
avatar fitting and ensures temporal coherence; and (2) a Non-Linear Deformer,
which decomposes Gaussian maps into view-pose-invariant and view-pose-specific
components, enabling adaptive, non-linear garment deformations. To establish a
benchmark for 4D virtual try-on, we extend existing baselines with unified
modules for fair qualitative and quantitative comparisons. Extensive
experiments show that AvatarVTON achieves high fidelity, diversity, and dynamic
garment realism, making it well-suited for AR/VR, gaming, and digital-human
applications.

</details>


### [348] [Flow Matching for Conditional MRI-CT and CBCT-CT Image Synthesis](https://arxiv.org/abs/2510.04823)
*Arnela Hadzic,Simon Johannes Joham,Martin Urschler*

Main category: cs.CV

TL;DR: This paper focuses on generating synthetic CT (sCT) images from MRI or CBCT using 3D Flow Matching (FM) for applications in adaptive radiotherapy.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve the precision of adaptive radiotherapy while minimizing radiation exposure for patients by generating high-quality synthetic CT (sCT) images from MRI or CBCT.

Method: The paper uses a fully 3D Flow Matching (FM) framework, where Gaussian noise is transformed into sCT images through a learned FM velocity field and a 3D encoder that processes MRI or CBCT inputs.

Result: The proposed method showed success in reconstructing global anatomical structures but faced limitations in capturing fine details due to constraints on training resolution, validation, and testing through the SynthRAD2025 Challenge system.

Conclusion: The study concludes that while the method demonstrates potential, further improvements—such as patch-based training and latent-space flow models—are required to enhance resolution and structural fidelity.

Abstract: Generating synthetic CT (sCT) from MRI or CBCT plays a crucial role in
enabling MRI-only and CBCT-based adaptive radiotherapy, improving treatment
precision while reducing patient radiation exposure. To address this task, we
adopt a fully 3D Flow Matching (FM) framework, motivated by recent work
demonstrating FM's efficiency in producing high-quality images. In our
approach, a Gaussian noise volume is transformed into an sCT image by
integrating a learned FM velocity field, conditioned on features extracted from
the input MRI or CBCT using a lightweight 3D encoder. We evaluated the method
on the SynthRAD2025 Challenge benchmark, training separate models for MRI
$\rightarrow$ sCT and CBCT $\rightarrow$ sCT across three anatomical regions:
abdomen, head and neck, and thorax. Validation and testing were performed
through the challenge submission system. The results indicate that the method
accurately reconstructs global anatomical structures; however, preservation of
fine details was limited, primarily due to the relatively low training
resolution imposed by memory and runtime constraints. Future work will explore
patch-based training and latent-space flow models to improve resolution and
local structural fidelity.

</details>


### [349] [Beyond Random: Automatic Inner-loop Optimization in Dataset Distillation](https://arxiv.org/abs/2510.04838)
*Muquan Li,Hang Gou,Dongyang Zhang,Shuang Liang,Xiurui Xie,Deqiang Ouyang,Ke Qin*

Main category: cs.CV

TL;DR: The paper introduces AT-BPTT, a dynamic dataset distillation optimization method that adapts truncation based on learning dynamics, achieving significant accuracy, speed, and memory improvements.


<details>
  <summary>Details</summary>
Motivation: Existing methods for dataset distillation use random truncation which is rigid and often suboptimal, failing to account for neural networks' distinct learning stages.

Method: AT-BPTT employs a probabilistic mechanism for timestep selection, adaptive window sizing based on gradient variation, and low-rank Hessian approximation to dynamically tailor truncation.

Result: AT-BPTT outperforms baseline methods by improving accuracy by 6.16% on average, accelerating optimization by 3.9x, and reducing memory costs by 63% in experiments across multiple datasets like CIFAR-10 and ImageNet-1K.

Conclusion: The proposed AT-BPTT framework effectively captures neural networks' learning dynamics, optimizing dataset distillation with superior performance, efficiency, and reduced computational costs.

Abstract: The growing demand for efficient deep learning has positioned dataset
distillation as a pivotal technique for compressing training dataset while
preserving model performance. However, existing inner-loop optimization methods
for dataset distillation typically rely on random truncation strategies, which
lack flexibility and often yield suboptimal results. In this work, we observe
that neural networks exhibit distinct learning dynamics across different
training stages-early, middle, and late-making random truncation ineffective.
To address this limitation, we propose Automatic Truncated Backpropagation
Through Time (AT-BPTT), a novel framework that dynamically adapts both
truncation positions and window sizes according to intrinsic gradient behavior.
AT-BPTT introduces three key components: (1) a probabilistic mechanism for
stage-aware timestep selection, (2) an adaptive window sizing strategy based on
gradient variation, and (3) a low-rank Hessian approximation to reduce
computational overhead. Extensive experiments on CIFAR-10, CIFAR-100,
Tiny-ImageNet, and ImageNet-1K show that AT-BPTT achieves state-of-the-art
performance, improving accuracy by an average of 6.16% over baseline methods.
Moreover, our approach accelerates inner-loop optimization by 3.9x while saving
63% memory cost.

</details>


### [350] [Detailed Aerial Mapping of Photovoltaic Power Plants Through Semantically Significant Keypoints](https://arxiv.org/abs/2510.04840)
*Viktor Kozák,Jan Chudoba,Libor Přeučil*

Main category: cs.CV

TL;DR: This paper introduces a method to map PV power plants using aerial images, focusing on automation and detailed module-level modeling.


<details>
  <summary>Details</summary>
Motivation: Existing PV power plant models are often inaccessible or outdated, impacting their maintenance and operational efficiency.

Method: The authors use aerial image segmentation of PV modules combined with visual keypoint identification to map and fuse structural information into georeferenced 3D models.

Result: The method was tested on two PV plants, successfully creating detailed and georeferenced models down to individual PV modules.

Conclusion: The approach offers an automated solution for accurate PV power plant mapping, enhancing operational maintenance capabilities without third-party dependence.

Abstract: An accurate and up-to-date model of a photovoltaic (PV) power plant is
essential for its optimal operation and maintenance. However, such a model may
not be easily available. This work introduces a novel approach for PV power
plant mapping based on aerial overview images. It enables the automation of the
mapping process while removing the reliance on third-party data. The presented
mapping method takes advantage of the structural layout of the power plants to
achieve detailed modeling down to the level of individual PV modules. The
approach relies on visual segmentation of PV modules in overview images and the
inference of structural information in each image, assigning modules to
individual benches, rows, and columns. We identify visual keypoints related to
the layout and use these to merge detections from multiple images while
maintaining their structural integrity. The presented method was experimentally
verified and evaluated on two different power plants. The final fusion of 3D
positions and semantic structures results in a compact georeferenced model
suitable for power plant maintenance.

</details>


### [351] [From Actions to Kinesics: Extracting Human Psychological States through Bodily Movements](https://arxiv.org/abs/2510.04844)
*Cheyu Lin,Katherine A. Flanigan*

Main category: cs.CV

TL;DR: The paper proposes a privacy-preserving framework to infer human psychological states from 3D skeleton data using machine learning.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of modeling human-environment interactions while ensuring psychological state generalization and privacy.

Method: A kinesics recognition framework combining ST-GCN and CNN with transfer learning to analyze 3D skeleton joint data.

Result: Results on the DUET dataset show scalable and accurate modeling of human behaviors related to cognitive and emotional states.

Conclusion: The framework presents a novel way to enhance RL-driven simulations of human-environment interactions, maintaining privacy and generalization.

Abstract: Understanding the dynamic relationship between humans and the built
environment is a key challenge in disciplines ranging from environmental
psychology to reinforcement learning (RL). A central obstacle in modeling these
interactions is the inability to capture human psychological states in a way
that is both generalizable and privacy preserving. Traditional methods rely on
theoretical models or questionnaires, which are limited in scope, static, and
labor intensive. We present a kinesics recognition framework that infers the
communicative functions of human activity -- known as kinesics -- directly from
3D skeleton joint data. Combining a spatial-temporal graph convolutional
network (ST-GCN) with a convolutional neural network (CNN), the framework
leverages transfer learning to bypass the need for manually defined mappings
between physical actions and psychological categories. The approach preserves
user anonymity while uncovering latent structures in bodily movements that
reflect cognitive and emotional states. Our results on the Dyadic User
EngagemenT (DUET) dataset demonstrate that this method enables scalable,
accurate, and human-centered modeling of behavior, offering a new pathway for
enhancing RL-driven simulations of human-environment interaction.

</details>


### [352] [Read the Room: Inferring Social Context Through Dyadic Interaction Recognition in Cyber-physical-social Infrastructure Systems](https://arxiv.org/abs/2510.04854)
*Cheyu Lin,John Martins,Katherine A. Flanigan,Ph. D*

Main category: cs.CV

TL;DR: Cyber-physical-social infrastructure systems aim to enhance social benefits of CPS by recognizing human interactions through privacy-conscious methods like depth sensors.


<details>
  <summary>Details</summary>
Motivation: To integrate human-centered benefits into cyber-physical systems by understanding and predicting dyadic human interactions for fostering social well-being.

Method: The study employs depth sensors for privacy-preserving analysis of skeletal movements and compares five skeleton-based interaction recognition algorithms on 12 categorized dyadic interaction datasets.

Result: The comparison provided insights into cultural and emotional aspects of human interactions, particularly in the context of communication types like emblems and affect displays.

Conclusion: Depth sensors serve as an effective and privacy-aware means to advance interaction recognition, laying groundwork for enhancing social aspects of CPS systems.

Abstract: Cyber-physical systems (CPS) integrate sensing, computing, and control to
improve infrastructure performance, focusing on economic goals like performance
and safety. However, they often neglect potential human-centered (or
''social'') benefits. Cyber-physical-social infrastructure systems (CPSIS) aim
to address this by aligning CPS with social objectives. This involves defining
social benefits, understanding human interactions with each other and
infrastructure, developing privacy-preserving measurement methods, modeling
these interactions for prediction, linking them to social benefits, and
actuating the physical environment to foster positive social outcomes. This
paper delves into recognizing dyadic human interactions using real-world data,
which is the backbone to measuring social behavior. This lays a foundation to
address the need to enhance understanding of the deeper meanings and mutual
responses inherent in human interactions. While RGB cameras are informative for
interaction recognition, privacy concerns arise. Depth sensors offer a
privacy-conscious alternative by analyzing skeletal movements. This study
compares five skeleton-based interaction recognition algorithms on a dataset of
12 dyadic interactions. Unlike single-person datasets, these interactions,
categorized into communication types like emblems and affect displays, offer
insights into the cultural and emotional aspects of human interactions.

</details>


### [353] [ERDE: Entropy-Regularized Distillation for Early-exit](https://arxiv.org/abs/2510.04856)
*Martial Guidez,Stefan Duffner,Yannick Alpou,Oscar Röth,Christophe Garcia*

Main category: cs.CV

TL;DR: The paper proposes a technique that combines early exits and entropy-enhanced knowledge distillation to train student models, optimizing the trade-off between accuracy and computational efficiency for image classification.


<details>
  <summary>Details</summary>
Motivation: Deep neural networks are powerful but computationally expensive, making them less feasible for real-time or edge applications. The study aims to address this gap by developing a method that reduces computational costs while maintaining classification performance.

Method: The method integrates early exits and knowledge distillation. A student early-exit model is trained using entropy-based loss for cases where teacher classifications are incorrect. This helps improve efficiency without losing accuracy.

Result: The proposed method demonstrated effectiveness on datasets including CIFAR10, CIFAR100, and SVHN, achieving a balance between low computational cost and high accuracy.

Conclusion: This research introduces an entropy-based loss approach for dynamic model compression that optimizes efficiency and accuracy, offering new directions for knowledge distillation in other applications.

Abstract: Although deep neural networks and in particular Convolutional Neural Networks
have demonstrated state-of-the-art performance in image classification with
relatively high efficiency, they still exhibit high computational costs, often
rendering them impractical for real-time and edge applications. Therefore, a
multitude of compression techniques have been developed to reduce these costs
while maintaining accuracy. In addition, dynamic architectures have been
introduced to modulate the level of compression at execution time, which is a
desirable property in many resource-limited application scenarios. The proposed
method effectively integrates two well-established optimization techniques:
early exits and knowledge distillation, where a reduced student early-exit
model is trained from a more complex teacher early-exit model. The primary
contribution of this research lies in the approach for training the student
early-exit model. In comparison to the conventional Knowledge Distillation
loss, our approach incorporates a new entropy-based loss for images where the
teacher's classification was incorrect. The proposed method optimizes the
trade-off between accuracy and efficiency, thereby achieving significant
reductions in computational complexity without compromising classification
performance. The validity of this approach is substantiated by experimental
results on image classification datasets CIFAR10, CIFAR100 and SVHN, which
further opens new research perspectives for Knowledge Distillation in other
contexts.

</details>


### [354] [μDeepIQA: deep learning-based fast and robust image quality assessment with local predictions for optical microscopy](https://arxiv.org/abs/2510.04859)
*Elena Corbetta,Thomas Bocklitz*

Main category: cs.CV

TL;DR: This paper proposes a deep learning-based method, $
DeepIQA$, to enhance image quality assessment (IQA) for optical microscopy by leveraging convolutional neural networks for faster, stable, and generalizable quality evaluations.


<details>
  <summary>Details</summary>
Motivation: Existing IQA methods for optical microscopy are computationally expensive, time-consuming for large datasets, and unstable for images outside the ideal domain. The goal is to address these limitations with a faster and more generalizable approach.

Method: The authors retrain a deep convolutional neural network (originally designed for natural image IQA) to predict quality metrics and global scores for optical microscopy datasets. Key features include patch-wise quality prediction for spatial visualization.

Result: The $
DeepIQA model achieves rapid and stable predictions for IQA, generalizing well across non-standard image domains and visualizing spatially varying quality in microscopy images.

Conclusion: Deep learning models can significantly benefit optical microscopy-based research by providing robust, high-speed, and generalizable solutions for image quality assessment.

Abstract: Optical microscopy is one of the most widely used techniques in research
studies for life sciences and biomedicine. These applications require reliable
experimental pipelines to extract valuable knowledge from the measured samples
and must be supported by image quality assessment (IQA) to ensure correct
processing and analysis of the image data. IQA methods are implemented with
variable complexity. However, while most quality metrics have a straightforward
implementation, they might be time consuming and computationally expensive when
evaluating a large dataset. In addition, quality metrics are often designed for
well-defined image features and may be unstable for images out of the ideal
domain.
  To overcome these limitations, recent works have proposed deep learning-based
IQA methods, which can provide superior performance, increased generalizability
and fast prediction. Our method, named $\mathrm{\mu}$DeepIQA, is inspired by
previous studies and applies a deep convolutional neural network designed for
IQA on natural images to optical microscopy measurements. We retrained the same
architecture to predict individual quality metrics and global quality scores
for optical microscopy data. The resulting models provide fast and stable
predictions of image quality by generalizing quality estimation even outside
the ideal range of standard methods. In addition, $\mathrm{\mu}$DeepIQA
provides patch-wise prediction of image quality and can be used to visualize
spatially varying quality in a single image. Our study demonstrates that
optical microscopy-based studies can benefit from the generalizability of deep
learning models due to their stable performance in the presence of outliers,
the ability to assess small image patches, and rapid predictions.

</details>


### [355] [In-Field Mapping of Grape Yield and Quality with Illumination-Invariant Deep Learning](https://arxiv.org/abs/2510.04864)
*Ciem Cornelissen,Sander De Coninck,Axel Willekens,Sam Leroux,Pieter Simoens*

Main category: cs.CV

TL;DR: This paper introduces an IoT-enabled robotic system for real-time analysis of grape yield and quality in vineyards, employing advanced detection and domain-adaptive deep learning.


<details>
  <summary>Details</summary>
Motivation: There is a need for non-destructive, real-time systems to map grape yield and quality, addressing issues like illumination variability in hyperspectral imaging data.

Method: The authors developed two modules: a model for grape bunch detection and weight estimation, and a spectral autoencoder (LISA) for quality assessment under variable lighting conditions.

Result: The system achieved a recall of 0.82 for bunch detection, an R² of 0.76 for weight prediction, and a 20% improvement in quality prediction generalization with LISA.

Conclusion: The system effectively generates georeferenced, high-resolution data on grape yield and quality, enabling data-driven decisions in precision viticulture.

Abstract: This paper presents an end-to-end, IoT-enabled robotic system for the
non-destructive, real-time, and spatially-resolved mapping of grape yield and
quality (Brix, Acidity) in vineyards. The system features a comprehensive
analytical pipeline that integrates two key modules: a high-performance model
for grape bunch detection and weight estimation, and a novel deep learning
framework for quality assessment from hyperspectral (HSI) data. A critical
barrier to in-field HSI is the ``domain shift" caused by variable illumination.
To overcome this, our quality assessment is powered by the Light-Invariant
Spectral Autoencoder (LISA), a domain-adversarial framework that learns
illumination-invariant features from uncalibrated data. We validated the
system's robustness on a purpose-built HSI dataset spanning three distinct
illumination domains: controlled artificial lighting (lab), and variable
natural sunlight captured in the morning and afternoon. Results show the
complete pipeline achieves a recall (0.82) for bunch detection and a $R^2$
(0.76) for weight prediction, while the LISA module improves quality prediction
generalization by over 20% compared to the baselines. By combining these robust
modules, the system successfully generates high-resolution, georeferenced data
of both grape yield and quality, providing actionable, data-driven insights for
precision viticulture.

</details>


### [356] [BenthiCat: An opti-acoustic dataset for advancing benthic classification and habitat mapping](https://arxiv.org/abs/2510.04876)
*Hayat Rajani,Valerio Franchi,Borja Martinez-Clavel Valles,Raimon Ramos,Rafael Garcia,Nuno Gracias*

Main category: cs.CV

TL;DR: This paper presents a large multi-modal dataset for underwater habitat mapping using side-scan sonar, bathymetric maps, and optical images, advancing machine learning research in marine ecosystems.


<details>
  <summary>Details</summary>
Motivation: The study aims to fill the gap in large annotated datasets, crucial for improving machine learning models in benthic habitat mapping.

Method: The paper gathers 1 million side-scan sonar tiles, segmented 36,000 of them manually, and associates them with bathymetric maps and optical images. It also explores self-supervised learning for multi-sensor integration.

Result: A comprehensive, annotated dataset along the Catalonian coast is created, complemented by preprocessing tools and aligned multi-sensor data for robust model development.

Conclusion: The resource provides a foundation for standardized benchmarks and advancements in autonomous underwater classification and multi-sensor data integration.

Abstract: Benthic habitat mapping is fundamental for understanding marine ecosystems,
guiding conservation efforts, and supporting sustainable resource management.
Yet, the scarcity of large, annotated datasets limits the development and
benchmarking of machine learning models in this domain. This paper introduces a
thorough multi-modal dataset, comprising about a million side-scan sonar (SSS)
tiles collected along the coast of Catalonia (Spain), complemented by
bathymetric maps and a set of co-registered optical images from targeted
surveys using an autonomous underwater vehicle (AUV). Approximately \num{36000}
of the SSS tiles have been manually annotated with segmentation masks to enable
supervised fine-tuning of classification models. All the raw sensor data,
together with mosaics, are also released to support further exploration and
algorithm development. To address challenges in multi-sensor data fusion for
AUVs, we spatially associate optical images with corresponding SSS tiles,
facilitating self-supervised, cross-modal representation learning. Accompanying
open-source preprocessing and annotation tools are provided to enhance
accessibility and encourage research. This resource aims to establish a
standardized benchmark for underwater habitat mapping, promoting advancements
in autonomous seafloor classification and multi-sensor integration.

</details>


### [357] [Comparative Analysis of YOLOv5, Faster R-CNN, SSD, and RetinaNet for Motorbike Detection in Kigali Autonomous Driving Context](https://arxiv.org/abs/2510.04912)
*Ngeyen Yinkfu,Sunday Nwovu,Jonathan Kayizzi,Angelique Uwamahoro*

Main category: cs.CV

TL;DR: The paper evaluates four object detection models (YOLOv5, Faster R-CNN, SSD, RetinaNet) for motorcycle detection in Kigali, Rwanda, using a dataset of 198 images.


<details>
  <summary>Details</summary>
Motivation: Motorcycle taxis in Kigali operate unpredictably and pose challenges for autonomous driving systems, necessitating robust detection models for real-time navigation.

Method: The study implements four object detection models (YOLOv5, Faster R-CNN, SSD, RetinaNet) in PyTorch using transfer learning and evaluates them on accuracy, localization, and inference speed.

Result: Findings highlight trade-offs between model performance and speed, identifying challenges like dataset limitations and complexities of implementations.

Conclusion: Simplified model architectures are recommended for improving accessibility of autonomous systems in resource-limited environments like Rwanda.

Abstract: In Kigali, Rwanda, motorcycle taxis are a primary mode of transportation,
often navigating unpredictably and disregarding traffic rules, posing
significant challenges for autonomous driving systems. This study compares four
object detection models--YOLOv5, Faster R-CNN, SSD, and RetinaNet--for
motorbike detection using a custom dataset of 198 images collected in Kigali.
Implemented in PyTorch with transfer learning, the models were evaluated for
accuracy, localization, and inference speed to assess their suitability for
real-time navigation in resource-constrained settings. We identify
implementation challenges, including dataset limitations and model
complexities, and recommend simplified architectures for future work to enhance
accessibility for autonomous systems in developing countries like Rwanda.

</details>


### [358] [A Semantics-Aware Hierarchical Self-Supervised Approach to Classification of Remote Sensing Images](https://arxiv.org/abs/2510.04916)
*Giulio Weikmann,Gianmarco Perantoni,Lorenzo Bruzzone*

Main category: cs.CV

TL;DR: The paper proposes a Semantics-Aware Hierarchical Consensus (SAHC) method to improve remote sensing image classification using hierarchical features and relationships, showing effectiveness and robustness across datasets.


<details>
  <summary>Details</summary>
Motivation: To address the overlooked aspect of predefined label hierarchies in remote sensing image classification and improve classification by leveraging the semantic relationships among classes.

Method: The SAHC method integrates hierarchy-specific classification heads in a deep network, uses trainable hierarchy matrices for self-supervised learning of hierarchical structures, and introduces a hierarchical consensus mechanism for consistent probability distributions.

Result: The SAHC method demonstrates effectiveness and robustness on three benchmark datasets with varying hierarchical complexities and tasks, using different backbone architectures.

Conclusion: The proposed method successfully enhances hierarchical learning and classification in remote sensing tasks, demonstrating adaptability, effectiveness, and robustness.

Abstract: Deep learning has become increasingly important in remote sensing image
classification due to its ability to extract semantic information from complex
data. Classification tasks often include predefined label hierarchies that
represent the semantic relationships among classes. However, these hierarchies
are frequently overlooked, and most approaches focus only on fine-grained
classification schemes. In this paper, we present a novel Semantics-Aware
Hierarchical Consensus (SAHC) method for learning hierarchical features and
relationships by integrating hierarchy-specific classification heads within a
deep network architecture, each specialized in different degrees of class
granularity. The proposed approach employs trainable hierarchy matrices, which
guide the network through the learning of the hierarchical structure in a
self-supervised manner. Furthermore, we introduce a hierarchical consensus
mechanism to ensure consistent probability distributions across different
hierarchical levels. This mechanism acts as a weighted ensemble being able to
effectively leverage the inherent structure of the hierarchical classification
task. The proposed SAHC method is evaluated on three benchmark datasets with
different degrees of hierarchical complexity on different tasks, using distinct
backbone architectures to effectively emphasize its adaptability. Experimental
results show both the effectiveness of the proposed approach in guiding network
learning and the robustness of the hierarchical consensus for remote sensing
image classification tasks.

</details>


### [359] [REN: Anatomically-Informed Mixture-of-Experts for Interstitial Lung Disease Diagnosis](https://arxiv.org/abs/2510.04923)
*Alec K. Peltekian,Halil Ertugrul Aktas,Gorkem Durak,Kevin Grudzinski,Bradford C. Bemiss,Carrie Richardson,Jane E. Dematte,G. R. Scott Budinger,Anthony J. Esposito,Alexander Misharin,Alok Choudhary,Ankit Agrawal,Ulas Bagci*

Main category: cs.CV

TL;DR: This paper introduces Regional Expert Networks (REN), a novel anatomically-informed mixture-of-experts (MoE) framework for medical image classification, achieving superior performance in interstitial lung disease classification with an AUC improvement of +12.5% over baseline models.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from limitations in traditional Mixture-of-Experts systems, which lack domain-specific constraints necessary for medical imaging tasks that are influenced by anatomical structure and heterogeneous pathological patterns.

Method: The paper proposes the REN framework, which incorporates anatomical priors to train specialized subnetworks (experts) for different lung regions. Multi-modal gating mechanisms use radiomics biomarkers and deep learning features to dynamically integrate expert contributions.

Result: The proposed REN achieves an average AUC of 0.8646, a significant +12.5% improvement over the SwinUNETR baseline, with region-specific experts providing enhanced performance that aligns with known disease progression patterns.

Conclusion: REN provides a scalable and anatomically-guided approach that demonstrates clinical interpretability and strong generalizability, making it suitable for other structured medical imaging applications beyond lung diseases.

Abstract: Mixture-of-Experts (MoE) architectures have significantly contributed to
scalable machine learning by enabling specialized subnetworks to tackle complex
tasks efficiently. However, traditional MoE systems lack domain-specific
constraints essential for medical imaging, where anatomical structure and
regional disease heterogeneity strongly influence pathological patterns. Here,
we introduce Regional Expert Networks (REN), the first anatomically-informed
MoE framework tailored specifically for medical image classification. REN
leverages anatomical priors to train seven specialized experts, each dedicated
to distinct lung lobes and bilateral lung combinations, enabling precise
modeling of region-specific pathological variations. Multi-modal gating
mechanisms dynamically integrate radiomics biomarkers and deep learning (DL)
features (CNN, ViT, Mamba) to weight expert contributions optimally. Applied to
interstitial lung disease (ILD) classification, REN achieves consistently
superior performance: the radiomics-guided ensemble reached an average AUC of
0.8646 +/- 0.0467, a +12.5 percent improvement over the SwinUNETR baseline (AUC
0.7685, p = 0.031). Region-specific experts further revealed that lower-lobe
models achieved AUCs of 0.88-0.90, surpassing DL counterparts (CNN: 0.76-0.79)
and aligning with known disease progression patterns. Through rigorous
patient-level cross-validation, REN demonstrates strong generalizability and
clinical interpretability, presenting a scalable, anatomically-guided approach
readily extensible to other structured medical imaging applications.

</details>


### [360] [Unsupervised Active Learning via Natural Feature Progressive Framework](https://arxiv.org/abs/2510.04939)
*Yuxi Liu,Catherine Lalman,Yimin Yang*

Main category: cs.CV

TL;DR: The paper introduces the Natural Feature Progressive Framework (NFPF), an unsupervised active learning method, that outperforms established UAL methods and matches some supervised AL techniques.


<details>
  <summary>Details</summary>
Motivation: Traditional deep learning requires large, annotated datasets, which are costly and time-consuming. Unsupervised Active Learning (UAL) can reduce this burden but faces performance limitations.

Method: NFPF uses a Specific Feature Learning Machine (SFLM) for quantifying sample importance and introduces a Reconstruction Difference metric to select the most relevant data samples.

Result: NFPF surpasses established UAL methods and achieves performance comparable to supervised AL methods in vision tasks while offering better robustness and data distribution coverage.

Conclusion: NFPF is a significant advancement in UAL, reducing annotation burdens and improving model performance, robustness, and data coverage compared to existing methods.

Abstract: The effectiveness of modern deep learning models is predicated on the
availability of large-scale, human-annotated datasets, a process that is
notoriously expensive and time-consuming. While Active Learning (AL) offers a
strategic solution by labeling only the most informative and representative
data, its iterative nature still necessitates significant human involvement.
Unsupervised Active Learning (UAL) presents an alternative by shifting the
annotation burden to a single, post-selection step. Unfortunately, prevailing
UAL methods struggle to achieve state-of-the-art performance. These approaches
typically rely on local, gradient-based scoring for sample importance
estimation, which not only makes them vulnerable to ambiguous and noisy data
but also hinders their capacity to select samples that adequately represent the
full data distribution. Moreover, their use of shallow, one-shot linear
selection falls short of a true UAL paradigm. In this paper, we propose the
Natural Feature Progressive Framework (NFPF), a UAL method that revolutionizes
how sample importance is measured. At its core, NFPF employs a Specific Feature
Learning Machine (SFLM) to effectively quantify each sample's contribution to
model performance. We further utilize the SFLM to define a powerful
Reconstruction Difference metric for initial sample selection. Our
comprehensive experiments show that NFPF significantly outperforms all
established UAL methods and achieves performance on par with supervised AL
methods on vision datasets. Detailed ablation studies and qualitative
visualizations provide compelling evidence for NFPF's superior performance,
enhanced robustness, and improved data distribution coverage.

</details>


### [361] [Bidirectional Mammogram View Translation with Column-Aware and Implicit 3D Conditional Diffusion](https://arxiv.org/abs/2510.04947)
*Xin Li,Kaixiang Yang,Qiang Li,Zhiwei Wang*

Main category: cs.CV

TL;DR: The study introduces CA3D-Diff, a framework for bidirectional mammographic view translation, leveraging conditional diffusion models to address structural misalignment challenges and improve clinical diagnostics.


<details>
  <summary>Details</summary>
Motivation: Current mammographic workflows are hindered when one of the dual-view projections (CC or MLO) is missing or degraded, limiting diagnostic accuracy. Developing methods to recover or translate missing views can enhance breast cancer screening.

Method: The paper presents CA3D-Diff, a framework based on a conditional diffusion model. It uses a column-aware cross-attention mechanism to address structural misalignments and an implicit 3D structure reconstruction module to enhance anatomical precision in view-to-view translation.

Result: CA3D-Diff outperforms state-of-the-art methods in terms of visual fidelity and structural consistency for bidirectional mammogram translation tasks. Synthesized views also improve malignancy classification in single-view screenings.

Conclusion: CA3D-Diff proves effective for recovering mammographic views with high structural accuracy, offering practical benefits for breast cancer diagnostics in clinical settings.

Abstract: Dual-view mammography, including craniocaudal (CC) and mediolateral oblique
(MLO) projections, offers complementary anatomical views crucial for breast
cancer diagnosis. However, in real-world clinical workflows, one view may be
missing, corrupted, or degraded due to acquisition errors or compression
artifacts, limiting the effectiveness of downstream analysis. View-to-view
translation can help recover missing views and improve lesion alignment. Unlike
natural images, this task in mammography is highly challenging due to large
non-rigid deformations and severe tissue overlap in X-ray projections, which
obscure pixel-level correspondences. In this paper, we propose Column-Aware and
Implicit 3D Diffusion (CA3D-Diff), a novel bidirectional mammogram view
translation framework based on conditional diffusion model. To address
cross-view structural misalignment, we first design a column-aware
cross-attention mechanism that leverages the geometric property that
anatomically corresponding regions tend to lie in similar column positions
across views. A Gaussian-decayed bias is applied to emphasize local column-wise
correlations while suppressing distant mismatches. Furthermore, we introduce an
implicit 3D structure reconstruction module that back-projects noisy 2D latents
into a coarse 3D feature volume based on breast-view projection geometry. The
reconstructed 3D structure is refined and injected into the denoising UNet to
guide cross-view generation with enhanced anatomical awareness. Extensive
experiments demonstrate that CA3D-Diff achieves superior performance in
bidirectional tasks, outperforming state-of-the-art methods in visual fidelity
and structural consistency. Furthermore, the synthesized views effectively
improve single-view malignancy classification in screening settings,
demonstrating the practical value of our method in real-world diagnostics.

</details>


### [362] [SSDD: Single-Step Diffusion Decoder for Efficient Image Tokenization](https://arxiv.org/abs/2510.04961)
*Théophane Vallaeys,Jakob Verbeek,Matthieu Cord*

Main category: cs.CV

TL;DR: This paper introduces SSDD, a novel GAN-free single-step diffusion decoder for tokenizing images, surpassing KL-VAEs in reconstruction quality and sampling speed.


<details>
  <summary>Details</summary>
Motivation: To address the performance and speed limitations of existing image tokenizers like KL-VAE, which rely on adversarial losses and iterative sampling.

Method: The authors propose a new pixel diffusion decoder architecture leveraging transformer components and distillation to optimize for single-step reconstruction without adversarial losses.

Result: The proposed SSDD achieves higher reconstruction quality (FID from 0.87 to 0.50) and faster sampling (1.4× higher throughput, 3.8× faster than KL-VAE and DiTs).

Conclusion: The SSDD serves as a superior alternative to KL-VAE, enabling faster and higher-quality generative image modeling while eliminating the need for adversarial training.

Abstract: Tokenizers are a key component of state-of-the-art generative image models,
extracting the most important features from the signal while reducing data
dimension and redundancy. Most current tokenizers are based on KL-regularized
variational autoencoders (KL-VAE), trained with reconstruction, perceptual and
adversarial losses. Diffusion decoders have been proposed as a more principled
alternative to model the distribution over images conditioned on the latent.
However, matching the performance of KL-VAE still requires adversarial losses,
as well as a higher decoding time due to iterative sampling. To address these
limitations, we introduce a new pixel diffusion decoder architecture for
improved scaling and training stability, benefiting from transformer components
and GAN-free training. We use distillation to replicate the performance of the
diffusion decoder in an efficient single-step decoder. This makes SSDD the
first diffusion decoder optimized for single-step reconstruction trained
without adversarial losses, reaching higher reconstruction quality and faster
sampling than KL-VAE. In particular, SSDD improves reconstruction FID from
$0.87$ to $0.50$ with $1.4\times$ higher throughput and preserve generation
quality of DiTs with $3.8\times$ faster sampling. As such, SSDD can be used as
a drop-in replacement for KL-VAE, and for building higher-quality and faster
generative models.

</details>


### [363] [ActiveMark: on watermarking of visual foundation models via massive activations](https://arxiv.org/abs/2510.04966)
*Anna Chistyakova,Mikhail Pautov*

Main category: cs.CV

TL;DR: The paper presents a method for embedding watermarks in Visual Foundation Models (VFMs) for ownership verification, ensuring they are detectable even after fine-tuning for downstream tasks.


<details>
  <summary>Details</summary>
Motivation: To address the problem of intellectual property infringement of VFMs by enabling reliable ownership verification methods.

Method: A fine-tuning approach using a small set of expressive layers and an encoder-decoder network to embed watermarks into the model's internal representation.

Result: The proposed method achieves low probabilities of false detection and false misdetection for ownership verification of VFMs.

Conclusion: This watermarking method offers effective and reliable tools for verifying VFM ownership, maintaining functionality post fine-tuning.

Abstract: Being trained on large and vast datasets, visual foundation models (VFMs) can
be fine-tuned for diverse downstream tasks, achieving remarkable performance
and efficiency in various computer vision applications. The high computation
cost of data collection and training motivates the owners of some VFMs to
distribute them alongside the license to protect their intellectual property
rights. However, a dishonest user of the protected model's copy may illegally
redistribute it, for example, to make a profit. As a consequence, the
development of reliable ownership verification tools is of great importance
today, since such methods can be used to differentiate between a redistributed
copy of the protected model and an independent model. In this paper, we propose
an approach to ownership verification of visual foundation models by
fine-tuning a small set of expressive layers of a VFM along with a small
encoder-decoder network to embed digital watermarks into an internal
representation of a hold-out set of input images. Importantly, the watermarks
embedded remain detectable in the functional copies of the protected model,
obtained, for example, by fine-tuning the VFM for a particular downstream task.
Theoretically and experimentally, we demonstrate that the proposed method
yields a low probability of false detection of a non-watermarked model and a
low probability of false misdetection of a watermarked model.

</details>


### [364] [Latent Uncertainty Representations for Video-based Driver Action and Intention Recognition](https://arxiv.org/abs/2510.05006)
*Koen Vellenga,H. Joe Steinhauer,Jonas Andersson,Anders Sjögren*

Main category: cs.CV

TL;DR: The paper introduces a new method for driver action and intention recognition by extending pre-trained DNNs with transformation layers for better OOD detection, outperforming existing LL-PDL methods in efficiency and reliability.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve out-of-distribution (OOD) detection and uncertainty estimation for safety-critical tasks in resource-constrained environments, such as video-based driver action and intention recognition.

Method: The method involves extending pre-trained deep neural networks with transformation layers to generate multiple latent representations for uncertainty estimation. Two approaches, latent uncertainty representation (LUR) and repulsively trained LUR (RLUR), are proposed and evaluated.

Result: LUR and RLUR showed performance comparable to or better than eight probabilistic deep learning (PDL) methods in terms of classification, calibration, and OOD detection. Additionally, the paper contributes substantial labeled data to the NuScenes dataset.

Conclusion: The proposed LUR and RLUR methods are effective, efficient, easy to tune, and achieve strong OOD detection performance, offering practical advantages over competing methodologies that rely on complex sampling or training techniques.

Abstract: Deep neural networks (DNNs) are increasingly applied to safety-critical tasks
in resource-constrained environments, such as video-based driver action and
intention recognition. While last layer probabilistic deep learning (LL-PDL)
methods can detect out-of-distribution (OOD) instances, their performance
varies. As an alternative to last layer approaches, we propose extending
pre-trained DNNs with transformation layers to produce multiple latent
representations to estimate the uncertainty. We evaluate our latent uncertainty
representation (LUR) and repulsively trained LUR (RLUR) approaches against
eight PDL methods across four video-based driver action and intention
recognition datasets, comparing classification performance, calibration, and
uncertainty-based OOD detection. We also contribute 28,000 frame-level action
labels and 1,194 video-level intention labels for the NuScenes dataset. Our
results show that LUR and RLUR achieve comparable in-distribution
classification performance to other LL-PDL approaches. For uncertainty-based
OOD detection, LUR matches top-performing PDL methods while being more
efficient to train and easier to tune than approaches that require Markov-Chain
Monte Carlo sampling or repulsive training procedures.

</details>


### [365] [Exploring the Efficacy of Modified Transfer Learning in Identifying Parkinson's Disease Through Drawn Image Patterns](https://arxiv.org/abs/2510.05015)
*Nabil Daiyan,Md Rakibul Haque*

Main category: cs.CV

TL;DR: This study proposes a machine learning method to diagnose Parkinson's disease (PD) using hand-drawn spirals and waves, achieving high accuracy rates through enhanced CNN architectures.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the need for early and cost-effective diagnosis of Parkinson's disease, as traditional methods are cumbersome and costly.

Method: The study uses CNNs, transfer learning, and attention mechanisms on augmented datasets of hand-drawn spirals and waves. It combines pre-trained and custom convolutional layers, with ensemble hard voting for predictions.

Result: The proposed model demonstrates high performance, achieving 90% F1-scores for spirals, 96.67% for waves, and an overall accuracy of 93.3% using ensemble voting.

Conclusion: The findings highlight the feasibility of using machine learning for early, accurate, and non-invasive diagnosis of Parkinson's disease, potentially improving patient care outcomes.

Abstract: Parkinson's disease (PD) is a progressive neurodegenerative condition
characterized by the death of dopaminergic neurons, leading to various movement
disorder symptoms. Early diagnosis of PD is crucial to prevent adverse effects,
yet traditional diagnostic methods are often cumbersome and costly. In this
study, a machine learning-based approach is proposed using hand-drawn spiral
and wave images as potential biomarkers for PD detection. Our methodology
leverages convolutional neural networks (CNNs), transfer learning, and
attention mechanisms to improve model performance and resilience against
overfitting. To enhance the diversity and richness of both spiral and wave
categories, the training dataset undergoes augmentation to increase the number
of images. The proposed architecture comprises three phases: utilizing
pre-trained CNNs, incorporating custom convolutional layers, and ensemble
voting. Employing hard voting further enhances performance by aggregating
predictions from multiple models. Experimental results show promising accuracy
rates. For spiral images, weighted average precision, recall, and F1-score are
90%, and for wave images, they are 96.67%. After combining the predictions
through ensemble hard voting, the overall accuracy is 93.3%. These findings
underscore the potential of machine learning in early PD diagnosis, offering a
non-invasive and cost-effective solution to improve patient outcomes.

</details>


### [366] [Video-LMM Post-Training: A Deep Dive into Video Reasoning with Large Multimodal Models](https://arxiv.org/abs/2510.05034)
*Yunlong Tang,Jing Bi,Pinxin Liu,Zhenyu Pan,Zhangyun Tan,Qianxiang Shen,Jiani Liu,Hang Hua,Junjia Guo,Yunzhong Xiao,Chao Huang,Zhiyuan Wang,Susan Liang,Xinyi Liu,Yizhi Song,Yuhe Nie,Jia-Xing Zhong,Bozheng Li,Daiqing Qi,Ziyun Zeng,Ali Vosoughi,Luchuan Song,Zeliang Zhang,Daiki Shimada,Han Liu,Jiebo Luo,Chenliang Xu*

Main category: cs.CV

TL;DR: This paper surveys post-training methodologies for video-large multimodal models (Video-LMMs), offering a structured taxonomy and insights to address video-specific challenges.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive understanding of the fragmented literature on post-training techniques for Video-LMMs, ensuring future advancements are based on a unified framework.

Method: A taxonomy is developed to explore post-training methodologies like supervised fine-tuning, reinforcement learning, and test-time scaling, addressing challenges in temporal and multimodal aspects.

Result: Key principles, insights, and challenges are synthesized, along with curated benchmarks, strategies, and evaluation protocols for assessing post-training success.

Conclusion: The survey aims to guide researchers and practitioners in advancing Video-LMMs by clarifying effective post-training practices and highlighting areas needing further exploration.

Abstract: Video understanding represents the most challenging frontier in computer
vision, requiring models to reason about complex spatiotemporal relationships,
long-term dependencies, and multimodal evidence. The recent emergence of
Video-Large Multimodal Models (Video-LMMs), which integrate visual encoders
with powerful decoder-based language models, has demonstrated remarkable
capabilities in video understanding tasks. However, the critical phase that
transforms these models from basic perception systems into sophisticated
reasoning engines, post-training, remains fragmented across the literature.
This survey provides the first comprehensive examination of post-training
methodologies for Video-LMMs, encompassing three fundamental pillars:
supervised fine-tuning (SFT) with chain-of-thought, reinforcement learning (RL)
from verifiable objectives, and test-time scaling (TTS) through enhanced
inference computation. We present a structured taxonomy that clarifies the
roles, interconnections, and video-specific adaptations of these techniques,
addressing unique challenges such as temporal localization, spatiotemporal
grounding, long video efficiency, and multimodal evidence integration. Through
systematic analysis of representative methods, we synthesize key design
principles, insights, and evaluation protocols while identifying critical open
challenges in reward design, scalability, and cost-performance optimization. We
further curate essential benchmarks, datasets, and metrics to facilitate
rigorous assessment of post-training effectiveness. This survey aims to provide
researchers and practitioners with a unified framework for advancing Video-LMM
capabilities. Additional resources and updates are maintained at:
https://github.com/yunlong10/Awesome-Video-LMM-Post-Training

</details>


### [367] [SegMASt3R: Geometry Grounded Segment Matching](https://arxiv.org/abs/2510.05051)
*Rohit Jayanti,Swayam Agrawal,Vansh Garg,Siddharth Tourani,Muhammad Haris Khan,Sourav Garg,Madhava Krishna*

Main category: cs.CV

TL;DR: The paper proposes a new architecture leveraging 3D foundation models to enhance wide-baseline segment matching, achieving up to 30% improvement on ScanNet++ and Replica datasets.


<details>
  <summary>Details</summary>
Motivation: To address limitations of traditional segment matching methods in handling extreme viewpoint changes, occlusions, and lighting variations by leveraging 3D spatial understanding.

Method: The authors designed a model architecture incorporating inductive bias of 3D foundation models for segment matching across image pairs with up to 180-degree viewpoint variability.

Result: The approach significantly outperforms state-of-the-art methods, achieving up to 30% improvement on the AUPRC metric for wide-baseline segment matching.

Conclusion: The proposed method enhances robustness in challenging computer vision tasks and confirms its utility in applications like 3D instance segmentation and navigation.

Abstract: Segment matching is an important intermediate task in computer vision that
establishes correspondences between semantically or geometrically coherent
regions across images. Unlike keypoint matching, which focuses on localized
features, segment matching captures structured regions, offering greater
robustness to occlusions, lighting variations, and viewpoint changes. In this
paper, we leverage the spatial understanding of 3D foundation models to tackle
wide-baseline segment matching, a challenging setting involving extreme
viewpoint shifts. We propose an architecture that uses the inductive bias of
these 3D foundation models to match segments across image pairs with up to 180
degree view-point change. Extensive experiments show that our approach
outperforms state-of-the-art methods, including the SAM2 video propagator and
local feature matching methods, by upto 30% on the AUPRC metric, on ScanNet++
and Replica datasets. We further demonstrate benefits of the proposed model on
relevant downstream tasks, including 3D instance segmentation and image-goal
navigation. Project Page: https://segmast3r.github.io/

</details>


### [368] [No-reference Quality Assessment of Contrast-distorted Images using Contrast-enhanced Pseudo Reference](https://arxiv.org/abs/2510.05053)
*Mohammad-Ali Mahmoudpour,Saeed Mahmoudpour*

Main category: cs.CV

TL;DR: This paper introduces a no-reference image quality assessment method for contrast-distorted images by generating pseudo-reference images and employing full-reference evaluation techniques.


<details>
  <summary>Details</summary>
Motivation: Contrast distortion is often overlooked in image quality assessments, despite its distinct visual impacts compared to other distortions like blur or noise.

Method: The method involves generating pseudo-reference images using contrast enhancement algorithms and training a classification network to select optimal algorithms for specific image content and distortions.

Result: The proposed method demonstrated promising performance on databases including CCID2014, TID2013, and CSIQ, successfully assessing contrast distortions.

Conclusion: The approach transforms no-reference assessments into full-reference evaluations for higher accuracy, effectively handling contrast distortions in images.

Abstract: Contrast change is an important factor that affects the quality of images.
During image capturing, unfavorable lighting conditions can cause contrast
change and visual quality loss. While various methods have been proposed to
assess the quality of images under different distortions such as blur and
noise, contrast distortion has been largely overlooked as its visual impact and
properties are different from other conventional types of distortions. In this
paper, we propose a no-reference image quality assessment (NR-IQA) metric for
contrast-distorted images. Using a set of contrast enhancement algorithms, we
aim to generate pseudo-reference images that are visually close to the actual
reference image, such that the NR problem is transformed to a Full-reference
(FR) assessment with higher accuracy. To this end, a large dataset of
contrast-enhanced images is produced to train a classification network that can
select the most suitable contrast enhancement algorithm based on image content
and distortion for pseudo-reference image generation. Finally, the evaluation
is performed in the FR manner to assess the quality difference between the
contrast-enhanced (pseudoreference) and degraded images. Performance evaluation
of the proposed method on three databases containing contrast distortions
(CCID2014, TID2013, and CSIQ), indicates the promising performance of the
proposed method.

</details>


### [369] [Neuroplastic Modular Framework: Cross-Domain Image Classification of Garbage and Industrial Surfaces](https://arxiv.org/abs/2510.05071)
*Debojyoti Ghosh,Soumya K Ghosh,Adrijit Goswami*

Main category: cs.CV

TL;DR: The paper presents the Neuroplastic Modular Classifier, a hybrid architecture for adaptive and robust image classification, excelling in both waste management and industrial defect detection.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of accurate and efficient classification in sustainable waste management and high-quality industrial defect detection.

Method: The method integrates a ResNet-50 backbone for local feature extraction, a Vision Transformer for global context, and FAISS-based similarity retrieval for memory-like reference. It further introduces a neuroplastic modular design that dynamically grows during training to enhance adaptability and generalization.

Result: Experimental results indicate superior accuracy and adaptability of the proposed model compared to traditional static models, validated on garbage classification datasets and the KolektorSDD2 dataset.

Conclusion: The Neuroplastic Modular Classifier proves to be a scalable and high-performance solution for image classification tasks in both environmental and industrial domains, demonstrating adaptability and improved generalization.

Abstract: Efficient and accurate classification of waste and industrial surface defects
is essential for ensuring sustainable waste management and maintaining high
standards in quality control. This paper introduces the Neuroplastic Modular
Classifier, a novel hybrid architecture designed for robust and adaptive image
classification in dynamic environments. The model combines a ResNet-50 backbone
for localized feature extraction with a Vision Transformer (ViT) to capture
global semantic context. Additionally, FAISS-based similarity retrieval is
incorporated to provide a memory-like reference to previously encountered data,
enriching the model's feature space. A key innovation of our architecture is
the neuroplastic modular design composed of expandable, learnable blocks that
dynamically grow during training when performance plateaus. Inspired by
biological learning systems, this mechanism allows the model to adapt to data
complexity over time, improving generalization. Beyond garbage classification,
we validate the model on the Kolektor Surface Defect Dataset 2 (KolektorSDD2),
which involves industrial defect detection on metal surfaces. Experimental
results across domains show that the proposed architecture outperforms
traditional static models in both accuracy and adaptability. The Neuroplastic
Modular Classifier offers a scalable, high-performance solution for real-world
image classification, with strong applicability in both environmental and
industrial domains.

</details>


### [370] [Factuality Matters: When Image Generation and Editing Meet Structured Visuals](https://arxiv.org/abs/2510.05091)
*Le Zhuo,Songhao Han,Yuandong Pu,Boxiang Qiu,Sayak Paul,Yue Liao,Yihao Liu,Jie Shao,Xi Chen,Si Liu,Hongsheng Li*

Main category: cs.CV

TL;DR: The paper addresses existing challenges in creating and editing structured visuals, proposing a novel dataset, model, and benchmark to improve factual accuracy and multimodal reasoning. Key contributions include StructBench and StructScore.


<details>
  <summary>Details</summary>
Motivation: Modern visual generation models struggle to produce or edit structured visuals such as charts and diagrams, necessitating improvements in composition planning, text rendering, and multimodal reasoning.

Method: The authors constructed a dataset of 1.3M structured image pairs with reasoning annotations, trained a model integrating a VLM and FLUX framework, employed a three-stage training curriculum, and introduced an external reasoning component during inference.

Result: Their model demonstrated strong editing capabilities, achieved consistent improvements across architectures, and outperformed existing systems on the novel StructBench benchmark, supported by the StructScore metric.

Conclusion: This work aims to advance structured visual generation and editing by releasing the dataset, model, and benchmark for wider community adoption and further multimodal research development.

Abstract: While modern visual generation models excel at creating aesthetically
pleasing natural images, they struggle with producing or editing structured
visuals like charts, diagrams, and mathematical figures, which demand
composition planning, text rendering, and multimodal reasoning for factual
fidelity. To address this, we present the first comprehensive, systematic
investigation of this domain, encompassing data construction, model training,
and an evaluation benchmark. First, we construct a large-scale dataset of 1.3
million high-quality structured image pairs derived from executable drawing
programs and augmented with chain-of-thought reasoning annotations. Building on
it, we train a unified model that integrates a VLM with FLUX.1 Kontext via a
lightweight connector for enhanced multimodal understanding. A three-stage
training curriculum enables progressive feature alignment, knowledge infusion,
and reasoning-augmented generation, further boosted by an external reasoner at
inference time. Finally, we introduce StructBench, a novel benchmark for
generation and editing with over 1,700 challenging instances, and an
accompanying evaluation metric, StructScore, which employs a multi-round Q\&A
protocol to assess fine-grained factual accuracy. Evaluations of 15 models
reveal that even leading closed-source systems remain far from satisfactory.
Our model attains strong editing performance, and inference-time reasoning
yields consistent gains across diverse architectures. By releasing the dataset,
model, and benchmark, we aim to advance unified multimodal foundations for
structured visuals.

</details>


### [371] [Character Mixing for Video Generation](https://arxiv.org/abs/2510.05093)
*Tingting Liao,Chongjian Ge,Guangyi Liu,Hao Li,Yi Zhou*

Main category: cs.CV

TL;DR: The paper introduces techniques for generating videos where characters from different styles or "worlds" interact naturally while preserving their identities and behaviors.


<details>
  <summary>Details</summary>
Motivation: To enable natural and coherent interactions between characters from different styles or "worlds" in generated videos, overcoming challenges such as identity preservation and style delusion.

Method: The authors propose two core techniques: Cross-Character Embedding (CCE) to learn identity and behavioral logic, and Cross-Character Augmentation (CCA) to create synthetic data for training. These methods tackle issues like style delusion and interaction coherence.

Result: Experiments demonstrate clear improvements in preserving character identity, interaction quality, and maintaining stylistic fidelity while enabling cross-context interactions.

Conclusion: The proposed framework allows for natural, stylistically faithful interactions between characters from different styles, expanding possibilities for generative storytelling.

Abstract: Imagine Mr. Bean stepping into Tom and Jerry--can we generate videos where
characters interact naturally across different worlds? We study inter-character
interaction in text-to-video generation, where the key challenge is to preserve
each character's identity and behaviors while enabling coherent cross-context
interaction. This is difficult because characters may never have coexisted and
because mixing styles often causes style delusion, where realistic characters
appear cartoonish or vice versa. We introduce a framework that tackles these
issues with Cross-Character Embedding (CCE), which learns identity and
behavioral logic across multimodal sources, and Cross-Character Augmentation
(CCA), which enriches training with synthetic co-existence and mixed-style
data. Together, these techniques allow natural interactions between previously
uncoexistent characters without losing stylistic fidelity. Experiments on a
curated benchmark of cartoons and live-action series with 10 characters show
clear improvements in identity preservation, interaction quality, and
robustness to style delusion, enabling new forms of generative
storytelling.Additional results and videos are available on our project page:
https://tingtingliao.github.io/mimix/.

</details>


### [372] [VChain: Chain-of-Visual-Thought for Reasoning in Video Generation](https://arxiv.org/abs/2510.05094)
*Ziqi Huang,Ning Yu,Gordon Chen,Haonan Qiu,Paul Debevec,Ziwei Liu*

Main category: cs.CV

TL;DR: VChain enhances video generation by integrating multimodal models' visual reasoning to create keyframes that guide a pre-trained generator, improving complex scenario outputs.


<details>
  <summary>Details</summary>
Motivation: To overcome the challenge of synthesizing complex video dynamics and coherent transitions, as current models often lack robust visual reasoning.

Method: VChain uses multimodal models to generate keyframes that inform sparse, inference-time tuning of a pre-trained video generator.

Result: VChain significantly improves the quality of generated videos in complex and multi-step scenarios.

Conclusion: Introducing multimodal reasoning into video generation boosts coherence and dynamic accuracy with minimal tuning overhead.

Abstract: Recent video generation models can produce smooth and visually appealing
clips, but they often struggle to synthesize complex dynamics with a coherent
chain of consequences. Accurately modeling visual outcomes and state
transitions over time remains a core challenge. In contrast, large language and
multimodal models (e.g., GPT-4o) exhibit strong visual state reasoning and
future prediction capabilities. To bridge these strengths, we introduce VChain,
a novel inference-time chain-of-visual-thought framework that injects visual
reasoning signals from multimodal models into video generation. Specifically,
VChain contains a dedicated pipeline that leverages large multimodal models to
generate a sparse set of critical keyframes as snapshots, which are then used
to guide the sparse inference-time tuning of a pre-trained video generator only
at these key moments. Our approach is tuning-efficient, introduces minimal
overhead and avoids dense supervision. Extensive experiments on complex,
multi-step scenarios show that VChain significantly enhances the quality of
generated videos.

</details>


### [373] [Paper2Video: Automatic Video Generation from Scientific Papers](https://arxiv.org/abs/2510.05096)
*Zeyu Zhu,Kevin Qinghong Lin,Mike Zheng Shou*

Main category: cs.CV

TL;DR: This paper introduces PaperTalker, a benchmark and framework for automating academic presentation video generation from research papers.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of labor-intensive academic presentation video creation, which involves translating research papers into videos incorporating dense multi-modal information, such as slides, speech, and subtitles.

Method: The authors introduced a dataset containing 101 research papers paired with presentation videos and designed evaluation metrics to assess video informativeness. They proposed a multi-agent framework leveraging slide generation, speech synthesis, subtitling, and talking-head rendering, optimized by parallel slide-wise generation.

Result: The PaperTalker framework produced academic presentation videos that were more faithful and informative compared to existing methods, as demonstrated through experiments on the Paper2Video benchmark.

Conclusion: The study provides a practical step toward automating academic presentation video creation, offering tools and benchmarks for effective and efficient research communication.

Abstract: Academic presentation videos have become an essential medium for research
communication, yet producing them remains highly labor-intensive, often
requiring hours of slide design, recording, and editing for a short 2 to 10
minutes video. Unlike natural video, presentation video generation involves
distinctive challenges: inputs from research papers, dense multi-modal
information (text, figures, tables), and the need to coordinate multiple
aligned channels such as slides, subtitles, speech, and human talker. To
address these challenges, we introduce PaperTalker, the first benchmark of 101
research papers paired with author-created presentation videos, slides, and
speaker metadata. We further design four tailored evaluation metrics--Meta
Similarity, PresentArena, PresentQuiz, and IP Memory--to measure how videos
convey the paper's information to the audience. Building on this foundation, we
propose PaperTalker, the first multi-agent framework for academic presentation
video generation. It integrates slide generation with effective layout
refinement by a novel effective tree search visual choice, cursor grounding,
subtitling, speech synthesis, and talking-head rendering, while parallelizing
slide-wise generation for efficiency. Experiments on Paper2Video demonstrate
that the presentation videos produced by our approach are more faithful and
informative than existing baselines, establishing a practical step toward
automated and ready-to-use academic video generation. Our dataset, agent, and
code are available at https://github.com/showlab/Paper2Video.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [374] [Cosmological Hydrodynamics at Exascale: A Trillion-Particle Leap in Capability](https://arxiv.org/abs/2510.03557)
*Nicholas Frontiere,J. D. Emberson,Michael Buehlmann,Esteban M. Rangel,Salman Habib,Katrin Heitmann,Patricia Larsen,Vitali Morozov,Adrian Pope,Claude-André Faucher-Giguère,Antigoni Georgiadou,Damien Lebrun-Grandié,Andrey Prokopenko*

Main category: cs.DC

TL;DR: The paper introduces CRK-HACC, a cosmological hydrodynamics code designed for extreme scalability, demonstrated by a frontier simulation involving four trillion particles.


<details>
  <summary>Details</summary>
Motivation: Understanding fundamental cosmological questions and structures requires realistic and large-scale simulations integrated with detailed astrophysical processes.

Method: CRK-HACC employs techniques like GPU-resident tree solvers, separation-of-scale methods, in situ analysis pipelines, and advanced I/O systems to conduct massive simulations.

Result: CRK-HACC successfully performed a four trillion particle simulation utilizing Frontier-E, achieving 513.1 PFLOPs peak performance, processing 46.6 billion particles per second, and generating over 100 PB of data within a week.

Conclusion: The paper showcases CRK-HACC's capability to handle large-scale, highly detailed cosmological simulations, signifying progress in meeting exascale computing demands for next-generation scientific projects.

Abstract: Resolving the most fundamental questions in cosmology requires simulations
that match the scale, fidelity, and physical complexity demanded by
next-generation sky surveys. To achieve the realism needed for this critical
scientific partnership, detailed gas dynamics, along with a host of
astrophysical effects, must be treated self-consistently with gravity for
end-to-end modeling of structure formation. As an important step on this
roadmap, exascale computing enables simulations that span survey-scale volumes
while incorporating key subgrid processes that shape complex cosmic structures.
We present results from CRK-HACC, a cosmological hydrodynamics code built for
the extreme scalability requirements set by modern cosmological surveys. Using
separation-of-scale techniques, GPU-resident tree solvers, in situ analysis
pipelines, and multi-tiered I/O, CRK-HACC executed Frontier-E: a four trillion
particle full-sky simulation, over an order of magnitude larger than previous
efforts. The run achieved 513.1 PFLOPs peak performance, processing 46.6
billion particles per second and writing more than 100 PB of data in just over
one week of runtime.

</details>


### [375] [Datacenter Energy Optimized Power Profiles](https://arxiv.org/abs/2510.03872)
*Sreedhar Narayanaswamy,Pratikkumar Dilipkumar Patel,Ian Karlin,Apoorv Gupta,Sudhir Saripalli,Janey Guo*

Main category: cs.DC

TL;DR: The paper introduces NVIDIA's datacenter power profiles, aimed at optimizing energy efficiency and performance for HPC and AI workloads using Blackwell B200 architecture.


<details>
  <summary>Details</summary>
Motivation: To address energy efficiency and maximize performance in High-Performance Computing (HPC) and AI workloads under strict power constraints.

Method: Introduces intelligent power management leveraging hardware-software innovations and domain knowledge of workload behaviors to create optimization recipes.

Result: Achieves up to 15% energy savings while maintaining above 97% performance, resulting in a throughput increase of up to 13% under power-constrained conditions.

Conclusion: The proposed power management feature effectively balances energy efficiency and performance, enhancing datacenter operations under power limitations.

Abstract: This paper presents datacenter power profiles, a new NVIDIA software feature
released with Blackwell B200, aimed at improving energy efficiency and/or
performance. The initial feature provides coarse-grain user control for HPC and
AI workloads leveraging hardware and software innovations for intelligent power
management and domain knowledge of HPC and AI workloads. The resulting
workload-aware optimization recipes maximize computational throughput while
operating within strict facility power constraints. The phase-1 Blackwell
implementation achieves up to 15% energy savings while maintaining performance
levels above 97% for critical applications, enabling an overall throughput
increase of up to 13% in a power-constrained facility.
  KEYWORDS GPU power management, energy efficiency, power profile, HPC
optimization, Max-Q, Blackwell architecture

</details>


### [376] [Toward Co-adapting Machine Learning Job Shape and Cluster Topology](https://arxiv.org/abs/2510.03891)
*Shawn Shuoshuo Chen,Daiyaan Arfeen,Minlan Yu,Peter Steenkiste,Srinivasan Seshan*

Main category: cs.DC

TL;DR: The paper proposes RFold, a mechanism for optimizing resource allocation in torus-topology clusters for distributed machine learning jobs, enhancing both cluster utilization and job completion time.


<details>
  <summary>Details</summary>
Motivation: Optimizing resource allocation in multi-tenant clusters with diverse job requirements often leads to compromises between minimizing network contention and maximizing cluster utilization.

Method: RFold dynamically adapts job shapes and cluster topology using techniques like identifying compatible job shapes and reconfiguring circuitry via optical switches.

Result: On a 4096-node simulator, RFold boosts cluster utilization by 57% and reduces job completion time up to 11x compared to prior methods.

Conclusion: RFold demonstrates the feasibility of simultaneously optimizing network performance and cluster utilization, offering substantial benefits for distributed learning workloads in complex cluster systems.

Abstract: Allocating resources to distributed machine learning jobs in multi-tenant
torus-topology clusters must meet each job's specific placement and
communication requirements, which are typically described using shapes. There
is an inherent tension between minimizing network contention and maximizing
cluster utilization when placing various-shaped jobs. While existing schedulers
typically optimize for one objective at the expense of the other, we
demonstrate that both can be achieved simultaneously.
  Our proposed approach, RFold, adapts both job shapes and the underlying
cluster topology at runtime. This is accomplished by combining two techniques:
(1) identifying homomorphic job shapes that support the jobs communication
needs, and (2) reconfiguring the optical circuit switch-enabled topology to
support more diverse job shapes. Preliminary evaluation performed on a
4096-node torus cluster simulator indicates that RFold can improve absolute
cluster utilization by 57% and reduce job completion time by up to 11x relative
to existing methods

</details>


### [377] [Towards Carbon-Aware Container Orchestration: Predicting Workload Energy Consumption with Federated Learning](https://arxiv.org/abs/2510.03970)
*Zainab Saad,Jialin Yang,Henry Leung,Steve Drew*

Main category: cs.DC

TL;DR: Proposes a federated learning (FL) method for energy prediction in data centers, reducing privacy concerns and enhancing prediction accuracy.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the growing carbon footprint of large-scale data centers while highlighting privacy concerns with centralized machine learning models in existing solutions.

Method: Develops an FL framework by extending Kubernetes' Kepler, utilizing Flower's FedXgbBagging with XGBoost models to predict energy consumption while maintaining data privacy.

Result: The FL approach achieved an 11.7% lower Mean Absolute Error compared to a centralized model on the SPECPower dataset.

Conclusion: The proposed framework effectively resolves the trade-off between data privacy and energy prediction efficiency, offering a sustainable and private cloud computing solution.

Abstract: The growing reliance on large-scale data centers to run resource-intensive
workloads has significantly increased the global carbon footprint, underscoring
the need for sustainable computing solutions. While container orchestration
platforms like Kubernetes help optimize workload scheduling to reduce carbon
emissions, existing methods often depend on centralized machine learning models
that raise privacy concerns and struggle to generalize across diverse
environments. In this paper, we propose a federated learning approach for
energy consumption prediction that preserves data privacy by keeping sensitive
operational data within individual enterprises. By extending the Kubernetes
Efficient Power Level Exporter (Kepler), our framework trains XGBoost models
collaboratively across distributed clients using Flower's FedXgbBagging
aggregation using a bagging strategy, eliminating the need for centralized data
sharing. Experimental results on the SPECPower benchmark dataset show that our
FL-based approach achieves 11.7 percent lower Mean Absolute Error compared to a
centralized baseline. This work addresses the unresolved trade-off between data
privacy and energy prediction efficiency in prior systems such as Kepler and
CASPER and offers enterprises a viable pathway toward sustainable cloud
computing without compromising operational privacy.

</details>


### [378] [From Patchwork to Network: A Comprehensive Framework for Demand Analysis and Fleet Optimization of Urban Air Mobility](https://arxiv.org/abs/2510.04186)
*Xuan Jiang,Xuanyu Zhou,Yibo Zhao,Shangqing Cao,Jinhua Zhao,Mark Hansen,Raja Sengupta*

Main category: cs.DC

TL;DR: The paper addresses challenges in Urban Air Mobility (UAM) implementation by using existing regional airports and optimizing fleet operations through a simulation framework (LPSim), achieving significant travel time savings.


<details>
  <summary>Details</summary>
Motivation: To overcome the barriers of high infrastructure costs and operational complexities in UAM and make metropolitan transportation more practical.

Method: Introduced LPSim, a parallel simulation framework using multi-GPU computing to optimize UAM demand, fleet operations, and integration with ground transportation.

Result: In a San Francisco Bay Area case study, the model achieved over 20 minutes of travel time savings for 230,000 trips, but highlighted the need for efficient ground access and scheduling.

Conclusion: Successful implementation of UAM requires not only optimized aircraft fleets but also effective integration with ground transportation and dynamic scheduling.

Abstract: Urban Air Mobility (UAM) presents a transformative vision for metropolitan
transportation, but its practical implementation is hindered by substantial
infrastructure costs and operational complexities. We address these challenges
by modeling a UAM network that leverages existing regional airports and
operates with an optimized, heterogeneous fleet of aircraft. We introduce
LPSim, a Large-Scale Parallel Simulation framework that utilizes multi-GPU
computing to co-optimize UAM demand, fleet operations, and ground
transportation interactions simultaneously. Our equilibrium search algorithm is
extended to accurately forecast demand and determine the most efficient fleet
composition. Applied to a case study of the San Francisco Bay Area, our results
demonstrate that this UAM model can yield over 20 minutes' travel time savings
for 230,000 selected trips. However, the analysis also reveals that system-wide
success is critically dependent on seamless integration with ground access and
dynamic scheduling.

</details>


### [379] [Beyond Canonical Rounds: Communication Abstractions for Optimal Byzantine Resilience](https://arxiv.org/abs/2510.04310)
*Hagit Attiya,Itay Flam,Jennifer L. Welch*

Main category: cs.DC

TL;DR: The paper critiques traditional asynchronous round-based frameworks for Byzantine fault-tolerant systems and proposes richer communication patterns like 'gather' for better resilience and modular algorithm design.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address challenges in designing distributed algorithms with optimal failure resilience in the critical regime $3f < n \le 5f$, where existing frameworks struggle.

Method: The authors examine the limitations of canonical asynchronous rounds and communication-closed layers, demonstrate unsolvability in certain conditions, and propose the 'gather' abstraction as a better alternative.

Result: Key tasks like approximate agreement and reliable broadcast were proven unsolvable under traditional models in the critical regime. A constant-time gather abstraction was shown to enable optimal resilience and support modular reductions.

Conclusion: Round-based abstractions are analytically convenient but unsuitable for optimal-resilience design in Byzantine fault tolerance. The gather abstraction is a superior alternative for modular solutions.

Abstract: We study communication abstractions for asynchronous Byzantine fault
tolerance with optimal failure resilience, where $n > 3f$. Two classic patterns
-- canonical asynchronous rounds and communication-closed layers -- have long
been considered as general frameworks for designing distributed algorithms,
making asynchronous executions appear synchronous and enabling modular
reasoning.
  We show that these patterns are inherently limited in the critical resilience
regime $3f < n \le 5f$. Several key tasks -- such as approximate and crusader
agreement, reliable broadcast and gather -- cannot be solved by bounded-round
canonical-round algorithms, and are unsolvable if communication closure is
imposed. These results explain the historical difficulty of achieving
optimal-resilience algorithms within round-based frameworks.
  On the positive side, we show that the gather abstraction admits
constant-time solutions with optimal resilience ($n > 3f$), and supports
modular reductions. Specifically, we present the first optimally-resilient
algorithm for connected consensus by reducing it to gather.
  Our results demonstrate that while round-based abstractions are analytically
convenient, they obscure the true complexity of Byzantine fault-tolerant
algorithms. Richer communication patterns such as gather provide a better
foundation for modular, optimal-resilience design.

</details>


### [380] [Next-Generation Event-Driven Architectures: Performance, Scalability, and Intelligent Orchestration Across Messaging Frameworks](https://arxiv.org/abs/2510.04404)
*Jahidul Arafat,Fariha Tasmin,Sanjaya Poudel,Ahsan Habib Tareq*

Main category: cs.DC

TL;DR: This study introduces a benchmarking framework for analyzing 12 messaging systems under three workloads and evaluates their performance trade-offs using AIEO—a machine learning and optimization-driven orchestration.


<details>
  <summary>Details</summary>
Motivation: To address the lack of unified comparative studies evaluating contemporary messaging systems under standardized conditions.

Method: The paper uses AIEO (AI-Enhanced Event Orchestration) for predictive scaling, dynamic resource allocation, and optimization, alongside comprehensive benchmarking for 12 messaging systems across standardized workloads.

Result: Apache Kafka achieves highest throughput but has operational demands; Apache Pulsar offers balance with better multi-tenancy; Serverless solutions excel in elasticity but have higher latency. AIEO improves latency, resource utilization, and cost efficiency significantly.

Conclusion: This study provides benchmarking methodologies, open-source orchestration tools, and actionable insights for distributed system design, enabling informed technology decisions.

Abstract: Modern distributed systems demand low-latency, fault-tolerant event
processing that exceeds traditional messaging architecture limits. While
frameworks including Apache Kafka, RabbitMQ, Apache Pulsar, NATS JetStream, and
serverless event buses have matured significantly, no unified comparative study
evaluates them holistically under standardized conditions. This paper presents
the first comprehensive benchmarking framework evaluating 12 messaging systems
across three representative workloads: e-commerce transactions, IoT telemetry
ingestion, and AI inference pipelines. We introduce AIEO (AI-Enhanced Event
Orchestration), employing machine learning-driven predictive scaling,
reinforcement learning for dynamic resource allocation, and multi-objective
optimization. Our evaluation reveals fundamental trade-offs: Apache Kafka
achieves peak throughput (1.2M messages/sec, 18ms p95 latency) but requires
substantial operational expertise; Apache Pulsar provides balanced performance
(950K messages/sec, 22ms p95) with superior multi-tenancy; serverless solutions
offer elastic scaling for variable workloads despite higher baseline latency
(80-120ms p95). AIEO demonstrates 34\% average latency reduction, 28\% resource
utilization improvement, and 42% cost optimization across all platforms. We
contribute standardized benchmarking methodologies, open-source intelligent
orchestration, and evidence-based decision guidelines. The evaluation
encompasses 2,400+ experimental configurations with rigorous statistical
analysis, providing comprehensive performance characterization and establishing
foundations for next-generation distributed system design.

</details>


### [381] [The R(1)W(1) Communication Model for Self-Stabilizing Distributed Algorithms](https://arxiv.org/abs/2510.04644)
*Hirotsugu Kakugawa,Sayaka Kamei,Masahiro Shibata,Fukuhito Ooshita*

Main category: cs.DC

TL;DR: This paper introduces the R(1)W(1) model for designing self-stabilizing distributed algorithms, addressing problems like maximal matching and minimal k-dominating sets, and provides a simulation transformer for adapting these algorithms to a synchronous message-passing model.


<details>
  <summary>Details</summary>
Motivation: The paper aims to improve fault-tolerant distributed systems by developing methods that can automatically recover from transient faults in systems with numerous components.

Method: The authors introduce the R(1)W(1) model, where processes can read and write their own and neighbors' variables in a single step. They also design self-stabilizing algorithms for specific distributed problems and propose a transformer to adapt these algorithms to another model.

Result: Self-stabilizing algorithms are proposed for maximal matching, minimal k-dominating set, and maximal k-dependent set under the R(1)W(1) model. A transformer enables simulation in synchronous message-passing systems.

Conclusion: The R(1)W(1) model and its corresponding algorithms enhance the fault tolerance of distributed systems, and the transformer bridges the gap for use in practical synchronous environments.

Abstract: Self-stabilization is a versatile methodology in the design of fault-tolerant
distributed algorithms for transient faults. A self-stabilizing system
automatically recovers from any kind and any finite number of transient faults.
This property is specifically useful in modern distributed systems with a large
number of components. In this paper, we propose a new communication and
execution model named the R(1)W(1) model in which each process can read and
write its own and neighbors' local variables in a single step. We propose
self-stabilizing distributed algorithms in the R(1)W(1) model for the problems
of maximal matching, minimal k-dominating set and maximal k-dependent set.
Finally, we propose an example transformer, based on randomized distance-two
local mutual exclusion, to simulate algorithms designed for the R(1)W(1) model
in the synchronous message passing model with synchronized clocks.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [382] [PARS: Low-Latency LLM Serving via Pairwise Learning-to-Rank](https://arxiv.org/abs/2510.03243)
*Yiheng Tao,Yihe Zhang,Matthew T. Dearing,Xin Wang,Yuping Fan,Zhiling Lan*

Main category: cs.LG

TL;DR: PARS optimizes LLM inference scheduling by predicting task response lengths, minimizing latency, and overcoming FCFS limitations.


<details>
  <summary>Details</summary>
Motivation: Traditional FCFS scheduling leads to inefficiencies like Head-of-Line blocking, particularly with LLM-scale workloads.

Method: PARS uses a margin-ranking loss to approximate shortest-job-first scheduling and integrates into the vLLM system.

Result: PARS reduces scheduling latency and overhead, performing well across various LLMs and datasets.

Conclusion: PARS generalizes effectively, enabling consistent performance improvement, even across different LLM models.

Abstract: Efficient scheduling of LLM inference tasks is essential for achieving low
latency and high throughput, particularly with the growing use of
reasoning-capable LLMs. Traditional strategies like First-Come-First-Serve
(FCFS) often suffer from Head-of-Line (HOL) blocking, where long-running tasks
delay shorter ones queued behind them. In this paper, we introduce PARS, a
prompt-aware LLM task scheduler that improves serving efficiency by
approximating shortest-job-first (SJF) scheduling through pairwise ranking with
margin ranking loss. PARS focuses on impactful scheduling decisions and is
seamlessly integrated into the state-of-the-art LLM serving system vLLM. It
effectively predicts response-length-based task ordering, reducing latency with
minimal overhead. Extensive experiments across multiple LLMs and real-world
inference datasets show that PARS significantly improves performance, including
for reasoning workloads. Furthermore, our cross-model evaluations demonstrate
that the design generalizes well, enabling effective scheduling even when
predictors are trained on different LLMs.

</details>


### [383] [VIFO: Visual Feature Empowered Multivariate Time Series Forecasting with Cross-Modal Fusion](https://arxiv.org/abs/2510.03244)
*Yanlong Wang,Hang Yu,Jian Xu,Fei Ma,Hongkang Zhang,Tongtong Feng,Zijian Zhang,Shao-Lun Huang,Danny Dongning Sun,Xiao-Ping Zhang*

Main category: cs.LG

TL;DR: The paper introduces VIFO, a forecasting model that converts time series data into images to leverage pre-trained large vision models (LVMs) for improved cross-channel dependency extraction and forecasting.


<details>
  <summary>Details</summary>
Motivation: Existing time series models often ignore cross-channel dependencies, and multimodal methods haven't fully utilized large vision models or the strengths of multiple modalities for forecasting.

Method: The VIFO model renders multivariate time series data into visual format and utilizes pre-trained LVMs to extract patterns. Then, it aligns and merges these visual features with time series representations, freezing most LVM parameters for efficient training.

Result: VIFO achieves competitive forecasting performance on multiple benchmarks while reducing computational requirements, as only 7.45% of the LVM parameters are trained.

Conclusion: VIFO effectively captures cross-channel relationships and offers an efficient, multimodal approach for time series forecasting by exploiting the strengths of LVMs and multiple modalities.

Abstract: Large time series foundation models often adopt channel-independent
architectures to handle varying data dimensions, but this design ignores
crucial cross-channel dependencies. Concurrently, existing multimodal
approaches have not fully exploited the power of large vision models (LVMs) to
interpret spatiotemporal data. Additionally, there remains significant
unexplored potential in leveraging the advantages of information extraction
from different modalities to enhance time series forecasting performance. To
address these gaps, we propose the VIFO, a cross-modal forecasting model. VIFO
uniquely renders multivariate time series into image, enabling pre-trained LVM
to extract complex cross-channel patterns that are invisible to
channel-independent models. These visual features are then aligned and fused
with representations from the time series modality. By freezing the LVM and
training only 7.45% of its parameters, VIFO achieves competitive performance on
multiple benchmarks, offering an efficient and effective solution for capturing
cross-variable relationships in

</details>


### [384] [Frequency-Aware Model Parameter Explorer: A new attribution method for improving explainability](https://arxiv.org/abs/2510.03245)
*Ali Yavari,Alireza Mohamadi,Elham Beydaghi,Rainer A. Leitgeb*

Main category: cs.LG

TL;DR: The paper introduces transferable frequency-aware adversarial attacks to improve DNN reliability, alongside a novel attribution method, FAMPE, which enhances explainability by leveraging frequency components.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the challenge of ensuring DNN reliability amidst real-world noise and intentional perturbations, given the limitations of current attribution methods.

Method: The approach involves introducing transferable frequency-aware attacks that leverage high- and low-frequency components and creating the Frequency-Aware Model Parameter Explorer (FAMPE) to enhance DNN explainability.

Result: FAMPE exhibits an average improvement of 13.02% in the Insertion Score compared to existing methods like AttEXplore, validating its efficacy.

Conclusion: The findings demonstrate that frequency-aware exploration enhances explainability of DNNs and improves over prior attribution methods, with insights from high- and low-frequency components further enriching understanding.

Abstract: Ensuring the reliability of deep neural networks (DNNs) in the presence of
real world noise and intentional perturbations remains a significant challenge.
To address this, attribution methods have been proposed, though their efficacy
remains suboptimal and necessitates further refinement. In this paper, we
propose a novel category of transferable adversarial attacks, called
transferable frequency-aware attacks, enabling frequency-aware exploration via
both high-and low-frequency components. Based on this type of attacks, we also
propose a novel attribution method, named Frequency-Aware Model Parameter
Explorer (FAMPE), which improves the explainability for DNNs. Relative to the
current state-of-the-art method AttEXplore, our FAMPE attains an average gain
of 13.02% in Insertion Score, thereby outperforming existing approaches.
Through detailed ablation studies, we also investigate the role of both high-
and low-frequency components in explainability.

</details>


### [385] [StructPrune: Structured Global Pruning asymptotics with $\mathcal{O}(\sqrt{N})$ GPU Memory](https://arxiv.org/abs/2510.03246)
*Xinyuan Song,Guangji Bai,Liang Zhao*

Main category: cs.LG

TL;DR: STRUPRUNE is a structured pruning framework for large language models that balances hardware efficiency, memory constraints, and performance. It uses a divide-and-conquer strategy to achieve sparsity at scale.


<details>
  <summary>Details</summary>
Motivation: Enable structured pruning for billion-parameter language models without the excessive memory requirements of global pruning, while maintaining performance.

Method: Proposed STRUPRUNE, which uses a divide-and-conquer strategy with ADMM-based optimization to perform memory-efficient structured pruning. It provides rules for sparsity allocation using closed-form analytical and energy-based frameworks.

Result: STRUPRUNE achieves performance comparable to global structured pruning in terms of perplexity, while reducing memory usage from O(N) to O(sqrt(N)).

Conclusion: STRUPRUNE enables scalable and hardware-efficient structured pruning for large language models, making it practical to deploy sparsity in billion-parameter scales.

Abstract: Pruning is critical for scaling large language models (LLMs). Global pruning
achieves strong performance but requires $\mathcal{O}(N)$ memory, which is
infeasible for billion-parameter models. Local pruning reduces GPU memory usage
to that of a single layer by pruning layers independently, but it neglects
inter-layer dependencies and often leads to suboptimal performance in
high-sparsity regimes. Unlike unstructured pruning, structured pruning produces
regular sparsity patterns that align well with GPU kernels and library
optimizations, making it more hardware-efficient. However, structured pruning
typically relies on global pruning, since structured patterns are more prone to
severe performance degradation under local optimization. To jointly achieve
structured pruning and the memory efficiency of local pruning, we propose a
divide-and-conquer strategy that decomposes the global pruning problem into
coordinated subproblems across different modules, each of which fits within
limited GPU memory. Building on this idea, we design \textbf{STRUPRUNE}, an
ADMM-based framework that integrates structured sparsity into the pruning
process, combining the memory efficiency of local pruning with the hardware
compatibility of structured methods. We derive a closed-form analytical
solution for structured pruning masks that provides an explicit rule for
layer-wise sparsity allocation, and further develop an energy-based asymptotic
framework yielding a softmax-form allocation scheme that simplifies
optimization while adapting to heterogeneous layer importance. Experiments
demonstrate that STRUPRUNE matches the perplexity of global structured pruning
while reducing memory cost from $\mathcal{O}(N)$ to $\mathcal{O}(\sqrt{N})$,
enabling practical deployment at the billion-parameter scale.

</details>


### [386] [Towards Multimodal Active Learning: Efficient Learning with Limited Paired Data](https://arxiv.org/abs/2510.03247)
*Jiancheng Zhang,Yinglun Zhu*

Main category: cs.LG

TL;DR: The paper presents a novel framework for multimodal active learning targeting the annotation cost burden in acquiring cross-modal alignments, leading to up to 40% reduction in annotation needs without loss in accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing active learning approaches address unimodal data but fail to tackle the substantial annotation cost associated with multimodal learning, particularly in acquiring cross-modal alignments.

Method: The proposed algorithm integrates uncertainty and diversity principles in a modality-aware design, operates in linear-time acquisition, and adapts to both pool-based and streaming-based scenarios for multimodal active learning.

Result: The approach consistently reduces the annotation cost in multimodal tasks while maintaining model performance, achieving up to a 40% decrease in annotation requirements on datasets like ColorSwap.

Conclusion: The study introduces an effective, scalable multimodal active learning framework that addresses the practical bottlenecks in multimodal pipelines, optimizing annotation efficiency without sacrificing accuracy.

Abstract: Active learning (AL) is a principled strategy to reduce annotation cost in
data-hungry deep learning. However, existing AL algorithms focus almost
exclusively on unimodal data, overlooking the substantial annotation burden in
multimodal learning. We introduce the first framework for multimodal active
learning with unaligned data, where the learner must actively acquire
cross-modal alignments rather than labels on pre-aligned pairs. This setting
captures the practical bottleneck in modern multimodal pipelines such as CLIP
and SigLIP, where unimodal features are easy to obtain but high-quality
alignment is costly. We develop a new algorithm that combines uncertainty and
diversity principles in a modality-aware design, achieves linear-time
acquisition, and applies seamlessly to both pool-based and streaming-based
settings. Extensive experiments on benchmark datasets demonstrate that our
approach consistently reduces multimodal annotation cost while preserving
performance; for instance, on the ColorSwap dataset it cuts annotation
requirements by up to $40\%$ without loss in accuracy.

</details>


### [387] [LLM-Guided Evolutionary Program Synthesis for Quasi-Monte Carlo Design](https://arxiv.org/abs/2510.03650)
*Amir Sadikov*

Main category: cs.LG

TL;DR: The paper introduces an LLM-guided evolutionary approach to improve quasi-Monte Carlo (QMC) methods for high-dimensional integration, setting new benchmarks for specific QMC designs and reducing errors in 32-dimensional finance tasks.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address two long-standing QMC design challenges by automating the discovery and optimization of QMC constructions using a program synthesis framework driven by large language models (LLMs).

Method: The authors employ a two-phase procedure combining LLM-guided evolutionary programming, incorporating constructive code generation and iterative numerical refinement, to evolve solutions for two QMC problems.

Result: The method rediscovered known optimal designs in 2D cases and achieved new benchmarks for larger 2D sets (N >= 40). It also matched or set improved benchmarks for 3D sets and reduced mean-squared errors in 32-dimensional randomized QMC tasks compared to existing methods.

Conclusion: LLM-driven evolutionary synthesis is an effective tool for improving QMC constructions, capable of uncovering classical designs when they are optimal and discovering improved designs where current methods fall short.

Abstract: Low-discrepancy point sets and digital sequences underpin quasi-Monte Carlo
(QMC) methods for high-dimensional integration. We cast two long-standing QMC
design problems as program synthesis and solve them with an LLM-guided
evolutionary loop that mutates and selects code under task-specific fitness:
(i) constructing finite 2D/3D point sets with low star discrepancy, and (ii)
choosing Sobol' direction numbers that minimize randomized QMC error on
downstream integrands. Our two-phase procedure combines constructive code
proposals with iterative numerical refinement. On finite sets, we rediscover
known optima in small 2D cases and set new best-known 2D benchmarks for N >=
40, while matching most known 3D optima up to the proven frontier (N <= 8) and
reporting improved 3D benchmarks beyond. On digital sequences, evolving Sobol'
parameters yields consistent reductions in randomized quasi-Monte Carlo (rQMC)
mean-squared error for several 32-dimensional option-pricing tasks relative to
widely used Joe--Kuo parameters, while preserving extensibility to any sample
size and compatibility with standard randomizations. Taken together, the
results demonstrate that LLM-driven evolutionary program synthesis can automate
the discovery of high-quality QMC constructions, recovering classical designs
where they are optimal and improving them where finite-N structure matters.
Data and code are available at
https://github.com/hockeyguy123/openevolve-star-discrepancy.git.

</details>


### [388] [Real-Time Brain Biomechanics Prediction with Neural Operators: Toward Clinically Deployable Traumatic Brain Injury Models](https://arxiv.org/abs/2510.03248)
*Anusha Agarwal,Dibakar Roy Sarkar,Somdatta Goswami*

Main category: cs.LG

TL;DR: This paper evaluates neural operator (NO) architectures for fast and precise modeling of brain deformation in traumatic brain injury (TBI) cases, reducing simulation times from hours to milliseconds.


<details>
  <summary>Details</summary>
Motivation: TBI impacts over 69 million people annually, and finite element models, while accurate, are too slow for clinical applications. Quick, patient-specific predictions are needed for timely assessments.

Method: The study trained and benchmarked four NO architectures (FNO, F-FNO, MG-FNO, DeepONet) using MRI, stiffness maps, and demographic data to predict 3D brain displacement fields from TBI cases across varying frequencies.

Result: MG-FNO achieved the highest accuracy and spatial fidelity, while DeepONet offered the fastest inference with significant speed-ups. All architectures drastically reduced computation times without losing anatomical realism.

Conclusion: NOs demonstrate substantial potential for real-time TBI risk assessment and modeling, paving the way for scalable and clinically viable biomechanical simulations that enhance decision-making in health contexts.

Abstract: Traumatic brain injury (TBI) remains a major public health concern, with over
69 million cases annually worldwide. Finite element (FE) models offer
high-fidelity predictions of brain deformation but are computationally
expensive, requiring hours per simulation and limiting their clinical utility
for rapid decision-making. This study benchmarks state-of-the-art neural
operator (NO) architectures for rapid, patient-specific prediction of brain
displacement fields, aiming to enable real-time TBI modeling in clinical and
translational settings. We formulated TBI modeling as an operator learning
problem, mapping subject-specific anatomical MRI, magnetic resonance
elastography (MRE) stiffness maps, and demographic features to full-field 3D
brain displacement predictions. Four architectures - Fourier Neural Operator
(FNO), Factorized FNO (F-FNO), Multi-Grid FNO (MG-FNO), and Deep Operator
Network (DeepONet) were trained and evaluated on 249 MRE datasets across
physiologically relevant frequencies (20 - 90 Hz). MG-FNO achieved the highest
accuracy (MSE = 0.0023, 94.3\% spatial fidelity) and preserved fine-scale
features, while F-FNO converged 2$\times$ faster than standard FNO. DeepONet
offered the fastest inference (14.5 iterations/s) with a 7$\times$
computational speed-up over MG-FNO, suggesting utility for embedded or edge
computing applications. All NOs reduced computation time from hours to
milliseconds without sacrificing anatomical realism. NOs provide an efficient,
resolution-invariant approach for predicting brain deformation, opening the
door to real-time, patient-specific TBI risk assessment, clinical triage
support, and optimization of protective equipment. These results highlight the
potential for NO-based digital twins of the human brain, enabling scalable,
on-demand biomechanical modeling in both clinical and population health
contexts.

</details>


### [389] [Light Differentiable Logic Gate Networks](https://arxiv.org/abs/2510.03250)
*Lukas Rüttgers,Till Aczel,Andreas Plesner,Roger Wattenhofer*

Main category: cs.LG

TL;DR: Differentiable logic gate networks (DLGNs) encounter challenges like vanishing gradients and high training costs when depth increases. A novel reparametrization addresses these issues, reducing model size, speeding training, and maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to tackle efficiency and scalability challenges in DLGNs, which face issues like vanishing gradients, discretization errors, and poor accuracy at increased depths despite parameter initialization efforts from previous works.

Method: The authors propose a novel reparametrization of logic gate neurons, which reduces parameter size logarithmically with the number of inputs per gate and optimizes training efficiency and convergence.

Result: The new reparametrization reduces model size by 4x, accelerates the backward pass by up to 1.86x, and converges in 8.5x fewer training steps. Additionally, it maintains or improves accuracy on CIFAR-100.

Conclusion: Reparametrizing logic gate neurons resolves key scalability issues, improving efficiency, convergence speed, and preserving accuracy, making DLGNs more practical for deep and complex applications.

Abstract: Differentiable logic gate networks (DLGNs) exhibit extraordinary efficiency
at inference while sustaining competitive accuracy. But vanishing gradients,
discretization errors, and high training cost impede scaling these networks.
Even with dedicated parameter initialization schemes from subsequent works,
increasing depth still harms accuracy. We show that the root cause of these
issues lies in the underlying parametrization of logic gate neurons themselves.
To overcome this issue, we propose a reparametrization that also shrinks the
parameter size logarithmically in the number of inputs per gate. For binary
inputs, this already reduces the model size by 4x, speeds up the backward pass
by up to 1.86x, and converges in 8.5x fewer training steps. On top of that, we
show that the accuracy on CIFAR-100 remains stable and sometimes superior to
the original parametrization.

</details>


### [390] [HydroFusion-LMF: Semi-Supervised Multi-Network Fusion with Large-Model Adaptation for Long-Term Daily Runoff Forecasting](https://arxiv.org/abs/2510.03744)
*Qianfei Fan,Jiayu Wei,Peijun Zhu,Wensheng Ye,Meie Fang*

Main category: cs.LG

TL;DR: This paper presents HydroFusion-LMF, a novel framework for decade-scale daily runoff forecasting in small watersheds, addressing challenges posed by non-stationarity and complex hydrologic signals. It improves on prior methods by combining trend-seasonal-residual decomposition, heterogeneous expert models, and hydrology-aware fusion strategies with semi-supervised multi-task objectives.


<details>
  <summary>Details</summary>
Motivation: Accurate forecasting of daily runoff over long periods in small watersheds is challenging due to complex environmental factors like non-stationarity, extreme events, and multi-scale seasonal variations. Existing models often focus on singular aspects of the problem, limiting their adaptability in dynamic hydrologic regimes.

Method: The paper introduces HydroFusion-LMF, which combines a learnable decomposition of hydrologic signals, expert routing mechanisms for residuals, context-aware fusion gates, and a semi-supervised loss framework for training. It also supports efficient integration via adapter layers for frozen foundational time-series encoders.

Result: Using a 10-year daily runoff dataset, HydroFusion-LMF achieves MSE of 1.0128 and MAE of 0.5818, outperforming the best baseline (DLinear) by over 10% and the average baseline by 24.6% in MSE and 17.1% in MAE.

Conclusion: HydroFusion-LMF enhances both interpretability and performance in hydrologic forecasting, demonstrating robust improvements over existing models under challenging non-stationary conditions, while being more label-efficient and scalable.

Abstract: Accurate decade-scale daily runoff forecasting in small watersheds is
difficult because signals blend drifting trends, multi-scale seasonal cycles,
regime shifts, and sparse extremes. Prior deep models (DLinear, TimesNet,
PatchTST, TiDE, Nonstationary Transformer, LSTNet, LSTM) usually target single
facets and under-utilize unlabeled spans, limiting regime adaptivity. We
propose HydroFusion-LMF, a unified framework that (i) performs a learnable
trend-seasonal-residual decomposition to reduce non-stationarity, (ii) routes
residuals through a compact heterogeneous expert set (linear refinement,
frequency kernel, patch Transformer, recurrent memory, dynamically normalized
attention), (iii) fuses expert outputs via a hydrologic context-aware gate
conditioned on day-of-year phase, antecedent precipitation, local variance,
flood indicators, and static basin attributes, and (iv) augments supervision
with a semi-supervised multi-task objective (composite MSE/MAE + extreme
emphasis + NSE/KGE, masked reconstruction, multi-scale contrastive alignment,
augmentation consistency, variance-filtered pseudo-labeling). Optional adapter
/ LoRA layers inject a frozen foundation time-series encoder efficiently. On a
~10-year daily dataset HydroFusion-LMF attains MSE 1.0128 / MAE 0.5818,
improving the strongest baseline (DLinear) by 10.2% / 10.3% and the mean
baseline by 24.6% / 17.1%. We observe simultaneous MSE and MAE reductions
relative to baselines. The framework balances interpretability (explicit
components, sparse gating) with performance, advancing label-efficient
hydrologic forecasting under non-stationarity.

</details>


### [391] [Numerion: A Multi-Hypercomplex Model for Time Series Forecasting](https://arxiv.org/abs/2510.03251)
*Hanzhong Cao,Wenbo Yan,Ying Tan*

Main category: cs.LG

TL;DR: This paper introduces Numerion, a forecasting model leveraging hypercomplex spaces to enhance time series prediction, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: To address computational complexity and robustness issues in time series forecasting by exploring hypercomplex spaces.

Method: Developing Numerion, which employs RHR-MLP architecture to decompose and fuse time series data in multi-dimensional hypercomplex spaces.

Result: Numerion achieves state-of-the-art performance on several public datasets and demonstrates effective decomposition with higher-dimensional spaces emphasizing lower frequency features.

Conclusion: Hypercomplex spaces provide natural advantages for time series forecasting by enabling decomposition and capturing latent patterns, validated through Numerion.

Abstract: Many methods aim to enhance time series forecasting by decomposing the series
through intricate model structures and prior knowledge, yet they are inevitably
limited by computational complexity and the robustness of the assumptions. Our
research uncovers that in the complex domain and higher-order hypercomplex
spaces, the characteristic frequencies of time series naturally decrease.
Leveraging this insight, we propose Numerion, a time series forecasting model
based on multiple hypercomplex spaces. Specifically, grounded in theoretical
support, we generalize linear layers and activation functions to hypercomplex
spaces of arbitrary power-of-two dimensions and introduce a novel
Real-Hypercomplex-Real Domain Multi-Layer Perceptron (RHR-MLP) architecture.
Numerion utilizes multiple RHR-MLPs to map time series into hypercomplex spaces
of varying dimensions, naturally decomposing and independently modeling the
series, and adaptively fuses the latent patterns exhibited in different spaces
through a dynamic fusion mechanism. Experiments validate the model`s
performance, achieving state-of-the-art results on multiple public datasets.
Visualizations and quantitative analyses comprehensively demonstrate the
ability of multi-dimensional RHR-MLPs to naturally decompose time series and
reveal the tendency of higher dimensional hypercomplex spaces to capture lower
frequency features.

</details>


### [392] [Universal Multi-Domain Translation via Diffusion Routers](https://arxiv.org/abs/2510.03252)
*Duc Kieu,Kien Do,Tuan Hoang,Thao Minh Le,Tung Kieu,Dang Nguyen,Thin Nguyen*

Main category: cs.LG

TL;DR: The paper introduces a universal multi-domain translation (UMDT) technique using a framework called Diffusion Router (DR) to achieve state-of-the-art results in translations between multiple domains with minimal dataset alignment.


<details>
  <summary>Details</summary>
Motivation: Existing multi-domain translation (MDT) methods are limited by their reliance on fully aligned domain tuples or their inability to handle untrained domain mappings, making them impractical for broader, real-world applications.

Method: The proposed method, Diffusion Router (DR), uses a single noise predictor conditioned on source and target domains to model translations between a central domain and other non-central domains. It also includes a variational-bound objective and Tweedie refinement for efficient direct non-central translations.

Result: Evaluation on three large-scale UMDT benchmarks shows DR achieves state-of-the-art performance in both direct and indirect translations, while also reducing computational costs and enabling novel translation tasks such as sketch-to-segmentation.

Conclusion: Diffusion Router (DR) provides a scalable and flexible framework for universal multi-domain translation, overcoming traditional MDT limitations and expanding the range of feasible cross-domain tasks.

Abstract: Multi-domain translation (MDT) aims to learn translations between multiple
domains, yet existing approaches either require fully aligned tuples or can
only handle domain pairs seen in training, limiting their practicality and
excluding many cross-domain mappings. We introduce universal MDT (UMDT), a
generalization of MDT that seeks to translate between any pair of $K$ domains
using only $K-1$ paired datasets with a central domain. To tackle this problem,
we propose Diffusion Router (DR), a unified diffusion-based framework that
models all central$\leftrightarrow$non-central translations with a single noise
predictor conditioned on the source and target domain labels. DR enables
indirect non-central translations by routing through the central domain. We
further introduce a novel scalable learning strategy with a variational-bound
objective and an efficient Tweedie refinement procedure to support direct
non-central mappings. Through evaluation on three large-scale UMDT benchmarks,
DR achieves state-of-the-art results for both indirect and direct translations,
while lowering sampling cost and unlocking novel tasks such as
sketch$\leftrightarrow$segmentation. These results establish DR as a scalable
and versatile framework for universal translation across multiple domains.

</details>


### [393] [MACE: A Hybrid LLM Serving System with Colocated SLO-aware Continuous Retraining Alignment](https://arxiv.org/abs/2510.03283)
*Yufei Li,Yu Fu,Yue Dong,Cong Liu*

Main category: cs.LG

TL;DR: The paper introduces MACE, a hybrid system for balancing inference latency and model accuracy during LLM retraining on constrained GPUs, showing improvements in latency and resource utilization.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of balancing inference latency and model accuracy for LLMs on edge servers, given the non-stationary nature of user data and constrained GPU resources.

Method: MACE integrates intelligent memory management to colocate inference and fine-tuning on the same hardware, adapting iteration-level scheduling to optimize resource allocation and retraining frequency.

Result: MACE reduces inference latency by up to 63%, maintains GPU utilization above 85%, and exceeds performance of continuous and periodic retraining strategies in terms of throughput and latency breakdown.

Conclusion: Iteration-level hybrid scheduling is a promising approach for deploying LLMs with continual learning capabilities on edge platforms, offering significant performance improvements under resource constraints.

Abstract: Large language models (LLMs) deployed on edge servers are increasingly used
in latency-sensitive applications such as personalized assistants,
recommendation, and content moderation. However, the non-stationary nature of
user data necessitates frequent retraining, which introduces a fundamental
tension between inference latency and model accuracy under constrained GPU
resources. Existing retraining strategies either delay model updates,
over-commit resources to retraining, or overlook iteration-level retraining
granularity. In this paper, we identify that iteration-level scheduling is
crucial for adapting retraining frequency to model drift without violating
service-level objectives (SLOs). We propose MACE, a hybrid LLM system that
colocates concurrent inference (prefill, decode) and fine-tuning, with
intelligent memory management to maximize task performance while promising
inference throughput. MACE leverages the insight that not all model updates
equally affect output alignment and allocates GPU cycles accordingly to balance
throughput, latency, and update freshness. Our trace-driven evaluation shows
that MACE matches or exceeds continuous retraining while reducing inference
latency by up to 63% and maintaining throughput under resource constraints.
Compared to periodic retraining, MACE improves latency breakdown across
prefill, decode, and finetune stages, and sustains GPU utilization above 85% in
NVIDIA AGX Orin. These results demonstrate that iteration-level hybrid
scheduling is a promising direction for deploying LLMs with continual learning
capabilities on edge platforms.

</details>


### [394] [Solving the Granularity Mismatch: Hierarchical Preference Learning for Long-Horizon LLM Agents](https://arxiv.org/abs/2510.03253)
*Heyang Gao,Zexu Sun,Erxue Min,Hengyi Cai,Shuaiqiang Wang,Dawei Yin,Xu Chen*

Main category: cs.LG

TL;DR: The paper introduces Hierarchical Preference Learning (HPL), a method for optimizing LLM-based agents using preference signals at multiple levels of granularity. HPL improves upon existing methods by balancing local and global stability and employing a dual-layer curriculum.


<details>
  <summary>Details</summary>
Motivation: There is a granularity mismatch in existing preference-based methods for training LLM agents: trajectory-level methods are too coarse and step-level methods are too narrow-focused for solving complex, multi-step tasks.

Method: HPL combines trajectory- and step-level optimization while introducing group-level preference learning at finer granularity. It uses a dual-layer curriculum that organizes learning via sub-task complexity and reward gap difficulty.

Result: HPL outperforms state-of-the-art methods in experiments on three agent benchmarks, demonstrating superior capability in solving various tasks.

Conclusion: HPL resolves granularity mismatches in preference optimization and achieves better performance through hierarchical loss integration and a dual-layer curriculum. This method bridges local and global policy learning effectively for diverse tasks.

Abstract: Large Language Models (LLMs) as autonomous agents are increasingly tasked
with solving complex, long-horizon problems. Aligning these agents via
preference-based offline methods like Direct Preference Optimization (DPO) is a
promising direction, yet it faces a critical granularity mismatch.
Trajectory-level DPO provides a signal that is too coarse for precise credit
assignment, while step-level DPO is often too myopic to capture the value of
multi-step behaviors. To resolve this challenge, we introduce Hierarchical
Preference Learning (HPL), a hierarchical framework that optimizes LLM agents
by leveraging preference signals at multiple, synergistic granularities. While
HPL incorporates trajectory- and step-level DPO for global and local policy
stability, its core innovation lies in group-level preference optimization
guided by a dual-layer curriculum. Our approach first decomposes expert
trajectories into semantically coherent action groups and then generates
contrasting suboptimal groups to enable preference learning at a fine-grained,
sub-task level. Then, instead of treating all preference pairs equally, HPL
introduces a curriculum scheduler that organizes the learning process from
simple to complex. This curriculum is structured along two axes: the group
length, representing sub-task complexity, and the sample difficulty, defined by
the reward gap between preferred and dispreferred action groups. Experiments on
three challenging agent benchmarks show that HPL outperforms existing
state-of-the-art methods. Our analyses demonstrate that the hierarchical DPO
loss effectively integrates preference signals across multiple granularities,
while the dual-layer curriculum is crucial for enabling the agent to solve a
wide range of tasks, from simple behaviors to complex multi-step sequences.

</details>


### [395] [LogAction: Consistent Cross-system Anomaly Detection through Logs via Active Domain](https://arxiv.org/abs/2510.03288)
*Chiming Duan,Minghua He,Pei Xiao,Tong Jia,Xin Zhang,Zhewei Zhong,Xiang Luo,Yan Niu,Lingzhe Zhang,Yifan Wu,Siyu Yu,Weijie Hong,Ying Li,Gang Huang*

Main category: cs.LG

TL;DR: LogAction is an active domain adaptation model combining transfer and active learning for log-based anomaly detection, achieving high accuracy with minimal manual labeling.


<details>
  <summary>Details</summary>
Motivation: Existing log-based anomaly detection methods rely heavily on labor-intensive labeling, and transfer/active learning methods face challenges like data distribution gaps and cold-start problems.

Method: LogAction integrates transfer learning to use data from mature systems and active learning techniques to strategically select logs for manual labeling. It employs free-energy and uncertainty-based sampling to mitigate distribution gaps.

Result: LogAction outperformed state-of-the-art methods with a 93.01% average F1 score using only 2% of manual labels tested across six dataset combinations.

Conclusion: The proposed model demonstrates effective anomaly detection with minimal labeling effort, addressing challenges faced by existing methods and showcasing significant performance improvements.

Abstract: Log-based anomaly detection is a essential task for ensuring the reliability
and performance of software systems. However, the performance of existing
anomaly detection methods heavily relies on labeling, while labeling a large
volume of logs is highly challenging. To address this issue, many approaches
based on transfer learning and active learning have been proposed.
Nevertheless, their effectiveness is hindered by issues such as the gap between
source and target system data distributions and cold-start problems. In this
paper, we propose LogAction, a novel log-based anomaly detection model based on
active domain adaptation. LogAction integrates transfer learning and active
learning techniques. On one hand, it uses labeled data from a mature system to
train a base model, mitigating the cold-start issue in active learning. On the
other hand, LogAction utilize free energy-based sampling and uncertainty-based
sampling to select logs located at the distribution boundaries for manual
labeling, thus addresses the data distribution gap in transfer learning with
minimal human labeling efforts. Experimental results on six different
combinations of datasets demonstrate that LogAction achieves an average 93.01%
F1 score with only 2% of manual labels, outperforming some state-of-the-art
methods by 26.28%. Website: https://logaction.github.io

</details>


### [396] [Adversarial training with restricted data manipulation](https://arxiv.org/abs/2510.03254)
*David Benfield,Stefano Coniglio,Phan Tu Vuong,Alain Zemkoho*

Main category: cs.LG

TL;DR: The paper introduces a constrained pessimistic bilevel optimization model to improve the training of resilient classifiers against adversarial attacks and enhance realistic adversarial data handling.


<details>
  <summary>Details</summary>
Motivation: Current pessimistic bilevel optimization methods for adversarial training can lead to overly pessimistic, unrealistic adversary models, which harm classifier performance in real-world scenarios.

Method: The authors propose a constrained bilevel optimization model that limits the adversary's ability to generate nonsensical data, resulting in more realistic adversarial scenarios.

Result: Through experiments, the constrained model demonstrates better average performance compared to existing pessimistic bilevel approaches when applied to adversarial machine learning tasks.

Conclusion: Constraining the adversary in pessimistic bilevel optimization models leads to solutions that more closely simulate real-world adversarial conditions, enhancing classifier effectiveness.

Abstract: Adversarial machine learning concerns situations in which learners face
attacks from active adversaries. Such scenarios arise in applications such as
spam email filtering, malware detection and fake image generation, where
security methods must be actively updated to keep up with the everimproving
generation of malicious data. Pessimistic Bilevel optimisation has been shown
to be an effective method of training resilient classifiers against such
adversaries. By modelling these scenarios as a game between the learner and the
adversary, we anticipate how the adversary will modify their data and then
train a resilient classifier accordingly. However, since existing pessimistic
bilevel approaches feature an unrestricted adversary, the model is vulnerable
to becoming overly pessimistic and unrealistic. When finding the optimal
solution that defeats the classifier, it is possible that the adversary's data
becomes nonsensical and loses its intended nature. Such an adversary will not
properly reflect reality, and consequently, will lead to poor classifier
performance when implemented on real-world data. By constructing a constrained
pessimistic bilevel optimisation model, we restrict the adversary's movements
and identify a solution that better reflects reality. We demonstrate through
experiments that this model performs, on average, better than the existing
approach.

</details>


### [397] [From Score Distributions to Balance: Plug-and-Play Mixture-of-Experts Routing](https://arxiv.org/abs/2510.03293)
*Rana Shahout,Colin Cai,Yilun Du,Minlan Yu,Michael Mitzenmacher*

Main category: cs.LG

TL;DR: LASER is an inference-time routing algorithm for Mixture-of-Experts (MoE) models that balances expert load and improves performance while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Addressing the imbalance in load distribution across experts in MoE models, which leads to degraded performance in terms of latency and throughput.

Method: LASER dynamically adapts routing at inference time based on the gate scores of trained MoE models, balancing expert load without requiring retraining or finetuning.

Result: Experiments on multiple datasets show LASER reduces latency and improves throughput while maintaining negligible accuracy changes.

Conclusion: LASER is a plug-and-play solution to enhance MoE inference efficiency, providing better load distribution and system performance without compromising on model accuracy.

Abstract: Mixture-of-Experts (MoE) models can scale parameter capacity by routing each
token to a subset of experts through a learned gate function. While conditional
routing reduces training costs, it shifts the burden on inference memory:
expert parameters and activations consume memory, limiting the number of
experts per device. As tokens are routed, some experts become overloaded while
others are underutilized. Because experts are mapped to GPUs, this imbalance
translates directly into degraded system performance in terms of latency,
throughput, and cost. We present LASER, a plug-and-play, inference-time routing
algorithm that balances load while preserving accuracy. LASER adapts to the
shape of the gate's score distribution. When scores provide a clear preference,
it routes to the strongest experts; when scores are more uniform, it broadens
the set of viable experts and routes to the least-loaded among them. Because
LASER relies only on gate scores from a trained model, it integrates directly
into existing MoE inference pipelines without retraining or finetuning. We
evaluate LASER on Mixtral-8x7B and DeepSeek-MoE-16b-chat across four datasets
(ARC-Easy, ARC-Challenge, MMLU, and GSM8K). LASER improves load balancing,
translating into lower latency and higher throughput, while keeping the
accuracy changes negligible.

</details>


### [398] [SciTS: Scientific Time Series Understanding and Generation with LLMs](https://arxiv.org/abs/2510.03255)
*Wen Wu,Ziyang Zhang,Liwei Liu,Xuenan Xu,Junlin Liu,Ke Fan,Qitan Lv,Jimin Zhuang,Chen Zhang,Zheqi Yuan,Siyuan Hou,Tianyi Lin,Kai Chen,Bowen Zhou,Chao Zhang*

Main category: cs.LG

TL;DR: The paper explores the scientific reasoning abilities of LLMs in handling time series data, introduces the SciTS benchmark across 12 domains and 43 tasks, and proposes a new framework, TimeOmni.


<details>
  <summary>Details</summary>
Motivation: There are unique challenges in processing scientific time series data with current approaches like encoding sequences as text or images. Existing time series models either focus narrowly on forecasting or analysis, and their effectiveness on heterogeneous, non-periodic data is uncertain.

Method: The authors create SciTS, a benchmark comprising 12 scientific domains and 43 tasks with over 50k time series instances. They evaluate 17 models, including general LLMs, multimodal LLMs, and specialized models. They also develop TimeOmni, a new framework for equipping LLMs with time series comprehension and generation capabilities.

Result: General-purpose LLMs demonstrate better generalizability than specialized time series models. Encoding time series as text or images leads to performance issues due to excessively long sequences or numerical precision loss.

Conclusion: The work bridges gaps in scientific time series benchmarks and frameworks. TimeOmni enhances LLMs' ability to process complex scientific temporal data, paving the path for advancements in this area.

Abstract: The scientific reasoning ability of large language models (LLMs) has recently
attracted significant attention. Time series, as a fundamental modality in
scientific data, presents unique challenges that are often overlooked in
current multimodal LLMs, which either encode numerical sequences as text or
convert them into images. Such approaches may be insufficient for comprehensive
scientific time series understanding and generation. Existing unified time
series models typically specialise in either forecasting or analysis, and their
effectiveness on non-periodic, heterogeneous scientific signals remains
unclear. To address these gaps, we introduce SciTS, a benchmark spanning 12
scientific domains and 43 tasks, with over 50k+ instances, both univariate and
multivariate signals ranging from $10^0$ to $10^7$ in length and up to 10~MHz
in frequency. We benchmark 17 models, including text-only LLMs, multimodal
LLMs, and unified time series models, and find that general-purpose LLMs
exhibit stronger generalisability than specialised time series models, while
representing time series as text or images limits their performance due to
excessively long sequences and loss of numerical precision, respectively. We
then introduce TimeOmni, a framework that equips LLMs with the ability to
understand and generate time series while remaining compatible with
general-purpose LLM training. This work fills a gap in both dedicated
benchmarks and modelling frameworks for scientific time series, paving the way
for LLMs to understand and generate complex temporal scientific data.

</details>


### [399] [CAFL-L: Constraint-Aware Federated Learning with Lagrangian Dual Optimization for On-Device Language Models](https://arxiv.org/abs/2510.03298)
*Dongqi Zheng,Wenjin Fu*

Main category: cs.LG

TL;DR: The paper introduces CAFL-L, an improved Federated Learning method that optimally handles device-level constraints while maintaining competitive performance.


<details>
  <summary>Details</summary>
Motivation: Current Federated Learning methods like FedAvg do not account for constraints of resource-limited edge devices, leading to inefficiencies in deployment.

Method: The authors propose CAFL-L, which uses Lagrangian dual optimization to dynamically adjust training parameters, with techniques like gradient accumulation to ensure stability.

Result: Experiments show CAFL-L reduces memory usage by 20% and communication by 95% compared to FedAvg, with similar validation performance.

Conclusion: CAFL-L offers a practical Federated Learning solution for resource-constrained environments, balancing constraint satisfaction and model performance effectively.

Abstract: We introduce Constraint-Aware Federated Learning with Lagrangian Dual
Optimization (CAFL-L), a principled extension of FedAvg that explicitly
incorporates device-level resource constraints including energy, communication,
memory, and thermal budgets. CAFL-L employs Lagrangian dual optimization to
dynamically adapt training hyperparameters -- freezing depth, local steps,
batch size, and communication compression -- while preserving training
stability through token-budget preservation via gradient accumulation.
Experiments on a character-level language model demonstrate that CAFL-L
achieves superior constraint satisfaction compared to standard FedAvg (reducing
memory usage by 20% and communication by 95%) while maintaining competitive
validation performance, making it practical for deployment on
resource-constrained edge devices.

</details>


### [400] [Triple-BERT: Do We Really Need MARL for Order Dispatch on Ride-Sharing Platforms?](https://arxiv.org/abs/2510.03257)
*Zijian Zhao,Sen Li*

Main category: cs.LG

TL;DR: The paper tackles ride-sharing dispatch challenges using a proposed Triple-BERT Single Agent RL method, achieving notable improvements in efficiency.


<details>
  <summary>Details</summary>
Motivation: The platform faces challenges in real-time passenger-vehicle pairing due to system uncertainties and observation space limitations.

Method: Triple-BERT utilizes TD3-based reinforcement learning with action decomposition and a BERT-based network for handling large-scale driver-order interactions.

Result: Validated on Manhattan dataset, it improves metrics like served orders (4.26%) and pickup times (22.25%) compared to current methods.

Conclusion: Triple-BERT enhances ride-sharing efficiency, offering an open-source solution for better order dispatching.

Abstract: On-demand ride-sharing platforms, such as Uber and Lyft, face the intricate
real-time challenge of bundling and matching passengers-each with distinct
origins and destinations-to available vehicles, all while navigating
significant system uncertainties. Due to the extensive observation space
arising from the large number of drivers and orders, order dispatching, though
fundamentally a centralized task, is often addressed using Multi-Agent
Reinforcement Learning (MARL). However, independent MARL methods fail to
capture global information and exhibit poor cooperation among workers, while
Centralized Training Decentralized Execution (CTDE) MARL methods suffer from
the curse of dimensionality. To overcome these challenges, we propose
Triple-BERT, a centralized Single Agent Reinforcement Learning (MARL) method
designed specifically for large-scale order dispatching on ride-sharing
platforms. Built on a variant TD3, our approach addresses the vast action space
through an action decomposition strategy that breaks down the joint action
probability into individual driver action probabilities. To handle the
extensive observation space, we introduce a novel BERT-based network, where
parameter reuse mitigates parameter growth as the number of drivers and orders
increases, and the attention mechanism effectively captures the complex
relationships among the large pool of driver and orders. We validate our method
using a real-world ride-hailing dataset from Manhattan. Triple-BERT achieves
approximately an 11.95% improvement over current state-of-the-art methods, with
a 4.26% increase in served orders and a 22.25% reduction in pickup times. Our
code, trained model parameters, and processed data are publicly available at
the repository https://github.com/RS2002/Triple-BERT .

</details>


### [401] [Semantic-Aware Scheduling for GPU Clusters with Large Language Models](https://arxiv.org/abs/2510.03334)
*Zerui Wang,Qinghao Hu,Ana Klimovic,Tianwei Zhang,Yonggang Wen,Peng Sun,Dahua Lin*

Main category: cs.LG

TL;DR: SchedMate is a framework that integrates semantic understanding into deep learning schedulers using LLM-based components, reducing job completion times significantly.


<details>
  <summary>Details</summary>
Motivation: Existing DL schedulers lack semantic awareness, relying solely on limited metadata, which leads to inefficiencies in job profiling, duration estimation, and failure handling.

Method: SchedMate extracts insights from unstructured data sources such as source code, runtime logs, and historical jobs, and integrates them into existing schedulers via LLM-based components.

Result: Evaluations on physical and simulated GPU clusters indicate SchedMate reduces average job completion times by up to 1.91x, improving scheduling performance.

Conclusion: Incorporating semantic-awareness into DL schedulers is essential for improving resource allocation efficiency, as demonstrated by SchedMate's significant enhancements.

Abstract: Deep learning (DL) schedulers are pivotal in optimizing resource allocation
in GPU clusters, but operate with a critical limitation: they are largely blind
to the semantic context of the jobs they manage. This forces them to rely on
limited metadata, leading to high profiling overhead, unreliable duration
estimation, inadequate failure handling, and poor observability. To this end,
we propose SchedMate, a framework that bridges this semantic gap by
systematically extracting deep insights from overlooked, unstructured data
sources: source code, runtime logs, and historical jobs. SchedMate enhances
existing schedulers non-intrusively through three LLM-based components. Our
implementation integrates seamlessly with existing deep learning schedulers.
Evaluations on a 128-GPU physical cluster and extensive simulations on
production traces show SchedMate reduces average job completion times by up to
1.91x, substantially enhancing the scheduling performance, demonstrating the
critical role of semantic-awareness in modern DL scheduling.

</details>


### [402] [POEM: Explore Unexplored Reliable Samples to Enhance Test-Time Adaptation](https://arxiv.org/abs/2510.03258)
*Chang'an Yi,Xiaohui Deng,Shuaicheng Niu,Yan Zhou*

Main category: cs.LG

TL;DR: POEM proposes a novel approach to enhance test-time adaptation (TTA) by focusing on reliable yet overlooked samples and introducing an Adapt Branch network for improved balance and performance.


<details>
  <summary>Details</summary>
Motivation: The paper addresses limitations in existing TTA methods that depend on entropy thresholds, which often lead to underutilization of reliable samples and less effective adaptation.

Method: The POEM framework explores previously overlooked reliable samples and incorporates an Adapt Branch network to balance domain-agnostic representation learning with high target performance.

Result: POEM outperforms existing TTA methods across diverse scenarios and real-world data shifts, demonstrating computational efficiency and effectiveness.

Conclusion: POEM is a reliable and efficient improvement in TTA, with the ability to augment existing methods and enhance adaptation performance. The source code is publicly available for use.

Abstract: Test-time adaptation (TTA) aims to transfer knowledge from a source model to
unknown test data with potential distribution shifts in an online manner. Many
existing TTA methods rely on entropy as a confidence metric to optimize the
model. However, these approaches are sensitive to the predefined entropy
threshold, influencing which samples are chosen for model adaptation.
Consequently, potentially reliable target samples are often overlooked and
underutilized. For instance, a sample's entropy might slightly exceed the
threshold initially, but fall below it after the model is updated. Such samples
can provide stable supervised information and offer a normal range of gradients
to guide model adaptation. In this paper, we propose a general approach,
\underline{POEM}, to promote TTA via ex\underline{\textbf{p}}loring the
previously unexpl\underline{\textbf{o}}red reliabl\underline{\textbf{e}}
sa\underline{\textbf{m}}ples. Additionally, we introduce an extra Adapt Branch
network to strike a balance between extracting domain-agnostic representations
and achieving high performance on target data. Comprehensive experiments across
multiple architectures demonstrate that POEM consistently outperforms existing
TTA methods in both challenging scenarios and real-world domain shifts, while
remaining computationally efficient. The effectiveness of POEM is evaluated
through extensive analyses and thorough ablation studies. Moreover, the core
idea behind POEM can be employed as an augmentation strategy to boost the
performance of existing TTA approaches. The source code is publicly available
at \emph{https://github.com/ycarobot/POEM}

</details>


### [403] [Distributed Low-Communication Training with Decoupled Momentum Optimization](https://arxiv.org/abs/2510.03371)
*Sasho Nedelkoski,Alexander Acker,Odej Kao,Soeren Becker,Dominik Scheinert*

Main category: cs.LG

TL;DR: This paper introduces a method to lower communication requirements for distributed training of large models by using infrequent synchronizations and gradient momentum compression.


<details>
  <summary>Details</summary>
Motivation: Large models require significant computational resources, typically exclusive to high-bandwidth data centers. This paper aims to enable distributed model training using lower-bandwidth resources.

Method: The approach uses infrequent synchronization among nodes combined with gradient momentum compression, leveraging the discrete cosine transform (DCT) to synchronize only high-frequency components of the optimizer's momentum every few steps.

Result: The proposed method reduces communication by up to 16 times compared to existing methods, successfully applying it across both transformer-based language models and convolutional neural networks.

Conclusion: This technique furthers the viability of training large models on distributed nodes, even with limited bandwidth interconnects.

Abstract: The training of large models demands substantial computational resources,
typically available only in data centers with high-bandwidth interconnects.
However, reducing the reliance on high-bandwidth interconnects between nodes
enables the use of distributed compute resources as an alternative to
centralized data center training. Building on recent advances in distributed
model training, we propose an approach that further reduces communication by
combining infrequent synchronizations across distributed model replicas with
gradient momentum compression. In particular, we treat the optimizer momentum
as a signal and decompose the Nesterov momentum into high- and low-frequency
components via the discrete cosine transform (DCT). Only the high-frequency
components are synchronized across model replicas every $H$ steps. Empirically,
our method achieves up to a $16\times$ reduction in communication compared to
the baseline DiLoCo, and it generalizes across architectures, including
transformer-based language models and convolutional neural networks for images.
Overall, this work advances the feasibility of training large models on
distributed nodes with low-bandwidth interconnects.

</details>


### [404] [Meta-Awareness Enhances Reasoning Models: Self-Alignment Reinforcement Learning](https://arxiv.org/abs/2510.03259)
*Yoonjeon Kim,Doohyuk Jang,Eunho Yang*

Main category: cs.LG

TL;DR: The paper addresses the lack of meta-awareness in reasoning models and introduces a training method (MASA) to improve accuracy and efficiency using self-generated signals.


<details>
  <summary>Details</summary>
Motivation: Large reasoning models lack meta-awareness, leading to misalignment between predicted and true rollouts of reasoning tasks.

Method: Authors propose the MASA training pipeline, which enhances meta-awareness by self-aligning predicted meta-awareness with actual reasoning outcomes, filtering ineffective prompts, and truncating inefficient rollouts.

Result: MASA improves training efficiency (over 1.28x faster for GRPO) and achieves notable accuracy gains (19.3% on AIME25, 6.2% average on math benchmarks, and boosts for generalization tasks).

Conclusion: Improving meta-awareness using self-aligned training leads to better reasoning efficiency, accuracy, and out-of-domain generalization capabilities.

Abstract: Recent studies on reasoning models explore the meta-awareness of language
models, the ability to know how to think by itself. We argue that large
reasoning models lack this meta-awareness property by proving severe
misalignment between true rollouts and predicted meta information. We posit
that aligning meta-prediction with true rollouts will lead to significant
performance gains. To verify this hypothesis, we design a training pipeline
that boosts Meta-Awareness via Self-Alignment (MASA), and prove that enhanced
meta-awareness directly translates to improved accuracy. Unlike existing
meta-cognitive reasoning models, our method does not require external training
sources but leverages self-generated signals to train meta-awareness. Moreover,
our method enables efficient training by i) filtering out zero-variance prompts
that are either trivial or unsolvable and ii) cutting off lengthy rollouts when
they are unlikely to lead to correct answers. The results are inspiring: our
strategy yields significant improvements in both accuracy and training
efficiency on in-domain tasks and shows strong generalization to out-of-domain
benchmarks. More specifically, our method can speed up GRPO training by over
1.28x to reach the same performance, and achieve a 19.3% gain in accuracy on
AIME25, and a 6.2 % average gain over six mathematics benchmarks. Training with
meta-cognitive guidance enhances out-of-domain generalization, giving a 3.87 %
boost on GPQA-Diamond and a 2.08 % overall accuracy gain across 13 benchmarks
spanning logical, scientific, and coding domains.

</details>


### [405] [Semantic-Inductive Attribute Selection for Zero-Shot Learning](https://arxiv.org/abs/2510.03260)
*Juan Jose Herrera-Aranda,Guillermo Gomez-Trenado,Francisco Herrera,Isaac Triguero*

Main category: cs.LG

TL;DR: The paper addresses issues in Zero-Shot Learning (ZSL) by introducing a partitioning scheme and two feature-selection strategies to handle redundancy and irrelevance in semantic spaces, boosting unseen class accuracy.


<details>
  <summary>Details</summary>
Motivation: The study aims to improve the adaptability of zero-shot learning systems in open-world scenarios by addressing the noisy, redundant, or irrelevant attributes often present in semantic spaces.

Method: The proposed partitioning scheme evaluates attribute relevance without using semantic information from unseen classes. Two feature-selection strategies are studied: embedded feature selection adapted for ZSL and an evolutionary computation method for broader attribute subset exploration.

Result: Tests on five benchmark datasets confirm that the proposed strategies improve accuracy on unseen classes. RFS is efficient but reliant on hyperparameters, while GA offers a more robust exploration at a computational cost.

Conclusion: Semantic spaces in ZSL inherently contain redundancy, but the partitioning scheme and feature-selection strategies effectively refine them, enhancing generalization under inductive conditions.

Abstract: Zero-Shot Learning is an important paradigm within General-Purpose Artificial
Intelligence Systems, particularly in those that operate in open-world
scenarios where systems must adapt to new tasks dynamically. Semantic spaces
play a pivotal role as they bridge seen and unseen classes, but whether
human-annotated or generated by a machine learning model, they often contain
noisy, redundant, or irrelevant attributes that hinder performance. To address
this, we introduce a partitioning scheme that simulates unseen conditions in an
inductive setting (which is the most challenging), allowing attribute relevance
to be assessed without access to semantic information from unseen classes.
Within this framework, we study two complementary feature-selection strategies
and assess their generalisation. The first adapts embedded feature selection to
the particular demands of ZSL, turning model-driven rankings into meaningful
semantic pruning; the second leverages evolutionary computation to directly
explore the space of attribute subsets more broadly. Experiments on five
benchmark datasets (AWA2, CUB, SUN, aPY, FLO) show that both methods
consistently improve accuracy on unseen classes by reducing redundancy, but in
complementary ways: RFS is efficient and competitive though dependent on
critical hyperparameters, whereas GA is more costly yet explores the search
space more broadly and avoids such dependence. These results confirm that
semantic spaces are inherently redundant and highlight the proposed
partitioning scheme as an effective tool to refine them under inductive
conditions.

</details>


### [406] [Data-Driven Temperature Modelling of Machine Tools by Neural Networks: A Benchmark](https://arxiv.org/abs/2510.03261)
*C. Coelho,M. Hohmann,D. Fernández,L. Penter,S. Ihlenfeldt,O. Niggemann*

Main category: cs.LG

TL;DR: The paper proposes a new data-driven approach to predict thermal errors in machine tools by modeling temperature and heat flux fields using neural networks.


<details>
  <summary>Details</summary>
Motivation: Thermal errors significantly affect machine precision and productivity, but current methods offer limited adaptability and generality.

Method: Neural networks predict temperature and heat flux fields, trained on finite element method data, with correlation-based measurement point selection strategy and benchmarking multiple NN architectures.

Result: Accurate and cost-effective prediction of temperature and heat flux fields validated using specialized and generalized NN models.

Conclusion: The framework introduces modular and generalizable solutions for thermal error correction in machine tools, overcoming limitations of current methods.

Abstract: Thermal errors in machine tools significantly impact machining precision and
productivity. Traditional thermal error correction/compensation methods rely on
measured temperature-deformation fields or on transfer functions. Most existing
data-driven compensation strategies employ neural networks (NNs) to directly
predict thermal errors or specific compensation values. While effective, these
approaches are tightly bound to particular error types, spatial locations, or
machine configurations, limiting their generality and adaptability. In this
work, we introduce a novel paradigm in which NNs are trained to predict
high-fidelity temperature and heat flux fields within the machine tool. The
proposed framework enables subsequent computation and correction of a wide
range of error types using modular, swappable downstream components. The NN is
trained using data obtained with the finite element method under varying
initial conditions and incorporates a correlation-based selection strategy that
identifies the most informative measurement points, minimising hardware
requirements during inference. We further benchmark state-of-the-art
time-series NN architectures, namely Recurrent NN, Gated Recurrent Unit,
Long-Short Term Memory (LSTM), Bidirectional LSTM, Transformer, and Temporal
Convolutional Network, by training both specialised models, tailored for
specific initial conditions, and general models, capable of extrapolating to
unseen scenarios. The results show accurate and low-cost prediction of
temperature and heat flux fields, laying the basis for enabling flexible and
generalisable thermal error correction in machine tool environments.

</details>


### [407] [A Lightweight Federated Learning Approach for Privacy-Preserving Botnet Detection in IoT](https://arxiv.org/abs/2510.03513)
*Taha M. Mahmoud,Naima Kaabouch*

Main category: cs.LG

TL;DR: The paper proposes a federated learning-based framework for botnet detection in IoT environments, focusing on privacy preservation and communication efficiency.


<details>
  <summary>Details</summary>
Motivation: To overcome challenges of scalability, privacy, and adaptability in conventional botnet detection methods within IoT ecosystems.

Method: A lightweight, privacy-preserving botnet detection framework leveraging federated learning with a communication-efficient aggregation strategy.

Result: Experiments on IoT botnet datasets showed high detection accuracy and reduced communication overhead.

Conclusion: Federated learning provides a scalable, secure, and privacy-conscious solution for intrusion detection in IoT systems.

Abstract: The rapid growth of the Internet of Things (IoT) has expanded opportunities
for innovation but also increased exposure to botnet-driven cyberattacks.
Conventional detection methods often struggle with scalability, privacy, and
adaptability in resource-constrained IoT environments. To address these
challenges, we present a lightweight and privacy-preserving botnet detection
framework based on federated learning. This approach enables distributed
devices to collaboratively train models without exchanging raw data, thus
maintaining user privacy while preserving detection accuracy. A
communication-efficient aggregation strategy is introduced to reduce overhead,
ensuring suitability for constrained IoT networks. Experiments on benchmark IoT
botnet datasets demonstrate that the framework achieves high detection accuracy
while substantially reducing communication costs. These findings highlight
federated learning as a practical path toward scalable, secure, and
privacy-aware intrusion detection for IoT ecosystems.

</details>


### [408] [Rethinking Inter-LoRA Orthogonality in Adapter Merging: Insights from Orthogonal Monte Carlo Dropout](https://arxiv.org/abs/2510.03262)
*Andi Zhang,Xuan Ding,Haofan Wang,Steven McDonagh,Samuel Kaski*

Main category: cs.LG

TL;DR: The paper introduces Orthogonal Monte Carlo Dropout, ensuring orthogonality in merging sparse semantic vectors, particularly applicable to LoRAs, a fine-tuning technique for large models. However, while orthogonality can prevent interference, it does not guarantee semantic compositionality or disentanglement.


<details>
  <summary>Details</summary>
Motivation: The study aims to address interference issues when merging LoRAs, which are used to represent distinct concepts in fine-tuned large models.

Method: The proposed method enforces strict orthogonality in combining sparse semantic vectors without adding extra computational complexity, ensuring that merged LoRAs do not interfere theoretically or at runtime.

Result: Empirical analysis reveals that orthogonality alone does not result in semantic disentanglement or compositional adaptation, countering assumptions from prior work.

Conclusion: Orthogonality in LoRA merging is helpful in preventing interference but is insufficient for achieving true semantic compositionality, indicating a need for further investigation into adapter merging strategies.

Abstract: We propose Orthogonal Monte Carlo Dropout, a mechanism that enforces strict
orthogonality when combining sparse semantic vectors without extra time
complexity. LoRA, a popular fine-tuning method for large models, typically
trains a module to represent a specific concept such as an object or a style.
When multiple LoRAs are merged, for example to generate an object in a
particular style, their semantic vectors may interfere with each other. Our
method guarantees, at the theoretical and runtime levels, that merged LoRAs
remain orthogonal and thus free from direct interference. However, empirical
analysis reveals that such orthogonality does not lead to the semantic
disentanglement or compositionality highlighted in prior work on compositional
adaptation. This finding suggests that inter-LoRA orthogonality alone may be
insufficient for achieving true semantic compositionality, prompting a
re-examination of its role in adapter merging.

</details>


### [409] [MECKD: Deep Learning-Based Fall Detection in Multilayer Mobile Edge Computing With Knowledge Distillation](https://arxiv.org/abs/2510.03601)
*Wei-Lung Mao,Chun-Chi Wang,Po-Heng Chou,Kai-Chun Liu,Yu Tsao*

Main category: cs.LG

TL;DR: The paper introduces a multi-layer mobile edge computing (MLMEC) framework for fall detection (FD) systems that leverages a knowledge distillation (KD) approach to enhance accuracy and reduce data latency.


<details>
  <summary>Details</summary>
Motivation: The need to improve the accuracy and reduce the latency of FD systems, which monitor aging populations using edge devices, inspired this work. Traditional architectures face limitations due to constrained device capacities and transmission delays.

Method: The authors propose splitting the system architecture into different stations, with neural network models at each station. The MLMEC framework transfers data to higher-computing stations if required. Knowledge distillation is applied to improve the performance of front-end devices by leveraging the learnings of back-end stations.

Result: The proposed approach enhanced accuracy by 11.65% for the SisFall dataset and 2.78% for the FallAllD dataset. Latency was reduced by 54.15% for the FallAllD dataset and 46.67% for the SisFall dataset compared to MLMEC without KD.

Conclusion: The MLMEC framework, enhanced by KD, offers an effective solution for improving fall detection accuracy and lowering latency, which are critical for aging population monitoring systems.

Abstract: The rising aging population has increased the importance of fall detection
(FD) systems as an assistive technology, where deep learning techniques are
widely applied to enhance accuracy. FD systems typically use edge devices (EDs)
worn by individuals to collect real-time data, which are transmitted to a cloud
center (CC) or processed locally. However, this architecture faces challenges
such as a limited ED model size and data transmission latency to the CC. Mobile
edge computing (MEC), which allows computations at MEC servers deployed between
EDs and CC, has been explored to address these challenges. We propose a
multilayer MEC (MLMEC) framework to balance accuracy and latency. The MLMEC
splits the architecture into stations, each with a neural network model. If
front-end equipment cannot detect falls reliably, data are transmitted to a
station with more robust back-end computing. The knowledge distillation (KD)
approach was employed to improve front-end detection accuracy by allowing
high-power back-end stations to provide additional learning experiences,
enhancing precision while reducing latency and processing loads. Simulation
results demonstrate that the KD approach improved accuracy by 11.65% on the
SisFall dataset and 2.78% on the FallAllD dataset. The MLMEC with KD also
reduced the data latency rate by 54.15% on the FallAllD dataset and 46.67% on
the SisFall dataset compared to the MLMEC without KD. In summary, the MLMEC FD
system exhibits improved accuracy and reduced latency.

</details>


### [410] [Memory Self-Regeneration: Uncovering Hidden Knowledge in Unlearned Models](https://arxiv.org/abs/2510.03263)
*Agnieszka Polowczyk,Alicja Polowczyk,Joanna Waczyńska,Piotr Borycki,Przemysław Spurek*

Main category: cs.LG

TL;DR: The paper explores challenges in enabling text-to-image models to 'unlearn' specific knowledge, presents a novel Memory Self-Regeneration task, MemoRa strategy for recovering lost knowledge, and highlights robustness in retrieval as a key evaluation measure.


<details>
  <summary>Details</summary>
Motivation: Modern text-to-image models' ability to generate harmful or unlawful content has accelerated the need for selective unlearning without compromising overall performance.

Method: The researchers introduce the Memory Self-Regeneration task, MemoRa strategy for regenerating lost knowledge, and propose robustness in retrieval as a measure for evaluating unlearning techniques.

Result: They find that unlearning occurs in two distinct ways: short-term forgetting allows easy recall, whereas long-term forgetting makes recovery difficult.

Conclusion: Selective unlearning techniques must address challenges in knowledge recovery, recognizing the distinction between short- and long-term forgetting and strengthening retrieval robustness.

Abstract: The impressive capability of modern text-to-image models to generate
realistic visuals has come with a serious drawback: they can be misused to
create harmful, deceptive or unlawful content. This has accelerated the push
for machine unlearning. This new field seeks to selectively remove specific
knowledge from a model's training data without causing a drop in its overall
performance. However, it turns out that actually forgetting a given concept is
an extremely difficult task. Models exposed to attacks using adversarial
prompts show the ability to generate so-called unlearned concepts, which can be
not only harmful but also illegal. In this paper, we present considerations
regarding the ability of models to forget and recall knowledge, introducing the
Memory Self-Regeneration task. Furthermore, we present MemoRa strategy, which
we consider to be a regenerative approach supporting the effective recovery of
previously lost knowledge. Moreover, we propose that robustness in knowledge
retrieval is a crucial yet underexplored evaluation measure for developing more
robust and effective unlearning techniques. Finally, we demonstrate that
forgetting occurs in two distinct ways: short-term, where concepts can be
quickly recalled, and long-term, where recovery is more challenging.

</details>


### [411] [Front-Loading Reasoning: The Synergy between Pretraining and Post-Training Data](https://arxiv.org/abs/2510.03264)
*Syeda Nahida Akter,Shrimai Prabhumoye,Eric Nyberg,Mostofa Patwary,Mohammad Shoeybi,Yejin Choi,Bryan Catanzaro*

Main category: cs.LG

TL;DR: The study investigates the impact of reasoning data in different stages of training LLMs and finds that incorporating reasoning data in pretraining significantly enhances performance compared to post-training adjustments.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to address the unclear and underreported role of reasoning data during the pretraining phase of LLMs, as compared to its use during post-training.

Method: The authors systematically analyze how reasoning data of varying scale, diversity, and quality affects LLM capabilities when introduced at different training stages.

Result: The study finds that reasoning data in pretraining yields significant performance improvements (19% average gain) and highlights an asymmetry in data allocation: diversity benefits pretraining, while quality benefits post-training.

Conclusion: Incorporating reasoning data early in pretraining is critical for improving foundational capabilities, challenging conventional practices of reasoning injection during post-training or SFT.

Abstract: The prevailing paradigm for enhancing the reasoning abilities of LLMs
revolves around post-training on high-quality, reasoning-intensive data. While
emerging literature suggests that reasoning data is increasingly incorporated
also during the mid-training stage-a practice that is relatively more
proprietary and less openly characterized-the role of such data in pretraining
remains unclear. In particular, due to the opaqueness of pretraining corpora in
most frontier models, the effect of reasoning data introduced at different
phases of pre- and/or post-training is relatively less reported in the
scientific literature. This raises several important questions: Is adding
reasoning data earlier during pretraining any better than introducing it during
post-training? Could earlier inclusion risk overfitting and harm
generalization, or instead establish durable foundations that later fine-tuning
cannot recover? We conduct the first systematic study of how reasoning
data-varying in scale, diversity, and quality-affects LLM performance when
introduced at different stages of training. We find that front-loading
reasoning data into pretraining is critical (19% avg gain), establishing
foundational capabilities that cannot be fully replicated by later-stage SFT,
even with more data. We uncover an asymmetric principle for optimal data
allocation: pretraining benefits most from broad diversity in reasoning
patterns (11% avg gain), while SFT is more sensitive to data quality (15% avg
gain). We show that high-quality pretraining data has latent effects, activated
only after SFT, and that naively scaling SFT data can be detrimental, washing
away the benefits of early reasoning injection. Our results challenge the
conventional separation of language modeling and reasoning, providing a
principled guide for strategically allocating data across the entire training
pipeline to build more capable models.

</details>


### [412] [MindCraft: How Concept Trees Take Shape In Deep Models](https://arxiv.org/abs/2510.03265)
*Bowei Tian,Yexiao He,Wanghao Ye,Ziyao Wang,Meng Liu,Ang Li*

Main category: cs.LG

TL;DR: The paper introduces the MindCraft framework using Concept Trees to uncover and analyze hierarchical conceptual structures within large-scale foundation models.


<details>
  <summary>Details</summary>
Motivation: To explore how foundation models internally organize and stabilize concepts, which remains unclear despite their strong performance across various tasks.

Method: The authors propose Concept Trees, applying spectral decomposition at each layer to construct Concept Paths that trace the hierarchical emergence and separation of concepts.

Result: Empirical evaluations demonstrate that Concept Trees successfully recover semantic hierarchies, disentangle latent concepts, and are applicable across varied domains such as medical diagnosis and political decision-making.

Conclusion: Concept Trees provide a versatile and powerful approach for deeply analyzing conceptual representations, advancing interpretable AI research.

Abstract: Large-scale foundation models demonstrate strong performance across language,
vision, and reasoning tasks. However, how they internally structure and
stabilize concepts remains elusive. Inspired by causal inference, we introduce
the MindCraft framework built upon Concept Trees. By applying spectral
decomposition at each layer and linking principal directions into branching
Concept Paths, Concept Trees reconstruct the hierarchical emergence of
concepts, revealing exactly when they diverge from shared representations into
linearly separable subspaces. Empirical evaluations across diverse scenarios
across disciplines, including medical diagnosis, physics reasoning, and
political decision-making, show that Concept Trees recover semantic
hierarchies, disentangle latent concepts, and can be widely applied across
multiple domains. The Concept Tree establishes a widely applicable and powerful
framework that enables in-depth analysis of conceptual representations in deep
models, marking a significant step forward in the foundation of interpretable
AI.

</details>


### [413] [Variational Autoencoders-based Detection of Extremes in Plant Productivity in an Earth System Model](https://arxiv.org/abs/2510.03266)
*Bharat Sharma,Jitendra Kumar*

Main category: cs.LG

TL;DR: This paper uses variational autoencoders (VAE) to detect anomalies in gross primary productivity (GPP), comparing its performance to singular spectral analysis (SSA), and finds increasing negative carbon cycle extremes toward 2050-80.


<details>
  <summary>Details</summary>
Motivation: To improve methods for detecting climate-induced anomalies in plant productivity and understand their impact on terrestrial carbon cycle dynamics.

Method: The study applied a VAE with dense layers to analyze normalized GPP time series from climate simulations. Anomalies were identified using reconstruction errors and compared to SSA-based anomaly detection.

Result: Both VAE and SSA methods showed increasing frequency and magnitude of GPP extremes toward 2050-80. VAE provided computational advantages and greater ability to capture non-linear dependencies.

Conclusion: VAE is a viable alternative to SSA for detecting climate change-induced anomalies in GPP, offering flexibility in identifying data patterns without predefined periodicity.

Abstract: Climate anomalies significantly impact terrestrial carbon cycle dynamics,
necessitating robust methods for detecting and analyzing anomalous behavior in
plant productivity. This study presents a novel application of variational
autoencoders (VAE) for identifying extreme events in gross primary productivity
(GPP) from Community Earth System Model version 2 simulations across four AR6
regions in the Continental United States. We compare VAE-based anomaly
detection with traditional singular spectral analysis (SSA) methods across
three time periods: 1850-80, 1950-80, and 2050-80 under the SSP585 scenario.
The VAE architecture employs three dense layers and a latent space with an
input sequence length of 12 months, trained on a normalized GPP time series to
reconstruct the GPP and identifying anomalies based on reconstruction errors.
Extreme events are defined using 5th percentile thresholds applied to both VAE
and SSA anomalies. Results demonstrate strong regional agreement between VAE
and SSA methods in spatial patterns of extreme event frequencies, despite VAE
producing higher threshold values (179-756 GgC for VAE vs. 100-784 GgC for SSA
across regions and periods). Both methods reveal increasing magnitudes and
frequencies of negative carbon cycle extremes toward 2050-80, particularly in
Western and Central North America. The VAE approach shows comparable
performance to established SSA techniques, while offering computational
advantages and enhanced capability for capturing non-linear temporal
dependencies in carbon cycle variability. Unlike SSA, the VAE method does not
require one to define the periodicity of the signals in the data; it discovers
them from the data.

</details>


### [414] [Machine Learning Workflows in Climate Modeling: Design Patterns and Insights from Case Studies](https://arxiv.org/abs/2510.03305)
*Tian Zheng,Subashree Venkatasubramanian,Shuolin Li,Amy Braverman,Xinyi Ke,Zhewen Hou,Peter Jin,Samarth Sanjay Agrawal*

Main category: cs.LG

TL;DR: This paper examines workflow design patterns in machine learning applications in climate modeling to ensure rigor and foster interdisciplinary collaboration.


<details>
  <summary>Details</summary>
Motivation: Address challenges in climate modeling using machine learning, such as physical consistency, data sparsity, and integration with scientific workflows.

Method: Analyzed case studies in ML for climate modeling, focusing on design patterns like surrogate modeling, ML parameterization, and physics-informed transfer learning.

Result: Outlined how workflows are grounded in physical knowledge, simulation data, and observations, emphasizing critical evaluation, transparency, and reproducibility.

Conclusion: Offers a framework to ensure rigorous and collaborative scientific machine learning for climate modeling.

Abstract: Machine learning has been increasingly applied in climate modeling on system
emulation acceleration, data-driven parameter inference, forecasting, and
knowledge discovery, addressing challenges such as physical consistency,
multi-scale coupling, data sparsity, robust generalization, and integration
with scientific workflows. This paper analyzes a series of case studies from
applied machine learning research in climate modeling, with a focus on design
choices and workflow structure. Rather than reviewing technical details, we aim
to synthesize workflow design patterns across diverse projects in ML-enabled
climate modeling: from surrogate modeling, ML parameterization, probabilistic
programming, to simulation-based inference, and physics-informed transfer
learning. We unpack how these workflows are grounded in physical knowledge,
informed by simulation data, and designed to integrate observations. We aim to
offer a framework for ensuring rigor in scientific machine learning through
more transparent model development, critical evaluation, informed adaptation,
and reproducibility, and to contribute to lowering the barrier for
interdisciplinary collaboration at the interface of data science and climate
modeling.

</details>


### [415] [PT$^2$-LLM: Post-Training Ternarization for Large Language Models](https://arxiv.org/abs/2510.03267)
*Xianglong Yan,Chengzhu Bao,Zhiteng Li,Tianao Zhang,Kaicheng Yang,Haotong Qin,Ruobing Xie,Xingwu Sun,Yulun Zhang*

Main category: cs.LG

TL;DR: The paper introduces PT$^2$-LLM, a novel post-training ternarization framework for Large Language Models aimed at reducing memory and computational demands while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of LLM deployment caused by high memory and compute requirements, and explore the untapped potential of ternarization in post-training quantization.

Method: The proposed method employs an Asymmetric Ternary Quantizer with two-stage refinement: Iterative Ternary Fitting (ITF) for minimizing quantization error and Activation-aware Grid Alignment (AGA) for matching outputs. It also introduces a Structural Similarity-based Reordering (SSR) technique for managing outliers.

Result: PT$^2$-LLM achieves competitive performance compared to state-of-the-art 2-bit PTQ methods while reducing memory costs and accelerating operations for quicker end-to-end execution.

Conclusion: PT$^2$-LLM effectively addresses quantization challenges for LLMs, showcasing its potential as a practical solution for efficient deployment. Its advanced techniques improve computational and memory efficiency without compromising performance.

Abstract: Large Language Models (LLMs) have shown impressive capabilities across
diverse tasks, but their large memory and compute demands hinder deployment.
Ternarization has gained attention as a promising compression technique,
delivering substantial size reduction and high computational efficiency.
However, its potential in the post-training quantization (PTQ) setting remains
underexplored, due to the challenge of training-free parameter optimization and
the quantization difficulty posed by outliers and dispersed weights. To address
these issues, we propose PT$^2$-LLM, a post-training ternarization framework
tailored for LLMs. At its core is an Asymmetric Ternary Quantizer equipped with
a two-stage refinement pipeline: (1) Iterative Ternary Fitting (ITF), which
alternates between optimal ternary grid construction and flexible rounding to
minimize quantization error, and (2) Activation-aware Grid Alignment (AGA),
which further refines the ternary grid to better match full-precision outputs.
In addition, we propose a plug-and-play Structural Similarity-based Reordering
(SSR) strategy that leverages inter-column structural similarity to ease
quantization and mitigate outlier effects, further enhancing overall
performance. Extensive experiments demonstrate that PT$^2$-LLM delivers
competitive performance against state-of-the-art (SOTA) 2-bit PTQ methods with
lower memory cost, while also accelerating both prefill and decoding to achieve
end-to-end speedup. The code and models will be available at
https://github.com/XIANGLONGYAN/PT2-LLM.

</details>


### [416] [Estimating link level traffic emissions: enhancing MOVES with open-source data](https://arxiv.org/abs/2510.03362)
*Lijiao Wang,Muhammad Usama,Haris N. Koutsopoulos,Zhengbing He*

Main category: cs.LG

TL;DR: This paper presents a data-driven framework integrating open-source data with MOVES to enhance vehicle emission estimation in urban areas.


<details>
  <summary>Details</summary>
Motivation: To provide a scalable and cost-effective approach for estimating vehicle activity and emissions using open-source resources.

Method: The framework uses MOVES, GPS trajectory data, road networks, traffic datasets, and satellite imagery with a neural network to predict emission-related operating mode distributions.

Result: The proposed model achieves over 50% reduction in RMSE for key pollutant emissions compared to the baseline, demonstrating improved accuracy.

Conclusion: Fully open-source and data-driven emissions estimation methods are feasible, replicable, and effective at a regional scale.

Abstract: Open-source data offers a scalable and transparent foundation for estimating
vehicle activity and emissions in urban regions. In this study, we propose a
data-driven framework that integrates MOVES and open-source GPS trajectory
data, OpenStreetMap (OSM) road networks, regional traffic datasets and
satellite imagery-derived feature vectors to estimate the link level operating
mode distribution and traffic emissions. A neural network model is trained to
predict the distribution of MOVES-defined operating modes using only features
derived from readily available data. The proposed methodology was applied using
open-source data related to 45 municipalities in the Boston Metropolitan area.
The "ground truth" operating mode distribution was established using OSM
open-source GPS trajectories. Compared to the MOVES baseline, the proposed
model reduces RMSE by over 50% for regional scale traffic emissions of key
pollutants including CO, NOx, CO2, and PM2.5. This study demonstrates the
feasibility of low-cost, replicable, and data-driven emissions estimation using
fully open data sources.

</details>


### [417] [Decrypt Modality Gap in Multimodal Contrastive Learning: From Convergent Representation to Pair Alignment](https://arxiv.org/abs/2510.03268)
*Lingjie Yi,Raphael Douady,Chao Chen*

Main category: cs.LG

TL;DR: The paper examines the modality gap in Multimodal Contrastive Learning (MCL) and offers a theoretical framework to explain its causes and effects on downstream tasks.


<details>
  <summary>Details</summary>
Motivation: The research is motivated by the observation that representations from different modalities in MCL often occupy separate embedding space areas, called modality gap, with inconsistent evidence on its influence on downstream tasks.

Method: The paper develops a theoretical framework to analyze MCL’s convergent optimal representations and modality alignment under different constraints, such as the cone and subspace constraints.

Result: It identifies dimension collapse as the root of the modality gap and theoretically proves the relationship between modality constraints and the modality gap, highlighting its impact on sample alignment.

Conclusion: Perfect alignment across modalities is achievable by hyperplane rotation or shared space projection despite the subspace constraint, advancing the understanding of modality gaps in MCL.

Abstract: Multimodal contrastive learning (MCL) aims to embed data from different
modalities in a shared embedding space. However, empirical evidence shows that
representations from different modalities occupy completely separate regions of
embedding space, a phenomenon referred to as the modality gap. Moreover,
experimental findings on how the size of the modality gap influences downstream
performance are inconsistent. These observations raise two key questions: (1)
What causes the modality gap? (2) How does it affect downstream tasks? To
address these questions, this paper introduces the first theoretical framework
for analyzing the convergent optimal representations of MCL and the modality
alignment when training is optimized. Specifically, we prove that without any
constraint or under the cone constraint, the modality gap converges to zero.
Under the subspace constraint (i.e., representations of two modalities fall
into two distinct hyperplanes due to dimension collapse), the modality gap
converges to the smallest angle between the two hyperplanes. This result
identifies \emph{dimension collapse} as the fundamental origin of the modality
gap. Furthermore, our theorems demonstrate that paired samples cannot be
perfectly aligned under the subspace constraint. The modality gap influences
downstream performance by affecting the alignment between sample pairs. We
prove that, in this case, perfect alignment between two modalities can still be
achieved via two ways: hyperplane rotation and shared space projection.

</details>


### [418] [General Exploratory Bonus for Optimistic Exploration in RLHF](https://arxiv.org/abs/2510.03269)
*Wendi Li,Changdae Oh,Yixuan Li*

Main category: cs.LG

TL;DR: The paper addresses the limitations in current exploratory bonus methods for reinforcement learning with human feedback (RLHF) and introduces the General Exploratory Bonus (GEB) framework, which improves exploration and alignment tasks.


<details>
  <summary>Details</summary>
Motivation: Existing exploratory bonus methods fail to adhere to the optimism principle, unintentionally favoring known, high-probability regions and suppressing exploration of uncertain areas.

Method: The authors propose the General Exploratory Bonus (GEB), which adjusts reward structures to reduce divergence-induced bias and incorporates reference-dependent regulation. It unifies previous heuristic bonuses and extends across the $
alpha$-divergence family.

Result: GEB consistently outperforms baseline methods in alignment tasks, utilizing various divergence settings and large language model architectures.

Conclusion: GEB provides a robust and theoretically grounded framework for optimistic exploration, addressing fundamental inadequacies in existing methods and delivering enhanced performance in RLHF tasks.

Abstract: Optimistic exploration is central to improving sample efficiency in
reinforcement learning with human feedback, yet existing exploratory bonus
methods to incentivize exploration often fail to realize optimism. We provide a
theoretical analysis showing that current formulations, under KL or
$\alpha$-divergence regularization, unintentionally bias exploration toward
high-probability regions of the reference model, thereby reinforcing
conservative behavior instead of promoting discovery of uncertain regions. To
address this pitfall, we introduce the General Exploratory Bonus (GEB), a novel
theoretical framework that provably satisfies the optimism principle. GEB
counteracts divergence-induced bias via reference-dependent reward regulation
and unifies prior heuristic bonuses as special cases, while extending naturally
across the full $\alpha$-divergence family. Empirically, GEB consistently
outperforms baselines on alignment tasks across multiple divergence settings
and large language model backbones. These results demonstrate that GEB offers
both a principled and practical solution for optimistic exploration in RLHF.

</details>


### [419] [Multi-task neural diffusion processes for uncertainty-quantified wind power prediction](https://arxiv.org/abs/2510.03419)
*Joseph Rawson,Domniki Ladopoulou,Petros Dellaportas*

Main category: cs.LG

TL;DR: The paper develops a Multi-Task Neural Diffusion Process (MT-NDP) framework for uncertainty-aware wind power prediction, targeting improved accuracy and calibration.


<details>
  <summary>Details</summary>
Motivation: The reliability and integration of wind power into the grid demand accurate, uncertainty-aware prediction models due to variabilities in turbine behavior.

Method: A novel MT-NDP framework is introduced, incorporating a task encoder to capture cross-turbine correlations and enable few-shot adaptation for unseen turbines.

Result: MT-NDPs outperform existing models, like single-task NDPs and Gaussian Processes (GPs), especially for turbines deviating from average fleet behavior.

Conclusion: MT-NDPs provide calibrated and scalable predictions that enhance operational decision-making for wind farms, benefiting dispatch and maintenance tasks.

Abstract: Uncertainty-aware wind power prediction is essential for grid integration and
reliable wind farm operation. We apply neural diffusion processes (NDPs)-a
recent class of models that learn distributions over functions-and extend them
to a multi-task NDP (MT-NDP) framework for wind power prediction. We provide
the first empirical evaluation of NDPs in real supervisory control and data
acquisition (SCADA) data. We introduce a task encoder within MT-NDPs to capture
cross-turbine correlations and enable few-shot adaptation to unseen turbines.
The proposed MT-NDP framework outperforms single-task NDPs and GPs in terms of
point accuracy and calibration, particularly for wind turbines whose behaviour
deviates from the fleet average. In general, NDP-based models deliver
calibrated and scalable predictions suitable for operational deployment,
offering sharper, yet trustworthy, predictive intervals that can support
dispatch and maintenance decisions in modern wind farms.

</details>


### [420] [CoDA: Coding LM via Diffusion Adaptation](https://arxiv.org/abs/2510.03270)
*Haolin Chen,Shiyu Wang,Can Qin,Bo Pang,Zuxin Liu,Jielin Qiu,Jianguo Zhang,Yingbo Zhou,Zeyuan Chen,Ran Xu,Shelby Heinecke,Silvio Savarese,Caiming Xiong,Huan Wang,Weiran Yao*

Main category: cs.LG

TL;DR: This paper introduces CoDA, a 1.7-billion-parameter diffusion language model designed to offer competitive bidirectional context and infilling while keeping inference latency low.


<details>
  <summary>Details</summary>
Motivation: Diffusion language models offer advantages in bidirectional context understanding and infilling, but previous systems are resource-heavy and impractical for lightweight applications.

Method: The authors developed CoDA by combining large-scale diffusion pre-training, code-centric mid-training, and instruction tuning, ensuring efficient confidence-guided sampling for coding tasks.

Result: CoDA achieves strong performance on coding benchmarks like Humaneval, MBPP, and EvalPlus, surpassing diffusion models up to 7 billion parameters while maintaining efficiency.

Conclusion: The introduction of CoDA demonstrates the feasibility of lightweight, diffusion-based coding assistants, aiding research with open-source tools and resources.

Abstract: Diffusion language models promise bidirectional context and infilling
capabilities that autoregressive coders lack, yet practical systems remain
heavyweight. We introduce CoDA, a 1.7B-parameter diffusion coder trained on TPU
with a fully open-source training pipeline. CoDA pairs large-scale diffusion
pre-training with code-centric mid-training and instruction tuning, enabling
confidence-guided sampling that keeps inference latency competitive. On
Humaneval, MBPP, and EvalPlus, CoDA-1.7B-Instruct matches or surpasses
diffusion models up to 7B parameters. Our release includes model checkpoints,
evaluation harnesses, and TPU training pipelines to accelerate research on
lightweight diffusion-based coding assistants.

</details>


### [421] [Consistent Kernel Change-Point Detection under m-Dependence for Text Segmentation](https://arxiv.org/abs/2510.03437)
*Jairo Diaz-Rodriguez,Mumin Jia*

Main category: cs.LG

TL;DR: This paper advances kernel change-point detection (KCPD) theory for data with dependencies, validates it through simulations and embeddings, and demonstrates its effectiveness across text segmentation tasks.


<details>
  <summary>Details</summary>
Motivation: Despite its utility, KCPD lacked established guarantees for data with dependencies like sequential text. The paper aims to bridge this gap and evaluate its practical applications.

Method: The authors developed theoretical guarantees for KCPD under m-dependent data assumptions, validated through simulations with synthetic text, and conducted an extensive empirical study using text embeddings for segmentation tasks.

Result: The paper proves KCPD's consistency under m-dependent data, validates this via LLM-based simulations, and empirically demonstrates its superior performance in text segmentation benchmarks.

Conclusion: KCPD is theoretically sound for dependent data, practically effective for text segmentation, and applicable to varied real-world tasks.

Abstract: Kernel change-point detection (KCPD) has become a widely used tool for
identifying structural changes in complex data. While existing theory
establishes consistency under independence assumptions, real-world sequential
data such as text exhibits strong dependencies. We establish new guarantees for
KCPD under $m$-dependent data: specifically, we prove consistency in the number
of detected change points and weak consistency in their locations under mild
additional assumptions. We perform an LLM-based simulation that generates
synthetic $m$-dependent text to validate the asymptotics. To complement these
results, we present the first comprehensive empirical study of KCPD for text
segmentation with modern embeddings. Across diverse text datasets, KCPD with
text embeddings outperforms baselines in standard text segmentation metrics. We
demonstrate through a case study on Taylor Swift's tweets that KCPD not only
provides strong theoretical and simulated reliability but also practical
effectiveness for text segmentation tasks.

</details>


### [422] [Decision Potential Surface: A Theoretical and Practical Approximation of LLM's Decision Boundary](https://arxiv.org/abs/2510.03271)
*Zi Liang,Zhiyao Wu,Haoyang Shang,Yulin Jin,Qingqing Ye,Huadi Zheng,Peizhao Hu,Haibo Hu*

Main category: cs.LG

TL;DR: The paper introduces a novel methodology, Decision Potential Surface (DPS), for approximating the decision boundary of large language models (LLMs) efficiently, addressing computational constraints.


<details>
  <summary>Details</summary>
Motivation: Analyzing decision boundaries in LLMs is critical for understanding model behaviors and properties but existing methods are computationally unfeasible.

Method: Introduced Decision Potential Surface (DPS), which uses confidence levels in distinguishing different sampling sequences; developed $K$-DPS algorithm leveraging limited sequence sampling for boundary approximation.

Result: Presented theoretical analysis showcasing low-error metrics and validated results empirically using experiments across various LLMs and datasets.

Conclusion: DPS effectively captures the decision boundary of LLMs and offers a computationally viable method for analysis, filling an existing gap in the literature.

Abstract: Decision boundary, the subspace of inputs where a machine learning model
assigns equal classification probabilities to two classes, is pivotal in
revealing core model properties and interpreting behaviors. While analyzing the
decision boundary of large language models (LLMs) has raised increasing
attention recently, constructing it for mainstream LLMs remains computationally
infeasible due to the enormous vocabulary-sequence sizes and the
auto-regressive nature of LLMs. To address this issue, in this paper we propose
Decision Potential Surface (DPS), a new notion for analyzing LLM decision
boundary. DPS is defined on the confidences in distinguishing different
sampling sequences for each input, which naturally captures the potential of
decision boundary. We prove that the zero-height isohypse in DPS is equivalent
to the decision boundary of an LLM, with enclosed regions representing decision
regions. By leveraging DPS, for the first time in the literature, we propose an
approximate decision boundary construction algorithm, namely $K$-DPS, which
only requires K-finite times of sequence sampling to approximate an LLM's
decision boundary with negligible error. We theoretically derive the upper
bounds for the absolute error, expected error, and the error concentration
between K-DPS and the ideal DPS, demonstrating that such errors can be
trade-off with sampling times. Our results are empirically validated by
extensive experiments across various LLMs and corpora.

</details>


### [423] [PDE-Transformer: A Continuous Dynamical Systems Approach to Sequence Modeling](https://arxiv.org/abs/2510.03272)
*Yukun Zhang,Xueqing Zhou*

Main category: cs.LG

TL;DR: The paper provides a theoretical framework that models Transformer architecture as a continuous dynamical system governed by a PDE, highlighting the stabilizing role of residual connections and layer normalization.


<details>
  <summary>Details</summary>
Motivation: To achieve a principled understanding of the internal mechanisms of the Transformer architecture, which has transformed AI but lacks theoretical clarity.

Method: The authors reformulate the Transformer as a spatiotemporal dynamical system using PDEs, mapping its components to mathematical operators, and compare standard Transformers with PDE simulators lacking stabilization mechanisms.

Result: Empirical evidence demonstrates that residual connections prevent catastrophic representational drift, while layer normalization averts unstable training dynamics, proving their necessity as mathematical stabilizers.

Conclusion: Transformer's design elements like residual connections and layer normalization are essential stabilizers rooted in its mathematical characterization, providing deep insights into its architecture and paving the way for analyzing neural networks through continuous dynamics.

Abstract: The Transformer architecture has revolutionized artificial intelligence, yet
a principled theoretical understanding of its internal mechanisms remains
elusive. This paper introduces a novel analytical framework that
reconceptualizes the Transformer's discrete, layered structure as a continuous
spatiotemporal dynamical system governed by a master Partial Differential
Equation (PDE). Within this paradigm, we map core architectural components to
distinct mathematical operators: self-attention as a non-local interaction, the
feed-forward network as a local reaction, and, critically, residual connections
and layer normalization as indispensable stabilization mechanisms. We do not
propose a new model, but rather employ the PDE system as a theoretical probe to
analyze the mathematical necessity of these components. By comparing a standard
Transformer with a PDE simulator that lacks explicit stabilizers, our
experiments provide compelling empirical evidence for our central thesis. We
demonstrate that without residual connections, the system suffers from
catastrophic representational drift, while the absence of layer normalization
leads to unstable, explosive training dynamics. Our findings reveal that these
seemingly heuristic "tricks" are, in fact, fundamental mathematical stabilizers
required to tame an otherwise powerful but inherently unstable continuous
system. This work offers a first-principles explanation for the Transformer's
design and establishes a new paradigm for analyzing deep neural networks
through the lens of continuous dynamics.

</details>


### [424] [On residual network depth](https://arxiv.org/abs/2510.03470)
*Benoit Dherin,Michael Munn*

Main category: cs.LG

TL;DR: This paper mathematically proves that deep residual networks function like hierarchical ensembles of shallower models, explaining their effectiveness and dependence on normalization layers.


<details>
  <summary>Details</summary>
Motivation: To develop a formal understanding of why depth is effective in deep residual architectures such as ResNet and Transformers.

Method: A key analytical formula, called the Residual Expansion Theorem, was derived to show how increasing depth expands the implicit ensemble size of computation paths.

Result: The analysis reveals normalization layers' necessity for controlling output explosions and explains normalization-free techniques like SkipInit and Fixup. Scaling residual modules effectively regularizes model complexity and resolves inherent combinatorial growth.

Conclusion: Scaling residual modules provides principled control over the explosion of computation paths and regularizes complexity, deepening understanding of architectural design choice in deep networks.

Abstract: Deep residual architectures, such as ResNet and the Transformer, have enabled
models of unprecedented depth, yet a formal understanding of why depth is so
effective remains an open question. A popular intuition, following Veit et al.
(2016), is that these residual networks behave like ensembles of many shallower
models. Our key finding is an explicit analytical formula that verifies this
ensemble perspective, proving that increasing network depth is mathematically
equivalent to expanding the size of this implicit ensemble. Furthermore, our
expansion reveals a hierarchical ensemble structure in which the combinatorial
growth of computation paths leads to an explosion in the output signal,
explaining the historical necessity of normalization layers in training deep
models. This insight offers a first principles explanation for the historical
dependence on normalization layers and sheds new light on a family of
successful normalization-free techniques like SkipInit and Fixup. However,
while these previous approaches infer scaling factors through optimizer
analysis or a heuristic analogy to Batch Normalization, our work offers the
first explanation derived directly from the network's inherent functional
structure. Specifically, our Residual Expansion Theorem reveals that scaling
each residual module provides a principled solution to taming the combinatorial
explosion inherent to these architectures. We further show that this scaling
acts as a capacity controls that also implicitly regularizes the model's
complexity.

</details>


### [425] [Learning without Global Backpropagation via Synergistic Information Distillation](https://arxiv.org/abs/2510.03273)
*Chenhao Ye,Ming Tang*

Main category: cs.LG

TL;DR: Synergistic Information Distillation (SID) addresses backpropagation's scalability and memory bottlenecks by enabling parallel training and reducing memory usage, while maintaining or improving classification accuracy.


<details>
  <summary>Details</summary>
Motivation: Backpropagation presents challenges such as update locking and high memory consumption, limiting scalability in deep learning applications.

Method: SID divides a deep network into pipeline modules, each with local objectives for refining probabilistic beliefs about targets. Modules train independently, allowing parallelism and reduced memory usage.

Result: Theoretical proof supports SID's monotonic depth performance improvement, and empirical results show SID matches or exceeds backpropagation's accuracy and robustness, especially with label noise.

Conclusion: SID provides a scalable, memory-efficient alternative to backpropagation, with a versatile drop-in framework suitable for deep networks.

Abstract: Backpropagation (BP), while foundational to deep learning, imposes two
critical scalability bottlenecks: update locking, where network modules remain
idle until the entire backward pass completes, and high memory consumption due
to storing activations for gradient computation. To address these limitations,
we introduce Synergistic Information Distillation (SID), a novel training
framework that reframes deep learning as a cascade of local cooperative
refinement problems. In SID, a deep network is structured as a pipeline of
modules, each imposed with a local objective to refine a probabilistic belief
about the ground-truth target. This objective balances fidelity to the target
with consistency to the belief from its preceding module. By decoupling the
backward dependencies between modules, SID enables parallel training and hence
eliminates update locking and drastically reduces memory requirements.
Meanwhile, this design preserves the standard feed-forward inference pass,
making SID a versatile drop-in replacement for BP. We provide a theoretical
foundation, proving that SID guarantees monotonic performance improvement with
network depth. Empirically, SID consistently matches or surpasses the
classification accuracy of BP, exhibiting superior scalability and pronounced
robustness to label noise.Code is available at:
https://github.com/ychAlbert/sid-bp

</details>


### [426] [Trajectory Data Suffices for Statistically Efficient Policy Evaluation in Finite-Horizon Offline RL with Linear $q^π$-Realizability and Concentrability](https://arxiv.org/abs/2510.03494)
*Volodymyr Tkachuk,Csaba Szepesvári,Xiaoqi Tan*

Main category: cs.LG

TL;DR: The paper studies finite-horizon offline reinforcement learning (RL) with function approximation and presents advancements in both policy evaluation and optimization.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of achieving statistically efficient learning in offline RL under common assumptions and explore improvements in sample complexity for policy-related tasks.

Method: The authors assume datasets with good coverage and $q^\pi$-realizability, and adopt trajectory-based data to formulate a new approach for policy evaluation, while tightening the analysis for policy optimization methodologies.

Result: The proposed method enables statistically efficient learning for policy evaluation. Additionally, it improves the sample complexity in policy optimization approaches from prior work.

Conclusion: Advanced techniques for offline RL can achieve statistical efficiency for both policy evaluation and optimization, given suitable assumptions; this work contributes key refinements to these domains.

Abstract: We study finite-horizon offline reinforcement learning (RL) with function
approximation for both policy evaluation and policy optimization. Prior work
established that statistically efficient learning is impossible for either of
these problems when the only assumptions are that the data has good coverage
(concentrability) and the state-action value function of every policy is
linearly realizable ($q^\pi$-realizability) (Foster et al., 2021). Recently,
Tkachuk et al. (2024) gave a statistically efficient learner for policy
optimization, if in addition the data is assumed to be given as trajectories.
In this work we present a statistically efficient learner for policy evaluation
under the same assumptions. Further, we show that the sample complexity of the
learner used by Tkachuk et al. (2024) for policy optimization can be improved
by a tighter analysis.

</details>


### [427] [Quant-dLLM: Post-Training Extreme Low-Bit Quantization for Diffusion Large Language Models](https://arxiv.org/abs/2510.03274)
*Tianao Zhang,Zhiteng Li,Xianglong Yan,Haotong Qin,Yong Guo,Yulun Zhang*

Main category: cs.LG

TL;DR: The paper proposes Quant-dLLM, a 2-bit post-training quantization (PTQ) framework tailored for diffusion large language models (dLLMs), achieving improved accuracy over existing methods.


<details>
  <summary>Details</summary>
Motivation: The rapid growth in the size of dLLMs necessitates efficient weight compression techniques for practical deployment, as existing PTQ methods designed for autoregressive (AR) LLMs underperform on dLLMs at low bit precision.

Method: The authors introduced three methods tailored for dLLMs: Masked Calibration Simulation (MCS) to align calibration with timestep-specific masking, Data-aware Any-order Quantizer (DAQ) for optimized low-bit weight representations, and Adaptive Blockwise Mixed Precision (ABMP) to allocate bit width based on sensitivity.

Result: Quant-dLLM achieves superior accuracy compared to state-of-the-art AR-derived PTQ methods on dLLMs, even at strict 2-bit precision budgets.

Conclusion: Quant-dLLM effectively addresses the challenges of applying ultra-low-bit PTQ to dLLMs by leveraging problem-specific advancements, offering improved accuracy and practical deployment feasibility.

Abstract: Diffusion large language models (dLLMs), which offer bidirectional context
and flexible masked-denoising generation, are emerging as a compelling
alternative to autoregressive (AR) LLMs. However, like AR LLMs, their model
sizes continue to grow, motivating weight compression for deployment. Although
post-training quantization (PTQ) is effective for AR LLMs, directly
transferring it to dLLMs at 2-bit leads to unsatisfactory performance. To
tackle these challenges, we propose Quant-dLLM, an ultra-low-bit PTQ framework
tailored to dLLMs. Since masked-denoising activations in dLLMs differ from the
fully visible signals assumed by standard PTQ methods, we introduce Masked
Calibration Simulation (MCS) to align calibration with the timestep-dependent
masking, which yields more reliable calibrations. Moreover, we propose a
Data-aware Any-order Quantizer (DAQ) that learns ultra-low-bit weight
representations via an optimization algorithm. It performs iterative
approximation guided by our simulated calibration data. In addition, under a
strict 2-bit budget, we introduce Adaptive Blockwise Mixed Precision (ABMP), a
sensitivity-based precision allocation scheme that adaptively assigns bit width
across channel groups. When restricted to 2-bit precision, Quant-dLLM
consistently achieves higher accuracy than state-of-the-art (SOTA) AR-transfer
PTQ methods on dLLMs. The code and models will be available at:
https://github.com/ZTA2785/Quant-dLLM.

</details>


### [428] [SDQ-LLM: Sigma-Delta Quantization for 1-bit LLMs of any size](https://arxiv.org/abs/2510.03275)
*Junhao Xia,Ming Zhao,Limin Xiao,Xiujun Zhang*

Main category: cs.LG

TL;DR: SDQ-LLM introduces a framework for extremely low-bit quantization in large language models, achieving efficient deployment with minimal loss in linguistic reasoning capabilities.


<details>
  <summary>Details</summary>
Motivation: Existing large language models (LLMs) face computational and memory challenges, necessitating innovative solutions for extremely low-bit quantization.

Method: The paper presents the SDQ-LLM framework, which uses Sigma-Delta Quantization, Hadamard-based weight smoothing, and a MultiOSR strategy for distributing weight precision dynamically across layers.

Result: Experiments demonstrate SDQ-LLM's ability to retain model accuracy while achieving significantly higher efficiency, even under aggressive quantization settings.

Conclusion: SDQ-LLM provides an effective solution to enhance inference efficiency for LLMs, ensuring linguistic reasoning capabilities while addressing memory and computational constraints.

Abstract: Large language models (LLMs) face significant computational and memory
challenges, making extremely low-bit quantization crucial for their efficient
deployment. In this work, we introduce SDQ-LLM: Sigma-Delta Quantization for
1-bit LLMs of any size, a novel framework that enables extremely low-bit
quantization of LLMs while preserving their linguistic reasoning capabilities.
A distinctive feature of SDQ-LLM is the continuous adjustability of the
Over-Sampling Ratio (OSR), enabling dynamic adaptation to memory or VRAM
constraints by selecting fractional OSR (e.g. 2.5 times) for an optimal
trade-off between model size and accuracy. SDQ-LLM uses upsampling combined
with Sigma-Delta Quantizer to binarize or ternarize LLMs weights, encoding
high-precision parameters into 1-bit or 1.58-bit representations, replacing the
multiplication operations within linear layers with addition. This approach
significantly enhances inference efficiency under extremely low-bit
quantization. To further reduce the loss of quantization precision, we
incorporate Hadamard-based weight smoothing prior to quantization, improving
the stability and robustness of the weight representations. Furthermore, to
fully leverage the continuity of the OSR and reduce precision loss, recognizing
the correlation between quantization sensitivity and weight variance, we
propose a fine-grained, layer- and linear-wise OSR allocation strategy,
MultiOSR. This strategy distributes OSR both across layers and within each
layer, based on weight variance and parameter scale. Finally, extensive
experiments on OPT and LLaMA model families demonstrate that SDQ-LLM achieves a
more efficient and high-precision performance even under highly aggressive
low-OSR settings. Our code is available at
https://github.com/Dreamlittlecat/LLM-Quant-Factory.

</details>


### [429] [QuadEnhancer: Leveraging Quadratic Transformations to Enhance Deep Neural Networks](https://arxiv.org/abs/2510.03276)
*Qian Chen,Linxin Yang,Akang Wang,Xiaodong Luo,Yin Zhang*

Main category: cs.LG

TL;DR: This paper proposes adding lightweight quadratic transformations to neural networks, showcasing improved performance with minimal additional complexity across multiple tasks.


<details>
  <summary>Details</summary>
Motivation: To enhance the performance of neural networks by increasing their nonlinearity without significantly increasing parameter and computational complexity.

Method: Introduces a quadratic transformation technique using low-rankness, weight sharing, and sparsification techniques to add quadratic interactions between features while keeping complexity low.

Result: The method is evaluated through experiments on image classification, text classification, and large-language model fine-tuning, showing clear performance gains in all cases.

Conclusion: The lightweight quadratic enhancer effectively boosts the capabilities of neural networks by introducing quadratic feature interactions with minimal additional computational burden.

Abstract: The combination of linear transformations and non-linear activation functions
forms the foundation of most modern deep neural networks, enabling them to
approximate highly complex functions. This paper explores the introduction of
quadratic transformations to further increase nonlinearity in neural networks,
with the aim of enhancing the performance of existing architectures. To reduce
parameter complexity and computational complexity, we propose a lightweight
quadratic enhancer that uses low-rankness, weight sharing, and sparsification
techniques. For a fixed architecture, the proposed approach introduces
quadratic interactions between features at every layer, while only adding
negligible amounts of additional model parameters and forward computations. We
conduct a set of proof-of-concept experiments for the proposed method across
three tasks: image classification, text classification, and fine-tuning
large-language models. In all tasks, the proposed approach demonstrates clear
and substantial performance gains.

</details>


### [430] [Sequential decoder training for improved latent space dynamics identification](https://arxiv.org/abs/2510.03535)
*William Anderson,Seung Whan Chung,Youngsoo Choi*

Main category: cs.LG

TL;DR: This paper introduces the multi-stage LaSDI (mLaSDI) framework that enhances reconstruction and prediction accuracy for reduced-order models using a sequential learning approach.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the limitation of traditional ROM frameworks like LaSDI, where enforcing latent dynamics during training compromises model reconstruction accuracy for simulation data.

Method: The multi-stage LaSDI (mLaSDI) framework organizes learning into sequential stages, in which additional decoders iteratively correct residual errors from earlier stages.

Result: mLaSDI demonstrated superior performance over LaSDI on the 1D-1V Vlasov equation, achieving lower prediction errors and reducing training times across various architectures.

Conclusion: The mLaSDI framework improves over standard LaSDI by delivering both higher accuracy and computational efficiency in solving partial differential equations for reduced-order modeling tasks.

Abstract: Accurate numerical solutions of partial differential equations are essential
in many scientific fields but often require computationally expensive solvers,
motivating reduced-order models (ROMs). Latent Space Dynamics Identification
(LaSDI) is a data-driven ROM framework that combines autoencoders with equation
discovery to learn interpretable latent dynamics. However, enforcing latent
dynamics during training can compromise reconstruction accuracy of the model
for simulation data. We introduce multi-stage LaSDI (mLaSDI), a framework that
improves reconstruction and prediction accuracy by sequentially learning
additional decoders to correct residual errors from previous stages. Applied to
the 1D-1V Vlasov equation, mLaSDI consistently outperforms standard LaSDI,
achieving lower prediction errors and reduced training time across a wide range
of architectures.

</details>


### [431] [Quantifying constraint hierarchies in Bayesian PINNs via per-constraint Hessian decomposition](https://arxiv.org/abs/2510.03278)
*Filip Landgren*

Main category: cs.LG

TL;DR: This paper introduces a matrix-free Laplace framework to study the influence of physical constraints on Bayesian physics-informed neural networks (B-PINNs) to better interpret their handling of uncertainty.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of interpreting uncertainties and overconfidence in B-PINNs, especially regarding how physical constraints affect the model's calibration and precision.

Method: The authors propose a scalable Laplace framework that analyzes the posterior Hessian to decompose contributions from individual constraints and quantify their impact on the network's loss landscape.

Result: Applying the framework to the Van der Pol equation, they demonstrate how constraints shape the network geometry and redistribute influence when loss weights are adjusted.

Conclusion: The findings provide valuable insights into how physical constraints sculpt loss landscapes, offering a tool to better understand and calibrate B-PINNs in solving differential equations under uncertainty.

Abstract: Bayesian physics-informed neural networks (B-PINNs) merge data with governing
equations to solve differential equations under uncertainty. However,
interpreting uncertainty and overconfidence in B-PINNs requires care due to the
poorly understood effects the physical constraints have on the network;
overconfidence could reflect warranted precision, enforced by the constraints,
rather than miscalibration. Motivated by the need to further clarify how
individual physical constraints shape these networks, we introduce a scalable,
matrix-free Laplace framework that decomposes the posterior Hessian into
contributions from each constraint and provides metrics to quantify their
relative influence on the loss landscape. Applied to the Van der Pol equation,
our method tracks how constraints sculpt the network's geometry and shows,
directly through the Hessian, how changing a single loss weight non-trivially
redistributes curvature and effective dominance across the others.

</details>


### [432] [Longitudinal Flow Matching for Trajectory Modeling](https://arxiv.org/abs/2510.03569)
*Mohammad Mohaiminul Islam,Thijs P. Kuipers,Sharvaree Vadgama,Coen de Vente,Afsana Khan,Clara I. Sánchez,Erik J. Bekkers*

Main category: cs.LG

TL;DR: IMMFM proposes a generative model framework for sequential data that learns continuous stochastic dynamics consistent with multiple time points, demonstrating superior forecasting accuracy and relevance for downstream tasks.


<details>
  <summary>Details</summary>
Motivation: Address the difficulties in learning dynamics from sparsely sampled and high-dimensional trajectories, where traditional methods focus only on pairwise transitions.

Method: Introduces IMMFM, utilizing piecewise-quadratic interpolation paths and optimizing drift and a data-driven diffusion coefficient. It includes theoretical stability conditions for learning.

Result: Experiments showed IMMFM outperforms existing methods in forecasting accuracy and downstream task performance, validated through synthetic and real-world neuroimaging datasets.

Conclusion: IMMFM effectively handles sparse sampling and captures stochastic dynamics, making it valuable for applications requiring precise subject-specific trajectory modeling.

Abstract: Generative models for sequential data often struggle with sparsely sampled
and high-dimensional trajectories, typically reducing the learning of dynamics
to pairwise transitions. We propose \textit{Interpolative Multi-Marginal Flow
Matching} (IMMFM), a framework that learns continuous stochastic dynamics
jointly consistent with multiple observed time points. IMMFM employs a
piecewise-quadratic interpolation path as a smooth target for flow matching and
jointly optimizes drift and a data-driven diffusion coefficient, supported by a
theoretical condition for stable learning. This design captures intrinsic
stochasticity, handles irregular sparse sampling, and yields subject-specific
trajectories. Experiments on synthetic benchmarks and real-world longitudinal
neuroimaging datasets show that IMMFM outperforms existing methods in both
forecasting accuracy and further downstream tasks.

</details>


### [433] [MemMamba: Rethinking Memory Patterns in State Space Model](https://arxiv.org/abs/2510.03279)
*Youjin Wang,Yangjingyi Chen,Jiahao Yan,Jiaxuan Lu,Xiao Sun*

Main category: cs.LG

TL;DR: MemMamba enhances long-sequence modeling by addressing memory decay issues in Mamba through state summarization and cross-layer/token attention, providing improved performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: The motivation lies in addressing the limitations of long-sequence modeling, especially tackling the inefficiencies of existing methods like RNNs suffering from gradient issues and Transformers facing quadratic complexity. Mamba, despite its efficiency, suffers from exponential memory decay, which hinders its scalability for long-term information retention.

Method: The paper introduces MemMamba, a novel architecture featuring a state summarization mechanism and cross-layer/token attention to mitigate long-range memory loss. It integrates mathematical derivations, information-theoretic analysis, and new fidelity metrics to capture memory degradation.

Result: MemMamba outperforms prior models, including Mamba variants and Transformers, on benchmarks like PG19 and Passkey Retrieval. It achieves significant performance gains and a 48% speedup in inference efficiency.

Conclusion: MemMamba represents a breakthrough in the trade-off between memory and computational complexity, setting a new standard for ultra-long sequence modeling with its efficient and effective design.

Abstract: With the explosive growth of data, long-sequence modeling has become
increasingly important in tasks such as natural language processing and
bioinformatics. However, existing methods face inherent trade-offs between
efficiency and memory. Recurrent neural networks suffer from gradient vanishing
and explosion, making them hard to scale. Transformers can model global
dependencies but are constrained by quadratic complexity. Recently, selective
state-space models such as Mamba have demonstrated high efficiency with O(n)
time and O(1) recurrent inference, yet their long-range memory decays
exponentially. In this work, we conduct mathematical derivations and
information-theoretic analysis to systematically uncover the memory decay
mechanism of Mamba, answering a fundamental question: what is the nature of
Mamba's long-range memory and how does it retain information? To quantify key
information loss, we further introduce horizontal-vertical memory fidelity
metrics that capture degradation both within and across layers. Inspired by how
humans distill and retain salient information when reading long documents, we
propose MemMamba, a novel architectural framework that integrates state
summarization mechanism together with cross-layer and cross-token attention,
which alleviates long-range forgetting while preserving linear complexity.
MemMamba achieves significant improvements over existing Mamba variants and
Transformers on long-sequence benchmarks such as PG19 and Passkey Retrieval,
while delivering a 48% speedup in inference efficiency. Both theoretical
analysis and empirical results demonstrate that MemMamba achieves a
breakthrough in the complexity-memory trade-off, offering a new paradigm for
ultra-long sequence modeling.

</details>


### [434] [BEKAN: Boundary condition-guaranteed evolutionary Kolmogorov-Arnold networks with radial basis functions for solving PDE problems](https://arxiv.org/abs/2510.03576)
*Bongseok Kim,Jiahao Zhang,Guang Lin*

Main category: cs.LG

TL;DR: BEKAN is a novel network designed to solve partial differential equations (PDEs) with rigorous enforcement of boundary conditions, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Deep learning shows promise in solving PDEs but struggles with strictly enforcing boundary conditions due to the black-box nature of neural networks.

Method: The Boundary Condition-Guaranteed Kolmogorov-Arnold Network (BEKAN) incorporates boundary conditions into the network using Gaussian radial basis functions, periodic sinusoidal layers, and a least-squares formulation for various types of boundary conditions (Dirichlet, periodic, Neumann).

Result: BEKAN demonstrates superior accuracy compared to multilayer perceptron (MLP) and B-splines KAN in extensive experiments covering diverse boundary problems.

Conclusion: The use of BEKAN enhances the solution accuracy for PDEs and meets boundary conditions precisely, advancing its utility in scientific computing and engineering.

Abstract: Deep learning has gained attention for solving PDEs, but the black-box nature
of neural networks hinders precise enforcement of boundary conditions. To
address this, we propose a boundary condition-guaranteed evolutionary
Kolmogorov-Arnold Network (KAN) with radial basis functions (BEKAN). In BEKAN,
we propose three distinct and combinable approaches for incorporating
Dirichlet, periodic, and Neumann boundary conditions into the network. For
Dirichlet problem, we use smooth and global Gaussian RBFs to construct
univariate basis functions for approximating the solution and to encode
boundary information at the activation level of the network. To handle periodic
problems, we employ a periodic layer constructed from a set of sinusoidal
functions to enforce the boundary conditions exactly. For a Neumann problem, we
devise a least-squares formulation to guide the parameter evolution toward
satisfying the Neumann condition. By virtue of the boundary-embedded RBFs, the
periodic layer, and the evolutionary framework, we can perform accurate PDE
simulations while rigorously enforcing boundary conditions. For demonstration,
we conducted extensive numerical experiments on Dirichlet, Neumann, periodic,
and mixed boundary value problems. The results indicate that BEKAN outperforms
both multilayer perceptron (MLP) and B-splines KAN in terms of accuracy. In
conclusion, the proposed approach enhances the capability of KANs in solving
PDE problems while satisfying boundary conditions, thereby facilitating
advancements in scientific computing and engineering applications.

</details>


### [435] [Training Optimal Large Diffusion Language Models](https://arxiv.org/abs/2510.03280)
*Jinjie Ni,Qian Liu,Chao Du,Longxu Dou,Hang Yan,Zili Wang,Tianyu Pang,Michael Qizhe Shieh*

Main category: cs.LG

TL;DR: Quokka establishes systematic scaling laws for diffusion language models, addressing compute and data constraints.


<details>
  <summary>Details</summary>
Motivation: To provide practical guidance and long-term insights in training diffusion language models.

Method: Developing scaling laws by studying modeling and optimization designs for diffusion language models across compute- and data-constrained regimes.

Result: Quokka offers a wider scope compared to Chinchilla, addressing key challenges in diffusion language model training.

Conclusion: The study aims to aid both immediate training strategies for DLMs and broader AI advancements.

Abstract: We introduce Quokka, the first systematic scaling law for diffusion language
models (DLMs), encompassing both compute-constrained and data-constrained
regimes, and studying the key modeling and optimization designs. Quokka is a
good friend of Chinchilla and provides wider scopes. We hope the results would
bring short-term practical guidance in DLMs training and long-term inspirations
for the whole AI community.

</details>


### [436] [Latent Mixture of Symmetries for Sample-Efficient Dynamic Learning](https://arxiv.org/abs/2510.03578)
*Haoran Li,Chenhan Xiao,Muhao Guo,Yang Weng*

Main category: cs.LG

TL;DR: The paper introduces Latent MoS, an approach to dynamically learn latent factors influenced by symmetry, enhancing sample efficiency in systems with limited measurements.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the challenge of sample-efficient dynamic learning in systems with restricted data, leveraging symmetry as an inductive bias.

Method: They propose Latent Mixture of Symmetries (Latent MoS), a model that incorporates symmetry-governed latent factors and employs a hierarchical model architecture.

Result: Latent MoS showed superior performance in interpolation and extrapolation tasks in numerical tests across diverse physical systems, while yielding interpretable results.

Conclusion: The study concludes that Latent MoS is effective not only in dynamic tasks but also facilitates future analysis in geometric and safety-critical frameworks.

Abstract: Learning dynamics is essential for model-based control and Reinforcement
Learning in engineering systems, such as robotics and power systems. However,
limited system measurements, such as those from low-resolution sensors, demand
sample-efficient learning. Symmetry provides a powerful inductive bias by
characterizing equivariant relations in system states to improve sample
efficiency. While recent methods attempt to discover symmetries from data, they
typically assume a single global symmetry group and treat symmetry discovery
and dynamic learning as separate tasks, leading to limited expressiveness and
error accumulation. In this paper, we propose the Latent Mixture of Symmetries
(Latent MoS), an expressive model that captures a mixture of symmetry-governed
latent factors from complex dynamical measurements. Latent MoS focuses on
dynamic learning while locally and provably preserving the underlying symmetric
transformations. To further capture long-term equivariance, we introduce a
hierarchical architecture that stacks MoS blocks. Numerical experiments in
diverse physical systems demonstrate that Latent MoS outperforms
state-of-the-art baselines in interpolation and extrapolation tasks while
offering interpretable latent representations suitable for future geometric and
safety-critical analyses.

</details>


### [437] [Discovering Transformer Circuits via a Hybrid Attribution and Pruning Framework](https://arxiv.org/abs/2510.03282)
*Hao Gu,Vibhas Nair,Amrithaa Ashok Kumar,Jayvart Sharma,Ryan Lagasse*

Main category: cs.LG

TL;DR: The paper proposes a hybrid framework that combines attribution patching and edge pruning for efficient and faithful circuit analysis in language models.


<details>
  <summary>Details</summary>
Motivation: To address the trade-off in circuit analysis between speed and faithfulness using existing algorithms.

Method: The hybrid attribution and pruning (HAP) framework initially identifies potential subgraphs using attribution patching and then applies edge pruning for faithful circuit extraction.

Result: HAP improves speed by 46% compared to baseline methods while maintaining circuit faithfulness, as demonstrated in a case study on the Indirect Object Identification task.

Conclusion: HAP offers a scalable and efficient approach for circuit discovery in larger language models, preserving important components often lost by other methods.

Abstract: Interpreting language models often involves circuit analysis, which aims to
identify sparse subnetworks, or circuits, that accomplish specific tasks.
Existing circuit discovery algorithms face a fundamental trade-off: attribution
patching is fast but unfaithful to the full model, while edge pruning is
faithful but computationally expensive. This research proposes a hybrid
attribution and pruning (HAP) framework that uses attribution patching to
identify a high-potential subgraph, then applies edge pruning to extract a
faithful circuit from it. We show that HAP is 46\% faster than baseline
algorithms without sacrificing circuit faithfulness. Furthermore, we present a
case study on the Indirect Object Identification task, showing that our method
preserves cooperative circuit components (e.g. S-inhibition heads) that
attribution patching methods prune at high sparsity. Our results show that HAP
could be an effective approach for improving the scalability of mechanistic
interpretability research to larger models. Our code is available at
https://anonymous.4open.science/r/HAP-circuit-discovery.

</details>


### [438] [Edge-FIT: Federated Instruction Tuning of Quantized LLMs for Privacy-Preserving Smart Home Environments](https://arxiv.org/abs/2510.03284)
*Vinay Venkatesh,Vamsidhar R Kamanuru,Lav Kumar,Nikita Kothari*

Main category: cs.LG

TL;DR: The paper introduces Edge-FIT, a scalable framework for instruction tuning large language models in a federated manner to address challenges in computational overhead and communication using QLORA, achieving high F1-scores.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiencies of traditional federated learning methods when applied to massive large language models in decentralized setups, especially in IoT domains.

Method: Edge-FIT combines federated learning with 4-bit Quantized Low-Rank Adaptation (QLORA) while utilizing a filtered IoT-specific dataset for instruction tuning.

Result: Edge-FIT-tuned Llama 2 (7B) achieved an F1-score of 0.89 and showed a scalable trade-off on the 3.8B Phi-3-mini model in decentralized setups.

Conclusion: Edge-FIT enables scalable tuning and decentralized deployment of LLMs, offering an efficient solution for edge computing systems like home gateways.

Abstract: This paper proposes Edge-FIT (Federated Instruction Tuning on the Edge), a
scalable framework for Federated Instruction Tuning (FIT) of Large Language
Models (LLMs). Traditional Federated Learning (TFL) methods, like FedAvg, fail
when confronted with the massive parameter size of LLMs [3], [6]. Our Edge-FIT
framework combines federated learning with 4-bit Quantized Low-Rank Adaptation
(QLORA), mitigating the core issues of communication and computational
overhead. We demonstrate this by filtering the general-purpose Databricks Dolly
15k dataset for the IoT domain. Experimental results show the Edge-FIT tuned
Llama 2(7B) achieves an F1-Score of 0.89. We also demonstrate a viable
trade-off using the 3.8B Phi-3-mini model, validating Edge-FIT as a scalable
framework for decentralized LLM deployment on home compute gateways.

</details>


### [439] [Explore the Loss space with Hill-ADAM](https://arxiv.org/abs/2510.03613)
*Meenakshi Manikandan,Leilani Gilpin*

Main category: cs.LG

TL;DR: Hill-ADAM introduces a deterministic optimizer designed to escape local minima and find global minima in loss landscapes by alternating between minimization and maximization steps.


<details>
  <summary>Details</summary>
Motivation: Determining the global minimum in complex loss landscapes is challenging due to many local minima. Current stochastic optimizers struggle with escaping these minima.

Method: The proposed Hill-ADAM optimizer deterministically alternates between error minimization and maximization to explore the state space more effectively, avoiding convergence at local minima.

Result: Hill-ADAM was validated by testing it on 5 loss functions and 12 image color correction tasks, demonstrating its exploratory capabilities.

Conclusion: Hill-ADAM provides a robust approach for optimizing tasks by systematically alternating between minimizing and maximizing errors, achieving reliable exploration for better global minimum identification.

Abstract: This paper introduces Hill-ADAM. Hill-ADAM is an optimizer with its focus
towards escaping local minima in prescribed loss landscapes to find the global
minimum. Hill-ADAM escapes minima by deterministically exploring the state
space. This eliminates uncertainty from random gradient updates in stochastic
algorithms while seldom converging at the first minimum that visits. In the
paper we first derive an analytical approximation of the ADAM Optimizer step
size at a particular model state. From there define the primary condition
determining ADAM limitations in escaping local minima. The proposed optimizer
algorithm Hill-ADAM alternates between error minimization and maximization. It
maximizes to escape the local minimum and minimizes again afterward. This
alternation provides an overall exploration throughout the loss space. This
allows the deduction of the global minimum's state. Hill-ADAM was tested with 5
loss functions and 12 amber-saturated to cooler-shade image color correction
instances.

</details>


### [440] [Neural Bayesian Filtering](https://arxiv.org/abs/2510.03614)
*Christopher Solinas,Radovan Haluska,David Sychrovsky,Finbarr Timbers,Nolan Bard,Michael Buro,Martin Schmid,Nathan R. Sturtevant,Michael Bowling*

Main category: cs.LG

TL;DR: The paper introduces Neural Bayesian Filtering (NBF), a method for tracking hidden states in partially observable systems by combining classical filtering techniques with deep generative models.


<details>
  <summary>Details</summary>
Motivation: The paper aims to improve the ability of filters to track dynamic, multimodal beliefs in partially observable environments while making the updating of beliefs computationally efficient.

Method: NBF employs latent representations for hidden states, utilizes fixed-length embedding vectors for beliefs, applies particle-style updates in embedding space, and integrates generative models for state estimation.

Result: NBF successfully tracks multimodal beliefs and mitigates particle impoverishment in three partially observable environments, showcasing its accuracy and efficiency.

Conclusion: NBF demonstrates a promising combination of classical filtering efficiency and the expressiveness of generative models, achieving effective state estimation in complex systems.

Abstract: We present Neural Bayesian Filtering (NBF), an algorithm for maintaining
distributions over hidden states, called beliefs, in partially observable
systems. NBF is trained to find a good latent representation of the beliefs
induced by a task. It maps beliefs to fixed-length embedding vectors, which
condition generative models for sampling. During filtering, particle-style
updates compute posteriors in this embedding space using incoming observations
and the environment's dynamics. NBF combines the computational efficiency of
classical filters with the expressiveness of deep generative models - tracking
rapidly shifting, multimodal beliefs while mitigating the risk of particle
impoverishment. We validate NBF in state estimation tasks in three partially
observable environments.

</details>


### [441] [Why mask diffusion does not work](https://arxiv.org/abs/2510.03289)
*Haocheng Sun,Cynthia Xin Wen,Edward Hong Wang*

Main category: cs.LG

TL;DR: Diffusion language models are promising for parallel generation and bidirectional attention, but mask diffusion has inherent limitations, which are examined in this paper along with improved training and inference strategies.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of mask diffusion in its ability to support parallel generation and bidirectional attention, and to provide strategies for improvement.

Method: Demonstrate mask diffusion's inherent difficulties through analysis and propose optimized training and inference strategies.

Result: The paper identifies challenges with mask diffusion and introduces effective strategies to overcome these limitations.

Conclusion: Mask diffusion has drawbacks in achieving its theoretical advantages, but with proper strategies, its potential can be further unlocked.

Abstract: The main advantages of diffusion language models over autoregressive (AR)
models lie in their ability to support parallel generation and bidirectional
attention, enabling a more controllable generation process. In recent years,
open-source mask diffusion language models have emerged, most of which are
based on a variant known as absorbing diffusion. However, this paper
demonstrates why mask diffusion faces inherent difficulties in achieving
parallel generation and bidirectional attention. We also propose the most
effective training and inference strategies for mask diffusion.

</details>


### [442] [Single-Core Superscalar Optimization of Clifford Neural Layers](https://arxiv.org/abs/2510.03290)
*X. Angelo Huang,Ruben Ciranni,Giovanni Spadaccini,Carla J. López Zurita*

Main category: cs.LG

TL;DR: The paper focuses on optimizing Clifford neural layers for $E(n)$ and $O(n)$ equivariance, resulting in a 21.35x speedup in some cases compared to baseline.


<details>
  <summary>Details</summary>
Motivation: To improve the efficiency of Clifford convolutional layers used in equivariant neural networks.

Method: Analyzes the theoretical foundations of Clifford algebras to eliminate computational redundancies and applies systematic optimization techniques.

Result: Achieved an average speedup of 21.35x for eleven functions and comparable or better performance than PyTorch in six cases.

Conclusion: The proposed optimizations significantly enhance the inference speed of Clifford neural layers without compromising correctness.

Abstract: Within the growing interest in the physical sciences in developing networks
with equivariance properties, Clifford neural layers shine as one approach that
delivers $E(n)$ and $O(n)$ equivariances given specific group actions. In this
paper, we analyze the inner structure of the computation within Clifford
convolutional layers and propose and implement several optimizations to speed
up the inference process while maintaining correctness. In particular, we begin
by analyzing the theoretical foundations of Clifford algebras to eliminate
redundant matrix allocations and computations, then systematically apply
established optimization techniques to enhance performance further. We report a
final average speedup of 21.35x over the baseline implementation of eleven
functions and runtimes comparable to and faster than the original PyTorch
implementation in six cases. In the remaining cases, we achieve performance in
the same order of magnitude as the original library.

</details>


### [443] [Implicit Models: Expressive Power Scales with Test-Time Compute](https://arxiv.org/abs/2510.03638)
*Jialin Liu,Lisang Ding,Stanley Osher,Wotao Yin*

Main category: cs.LG

TL;DR: The paper investigates implicit models, which compute outputs by iterating a single parameter block and can achieve high expressive power as test-time compute increases.


<details>
  <summary>Details</summary>
Motivation: To understand how implicit models, with compact architectures and constant memory requirements, can rival or surpass larger explicit models by increasing test-time compute.

Method: The authors perform a nonparametric analysis of expressive power, provide mathematical proofs, and validate their theory in image reconstruction, scientific computing, and operations research domains.

Result: The study shows that implicit models, through iterative computation, progressively express more complex mappings and achieve higher solution quality with increased test-time iterations.

Conclusion: Implicit models can benefit from scalable expressive power and improved stability with increased test-time compute, making them an efficient alternative to explicit models.

Abstract: Implicit models, an emerging model class, compute outputs by iterating a
single parameter block to a fixed point. This architecture realizes an
infinite-depth, weight-tied network that trains with constant memory,
significantly reducing memory needs for the same level of performance compared
to explicit models. While it is empirically known that these compact models can
often match or even exceed larger explicit networks by allocating more
test-time compute, the underlying mechanism remains poorly understood.
  We study this gap through a nonparametric analysis of expressive power. We
provide a strict mathematical characterization, showing that a simple and
regular implicit operator can, through iteration, progressively express more
complex mappings. We prove that for a broad class of implicit models, this
process lets the model's expressive power scale with test-time compute,
ultimately matching a much richer function class. The theory is validated
across three domains: image reconstruction, scientific computing, and
operations research, demonstrating that as test-time iterations increase, the
complexity of the learned mapping rises, while the solution quality
simultaneously improves and stabilizes.

</details>


### [444] [UniPruning: Unifying Local Metric and Global Feedback for Scalable Sparse LLMs](https://arxiv.org/abs/2510.03291)
*Yizhuo Ding,Wanying Qu,Jiawei Geng,Wenqi Shao,Yanwei Fu*

Main category: cs.LG

TL;DR: UniPruning is a post-training pruning framework for Large Language Models (LLMs) that combines local and global methods, achieving efficient sparsity and strong performance.


<details>
  <summary>Details</summary>
Motivation: LLMs are powerful but require high computational and memory resources. Pruning addresses this challenge by introducing sparsity, yet existing methods fail to balance efficiency and robustness.

Method: UniPruning incorporates mirror descent optimization with layer-wise local saliency metrics and a lightweight global controller. It supports both unstructured and N:M sparsity formats, enabling hardware-aware constraints.

Result: Experiments demonstrate that UniPruning achieves competitive or superior results in perplexity, zero-shot accuracy, and adaptability across multiple pretrained LLM families and benchmarks.

Conclusion: UniPruning offers a scalable, efficient, and principled approach to reduce computational costs in LLMs, advancing their practical deployment without sacrifice to performance.

Abstract: Large Language Models (LLMs) achieve strong performance across diverse tasks
but face prohibitive computational and memory costs. Pruning offers a promising
path by inducing sparsity while preserving architectural flexibility. However,
existing methods struggle to balance efficiency and robustness: local metric
approaches prune layer by layer but often collapse under high sparsity, whereas
global feedback methods enforce consistency at the cost of expensive weight
updates or restrictive semi-structured formats. We present UniPruning, a
unified post-training pruning framework that combines the speed of local
saliency metrics with the stability of global coordination, enabled by a mirror
descent based optimization, all without updating model weights. UniPruning
leverages fast layer-wise scoring and a lightweight global controller to
allocate a single sparsity budget, supporting both unstructured and
semi-structured N :M pruning within one framework. After a brief calibration,
it can generate pruning masks for arbitrary sparsity levels in one shot, and
adapts seamlessly to hardware-aware constraints. Extensive experiments on
multiple pretrained LLM families and standard benchmarks show that UniPruning
consistently delivers competitive or superior perplexity and zero-shot
accuracy. Ablation studies further highlight the importance of mirror descent
and local saliency anchoring. Overall, UniPruning provides an efficient,
principled, and scalable solution for sparsifying large-scale LLMs. Our code is
available at: https://github.com/RainbowQTT/UniPruning.

</details>


### [445] [Does higher interpretability imply better utility? A Pairwise Analysis on Sparse Autoencoders](https://arxiv.org/abs/2510.03659)
*Xu Wang,Yan Hu,Benyou Wang,Difan Zou*

Main category: cs.LG

TL;DR: The paper investigates whether the interpretability of sparse autoencoders (SAEs) directly correlates with their effectiveness in steering large language models (LLMs). Despite assumptions, results reveal only a weak correlation, emphasizing a gap between interpretability and utility for steering.


<details>
  <summary>Details</summary>
Motivation: To address the assumption that interpretable features in sparse autoencoders enable effective steering of large language models and determine if this interpretability implies better utility.

Method: Analyzing 90 sparse autoencoders across three large language models (LLMs) with various architectures and sparsity levels, using established benchmarks (SAEBench and AxBench), and proposing a new feature selection criterion called Delta Token Confidence to identify features that enhance steering effectiveness.

Result: The study finds a weak positive correlation (Kendall's tau_b ≈ 0.298) between interpretability and steering utility, and shows that the Delta Token Confidence method significantly improves steering performance by 52.52%. Furthermore, after applying this method, the correlation between interpretability and utility becomes negligible or even negative.

Conclusion: The findings challenge the assumption that interpretability ensures steering utility, demonstrating that the most effective steering features may not align with interpretable ones. Delta Token Confidence proves a valuable tool for advancing steering performance in LLMs.

Abstract: Sparse Autoencoders (SAEs) are widely used to steer large language models
(LLMs), based on the assumption that their interpretable features naturally
enable effective model behavior steering. Yet, a fundamental question remains
unanswered: does higher interpretability indeed imply better steering utility?
To answer this question, we train 90 SAEs across three LLMs (Gemma-2-2B,
Qwen-2.5-3B, Gemma-2-9B), spanning five architectures and six sparsity levels,
and evaluate their interpretability and steering utility based on SAEBench
(arXiv:2501.12345) and AxBench (arXiv:2502.23456) respectively, and perform a
rank-agreement analysis via Kendall's rank coefficients (tau b). Our analysis
reveals only a relatively weak positive association (tau b approx 0.298),
indicating that interpretability is an insufficient proxy for steering
performance. We conjecture the interpretability utility gap may stem from the
selection of SAE features, as not all of them are equally effective for
steering. To further find features that truly steer the behavior of LLMs, we
propose a novel selection criterion called Delta Token Confidence, which
measures how much amplifying a feature changes the next token distribution. We
show that our method improves the steering performance of three LLMs by 52.52
percent compared to the current best output score based criterion
(arXiv:2503.34567). Strikingly, after selecting features with high Delta Token
Confidence, the correlation between interpretability and utility vanishes (tau
b approx 0), and can even become negative. This further highlights the
divergence between interpretability and utility for the most effective steering
features.

</details>


### [446] [Towards Sampling Data Structures for Tensor Products in Turnstile Streams](https://arxiv.org/abs/2510.03678)
*Zhao Song,Shenghao Xie,Samson Zhou*

Main category: cs.LG

TL;DR: The paper introduces the 'attention sampler' to reduce the computational demands of large-scale attention models using importance sampling in streaming.


<details>
  <summary>Details</summary>
Motivation: Address the computational challenges posed by traditional attention mechanisms in large language models through innovative sampling methods.

Method: Proposing the 'attention sampler,' which builds on importance sampling and draws inspiration from classical l2 sampling and modern attention mechanisms.

Result: The attention sampler demonstrates reduced computational requirements and theoretical efficiency in terms of space and update time.

Conclusion: The framework is both scalable and versatile, applicable across different architectures and domains.

Abstract: This paper studies the computational challenges of large-scale
attention-based models in artificial intelligence by utilizing importance
sampling methods in the streaming setting. Inspired by the classical definition
of the $\ell_2$ sampler and the recent progress of the attention scheme in
Large Language Models (LLMs), we propose the definition of the attention
sampler. Our approach significantly reduces the computational burden of
traditional attention mechanisms. We analyze the effectiveness of the attention
sampler from a theoretical perspective, including space and update time.
Additionally, our framework exhibits scalability and broad applicability across
various model architectures and domains.

</details>


### [447] [Group Policy Gradient](https://arxiv.org/abs/2510.03679)
*Junhua Chen,Zixi Zhang,Hantao Zhong,Rika Antonova*

Main category: cs.LG

TL;DR: Group Policy Gradient (GPG) introduces a critic-free policy-gradient method for Markov Decision Processes, achieving performance comparable or superior to PPO while being computationally more efficient.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address limitations in reinforcement learning methods involving critics, such as high computational costs and complexity, by proposing a simpler and more resource-efficient alternative.

Method: The paper proposes GPG, which uses a group-based Monte Carlo advantage estimator instead of a learned value function while maintaining PPO's clipped-objective structure.

Result: GPG is shown to match or outperform PPO on standard benchmarks and demonstrates more efficient use of computational resources through better utilization of parallel simulations.

Conclusion: GPG provides a viable and effective alternative to critic-based reinforcement learning methodologies, optimizing computational efficiency without sacrificing performance.

Abstract: We introduce Group Policy Gradient (GPG), a family of critic-free
policy-gradient estimators for general MDPs. Inspired by the success of GRPO's
approach in Reinforcement Learning from Human Feedback (RLHF), GPG replaces a
learned value function with a group-based Monte Carlo advantage estimator,
removing the memory, compute, and hyperparameter costs of training a critic
while preserving PPO's clipped-objective structure. We prove the consistency of
the GPG estimator, analyze the bias-variance tradeoffs, and demonstrate
empirically that GPG matches or outperforms PPO on standard benchmarks. GPG
makes better use of parallel simulations, which, together with its critic-free
design, results in more efficient use of computational resources than PPO.

</details>


### [448] [Dynamic Meta-Learning for Adaptive XGBoost-Neural Ensembles](https://arxiv.org/abs/2510.03301)
*Arthur Sedek*

Main category: cs.LG

TL;DR: The paper presents an adaptive ensemble approach combining XGBoost and neural networks using meta-learning for improved performance and interpretability.


<details>
  <summary>Details</summary>
Motivation: To improve predictive performance and interpretability through a dynamic combination of machine learning models.

Method: The approach combines XGBoost and neural networks via meta-learning, utilizing advanced uncertainty quantification and feature importance techniques.

Result: The method demonstrated better predictive performance and interpretability across various datasets.

Conclusion: This approach contributes to smarter and more adaptable machine learning systems.

Abstract: This paper introduces a novel adaptive ensemble framework that
synergistically combines XGBoost and neural networks through sophisticated
meta-learning. The proposed method leverages advanced uncertainty
quantification techniques and feature importance integration to dynamically
orchestrate model selection and combination. Experimental results demonstrate
superior predictive performance and enhanced interpretability across diverse
datasets, contributing to the development of more intelligent and flexible
machine learning systems.

</details>


### [449] [From Moments to Models: Graphon Mixture-Aware Mixup and Contrastive Learning](https://arxiv.org/abs/2510.03690)
*Ali Azizpour,Reza Ramezanpour,Ashutosh Sabharwal,Santiago Segarra*

Main category: cs.LG

TL;DR: The paper introduces a framework leveraging graph mixture models (graphons) for improved graph representation learning and data augmentation, achieving state-of-the-art results in multiple academic benchmarks.


<details>
  <summary>Details</summary>
Motivation: Modern graph representation learning methods often fail to account for the underlying mixed distributions of real-world graph datasets, leading to suboptimal performance.

Method: The authors utilize graphons and graph moments (motif densities) to cluster graphs and disentangle their generative mechanisms. They propose techniques like GMAM for graph mixup and MGCL for adaptive augmentations and improved negative sampling.

Result: The proposed approach demonstrates significant empirical performance, achieving state-of-the-art accuracy in both unsupervised and supervised graph learning tasks on benchmark datasets.

Conclusion: By explicitly modeling mixture structures in graph datasets and leveraging graphons, the framework enhances representation learning methods, providing clearer theoretical guarantees and superior empirical outcomes.

Abstract: Real-world graph datasets often consist of mixtures of populations, where
graphs are generated from multiple distinct underlying distributions. However,
modern representation learning approaches, such as graph contrastive learning
(GCL) and augmentation methods like Mixup, typically overlook this mixture
structure. In this work, we propose a unified framework that explicitly models
data as a mixture of underlying probabilistic graph generative models
represented by graphons. To characterize these graphons, we leverage graph
moments (motif densities) to cluster graphs arising from the same model. This
enables us to disentangle the mixture components and identify their distinct
generative mechanisms. This model-aware partitioning benefits two key graph
learning tasks: 1) It enables a graphon-mixture-aware mixup (GMAM), a data
augmentation technique that interpolates in a semantically valid space guided
by the estimated graphons, instead of assuming a single graphon per class. 2)
For GCL, it enables model-adaptive and principled augmentations. Additionally,
by introducing a new model-aware objective, our proposed approach (termed MGCL)
improves negative sampling by restricting negatives to graphs from other
models. We establish a key theoretical guarantee: a novel, tighter bound
showing that graphs sampled from graphons with small cut distance will have
similar motif densities with high probability. Extensive experiments on
benchmark datasets demonstrate strong empirical performance. In unsupervised
learning, MGCL achieves state-of-the-art results, obtaining the top average
rank across eight datasets. In supervised learning, GMAM consistently
outperforms existing strategies, achieving new state-of-the-art accuracy in 6
out of 7 datasets.

</details>


### [450] [Revoking Amnesia: RL-based Trajectory Optimization to Resurrect Erased Concepts in Diffusion Models](https://arxiv.org/abs/2510.03302)
*Daiheng Gao,Nanxiang Jiang,Andi Zhang,Shilin Lu,Yufei Tang,Wenbo Zhou,Weiming Zhang,Zhaoxin Fan*

Main category: cs.LG

TL;DR: The paper critiques current concept erasure methods in T2I diffusion models, explaining their superficiality and proposing a reversible approach to resurrect erased concepts using RL-based trajectory optimization.


<details>
  <summary>Details</summary>
Motivation: To investigate the effectiveness and limitations of current concept erasure techniques in text-to-image diffusion models, which aim to enforce ethical and copyright restrictions but may only superficially manipulate sampling trajectories.

Method: The authors propose RevAm, an RL-based trajectory optimization framework leveraging modified Group Relative Policy Optimization (GRPO) to dynamically recover erased concepts by steering the denoising process, without changing model weights.

Result: RevAm demonstrates high fidelity in resurrecting erased concepts, achieves a 10x reduction in computational time, and highlights critical flaws in existing concept erasure mechanisms.

Conclusion: The research reveals that current concept erasure methods offer only superficial safety by biasing rather than removing target concepts, and underscores the need for more robust approaches beyond trajectory manipulation techniques.

Abstract: Concept erasure techniques have been widely deployed in T2I diffusion models
to prevent inappropriate content generation for safety and copyright
considerations. However, as models evolve to next-generation architectures like
Flux, established erasure methods (\textit{e.g.}, ESD, UCE, AC) exhibit
degraded effectiveness, raising questions about their true mechanisms. Through
systematic analysis, we reveal that concept erasure creates only an illusion of
``amnesia": rather than genuine forgetting, these methods bias sampling
trajectories away from target concepts, making the erasure fundamentally
reversible. This insight motivates the need to distinguish superficial safety
from genuine concept removal. In this work, we propose \textbf{RevAm}
(\underline{Rev}oking \underline{Am}nesia), an RL-based trajectory optimization
framework that resurrects erased concepts by dynamically steering the denoising
process without modifying model weights. By adapting Group Relative Policy
Optimization (GRPO) to diffusion models, RevAm explores diverse recovery
trajectories through trajectory-level rewards, overcoming local optima that
limit existing methods. Extensive experiments demonstrate that RevAm achieves
superior concept resurrection fidelity while reducing computational time by
10$\times$, exposing critical vulnerabilities in current safety mechanisms and
underscoring the need for more robust erasure techniques beyond trajectory
manipulation.

</details>


### [451] [Balancing Interpretability and Performance in Reinforcement Learning: An Adaptive Spectral Based Linear Approach](https://arxiv.org/abs/2510.03722)
*Qianxin Yi,Shao-Bo Lin,Jun Fan,Yao Wang*

Main category: cs.LG

TL;DR: The paper introduces a novel reinforcement learning approach focusing on both interpretability and performance by using a spectral-based linear RL method.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of traditional RL methods, which prioritize performance over interpretability, and to develop an approach that integrates both aspects for practical applications.

Method: A spectral-based linear RL method is proposed, which extends ridge regression using a spectral filter function and incorporates an adaptive regularization parameter guided by the bias-variance trade-off principle. Theoretical analysis supports this with near-optimal error bounds.

Result: Experiments on simulated environments and real-world datasets (e.g., Kuaishou and Taobao) demonstrate that the approach matches or outperforms current baselines while also offering enhanced decision interpretability.

Conclusion: The proposed method bridges the gap between RL theory and practical application by offering a solution that combines accuracy, interpretability, and adaptability, making it highly suitable for management contexts.

Abstract: Reinforcement learning (RL) has been widely applied to sequential decision
making, where interpretability and performance are both critical for practical
adoption. Current approaches typically focus on performance and rely on post
hoc explanations to account for interpretability. Different from these
approaches, we focus on designing an interpretability-oriented yet
performance-enhanced RL approach. Specifically, we propose a spectral based
linear RL method that extends the ridge regression-based approach through a
spectral filter function. The proposed method clarifies the role of
regularization in controlling estimation error and further enables the design
of an adaptive regularization parameter selection strategy guided by the
bias-variance trade-off principle. Theoretical analysis establishes
near-optimal bounds for both parameter estimation and generalization error.
Extensive experiments on simulated environments and real-world datasets from
Kuaishou and Taobao demonstrate that our method either outperforms or matches
existing baselines in decision quality. We also conduct interpretability
analyses to illustrate how the learned policies make decisions, thereby
enhancing user trust. These results highlight the potential of our approach to
bridge the gap between RL theory and practical decision making, providing
interpretability, accuracy, and adaptability in management contexts.

</details>


### [452] [Cost Efficient Fairness Audit Under Partial Feedback](https://arxiv.org/abs/2510.03734)
*Nirjhar Das,Mohit Sharma,Praharsh Nanavati,Kirankumar Shiragur,Amit Deshpande*

Main category: cs.LG

TL;DR: This paper proposes cost-effective algorithms to audit the fairness of classifiers under partial feedback, outperforming baselines by 50% on real-world datasets.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of auditing fairness in machine learning classifiers where feedback is limited to only positively classified instances, making typical fairness evaluation expensive or unreliable.

Method: The authors developed two algorithms: one for a black-box model with minimal assumptions and another for mixture models leveraging truncated sampling and maximum-a-posteriori methods. These methodologies were designed to handle fairness metrics like demographic parity, equal opportunity, and equalized odds.

Result: The proposed algorithms demonstrated a significant reduction in audit costs compared to natural baselines, including a 50% improvement on datasets such as Adult Income and Law School.

Conclusion: The study highlights the feasibility of cost-effective fairness auditing through tailored algorithms that address partial feedback constraints while achieving superior performance over conventional methods.

Abstract: We study the problem of auditing the fairness of a given classifier under
partial feedback, where true labels are available only for positively
classified individuals, (e.g., loan repayment outcomes are observed only for
approved applicants). We introduce a novel cost model for acquiring additional
labeled data, designed to more accurately reflect real-world costs such as
credit assessment, loan processing, and potential defaults. Our goal is to find
optimal fairness audit algorithms that are more cost-effective than random
exploration and natural baselines.
  In our work, we consider two audit settings: a black-box model with no
assumptions on the data distribution, and a mixture model, where features and
true labels follow a mixture of exponential family distributions. In the
black-box setting, we propose a near-optimal auditing algorithm under mild
assumptions and show that a natural baseline can be strictly suboptimal. In the
mixture model setting, we design a novel algorithm that achieves significantly
lower audit cost than the black-box case. Our approach leverages prior work on
learning from truncated samples and maximum-a-posteriori oracles, and extends
known results on spherical Gaussian mixtures to handle exponential family
mixtures, which may be of independent interest. Moreover, our algorithms apply
to popular fairness metrics including demographic parity, equal opportunity,
and equalized odds. Empirically, we demonstrate strong performance of our
algorithms on real-world fair classification datasets like Adult Income and Law
School, consistently outperforming natural baselines by around 50% in terms of
audit cost.

</details>


### [453] [Thin Bridges for Drug Text Alignment: Lightweight Contrastive Learning for Target Specific Drug Retrieval](https://arxiv.org/abs/2510.03309)
*Mallikarjuna Tupakula*

Main category: cs.LG

TL;DR: The study explores lightweight multimodal models for drug discovery using thin projection layers rather than heavy pretraining, showing improved alignment between chemical structures and text representations.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the challenges of multimodal drug discovery systems that rely on computationally expensive pretraining and large-scale multimodal datasets.

Method: They train dual linear projection heads over frozen unimodal encoders to align molecular fingerprints with biomedical sentence embeddings using a contrastive objective, incorporating hard negative weighting and margin loss for improved results.

Result: Their method achieves non-trivial alignment between chemical and text representations and shows improved discrimination for drugs sharing the same target, especially under scaffold-based splits.

Conclusion: Lightweight projection layers (thin bridges) are a compute-efficient alternative to large-scale multimodal pretraining, facilitating scaffold-aware drug text alignment and precise target-specific retrieval.

Abstract: Multimodal foundation models hold promise for drug discovery and biomedical
applications, but most existing approaches rely on heavy pretraining or large
scale multimodal corpora. We investigate whether thin contrastive bridges,
lightweight projection heads over frozen unimodal encoders can align chemical
and textual representations without training a full multimodal model. Using
paired mechanisms from ChEMBL, we align ECFP4 molecular fingerprints with
biomedical sentence embeddings through dual linear projections trained with a
contrastive objective. To better handle drugs sharing the same therapeutic
target, we incorporate hard negative weighting and a margin loss. Evaluation
under scaffold based splits, which require generalization across disjoint
chemical cores, demonstrates that our approach achieves non-trivial cross modal
alignment and substantially improves within target discrimination compared to
frozen baselines. These results suggest that thin bridges offer a compute
efficient alternative to large scale multimodal pretraining, enabling scaffold
aware drug text alignment and target specific retrieval in precision medicine.

</details>


### [454] [Allocation of Parameters in Transformers](https://arxiv.org/abs/2510.03784)
*Ruoxi Yu,Haotian Jiang,Jingpu Cheng,Penghao Yu,Qianxiao Li,Zhong Li*

Main category: cs.LG

TL;DR: The paper investigates efficient parameter allocation in Transformer architectures, focusing on attention heads and dimensions.


<details>
  <summary>Details</summary>
Motivation: Transformers have shown exceptional performance in diverse areas, but understanding how to allocate resources like attention heads and head dimensions efficiently remains unclear.

Method: Through mathematical approximations and experiments, the study examines the balance between the number of attention heads and their dimension, as well as the role of early and later layers in processing sequences.

Result: The study reveals a saturation effect in softmax activations where increasing head dimensions yields diminishing returns, especially for long sequences. This suggests later layers can work efficiently with fewer parameters.

Conclusion: The paper proposes optimized, theoretically-supported strategies for parameter allocation in Transformer models to enhance model efficiency.

Abstract: Transformers have achieved remarkable successes across a wide range of
applications, yet the theoretical foundation of their model efficiency remains
underexplored. In this work, we investigate how the model parameters -- mainly
attention heads and head dimensions -- should be allocated across layers to
balance expressivity and efficiency. We first provide mathematical analysis on
the role of early layers in information extraction from an approximation
perspective, with a theoretical characterization on the trade-off between the
number of heads and head dimension under a fixed parameter budget. In addition,
we uncover and prove the \emph{saturation} behavior of softmax activations:
Continuously increasing head dimensions can lead to diminishing returns in
learning errors, particularly for long sequences. Supported by both theory and
experiments, this saturation pattern suggests that later layers can operate
more efficiently with reduced parameters. Combining these insights, we propose
principled strategies for allocating attention heads and dimensions across
Transformers' layers, shedding light on theoretically-grounded model efficiency
of Transformer-based architectures.

</details>


### [455] [Predicting Effects, Missing Distributions: Evaluating LLMs as Human Behavior Simulators in Operations Management](https://arxiv.org/abs/2510.03310)
*Runze Zhang,Xiaowei Zhang,Mingyang Zhao*

Main category: cs.LG

TL;DR: This paper examines how well LLMs replicate human behavior in operations management and emphasizes their ability to reproduce decision biases though with distributional divergence.


<details>
  <summary>Details</summary>
Motivation: To verify the capability of LLMs in simulating human behavior for behavioral operations management, offering an alternative for costly traditional experimental methods.

Method: Nine published behavioral operations experiments were used to test hypothesis replication and response distributional alignment, alongside interventions like chain-of-thought prompting and hyperparameter tuning.

Result: LLMs replicate most hypothesis-level behavioral effects but show divergence in response distributions. Lightweight interventions improved alignment and allowed smaller models to compete with larger systems.

Conclusion: LLMs show promise for simulating human behaviors, but distributional misalignments require attention. Simple prompts and model adjustments enhance their utility.

Abstract: LLMs are emerging tools for simulating human behavior in business, economics,
and social science, offering a lower-cost complement to laboratory experiments,
field studies, and surveys. This paper evaluates how well LLMs replicate human
behavior in operations management. Using nine published experiments in
behavioral operations, we assess two criteria: replication of hypothesis-test
outcomes and distributional alignment via Wasserstein distance. LLMs reproduce
most hypothesis-level effects, capturing key decision biases, but their
response distributions diverge from human data, including for strong commercial
models. We also test two lightweight interventions -- chain-of-thought
prompting and hyperparameter tuning -- which reduce misalignment and can
sometimes let smaller or open-source models match or surpass larger systems.

</details>


### [456] [Robust Batched Bandits](https://arxiv.org/abs/2510.03798)
*Yunwen Guo,Yunlun Shu,Gongyi Zhuo,Tianyu Wang*

Main category: cs.LG

TL;DR: The paper introduces robust batched bandit algorithms tailored for heavy-tailed reward distributions, challenging existing assumptions of light-tailed rewards.


<details>
  <summary>Details</summary>
Motivation: To address real-world scenarios where outcomes exhibit heavy-tailed reward distributions, such as clinical trials.

Method: The authors propose batched multi-armed bandit algorithms for both finite-arm and Lipschitz-continuous settings, analyzing their performance under varying reward tail behaviors.

Result: Heavier-tailed rewards can reduce the required number of batches in instance-independent and Lipschitz settings, but batch requirements remain unchanged under instance-dependent settings.

Conclusion: Tail heaviness influences batch efficiency differently across settings, offering new insights for optimizing batched MAB algorithms in practical applications.

Abstract: The batched multi-armed bandit (MAB) problem, in which rewards are collected
in batches, is crucial for applications such as clinical trials. Existing
research predominantly assumes light-tailed reward distributions, yet many
real-world scenarios, including clinical outcomes, exhibit heavy-tailed
characteristics. This paper bridges this gap by proposing robust batched bandit
algorithms designed for heavy-tailed rewards, within both finite-arm and
Lipschitz-continuous settings. We reveal a surprising phenomenon: in the
instance-independent regime, as well as in the Lipschitz setting,
heavier-tailed rewards necessitate a smaller number of batches to achieve
near-optimal regret. In stark contrast, for the instance-dependent setting, the
required number of batches to attain near-optimal regret remains invariant with
respect to tail heaviness.

</details>


### [457] [Scaling Laws Revisited: Modeling the Role of Data Quality in Language Model Pretraining](https://arxiv.org/abs/2510.03313)
*Anirudh Subramanyam,Yuxin Chen,Robert L. Grossman*

Main category: cs.LG

TL;DR: The paper introduces a formalized data-quality parameter (Q) and extends scaling laws to account for model size, data volume, and data quality, providing practical guidance for large-scale language model pretraining.


<details>
  <summary>Details</summary>
Motivation: Previous work on scaling laws has explored model size and dataset volume but lacked a principled approach to incorporate data quality, which is crucial for improving efficiency and reducing computational demands.

Method: The researchers propose a dimensionless parameter, Q, to quantify data quality and integrate it into scaling laws. They offer two methods to estimate Q (corruption rate proxy and deficiency measure) and validate the approach through synthetic experiments involving noise injection and coverage variation in datasets.

Result: Findings reveal that loss predictably scales with data quality and higher-quality data considerably reduces the model size and compute resources needed. The results demonstrate robustness to moderate data corruption and validate the new law through out-of-sample evaluations.

Conclusion: The study provides a generalizable scaling law for incorporating data quality, offering actionable insights into optimizing data curation and model scaling in language model training.

Abstract: Scaling laws for language model training traditionally characterize how
performance scales with model size and dataset volume. Prior work has explored
architecture variants and data treatments such as dataset filtering and noise
injection in language model pretraining; however, these studies have not
formalized data quality within a principled scaling law. We introduce a
dimensionless data-quality parameter Q, and propose a quality-aware scaling law
extending the Chinchilla framework to predict loss as a joint function of model
size, data volume, and data quality. The law is motivated by an
effective-sample-size and information-theoretic view of noisy or redundant
corpora, and it admits two practical estimators for Q: (i) a corruption rate
proxy and (ii) a deficiency measure. Through synthetic experiments in neural
machine translation and autoregressive modeling -- where we systematically
control data quality via multiple levels of noise injection and coverage
variation -- we show that loss scales predictably with data quality and that
higher-quality data can substantially reduce model size and hence compute
requirements. Our results demonstrate a sublinear decay of effective data with
quality and robustness to moderate data corruption; out-of-sample evaluations
further validate the predictive form of the law. Unlike prior empirical
analyses, our work establishes an explicit, generalizable law for data quality,
offering concrete guidance for balancing data curation effort and model scale
in large-scale pretraining.

</details>


### [458] [TROLL: Trust Regions improve Reinforcement Learning for Large Language Models](https://arxiv.org/abs/2510.03817)
*Philipp Becker,Niklas Freymuth,Serge Thilges,Fabian Otto,Gerhard Neumann*

Main category: cs.LG

TL;DR: This study introduces Trust Region Optimization for Large Language Models (TROLL), a replacement for PPO-like clipping objectives in reinforcement learning fine-tuning of LLMs, which enhances performance, training stability, and efficiency.


<details>
  <summary>Details</summary>
Motivation: Clipping mechanisms in PPO-like objectives are a crude approximation and often lead to instability and suboptimal performance during reinforcement learning fine-tuning of large language models (LLMs).

Method: The authors propose the TROLL approach, which uses a novel discrete differentiable trust region projection to impose principled token-level KL constraints. This projection focuses on a sparse subset of token logits to optimize computational efficiency and effectiveness.

Result: TROLL outperforms PPO-like clipping in terms of training speed, stability, and success rates across datasets, model families, and advantage-estimation methods.

Conclusion: Replacing PPO-like clip objectives with Trust Region Optimization using TROLL improves reward-based fine-tuning for LLMs, providing consistent gains in stability, efficiency, and model performance.

Abstract: On-policy Reinforcement Learning (RL) with PPO-like clip objectives has
become the standard choice for reward-based fine-tuning of large language
models (LLMs). Although recent work has explored improved estimators of
advantages and normalization, the clipping mechanism itself has remained
untouched. Originally introduced as a proxy for principled KL-based trust
regions, clipping is a crude approximation that often causes unstable updates
and suboptimal performance. We replace the clip objective with a novel discrete
differentiable trust region projection, which provides principled token-level
KL constraints. The projection operates on a sparse subset of the model's most
important token logits to balance computational cost and projection
effectiveness. Our approach, Trust Region Optimization for Large Language
Models (TROLL), serves as a direct replacement for PPO-like clipping during
training and does not alter the model's inference behavior. Across datasets,
model families, and advantage-estimation methods, TROLL consistently
outperforms PPO-like clipping in terms of training speed, stability, and final
success rates.

</details>


### [459] [Fast frequency reconstruction using Deep Learning for event recognition in ring laser data](https://arxiv.org/abs/2510.03325)
*Giuseppe Di Somma,Giorgio Carelli,Angela D. V. Di Virgilio,Francesco Fuso,Enrico Maccioni,Paolo Marsili*

Main category: cs.LG

TL;DR: This paper proposes a neural network approach for rapid frequency reconstruction and classification of disturbances in sinusoidal signals, achieving high precision and accuracy.


<details>
  <summary>Details</summary>
Motivation: Current frequency reconstruction methods are slower and less precise, necessitating an improved approach for geophysical applications, such as those involving Ring Laser Gyroscopes.

Method: A neural network-based framework for frequency estimation and disturbance classification in sinusoidal signals is developed, achieving outputs in milliseconds.

Result: The method achieves frequency estimation precision improvement by a factor of 2 and classification accuracy rates of up to 100% on independent datasets.

Conclusion: Neural network integration in signal analysis provides significant advancements in speed and accuracy, paving the way for more effective geophysical signal processing methods.

Abstract: The reconstruction of a frequency with minimal delay from a sinusoidal signal
is a common task in several fields; for example Ring Laser Gyroscopes, since
their output signal is a beat frequency. While conventional methods require
several seconds of data, we present a neural network approach capable of
reconstructing frequencies of several hundred Hertz within approximately 10
milliseconds. This enables rapid trigger generation. The method outperforms
standard Fourier-based techniques, improving frequency estimation precision by
a factor of 2 in the operational range of GINGERINO, our Ring Laser
Gyroscope.\\ In addition to fast frequency estimation, we introduce an
automated classification framework to identify physical disturbances in the
signal, such as laser instabilities and seismic events, achieving accuracy
rates between 99\% and 100\% on independent test datasets for the seismic
class. These results mark a step forward in integrating artificial intelligence
into signal analysis for geophysical applications.

</details>


### [460] [Proximal Diffusion Neural Sampler](https://arxiv.org/abs/2510.03824)
*Wei Guo,Jaemoo Choi,Yuchen Zhu,Molei Tao,Yongxin Chen*

Main category: cs.LG

TL;DR: The "Proximal Diffusion Neural Sampler (PDNS)" framework is proposed to address mode collapse issues in training diffusion-based neural samplers for sampling from multimodal target distributions.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the difficulty of training diffusion-based neural samplers where multimodal distributions create barriers, leading to mode collapse and inefficiency.

Method: The PDNS framework tackles the stochastic optimal control problem by using the proximal point method on path measures, decomposing learning into simpler subproblems and employing proximal weighted denoising cross-entropy objectives for proximal steps.

Result: PDNS effectively promotes exploration across modes and demonstrates robustness through experiments spanning continuous and discrete sampling tasks, particularly in molecular dynamics and statistical physics scenarios.

Conclusion: The approach provides a staged solution for overcoming barriers and enhancing neural sampling efficiency for multimodal distributions.

Abstract: The task of learning a diffusion-based neural sampler for drawing samples
from an unnormalized target distribution can be viewed as a stochastic optimal
control problem on path measures. However, the training of neural samplers can
be challenging when the target distribution is multimodal with significant
barriers separating the modes, potentially leading to mode collapse. We propose
a framework named \textbf{Proximal Diffusion Neural Sampler (PDNS)} that
addresses these challenges by tackling the stochastic optimal control problem
via proximal point method on the space of path measures. PDNS decomposes the
learning process into a series of simpler subproblems that create a path
gradually approaching the desired distribution. This staged procedure traces a
progressively refined path to the desired distribution and promotes thorough
exploration across modes. For a practical and efficient realization, we
instantiate each proximal step with a proximal weighted denoising cross-entropy
(WDCE) objective. We demonstrate the effectiveness and robustness of PDNS
through extensive experiments on both continuous and discrete sampling tasks,
including challenging scenarios in molecular dynamics and statistical physics.

</details>


### [461] [Constant in an Ever-Changing World](https://arxiv.org/abs/2510.03330)
*Andy Wu,Chun-Cheng Lin,Yuehua Huang,Rung-Tzuo Liaw*

Main category: cs.LG

TL;DR: The paper proposes the CIC framework to stabilize reinforcement learning by selectively updating policies and using adaptive adjustments, achieving better results without extra computational cost.


<details>
  <summary>Details</summary>
Motivation: Reinforcement learning often suffers from severe oscillations, causing instability and reduced performance.

Method: The CIC framework uses a representative policy and a current policy. The representative policy is updated selectively based on the performance of the current policy, and both policies collaborate adaptively to train the critic.

Result: CIC was tested on five MuJoCo environments and demonstrated improved performance over traditional algorithms without increasing computational demands.

Conclusion: CIC enhances reinforcement learning stability and performance by addressing algorithmic instability in a computationally efficient manner.

Abstract: The training process of reinforcement learning often suffers from severe
oscillations, leading to instability and degraded performance. In this paper,
we propose a Constant in an Ever-Changing World (CIC) framework that enhances
algorithmic stability to improve performance. CIC maintains both a
representative policy and a current policy. Instead of updating the
representative policy blindly, CIC selectively updates it only when the current
policy demonstrates superiority. Furthermore, CIC employs an adaptive
adjustment mechanism, enabling the representative and current policies to
jointly facilitate critic training. We evaluate CIC on five MuJoCo
environments, and the results show that CIC improves the performance of
conventional algorithms without incurring additional computational cost.

</details>


### [462] [HOFLON: Hybrid Offline Learning and Online Optimization for Process Start-Up and Grade-Transition Control](https://arxiv.org/abs/2510.03830)
*Alex Durkin,Jasper Stolte,Mehmet Mercangöz*

Main category: cs.LG

TL;DR: This paper presents HOFLON, a hybrid offline-online learning framework designed to optimize transition operations in continuous-process plants, leveraging historical data to surpass human expertise.


<details>
  <summary>Details</summary>
Motivation: Current transition processes in plants are prone to errors and inefficiencies, with reliance on retiring experts leading to a loss of critical operational know-how.

Method: HOFLON employs offline learning to estimate feasible transition paths and cumulative rewards, paired with online optimization for real-time decision-making, combining reinforcement learning with penalty mechanisms.

Result: Empirical evaluations on industrial case studies show HOFLON outperforms top offline RL methods like IQL and even surpasses historical best human-executed transitions.

Conclusion: HOFLON offers a scalable approach to improve the reliability and efficiency of plant operations, potentially automating complex transitions beyond current human capability.

Abstract: Start-ups and product grade-changes are critical steps in continuous-process
plant operation, because any misstep immediately affects product quality and
drives operational losses. These transitions have long relied on manual
operation by a handful of expert operators, but the progressive retirement of
that workforce is leaving plant owners without the tacit know-how needed to
execute them consistently. In the absence of a process model, offline
reinforcement learning (RL) promises to capture and even surpass human
expertise by mining historical start-up and grade-change logs, yet standard
offline RL struggles with distribution shift and value-overestimation whenever
a learned policy ventures outside the data envelope. We introduce HOFLON
(Hybrid Offline Learning + Online Optimization) to overcome those limitations.
Offline, HOFLON learns (i) a latent data manifold that represents the feasible
region spanned by past transitions and (ii) a long-horizon Q-critic that
predicts the cumulative reward from state-action pairs. Online, it solves a
one-step optimization problem that maximizes the Q-critic while penalizing
deviations from the learned manifold and excessive rates of change in the
manipulated variables. We test HOFLON on two industrial case studies: a
polymerization reactor start-up and a paper-machine grade-change problem, and
benchmark it against Implicit Q-Learning (IQL), a leading offline-RL algorithm.
In both plants HOFLON not only surpasses IQL but also delivers, on average,
better cumulative rewards than the best start-up or grade-change observed in
the historical data, demonstrating its potential to automate transition
operations beyond current expert capability.

</details>


### [463] [Technical note on Fisher Information for Robust Federated Cross-Validation](https://arxiv.org/abs/2510.03838)
*Behraj Khan,Tahir Qasim Syed*

Main category: cs.LG

TL;DR: The paper introduces FIRE, a method that uses a Fisher information-based approach for addressing performance degradation in fragmented or federated learning data by penalizing fragmentation-induced covariate shifts, achieving superior validation performance.


<details>
  <summary>Details</summary>
Motivation: Performance degradation of trained models is common due to covariate shift when training data is fragmented across batches or geographical regions.

Method: The authors propose FIRE, leveraging an approximate Fisher information as a per-fragment loss penalty to address covariate distribution misalignment.

Result: FIRE showed substantial improvement, outperforming importance weighting and federated learning benchmarks by 5.1% and 5.3% respectively on shifted validation sets.

Conclusion: FIRE provides a computationally scalable method to align fragmented training distributions, effectively mitigating performance losses in federated and fragmented learning scenarios.

Abstract: When training data are fragmented across batches or federated-learned across
different geographic locations, trained models manifest performance
degradation. That degradation partly owes to covariate shift induced by data
having been fragmented across time and space and producing dissimilar empirical
training distributions. Each fragment's distribution is slightly different to a
hypothetical unfragmented training distribution of covariates, and to the
single validation distribution. To address this problem, we propose Fisher
Information for Robust fEderated validation (\textbf{FIRE}). This method
accumulates fragmentation-induced covariate shift divergences from the global
training distribution via an approximate Fisher information. That term, which
we prove to be a more computationally-tractable estimate, is then used as a
per-fragment loss penalty, enabling scalable distribution alignment. FIRE
outperforms importance weighting benchmarks by $5.1\%$ at maximum and federated
learning (FL) benchmarks by up to $5.3\%$ on shifted validation sets.

</details>


### [464] [Matching the Optimal Denoiser in Point Cloud Diffusion with (Improved) Rotational Alignment](https://arxiv.org/abs/2510.03335)
*Ameya Daigavane,YuQing Xie,Bodhi P. Vani,Saeed Saremi,Joseph Kleinhenz,Tess Smidt*

Main category: cs.LG

TL;DR: The paper investigates the alignment step in training diffusion models for point clouds, where rotational symmetry is a challenge. It introduces a theoretical framework based on the matrix Fisher distribution and evaluates alignment’s effectiveness under small noise levels.


<details>
  <summary>Details</summary>
Motivation: The authors aim to understand the role and effectiveness of the alignment step in training diffusion models for point clouds, specifically in scenarios with rotational symmetry such as molecules and proteins.

Method: The paper models the optimal denoiser using the matrix Fisher distribution over SO(3), providing a theoretical explanation of the alignment's role. It also derives approximators to the optimal denoiser for small noise scenarios.

Result: The alignment step, based on sampling the mode of the matrix Fisher distribution, is shown to be an effective zeroth-order approximation for small noise levels in training diffusion models.

Conclusion: Alignment proves to be a sufficiently effective technique for handling rotational symmetry in diffusion models for point clouds, making it a practical choice for small noise levels during training.

Abstract: Diffusion models are a popular class of generative models trained to reverse
a noising process starting from a target data distribution. Training a
diffusion model consists of learning how to denoise noisy samples at different
noise levels. When training diffusion models for point clouds such as molecules
and proteins, there is often no canonical orientation that can be assigned. To
capture this symmetry, the true data samples are often augmented by
transforming them with random rotations sampled uniformly over $SO(3)$. Then,
the denoised predictions are often rotationally aligned via the Kabsch-Umeyama
algorithm to the ground truth samples before computing the loss. However, the
effect of this alignment step has not been well studied. Here, we show that the
optimal denoiser can be expressed in terms of a matrix Fisher distribution over
$SO(3)$. Alignment corresponds to sampling the mode of this distribution, and
turns out to be the zeroth order approximation for small noise levels,
explaining its effectiveness. We build on this perspective to derive better
approximators to the optimal denoiser in the limit of small noise. Our
experiments highlight that alignment is often a `good enough' approximation for
the noise levels that matter most for training diffusion models.

</details>


### [465] [Technical note on Sequential Test-Time Adaptation via Martingale-Driven Fisher Prompting](https://arxiv.org/abs/2510.03839)
*Behraj Khan,Tahir Qasim Syed*

Main category: cs.LG

TL;DR: M-FISHER is introduced as a method for detecting and adapting to distribution shifts in streaming data with statistical validity and stability.


<details>
  <summary>Details</summary>
Motivation: To address the issue of detecting distribution shifts and achieving stable adaptation in streaming data for sequential decision-making.

Method: M-FISHER employs exponential martingales for shift detection (with controls on false alarms and detection delays) and Fisher-preconditioned updates to adapt using natural gradient descent.

Result: M-FISHER offers time-uniform guarantees for shift detection, efficient detection delays, and geometrically stable parameter updates.

Conclusion: The paper establishes M-FISHER as an effective method for detecting and adapting to covariate shifts with robustness and invariance in sequential decision tasks.

Abstract: We present a theoretical framework for M-FISHER, a method for sequential
distribution shift detection and stable adaptation in streaming data. For
detection, we construct an exponential martingale from non-conformity scores
and apply Ville's inequality to obtain time-uniform guarantees on false alarm
control, ensuring statistical validity at any stopping time. Under sustained
shifts, we further bound the expected detection delay as
$\mathcal{O}(\log(1/\delta)/\Gamma)$, where $\Gamma$ reflects the post-shift
information gain, thereby linking detection efficiency to distributional
divergence. For adaptation, we show that Fisher-preconditioned updates of
prompt parameters implement natural gradient descent on the distributional
manifold, yielding locally optimal updates that minimize KL divergence while
preserving stability and parameterization invariance. Together, these results
establish M-FISHER as a principled approach for robust, anytime-valid detection
and geometrically stable adaptation in sequential decision-making under
covariate shift.

</details>


### [466] [Pool Me Wisely: On the Effect of Pooling in Transformer-Based Models](https://arxiv.org/abs/2510.03339)
*Sofiane Ennadir,Levente Zólyomi,Oleg Smirnov,Tianze Wang,John Pertoft,Filip Cornell,Lele Cao*

Main category: cs.LG

TL;DR: The paper investigates the neglected role of pooling in Transformer models, offering theoretical bounds and empirical insights into its impact on model behavior across diverse tasks.


<details>
  <summary>Details</summary>
Motivation: While attention mechanisms in Transformers are extensively studied, pooling, a crucial part of the architecture, remains underexplored, despite its potential influence on model performance.

Method: Researchers developed a theoretical framework with closed-form bounds to characterize Transformer expressivity with common pooling methods, extending analysis across various attention types and validating via empirical evaluations across three modalities.

Result: Empirical data shows pooling choices significantly influence accuracy, sensitivity, and optimization in tasks requiring global/local contextual understanding across computer vision, NLP, and time-series analysis.

Conclusion: Pooling is vital for the architectural design of Transformer models, guiding practical selection or development of effective pooling strategies for specified tasks.

Abstract: Transformer models have become the dominant backbone for sequence modeling,
leveraging self-attention to produce contextualized token representations.
These are typically aggregated into fixed-size vectors via pooling operations
for downstream tasks. While much of the literature has focused on attention
mechanisms, the role of pooling remains underexplored despite its critical
impact on model behavior. In this paper, we introduce a theoretical framework
that rigorously characterizes the expressivity of Transformer-based models
equipped with widely used pooling methods by deriving closed-form bounds on
their representational capacity and the ability to distinguish similar inputs.
Our analysis extends to different variations of attention formulations,
demonstrating that these bounds hold across diverse architectural variants. We
empirically evaluate pooling strategies across tasks requiring both global and
local contextual understanding, spanning three major modalities: computer
vision, natural language processing, and time-series analysis. Results reveal
consistent trends in how pooling choices affect accuracy, sensitivity, and
optimization behavior. Our findings unify theoretical and empirical
perspectives, providing practical guidance for selecting or designing pooling
mechanisms suited to specific tasks. This work positions pooling as a key
architectural component in Transformer models and lays the foundation for more
principled model design beyond attention alone.

</details>


### [467] [Learning Pareto-Optimal Pandemic Intervention Policies with MORL](https://arxiv.org/abs/2510.03340)
*Marian Chen,Miri Zilka*

Main category: cs.LG

TL;DR: This paper develops a versatile framework using multi-objective reinforcement learning (MORL) and a novel pandemic simulator to balance disease containment and economic stability during public health crises.


<details>
  <summary>Details</summary>
Motivation: The COVID-19 pandemic revealed the necessity of tools that can balance health-driven disease containment strategies with economic stability to enable better policymaking.

Method: The researchers integrated MORL with a new stochastic differential equation pandemic simulator, which was calibrated using global data, to model and evaluate intervention strategies. They trained a Pareto-Conditioned Network (PCN) agent to discover trade-offs between epidemiological control and economic impacts.

Result: The framework successfully illustrated trade-offs between pandemic control and economic stability for COVID-19. It demonstrated general applicability to other diseases like polio, influenza, and measles, showing how agents adapt intervention policies for pathogens with diverse epidemiological profiles.

Conclusion: The paper offers a flexible, validated methodology for informing policymakers on effective strategies to mitigate public health crises while considering both health and economic stability.

Abstract: The COVID-19 pandemic underscored a critical need for intervention strategies
that balance disease containment with socioeconomic stability. We approach this
challenge by designing a framework for modeling and evaluating disease-spread
prevention strategies. Our framework leverages multi-objective reinforcement
learning (MORL) - a formulation necessitated by competing objectives - combined
with a new stochastic differential equation (SDE) pandemic simulator,
calibrated and validated against global COVID-19 data. Our simulator reproduces
national-scale pandemic dynamics with orders of magnitude higher fidelity than
other models commonly used in reinforcement learning (RL) approaches to
pandemic intervention. Training a Pareto-Conditioned Network (PCN) agent on
this simulator, we illustrate the direct policy trade-offs between
epidemiological control and economic stability for COVID-19. Furthermore, we
demonstrate the framework's generality by extending it to pathogens with
different epidemiological profiles, such as polio and influenza, and show how
these profiles lead the agent to discover fundamentally different intervention
policies. To ground our work in contemporary policymaking challenges, we apply
the model to measles outbreaks, quantifying how a modest 5% drop in vaccination
coverage necessitates significantly more stringent and costly interventions to
curb disease spread. This work provides a robust and adaptable framework to
support transparent, evidence-based policymaking for mitigating public health
crises.

</details>


### [468] [On Provable Benefits of Muon in Federated Learning](https://arxiv.org/abs/2510.03866)
*Xinwen Zhang,Hongchang Gao*

Main category: cs.LG

TL;DR: This paper introduces FedMuon, an adaptation of the Muon optimizer for federated learning, demonstrating its effectiveness through theoretical analysis and experiments.


<details>
  <summary>Details</summary>
Motivation: Muon optimizer has shown notable performance, but its application to federated learning remains unexplored.

Method: They propose FedMuon, a federated learning algorithm, with orthonormalized update direction and analyze its convergence rate for nonconvex problems.

Result: FedMuon exhibits independence from problem-specific parameters and accommodates heavy-tailed noise, validated through experiments on various neural network architectures.

Conclusion: The study demonstrates FedMuon's theoretical and practical advantages, making it a viable optimizer for federated learning scenarios.

Abstract: The recently introduced optimizer, Muon, has gained increasing attention due
to its superior performance across a wide range of applications. However, its
effectiveness in federated learning remains unexplored. To address this gap,
this paper investigates the performance of Muon in the federated learning
setting. Specifically, we propose a new algorithm, FedMuon, and establish its
convergence rate for nonconvex problems. Our theoretical analysis reveals
multiple favorable properties of FedMuon. In particular, due to its
orthonormalized update direction, the learning rate of FedMuon is independent
of problem-specific parameters, and, importantly, it can naturally accommodate
heavy-tailed noise. The extensive experiments on a variety of neural network
architectures validate the effectiveness of the proposed algorithm.

</details>


### [469] [Pilot selection in the era of Virtual reality: algorithms for accurate and interpretable machine learning models](https://arxiv.org/abs/2510.03345)
*Luoma Ke,Guangpeng Zhang,Jibo He,Yajing Li,Yan Li,Xufeng Liu,Peng Fang*

Main category: cs.LG

TL;DR: The study uses machine learning and virtual reality to predict pilot skills, achieving high accuracy using SVM with MIC feature selection.


<details>
  <summary>Details</summary>
Motivation: To address the growing demand for efficient and accurate pilot selection in the aviation industry.

Method: Pilot skill features were analyzed using eye tracking and flight dynamics data, evaluated through machine learning (SVM with MIC feature selection) on a VR simulation platform.

Result: SVM + MIC achieved the highest prediction metrics: 93% accuracy, 96% AUC, and 93% F1-score, surpassing other algorithms and feature selection methods.

Conclusion: The study's approach offers an innovative and effective solution for pilot selection and training using VR and machine learning technologies.

Abstract: With the rapid growth of the aviation industry, there is a need for a large
number of flight crew. How to select the right pilots in a cost-efficient
manner has become an important research question. In the current study,
twenty-three pilots were recruited from China Eastern Airlines, and 23 novices
were from the community of Tsinghua University. A novel approach incorporating
machine learning and virtual reality technology was applied to distinguish
features between these participants with different flight skills. Results
indicate that SVM with the MIC feature selection method consistently achieved
the highest prediction performance on all metrics with an Accuracy of 0.93, an
AUC of 0.96, and an F1 of 0.93, which outperforms four other classifier
algorithms and two other feature selection methods. From the perspective of
feature selection methods, the MIC method can select features with a nonlinear
relationship to sampling labels, instead of a simple filter-out. Our new
implementation of the SVM + MIC algorithm outperforms all existing pilot
selection algorithms and perhaps provides the first implementation based on eye
tracking and flight dynamics data. This study's VR simulation platforms and
algorithms can be used for pilot selection and training.

</details>


### [470] [Optimal Scaling Needs Optimal Norm](https://arxiv.org/abs/2510.03871)
*Oleg Filatov,Jiangtao Wang,Jan Ebert,Stefan Kesselheim*

Main category: cs.LG

TL;DR: This paper introduces the concept of "norm transfer" as a unifying principle for optimal scaling in model and dataset sizes during training, showing the operator norm of the output layer remains constant at optimal parameter settings.


<details>
  <summary>Details</summary>
Motivation: To address the absence of a unifying explanatory principle for hyperparameter transfer under model and dataset scaling, and improve understanding of optimal training dynamics in large language models.

Method: The study uses the Scion optimizer and analyzes training dynamics across models up to 1.3B parameters on datasets up to 138B tokens. Key observations include the relationship between the operator norm of the output layer and learning rate/batch size pairs. Experiments incorporate varied per-layer-group learning rates.

Result: Norm transfer was identified as invariant across optimal hyperparameter settings for joint scaling. They found scaling consistency between Scion and Adam optimizers and demonstrated enhanced performance by tuning layer-group learning rates.

Conclusion: Norm transfer offers a practical principle for guiding hyperparameter scaling in large language model training. Implementation details and logs are shared to support further research, emphasizing the need for precise learning rate adjustments per layer group, especially in output layers.

Abstract: Despite recent progress in optimal hyperparameter transfer under model and
dataset scaling, no unifying explanatory principle has been established. Using
the Scion optimizer, we discover that joint optimal scaling across model and
dataset sizes is governed by a single invariant: the operator norm of the
output layer. Across models with up to 1.3B parameters trained on up to 138B
tokens, the optimal learning rate/batch size pair $(\eta^{\ast}, B^{\ast})$
consistently has the same operator norm value - a phenomenon we term norm
transfer. This constant norm condition is necessary but not sufficient: while
for each dataset size, multiple $(\eta, B)$ reach the optimal norm, only a
unique $(\eta^{\ast}, B^{\ast})$ achieves the best loss. As a sufficient
condition, we provide the first measurement of $(\eta^{\ast}, B^{\ast})$
scaling with dataset size for Scion, and find that the scaling rules are
consistent with those of the Adam optimizer. Tuning per-layer-group learning
rates also improves model performance, with the output layer being the most
sensitive and hidden layers benefiting from lower learning rates. We provide
practical insights on norm-guided optimal scaling and release our Distributed
Scion (Disco) implementation with logs from over two thousand runs to support
research on LLM training dynamics at scale.

</details>


### [471] [KVComm: Enabling Efficient LLM Communication through Selective KV Sharing](https://arxiv.org/abs/2510.03346)
*Xiangyu Shi,Marco Chiesa,Gerald Q. Maguire Jr.,Dejan Kostic*

Main category: cs.LG

TL;DR: This paper introduces KVComm, a communication framework enabling efficient information sharing between large language models (LLMs) by leveraging selective transmission of key-value (KV) pairs.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address inefficiencies in existing inter-LLM communication methods, such as high inference costs with natural language and inefficiencies of hidden state protocols.

Method: It proposes KVComm, which uses selective sharing of KV pairs. A strategy based on attention importance scores and a Gaussian prior identifies the most informative KV pairs for communication.

Result: KVComm achieves performance comparable to the optimal method of direct input merging while reducing KV pair transmission to just 30% of layers in tests across various tasks and models.

Conclusion: KV pairs offer a scalable and efficient medium for inter-LLM communication, making systems both effective and resource-efficient.

Abstract: Large Language Models (LLMs) are increasingly deployed in multi-agent
systems, where effective inter-model communication is crucial. Existing
communication protocols either rely on natural language, incurring high
inference costs and information loss, or on hidden states, which suffer from
information concentration bias and inefficiency. To address these limitations,
we propose KVComm, a novel communication framework that enables efficient
communication between LLMs through selective sharing of KV pairs. KVComm
leverages the rich information encoded in the KV pairs while avoiding the
pitfalls of hidden states. We introduce a KV layer-wise selection strategy
based on attention importance scores with a Gaussian prior to identify the most
informative KV pairs for communication. Extensive experiments across diverse
tasks and model pairs demonstrate that KVComm achieves comparable performance
to the upper-bound method, which directly merges inputs to one model without
any communication, while transmitting as few as 30\% of layers' KV pairs. Our
study highlights the potential of KV pairs as an effective medium for inter-LLM
communication, paving the way for scalable and efficient multi-agent systems.

</details>


### [472] [AgentCaster: Reasoning-Guided Tornado Forecasting](https://arxiv.org/abs/2510.03349)
*Michael Chen*

Main category: cs.LG

TL;DR: This paper introduces AgentCaster, a framework using multimodal LLMs to tackle the complex task of tornado forecasting for assessing LLM readiness as reasoning agents.


<details>
  <summary>Details</summary>
Motivation: There is a growing need to evaluate LLMs on complex, real-world tasks requiring advanced reasoning abilities, such as weather forecasting.

Method: AgentCaster uses multimodal LLMs to interpret spatiotemporal data, interact with forecast maps, and generate probabilistic tornado-risk predictions based on domain-specific metrics.

Result: Human experts outperformed LLMs, which struggled with spatiotemporal reasoning, hallucinated risk intensity, and made errors in geographic placement during tornado forecasting.

Conclusion: AgentCaster highlights the deficiencies of current LLMs in complex reasoning tasks and offers a pathway for their improvement in critical domains like weather forecasting.

Abstract: There is a growing need to evaluate Large Language Models (LLMs) on complex,
high-impact, real-world tasks to assess their true readiness as reasoning
agents. To address this gap, we introduce AgentCaster, a contamination-free
framework employing multimodal LLMs end-to-end for the challenging,
long-horizon task of tornado forecasting. Within AgentCaster, models interpret
heterogeneous spatiotemporal data from a high-resolution convection-allowing
forecast archive. We assess model performance over a 40-day period featuring
diverse historical data, spanning several major tornado outbreaks and including
over 500 tornado reports. Each day, models query interactively from a pool of
3,625 forecast maps and 40,125 forecast soundings for a forecast horizon of
12-36 hours. Probabilistic tornado-risk polygon predictions are verified
against ground truths derived from geometric comparisons across disjoint risk
bands in projected coordinate space. To quantify accuracy, we propose
domain-specific TornadoBench and TornadoHallucination metrics, with
TornadoBench highly challenging for both LLMs and domain expert human
forecasters. Notably, human experts significantly outperform state-of-the-art
models, which demonstrate a strong tendency to hallucinate and overpredict risk
intensity, struggle with precise geographic placement, and exhibit poor
spatiotemporal reasoning in complex, dynamically evolving systems. AgentCaster
aims to advance research on improving LLM agents for challenging reasoning
tasks in critical domains.

</details>


### [473] [Slow-Fast Policy Optimization: Reposition-Before-Update for LLM Reasoning](https://arxiv.org/abs/2510.04072)
*Ziyan Wang,Zheng Wang,Jie Fu,Xingwei Qu,Qi Cheng,Shengpu Tang,Minjia Zhang,Xiaoming Huo*

Main category: cs.LG

TL;DR: The paper introduces Slow-Fast Policy Optimization (SFPO), a framework aimed at improving the efficiency and stability of reinforcement learning (RL) in large language models.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the inefficiencies in on-policy RL algorithms, such as GRPO, which suffer from noisy gradients leading to instability and exploration issues during early training.

Method: SFPO achieves its goals through a three-stage process: (1) short, fast inner steps on the same batch, (2) a reposition mechanism to mitigate off-policy drift, and (3) a final slow correction step. This design ensures compatibility with existing policy-gradient methods.

Result: SFPO delivers better stability, fewer rollouts, and faster convergence in RL training for reasoning tasks. It outperforms GRPO in math reasoning benchmarks by up to 2.80 points, reduces rollouts by up to 4.93x, and cuts wall-clock time by 4.19x.

Conclusion: SFPO is a robust, efficient enhancement for RL in LLMs, addressing key inefficiencies in current methods while being easily implementable in existing systems.

Abstract: Reinforcement learning (RL) has become central to enhancing reasoning in
large language models (LLMs). Yet on-policy algorithms such as Group Relative
Policy Optimization (GRPO) often suffer in early training: noisy gradients from
low-quality rollouts lead to unstable updates and inefficient exploration. We
introduce Slow-Fast Policy Optimization (SFPO), a simple yet efficient
framework to address these limitations via decomposing each step into three
stages: a short fast trajectory of inner steps on the same batch, a reposition
mechanism to control off-policy drift, and a final slow correction. This
reposition-before-update design preserves the objective and rollout process
unchanged, making SFPO plug-compatible with existing policy-gradient pipelines.
Extensive experiments demonstrate that SFPO consistently improves stability,
reduces rollouts, and accelerates convergence of reasoning RL training.
Specifically, it outperforms GRPO by up to 2.80 points in average on math
reasoning benchmarks. It also achieves up to 4.93\texttimes{} fewer rollouts
and a 4.19\texttimes{} reduction in wall-clock time to match GRPO's best
accuracy.

</details>


### [474] [Interpretable Neuropsychiatric Diagnosis via Concept-Guided Graph Neural Networks](https://arxiv.org/abs/2510.03351)
*Song Wang,Zhenyu Lei,Zhen Tan,Jundong Li,Javier Rasero,Aiying Zhang,Chirag Agarwal*

Main category: cs.LG

TL;DR: The paper introduces CONCEPTNEURO, a novel framework that integrates large language models and neurobiological knowledge to diagnose psychiatric disorders using interpretable functional connectivity patterns from rs-fMRI data.


<details>
  <summary>Details</summary>
Motivation: A significant number of adolescents suffer from diagnosed mental health conditions, urging the need for accurate and interpretable diagnostic tools. Traditional GNNs used for disorder prediction lack interpretability, hindering clinical reliability.

Method: CONCEPTNEURO blends large language models and neurobiological domain expertise to create structured subgraph-based concepts representing functional connectivity patterns, using them for predictive modeling.

Result: CONCEPTNEURO-enhanced GNNs surpass standard GNN models in accuracy and offer transparent predictions through clinically meaningful explanations. The framework highlights disorder-specific connectivity patterns validated by domain experts.

Conclusion: CONCEPTNEURO serves as an innovative framework for psychiatric disorder diagnosis, enabling both enhanced prediction performance and interpretability, while also contributing new insights for research.

Abstract: Nearly one in five adolescents currently live with a diagnosed mental or
behavioral health condition, such as anxiety, depression, or conduct disorder,
underscoring the urgency of developing accurate and interpretable diagnostic
tools. Resting-state functional magnetic resonance imaging (rs-fMRI) provides a
powerful lens into large-scale functional connectivity, where brain regions are
modeled as nodes and inter-regional synchrony as edges, offering clinically
relevant biomarkers for psychiatric disorders. While prior works use graph
neural network (GNN) approaches for disorder prediction, they remain complex
black-boxes, limiting their reliability and clinical translation. In this work,
we propose CONCEPTNEURO, a concept-based diagnosis framework that leverages
large language models (LLMs) and neurobiological domain knowledge to
automatically generate, filter, and encode interpretable functional
connectivity concepts. Each concept is represented as a structured subgraph
linking specific brain regions, which are then passed through a concept
classifier. Our design ensures predictions through clinically meaningful
connectivity patterns, enabling both interpretability and strong predictive
performance. Extensive experiments across multiple psychiatric disorder
datasets demonstrate that CONCEPTNEURO-augmented GNNs consistently outperform
their vanilla counterparts, improving accuracy while providing transparent,
clinically aligned explanations. Furthermore, concept analyses highlight
disorder-specific connectivity patterns that align with expert knowledge and
suggest new hypotheses for future investigation, establishing CONCEPTNEURO as
an interpretable, domain-informed framework for psychiatric disorder diagnosis.

</details>


### [475] [Deep Reinforcement Learning for Multi-Agent Coordination](https://arxiv.org/abs/2510.03592)
*Kehinde O. Aina,Sehoon Ha*

Main category: cs.LG

TL;DR: The paper introduces S-MADRL, a decentralized coordination framework for multi-robot systems using virtual pheromones inspired by insect stigmergy. This method addresses challenges of congestion and interference in narrow environments.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance the performance of multi-robot systems in tightly constrained environments, overcoming issues of congestion and inefficiency.

Method: The authors propose a Stigmergic Multi-Agent Deep Reinforcement Learning (S-MADRL) framework, incorporating virtual pheromones and curriculum learning to achieve decentralized coordination among robots.

Result: Simulation results reveal the framework's ability to effectively coordinate up to eight robots, enabling self-organization into asymmetric workload distributions that minimize congestion.

Conclusion: The study provides a scalable, decentralized solution for multi-agent coordination in crowded environments, drawing upon strategies inspired by nature and demonstrating effective emergent behaviors without explicit communication mechanisms.

Abstract: We address the challenge of coordinating multiple robots in narrow and
confined environments, where congestion and interference often hinder
collective task performance. Drawing inspiration from insect colonies, which
achieve robust coordination through stigmergy -- modifying and interpreting
environmental traces -- we propose a Stigmergic Multi-Agent Deep Reinforcement
Learning (S-MADRL) framework that leverages virtual pheromones to model local
and social interactions, enabling decentralized emergent coordination without
explicit communication. To overcome the convergence and scalability limitations
of existing algorithms such as MADQN, MADDPG, and MAPPO, we leverage curriculum
learning, which decomposes complex tasks into progressively harder
sub-problems. Simulation results show that our framework achieves the most
effective coordination of up to eight agents, where robots self-organize into
asymmetric workload distributions that reduce congestion and modulate group
performance. This emergent behavior, analogous to strategies observed in
nature, demonstrates a scalable solution for decentralized multi-agent
coordination in crowded environments with communication constraints.

</details>


### [476] [Offline Reinforcement Learning in Large State Spaces: Algorithms and Guarantees](https://arxiv.org/abs/2510.04088)
*Nan Jiang,Tengyang Xie*

Main category: cs.LG

TL;DR: This article explores offline reinforcement learning (RL) in large state spaces, using historical data to learn policies without online interactions.


<details>
  <summary>Details</summary>
Motivation: The motivation is to advance understanding and methodologies for offline RL, emphasizing conditions like function approximation and data coverage.

Method: The paper outlines various algorithms and theoretical concepts, focusing on matters like Bellman completeness, realizability, and data coverage.

Result: It offers a classification of methods based on assumptions and their guarantees in terms of sample and computational complexity.

Conclusion: The discussion provides insights, raises open questions, and highlights connections to related research areas.

Abstract: This article introduces the theory of offline reinforcement learning in large
state spaces, where good policies are learned from historical data without
online interactions with the environment. Key concepts introduced include
expressivity assumptions on function approximation (e.g., Bellman completeness
vs. realizability) and data coverage (e.g., all-policy vs. single-policy
coverage). A rich landscape of algorithms and results is described, depending
on the assumptions one is willing to make and the sample and computational
complexity guarantees one wishes to achieve. We also discuss open questions and
connections to adjacent areas.

</details>


### [477] [High Cycle S-N curve prediction for Al 7075-T6 alloy using Recurrent Neural Networks (RNNs)](https://arxiv.org/abs/2510.03355)
*Aryan Patel*

Main category: cs.LG

TL;DR: This paper proposes a transfer learning framework using LSTMs to predict high cycle fatigue torsional S-N curves for Aluminum 7075-T6, reducing time and cost of fatigue characterization.


<details>
  <summary>Details</summary>
Motivation: Characterizing material fatigue performance, especially for high cycle ranges, is expensive and time-consuming, prompting the need for efficient methodologies.

Method: The study developed an LSTM-based transfer learning model, where a source model trained on axial fatigue data of Aluminum 7075-T6 was transferred to predict torsional fatigue S-N curves for high cycles.

Result: The LSTM framework successfully and accurately predicted aluminum torsional S-N curves over a high cycle range.

Conclusion: The proposed framework has the potential to significantly reduce costs and time associated with fatigue testing in various materials, helping optimize resource allocation for testing.

Abstract: Aluminum is a widely used alloy, which is susceptible to fatigue failure.
Characterizing fatigue performance for materials is extremely time and cost
demanding, especially for high cycle data. To help mitigate this, a transfer
learning based framework has been developed using Long short-term memory
networks (LSTMs) in which a source LSTM model is trained based on pure axial
fatigue data for Aluminum 7075-T6 alloy which is then transferred to predict
high cycle torsional S-N curves. The framework was able to accurately predict
Al torsional S-N curves for a much higher cycle range. It is the belief that
this framework will help to drastically mitigate the cost of gathering fatigue
characteristics for different materials and help prioritize tests with better
cost and time constraints.

</details>


### [478] [Distributed Area Coverage with High Altitude Balloons Using Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2510.03823)
*Adam Haroon,Tristan Schuler*

Main category: cs.LG

TL;DR: The paper presents the first application of multi-agent reinforcement learning (MARL) for High Altitude Balloons (HAB) coordination to achieve distributed area coverage.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the limitations of deterministic methods, such as Voronoi partitioning, for small teams and localized missions in HAB coordination, and explore MARL as an alternative solution.

Method: The researchers extend an existing reinforcement learning simulation environment, adapt QMIX (a MARL algorithm) with Centralized Training and Decentralized Execution, and design hierarchical rewards with specific observation spaces.

Result: The adapted QMIX method matched the performance of a theoretically optimal deterministic method for distributed area coverage, validating the MARL approach.

Conclusion: The study demonstrates the feasibility of MARL for HAB coordination, providing a foundation for advanced autonomous multi-HAB missions that deterministic methods cannot handle effectively.

Abstract: High Altitude Balloons (HABs) can leverage stratospheric wind layers for
limited horizontal control, enabling applications in reconnaissance,
environmental monitoring, and communications networks. Existing multi-agent HAB
coordination approaches use deterministic methods like Voronoi partitioning and
extremum seeking control for large global constellations, which perform poorly
for smaller teams and localized missions. While single-agent HAB control using
reinforcement learning has been demonstrated on HABs, coordinated multi-agent
reinforcement learning (MARL) has not yet been investigated. This work presents
the first systematic application of multi-agent reinforcement learning (MARL)
to HAB coordination for distributed area coverage. We extend our previously
developed reinforcement learning simulation environment (RLHAB) to support
cooperative multi-agent learning, enabling multiple agents to operate
simultaneously in realistic atmospheric conditions. We adapt QMIX for HAB area
coverage coordination, leveraging Centralized Training with Decentralized
Execution to address atmospheric vehicle coordination challenges. Our approach
employs specialized observation spaces providing individual state,
environmental context, and teammate data, with hierarchical rewards
prioritizing coverage while encouraging spatial distribution. We demonstrate
that QMIX achieves similar performance to the theoretically optimal geometric
deterministic method for distributed area coverage, validating the MARL
approach and providing a foundation for more complex autonomous multi-HAB
missions where deterministic methods become intractable.

</details>


### [479] [Understanding Transformers for Time Series: Rank Structure, Flow-of-ranks, and Compressibility](https://arxiv.org/abs/2510.03358)
*Annan Yu,Danielle C. Maddix,Boran Han,Xiyuan Zhang,Abdul Fatir Ansari,Oleksandr Shchur,Christos Faloutsos,Andrew Gordon Wilson,Michael W. Mahoney,Yuyang Wang*

Main category: cs.LG

TL;DR: The paper analyzes Transformers in the context of time series data, revealing their low-rank and compressible nature, and utilizes these insights to improve model efficiency.


<details>
  <summary>Details</summary>
Motivation: To understand and optimize how principles from Transformers can better transfer across different data modalities, especially for time series data, which have distinct structural properties.

Method: Explores the rank structure of time series embeddings, proving the compressibility of $Q/K/V$ projections and attention layers. Introduces the concept of 'flow-of-ranks' to study rank growth in deeper layers.

Result: Chronos, a large time series foundation model, was compressed to achieve 65% reduction in inference time and 81% in memory usage without compromising accuracy.

Conclusion: Time series models can be made significantly more efficient by leveraging their inherent low-rank nature and understanding the rank dynamics, which has practical implications for designing more efficient Transformers.

Abstract: Transformers are widely used across data modalities, and yet the principles
distilled from text models often transfer imperfectly to models trained to
other modalities. In this paper, we analyze Transformers through the lens of
rank structure. Our focus is on the time series setting, where the structural
properties of the data differ remarkably from those of text or vision. We show
that time-series embeddings, unlike text or vision, exhibit sharply decaying
singular value spectra: small patch sizes and smooth continuous mappings
concentrate the data into low-rank subspaces. From this, we prove that the
associated $Q/K/V$ projections admit accurate low-rank approximations, and that
attention layers become compressible in proportion to the decay of the
embedding spectrum. We introduce the concept of flow-of-ranks, a phenomenon by
which nonlinear mixing across depth inflates the rank, explaining why early
layers are most amenable to compression and why ranks grow with depth. Guided
by these theoretical and empirical results, we use these insights to compress
Chronos, a large time series foundation model, achieving a reduction of $65\%$
in inference time and $81\%$ in memory, without loss of accuracy. Our findings
provide principled guidance for allocating width, depth, and heads in time
series foundation models, and for exploiting their inherent compressibility.

</details>


### [480] [Physics-informed Neural-operator Predictive Control for Drag Reduction in Turbulent Flows](https://arxiv.org/abs/2510.03360)
*Zelin Zhao,Zongyi Li,Kimia Hassibi,Kamyar Azizzadenesheli,Junchi Yan,H. Jane Bae,Di Zhou,Anima Anandkumar*

Main category: cs.LG

TL;DR: The paper introduces a physics-informed deep reinforcement learning framework for efficient numerical modeling and control of turbulent flows, achieving significant drag reduction.


<details>
  <summary>Details</summary>
Motivation: Simulating and controlling turbulence in fluid dynamics is computationally expensive, especially at high Reynolds numbers.

Method: The authors propose a predictive control system using model-based reinforcement learning with jointly trained policies and observer models via Physics Informed Neural Operators (PINO).

Result: The proposed framework achieves a drag reduction of 39% in simulations involving high Reynolds numbers, surpassing previous methods by over 32%.

Conclusion: The physics-informed RL approach efficiently models and controls turbulent flows, demonstrating superior performance in challenging high-Reynolds-number scenarios compared to previous methods.

Abstract: Assessing turbulence control effects for wall friction numerically is a
significant challenge since it requires expensive simulations of turbulent
fluid dynamics. We instead propose an efficient deep reinforcement learning
(RL) framework for modeling and control of turbulent flows. It is model-based
RL for predictive control (PC), where both the policy and the observer models
for turbulence control are learned jointly using Physics Informed Neural
Operators (PINO), which are discretization invariant and can capture fine
scales in turbulent flows accurately. Our PINO-PC outperforms prior model-free
reinforcement learning methods in various challenging scenarios where the flows
are of high Reynolds numbers and unseen, i.e., not provided during model
training. We find that PINO-PC achieves a drag reduction of 39.0\% under a
bulk-velocity Reynolds number of 15,000, outperforming previous fluid control
methods by more than 32\%.

</details>


### [481] [Arithmetic-Mean $μ$P for Modern Architectures: A Unified Learning-Rate Scale for CNNs and ResNets](https://arxiv.org/abs/2510.04327)
*Haosong Zhang,Shenxi Wu,Yichi Zhang,Wei Lin*

Main category: cs.LG

TL;DR: This paper introduces Arithmetic-Mean maximal update parameterization (AM-μP) for adjusting learning rates in deep networks, providing consistent scalability across diverse architectures and depths with improved theoretical and empirical results.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address challenges in setting appropriate learning rates for scaling modern deep networks, especially in heterogeneous architectures where existing methods like $\mu$P are inadequate.

Method: The paper proposes AM-μP, which constrains the average network-wide pre-activation moment rather than individual layers. It combines this with a residual-aware initialization for consistent scaling across depths and architectures.

Result: The authors prove a $L^{-3/2}$ learning rate scaling law in convolutional and residual networks, supported by empirical validation showing consistent zero-shot learning-rate transfer across various depths.

Conclusion: AM-μP offers a unified learning rate principle for deep networks, enabling robust scalability and reducing the need for manual tuning, addressing major hurdles in network design.

Abstract: Choosing an appropriate learning rate remains a key challenge in scaling
depth of modern deep networks. The classical maximal update parameterization
($\mu$P) enforces a fixed per-layer update magnitude, which is well suited to
homogeneous multilayer perceptrons (MLPs) but becomes ill-posed in
heterogeneous architectures where residual accumulation and convolutions
introduce imbalance across layers. We introduce Arithmetic-Mean $\mu$P
(AM-$\mu$P), which constrains not each individual layer but the network-wide
average one-step pre-activation second moment to a constant scale. Combined
with a residual-aware He fan-in initialization - scaling residual-branch
weights by the number of blocks ($\mathrm{Var}[W]=c/(K\cdot
\mathrm{fan\text{-}in})$) - AM-$\mu$P yields width-robust depth laws that
transfer consistently across depths. We prove that, for one- and
two-dimensional convolutional networks, the maximal-update learning rate
satisfies $\eta^\star(L)\propto L^{-3/2}$; with zero padding, boundary effects
are constant-level as $N\gg k$. For standard residual networks with general
conv+MLP blocks, we establish $\eta^\star(L)=\Theta(L^{-3/2})$, with $L$ the
minimal depth. Empirical results across a range of depths confirm the $-3/2$
scaling law and enable zero-shot learning-rate transfer, providing a unified
and practical LR principle for convolutional and deep residual networks without
additional tuning overhead.

</details>


### [482] [Diffusion-Based, Data-Assimilation-Enabled Super-Resolution of Hub-height Winds](https://arxiv.org/abs/2510.03364)
*Xiaolong Ma,Xu Dong,Ashley Tarrant,Lei Yang,Rao Kotamarthi,Jiali Wang,Feng Yan,Rajkumar Kettimuthu*

Main category: cs.LG

TL;DR: The paper introduces WindSR, a diffusion model for downscaling hub-height wind observations with improved accuracy by integrating sparse observational data and simulations.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the limitations of coarse and biased wind simulations and sparse observational data by developing a method to better assess wind farm siting and infrastructure-related risks.

Method: WindSR uses a diffusion model with data assimilation, blending sparse observational data dynamically, and incorporates terrain features during training and inference.

Result: WindSR demonstrates superior performance in both efficiency and accuracy compared to traditional CNN and GAN models for wind downscaling.

Conclusion: By reducing model bias by about 20% and improving downscaling, WindSR offers a promising approach for high-resolution wind predictions at hub height.

Abstract: High-quality observations of hub-height winds are valuable but sparse in
space and time. Simulations are widely available on regular grids but are
generally biased and too coarse to inform wind-farm siting or to assess
extreme-weather-related risks (e.g., gusts) at infrastructure scales. To fully
utilize both data types for generating high-quality, high-resolution hub-height
wind speeds (tens to ~100m above ground), this study introduces WindSR, a
diffusion model with data assimilation for super-resolution downscaling of
hub-height winds. WindSR integrates sparse observational data with simulation
fields during downscaling using state-of-the-art diffusion models. A
dynamic-radius blending method is introduced to merge observations with
simulations, providing conditioning for the diffusion process. Terrain
information is incorporated during both training and inference to account for
its role as a key driver of winds. Evaluated against
convolutional-neural-network and generative-adversarial-network baselines,
WindSR outperforms them in both downscaling efficiency and accuracy. Our data
assimilation reduces WindSR's model bias by approximately 20% relative to
independent observations.

</details>


### [483] [A KL-regularization framework for learning to plan with adaptive priors](https://arxiv.org/abs/2510.04280)
*Álvaro Serra-Gomez,Daniel Jarne Ornia,Dhruva Tirumala,Thomas Moerland*

Main category: cs.LG

TL;DR: This paper proposes Policy Optimization-Model Predictive Control (PO-MPC), a unified family of model-based reinforcement learning (MBRL) methods, which aligns the learned policy with the planner's action distribution to improve sample efficiency and performance in high-dimensional tasks.


<details>
  <summary>Details</summary>
Motivation: High-dimensional continuous control tasks in MBRL require sample-efficient exploration methods, yet existing techniques lack alignment between the learned policy and planner, leading to suboptimal performance.

Method: The authors introduce PO-MPC, a KL-regularized MBRL approach that incorporates the planner's action distribution as a prior in policy optimization, unifying and extending prior MPPI-based RL methods under this framework.

Result: Their experiments demonstrate that PO-MPC's extended configurations achieve significant performance improvements, setting new benchmarks in MPPI-based reinforcement learning.

Conclusion: Aligning the policy with the planner's distribution in MBRL using PO-MPC enhances value estimation accuracy, flexibility in policy updates, and overall long-term performance, advancing the state of the art in the field.

Abstract: Effective exploration remains a central challenge in model-based
reinforcement learning (MBRL), particularly in high-dimensional continuous
control tasks where sample efficiency is crucial. A prominent line of recent
work leverages learned policies as proposal distributions for Model-Predictive
Path Integral (MPPI) planning. Initial approaches update the sampling policy
independently of the planner distribution, typically maximizing a learned value
function with deterministic policy gradient and entropy regularization.
However, because the states encountered during training depend on the MPPI
planner, aligning the sampling policy with the planner improves the accuracy of
value estimation and long-term performance. To this end, recent methods update
the sampling policy by minimizing KL divergence to the planner distribution or
by introducing planner-guided regularization into the policy update. In this
work, we unify these MPPI-based reinforcement learning methods under a single
framework by introducing Policy Optimization-Model Predictive Control (PO-MPC),
a family of KL-regularized MBRL methods that integrate the planner's action
distribution as a prior in policy optimization. By aligning the learned policy
with the planner's behavior, PO-MPC allows more flexibility in the policy
updates to trade off Return maximization and KL divergence minimization. We
clarify how prior approaches emerge as special cases of this family, and we
explore previously unstudied variations. Our experiments show that these
extended configurations yield significant performance improvements, advancing
the state of the art in MPPI-based RL.

</details>


### [484] [Disentangling Recall and Reasoning in Transformer Models through Layer-wise Attention and Activation Analysis](https://arxiv.org/abs/2510.03366)
*Harshwardhan Fartale,Ashish Kattamuri,Rahul Raja,Arpita Vats,Ishita Prasad,Akshata Kishore Moharir*

Main category: cs.LG

TL;DR: The paper explores whether transformer-based language models utilize distinct internal mechanisms for recall and reasoning, demonstrating separable but interacting circuits.


<details>
  <summary>Details</summary>
Motivation: Understanding if recall and reasoning rely on separate mechanisms in transformer models is vital for predicting model generalization, creating targeted evaluations, and implementing safer interventions that affect one ability without harming the other.

Method: The study uses controlled datasets of synthetic linguistic puzzles and applies mechanistic interpretability techniques, such as activation patching and structured ablations, to analyze transformers (Qwen, LLaMA) at layer, head, and neuron levels.

Result: The research identifies distinct 'recall circuits' and 'reasoning circuits' in transformer models, with separate effects on fact-retrieval and reasoning when targeted interventions are applied.

Conclusion: This study provides causal evidence of functional specialization in transformer models, linking circuit-level structures to tasks like recall and reasoning, and offers insights for safer and more effective deployment of large language models.

Abstract: Transformer-based language models excel at both recall (retrieving memorized
facts) and reasoning (performing multi-step inference), but whether these
abilities rely on distinct internal mechanisms remains unclear. Distinguishing
recall from reasoning is crucial for predicting model generalization, designing
targeted evaluations, and building safer interventions that affect one ability
without disrupting the other.We approach this question through mechanistic
interpretability, using controlled datasets of synthetic linguistic puzzles to
probe transformer models at the layer, head, and neuron level. Our pipeline
combines activation patching and structured ablations to causally measure
component contributions to each task type. Across two model families (Qwen and
LLaMA), we find that interventions on distinct layers and attention heads lead
to selective impairments: disabling identified "recall circuits" reduces
fact-retrieval accuracy by up to 15\% while leaving reasoning intact, whereas
disabling "reasoning circuits" reduces multi-step inference by a comparable
margin. At the neuron level, we observe task-specific firing patterns, though
these effects are less robust, consistent with neuronal polysemanticity.Our
results provide the first causal evidence that recall and reasoning rely on
separable but interacting circuits in transformer models. These findings
advance mechanistic interpretability by linking circuit-level structure to
functional specialization and demonstrate how controlled datasets and causal
interventions can yield mechanistic insights into model cognition, informing
safer deployment of large language models.

</details>


### [485] [Domain Generalization: A Tale of Two ERMs](https://arxiv.org/abs/2510.04441)
*Yilun Zhu,Naihao Deng,Naichen Shi,Aditya Gangrade,Clayton Scott*

Main category: cs.LG

TL;DR: This paper explores domain generalization and shows that domain-informed empirical risk minimization (ERM) can outperform standard ERM under certain conditions (posterior drift).


<details>
  <summary>Details</summary>
Motivation: The authors aim to investigate why empirical risk minimization (ERM) often performs well in domain generalization (DG) and explore scenarios where alternative methods, like domain-informed ERM, could provide better results.

Method: They provide a theoretical framework distinguishing covariate shift and posterior drift cases, and demonstrate the effectiveness of domain-informed ERM experimentally on tasks in language and vision.

Result: Domain-informed ERM outperforms pooling ERM under the posterior drift assumption, confirmed through both theoretical analysis and experimental validation.

Conclusion: Domain generalization benefits from incorporating domain-specific data in ERM when dealing with posterior drift, revealing opportunities for better model design in DG applications.

Abstract: Domain generalization (DG) is the problem of generalizing from several
distributions (or domains), for which labeled training data are available, to a
new test domain for which no labeled data is available. A common finding in the
DG literature is that it is difficult to outperform empirical risk minimization
(ERM) on the pooled training data.
  In this work, we argue that this finding has primarily been reported for
datasets satisfying a \emph{covariate shift} assumption. When the dataset
satisfies a \emph{posterior drift} assumption instead, we show that
``domain-informed ERM,'' wherein feature vectors are augmented with
domain-specific information, outperforms pooling ERM. These claims are
supported by a theoretical framework and experiments on language and vision
tasks.

</details>


### [486] [Conditional Pseudo-Supervised Contrast for Data-Free Knowledge Distillation](https://arxiv.org/abs/2510.03375)
*Renrong Shao,Wei Zhang,Jun wang*

Main category: cs.LG

TL;DR: This paper proposes a novel method for data-free knowledge distillation (DFKD) using a conditional generative adversarial network (GAN) to synthesize category-specific images, enabling pseudo-supervised learning and improved diversity.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address limitations in current DFKD methods, which struggle with generating category-distinct samples and optimizing category-wise diversity, leading to suboptimal student model performance.

Method: The proposed Conditional Pseudo-Supervised Contrast for DFKD (CPSC-DFKD) introduces a conditional GAN for category-specific image generation, improves generator modules for better category distinction, and implements pseudo-supervised contrastive learning between teacher and student views.

Result: Experiments on three standard datasets validate that the proposed method significantly improves the performance of both the student model and the generator.

Conclusion: CPSC-DFKD is a more effective and efficient approach for data-free knowledge distillation, achieving better performance through category-specific image synthesis and improved diversity. Code provided for reproducibility.

Abstract: Data-free knowledge distillation~(DFKD) is an effective manner to solve model
compression and transmission restrictions while retaining privacy protection,
which has attracted extensive attention in recent years. Currently, the
majority of existing methods utilize a generator to synthesize images to
support the distillation. Although the current methods have achieved great
success, there are still many issues to be explored. Firstly, the outstanding
performance of supervised learning in deep learning drives us to explore a
pseudo-supervised paradigm on DFKD. Secondly, current synthesized methods
cannot distinguish the distributions of different categories of samples, thus
producing ambiguous samples that may lead to an incorrect evaluation by the
teacher. Besides, current methods cannot optimize the category-wise diversity
samples, which will hinder the student model learning from diverse samples and
further achieving better performance. In this paper, to address the above
limitations, we propose a novel learning paradigm, i.e., conditional
pseudo-supervised contrast for data-free knowledge distillation~(CPSC-DFKD).
The primary innovations of CPSC-DFKD are: (1) introducing a conditional
generative adversarial network to synthesize category-specific diverse images
for pseudo-supervised learning, (2) improving the modules of the generator to
distinguish the distributions of different categories, and (3) proposing
pseudo-supervised contrastive learning based on teacher and student views to
enhance diversity. Comprehensive experiments on three commonly-used datasets
validate the performance lift of both the student and generator brought by
CPSC-DFKD. The code is available at https://github.com/RoryShao/CPSC-DFKD.git

</details>


### [487] [A Robust Clustered Federated Learning Approach for Non-IID Data with Quantity Skew](https://arxiv.org/abs/2510.03380)
*Michael Ben Ali,Imen Megdiche,André Peninou,Olivier Teste*

Main category: cs.LG

TL;DR: The paper addresses Quantity Skew (QS), a challenge in Federated Learning with Non-IID data, by proposing CORNFLQS, an iterative CFL algorithm, demonstrating its improved performance under QS scenarios.


<details>
  <summary>Details</summary>
Motivation: Federated Learning faces the issue of Quantity Skew in Non-IID data environments, which affects model performance and robustness, prompting the need for effective solutions.

Method: The paper evaluates state-of-the-art CFL algorithms under Non-IID settings with QS-related scenarios and proposes CORNFLQS, which synthesizes client-based cluster selection and server-driven grouping strategies.

Result: Experimented on six image classification datasets across 270 Non-IID configurations, the proposed approach achieved the highest rankings in accuracy, clustering quality, and robustness against QS variations.

Conclusion: CORNFLQS outperforms current CFL algorithms by addressing QS more effectively, ensuring better robustness and model performance, making it a promising solution for Quantity Skew in Federated Learning.

Abstract: Federated Learning (FL) is a decentralized paradigm that enables a
client-server architecture to collaboratively train a global Artificial
Intelligence model without sharing raw data, thereby preserving privacy. A key
challenge in FL is Non-IID data. Quantity Skew (QS) is a particular problem of
Non-IID, where clients hold highly heterogeneous data volumes. Clustered
Federated Learning (CFL) is an emergent variant of FL that presents a promising
solution to Non-IID problem. It improves models' performance by grouping
clients with similar data distributions into clusters. CFL methods generally
fall into two operating strategies. In the first strategy, clients select the
cluster that minimizes the local training loss. In the second strategy, the
server groups clients based on local model similarities. However, most CFL
methods lack systematic evaluation under QS but present significant challenges
because of it. In this paper, we present two main contributions. The first one
is an evaluation of state-of-the-art CFL algorithms under various Non-IID
settings, applying multiple QS scenarios to assess their robustness. Our second
contribution is a novel iterative CFL algorithm, named CORNFLQS, which proposes
an optimal coordination between both operating strategies of CFL. Our approach
is robust against the different variations of QS settings. We conducted
intensive experiments on six image classification datasets, resulting in 270
Non-IID configurations. The results show that CORNFLQS achieves the highest
average ranking in both accuracy and clustering quality, as well as strong
robustness to QS perturbations. Overall, our approach outperforms actual CFL
algorithms.

</details>


### [488] [Demystifying MaskGIT Sampler and Beyond: Adaptive Order Selection in Masked Diffusion](https://arxiv.org/abs/2510.04525)
*Satoshi Hayakawa,Yuhta Takida,Masaaki Imaizumi,Hiromi Wakaki,Yuki Mitsufuji*

Main category: cs.LG

TL;DR: This paper presents innovations to improve and accelerate masked diffusion models' sampling process through new sampling strategies and theoretical insights.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the inefficiency in the sampling processes of masked diffusion models, despite their high-quality generative capabilities across various domains.

Method: The paper analyzes the MaskGIT sampler to uncover its temperature sampling mechanism and introduces the 'moment sampler,' alongside innovations like partial caching for transformers and a hybrid strategy for adaptive unmasking.

Result: The proposed methods, tested on image and text domains, confirm improved sampling efficiency and align with the introduced theoretical insights.

Conclusion: The study advances the theoretical understanding and practical efficiency of masked diffusion samplers, making them more tractable for generating high-quality samples.

Abstract: Masked diffusion models have shown promising performance in generating
high-quality samples in a wide range of domains, but accelerating their
sampling process remains relatively underexplored. To investigate efficient
samplers for masked diffusion, this paper theoretically analyzes the MaskGIT
sampler for image modeling, revealing its implicit temperature sampling
mechanism. Through this analysis, we introduce the "moment sampler," an
asymptotically equivalent but more tractable and interpretable alternative to
MaskGIT, which employs a "choose-then-sample" approach by selecting unmasking
positions before sampling tokens. In addition, we improve the efficiency of
choose-then-sample algorithms through two key innovations: a partial caching
technique for transformers that approximates longer sampling trajectories
without proportional computational cost, and a hybrid approach formalizing the
exploration-exploitation trade-off in adaptive unmasking. Experiments in image
and text domains demonstrate our theory as well as the efficiency of our
proposed methods, advancing both theoretical understanding and practical
implementation of masked diffusion samplers.

</details>


### [489] [Cross-Modal Reconstruction Pretraining for Ramp Flow Prediction at Highway Interchanges](https://arxiv.org/abs/2510.03381)
*Yongchao Li,Jun Chen,Zhuoxuan Li,Chao Gao,Yang Li,Chu Zhang,Changyin Dong*

Main category: cs.LG

TL;DR: The paper introduces a novel Spatio-Temporal Decoupled Autoencoder (STDAE) to predict traffic in interchanges without real-time ramp detectors.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the problem of traffic prediction at interchanges caused by the lack of real-time ramp detectors, leading to blind spots.

Method: The STDAE framework first uses cross-modal reconstruction pretraining to predict ramp flow from mainline data, decoupling spatial and temporal feature extraction. Later, it integrates these features with other models, such as GWNet, for prediction.

Result: STDAE combined with GWNet outperformed thirteen state-of-the-art baselines in traffic prediction and matched models using historical ramp data on three real-world datasets.

Conclusion: STDAE is effective in overcoming the challenges of detector scarcity and can be plugged into diverse forecasting pipelines.

Abstract: Interchanges are crucial nodes for vehicle transfers between highways, yet
the lack of real-time ramp detectors creates blind spots in traffic prediction.
To address this, we propose a Spatio-Temporal Decoupled Autoencoder (STDAE), a
two-stage framework that leverages cross-modal reconstruction pretraining. In
the first stage, STDAE reconstructs historical ramp flows from mainline data,
forcing the model to capture intrinsic spatio-temporal relations. Its decoupled
architecture with parallel spatial and temporal autoencoders efficiently
extracts heterogeneous features. In the prediction stage, the learned
representations are integrated with models such as GWNet to enhance accuracy.
Experiments on three real-world interchange datasets show that STDAE-GWNET
consistently outperforms thirteen state-of-the-art baselines and achieves
performance comparable to models using historical ramp data. This demonstrates
its effectiveness in overcoming detector scarcity and its plug-and-play
potential for diverse forecasting pipelines.

</details>


### [490] [Graph-based Tabular Deep Learning Should Learn Feature Interactions, Not Just Make Predictions](https://arxiv.org/abs/2510.04543)
*Elias Dubbeldam,Reza Mohammadi,Marit Schoonhoven,S. Ilker Birbil*

Main category: cs.LG

TL;DR: Graph-based tabular deep learning (GTDL) struggles to properly capture dataset-specific feature interactions in tabular data, unlike traditional tree-based models.


<details>
  <summary>Details</summary>
Motivation: To address the performance gap between deep learning methods and tree-based models on tabular data by better capturing dataset-specific feature interactions.

Method: The paper uses synthetic datasets with known ground-truth graph structures to evaluate and demonstrate the limitations of existing GTDL methods in learning meaningful feature interactions.

Result: Findings reveal that existing GTDL methods fail to recover accurate feature interactions, and enforcing true interaction structures can enhance predictive performance.

Conclusion: A shift toward structure-aware modeling in GTDL is recommended to improve accuracy, interpretability, and trustworthiness of models.

Abstract: Despite recent progress, deep learning methods for tabular data still
struggle to compete with traditional tree-based models. A key challenge lies in
modeling complex, dataset-specific feature interactions that are central to
tabular data. Graph-based tabular deep learning (GTDL) methods aim to address
this by representing features and their interactions as graphs. However,
existing methods predominantly optimize predictive accuracy, neglecting
accurate modeling of the graph structure. This position paper argues that GTDL
should move beyond prediction-centric objectives and prioritize the explicit
learning and evaluation of feature interactions. Using synthetic datasets with
known ground-truth graph structures, we show that existing GTDL methods fail to
recover meaningful feature interactions. Moreover, enforcing the true
interaction structure improves predictive performance. This highlights the need
for GTDL methods to prioritize quantitative evaluation and accurate structural
learning. We call for a shift toward structure-aware modeling as a foundation
for building GTDL systems that are not only accurate but also interpretable,
trustworthy, and grounded in domain understanding.

</details>


### [491] [Studying the Korean Word-Chain Game with RLVR:Mitigating Reward Conflicts via Curriculum Learning](https://arxiv.org/abs/2510.03394)
*Donghwan Rho*

Main category: cs.LG

TL;DR: The paper explores reinforcement learning with verifiable rewards (RLVR) applied to Korean word-chain games, addressing reward conflicts through curriculum learning.


<details>
  <summary>Details</summary>
Motivation: Improve reasoning abilities in large language models (LLMs) and apply RLVR to language-specific puzzle tasks.

Method: They use RLVR in the Korean word-chain game and introduce a curriculum-learning scheme to resolve reward conflicts.

Result: Curriculum learning mitigates conflicts arising from rule-derived rewards, showcasing applicability to diverse linguistic puzzles.

Conclusion: Their approach motivates further exploration of RLVR methods for puzzles in different languages, enhancing LLM capabilities.

Abstract: Reinforcement learning with verifiable rewards (RLVR) is a promising approach
for training large language models (LLMs) with stronger reasoning abilities. It
has also been applied to a variety of logic puzzles. In this work, we study the
Korean word-chain game using RLVR. We show that rule-derived rewards can
naturally conflict, and demonstrate through experiments that a
curriculum-learning scheme mitigates these conflicts. Our findings motivate
further studies of puzzle tasks in diverse languages.

</details>


### [492] [Training Variation of Physically-Informed Deep Learning Models](https://arxiv.org/abs/2510.03416)
*Ashley Lenau,Dennis Dimiduk,Stephen R. Niezgoda*

Main category: cs.LG

TL;DR: The paper explores the role of loss functions in training deep learning models, emphasizing the need for reporting model variations to ensure reliable and reproducible training algorithms.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address gaps in the discussion around the reliability and reproducibility of training algorithms, particularly concerning the ability of physics-informed loss functions to condition neural networks for boundary condition enforcement.

Method: A Pix2Pix network is used as a case study to predict stress fields in high elastic contrast composites, with different loss functions employed to evaluate their performance in training consistency, convergence, and stress equilibrium.

Result: Different loss functions showed varying levels of convergence, accuracy, and ability to enforce stress equilibrium across multiple training runs.

Conclusion: The findings highlight the importance of reporting model variation as a measure of a training algorithm's reliability, and the study proposes best practices for such reporting.

Abstract: A successful deep learning network is highly dependent not only on the
training dataset, but the training algorithm used to condition the network for
a given task. The loss function, dataset, and tuning of hyperparameters all
play an essential role in training a network, yet there is not much discussion
on the reliability or reproducibility of a training algorithm. With the rise in
popularity of physics-informed loss functions, this raises the question of how
reliable one's loss function is in conditioning a network to enforce a
particular boundary condition. Reporting the model variation is needed to
assess a loss function's ability to consistently train a network to obey a
given boundary condition, and provides a fairer comparison among different
methods. In this work, a Pix2Pix network predicting the stress fields of high
elastic contrast composites is used as a case study. Several different loss
functions enforcing stress equilibrium are implemented, with each displaying
different levels of variation in convergence, accuracy, and enforcing stress
equilibrium across many training sessions. Suggested practices in reporting
model variation are also shared.

</details>


### [493] [SONA: Learning Conditional, Unconditional, and Mismatching-Aware Discriminator](https://arxiv.org/abs/2510.04576)
*Yuhta Takida,Satoshi Hayakawa,Takashi Shibuya,Masaaki Imaizumi,Naoki Murata,Bac Nguyen,Toshimitsu Uesaka,Chieh-Hsin Lai,Yuki Mitsufuji*

Main category: cs.LG

TL;DR: The paper introduces "SONA," a novel discriminator for conditional generative models, enhancing both authenticity and alignment, achieving state-of-the-art results in tasks like class-conditional and text-to-image generation.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of conditional generation in generative adversarial networks, where existing methods struggle to balance authenticity and conditional alignment.

Method: The method revolves around introducing the SONA discriminator with independent projections for authenticity and alignment, matched by adaptive weighting mechanisms and tailored objective functions.

Result: Experimentation demonstrates that SONA surpasses state-of-the-art conditional generative methods in sample quality and alignment, highlighting its versatility in applications such as text-to-image generation.

Conclusion: SONA improves conditional generative tasks by dynamically balancing authenticity and alignment objectives, offering robustness and high performance across various applications.

Abstract: Deep generative models have made significant advances in generating complex
content, yet conditional generation remains a fundamental challenge. Existing
conditional generative adversarial networks often struggle to balance the dual
objectives of assessing authenticity and conditional alignment of input samples
within their conditional discriminators. To address this, we propose a novel
discriminator design that integrates three key capabilities: unconditional
discrimination, matching-aware supervision to enhance alignment sensitivity,
and adaptive weighting to dynamically balance all objectives. Specifically, we
introduce Sum of Naturalness and Alignment (SONA), which employs separate
projections for naturalness (authenticity) and alignment in the final layer
with an inductive bias, supported by dedicated objective functions and an
adaptive weighting mechanism. Extensive experiments on class-conditional
generation tasks show that \ours achieves superior sample quality and
conditional alignment compared to state-of-the-art methods. Furthermore, we
demonstrate its effectiveness in text-to-image generation, confirming the
versatility and robustness of our approach.

</details>


### [494] [Busemann Functions in the Wasserstein Space: Existence, Closed-Forms, and Applications to Slicing](https://arxiv.org/abs/2510.04579)
*Clément Bonet,Elsa Cazelles,Lucas Drumetz,Nicolas Courty*

Main category: cs.LG

TL;DR: The paper studies the Busemann function in Wasserstein space, establishes closed-form expressions for specific cases, and introduces applications like Sliced-Wasserstein distances for practical scenarios.


<details>
  <summary>Details</summary>
Motivation: To explore Busemann functions in the context of Wasserstein space and their utility for analyzing probability distributions in geometric machine learning.

Method: The paper derives closed-form solutions for Busemann functions in one-dimensional distributions and Gaussian measures, enabling novel projection schemes.

Result: The authors define new Sliced-Wasserstein distances applicable to Gaussian mixtures and labeled datasets, demonstrating their efficiency on synthetic and real-world transfer learning problems.

Conclusion: The study advances the theoretical understanding of Busemann functions in Wasserstein space while introducing practical tools that perform well in machine learning tasks.

Abstract: The Busemann function has recently found much interest in a variety of
geometric machine learning problems, as it naturally defines projections onto
geodesic rays of Riemannian manifolds and generalizes the notion of
hyperplanes. As several sources of data can be conveniently modeled as
probability distributions, it is natural to study this function in the
Wasserstein space, which carries a rich formal Riemannian structure induced by
Optimal Transport metrics. In this work, we investigate the existence and
computation of Busemann functions in Wasserstein space, which admits geodesic
rays. We establish closed-form expressions in two important cases:
one-dimensional distributions and Gaussian measures. These results enable
explicit projection schemes for probability distributions on $\mathbb{R}$,
which in turn allow us to define novel Sliced-Wasserstein distances over
Gaussian mixtures and labeled datasets. We demonstrate the efficiency of those
original schemes on synthetic datasets as well as transfer learning problems.

</details>


### [495] [Memory-Efficient Backpropagation for Fine-Tuning LLMs on Resource-Constrained Mobile Devices](https://arxiv.org/abs/2510.03425)
*Congzheng Song,Xinyu Tang*

Main category: cs.LG

TL;DR: The paper introduces a memory-efficient backpropagation (MeBP) method for fine-tuning large language models (LLMs) on mobile devices, achieving better speed and performance than existing zeroth-order methods.


<details>
  <summary>Details</summary>
Motivation: Current fine-tuning methods for large language models are impractical for resource-constrained devices, due to high memory and compute demands.

Method: The authors propose MeBP, a memory-efficient backpropagation technique, as an alternative to traditional fine-tuning methods and zeroth-order optimization.

Result: MeBP allows fine-tuning various LLMs (0.5B to 4B parameters) on mobile devices like the iPhone 15 Pro Max, using less than 1GB of memory while performing faster and with better convergence compared to zeroth-order optimization.

Conclusion: MeBP provides an effective and resource-efficient solution for fine-tuning large language models on memory-constrained mobile devices, enabling practical on-device machine learning.

Abstract: Fine-tuning large language models (LLMs) with backpropagation\textemdash even
for a subset of parameters such as LoRA\textemdash can be much more
memory-consuming than inference and is often deemed impractical for
resource-constrained mobile devices. Alternative methods, such as zeroth-order
optimization (ZO), can greatly reduce the memory footprint but come at the cost
of significantly slower model convergence (10$\times$ to 100$\times$ more steps
than backpropagation). We propose a memory-efficient implementation of
backpropagation (MeBP) on mobile devices that provides better trade-off between
memory usage and compute time, while converging faster and achieving better
performance than the ZO baseline. We verify the effectiveness of MeBP on an
iPhone 15 Pro Max and show that various LLMs, ranging from 0.5B to 4B
parameters, can be fine-tuned using less than 1GB of memory. We release an
example of the MeBP implementation at https://github.com/apple/ml-mebp.

</details>


### [496] [Closed-Form Last Layer Optimization](https://arxiv.org/abs/2510.04606)
*Alexandre Galashov,Nathaël Da Costa,Liyuan Xu,Philipp Hennig,Arthur Gretton*

Main category: cs.LG

TL;DR: This paper explores a strategy where neural networks optimize only the backbone parameters while utilizing a closed-form solution for the linear last layer weights instead of relying solely on stochastic gradient descent (SGD).


<details>
  <summary>Details</summary>
Motivation: The motivation arises from the fact that the linear last layer has a known closed-form optimal solution under squared loss, which can be leveraged during optimization to potentially improve performance and efficiency.

Method: The method involves alternately performing gradient descent steps on the backbone parameters and closed-form updates on the last layer weights. Additionally, it modifies traditional SGD by balancing loss between the current batch and accumulated information.

Result: The paper demonstrates that the proposed method achieves optimal convergence under Neural Tangent Kernel assumptions and outperforms standard SGD in several supervised regression and classification tasks.

Conclusion: This approach provides a more effective alternative to standard SGD in scenarios involving squared loss by incorporating closed-form solutions for last-layer weights with iterative gradient optimization of backbone.

Abstract: Neural networks are typically optimized with variants of stochastic gradient
descent. Under a squared loss, however, the optimal solution to the linear last
layer weights is known in closed-form. We propose to leverage this during
optimization, treating the last layer as a function of the backbone parameters,
and optimizing solely for these parameters. We show this is equivalent to
alternating between gradient descent steps on the backbone and closed-form
updates on the last layer. We adapt the method for the setting of stochastic
gradient descent, by trading off the loss on the current batch against the
accumulated information from previous batches. Further, we prove that, in the
Neural Tangent Kernel regime, convergence of this method to an optimal solution
is guaranteed. Finally, we demonstrate the effectiveness of our approach
compared with standard SGD on a squared loss in several supervised tasks --
both regression and classification -- including Fourier Neural Operators and
Instrumental Variable Regression.

</details>


### [497] [Generalized Orders of Magnitude for Scalable, Parallel, High-Dynamic-Range Computation](https://arxiv.org/abs/2510.03426)
*Franz A. Heinsen,Leo Kozachkov*

Main category: cs.LG

TL;DR: The paper introduces Generalized Orders of Magnitude (GOOMs) to enhance numerical computation stability across large real-number ranges, outperforming traditional methods in several challenging experiments.


<details>
  <summary>Details</summary>
Motivation: Address numerical instability during long sequences of computations in domains like deep learning and finance due to underflow or overflow.

Method: Introduced GOOMs as an extension of orders of magnitude, implemented custom parallel prefix scan for parallel hardware execution.

Result: GOOMs outperformed traditional approaches in three tasks: compounding matrix products beyond floating-point limits, faster Lyapunov spectra estimation, and capturing long-range dependencies in deep RNNs.

Conclusion: GOOMs provide a scalable and robust alternative for high-dynamic-range applications, improving numerical stability and computational efficiency on parallel hardware.

Abstract: Many domains, from deep learning to finance, require compounding real numbers
over long sequences, often leading to catastrophic numerical underflow or
overflow. We introduce generalized orders of magnitude (GOOMs), a principled
extension of traditional orders of magnitude that incorporates floating-point
numbers as a special case, and which in practice enables stable computation
over significantly larger dynamic ranges of real numbers than previously
possible. We implement GOOMs, along with an efficient custom parallel prefix
scan, to support native execution on parallel hardware such as GPUs. We
demonstrate that our implementation of GOOMs outperforms traditional approaches
with three representative experiments, all of which were previously considered
impractical or impossible, and now become possible and practical: (1)
compounding real matrix products far beyond standard floating-point limits; (2)
estimating spectra of Lyapunov exponents in parallel, orders of magnitude
faster than with previous methods, applying a novel selective-resetting method
to prevent state colinearity; and (3) capturing long-range dependencies in deep
recurrent neural networks with non-diagonal recurrent states, computed in
parallel via a prefix scan, without requiring any form of stabilization. Our
results show that our implementation of GOOMs, combined with efficient parallel
scanning, offers a scalable and numerically robust alternative to conventional
floating-point numbers for high-dynamic-range applications.

</details>


### [498] [LHGEL: Large Heterogeneous Graph Ensemble Learning using Batch View Aggregation](https://arxiv.org/abs/2510.03432)
*Jiajun Shen,Yufei Jin,Yi He,Xingquan Zhu*

Main category: cs.LG

TL;DR: The paper introduces LHGEL, an ensemble learning framework to address challenges in learning from large heterogeneous graphs using components like batch view aggregation, residual attention, and diversity regularization.


<details>
  <summary>Details</summary>
Motivation: The authors aim to overcome difficulties posed by learning from large heterogeneous graphs, including issues like scale, heterogeneity, and node feature variations.

Method: The proposed solution, LHGEL, employs batch sampling combined with three strategies: batch view aggregation forming subgraph views, residual attention for adaptive weighting, and diversity regularization to boost model robustness.

Result: Theoretical studies show residual attention combats gradient vanishing, and empirical tests on five heterogeneous networks confirm LHGEL's superior performance over existing approaches.

Conclusion: LHGEL effectively addresses heterogeneity and scalability challenges in graph learning, combining different techniques to ensure robustness and efficiency while outperforming competitors.

Abstract: Learning from large heterogeneous graphs presents significant challenges due
to the scale of networks, heterogeneity in node and edge types, variations in
nodal features, and complex local neighborhood structures. This paper advocates
for ensemble learning as a natural solution to this problem, whereby training
multiple graph learners under distinct sampling conditions, the ensemble
inherently captures different aspects of graph heterogeneity. Yet, the crux
lies in combining these learners to meet global optimization objective while
maintaining computational efficiency on large-scale graphs. In response, we
propose LHGEL, an ensemble framework that addresses these challenges through
batch sampling with three key components, namely batch view aggregation,
residual attention, and diversity regularization. Specifically, batch view
aggregation samples subgraphs and forms multiple graph views, while residual
attention adaptively weights the contributions of these views to guide node
embeddings toward informative subgraphs, thereby improving the accuracy of base
learners. Diversity regularization encourages representational disparity across
embedding matrices derived from different views, promoting model diversity and
ensemble robustness. Our theoretical study demonstrates that residual attention
mitigates gradient vanishing issues commonly faced in ensemble learning.
Empirical results on five real heterogeneous networks validate that our LHGEL
approach consistently outperforms its state-of-the-art competitors by
substantial margin. Codes and datasets are available at
https://github.com/Chrisshen12/LHGEL.

</details>


### [499] [When Do Credal Sets Stabilize? Fixed-Point Theorems for Credal Set Updates](https://arxiv.org/abs/2510.04769)
*Michele Caprio,Siu Lun Chau,Krikamol Muandet*

Main category: cs.LG

TL;DR: This paper analyzes whether iterative updates on credal sets, representing imprecise probabilistic beliefs, in IPML converge to stable fixed points.


<details>
  <summary>Details</summary>
Motivation: To explore whether iterative learning processes using credal sets lead to stable fixed points and to improve the representation of uncertainty in machine learning.

Method: The authors analyze the dynamics of iterative update rules on credal sets and demonstrate the findings using Credal Bayesian Deep Learning as an example.

Result: The analysis reveals structural conditions under which stability emerges in the iterative learning processes under imprecision.

Conclusion: Incorporating imprecision enriches uncertainty representation and offers insights into the stability dynamics of iterative learning processes.

Abstract: Many machine learning algorithms rely on iterative updates of uncertainty
representations, ranging from variational inference and
expectation-maximization, to reinforcement learning, continual learning, and
multi-agent learning. In the presence of imprecision and ambiguity, credal sets
-- closed, convex sets of probability distributions -- have emerged as a
popular framework for representing imprecise probabilistic beliefs. Under such
imprecision, many learning problems in imprecise probabilistic machine learning
(IPML) may be viewed as processes involving successive applications of update
rules on credal sets. This naturally raises the question of whether this
iterative process converges to stable fixed points -- or, more generally, under
what conditions on the updating mechanism such fixed points exist, and whether
they can be attained. We provide the first analysis of this problem and
illustrate our findings using Credal Bayesian Deep Learning as a concrete
example. Our work demonstrates that incorporating imprecision into the learning
process not only enriches the representation of uncertainty, but also reveals
structural conditions under which stability emerges, thereby offering new
insights into the dynamics of iterative learning under imprecision.

</details>


### [500] [The Argument is the Explanation: Structured Argumentation for Trust in Agents](https://arxiv.org/abs/2510.03442)
*Ege Cakar,Per Ola Kristensson*

Main category: cs.LG

TL;DR: The paper argues that AI explainability should focus on creating verifiable reasoning chains rather than emphasizing full mechanistic transparency. It introduces structured argumentation as a novel method, achieving state-of-the-art performance in argument classification tasks and demonstrating practical applications in multi-agent risk assessment.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of current AI explainability methods, which fail to provide robust and verifiable reasoning processes akin to human evaluative practices.

Method: The paper utilizes structured argumentation pipelines, converting LLM-generated text into argument graphs to map inferential relationships. It employs Bipolar Assumption-Based Argumentation to model support and attack relationships and offers automatic detection of hallucinations and test-time feedback for iterative verification.

Result: Achieving state-of-the-art metrics: 94.44 macro F1 on AAEC tasks and 0.81 macro F1 for Argumentative MicroTexts relation classification, surpassing previous benchmarks.

Conclusion: Structured argumentation offers a promising pathway for creating explainable AI systems that are verifiable, practical, and transparent, with potential for broad applications beyond mechanics-focused interpretation methods.

Abstract: Humans are black boxes -- we cannot observe their neural processes, yet
society functions by evaluating verifiable arguments. AI explainability should
follow this principle: stakeholders need verifiable reasoning chains, not
mechanistic transparency. We propose using structured argumentation to provide
a level of explanation and verification neither interpretability nor
LLM-generated explanation is able to offer. Our pipeline achieves
state-of-the-art 94.44 macro F1 on the AAEC published train/test split (5.7
points above prior work) and $0.81$ macro F1, $\sim$0.07 above previous
published results with comparable data setups, for Argumentative MicroTexts
relation classification, converting LLM text into argument graphs and enabling
verification at each inferential step. We demonstrate this idea on multi-agent
risk assessment using the Structured What-If Technique, where specialized
agents collaborate transparently to carry out risk assessment otherwise
achieved by humans alone. Using Bipolar Assumption-Based Argumentation, we
capture support/attack relationships, thereby enabling automatic hallucination
detection via fact nodes attacking arguments. We also provide a verification
mechanism that enables iterative refinement through test-time feedback without
retraining. For easy deployment, we provide a Docker container for the
fine-tuned AMT model, and the rest of the code with the Bipolar ABA Python
package on GitHub.

</details>


### [501] [On Structured State-Space Duality](https://arxiv.org/abs/2510.04944)
*Jerry Yao-Chieh Hu,Xiwen Zhang,Weimin Wu,Han Liu*

Main category: cs.LG

TL;DR: The paper exposes a duality between certain structured state-space models (SSM) and masked attention mechanisms, showing computational equivalences and limitations.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between recurrent state-space models and transformer-based attention and explore their equivalences and differences.

Method: Mathematical formalization and generalization of the equivalence between structured SSMs and masked attention, along with theoretical analysis.

Result: Established equivalence for diagonal SSMs with masked attention and determined conditions where the equivalence exists or fails.

Conclusion: Strengthened connections between recurrent SSMs and transformers, expanding possibilities for efficient sequence modeling.

Abstract: Structured State-Space Duality (SSD) [Dao & Gu, ICML 2024] is an equivalence
between a simple Structured State-Space Model (SSM) and a masked attention
mechanism. In particular, a state-space model with a scalar-times-identity
state matrix is equivalent to a masked self-attention with a $1$-semiseparable
causal mask. Consequently, the same sequence transformation (model) has two
algorithmic realizations: as a linear-time $O(T)$ recurrence or as a
quadratic-time $O(T^2)$ attention. In this note, we formalize and generalize
this duality: (i) we extend SSD from the scalar-identity case to general
diagonal SSMs (diagonal state matrices); (ii) we show that these diagonal SSMs
match the scalar case's training complexity lower bounds while supporting
richer dynamics; (iii) we establish a necessary and sufficient condition under
which an SSM is equivalent to $1$-semiseparable masked attention; and (iv) we
show that such duality fails to extend to standard softmax attention due to
rank explosion. Together, these results tighten bridge between recurrent SSMs
and Transformers, and widen the design space for expressive yet efficient
sequence models.

</details>


### [502] [Reinforce-Ada: An Adaptive Sampling Framework for Reinforce-Style LLM Training](https://arxiv.org/abs/2510.04996)
*Wei Xiong,Chenlu Ye,Baohao Liao,Hanze Dong,Xinxing Xu,Christof Monz,Jiang Bian,Nan Jiang,Tong Zhang*

Main category: cs.LG

TL;DR: Reinforce-Ada proposes an adaptive sampling method for reinforcement learning in large language models to increase efficiency by reallocating sampling dynamically based on uncertainty.


<details>
  <summary>Details</summary>
Motivation: Reinforcement learning for reasoning tasks with large language models suffers from instability due to uniform sampling across prompts, necessitating more adaptive solutions.

Method: Reinforce-Ada reallocates sampling effort dynamically, interleaving estimation and sampling in an online successive elimination process, and halts sampling once sufficient data is collected.

Result: Empirical tests show Reinforce-Ada accelerates convergence and improves performance compared to conventional methods across different architectures and benchmarks.

Conclusion: Variance-aware, adaptive sampling is critical for enhancing reinforcement learning efficiency and reliability in reasoning tasks for large language models.

Abstract: Reinforcement learning applied to large language models (LLMs) for reasoning
tasks is often bottlenecked by unstable gradient estimates due to fixed and
uniform sampling of responses across prompts. Prior work such as GVM-RAFT
addresses this by dynamically allocating inference budget per prompt to
minimize stochastic gradient variance under a budget constraint. Inspired by
this insight, we propose Reinforce-Ada, an adaptive sampling framework for
online RL post-training of LLMs that continuously reallocates sampling effort
to the prompts with the greatest uncertainty or learning potential. Unlike
conventional two-stage allocation methods, Reinforce-Ada interleaves estimation
and sampling in an online successive elimination process, and automatically
stops sampling for a prompt once sufficient signal is collected. To stabilize
updates, we form fixed-size groups with enforced reward diversity and compute
advantage baselines using global statistics aggregated over the adaptive
sampling phase. Empirical results across multiple model architectures and
reasoning benchmarks show that Reinforce-Ada accelerates convergence and
improves final performance compared to GRPO, especially when using the balanced
sampling variant. Our work highlights the central role of variance-aware,
adaptive data curation in enabling efficient and reliable reinforcement
learning for reasoning-capable LLMs. Code is available at
https://github.com/RLHFlow/Reinforce-Ada.

</details>


### [503] [How to Set $β_1, β_2$ in Adam: An Online Learning Perspective](https://arxiv.org/abs/2510.03478)
*Quan Nguyen*

Main category: cs.LG

TL;DR: The paper investigates optimal settings for Adam optimizer's momentum factors, $\\beta_1$ and $\\beta_2$, by conducting generalized and tighter analyses compared to prior work.


<details>
  <summary>Details</summary>
Motivation: To address the incomplete theoretical understanding of optimal parameter settings for Adam optimizer, especially in practical scenarios where $\\beta_1$ and $\\beta_2$ do not satisfy $\\beta_1 = \\sqrt{\\beta_2}$.

Method: Develop a generalized analytical framework to evaluate the performance of Adam optimizer for cases with $\\beta_1$ greater or less than $\\sqrt{\\beta_2}$. Provide bounds for its worst-case performance and assess optimization settings under different adversary types.

Result: The analysis generalized existing bounds for both cases ($\\beta_1 \geq \\sqrt{\\beta_2}$ and $\\beta_1 \leq \\sqrt{\\beta_2}$), proving their tightness in worst-case scenarios. It was shown that $\\beta_1 = \\sqrt{\\beta_2}$ is optimal for an oblivious adversary but sub-optimal for a non-oblivious adversary.

Conclusion: The paper improves understanding of parameter settings in Adam optimizer by proposing generalized and theoretically tight analyses, providing better guidance for scenarios with adversary differences.

Abstract: While Adam is one of the most effective optimizer for training large-scale
machine learning models, a theoretical understanding of how to optimally set
its momentum factors, $\beta_1$ and $\beta_2$, remains largely incomplete.
  Prior works have shown that Adam can be seen as an instance of
Follow-the-Regularized-Leader (FTRL), one of the most important class of
algorithms in online learning.
  The prior analyses in these works required setting $\beta_1 =
\sqrt{\beta_2}$, which does not cover the more practical cases with $\beta_1
\neq \sqrt{\beta_2}$.
  We derive novel, more general analyses that hold for both $\beta_1 \geq
\sqrt{\beta_2}$ and $\beta_1 \leq \sqrt{\beta_2}$.
  In both cases, our results strictly generalize the existing bounds.
  Furthermore, we show that our bounds are tight in the worst case.
  We also prove that setting $\beta_1 = \sqrt{\beta_2}$ is optimal for an
oblivious adversary, but sub-optimal for an non-oblivious adversary.

</details>


### [504] [Rethinking Langevin Thompson Sampling from A Stochastic Approximation Perspective](https://arxiv.org/abs/2510.05023)
*Weixin Wang,Haoyang Zheng,Guang Lin,Wei Deng,Pan Xu*

Main category: cs.LG

TL;DR: This paper introduces TS-SA, a modified Thompson Sampling approach, that reduces challenges of non-stationarity and improves performance in multi-armed bandit tasks.


<details>
  <summary>Details</summary>
Motivation: Existing approximate Thompson Sampling methods face difficulties due to non-stationary posterior distributions, making hyperparameter tuning complex and hindering both practical and theoretical advancements.

Method: The TS-SA algorithm utilizes stochastic approximation, Langevin Monte Carlo updates, and temporal averaging to maintain a stationary posterior throughout iterations, allowing for fixed step-sizes and unified analysis.

Result: TS-SA achieves near-optimal regret bounds and demonstrates significant empirical performance improvements over existing methods in multi-armed bandit tasks.

Conclusion: The proposed TS-SA provides a more theoretically intuitive and practically effective method for approximate Thompson Sampling, alleviating non-stationarity issues and enhancing robustness.

Abstract: Most existing approximate Thompson Sampling (TS) algorithms for multi-armed
bandits use Stochastic Gradient Langevin Dynamics (SGLD) or its variants in
each round to sample from the posterior, relaxing the need for conjugacy
assumptions between priors and reward distributions in vanilla TS. However,
they often require approximating a different posterior distribution in
different round of the bandit problem. This requires tricky, round-specific
tuning of hyperparameters such as dynamic learning rates, causing challenges in
both theoretical analysis and practical implementation. To alleviate this
non-stationarity, we introduce TS-SA, which incorporates stochastic
approximation (SA) within the TS framework. In each round, TS-SA constructs a
posterior approximation only using the most recent reward(s), performs a
Langevin Monte Carlo (LMC) update, and applies an SA step to average noisy
proposals over time. This can be interpreted as approximating a stationary
posterior target throughout the entire algorithm, which further yields a fixed
step-size, a unified convergence analysis framework, and improved posterior
estimates through temporal averaging. We establish near-optimal regret bounds
for TS-SA, with a simplified and more intuitive theoretical analysis enabled by
interpreting the entire algorithm as a simulation of a stationary SGLD process.
Our empirical results demonstrate that even a single-step Langevin update with
certain warm-up outperforms existing methods substantially on bandit tasks.

</details>


### [505] [Reasoning-based Anomaly Detection Framework: A Real-time, Scalable, and Automated Approach to Anomaly Detection Across Domains](https://arxiv.org/abs/2510.03486)
*Anupam Panwar,Himadri Pal,Jiali Chen,Kyle Cho,Riddick Jiang,Miao Zhao,Rajiv Krishnamurthy*

Main category: cs.LG

TL;DR: The paper introduces a framework called RADF, which uses an automated technique to enhance anomaly detection in large, heterogeneous datasets and improve root-cause analysis.


<details>
  <summary>Details</summary>
Motivation: The paper addresses three key challenges in anomaly detection for large, distributed systems: processing large data volumes, handling heterogeneous time-series datasets requiring fine-tuned solutions, and identifying root causes of anomalies efficiently.

Method: The authors propose the Reasoning based Anomaly Detection Framework (RADF), which includes a novel technique called mSelect for automating algorithm selection and hyper-parameter tuning. It also incorporates post-detection capabilities to facilitate triaging and root-cause analysis.

Result: RADF outperforms state-of-the-art models, achieving an AUC of over 0.85 for 7 out of 9 public benchmarking datasets and exceeding the performance of comparable models in 5 out of 9 datasets.

Conclusion: RADF successfully addresses major challenges in anomaly detection for large datasets and provides superior real-time performance, making it a significant advancement over existing methods.

Abstract: Detecting anomalies in large, distributed systems presents several
challenges. The first challenge arises from the sheer volume of data that needs
to be processed. Flagging anomalies in a high-throughput environment calls for
a careful consideration of both algorithm and system design. The second
challenge comes from the heterogeneity of time-series datasets that leverage
such a system in production. In practice, anomaly detection systems are rarely
deployed for a single use case. Typically, there are several metrics to
monitor, often across several domains (e.g. engineering, business and
operations). A one-size-fits-all approach rarely works, so these systems need
to be fine-tuned for every application - this is often done manually. The third
challenge comes from the fact that determining the root-cause of anomalies in
such settings is akin to finding a needle in a haystack. Identifying (in real
time) a time-series dataset that is associated causally with the anomalous
time-series data is a very difficult problem. In this paper, we describe a
unified framework that addresses these challenges. Reasoning based Anomaly
Detection Framework (RADF) is designed to perform real time anomaly detection
on very large datasets. This framework employs a novel technique (mSelect) that
automates the process of algorithm selection and hyper-parameter tuning for
each use case. Finally, it incorporates a post-detection capability that allows
for faster triaging and root-cause determination. Our extensive experiments
demonstrate that RADF, powered by mSelect, surpasses state-of-the-art anomaly
detection models in AUC performance for 5 out of 9 public benchmarking
datasets. RADF achieved an AUC of over 0.85 for 7 out of 9 datasets, a
distinction unmatched by any other state-of-the-art model.

</details>


### [506] [ResCP: Reservoir Conformal Prediction for Time Series Forecasting](https://arxiv.org/abs/2510.05060)
*Roberto Neglia,Andrea Cini,Michael M. Bronstein,Filippo Maria Bianchi*

Main category: cs.LG

TL;DR: The paper introduces Reservoir Conformal Prediction (ResCP), a training-free method using reservoir computing to generate adaptive conformal prediction intervals for time series, enhancing efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve conformal prediction methods for sequential data, addressing current limitations like poor performance with small sample sizes and the need for expensive model retraining when the data distribution changes.

Method: The method utilizes reservoir computing to dynamically reweight conformity scores based on similarity among reservoir states. This allows adaptation to temporal dynamics while retaining computational efficiency.

Result: ResCP achieves asymptotic conditional coverage under reasonable assumptions and shows strong empirical performance across various forecasting tasks.

Conclusion: ResCP provides a computationally scalable and adaptive solution for building reliable prediction intervals in time series forecasting.

Abstract: Conformal prediction offers a powerful framework for building
distribution-free prediction intervals for exchangeable data. Existing methods
that extend conformal prediction to sequential data rely on fitting a
relatively complex model to capture temporal dependencies. However, these
methods can fail if the sample size is small and often require expensive
retraining when the underlying data distribution changes. To overcome these
limitations, we propose Reservoir Conformal Prediction (ResCP), a novel
training-free conformal prediction method for time series. Our approach
leverages the efficiency and representation learning capabilities of reservoir
computing to dynamically reweight conformity scores. In particular, we compute
similarity scores among reservoir states and use them to adaptively reweight
the observed residuals at each step. With this approach, ResCP enables us to
account for local temporal dynamics when modeling the error distribution
without compromising computational scalability. We prove that, under reasonable
assumptions, ResCP achieves asymptotic conditional coverage, and we empirically
demonstrate its effectiveness across diverse forecasting tasks.

</details>


### [507] [TopInG: Topologically Interpretable Graph Learning via Persistent Rationale Filtration](https://arxiv.org/abs/2510.05102)
*Cheng Xin,Fan Xu,Xin Ding,Jie Gao,Jiaxin Ding*

Main category: cs.LG

TL;DR: The paper introduces TopInG, a novel framework to improve the interpretability of graph neural networks through persistent homology and rationale subgraph generation.


<details>
  <summary>Details</summary>
Motivation: Graph Neural Networks (GNNs) struggle with adoption in critical decision-making tasks due to a lack of interpretability, especially when rationale subgraphs are complex.

Method: The authors use persistent homology to identify rationale subgraphs and propose a rationale filtration learning approach combined with a topological discrepancy constraint to define subgraph distinction and interpretability.

Result: TopInG outperforms existing methods by achieving better predictive accuracy and interpretability, tackling challenges such as spurious correlations and complex rationale subgraphs.

Conclusion: TopInG addresses interpretability issues in GNNs and demonstrates enhanced model performance and understanding, suggesting broader applicability in decision-critical domains.

Abstract: Graph Neural Networks (GNNs) have shown remarkable success across various
scientific fields, yet their adoption in critical decision-making is often
hindered by a lack of interpretability. Recently, intrinsically interpretable
GNNs have been studied to provide insights into model predictions by
identifying rationale substructures in graphs. However, existing methods face
challenges when the underlying rationale subgraphs are complex and varied. In
this work, we propose TopInG: Topologically Interpretable Graph Learning, a
novel topological framework that leverages persistent homology to identify
persistent rationale subgraphs. TopInG employs a rationale filtration learning
approach to model an autoregressive generation process of rationale subgraphs,
and introduces a self-adjusted topological constraint, termed topological
discrepancy, to enforce a persistent topological distinction between rationale
subgraphs and irrelevant counterparts. We provide theoretical guarantees that
our loss function is uniquely optimized by the ground truth under specific
conditions. Extensive experiments demonstrate TopInG's effectiveness in
tackling key challenges, such as handling variform rationale subgraphs,
balancing predictive performance with interpretability, and mitigating spurious
correlations. Results show that our approach improves upon state-of-the-art
methods on both predictive accuracy and interpretation quality.

</details>


### [508] [D2 Actor Critic: Diffusion Actor Meets Distributional Critic](https://arxiv.org/abs/2510.03508)
*Lunjun Zhang,Shuo Han,Hanrui Lyu,Bradly C Stadie*

Main category: cs.LG

TL;DR: The paper introduces D2AC, a model-free reinforcement learning algorithm leveraging diffusion policies and a robust distributional critic for superior performance in RL tasks.


<details>
  <summary>Details</summary>
Motivation: The paper aims to create a reinforcement learning framework with stable learning and reduced policy gradient variance, optimizing performance across diverse and challenging RL tasks.

Method: The approach involves a policy improvement objective alongside a robust distributional critic that utilizes distributional RL and clipped double Q-learning.

Result: D2AC achieves state-of-the-art performance across eighteen challenging RL tasks and demonstrates behavioral robustness in predator-prey simulations.

Conclusion: The proposed framework effectively handles demanding RL benchmarks and generalizes well to complex tasks, showcasing its robustness and adaptability.

Abstract: We introduce D2AC, a new model-free reinforcement learning (RL) algorithm
designed to train expressive diffusion policies online effectively. At its core
is a policy improvement objective that avoids the high variance of typical
policy gradients and the complexity of backpropagation through time. This
stable learning process is critically enabled by our second contribution: a
robust distributional critic, which we design through a fusion of
distributional RL and clipped double Q-learning. The resulting algorithm is
highly effective, achieving state-of-the-art performance on a benchmark of
eighteen hard RL tasks, including Humanoid, Dog, and Shadow Hand domains,
spanning both dense-reward and goal-conditioned RL scenarios. Beyond standard
benchmarks, we also evaluate a biologically motivated predator-prey task to
examine the behavioral robustness and generalization capacity of our approach.

</details>


### [509] [Task-Level Contrastiveness for Cross-Domain Few-Shot Learning](https://arxiv.org/abs/2510.03509)
*Kristi Topollai,Anna Choromanska*

Main category: cs.LG

TL;DR: The paper introduces task-level contrastiveness and contrastive loss to enhance few-shot classification and meta-learning generalization across diverse domains, achieving improved computational efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: Few-shot classification and meta-learning methods struggle with generalization across domains due to their reliance on single datasets and restrictive assumptions.

Method: Introduces task augmentations and a novel task-level contrastive loss to enable unsupervised clustering of task representations, integrated within existing few-shot/meta-learning algorithms.

Result: Experiments on the MetaDataset benchmark show superior performance and efficiency without added complexity or prior knowledge of task domains.

Conclusion: The approach enhances generalization, reduces computational costs, and requires no restrictive assumptions, offering significant improvements to few-shot classification/meta-learning methods.

Abstract: Few-shot classification and meta-learning methods typically struggle to
generalize across diverse domains, as most approaches focus on a single
dataset, failing to transfer knowledge across various seen and unseen domains.
Existing solutions often suffer from low accuracy, high computational costs,
and rely on restrictive assumptions. In this paper, we introduce the notion of
task-level contrastiveness, a novel approach designed to address issues of
existing methods. We start by introducing simple ways to define task
augmentations, and thereafter define a task-level contrastive loss that
encourages unsupervised clustering of task representations. Our method is
lightweight and can be easily integrated within existing few-shot/meta-learning
algorithms while providing significant benefits. Crucially, it leads to
improved generalization and computational efficiency without requiring prior
knowledge of task domains. We demonstrate the effectiveness of our approach
through different experiments on the MetaDataset benchmark, where it achieves
superior performance without additional complexity.

</details>


### [510] [RAPID: An Efficient Reinforcement Learning Algorithm for Small Language Models](https://arxiv.org/abs/2510.03515)
*Lianghuan Huang,Sagnik Anupam,Insup Lee,Shuo Li,Osbert Bastani*

Main category: cs.LG

TL;DR: The paper introduces RAPID, an RL algorithm that reduces training time while maintaining or improving accuracy for small language models (SLMs) on specialized tasks.


<details>
  <summary>Details</summary>
Motivation: Existing RL approaches for fine-tuning small language models are computationally intensive, requiring significant resources and time.

Method: RAPID leverages large-batch inference and mini-batch off-policy updates, enhanced with group advantage estimation and bias-correcting importance weights, to optimize computational efficiency.

Result: RAPID demonstrated 11%-34% reductions in running time across three benchmarks while achieving comparable or better results than state-of-the-art RL algorithms.

Conclusion: RAPID offers a resource-efficient alternative for training SLMs using RL, balancing speed and accuracy effectively.

Abstract: Reinforcement learning (RL) has emerged as a promising strategy for
finetuning small language models (SLMs) to solve targeted tasks such as math
and coding. However, RL algorithms tend to be resource-intensive, taking a
significant amount of time to train. We propose RAPID, a novel RL algorithm
that can substantially reduce the running time of RL. Our key insight is that
RL tends to be costly due to the need to perform both inference and
backpropagation during training. To maximize use of computational resources,
our algorithm performs inference in large batches, and then performs off-policy
policy gradient updates in mini-batches. For off-policy updates, we incorporate
group advantage estimation into the policy gradient algorithm, and derive an
importance weighted estimator to correct for the bias arising from off-policy
learning. Our experiments demonstrate that our algorithm can reduce running
time by 11%-34% on three benchmarks compared to state-of-the-art RL algorithms
while maintaining similar or better accuracy.

</details>


### [511] [Certifiable Safe RLHF: Fixed-Penalty Constraint Optimization for Safer Language Models](https://arxiv.org/abs/2510.03520)
*Kartik Pandit,Sourav Ganguly,Arnesh Banerjee,Shaahin Angizi,Arnob Ghosh*

Main category: cs.LG

TL;DR: This paper introduces Certifiable Safe-RLHF (CS-RLHF), a method to improve safety and efficiency in large language models (LLMs) by incorporating semantically-grounded safety scores and avoiding computational inefficiencies of previous methods.


<details>
  <summary>Details</summary>
Motivation: The paper addresses two major limitations of contemporary CMDP optimization methods used for ensuring the safety of LLMs: dependency on reward/cost functions sensitive to semantic scoring and inefficiencies in tuning dual variables without guarantees against adversarial jailbreaks.

Method: CS-RLHF employs a cost model trained on a large corpus to assign safety scores based on semantic meaning and replaces CMDP's dual-variable tuning process with a rectified penalty-based formulation for direct constraint enforcement.

Result: Empirical evaluations show that CS-RLHF improves response safety and efficiency, achieving a performance that is at least 5 times better against nominal and jailbreak prompts compared to existing methods.

Conclusion: CS-RLHF effectively addresses key limitations in existing CMDP approaches for LLM safety, offering a more computationally efficient and provably safer methodology without compromising response quality.

Abstract: Ensuring safety is a foundational requirement for large language models
(LLMs). Achieving an appropriate balance between enhancing the utility of model
outputs and mitigating their potential for harm is a complex and persistent
challenge. Contemporary approaches frequently formalize this problem within the
framework of Constrained Markov Decision Processes (CMDPs) and employ
established CMDP optimization techniques. However, these methods exhibit two
notable limitations. First, their reliance on reward and cost functions renders
performance highly sensitive to the underlying scoring mechanism, which must
capture semantic meaning rather than being triggered by superficial keywords.
Second, CMDP-based training entails tuning dual-variable, a process that is
both computationally expensive and does not provide any provable safety
guarantee for a fixed dual variable that can be exploitable through adversarial
jailbreaks. To overcome these limitations, we introduce Certifiable Safe-RLHF
(CS-RLHF) that introduces a cost model trained on a large-scale corpus to
assign semantically grounded safety scores. In contrast to the lagrangian-based
approach, CS-RLHF adopts a rectified penalty-based formulation. This design
draws on the theory of exact penalty functions in constrained optimization,
wherein constraint satisfaction is enforced directly through a suitably chosen
penalty term. With an appropriately scaled penalty, feasibility of the safety
constraints can be guaranteed at the optimizer, eliminating the need for
dual-variable updates. Empirical evaluation demonstrates that CS-RLHF
outperforms state-of-the-art LLM model responses rendering at-least 5 times
efficient against nominal and jail-breaking prompts

</details>


### [512] [CrossLag: Predicting Major Dengue Outbreaks with a Domain Knowledge Informed Transformer](https://arxiv.org/abs/2510.03566)
*Ashwin Prabu,Nhat Thanh Tran,Guofa Zhou,Jack Xin*

Main category: cs.LG

TL;DR: The paper introduces "CrossLag," a framework that enhances prediction of major dengue outbreaks using environmentally informed attention and lagged signals in exogenous data applied to transformer models.


<details>
  <summary>Details</summary>
Motivation: Predicting major dengue outbreaks remains a challenge, despite various predictive modeling efforts. Effective forecasting is necessary for timely public warnings.

Method: The proposed "CrossLag" integrates lagging signals into the attention mechanism of transformer architecture to improve dengue outbreak forecasting. It uses TimeXer as a baseline.

Result: CrossLag significantly outperformed TimeXer in detecting and predicting major dengue outbreaks in Singapore data over a 24-week prediction period.

Conclusion: CrossLag demonstrates superior capabilities in enhancing outbreak predictions by leveraging lagged endogenous signals in exogenous data into transformer-based models.

Abstract: A variety of models have been developed to forecast dengue cases to date.
However, it remains a challenge to predict major dengue outbreaks that need
timely public warnings the most. In this paper, we introduce CrossLag, an
environmentally informed attention that allows for the incorporation of lagging
endogenous signals behind the significant events in the exogenous data into the
architecture of the transformer at low parameter counts. Outbreaks typically
lag behind major changes in climate and oceanic anomalies. We use TimeXer, a
recent general-purpose transformer distinguishing exogenous-endogenous inputs,
as the baseline for this study. Our proposed model outperforms TimeXer by a
considerable margin in detecting and predicting major outbreaks in Singapore
dengue data over a 24-week prediction window.

</details>


### [513] [Machine Unlearning Meets Adversarial Robustness via Constrained Interventions on LLMs](https://arxiv.org/abs/2510.03567)
*Fatmazohra Rezkellah,Ramzi Dakhmouche*

Main category: cs.LG

TL;DR: The paper focuses on enhancing LLMs through techniques ensuring privacy and robustness by minimizing intervention on model weights.


<details>
  <summary>Details</summary>
Motivation: With widespread use of LLMs, ensuring privacy and robustness is critical, particularly against sensitive data exposure and vulnerability to jail-breaking attacks.

Method: The study proposes constrained optimization formulations for minimal weight alterations that restrict certain vocabulary or increase robustness, eliminating reliance on oracle classifiers.

Result: Simplistic point-wise constraint interventions showed better results than more complex max-min approaches, and outperformed existing defense methods while reducing computational costs.

Conclusion: The unified constrained approach effectively enhances LLM security and robustness with efficient computational requirements, marking an improvement over prior methods.

Abstract: With the increasing adoption of Large Language Models (LLMs), more
customization is needed to ensure privacy-preserving and safe generation. We
address this objective from two critical aspects: unlearning of sensitive
information and robustness to jail-breaking attacks. We investigate various
constrained optimization formulations that address both aspects in a
\emph{unified manner}, by finding the smallest possible interventions on LLM
weights that either make a given vocabulary set unreachable or embed the LLM
with robustness to tailored attacks by shifting part of the weights to a
\emph{safer} region. Beyond unifying two key properties, this approach
contrasts with previous work in that it doesn't require an oracle classifier
that is typically not available or represents a computational overhead.
Surprisingly, we find that the simplest point-wise constraint-based
intervention we propose leads to better performance than max-min interventions,
while having a lower computational cost. Comparison against state-of-the-art
defense methods demonstrates superior performance of the proposed approach.

</details>


### [514] [Generalization of Graph Neural Network Models for Distribution Grid Fault Detection](https://arxiv.org/abs/2510.03571)
*Burak Karabulut,Carlo Manna,Chris Develder*

Main category: cs.LG

TL;DR: The paper benchmarks different Graph Neural Network (GNN) architectures for fault detection in power grids, showing RGATv2's superior performance and generalization capabilities.


<details>
  <summary>Details</summary>
Motivation: Fault detection in power grids needs to ensure high system reliability and adapt to evolving grid topologies, addressing limitations in current data-driven models.

Method: The study systematically compares GNN architectures in RNN+GNN pipeline models for fault diagnosis, introducing GraphSAGE, GAT, and GATv2 for evaluation.

Result: RGATv2 demonstrated superior performance and generalization with only ~12% F1-score reduction across different topology settings, outperforming RNN and other RGNN models.

Conclusion: RGATv2 provides robust fault detection with better adaptability to grid topology changes, making it suitable for deployment in varied settings.

Abstract: Fault detection in power distribution grids is critical for ensuring system
reliability and preventing costly outages. Moreover, fault detection
methodologies should remain robust to evolving grid topologies caused by
factors such as reconfigurations, equipment failures, and Distributed Energy
Resource (DER) integration. Current data-driven state-of-the-art methods use
Recurrent Neural Networks (RNNs) for temporal modeling and Graph Neural
Networks (GNNs) for spatial learning, in an RNN+GNN pipeline setting (RGNN in
short). Specifically, for power system fault diagnosis, Graph Convolutional
Networks (GCNs) have been adopted. Yet, various more advanced GNN architectures
have been proposed and adopted in domains outside of power systems. In this
paper, we set out to systematically and consistently benchmark various GNN
architectures in an RNN+GNN pipeline model. Specifically, to the best of our
knowledge, we are the first to (i) propose to use GraphSAGE and Graph Attention
(GAT, GATv2) in an RGNN for fault diagnosis, and (ii) provide a comprehensive
benchmark against earlier proposed RGNN solutions (RGCN) as well as pure RNN
models (especially Gated Recurrent Unit (GRU)), particularly (iii) exploring
their generalization potential for deployment in different settings than those
used for training them. Our experimental results on the IEEE 123-node
distribution network show that RGATv2 has superior generalization capabilities,
maintaining high performance with an F1-score reduction of $\sim$12% across
different topology settings. In contrast, pure RNN models largely fail,
experiencing an F1-score reduction of up to $\sim$60%, while other RGNN
variants also exhibit significant performance degradation, i.e., up to
$\sim$25% lower F1-scores.

</details>


### [515] [Efficient Test-Time Scaling for Small Vision-Language Models](https://arxiv.org/abs/2510.03574)
*Mehmet Onurcan Kaya,Desmond Elliott,Dim P. Papadopoulos*

Main category: cs.LG

TL;DR: This paper introduces two efficient strategies to enhance the test-time performance of small Vision-Language Models (VLMs) without compromising computational efficiency.


<details>
  <summary>Details</summary>
Motivation: The study aims to improve the generalization abilities and downstream task performance of small VLMs while preserving their computational efficiency.

Method: Two methods are proposed: (i) Test-Time Augmentation (TTAug) for aggregating outputs without updating parameters, and (ii) Test-Time Adaptation (TTAdapt) for adjusting model parameters using pseudo-labels.

Result: Experiments across nine benchmarks show consistent performance gains while retaining computational efficiency, enhancing utility in constrained resource environments.

Conclusion: The proposed strategies effectively boost small VLMs' test-time capabilities, proving their applicability across various model scales and types without requiring further tuning.

Abstract: Small Vision-Language Models (VLMs) provide a computationally efficient
alternative to larger models, at the cost of weaker generalization abilities
and downstream task performance. These shortcomings could be addressed by
test-time scaling techniques, but existing methods are typically
computationally demanding, contradicting the resource-efficient design goals of
small models. To address these limitations, we propose two novel and efficient
test-time scaling strategies that leverage the model-internal features rather
than external supervision: (i) Test-Time Augmentation (TTAug), which generates
multiple augmented inputs and aggregates outputs at the token level without
parameter updates, and (ii) Test-Time Adaptation (TTAdapt), which adapts model
parameters during inference using consensus-based pseudolabels from TTAug.
Through extensive experiments across nine benchmarks, we demonstrate consistent
performance improvements while maintaining computational efficiency suitable
for resource-constrained environments. The generality of our approach is
demonstrated both within models at different scales and across different VLMs
without additional tuning.

</details>


### [516] [FieldFormer: Physics-Informed Transformers for Spatio-Temporal Field Reconstruction from Sparse Sensors](https://arxiv.org/abs/2510.03589)
*Ankit Bhardwaj,Ananth Balashankar,Lakshminarayanan Subramanian*

Main category: cs.LG

TL;DR: FieldFormer is a transformer-based framework designed for accurate and efficient spatio-temporal field reconstruction using sparse and noisy data, leveraging physics-informed constraints.


<details>
  <summary>Details</summary>
Motivation: To address challenges in spatio-temporal sensor data reconstruction, including sparsity, noise, and irregularity, where existing methods struggle due to lack of scalability or ignorance of underlying PDEs.

Method: FieldFormer employs a transformer-based architecture that uses a velocity-scaled distance metric for neighborhood gathering, expectation-maximization style updates for parameter refinement, local transformer encoder-based predictions, and physics consistency enforced with PDE residuals and boundary penalties.

Result: FieldFormer outperformed existing methods by over 40% across three benchmarks (anisotropic heat equation, shallow-water system, and pollution simulation), achieving highly accurate (RMSE < 10^-2) and physically consistent reconstructions using sparse (0.4%-2%) and noisy (10%) data.

Conclusion: FieldFormer demonstrates that combining physics-based structure with data-driven flexibility enables effective and scalable spatio-temporal field reconstruction under challenging conditions.

Abstract: Spatio-temporal sensor data is often sparse, noisy, and irregular, and
existing interpolation or learning methods struggle here because they either
ignore governing PDEs or do not scale. We introduce FieldFormer, a
transformer-based framework for mesh-free spatio-temporal field reconstruction
that combines data-driven flexibility with physics-based structure. For each
query, FieldFormer gathers a local neighborhood using a learnable
velocity-scaled distance metric, enabling anisotropic adaptation to different
propagation regimes. Neighborhoods are built efficiently via per-batch offset
recomputation, and refined in an expectation-maximization style as the velocity
scales evolve. Predictions are made by a local transformer encoder, and physics
consistency is enforced through autograd-based PDE residuals and
boundary-specific penalties. Across three benchmarks--a scalar anisotropic heat
equation, a vector-valued shallow-water system, and a realistic
advection-diffusion pollution simulation--FieldFormer consistently outperforms
strong baselines by more than 40%. Our results demonstrate that FieldFormer
enables accurate (RMSE$<10^{-2}$), efficient, and physically consistent field
reconstruction from sparse (0.4%-2%) and noisy(10%) data.

</details>


### [517] [Deep Domain Adaptation for Turbofan Engine Remaining Useful Life Prediction: Methodologies, Evaluation and Future Trends](https://arxiv.org/abs/2510.03604)
*Yucheng Wang,Mohamed Ragab,Yubo Hou,Zhenghua Chen,Min Wu,Xiaoli Li*

Main category: cs.LG

TL;DR: This paper reviews domain adaptation (DA) techniques for predicting the Remaining Useful Life (RUL) of turbofan engines, addressing data scarcity and distribution shifts. It introduces a taxonomy and evaluates DA methods on engine datasets.


<details>
  <summary>Details</summary>
Motivation: Limited data and distribution shifts in operating conditions complicate machine learning-based predictive maintenance for turbofan engines.

Method: The paper provides a review, proposes a taxonomy based on methodologies, alignment factors, and challenges, and evaluates DA techniques on turbofan engine datasets.

Result: The taxonomy organizes DA approaches effectively, and the evaluations offer practical insights for improving RUL prediction methods.

Conclusion: This study advances understanding in turbofan RUL prediction with DA, recommending future research directions to enhance techniques further.

Abstract: Remaining Useful Life (RUL) prediction for turbofan engines plays a vital
role in predictive maintenance, ensuring operational safety and efficiency in
aviation. Although data-driven approaches using machine learning and deep
learning have shown potential, they face challenges such as limited data and
distribution shifts caused by varying operating conditions. Domain Adaptation
(DA) has emerged as a promising solution, enabling knowledge transfer from
source domains with abundant data to target domains with scarce data while
mitigating distributional shifts. Given the unique properties of turbofan
engines, such as complex operating conditions, high-dimensional sensor data,
and slower-changing signals, it is essential to conduct a focused review of DA
techniques specifically tailored to turbofan engines. To address this need,
this paper provides a comprehensive review of DA solutions for turbofan engine
RUL prediction, analyzing key methodologies, challenges, and recent
advancements. A novel taxonomy tailored to turbofan engines is introduced,
organizing approaches into methodology-based (how DA is applied),
alignment-based (where distributional shifts occur due to operational
variations), and problem-based (why certain adaptations are needed to address
specific challenges). This taxonomy offers a multidimensional view that goes
beyond traditional classifications by accounting for the distinctive
characteristics of turbofan engine data and the standard process of applying DA
techniques to this area. Additionally, we evaluate selected DA techniques on
turbofan engine datasets, providing practical insights for practitioners and
identifying key challenges. Future research directions are identified to guide
the development of more effective DA techniques, advancing the state of RUL
prediction for turbofan engines.

</details>


### [518] [Predicting Stock Price Movement with LLM-Enhanced Tweet Emotion Analysis](https://arxiv.org/abs/2510.03633)
*An Vuong,Susan Gauch*

Main category: cs.LG

TL;DR: This paper presents a deep learning framework that leverages emotion analysis of tweet data and historical stock prices for predicting significant short-term stock price movements.


<details>
  <summary>Details</summary>
Motivation: Accurate prediction of short-term stock price movement is challenging due to market volatility and the impact of investor sentiment.

Method: The approach integrates emotion features from tweet data with stock price history, utilizing LLaMA 3.1-8B-Instruct for preprocessing and leveraging DistilRoBERTa and NRC lexicon methods for emotion analysis. An LSTM model is trained to forecast price changes.

Result: Experimental results show emotion analysis methods improve prediction accuracy, with the best model (DistilRoBERTa-based) achieving 38.5% accuracy, compared to a baseline of 13.5%.

Conclusion: Integrating emotion features derived from tweets with historical stock data significantly enhances stock price prediction accuracy, validating the utility of large language models in sentiment preprocessing.

Abstract: Accurately predicting short-term stock price movement remains a challenging
task due to the market's inherent volatility and sensitivity to investor
sentiment. This paper discusses a deep learning framework that integrates
emotion features extracted from tweet data with historical stock price
information to forecast significant price changes on the following day. We
utilize Meta's Llama 3.1-8B-Instruct model to preprocess tweet data, thereby
enhancing the quality of emotion features derived from three emotion analysis
approaches: a transformer-based DistilRoBERTa classifier from the Hugging Face
library and two lexicon-based methods using National Research Council Canada
(NRC) resources. These features are combined with previous-day stock price data
to train a Long Short-Term Memory (LSTM) model. Experimental results on TSLA,
AAPL, and AMZN stocks show that all three emotion analysis methods improve the
average accuracy for predicting significant price movements, compared to the
baseline model using only historical stock prices, which yields an accuracy of
13.5%. The DistilRoBERTa-based stock prediction model achieves the best
performance, with accuracy rising from 23.6% to 38.5% when using LLaMA-enhanced
emotion analysis. These results demonstrate that using large language models to
preprocess tweet content enhances the effectiveness of emotion analysis which
in turn improves the accuracy of predicting significant stock price movements.

</details>


### [519] [From Theory to Practice: Evaluating Data Poisoning Attacks and Defenses in In-Context Learning on Social Media Health Discourse](https://arxiv.org/abs/2510.03636)
*Rabeya Amin Jhuma,Mostafa Mohaimen Akand Faisal*

Main category: cs.LG

TL;DR: This paper investigates vulnerabilities of in-context learning (ICL) in large language models to data poisoning in public health sentiment analysis and tests a defense strategy.


<details>
  <summary>Details</summary>
Motivation: To understand the susceptibility of ICL in large language models to data poisoning and propose a defense strategy for robust public health sentiment analysis.

Method: Introduced small adversarial perturbations in support tweet examples and employed the Spectral Signature Defense to filter out poisoned data.

Result: Data poisoning flipped sentiment labels in up to 67% of cases. After applying the Spectral Signature Defense, ICL accuracy stabilized at 46.7%, while logistic regression validation achieved 100% accuracy.

Conclusion: This research demonstrates ICL's vulnerability to data poisoning attacks and validates spectral defenses as a tool to enhance AI robustness in public health sentiment analysis.

Abstract: This study explored how in-context learning (ICL) in large language models
can be disrupted by data poisoning attacks in the setting of public health
sentiment analysis. Using tweets of Human Metapneumovirus (HMPV), small
adversarial perturbations such as synonym replacement, negation insertion, and
randomized perturbation were introduced into the support examples. Even these
minor manipulations caused major disruptions, with sentiment labels flipping in
up to 67% of cases. To address this, a Spectral Signature Defense was applied,
which filtered out poisoned examples while keeping the data's meaning and
sentiment intact. After defense, ICL accuracy remained steady at around 46.7%,
and logistic regression validation reached 100% accuracy, showing that the
defense successfully preserved the dataset's integrity. Overall, the findings
extend prior theoretical studies of ICL poisoning to a practical, high-stakes
setting in public health discourse analysis, highlighting both the risks and
potential defenses for robust LLM deployment. This study also highlights the
fragility of ICL under attack and the value of spectral defenses in making AI
systems more reliable for health-related social media monitoring.

</details>


### [520] [In-Vivo Training for Deep Brain Stimulation](https://arxiv.org/abs/2510.03643)
*Nicholas Carter,Arkaprava Gupta,Prateek Ganguli,Benedikt Dietrich,Vibhor Krishna,Samarjit Chakraborty*

Main category: cs.LG

TL;DR: This paper develops a reinforcement learning (RL)-based Deep Brain Stimulation (DBS) approach that adapts parameters using measurable brain activity, surpassing standard clinical DBS in mitigating Parkinson's Disease (PD) biomarkers.


<details>
  <summary>Details</summary>
Motivation: Existing RL models for DBS rely on biomarkers measurable only in brain-on-chip simulations, limiting their real-world applicability for Parkinson's Disease treatment.

Method: The study employs a TD3-based RL agent trained on a basal ganglia brain model to adapt stimulation parameters based on in vivo brain activity.

Result: The RL-based DBS system achieves superior suppression of Parkinson's Disease biomarkers compared to conventional clinical DBS methods, using real-world measurable data.

Conclusion: The approach showcases the feasibility of training personalized RL-based DBS systems for individual patients, improving treatment effectiveness for Parkinson's Disease.

Abstract: Deep Brain Stimulation (DBS) is a highly effective treatment for Parkinson's
Disease (PD). Recent research uses reinforcement learning (RL) for DBS, with RL
agents modulating the stimulation frequency and amplitude. But, these models
rely on biomarkers that are not measurable in patients and are only present in
brain-on-chip (BoC) simulations. In this work, we present an RL-based DBS
approach that adapts these stimulation parameters according to brain activity
measurable in vivo. Using a TD3 based RL agent trained on a model of the basal
ganglia region of the brain, we see a greater suppression of biomarkers
correlated with PD severity compared to modern clinical DBS implementations.
Our agent outperforms the standard clinical approaches in suppressing PD
biomarkers while relying on information that can be measured in a real world
environment, thereby opening up the possibility of training personalized RL
agents specific to individual patient needs.

</details>


### [521] [SAFA-SNN: Sparsity-Aware On-Device Few-Shot Class-Incremental Learning with Fast-Adaptive Structure of Spiking Neural Network](https://arxiv.org/abs/2510.03648)
*Huijing Zhang,Muyang Cao,Linshan Jiang,Xin Du,Di Yu,Changze Lv,Shuiguang Deng*

Main category: cs.LG

TL;DR: The paper proposes an energy-efficient Spiking Neural Network (SNN) method for on-device few-shot class-incremental learning, termed SAFA-SNN, which mitigates catastrophic forgetting and enhances class discriminability.


<details>
  <summary>Details</summary>
Motivation: Edge devices face challenges in continuously learning new classes with insufficient data, while preserving energy efficiency and data privacy.

Method: The proposed SAFA-SNN introduces sparsity-conditioned neuronal dynamics to stabilize most neurons, employs zeroth-order optimization for gradient estimation, and uses subspace projection to improve class discriminability during incremental learning.

Result: The method achieves at least 4.01% improvement on Mini-ImageNet's last incremental session and reduces energy costs by 20% compared to baseline methods.

Conclusion: SAFA-SNN offers a practical and energy-efficient SNN solution for on-device incremental learning, demonstrating superior performance while being resource-friendly.

Abstract: Continuous learning of novel classes is crucial for edge devices to preserve
data privacy and maintain reliable performance in dynamic environments.
However, the scenario becomes particularly challenging when data samples are
insufficient, requiring on-device few-shot class-incremental learning (FSCIL)
to maintain consistent model performance. Although existing work has explored
parameter-efficient FSCIL frameworks based on artificial neural networks
(ANNs), their deployment is still fundamentally constrained by limited device
resources. Inspired by neural mechanisms, Spiking neural networks (SNNs)
process spatiotemporal information efficiently, offering lower energy
consumption, greater biological plausibility, and compatibility with
neuromorphic hardware than ANNs. In this work, we present an SNN-based method
for On-Device FSCIL, i.e., Sparsity-Aware and Fast Adaptive SNN (SAFA-SNN). We
first propose sparsity-conditioned neuronal dynamics, in which most neurons
remain stable while a subset stays active, thereby mitigating catastrophic
forgetting. To further cope with spike non-differentiability in gradient
estimation, we employ zeroth-order optimization. Moreover, during incremental
learning sessions, we enhance the discriminability of new classes through
subspace projection, which alleviates overfitting to novel classes. Extensive
experiments conducted on two standard benchmark datasets (CIFAR100 and
Mini-ImageNet) and three neuromorphic datasets (CIFAR-10-DVS, DVS128gesture,
and N-Caltech101) demonstrate that SAFA-SNN outperforms baseline methods,
specifically achieving at least 4.01% improvement at the last incremental
session on Mini-ImageNet and 20% lower energy cost over baseline methods with
practical implementation.

</details>


### [522] [Optimising Battery Energy Storage System Trading via Energy Market Operator Price Forecast](https://arxiv.org/abs/2510.03657)
*Aymeric Fabre*

Main category: cs.LG

TL;DR: This paper investigates the use of price forecasts from Australia's National Electricity Market to enhance battery energy storage system (BESS) trading strategies for financial optimization.


<details>
  <summary>Details</summary>
Motivation: The study addresses the gap in utilizing forecast data to improve real-world BESS trading decisions in volatile electricity markets.

Method: The paper analyzes the accuracy of AEMO price forecasts and incorporates this data into a novel trading algorithm. It also explores machine learning to refine forecast predictions for advanced strategies.

Result: The forecast-informed trading algorithm outperformed a basic trading model, demonstrating the benefit of leveraging forecast data.

Conclusion: Incorporating forecast reliability into BESS trading strategies enhances financial returns and could improve energy market models moving forward.

Abstract: In electricity markets around the world, the ability to anticipate price
movements with precision can be the difference between profit and loss,
especially for fast-acting assets like battery energy storage systems (BESS).
As grid volatility increases due to renewables and market decentralisation,
operators and forecasters alike face growing pressure to transform prediction
into strategy. Yet while forecast data is abundant, especially in advanced
markets like Australia's National Electricity Market (NEM), its practical value
in driving real-world BESS trading decisions remains largely unexplored. This
thesis dives into that gap. This work addresses a key research question: Can
the accuracy of the Australian Energy Market Operator (AEMO) energy price
forecasts be systematically leveraged to develop a reliable and profitable
battery energy storage system trading algorithm? Despite the availability of
AEMO price forecasts, no existing framework evaluates their reliability or
incorporates them into practical BESS trading strategies. By analysing patterns
in forecast accuracy based on time of day, forecast horizon, and regional
variations, this project creates a novel, forecast-informed BESS trading model
to optimise arbitrage financial returns. The performance of this
forecast-driven algorithm is benchmarked against a basic trading algorithm with
no knowledge of forecast data. The study further explores the potential of
machine learning techniques to predict future energy prices by enhancing AEMO
forecasts to govern a more advanced trading strategy. The research outcomes
will inform future improvements in energy market trading models and promote
more efficient BESS integration into market operations.

</details>


### [523] [Operationalizing Data Minimization for Privacy-Preserving LLM Prompting](https://arxiv.org/abs/2510.03662)
*Jijie Zhou,Niloofar Mireshghallah,Tianshi Li*

Main category: cs.LG

TL;DR: The paper introduces a framework to minimize privacy risks in interactions with large language models (LLMs) by quantifying the least privacy-revealing data needed for effective responses.


<details>
  <summary>Details</summary>
Motivation: With the rise of LLMs in consumer applications, users often provide unnecessary personal information, raising privacy concerns such as memorization, personalization risks, and security threats.

Method: The authors defined and operationalized a concept of data minimization and developed a priority-queue tree search to identify the least privacy-revealing data for user prompts within a privacy-ordered transformation space. They evaluated the method using datasets involving both open-ended conversations and knowledge-intensive tasks.

Result: The study found that more advanced LLMs achieved stronger data minimization while preserving task quality compared to smaller models (e.g., GPT-5 with 85.7% redaction vs. Qwen2.5-0.5B with 19.3%). However, LLMs struggled to predict optimal minimization without external assistance, favoring oversharing due to an abstraction bias.

Conclusion: LLMs show a privacy and capability gap, as they are not fully aware of the minimal information required for task completion. Larger models handle stricter privacy requirements more effectively than smaller ones.

Abstract: The rapid deployment of large language models (LLMs) in consumer applications
has led to frequent exchanges of personal information. To obtain useful
responses, users often share more than necessary, increasing privacy risks via
memorization, context-based personalization, or security breaches. We present a
framework to formally define and operationalize data minimization: for a given
user prompt and response model, quantifying the least privacy-revealing
disclosure that maintains utility, and we propose a priority-queue tree search
to locate this optimal point within a privacy-ordered transformation space. We
evaluated the framework on four datasets spanning open-ended conversations
(ShareGPT, WildChat) and knowledge-intensive tasks with single-ground-truth
answers (CaseHold, MedQA), quantifying achievable data minimization with nine
LLMs as the response model. Our results demonstrate that larger frontier LLMs
can tolerate stronger data minimization while maintaining task quality than
smaller open-source models (85.7% redaction for GPT-5 vs. 19.3% for
Qwen2.5-0.5B). By comparing with our search-derived benchmarks, we find that
LLMs struggle to predict optimal data minimization directly, showing a bias
toward abstraction that leads to oversharing. This suggests not just a privacy
gap, but a capability gap: models may lack awareness of what information they
actually need to solve a task.

</details>


### [524] [Token Hidden Reward: Steering Exploration-Exploitation in Group Relative Deep Reinforcement Learning](https://arxiv.org/abs/2510.03669)
*Wenlong Deng,Yi Ren,Yushu Li,Boying Gong,Danica J. Sutherland,Xiaoxiao Li,Christos Thrampoulidis*

Main category: cs.LG

TL;DR: The paper introduces Token Hidden Reward (THR), a metric to manage exploration and exploitation during reinforcement learning for large language models. THR-guided training improves reasoning tasks, significantly enhancing both greedy decoding and probabilistic exploration.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of explicitly steering training in reinforcement learning from exploration (diverse output generation) to exploitation (confidence in correct answers), as this remains an unaddressed problem in the fine-tuning of large language models.

Method: The authors propose Token Hidden Reward (THR), a token-level metric used in conjunction with Group Relative Policy Optimization (GRPO). They develop a THR-guided reweighting algorithm to modulate learning signals for either exploration or exploitation by amplifying positive THR (favoring correct answers) and diminishing negative THR values (preserving alternative outputs).

Result: The THR-guided reweighting algorithm improved greedy-decoding accuracy when focusing on exploitation and enhanced Pass@K accuracy when prioritizing exploration. It consistently worked across different architectures like Llama and integrated well with other RL objectives like GSPO.

Conclusion: THR provides a fine-grained and dynamic mechanism for steering the balance between exploration and exploitation in training reinforcement-learned large language models, offering pivotal advantages for reasoning-centric applications and further advancements in RL fine-tuning approaches.

Abstract: Reinforcement learning with verifiable rewards has significantly advanced the
reasoning capabilities of large language models, yet how to explicitly steer
training toward exploration or exploitation remains an open problem. We
introduce Token Hidden Reward (THR), a token-level metric that quantifies each
token's influence on the likelihood of correct responses under Group Relative
Policy Optimization (GRPO). We find that training dynamics are dominated by a
small subset of tokens with high absolute THR values. Most interestingly,
tokens with positive THR strengthen confidence in correct outputs, thus
favoring exploitation, while tokens with negative THR preserve probability mass
for alternative outputs, enabling exploration. This insight suggests a natural
intervention: a THR-guided reweighting algorithm that modulates GRPO's learning
signals to explicitly bias training toward exploitation or exploration. We
validate the efficacy of this algorithm on diverse math reasoning benchmarks.
By amplifying tokens with positive THR value and weakening negative ones, our
algorithm improves greedy-decoding accuracy, favoring exploitation. The reverse
strategy yields consistent gains in Pass@K accuracy, favoring exploration. We
further demonstrate that our algorithm integrates seamlessly with other RL
objectives such as GSPO and generalizes across architectures including Llama.
These findings establish THR as a principled and fine-grained mechanism for
dynamically controlling exploration and exploitation in RL-tuned LLMs,
providing new tools for targeted fine-tuning in reasoning-intensive
applications.

</details>


### [525] [REG: A Regularization Optimizer for Robust Training Dynamics](https://arxiv.org/abs/2510.03691)
*Zehua Liu,Han Wu,Xiaojin Fu,Shuqi Liu,Xiongwei Han,Tao Zhong,Mingxuan Yuan*

Main category: cs.LG

TL;DR: Introducing the REG optimizer, a novel method for training Large Language Models (LLMs) that offers enhanced performance and stability compared to AdamW and eliminates training instability issues tied to the Muon optimizer.


<details>
  <summary>Details</summary>
Motivation: To address limitations of Muon optimizer—instability due to the matrix sign function and incompatibility with fine-tuning models pre-trained with AdamW.

Method: Proposed the REG optimizer, which replaces Muon's matrix sign operation with Row-and-Column-Scaling (RACS), a theoretically grounded operator aiding in balanced matrix updates and ensuring compatibility with conventional training practices.

Result: Experiments showcase that REG outperforms AdamW in training stability and performance, while avoiding fine-tuning issues seen with Muon.

Conclusion: REG is presented as a simpler, robust solution for LLM training, maintaining consistency with AdamW’s training dynamics and overcoming issues faced by Muon during fine-tuning.

Abstract: Optimizers are crucial for the efficient training of Large Language Models
(LLMs). While AdamW is the de facto standard, recent structure-aware optimizers
like Muon have emerged, which regularize gradient updates by operating on
entire weight matrices. The Muon optimizer balances the gradient updates along
all the directions. However, Muon's reliance on the matrix sign function can
lead to training instability, exhibits incompatibility when fine-tuning models
pre-trained with AdamW. To address these limitations, we propose \textbf{REG},
a novel optimizer that replaces Muon's aggressive matrix sign operator with the
Row-and-Column-Scaling (RACS) operator. Theoretically grounded in balancing a
matrix, the RACS operator regularizes the update steps in a less drastic
manner, making it simpler to implement and more compatible with established
training dynamics. Through extensive empirical experiments on LLM training, we
demonstrate that our REG optimizer not only achieves superior performance and
stability over AdamW, but also maintains consistency with the AdamW training
paradigm. This consistency is particularly evident during the fine-tuning
stage, where REG optimizer avoids the performance degradation observed with
Muon.

</details>


### [526] [Personalized federated prototype learning in mixed heterogeneous data scenarios](https://arxiv.org/abs/2510.03726)
*Jiahao Zeng,Wolong Xing,Liangtao Shi,Xin Huang,Jialin Wang,Zhile Cao,Zhenkui Shi*

Main category: cs.LG

TL;DR: The paper introduces PFPL, a federated learning approach to handle data heterogeneity in mixed scenarios, using personalized unbiased prototypes and consistent regularization.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of skewed feature or label distributions in federated learning and leverage data heterogeneity for enhanced model performance.

Method: Introducing PFPL, creating personalized unbiased prototypes for clients, and employing consistent regularization to align local instances with prototypes during local updates.

Result: The method shows effectiveness on Digits and Office Caltech datasets and successfully reduces communication costs.

Conclusion: PFPL improves convergence, leverages data heterogeneity, and achieves better model performance while lowering communication overhead.

Abstract: Federated learning has received significant attention for its ability to
simultaneously protect customer privacy and leverage distributed data from
multiple devices for model training. However, conventional approaches often
focus on isolated heterogeneous scenarios, resulting in skewed feature
distributions or label distributions. Meanwhile, data heterogeneity is actually
a key factor in improving model performance. To address this issue, we propose
a new approach called PFPL in mixed heterogeneous scenarios. The method
provides richer domain knowledge and unbiased convergence targets by
constructing personalized, unbiased prototypes for each client. Moreover, in
the local update phase, we introduce consistent regularization to align local
instances with their personalized prototypes, which significantly improves the
convergence of the loss function. Experimental results on Digits and Office
Caltech datasets validate the effectiveness of our approach and successfully
reduce the communication cost.

</details>


### [527] [Optimizing Fine-Tuning through Advanced Initialization Strategies for Low-Rank Adaptation](https://arxiv.org/abs/2510.03731)
*Yongfu Xue*

Main category: cs.LG

TL;DR: This paper introduces IniLoRA, an enhanced initialization strategy improving upon LoRA by initializing low-rank matrices to better leverage original model weights.


<details>
  <summary>Details</summary>
Motivation: LoRA is efficient but limited in activating original model weights, prompting the need for more effective initialization techniques.

Method: IniLoRA uses a novel initialization strategy and introduces two variants, IniLoRA-α and IniLoRA-β, based on distinct initialization methods.

Result: IniLoRA outperformed LoRA across various models and tasks as shown in experimental results.

Conclusion: IniLoRA improves parameter efficiency while enhancing large language model adaptation, addressing shortcomings of LoRA's initialization strategy.

Abstract: The rapid development of parameter-efficient fine-tuning methods has
noticeably improved the efficiency of adapting large language models. Among
these, LoRA has gained widespread popularity due to its strong balance of
effectiveness and parameter efficiency. However, LoRA relies on initializing
two low-rank matrices whose product is zero, which limits its ability to
effectively activate and leverage the original model weights-creating a
potential bottleneck for optimal performance. To address this limitation, we
propose \textbf{IniLoRA}, a novel initialization strategy that initializes the
low-rank matrices to closely approximate the original model weights.
Experimental results indicate that IniLoRA achieves better performance than
LoRA across a range of models and tasks. Additionally, we introduce two
variants, IniLoRA-$\alpha$ and IniLoRA-$\beta$, both leveraging distinct
initialization methods to enhance performance further.

</details>


### [528] [Neural Low-Discrepancy Sequences](https://arxiv.org/abs/2510.03745)
*Michael Etienne Van Huffel,Nathan Kirk,Makram Chahine,Daniela Rus,T. Konstantin Rusch*

Main category: cs.LG

TL;DR: This paper introduces Neural Low-Discrepancy Sequences ($NeuroLDS$), the first machine learning-based framework for generating low-discrepancy sequences, significantly improving discrepancy measures and proving effective across various applications like numerical integration and robot motion planning.


<details>
  <summary>Details</summary>
Motivation: Previous methods for constructing low-discrepancy points rely on abstract algebra and number theory, which limit the ability to generate low-discrepancy sequences (LDS) essential for applications requiring uniform prefix distribution.

Method: The authors propose $NeuroLDS$, a machine learning framework that employs a two-stage training process: supervised learning for approximating classical LDS methods and unsupervised fine-tuning to reduce prefix discrepancy.

Result: $NeuroLDS$ achieves lower discrepancies across all prefixes than prior constructions, outperforming classical LDS methods. It also demonstrates effectiveness in applications like numerical integration, robot motion planning, and scientific machine learning.

Conclusion: $NeuroLDS$ establishes machine learning for generating low-discrepancy sequences as a promising alternative to traditional methods, broadening its application potential and enhancing performance in diverse fields.

Abstract: Low-discrepancy points are designed to efficiently fill the space in a
uniform manner. This uniformity is highly advantageous in many problems in
science and engineering, including in numerical integration, computer vision,
machine perception, computer graphics, machine learning, and simulation.
Whereas most previous low-discrepancy constructions rely on abstract algebra
and number theory, Message-Passing Monte Carlo (MPMC) was recently introduced
to exploit machine learning methods for generating point sets with lower
discrepancy than previously possible. However, MPMC is limited to generating
point sets and cannot be extended to low-discrepancy sequences (LDS), i.e.,
sequences of points in which every prefix has low discrepancy, a property
essential for many applications. To address this limitation, we introduce
Neural Low-Discrepancy Sequences ($NeuroLDS$), the first machine learning-based
framework for generating LDS. Drawing inspiration from classical LDS, we train
a neural network to map indices to points such that the resulting sequences
exhibit minimal discrepancy across all prefixes. To this end, we deploy a
two-stage learning process: supervised approximation of classical constructions
followed by unsupervised fine-tuning to minimize prefix discrepancies. We
demonstrate that $NeuroLDS$ outperforms all previous LDS constructions by a
significant margin with respect to discrepancy measures. Moreover, we
demonstrate the effectiveness of $NeuroLDS$ across diverse applications,
including numerical integration, robot motion planning, and scientific machine
learning. These results highlight the promise and broad significance of Neural
Low-Discrepancy Sequences. Our code can be found at
https://github.com/camail-official/neuro-lds.

</details>


### [529] [EvoEngineer: Mastering Automated CUDA Kernel Code Evolution with Large Language Models](https://arxiv.org/abs/2510.03760)
*Ping Guo,Chenyu Zhu,Siyuan Chen,Fei Liu,Xi Lin,Zhichao Lu,Qingfu Zhang*

Main category: cs.LG

TL;DR: The paper introduces EvoEngineer, a systematic LLM-based framework for CUDA kernel optimization that outperforms existing methods in performance and correctness.


<details>
  <summary>Details</summary>
Motivation: To address the fragmented ecosystem and unclear methods in CUDA kernel optimization and to meet high correctness standards required for AI performance.

Method: Formalize kernel optimization as a code optimization task with defined objectives, constraints, and metrics. Develop EvoEngineer, a framework for LLM-guided code evolution balancing performance and correctness.

Result: EvoEngineer achieves a 2.72x average median speedup, 36.75x maximum speedup, a validity rate of 69.8%, and highest performance in 56% of operations tested.

Conclusion: EvoEngineer effectively balances performance and correctness in CUDA kernel optimization, outperforming existing methodologies and demonstrating its potential in real-world AI scenarios.

Abstract: CUDA kernel optimization has become a critical bottleneck for AI performance,
as deep learning training and inference efficiency directly depends on highly
optimized GPU kernels.
  Despite the promise of Large Language Models (LLMs) for automating kernel
optimization, this field suffers from a fragmented ecosystem of isolated and
incomparable approaches with unclear problem formulations.
  Furthermore, general-purpose LLM code evolution methods cannot meet strict
correctness requirements of CUDA kernel optimization.
  We address these fundamental challenges by first formalizing CUDA kernel
optimization as a code optimization task with a clear objective, constraints,
and evaluation metrics.
  We then establish the first systematic LLM-based code evolution framework,
EvoEngineer, that provides guidance for designing and adapting optimization
strategies to achieve a balance between performance and correctness.
  Finally, we implement a kernel optimization system based on this framework
and conduct extensive experiments on 91 real-world CUDA kernels.
  Our results demonstrate that EvoEngineer achieves a principled balance
between performance and correctness, with the highest averaged median speedup
of \textbf{2.72}$\times$ over baseline CUDA kernels and a code validity rate of
\textbf{69.8}\%, outperforming existing methods on both dimensions.
  Our method achieves a maximum speedup of \textbf{36.75}$\times$ among all
operations over PyTorch kernels and delivers the highest speedup on \textbf{28}
(\textbf{56.0\%}) of 50 operations that achieve over \textbf{2$\times$}
acceleration.

</details>


### [530] [Merge and Guide: Unifying Model Merging and Guided Decoding for Controllable Multi-Objective Generation](https://arxiv.org/abs/2510.03782)
*Guofu Xie,Chen Zhang,Xiao Zhang,Yunsheng Shi,Ting Yao,Jun Xu*

Main category: cs.LG

TL;DR: The paper presents MAGE, a two-stage framework designed to address challenges in controllable multi-objective generation by combining model merging with guided decoding for improved controllability and adaptability.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenges of adapting to diverse user needs in controllable multi-objective generation, where existing methods either provide indirect control at a parameter level or aggregate logits inefficiently from multiple models.

Method: MAGE involves a two-stage process: (1) merging backbone models to create a robust base model that accounts for multiple objectives, and (2) merging explicit and implicit value models into a unified guidance proxy to steer the decoding of the base model.

Result: MAGE demonstrates superior controllability, Pareto-optimal performance, and adaptability, validated through experiments and empirical analysis, including the use of Linear Mode Connectivity in value models.

Conclusion: The MAGE framework effectively combines model merging and guided decoding, addressing limitations in previous methods and offering a robust solution for controllable multi-objective generation.

Abstract: Adapting to diverse user needs at test time is a key challenge in
controllable multi-objective generation. Existing methods are insufficient:
merging-based approaches provide indirect, suboptimal control at the parameter
level, often disregarding the impacts of multiple objectives. While
decoding-based guidance is more direct, it typically requires aggregating
logits from multiple expert models, incurring significant space overhead and
relying heavily on individual model capacity. To address these issues, we
introduce Merge-And-GuidE (MAGE), a two-stage framework that leverages model
merging for guided decoding. We first identify a critical compatibility problem
between the guidance and base models. In Stage 1, MAGE resolves this by
dynamically constructing a more robust base model, merging a series of backbone
models that account for multiple objectives. In Stage 2, we merge explicit and
implicit value models into a unified guidance proxy, which then steers the
decoding of the base model from Stage 1. Our analysis empirically validates
Linear Mode Connectivity (LMC) in value models, explores the relationship
between model merging and prediction ensembling, and demonstrates the enhanced
controllability afforded by our approach. Extensive experiments show that our
method outperforms existing approaches, achieving superior controllability,
Pareto-optimal performance, and enhanced adaptability.

</details>


### [531] [Curriculum-Augmented GFlowNets For mRNA Sequence Generation](https://arxiv.org/abs/2510.03811)
*Aya Laajil,Abduragim Shtanchaev,Sajan Muhammad,Eric Moulines,Salem Lahlou*

Main category: cs.LG

TL;DR: This paper proposes Curriculum-Augmented GFlowNets (CAGFN) to efficiently design mRNA sequences with improved handling of trade-offs between biological objectives and complex reward structures.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the problem of designing mRNA sequences with optimized properties like stability, translation efficiency, and protein expression. Despite promises from Generative Flow Networks (GFlowNets), existing methods struggle with sparse rewards, long-horizon objectives, and multi-objective trade-offs.

Method: CAGFN integrates curriculum learning into GFlowNets, progressively working with longer sequences using a length-based curriculum. A new mRNA design environment tailored for GFlowNets is also provided, targeting multiple biological metrics and enhancing the training process.

Result: CAGFN achieves better Pareto performance, produces biologically plausible sequences, and maintains diversity. It outperforms traditional GFlowNets in both speed and solution quality across various mRNA design tasks and generalizes well to out-of-distribution sequences.

Conclusion: Integrating curriculum learning into GFlowNets is effective for mRNA design, improving both performance and efficiency in generating high-quality and diverse sequences for therapeutic applications.

Abstract: Designing mRNA sequences is a major challenge in developing next-generation
therapeutics, since it involves exploring a vast space of possible nucleotide
combinations while optimizing sequence properties like stability, translation
efficiency, and protein expression. While Generative Flow Networks are
promising for this task, their training is hindered by sparse, long-horizon
rewards and multi-objective trade-offs. We propose Curriculum-Augmented
GFlowNets (CAGFN), which integrate curriculum learning with multi-objective
GFlowNets to generate de novo mRNA sequences. CAGFN integrates a length-based
curriculum that progressively adapts the maximum sequence length guiding
exploration from easier to harder subproblems. We also provide a new mRNA
design environment for GFlowNets which, given a target protein sequence and a
combination of biological objectives, allows for the training of models that
generate plausible mRNA candidates. This provides a biologically motivated
setting for applying and advancing GFlowNets in therapeutic sequence design. On
different mRNA design tasks, CAGFN improves Pareto performance and biological
plausibility, while maintaining diversity. Moreover, CAGFN reaches
higher-quality solutions faster than a GFlowNet trained with random sequence
sampling (no curriculum), and enables generalization to out-of-distribution
sequences.

</details>


### [532] [Detecting Invariant Manifolds in ReLU-Based RNNs](https://arxiv.org/abs/2510.03814)
*Lukas Eisenmann,Alena Brändle,Zahra Monfared,Daniel Durstewitz*

Main category: cs.LG

TL;DR: This paper introduces an algorithm to analyze the state space dynamics of piecewise-linear RNNs with ReLU activation, focusing on stable and unstable manifolds and their intersections to detect multistability and chaos.


<details>
  <summary>Details</summary>
Motivation: Understanding trained RNN behaviors is critical for applications like scientific analysis, medical diagnostics, and explainable AI.

Method: The proposed algorithm analyzes the stable and unstable manifolds of periodic points in PLRNN state space to identify boundaries between basins of attraction and detect intersection points indicating chaotic dynamics.

Result: The algorithm enables detailed tracing of attraction basins, identification of homoclinic points, and demonstrates chaos existence in PLRNNs. Insights are applied to cortical neuron dynamics.

Conclusion: This method provides crucial tools for studying RNN dynamics by revealing key geometrical features and chaotic behaviors, aiding applications in computational neuroscience and AI interpretability.

Abstract: Recurrent Neural Networks (RNNs) have found widespread applications in
machine learning for time series prediction and dynamical systems
reconstruction, and experienced a recent renaissance with improved training
algorithms and architectural designs. Understanding why and how trained RNNs
produce their behavior is important for scientific and medical applications,
and explainable AI more generally. An RNN's dynamical repertoire depends on the
topological and geometrical properties of its state space. Stable and unstable
manifolds of periodic points play a particularly important role: They dissect a
dynamical system's state space into different basins of attraction, and their
intersections lead to chaotic dynamics with fractal geometry. Here we introduce
a novel algorithm for detecting these manifolds, with a focus on
piecewise-linear RNNs (PLRNNs) employing rectified linear units (ReLUs) as
their activation function. We demonstrate how the algorithm can be used to
trace the boundaries between different basins of attraction, and hence to
characterize multistability, a computationally important property. We further
show its utility in finding so-called homoclinic points, the intersections
between stable and unstable manifolds, and thus establish the existence of
chaos in PLRNNs. Finally we show for an empirical example, electrophysiological
recordings from a cortical neuron, how insights into the underlying dynamics
could be gained through our method.

</details>


### [533] [On Using Large Language Models to Enhance Clinically-Driven Missing Data Recovery Algorithms in Electronic Health Records](https://arxiv.org/abs/2510.03844)
*Sarah C. Lotspeich,Abbey Collins,Brian J. Wells,Ashish K. Khanna,Joseph Rigdon,Lucy D'Agostino McGowan*

Main category: cs.LG

TL;DR: This study evaluates an LLM-enhanced algorithm to recover missing EHR data based on ICD-10 codes and compares its effectiveness to expert chart reviews.


<details>
  <summary>Details</summary>
Motivation: EHR data is often plagued by missing values and errors, but manual chart reviews are costly and time-intensive, necessitating scalable and automated recovery methods.

Method: The study used various roadmap versions, created with clinician input and LLM refinements, to develop an algorithm and applied it to both 100-patient and 1000-patient datasets.

Result: The algorithm matched or exceeded the data recovery capabilities of expert chart reviewers, depending on the roadmap used.

Conclusion: LLM-enhanced clinically-driven algorithms can accurately recover missing EHR data at scale and show potential for addressing other dimensions of data quality.

Abstract: Objective: Electronic health records (EHR) data are prone to missingness and
errors. Previously, we devised an "enriched" chart review protocol where a
"roadmap" of auxiliary diagnoses (anchors) was used to recover missing values
in EHR data (e.g., a diagnosis of impaired glycemic control might imply that a
missing hemoglobin A1c value would be considered unhealthy). Still, chart
reviews are expensive and time-intensive, which limits the number of patients
whose data can be reviewed. Now, we investigate the accuracy and scalability of
a roadmap-driven algorithm, based on ICD-10 codes (International Classification
of Diseases, 10th revision), to mimic expert chart reviews and recover missing
values. Materials and Methods: In addition to the clinicians' original roadmap
from our previous work, we consider new versions that were iteratively refined
using large language models (LLM) in conjunction with clinical expertise to
expand the list of auxiliary diagnoses. Using chart reviews for 100 patients
from the EHR at an extensive learning health system, we examine algorithm
performance with different roadmaps. Using the larger study of $1000$ patients,
we applied the final algorithm, which used a roadmap with clinician-approved
additions from the LLM. Results: The algorithm recovered as much, if not more,
missing data as the expert chart reviewers, depending on the roadmap.
Discussion: Clinically-driven algorithms (enhanced by LLM) can recover missing
EHR data with similar accuracy to chart reviews and can feasibly be applied to
large samples. Extending them to monitor other dimensions of data quality
(e.g., plausability) is a promising future direction.

</details>


### [534] [Unlocking Reasoning Capabilities in LLMs via Reinforcement Learning Exploration](https://arxiv.org/abs/2510.03865)
*Wenhao Deng,Long Wei,Chenglei Yu,Tailin Wu*

Main category: cs.LG

TL;DR: This paper addresses the limitations of Reinforcement Learning with Verifiable Rewards (RLVR) in reasoning tasks caused by the reverse KL divergence regularizer. It introduces RAPO, a new training algorithm that enhances policy exploration and improves problem-solving performance beyond base model capabilities.


<details>
  <summary>Details</summary>
Motivation: The authors aim to overcome the issue where RLVR-trained models lose their advantage over base models as the sampling budget increases, owing to restrictive exploration caused by the reverse KL divergence regularizer.

Method: The paper proposes RAPO, a novel algorithm that replaces the reverse KL penalty with a forward KL penalty for broader exploration and reweights the reference policy for adaptive in-distribution exploration.

Result: RAPO-trained Qwen2.5-3B and 7B models on the 8K SimpleRL-Zero dataset showed consistent improvements in solving complex reasoning tasks, surpassing the base models and handling problems previously unsolvable.

Conclusion: RAPO enables models to break through the performance limitations imposed by base models, advancing the state-of-the-art in RLVR for reasoning tasks.

Abstract: Reinforcement learning with verifiable rewards (RLVR) has recently enhanced
the reasoning capabilities of large language models (LLMs), particularly for
mathematical problem solving. However, a fundamental limitation remains: as the
sampling budget increases, the advantage of RLVR-trained models over their
pretrained bases often diminishes or even vanishes, revealing a strong
dependence on the base model's restricted search space. We attribute this
phenomenon to the widespread use of the reverse Kullback-Leibler (KL)
divergence regularizer, whose mode-seeking behavior keeps the policy trapped
inside the base model's support region and hampers wider exploration. To
address this issue, we propose RAPO (Rewards-Aware Policy Optimization), an
algorithm to promote broader yet focused exploration. Our method (i) utilizes
the forward KL penalty to replace the reverse KL penalty for
out-of-distribution exploration, and (ii) reweights the reference policy to
facilitate adaptive in-distribution exploration. We train Qwen2.5-3B and 7B
models with RAPO on the 8K SimpleRL-Zero dataset, without supervised
fine-tuning, and evaluate them on AIME2024 and AIME2025. Results show that RAPO
consistently improves problem-solving performance. Notably, RAPO enables models
to surpass the base model's performance ceiling and solves previously
intractable problems, advancing the frontier of RLVR for challenging reasoning
tasks.

</details>


### [535] [LLM Chemistry Estimation for Multi-LLM Recommendation](https://arxiv.org/abs/2510.03930)
*Huascar Sanchez,Briland Hitaj*

Main category: cs.LG

TL;DR: The paper introduces the LLM Chemistry framework to analyze how combinations of multiple Large Language Models interact (synergistically or antagonistically) to improve collective performance.


<details>
  <summary>Details</summary>
Motivation: Existing multi-LLM collaboration methods lack mechanisms to assess whether the models complement or conflict in their outputs or processes.

Method: The authors formalize the concept of 'chemistry' among LLMs, propose algorithms to measure model interaction dependencies, and analyze theoretical aspects under different conditions, such as heterogeneous profiles and task complexities.

Result: The evaluation on tasks such as classification, summarization, and program repair confirms that LLM chemistry impacts collective performance and is influenced by task type, group size, and complexity.

Conclusion: LLM Chemistry is proposed as a critical diagnostic tool and a basis for recommending the optimal ensemble configurations in multi-LLM systems.

Abstract: Multi-LLM collaboration promises accurate, robust, and context-aware
solutions, yet existing approaches rely on implicit selection and output
assessment without analyzing whether collaborating models truly complement or
conflict. We introduce LLM Chemistry -- a framework that measures when LLM
combinations exhibit synergistic or antagonistic behaviors that shape
collective performance beyond individual capabilities. We formalize the notion
of chemistry among LLMs, propose algorithms that quantify it by analyzing
interaction dependencies, and recommend optimal model ensembles accordingly.
Our theoretical analysis shows that chemistry among collaborating LLMs is most
evident under heterogeneous model profiles, with its outcome impact shaped by
task type, group size, and complexity. Evaluation on classification,
summarization, and program repair tasks provides initial evidence for these
task-dependent effects, thereby reinforcing our theoretical results. This
establishes LLM Chemistry as both a diagnostic factor in multi-LLM systems and
a foundation for ensemble recommendation.

</details>


### [536] [BONSAI: Structure-exploiting robust Bayesian optimization for networked black-box systems under uncertainty](https://arxiv.org/abs/2510.03893)
*Akshay Kudva,Joel A. Paulson*

Main category: cs.LG

TL;DR: BONSAI, a new robust Bayesian optimization (RBO) framework, efficiently solves uncertainty-aware design challenges by leveraging partial structural knowledge of models.


<details>
  <summary>Details</summary>
Motivation: To address limitations of traditional robust optimization methods and existing RBO frameworks that struggle with black-box settings and high-dimensional scalability in engineering systems.

Method: BONSAI integrates partial structural knowledge via graph-based component representation and employs a Thompson sampling-based acquisition function optimized with gradient-based methods.

Result: BONSAI exhibited improved sample efficiency and quality of robust solutions across diverse synthetic and real-world engineering case studies.

Conclusion: BONSAI is a practical advancement in simulation-based robust optimization, suitable for high-fidelity engineering design under uncertainty.

Abstract: Optimal design under uncertainty remains a fundamental challenge in advancing
reliable, next-generation process systems. Robust optimization (RO) offers a
principled approach by safeguarding against worst-case scenarios across a range
of uncertain parameters. However, traditional RO methods typically require
known problem structure, which limits their applicability to high-fidelity
simulation environments. To overcome these limitations, recent work has
explored robust Bayesian optimization (RBO) as a flexible alternative that can
accommodate expensive, black-box objectives. Existing RBO methods, however,
generally ignore available structural information and struggle to scale to
high-dimensional settings. In this work, we introduce BONSAI (Bayesian
Optimization of Network Systems under uncertAInty), a new RBO framework that
leverages partial structural knowledge commonly available in simulation-based
models. Instead of treating the objective as a monolithic black box, BONSAI
represents it as a directed graph of interconnected white- and black-box
components, allowing the algorithm to utilize intermediate information within
the optimization process. We further propose a scalable Thompson sampling-based
acquisition function tailored to the structured RO setting, which can be
efficiently optimized using gradient-based methods. We evaluate BONSAI across a
diverse set of synthetic and real-world case studies, including applications in
process systems engineering. Compared to existing simulation-based RO
algorithms, BONSAI consistently delivers more sample-efficient and
higher-quality robust solutions, highlighting its practical advantages for
uncertainty-aware design in complex engineering systems.

</details>


### [537] [LLM as an Algorithmist: Enhancing Anomaly Detectors via Programmatic Synthesis](https://arxiv.org/abs/2510.03904)
*Hangting Ye,Jinmeng Li,He Zhao,Mingchen Zhuge,Dandan Guo,Yi Chang,Hongyuan Zha*

Main category: cs.LG

TL;DR: LLM-DAS is a framework that uses Large Language Models (LLMs) as "algorithmists" to enhance the robustness of anomaly detectors by generating code to synthesize challenging anomalies.


<details>
  <summary>Details</summary>
Motivation: Current anomaly detection methods struggle in real-world scenarios due to reliance on predefined assumptions. LLMs show reasoning capabilities but face challenges in processing heterogeneous tabular data and privacy risks when applied directly.

Method: LLM-DAS uses LLMs to analyze a high-level description of anomaly detectors, identify their weaknesses, and generate detector-specific Python code to create anomalies that exploit these weaknesses. This synthesis program augments training data to improve detector robustness.

Result: Experiments on 36 tabular anomaly detection benchmarks demonstrate that LLM-DAS consistently improves the performance of existing anomaly detection methods.

Conclusion: LLM-DAS effectively bridges LLM reasoning and classic anomaly detection techniques, offering a scalable, privacy-preserving way to improve detector performance by addressing their logical blind spots.

Abstract: Existing anomaly detection (AD) methods for tabular data usually rely on some
assumptions about anomaly patterns, leading to inconsistent performance in
real-world scenarios. While Large Language Models (LLMs) show remarkable
reasoning capabilities, their direct application to tabular AD is impeded by
fundamental challenges, including difficulties in processing heterogeneous data
and significant privacy risks. To address these limitations, we propose
LLM-DAS, a novel framework that repositions the LLM from a ``data processor''
to an ``algorithmist''. Instead of being exposed to raw data, our framework
leverages the LLM's ability to reason about algorithms. It analyzes a
high-level description of a given detector to understand its intrinsic
weaknesses and then generates detector-specific, data-agnostic Python code to
synthesize ``hard-to-detect'' anomalies that exploit these vulnerabilities.
This generated synthesis program, which is reusable across diverse datasets, is
then instantiated to augment training data, systematically enhancing the
detector's robustness by transforming the problem into a more discriminative
two-class classification task. Extensive experiments on 36 TAD benchmarks show
that LLM-DAS consistently boosts the performance of mainstream detectors. By
bridging LLM reasoning with classic AD algorithms via programmatic synthesis,
LLM-DAS offers a scalable, effective, and privacy-preserving approach to
patching the logical blind spots of existing detectors.

</details>


### [538] [THEMIS: Unlocking Pretrained Knowledge with Foundation Model Embeddings for Anomaly Detection in Time Series](https://arxiv.org/abs/2510.03911)
*Yadav Mahesh Lorik,Kaushik Sarveswaran,Nagaraj Sundaramahalingam,Aravindakumar Venugopalan*

Main category: cs.LG

TL;DR: The paper introduces THEMIS, a framework for time series anomaly detection using pretrained knowledge from foundation models. It outperforms specialized models and achieves SOTA results on certain datasets.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the complexities of time series anomaly detection, which include seasonality, trends, noise, evolving patterns, dimensionality, and interpretability challenges.

Method: THEMIS uses embeddings from the Chronos foundation model's encoder and detects anomalies through techniques like Local Outlier Factor and Spectral Decomposition applied to the self-similarity matrix.

Result: The approach achieves state-of-the-art results on the MSL dataset and competitive performance on SMAP and SWAT$^*$ datasets, surpassing specialized anomaly detection models.

Conclusion: The paper concludes that pretrained representations from foundation models enable efficient, robust, and interpretable anomaly detection for time series data.

Abstract: Time series anomaly detection forms a very crucial area in several domains
but poses substantial challenges. Due to time series data possessing
seasonality, trends, noise, and evolving patterns (concept drift), it becomes
very difficult to set a general notion of what constitutes normal behavior.
Anomalies themselves could be varied, ranging from a single outlier to
contextual or collective anomalies, and are normally very rare; hence, the
dataset is largely imbalanced. Additional layers of complexities arise due to
the problems of increased dimensionality of modern time series, real-time
detection criteria, setting up appropriate detection thresholds, and arriving
at results that are interpretable. To embrace these multifaceted challenges,
very strong, flexible, and interpretable approaches are required. This paper
presents THEMIS, a new framework for time series anomaly detection that
exploits pretrained knowledge from foundation models. THEMIS extracts
embeddings from the encoder of the Chronos time series foundation model and
applies outlier detection techniques like Local Outlier Factor and Spectral
Decomposition on the self-similarity matrix, to spot anomalies in the data. Our
experiments show that this modular method achieves SOTA results on the MSL
dataset and performs quite competitively on the SMAP and SWAT$^*$ datasets.
Notably, THEMIS exceeds models trained specifically for anomaly detection,
presenting hyperparameter robustness and interpretability by default. This
paper advocates for pretrained representations from foundation models for
performing efficient and adaptable anomaly detection for time series data.

</details>


### [539] [Generalized Fitted Q-Iteration with Clustered Data](https://arxiv.org/abs/2510.03912)
*Liyuan Hu,Jitao Wang,Zhenke Wu,Chengchun Shi*

Main category: cs.LG

TL;DR: This study introduces a generalized fitted Q-iteration (FQI) algorithm to deal with reinforcement learning on clustered data, achieving better results in healthcare-related tasks.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of intra-cluster correlations in reinforcement learning, which is prevalent in healthcare applications.

Method: A generalized fitted Q-iteration (FQI) algorithm is developed by integrating generalized estimating equations to handle intra-cluster correlations.

Result: The proposed algorithm achieves optimal and consistent estimators theoretically, and it reduces regret by half compared to standard FQI in simulations and healthcare data applications.

Conclusion: Generalized FQI enhances reinforcement learning performance on clustered data, offering significant improvement over standard methods.

Abstract: This paper focuses on reinforcement learning (RL) with clustered data, which
is commonly encountered in healthcare applications. We propose a generalized
fitted Q-iteration (FQI) algorithm that incorporates generalized estimating
equations into policy learning to handle the intra-cluster correlations.
Theoretically, we demonstrate (i) the optimalities of our Q-function and policy
estimators when the correlation structure is correctly specified, and (ii)
their consistencies when the structure is mis-specified. Empirically, through
simulations and analyses of a mobile health dataset, we find the proposed
generalized FQI achieves, on average, a half reduction in regret compared to
the standard FQI.

</details>


### [540] [Transductive and Learning-Augmented Online Regression](https://arxiv.org/abs/2510.03917)
*Vinod Raman,Shenghao Xie,Samson Zhou*

Main category: cs.LG

TL;DR: The paper focuses on online regression leveraging predictions about future examples, characterizing minimax expected regret and developing methods that improve performance based on prediction quality.


<details>
  <summary>Details</summary>
Motivation: To explore how predictions about future data can enhance the effectiveness of online regression and distinguish between predictable and adversarial scenarios.

Method: The authors characterize minimax expected regret in transductive online learning using fat-shattering dimension and extend this to noisy predictions, developing an adaptive online learner.

Result: The learner achieves minimax expected regret, improves with prediction accuracy, and performs comparably to transductive models when predictions are precise.

Conclusion: Predictions about future examples can enable learning for otherwise unlearnable classes, integrating with the learning-augmented model paradigm.

Abstract: Motivated by the predictable nature of real-life in data streams, we study
online regression when the learner has access to predictions about future
examples. In the extreme case, called transductive online learning, the
sequence of examples is revealed to the learner before the game begins. For
this setting, we fully characterize the minimax expected regret in terms of the
fat-shattering dimension, establishing a separation between transductive online
regression and (adversarial) online regression. Then, we generalize this
setting by allowing for noisy or \emph{imperfect} predictions about future
examples. Using our results for the transductive online setting, we develop an
online learner whose minimax expected regret matches the worst-case regret,
improves smoothly with prediction quality, and significantly outperforms the
worst-case regret when future example predictions are precise, achieving
performance similar to the transductive online learner. This enables
learnability for previously unlearnable classes under predictable examples,
aligning with the broader learning-augmented model paradigm.

</details>


### [541] [Principled and Tractable RL for Reasoning with Diffusion Language Models](https://arxiv.org/abs/2510.04019)
*Anthony Zhan*

Main category: cs.LG

TL;DR: The paper introduces AGRPO, a new on-policy RL algorithm tailored for diffusion-based large language models (dLLMs), achieving significant performance gains over existing models and methods.


<details>
  <summary>Details</summary>
Motivation: To enhance the post-training of diffusion large language models (dLLMs) using reinforcement learning (RL) methods, which have proven effective for autoregressive models but face compatibility challenges with diffusion frameworks.

Method: Introduced Amortized Group Relative Policy Optimization (AGRPO), an RL algorithm that uses Monte Carlo sampling for unbiased policy gradient estimation, specifically designed for dLLMs.

Result: AGRPO showed better performance on reasoning tasks such as GSM8K and Countdown, including up to +7.6% improvement on GSM8K and 1.3x gains over comparable RL methods.

Conclusion: AGRPO successfully adapts online RL algorithms for dLLMs in a principled way, achieving theoretical soundness and practical performance improvements.

Abstract: Diffusion large language models (dLLMs) are a new paradigm of
non-autoregressive language models that are trained to predict multiple tokens
in parallel and generate text via iterative unmasking. Recent works have
successfully pretrained dLLMs to parity with autoregressive LLMs at the 8B
scale, but dLLMs have yet to benefit from modern post-training techniques, e.g.
reinforcement learning (RL), that have proven effective for autoregressive
models. Crucially, algorithms designed for traditional LLMs aren't directly
compatible with diffusion frameworks due to inherent differences in modeling
assumptions. Moreover, existing attempts at dLLM post-training with RL rely on
heuristic-based objectives with no theoretical grounding. In this work, we
present Amortized Group Relative Policy Optimization (AGRPO), a principled
on-policy RL algorithm designed specifically for dLLMs. AGRPO uses Monte Carlo
sampling to compute an unbiased policy gradient estimate, making it the first
tractable, faithful adaptation of policy gradient methods for dLLMs. We
demonstrate AGRPO's effectiveness on different math/reasoning tasks, a common
setting for RL with LLMs, achieving up to +7.6% absolute gain on GSM8K and 3.8x
performance on the Countdown task over the baseline LLaDA-8B-Instruct model and
1.3x performance gains over comparable RL methods such as diffu-GRPO.
Furthermore, these gains persist across different numbers of sampling steps at
inference time, achieving better tradeoffs between compute and performance. Our
results demonstrate that online RL algorithms can be extended to diffusion LLMs
in principled ways, maintaining both theoretical soundness and practical
effectiveness.

</details>


### [542] [On the Convergence and Size Transferability of Continuous-depth Graph Neural Networks](https://arxiv.org/abs/2510.03923)
*Mingsong Yan,Charles Kulick,Sui Tang*

Main category: cs.LG

TL;DR: This paper introduces Graphon Neural Differential Equations (Graphon-NDEs) as the infinite-node limit of Graph Neural Differential Equations (GNDEs), providing rigorous convergence analysis and size transferability of GNDEs.


<details>
  <summary>Details</summary>
Motivation: The study aims to enhance the scalability and theoretical understanding of GNDEs, a framework that integrates graph structural properties with continuous-depth architectures, by addressing their convergence and size transferability in the infinite-node limit.

Method: The authors employ graphon theory and dynamical systems to establish the well-posedness of Graphon-NDEs, prove trajectory-wise convergence of GNDEs to Graphon-NDEs, derive explicit convergence rates under specific graph sampling regimes, and analyze size transferability bounds. Numerical experiments validate the theoretical findings.

Result: The paper demonstrates the trajectory-wise convergence of GNDE solutions to their infinite-node counterparts (Graphon-NDEs), with explicit convergence rates for different graph sampling regimes, and confirms the practical benefits of size transferability through numerical experiments.

Conclusion: Graphon-NDEs offer rigorous theoretical grounding for GNDEs, justifying their scalability and transferability when applied to larger but structurally similar graphs, which is further supported by empirical data.

Abstract: Continuous-depth graph neural networks, also known as Graph Neural
Differential Equations (GNDEs), combine the structural inductive bias of Graph
Neural Networks (GNNs) with the continuous-depth architecture of Neural ODEs,
offering a scalable and principled framework for modeling dynamics on graphs.
In this paper, we present a rigorous convergence analysis of GNDEs with
time-varying parameters in the infinite-node limit, providing theoretical
insights into their size transferability. To this end, we introduce Graphon
Neural Differential Equations (Graphon-NDEs) as the infinite-node limit of
GNDEs and establish their well-posedness. Leveraging tools from graphon theory
and dynamical systems, we prove the trajectory-wise convergence of GNDE
solutions to Graphon-NDE solutions. Moreover, we derive explicit convergence
rates under two deterministic graph sampling regimes: (1) weighted graphs
sampled from smooth graphons, and (2) unweighted graphs sampled from
$\{0,1\}$-valued (discontinuous) graphons. We further establish size
transferability bounds, providing theoretical justification for the practical
strategy of transferring GNDE models trained on moderate-sized graphs to
larger, structurally similar graphs without retraining. Numerical experiments
using synthetic and real data support our theoretical findings.

</details>


### [543] [What Scales in Cross-Entropy Scaling Law?](https://arxiv.org/abs/2510.04067)
*Junxi Yan,Zixi Wei,Jingtao Zhan,Qingyao Ai,Yiqun Liu*

Main category: cs.LG

TL;DR: The paper challenges the traditional cross-entropy scaling law's accuracy at large-language model scales and proposes a refined decomposition to better understand training dynamics.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the breakdown of cross-entropy scaling law at very large language model scales, causing hurdles in development and necessitating better theoretical understanding.

Method: The authors introduce a decomposition of cross-entropy into three parts: Error-Entropy, Self-Alignment, and Confidence. They validate this using theoretical insights and empirical tests across 32 models of varying sizes and multiple datasets.

Result: It was found that only Error-Entropy scales predictably with model size, while the other components remain invariant. Further, Error-Entropy dominates small models but reduces in significance in larger models.

Conclusion: A new Error-Entropy based scaling law offers better predictive power for training dynamics and will aid in the advancement of large language models.

Abstract: The cross-entropy scaling law has long served as a key tool for guiding the
development of large language models. It shows that cross-entropy loss
decreases in a predictable power-law rate as the model size increases. However,
recent evidence indicates that this law breaks down at very large scales: the
loss decreases more slowly than expected, which causes significant trouble for
developing large language models. In this paper, we hypothesize that the root
cause lies in the fact that cross-entropy itself does not truly scale; instead,
only one of its hidden components does. To investigate this, we introduce a
novel decomposition of cross-entropy into three parts: Error-Entropy,
Self-Alignment, and Confidence. We show both theoretically and empirically that
this decomposition precisely captures the training dynamics and optimization
objectives. Through extensive experiments on multiple datasets and 32 models
spanning five orders of magnitude in size, we find that only error-entropy
follows a robust power-law scaling, while the other two terms remain largely
invariant. Moreover, error-entropy constitutes the dominant share of
cross-entropy in small models but diminishes in proportion as models grow
larger. This explains why the cross-entropy scaling law appears accurate at
small scales but fails at very large ones. Our findings establish the
error-entropy scaling law as a more accurate description of model behavior. We
believe it will have wide applications in the training, understanding, and
future development of large language models.

</details>


### [544] [On the Empirical Power of Goodness-of-Fit Tests in Watermark Detection](https://arxiv.org/abs/2510.03944)
*Weiqing He,Xiang Li,Tianqi Shang,Li Shen,Weijie Su,Qi Long*

Main category: cs.LG

TL;DR: This paper explores the application of goodness-of-fit (GoF) tests in detecting statistical watermarks embedded in large language models (LLM)-generated text, showing their potential to improve detection power and robustness.


<details>
  <summary>Details</summary>
Motivation: The concern is to address authenticity and integrity issues in text generated by large language models, as they can efficiently produce human-like text at scale. Text watermarks are a viable method to verify content origin, but detection techniques require further exploration.

Method: The authors systematically evaluate eight GoF tests across three watermarking schemes, utilizing three open-source LLMs, two datasets, different generation temperatures, and various post-editing methods.

Result: The analysis reveals that GoF tests enhance the detection capabilities and robustness for watermark detectors. Text repetition at low-temperature settings provides a distinct advantage for GoF tests compared to existing methods.

Conclusion: Classic GoF tests, though underutilized, are effective tools for watermark detection in LLM-generated text and can significantly improve detection systems.

Abstract: Large language models (LLMs) raise concerns about content authenticity and
integrity because they can generate human-like text at scale. Text watermarks,
which embed detectable statistical signals into generated text, offer a
provable way to verify content origin. Many detection methods rely on pivotal
statistics that are i.i.d. under human-written text, making goodness-of-fit
(GoF) tests a natural tool for watermark detection. However, GoF tests remain
largely underexplored in this setting. In this paper, we systematically
evaluate eight GoF tests across three popular watermarking schemes, using three
open-source LLMs, two datasets, various generation temperatures, and multiple
post-editing methods. We find that general GoF tests can improve both the
detection power and robustness of watermark detectors. Notably, we observe that
text repetition, common in low-temperature settings, gives GoF tests a unique
advantage not exploited by existing methods. Our results highlight that classic
GoF tests are a simple yet powerful and underused tool for watermark detection
in LLMs.

</details>


### [545] [What Is The Performance Ceiling of My Classifier? Utilizing Category-Wise Influence Functions for Pareto Frontier Analysis](https://arxiv.org/abs/2510.03950)
*Shahriar Kabir Nahin,Wenxiao Xiao,Joshua Liu,Anshuman Chhabra,Hongfu Liu*

Main category: cs.LG

TL;DR: The paper investigates tools and methods to determine and achieve the performance ceiling for machine learning models while ensuring category-specific accuracy improvements without trade-offs.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of current studies focusing mainly on overall accuracy improvements, the paper aims to explore the performance ceiling of a learning model on a category-wise basis and ensure improvements benefit all classes consistently.

Method: The authors propose category-wise influence functions and introduce an influence vector to quantify each training sample's impact across all categories. A linear programming-based sample reweighting framework is developed to achieve Pareto performance improvements.

Result: Experiments on synthetic datasets and vision and text benchmarks showed that the proposed approach effectively estimates and improves model performance across all defined categories.

Conclusion: The study provides a framework and tools to analyze and enhance the performance ceiling of machine learning models, improving category-wise accuracy in a balanced manner.

Abstract: Data-centric learning seeks to improve model performance from the perspective
of data quality, and has been drawing increasing attention in the machine
learning community. Among its key tools, influence functions provide a powerful
framework to quantify the impact of individual training samples on model
predictions, enabling practitioners to identify detrimental samples and retrain
models on a cleaner dataset for improved performance. However, most existing
work focuses on the question: "what data benefits the learning model?" In this
paper, we take a step further and investigate a more fundamental question:
"what is the performance ceiling of the learning model?" Unlike prior studies
that primarily measure improvement through overall accuracy, we emphasize
category-wise accuracy and aim for Pareto improvements, ensuring that every
class benefits, rather than allowing tradeoffs where some classes improve at
the expense of others. To address this challenge, we propose category-wise
influence functions and introduce an influence vector that quantifies the
impact of each training sample across all categories. Leveraging these
influence vectors, we develop a principled criterion to determine whether a
model can still be improved, and further design a linear programming-based
sample reweighting framework to achieve Pareto performance improvements.
Through extensive experiments on synthetic datasets, vision, and text
benchmarks, we demonstrate the effectiveness of our approach in estimating and
achieving a model's performance improvement across multiple categories of
interest.

</details>


### [546] [Optimizing Resources for On-the-Fly Label Estimation with Multiple Unknown Medical Experts](https://arxiv.org/abs/2510.03954)
*Tim Bary,Tiffanie Godelaine,Axel Abels,Benoît Macq*

Main category: cs.LG

TL;DR: The paper introduces a novel adaptive algorithm for real-time medical data annotation that reduces expert involvement by up to 50% without compromising accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing algorithms in medical screening that cannot efficiently handle real-time continuous annotations and operate without pre-labeled data or expert understanding.

Method: The proposed method incrementally gathers expert opinions, dynamically queries additional experts based on case difficulty, and reaches a confidence threshold for accurate labeling.

Result: The approach successfully decreases expert queries by up to 50% while maintaining comparable accuracy across three multi-annotator classification datasets in different modalities.

Conclusion: The adaptive querying strategy enhances medical screening workflows by reducing annotation overhead and providing accurate ground truth labels in real-time scenarios.

Abstract: Accurate ground truth estimation in medical screening programs often relies
on coalitions of experts and peer second opinions. Algorithms that efficiently
aggregate noisy annotations can enhance screening workflows, particularly when
data arrive continuously and expert proficiency is initially unknown. However,
existing algorithms do not meet the requirements for seamless integration into
screening pipelines. We therefore propose an adaptive approach for real-time
annotation that (I) supports on-the-fly labeling of incoming data, (II)
operates without prior knowledge of medical experts or pre-labeled data, and
(III) dynamically queries additional experts based on the latent difficulty of
each instance. The method incrementally gathers expert opinions until a
confidence threshold is met, providing accurate labels with reduced annotation
overhead. We evaluate our approach on three multi-annotator classification
datasets across different modalities. Results show that our adaptive querying
strategy reduces the number of expert queries by up to 50% while achieving
accuracy comparable to a non-adaptive baseline. Our code is available at
https://github.com/tbary/MEDICS

</details>


### [547] [Early-Warning of Thunderstorm-Driven Power Outages with a Two-Stage Machine Learning Model](https://arxiv.org/abs/2510.03959)
*Iryna Stanishevska*

Main category: cs.LG

TL;DR: The paper introduces a 24-48 hour early-warning model for forecasting thunderstorm-related power outages in Michigan using publicly available data.


<details>
  <summary>Details</summary>
Motivation: Thunderstorm outages are difficult to predict due to rapid, chaotic processes and noisy datasets, leading to a need for actionable early-warning systems based on open-source data.

Method: The model leverages data from public sources (EAGLE-I for outages and METAR for weather), employing a two-stage approach—a logistic gate and an LSTM regressor—along with feature engineering techniques like kriging and spatio-temporal analysis.

Result: The two-stage model outperforms the LSTM baseline in detecting thunderstorm peaks, showing improved detection rates (F1 66.7% vs. 57.1%) and better performance near peaks (+/-0-12 h) despite noise in open datasets.

Conclusion: The study demonstrates that feature-driven pipelines can provide actionable, event-centric early warnings for thunderstorm-related outages, even in noisy open-data environments.

Abstract: Thunderstorm-driven outages are difficult to predict because most storms do
not cause damage, convective processes occur rapidly and chaotically, and the
available public data are both noisy and incomplete. We develop a 24-48 h
early-warning model for summer, thunderstorm-related outages in Michigan using
only open sources (EAGLE-I for ground truth; METAR for weather). We use the
publicly released EAGLE-I outage dataset (2014-2022), maintained by Oak Ridge
National Laboratory for the U.S. Department of Energy. The pipeline preserves
convective micro-signals from a sparse station network via parameter-specific
kriging with hourly variograms and targeted overdrafting to retain extremes,
and builds causal spatio-temporal features (lags/rolling statistics; k-NN/IDW
spatial aggregates) capturing precursors of severe convection (moisture
advection, wind shifts, and pressure drops). The two-stage model design,
combining a logistic gate and an LSTM regressor, limits routine periods and
reduces noise exposure. The study uses event-centric metrics (cluster-based
hits/misses/false alarms) and peak-conditional MASE (cMASE) in +/-Delta-hour
windows around state-level peaks (>= 50,000), with uncertainty quantified by
hourly moving-block bootstrap.
  On the test sample, Two-Stage detects more reference peaks across all windows
(e.g., at +/-48 h it records 3/4 vs. 2/4; F1 66.7% vs. 57.1%) with one extra
false alarm. Near peaks, it shows modest amplitude gains (2-3% lower cMASE at
+/-0-12 h; bootstrap medians +9-13% at +/-6-12 h) but small losses at +/-36-48
h (~3-4%). Overall, errors are comparable to the one-step LSTM baseline. SHAP
analysis confirms moisture-advection and wind/gust precursors, underscoring the
value of the feature engineering. Despite open-data noise, the feature-driven
pipeline yields actionable, event-focused early warnings for thunderstorm
outages.

</details>


### [548] [SPEAR: Soft Prompt Enhanced Anomaly Recognition for Time Series Data](https://arxiv.org/abs/2510.03962)
*Hanzhe Wei,Jiajun Wu,Jialin Yang,Henry Leung,Steve Drew*

Main category: cs.LG

TL;DR: The paper introduces SPEAR, leveraging large language models (LLMs) with soft prompts and quantization for improved time series anomaly detection.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address challenges in traditional anomaly detection methods, especially with variable-length sequences and context-based anomalies, by exploiting the capabilities of LLMs.

Method: Time series data is transformed into embeddings, combined with learnable soft prompts, and fed into a frozen LLM, iteratively updating prompts using cross-entropy loss.

Result: Experimental results show significant improvement in LLMs' performance for time series anomaly detection tasks using soft prompts.

Conclusion: SPEAR effectively adapts LLMs for handling time series anomaly detection, leveraging the benefits of soft prompts and quantization.

Abstract: Time series anomaly detection plays a crucial role in a wide range of fields,
such as healthcare and internet traffic monitoring. The emergence of large
language models (LLMs) offers new opportunities for detecting anomalies in the
ubiquitous time series data. Traditional approaches struggle with
variable-length time series sequences and context-based anomalies. We propose
Soft Prompt Enhanced Anomaly Recognition (SPEAR), a novel approach to leverage
LLMs for anomaly detection with soft prompts and quantization. Our methodology
involves quantizing and transforming the time series data into input embeddings
and combining them with learnable soft prompt embeddings. These combined
embeddings are then fed into a frozen LLM. The soft prompts are updated
iteratively based on a cross-entropy loss, allowing the model to adapt to time
series anomaly detection. The use of soft prompts helps adapt LLMs effectively
to time series tasks, while quantization ensures optimal handling of sequences,
as LLMs are designed to handle discrete sequences. Our experimental results
demonstrate that soft prompts effectively increase LLMs' performance in
downstream tasks regarding time series anomaly detection.

</details>


### [549] [Beyond Next-Token Prediction: A Performance Characterization of Diffusion versus Autoregressive Language Models](https://arxiv.org/abs/2510.04146)
*Minseo Kim,Coleman Hooper,Aditya Tomar,Chenfeng Xu,Mehrdad Farajtabar,Michael W. Mahoney,Kurt Keutzer,Amir Gholami*

Main category: cs.LG

TL;DR: This paper compares Autoregressive Language Models (ARMs) and Diffusion Language Models (DLMs) in terms of performance, analyzing computation intensity, scalability, and opportunities for optimization.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the performance bottlenecks faced by Autoregressive Language Models (ARMs) due to their sequential decoding approach and evaluates Diffusion Language Models (DLMs) as an alternative for more efficient text generation.

Method: The researchers conducted both theoretical analyses and profiling experiments to examine performance characteristics of ARMs and DLMs. They further explored DLMs enhanced by block-wise decoding and investigated latency improvements.

Result: DLMs were found to have higher arithmetic intensity due to parallel decoding but struggled with scaling to longer contexts. By incorporating block-wise decoding, DLMs improved scalability. However, ARMs still showed superior throughput for batched inference.

Conclusion: While DLMs present promising alternatives with potential optimization opportunities, ARMs remain more effective in specific scenarios, such as batched inference. Reducing sampling steps is vital for improving DLM latency.

Abstract: Large Language Models (LLMs) have achieved state-of-the-art performance on a
broad range of Natural Language Processing (NLP) tasks, including document
processing and coding. Autoregressive Language Models (ARMs), which generate
tokens sequentially conditioned on all previous tokens, have been the
predominant paradigm for LLMs. However, while these networks have achieved high
accuracy across a range of downstream tasks, they exhibit low arithmetic
intensity due to the inherent sequential dependency with next-token prediction.
Recently, Diffusion Language Models (DLMs) have emerged as a promising
alternative architecture. DLMs generate output text in parallel, breaking the
limitations of sequential dependency. However, the performance implications of
DLMs relative to commonly deployed ARMs are not fully understood. In this work,
we present a comprehensive performance study analyzing the performance
characteristics of ARMs and DLMs, using both theoretical analysis and profiling
data to characterize the trade-offs between these approaches. We illustrate
that although DLMs exhibit higher arithmetic intensity compared to ARMs because
of their capability to utilize parallelism across sequence lengths, they fail
to scale effectively to longer contexts. We then explore DLMs with block-wise
decoding, outlining how this approach allows for increased arithmetic
intensity, while still scaling well to long contexts (similar to ARMs). We also
show interesting trade-offs for batched inference, where we find that ARMs
exhibit superior throughput, as they benefit more from parallelism across
sequences in the batch. Finally, we highlight opportunities for accelerating
DLM inference, and, in particular, highlight the importance of reducing the
number of sampling steps for allowing open-source DLMs to provide improved
latency relative to ARMs.

</details>


### [550] [What Can You Do When You Have Zero Rewards During RL?](https://arxiv.org/abs/2510.03971)
*Jatin Prakash,Anirudh Buvanesh*

Main category: cs.LG

TL;DR: The paper addresses the zero-reward problem in reinforcement learning for large language models (LLMs) by using data-centric interventions instead of modifying RL methods, showing that simpler training examples help solve harder tasks.


<details>
  <summary>Details</summary>
Motivation: To overcome the zero-reward barrier in reinforcement learning scenarios where base models fail to generate correct solutions, and to enhance LLM performance on reasoning tasks.

Method: The authors evaluated recent RL methods and introduced a simple data-centric intervention by adding easier samples to the training set, rather than modifying RL algorithms.

Result: None of the tested RL methods overcame the zero-reward problem, but adding easier examples allows LLMs to eventually solve harder tasks. Detailed failure mode analysis was performed for the methods tested.

Conclusion: Adding simpler samples to the training data can break the zero-reward barrier in RL without modifying the algorithm, offering an effective solution to enhance reasoning abilities in LLMs.

Abstract: Reinforcement learning (RL) with outcome-based rewards has proven effective
for improving large language models (LLMs) on complex reasoning tasks. However,
its success often depends on the base model occasionally sampling correct
solutions. When no correct solutions are sampled, training encounters a
zero-reward barrier where learning stalls due to zero gradients. We study this
scenario through the graph search task introduced in Bachmann et al. (2024) and
evaluate recent methods that incorporate desirable components such as dense
rewards, diversity incentives, and improved credit assignment. Our experiments
show that none of these approaches overcome the zero-reward barrier if the base
model never produces a correct answer. In contrast, we find that a simple
data-centric intervention of adding easier samples to the training set enables
the model to eventually solve the original hard task despite starting from zero
reward. Importantly, this succeeds without modifying the RL algorithm itself.
Because official implementations of several baselines were unavailable, we
developed our own, which allowed us to conduct a detailed analysis of their
failure modes. We release these implementations to support further research at:
https://github.com/rl4reasoning/rl-baselines

</details>


### [551] [Beyond Softmax: A New Perspective on Gradient Bandits](https://arxiv.org/abs/2510.03979)
*Emerson Melo,David Müller*

Main category: cs.LG

TL;DR: This paper connects discrete choice models to multi-armed bandits and introduces novel algorithms, showing sublinear regret bounds and improving model flexibility with correlated learning dynamics.


<details>
  <summary>Details</summary>
Motivation: The paper aims to bridge the gap between discrete choice theory and online learning, enhancing bandit algorithms by addressing restrictive modeling assumptions like independence in softmax.

Method: The authors propose new adversarial and gradient bandit algorithms based on generalized nested logit models while relaxing independence assumptions and ensuring computational efficiency.

Result: Numerical experiments in stochastic bandit settings indicate the proposed algorithms offer practical effectiveness with improved regret bounds.

Conclusion: The developed framework widens the scope of bandit methods by introducing flexible and efficient algorithmic classes applicable to diverse learning environments.

Abstract: We establish a link between a class of discrete choice models and the theory
of online learning and multi-armed bandits. Our contributions are: (i)
sublinear regret bounds for a broad algorithmic family, encompassing Exp3 as a
special case; (ii) a new class of adversarial bandit algorithms derived from
generalized nested logit models \citep{wen:2001}; and (iii)
\textcolor{black}{we introduce a novel class of generalized gradient bandit
algorithms that extends beyond the widely used softmax formulation. By relaxing
the restrictive independence assumptions inherent in softmax, our framework
accommodates correlated learning dynamics across actions, thereby broadening
the applicability of gradient bandit methods.} Overall, the proposed algorithms
combine flexible model specification with computational efficiency via
closed-form sampling probabilities. Numerical experiments in stochastic bandit
settings demonstrate their practical effectiveness.

</details>


### [552] [ICEPool: Enhancing Graph Pooling Networks with Inter-cluster Connectivity](https://arxiv.org/abs/2510.03987)
*Michael Yang*

Main category: cs.LG

TL;DR: ICEPool is a novel hierarchical pooling framework for graph neural networks (GNNs) focusing on inter-cluster connectivity and enhancing structural integrity in graph data.


<details>
  <summary>Details</summary>
Motivation: Existing hierarchical pooling models for graph classification often overlook inter-cluster relationships, necessitating improved methods for preserving structural integrity.

Method: ICEPool introduces an enhancement mechanism that integrates with existing pooling-based GNN models to emphasize inter-cluster connectivity and graph-level representation.

Result: Theoretical analysis confirms ICEPool's effectiveness in graph reconstruction, and experiments demonstrate its compatibility and ability to improve performance across various GNN architectures.

Conclusion: ICEPool strengthens existing GNN models by addressing overlooked inter-cluster relationships, enhancing graph-level representations, and boosting performance.

Abstract: Hierarchical Pooling Models have demonstrated strong performance in
classifying graph-structured data. While numerous innovative methods have been
proposed to design cluster assignments and coarsening strategies, the
relationships between clusters are often overlooked. In this paper, we
introduce Inter-cluster Connectivity Enhancement Pooling (ICEPool), a novel
hierarchical pooling framework designed to enhance model's understanding of
inter-cluster connectivity and ability of preserving the structural integrity
in the original graph. ICEPool is compatible with a wide range of pooling-based
GNN models. The deployment of ICEPool as an enhancement to existing models
effectively combines the strengths of the original model with ICEPool's
capability to emphasize the integration of inter-cluster connectivity,
resulting in a more comprehensive and robust graph-level representation.
Moreover, we make theoretical analysis to ICEPool's ability of graph
reconstruction to demonstrate its effectiveness in learning inter-cluster
relationship that is overlooked by conventional models. Finally, the
experimental results show the compatibility of ICEPool with wide varieties of
models and its potential to boost the performance of existing graph neural
network architectures.

</details>


### [553] [Wave-PDE Nets: Trainable Wave-Equation Layers as an Alternative to Attention](https://arxiv.org/abs/2510.04304)
*Harshil Vejendla*

Main category: cs.LG

TL;DR: Wave-PDE Nets leverage differentiable wave equation simulations, providing a new alternative to attention mechanisms and state-space models, with practical efficiency and competitive performance.


<details>
  <summary>Details</summary>
Motivation: The paper aims to introduce a computationally efficient and robust neural architecture inspired by physical principles to match or exceed the performance of existing models like Transformers.

Method: Wave-PDE Nets employ the second-order wave equation as their foundation, using differentiable simulation layers combined with a symplectic spectral solver based on FFTs.

Result: The model achieves competitive performance on language and vision tasks, reducing wall-clock time by up to 30% and peak memory usage by 25%, demonstrating efficiency in usage and performance.

Conclusion: Wave-PDE Nets emerge as a novel model with physical inductive biases, combining stability, efficiency, and an innovative approach to information propagation.

Abstract: We introduce Wave-PDE Nets, a neural architecture whose elementary operation
is a differentiable simulation of the second-order wave equation. Each layer
propagates its hidden state as a continuous field through a medium with
trainable spatial velocity c(x) and damping {\gamma}(x). A symplectic spectral
solver based on FFTs realises this propagation in O(nlog n) time. This
oscillatory, global mechanism provides a powerful alternative to attention and
first-order state-space models. We prove that a single Wave-PDE layer is a
universal approximator. On language and vision benchmarks, Wave-PDE Nets match
or exceed Transformer performance while demonstrating superior practical
efficiency, reducing wall-clock time by up to 30% and peak memory by 25%.
Ablation studies confirm the critical role of symplectic integration and a
spectral Laplacian for stability and performance. Visualizations of the learned
physical parameters reveal that the model learns intuitive strategies for
information propagation. These results position Wave-PDE Nets as a
computationally efficient and robust architecture with a strong physical
inductive bias.

</details>


### [554] [Distilling Reasoning into Student LLMs: Local Naturalness for Selecting Teacher Data](https://arxiv.org/abs/2510.03988)
*Hoang Anh Just,Myeongseob Ko,Ruoxi Jia*

Main category: cs.LG

TL;DR: This paper explores the challenge of selecting teacher model responses for reasoning distillation when using multiple teachers.


<details>
  <summary>Details</summary>
Motivation: Improving reasoning distillation for smaller student LLMs by addressing the gap in selecting the best teacher outputs in multi-teacher setups.

Method: Introduces Local Naturalness, a scoring technique that uses log probabilities on short reasoning steps, enabling better teacher and response selection.

Result: Using Local Naturalness improved a 32B student model's benchmark accuracy by 9.4 percentage points compared to existing methods.

Conclusion: Localized data evaluation and mixing of teacher responses yield superior performance in reasoning distillation processes.

Abstract: Distilling long reasoning traces (10K+ tokens) from stronger teacher models
into smaller student LLMs via SFT has emerged as a standard paradigm. This
approach is practical and efficient: it leverages the ease of generating
abundant reasoning data from stronger models and provides a direct, data-driven
way to teach less capable models better reasoning. While previous work has
largely focused on prompt selection with responses from a single teacher, the
equally important problem of choosing the best response when multiple teacher
outputs are available for a single prompt remains underexplored. This challenge
becomes important in a multi-teacher setting, where different students may
benefit from the outputs of different teachers. This paper fills that gap with
a systematic study of response selection for reasoning distillation. We first
show that the current method, which picks responses the student assigns the
highest global log-probability (global naturalness), fails when responses come
from multiple teachers, i.e., global naturalness no longer correlates with
downstream performance, especially as the reasoning traces from strong teachers
become longer. To overcome this problem, we introduce Local Naturalness, which
measures the student's log-probabilities over short, sequential reasoning steps
conditioned only on a small local window. Local Naturalness enables two
applications: 1) Teacher Selection: Aggregating local scores across prompts
reliably identifies the most helpful teacher. 2) Response Selection from a
Multiple Teachers: When mixing answers from many teachers, Local Naturalness
boosts a 32B student's accuracy on math benchmarks by 9.4pp over global
selection, also surpassing the performance achieved by training on data from
the single best teacher. These results highlight the power of localized data
quality evaluation and data mixing for more effective reasoning distillation.

</details>


### [555] [A Mathematical Explanation of Transformers for Large Language Models and GPTs](https://arxiv.org/abs/2510.03989)
*Xue-Cheng Tai,Hao Liu,Lingfeng Li,Raymond H. Chan*

Main category: cs.LG

TL;DR: This paper provides a structured integro-differential framework to interpret Transformer architecture mathematically, emphasizing its theoretical foundation and offering insights for future design.


<details>
  <summary>Details</summary>
Motivation: To address the lack of a comprehensive mathematical theory explaining the structure and operations of Transformer architecture.

Method: The researchers introduce a continuous formulation viewing the Transformer as a discretization of an integro-differential equation, interpreting self-attention as a non-local integral operator and layer normalization as a projection to a time-dependent constraint.

Result: The study unifies the core components of Transformers—attention, feedforward layers, and normalization—into an operator-theoretic framework, enabling theoretical understanding and flexibility for further development.

Conclusion: By embedding Transformers into continuous mathematical domains, the paper bridges the gap between architecture design and continuous modeling, offering a theoretically grounded perspective for neural network understanding.

Abstract: The Transformer architecture has revolutionized the field of sequence
modeling and underpins the recent breakthroughs in large language models
(LLMs). However, a comprehensive mathematical theory that explains its
structure and operations remains elusive. In this work, we propose a novel
continuous framework that rigorously interprets the Transformer as a
discretization of a structured integro-differential equation. Within this
formulation, the self-attention mechanism emerges naturally as a non-local
integral operator, and layer normalization is characterized as a projection to
a time-dependent constraint. This operator-theoretic and variational
perspective offers a unified and interpretable foundation for understanding the
architecture's core components, including attention, feedforward layers, and
normalization. Our approach extends beyond previous theoretical analyses by
embedding the entire Transformer operation in continuous domains for both token
indices and feature dimensions. This leads to a principled and flexible
framework that not only deepens theoretical insight but also offers new
directions for architecture design, analysis, and control-based
interpretations. This new interpretation provides a step toward bridging the
gap between deep learning architectures and continuous mathematical modeling,
and contributes a foundational perspective to the ongoing development of
interpretable and theoretically grounded neural network models.

</details>


### [556] [Incorporating Multivariate Consistency in ML-Based Weather Forecasting with Latent-space Constraints](https://arxiv.org/abs/2510.04006)
*Hang Fan,Yi Xiao,Yongquan Qu,Fenghua Ling,Ben Fei,Lei Bai,Pierre Gentine*

Main category: cs.LG

TL;DR: The paper improves machine learning-based weather forecasting by addressing limitations in current methods, using a novel training framework with latent-space constraints.


<details>
  <summary>Details</summary>
Motivation: To address shortcomings in ML-based weather forecasting, such as reliance on reanalysis as ground truth and unrealistic long-term forecasts.

Method: Reinterpreting training as weak-constraint 4D variational data assimilation, utilizing latent space with an autoencoder to account for reanalysis error covariance.

Result: Improved long-term forecast skill, finer structures, and better physical realism in predictions compared to traditional methods.

Conclusion: The proposed framework enhances weather forecasting and offers scalability with heterogeneous data sources for joint training.

Abstract: Data-driven machine learning (ML) models have recently shown promise in
surpassing traditional physics-based approaches for weather forecasting,
leading to a so-called second revolution in weather forecasting. However, most
ML-based forecast models treat reanalysis as the truth and are trained under
variable-specific loss weighting, ignoring their physical coupling and spatial
structure. Over long time horizons, the forecasts become blurry and physically
unrealistic under rollout training. To address this, we reinterpret model
training as a weak-constraint four-dimensional variational data assimilation
(WC-4DVar) problem, treating reanalysis data as imperfect observations. This
allows the loss function to incorporate reanalysis error covariance and capture
multivariate dependencies. In practice, we compute the loss in a latent space
learned by an autoencoder (AE), where the reanalysis error covariance becomes
approximately diagonal, thus avoiding the need to explicitly model it in the
high-dimensional model space. We show that rollout training with latent-space
constraints improves long-term forecast skill and better preserves fine-scale
structures and physical realism compared to training with model-space loss.
Finally, we extend this framework to accommodate heterogeneous data sources,
enabling the forecast model to be trained jointly on reanalysis and
multi-source observations within a unified theoretical formulation.

</details>


### [557] [Replacing Softmax Similarity with a Sharpened Angular Similarity: Theory and Practice of Scaling To Billion-Context Attention](https://arxiv.org/abs/2510.04008)
*Sahil Joshi,Agniva Chowdhury,Amar Kanakamedala,Ekam Singh,Evan Tu,Anshumali Shrivastava*

Main category: cs.LG

TL;DR: This paper proposes RACE Attention, a scalable alternative to Softmax Attention for handling outrageously long context windows, based on randomized projections and angular similarity.


<details>
  <summary>Details</summary>
Motivation: The computational cost of Softmax Attention scales quadratically with sequence length, making it impractical for long contexts, even on high-performance GPUs.

Method: RACE Attention utilizes angular (cosine) similarity instead of exponential kernels and employs randomized projections and soft Locality-Sensitive Hashing (LSH) to approximate attention outputs efficiently.

Result: The method achieves comparable accuracy to state-of-the-art models while significantly improving runtime and memory. It processes up to 12 million tokens on an NVIDIA GH200 GPU and 75 million tokens on a CPU, outperforming existing approaches.

Conclusion: RACE Attention is a practical, scalable, and theoretically grounded solution for handling very long sequences, exceeding the limitations of current attention mechanisms on modern hardware.

Abstract: Softmax Attention has a quadratic time complexity, which becomes prohibitive
to run at long contexts, even with highly optimized GPU kernels. For example,
FlashAttention (an exact, GPU-optimized implementation of Softmax Attention)
cannot complete a single forward-backward pass of a multi-head attention layer
once the context exceeds ~4 million tokens on an NVIDIA GH200 (96 GB). We
introduce RACE Attention, a kernel-inspired alternative to Softmax Attention
that is linear in sequence length and embedding dimension. RACE Attention
replaces the exponential kernel with a sharpened angular (cosine) similarity,
and approximates attention outputs via randomized projections and soft
Locality-Sensitive Hashing (LSH). Across language modeling, masked language
modeling, and text classification, RACE Attention matches the accuracy of
strong baselines while reducing runtime and memory. In a controlled scale test,
it processes up to 12 million tokens during a single forward-backward pass on
an NVIDIA GH200 GPU and 75 million tokens on an Intel Xeon Gold 5220R CPU, well
beyond the practical limits of the current state-of-the-art attention
implementations. RACE Attention thus offers a practical, theoretically grounded
mechanism for outrageously long context windows on today's hardware. We hope
that it gets adopted in practice.

</details>


### [558] [Partial Information Decomposition via Normalizing Flows in Latent Gaussian Distributions](https://arxiv.org/abs/2510.04417)
*Wenyuan Zhao,Adithya Balachandran,Chao Tian,Paul Pu Liang*

Main category: cs.LG

TL;DR: This paper introduces Gaussian PID (GPID), an efficient method for Partial Information Decomposition (PID) based on Gaussian distributions and extends its application to non-Gaussian data through information-preserving encoders. The method shows improved efficiency and accuracy in synthetic and real-world multimodal datasets.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge in existing PID methods, which are computationally expensive and less accurate for continuous and high-dimensional modalities, limiting their utility in analyzing multimodal datasets.

Method: The authors propose Gaussian PID (GPID), a more efficient approach for PID when pairwise distributions are multivariate Gaussians. For non-Gaussian data, they use information-preserving encoders to transform data into Gaussian distributions. Additionally, they provide a new gradient-based algorithm to enhance computational efficiency.

Result: The proposed method demonstrates superior PID estimation in diverse synthetic scenarios and large-scale multimodal benchmarks compared to existing methods.

Conclusion: The paper contributes an innovative approach to PID computation, enabling efficient and accurate analysis of multimodal datasets. This advances predictive modeling, interpretability, and model selection.

Abstract: The study of multimodality has garnered significant interest in fields where
the analysis of interactions among multiple information sources can enhance
predictive modeling, data fusion, and interpretability. Partial information
decomposition (PID) has emerged as a useful information-theoretic framework to
quantify the degree to which individual modalities independently, redundantly,
or synergistically convey information about a target variable. However,
existing PID methods depend on optimizing over a joint distribution constrained
by estimated pairwise probability distributions, which are costly and
inaccurate for continuous and high-dimensional modalities. Our first key
insight is that the problem can be solved efficiently when the pairwise
distributions are multivariate Gaussians, and we refer to this problem as
Gaussian PID (GPID). We propose a new gradient-based algorithm that
substantially improves the computational efficiency of GPID based on an
alternative formulation of the underlying optimization problem. To generalize
the applicability to non-Gaussian data, we learn information-preserving
encoders to transform random variables of arbitrary input distributions into
pairwise Gaussian random variables. Along the way, we resolved an open problem
regarding the optimality of joint Gaussian solutions for GPID. Empirical
validation in diverse synthetic examples demonstrates that our proposed method
provides more accurate and efficient PID estimates than existing baselines. We
further evaluate a series of large-scale multimodal benchmarks to show its
utility in real-world applications of quantifying PID in multimodal datasets
and selecting high-performing models.

</details>


### [559] [Spatiotemporal Forecasting as Planning: A Model-Based Reinforcement Learning Approach with Generative World Models](https://arxiv.org/abs/2510.04020)
*Hao Wu,Yuan Gao,Xingjian Shi,Shuaipeng Li,Fan Xu,Fan Zhang,Zhihong Zhu,Weiyan Wang,Xiao Luo,Kun Wang,Xian Wu,Xiaomeng Huang*

Main category: cs.LG

TL;DR: The paper introduces Spatiotemporal Forecasting as Planning (SFP), a model-based reinforcement learning system addressing stochasticity and non-differentiable metrics in forecasting.


<details>
  <summary>Details</summary>
Motivation: The aim is to tackle the challenges of inherent stochasticity and non-differentiable metrics in physical spatiotemporal forecasting.

Method: SFP leverages a Generative World Model for environment simulation, uses beam search-based planning guided by non-differentiable metrics as rewards, and employs iterative self-training for optimization.

Result: SFP reduces prediction error and performs exceptionally on critical metrics, such as capturing extreme events.

Conclusion: The framework successfully redefines forecasting as planning, showing significant improvements in modeling and capturing high-fidelity future states.

Abstract: To address the dual challenges of inherent stochasticity and
non-differentiable metrics in physical spatiotemporal forecasting, we propose
Spatiotemporal Forecasting as Planning (SFP), a new paradigm grounded in
Model-Based Reinforcement Learning. SFP constructs a novel Generative World
Model to simulate diverse, high-fidelity future states, enabling an
"imagination-based" environmental simulation. Within this framework, a base
forecasting model acts as an agent, guided by a beam search-based planning
algorithm that leverages non-differentiable domain metrics as reward signals to
explore high-return future sequences. These identified high-reward candidates
then serve as pseudo-labels to continuously optimize the agent's policy through
iterative self-training, significantly reducing prediction error and
demonstrating exceptional performance on critical domain metrics like capturing
extreme events.

</details>


### [560] [Multi-Class Support Vector Machine with Differential Privacy](https://arxiv.org/abs/2510.04027)
*Jinseong Park,Yujin Choi,Jaewook Lee*

Main category: cs.LG

TL;DR: This paper proposes a novel differentially private multi-class SVM (PMSVM) with weight and gradient perturbation to overcome privacy challenges in traditional multi-class SVM methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the inadequacy of traditional methods like one-versus-rest (OvR) and one-versus-one (OvO) in multi-class SVMs under differential privacy due to high privacy budget consumption.

Method: The paper introduces an all-in-one differentially private SVM approach using weight and gradient perturbation, supported by rigorous sensitivity and convergence analyses.

Result: Empirical results indicate that the proposed PMSVM outperforms existing DP-SVM methods in multi-class scenarios.

Conclusion: The PMSVM method effectively ensures privacy while improving performance, marking a significant advancement in privacy-preserving multi-class SVM models.

Abstract: With the increasing need to safeguard data privacy in machine learning
models, differential privacy (DP) is one of the major frameworks to build
privacy-preserving models. Support Vector Machines (SVMs) are widely used
traditional machine learning models due to their robust margin guarantees and
strong empirical performance in binary classification. However, applying DP to
multi-class SVMs is inadequate, as the standard one-versus-rest (OvR) and
one-versus-one (OvO) approaches repeatedly query each data sample when building
multiple binary classifiers, thus consuming the privacy budget proportionally
to the number of classes. To overcome this limitation, we explore all-in-one
SVM approaches for DP, which access each data sample only once to construct
multi-class SVM boundaries with margin maximization properties. We propose a
novel differentially Private Multi-class SVM (PMSVM) with weight and gradient
perturbation methods, providing rigorous sensitivity and convergence analyses
to ensure DP in all-in-one SVMs. Empirical results demonstrate that our
approach surpasses existing DP-SVM methods in multi-class scenarios.

</details>


### [561] [The Debate on RLVR Reasoning Capability Boundary: Shrinkage, Expansion, or Both? A Two-Stage Dynamic View](https://arxiv.org/abs/2510.04028)
*Xinhao Yao,Lu Yu,Xiaolin Hu,Fengwei Teng,Qing Cui,Jun Zhou,Yong Liu*

Main category: cs.LG

TL;DR: The abstract explores the impact of reinforcement learning with verifiable rewards (RLVR) on reasoning abilities of large language models (LLMs), presenting a two-stage dynamic where initial exploitation might shrink capabilities but prolonged exploration can expand boundaries.


<details>
  <summary>Details</summary>
Motivation: Resolving the debated effect of RLVR on the reasoning capabilities of LLMs, where studies suggest conflicting outcomes regarding capability shrinkage or expansion.

Method: The paper uses theoretical and empirical analysis to explain a two-stage probability mass dynamic (exploitation and exploration stages) during RLVR training, affecting token selection probabilities and reasoning boundaries.

Result: During the exploitation stage, reasoning capability boundaries might shrink due to over-sampling of high-reward tokens. In the exploration stage, prolonged training can increase probabilities of optimal tokens and promote reasoning expansion.

Conclusion: Over-exploitation can lead to decreased diversity and capability boundaries, but extended training fosters exploration and potentially enhances reasoning abilities.

Abstract: The ongoing debate on whether reinforcement learning with verifiable rewards
(RLVR) expands or shrinks the reasoning capabilities of large language models
(LLMs) remains unresolved. Some studies contend that RLVR mainly improves
sampling efficiency but at the expense of diversity and exploratory capacity,
resulting in capability boundary shrinkage. In contrast, others demonstrate
that prolonged training can lead to the emergence of novel reasoning
strategies, suggesting capability boundary expansion. To reconcile these
contradictory findings, we theoretically and empirically show that both
perspectives are partially valid-each aligning with a separate phase in an
inherent two-stage probability mass dynamic: (1) Exploitation stage: initially,
the model primarily samples explored high-reward and low-reward tokens, while
rarely selecting the potentially optimal token. Positive advantage estimates
increase the probability of high-reward tokens and decrease those of low-reward
tokens, yet the optimal token's probability remains largely unchanged during
this stage. (2) Exploration stage: as training advances, the growth rate of
previously acquired high-reward tokens slows as their probabilities approach
saturation. When a potentially optimal token-now receiving positive advantage
estimates-is occasionally sampled, its probability increases, while those of
the originally high-reward tokens decrease. This dynamic suggests that
over-exploitation during the exploitation stage may lead to capability boundary
shrinkage, whereas prolonged training into the exploration stage can promote an
expansion of the reasoning capability boundary. Building upon our insights, we
revisit the potential of only using relative negative gradients for prolonging
training, providing a theoretical and empirical foundation for the development
of more advanced reasoning capabilities.

</details>


### [562] [Adaptive kernel-density approach for imbalanced binary classification](https://arxiv.org/abs/2510.04046)
*Kotaro J. Nishimura,Yuichi Sakumura,Kazushi Ikeda*

Main category: cs.LG

TL;DR: The paper introduces KOTARO, a method for improving binary classification under extreme class imbalance. It showed superior performance compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of class imbalance in binary classification, especially in key areas like medical diagnosis and anomaly detection, where identifying the minority class accurately is crucial.

Method: The method, KOTARO, is based on a novel approach that extends Kernel Density Estimation (KDE) by dynamically adjusting bandwidths of Gaussian functions according to local sample densities, optimizing decision boundaries adaptively.

Result: Through experiments on synthetic and real-world datasets, KOTARO demonstrated improved performance over existing methods, particularly in handling conditions with severe class imbalance.

Conclusion: KOTARO is a significant advancement for imbalanced classification problems, offering improved minority class recognition and potential for broad application.

Abstract: Class imbalance is a common challenge in real-world binary classification
tasks, often leading to predictions biased toward the majority class and
reduced recognition of the minority class. This issue is particularly critical
in domains such as medical diagnosis and anomaly detection, where correct
classification of minority classes is essential. Conventional methods often
fail to deliver satisfactory performance when the imbalance ratio is extremely
severe. To address this challenge, we propose a novel approach called
Kernel-density-Oriented Threshold Adjustment with Regional Optimization
(KOTARO), which extends the framework of kernel density estimation (KDE) by
adaptively adjusting decision boundaries according to local sample density. In
KOTARO, the bandwidth of Gaussian basis functions is dynamically tuned based on
the estimated density around each sample, thereby enhancing the classifier's
ability to capture minority regions. We validated the effectiveness of KOTARO
through experiments on both synthetic and real-world imbalanced datasets. The
results demonstrated that KOTARO outperformed conventional methods,
particularly under conditions of severe imbalance, highlighting its potential
as a promising solution for a wide range of imbalanced classification problems

</details>


### [563] [Variational Diffusion Unlearning: A Variational Inference Framework for Unlearning in Diffusion Models under Data Constraints](https://arxiv.org/abs/2510.04058)
*Subhodip Panda,MS Varun,Shreyans Jain,Sarthak Kumar Maharana,Prathosh A. P*

Main category: cs.LG

TL;DR: The paper introduces Variational Diffusion Unlearning (VDU), a method for removing undesired features from pre-trained diffusion models in a data-constrained setting. The method uses variational inference to balance unlearning performance and image generation quality.


<details>
  <summary>Details</summary>
Motivation: Diffusion models can generate violent, obscene, or otherwise undesired outputs, posing challenges for responsible deployment. Existing unlearning methods fail in data-constrained settings where the full training dataset is unavailable.

Method: The authors propose VDU, leveraging variational inference with two key components: a 'plasticity inducer' to reduce log-likelihoods of undesired data and a 'stability regularizer' to maintain generation quality. The method requires only a subset of training data with undesired features and is computationally efficient.

Result: Experimental results validate the proposed approach for class unlearning and feature unlearning. The method successfully unlearned specific classes from datasets like MNIST, CIFAR-10, and tinyImageNet, as well as high-level features from the Stable Diffusion model.

Conclusion: VDU effectively addresses the challenges of machine unlearning in data-constrained settings while preserving image generation quality, offering practical solutions for safer diffusion model deployment.

Abstract: For a responsible and safe deployment of diffusion models in various domains,
regulating the generated outputs from these models is desirable because such
models could generate undesired, violent, and obscene outputs. To tackle this
problem, recent works use machine unlearning methodology to forget training
data points containing these undesired features from pre-trained generative
models. However, these methods proved to be ineffective in data-constrained
settings where the whole training dataset is inaccessible. Thus, the principal
objective of this work is to propose a machine unlearning methodology that can
prevent the generation of outputs containing undesired features from a
pre-trained diffusion model in such a data-constrained setting. Our proposed
method, termed as Variational Diffusion Unlearning (VDU), is a computationally
efficient method that only requires access to a subset of training data
containing undesired features. Our approach is inspired by the variational
inference framework with the objective of minimizing a loss function consisting
of two terms: plasticity inducer and stability regularizer. Plasticity inducer
reduces the log-likelihood of the undesired training data points, while the
stability regularizer, essential for preventing loss of image generation
quality, regularizes the model in parameter space. We validate the
effectiveness of our method through comprehensive experiments for both class
unlearning and feature unlearning. For class unlearning, we unlearn some
user-identified classes from MNIST, CIFAR-10, and tinyImageNet datasets from a
pre-trained unconditional denoising diffusion probabilistic model (DDPM).
Similarly, for feature unlearning, we unlearn the generation of certain
high-level features from a pre-trained Stable Diffusion model

</details>


### [564] [LaDiR: Latent Diffusion Enhances LLMs for Text Reasoning](https://arxiv.org/abs/2510.04573)
*Haoqiang Kang,Yizhe Zhang,Nikki Lijing Kuang,Nicklas Majamaki,Navdeep Jaitly,Yi-An Ma,Lianhui Qin*

Main category: cs.LG

TL;DR: The paper introduces LaDiR, a framework combining latent diffusion models with structured latent reasoning for improved text reasoning.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with autoregressive decoding, leading to inefficiencies in revising previous reasoning steps and exploring diverse solutions.

Method: The authors propose LaDiR, which constructs latent reasoning spaces using Variational Autoencoder (VAE) to encode reasoning steps and employs latent diffusion models for iterative refinement and parallel generation.

Result: Empirical evaluations show LaDiR improves accuracy, diversity, and interpretability in reasoning, outperforming existing methods.

Conclusion: LaDiR offers a new paradigm in text reasoning leveraging latent diffusion to enable more holistic reasoning refinement and diverse solution generation.

Abstract: Large Language Models (LLMs) demonstrate their reasoning ability through
chain-of-thought (CoT) generation. However, LLM's autoregressive decoding may
limit the ability to revisit and refine earlier tokens in a holistic manner,
which can also lead to inefficient exploration for diverse solutions. In this
paper, we propose LaDiR (Latent Diffusion Reasoner), a novel reasoning
framework that unifies the expressiveness of continuous latent representation
with the iterative refinement capabilities of latent diffusion models for an
existing LLM. We first construct a structured latent reasoning space using a
Variational Autoencoder (VAE) that encodes text reasoning steps into blocks of
thought tokens, preserving semantic information and interpretability while
offering compact but expressive representations. Subsequently, we utilize a
latent diffusion model that learns to denoise a block of latent thought tokens
with a blockwise bidirectional attention mask, enabling longer horizon and
iterative refinement with adaptive test-time compute. This design allows
efficient parallel generation of diverse reasoning trajectories, allowing the
model to plan and revise the reasoning process holistically. We conduct
evaluations on a suite of mathematical reasoning and planning benchmarks.
Empirical results show that LaDiR consistently improves accuracy, diversity,
and interpretability over existing autoregressive, diffusion-based, and latent
reasoning methods, revealing a new paradigm for text reasoning with latent
diffusion.

</details>


### [565] [Agentic Context Engineering: Evolving Contexts for Self-Improving Language Models](https://arxiv.org/abs/2510.04618)
*Qizheng Zhang,Changran Hu,Shubhangi Upasani,Boyuan Ma,Fenglu Hong,Vamsidhar Kamanuru,Jay Rainton,Chen Wu,Mengmeng Ji,Hanchen Li,Urmish Thakker,James Zou,Kunle Olukotun*

Main category: cs.LG

TL;DR: ACE framework addresses brevity bias and context collapse in large language models, optimizing contexts through structured and modular processes of generation, reflection, and curation.


<details>
  <summary>Details</summary>
Motivation: To improve the ability of LLMs to adapt and refine their inputs without weight updates, addressing issues like brevity bias and context collapse.

Method: ACE uses a modular process involving generation, reflection, and curation to incrementally update and refine contexts for LLMs. It also leverages execution feedback instead of labeled supervision.

Result: ACE achieves +10.6% improvement in agent-related benchmarks and +8.6% improvement in finance-specific benchmarks, surpassing leading systems despite using smaller models and reducing latency and costs.

Conclusion: Comprehensive, evolving contexts enabled by ACE framework enhance the scalability, efficiency, and self-improvement capabilities of LLM systems, offering practical benefits with minimal overhead.

Abstract: Large language model (LLM) applications such as agents and domain-specific
reasoning increasingly rely on context adaptation -- modifying inputs with
instructions, strategies, or evidence, rather than weight updates. Prior
approaches improve usability but often suffer from brevity bias, which drops
domain insights for concise summaries, and from context collapse, where
iterative rewriting erodes details over time. Building on the adaptive memory
introduced by Dynamic Cheatsheet, we introduce ACE (Agentic Context
Engineering), a framework that treats contexts as evolving playbooks that
accumulate, refine, and organize strategies through a modular process of
generation, reflection, and curation. ACE prevents collapse with structured,
incremental updates that preserve detailed knowledge and scale with
long-context models. Across agent and domain-specific benchmarks, ACE optimizes
contexts both offline (e.g., system prompts) and online (e.g., agent memory),
consistently outperforming strong baselines: +10.6% on agents and +8.6% on
finance, while significantly reducing adaptation latency and rollout cost.
Notably, ACE could adapt effectively without labeled supervision and instead by
leveraging natural execution feedback. On the AppWorld leaderboard, ACE matches
the top-ranked production-level agent on the overall average and surpasses it
on the harder test-challenge split, despite using a smaller open-source model.
These results show that comprehensive, evolving contexts enable scalable,
efficient, and self-improving LLM systems with low overhead.

</details>


### [566] [Using predefined vector systems as latent space configuration for neural network supervised training on data with arbitrarily large number of classes](https://arxiv.org/abs/2510.04090)
*Nikita Gabdullin*

Main category: cs.LG

TL;DR: This paper proposes a novel methodology for supervised learning (SL) that uses predefined latent space configurations to train neural networks, enabling classification tasks regardless of the number of classes.


<details>
  <summary>Details</summary>
Motivation: Current supervised learning methods tie neural network parameters to the number of classes, limiting scalability and applicability to datasets with an extremely large or undetermined number of classes.

Method: The authors use predefined vector systems, specifically randomly perturbed An root system vectors, as target latent space configurations (LSC) during neural network training. This aligns NN predictions with predefined vectors.

Result: Encoders and visual transformers (ViT) were successfully trained on datasets like Cinic-10, ImageNet-1K, and a dataset with 1.28 million classes, showcasing the method's scalability.

Conclusion: The proposed methodology enables neural network training independent of class count, with potential applications in lifelong learning and neural network distillation, enhancing versatility and applicability.

Abstract: Supervised learning (SL) methods are indispensable for neural network (NN)
training used to perform classification tasks. While resulting in very high
accuracy, SL training often requires making NN parameter number dependent on
the number of classes, limiting their applicability when the number of classes
is extremely large or unknown in advance. In this paper we propose a
methodology that allows one to train the same NN architecture regardless of the
number of classes. This is achieved by using predefined vector systems as the
target latent space configuration (LSC) during NN training. We discuss the
desired properties of target configurations and choose randomly perturbed
vectors of An root system for our experiments. These vectors are used to
successfully train encoders and visual transformers (ViT) on Cinic-10 and
ImageNet-1K in low- and high-dimensional cases by matching NN predictions with
the predefined vectors. Finally, ViT is trained on a dataset with 1.28 million
classes illustrating the applicability of the method to training on datasets
with extremely large number of classes. In addition, potential applications of
LSC in lifelong learning and NN distillation are discussed illustrating
versatility of the proposed methodology.

</details>


### [567] [Rethinking Consistent Multi-Label Classification under Inexact Supervision](https://arxiv.org/abs/2510.04091)
*Wei Wang,Tianhao Ma,Ming-Kun Xie,Gang Niu,Masashi Sugiyama*

Main category: cs.LG

TL;DR: The paper proposes consistent methodologies for partial and complementary multi-label learning using unbiased risk estimators, overcoming practical limitations of previous approaches.


<details>
  <summary>Details</summary>
Motivation: To address challenges in weakly supervised multi-label classification where accurate label annotation is costly and existing methods face practical limitations.

Method: Developed unbiased risk estimators using first- and second-order strategies, and proved theoretical consistency and convergence rates for multi-label classification metrics.

Result: Experimental validation shows that the proposed approaches perform effectively compared to state-of-the-art methods.

Conclusion: The proposed methods generalize weakly supervised multi-label learning by eliminating challenging assumptions, showcasing promising theoretical and empirical results.

Abstract: Partial multi-label learning and complementary multi-label learning are two
popular weakly supervised multi-label classification paradigms that aim to
alleviate the high annotation costs of collecting precisely annotated
multi-label data. In partial multi-label learning, each instance is annotated
with a candidate label set, among which only some labels are relevant; in
complementary multi-label learning, each instance is annotated with
complementary labels indicating the classes to which the instance does not
belong. Existing consistent approaches for the two paradigms either require
accurate estimation of the generation process of candidate or complementary
labels or assume a uniform distribution to eliminate the estimation problem.
However, both conditions are usually difficult to satisfy in real-world
scenarios. In this paper, we propose consistent approaches that do not rely on
the aforementioned conditions to handle both problems in a unified way.
Specifically, we propose two unbiased risk estimators based on first- and
second-order strategies. Theoretically, we prove consistency w.r.t. two widely
used multi-label classification evaluation metrics and derive convergence rates
for the estimation errors of the proposed risk estimators. Empirically,
extensive experimental results validate the effectiveness of our proposed
approaches against state-of-the-art methods.

</details>


### [568] [Why Cannot Neural Networks Master Extrapolation? Insights from Physical Laws](https://arxiv.org/abs/2510.04102)
*Ramzi Dakhmouche,Hossein Gorji*

Main category: cs.LG

TL;DR: The paper investigates the extrapolation limitations of Foundation Models (FMs) for time-series forecasting and identifies a fundamental property explaining performance deterioration in such models.


<details>
  <summary>Details</summary>
Motivation: Success of Foundation Models in language modeling drives interest in applying them to time-series prediction, especially for long-range forecasting where FMs underperform compared to simpler alternatives.

Method: The authors formalize a fundamental property affecting extrapolation abilities and conduct both theoretical analyses and empirical studies on current deep learning architectures.

Result: The study provides insights into the causes behind the extrapolation gap and demonstrates its implications on popular deep learning approaches.

Conclusion: Addressing extrapolation limitations, the paper suggests design directions for building advanced forecasting models capable of surpassing current challenges.

Abstract: Motivated by the remarkable success of Foundation Models (FMs) in language
modeling, there has been growing interest in developing FMs for time series
prediction, given the transformative power such models hold for science and
engineering. This culminated in significant success of FMs in short-range
forecasting settings. However, extrapolation or long-range forecasting remains
elusive for FMs, which struggle to outperform even simple baselines. This
contrasts with physical laws which have strong extrapolation properties, and
raises the question of the fundamental difference between the structure of
neural networks and physical laws. In this work, we identify and formalize a
fundamental property characterizing the ability of statistical learning models
to predict more accurately outside of their training domain, hence explaining
performance deterioration for deep learning models in extrapolation settings.
In addition to a theoretical analysis, we present empirical results showcasing
the implications of this property on current deep learning architectures. Our
results not only clarify the root causes of the extrapolation gap but also
suggest directions for designing next-generation forecasting models capable of
mastering extrapolation.

</details>


### [569] [Can Linear Probes Measure LLM Uncertainty?](https://arxiv.org/abs/2510.04108)
*Ramzi Dakhmouche,Adrien Letellier,Hossein Gorji*

Main category: cs.LG

TL;DR: This paper introduces a Bayesian statistics-based uncertainty quantification (UQ) approach for Large Language Models (LLMs), yielding improved performance over current methods.


<details>
  <summary>Details</summary>
Motivation: Current uncertainty quantification methods, such as using the maximum softmax score, fail to adequately measure uncertainty in LLM outputs, especially for multiple-choice generation.

Method: The paper proposes training multiple Bayesian linear models to predict layer-level outputs and combine distributional features to infer overall uncertainty more efficiently.

Result: The method demonstrates consistent performance improvements in UQ across different LLMs compared to existing baselines.

Conclusion: A Bayesian framework for UQ with LLMs is effective, even using a simple linear regression model, and offers a more reliable uncertainty estimation method.

Abstract: Effective Uncertainty Quantification (UQ) represents a key aspect for
reliable deployment of Large Language Models (LLMs) in automated
decision-making and beyond. Yet, for LLM generation with multiple choice
structure, the state-of-the-art in UQ is still dominated by the naive baseline
given by the maximum softmax score. To address this shortcoming, we demonstrate
that taking a principled approach via Bayesian statistics leads to improved
performance despite leveraging the simplest possible model, namely linear
regression. More precisely, we propose to train multiple Bayesian linear
models, each predicting the output of a layer given the output of the previous
one. Based on the obtained layer-level posterior distributions, we infer the
global uncertainty level of the LLM by identifying a sparse combination of
distributional features, leading to an efficient UQ scheme. Numerical
experiments on various LLMs show consistent improvement over state-of-the-art
baselines.

</details>


### [570] [Wasserstein projection distance for fairness testing of regression models](https://arxiv.org/abs/2510.04114)
*Wanxin Li,Yongjin P. Park,Khanh Dao Duc*

Main category: cs.LG

TL;DR: This paper introduces a fairness testing framework for regression models using a Wasserstein projection-based approach.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the lack of focus on fairness in regression models, as most fairness research has concentrated on classification tasks.

Method: The method involves a Wasserstein projection-based framework with a hypothesis-testing approach and an optimal data perturbation technique. The paper provides theoretical underpinnings, including fairness criteria categorization, dual reformulation of the test statistic, and derivation of asymptotic bounds.

Result: Experiments on synthetic and real datasets show that the approach improves fairness detection, offering higher specificity and effectively reducing bias in applications like student performance and housing price prediction.

Conclusion: The proposed framework is effective in detecting and mitigating biases in regression models, advancing fairness-focused methods in machine learning beyond classification tasks.

Abstract: Fairness in machine learning is a critical concern, yet most research has
focused on classification tasks, leaving regression models underexplored. This
paper introduces a Wasserstein projection-based framework for fairness testing
in regression models, focusing on expectation-based criteria. We propose a
hypothesis-testing approach and an optimal data perturbation method to improve
fairness while balancing accuracy. Theoretical results include a detailed
categorization of fairness criteria for regression, a dual reformulation of the
Wasserstein projection test statistic, and the derivation of asymptotic bounds
and limiting distributions. Experiments on synthetic and real-world datasets
demonstrate that the proposed method offers higher specificity compared to
permutation-based tests, and effectively detects and mitigates biases in real
applications such as student performance and housing price prediction.

</details>


### [571] [ONNX-Net: Towards Universal Representations and Instant Performance Prediction for Neural Architectures](https://arxiv.org/abs/2510.04938)
*Shiwen Qin,Alexander Auras,Shay B. Cohen,Elliot J. Crowley,Michael Moeller,Linus Ericsson,Jovita Lukasik*

Main category: cs.LG

TL;DR: The paper introduces ONNX-Bench, a benchmark for neural architecture search (NAS) that uses text-based network representations for generalization across diverse architectures.


<details>
  <summary>Details</summary>
Motivation: Current NAS methods are constrained by expensive performance evaluations and limited generalization due to reliance on cell-based search spaces and specific graph encodings, inhibiting scalability.

Method: ONNX-Bench standardizes neural network descriptions using ONNX format and introduces ONNX-Net, a natural language-based representation for architectures, enabling performance prediction across a broad range of search spaces.

Result: Experiments demonstrate effective zero-shot performance with ONNX-Net across distinct search spaces using minimal pretraining, accelerating architecture evaluations.

Conclusion: ONNX-Bench and ONNX-Net provide a flexible and scalable solution to NAS, eliminating reliance on specific search space encodings and enabling instant evaluation of diverse neural architectures.

Abstract: Neural architecture search (NAS) automates the design process of
high-performing architectures, but remains bottlenecked by expensive
performance evaluation. Most existing studies that achieve faster evaluation
are mostly tied to cell-based search spaces and graph encodings tailored to
those individual search spaces, limiting their flexibility and scalability when
applied to more expressive search spaces. In this work, we aim to close the gap
of individual search space restrictions and search space dependent network
representations. We present ONNX-Bench, a benchmark consisting of a collection
of neural networks in a unified format based on ONNX files. ONNX-Bench includes
all open-source NAS-bench-based neural networks, resulting in a total size of
more than 600k {architecture, accuracy} pairs. This benchmark allows creating a
shared neural network representation, ONNX-Net, able to represent any neural
architecture using natural language descriptions acting as an input to a
performance predictor. This text-based encoding can accommodate arbitrary layer
types, operation parameters, and heterogeneous topologies, enabling a single
surrogate to generalise across all neural architectures rather than being
confined to cell-based search spaces. Experiments show strong zero-shot
performance across disparate search spaces using only a small amount of
pretraining samples, enabling the unprecedented ability to evaluate any neural
network architecture instantly.

</details>


### [572] [On the Statistical Query Complexity of Learning Semiautomata: a Random Walk Approach](https://arxiv.org/abs/2510.04115)
*George Giapitzakis,Kimon Fountoulakis,Eshaan Nichani,Jason D. Lee*

Main category: cs.LG

TL;DR: The paper establishes Statistical Query hardness for semiautomata using random walks and group theory.


<details>
  <summary>Details</summary>
Motivation: To understand the computational hardness of semiautomata under Statistical Query frameworks, particularly under uniform distribution conditions.

Method: The authors analyze semiautomata by reducing distinguishing task to a random walk on $S_N \times S_N$, leveraging tools like Fourier analysis and symmetric group representation theory.

Result: They obtain spectral gap bounds that show near uncorrelation of distinct semiautomata after a polynomial number of steps, proving Statistical Query hardness.

Conclusion: The study reveals that the Statistical Query hardness of semiautomata arises from their internal state-transition structure, rather than the complexity of the languages recognized.

Abstract: Semiautomata form a rich class of sequence-processing algorithms with
applications in natural language processing, robotics, computational biology,
and data mining. We establish the first Statistical Query hardness result for
semiautomata under the uniform distribution over input words and initial
states. We show that Statistical Query hardness can be established when both
the alphabet size and input length are polynomial in the number of states.
Unlike the case of deterministic finite automata, where hardness typically
arises through the hardness of the language they recognize (e.g., parity), our
result is derived solely from the internal state-transition structure of
semiautomata. Our analysis reduces the task of distinguishing the final states
of two semiautomata to studying the behavior of a random walk on the group
$S_{N} \times S_{N}$. By applying tools from Fourier analysis and the
representation theory of the symmetric group, we obtain tight spectral gap
bounds, demonstrating that after a polynomial number of steps in the number of
states, distinct semiautomata become nearly uncorrelated, yielding the desired
hardness result.

</details>


### [573] [Attending on Multilevel Structure of Proteins enables Accurate Prediction of Cold-Start Drug-Target Interactions](https://arxiv.org/abs/2510.04126)
*Ziying Zhang,Yaqing Wang,Yuxuan Sun,Min Ye,Quanming Yao*

Main category: cs.LG

TL;DR: ColdDTI enhances cold-start drug-target interaction prediction by attending to multi-level protein structures using hierarchical attention and avoids overfitting while outperforming existing models.


<details>
  <summary>Details</summary>
Motivation: To improve prediction accuracy for drug-target interactions involving novel drugs and proteins by considering multi-level protein structures.

Method: A hierarchical attention mechanism mines interactions between multi-level protein structures and drug structures, fusing them into final predictive representations.

Result: ColdDTI outperformed existing methods consistently on benchmark datasets in cold-start scenarios.

Conclusion: ColdDTI effectively utilizes biologically transferable priors to address limitations in previous models and improves prediction reliability without overfitting.

Abstract: Cold-start drug-target interaction (DTI) prediction focuses on interaction
between novel drugs and proteins. Previous methods typically learn transferable
interaction patterns between structures of drug and proteins to tackle it.
However, insight from proteomics suggest that protein have multi-level
structures and they all influence the DTI. Existing works usually represent
protein with only primary structures, limiting their ability to capture
interactions involving higher-level structures. Inspired by this insight, we
propose ColdDTI, a framework attending on protein multi-level structure for
cold-start DTI prediction. We employ hierarchical attention mechanism to mine
interaction between multi-level protein structures (from primary to quaternary)
and drug structures at both local and global granularities. Then, we leverage
mined interactions to fuse structure representations of different levels for
final prediction. Our design captures biologically transferable priors,
avoiding the risk of overfitting caused by excessive reliance on representation
learning. Experiments on benchmark datasets demonstrate that ColdDTI
consistently outperforms previous methods in cold-start settings.

</details>


### [574] [On the Limitations and Capabilities of Position Embeddings for Length Generalization](https://arxiv.org/abs/2510.04130)
*Yang Chen,Yitao Liang,Zhouchen Lin*

Main category: cs.LG

TL;DR: Position Embeddings (PEs) are central to Length Generalization (LG) in Transformers, and this study explores their limitations and potential with theoretical analysis and practical strategies.


<details>
  <summary>Details</summary>
Motivation: Understanding how Position Embeddings (PEs) influence Length Generalization (LG) in Transformers and addressing their theoretical and practical capabilities.

Method: The study introduces Linear Representation Complexity (LRC) and Sequential Representation Complexity (SRC) for theoretical and empirical analysis, and proposes strategies like Scale Hint and a Learning-Based Position Embedding framework.

Result: The analysis reveals PEs structure computations rather than expanding computational capabilities. The proposed SRC theory is supported empirically, and introduced strategies enhance LG in practical contexts.

Conclusion: The research highlights the theoretical role of Position Embeddings, affirms their scalability requirements for LG, and provides methods to improve LG in Transformers.

Abstract: In Transformers, Position Embeddings (PEs) significantly influence Length
Generalization (LG) performance, yet their fundamental role remains unclear. In
this work, we investigate the limitations and capabilities of PEs in achieving
LG. We theoretically analyze PEs in Position-Only Linear Attentions (POLAs),
introducing Linear Representation Complexity (LRC) to characterize when PEs
enable LG. Our analysis shows that PEs do not expand computational capabilities
but structure learned computations across positions. Extending to practical
Transformers, we propose Sequential Representation Complexity (SRC) and
conjecture that LG is possible if and only if SRC remains invariant across
scales. We support this hypothesis with empirical evidence in various reasoning
tasks. To enhance LG, we introduce Scale Hint, allowing flexible instance
scaling, and a Learning-Based Position Embedding framework that automatically
learns positional relations. Our work provides theoretical insights and
practical strategies for improving LG in Transformers.

</details>


### [575] [Modeling Time Series Dynamics with Fourier Ordinary Differential Equations](https://arxiv.org/abs/2510.04133)
*Muhao Guo,Yang Weng*

Main category: cs.LG

TL;DR: This paper addresses limitations of Neural ODEs in modeling time series by proposing Fourier ODEs, which operate in the frequency domain for improved pattern recognition and predictive accuracy.


<details>
  <summary>Details</summary>
Motivation: Current Neural ODEs struggle with capturing long-term dependencies and periodic structures due to their reliance on time-domain representations, and face challenges reconciling continuous-time formulations with discrete data.

Method: The approach transforms time-series data into the frequency domain using FFT, incorporates an element-wise filtering mechanism to align outputs with discrete observations, and integrates these enhancements into a predictive framework.

Result: Experiments on various datasets demonstrate improved accuracy and efficiency for time series modeling using Fourier ODEs compared to existing methods.

Conclusion: Fourier ODEs successfully address the limitations of Neural ODEs in time series applications, showcasing superior ability to capture both global and granular patterns.

Abstract: Neural ODEs (NODEs) have emerged as powerful tools for modeling time series
data, offering the flexibility to adapt to varying input scales and capture
complex dynamics. However, they face significant challenges: first, their
reliance on time-domain representations often limits their ability to capture
long-term dependencies and periodic structures; second, the inherent mismatch
between their continuous-time formulation and the discrete nature of real-world
data can lead to loss of granularity and predictive accuracy. To address these
limitations, we propose Fourier Ordinary Differential Equations (FODEs), an
approach that embeds the dynamics in the Fourier domain. By transforming
time-series data into the frequency domain using the Fast Fourier Transform
(FFT), FODEs uncover global patterns and periodic behaviors that remain elusive
in the time domain. Additionally, we introduce a learnable element-wise
filtering mechanism that aligns continuous model outputs with discrete
observations, preserving granularity and enhancing accuracy. Experiments on
various time series datasets demonstrate that FODEs outperform existing methods
in terms of both accuracy and efficiency. By effectively capturing both long-
and short-term patterns, FODEs provide a robust framework for modeling time
series dynamics.

</details>


### [576] [PhaseFormer: From Patches to Phases for Efficient and Effective Time Series Forecasting](https://arxiv.org/abs/2510.04134)
*Yiming Niu,Jinliang Deng,Yongxin Tong*

Main category: cs.LG

TL;DR: The paper introduces PhaseFormer, a model for periodic time series forecasting that is efficient and effective, showing state-of-the-art results with minimal computational overhead.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the inefficiency of existing methods that rely on patch-level processing for handling periodicity in time series data, which leads to heavy computational costs.

Method: PhaseFormer uses a phase-driven approach with compact phase embeddings and a lightweight cross-phase routing mechanism to improve efficiency and effectiveness in modeling periodicity.

Result: PhaseFormer outperforms other models, achieving state-of-the-art performance with around 1k parameters, especially excelling in large-scale and complex datasets.

Conclusion: PhaseFormer provides an efficient and effective solution for time series forecasting, advancing the field by significantly reducing computational costs without compromising predictive accuracy.

Abstract: Periodicity is a fundamental characteristic of time series data and has long
played a central role in forecasting. Recent deep learning methods strengthen
the exploitation of periodicity by treating patches as basic tokens, thereby
improving predictive effectiveness. However, their efficiency remains a
bottleneck due to large parameter counts and heavy computational costs. This
paper provides, for the first time, a clear explanation of why patch-level
processing is inherently inefficient, supported by strong evidence from
real-world data. To address these limitations, we introduce a phase perspective
for modeling periodicity and present an efficient yet effective solution,
PhaseFormer. PhaseFormer features phase-wise prediction through compact phase
embeddings and efficient cross-phase interaction enabled by a lightweight
routing mechanism. Extensive experiments demonstrate that PhaseFormer achieves
state-of-the-art performance with around 1k parameters, consistently across
benchmark datasets. Notably, it excels on large-scale and complex datasets,
where models with comparable efficiency often struggle. This work marks a
significant step toward truly efficient and effective time series forecasting.
Code is available at this repository:
https://github.com/neumyor/PhaseFormer_TSL

</details>


### [577] [Efficient Manifold-Constrained Neural ODE for High-Dimensional Datasets](https://arxiv.org/abs/2510.04138)
*Muhao Guo,Haoran Li,Yang Weng*

Main category: cs.LG

TL;DR: This paper introduces a novel method combining neural ordinary differential equations (NODEs) with manifold learning to improve computational efficiency and accuracy in high-dimensional datasets.


<details>
  <summary>Details</summary>
Motivation: Estimating dynamics in high-dimensional systems with NODEs involves high computational costs and truncation errors, especially due to unknown data distribution topology.

Method: The researchers employ a structure-preserved encoder to approximate the manifold underlying the data and combine this encoding with NODE learning to constrain the ODE process effectively.

Result: The proposed model outperforms existing baselines in terms of accuracy, efficiency (number of function evaluations), and convergence speed across multiple datasets.

Conclusion: The proposed approach addresses key challenges of high-dimensional NODE learning by exploring and utilizing the underlying manifold, leading to faster and more accurate model performance.

Abstract: Neural ordinary differential equations (NODE) have garnered significant
attention for their design of continuous-depth neural networks and the ability
to learn data/feature dynamics. However, for high-dimensional systems,
estimating dynamics requires extensive calculations and suffers from high
truncation errors for the ODE solvers. To address the issue, one intuitive
approach is to consider the non-trivial topological space of the data
distribution, i.e., a low-dimensional manifold. Existing methods often rely on
knowledge of the manifold for projection or implicit transformation,
restricting the ODE solutions on the manifold. Nevertheless, such knowledge is
usually unknown in realistic scenarios. Therefore, we propose a novel approach
to explore the underlying manifold to restrict the ODE process. Specifically,
we employ a structure-preserved encoder to process data and find the underlying
graph to approximate the manifold. Moreover, we propose novel methods to
combine the NODE learning with the manifold, resulting in significant gains in
computational speed and accuracy. Our experimental evaluations encompass
multiple datasets, where we compare the accuracy, number of function
evaluations (NFEs), and convergence speed of our model against existing
baselines. Our results demonstrate superior performance, underscoring the
effectiveness of our approach in addressing the challenges of high-dimensional
datasets.

</details>


### [578] [Learning to Interpret Weight Differences in Language Models](https://arxiv.org/abs/2510.05092)
*Avichal Goel,Yoon Kim,Nir Shavit,Tony T. Wang*

Main category: cs.LG

TL;DR: The paper presents Diff Interpretation Tuning (DIT), a method that enables models to describe how finetuning changes their knowledge.


<details>
  <summary>Details</summary>
Motivation: Understanding the modifications in weights of language models after finetuning is pivotal for interpretability, but traditional approaches lack clarity and depend on inaccessible or large datasets.

Method: The authors introduce DIT, which uses synthetic labeled weight diffs to train a dedicated adapter. This adapter empowers finetuned models to describe their weight changes in natural language.

Result: DIT demonstrates effectiveness in two settings—reporting hidden behaviors and summarizing finetuned knowledge—providing accurate descriptions of model changes due to finetuning.

Conclusion: The proposed DIT helps achieve interpretability in finetuned models, allowing them to explain their internal modifications effectively, paving the way for future insights into model behavior.

Abstract: Finetuning (pretrained) language models is a standard approach for updating
their internal parametric knowledge and specializing them to new tasks and
domains. However, the corresponding model weight changes ("weight diffs") are
not generally interpretable. While inspecting the finetuning dataset can give a
sense of how the model might have changed, these datasets are often not
publicly available or are too large to work with directly. Towards the goal of
comprehensively understanding weight diffs in natural language, we introduce
Diff Interpretation Tuning (DIT), a method that trains models to describe their
own finetuning-induced modifications. Our approach uses synthetic, labeled
weight diffs to train a DIT adapter, which can be applied to a compatible
finetuned model to make it describe how it has changed. We demonstrate in two
proof-of-concept settings (reporting hidden behaviors and summarizing finetuned
knowledge) that our method enables models to describe their finetuning-induced
modifications using accurate natural language descriptions.

</details>


### [579] [From Noisy Traces to Stable Gradients: Bias-Variance Optimized Preference Optimization for Aligning Large Reasoning Models](https://arxiv.org/abs/2510.05095)
*Mingkang Zhu,Xi Chen,Bei Yu,Hengshuang Zhao,Jiaya Jia*

Main category: cs.LG

TL;DR: The paper introduces BVPO, a method to better align large reasoning models (LRMs) with human preferences by optimizing the bias-variance trade-off in preference optimization. This approach reduces gradient variance and enhances both alignment and reasoning performance.


<details>
  <summary>Details</summary>
Motivation: Current methods for aligning LRMs with human preferences suffer from high variance due to stochastic trace sampling during optimization. There is a need for more stable training that effectively addresses this bottleneck.

Method: The paper proposes BVPO, which combines two gradient estimators—a high-variance trace-based estimator and a low-variance empty-trace estimator—using a mixing weight that minimizes mean-squared error. The method is grounded in a theoretical framework that improves convergence bounds.

Result: BVPO outperforms baselines by improving alignment by up to 7.8 points on AlpacaEval~2 and 6.8 points on Arena-Hard, and boosts reasoning performance on math benchmarks by up to 4.0 points.

Conclusion: Trace-induced variance is a critical limitation in preference optimization. Addressing this via BVPO leads to more stable training and substantial gains in alignment and reasoning capabilities of LRMs.

Abstract: Large reasoning models (LRMs) generate intermediate reasoning traces before
producing final answers, yielding strong gains on multi-step and mathematical
tasks. Yet aligning LRMs with human preferences, a crucial prerequisite for
model deployment, remains underexplored. The statistically correct objective
for preference alignment requires marginalizing over reasoning traces, but this
computation is intractable in practice. A common workaround optimizes a single
sampled trajectory, which introduces substantial gradient variance from
stochastic trace sampling. To address this challenge, we frame preference
optimization for LRMs through the lens of the bias--variance trade-off and
propose Bias--Variance Optimized Preference Optimization (BVPO), a simple,
drop-in method that mixes two gradient estimators: a high-variance trace-based
estimator and a low-variance empty-trace estimator obtained by disabling
reasoning trace generation. Our theory shows that BVPO strictly reduces
trace-induced variance for any nontrivial mixture, provides a closed-form
choice of the mixing weight that minimizes mean-squared error relative to the
true marginal gradient, and under standard smoothness and step-size conditions,
tightens classical convergence bounds for stochastic gradient descent.
Empirically, BVPO improves alignment over the best baseline by up to 7.8 points
on AlpacaEval~2 and 6.8 points on Arena-Hard. Despite being trained only on
general conversational data, BVPO also boosts reasoning performance for base
models by up to 4.0 points on the average of six math reasoning benchmarks.
These results identify variance from trace sampling as a key bottleneck and
demonstrate that directly optimizing the bias--variance trade-off yields more
stable training and stronger overall performance.

</details>


### [580] [Finite Time Analysis of Constrained Natural Critic-Actor Algorithm with Improved Sample Complexity](https://arxiv.org/abs/2510.04189)
*Prashansa Panda,Shalabh Bhatnagar*

Main category: cs.LG

TL;DR: This paper introduces a natural critic-actor algorithm for long-run average cost with inequality constraints, and guarantees optimal non-asymptotic convergence with experimental validation.


<details>
  <summary>Details</summary>
Motivation: To address the gap in non-asymptotic convergence analyses for actor-critic algorithms in the long-run average cost setting with inequality constraints.

Method: The authors develop a critic-actor algorithm with function approximation, analyze its non-asymptotic convergence, propose modifications for better sample efficiency, and validate with experiments in Safety-Gym environments.

Result: The paper demonstrates optimal learning rates, enhanced sample complexity through a modification, and algorithm competitiveness in experiments against well-known baselines.

Conclusion: The proposed critic-actor algorithm is effective for constrained long-run cost problems, offering strong convergence guarantees and practical competitiveness.

Abstract: Recent studies have increasingly focused on non-asymptotic convergence
analyses for actor-critic (AC) algorithms. One such effort introduced a
two-timescale critic-actor algorithm for the discounted cost setting using a
tabular representation, where the usual roles of the actor and critic are
reversed. However, only asymptotic convergence was established there.
Subsequently, both asymptotic and non-asymptotic analyses of the critic-actor
algorithm with linear function approximation were conducted. In our work, we
introduce the first natural critic-actor algorithm with function approximation
for the long-run average cost setting and under inequality constraints. We
provide the non-asymptotic convergence guarantees for this algorithm. Our
analysis establishes optimal learning rates and we also propose a modification
to enhance sample complexity. We further show the results of experiments on
three different Safety-Gym environments where our algorithm is found to be
competitive in comparison with other well known algorithms.

</details>


### [581] [Spectral Alignment as Predictor of Loss Explosion in Neural Network Training](https://arxiv.org/abs/2510.04202)
*Haiquan Qiu,You Wu,Yingjie Tan,Yaqing Wang,Quanming Yao*

Main category: cs.LG

TL;DR: The paper introduces a new metric called Spectral Alignment (SA) to predict and prevent loss explosions during training of deep neural networks, outperforming traditional metrics.


<details>
  <summary>Details</summary>
Motivation: Prevent and predict loss explosions during expensive training processes of deep neural networks, where existing monitoring metrics are lagging or inconsistent.

Method: Proposing Spectral Alignment (SA), a metric that evaluates the alignment between layer inputs and the principal singular vectors of weight matrices as a predictor for training divergence.

Result: Empirical tests on language models reveal that SA provides earlier and clearer warnings of loss explosions compared to conventional metrics, while being computationally lightweight.

Conclusion: SA is a practical and theoretically grounded tool for early detection of training instability, preventing representational collapse in deep learning models.

Abstract: Loss explosions in training deep neural networks can nullify multi-million
dollar training runs. Conventional monitoring metrics like weight and gradient
norms are often lagging and ambiguous predictors, as their values vary
dramatically across different models and even between layers of the same model,
making it difficult to establish a unified standard for detecting impending
failure. We introduce Spectral Alignment (SA), a novel, theoretically-grounded
metric that monitors the distributional alignment between layer inputs and the
principal singular vectors of weight matrices. We show that a collapse in the
sign diversity of this alignment is a powerful early predictor of
representational collapse and training divergence. Empirical results on
language models demonstrate that monitoring the SA distribution provides a
significantly earlier and clearer warning of loss explosions than traditional
scalar metrics. SA's low computational overhead makes it a practical tool for
safeguarding model training.

</details>


### [582] [Adaptive Federated Learning via Dynamical System Model](https://arxiv.org/abs/2510.04203)
*Aayushya Agarwal,Larry Pileggi,Gauri Joshi*

Main category: cs.LG

TL;DR: The paper proposes an adaptive federated learning method that automatically selects hyperparameters to handle computational differences and data heterogeneity across clients.


<details>
  <summary>Details</summary>
Motivation: Hyperparameter tuning in heterogeneous federated learning is challenging, computationally expensive, and grows combinatorially with the number of clients.

Method: The method models federated learning as a dynamical system, using numerical simulation principles to adaptively select local learning rates and momentum parameters for both clients and servers.

Result: The adaptive algorithm achieves fast convergence, handles client drift and objective inconsistencies, and requires only one global hyperparameter.

Conclusion: This approach simplifies hyperparameter tuning, offers scalability, and delivers superior convergence for heterogeneous federated systems.

Abstract: Hyperparameter selection is critical for stable and efficient convergence of
heterogeneous federated learning, where clients differ in computational
capabilities, and data distributions are non-IID. Tuning hyperparameters is a
manual and computationally expensive process as the hyperparameter space grows
combinatorially with the number of clients. To address this, we introduce an
end-to-end adaptive federated learning method in which both clients and central
agents adaptively select their local learning rates and momentum parameters.
Our approach models federated learning as a dynamical system, allowing us to
draw on principles from numerical simulation and physical design. Through this
perspective, selecting momentum parameters equates to critically damping the
system for fast, stable convergence, while learning rates for clients and
central servers are adaptively selected to satisfy accuracy properties from
numerical simulation. The result is an adaptive, momentum-based federated
learning algorithm in which the learning rates for clients and servers are
dynamically adjusted and controlled by a single, global hyperparameter. By
designing a fully integrated solution for both adaptive client updates and
central agent aggregation, our method is capable of handling key challenges of
heterogeneous federated learning, including objective inconsistency and client
drift. Importantly, our approach achieves fast convergence while being
insensitive to the choice of the global hyperparameter, making it well-suited
for rapid prototyping and scalable deployment. Compared to state-of-the-art
adaptive methods, our framework is shown to deliver superior convergence for
heterogeneous federated learning while eliminating the need for hyperparameter
tuning both client and server updates.

</details>


### [583] [PolyKAN: A Polyhedral Analysis Framework for Provable and Minimal KAN Compression](https://arxiv.org/abs/2510.04205)
*Di Zhang*

Main category: cs.LG

TL;DR: The paper introduces PolyKAN, a theoretical framework for compressing Kolmogorov-Arnold Networks (KANs) to reduce model size while maintaining approximation accuracy.


<details>
  <summary>Details</summary>
Motivation: KANs offer interpretability and a strong theoretical base but face challenges in parameter efficiency for practical use.

Method: The authors characterize KANs polyhedrally and propose an $
$-equivalent compression theory, leveraging dynamic programming to optimize merging regions within error bounds.

Result: PolyKAN achieves guaranteed minimal compression with provable control of approximation error and polynomial-time complexity in network parameters.

Conclusion: The framework sets a foundation for efficient and interpretable KAN deployment, potentially driving further advances in compact neural architectures.

Abstract: Kolmogorov-Arnold Networks (KANs) have emerged as a promising alternative to
traditional Multi-Layer Perceptrons (MLPs), offering enhanced interpretability
and a strong mathematical foundation. However, their parameter efficiency
remains a significant challenge for practical deployment. This paper introduces
PolyKAN, a novel theoretical framework for KAN compression that provides formal
guarantees on both model size reduction and approximation error. By leveraging
the inherent piecewise polynomial structure of KANs, we formulate the
compression problem as one of optimal polyhedral region merging. We establish a
rigorous polyhedral characterization of KANs, develop a complete theory of
$\epsilon$-equivalent compression, and design an optimal dynamic programming
algorithm that guarantees minimal compression under specified error bounds. Our
theoretical analysis demonstrates that PolyKAN achieves provably minimal
compression while maintaining strict error control, with polynomial-time
complexity in all network parameters. The framework provides the first formal
foundation for KAN compression with mathematical guarantees, opening new
directions for efficient deployment of interpretable neural architectures.

</details>


### [584] [Why Low-Precision Transformer Training Fails: An Analysis on Flash Attention](https://arxiv.org/abs/2510.04212)
*Haiquan Qiu,Quanming Yao*

Main category: cs.LG

TL;DR: The paper explains how low-precision training with flash attention can cause training instabilities and proposes a simple fix to stabilize it.


<details>
  <summary>Details</summary>
Motivation: To address the issue of catastrophic loss explosions when using flash attention with low-precision formats during transformer training.

Method: The authors conduct an analysis to identify root causes of training instabilities, revealing the interplay between low-rank representations and biased rounding errors. They propose a minimal modification to flash attention for mitigation.

Result: The modification mitigates biased rounding errors, stabilizing the training process and validating the explanation presented.

Conclusion: A simple adjustment in flash attention mechanisms effectively addresses training failures caused by low-precision instability, offering a practical solution.

Abstract: The pursuit of computational efficiency has driven the adoption of
low-precision formats for training transformer models. However, this progress
is often hindered by notorious training instabilities. This paper provides the
first mechanistic explanation for a long-standing and unresolved failure case
where training with flash attention in low-precision settings leads to
catastrophic loss explosions. Our in-depth analysis reveals that the failure is
not a random artifact but caused by two intertwined phenomena: the emergence of
similar low-rank representations within the attention mechanism and the
compounding effect of biased rounding errors inherent in low-precision
arithmetic. We demonstrate how these factors create a vicious cycle of error
accumulation that corrupts weight updates, ultimately derailing the training
dynamics. To validate our findings, we introduce a minimal modification to the
flash attention that mitigates the bias in rounding errors. This simple change
stabilizes the training process, confirming our analysis and offering a
practical solution to this persistent problem.

</details>


### [585] [MLLMEraser: Achieving Test-Time Unlearning in Multimodal Large Language Models through Activation Steering](https://arxiv.org/abs/2510.04217)
*Chenlu Ding,Jiancan Wu,Leheng Sheng,Fan Zhang,Yancheng Yuan,Xiang Wang,Xiangnan He*

Main category: cs.LG

TL;DR: The paper proposes MLLMEraser, a training-free framework for dynamically erasing specific knowledge from multimodal large language models at test-time using activation steering.


<details>
  <summary>Details</summary>
Motivation: The large-scale deployment of multimodal large language models has raised concerns about memorized private data, outdated knowledge, and harmful content, which necessitates effective methods for selective unlearning.

Method: The proposed method, MLLMEraser, uses an input-aware, training-free mechanism called activation steering to achieve dynamic knowledge erasure without parameter updates. It employs adversarially perturbed image-text pairs and input-aware steering for efficient and accurate erasure.

Result: Experiments on LLaVA-1.5 and Qwen-2.5-VL show that MLLMEraser outperforms existing unlearning methods, ensuring better forgetting performance with reduced computational cost and minimal impact on retained knowledge.

Conclusion: MLLMEraser offers an efficient and effective solution for selective unlearning in multimodal large language models, addressing privacy and accuracy concerns with reduced computational overhead.

Abstract: Multimodal large language models (MLLMs) have demonstrated remarkable
capabilities across vision-language tasks, yet their large-scale deployment
raises pressing concerns about memorized private data, outdated knowledge, and
harmful content. Existing unlearning approaches for MLLMs typically adapt
training-based strategies such as gradient ascent or preference optimization,
but these methods are computationally expensive, irreversible, and often
distort retained knowledge. In this work, we propose MLLMEraser, an
input-aware, training-free framework for test-time unlearning. Our approach
leverages activation steering to enable dynamic knowledge erasure without
parameter updates. Specifically, we construct a multimodal erasure direction by
contrasting adversarially perturbed, knowledge-recall image-text pairs with
knowledge-erasure counterparts, capturing both textual and visual
discrepancies. To prevent unnecessary interference, we further design an
input-aware steering mechanism that adaptively determines when and how the
erasure direction should be applied, preserving utility on retained knowledge
while enforcing forgetting on designated content. Experiments on LLaVA-1.5 and
Qwen-2.5-VL demonstrate that MLLMEraser consistently outperforms
state-of-the-art MLLM unlearning baselines, achieving stronger forgetting
performance with lower computational cost and minimal utility degradation.

</details>


### [586] [Physics-Inspired All-Pair Interaction Learning for 3D Dynamics Modeling](https://arxiv.org/abs/2510.04233)
*Kai Yang,Yuqi Huang,Junheng Tao,Wanyu Wang,Qitian Wu*

Main category: cs.LG

TL;DR: The paper introduces PAINET, an SE(3)-equivariant neural architecture for modeling 3D multi-body dynamics, achieving superior performance across diverse benchmarks.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the limitations of existing GNN-based approaches, which struggle with unobserved interactions crucial for modeling complex physical dynamics.

Method: PAINET employs a physics-inspired attention network derived from energy function minimization and a parallel decoder that maintains equivariance for efficient inference.

Result: Empirical evaluations demonstrate that PAINET achieves 4.7% to 41.5% error reductions in 3D dynamics prediction across benchmarks such as human motion capture, molecular dynamics, and protein simulations, with competitive computational costs.

Conclusion: PAINET offers a significant improvement in capturing all-pair interactions in 3D multi-body systems, paving the way for more accurate and efficient 3D dynamics modeling.

Abstract: Modeling 3D dynamics is a fundamental problem in multi-body systems across
scientific and engineering domains and has important practical implications in
trajectory prediction and simulation. While recent GNN-based approaches have
achieved strong performance by enforcing geometric symmetries, encoding
high-order features or incorporating neural-ODE mechanics, they typically
depend on explicitly observed structures and inherently fail to capture the
unobserved interactions that are crucial to complex physical behaviors and
dynamics mechanism. In this paper, we propose PAINET, a principled
SE(3)-equivariant neural architecture for learning all-pair interactions in
multi-body systems. The model comprises: (1) a novel physics-inspired attention
network derived from the minimization trajectory of an energy function, and (2)
a parallel decoder that preserves equivariance while enabling efficient
inference. Empirical results on diverse real-world benchmarks, including human
motion capture, molecular dynamics, and large-scale protein simulations, show
that PAINET consistently outperforms recently proposed models, yielding 4.7% to
41.5% error reductions in 3D dynamics prediction with comparable computation
costs in terms of time and memory.

</details>


### [587] [Truncated Kernel Stochastic Gradient Descent with General Losses and Spherical Radial Basis Functions](https://arxiv.org/abs/2510.04237)
*Jinhui Bai,Andreas Christmann,Lei Shi*

Main category: cs.LG

TL;DR: The paper introduces a new kernel stochastic gradient descent (SGD) algorithm that enhances scalability and efficiency with an innovative regularization strategy, achieving strong theoretical guarantees and practical performance for large-scale supervised learning problems.


<details>
  <summary>Details</summary>
Motivation: The work seeks to address the inefficiencies and scalability limitations of traditional kernel SGD algorithms when applied to large-scale supervised learning with general loss functions.

Method: The proposed method uses a finite-dimensional hypothesis space projected from the infinite series expansion of spherical radial basis functions. It uses a unified analytical framework for optimization and generalization, along with coordinate-wise updates to minimize computational and storage overhead.

Result: The algorithm achieves minimax-optimal convergence rates for both the last iterate and suffix average. It also achieves optimal strong convergence in the reproducing kernel Hilbert space while reducing computational and storage complexities.

Conclusion: The novel kernel SGD algorithm is theoretically robust and practically scalable, making it suitable for real-world large-scale supervised learning problems. Experimental results validate its efficiency and effectiveness.

Abstract: In this paper, we propose a novel kernel stochastic gradient descent (SGD)
algorithm for large-scale supervised learning with general losses. Compared to
traditional kernel SGD, our algorithm improves efficiency and scalability
through an innovative regularization strategy. By leveraging the infinite
series expansion of spherical radial basis functions, this strategy projects
the stochastic gradient onto a finite-dimensional hypothesis space, which is
adaptively scaled according to the bias-variance trade-off, thereby enhancing
generalization performance. Based on a new estimation of the spectral structure
of the kernel-induced covariance operator, we develop an analytical framework
that unifies optimization and generalization analyses. We prove that both the
last iterate and the suffix average converge at minimax-optimal rates, and we
further establish optimal strong convergence in the reproducing kernel Hilbert
space. Our framework accommodates a broad class of classical loss functions,
including least-squares, Huber, and logistic losses. Moreover, the proposed
algorithm significantly reduces computational complexity and achieves optimal
storage complexity by incorporating coordinate-wise updates from linear SGD,
thereby avoiding the costly pairwise operations typical of kernel SGD and
enabling efficient processing of streaming data. Finally, extensive numerical
experiments demonstrate the efficiency of our approach.

</details>


### [588] [Diffusion-Assisted Distillation for Self-Supervised Graph Representation Learning with MLPs](https://arxiv.org/abs/2510.04241)
*Seong Jin Ahn,Myoung-Ho Kim*

Main category: cs.LG

TL;DR: The paper introduces DAD-SGM, a novel method using denoising diffusion models to distill self-supervised knowledge from GNNs to lightweight MLPs for more effective and robust graph representation learning.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of distilling self-supervised graph representation learning knowledge from GNNs into MLPs due to the performance dependence on inductive biases, unlike supervised learning.

Method: DAD-SGM employs denoising diffusion models as a teacher assistant to bridge the capacity gap, enhancing the transfer of self-supervised knowledge from GNNs to MLPs.

Result: Experiments show that DAD-SGM outperforms existing GNN-to-MLP distillation methods in effectively transferring self-supervised knowledge for robust graph representation learning.

Conclusion: DAD-SGM is an effective solution for distilling self-supervised GNN knowledge into MLPs, improving performance, generalizability, and robustness, with results validated through extensive experimentation.

Abstract: For large-scale applications, there is growing interest in replacing Graph
Neural Networks (GNNs) with lightweight Multi-Layer Perceptrons (MLPs) via
knowledge distillation. However, distilling GNNs for self-supervised graph
representation learning into MLPs is more challenging. This is because the
performance of self-supervised learning is more related to the model's
inductive bias than supervised learning. This motivates us to design a new
distillation method to bridge a huge capacity gap between GNNs and MLPs in
self-supervised graph representation learning. In this paper, we propose
\textbf{D}iffusion-\textbf{A}ssisted \textbf{D}istillation for
\textbf{S}elf-supervised \textbf{G}raph representation learning with
\textbf{M}LPs (DAD-SGM). The proposed method employs a denoising diffusion
model as a teacher assistant to better distill the knowledge from the teacher
GNN into the student MLP. This approach enhances the generalizability and
robustness of MLPs in self-supervised graph representation learning. Extensive
experiments demonstrate that DAD-SGM effectively distills the knowledge of
self-supervised GNNs compared to state-of-the-art GNN-to-MLP distillation
methods. Our implementation is available at
https://github.com/SeongJinAhn/DAD-SGM.

</details>


### [589] [Efficient Latent Variable Causal Discovery: Combining Score Search and Targeted Testing](https://arxiv.org/abs/2510.04263)
*Joseph Ramsey,Bryan Andrews*

Main category: cs.LG

TL;DR: The paper introduces several score-guided algorithms aimed at improving causal structure discovery in the presence of latent variables. The methods increase precision and efficiency compared to previous approaches.


<details>
  <summary>Details</summary>
Motivation: Overcoming the challenges of causal structure learning with latent variables and selection bias, which cause scalability and reliability issues in observational data.

Method: The authors propose score-guided algorithms (BOSS-FCI, GRaSP-FCI, FCIT, LV-Dumb) to improve scalability and accuracy by targeting key shortfalls in exhaustive conditional independence testing.

Result: Simulations and real-data analyses show that these algorithms enhance efficiency, precision, and practical applicability in causal discovery tasks.

Conclusion: Targeted and score-guided strategies improve causal discovery, demonstrating scalability and reliability advantages in real-world settings.

Abstract: Learning causal structure from observational data is especially challenging
when latent variables or selection bias are present. The Fast Causal Inference
(FCI) algorithm addresses this setting but often performs exhaustive
conditional independence tests across many subsets, leading to spurious
independence claims, extra or missing edges, and unreliable orientations. We
present a family of score-guided mixed-strategy causal search algorithms that
build on this tradition. First, we introduce BOSS-FCI and GRaSP-FCI,
straightforward variants of GFCI that substitute BOSS or GRaSP for FGES,
thereby retaining correctness while incurring different scalability tradeoffs.
Second, we develop FCI Targeted-testing (FCIT), a novel mixed-strategy method
that improves upon these variants by replacing exhaustive all-subsets testing
with targeted tests guided by BOSS, yielding well-formed PAGs with higher
precision and efficiency. Finally, we propose a simple heuristic, LV-Dumb (also
known as BOSS-POD), which bypasses latent-variable-specific reasoning and
directly returns the PAG of the BOSS DAG. Although not strictly correct in the
FCI sense, it scales better and often achieves superior accuracy in practice.
Simulations and real-data analyses demonstrate that BOSS-FCI and GRaSP-FCI
provide sound baselines, FCIT improves both efficiency and reliability, and
LV-Dumb offers a practical heuristic with strong empirical performance.
Together, these method highlight the value of score-guided and targeted
strategies for scalable latent-variable causal discovery.

</details>


### [590] [Influence branching for learning to solve mixed-integer programs online](https://arxiv.org/abs/2510.04273)
*Paul Strang,Zacharie Alès,Côme Bissuel,Olivier Juan,Safia Kedad-Sidhoum,Emmanuel Rachelson*

Main category: cs.LG

TL;DR: The paper introduces 'Influence branching,' an online learning method for solving Mixed Integer Programs (MIPs) using graph-oriented variable selection and Thompson sampling, achieving competitive and generalizable results.


<details>
  <summary>Details</summary>
Motivation: To improve variable selection strategies in solving Mixed Integer Programs (MIPs) and address challenges in generalizing online frameworks for varied problem settings.

Method: Introducing Influence branching—a graph-oriented variable selection heuristic applied during branch and bound. Thompson sampling is used to optimize the heuristic by ranking graph representations based on computational speed compared to SCIP.

Result: The method achieves performance comparable to state-of-the-art online learning approaches and demonstrates strong generalizability to diverse MIP problem variations.

Conclusion: Influence branching provides a significant contribution to improving the efficiency and adaptability of online learning frameworks for solving MIPs in varied problem settings, supported by competitive results.

Abstract: On the occasion of the 20th Mixed Integer Program Workshop's computational
competition, this work introduces a new approach for learning to solve MIPs
online. Influence branching, a new graph-oriented variable selection strategy,
is applied throughout the first iterations of the branch and bound algorithm.
This branching heuristic is optimized online with Thompson sampling, which
ranks the best graph representations of MIP's structure according to
computational speed up over SCIP. We achieve results comparable to state of the
art online learning methods. Moreover, our results indicate that our method
generalizes well to more general online frameworks, where variations in
constraint matrix, constraint vector and objective coefficients can all occur
and where more samples are available.

</details>


### [591] [HoRA: Cross-Head Low-Rank Adaptation with Joint Hypernetworks](https://arxiv.org/abs/2510.04295)
*Nghiem T. Diep,Dung Le,Tuan Truong,Tan Dinh,Huy Nguyen,Nhat Ho*

Main category: cs.LG

TL;DR: The paper introduces HoRA, a new method for parameter-efficient fine-tuning that leverages hypernetworks to jointly adapt attention heads, outperforming LoRA in efficiency and results.


<details>
  <summary>Details</summary>
Motivation: LoRA independently adapts each attention head, missing the opportunity for cross-head synergies, which can limit its effectiveness.

Method: HoRA employs joint hypernetworks to generate low-rank matrices for all attention heads, enabling cross-head information sharing.

Result: HoRA achieves better sample efficiency and surpasses LoRA and other PEFT methods in experimental performance, with minimal extra trainable parameters.

Conclusion: HoRA addresses LoRA's limitations by promoting cross-attention head synergies, making it a more efficient and effective PEFT approach.

Abstract: Low-Rank Adaptation (LoRA) is a parameter-efficient fine-tuning (PEFT)
technique that adapts large pre-trained models by adding low-rank matrices to
their weight updates. However, in the context of fine-tuning multi-head
self-attention (MHA), LoRA has been employed to adapt each attention head
separately, thereby overlooking potential synergies across different heads. To
mitigate this issue, we propose a novel Hyper-shared Low-Rank Adaptation (HoRA)
method, which utilizes joint hypernetworks to generate low-rank matrices across
attention heads. By coupling their adaptation through a shared generator, HoRA
encourages cross-head information sharing, and thus directly addresses the
aforementioned limitation of LoRA. By comparing LoRA and HoRA through the lens
of hierarchical mixture of experts, our theoretical findings reveal that the
latter achieves superior sample efficiency to the former. Furthermore, through
extensive experiments across diverse language and vision benchmarks, we
demonstrate that HoRA outperforms LoRA and other PEFT methods while requiring
only a marginal increase in the number of trainable parameters.

</details>


### [592] [Activation Steering with a Feedback Controller](https://arxiv.org/abs/2510.04309)
*Dung V. Nguyen,Hieu M. Vu,Nhi Y. Pham,Lei Zhang,Tan M. Nguyen*

Main category: cs.LG

TL;DR: The paper proposes a novel steering method for language models based on control theory, offering a more reliable and theoretically backed approach.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve the safety and reliable deployment of large language models by addressing the lack of theoretical guarantees in current steering methods.

Method: The authors introduce PID Steering, which leverages activation steering using a control-theoretic approach featuring proportional, integral, and derivative controllers.

Result: Experiments across multiple benchmarks show that PID Steering outperforms existing methods in achieving robust and consistent control of language model behaviors.

Conclusion: PID Steering provides a principled, theoretically grounded framework that enhances control and behavioral reliability in language models, while being lightweight and modular.

Abstract: Controlling the behaviors of large language models (LLM) is fundamental to
their safety alignment and reliable deployment. However, existing steering
methods are primarily driven by empirical insights and lack theoretical
performance guarantees. In this work, we develop a control-theoretic foundation
for activation steering by showing that popular steering methods correspond to
the proportional (P) controllers, with the steering vector serving as the
feedback signal. Building on this finding, we propose
Proportional-Integral-Derivative (PID) Steering, a principled framework that
leverages the full PID controller for activation steering in LLMs. The
proportional (P) term aligns activations with target semantic directions, the
integral (I) term accumulates errors to enforce persistent corrections across
layers, and the derivative (D) term mitigates overshoot by counteracting rapid
activation changes. This closed-loop design yields interpretable error dynamics
and connects activation steering to classical stability guarantees in control
theory. Moreover, PID Steering is lightweight, modular, and readily integrates
with state-of-the-art steering methods. Extensive experiments across multiple
LLM families and benchmarks demonstrate that PID Steering consistently
outperforms existing approaches, achieving more robust and reliable behavioral
control.

</details>


### [593] [Crash Severity Prediction Using Deep Learning Approaches: A Hybrid CNN-RNN Framework](https://arxiv.org/abs/2510.04316)
*Sahar Koohfar*

Main category: cs.LG

TL;DR: The paper implements a hybrid CNN-RNN deep learning model to predict crash severity and finds it outperforms multiple statistical and machine learning benchmarks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance crash severity prediction for intelligent transportation systems, aiding timely medical and transportation responses to mitigate traffic accident consequences.

Method: The study employs a hybrid deep learning CNN-RNN model, compared against statistical and machine learning models using a dataset of 15,870 accidents from Virginia highway spanning seven years.

Result: The CNN-RNN hybrid model outperformed logistic regression, naive Bayes, KNN, decision tree, RNN, and CNN models in prediction accuracy.

Conclusion: The results underscore the effectiveness of combining RNN and CNN models to improve crash severity prediction accuracy in intelligent transportation systems.

Abstract: Accurate and timely prediction of crash severity is crucial in mitigating the
severe consequences of traffic accidents. Accurate and timely prediction of
crash severity is crucial in mitigating the severe consequences of traffic
accidents. In order to provide appropriate levels of medical assistance and
transportation services, an intelligent transportation system relies on
effective prediction methods. Deep learning models have gained popularity in
this domain due to their capability to capture non-linear relationships among
variables. In this research, we have implemented a hybrid CNN-RNN deep learning
model for crash severity prediction and compared its performance against widely
used statistical and machine learning models such as logistic regression,
na\"ive bayes classifier, K-Nearest Neighbors (KNN), decision tree, and
individual deep learning models: RNN and CNN. This study employs a methodology
that considers the interconnected relationships between various features of
traffic accidents. The study was conducted using a dataset of 15,870 accident
records gathered over a period of seven years between 2015 and 2021 on Virginia
highway I-64. The findings demonstrate that the proposed CNN-RNN hybrid model
has outperformed all benchmark models in terms of predicting crash severity.
This result illustrates the effectiveness of the hybrid model as it combines
the advantages of both RNN and CNN models in order to achieve greater accuracy
in the prediction process.

</details>


### [594] [FairAgent: Democratizing Fairness-Aware Machine Learning with LLM-Powered Agents](https://arxiv.org/abs/2510.04317)
*Yucong Dai,Lu Zhang,Feng Luo,Mashrur Chowdhury,Yongkai Wu*

Main category: cs.LG

TL;DR: FairAgent is an automated system leveraging LLMs to simplify fairness-aware machine learning model development, eliminating the need for deep technical expertise and significantly reducing the required development time.


<details>
  <summary>Details</summary>
Motivation: The complexity of training unbiased ML models for high-stakes applications, requiring expertise in fairness definitions, metrics, and balancing performance with fairness.

Method: FairAgent automatically analyzes datasets, handles preprocessing, performs feature engineering, and integrates suitable bias mitigation strategies based on user needs.

Result: Experiments show FairAgent achieves notable performance improvements while reducing development time and expertise barriers.

Conclusion: FairAgent makes fairness-aware machine learning accessible to practitioners by removing technical barriers and improving efficiency.

Abstract: Training fair and unbiased machine learning models is crucial for high-stakes
applications, yet it presents significant challenges. Effective bias mitigation
requires deep expertise in fairness definitions, metrics, data preprocessing,
and machine learning techniques. In addition, the complex process of balancing
model performance with fairness requirements while properly handling sensitive
attributes makes fairness-aware model development inaccessible to many
practitioners. To address these challenges, we introduce FairAgent, an
LLM-powered automated system that significantly simplifies fairness-aware model
development. FairAgent eliminates the need for deep technical expertise by
automatically analyzing datasets for potential biases, handling data
preprocessing and feature engineering, and implementing appropriate bias
mitigation strategies based on user requirements. Our experiments demonstrate
that FairAgent achieves significant performance improvements while
significantly reducing development time and expertise requirements, making
fairness-aware machine learning more accessible to practitioners.

</details>


### [595] [FoilDiff: A Hybrid Transformer Backbone for Diffusion-based Modelling of 2D Airfoil Flow Fields](https://arxiv.org/abs/2510.04325)
*Kenechukwu Ogbuagu,Sepehr Maleki,Giuseppe Bruni,Senthil Krishnababu*

Main category: cs.LG

TL;DR: The paper introduces FoilDiff, a diffusion-based model combining CNNs and transformers, reducing prediction errors by 85% in airfoil flow predictions.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the computational expense of CFD models by creating accurate and quicker surrogate models for aerodynamic predictions.

Method: FoilDiff employs a hybrid-backbone denoising network combining convolutional and transformer-based attention mechanisms, leveraging DDIM sampling and encoded aerodynamic conditions for generalisation.

Result: FoilDiff achieves 85% reduction in mean prediction errors compared to state-of-the-art models and provides better-calibrated predictive uncertainty.

Conclusion: FoilDiff offers a highly accurate and efficient solution for predicting flow fields around airfoils, outperforming existing diffusion-based models.

Abstract: The accurate prediction of flow fields around airfoils is crucial for
aerodynamic design and optimisation. Computational Fluid Dynamics (CFD) models
are effective but computationally expensive, thus inspiring the development of
surrogate models to enable quicker predictions. These surrogate models can be
based on deep learning architectures, such as Convolutional Neural Networks
(CNNs), Graph Neural Networks (GNNs), and Diffusion Models (DMs). Diffusion
models have shown significant promise in predicting complex flow fields. In
this work, we propose FoilDiff, a diffusion-based surrogate model with a
hybrid-backbone denoising network. This hybrid design combines the power of
convolutional feature extraction and transformer-based global attention to
generate more adaptable and accurate representations of flow structures.
FoilDiff takes advantage of Denoising Diffusion Implicit Model (DDIM) sampling
to optimise the efficiency of the sampling process at no additional cost to
model generalisation. We used encoded representations of Reynolds number, angle
of attack, and airfoil geometry to define the input space for generalisation
across a wide range of aerodynamic conditions. When evaluated against
state-of-the-art models, FoilDiff shows significant performance improvements,
with mean prediction errors reducing by up to 85\% on the same datasets. The
results have demonstrated that FoilDiff can provide both more accurate
predictions and better-calibrated predictive uncertainty than existing
diffusion-based models.

</details>


### [596] [DoRAN: Stabilizing Weight-Decomposed Low-Rank Adaptation via Noise Injection and Auxiliary Networks](https://arxiv.org/abs/2510.04331)
*Nghiem T. Diep,Hien Dang,Tuan Truong,Tan Dinh,Huy Nguyen,Nhat Ho*

Main category: cs.LG

TL;DR: DoRAN improves parameter-efficient fine-tuning methods with stabilization through noise injection and dynamic low-rank matrix generation, outperforming existing approaches.


<details>
  <summary>Details</summary>
Motivation: Enhance the learning capacity, training stability, and sample efficiency of PEFT methods, particularly DoRA.

Method: Introduce noise-based regularization into weight decomposition and use auxiliary networks to dynamically generate low-rank matrices for better parameter coupling.

Result: DoRAN surpasses LoRA, DoRA, and other PEFT baselines in vision and language benchmarks with improved stability and sample efficiency.

Conclusion: Combining noise-based stabilization and dynamic parameter generation is effective for robust fine-tuning of large-scale models.

Abstract: Parameter-efficient fine-tuning (PEFT) methods have become the standard
paradigm for adapting large-scale models. Among these techniques,
Weight-Decomposed Low-Rank Adaptation (DoRA) has been shown to improve both the
learning capacity and training stability of the vanilla Low-Rank Adaptation
(LoRA) method by explicitly decomposing pre-trained weights into magnitude and
directional components. In this work, we propose DoRAN, a new variant of DoRA
designed to further stabilize training and boost the sample efficiency of DoRA.
Our approach includes two key stages: (i) injecting noise into the denominator
of DoRA's weight decomposition, which serves as an adaptive regularizer to
mitigate instabilities; and (ii) replacing static low-rank matrices with
auxiliary networks that generate them dynamically, enabling parameter coupling
across layers and yielding better sample efficiency in both theory and
practice. Comprehensive experiments on vision and language benchmarks show that
DoRAN consistently outperforms LoRA, DoRA, and other PEFT baselines. These
results underscore the effectiveness of combining stabilization through
noise-based regularization with network-based parameter generation, offering a
promising direction for robust and efficient fine-tuning of foundation models.

</details>


### [597] [Critical appraisal of artificial intelligence for rare-event recognition: principles and pharmacovigilance case studies](https://arxiv.org/abs/2510.04341)
*G. Niklas Noren,Eva-Lisa Meldau,Johan Ellenius*

Main category: cs.LG

TL;DR: The paper addresses the challenge of evaluating AI performance in rare-event recognition, with a specific emphasis on pharmacovigilance applications, and provides a framework and checklist for better evaluation and integration.


<details>
  <summary>Details</summary>
Motivation: Rare-event recognition, such as detecting low-prevalence events in high-stakes scenarios, requires robust evaluation methods to ensure real-world impact beyond superficial accuracy.

Method: The authors propose a structured framework including statistical evaluation, robustness testing, and integration into workflows, along with a Structured Case-Level Examination (SCLE) approach and a comprehensive checklist for better model appraisal.

Result: By applying their framework to pharmacovigilance, the authors identify pitfalls like unrealistic class balance and propose refined cost-sensitive targets for aligning AI models with operational goals.

Conclusion: The outlined principles and framework are applicable to fields with scarce positives and asymmetric error costs, providing actionable strategies for enhancing AI's real-world impact in rare-event contexts.

Abstract: Many high-stakes AI applications target low-prevalence events, where apparent
accuracy can conceal limited real-world value. Relevant AI models range from
expert-defined rules and traditional machine learning to generative LLMs
constrained for classification. We outline key considerations for critical
appraisal of AI in rare-event recognition, including problem framing and test
set design, prevalence-aware statistical evaluation, robustness assessment, and
integration into human workflows. In addition, we propose an approach to
structured case-level examination (SCLE), to complement statistical performance
evaluation, and a comprehensive checklist to guide procurement or development
of AI models for rare-event recognition. We instantiate the framework in
pharmacovigilance, drawing on three studies: rule-based retrieval of
pregnancy-related reports; duplicate detection combining machine learning with
probabilistic record linkage; and automated redaction of person names using an
LLM. We highlight pitfalls specific to the rare-event setting including
optimism from unrealistic class balance and lack of difficult positive controls
in test sets - and show how cost-sensitive targets align model performance with
operational value. While grounded in pharmacovigilance practice, the principles
generalize to domains where positives are scarce and error costs may be
asymmetric.

</details>


### [598] [Learning to Predict Chaos: Curriculum-Driven Training for Robust Forecasting of Chaotic Dynamics](https://arxiv.org/abs/2510.04342)
*Harshil Vejendla*

Main category: cs.LG

TL;DR: The paper introduces Curriculum Chaos Forecasting (CCF), a training paradigm designed to improve forecasting in chaotic systems by gradually training AI models on increasingly complex dynamical systems.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of forecasting chaotic systems, which involves exponential amplification of small prediction errors, and to avoid the limitations of over-specialization or indiscriminate data mixing in modern machine learning approaches.

Method: CCF involves organizing training data based on dynamical systems theory, using a curriculum from simple, periodic systems to complex chaotic dynamics. Complexity is quantified using metrics like the largest Lyapunov exponent and attractor dimension. Over 50 synthetic ODE/PDE systems are curated for this curriculum.

Result: CCF significantly enhances prediction performance on unseen benchmarks like Sunspot numbers, electricity demand, and human ECG signals, extending the valid prediction horizon by up to 40% and doubling it compared to models trained on real-world data alone.

Conclusion: The proposed CCF approach effectively builds robust and generalizable representations of chaotic systems, improving prediction performance while maintaining consistency across different neural architectures. Its structured curriculum is key to these improvements.

Abstract: Forecasting chaotic systems is a cornerstone challenge in many scientific
fields, complicated by the exponential amplification of even infinitesimal
prediction errors. Modern machine learning approaches often falter due to two
opposing pitfalls: over-specializing on a single, well-known chaotic system
(e.g., Lorenz-63), which limits generalizability, or indiscriminately mixing
vast, unrelated time-series, which prevents the model from learning the nuances
of any specific dynamical regime. We propose Curriculum Chaos Forecasting
(CCF), a training paradigm that bridges this gap. CCF organizes training data
based on fundamental principles of dynamical systems theory, creating a
curriculum that progresses from simple, periodic behaviors to highly complex,
chaotic dynamics. We quantify complexity using the largest Lyapunov exponent
and attractor dimension, two well-established metrics of chaos. By first
training a sequence model on predictable systems and gradually introducing more
chaotic trajectories, CCF enables the model to build a robust and generalizable
representation of dynamical behaviors. We curate a library of over 50 synthetic
ODE/PDE systems to build this curriculum. Our experiments show that
pre-training with CCF significantly enhances performance on unseen, real-world
benchmarks. On datasets including Sunspot numbers, electricity demand, and
human ECG signals, CCF extends the valid prediction horizon by up to 40%
compared to random-order training and more than doubles it compared to training
on real-world data alone. We demonstrate that this benefit is consistent across
various neural architectures (GRU, Transformer) and provide extensive ablations
to validate the importance of the curriculum's structure.

</details>


### [599] [From News to Returns: A Granger-Causal Hypergraph Transformer on the Sphere](https://arxiv.org/abs/2510.04357)
*Anoushka Harit,Zhongtian Sun,Jongmin Yu*

Main category: cs.LG

TL;DR: The paper introduces the Causal Sphere Hypergraph Transformer (CSHT), a method for interpretable financial time-series forecasting that combines Granger-causal structures, Riemannian geometry, and masked attention to model financial influences.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance financial time-series forecasting by integrating causal structures and geometric consistency to address challenges of uncertainty, regime shifts, and interpretability in financial modeling.

Method: The CSHT framework leverages Granger-causal hypergraphs and encodes dependencies on the surface of a hypersphere, while using causally masked Transformer attention to ensure directional and geometric consistency.

Result: The approach outperforms baseline methods on tasks such as return prediction, regime classification, and asset ranking when evaluated on S&P 500 data from 2018 to 2023, including the 2020 COVID-19 shock.

Conclusion: The results demonstrate that CSHT provides robust generalization and interpretable pathways for understanding the impact of macroeconomic events on stocks, offering a trustworthy approach to financial forecasting.

Abstract: We propose the Causal Sphere Hypergraph Transformer (CSHT), a novel
architecture for interpretable financial time-series forecasting that unifies
\emph{Granger-causal hypergraph structure}, \emph{Riemannian geometry}, and
\emph{causally masked Transformer attention}. CSHT models the directional
influence of financial news and sentiment on asset returns by extracting
multivariate Granger-causal dependencies, which are encoded as directional
hyperedges on the surface of a hypersphere. Attention is constrained via
angular masks that preserve both temporal directionality and geometric
consistency. Evaluated on S\&P 500 data from 2018 to 2023, including the 2020
COVID-19 shock, CSHT consistently outperforms baselines across return
prediction, regime classification, and top-asset ranking tasks. By enforcing
predictive causal structure and embedding variables in a Riemannian manifold,
CSHT delivers both \emph{robust generalisation across market regimes} and
\emph{transparent attribution pathways} from macroeconomic events to
stock-level responses. These results suggest that CSHT is a principled and
practical solution for trustworthy financial forecasting under uncertainty.

</details>


### [600] [Quantifying Ambiguity in Categorical Annotations: A Measure and Statistical Inference Framework](https://arxiv.org/abs/2510.04366)
*Christopher Klugmann,Daniel Kondermann*

Main category: cs.LG

TL;DR: The paper introduces a novel ambiguity measure to quantify uncertainty in categorical tasks, separating class indistinguishability from explicit unresolvability. Tools for inference are developed to improve its application and understanding.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address ambiguity in human-generated categorical annotations, moving beyond the concept of annotator errors to quantify true aleatoric uncertainty.

Method: The paper proposes a scalar measure, inspired by quadratic entropy but treating a 'can't solve' category asymmetrically. It defines frequentist estimators and Bayesian methods using Dirichlet priors for statistical insights.

Result: The ambiguity measure is shown to improve dataset-quality assessments and machine-learning workflows via estimation examples and calibration.

Conclusion: The paper concludes that the new ambiguity measure effectively distinguishes between types of uncertainty and provides powerful statistical tools for practical usage in inference and downstream tasks.

Abstract: Human-generated categorical annotations frequently produce empirical response
distributions (soft labels) that reflect ambiguity rather than simple annotator
error. We introduce an ambiguity measure that maps a discrete response
distribution to a scalar in the unit interval, designed to quantify aleatoric
uncertainty in categorical tasks. The measure bears a close relationship to
quadratic entropy (Gini-style impurity) but departs from those indices by
treating an explicit "can't solve" category asymmetrically, thereby separating
uncertainty arising from class-level indistinguishability from uncertainty due
to explicit unresolvability. We analyze the measure's formal properties and
contrast its behavior with a representative ambiguity measure from the
literature. Moving beyond description, we develop statistical tools for
inference: we propose frequentist point estimators for population ambiguity and
derive the Bayesian posterior over ambiguity induced by Dirichlet priors on the
underlying probability vector, providing a principled account of epistemic
uncertainty. Numerical examples illustrate estimation, calibration, and
practical use for dataset-quality assessment and downstream machine-learning
workflows.

</details>


### [601] [GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks](https://arxiv.org/abs/2510.04374)
*Tejal Patwardhan,Rachel Dias,Elizabeth Proehl,Grace Kim,Michele Wang,Olivia Watkins,Simón Posada Fishman,Marwan Aljubeh,Phoebe Thacker,Laurance Fauconnet,Natalie S. Kim,Patrick Chao,Samuel Miserendino,Gildas Chabot,David Li,Michael Sharman,Alexandra Barr,Amelia Glaese,Jerry Tworek*

Main category: cs.LG

TL;DR: GDPval is a benchmark assessing AI model performance on tasks relevant to U.S. economic activities across various industries, showing improving alignment with expert quality over time.


<details>
  <summary>Details</summary>
Motivation: The paper aims to measure and analyze the ability of AI models to perform economically valuable tasks, emphasizing their potential for improving efficiency and cost-effectiveness in professional industries.

Method: The authors develop GDPval, a benchmark covering work activities for 44 occupations across 9 major industries in the U.S., using tasks derived from professionals with extensive experience. They also provide tools for assessing model performance, including automated grading.

Result: Frontier models demonstrate linear performance improvements over time, approaching expert deliverable quality, with enhancements observed from increased reasoning effort and contextual scaffolding.

Conclusion: AI models paired with human oversight show promise in delivering high-quality outputs faster and cheaper, fostering advancements in real-world applications. The authors contribute a public automated grading service to aid further research.

Abstract: We introduce GDPval, a benchmark evaluating AI model capabilities on
real-world economically valuable tasks. GDPval covers the majority of U.S.
Bureau of Labor Statistics Work Activities for 44 occupations across the top 9
sectors contributing to U.S. GDP (Gross Domestic Product). Tasks are
constructed from the representative work of industry professionals with an
average of 14 years of experience. We find that frontier model performance on
GDPval is improving roughly linearly over time, and that the current best
frontier models are approaching industry experts in deliverable quality. We
analyze the potential for frontier models, when paired with human oversight, to
perform GDPval tasks cheaper and faster than unaided experts. We also
demonstrate that increased reasoning effort, increased task context, and
increased scaffolding improves model performance on GDPval. Finally, we
open-source a gold subset of 220 tasks and provide a public automated grading
service at evals.openai.com to facilitate future research in understanding
real-world model capabilities.

</details>


### [602] [Adaptive Weighted Loss for Sequential Recommendations on Sparse Domains](https://arxiv.org/abs/2510.04375)
*Akshay Mittal,Vinay Venkatesh,Krishna Kandi,Shalini Sudarshan*

Main category: cs.LG

TL;DR: The paper presents a dynamic weighted loss approach tailored to enhance sequential recommendation systems, addressing challenges in sparse or niche domains with significant empirical and theoretical backing.


<details>
  <summary>Details</summary>
Motivation: Traditional single-model sequential recommendation frameworks struggle with underperformance in sparse or niche domains. Previous fixed-weight methods like PinnerFormerLite proved insufficient due to uniform loss weighting in vast datasets.

Method: The approach employs a novel, adaptive Dynamic Weighted Loss function that dynamically adjusts loss weights based on domain sparsity, supported by convergence proofs, stability analysis, and bounded complexity.

Result: The proposed method, tested on four diverse datasets with baseline comparisons, demonstrates superior performance, particularly in sparse domains, with improvements in Recall@10 and NDCG@10 metrics, while adding minimal computational overhead.

Conclusion: The dynamic weighting system is a breakthrough in personalized recommendation, effectively elevating sparse domain performance without compromising on denser domains or computational efficiency.

Abstract: The effectiveness of single-model sequential recommendation architectures,
while scalable, is often limited when catering to "power users" in sparse or
niche domains. Our previous research, PinnerFormerLite, addressed this by using
a fixed weighted loss to prioritize specific domains. However, this approach
can be sub-optimal, as a single, uniform weight may not be sufficient for
domains with very few interactions, where the training signal is easily diluted
by the vast, generic dataset.
  This paper proposes a novel, data-driven approach: a Dynamic Weighted Loss
function with comprehensive theoretical foundations and extensive empirical
validation. We introduce an adaptive algorithm that adjusts the loss weight for
each domain based on its sparsity in the training data, assigning a higher
weight to sparser domains and a lower weight to denser ones. This ensures that
even rare user interests contribute a meaningful gradient signal, preventing
them from being overshadowed.
  We provide rigorous theoretical analysis including convergence proofs,
complexity analysis, and bounds analysis to establish the stability and
efficiency of our approach. Our comprehensive empirical validation across four
diverse datasets (MovieLens, Amazon Electronics, Yelp Business, LastFM Music)
with state-of-the-art baselines (SIGMA, CALRec, SparseEnNet) demonstrates that
this dynamic weighting system significantly outperforms all comparison methods,
particularly for sparse domains, achieving substantial lifts in key metrics
like Recall at 10 and NDCG at 10 while maintaining performance on denser
domains and introducing minimal computational overhead.

</details>


### [603] [Real-time Prediction of Urban Sound Propagation with Conditioned Normalizing Flows](https://arxiv.org/abs/2510.04510)
*Achim Eckerle,Martin Spitznagel,Janis Keuper*

Main category: cs.LG

TL;DR: The paper introduces a fast and accurate method for urban noise prediction using conditional Normalizing Flows, enabling real-time sound-pressure map generation for urban planning and operational use.


<details>
  <summary>Details</summary>
Motivation: Urban noise prediction is crucial for public health and regulatory workflows, but traditional physics-based methods are too slow for iterative studies.

Method: The study uses conditional Normalizing Flows (Full-Glow) to generate standards-compliant sound-pressure maps in real-time from 2D urban layouts on commodity hardware.

Result: The proposed model accelerates noise map generation by over 2000x compared to reference solvers and improves NLoS accuracy by up to 24%, achieving high structural fidelity and low MAE.

Conclusion: The approach provides a practical and efficient tool for urban planning and regulatory compliance, offering instant recomputation under changing conditions.

Abstract: Accurate and fast urban noise prediction is pivotal for public health and for
regulatory workflows in cities, where the Environmental Noise Directive
mandates regular strategic noise maps and action plans, often needed in
permission workflows, right-of-way allocation, and construction scheduling.
Physics-based solvers are too slow for such time-critical, iterative "what-if"
studies. We evaluate conditional Normalizing Flows (Full-Glow) for generating
for generating standards-compliant urban sound-pressure maps from 2D urban
layouts in real time per 256x256 map on a single RTX 4090), enabling
interactive exploration directly on commodity hardware. On datasets covering
Baseline, Diffraction, and Reflection regimes, our model accelerates map
generation by >2000 times over a reference solver while improving NLoS accuracy
by up to 24% versus prior deep models; in Baseline NLoS we reach 0.65 dB MAE
with high structural fidelity. The model reproduces diffraction and
interference patterns and supports instant recomputation under source or
geometry changes, making it a practical engine for urban planning, compliance
mapping, and operations (e.g., temporary road closures, night-work variance
assessments).

</details>


### [604] [Categorical Invariants of Learning Dynamics](https://arxiv.org/abs/2510.04376)
*Abdulrahman Tamim*

Main category: cs.LG

TL;DR: This paper introduces a categorical perspective to neural network training, focusing on homotopy classes of optimization paths and their relation to model generalization.


<details>
  <summary>Details</summary>
Motivation: To provide a new theoretical framework for understanding deep learning training and its generalization capabilities using category theory.

Method: The paper analyzes neural network training as a structure-preserving transformation between network parameters and learned representations, using experiments and categorical tools like persistent homology and pullback constructions.

Result: It demonstrates that training paths belonging to the same homotopy class have highly similar generalization performance (within 0.5%), while differing homotopy paths show a difference of over 3%.

Conclusion: The categorical framework offers theoretical insights into deep learning's mechanisms and introduces practical tools for improving model generalization and robustness.

Abstract: Neural network training is typically viewed as gradient descent on a loss
surface. We propose a fundamentally different perspective: learning is a
structure-preserving transformation (a functor L) between the space of network
parameters (Param) and the space of learned representations (Rep). This
categorical framework reveals that different training runs producing similar
test performance often belong to the same homotopy class (continuous
deformation family) of optimization paths. We show experimentally that networks
converging via homotopic trajectories generalize within 0.5% accuracy of each
other, while non-homotopic paths differ by over 3%. The theory provides
practical tools: persistent homology identifies stable minima predictive of
generalization (R^2 = 0.82 correlation), pullback constructions formalize
transfer learning, and 2-categorical structures explain when different
optimization algorithms yield functionally equivalent models. These categorical
invariants offer both theoretical insight into why deep learning works and
concrete algorithmic principles for training more robust networks.

</details>


### [605] [Score-based Greedy Search for Structure Identification of Partially Observed Linear Causal Models](https://arxiv.org/abs/2510.04378)
*Xinshuai Dong,Ignavier Ng,Haoyue Dai,Jiaqi Sun,Xiangchen Song,Peter Spirtes,Kun Zhang*

Main category: cs.LG

TL;DR: This paper introduces a score-based greedy search method for identifying causal structures in partially observed systems, including latent variables.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address challenges like multiple testing and error propagation in constraint-based causal discovery, and to explore if a score-based greedy search method can handle scenarios involving latent variables.

Method: The authors propose the Generalized N Factor Model for identifying structures with latent variables, ensuring identifiability guarantees, and develop the Latent variable Greedy Equivalence Search (LGES) algorithm, designed to efficiently navigate graph space.

Result: Experiments on synthetic and real-life datasets demonstrate the method's effectiveness, ensuring accurate structural identification up to the Markov equivalence class.

Conclusion: This novel approach proves score-based greedy search is viable for partially observed systems, providing reliable identification of latent variable structures.

Abstract: Identifying the structure of a partially observed causal system is essential
to various scientific fields. Recent advances have focused on constraint-based
causal discovery to solve this problem, and yet in practice these methods often
face challenges related to multiple testing and error propagation. These issues
could be mitigated by a score-based method and thus it has raised great
attention whether there exists a score-based greedy search method that can
handle the partially observed scenario. In this work, we propose the first
score-based greedy search method for the identification of structure involving
latent variables with identifiability guarantees. Specifically, we propose
Generalized N Factor Model and establish the global consistency:
  the true structure including latent variables can be identified up to the
Markov equivalence class by using score. We then design
  Latent variable Greedy Equivalence Search (LGES), a greedy search algorithm
for this class of model with well-defined operators,
  which search very efficiently over the graph space to find the optimal
structure. Our experiments on both synthetic and real-life data validate the
effectiveness of our method (code will be publicly available).

</details>


### [606] [SSM-CGM: Interpretable State-Space Forecasting Model of Continuous Glucose Monitoring for Personalized Diabetes Management](https://arxiv.org/abs/2510.04386)
*Shakson Isaac,Yentl Collin,Chirag Patel*

Main category: cs.LG

TL;DR: The paper introduces SSM-CGM, a neural model for glucose forecasting with improved accuracy, interpretability, and personalized diabetes management.


<details>
  <summary>Details</summary>
Motivation: Continuous glucose monitoring data lacks interpretability for clinical use, making personalized diabetes management challenging.

Method: The paper proposes SSM-CGM, a Mamba-based neural state-space model using CGM and wearable signals with features like variable selection, temporal attribution, and counterfactual forecasting.

Result: SSM-CGM improves short-term forecasting accuracy over a Temporal Fusion Transformer baseline and provides insights into how physiological changes impact glucose levels.

Conclusion: SSM-CGM offers an interpretable, physiologically informed tool for better diabetes management through personalized forecasting.

Abstract: Continuous glucose monitoring (CGM) generates dense data streams critical for
diabetes management, but most used forecasting models lack interpretability for
clinical use. We present SSM-CGM, a Mamba-based neural state-space forecasting
model that integrates CGM and wearable activity signals from the AI-READI
cohort. SSM-CGM improves short-term accuracy over a Temporal Fusion Transformer
baseline, adds interpretability through variable selection and temporal
attribution, and enables counterfactual forecasts simulating how planned
changes in physiological signals (e.g., heart rate, respiration) affect
near-term glucose. Together, these features make SSM-CGM an interpretable,
physiologically grounded framework for personalized diabetes management.

</details>


### [607] [Post-training quantization of vision encoders needs prefixing registers](https://arxiv.org/abs/2510.04547)
*Seunghyeon Kim,Jinho Kim,Taesun Yeom,Wonpyo Park,Kyuyeun Kim,Jaeho Lee*

Main category: cs.LG

TL;DR: The paper proposes RegCache, a training-free algorithm to address outliers in transformer-based vision encoders like CLIP, improving post-training quantization for reduced inference cost.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enable real-time processing of large-scale visual data for applications like robotics and AI agents, where reducing inference costs of vision encoders without significant accuracy losses is crucial.

Method: The paper introduces RegCache, which reduces outliers in vision encoders by adding prefix tokens that are semantically irrelevant, using middle-layer prefixing and token deletion strategies.

Result: RegCache successfully improves the accuracy of quantized models on both text-supervised and self-supervised vision encoders.

Conclusion: RegCache effectively mitigates outliers in vision encoders, providing a practical solution for improving quantization accuracy without requiring additional training.

Abstract: Transformer-based vision encoders -- such as CLIP -- are central to
multimodal intelligence, powering applications from autonomous web agents to
robotic control. Since these applications often demand real-time processing of
massive visual data, reducing the inference cost of vision encoders is
critical. Post-training quantization offers a practical path, but remains
challenging even at 8-bit precision due to massive-scale activations (i.e.,
outliers). In this work, we propose $\textit{RegCache}$, a training-free
algorithm to mitigate outliers in vision encoders, enabling quantization with
significantly smaller accuracy drops. The proposed RegCache introduces
outlier-prone yet semantically meaningless prefix tokens to the target vision
encoder, which prevents other tokens from having outliers. Notably, we observe
that outliers in vision encoders behave differently from those in language
models, motivating two technical innovations: middle-layer prefixing and token
deletion. Experiments show that our method consistently improves the accuracy
of quantized models across both text-supervised and self-supervised vision
encoders.

</details>


### [608] [Achieve Performatively Optimal Policy for Performative Reinforcement Learning](https://arxiv.org/abs/2510.04430)
*Ziyi Chen,Heng Huang*

Main category: cs.LG

TL;DR: This paper introduces a zeroth-order Frank-Wolfe algorithm (0-FW) to achieve polynomial-time convergence to performatively optimal (PO) policies in performative reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: Existing performative reinforcement learning methods only aim for performatively stable (PS) policies, leaving a performance gap relative to the optimal PO policies. The motivation is to address this gap using an efficient algorithm.

Method: The authors propose a zeroth-order Frank-Wolfe (0-FW) algorithm that approximates the performative policy gradient within the Frank-Wolfe optimization framework. This method includes convergence analysis based on gradient dominance and bounded stationary points.

Result: The proposed 0-FW algorithm achieves the first polynomial-time convergence to PO policies under specific conditions. Experimental results show that it outperforms existing methods in locating PO policies.

Conclusion: The 0-FW algorithm is a significant advancement in performative reinforcement learning, effectively bridging the gap between PS and PO policies and demonstrating strong empirical success.

Abstract: Performative reinforcement learning is an emerging dynamical decision making
framework, which extends reinforcement learning to the common applications
where the agent's policy can change the environmental dynamics. Existing works
on performative reinforcement learning only aim at a performatively stable (PS)
policy that maximizes an approximate value function. However, there is a
provably positive constant gap between the PS policy and the desired
performatively optimal (PO) policy that maximizes the original value function.
In contrast, this work proposes a zeroth-order Frank-Wolfe algorithm (0-FW)
algorithm with a zeroth-order approximation of the performative policy gradient
in the Frank-Wolfe framework, and obtains \textbf{the first polynomial-time
convergence to the desired PO} policy under the standard regularizer dominance
condition. For the convergence analysis, we prove two important properties of
the nonconvex value function. First, when the policy regularizer dominates the
environmental shift, the value function satisfies a certain gradient dominance
property, so that any stationary point (not PS) of the value function is a
desired PO. Second, though the value function has unbounded gradient, we prove
that all the sufficiently stationary points lie in a convex and compact policy
subspace $\Pi_{\Delta}$, where the policy value has a constant lower bound
$\Delta>0$ and thus the gradient becomes bounded and Lipschitz continuous.
Experimental results also demonstrate that our 0-FW algorithm is more effective
than the existing algorithms in finding the desired PO policy.

</details>


### [609] [Trade-off in Estimating the Number of Byzantine Clients in Federated Learning](https://arxiv.org/abs/2510.04432)
*Ziyi Chen,Su Zhang,Heng Huang*

Main category: cs.LG

TL;DR: The paper explores federated learning's vulnerability to Byzantine clients and systematically analyzes the effect of estimating the number of such clients on robust aggregator performance.


<details>
  <summary>Details</summary>
Motivation: To understand the impact of estimating the number of Byzantine clients on robust aggregators and federated learning performance, as this has not been systematically studied before.

Method: Theoretical analysis of worst-case error bounds of aggregators and federated learning algorithms under different cases of estimated ($\hat{f}$) and actual ($f$) numbers of Byzantine clients.

Result: The study finds that underestimation of $\hat{f}$ leads to arbitrarily poor performance, while overestimation bounds errors optimally but introduces trade-offs in performance. The error increases proportionally to $\hat{f}/(n-f-\hat{f})$ with $n$ clients.

Conclusion: A fundamental trade-off arises: larger robustness degree $\hat{f}$ increases the range of solvable federated learning problems but can degrade performance when fewer Byzantine clients exist.

Abstract: Federated learning has attracted increasing attention at recent large-scale
optimization and machine learning research and applications, but is also
vulnerable to Byzantine clients that can send any erroneous signals. Robust
aggregators are commonly used to resist Byzantine clients. This usually
requires to estimate the unknown number $f$ of Byzantine clients, and thus
accordingly select the aggregators with proper degree of robustness (i.e., the
maximum number $\hat{f}$ of Byzantine clients allowed by the aggregator). Such
an estimation should have important effect on the performance, which has not
been systematically studied to our knowledge. This work will fill in the gap by
theoretically analyzing the worst-case error of aggregators as well as its
induced federated learning algorithm for any cases of $\hat{f}$ and $f$.
Specifically, we will show that underestimation ($\hat{f}<f$) can lead to
arbitrarily poor performance for both aggregators and federated learning. For
non-underestimation ($\hat{f}\ge f$), we have proved optimal lower and upper
bounds of the same order on the errors of both aggregators and federated
learning. All these optimal bounds are proportional to $\hat{f}/(n-f-\hat{f})$
with $n$ clients, which monotonically increases with larger $\hat{f}$. This
indicates a fundamental trade-off: while an aggregator with a larger robustness
degree $\hat{f}$ can solve federated learning problems of wider range $f\in
[0,\hat{f}]$, the performance can deteriorate when there are actually fewer or
even no Byzantine clients (i.e., $f\in [0,\hat{f})$).

</details>


### [610] [Fractional Heat Kernel for Semi-Supervised Graph Learning with Small Training Sample Size](https://arxiv.org/abs/2510.04440)
*Farid Bozorgnia,Vyacheslav Kungurtsev,Shirali Kadyrov,Mohsen Yousefnezhad*

Main category: cs.LG

TL;DR: The paper introduces fractional heat kernel-driven algorithms for label propagation and self-training to enhance Graph Neural Networks (GNNs).


<details>
  <summary>Details</summary>
Motivation: To improve the efficiency and expressiveness of Graph Neural Networks (GNNs) for scenarios with limited labeled examples.

Method: The fractional heat kernel was incorporated into GNN architectures with Chebyshev polynomial approximations enabling scalable, multi-hop diffusion through variational formulations.

Result: The proposed algorithms showed improved label diffusion and expressiveness on standard datasets, particularly with minimal labeled data.

Conclusion: The integration of fractional heat kernels significantly enhances the performance of GNNs by globally diffusing labels and balancing supervision with data scarcity.

Abstract: In this work, we introduce novel algorithms for label propagation and
self-training using fractional heat kernel dynamics with a source term. We
motivate the methodology through the classical correspondence of information
theory with the physics of parabolic evolution equations. We integrate the
fractional heat kernel into Graph Neural Network architectures such as Graph
Convolutional Networks and Graph Attention, enhancing their expressiveness
through adaptive, multi-hop diffusion. By applying Chebyshev polynomial
approximations, large graphs become computationally feasible. Motivating
variational formulations demonstrate that by extending the classical diffusion
model to fractional powers of the Laplacian, nonlocal interactions deliver more
globally diffusing labels. The particular balance between supervision of known
labels and diffusion across the graph is particularly advantageous in the case
where only a small number of labeled training examples are present. We
demonstrate the effectiveness of this approach on standard datasets.

</details>


### [611] [Forking-Sequences](https://arxiv.org/abs/2510.04487)
*Willa Potosnak,Malcolm Wolff,Boris Oreshkin,Mengfei Cao,Michael W. Mahoney,Dmitry Efimov,Kin G. Olivares*

Main category: cs.LG

TL;DR: The paper formalizes the forking-sequences technique for time series forecasting to improve stability across forecast creation dates and demonstrates its benefits across multiple datasets.


<details>
  <summary>Details</summary>
Motivation: Forecasting models often yield unstable predictions between different forecast creation dates, making them less trustworthy and disruptive for decision-making.

Method: The forking-sequences technique encodes and decodes the entire time series across forecast creation dates, mirroring time series cross-validation, for stable training and inference.

Result: Experiments on 16 datasets show improvements in forecast stability across various architectures (MLP, RNN, LSTM, CNN, Transformer) by up to 37.9%.

Conclusion: Forking-sequences are effective in enhancing forecast stability, warranting broader adoption within the neural forecasting community.

Abstract: While accuracy is a critical requirement for time series forecasting models,
an equally important (yet often overlooked) desideratum is forecast stability
across forecast creation dates (FCDs). Even highly accurate models can produce
erratic revisions between FCDs, undermining stakeholder trust and disrupting
downstream decision-making. To improve forecast stability, models like MQCNN,
MQT, and SPADE employ a little-known but highly effective technique:
forking-sequences. Unlike standard statistical and neural forecasting methods
that treat each FCD independently, the forking-sequences method jointly encodes
and decodes the entire time series across all FCDs, in a way mirroring time
series cross-validation. Since forking sequences remains largely unknown in the
broader neural forecasting community, in this work, we formalize the
forking-sequences approach, and we make a case for its broader adoption. We
demonstrate three key benefits of forking-sequences: (i) more stable and
consistent gradient updates during training; (ii) reduced forecast variance
through ensembling; and (iii) improved inference computational efficiency. We
validate forking-sequences' benefits using 16 datasets from the M1, M3, M4, and
Tourism competitions, showing improvements in forecast percentage change
stability of 28.8%, 28.8%, 37.9%, and 31.3%, and 8.8%, on average, for MLP,
RNN, LSTM, CNN, and Transformer-based architectures, respectively.

</details>


### [612] [Expand Neurons, Not Parameters](https://arxiv.org/abs/2510.04500)
*Linghao Kong,Inimai Subramanian,Yonadav Shavit,Micah Adler,Dan Alistarh,Nir Shavit*

Main category: cs.LG

TL;DR: The paper investigates how increasing neuron count without adding non-zero parameters can enhance model performance by reducing feature interference, introducing Fixed Parameter Expansion (FPE) to address polysemanticity.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to address the challenge of feature interference in neural networks and reduce the polysemanticity caused by overcrowded neurons sharing weights.

Method: It proposes the Fixed Parameter Expansion (FPE) technique, where parent neurons are replaced with multiple child neurons that inherit disjoint weight subsets, reducing collisions in feature representation.

Result: FPE reduces polysemanticity metrics, improves accuracy on symbolic and real tasks, and shows higher benefits under high polysemantic load while maintaining constant non-zero parameter counts.

Conclusion: Using Fixed Parameter Expansion allows for leveraging width to counter superposition effects, enhancing performance while being computationally efficient and suitable for modern hardware bottlenecks.

Abstract: This work demonstrates how increasing the number of neurons in a network
without increasing its number of non-zero parameters improves performance. We
show that this gain corresponds with a decrease in interference between
multiple features that would otherwise share the same neurons. To reduce such
entanglement at a fixed non-zero parameter count, we introduce Fixed Parameter
Expansion (FPE): replace a neuron with multiple children and partition the
parent's weights disjointly across them, so that each child inherits a
non-overlapping subset of connections. On symbolic tasks, specifically Boolean
code problems, clause-aligned FPE systematically reduces polysemanticity
metrics and yields higher task accuracy. Notably, random splits of neuron
weights approximate these gains, indicating that reduced collisions, not
precise assignment, are a primary driver. Consistent with the superposition
hypothesis, the benefits of FPE grow with increasing interference: when
polysemantic load is high, accuracy improvements are the largest. Transferring
these insights to real models (classifiers over CLIP embeddings and deeper
multilayer networks) we find that widening networks while maintaining a
constant non-zero parameter count consistently increases accuracy. These
results identify an interpretability-grounded mechanism to leverage width
against superposition, improving performance without increasing the number of
non-zero parameters. Such a direction is well matched to modern accelerators,
where memory movement of non-zero parameters, rather than raw compute, is the
dominant bottleneck.

</details>


### [613] [Wavelet Predictive Representations for Non-Stationary Reinforcement Learning](https://arxiv.org/abs/2510.04507)
*Min Wang,Xin Li,Ye He,Yao-Hui Li,Hasnaa Bennis,Riashat Islam,Mingzhong Wang*

Main category: cs.LG

TL;DR: The paper proposes WISDOM, a novel approach utilizing wavelet-domain predictive task representations to improve non-stationary reinforcement learning (NSRL) in dynamic environments.


<details>
  <summary>Details</summary>
Motivation: To address the difficulty of adapting reinforcement learning agents to non-stationary and stochastically evolving tasks, which are common in real-world scenarios like weather and traffic.

Method: The authors designed WISDOM, which applies wavelet analysis to capture multi-scale features of Markov Decision Processes (MDPs) in the wavelet domain and incorporates a wavelet temporal difference (TD) update operator for task tracking and prediction.

Result: Experiments on various benchmarks demonstrate that WISDOM outperforms existing methods in sample efficiency and asymptotic performance while showing stronger adaptability in dynamic environments.

Conclusion: WISDOM effectively enhances NSRL by leveraging wavelet-based task representations, offering improved adaptability and performance in complex, non-stationary settings.

Abstract: The real world is inherently non-stationary, with ever-changing factors, such
as weather conditions and traffic flows, making it challenging for agents to
adapt to varying environmental dynamics. Non-Stationary Reinforcement Learning
(NSRL) addresses this challenge by training agents to adapt rapidly to
sequences of distinct Markov Decision Processes (MDPs). However, existing NSRL
approaches often focus on tasks with regularly evolving patterns, leading to
limited adaptability in highly dynamic settings. Inspired by the success of
Wavelet analysis in time series modeling, specifically its ability to capture
signal trends at multiple scales, we propose WISDOM to leverage wavelet-domain
predictive task representations to enhance NSRL. WISDOM captures these
multi-scale features in evolving MDP sequences by transforming task
representation sequences into the wavelet domain, where wavelet coefficients
represent both global trends and fine-grained variations of non-stationary
changes. In addition to the auto-regressive modeling commonly employed in time
series forecasting, we devise a wavelet temporal difference (TD) update
operator to enhance tracking and prediction of MDP evolution. We theoretically
prove the convergence of this operator and demonstrate policy improvement with
wavelet task representations. Experiments on diverse benchmarks show that
WISDOM significantly outperforms existing baselines in both sample efficiency
and asymptotic performance, demonstrating its remarkable adaptability in
complex environments characterized by non-stationary and stochastically
evolving tasks.

</details>


### [614] [Toward a Unified Geometry Understanding: Riemannian Diffusion Framework for Graph Generation and Prediction](https://arxiv.org/abs/2510.04522)
*Yisen Gao,Xingcheng Fu,Qingyun Sun,Jianxin Li,Xianxian Li*

Main category: cs.LG

TL;DR: The paper introduces GeoMancer, a novel Riemannian graph diffusion model, to better handle non-Euclidean graph data for generation and prediction tasks.


<details>
  <summary>Details</summary>
Motivation: Existing graph diffusion models poorly utilize geometric information because they entangle features with different curvatures in a unified space, limiting their potential for structured graph data.

Method: GeoMancer incorporates a Riemannian gyrokernel approach to replace exponential mapping, decouples multi-level features onto task-specific manifolds, and uses manifold-constrained diffusion for better alignment with geometric data.

Result: GeoMancer shows superior performance across diverse graph learning tasks in extensive experiments.

Conclusion: The proposed Riemannian graph diffusion framework effectively captures manifold structures in graph data, leading to improved results in predictive and generative tasks.

Abstract: Graph diffusion models have made significant progress in learning structured
graph data and have demonstrated strong potential for predictive tasks.
Existing approaches typically embed node, edge, and graph-level features into a
unified latent space, modeling prediction tasks including classification and
regression as a form of conditional generation. However, due to the
non-Euclidean nature of graph data, features of different curvatures are
entangled in the same latent space without releasing their geometric potential.
To address this issue, we aim to construt an ideal Riemannian diffusion model
to capture distinct manifold signatures of complex graph data and learn their
distribution. This goal faces two challenges: numerical instability caused by
exponential mapping during the encoding proces and manifold deviation during
diffusion generation. To address these challenges, we propose GeoMancer: a
novel Riemannian graph diffusion framework for both generation and prediction
tasks. To mitigate numerical instability, we replace exponential mapping with
an isometric-invariant Riemannian gyrokernel approach and decouple multi-level
features onto their respective task-specific manifolds to learn optimal
representations. To address manifold deviation, we introduce a
manifold-constrained diffusion method and a self-guided strategy for
unconditional generation, ensuring that the generated data remains aligned with
the manifold signature. Extensive experiments validate the effectiveness of our
approach, demonstrating superior performance across a variety of tasks.

</details>


### [615] [Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF--QP Safety Layer in Arbitrage-Free Markets](https://arxiv.org/abs/2510.04555)
*Jian'an Zhang*

Main category: cs.LG

TL;DR: Tail-Safe is a framework blending reinforcement learning with control-barrier safety measures to improve derivatives hedging under financial constraints, enhancing tail risk management and ensuring constraint enforcement.


<details>
  <summary>Details</summary>
Motivation: The main motivation is to create a deployable framework for derivatives hedging that addresses tail risk while maintaining financial safety and governance, especially under practical financial constraints.

Method: The approach integrates an IQN-CVaR-PPO reinforcement learning model with a control-barrier-function quadratic program (CBF-QP) to enforce constraints. The learning model includes temperature tilting and tail boosting mechanisms, while the QP enforces safety and domain-specific constraints using convex optimization.

Result: The framework reduces left-tail risk without affecting central performance and avoids hard-constraint violations when feasible, validated in synthetic markets.

Conclusion: Tail-Safe significantly enhances derivatives hedging by improving risk-sensitive learning and enforcing strict financial constraints, although it is limited by reliance on synthetic data and simplified modeling.

Abstract: We introduce Tail-Safe, a deployability-oriented framework for derivatives
hedging that unifies distributional, risk-sensitive reinforcement learning with
a white-box control-barrier-function (CBF) quadratic-program (QP) safety layer
tailored to financial constraints. The learning component combines an IQN-based
distributional critic with a CVaR objective (IQN--CVaR--PPO) and a
Tail-Coverage Controller that regulates quantile sampling through temperature
tilting and tail boosting to stabilize small-$\alpha$ estimation. The safety
component enforces discrete-time CBF inequalities together with domain-specific
constraints -- ellipsoidal no-trade bands, box and rate limits, and a
sign-consistency gate -- solved as a convex QP whose telemetry (active sets,
tightness, rate utilization, gate scores, slack, and solver status) forms an
auditable trail for governance. We provide guarantees of robust forward
invariance of the safe set under bounded model mismatch, a minimal-deviation
projection interpretation of the QP, a KL-to-DRO upper bound linking per-state
KL regularization to worst-case CVaR, concentration and sample-complexity
results for the temperature-tilted CVaR estimator, and a CVaR trust-region
improvement inequality under KL limits, together with feasibility persistence
under expiry-aware tightening. Empirically, in arbitrage-free,
microstructure-aware synthetic markets (SSVI $\to$ Dupire $\to$ VIX with
ABIDES/MockLOB execution), Tail-Safe improves left-tail risk without degrading
central performance and yields zero hard-constraint violations whenever the QP
is feasible with zero slack. Telemetry is mapped to governance dashboards and
incident workflows to support explainability and auditability. Limitations
include reliance on synthetic data and simplified execution to isolate
methodological contributions.

</details>


### [616] [Challenger-Based Combinatorial Bandits for Subcarrier Selection in OFDM Systems](https://arxiv.org/abs/2510.04559)
*Mohsen Amiri,V Venktesh,Sindri Magnússon*

Main category: cs.LG

TL;DR: The paper solves the problem of selecting optimal user subsets in MIMO systems using a novel framework for efficient exploration and comparison methods.


<details>
  <summary>Details</summary>
Motivation: To efficiently identify user subsets for scheduling in large multi-user MIMO downlink systems without exhaustive search methods.

Method: Introduces a gap-index framework with champion and challenger arms for efficient comparison, reducing runtime and computation.

Result: Significant improvements in runtime, computation efficiency, and identification accuracy using simulations in realistic OFDM downlink settings.

Conclusion: The approach makes measurement-efficient subcarrier selection practical for AI-enabled systems, offering tunable accuracy and speed trade-offs.

Abstract: This paper investigates the identification of the top-m user-scheduling sets
in multi-user MIMO downlink, which is cast as a combinatorial pure-exploration
problem in stochastic linear bandits. Because the action space grows
exponentially, exhaustive search is infeasible. We therefore adopt a linear
utility model to enable efficient exploration and reliable selection of
promising user subsets. We introduce a gap-index framework that maintains a
shortlist of current estimates of champion arms (top-m sets) and a rotating
shortlist of challenger arms that pose the greatest threat to the champions.
This design focuses on measurements that yield the most informative
gap-index-based comparisons, resulting in significant reductions in runtime and
computation compared to state-of-the-art linear bandit methods, with high
identification accuracy. The method also exposes a tunable trade-off between
speed and accuracy. Simulations on a realistic OFDM downlink show that
shortlist-driven pure exploration makes online, measurement-efficient
subcarrier selection practical for AI-enabled communication systems.

</details>


### [617] [Stochastic Approximation Methods for Distortion Risk Measure Optimization](https://arxiv.org/abs/2510.04563)
*Jinyang Jiang,Bernd Heidergott,Jiaqiao Hu,Yijie Peng*

Main category: cs.LG

TL;DR: This paper presents algorithms for optimizing Distortion Risk Measures (DRMs) with strong convergence proofs and their application in tasks like portfolio selection and reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: To enhance decision-making under uncertainty by optimizing Distortion Risk Measures and addressing challenges in scalability and robustness.

Method: Two main approaches for DRM optimization are developed: the DM-form (a three-timescale method) and the QF-form (a simpler two-timescale method), along with a hybrid form for combining their strengths.

Result: Proofs of strong convergence are offered with optimal rates for the DM-form ($O(k^{-4/7})$) and faster rates for the QF-form ($O(k^{-2/3})$). Numerical experiments validate their performance in portfolio tasks, and their integration with deep learning demonstrates scalability.

Conclusion: The proposed algorithms are effective and efficient for DRM-related optimization tasks and show promise in practical applications like inventory management through reinforcement learning.

Abstract: Distortion Risk Measures (DRMs) capture risk preferences in decision-making
and serve as general criteria for managing uncertainty. This paper proposes
gradient descent algorithms for DRM optimization based on two dual
representations: the Distortion-Measure (DM) form and Quantile-Function (QF)
form. The DM-form employs a three-timescale algorithm to track quantiles,
compute their gradients, and update decision variables, utilizing the
Generalized Likelihood Ratio and kernel-based density estimation. The QF-form
provides a simpler two-timescale approach that avoids the need for complex
quantile gradient estimation. A hybrid form integrates both approaches,
applying the DM-form for robust performance around distortion function jumps
and the QF-form for efficiency in smooth regions. Proofs of strong convergence
and convergence rates for the proposed algorithms are provided. In particular,
the DM-form achieves an optimal rate of $O(k^{-4/7})$, while the QF-form
attains a faster rate of $O(k^{-2/3})$. Numerical experiments confirm their
effectiveness and demonstrate substantial improvements over baselines in robust
portfolio selection tasks. The method's scalability is further illustrated
through integration into deep reinforcement learning. Specifically, a DRM-based
Proximal Policy Optimization algorithm is developed and applied to
multi-echelon dynamic inventory management, showcasing its practical
applicability.

</details>


### [618] [GILT: An LLM-Free, Tuning-Free Graph Foundational Model for In-Context Learning](https://arxiv.org/abs/2510.04567)
*Weishuo Ma,Yanbo Wang,Xiyuan Wang,Lei Zou,Muhan Zhang*

Main category: cs.LG

TL;DR: This paper introduces GILT, a novel framework for graph in-context learning, addressing heterogeneity and achieving better few-shot performance without tuning.


<details>
  <summary>Details</summary>
Motivation: Current Graph Foundational Models face challenges due to the extreme heterogeneity in graph data which affects generalization and efficiency.

Method: The paper proposes GILT, a token-based framework designed for numerical features, enabling in-context learning on graphs without tuning.

Result: Experiments demonstrate GILT's stronger few-shot performance, outperforming LLM-based and tuning-based models with greater efficiency.

Conclusion: The GILT framework effectively handles graph heterogeneity while providing a unified tuning-free approach, showing superior adaptability and efficiency in graph-related tasks.

Abstract: Graph Neural Networks (GNNs) are powerful tools for precessing relational
data but often struggle to generalize to unseen graphs, giving rise to the
development of Graph Foundational Models (GFMs). However, current GFMs are
challenged by the extreme heterogeneity of graph data, where each graph can
possess a unique feature space, label set, and topology. To address this, two
main paradigms have emerged. The first leverages Large Language Models (LLMs),
but is fundamentally text-dependent, thus struggles to handle the numerical
features in vast graphs. The second pre-trains a structure-based model, but the
adaptation to new tasks typically requires a costly, per-graph tuning stage,
creating a critical efficiency bottleneck. In this work, we move beyond these
limitations and introduce \textbf{G}raph \textbf{I}n-context \textbf{L}earning
\textbf{T}ransformer (GILT), a framework built on an LLM-free and tuning-free
architecture. GILT introduces a novel token-based framework for in-context
learning (ICL) on graphs, reframing classification tasks spanning node, edge
and graph levels in a unified framework. This mechanism is the key to handling
heterogeneity, as it is designed to operate on generic numerical features.
Further, its ability to understand class semantics dynamically from the context
enables tuning-free adaptation. Comprehensive experiments show that GILT
achieves stronger few-shot performance with significantly less time than
LLM-based or tuning-based baselines, validating the effectiveness of our
approach.

</details>


### [619] [Improved probabilistic regression using diffusion models](https://arxiv.org/abs/2510.04583)
*Carlo Kneissl,Christopher Bülte,Philipp Scholl,Gitta Kutyniok*

Main category: cs.LG

TL;DR: Proposes a novel framework for probabilistic regression using diffusion-based models to learn predictive distributions nonparametrically, achieving superior performance and uncertainty quantification across diverse tasks.


<details>
  <summary>Details</summary>
Motivation: Probabilistic regression offers comprehensive predictions and uncertainty quantification, but diffusion-based methods lack adequate uncertainty evaluations and are confined to specific applications.

Method: Develops a diffusion-based framework to model the entire distribution of diffusion noise for learning predictive distributions nonparametrically. Analyzes different parameterizations and evaluates the framework across various regression tasks.

Result: Superior performance compared to existing baselines and calibrated uncertainty estimates in diverse regression settings, both low- and high-dimensional.

Conclusion: Demonstrates versatility and effectiveness as a probabilistic prediction tool, offering enhanced performance and reliable uncertainty quantification.

Abstract: Probabilistic regression models the entire predictive distribution of a
response variable, offering richer insights than classical point estimates and
directly allowing for uncertainty quantification. While diffusion-based
generative models have shown remarkable success in generating complex,
high-dimensional data, their usage in general regression tasks often lacks
uncertainty-related evaluation and remains limited to domain-specific
applications. We propose a novel diffusion-based framework for probabilistic
regression that learns predictive distributions in a nonparametric way. More
specifically, we propose to model the full distribution of the diffusion noise,
enabling adaptation to diverse tasks and enhanced uncertainty quantification.
We investigate different noise parameterizations, analyze their trade-offs, and
evaluate our framework across a broad range of regression tasks, covering low-
and high-dimensional settings. For several experiments, our approach shows
superior performance against existing baselines, while delivering calibrated
uncertainty estimates, demonstrating its versatility as a tool for
probabilistic prediction.

</details>


### [620] [Forecasting-Based Biomedical Time-series Data Synthesis for Open Data and Robust AI](https://arxiv.org/abs/2510.04622)
*Youngjoon Lee,Seongmin Cho,Yehhyun Jo,Jinu Gong,Hyunjoo Jenny Lee,Joonhyuk Kang*

Main category: cs.LG

TL;DR: This paper introduces a framework that generates synthetic biomedical time-series data to address data scarcity and privacy concerns, while maintaining fidelity to real datasets.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the challenges posed by strict privacy regulations and resource demands, which limit the availability of biomedical time-series data for AI research.

Method: The authors developed a synthetic data generation framework using advanced forecasting models that replicate electrophysiological signals like EEG and EMG while preserving their statistical properties.

Result: The evaluations across subjects show that the generated synthetic data acts as an effective substitute for real data and improves AI model performance.

Conclusion: The framework successfully addresses privacy and scalability issues, while embedding into open-source repositories to enhance AI-driven biomedical research.

Abstract: The limited data availability due to strict privacy regulations and
significant resource demands severely constrains biomedical time-series AI
development, which creates a critical gap between data requirements and
accessibility. Synthetic data generation presents a promising solution by
producing artificial datasets that maintain the statistical properties of real
biomedical time-series data without compromising patient confidentiality. We
propose a framework for synthetic biomedical time-series data generation based
on advanced forecasting models that accurately replicates complex
electrophysiological signals such as EEG and EMG with high fidelity. These
synthetic datasets preserve essential temporal and spectral properties of real
data, which enables robust analysis while effectively addressing data scarcity
and privacy challenges. Our evaluations across multiple subjects demonstrate
that the generated synthetic data can serve as an effective substitute for real
data and also significantly boost AI model performance. The approach maintains
critical biomedical features while provides high scalability for various
applications and integrates seamlessly into open-source repositories,
substantially expanding resources for AI-driven biomedical research.

</details>


### [621] [Compressed Concatenation of Small Embedding Models](https://arxiv.org/abs/2510.04626)
*Mohamed Ayoub Ben Ayad,Michael Dinzinger,Kanishka Ghosh Dastidar,Jelena Mitrovic,Michael Granitzer*

Main category: cs.LG

TL;DR: The study explores using concatenated small embedding models to enhance performance in dense retrieval, proposing a method (MRL loss with a unified decoder) to reduce dimensionality while maintaining effectiveness.


<details>
  <summary>Details</summary>
Motivation: To find a solution that enables embedding models to be effective yet practical for deployment in resource-limited environments.

Method: Small embedding models are concatenated, and a lightweight unified decoder, trained with Matryoshka Representation Learning (MRL) loss, reduces dimensionality without fine-tuning the base models.

Result: The pipeline retained 89% of performance with a 48x size compression when applied to concatenated small models on standard retrieval tasks.

Conclusion: Concatenating small models, coupled with MRL loss and quantization, offers an efficient and effective alternative to large embedding models in constrained environments.

Abstract: Embedding models are central to dense retrieval, semantic search, and
recommendation systems, but their size often makes them impractical to deploy
in resource-constrained environments such as browsers or edge devices. While
smaller embedding models offer practical advantages, they typically
underperform compared to their larger counterparts. To bridge this gap, we
demonstrate that concatenating the raw embedding vectors of multiple small
models can outperform a single larger baseline on standard retrieval
benchmarks. To overcome the resulting high dimensionality of naive
concatenation, we introduce a lightweight unified decoder trained with a
Matryoshka Representation Learning (MRL) loss. This decoder maps the
high-dimensional joint representation to a low-dimensional space, preserving
most of the original performance without fine-tuning the base models. We also
show that while concatenating more base models yields diminishing gains, the
robustness of the decoder's representation under compression and quantization
improves. Our experiments show that, on a subset of MTEB retrieval tasks, our
concat-encode-quantize pipeline recovers 89\% of the original performance with
a 48x compression factor when the pipeline is applied to a concatenation of
four small embedding models.

</details>


### [622] [Predictive Feature Caching for Training-free Acceleration of Molecular Geometry Generation](https://arxiv.org/abs/2510.04646)
*Johanna Sommer,John Rachwan,Nils Fleischmann,Stephan Günnemann,Bertrand Charpentier*

Main category: cs.LG

TL;DR: This paper presents a training-free caching strategy that accelerates molecular geometry generation by predicting intermediate states, achieving up to 7x speedup with minimal quality loss.


<details>
  <summary>Details</summary>
Motivation: Significant inference overhead in flow matching models for molecular geometry generation hampers practical sampling of large molecular datasets.

Method: A training-free caching methodology predicts intermediate hidden states during solver steps, operates on SE(3)-equivariant backbones, and integrates seamlessly with pretrained models.

Result: The caching strategy reduces inference time by up to 3x compared to the base model and achieves a composite speedup of 7x when integrated with other optimizations.

Conclusion: The proposed caching strategy offers a practical, efficient solution to speed up molecular geometry generation with minimal compromise in sample quality, and its benefits amplify when combined with other accelerations.

Abstract: Flow matching models generate high-fidelity molecular geometries but incur
significant computational costs during inference, requiring hundreds of network
evaluations. This inference overhead becomes the primary bottleneck when such
models are employed in practice to sample large numbers of molecular
candidates. This work discusses a training-free caching strategy that
accelerates molecular geometry generation by predicting intermediate hidden
states across solver steps. The proposed method operates directly on the
SE(3)-equivariant backbone, is compatible with pretrained models, and is
orthogonal to existing training-based accelerations and system-level
optimizations. Experiments on the GEOM-Drugs dataset demonstrate that caching
achieves a twofold reduction in wall-clock inference time at matched sample
quality and a speedup of up to 3x compared to the base model with minimal
sample quality degradation. Because these gains compound with other
optimizations, applying caching alongside other general, lossless optimizations
yield as much as a 7x speedup.

</details>


### [623] [IMLP: An Energy-Efficient Continual Learning Method for Tabular Data Streams](https://arxiv.org/abs/2510.04660)
*Yuandou Wang,Filip Gunnarsson,Rihan Hai*

Main category: cs.LG

TL;DR: The paper presents an energy-efficient, compact continual learning model (IMLP) for tabular data streams, addressing challenges in edge and mobile devices.


<details>
  <summary>Details</summary>
Motivation: To enable real-time decision-making for dynamic tabular data streams in resource-constrained environments, like healthcare and IoT, by overcoming the energy and memory inefficiencies of existing continual learning approaches.

Method: A context-aware incremental Multi-Layer Perceptron (IMLP) with windowed scaled dot-product attention on a sliding latent feature buffer, ensuring constant-size memory and lightweight updates.

Result: IMLP achieves up to 27.6x higher energy efficiency than TabNet and 85.5x higher than TabPFN, with competitive accuracy on par with state-of-the-art models.

Conclusion: IMLP offers a deployable and energy-efficient alternative for tabular data streams, avoiding expensive full retraining, and providing balanced performance in accuracy and resource usage.

Abstract: Tabular data streams are rapidly emerging as a dominant modality for
real-time decision-making in healthcare, finance, and the Internet of Things
(IoT). These applications commonly run on edge and mobile devices, where energy
budgets, memory, and compute are strictly limited. Continual learning (CL)
addresses such dynamics by training models sequentially on task streams while
preserving prior knowledge and consolidating new knowledge. While recent CL
work has advanced in mitigating catastrophic forgetting and improving knowledge
transfer, the practical requirements of energy and memory efficiency for
tabular data streams remain underexplored. In particular, existing CL solutions
mostly depend on replay mechanisms whose buffers grow over time and exacerbate
resource costs.
  We propose a context-aware incremental Multi-Layer Perceptron (IMLP), a
compact continual learner for tabular data streams. IMLP incorporates a
windowed scaled dot-product attention over a sliding latent feature buffer,
enabling constant-size memory and avoiding storing raw data. The attended
context is concatenated with current features and processed by shared
feed-forward layers, yielding lightweight per-segment updates. To assess
practical deployability, we introduce NetScore-T, a tunable metric coupling
balanced accuracy with energy for Pareto-aware comparison across models and
datasets. IMLP achieves up to $27.6\times$ higher energy efficiency than TabNet
and $85.5\times$ higher than TabPFN, while maintaining competitive average
accuracy. Overall, IMLP provides an easy-to-deploy, energy-efficient
alternative to full retraining for tabular data streams.

</details>


### [624] [Noise or Signal? Deconstructing Contradictions and An Adaptive Remedy for Reversible Normalization in Time Series Forecasting](https://arxiv.org/abs/2510.04667)
*Fanzhe Fu,Yang Yang*

Main category: cs.LG

TL;DR: The paper critiques and evaluates reversible instance normalization techniques, demonstrating its failures with extreme outliers and exploring robust alternatives.


<details>
  <summary>Details</summary>
Motivation: Investigate the robustness of Reversible Instance Normalization in time-series forecasting and identify its limitations under challenging conditions.

Method: Analyzes normalization strategies, highlights contradictions, and tests robust alternatives like R²-IN and adaptive models (A-IN).

Result: Standard RevIN fails catastrophically with extreme outliers; R²-IN performs as the best baseline, while A-IN fails systematically, revealing critical issues.

Conclusion: The paper emphasizes diagnostics-driven analysis in time-series normalization to balance simplicity and robustness, avoiding pitfalls of naive adaptation.

Abstract: Reversible Instance Normalization (RevIN) is a key technique enabling simple
linear models to achieve state-of-the-art performance in time series
forecasting. While replacing its non-robust statistics with robust counterparts
(termed R$^2$-IN) seems like a straightforward improvement, our findings reveal
a far more complex reality. This paper deconstructs the perplexing performance
of various normalization strategies by identifying four underlying theoretical
contradictions. Our experiments provide two crucial findings: first, the
standard RevIN catastrophically fails on datasets with extreme outliers, where
its MSE surges by a staggering 683\%. Second, while the simple R$^2$-IN
prevents this failure and unexpectedly emerges as the best overall performer,
our adaptive model (A-IN), designed to test a diagnostics-driven heuristic,
unexpectedly suffers a complete and systemic failure. This surprising outcome
uncovers a critical, overlooked pitfall in time series analysis: the
instability introduced by a simple or counter-intuitive heuristic can be more
damaging than the statistical issues it aims to solve. The core contribution of
this work is thus a new, cautionary paradigm for time series normalization: a
shift from a blind search for complexity to a diagnostics-driven analysis that
reveals not only the surprising power of simple baselines but also the perilous
nature of naive adaptation.

</details>


### [625] [Semantic Channel Equalization Strategies for Deep Joint Source-Channel Coding](https://arxiv.org/abs/2510.04674)
*Lorenzo Pannacci,Simone Fiorellino,Mario Edoardo Pandolfo,Emilio Calvanese Strinati,Paolo Di Lorenzo*

Main category: cs.LG

TL;DR: This study introduces methods to tackle semantic noise in DeepJSCC for heterogeneous multi-vendor networks by aligning latent spaces to improve communication and reconstruction quality.


<details>
  <summary>Details</summary>
Motivation: To address the issue of semantic noise arising from mismatched latent spaces in DeepJSCC due to multi-vendor deployments, which degrades the quality of transmitted data and downstream task performance.

Method: The authors introduce semantic channel equalization methods, utilizing three aligners: linear maps, lightweight neural networks, and Parseval-frame equalizers, tested on noisy image reconstruction over AWGN and fading channels.

Result: Experiments demonstrated trade-offs among aligners in terms of complexity, data efficiency, and fidelity, showcasing effective alignment techniques for improving DeepJSCC performance.

Conclusion: The paper provides actionable guidelines for deploying DeepJSCC in environments with heterogeneous AI-native wireless networks, emphasizing robustness against semantic noise and improved communication reliability.

Abstract: Deep joint source-channel coding (DeepJSCC) has emerged as a powerful
paradigm for end-to-end semantic communications, jointly learning to compress
and protect task-relevant features over noisy channels. However, existing
DeepJSCC schemes assume a shared latent space at transmitter (TX) and receiver
(RX) - an assumption that fails in multi-vendor deployments where encoders and
decoders cannot be co-trained. This mismatch introduces "semantic noise",
degrading reconstruction quality and downstream task performance. In this
paper, we systematize and evaluate methods for semantic channel equalization
for DeepJSCC, introducing an additional processing stage that aligns
heterogeneous latent spaces under both physical and semantic impairments. We
investigate three classes of aligners: (i) linear maps, which admit closed-form
solutions; (ii) lightweight neural networks, offering greater expressiveness;
and (iii) a Parseval-frame equalizer, which operates in zero-shot mode without
the need for training. Through extensive experiments on image reconstruction
over AWGN and fading channels, we quantify trade-offs among complexity, data
efficiency, and fidelity, providing guidelines for deploying DeepJSCC in
heterogeneous AI-native wireless networks.

</details>


### [626] [Counterfactual Credit Guided Bayesian Optimization](https://arxiv.org/abs/2510.04676)
*Qiyu Wei,Haowei Wang,Richard Allmendinger,Mauricio A. Álvarez*

Main category: cs.LG

TL;DR: This paper proposes Counterfactual Credit Guided Bayesian Optimization (CCGBO), enhancing Bayesian optimization by prioritizing historical samples that contribute most to finding the global optimum.


<details>
  <summary>Details</summary>
Motivation: Existing Bayesian optimization methods overly emphasize constructing a global surrogate model, which may not be efficient for finding the global optimum in complex real-world scenarios.

Method: The proposed method introduces counterfactual credit to quantify the importance of individual observations and integrates it into the acquisition function to guide resource allocation towards promising areas.

Result: CCGBO achieves sublinear regret and exhibits reduced simple regret while accelerating convergence to the global optimum, as demonstrated through synthetic and real-world benchmarks.

Conclusion: CCGBO effectively refines Bayesian optimization by focusing on critical observations, offering improved performance and faster identification of the global optimum in various optimization problems.

Abstract: Bayesian optimization has emerged as a prominent methodology for optimizing
expensive black-box functions by leveraging Gaussian process surrogates, which
focus on capturing the global characteristics of the objective function.
However, in numerous practical scenarios, the primary objective is not to
construct an exhaustive global surrogate, but rather to quickly pinpoint the
global optimum. Due to the aleatoric nature of the sequential optimization
problem and its dependence on the quality of the surrogate model and the
initial design, it is restrictive to assume that all observed samples
contribute equally to the discovery of the optimum in this context. In this
paper, we introduce Counterfactual Credit Guided Bayesian Optimization (CCGBO),
a novel framework that explicitly quantifies the contribution of individual
historical observations through counterfactual credit. By incorporating
counterfactual credit into the acquisition function, our approach can
selectively allocate resources in areas where optimal solutions are most likely
to occur. We prove that CCGBO retains sublinear regret. Empirical evaluations
on various synthetic and real-world benchmarks demonstrate that CCGBO
consistently reduces simple regret and accelerates convergence to the global
optimum.

</details>


### [627] [Parameter-free Algorithms for the Stochastically Extended Adversarial Model](https://arxiv.org/abs/2510.04685)
*Shuche Wang,Adarsh Barik,Peng Zhao,Vincent Y. F. Tan*

Main category: cs.LG

TL;DR: This paper introduces the first parameter-free algorithms for the Stochastically Extended Adversarial (SEA) model, achieving regret bounds without prior knowledge of problem-specific parameters like domain diameter or Lipschitz constant.


<details>
  <summary>Details</summary>
Motivation: Existing methods for the SEA model depend on prior knowledge of parameters (e.g., domain diameter, Lipschitz constant), limiting their practical usability. This work aims to remove these dependencies to make algorithms adaptable and widely applicable.

Method: The authors build on the Optimistic Online Newton Step (OONS) algorithm, creating parameter-free methods for the SEA model. They design two algorithms: one for unknown domain diameter (but known Lipschitz constant) and another for the general case where both parameters are unknown.

Result: The new algorithms achieve regret bounds that depend on the comparator vector, cumulative stochastic variance, and cumulative adversarial variation, demonstrating strong performance even without knowledge of the parameters.

Conclusion: The parameter-free algorithms make significant progress in bridging stochastic and adversarial settings, enhancing the practicality and adaptability of methods in online convex optimization.

Abstract: We develop the first parameter-free algorithms for the Stochastically
Extended Adversarial (SEA) model, a framework that bridges adversarial and
stochastic online convex optimization. Existing approaches for the SEA model
require prior knowledge of problem-specific parameters, such as the diameter of
the domain $D$ and the Lipschitz constant of the loss functions $G$, which
limits their practical applicability. Addressing this, we develop
parameter-free methods by leveraging the Optimistic Online Newton Step (OONS)
algorithm to eliminate the need for these parameters. We first establish a
comparator-adaptive algorithm for the scenario with unknown domain diameter but
known Lipschitz constant, achieving an expected regret bound of
$\tilde{O}\big(\|u\|_2^2 + \|u\|_2(\sqrt{\sigma^2_{1:T}} +
\sqrt{\Sigma^2_{1:T}})\big)$, where $u$ is the comparator vector and
$\sigma^2_{1:T}$ and $\Sigma^2_{1:T}$ represent the cumulative stochastic
variance and cumulative adversarial variation, respectively. We then extend
this to the more general setting where both $D$ and $G$ are unknown, attaining
the comparator- and Lipschitz-adaptive algorithm. Notably, the regret bound
exhibits the same dependence on $\sigma^2_{1:T}$ and $\Sigma^2_{1:T}$,
demonstrating the efficacy of our proposed methods even when both parameters
are unknown in the SEA model.

</details>


### [628] [How does the optimizer implicitly bias the model merging loss landscape?](https://arxiv.org/abs/2510.04686)
*Chenxiang Zhang,Alexander Theus,Damien Teney,Antonio Orvieto,Jun Pang,Sjouke Mauw*

Main category: cs.LG

TL;DR: This paper investigates factors affecting model merging methods, particularly focusing on how optimization choices impact loss landscape geometry.


<details>
  <summary>Details</summary>
Motivation: Understanding the factors behind effective model merging is poorly explored, and the paper seeks to analyze the optimization process' role in this.

Method: Examining how optimization parameters (e.g., learning rates, weight decay) influence the effective noise scale and its correlation with merging success.

Result: Merging success correlates non-monotonically with effective noise scale. Various training dynamics independently modulate this scale, showing consistent patterns.

Conclusion: Effective noise scale is key to merging success, and training dynamics can be manipulated to improve model merging functionality.

Abstract: Model merging methods combine models with different capabilities into a
single one while maintaining the same inference cost. Two popular approaches
are linear interpolation, which linearly interpolates between model weights,
and task arithmetic, which combines task vectors obtained by the difference
between finetuned and base models. While useful in practice, what properties
make merging effective are poorly understood. This paper explores how the
optimization process affects the loss landscape geometry and its impact on
merging success. We show that a single quantity -- the effective noise scale --
unifies the impact of optimizer and data choices on model merging. Across
architectures and datasets, the effectiveness of merging success is a
non-monotonic function of effective noise, with a distinct optimum. Decomposing
this quantity, we find that larger learning rates, stronger weight decay,
smaller batch sizes, and data augmentation all independently modulate the
effective noise scale, exhibiting the same qualitative trend. Unlike prior work
that connects optimizer noise to the flatness or generalization of individual
minima, we show that it also affects the global loss landscape, predicting when
independently trained solutions can be merged. Our findings broaden the
understanding of how optimization shapes the loss landscape geometry and its
downstream consequences for model merging, suggesting the possibility of
further manipulating the training dynamics to improve merging effectiveness.

</details>


### [629] [ViTs: Teaching Machines to See Time Series Anomalies Like Human Experts](https://arxiv.org/abs/2510.04710)
*Zexin Wang,Changhua Pei,Yang Liu,Hengyue Jiang,Quan Zhou,Haotian Si,Hang Cui,Jianhui Li,Gaogang Xie,Jingjing Li,Dan Pei*

Main category: cs.LG

TL;DR: The paper introduces ViTs, a Vision-Language Model-based framework, for time series anomaly detection by converting time series data into visual representations to address challenges of flexible inference and context constraint limitations.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the limitations of time series models in zero-shot generalization, handling varying sequence lengths, and overcoming context constraints in anomaly detection.

Method: The authors propose converting time series curves into visual representations and employ an evolutionary algorithm to generate image-text pairs for training a Vision-Language Model framework using a three-stage pipeline.

Result: ViTs demonstrate an enhanced capability for understanding and detecting anomalies in time series data across various scenarios.

Conclusion: The adoption of ViTs can address fundamental challenges in time series anomaly detection, significantly improving zero-shot inference across scenarios without retraining and efficiently handling sequences of varying lengths.

Abstract: Web service administrators must ensure the stability of multiple systems by
promptly detecting anomalies in Key Performance Indicators (KPIs). Achieving
the goal of "train once, infer across scenarios" remains a fundamental
challenge for time series anomaly detection models. Beyond improving zero-shot
generalization, such models must also flexibly handle sequences of varying
lengths during inference, ranging from one hour to one week, without
retraining. Conventional approaches rely on sliding-window encoding and
self-supervised learning, which restrict inference to fixed-length inputs.
Large Language Models (LLMs) have demonstrated remarkable zero-shot
capabilities across general domains. However, when applied to time series data,
they face inherent limitations due to context length. To address this issue, we
propose ViTs, a Vision-Language Model (VLM)-based framework that converts time
series curves into visual representations. By rescaling time series images,
temporal dependencies are preserved while maintaining a consistent input size,
thereby enabling efficient processing of arbitrarily long sequences without
context constraints. Training VLMs for this purpose introduces unique
challenges, primarily due to the scarcity of aligned time series image-text
data. To overcome this, we employ an evolutionary algorithm to automatically
generate thousands of high-quality image-text pairs and design a three-stage
training pipeline consisting of: (1) time series knowledge injection, (2)
anomaly detection enhancement, and (3) anomaly reasoning refinement. Extensive
experiments demonstrate that ViTs substantially enhance the ability of VLMs to
understand and detect anomalies in time series data. All datasets and code will
be publicly released at: https://anonymous.4open.science/r/ViTs-C484/.

</details>


### [630] [Directional Sheaf Hypergraph Networks: Unifying Learning on Directed and Undirected Hypergraphs](https://arxiv.org/abs/2510.04727)
*Emanuele Mule,Stefano Fiorini,Antonio Purificato,Federico Siciliano,Stefano Coniglio,Fabrizio Silvestri*

Main category: cs.LG

TL;DR: The paper discusses Directional Sheaf Hypergraph Networks (DSHN), a framework leveraging sheaf theory to enhance representation learning for directed hypergraphs, outperforming 13 baselines across 7 real-world datasets.


<details>
  <summary>Details</summary>
Motivation: Directed hypergraphs, representing oriented group interactions, are under-explored in spite of their relevance. Current methods struggle with heterophilic settings due to homophily bias.

Method: Introduced DSHN, a framework combining sheaf theory with directed hypergraph analysis using the Directed Sheaf Hypergraph Laplacian, a complex-valued operator unifying previous approaches.

Result: DSHN achieves 2%-20% relative accuracy gains across seven datasets compared to 13 baseline models, demonstrating improved performance in handling directional and asymmetrical relations.

Conclusion: Integrating sheaf theory and directed hypergraph principles enhances representation learning by addressing inherent biases and leveraging expressive mathematical tools.

Abstract: Hypergraphs provide a natural way to represent higher-order interactions
among multiple entities. While undirected hypergraphs have been extensively
studied, the case of directed hypergraphs, which can model oriented group
interactions, remains largely under-explored despite its relevance for many
applications. Recent approaches in this direction often exhibit an implicit
bias toward homophily, which limits their effectiveness in heterophilic
settings. Rooted in the algebraic topology notion of Cellular Sheaves, Sheaf
Neural Networks (SNNs) were introduced as an effective solution to circumvent
such a drawback. While a generalization to hypergraphs is known, it is only
suitable for undirected hypergraphs, failing to tackle the directed case. In
this work, we introduce Directional Sheaf Hypergraph Networks (DSHN), a
framework integrating sheaf theory with a principled treatment of asymmetric
relations within a hypergraph. From it, we construct the Directed Sheaf
Hypergraph Laplacian, a complex-valued operator by which we unify and
generalize many existing Laplacian matrices proposed in the graph- and
hypergraph-learning literature. Across 7 real-world datasets and against 13
baselines, DSHN achieves relative accuracy gains from 2% up to 20%, showing how
a principled treatment of directionality in hypergraphs, combined with the
expressive power of sheaves, can substantially improve performance.

</details>


### [631] [EVaR-Optimal Arm Identification in Bandits](https://arxiv.org/abs/2510.04728)
*Mehrasa Ahmadipour,Aurélien Garivier*

Main category: cs.LG

TL;DR: This paper introduces an algorithm for identifying the best arm under a risk-sensitive measure in multi-armed bandits, providing theoretical bounds and implementation techniques.


<details>
  <summary>Details</summary>
Motivation: Address the need for risk-averse decision-making in environments like finance by focusing on optimizing under the Entropic Value-at-Risk (EVaR) criterion.

Method: Develop a $
abla$-correct Track-and-Stop algorithm and derive a matching lower bound for sample complexity, leveraging both convex and non-convex optimization problems.

Result: The proposed algorithm achieves theoretical soundness by matching the lower bound for expected sample complexity in identification tasks under EVaR.

Conclusion: The work contributes a robust, scalable approach for risk-averse multi-armed bandit problems with proven sample complexity bounds.

Abstract: We study the fixed-confidence best arm identification (BAI) problem within
the multi-armed bandit (MAB) framework under the Entropic Value-at-Risk (EVaR)
criterion. Our analysis considers a nonparametric setting, allowing for general
reward distributions bounded in [0,1]. This formulation addresses the critical
need for risk-averse decision-making in high-stakes environments, such as
finance, moving beyond simple expected value optimization. We propose a
$\delta$-correct, Track-and-Stop based algorithm and derive a corresponding
lower bound on the expected sample complexity, which we prove is asymptotically
matched. The implementation of our algorithm and the characterization of the
lower bound both require solving a complex convex optimization problem and a
related, simpler non-convex one.

</details>


### [632] [Provable Affine Identifiability of Nonlinear CCA under Latent Distributional Priors](https://arxiv.org/abs/2510.04758)
*Zhiwei Han,Stefan Matthes,Hao Shen*

Main category: cs.LG

TL;DR: The paper establishes conditions for nonlinear canonical correlation analysis (CCA) to recover latent factors up to an orthogonal transform after whitening, proving identifiability and outlining the importance of whitening in nonlinear CCA.


<details>
  <summary>Details</summary>
Motivation: Understanding under which conditions nonlinear CCA can achieve identifiability of latent factors is critical for advancing methods in latent variable analysis.

Method: They utilize reparameterization to transform the analysis to source space, provide theoretical proofs for affine identifiability, emphasize the role of whitening for boundedness, and extend results to finite-sample settings using ridge-regularized empirical CCA.

Result: Theoretical proofs confirm identifiability conditions, and experiments validate the theory using synthetic and rendered image datasets while showcasing the importance of underlying assumptions.

Conclusion: Nonlinear CCA can achieve identifiability under specific latent distributions, hinges on whitening, and performs well with finite samples given ridge-regularization, as demonstrated in both theory and experiments.

Abstract: In this work, we establish conditions under which nonlinear CCA recovers the
ground-truth latent factors up to an orthogonal transform after whitening.
Building on the classical result that linear mappings maximize canonical
correlations under Gaussian priors, we prove affine identifiability for a broad
class of latent distributions in the population setting. Central to our proof
is a reparameterization result that transports the analysis from observation
space to source space, where identifiability becomes tractable. We further show
that whitening is essential for ensuring boundedness and well-conditioning,
thereby underpinning identifiability. Beyond the population setting, we prove
that ridge-regularized empirical CCA converges to its population counterpart,
transferring these guarantees to the finite-sample regime. Experiments on a
controlled synthetic dataset and a rendered image dataset validate our theory
and demonstrate the necessity of its assumptions through systematic ablations.

</details>


### [633] [ParallelBench: Understanding the Trade-offs of Parallel Decoding in Diffusion LLMs](https://arxiv.org/abs/2510.04767)
*Wonjun Kang,Kevin Galim,Seunghyuk Oh,Minjae Lee,Yuchen Zeng,Shuibai Zhang,Coleman Hooper,Yuezhou Hu,Hyung Il Koo,Nam Ik Cho,Kangwook Lee*

Main category: cs.LG

TL;DR: The paper highlights fundamental challenges with parallel decoding in diffusion large language models (dLLMs), proposing a benchmark called ParallelBench to systematically analyze these models and suggest improvements to the existing decoding strategies.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the speed-quality trade-off issue in diffusion large language models caused by parallel decoding, which neglects token dependencies and impacts generation quality.

Method: The study involves an information-theoretic analysis and case studies of dLLMs using synthetic tasks, followed by the development of the ParallelBench benchmark to assess and compare decoding strategies.

Result: Key findings reveal that dLLMs suffer significant quality degradation in real-world scenarios under parallel decoding and fail to adapt parallelism effectively based on task difficulty.

Conclusion: Innovative decoding approaches are urgently needed to resolve the speed vs. quality trade-off, with ParallelBench serving as a tool to guide the development of more efficient diffusion LLMs.

Abstract: While most autoregressive LLMs are constrained to one-by-one decoding,
diffusion LLMs (dLLMs) have attracted growing interest for their potential to
dramatically accelerate inference through parallel decoding. Despite this
promise, the conditional independence assumption in dLLMs causes parallel
decoding to ignore token dependencies, inevitably degrading generation quality
when these dependencies are strong. However, existing works largely overlook
these inherent challenges, and evaluations on standard benchmarks (e.g., math
and coding) are not sufficient to capture the quality degradation caused by
parallel decoding. To address this gap, we first provide an
information-theoretic analysis of parallel decoding. We then conduct case
studies on analytically tractable synthetic list operations from both data
distribution and decoding strategy perspectives, offering quantitative insights
that highlight the fundamental limitations of parallel decoding. Building on
these insights, we propose ParallelBench, the first benchmark specifically
designed for dLLMs, featuring realistic tasks that are trivial for humans and
autoregressive LLMs yet exceptionally challenging for dLLMs under parallel
decoding. Using ParallelBench, we systematically analyze both dLLMs and
autoregressive LLMs, revealing that: (i) dLLMs under parallel decoding can
suffer dramatic quality degradation in real-world scenarios, and (ii) current
parallel decoding strategies struggle to adapt their degree of parallelism
based on task difficulty, thus failing to achieve meaningful speedup without
compromising quality. Our findings underscore the pressing need for innovative
decoding methods that can overcome the current speed-quality trade-off. We
release our benchmark to help accelerate the development of truly efficient
dLLMs.

</details>


### [634] [Distribution Preference Optimization: A Fine-grained Perspective for LLM Unlearning](https://arxiv.org/abs/2510.04773)
*Kai Qin,Jiaqi Wu,Jianxiang He,Haoyuan Sun,Yifei Zhao,Bin Liang,Yongzhe Chang,Tiantian Zhang,Houde Liu*

Main category: cs.LG

TL;DR: This paper introduces DiPO, a novel optimization-based unlearning method for Large Language Models (LLMs). By focusing on next-token probability distributions, it addresses the limitations of prior methods and achieves superior trade-offs in utility and forget quality.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the growing need to address privacy and data safety issues in LLMs by developing effective unlearning techniques.

Method: The proposed method, DiPO, targets next-token probability distributions, creating preference pairs by amplifying or suppressing high-confidence logits. This approach overcomes the limitations of prior unlearning methods like NPO.

Result: DiPO demonstrates the best forget quality on the TOFU benchmark and strong performance in preserving utility on the MUSE benchmark, validating its effectiveness.

Conclusion: The study concludes that DiPO provides a scalable and sustainable solution for LLM unlearning, achieving a balance between model utility and forgetting quality.

Abstract: As Large Language Models (LLMs) demonstrate remarkable capabilities learned
from vast corpora, concerns regarding data privacy and safety are receiving
increasing attention. LLM unlearning, which aims to remove the influence of
specific data while preserving overall model utility, is becoming an important
research area. One of the mainstream unlearning classes is optimization-based
methods, which achieve forgetting directly through fine-tuning, exemplified by
Negative Preference Optimization (NPO). However, NPO's effectiveness is limited
by its inherent lack of explicit positive preference signals. Attempts to
introduce such signals by constructing preferred responses often necessitate
domain-specific knowledge or well-designed prompts, fundamentally restricting
their generalizability. In this paper, we shift the focus to the
distribution-level, directly targeting the next-token probability distribution
instead of entire responses, and derive a novel unlearning algorithm termed
\textbf{Di}stribution \textbf{P}reference \textbf{O}ptimization (DiPO). We show
that the requisite preference distribution pairs for DiPO, which are
distributions over the model's output tokens, can be constructed by selectively
amplifying or suppressing the model's high-confidence output logits, thereby
effectively overcoming NPO's limitations. We theoretically prove the
consistency of DiPO's loss function with the desired unlearning direction.
Extensive experiments demonstrate that DiPO achieves a strong trade-off between
model utility and forget quality. Notably, DiPO attains the highest forget
quality on the TOFU benchmark, and maintains leading scalability and
sustainability in utility preservation on the MUSE benchmark.

</details>


### [635] [MetaMP: Seamless Metadata Enrichment and AI Application Framework for Enhanced Membrane Protein Visualization and Analysis](https://arxiv.org/abs/2510.04776)
*Ebenezer Awotoro,Chisom Ezekannagha,Florian Schwarz,Johannes Tauscher,Dominik Heider,Katharina Ladewig,Christel Le Bon,Karine Moncoq,Bruno Miroux,Georges Hattab*

Main category: cs.LG

TL;DR: The paper introduces MetaMP, a framework for membrane protein databases using machine learning for improved classification, data integration, and exploration.


<details>
  <summary>Details</summary>
Motivation: Motivated by challenges in membrane protein structure analysis, such as missing data and inconsistencies, the study aims to improve the integration and quality of existing databases.

Method: MetaMP leverages machine learning for classifying membrane proteins and provides a user-friendly web application with enriched metadata and diverse exploration views.

Result: MetaMP resolved 77% of data discrepancies, predicted membrane protein classes with 98% accuracy, and outperformed expert curation.

Conclusion: MetaMP streamlines membrane protein data exploration and supports AI-driven research, offering significant advancements in database integration, prediction accuracy, and usability.

Abstract: Structural biology has made significant progress in determining membrane
proteins, leading to a remarkable increase in the number of available
structures in dedicated databases. The inherent complexity of membrane protein
structures, coupled with challenges such as missing data, inconsistencies, and
computational barriers from disparate sources, underscores the need for
improved database integration. To address this gap, we present MetaMP, a
framework that unifies membrane-protein databases within a web application and
uses machine learning for classification. MetaMP improves data quality by
enriching metadata, offering a user-friendly interface, and providing eight
interactive views for streamlined exploration. MetaMP was effective across
tasks of varying difficulty, demonstrating advantages across different levels
without compromising speed or accuracy, according to user evaluations.
Moreover, MetaMP supports essential functions such as structure classification
and outlier detection.
  We present three practical applications of Artificial Intelligence (AI) in
membrane protein research: predicting transmembrane segments, reconciling
legacy databases, and classifying structures with explainable AI support. In a
validation focused on statistics, MetaMP resolved 77% of data discrepancies and
accurately predicted the class of newly identified membrane proteins 98% of the
time and overtook expert curation. Altogether, MetaMP is a much-needed resource
that harmonizes current knowledge and empowers AI-driven exploration of
membrane-protein architecture.

</details>


### [636] [Learning on the Job: Test-Time Curricula for Targeted Reinforcement Learning](https://arxiv.org/abs/2510.04786)
*Jonas Hübotter,Leander Diaz-Bone,Ido Hakimi,Andreas Krause,Moritz Hardt*

Main category: cs.LG

TL;DR: The paper proposes Test-Time Curriculum Reinforcement Learning (TTC-RL), enabling models to adaptively learn tasks post-deployment using relevant data and reinforcement learning. Results show significant performance improvements in math and coding benchmarks.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of task-specific model improvements without manual dataset curation by enabling automated selection of pertinent training data for continual learning.

Method: The authors introduce TTC-RL, a model using reinforcement learning to dynamically curate a task-specific training curriculum from a large pool of available data during test time.

Result: TTC-RL improved the Qwen3-8B model's pass@1 on AIME25 by 1.8x and CodeElo by 2.1x, and substantially enhanced pass@8 performance from 40% to 62% on AIME25 and from 28% to 43% on CodeElo.

Conclusion: Automatic task-relevant data selection via TTC-RL enables continual post-deployment model improvement and significantly raises task performance, demonstrating the potential of test-time curricula.

Abstract: Humans are good at learning on the job: We learn how to solve the tasks we
face as we go along. Can a model do the same? We propose an agent that
assembles a task-specific curriculum, called test-time curriculum (TTC-RL), and
applies reinforcement learning to continue training the model for its target
task. The test-time curriculum avoids time-consuming human curation of datasets
by automatically selecting the most task-relevant data from a large pool of
available training data. Our experiments demonstrate that reinforcement
learning on a test-time curriculum consistently improves the model on its
target tasks, across a variety of evaluations and models. Notably, on
challenging math and coding benchmarks, TTC-RL improves the pass@1 of Qwen3-8B
by approximately 1.8x on AIME25 and 2.1x on CodeElo. Moreover, we find that
TTC-RL significantly raises the performance ceiling compared to the initial
model, increasing pass@8 on AIME25 from 40% to 62% and on CodeElo from 28% to
43%. Our findings show the potential of test-time curricula in extending the
test-time scaling paradigm to continual training on thousands of task-relevant
experiences during test-time.

</details>


### [637] [On Predicting Post-Click Conversion Rate via Counterfactual Inference](https://arxiv.org/abs/2510.04816)
*Junhyung Ahn,Sanghack Lee*

Main category: cs.LG

TL;DR: The paper proposes ESCIM, a method for generating conversion labels for non-clicked samples in CVR prediction using causality principles, addressing data sparsity issues.


<details>
  <summary>Details</summary>
Motivation: To effectively predict CVRs despite the sparsity of clicked instances and overcome biases in current models, utilizing non-clicked data in a causal manner.

Method: The ESCIM framework trains a structural causal model to intervene on non-clicked items, infer counterfactual CVRs, transform them into binary labels, and integrate them into the training process.

Result: Experiments on public datasets and online A/B testing validate the ESCIM model's effectiveness and its robust performance on latent conversion data.

Conclusion: ESCIM improves CVR prediction accuracy with causality-based methods, showing superior robustness and generalization in real-world and latent data scenarios.

Abstract: Accurately predicting conversion rate (CVR) is essential in various
recommendation domains such as online advertising systems and e-commerce. These
systems utilize user interaction logs, which consist of exposures, clicks, and
conversions. CVR prediction models are typically trained solely based on
clicked samples, as conversions can only be determined following clicks.
However, the sparsity of clicked instances necessitates the collection of a
substantial amount of logs for effective model training. Recent works address
this issue by devising frameworks that leverage non-clicked samples. While
these frameworks aim to reduce biases caused by the discrepancy between clicked
and non-clicked samples, they often rely on heuristics. Against this
background, we propose a method to counterfactually generate conversion labels
for non-clicked samples by using causality as a guiding principle, attempting
to answer the question, "Would the user have converted if he or she had clicked
the recommended item?" Our approach is named the Entire Space Counterfactual
Inference Multi-task Model (ESCIM). We initially train a structural causal
model (SCM) of user sequential behaviors and conduct a hypothetical
intervention (i.e., click) on non-clicked items to infer counterfactual CVRs.
We then introduce several approaches to transform predicted counterfactual CVRs
into binary counterfactual conversion labels for the non-clicked samples.
Finally, the generated samples are incorporated into the training process.
Extensive experiments on public datasets illustrate the superiority of the
proposed algorithm. Online A/B testing further empirically validates the
effectiveness of our proposed algorithm in real-world scenarios. In addition,
we demonstrate the improved performance of the proposed method on latent
conversion data, showcasing its robustness and superior generalization
capabilities.

</details>


### [638] [On the Hardness of Learning Regular Expressions](https://arxiv.org/abs/2510.04834)
*Idan Attias,Lev Reyzin,Nathan Srebro,Gal Vardi*

Main category: cs.LG

TL;DR: Explores computational hardness of learning regular expressions, showing challenges in PAC learning and learning with membership queries under various conditions.


<details>
  <summary>Details</summary>
Motivation: Understanding the theoretical limits and computational challenges of learning regular expressions, a critical tool in computer science and practical applications.

Method: Analyzing PAC learning complexity, distribution-free learning, and providing proofs of computational hardness under different extensions and query conditions.

Result: Demonstrated hardness of PAC learning under uniform distribution and proving difficulty of distribution-free learning with membership queries. Extended findings to complement and intersection extensions of regular expressions.

Conclusion: Learning regular expressions is computationally hard under various conditions, highlighting theoretical limits and complexities beyond existing DFA or NFA learning hardness results.

Abstract: Despite the theoretical significance and wide practical use of regular
expressions, the computational complexity of learning them has been largely
unexplored. We study the computational hardness of improperly learning regular
expressions in the PAC model and with membership queries. We show that PAC
learning is hard even under the uniform distribution on the hypercube, and also
prove hardness of distribution-free learning with membership queries.
Furthermore, if regular expressions are extended with complement or
intersection, we establish hardness of learning with membership queries even
under the uniform distribution. We emphasize that these results do not follow
from existing hardness results for learning DFAs or NFAs, since the descriptive
complexity of regular languages can differ exponentially between DFAs, NFAs,
and regular expressions.

</details>


### [639] [Bond-Centered Molecular Fingerprint Derivatives: A BBBP Dataset Study](https://arxiv.org/abs/2510.04837)
*Guillaume Godin*

Main category: cs.LG

TL;DR: Bond-Centered Fingerprints (BCFP) offer a bond-focused alternative to Extended-Connectivity Fingerprints (ECFP) and enhance prediction performance when both are combined.


<details>
  <summary>Details</summary>
Motivation: To improve prediction performance in Brain-Blood Barrier Penetration (BBBP) tasks by exploring a complementary, bond-centric descriptor to traditional atom-centric fingerprints.

Method: The authors evaluated a static version of BCFP combined with ECFP using a Random Forest model across stratified cross-validation for BBBP classification tasks.

Result: Combining ECFP with BCFP improves AUROC and AUPRC metrics, with r = 1 performing best among radii, and achieving better performance than MGTP predictions.

Conclusion: Lightweight, bond-centric descriptors like BCFP can effectively complement atom-centered fingerprints for fast and accurate BBBP prediction baselines.

Abstract: Bond Centered FingerPrint (BCFP) are a complementary, bond-centric
alternative to Extended-Connectivity Fingerprints (ECFP). We introduce a static
BCFP that mirrors the bond-convolution used by directed message-passing GNNs
like ChemProp, and evaluate it with a fast rapid Random Forest model on
Brain-Blood Barrier Penetration (BBBP) classification task. Across stratified
cross-validation, concatenating ECFP with BCFP consistently improves AUROC and
AUPRC over either descriptor alone, as confirmed by Turkey HSD
multiple-comparison analysis. Among radii, r = 1 performs best; r = 2 does not
yield statistically separable gains under the same test. We further propose
BCFP-Sort&Slice, a simple feature-combination scheme that preserves the
out-of-vocabulary (OOV) count information native to ECFP count vectors while
enabling compact unhashed concatenation of BCFP variants. We also outperform
the MGTP prediction on our BBBP evaluation, using such composite new features
bond and atom features. These results show that lightweight, bond-centered
descriptors can complement atom-centered circular fingerprints and provide
strong, fast baselines for BBBP prediction.

</details>


### [640] [Distributionally Robust Causal Abstractions](https://arxiv.org/abs/2510.04842)
*Yorgos Felekis,Theodoros Damoulas,Paris Giampouras*

Main category: cs.LG

TL;DR: This paper introduces distributionally robust causal abstractions (CAs) with learning algorithms for enhancing resilience to environmental shifts and model misspecifications.


<details>
  <summary>Details</summary>
Motivation: Traditional causal abstraction learning models fail in accuracy and consistency when exposed to unseen environmental shifts and fixed exogenous distributions.

Method: The paper formulates robust CA learning as a constrained min-max optimization problem, employing Wasserstein ambiguity sets to manage environmental uncertainty.

Result: Theoretical insights were provided to choose robustness levels, with experiments confirming the framework's improved performance and reliability under varying scenarios.

Conclusion: The proposed robust causal abstraction framework enhances the reliability of causal inference in dynamic and uncertain environments, addressing existing limitations of classical CA methods.

Abstract: Causal Abstraction (CA) theory provides a principled framework for relating
causal models that describe the same system at different levels of granularity
while ensuring interventional consistency between them. Recently, several
approaches for learning CAs have been proposed, but all assume fixed and
well-specified exogenous distributions, making them vulnerable to environmental
shifts and misspecification. In this work, we address these limitations by
introducing the first class of distributionally robust CAs and their associated
learning algorithms. The latter cast robust causal abstraction learning as a
constrained min-max optimization problem with Wasserstein ambiguity sets. We
provide theoretical results, for both empirical and Gaussian environments,
leading to principled selection of the level of robustness via the radius of
these sets. Furthermore, we present empirical evidence across different
problems and CA learning methods, demonstrating our framework's robustness not
only to environmental shifts but also to structural model and intervention
mapping misspecification.

</details>


### [641] [Synthesising Counterfactual Explanations via Label-Conditional Gaussian Mixture Variational Autoencoders](https://arxiv.org/abs/2510.04855)
*Junqi Jiang,Francesco Leofante,Antonio Rago,Francesca Toni*

Main category: cs.LG

TL;DR: This paper proposes LAPACE, a generative framework combining L-GMVAE and latent path interpolation to create diverse and robust counterfactual explanations for algorithmic decision-making.


<details>
  <summary>Details</summary>
Motivation: The motivation arises from the lack of unified, model-agnostic approaches to generate robust counterfactual explanations that balance diversity and plausibility while addressing perturbations and user constraints.

Method: The authors introduce L-GMVAE, a structured latent space model representing class labels as Gaussian components, along with LAPACE, an algorithm that interpolates paths on this latent space to generate multiple counterfactuals for a single input.

Result: LAPACE ensures robustness to input/model perturbations, offers diverse recourse options, integrates user-specific constraints, and performs competitively across eight metrics in computational experiments.

Conclusion: The approach achieves computational efficiency, robustness, and versatility in generating counterfactual explanations, advancing the state-of-art practices in algorithmic decision recourse systems.

Abstract: Counterfactual explanations (CEs) provide recourse recommendations for
individuals affected by algorithmic decisions. A key challenge is generating
CEs that are robust against various perturbation types (e.g. input and model
perturbations) while simultaneously satisfying other desirable properties.
These include plausibility, ensuring CEs reside on the data manifold, and
diversity, providing multiple distinct recourse options for single inputs.
Existing methods, however, mostly struggle to address these multifaceted
requirements in a unified, model-agnostic manner. We address these limitations
by proposing a novel generative framework. First, we introduce the
Label-conditional Gaussian Mixture Variational Autoencoder (L-GMVAE), a model
trained to learn a structured latent space where each class label is
represented by a set of Gaussian components with diverse, prototypical
centroids. Building on this, we present LAPACE (LAtent PAth Counterfactual
Explanations), a model-agnostic algorithm that synthesises entire paths of CE
points by interpolating from inputs' latent representations to those learned
latent centroids. This approach inherently ensures robustness to input changes,
as all paths for a given target class converge to the same fixed centroids.
Furthermore, the generated paths provide a spectrum of recourse options,
allowing users to navigate the trade-off between proximity and plausibility
while also encouraging robustness against model changes. In addition,
user-specified actionability constraints can also be easily incorporated via
lightweight gradient optimisation through the L-GMVAE's decoder. Comprehensive
experiments show that LAPACE is computationally efficient and achieves
competitive performance across eight quantitative metrics.

</details>


### [642] [Alignment Tipping Process: How Self-Evolution Pushes LLM Agents Off the Rails](https://arxiv.org/abs/2510.04860)
*Siwei Han,Jiaqi Liu,Yaofeng Su,Wenbo Duan,Xinyuan Liu,Cihang Xie,Mohit Bansal,Mingyu Ding,Linjun Zhang,Huaxiu Yao*

Main category: cs.LG

TL;DR: This paper identifies a critical risk in self-evolving large language models (LLMs) called the Alignment Tipping Process (ATP), where models deviate from initial alignment constraints due to continual interaction and feedback.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the long-term reliability of self-evolving LLM agents, specifically focusing on their tendency to shift from aligned to self-interested behaviors during real-world interactions.

Method: The authors formalize ATP through two paradigms: Self-Interested Exploration and Imitative Strategy Diffusion. They create testbeds and benchmark evaluations using Qwen3-8B and Llama-3.1-8B-Instruct models, analyzing performance under self-evolution.

Result: Experiments reveal rapid erosion of alignment in self-evolving models, with collective misalignment observed in multi-agent settings. Reinforcement learning-based alignment methods are shown to offer only weak resistance to ATP.

Conclusion: Alignment in self-evolving LLM agents is a dynamic property prone to degradation over time, highlighting the need for more robust strategies to ensure sustained alignment during deployment.

Abstract: As Large Language Model (LLM) agents increasingly gain self-evolutionary
capabilities to adapt and refine their strategies through real-world
interaction, their long-term reliability becomes a critical concern. We
identify the Alignment Tipping Process (ATP), a critical post-deployment risk
unique to self-evolving LLM agents. Unlike training-time failures, ATP arises
when continual interaction drives agents to abandon alignment constraints
established during training in favor of reinforced, self-interested strategies.
We formalize and analyze ATP through two complementary paradigms:
Self-Interested Exploration, where repeated high-reward deviations induce
individual behavioral drift, and Imitative Strategy Diffusion, where deviant
behaviors spread across multi-agent systems. Building on these paradigms, we
construct controllable testbeds and benchmark Qwen3-8B and
Llama-3.1-8B-Instruct. Our experiments show that alignment benefits erode
rapidly under self-evolution, with initially aligned models converging toward
unaligned states. In multi-agent settings, successful violations diffuse
quickly, leading to collective misalignment. Moreover, current reinforcement
learning-based alignment methods provide only fragile defenses against
alignment tipping. Together, these findings demonstrate that alignment of LLM
agents is not a static property but a fragile and dynamic one, vulnerable to
feedback-driven decay during deployment. Our data and code are available at
https://github.com/aiming-lab/ATP.

</details>


### [643] [A Clinical-grade Universal Foundation Model for Intraoperative Pathology](https://arxiv.org/abs/2510.04861)
*Zihan Zhao,Fengtao Zhou,Ronggang Li,Bing Chu,Xinke Zhang,Xueyi Zheng,Ke Zheng,Xiaobo Wen,Jiabo Ma,Yihui Wang,Jiewei Chen,Chengyou Zheng,Jiangyu Zhang,Yongqin Wen,Jiajia Meng,Ziqi Zeng,Xiaoqing Li,Jing Li,Dan Xie,Yaping Ye,Yu Wang,Hao Chen,Muyan Cai*

Main category: cs.LG

TL;DR: The paper introduces CRISP, an AI foundation model trained on over 100,000 frozen pathology sections, showcasing its robust diagnostic performance and transformative potential in intraoperative pathology.


<details>
  <summary>Details</summary>
Motivation: To address the diagnostic complexity and limited availability of high-quality intraoperative pathology data, and to facilitate precision surgery by integrating AI into intraoperative workflows.

Method: CRISP was trained on a large-scale dataset of over 100,000 frozen sections from multiple centers and evaluated retrospectively on nearly 100 diagnostic tasks and prospectively on over 2,000 patient cases under real-world conditions.

Result: CRISP achieved strong generalization across institutions, tumor types, and rare cancers, informed surgical decisions in 92.6% of cases, reduced workload by 35%, and improved micrometastasis detection with 87.5% accuracy.

Conclusion: CRISP represents a transformative clinical-grade AI model, demonstrating practical utility, efficiency, and precision in intraoperative pathology, significantly enhancing surgical decision-making.

Abstract: Intraoperative pathology is pivotal to precision surgery, yet its clinical
impact is constrained by diagnostic complexity and the limited availability of
high-quality frozen-section data. While computational pathology has made
significant strides, the lack of large-scale, prospective validation has
impeded its routine adoption in surgical workflows. Here, we introduce CRISP, a
clinical-grade foundation model developed on over 100,000 frozen sections from
eight medical centers, specifically designed to provide Clinical-grade Robust
Intraoperative Support for Pathology (CRISP). CRISP was comprehensively
evaluated on more than 15,000 intraoperative slides across nearly 100
retrospective diagnostic tasks, including benign-malignant discrimination, key
intraoperative decision-making, and pan-cancer detection, etc. The model
demonstrated robust generalization across diverse institutions, tumor types,
and anatomical sites-including previously unseen sites and rare cancers. In a
prospective cohort of over 2,000 patients, CRISP sustained high diagnostic
accuracy under real-world conditions, directly informing surgical decisions in
92.6% of cases. Human-AI collaboration further reduced diagnostic workload by
35%, avoided 105 ancillary tests and enhanced detection of micrometastases with
87.5% accuracy. Together, these findings position CRISP as a clinical-grade
paradigm for AI-driven intraoperative pathology, bridging computational
advances with surgical precision and accelerating the translation of artificial
intelligence into routine clinical practice.

</details>


### [644] [Less is More: Recursive Reasoning with Tiny Networks](https://arxiv.org/abs/2510.04871)
*Alexia Jolicoeur-Martineau*

Main category: cs.LG

TL;DR: This study introduces the Tiny Recursive Model (TRM), a minimalistic neural network achieving impressive performance on challenging puzzle tasks with far fewer resources than large language models.


<details>
  <summary>Details</summary>
Motivation: The motivation for the paper is to explore efficient alternatives to large language models that are capable of solving hard reasoning tasks with minimal computational resources.

Method: The study proposes TRM, a recursive reasoning approach leveraging a single 2-layer network with just 7 million parameters.

Result: TRM achieves 45% test accuracy on ARC-AGI-1 and 8% on ARC-AGI-2, outperforming many large models with substantially fewer parameters.

Conclusion: TRM demonstrates potential for solving complex reasoning tasks more efficiently, proving that simpler models can achieve competitive or superior results compared to large-scale solutions.

Abstract: Hierarchical Reasoning Model (HRM) is a novel approach using two small neural
networks recursing at different frequencies. This biologically inspired method
beats Large Language models (LLMs) on hard puzzle tasks such as Sudoku, Maze,
and ARC-AGI while trained with small models (27M parameters) on small data
(around 1000 examples). HRM holds great promise for solving hard problems with
small networks, but it is not yet well understood and may be suboptimal. We
propose Tiny Recursive Model (TRM), a much simpler recursive reasoning approach
that achieves significantly higher generalization than HRM, while using a
single tiny network with only 2 layers. With only 7M parameters, TRM obtains
45% test-accuracy on ARC-AGI-1 and 8% on ARC-AGI-2, higher than most LLMs
(e.g., Deepseek R1, o3-mini, Gemini 2.5 Pro) with less than 0.01% of the
parameters.

</details>


### [645] [Flow-Matching Based Refiner for Molecular Conformer Generation](https://arxiv.org/abs/2510.04878)
*Xiangyang Xu,Hongyang Gao*

Main category: cs.LG

TL;DR: The paper introduces a flow-matching refiner to improve molecular conformer generation quality and sampling efficiency.


<details>
  <summary>Details</summary>
Motivation: Address the challenges of error accumulation and inefficiency in low-SNR steps in denoising-based methods for molecular conformer generation.

Method: A generator-refiner pipeline where the flow-matching refiner reschedules noise scales to bypass the problematic low-SNR phase and enhances sample quality.

Result: On benchmark datasets, the method achieves better conformer quality with fewer denoising steps while maintaining diversity.

Conclusion: The flow-matching refinement successfully resolves issues in denoising methods, enhancing the generation of molecular conformers in terms of quality and efficiency.

Abstract: Low-energy molecular conformers generation (MCG) is a foundational yet
challenging problem in drug discovery. Denoising-based methods include
diffusion and flow-matching methods that learn mappings from a simple base
distribution to the molecular conformer distribution. However, these approaches
often suffer from error accumulation during sampling, especially in the low SNR
steps, which are hard to train. To address these challenges, we propose a
flow-matching refiner for the MCG task. The proposed method initializes
sampling from mixed-quality outputs produced by upstream denoising models and
reschedules the noise scale to bypass the low-SNR phase, thereby improving
sample quality. On the GEOM-QM9 and GEOM-Drugs benchmark datasets, the
generator-refiner pipeline improves quality with fewer total denoising steps
while preserving diversity.

</details>


### [646] [Revealing Interconnections between Diseases: from Statistical Methods to Large Language Models](https://arxiv.org/abs/2510.04888)
*Alina Ermilova,Dmitrii Kornilov,Sofia Samoilova,Ekaterina Laptenkova,Anastasia Kolesnikova,Ekaterina Podplutova,Senotrusova Sofya,Maksim G. Sharaev*

Main category: cs.LG

TL;DR: The paper evaluates seven methods to discover disease relationships using ICD-10 codes and highlights that large language models (LLMs) are less effective compared to domain-specific approaches.


<details>
  <summary>Details</summary>
Motivation: Manual analysis of disease interconnections in large-scale clinical data is challenging due to labor-intensity, subjectivity, and expert disagreement. The study aims to address whether machine learning methods can overcome these limitations using real-world clinical data.

Method: A systematic evaluation of seven approaches, including statistical analysis, masked language modeling, domain-specific BERT models, general-purpose BERT models, and four large language models (LLMs) applied to ICD-10 code datasets.

Result: LLMs produced interconnections with lower diversity in ICD code connections compared to domain-specific statistical and textual approaches.

Conclusion: LLMs hold limited potential for discovering new disease interconnections. The study provides a foundational disease ontology resource for future clinical and AI research.

Abstract: Identifying disease interconnections through manual analysis of large-scale
clinical data is labor-intensive, subjective, and prone to expert disagreement.
While machine learning (ML) shows promise, three critical challenges remain:
(1) selecting optimal methods from the vast ML landscape, (2) determining
whether real-world clinical data (e.g., electronic health records, EHRs) or
structured disease descriptions yield more reliable insights, (3) the lack of
"ground truth," as some disease interconnections remain unexplored in medicine.
Large language models (LLMs) demonstrate broad utility, yet they often lack
specialized medical knowledge. To address these gaps, we conduct a systematic
evaluation of seven approaches for uncovering disease relationships based on
two data sources: (i) sequences of ICD-10 codes from MIMIC-IV EHRs and (ii) the
full set of ICD-10 codes, both with and without textual descriptions. Our
framework integrates the following: (i) a statistical co-occurrence analysis
and a masked language modeling (MLM) approach using real clinical data; (ii)
domain-specific BERT variants (Med-BERT and BioClinicalBERT); (iii) a
general-purpose BERT and document retrieval; and (iv) four LLMs (Mistral,
DeepSeek, Qwen, and YandexGPT). Our graph-based comparison of the obtained
interconnection matrices shows that the LLM-based approach produces
interconnections with the lowest diversity of ICD code connections to different
diseases compared to other methods, including text-based and domain-based
approaches. This suggests an important implication: LLMs have limited potential
for discovering new interconnections. In the absence of ground truth databases
for medical interconnections between ICD codes, our results constitute a
valuable medical disease ontology that can serve as a foundational resource for
future clinical research and artificial intelligence applications in
healthcare.

</details>


### [647] [Benchmarking M-LTSF: Frequency and Noise-Based Evaluation of Multivariate Long Time Series Forecasting Models](https://arxiv.org/abs/2510.04900)
*Nick Janßen,Melanie Schaller,Bodo Rosenhahn*

Main category: cs.LG

TL;DR: This paper proposes a simulation-based framework to evaluate deep learning models for multivariate long-term time series forecasting in a controlled manner, offering insights into their robustness under varying signal and noise conditions.


<details>
  <summary>Details</summary>
Motivation: Evaluating the robustness of deep learning models for multivariate long-term time series forecasting is difficult due to reliance on real-world datasets with unknown noise properties.

Method: The paper introduces a synthetic data generation framework that simulates datasets with configurable signal and noise properties to systematically test models' performances under controlled scenarios.

Result: The benchmarking of four model architectures (S-Mamba, iTransformer, R-Linear, Autoformer) reveals their strengths and weaknesses concerning seasonal patterns, signal types, and noise vulnerabilities. S-Mamba and iTransformer excel in frequency reconstruction, with differing sensitivities to noise types.

Conclusion: The framework offers systematic insights into model performance under varying conditions, providing guidance for model selection based on specific signal features and noise conditions.

Abstract: Understanding the robustness of deep learning models for multivariate
long-term time series forecasting (M-LTSF) remains challenging, as evaluations
typically rely on real-world datasets with unknown noise properties. We propose
a simulation-based evaluation framework that generates parameterizable
synthetic datasets, where each dataset instance corresponds to a different
configuration of signal components, noise types, signal-to-noise ratios, and
frequency characteristics. These configurable components aim to model
real-world multivariate time series data without the ambiguity of unknown
noise. This framework enables fine-grained, systematic evaluation of M-LTSF
models under controlled and diverse scenarios. We benchmark four representative
architectures S-Mamba (state-space), iTransformer (transformer-based), R-Linear
(linear), and Autoformer (decomposition-based). Our analysis reveals that all
models degrade severely when lookback windows cannot capture complete periods
of seasonal patters in the data. S-Mamba and Autoformer perform best on
sawtooth patterns, while R-Linear and iTransformer favor sinusoidal signals.
White and Brownian noise universally degrade performance with lower
signal-to-noise ratio while S-Mamba shows specific trend-noise and iTransformer
shows seasonal-noise vulnerability. Further spectral analysis shows that
S-Mamba and iTransformer achieve superior frequency reconstruction. This
controlled approach, based on our synthetic and principle-driven testbed,
offers deeper insights into model-specific strengths and limitations through
the aggregation of MSE scores and provides concrete guidance for model
selection based on signal characteristics and noise conditions.

</details>


### [648] [Focused Skill Discovery: Learning to Control Specific State Variables while Minimizing Side Effects](https://arxiv.org/abs/2510.04901)
*Jonathan Colaço Carr,Qinyi Sun,Cameron Allen*

Main category: cs.LG

TL;DR: The paper addresses the limitations of existing skill discovery algorithms in reinforcement learning, proposing a method to learn skills that specifically control state variables for improved task efficiency.


<details>
  <summary>Details</summary>
Motivation: To address inefficiencies in skill discovery algorithms, which often fail to target specific state variables in reinforcement learning tasks, impacting exploration, usability, and downstream performance.

Method: The authors developed a general framework for skill discovery algorithms to identify and focus on specific state variables, enabling better control and precision in the acquired skills.

Result: Their proposed approach significantly increased state space coverage (threefold), improved learning capabilities, and minimized negative downstream impacts.

Conclusion: The method enhances skill discovery by achieving better state control, improving exploration, and ensuring positive outcomes in reinforcement learning tasks.

Abstract: Skills are essential for unlocking higher levels of problem solving. A common
approach to discovering these skills is to learn ones that reliably reach
different states, thus empowering the agent to control its environment.
However, existing skill discovery algorithms often overlook the natural state
variables present in many reinforcement learning problems, meaning that the
discovered skills lack control of specific state variables. This can
significantly hamper exploration efficiency, make skills more challenging to
learn with, and lead to negative side effects in downstream tasks when the goal
is under-specified. We introduce a general method that enables these skill
discovery algorithms to learn focused skills -- skills that target and control
specific state variables. Our approach improves state space coverage by a
factor of three, unlocks new learning capabilities, and automatically avoids
negative side effects in downstream tasks.

</details>


### [649] [DP-HYPE: Distributed Differentially Private Hyperparameter Search](https://arxiv.org/abs/2510.04902)
*Johannes Liebenow,Thorsten Peinemann,Esfandiar Mohammadi*

Main category: cs.LG

TL;DR: The paper presents DP-HYPE, a distributed and privacy-preserving algorithm for tuning hyperparameters under client-level differential privacy, demonstrating high utility in various settings.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the privacy challenges in distributed hyperparameter tuning for machine learning when working with sensitive data, avoiding privacy-utility trade-offs of existing methods.

Method: DP-HYPE employs a distributed voting mechanism where clients perform local evaluations of hyperparameters to choose a majority-supported compromise, ensuring client-level differential privacy without dependency on the number of hyperparameters.

Result: The algorithm is proven to be differentially private at the client level, guarantees utility (probability of compromise), and demonstrates high performance across various datasets and privacy budgets when implemented in the Flower framework.

Conclusion: DP-HYPE successfully balances scalability, utility, and strong privacy guarantees for distributed machine learning, making it a practical solution for private hyperparameter tuning.

Abstract: The tuning of hyperparameters in distributed machine learning can
substantially impact model performance. When the hyperparameters are tuned on
sensitive data, privacy becomes an important challenge and to this end,
differential privacy has emerged as the de facto standard for provable privacy.
A standard setting when performing distributed learning tasks is that clients
agree on a shared setup, i.e., find a compromise from a set of hyperparameters,
like the learning rate of the model to be trained. Yet, prior work on
differentially private hyperparameter tuning either uses computationally
expensive cryptographic protocols, determines hyperparameters separately for
each client, or applies differential privacy locally, which can lead to
undesirable utility-privacy trade-offs.
  In this work, we present our algorithm DP-HYPE, which performs a distributed
and privacy-preserving hyperparameter search by conducting a distributed voting
based on local hyperparameter evaluations of clients. In this way, DP-HYPE
selects hyperparameters that lead to a compromise supported by the majority of
clients, while maintaining scalability and independence from specific learning
tasks. We prove that DP-HYPE preserves the strong notion of differential
privacy called client-level differential privacy and, importantly, show that
its privacy guarantees do not depend on the number of hyperparameters. We also
provide bounds on its utility guarantees, that is, the probability of reaching
a compromise, and implement DP-HYPE as a submodule in the popular Flower
framework for distributed machine learning. In addition, we evaluate
performance on multiple benchmark data sets in iid as well as multiple non-iid
settings and demonstrate high utility of DP-HYPE even under small privacy
budgets.

</details>


### [650] [How Different from the Past? Spatio-Temporal Time Series Forecasting with Self-Supervised Deviation Learning](https://arxiv.org/abs/2510.04908)
*Haotian Gao,Zheng Dong,Jiawei Yong,Shintaro Fukushima,Kenjiro Taura,Renhe Jiang*

Main category: cs.LG

TL;DR: The paper introduces ST-SSDL, a framework for spatio-temporal forecasting that incorporates deviation learning to address dynamic changes in data. It outperforms existing methods based on experiments across benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: Current forecasting methods frequently overlook dynamic deviations in input data from historical patterns, despite their significant impact on model performance.

Method: ST-SSDL employs a Self-Supervised Deviation Learning scheme with learnable prototypes to represent typical spatio-temporal patterns, along with two auxiliary objectives: a contrastive loss and a deviation loss, optimized alongside the forecasting objective.

Result: ST-SSDL achieves superior performance compared to state-of-the-art baselines across six benchmark datasets and demonstrates adaptive behavior under varying deviations.

Conclusion: Incorporating deviation learning enhances forecasting frameworks, and ST-SSDL successfully organizes latent spaces to better generalize in diverse scenarios.

Abstract: Spatio-temporal forecasting is essential for real-world applications such as
traffic management and urban computing. Although recent methods have shown
improved accuracy, they often fail to account for dynamic deviations between
current inputs and historical patterns. These deviations contain critical
signals that can significantly affect model performance. To fill this gap, we
propose ST-SSDL, a Spatio-Temporal time series forecasting framework that
incorporates a Self-Supervised Deviation Learning scheme to capture and utilize
such deviations. ST-SSDL anchors each input to its historical average and
discretizes the latent space using learnable prototypes that represent typical
spatio-temporal patterns. Two auxiliary objectives are proposed to refine this
structure: a contrastive loss that enhances inter-prototype discriminability
and a deviation loss that regularizes the distance consistency between input
representations and corresponding prototypes to quantify deviation. Optimized
jointly with the forecasting objective, these components guide the model to
organize its hidden space and improve generalization across diverse input
conditions. Experiments on six benchmark datasets show that ST-SSDL
consistently outperforms state-of-the-art baselines across multiple metrics.
Visualizations further demonstrate its ability to adaptively respond to varying
levels of deviation in complex spatio-temporal scenarios. Our code and datasets
are available at https://github.com/Jimmy-7664/ST-SSDL.

</details>


### [651] [Glocal Information Bottleneck for Time Series Imputation](https://arxiv.org/abs/2510.04910)
*Jie Yang,Kexin Zhang,Guibin Zhang,Philip S. Yu,Kaize Ding*

Main category: cs.LG

TL;DR: The paper introduces Glocal Information Bottleneck (Glocal-IB), a model-agnostic framework for Time Series Imputation (TSI) to handle high missing rates by using global alignment of latent representations.


<details>
  <summary>Details</summary>
Motivation: Existing TSI models fail under high missing rates due to overfitting on local noise and missing global data guidance in optimization objectives.

Method: Glocal-IB integrates a Global Alignment loss into the standard Information Bottleneck framework to align latent representations of masked and observed inputs.

Result: Experiments on nine datasets demonstrate that Glocal-IB improves imputation accuracy and better aligns latent representations, outperforming existing methods under high missingness.

Conclusion: The Glocal-IB framework effectively bridges the gap between local details and global structure, enabling improved generalization and robustness in TSI tasks.

Abstract: Time Series Imputation (TSI), which aims to recover missing values in
temporal data, remains a fundamental challenge due to the complex and often
high-rate missingness in real-world scenarios. Existing models typically
optimize the point-wise reconstruction loss, focusing on recovering numerical
values (local information). However, we observe that under high missing rates,
these models still perform well in the training phase yet produce poor
imputations and distorted latent representation distributions (global
information) in the inference phase. This reveals a critical optimization
dilemma: current objectives lack global guidance, leading models to overfit
local noise and fail to capture global information of the data. To address this
issue, we propose a new training paradigm, Glocal Information Bottleneck
(Glocal-IB). Glocal-IB is model-agnostic and extends the standard IB framework
by introducing a Global Alignment loss, derived from a tractable mutual
information approximation. This loss aligns the latent representations of
masked inputs with those of their originally observed counterparts. It helps
the model retain global structure and local details while suppressing noise
caused by missing values, giving rise to better generalization under high
missingness. Extensive experiments on nine datasets confirm that Glocal-IB
leads to consistently improved performance and aligned latent representations
under missingness. Our code implementation is available in
https://github.com/Muyiiiii/NeurIPS-25-Glocal-IB.

</details>


### [652] [Federated Self-Supervised Learning for Automatic Modulation Classification under Non-IID and Class-Imbalanced Data](https://arxiv.org/abs/2510.04927)
*Usman Akram,Yiyue Chen,Haris Vikalo*

Main category: cs.LG

TL;DR: The paper introduces FedSSL-AMC, a federated learning approach that uses self-supervision and SVMs to tackle challenges in automatic modulation classification (AMC) under non-IID and privacy-constrained settings.


<details>
  <summary>Details</summary>
Motivation: Current AMC models relying on centrally aggregated data face privacy issues, communication burdens, and fail to handle channel variability well. Federated learning, while promising, struggles with class imbalance, non-IID distributions, and small labeled datasets.

Method: The authors propose FedSSL-AMC, which employs a time-dilated CNN with triplet-loss-based self-supervised learning on unlabeled data across clients in federated settings. The model trains lightweight SVMs on small labeled datasets per client and demonstrates convergence of representation learning.

Result: Experimental results on both synthetic and real-world datasets show that FedSSL-AMC achieves superior performance compared to traditional supervised federated learning approaches, especially under challenging SNR conditions, frequency offsets, and label partition scenarios.

Conclusion: FedSSL-AMC addresses key challenges in federated learning for AMC by leveraging self-supervised learning and achieves robust performance in diverse and noisy conditions without centralized data aggregation.

Abstract: Training automatic modulation classification (AMC) models on centrally
aggregated data raises privacy concerns, incurs communication overhead, and
often fails to confer robustness to channel shifts. Federated learning (FL)
avoids central aggregation by training on distributed clients but remains
sensitive to class imbalance, non-IID client distributions, and limited labeled
samples. We propose FedSSL-AMC, which trains a causal, time-dilated CNN with
triplet-loss self-supervision on unlabeled I/Q sequences across clients,
followed by per-client SVMs on small labeled sets. We establish convergence of
the federated representation learning procedure and a separability guarantee
for the downstream classifier under feature noise. Experiments on synthetic and
over-the-air datasets show consistent gains over supervised FL baselines under
heterogeneous SNR, carrier-frequency offsets, and non-IID label partitions.

</details>


### [653] [Egalitarian Gradient Descent: A Simple Approach to Accelerated Grokking](https://arxiv.org/abs/2510.04930)
*Ali Saheb Pasand,Elvis Dohmatob*

Main category: cs.LG

TL;DR: Grokking is when generalization performance of models suddenly improves after prolonged stagnation during training. This paper introduces Egalitarian Gradient Descent (EGD) to speed up grokking by normalizing gradient dynamics.


<details>
  <summary>Details</summary>
Motivation: Understanding and reducing the stagnation phase (plateaus) before grokking happens is vital for improving model training efficiency.

Method: The authors propose Egalitarian Gradient Descent (EGD), a gradient normalization technique that ensures all principal gradient directions evolve at the same speed, facilitating faster grokking.

Result: Using EGD, the stagnation phase is shortened or entirely removed in problems like modular addition and sparse parity, leading to faster grokking.

Conclusion: EGD effectively accelerates grokking, improving training efficiency without the prolonged stagnation associated with traditional methods.

Abstract: Grokking is the phenomenon whereby, unlike the training performance, which
peaks early in the training process, the test/generalization performance of a
model stagnates over arbitrarily many epochs and then suddenly jumps to usually
close to perfect levels. In practice, it is desirable to reduce the length of
such plateaus, that is to make the learning process "grok" faster. In this
work, we provide new insights into grokking. First, we show both empirically
and theoretically that grokking can be induced by asymmetric speeds of
(stochastic) gradient descent, along different principal (i.e singular
directions) of the gradients. We then propose a simple modification that
normalizes the gradients so that dynamics along all the principal directions
evolves at exactly the same speed. Then, we establish that this modified
method, which we call egalitarian gradient descent (EGD) and can be seen as a
carefully modified form of natural gradient descent, groks much faster. In
fact, in some cases the stagnation is completely removed. Finally, we
empirically show that on classical arithmetic problems such as modular addition
and sparse parity problem which this stagnation has been widely observed and
intensively studied, that our proposed method eliminates the plateaus.

</details>


### [654] [Feasibility-Aware Decision-Focused Learning for Predicting Parameters in the Constraints](https://arxiv.org/abs/2510.04951)
*Jayanta Mandi,Marianne Defresne,Senne Berden,Tias Guns*

Main category: cs.LG

TL;DR: This paper proposes a decision-focused learning framework for predicting constraint parameters in constrained optimization problems, addressing both decision quality and feasibility issues.


<details>
  <summary>Details</summary>
Motivation: The motivation is to handle cases where predicted parameters in the constraints of a constrained optimization problem lead to infeasible solutions and to improve decision quality through better prediction methods.

Method: The paper introduces two loss functions based on maximum likelihood estimation—one penalizing infeasibility and the other penalizing suboptimal decisions. A tunable parameter balances these losses, enabling control over feasibility and suboptimality trade-offs. The approach is generic and does not assume the optimization problem is a linear or integer linear program.

Result: The experimental results show that varying the tunable parameter allows decision-makers to control the trade-off between suboptimality and feasibility. For one specific parameter value, the proposed method performs comparably to existing baselines on multiple constrained optimization problem instances.

Conclusion: The proposed framework offers a flexible and effective way to address the trade-off between feasibility and decision quality in constrained optimization problems, generalizing beyond linear program assumptions.

Abstract: When some parameters of a constrained optimization problem (COP) are
uncertain, this gives rise to a predict-then-optimize (PtO) problem, comprising
two stages -- the prediction of the unknown parameters from contextual
information and the subsequent optimization using those predicted parameters.
Decision-focused learning (DFL) implements the first stage by training a
machine learning (ML) model to optimize the quality of the decisions made using
the predicted parameters. When parameters in the constraints of a COP are
predicted, the predicted parameters can lead to infeasible solutions.
Therefore, it is important to simultaneously manage both feasibility and
decision quality. We develop a DFL framework for predicting constraint
parameters in a generic COP. While prior works typically assume that the
underlying optimization problem is a linear program (LP) or integer linear
program (ILP), our approach makes no such assumption. We derive two novel loss
functions based on maximum likelihood estimation (MLE): the first one penalizes
infeasibility (by penalizing when the predicted parameters lead to infeasible
solutions), and the second one penalizes suboptimal decisions (by penalizing
when the true optimal solution is infeasible under the predicted parameters).
We introduce a single tunable parameter to form a weighted average of the two
losses, allowing decision-makers to balance suboptimality and feasibility. We
experimentally demonstrate that adjusting this parameter provides a
decision-maker the control over the trade-off between the two. Moreover, across
several COP instances, we find that for a single value of the tunable
parameter, our method matches the performance of the existing baselines on
suboptimality and feasibility.

</details>


### [655] [StructuralDecompose: A Modular Framework for Robust Time Series Decomposition in R](https://arxiv.org/abs/2510.04974)
*Allen Daniel Sunny*

Main category: cs.LG

TL;DR: StructuralDecompose is an R package designed for modular and interpretable time series decomposition, offering flexibility by separating processes like changepoint detection, anomaly detection, smoothing, and decomposition.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to address limitations in existing time series decomposition methods that are treated as monolithic by providing a more modular and flexible approach.

Method: StructuralDecompose introduces an R package that breaks down time series decomposition into distinct processes such as changepoint and anomaly detection, allowing users to adapt the methods to specific datasets. Performance is then assessed through simulations, real-world applications, and comparison with tools like Rbeast and autostsm.

Result: The package demonstrates robust performance and versatility in both simulated and real-world datasets, benchmarking favorably against state-of-the-art tools.

Conclusion: StructuralDecompose enhances interpretability and adaptability in time series analysis workflows, making it a valuable tool in both research and practical applications.

Abstract: We present StructuralDecompose, an R package for modular and interpretable
time series decomposition. Unlike existing approaches that treat decomposition
as a monolithic process, StructuralDecompose separates the analysis into
distinct components: changepoint detection, anomaly detection, smoothing, and
decomposition. This design provides flexibility and robust- ness, allowing
users to tailor methods to specific time series characteristics. We demonstrate
the package on simulated and real-world datasets, benchmark its performance
against state-of-the- art tools such as Rbeast and autostsm, and discuss its
role in interpretable machine learning workflows.

</details>


### [656] [Federated Computation of ROC and PR Curves](https://arxiv.org/abs/2510.04979)
*Xuefeng Xu,Graham Cormode*

Main category: cs.LG

TL;DR: This paper introduces a method for approximating ROC and PR curves in federated learning setups while maintaining privacy and low communication costs.


<details>
  <summary>Details</summary>
Motivation: Federated Learning scenarios face challenges in evaluating classifiers due to privacy and communication constraints preventing centralized computation of ROC and PR curves.

Method: A novel method is proposed to estimate quantiles of the prediction score distribution using distributed differential privacy to approximate the ROC and PR curves.

Result: The method achieves high approximation accuracy, strong privacy guarantees, and minimizes communication costs in federated system evaluations.

Conclusion: This approach is effective for privacy-preserving model evaluation in federated learning environments by balancing approximation accuracy, privacy, and communication efficiency.

Abstract: Receiver Operating Characteristic (ROC) and Precision-Recall (PR) curves are
fundamental tools for evaluating machine learning classifiers, offering
detailed insights into the trade-offs between true positive rate vs. false
positive rate (ROC) or precision vs. recall (PR). However, in Federated
Learning (FL) scenarios, where data is distributed across multiple clients,
computing these curves is challenging due to privacy and communication
constraints. Specifically, the server cannot access raw prediction scores and
class labels, which are used to compute the ROC and PR curves in a centralized
setting. In this paper, we propose a novel method for approximating ROC and PR
curves in a federated setting by estimating quantiles of the prediction score
distribution under distributed differential privacy. We provide theoretical
bounds on the Area Error (AE) between the true and estimated curves,
demonstrating the trade-offs between approximation accuracy, privacy, and
communication cost. Empirical results on real-world datasets demonstrate that
our method achieves high approximation accuracy with minimal communication and
strong privacy guarantees, making it practical for privacy-preserving model
evaluation in federated systems.

</details>


### [657] [Adaptive Memory Momentum via a Model-Based Framework for Deep Learning Optimization](https://arxiv.org/abs/2510.04988)
*Kristi Topollai,Anna Choromanska*

Main category: cs.LG

TL;DR: The paper proposes an adaptive memory mechanism for momentum-based optimization that dynamically adjusts the momentum coefficient during training, showing improved performance over standard methods.


<details>
  <summary>Details</summary>
Motivation: Most optimizers in deep learning rely on a fixed momentum coefficient set to 0.9, which might not be optimal for all scenarios.

Method: The authors develop a proximal framework that uses two planes derived from current and past gradients to dynamically adjust the momentum coefficient during optimization.

Result: Adaptive memory variants of SGD and AdamW outperform standard versions with manually tuned momentum coefficients across diverse tasks.

Conclusion: The adaptive memory mechanism is novel, simple, and effective, paving the way for future innovations in optimization adaptivity.

Abstract: The vast majority of modern deep learning models are trained with
momentum-based first-order optimizers. The momentum term governs the
optimizer's memory by determining how much each past gradient contributes to
the current convergence direction. Fundamental momentum methods, such as
Nesterov Accelerated Gradient and the Heavy Ball method, as well as more recent
optimizers such as AdamW and Lion, all rely on the momentum coefficient that is
customarily set to $\beta = 0.9$ and kept constant during model training, a
strategy widely used by practitioners, yet suboptimal. In this paper, we
introduce an \textit{adaptive memory} mechanism that replaces constant momentum
with a dynamic momentum coefficient that is adjusted online during
optimization. We derive our method by approximating the objective function
using two planes: one derived from the gradient at the current iterate and the
other obtained from the accumulated memory of the past gradients. To the best
of our knowledge, such a proximal framework was never used for momentum-based
optimization. Our proposed approach is novel, extremely simple to use, and does
not rely on extra assumptions or hyperparameter tuning. We implement adaptive
memory variants of both SGD and AdamW across a wide range of learning tasks,
from simple convex problems to large-scale deep learning scenarios,
demonstrating that our approach can outperform standard SGD and Adam with
hand-tuned momentum coefficients. Finally, our work opens doors for new ways of
inducing adaptivity in optimization.

</details>


### [658] [Power Transform Revisited: Numerically Stable, and Federated](https://arxiv.org/abs/2510.04995)
*Xuefeng Xu,Graham Cormode*

Main category: cs.LG

TL;DR: This paper addresses numerical instability issues in power transforms and introduces robust solutions, extending them to federated learning, with experimentally validated improvements.


<details>
  <summary>Details</summary>
Motivation: Power transforms are widely utilized for preprocessing in statistical and machine learning tasks. However, their direct implementation suffers from severe numerical instabilities, underscoring the need for stable and effective solutions.

Method: The authors analyze the sources of numerical instabilities in power transforms, propose remedies, and adapt these solutions to address specific challenges in federated learning scenarios.

Result: Experiments on real-world datasets confirm the effectiveness and stability of the proposed methods, showing significant improvements over existing approaches.

Conclusion: The study mitigates key challenges in power transforms, providing robust solutions that enhance their applicability and reliability, particularly in federated learning contexts.

Abstract: Power transforms are popular parametric techniques for making data more
Gaussian-like, and are widely used as preprocessing steps in statistical
analysis and machine learning. However, we find that direct implementations of
power transforms suffer from severe numerical instabilities, which can lead to
incorrect results or even crashes. In this paper, we provide a comprehensive
analysis of the sources of these instabilities and propose effective remedies.
We further extend power transforms to the federated learning setting,
addressing both numerical and distributional challenges that arise in this
context. Experiments on real-world datasets demonstrate that our methods are
both effective and robust, substantially improving stability compared to
existing approaches.

</details>


### [659] [Inoculation Prompting: Instructing LLMs to misbehave at train-time improves test-time alignment](https://arxiv.org/abs/2510.05024)
*Nevan Wichers,Aram Ebtekar,Ariana Azarbal,Victor Gillioz,Christine Ye,Emil Ryd,Neil Rathi,Henry Sleight,Alex Mallen,Fabien Roger,Samuel Marks*

Main category: cs.LG

TL;DR: The paper introduces Inoculation Prompting (IP), a method to prevent undesired behaviors in language models, such as reward hacking, by modifying training prompts to explicitly request the undesired behavior during fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Standard training methods for language models are flawed due to imperfect oversight signals, which can result in undesired behaviors like reward hacking and sycophancy. There is a need for techniques that mitigate these behaviors without requiring expensive or infeasible improvements to oversight.

Method: Inoculation Prompting (IP) involves deliberately modifying training prompts during supervised fine-tuning to explicitly request undesired behaviors, thereby preventing the model from developing those behaviors. The researchers also evaluate the effectiveness of using prompts that strongly elicit undesired behaviors as a heuristic to improve inoculation.

Result: The authors demonstrate across four experimental settings that IP reduces undesired behaviors while maintaining the model's desired capabilities. Furthermore, prompts that strongly elicit undesired behaviors tend to lead to more effective inoculation when employed during training.

Conclusion: Inoculation Prompting is a simple and effective approach to enhance the training of language models. It controls the generalization of learned behaviors, reducing undesired behaviors without compromising desired capabilities.

Abstract: Large language models are sometimes trained with imperfect oversight signals,
leading to undesired behaviors such as reward hacking and sycophancy. Improving
oversight quality can be expensive or infeasible, motivating methods that
improve learned behavior despite an imperfect training signal. We introduce
Inoculation Prompting (IP), a simple but counterintuitive technique that
prevents learning of an undesired behavior by modifying training prompts to
explicitly request it. For example, to inoculate against reward hacking, we
modify the prompts used in supervised fine-tuning to request code that only
works on provided test cases but fails on other inputs. Across four settings we
find that IP reduces the learning of undesired behavior without substantially
reducing the learning of desired capabilities. We also show that prompts which
more strongly elicit the undesired behavior prior to fine-tuning more
effectively inoculate against the behavior when used during training; this
serves as a heuristic to identify promising inoculation prompts. Overall, IP is
a simple yet effective way to control how models generalize from fine-tuning,
preventing learning of undesired behaviors without substantially disrupting
desired capabilities.

</details>


### [660] [Graph-Aware Diffusion for Signal Generation](https://arxiv.org/abs/2510.05036)
*Sergio Rozada,Vimal K. B.,Andrea Cavallo,Antonio G. Marques,Hadi Jamali-Rad,Elvin Isufi*

Main category: cs.LG

TL;DR: The paper proposes a method, Graph-Aware Generative Diffusion model (GAD), leveraging graph-based generative diffusion processes for generating graph signals, with strong theoretical and practical benefits.


<details>
  <summary>Details</summary>
Motivation: There is a need for generating graph signals from unknown distributions for applications like recommender systems or sensor networks. Current methods lack generality in account for graph structures during the forward process.

Method: The authors propose incorporating graph structure using the heat equation in the forward process, applying a time-warped coefficient to address exponential decay in drift terms, leading to the proposed GAD model.

Result: The GAD model demonstrates convergence to Gaussian Markov Random Fields with graph Laplacians defining covariance. Through graph-signal denoising processes, its advantages are proven on synthetic data and practical datasets such as traffic speed and temperature sensor data.

Conclusion: Introducing GAD improves graph signal generation capabilities with theoretical justifications and superior performance validated through various datasets and applications.

Abstract: We study the problem of generating graph signals from unknown distributions
defined over given graphs, relevant to domains such as recommender systems or
sensor networks. Our approach builds on generative diffusion models, which are
well established in vision and graph generation but remain underexplored for
graph signals. Existing methods lack generality, either ignoring the graph
structure in the forward process or designing graph-aware mechanisms tailored
to specific domains. We adopt a forward process that incorporates the graph
through the heat equation. Rather than relying on the standard formulation, we
consider a time-warped coefficient to mitigate the exponential decay of the
drift term, yielding a graph-aware generative diffusion model (GAD). We analyze
its forward dynamics, proving convergence to a Gaussian Markov random field
with covariance parametrized by the graph Laplacian, and interpret the backward
dynamics as a sequence of graph-signal denoising problems. Finally, we
demonstrate the advantages of GAD on synthetic data, real traffic speed
measurements, and a temperature sensor network.

</details>


### [661] [Test-Time Scaling in Diffusion LLMs via Hidden Semi-Autoregressive Experts](https://arxiv.org/abs/2510.05040)
*Jihoon Lee,Hoyeon Moon,Kevin Zhai,Arun Kumar Chithanar,Anit Kumar Sahu,Soummya Kar,Chul Lee,Souradip Chakraborty,Amrit Singh Bedi*

Main category: cs.LG

TL;DR: This paper introduces HEX, a training-free inference method for diffusion-based large language models (dLLMs), which ensembles across heterogeneous generation paths, leading to significant performance improvements on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Diffusion-based large language models (dLLMs) implicitly learn a mixture of semi-autoregressive experts, yet common inference practices fail to leverage this latent ensemble. This limitation motivates the need for a better inference strategy.

Method: The paper proposes HEX, a method that avoids committing to a single fixed inference schedule. Instead, it performs a majority vote over diverse generation paths, leveraging the hidden semi-autoregressive experts within dLLMs.

Result: HEX improves performance significantly across various benchmarks: GSM8K (from 24.72% to 88.10%), MATH (from 16.40% to 40.00%), ARC-C (from 54.18% to 87.80%), and TruthfulQA (from 28.36% to 57.46%).

Conclusion: The study demonstrates that the choice of masking sequence during inference is crucial, and using HEX for test-time scaling unlocks the latent potential of dLLMs without additional training. This approach redefines test-time scaling for such models.

Abstract: Diffusion-based large language models (dLLMs) are trained flexibly to model
extreme dependence in the data distribution; however, how to best utilize this
information at inference time remains an open problem. In this work, we uncover
an interesting property of these models: dLLMs trained on textual data
implicitly learn a mixture of semi-autoregressive experts, where different
generation orders reveal different specialized behaviors. We show that
committing to any single, fixed inference time schedule, a common practice,
collapses performance by failing to leverage this latent ensemble. To address
this, we introduce HEX (Hidden semiautoregressive EXperts for test-time
scaling), a training-free inference method that ensembles across heterogeneous
block schedules. By doing a majority vote over diverse block-sized generation
paths, HEX robustly avoids failure modes associated with any single fixed
schedule. On reasoning benchmarks such as GSM8K, it boosts accuracy by up to
3.56X (from 24.72% to 88.10%), outperforming top-K margin inference and
specialized fine-tuned methods like GRPO, without additional training. HEX even
yields significant gains on MATH benchmark from 16.40% to 40.00%, scientific
reasoning on ARC-C from 54.18% to 87.80%, and TruthfulQA from 28.36% to 57.46%.
Our results establish a new paradigm for test-time scaling in diffusion-based
LLMs (dLLMs), revealing that the sequence in which masking is performed plays a
critical role in determining performance during inference.

</details>


### [662] [KEEP: Integrating Medical Ontologies with Clinical Data for Robust Code Embeddings](https://arxiv.org/abs/2510.05049)
*Ahmed Elhussein,Paul Meddeb,Abigail Newbury,Jeanne Mirone,Martin Stoll,Gamze Gursoy*

Main category: cs.LG

TL;DR: KEEP combines knowledge graph embeddings with adaptive learning to bridge the gap between structured medical codes and their real-world patterns, yielding improved representations for healthcare applications.


<details>
  <summary>Details</summary>
Motivation: Current methods used for structured medical code representation in healthcare either fail to capture real-world patterns or overlook structured knowledge, creating a trade-off.

Method: KEEP generates embeddings from knowledge graphs and refines them using regularized training on clinical data to integrate empirical patterns while preserving structured ontological relationships.

Result: Evaluations on datasets like UK Biobank and MIMIC IV show that KEEP outperforms existing approaches in capturing semantic relationships and predicting clinical outcomes.

Conclusion: KEEP is computationally efficient, supports diverse downstream applications without task-specific training, and is highly suitable for resource-constrained environments.

Abstract: Machine learning in healthcare requires effective representation of
structured medical codes, but current methods face a trade off: knowledge graph
based approaches capture formal relationships but miss real world patterns,
while data driven methods learn empirical associations but often overlook
structured knowledge in medical terminologies. We present KEEP (Knowledge
preserving and Empirically refined Embedding Process), an efficient framework
that bridges this gap by combining knowledge graph embeddings with adaptive
learning from clinical data. KEEP first generates embeddings from knowledge
graphs, then employs regularized training on patient records to adaptively
integrate empirical patterns while preserving ontological relationships.
Importantly, KEEP produces final embeddings without task specific auxiliary or
end to end training enabling KEEP to support multiple downstream applications
and model architectures. Evaluations on structured EHR from UK Biobank and
MIMIC IV demonstrate that KEEP outperforms both traditional and Language Model
based approaches in capturing semantic relationships and predicting clinical
outcomes. Moreover, KEEP's minimal computational requirements make it
particularly suitable for resource constrained environments.

</details>


### [663] [HybridFlow: Quantification of Aleatoric and Epistemic Uncertainty with a Single Hybrid Model](https://arxiv.org/abs/2510.05054)
*Peter Van Katwyk,Karianne J. Bergen*

Main category: cs.LG

TL;DR: The paper introduces HybridFlow, a framework that unifies modeling of aleatoric and epistemic uncertainties for improved robustness in machine learning.


<details>
  <summary>Details</summary>
Motivation: To address the need for robust uncertainty quantification in high-stakes machine learning applications.

Method: HybridFlow combines a Conditional Masked Autoregressive flow to model aleatoric uncertainty and a flexible probabilistic predictor to handle epistemic uncertainty.

Result: HybridFlow demonstrated improved uncertainty quantification over previous methods in tasks like depth estimation. Its uncertainty measures were better calibrated and aligned with model errors.

Conclusion: HybridFlow successfully integrates aleatoric and epistemic uncertainty modeling into one robust framework, enhancing Bayesian deep learning's reliability.

Abstract: Uncertainty quantification is critical for ensuring robustness in high-stakes
machine learning applications. We introduce HybridFlow, a modular hybrid
architecture that unifies the modeling of aleatoric and epistemic uncertainty
by combining a Conditional Masked Autoregressive normalizing flow for
estimating aleatoric uncertainty with a flexible probabilistic predictor for
epistemic uncertainty. The framework supports integration with any
probabilistic model class, allowing users to easily adapt HybridFlow to
existing architectures without sacrificing predictive performance. HybridFlow
improves upon previous uncertainty quantification frameworks across a range of
regression tasks, such as depth estimation, a collection of regression
benchmarks, and a scientific case study of ice sheet emulation. We also provide
empirical results of the quantified uncertainty, showing that the uncertainty
quantified by HybridFlow is calibrated and better aligns with model error than
existing methods for quantifying aleatoric and epistemic uncertainty.
HybridFlow addresses a key challenge in Bayesian deep learning, unifying
aleatoric and epistemic uncertainty modeling in a single robust framework.

</details>


### [664] [Modeling Student Learning with 3.8 Million Program Traces](https://arxiv.org/abs/2510.05056)
*Alexis Ross,Megha Srivastava,Jeremiah Blanchard,Jacob Andreas*

Main category: cs.LG

TL;DR: The paper investigates the value of editing traces in programming to improve the understanding of coders' reasoning and enhance model predictions of student behavior.


<details>
  <summary>Details</summary>
Motivation: To understand how interaction traces during programming reflect reasoning processes and skill levels of novice programmers.

Method: The authors use a dataset of 3.8 million programming reasoning traces from the educational platform Pencil Code, training language models on these traces to analyze coder behavior and assist students.

Result: Models trained on real programming traces exhibit stronger abilities in modeling student behavior and are able to predict code trace properties. These models also help steer code edits closer to correct solutions while retaining individual coding style.

Conclusion: Interaction traces are significant for understanding individual student behavior and can lead to steerable and predictive programming models, aiding novice programmers effectively.

Abstract: As programmers write code, they often edit and retry multiple times, creating
rich "interaction traces" that reveal how they approach coding tasks and
provide clues about their level of skill development. For novice programmers in
particular, these traces reflect the diverse reasoning processes they employ to
code, such as exploratory behavior to understand how a programming concept
works, re-strategizing in response to bugs, and personalizing stylistic
choices. In this work, we explore what can be learned from training language
models on such reasoning traces: not just about code, but about coders, and
particularly students learning to program. We introduce a dataset of over 3.8
million programming reasoning traces from users of Pencil Code, a free online
educational platform used by students to learn simple programming concepts.
Compared to models trained only on final programs or synthetically-generated
traces, we find that models trained on real traces are stronger at modeling
diverse student behavior. Through both behavioral and probing analyses, we also
find that many properties of code traces, such as goal backtracking or number
of comments, can be predicted from learned representations of the students who
write them. Building on this result, we show that we can help students recover
from mistakes by steering code generation models to identify a sequence of
edits that will results in more correct code while remaining close to the
original student's style. Together, our results suggest that many properties of
code are properties of individual students and that training on edit traces can
lead to models that are more steerable, more predictive of student behavior
while programming, and better at generating programs in their final states.
Code and data is available at https://github.com/meghabyte/pencilcode-public

</details>


### [665] [Boomerang Distillation Enables Zero-Shot Model Size Interpolation](https://arxiv.org/abs/2510.05064)
*Sara Kangaslahti,Nihal V. Nayak,Jonathan Geuter,Marco Fumero,Francesco Locatello,David Alvarez-Melis*

Main category: cs.LG

TL;DR: They propose a novel and efficient method, 'boomerang distillation,' to create flexible intermediate-sized language models without extensive retraining.


<details>
  <summary>Details</summary>
Motivation: LLMs often face deployment challenges due to diverse compute and memory constraints, and existing methods for scaling models are expensive and offer limited flexibility.

Method: The authors distill a large teacher model into a small student and reconstruct intermediate-sized models by incorporating teacher layers into the student without additional training.

Result: The interpolated models exhibit smooth performance scaling between student and teacher sizes, often matching or exceeding comparable models built with traditional methods.

Conclusion: Boomerang distillation simplifies the generation of model families, cuts training costs, and enables better adaptability to varying deployment needs.

Abstract: Large language models (LLMs) are typically deployed under diverse memory and
compute constraints. Existing approaches build model families by training each
size independently, which is prohibitively expensive and provides only
coarse-grained size options. In this work, we identify a novel phenomenon that
we call boomerang distillation: starting from a large base model (the teacher),
one first distills down to a small student and then progressively reconstructs
intermediate-sized models by re-incorporating blocks of teacher layers into the
student without any additional training. This process produces zero-shot
interpolated models of many intermediate sizes whose performance scales
smoothly between the student and teacher, often matching or surpassing
pretrained or distilled models of the same size. We further analyze when this
type of interpolation succeeds, showing that alignment between teacher and
student through pruning and distillation is essential. Boomerang distillation
thus provides a simple and efficient way to generate fine-grained model
families, dramatically reducing training cost while enabling flexible
adaptation across deployment environments. The code and models are available at
https://github.com/dcml-lab/boomerang-distillation.

</details>


### [666] [MICROTRIPS: MICRO-geography TRavel Intelligence and Pattern Synthesis](https://arxiv.org/abs/2510.05080)
*Yangyang Wang,Tayo Fabusuyi*

Main category: cs.LG

TL;DR: This paper introduces a novel framework utilizing machine learning and publicly available microdata for high-resolution travel behavior estimation to enhance urban transportation planning.


<details>
  <summary>Details</summary>
Motivation: The purpose of this paper is to improve urban transportation planning by addressing the limitations of the traditional four-step travel model, aiming for more detailed insights into travel behavior at small geographic scales.

Method: The study leverages machine learning techniques and publicly available microdata files to create a synthetic population for predicting travel behavior. This includes estimating key areas like trip generation, trip distribution, mode choice, and route assignment.

Result: Validation with datasets like ACS/PUMS work-commute data shows that the proposed framework offers higher accuracy than traditional methods in characterizing travel behavior.

Conclusion: The framework provides granular insights that support localized transportation interventions, benefiting policy applications such as micro-fulfillment center placement, curb-space management, and designing inclusive solutions for vulnerable communities.

Abstract: This study presents a novel small-area estimation framework to enhance urban
transportation planning through detailed characterization of travel behavior.
Our approach improves on the four-step travel model by employing publicly
available microdata files and machine learning methods to predict travel
behavior for a representative, synthetic population at small geographic areas.
This approach enables high-resolution estimation of trip generation, trip
distribution, mode choice, and route assignment. Validation using ACS/PUMS
work-commute datasets demonstrates that our framework achieves higher accuracy
compared to conventional approaches. The resulting granular insights enable the
tailoring of interventions to address localized situations and support a range
of policy applications and targeted interventions, including the optimal
placement of micro-fulfillment centers, effective curb-space management, and
the design of more inclusive transportation solutions particularly for
vulnerable communities.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [667] [Efficient Training of Spiking Neural Networks by Spike-aware Data Pruning](https://arxiv.org/abs/2510.04098)
*Chenxiang Ma,Xinyi Chen,Yujie Wu,Kay Chen Tan,Jibin Wu*

Main category: cs.NE

TL;DR: This paper introduces a novel spike-aware data pruning (SADP) method for spiking neural networks (SNNs), reducing training time while preserving accuracy.


<details>
  <summary>Details</summary>
Motivation: Scaling SNNs for better energy efficiency faces challenges due to high training overhead, especially for researchers with limited computational resources.

Method: The authors proposed SADP, where data pruning is guided by a spike-aware importance score based on gradient norms to address gradient variance and computational cost.

Result: SADP achieves significant training speedups without compromising accuracy, notably reducing training time by 35% on ImageNet while maintaining performance.

Conclusion: This work validates SADP as a promising approach for efficient SNN training, setting a new paradigm for scaling SNNs to larger models and datasets.

Abstract: Spiking neural networks (SNNs), recognized as an energy-efficient alternative
to traditional artificial neural networks (ANNs), have advanced rapidly through
the scaling of models and datasets. However, such scaling incurs considerable
training overhead, posing challenges for researchers with limited computational
resources and hindering the sustained development of SNNs. Data pruning is a
promising strategy for accelerating training by retaining the most informative
examples and discarding redundant ones, but it remains largely unexplored in
SNNs. Directly applying ANN-based data pruning methods to SNNs fails to capture
the intrinsic importance of examples and suffers from high gradient variance.
To address these challenges, we propose a novel spike-aware data pruning (SADP)
method. SADP reduces gradient variance by determining each example's selection
probability to be proportional to its gradient norm, while avoiding the high
cost of direct gradient computation through an efficient upper bound, termed
spike-aware importance score. This score accounts for the influence of
all-or-nothing spikes on the gradient norm and can be computed with negligible
overhead. Extensive experiments across diverse datasets and architectures
demonstrate that SADP consistently outperforms data pruning baselines and
achieves training speedups close to the theoretical maxima at different pruning
ratios. Notably, SADP reduces training time by 35% on ImageNet while
maintaining accuracy comparable to that of full-data training. This work,
therefore, establishes a data-centric paradigm for efficient SNN training and
paves the way for scaling SNNs to larger models and datasets. The source code
will be released publicly after the review process.

</details>


### [668] [SpikingMamba: Towards Energy-Efficient Large Language Models via Knowledge Distillation from Mamba](https://arxiv.org/abs/2510.04595)
*Yulong Huang,Jianxiong Tang,Chao Wang,Ziyi Wang,Jianguo Zhang,Zhichao Lu,Bojun Cheng,Luziwei Leng*

Main category: cs.NE

TL;DR: This paper introduces SpikingMamba, an energy-efficient modification of large language models (LLMs) leveraging spiking neural networks (SNNs) to reduce energy costs while minimizing accuracy loss. It employs novel neuron design and training techniques for efficient deployment.


<details>
  <summary>Details</summary>
Motivation: LLMs are highly energy-intensive due to dense matrix operations, making them less practical for edge devices. SNNs promise greater energy efficiency but often come at the cost of performance, necessitating expensive retraining.

Method: SpikingMamba uses TI-LIF neurons with signed multi-level spike representations and a training-exclusive Smoothed Gradient Compensation (SGC) path to mitigate quantization loss. A single-stage distillation transfers Mamba's zero-shot capabilities, enhanced further through reinforcement learning.

Result: The method achieves a 4.76× energy improvement, with only a 4.78% accuracy tradeoff compared to the original Mamba model, and an additional 2.55% accuracy gain after reinforcement learning.

Conclusion: SpikingMamba demonstrates that integrating SNNs into LLMs can significantly enhance energy efficiency with minimal accuracy sacrifice, making it suitable for edge-device deployments.

Abstract: Large Language Models (LLMs) have achieved remarkable performance across
tasks but remain energy-intensive due to dense matrix operations. Spiking
neural networks (SNNs) improve energy efficiency by replacing dense matrix
multiplications with sparse accumulations. Their sparse spike activity enables
efficient LLMs deployment on edge devices. However, prior SNN-based LLMs often
sacrifice performance for efficiency, and recovering accuracy typically
requires full pretraining, which is costly and impractical. To address this, we
propose SpikingMamba, an energy-efficient SNN-based LLMs distilled from Mamba
that improves energy efficiency with minimal accuracy sacrifice. SpikingMamba
integrates two key components: (a) TI-LIF, a ternary-integer spiking neuron
that preserves semantic polarity through signed multi-level spike
representations. (b) A training-exclusive Smoothed Gradient Compensation (SGC)
path mitigating quantization loss while preserving spike-driven efficiency. We
employ a single-stage distillation strategy to transfer the zero-shot ability
of pretrained Mamba and further enhance it via reinforcement learning (RL).
Experiments show that SpikingMamba-1.3B achieves a 4.76$\times$ energy benefit,
with only a 4.78\% zero-shot accuracy gap compared to the original Mamba, and
achieves a further 2.55\% accuracy improvement after RL.

</details>


### [669] [What your brain activity says about you: A review of neuropsychiatric disorders identified in resting-state and sleep EEG data](https://arxiv.org/abs/2510.04984)
*J. E. M. Scanlon,A. Pelzer,M. Gharleghi,K. C. Fuhrmeister,T. Köllmer,P. Aichroth,R. Göder,C. Hansen,K. I. Wolf*

Main category: cs.NE

TL;DR: The paper examines the potential privacy risks of task-free EEG data by analyzing its ability to classify disorders and personal information, showing that it holds sensitive, identifiable information.


<details>
  <summary>Details</summary>
Motivation: The study aims to explore the types of personal and health information detectable in task-free EEG data and evaluate the privacy risks associated with freely available EEG datasets.

Method: A systematic review was conducted using Google Scholar, Web of Science, and journal searches for studies classifying medical disorders or personal information from resting-state and sleep EEG data. Only peer-reviewed English-language journal articles or conference papers that met pre-set quality criteria were included.

Result: The review found that resting-state EEG could classify various disorders like Autism Spectrum Disorder and Parkinson's disease with high accuracy using brief recordings, while sleep EEG was effective for identifying sleep disorders but required longer recordings or multiple stages. Current classification methods are rapidly advancing.

Conclusion: EEG data contains sensitive health information that is vulnerable to re-identification with advanced machine learning techniques, highlighting the urgent need for robust anonymization protocols to protect privacy.

Abstract: Electroencephalogram monitoring devices and online data repositories hold
large amounts of data from individuals participating in research and medical
studies without direct reference to personal identifiers. This paper explores
what types of personal and health information have been detected and classified
within task-free EEG data. Additionally, we investigate key characteristics of
the collected resting-state and sleep data, in order to determine the privacy
risks involved with openly available EEG data. We used Google Scholar, Web of
Science and searched relevant journals to find studies which classified or
detected the presence of various disorders and personal information in resting
state and sleep EEG. Only English full-text peer-reviewed journal articles or
conference papers about classifying the presence of medical disorders between
individuals were included. A quality analysis carried out by 3 reviewers
determined general paper quality based on specified evaluation criteria. In
resting state EEG, various disorders including Autism Spectrum Disorder,
Parkinson's disease, and alcohol use disorder have been classified with high
classification accuracy, often requiring only 5 mins of data or less. Sleep EEG
tends to hold classifiable information about sleep disorders such as sleep
apnea, insomnia, and REM sleep disorder, but usually involve longer recordings
or data from multiple sleep stages. Many classification methods are still
developing but even today, access to a person's EEG can reveal sensitive
personal health information. With an increasing ability of machine learning
methods to re-identify individuals from their EEG data, this review
demonstrates the importance of anonymization, and the development of improved
tools for keeping study participants and medical EEG users' privacy safe.

</details>


### [670] [Exploration-Exploitation-Evaluation (EEE): A Framework for Metaheuristic Algorithms in Combinatorial Optimization](https://arxiv.org/abs/2510.05027)
*Ethan Davis*

Main category: cs.NE

TL;DR: This paper introduces a framework for applying metaheuristic algorithms to combinatorial optimization problems, focusing on ant colony optimization (ACO) applied to the traveling salesman problem.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance the effectiveness of metaheuristic algorithms like ACO in solving complex combinatorial optimization problems, such as the traveling salesman problem, by addressing parameter tuning and result reliability.

Method: The framework includes three phases: broad exploration of parameters, exploitation of effective parameters, and uncertainty quantification. ACO is applied to a standard traveling salesman problem dataset (berlin52) as a case study.

Result: Using the proposed framework, ACO's probability of finding the global optimum for the berlin52 dataset is estimated at 1/40 in a single run, improving to 1/5 across 10 runs.

Conclusion: The framework improves the reliability and effectiveness of ACO in solving combinatorial optimization problems and quantifies uncertainty in the results.

Abstract: We introduce a framework for applying metaheuristic algorithms, such as ant
colony optimization (ACO), to combinatorial optimization problems (COPs) like
the traveling salesman problem (TSP). The framework consists of three
sequential stages: broad exploration of the parameter space, exploitation of
top-performing parameters, and uncertainty quantification (UQ) to assess the
reliability of results. As a case study, we apply ACO to the TSPLIB berlin52
dataset, which has a known optimal tour length of 7542. Using our framework, we
calculate that the probability of ACO finding the global optimum is
approximately 1/40 in a single run and improves to 1/5 when aggregated over ten
runs.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [671] [Formal Analysis of Metastable Failures in Software Systems](https://arxiv.org/abs/2510.03551)
*Rebecca Isaacs,Peter Alvaro,Rupak Majumdar,Kiran-Kumar Muniswamy-Reddy,Mahmoud Salamati,Sadegh Soudjani*

Main category: cs.PF

TL;DR: This paper examines metastable failures in server systems, offering mathematical foundations and a modeling method using continuous-time Markov chains (CTMCs). It identifies system parameterizations causing metastability and provides tools for visualization and quantitative predictions.


<details>
  <summary>Details</summary>
Motivation: To address the catastrophic yet underexplored metastable failures in server systems that persist even after stressors are removed, leading to significant cloud system outages.

Method: The authors model server systems using a domain-specific language and approximate their behavior via CTMCs. They utilize these models for qualitative (visualization) and quantitative (e.g., recovery times) analysis to predict and understand metastable behaviors.

Result: The proposed methods successfully predict and visualize metastable behaviors, closely matching real-world observations. The analysis demonstrates increased recovery times in systems approaching metastable dynamics.

Conclusion: The research provides practical tools and insights for identifying, understanding, and predicting metastable behaviors in large-scale server systems, helping mitigate potential failures.

Abstract: Many large-scale software systems demonstrate metastable failures. In this
class of failures, a stressor such as a temporary spike in workload causes the
system performance to drop and, subsequently, the system performance continues
to remain low even when the stressor is removed. These failures have been
reported by many large corporations and considered to be a rare but
catastrophic source of availability outages in cloud systems.
  In this paper, we provide the mathematical foundations of metastability in
request-response server systems. We model such systems using a domain-specific
language. We show how to construct continuous-time Markov chains (CTMCs) that
approximate the semantics of the programs through modeling and data-driven
calibration. We use the structure of the CTMC models to provide a visualization
of the qualitative behavior of the model. The visualization is a surprisingly
effective way to identify system parameterizations that cause a system to show
metastable behaviors.
  We complement the qualitative analysis with quantitative predictions. We
provide a formal notion of metastable behaviors based on escape probabilities,
and show that metastable behaviors are related to the eigenvalue structure of
the CTMC. Our characterization leads to algorithmic tools to predict recovery
times in metastable models of server systems.
  We have implemented our technique in a tool for the modeling and analysis of
server systems. Through models inspired by failures in real request-response
systems, we show that our qualitative visual analysis captures and predicts
many instances of metastability that were observed in the field in a matter of
milliseconds. Our algorithms confirm that recovery times surge as the system
parameters approach metastable modes in the dynamics.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [672] [Retrofitting Control Flow Graphs in LLVM IR for Auto Vectorization](https://arxiv.org/abs/2510.04890)
*Shihan Fang,Wenxin Zheng*

Main category: cs.PL

TL;DR: This paper introduces a novel vectorization pipeline improving upon LLVM and GCC, achieving up to 58% performance improvement.


<details>
  <summary>Details</summary>
Motivation: To address limitations in current compilers like LLVM and GCC, which fail to fully exploit vectorization opportunities due to fragmented passes and limited extensibility.

Method: Developed two specialized IR extensions, namely SIR and VIR, to simplify control flow analysis and improve the identification of vectorization opportunities.

Result: The proposed pipeline achieves significant performance improvements, with speedups of up to 53% and 58% over LLVM and GCC, respectively.

Conclusion: The work demonstrates the potential of SIR and VIR in enhancing automatic vectorization, presenting a flexible and extensible alternative to existing compiler techniques.

Abstract: Modern processors increasingly rely on SIMD instruction sets, such as AVX and
RVV, to significantly enhance parallelism and computational performance.
However, production-ready compilers like LLVM and GCC often fail to fully
exploit available vectorization opportunities due to disjoint vectorization
passes and limited extensibility. Although recent attempts in heuristics and
intermediate representation (IR) designs have attempted to address these
problems, efficiently simplifying control flow analysis and accurately
identifying vectorization opportunities remain challenging tasks.
  To address these issues, we introduce a novel vectorization pipeline
featuring two specialized IR extensions: SIR, which encodes high-level
structural information, and VIR, which explicitly represents instruction
dependencies through data dependency analysis. Leveraging the detailed
dependency information provided by VIR, we develop a flexible and extensible
vectorization framework. This approach substantially improves interoperability
across vectorization passes and expands the search space for identifying
isomorphic instructions, ultimately enhancing both the scope and efficiency of
automatic vectorization. Experimental evaluations demonstrate that our proposed
vectorization pipeline achieves significant performance improvements,
delivering speedups of up to 53% and 58% compared to LLVM and GCC,
respectively.

</details>


### [673] [PLSEMANTICSBENCH: Large Language Models As Programming Language Interpreters](https://arxiv.org/abs/2510.03415)
*Aditya Thimmaiah,Jiyang Zhang,Jayanth Srinivasa,Junyi Jessy Li,Milos Gligoric*

Main category: cs.PL

TL;DR: This paper evaluates large language models (LLMs) for their ability to execute programs based on formal programming language semantics using evaluation sets and various semantic tasks.


<details>
  <summary>Details</summary>
Motivation: Explore the potential of LLMs acting as interpreters for programming languages by leveraging their reasoning abilities and rapid prototyping capabilities.

Method: The authors used a subset of C (IMP) and formal semantics (SOS and K-semantics), applied various program complexity metrics, and designed tasks like final-state prediction, semantic rule prediction, and execution trace prediction.

Result: LLMs perform well under standard semantics but struggle under nonstandard ones. They excel in coarse-grained tasks with high complexity but show mixed outcomes when dealing with formal semantics for complex programs.

Conclusion: While LLMs show promise as interpreters, their understanding of robust semantics is limited. Formal semantics may aid simpler programs but hinder more complex cases.

Abstract: As large language models (LLMs) excel at code reasoning, a natural question
arises: can an LLM execute programs (i.e., act as an interpreter) purely based
on a programming language's formal semantics? If so, it will enable rapid
prototyping of new programming languages and language features. We study this
question using the imperative language IMP (a subset of C), formalized via
small-step operational semantics (SOS) and rewriting-based operational
semantics (K-semantics). We introduce three evaluation sets-Human-Written,
LLM-Translated, and Fuzzer- Generated-whose difficulty is controlled by
code-complexity metrics spanning the size, control-flow, and data-flow axes.
Given a program and its semantics formalized with SOS/K-semantics, models are
evaluated on three tasks ranging from coarse to fine: (1) final-state
prediction, (2) semantic rule prediction, and (3) execution trace prediction.
To distinguish pretraining memorization from semantic competence, we define two
nonstandard semantics obtained through systematic mutations of the standard
rules. Across strong code/reasoning LLMs, performance drops under nonstandard
semantics despite high performance under the standard one. We further find that
(i) there are patterns to different model failures, (ii) most reasoning models
perform exceptionally well on coarse grained tasks involving reasoning about
highly complex programs often containing nested loop depths beyond five, and
surprisingly, (iii) providing formal semantics helps on simple programs but
often hurts on more complex ones. Overall, the results show a promise that LLMs
could serve as programming language interpreters, but points to the lack of
their robust semantics understanding. We release the benchmark and the
supporting code at https://github.com/EngineeringSoftware/PLSemanticsBench.

</details>


### [674] [Encoding Numeric Computations and Infusing Heuristic Knowledge Using Integrity Constraints in stableKanren](https://arxiv.org/abs/2510.04049)
*Xiangyu Guo,Ajay Bansal*

Main category: cs.PL

TL;DR: The paper introduces stableKanren, an extension of miniKanren, designed to enhance relational programming with numeric computations using integrity constraints, while improving problem-solving efficiency and ease of use.


<details>
  <summary>Details</summary>
Motivation: To overcome challenges in combining symbolic and numeric computations in relational programming for problem-solving, and improve the syntax and performance of such systems.

Method: Developing stableKanren, which uses a constraint store architecture for numeric computations while balancing symbolic computations, and exploring heuristic infusions to improve solver performance.

Result: stableKanren successfully integrates numeric computations in relational programming, achieving simpler syntax and improved numerics without grounding all numbers. Its application on the SEND+MORE=MONEY puzzle demonstrates significant performance improvement with heuristic knowledge infusion.

Conclusion: stableKanren offers a reliable, efficient, and simpler solution for numeric computations in relational programming, incorporating heuristic approaches for enhanced solver performance.

Abstract: This paper presents examples of using integrity constraints in stableKanren
to encode numeric computations for problem solving. Then, we use one of the
examples to introduce multiple ways to infuse heuristic knowledge and reduce
solving time. stableKanren is an extension of miniKanren that supports normal
logic programs under stable model semantics. stableKanren further supports
numeric computation by constructing a constraint store for integrity
constraints. There are three ways to extend a relational programming language
with numeric computations: relational number representation, grounding numbers
to symbols, and constraint store construction. We demonstrate that the numeric
computations in stableKanren have a straightforward numerical representation
compared to relational number representations. More importantly, stableKanren
balances symbolic and numeric computation in relational programming by avoiding
the grounding of all numbers to symbols. Lastly, it also has simpler syntax
compared to other constraint store construction approaches. stableKanren
supports combinatorial search problem solving under a declarative generate and
test paradigm. Such a paradigm generates all possible combinations of solutions
to the problem, then applies a set of constraints to prune out the unwanted
solutions. We demonstrate that different approaches to writing programs or
queries affect the solver's performance in the SEND+MORE=MONEY puzzle. The
performance gradually improves as more heuristic knowledge is infused through
the programs or queries. Additionally, we show how to use an external function
to achieve a hybrid solution.

</details>


### [675] [concurrentKanren: miniKanren for parallel execution](https://arxiv.org/abs/2510.04994)
*Sjoerd Dost*

Main category: cs.PL

TL;DR: This paper presents a parallel implementation of miniKanren in Go, showcasing its capability to improve performance via implicit parallelism.


<details>
  <summary>Details</summary>
Motivation: Concurrent logic programming has largely remained unexplored in the context of miniKanren, necessitating exploration for better performance opportunities.

Method: A parallel version of miniKanren was implemented in Go, utilizing implicit parallelism to enable existing programs to execute in parallel without modification.

Result: The paper demonstrates the feasibility of parallel miniKanren and evaluates its performance improvements, providing insights into implementation strategies.

Conclusion: The work lays the foundation for models that are language-agnostic, demonstrating the ability of miniKanren to benefit from concurrent logic programming through parallelism.

Abstract: Concurrent logic programming predates miniKanren, but concurrent
implementations of miniKanren have remained largely unexplored. In this work we
present a parallel implementation of miniKanren in Go, demonstrating its
feasibility and potential for performance improvements. Our approach leverages
implicit parallelism allowing legacy programs to benefit from parallel
execution. We discuss implementation strategies and evaluate the impact of
parallelism, laying groundwork for future language-agnostic models.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [676] [Gemini Robotics 1.5: Pushing the Frontier of Generalist Robots with Advanced Embodied Reasoning, Thinking, and Motion Transfer](https://arxiv.org/abs/2510.03342)
*Abbas Abdolmaleki,Saminda Abeyruwan,Joshua Ainslie,Jean-Baptiste Alayrac,Montserrat Gonzalez Arenas,Ashwin Balakrishna,Nathan Batchelor,Alex Bewley,Jeff Bingham,Michael Bloesch,Konstantinos Bousmalis,Philemon Brakel,Anthony Brohan,Thomas Buschmann,Arunkumar Byravan,Serkan Cabi,Ken Caluwaerts,Federico Casarini,Christine Chan,Oscar Chang,London Chappellet-Volpini,Jose Enrique Chen,Xi Chen,Hao-Tien Lewis Chiang,Krzysztof Choromanski,Adrian Collister,David B. D'Ambrosio,Sudeep Dasari,Todor Davchev,Meet Kirankumar Dave,Coline Devin,Norman Di Palo,Tianli Ding,Carl Doersch,Adil Dostmohamed,Yilun Du,Debidatta Dwibedi,Sathish Thoppay Egambaram,Michael Elabd,Tom Erez,Xiaolin Fang,Claudio Fantacci,Cody Fong,Erik Frey,Chuyuan Fu,Ruiqi Gao,Marissa Giustina,Keerthana Gopalakrishnan,Laura Graesser,Oliver Groth,Agrim Gupta,Roland Hafner,Steven Hansen,Leonard Hasenclever,Sam Haves,Nicolas Heess,Brandon Hernaez,Alex Hofer,Jasmine Hsu,Lu Huang,Sandy H. Huang,Atil Iscen,Mithun George Jacob,Deepali Jain,Sally Jesmonth,Abhishek Jindal,Ryan Julian,Dmitry Kalashnikov,M. Emre Karagozler,Stefani Karp,Matija Kecman,J. Chase Kew,Donnie Kim,Frank Kim,Junkyung Kim,Thomas Kipf,Sean Kirmani,Ksenia Konyushkova,Li Yang Ku,Yuheng Kuang,Thomas Lampe,Antoine Laurens,Tuan Anh Le,Isabel Leal,Alex X. Lee,Tsang-Wei Edward Lee,Guy Lever,Jacky Liang,Li-Heng Lin,Fangchen Liu,Shangbang Long,Caden Lu,Sharath Maddineni,Anirudha Majumdar,Kevis-Kokitsi Maninis,Andrew Marmon,Sergio Martinez,Assaf Hurwitz Michaely,Niko Milonopoulos,Joss Moore,Robert Moreno,Michael Neunert,Francesco Nori,Joy Ortiz,Kenneth Oslund,Carolina Parada,Emilio Parisotto,Amaris Paryag,Acorn Pooley,Thomas Power,Alessio Quaglino,Haroon Qureshi,Rajkumar Vasudeva Raju,Helen Ran,Dushyant Rao,Kanishka Rao,Isaac Reid,David Rendleman,Krista Reymann,Miguel Rivas,Francesco Romano,Yulia Rubanova,Peter Pastor Sampedro,Pannag R Sanketi,Dhruv Shah,Mohit Sharma,Kathryn Shea,Mohit Shridhar,Charles Shu,Vikas Sindhwani,Sumeet Singh,Radu Soricut,Rachel Sterneck,Ian Storz,Razvan Surdulescu,Jie Tan,Jonathan Tompson,Saran Tunyasuvunakool,Jake Varley,Grace Vesom,Giulia Vezzani,Maria Bauza Villalonga,Oriol Vinyals,René Wagner,Ayzaan Wahid,Stefan Welker,Paul Wohlhart,Chengda Wu,Markus Wulfmeier,Fei Xia,Ted Xiao,Annie Xie,Jinyu Xie,Peng Xu,Sichun Xu,Ying Xu,Zhuo Xu,Jimmy Yan,Sherry Yang,Skye Yang,Yuxiang Yang,Hiu Hong Yu,Wenhao Yu,Wentao Yuan,Yuan Yuan,Jingwei Zhang,Tingnan Zhang,Zhiyuan Zhang,Allan Zhou,Guangyao Zhou,Yuxiang Zhou*

Main category: cs.RO

TL;DR: The paper introduces Gemini Robotics 1.5, a general-purpose Vision-Language-Action (VLA) model focusing on multi-embodiment robotics, and Gemini Robotics-ER 1.5, excelling in embodied reasoning.


<details>
  <summary>Details</summary>
Motivation: To develop general-purpose robots with advanced physical understanding, reasoning, and control capabilities for solving complex tasks.

Method: The authors propose three innovations: a novel architecture with Motion Transfer to support multi-embodiment learning, multi-level internal reasoning in natural language for action processing, and a state-of-the-art embodied reasoning model for critical decision-making abilities.

Result: The models enhance robots' ability to learn from diverse data, think before acting, plan complex tasks, and improve execution interpretability.

Conclusion: The Gemini Robotics models push robotics closer to enabling physical agents capable of perceiving, reasoning, and acting effectively in real-world tasks.

Abstract: General-purpose robots need a deep understanding of the physical world,
advanced reasoning, and general and dexterous control. This report introduces
the latest generation of the Gemini Robotics model family: Gemini Robotics 1.5,
a multi-embodiment Vision-Language-Action (VLA) model, and Gemini Robotics-ER
1.5, a state-of-the-art Embodied Reasoning (ER) model. We are bringing together
three major innovations. First, Gemini Robotics 1.5 features a novel
architecture and a Motion Transfer (MT) mechanism, which enables it to learn
from heterogeneous, multi-embodiment robot data and makes the VLA more general.
Second, Gemini Robotics 1.5 interleaves actions with a multi-level internal
reasoning process in natural language. This enables the robot to "think before
acting" and notably improves its ability to decompose and execute complex,
multi-step tasks, and also makes the robot's behavior more interpretable to the
user. Third, Gemini Robotics-ER 1.5 establishes a new state-of-the-art for
embodied reasoning, i.e., for reasoning capabilities that are critical for
robots, such as visual and spatial understanding, task planning, and progress
estimation. Together, this family of models takes us a step towards an era of
physical agents-enabling robots to perceive, think and then act so they can
solve complex multi-step tasks.

</details>


### [677] [Optimal swimming with body compliance in an overdamped medium](https://arxiv.org/abs/2510.03457)
*Jianfeng Lin,Tianyu Wang,Baxi Chong,Matthew Fernandez,Zhaochen Xu,Daniel I. Goldman*

Main category: cs.RO

TL;DR: The paper extends geometric mechanics to model and optimize compliant, undulatory locomotion systems, validated with a physical robot.


<details>
  <summary>Details</summary>
Motivation: Existing geometric mechanics approaches assume precise execution of gaits, but in practice, compliance and environmental interactions perturb locomotion.

Method: A new compliant extension of Purcell's three-link swimmer was designed with springs at joints, integrating body dynamics via resistive force theory into modeling and optimization frameworks.

Result: The approach accurately predicts and optimizes locomotor performance, validated on a three-link robot in granular mediums.

Conclusion: Compliance in locomotion can be systematically modeled and leveraged for robust movement in diverse environments.

Abstract: Elongate animals and robots use undulatory body waves to locomote through
diverse environments. Geometric mechanics provides a framework to model and
optimize such systems in highly damped environments, connecting a prescribed
shape change pattern (gait) with locomotion displacement. However, existing
approaches assume precise execution of prescribed gaits, whereas in practice
environmental interactions with compliant bodies of animals or robots
frequently perturb the realized trajectories. In this work, we extend geometric
mechanics to predict locomotor performance and search for optimal swimming
strategy of compliant undulators. We introduce a compliant extension of
Purcell's three-link swimmer by incorporating series-connected springs at the
joints. Body dynamics are derived with resistive force theory. Geometric
mechanics is incorporated into movement prediction and into an optimization
framework that identifies strategies for controlling compliant swimmers to
achieve maximal displacement. We validate our framework on a physical
cable-driven three-link limbless robot, and demonstrate accurate prediction and
optimization of locomotor performance under varied programmed, state-dependent
compliance in a granular medium. Our results establish a systematic
physics-based approach for modeling and controlling compliant swimming
locomotion, highlighting compliance as a design feature that can be exploited
for robust movement in homogeneous and heterogeneous environments.

</details>


### [678] [Warm-Starting Optimization-Based Motion Planning for Robotic Manipulators via Point Cloud-Conditioned Flow Matching](https://arxiv.org/abs/2510.03460)
*Sibo Tian,Minghui Zheng,Xiao Liang*

Main category: cs.RO

TL;DR: The paper introduces a learning-based initialization method for trajectory optimization in robot motion planning, addressing challenges related to dynamic and high-dimensional environments.


<details>
  <summary>Details</summary>
Motivation: In Human-Robot Collaboration systems, robots must generate rapid, safe, and efficient motion plans in dynamic and complex environments. Current planners face inefficiencies due to post-processing needs or susceptibility to local minima.

Method: The proposed method utilizes a Flow Matching model conditioned on single-view point cloud data from a depth camera to generate near-optimal trajectory initializations without needing prior environmental information.

Result: Experiments with a UR5e robotic manipulator in cluttered workspaces show high success rates, reduced optimization iterations, and improved generalization to unseen environments.

Conclusion: The learning-based generative initializer improves trajectory optimization in robotics, offering a significant advantage over traditional and benchmark methods in dynamic and cluttered environments.

Abstract: Rapid robot motion generation is critical in Human-Robot Collaboration (HRC)
systems, as robots need to respond to dynamic environments in real time by
continuously observing their surroundings and replanning their motions to
ensure both safe interactions and efficient task execution. Current
sampling-based motion planners face challenges in scaling to high-dimensional
configuration spaces and often require post-processing to interpolate and
smooth the generated paths, resulting in time inefficiency in complex
environments. Optimization-based planners, on the other hand, can incorporate
multiple constraints and generate smooth trajectories directly, making them
potentially more time-efficient. However, optimization-based planners are
sensitive to initialization and may get stuck in local minima. In this work, we
present a novel learning-based method that utilizes a Flow Matching model
conditioned on a single-view point cloud to learn near-optimal solutions for
optimization initialization. Our method does not require prior knowledge of the
environment, such as obstacle locations and geometries, and can generate
feasible trajectories directly from single-view depth camera input. Simulation
studies on a UR5e robotic manipulator in cluttered workspaces demonstrate that
the proposed generative initializer achieves a high success rate on its own,
significantly improves the success rate of trajectory optimization compared
with traditional and learning-based benchmark initializers, requires fewer
optimization iterations, and exhibits strong generalization to unseen
environments.

</details>


### [679] [A Simulation Evaluation Suite for Robust Adaptive Quadcopter Control](https://arxiv.org/abs/2510.03471)
*Dingqi Zhang,Ran Tao,Sheng Cheng,Naira Hovakimyan,Mark W. Mueller*

Main category: cs.RO

TL;DR: This paper presents a modular simulation testbed for evaluating quadcopter control frameworks under various disturbances, facilitating systematic analysis and reproducibility.


<details>
  <summary>Details</summary>
Motivation: To address fragmented evaluations of quadcopter adaptive control methods due to differences in tasks, simulators, and implementations.

Method: A modular simulation testbed was developed using RotorPy, which integrates adaptive/non-adaptive controllers, disturbance models, and task-relevant metrics.

Result: The testbed allows evaluation under various disturbances like wind, payload shifts, rotor faults, and demonstrates its versatility with multiple scenarios and automated stress testing.

Conclusion: The framework streamlines reproducible, systematic comparisons of quadcopter control methods while eliminating redundant implementation effort.

Abstract: Robust adaptive control methods are essential for maintaining quadcopter
performance under external disturbances and model uncertainties. However,
fragmented evaluations across tasks, simulators, and implementations hinder
systematic comparison of these methods. This paper introduces an
easy-to-deploy, modular simulation testbed for quadcopter control, built on
RotorPy, that enables evaluation under a wide range of disturbances such as
wind, payload shifts, rotor faults, and control latency. The framework includes
a library of representative adaptive and non-adaptive controllers and provides
task-relevant metrics to assess tracking accuracy and robustness. The unified
modular environment enables reproducible evaluation across control methods and
eliminates redundant reimplementation of components such as disturbance models,
trajectory generators, and analysis tools. We illustrate the testbed's
versatility through examples spanning multiple disturbance scenarios and
trajectory types, including automated stress testing, to demonstrate its
utility for systematic analysis. Code is available at
https://github.com/Dz298/AdaptiveQuadBench.

</details>


### [680] [Destination-to-Chutes Task Mapping Optimization for Multi-Robot Coordination in Robotic Sorting Systems](https://arxiv.org/abs/2510.03472)
*Yulun Zhang,Alexandre O. G. Barbosa,Federico Pecora,Jiaoyang Li*

Main category: cs.RO

TL;DR: The paper addresses optimizing destination-to-chutes task mapping in robotic sorting systems to improve efficiency and throughput.


<details>
  <summary>Details</summary>
Motivation: Efficiently assigning destination tasks to chutes in robotic sorting systems is challenging due to interdependence with robot planning, chute closure times, and downstream processing impact.

Method: The authors define task mapping optimization formally, build a simulator for evaluation, and propose a method combining Evolutionary Algorithm and Mixed Integer Linear Programming. Quality Diversity algorithms are also utilized for further analysis.

Result: Optimized task mappings show superior performance compared to greedy methods across diverse setups involving different sizes, chute numbers, and destinations.

Conclusion: The proposed optimization method improves throughput and operational efficiency in robotic sorting systems, validated through extensive simulation experiments.

Abstract: We study optimizing a destination-to-chutes task mapping to improve
throughput in Robotic Sorting Systems (RSS), where a team of robots sort
packages on a sortation floor by transporting them from induct workstations to
eject chutes based on their shipping destinations (e.g. Los Angeles or
Pittsburgh). The destination-to-chutes task mapping is used to determine which
chutes a robot can drop its package. Finding a high-quality task mapping is
challenging because of the complexity of a real-world RSS. First, optimizing
task mapping is interdependent with robot target assignment and path planning.
Second, chutes will be CLOSED for a period of time once they receive sufficient
packages to allow for downstream processing. Third, task mapping quality
directly impacts the downstream processing, as scattered chutes for the same
destination increase package handling time. In this paper, we first formally
define task mappings and the problem of Task Mapping Optimization (TMO). We
then present a simulator of RSS to evaluate task mappings. We then present a
simple TMO method based on the Evolutionary Algorithm and Mixed Integer Linear
Programming, demonstrating the advantage of our optimized task mappings over
the greedily generated ones in various RSS setups with different map sizes,
numbers of chutes, and destinations. Finally, we use Quality Diversity
algorithms to analyze the throughput of a diverse set of task mappings. Our
code is available online at https://github.com/lunjohnzhang/tmo_public.

</details>


### [681] [Robust Permissive Controller Synthesis for Interval MDPs](https://arxiv.org/abs/2510.03481)
*Khang Vo Huynh,David Parker,Lu Feng*

Main category: cs.RO

TL;DR: The paper introduces a framework for synthesizing robust permissive controllers for robots operating under uncertain dynamics modeled as Interval Markov Decision Processes (IMDPs). It provides runtime flexibility and resilience while ensuring compliance with specifications.


<details>
  <summary>Details</summary>
Motivation: Robots often face real-world uncertainties from sensing noise, actuation imprecision, and coarse system models. Traditional deterministic strategies lack flexibility to adapt to these uncertainties. The authors aim to address this limitation by proposing robust permissive multi-strategies for uncertain environments.

Method: The authors modeled the problem using IMDPs and formulated robust permissive controller synthesis as mixed-integer linear programs (MILPs). They developed two methods: a vertex-enumeration approach and a scalable duality-based method to efficiently compute controllers without explicit enumeration.

Result: Both proposed methods successfully synthesize robust permissive controllers that ensure specification satisfaction under all admissible transitions. Experiments demonstrated scalability to IMDPs with hundreds of thousands of states across four benchmark domains.

Conclusion: The study offers a significant advance in controller synthesis for uncertain robotics environments by ensuring flexibility and resilience in decision-making while scaling to large systems.

Abstract: We address the problem of robust permissive controller synthesis for robots
operating under uncertain dynamics, modeled as Interval Markov Decision
Processes (IMDPs). IMDPs generalize standard MDPs by allowing transition
probabilities to vary within intervals, capturing epistemic uncertainty from
sensing noise, actuation imprecision, and coarse system abstractions-common in
robotics. Traditional controller synthesis typically yields a single
deterministic strategy, limiting adaptability. In contrast, permissive
controllers (multi-strategies) allow multiple actions per state, enabling
runtime flexibility and resilience. However, prior work on permissive
controller synthesis generally assumes exact transition probabilities, which is
unrealistic in many robotic applications. We present the first framework for
robust permissive controller synthesis on IMDPs, guaranteeing that all
strategies compliant with the synthesized multi-strategy satisfy reachability
or reward-based specifications under all admissible transitions. We formulate
the problem as mixed-integer linear programs (MILPs) and propose two encodings:
a baseline vertex-enumeration method and a scalable duality-based method that
avoids explicit enumeration. Experiments on four benchmark domains show that
both methods synthesize robust, maximally permissive controllers and scale to
large IMDPs with up to hundreds of thousands of states.

</details>


### [682] [Digital-Twin Evaluation for Proactive Human-Robot Collision Avoidance via Prediction-Guided A-RRT*](https://arxiv.org/abs/2510.03496)
*Vadivelan Murugesan,Rajasundaram Mathiazhagan,Sanjana Joshi,Aliasghar Arab*

Main category: cs.RO

TL;DR: This paper introduces a framework for human-robot collaboration using joint-by-joint human motion forecasting, combined with real-time validation in a physics-based digital twin, achieving high precision and proactive collision avoidance.


<details>
  <summary>Details</summary>
Motivation: The study aims to enhance human-robot collaboration by addressing the challenge of predicting human motion accurately over extended periods to mitigate collision risks and improve safety.

Method: The framework involves human motion prediction using 3D skeletal data processed by a CNN-BiLSTM model, which forecasts individual joint trajectories. A digital twin then validates these predictions, and an Adaptive RRT* planner adjusts trajectories in real-time based on collision risk metrics derived from capsule-based artificial potential fields.

Result: In 50 trials, the method achieved 100% proactive collision avoidance with greater than 250 mm clearance and replanning times under 2 seconds, outperforming kinematic-only planners in precision and reliability.

Conclusion: The integration of predictive human modeling with digital twin validation significantly improves the precision, reliability, and safety in human-robot collaborative planning, making it well-suited for real-time applications.

Abstract: Human-robot collaboration requires precise prediction of human motion over
extended horizons to enable proactive collision avoidance. Unlike existing
planners that rely solely on kinodynamic models, we present a prediction-driven
safe planning framework that leverages granular, joint-by-joint human motion
forecasting validated in a physics-based digital twin. A capsule-based
artificial potential field (APF) converts these granular predictions into
collision risk metrics, triggering an Adaptive RRT* (A-RRT*) planner when
thresholds are exceeded. The depth camera is used to extract 3D skeletal poses
and a convolutional neural network-bidirectional long short-term memory
(CNN-BiLSTM) model to predict individual joint trajectories ahead of time. A
digital twin model integrates real-time human posture prediction placed in
front of a simulated robot to evaluate motions and physical contacts. The
proposed method enables validation of planned trajectories ahead of time and
bridging potential latency gaps in updating planned trajectories in real-time.
In 50 trials, our method achieved 100% proactive avoidance with > 250 mm
clearance and sub-2 s replanning, demonstrating superior precision and
reliability compared to existing kinematic-only planners through the
integration of predictive human modeling with digital twin validation.

</details>


### [683] [Distributed Connectivity Maintenance and Recovery for Quadrotor Motion Planning](https://arxiv.org/abs/2510.03504)
*Yutong Wang,Yichun Qu,Tengxiang Wang,Lishuo Pan,Nora Ayanian*

Main category: cs.RO

TL;DR: This paper introduces a real-time framework ensuring robust connectivity among multi-robots during navigation using barrier and Lyapunov functions.


<details>
  <summary>Details</summary>
Motivation: Multi-robot systems require robust connectivity to operate in environments with obstacles and visual occlusions, which is often challenging due to inter-robot proximity issues.

Method: The framework combines high-order control barrier functions (HOCBFs) and control Lyapunov functions with a Bezier-parameterized trajectory generation approach within an MPC-CLF-CBF structure.

Result: Simulations and physical experiments with Crazyflie nano-quadrotors demonstrate the practical applicability and effectiveness in maintaining and recovering connectivity.

Conclusion: The unified approach achieves real-time collision avoidance, smooth trajectory generation, and robust connectivity recovery in multi-robot systems.

Abstract: Maintaining connectivity is crucial in many multi-robot applications, yet
fragile to obstacles and visual occlusions. We present a real-time distributed
framework for multi-robot navigation certified by high-order control barrier
functions (HOCBFs) that controls inter-robot proximity to maintain connectivity
while avoiding collisions. We incorporate control Lyapunov functions to enable
connectivity recovery from initial disconnected configurations and temporary
losses, providing robust connectivity during navigation in obstacle-rich
environments. Our trajectory generation framework concurrently produces
planning and control through a Bezier-parameterized trajectory, which naturally
provides smooth curves with arbitrary degree of derivatives. The main
contribution is the unified MPC-CLF-CBF framework, a continuous-time trajectory
generation and control method for connectivity maintenance and recovery of
multi-robot systems. We validate the framework through extensive simulations
and a physical experiment with 4 Crazyflie nano-quadrotors.

</details>


### [684] [LapSurgie: Humanoid Robots Performing Surgery via Teleoperated Handheld Laparoscopy](https://arxiv.org/abs/2510.03529)
*Zekai Liang,Xiao Liang,Soofiyan Atar,Sreyan Das,Zoe Chiu,Peihan Zhang,Florian Richter,Shanglei Liu,Michael C. Yip*

Main category: cs.RO

TL;DR: The paper introduces LapSurgie, a humanoid robot-based laparoscopic teleoperation framework aimed at addressing disparities in access to surgical robotic systems in underserved communities.


<details>
  <summary>Details</summary>
Motivation: High-resource centers dominate robotic laparoscopic surgery adoption, leaving rural and low-resource regions underserved. The paper seeks to bridge this gap.

Method: LapSurgie uses an inverse-mapping strategy for manual-wristed laparoscopic instruments while maintaining remote center-of-motion constraints. It integrates tools like stereo vision for real-time feedback.

Result: User studies validate the framework's effectiveness and endorse the potential of humanoid robots in laparoscopic surgeries.

Conclusion: LapSurgie offers initial evidence for deploying humanoid robots to enhance surgical accessibility in low-resource areas.

Abstract: Robotic laparoscopic surgery has gained increasing attention in recent years
for its potential to deliver more efficient and precise minimally invasive
procedures. However, adoption of surgical robotic platforms remains largely
confined to high-resource medical centers, exacerbating healthcare disparities
in rural and low-resource regions. To close this gap, a range of solutions has
been explored, from remote mentorship to fully remote telesurgery. Yet, the
practical deployment of surgical robotic systems to underserved communities
remains an unsolved challenge. Humanoid systems offer a promising path toward
deployability, as they can directly operate in environments designed for humans
without extensive infrastructure modifications -- including operating rooms. In
this work, we introduce LapSurgie, the first humanoid-robot-based laparoscopic
teleoperation framework. The system leverages an inverse-mapping strategy for
manual-wristed laparoscopic instruments that abides to remote center-of-motion
constraints, enabling precise hand-to-tool control of off-the-shelf surgical
laparoscopic tools without additional setup requirements. A control console
equipped with a stereo vision system provides real-time visual feedback.
Finally, a comprehensive user study across platforms demonstrates the
effectiveness of the proposed framework and provides initial evidence for the
feasibility of deploying humanoid robots in laparoscopic procedures.

</details>


### [685] [Efficient Surgical Robotic Instrument Pose Reconstruction in Real World Conditions Using Unified Feature Detection](https://arxiv.org/abs/2510.03532)
*Zekai Liang,Kazuya Miyata,Xiao Liang,Florian Richter,Michael C. Yip*

Main category: cs.RO

TL;DR: The paper introduces a novel camera-to-robot calibration framework tailored for minimally invasive surgical robots, enabling fast and accurate feature detection and pose estimation.


<details>
  <summary>Details</summary>
Motivation: Minimally invasive surgical robots require precise calibration due to the challenges posed by their long kinematic chains and limited visibility in camera views.

Method: The paper presents a unified framework for detecting both keypoints and shaft edges in a single inference, using a shared encoding schema trained on large-scale synthetic data with projective labeling.

Result: Experimental evaluations indicate that the method achieves rapid performance and state-of-the-art accuracy in feature detection and pose estimation under challenging surgical conditions.

Conclusion: The proposed framework effectively addresses challenges in surgical robot calibration, offering improvements in efficiency and accuracy suitable for online robot control systems.

Abstract: Accurate camera-to-robot calibration is essential for any vision-based
robotic control system and especially critical in minimally invasive surgical
robots, where instruments conduct precise micro-manipulations. However, MIS
robots have long kinematic chains and partial visibility of their degrees of
freedom in the camera, which introduces challenges for conventional
camera-to-robot calibration methods that assume stiff robots with good
visibility. Previous works have investigated both keypoint-based and
rendering-based approaches to address this challenge in real-world conditions;
however, they often struggle with consistent feature detection or have long
inference times, neither of which are ideal for online robot control. In this
work, we propose a novel framework that unifies the detection of geometric
primitives (keypoints and shaft edges) through a shared encoding, enabling
efficient pose estimation via projection geometry. This architecture detects
both keypoints and edges in a single inference and is trained on large-scale
synthetic data with projective labeling. This method is evaluated across both
feature detection and pose estimation, with qualitative and quantitative
results demonstrating fast performance and state-of-the-art accuracy in
challenging surgical environments.

</details>


### [686] [Shape-Space Graphs: Fast and Collision-Free Path Planning for Soft Robots](https://arxiv.org/abs/2510.03547)
*Carina Veil,Moritz Flaschel,Ellen Kuhl*

Main category: cs.RO

TL;DR: This paper introduces a graph-based path planning tool for soft robotic arms inspired by elephant trunks, addressing their nonlinear kinematics through a precomputed mechanically accurate shape library and efficient obstacle avoidance.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenge of motion planning for soft robots, particularly in cluttered environments, due to their highly nonlinear and infinite-dimensional kinematics.

Method: The authors utilize a biomechanical model inspired by morphoelasticity and active filament theory to precompute a shape library, constructing a $k$-nearest neighbor graph in shape space with energy-efficient planning and collision avoidance.

Result: The algorithm reliably avoids obstacles and generates feasible paths within milliseconds, showcasing drastic reductions in actuation effort by incorporating energy costs.

Conclusion: The proposed shape-space graph approach enables fast and reliable path planning for soft robotics, paving the way for real-time applications across surgical, industrial, and assistive technologies.

Abstract: Soft robots, inspired by elephant trunks or octopus arms, offer extraordinary
flexibility to bend, twist, and elongate in ways that rigid robots cannot.
However, their motion planning remains a challenge, especially in cluttered
environments with obstacles, due to their highly nonlinear and
infinite-dimensional kinematics. Here, we present a graph-based path planning
tool for an elephant-trunk-inspired soft robotic arm designed with three
artificial muscle fibers that allow for multimodal continuous deformation
through contraction. Using a biomechanical model inspired by morphoelasticity
and active filament theory, we precompute a shape library and construct a
$k$-nearest neighbor graph in \emph{shape space}, ensuring that each node
corresponds to a mechanically accurate and physically valid robot shape. For
the graph, we use signed distance functions to prune nodes and edges colliding
with obstacles, and define multi-objective edge costs based on geometric
distance and actuation effort, enabling energy-efficient planning with
collision avoidance. We demonstrate that our algorithm reliably avoids
obstacles and generates feasible paths within milliseconds from precomputed
graphs using Dijkstra's algorithm. We show that including energy costs can
drastically reduce the actuation effort compared to geometry-only planning, at
the expense of longer tip trajectories. Our results highlight the potential of
shape-space graph search for fast and reliable path planning in the field of
soft robotics, paving the way for real-time applications in surgical,
industrial, and assistive settings.

</details>


### [687] [Learning to Act Through Contact: A Unified View of Multi-Task Robot Learning](https://arxiv.org/abs/2510.03599)
*Shafeef Omar,Majid Khadiv*

Main category: cs.RO

TL;DR: The paper introduces a unified framework for multi-task locomotion and manipulation policy learning using contact-explicit representations.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of designing different policies for various tasks by unifying the structural definition of tasks through shared contact goals.

Method: A goal-conditioned reinforcement learning policy is trained to execute tasks through contact plans across diverse robotic systems.

Result: The framework enables robust performance of a wide range of locomotion and manipulation tasks across different robotic embodiments using a single policy.

Conclusion: Contact-explicit policy learning enhances generalization to unseen scenarios and offers a scalable foundation for loco-manipulation tasks.

Abstract: We present a unified framework for multi-task locomotion and manipulation
policy learning grounded in a contact-explicit representation. Instead of
designing different policies for different tasks, our approach unifies the
definition of a task through a sequence of contact goals-desired contact
positions, timings, and active end-effectors. This enables leveraging the
shared structure across diverse contact-rich tasks, leading to a single policy
that can perform a wide range of tasks. In particular, we train a
goal-conditioned reinforcement learning (RL) policy to realise given contact
plans. We validate our framework on multiple robotic embodiments and tasks: a
quadruped performing multiple gaits, a humanoid performing multiple biped and
quadrupedal gaits, and a humanoid executing different bimanual object
manipulation tasks. Each of these scenarios is controlled by a single policy
trained to execute different tasks grounded in contacts, demonstrating
versatile and robust behaviours across morphologically distinct systems. Our
results show that explicit contact reasoning significantly improves
generalisation to unseen scenarios, positioning contact-explicit policy
learning as a promising foundation for scalable loco-manipulation.

</details>


### [688] [Safety-Oriented Dynamic Path Planning for Automated Vehicles](https://arxiv.org/abs/2510.03640)
*Mostafa Emam,Matthias Gerdts*

Main category: cs.RO

TL;DR: The paper presents a bi-level control framework for safer autonomous driving with enhanced path planning and obstacle avoidance through Nonlinear Model Predictive Control (NMPC) and backup trajectory provision.


<details>
  <summary>Details</summary>
Motivation: Safety in autonomous vehicles demands improved path planning and navigation, especially in complex and dynamic environments.

Method: The framework uses a main loop (NMPC with homotopy-based constraint relaxation) for real-time path optimization and a backup loop for safe fallback trajectories under critical time constraints.

Result: Evaluation demonstrated the approach's real-time performance and robustness across various driving scenarios.

Conclusion: The framework advances autonomous driving through precise and adaptive path planning, ensuring safety and reliability in dynamic situations.

Abstract: Ensuring safety in autonomous vehicles necessitates advanced path planning
and obstacle avoidance capabilities, particularly in dynamic environments. This
paper introduces a bi-level control framework that efficiently augments road
boundaries by incorporating time-dependent grid projections of obstacle
movements, thus enabling precise and adaptive path planning. The main control
loop utilizes Nonlinear Model Predictive Control (NMPC) for real-time path
optimization, wherein homotopy-based constraint relaxation is employed to
improve the solvability of the optimal control problem (OCP). Furthermore, an
independent backup loop runs concurrently to provide safe fallback trajectories
when an optimal trajectory cannot be computed by the main loop within a
critical time frame, thus enhancing safety and real-time performance. Our
evaluation showcases the benefits of the proposed methods in various driving
scenarios, highlighting the real-time applicability and robustness of our
approach. Overall, the framework represents a significant step towards safer
and more reliable autonomous driving in complex and dynamic environments.

</details>


### [689] [Geometrically Exact Hard Magneto-Elastic Cosserat Shells: Static Formulation for Shape Morphing](https://arxiv.org/abs/2510.03644)
*Mohammadjavad Javadi,Robin Chhabra*

Main category: cs.RO

TL;DR: The paper presents a new coordinate-free Cosserat shell theory for modeling soft robots with 2D shell structures, enabling improved analysis and control in applications like manipulation and locomotion.


<details>
  <summary>Details</summary>
Motivation: To address limitations in modeling soft robots with large width-to-length ratios, which are more appropriately represented as 2D shells rather than 1D slender structures.

Method: Developed a coordinate-free Cosserat shell theory using the Special Euclidean group ($\mathbf{SE}(3)$), derived equilibrium equations based on virtual work principles, and implemented a finite element approach to avoid modeling challenges like singularity and locking phenomena.

Result: The model is experimentally and analytically validated, showing effectiveness in cases involving severe rotations and displacements.

Conclusion: The proposed method provides advanced and reliable tools for modeling and controlling soft robots with 2D shell structures, overcoming existing challenges in finite element modeling.

Abstract: Cosserat rod theory is the popular approach to modeling ferromagnetic soft
robots as 1-Dimensional (1D) slender structures in most applications, such as
biomedical. However, recent soft robots designed for locomotion and
manipulation often exhibit a large width-to-length ratio that categorizes them
as 2D shells. For analysis and shape-morphing control purposes, we develop an
efficient coordinate-free static model of hard-magnetic shells found in soft
magnetic grippers and walking soft robots. The approach is based on a novel
formulation of Cosserat shell theory on the Special Euclidean group
($\mathbf{SE}(3)$). The shell is assumed to be a 2D manifold of material points
with six degrees of freedom (position & rotation) suitable for capturing the
behavior of a uniformly distributed array of spheroidal hard magnetic particles
embedded in the rheological elastomer. The shell's configuration manifold is
the space of all smooth embeddings $\mathbb{R}^2\rightarrow\mathbf{SE}(3)$.
According to a novel definition of local deformation gradient based on the Lie
group structure of $\mathbf{SE}(3)$, we derive the strong and weak forms of
equilibrium equations, following the principle of virtual work. We extract the
linearized version of the weak form for numerical implementations. The
resulting finite element approach can avoid well-known challenges such as
singularity and locking phenomenon in modeling shell structures. The proposed
model is analytically and experimentally validated through a series of test
cases that demonstrate its superior efficacy, particularly when the shell
undergoes severe rotations and displacements.

</details>


### [690] [An Amphibious Untethered Inchworm Soft Robot for Fast Crawling Locomotion](https://arxiv.org/abs/2510.03660)
*Mohammadjavad Javadi,Charlie Wadds,Robin Chhabra*

Main category: cs.RO

TL;DR: This paper introduces a fully untethered soft robot inspired by an inchworm, capable of walking, swimming, steering, and payload transport using magnetic actuators and onboard control.


<details>
  <summary>Details</summary>
Motivation: To develop untethered soft robots for practical applications in complex environments, leveraging their inherent flexibility and adaptivity.

Method: Designed a magnetically actuated soft robot with a curved and flexible structure, onboard control, wireless command, and integrated camera for environmental perception. Structural optimization and system-level integration were conducted.

Result: The robot achieved a walking speed of 3.74 cm/s, swimming speed of 0.82 cm/s, and successfully demonstrated multimodal locomotion along with payload transport in experiments.

Conclusion: The developed robot showcases the potential of untethered soft robots for diverse tasks in real-world scenarios, supported by its validated dynamic performance and multimodal capabilities.

Abstract: Untethered soft robots are essential for advancing the real-world deployment
of soft robotic systems in diverse and multitasking environments. Inspired by
soft-bodied inchworm, we present a fully untethered soft robot with a curved,
flexible structure actuated by magnetic forces. The robot has a total mass of
102.63 g and demonstrates multimodal locomotion, achieving a maximum walking
speed of 3.74 cm/s and a swimming speed of 0.82 cm/s. A compact and lightweight
onboard control circuit enables wireless command transmission, while an
integrated camera provides environmental perception. Through structural
optimization and system-level integration, the robot successfully performs
walking, steering, swimming, and payload transport without reliance on external
infrastructure. The robot's dynamic performance and locomotion capabilities are
systematically validated through experimental characterization.

</details>


### [691] [Robust Visual Embodiment: How Robots Discover Their Bodies in Real Environments](https://arxiv.org/abs/2510.03677)
*Salim Rezvani,Ammar Jaleel Mahmood,Robin Chhabra*

Main category: cs.RO

TL;DR: The paper investigates how visual noise impacts robots' ability to self-model and presents a denoising framework to overcome these challenges, achieving robust performance.


<details>
  <summary>Details</summary>
Motivation: Existing autonomous robotic self-modeling processes are fragile in realistic, noisy visual environments, limiting adaptability and performance.

Method: A systematic analysis of visual noise effects on robotic self-modeling, coupled with introducing a task-aware denoising framework using classical restoration and morphology-preserving constraints.

Result: The proposed denoising framework significantly improves robotic self-modeling, restoring near-baseline performance even under various visual degradations.

Conclusion: The study enhances robotic self-modeling robustness, paving the way for deploying self-aware robots in challenging real-world conditions.

Abstract: Robots with internal visual self-models promise unprecedented adaptability,
yet existing autonomous modeling pipelines remain fragile under realistic
sensing conditions such as noisy imagery and cluttered backgrounds. This paper
presents the first systematic study quantifying how visual
degradations--including blur, salt-and-pepper noise, and Gaussian noise--affect
robotic self-modeling. Through both simulation and physical experiments, we
demonstrate their impact on morphology prediction, trajectory planning, and
damage recovery in state-of-the-art pipelines. To overcome these challenges, we
introduce a task-aware denoising framework that couples classical restoration
with morphology-preserving constraints, ensuring retention of structural cues
critical for self-modeling. In addition, we integrate semantic segmentation to
robustly isolate robots from cluttered and colorful scenes. Extensive
experiments show that our approach restores near-baseline performance across
simulated and physical platforms, while existing pipelines degrade
significantly. These contributions advance the robustness of visual
self-modeling and establish practical foundations for deploying self-aware
robots in unpredictable real-world environments.

</details>


### [692] [EmbodiSwap for Zero-Shot Robot Imitation Learning](https://arxiv.org/abs/2510.03706)
*Eadom Dessalene,Pavan Mantripragada,Michael Maynord,Yiannis Aloimonos*

Main category: cs.RO

TL;DR: This paper introduces EmbodiSwap, a method to produce synthetic robot overlays on human videos for zero-shot imitation learning, leveraging V-JEPA for enhanced robot manipulation policy training.


<details>
  <summary>Details</summary>
Motivation: To address the embodiment gap between human videos and robotic systems, facilitating direct imitation learning for robotic manipulation tasks.

Method: EmbodiSwap generates photorealistic synthetic robot overlays on human videos, paired with the use of V-JEPA as the visual backbone for training zero-shot manipulation policies.

Result: The model trained with this approach achieved an 82% success rate in real-world tests, outperforming other methods like few-shot trained networks and traditional backbones.

Conclusion: EmbodiSwap effectively bridges the embodiment gap for zero-shot robotic imitation learning and demonstrates superior performance, while releasing tools and datasets to promote further research.

Abstract: We introduce EmbodiSwap - a method for producing photorealistic synthetic
robot overlays over human video. We employ EmbodiSwap for zero-shot imitation
learning, bridging the embodiment gap between in-the-wild ego-centric human
video and a target robot embodiment. We train a closed-loop robot manipulation
policy over the data produced by EmbodiSwap. We make novel use of V-JEPA as a
visual backbone, repurposing V-JEPA from the domain of video understanding to
imitation learning over synthetic robot videos. Adoption of V-JEPA outperforms
alternative vision backbones more conventionally used within robotics. In
real-world tests, our zero-shot trained V-JEPA model achieves an $82\%$ success
rate, outperforming a few-shot trained $\pi_0$ network as well as $\pi_0$
trained over data produced by EmbodiSwap. We release (i) code for generating
the synthetic robot overlays which takes as input human videos and an arbitrary
robot URDF and generates a robot dataset, (ii) the robot dataset we synthesize
over EPIC-Kitchens, HOI4D and Ego4D, and (iii) model checkpoints and inference
code, to facilitate reproducible research and broader adoption.

</details>


### [693] [Model-Based Adaptive Precision Control for Tabletop Planar Pushing Under Uncertain Dynamics](https://arxiv.org/abs/2510.03768)
*Aydin Ahmadi,Baris Akgun*

Main category: cs.RO

TL;DR: The paper introduces a model-based framework for tabletop pushing tasks using a single learned model that generalizes across multiple tasks without retraining, validated through simulation and real-world experiments.


<details>
  <summary>Details</summary>
Motivation: Manual engineering for pushing tasks is effort-intensive, and prior data-driven methods are limited to narrow functionalities. The aim is to create a versatile model capable of addressing diverse tasks seamlessly.

Method: The method involves using a recurrent GRU-based architecture with non-linear layers capturing object-environment dynamics. A Model Predictive Path Integral (MPPI) controller is integrated to generate task-specific adaptive actions, and training is conducted with domain randomization in simulation.

Result: The framework achieves high success rates in precise positioning, trajectory tracking, and obstacle avoidance tasks. It supports sim-to-real transfer and demonstrates adaptability across diverse dynamics and objectives.

Conclusion: The proposed model-based framework simplifies pushing tasks across objectives without retraining, showcasing robust performance and adaptability. Future work aims to expand capabilities to varied object types and longer-horizon tasks.

Abstract: Data-driven planar pushing methods have recently gained attention as they
reduce manual engineering effort and improve generalization compared to
analytical approaches. However, most prior work targets narrow capabilities
(e.g., side switching, precision, or single-task training), limiting broader
applicability. We present a model-based framework for non-prehensile tabletop
pushing that uses a single learned model to address multiple tasks without
retraining. Our approach employs a recurrent GRU-based architecture with
additional non-linear layers to capture object-environment dynamics while
ensuring stability. A tailored state-action representation enables the model to
generalize across uncertain dynamics, variable push lengths, and diverse tasks.
For control, we integrate the learned dynamics with a sampling-based Model
Predictive Path Integral (MPPI) controller, which generates adaptive,
task-oriented actions. This framework supports side switching, variable-length
pushes, and objectives such as precise positioning, trajectory following, and
obstacle avoidance. Training is performed in simulation with domain
randomization to support sim-to-real transfer. We first evaluate the
architecture through ablation studies, showing improved prediction accuracy and
stable rollouts. We then validate the full system in simulation and real-world
experiments using a Franka Panda robot with markerless tracking. Results
demonstrate high success rates in precise positioning under strict thresholds
and strong performance in trajectory tracking and obstacle avoidance. Moreover,
multiple tasks are solved simply by changing the controller's objective
function, without retraining. While our current focus is on a single object
type, we extend the framework by training on wider push lengths and designing a
balanced controller that reduces the number of steps for longer-horizon goals.

</details>


### [694] [Trajectory prediction for heterogeneous agents: A performance analysis on small and imbalanced datasets](https://arxiv.org/abs/2510.03776)
*Tiago Rodrigues de Almeida,Yufei Zhu,Andrey Rudenko,Tomasz P. Kucner,Johannes A. Stork,Martin Magnusson,Achim J. Lilienthal*

Main category: cs.RO

TL;DR: The paper examines class-conditioned motion prediction for heterogeneous agents and proposes new methods evaluated on two datasets.


<details>
  <summary>Details</summary>
Motivation: Many intelligent systems struggle to efficiently navigate complex environments due to difficulties in predicting the future actions of diverse agents.

Method: The authors developed pattern-based and deep learning-based class-conditioned trajectory prediction methods and tested them on two datasets.

Result: Accuracy improved in most cases when class labels were considered, but results varied based on dataset balance and data availability in new environments.

Conclusion: Deep learning methods excel in balanced datasets, whereas pattern-based methods are favored in limited or imbalanced data scenarios, making them ideal for specific applications like robot cold starts.

Abstract: Robots and other intelligent systems navigating in complex dynamic
environments should predict future actions and intentions of surrounding agents
to reach their goals efficiently and avoid collisions. The dynamics of those
agents strongly depends on their tasks, roles, or observable labels.
Class-conditioned motion prediction is thus an appealing way to reduce forecast
uncertainty and get more accurate predictions for heterogeneous agents.
However, this is hardly explored in the prior art, especially for mobile robots
and in limited data applications. In this paper, we analyse different
class-conditioned trajectory prediction methods on two datasets. We propose a
set of conditional pattern-based and efficient deep learning-based baselines,
and evaluate their performance on robotics and outdoors datasets (TH\"OR-MAGNI
and Stanford Drone Dataset). Our experiments show that all methods improve
accuracy in most of the settings when considering class labels. More
importantly, we observe that there are significant differences when learning
from imbalanced datasets, or in new environments where sufficient data is not
available. In particular, we find that deep learning methods perform better on
balanced datasets, but in applications with limited data, e.g., cold start of a
robot in a new environment, or imbalanced classes, pattern-based methods may be
preferable.

</details>


### [695] [COVER:COverage-VErified Roadmaps for Fixed-time Motion Planning in Continuous Semi-Static Environments](https://arxiv.org/abs/2510.03875)
*Niranjan Kumar Ilampooranan,Constantinos Chamzas*

Main category: cs.RO

TL;DR: This paper introduces COVER, a framework to guarantee fixed-time motion planning in semi-static environments, overcoming limitations of prior approaches.


<details>
  <summary>Details</summary>
Motivation: Efficient fixed-time motion planning is crucial for robotic deployment, especially in semi-static environments with predictable obstacle variability.

Method: COVER incrementally partitions obstacle configurations and verifies feasible roadmaps for fixed-time queries in each partition.

Result: Validation on a 7-DOF simulated Panda robot showed COVER's broader coverage and higher query success rates compared to prior methods.

Conclusion: COVER enhances motion planning in semi-static spaces, offering stronger guarantees and better applicability in realistic domains.

Abstract: Having the ability to answer motion-planning queries within a fixed time
budget is critical for the widespread deployment of robotic systems.
Semi-static environments, where most obstacles remain static but a limited set
can vary across queries, exhibit structured variability that can be
systematically exploited to provide stronger guarantees than in general
motion-planning problems. However, prior approaches in this setting either lack
formal guarantees or rely on restrictive discretizations of obstacle
configurations, limiting their applicability in realistic domains. This paper
introduces COVER, a novel framework that incrementally constructs a
coverage-verified roadmap in semi-static environments. By partitioning the
obstacle configuration space and solving for feasible paths within each
partition, COVER systematically verifies feasibility of the roadmap in each
partition and guarantees fixed-time motion planning queries within the verified
regions. We validate COVER with a 7-DOF simulated Panda robot performing table
and shelf tasks, demonstrating that COVER achieves broader coverage with higher
query success rates than prior works.

</details>


### [696] [Seeing the Bigger Picture: 3D Latent Mapping for Mobile Manipulation Policy Learning](https://arxiv.org/abs/2510.03885)
*Sunghwan Kim,Woojeh Chung,Zhirui Dai,Dwait Bhatt,Arth Shukla,Hao Su,Yulun Tian,Nikolay Atanasov*

Main category: cs.RO

TL;DR: This paper introduces Seeing the Bigger Picture (SBP), a method for robot policy learning using a 3D latent map, showing better spatial and temporal reasoning than image-only approaches.


<details>
  <summary>Details</summary>
Motivation: Exploring the limitations of image-based policy learning for robot manipulation tasks and developing a method that utilizes a 3D latent map for improved reasoning over long horizons and unseen environments.

Method: The proposed SBP method uses a 3D latent map created by incrementally fusing multiview observations, employs a pre-trained decoder for scene-agnostic feature reconstruction, and optimizes map features online during tasks. The map serves as a global state variable for policy learning.

Result: Experiments reveal that SBP can globally reason about scenes, leverage the 3D map as long-horizon memory, and outperform image-based policies, with a 25% success rate improvement in sequential manipulation scenarios.

Conclusion: SBP demonstrates the effectiveness of using a 3D latent map in enhancing robot manipulation policies, providing better generalization and task success rates in both familiar and novel environments.

Abstract: In this paper, we demonstrate that mobile manipulation policies utilizing a
3D latent map achieve stronger spatial and temporal reasoning than policies
relying solely on images. We introduce Seeing the Bigger Picture (SBP), an
end-to-end policy learning approach that operates directly on a 3D map of
latent features. In SBP, the map extends perception beyond the robot's current
field of view and aggregates observations over long horizons. Our mapping
approach incrementally fuses multiview observations into a grid of
scene-specific latent features. A pre-trained, scene-agnostic decoder
reconstructs target embeddings from these features and enables online
optimization of the map features during task execution. A policy, trainable
with behavior cloning or reinforcement learning, treats the latent map as a
state variable and uses global context from the map obtained via a 3D feature
aggregator. We evaluate SBP on scene-level mobile manipulation and sequential
tabletop manipulation tasks. Our experiments demonstrate that SBP (i) reasons
globally over the scene, (ii) leverages the map as long-horizon memory, and
(iii) outperforms image-based policies in both in-distribution and novel
scenes, e.g., improving the success rate by 25% for the sequential manipulation
task.

</details>


### [697] [NoTVLA: Narrowing of Dense Action Trajectories for Generalizable Robot Manipulation](https://arxiv.org/abs/2510.03895)
*Zheng Huang,Mingyu Liu,Xiaoyi Lin,Muzhi Zhu,Canyu Zhao,Zongze Du,Xiaoman Li,Yiduo Jia,Hao Zhong,Hao Chen,Chunhua Shen*

Main category: cs.RO

TL;DR: The NoTVLA framework addresses catastrophic forgetting in Vision-Language-Action models by utilizing sparse trajectories rather than dense ones, achieving better multi-task generalization while using fewer resources.


<details>
  <summary>Details</summary>
Motivation: Catastrophic forgetting hinders the deployment of Vision-Language-Action models in real-world scenarios, primarily due to dense trajectory fine-tuning that disrupts task knowledge retention.

Method: NoTVLA employs trajectory planning focused on the robot end-effector, leveraging sparse trajectories through temporal compression and spatial reasoning pruning to train the model.

Result: NoTVLA outperforms the pi0 framework in multi-task evaluations, with less computing power and no wrist-mounted camera, achieving near-expert model accuracy and preserving language capabilities for zero-shot generalization.

Conclusion: NoTVLA enhances multi-task generalization, enables unified deployment across robot platforms, and addresses task perception challenges from novel perspectives without catastrophic forgetting.

Abstract: Vision-Language-Action (VLA) models represent a pivotal advance in embodied
intelligence, yet they confront critical barriers to real-world deployment,
most notably catastrophic forgetting. This issue stems from their overreliance
on continuous action sequences or action chunks, which inadvertently create
isolated data silos that disrupt knowledge retention across tasks. To tackle
these challenges, we propose the Narrowing of Trajectory VLA (NoTVLA)
framework: a novel approach that narrows its focus to sparse trajectories,
thereby avoiding the catastrophic forgetting associated with dense trajectory
fine-tuning. A key innovation of NoTVLA lies in its trajectory planning
strategy: instead of centering on the target object's trajectory, it leverages
temporal compression and spatial reasoning pruning specifically for the robot
end effector's trajectory. Furthermore, training is conducted using these
sparse trajectories rather than dense action trajectories, an optimization that
delivers remarkable practical advantages with better performance in zero-shot.
In multi-task evaluation scenarios, NoTVLA achieves superior performance and
generalization compared to pi0 while operating under two critical constraints:
it uses over an order of magnitude less computing power than pi0 and requires
no wrist-mounted camera. This design ensures that NoTVLA's operational accuracy
closely approximates that of single-task expert models. Crucially, it also
preserves the model's inherent language capabilities, enabling zero-shot
generalization in specific scenarios, supporting unified model deployment
across multiple robot platforms, and fostering a degree of generalization even
when perceiving tasks from novel perspectives.

</details>


### [698] [WAFFLE: A Wearable Approach to Bite Timing Estimation in Robot-Assisted Feeding](https://arxiv.org/abs/2510.03910)
*Akhil Padmanabha,Jessie Yuan,Tanisha Mehta,Rajat Kumar Jenamani,Eric Hu,Victoria de León,Anthony Wertz,Janavi Gupta,Ben Dodson,Yunting Yan,Carmel Majidi,Tapomayukh Bhattacharjee,Zackory Erickson*

Main category: cs.RO

TL;DR: The paper introduces WAFFLE, a wearable system that uses sensor data to predict bite timing for robotic feeding systems accurately. It improves user experience in feeding tasks, especially in individual and social dining contexts.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of robotic feeding systems, particularly the technical challenge of predicting optimal bite timing for users.

Method: Developed WAFFLE, which uses wearable sensors to detect user cues like head movements, chewing, and talking, and employs a supervised regression model to generate predictive commands for bite timing.

Result: WAFFLE demonstrated comparable or better performance than baseline methods in enhancing user experience and reducing workload during robotic feeding. It was preferred by most participants and shown to be effective across diverse conditions and users.

Conclusion: WAFFLE enhances bite timing prediction in robotic feeding systems, benefiting users with and without impairments, and supports further usage in varied environments and contexts.

Abstract: Millions of people around the world need assistance with feeding. Robotic
feeding systems offer the potential to enhance autonomy and quality of life for
individuals with impairments and reduce caregiver workload. However, their
widespread adoption has been limited by technical challenges such as estimating
bite timing, the appropriate moment for the robot to transfer food to a user's
mouth. In this work, we introduce WAFFLE: Wearable Approach For Feeding with
LEarned bite timing, a system that accurately predicts bite timing by
leveraging wearable sensor data to be highly reactive to natural user cues such
as head movements, chewing, and talking. We train a supervised regression model
on bite timing data from 14 participants and incorporate a user-adjustable
assertiveness threshold to convert predictions into proceed or stop commands.
In a study with 15 participants without motor impairments with the Obi feeding
robot, WAFFLE performs statistically on par with or better than baseline
methods across measures of feeling of control, robot understanding, and
workload, and is preferred by the majority of participants for both individual
and social dining. We further demonstrate WAFFLE's generalizability in a study
with 2 participants with motor impairments in their home environments using a
Kinova 7DOF robot. Our findings support WAFFLE's effectiveness in enabling
natural, reactive bite timing that generalizes across users, robot hardware,
robot positioning, feeding trajectories, foods, and both individual and social
dining contexts.

</details>


### [699] [TCB-VIO: Tightly-Coupled Focal-Plane Binary-Enhanced Visual Inertial Odometry](https://arxiv.org/abs/2510.03919)
*Matthew Lisondra,Junseo Kim,Glenn Takashi Shimoda,Kourosh Zareinia,Sajad Saeedi*

Main category: cs.RO

TL;DR: The paper introduces TCB-VIO, a high-frequency visual-inertial odometry framework running on FPSP sensors, addressing spatial and temporal drifts and outperforming existing methods like ROVIO and ORB-SLAM3.


<details>
  <summary>Details</summary>
Motivation: To address issues of latency and drift challenges in visual-inertial odometry systems, leveraging the potential of focal-plane sensor-processor arrays (FPSPs).

Method: Developed a tightly-coupled 6-DOF visual-inertial odometry (VIO) framework using Multi-State Constraint Kalman Filter (MSCKF), achieving high frame rates (250 FPS for FPSP and 400 Hz for IMU measurements).

Result: TCB-VIO demonstrates superior performance compared to state-of-the-art frameworks such as ROVIO, VINS-Mono, and ORB-SLAM3.

Conclusion: TCB-VIO effectively mitigates spatial and temporal drifts in VIO using FPSP, offering a significant improvement in both latency and pose estimation accuracy.

Abstract: Vision algorithms can be executed directly on the image sensor when
implemented on the next-generation sensors known as focal-plane
sensor-processor arrays (FPSP)s, where every pixel has a processor. FPSPs
greatly improve latency, reducing the problems associated with the bottleneck
of data transfer from a vision sensor to a processor. FPSPs accelerate
vision-based algorithms such as visual-inertial odometry (VIO). However, VIO
frameworks suffer from spatial drift due to the vision-based pose estimation,
whilst temporal drift arises from the inertial measurements. FPSPs circumvent
the spatial drift by operating at a high frame rate to match the high-frequency
output of the inertial measurements. In this paper, we present TCB-VIO, a
tightly-coupled 6 degrees-of-freedom VIO by a Multi-State Constraint Kalman
Filter (MSCKF), operating at a high frame-rate of 250 FPS and from IMU
measurements obtained at 400 Hz. TCB-VIO outperforms state-of-the-art methods:
ROVIO, VINS-Mono, and ORB-SLAM3.

</details>


### [700] [A Real-Time Framework for Intermediate Map Construction and Kinematically Feasible Off-Road Planning Without OSM](https://arxiv.org/abs/2510.03948)
*Otobong Jerome,Geesara Prathap Kulathunga,Devitt Dmitry,Eugene Murawjow,Alexandr Klimchik*

Main category: cs.RO

TL;DR: The paper presents a novel path-planning method tailored for off-road autonomous navigation, balancing real-time performance, kinematic feasibility, and memory efficiency.


<details>
  <summary>Details</summary>
Motivation: Off-road navigation is difficult due to complex, unstructured environments, and existing global path-planning methods fail to address essential factors like real-time performance, kinematics, and memory requirements.

Method: The proposed method constructs an intermediate map using off-road geographical features and divides the problem into three sub-problems: graph-based path planning, kinematic feasibility checking, and path smoothing.

Result: Tests on large-scale off-road maps demonstrated the method's efficiency, identifying paths in 1.5 seconds on average while using 1.5GB of memory under extreme conditions.

Conclusion: The framework is effective, versatile, and applicable to various off-road autonomous tasks such as search and rescue and agricultural operations.

Abstract: Off-road environments present unique challenges for autonomous navigation due
to their complex and unstructured nature. Traditional global path-planning
methods, which typically aim to minimize path length and travel time, perform
poorly on large-scale maps and fail to account for critical factors such as
real-time performance, kinematic feasibility, and memory efficiency. This paper
introduces a novel global path-planning method specifically designed for
off-road environments, addressing these essential factors. The method begins by
constructing an intermediate map within the pixel coordinate system,
incorporating geographical features like off-road trails, waterways, restricted
and passable areas, and trees. The planning problem is then divided into three
sub-problems: graph-based path planning, kinematic feasibility checking, and
path smoothing. This approach effectively meets real-time performance
requirements while ensuring kinematic feasibility and efficient memory use. The
method was tested in various off-road environments with large-scale maps up to
several square kilometers in size, successfully identifying feasible paths in
an average of 1.5 seconds and utilizing approximately 1.5GB of memory under
extreme conditions. The proposed framework is versatile and applicable to a
wide range of off-road autonomous navigation tasks, including search and rescue
missions and agricultural operations.

</details>


### [701] [SITCOM: Scaling Inference-Time COMpute for VLAs](https://arxiv.org/abs/2510.04041)
*Ayudh Saxena,Harsh Shah,Sandeep Routray,Rishi Rajesh Shah,Esha Pahwa*

Main category: cs.RO

TL;DR: The paper introduces SITCOM, a framework that enhances Vision-Language-Action (VLA) models by leveraging model-based rollouts and trajectory selection for improved robotic control in long-horizon tasks.


<details>
  <summary>Details</summary>
Motivation: The paper addresses challenges in robotic control such as high data collection costs, generalization to new environments, and effective long-horizon planning, which are limitations of current VLA models.

Method: SITCOM integrates pretrained VLAs with a transformer-based dynamics model, enabling multi-step action rollouts and reward-based trajectory selection to improve decision-making, inspired by Model Predictive Control.

Result: Experimentation in the SIMPLER environment showed that SITCOM, combined with an appropriate reward function, increased task completion rates from 48% to 72%.

Conclusion: SITCOM effectively transforms VLA models into robust, long-horizon planners by introducing model-based rollouts and leveraging a well-trained dynamics model, significantly enhancing task performance.

Abstract: Learning robust robotic control policies remains a major challenge due to the
high cost of collecting labeled data, limited generalization to unseen
environments, and difficulties in planning over long horizons. While
Vision-Language-Action (VLA) models offer a promising solution by grounding
natural language instructions into single-step control commands, they often
lack mechanisms for lookahead and struggle with compounding errors in dynamic
tasks. In this project, we introduce Scaling Inference-Time COMpute for VLAs
(SITCOM), a framework that augments any pretrained VLA with model-based
rollouts and reward-based trajectory selection, inspired by Model Predictive
Control algorithm. SITCOM leverages a learned dynamics model to simulate
multi-step action rollouts to select the best candidate plan for real-world
execution, transforming one-shot VLAs into robust long-horizon planners. We
develop an efficient transformer-based dynamics model trained on large-scale
BridgeV2 data and fine-tuned on SIMPLER environments to bridge the Real2Sim
gap, and score candidate rollouts using rewards from simulator. Through
comprehensive evaluation across multiple tasks and settings in the SIMPLER
environment, we demonstrate that SITCOM when combined with a good reward
function can significantly improve task completion rate from 48% to 72% using
trained dynamics model.

</details>


### [702] [Feedback Matters: Augmenting Autonomous Dissection with Visual and Topological Feedback](https://arxiv.org/abs/2510.04074)
*Chung-Pang Wang,Changwei Chen,Xiao Liang,Soofiyan Atar,Florian Richter,Michael Yip*

Main category: cs.RO

TL;DR: The paper presents a feedback-enabled framework for autonomous tissue dissection, integrating structured analysis with visibility metrics to enhance adaptability, precision, and robustness during dynamic surgical procedures.


<details>
  <summary>Details</summary>
Motivation: Adaptive autonomous surgical systems face challenges due to rapidly evolving tissue properties and visual cues, necessitating a robust feedback mechanism to improve reliability and precision.

Method: The study introduces a framework for topological reasoning from endoscopic images alongside visibility metrics to maximize tissue exposure, integrating these mechanisms with planning-based and learning-based dissection methods.

Result: Experimental results show that the proposed framework enhances system autonomy, reduces errors, and increases robustness in complex surgical scenarios.

Conclusion: The paper concludes that structured feedback and visibility optimization greatly improve adaptability and precision of autonomous surgical systems handling complex dissection tasks, paving the way for more reliable autonomy in surgical procedures.

Abstract: Autonomous surgical systems must adapt to highly dynamic environments where
tissue properties and visual cues evolve rapidly. Central to such adaptability
is feedback: the ability to sense, interpret, and respond to changes during
execution. While feedback mechanisms have been explored in surgical robotics,
ranging from tool and tissue tracking to error detection, existing methods
remain limited in handling the topological and perceptual challenges of tissue
dissection. In this work, we propose a feedback-enabled framework for
autonomous tissue dissection that explicitly reasons about topological changes
from endoscopic images after each dissection action. This structured feedback
guides subsequent actions, enabling the system to localize dissection progress
and adapt policies online. To improve the reliability of such feedback, we
introduce visibility metrics that quantify tissue exposure and formulate
optimal controller designs that actively manipulate tissue to maximize
visibility. Finally, we integrate these feedback mechanisms with both
planning-based and learning-based dissection methods, and demonstrate
experimentally that they significantly enhance autonomy, reduce errors, and
improve robustness in complex surgical scenarios.

</details>


### [703] [From Shadow to Light: Toward Safe and Efficient Policy Learning Across MPC, DeePC, RL, and LLM Agents](https://arxiv.org/abs/2510.04076)
*Amin Vahidi-Moghaddam,Sayed Pedram Haeri Boroujeni,Iman Jebellat,Ehsan Jebellat,Niloufar Mehrabi,Zhaojian Li*

Main category: cs.RO

TL;DR: This paper explores eight approaches to improve the practicality of data-driven control methods like DeePC and ML-based MPC, focusing on reducing computation and memory requirements for real-world applications.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of data-driven control methods, such as slow response times, high computational demands, and large memory needs, particularly in systems with fast dynamics or limited resources.

Method: The paper introduces and evaluates eight techniques, such as reduced-order modeling, function-approximated policy learning, and convex relaxations, to streamline data-driven control methods.

Result: The proposed approaches are shown to enhance computational efficiency and reduce resource requirements in various real-world applications including robotic arms, soft robots, and vehicle motion control.

Conclusion: By applying these methods, data-driven control policies become more viable for real-world scenarios, expanding their utility in fast, resource-constrained environments.

Abstract: One of the main challenges in modern control applications, particularly in
robot and vehicle motion control, is achieving accurate, fast, and safe
movement. To address this, optimal control policies have been developed to
enforce safety while ensuring high performance. Since basic first-principles
models of real systems are often available, model-based controllers are widely
used. Model predictive control (MPC) is a leading approach that optimizes
performance while explicitly handling safety constraints. However, obtaining
accurate models for complex systems is difficult, which motivates data-driven
alternatives. ML-based MPC leverages learned models to reduce reliance on
hand-crafted dynamics, while reinforcement learning (RL) can learn near-optimal
policies directly from interaction data. Data-enabled predictive control
(DeePC) goes further by bypassing modeling altogether, directly learning safe
policies from raw input-output data. Recently, large language model (LLM)
agents have also emerged, translating natural language instructions into
structured formulations of optimal control problems. Despite these advances,
data-driven policies face significant limitations. They often suffer from slow
response times, high computational demands, and large memory needs, making them
less practical for real-world systems with fast dynamics, limited onboard
computing, or strict memory constraints. To address this, various technique,
such as reduced-order modeling, function-approximated policy learning, and
convex relaxations, have been proposed to reduce computational complexity. In
this paper, we present eight such approaches and demonstrate their
effectiveness across real-world applications, including robotic arms, soft
robots, and vehicle motion control.

</details>


### [704] [HEHA: Hierarchical Planning for Heterogeneous Multi-Robot Exploration of Unknown Environments](https://arxiv.org/abs/2510.04161)
*Longrui Yang,Yiyu Wang,Jingfan Tang,Yunpeng Lv,Shizhe Zhao,Chao Cao,Zhongqiang Ren*

Main category: cs.RO

TL;DR: The paper proposes HEHA, a hierarchical approach, to improve path planning for multi-robot exploration of unknown environments while addressing optimization challenges.


<details>
  <summary>Details</summary>
Motivation: To improve path planning efficiency for heterogeneous robots exploring unknown environments while handling traversability constraints and ensuring quick iterative optimization.

Method: Introduces HEHA which integrates global and local planning. The global planning includes the PEAF algorithm for efficient and bounded sub-optimal routing, and the local planning avoids overlap in exploration.

Result: Experimental results show that HEHA reduces exploration time by up to 30% compared to baseline methods.

Conclusion: The hierarchical HEHA approach is effective in optimizing and accelerating autonomous exploration for heterogeneous robots.

Abstract: This paper considers the path planning problem for autonomous exploration of
an unknown environment using multiple heterogeneous robots such as drones,
wheeled, and legged robots, which have different capabilities to traverse
complex terrains. A key challenge there is to intelligently allocate the robots
to the unknown areas to be explored and determine the visiting order of those
spaces subject to traversablity constraints, which leads to a large scale
constrained optimization problem that needs to be quickly and iteratively
solved every time when new space are explored. To address the challenge, we
propose HEHA (Hierarchical Exploration with Heterogeneous Agents) by leveraging
a recent hierarchical method that decompose the exploration into global
planning and local planning. The major contribution in HEHA is its global
planning, where we propose a new routing algorithm PEAF (Partial Anytime Focal
search) that can quickly find bounded sub-optimal solutions to minimize the
maximum path length among the agents subject to traversability constraints.
Additionally, the local planner in HEHA also considers heterogeneity to avoid
repeated and duplicated exploration among the robots. The experimental results
show that, our HEHA can reduce up to 30% of the exploration time than the
baselines.

</details>


### [705] [Learning to Capture Rocks using an Excavator: A Reinforcement Learning Approach with Guiding Reward Formulation](https://arxiv.org/abs/2510.04168)
*Amirmasoud Molaei,Reza Ghabcheloo*

Main category: cs.RO

TL;DR: This paper introduces a data-driven control framework using reinforcement learning to autonomously capture rocks using standard excavator buckets, achieving human-level success rates without specialized grippers or explicit modeling.


<details>
  <summary>Details</summary>
Motivation: Rock capturing with excavators is challenging due to the irregular shapes and unstructured environments, which also involve complex contact dynamics, making model-based control methods impractical.

Method: The paper uses model-free reinforcement learning (PPO algorithm) in simulation to train an agent that controls excavator bucket movements, incorporating domain randomization to enhance robustness across rock geometry, density, and mass.

Result: The RL-trained policy generalizes well to unseen scenarios, achieves success rates comparable to human operators, and maintains stability across varying soil conditions.

Conclusion: This study demonstrates the potential of RL-based excavation strategies for manipulating discrete objects without requiring specialized hardware, paving the way for more autonomous construction tasks.

Abstract: Rock capturing with standard excavator buckets is a challenging task
typically requiring the expertise of skilled operators. Unlike soil digging, it
involves manipulating large, irregular rocks in unstructured environments where
complex contact interactions with granular material make model-based control
impractical. Existing autonomous excavation methods focus mainly on continuous
media or rely on specialized grippers, limiting their applicability to
real-world construction sites. This paper introduces a fully data-driven
control framework for rock capturing that eliminates the need for explicit
modeling of rock or soil properties. A model-free reinforcement learning agent
is trained in the AGX Dynamics simulator using the Proximal Policy Optimization
(PPO) algorithm and a guiding reward formulation. The learned policy outputs
joint velocity commands directly to the boom, arm, and bucket of a CAT365
excavator model. Robustness is enhanced through extensive domain randomization
of rock geometry, density, and mass, as well as the initial configurations of
the bucket, rock, and goal position. To the best of our knowledge, this is the
first study to develop and evaluate an RL-based controller for the rock
capturing task. Experimental results show that the policy generalizes well to
unseen rocks and varying soil conditions, achieving high success rates
comparable to those of human participants while maintaining machine stability.
These findings demonstrate the feasibility of learning-based excavation
strategies for discrete object manipulation without requiring specialized
hardware or detailed material models.

</details>


### [706] [VBM-NET: Visual Base Pose Learning for Mobile Manipulation using Equivariant TransporterNet and GNNs](https://arxiv.org/abs/2510.04171)
*Lakshadeep Naik,Adam Fischer,Daniel Duberg,Danica Kragic*

Main category: cs.RO

TL;DR: The paper introduces VBM-NET, a learning-based framework for optimal base pose selection in mobile manipulation using top-down scene projections, exploiting spatial symmetries with TransporterNet and optimizing poses with graph neural networks and reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: Improve mobile manipulation efficiency by addressing mobile base pose selection, eliminating reliance on precise state information and leveraging global scene projections.

Method: VBM-NET combines spatial symmetry exploitation (TransporterNet), graph neural networks for multi-candidate representation, and reinforcement learning for pose optimization.

Result: VBM-NET achieved comparable results to classical methods with reduced computation time and demonstrated effective sim-to-real policy transfer.

Conclusion: VBM-NET validates the feasibility of learning-based approaches for mobile manipulation tasks, offering computational efficiency and real-world applicability.

Abstract: In Mobile Manipulation, selecting an optimal mobile base pose is essential
for successful object grasping. Previous works have addressed this problem
either through classical planning methods or by learning state-based policies.
They assume access to reliable state information, such as the precise object
poses and environment models. In this work, we study base pose planning
directly from top-down orthographic projections of the scene, which provide a
global overview of the scene while preserving spatial structure. We propose
VBM-NET, a learning-based method for base pose selection using such top-down
orthographic projections. We use equivariant TransporterNet to exploit spatial
symmetries and efficiently learn candidate base poses for grasping. Further, we
use graph neural networks to represent a varying number of candidate base poses
and use Reinforcement Learning to determine the optimal base pose among them.
We show that VBM-NET can produce comparable solutions to the classical methods
in significantly less computation time. Furthermore, we validate sim-to-real
transfer by successfully deploying a policy trained in simulation to real-world
mobile manipulation.

</details>


### [707] [Using Robotics to Improve Transcatheter Edge-to-Edge Repair of the Mitral Valve](https://arxiv.org/abs/2510.04178)
*Léa Pistorius,Namrata U. Nayar,Phillip Tran,Sammy Elmariah,Pierre E. Dupont*

Main category: cs.RO

TL;DR: This paper explores robotics-enhanced transcatheter mitral valve repair, showing improved accuracy and efficiency over manual systems.


<details>
  <summary>Details</summary>
Motivation: Manual transcatheter valve repair is limited by mechanical challenges and a steep learning curve.

Method: A robotic joint-based control system using a game controller was compared to manual control in a phantom heart model.

Result: The robotic system reduced procedural time, motion errors, and improved clip placement accuracy.

Conclusion: Robotic assistance offers a more reliable and user-friendly solution for complex valve repair procedures.

Abstract: Transcatheter valve repair presents significant challenges due to the
mechanical limitations and steep learning curve associated with manual catheter
systems. This paper investigates the use of robotics to facilitate
transcatheter procedures in the context of mitral valve edge-to-edge repair.
The complex handle-based control of a clinical repair device is replaced by
intuitive robotic joint-based control via a game controller. Manual versus
robotic performance is analyzed by decomposing the overall device delivery task
into motion-specific steps and comparing capabilities on a step-by-step basis
in a phantom model of the heart and vasculature. Metrics include procedure
duration and clip placement accuracy. Results demonstrate that the robotic
system can reduce procedural time and motion errors while also improving
accuracy of clip placement. These findings suggest that robotic assistance can
address key limitations of manual systems, offering a more reliable and
user-friendly platform for complex transcatheter procedures.

</details>


### [708] [Zenbo Patrol: A Social Assistive Robot Based on Multimodal Deep Learning for Real-time Illegal Parking Recognition and Notification](https://arxiv.org/abs/2510.04190)
*Jian-jie Zheng,Chih-kai Yang,Po-han Chen,Lyn Chao-ling Chen*

Main category: cs.RO

TL;DR: The paper presents a social robot that utilizes GPT-4o multimodal model for real-time illegal parking detection and license plate recognition in indoor parking scenarios.


<details>
  <summary>Details</summary>
Motivation: To address illegal parking in indoor parking lots and demonstrate the application of a novel multimodal deep learning method in practical scenarios.

Method: The robot employs the GPT-4o multimodal model without preprocessing for license plate recognition. It navigates a simulated parking lot, autonomously adjusts its camera angle, captures images, and recognizes license plate numbers. Upon detecting an illegal parking event, the robot sends Line messages to notify the system manager.

Result: The multimodal deep learning method achieved high accuracy in license plate recognition, and the robot successfully functioned as a patrol in real-time illegal parking detection.

Conclusion: This study showcases effective integration of a social assistive robot and multimodal deep learning, providing a practical solution to illegal parking problems and validating the approach in a real-world scenario.

Abstract: In the study, the social robot act as a patrol to recognize and notify
illegal parking in real-time. Dual-model pipeline method and large multimodal
model were compared, and the GPT-4o multimodal model was adopted in license
plate recognition without preprocessing. For moving smoothly on a flat ground,
the robot navigated in a simulated parking lot in the experiments. The robot
changes angle view of the camera automatically to capture the images around
with the format of license plate number. From the captured images of the robot,
the numbers on the plate are recognized through the GPT-4o model, and
identifies legality of the numbers. When an illegal parking is detected, the
robot sends Line messages to the system manager immediately. The contribution
of the work is that a novel multimodal deep learning method has validated with
high accuracy in license plate recognition, and a social assistive robot is
also provided for solving problems in a real scenario, and can be applied in an
indoor parking lot.

</details>


### [709] [Flexible Locomotion Learning with Diffusion Model Predictive Control](https://arxiv.org/abs/2510.04234)
*Runhan Huang,Haldun Balim,Heng Yang,Yilun Du*

Main category: cs.RO

TL;DR: The paper introduces Diffusion-MPC, a method combining diffusion-based generative models and Model Predictive Control for adaptive locomotion. It aims to overcome the limitations of fixed RL policies and classical MPC dynamics dependencies.


<details>
  <summary>Details</summary>
Motivation: To address the shortcomings of model-free RL's inflexibility during test time and classical MPC's reliance on precise dynamics models, when applied in the domain of legged locomotion.

Method: The authors combine diffusion generative modeling with MPC by using a learned dynamics prior for planning. The method involves joint prediction of states and actions, incorporating task-specific rewards and physical constraints at each planning step, and utilizing interactive training for adaptation.

Result: Diffusion-MPC is shown to successfully enable robust locomotion and adaptable behavior in real-world environments without the need for retraining.

Conclusion: Diffusion-MPC presents a solution that combines the adaptability and flexibility of MPC with learned generative dynamics, enabling efficient adaptation to new tasks and constraints during test time.

Abstract: Legged locomotion demands controllers that are both robust and adaptable,
while remaining compatible with task and safety considerations. However,
model-free reinforcement learning (RL) methods often yield a fixed policy that
can be difficult to adapt to new behaviors at test time. In contrast, Model
Predictive Control (MPC) provides a natural approach to flexible behavior
synthesis by incorporating different objectives and constraints directly into
its optimization process. However, classical MPC relies on accurate dynamics
models, which are often difficult to obtain in complex environments and
typically require simplifying assumptions. We present Diffusion-MPC, which
leverages a learned generative diffusion model as an approximate dynamics prior
for planning, enabling flexible test-time adaptation through reward and
constraint based optimization. Diffusion-MPC jointly predicts future states and
actions; at each reverse step, we incorporate reward planning and impose
constraint projection, yielding trajectories that satisfy task objectives while
remaining within physical limits. To obtain a planning model that adapts beyond
imitation pretraining, we introduce an interactive training algorithm for
diffusion based planner: we execute our reward-and-constraint planner in
environment, then filter and reweight the collected trajectories by their
realized returns before updating the denoiser. Our design enables strong
test-time adaptability, allowing the planner to adjust to new reward
specifications without retraining. We validate Diffusion-MPC on real world,
demonstrating strong locomotion and flexible adaptation.

</details>


### [710] [ContextVLA: Vision-Language-Action Model with Amortized Multi-Frame Context](https://arxiv.org/abs/2510.04246)
*Huiwon Jang,Sihyun Yu,Heeseung Kwon,Hojin Jeon,Younggyo Seo,Jinwoo Shin*

Main category: cs.RO

TL;DR: The paper introduces ContextVLA, a policy model leveraging temporal context in robotic tasks via compressing multi-frame observations, improving efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: Current behavior cloning models show inconsistent gains with multi-frame contexts, especially due to computational inefficiencies of Vision-Language-Action (VLA) models.

Method: The authors developed ContextVLA, a model that compresses past observations into a single context token for efficient multi-frame action generation.

Result: Experiments demonstrate that ContextVLA enhances performance over single-frame models while reducing the computational overhead of multi-frame training.

Conclusion: ContextVLA achieves efficient and robust improvement in robotic task performance by effectively extracting meaningful temporal context from multi-frame observations.

Abstract: Leveraging temporal context is crucial for success in partially observable
robotic tasks. However, prior work in behavior cloning has demonstrated
inconsistent performance gains when using multi-frame observations. In this
paper, we introduce ContextVLA, a policy model that robustly improves robotic
task performance by effectively leveraging multi-frame observations. Our
approach is motivated by the key observation that Vision-Language-Action models
(VLA), i.e., policy models built upon a Vision-Language Model (VLM), more
effectively utilize multi-frame observations for action generation. This
suggests that VLMs' inherent temporal understanding capability enables them to
extract more meaningful context from multi-frame observations. However, the
high dimensionality of video inputs introduces significant computational
overhead, making VLA training and inference inefficient. To address this,
ContextVLA compresses past observations into a single context token, allowing
the policy to efficiently leverage temporal context for action generation. Our
experiments show that ContextVLA consistently improves over single-frame VLAs
and achieves the benefits of full multi-frame training but with reduced
training and inference times.

</details>


### [711] [Integrated Planning and Control on Manifolds: Factor Graph Representation and Toolkit](https://arxiv.org/abs/2510.04278)
*Peiwen Yang,Weisong Wen,Runqiu Yang,Yuanyuan Zhang,Jiahao Hu,Yingming Chen,Naigui Xiao,Jiaqi Zhao*

Main category: cs.RO

TL;DR: The paper introduces FactorMPC, an advanced factor-graph-based MPC framework that excels in systems involving nonlinear manifolds, offering improvements in performance, safety, and usability.


<details>
  <summary>Details</summary>
Motivation: Address the limitations faced by traditional model predictive control (MPC) when applied to systems with nonlinear manifolds, such as issues with singularities and over-parameterization.

Method: Developed FactorMPC, a factor-graph-based framework that integrates system dynamics, constraints, and objectives. It introduces manifold-valued state modeling, sparsity exploitation, and on-manifold control barrier function (CBF) factors for real-time safety-critical applications.

Result: Simulations and real-world experiments on a quadrotor showed superior trajectory tracking and obstacle avoidance compared to conventional methods.

Conclusion: FactorMPC provides a scalable and geometrically consistent solution tailored for robotic systems on nonlinear manifolds, boasting real-time efficiency, safety, and practical usability with open-source accessibility.

Abstract: Model predictive control (MPC) faces significant limitations when applied to
systems evolving on nonlinear manifolds, such as robotic attitude dynamics and
constrained motion planning, where traditional Euclidean formulations struggle
with singularities, over-parameterization, and poor convergence. To overcome
these challenges, this paper introduces FactorMPC, a factor-graph based MPC
toolkit that unifies system dynamics, constraints, and objectives into a
modular, user-friendly, and efficient optimization structure. Our approach
natively supports manifold-valued states with Gaussian uncertainties modeled in
tangent spaces. By exploiting the sparsity and probabilistic structure of
factor graphs, the toolkit achieves real-time performance even for
high-dimensional systems with complex constraints. The velocity-extended
on-manifold control barrier function (CBF)-based obstacle avoidance factors are
designed for safety-critical applications. By bridging graphical models with
safety-critical MPC, our work offers a scalable and geometrically consistent
framework for integrated planning and control. The simulations and experimental
results on the quadrotor demonstrate superior trajectory tracking and obstacle
avoidance performance compared to baseline methods. To foster research
reproducibility, we have provided open-source implementation offering
plug-and-play factors.

</details>


### [712] [Stability-Aware Retargeting for Humanoid Multi-Contact Teleoperation](https://arxiv.org/abs/2510.04353)
*Stephen McCrory,Romeo Orsolino,Dhruv Thanki,Luigi Penco,Robert Griffin*

Main category: cs.RO

TL;DR: The paper addresses teleoperation challenges for humanoid robots during tasks involving hand contacts and non-coplanar surfaces by proposing a stability-based retargeting method. This approach adjusts contact points and postures dynamically to enhance stability.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome stability and torque limitations in humanoid robot teleoperation, especially in unstable scenarios such as using hand contacts on non-coplanar surfaces.

Method: The method involves a centroidal stability-based retargeting framework that uses an efficient analytical gradient calculation of the stability margin to adjust control setpoints dynamically during teleoperation.

Result: The framework was validated in simulation and hardware, demonstrating improved stability margins, better impulse resilience, and enhanced joint torque margins during teleoperation tasks.

Conclusion: The proposed method successfully enhances stability and operational capability for humanoid robots in challenging teleoperation scenarios, enabling better task execution.

Abstract: Teleoperation is a powerful method to generate reference motions and enable
humanoid robots to perform a broad range of tasks. However, teleoperation
becomes challenging when using hand contacts and non-coplanar surfaces, often
leading to motor torque saturation or loss of stability through slipping. We
propose a centroidal stability-based retargeting method that dynamically
adjusts contact points and posture during teleoperation to enhance stability in
these difficult scenarios. Central to our approach is an efficient analytical
calculation of the stability margin gradient. This gradient is used to identify
scenarios for which stability is highly sensitive to teleoperation setpoints
and inform the local adjustment of these setpoints. We validate the framework
in simulation and hardware by teleoperating manipulation tasks on a humanoid,
demonstrating increased stability margins. We also demonstrate empirically that
higher stability margins correlate with improved impulse resilience and joint
torque margin.

</details>


### [713] [Reliable and Scalable Robot Policy Evaluation with Imperfect Simulators](https://arxiv.org/abs/2510.04354)
*Apurva Badithela,David Snyder,Lihan Zha,Joseph Mikhail,Matthew O'Kelly,Anushri Dixit,Anirudha Majumdar*

Main category: cs.RO

TL;DR: The paper introduces SureSim, a framework combining simulation and real-world tests for reliable robot policy performance evaluation.


<details>
  <summary>Details</summary>
Motivation: Improve the evaluation reliability of robot manipulation policies by addressing limitations in small-scale real-world testing and simulation biases.

Method: The SureSim framework integrates small paired real-simulation trials with large-scale simulation, using prediction-powered inference and mean estimation for confidence intervals.

Result: SureSim reduces hardware evaluation efforts by 20-25% while achieving similar confidence bounds on policy performance.

Conclusion: SureSim offers an efficient, statistically supported method for assessing robot policies, combining the strengths of simulation and real-world testing.

Abstract: Rapid progress in imitation learning, foundation models, and large-scale
datasets has led to robot manipulation policies that generalize to a wide-range
of tasks and environments. However, rigorous evaluation of these policies
remains a challenge. Typically in practice, robot policies are often evaluated
on a small number of hardware trials without any statistical assurances. We
present SureSim, a framework to augment large-scale simulation with relatively
small-scale real-world testing to provide reliable inferences on the real-world
performance of a policy. Our key idea is to formalize the problem of combining
real and simulation evaluations as a prediction-powered inference problem, in
which a small number of paired real and simulation evaluations are used to
rectify bias in large-scale simulation. We then leverage non-asymptotic mean
estimation algorithms to provide confidence intervals on mean policy
performance. Using physics-based simulation, we evaluate both diffusion policy
and multi-task fine-tuned \(\pi_0\) on a joint distribution of objects and
initial conditions, and find that our approach saves over \(20-25\%\) of
hardware evaluation effort to achieve similar bounds on policy performance.

</details>


### [714] [PAD-TRO: Projection-Augmented Diffusion for Direct Trajectory Optimization](https://arxiv.org/abs/2510.04436)
*Jushan Chen,Santiago Paternain*

Main category: cs.RO

TL;DR: The paper introduces a novel model-based diffusion approach for trajectory optimization that enforces dynamic feasibility and outperforms a recent state-of-the-art method.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in diffusion-based trajectory optimization, specifically enforcing dynamic feasibility for nonlinear equality constraints, as current methods often lead to sub-optimal solutions.

Method: The approach proposes a direct trajectory optimization framework generating state sequences using model-based diffusion. It incorporates a gradient-free projection mechanism for ensuring dynamic feasibility during the reverse diffusion process.

Result: The proposed method achieves zero dynamic feasibility error and approximately a 4x higher success rate in dense static obstacle navigation compared to a recent state-of-the-art baseline.

Conclusion: The novel framework effectively addresses dynamic feasibility challenges in diffusion-based trajectory optimization, demonstrating superiority in accuracy and success rate over existing methods.

Abstract: Recently, diffusion models have gained popularity and attention in trajectory
optimization due to their capability of modeling multi-modal probability
distributions. However, addressing nonlinear equality constraints, i.e, dynamic
feasi- bility, remains a great challenge in diffusion-based trajectory
optimization. Recent diffusion-based trajectory optimization frameworks rely on
a single-shooting style approach where the denoised control sequence is applied
to forward propagate the dynamical system, which cannot explicitly enforce
constraints on the states and frequently leads to sub-optimal solutions. In
this work, we propose a novel direct trajectory optimization approach via
model-based diffusion, which directly generates a sequence of states. To ensure
dynamic feasibility, we propose a gradient-free projection mechanism that is
incorporated into the reverse diffusion process. Our results show that,
compared to a recent state-of-the-art baseline, our approach leads to zero
dynamic feasibility error and approximately 4x higher success rate in a
quadrotor waypoint navigation scenario involving dense static obstacles.

</details>


### [715] [Velocity-Form Data-Enabled Predictive Control of Soft Robots under Unknown External Payloads](https://arxiv.org/abs/2510.04509)
*Huanqing Wang,Kaixiang Zhang,Kyungjoon Lee,Yu Mei,Vaibhav Srivastava,Jun Sheng,Ziyou Song,Zhaojian Li*

Main category: cs.RO

TL;DR: The paper presents a novel velocity-form data-enabled predictive control (DeePC) method for soft robots that is robust against unknown payloads, demonstrated experimentally with superior performance.


<details>
  <summary>Details</summary>
Motivation: Unknown external payloads and disturbances can alter soft robot dynamics and degrade control performance during object manipulation, necessitating a robust method.

Method: This paper introduces a velocity-form DeePC framework that uses an incremental data-driven approach to control soft robots without needing weighted datasets or disturbance estimators.

Result: The proposed method was experimentally validated on a planar soft robot and showed better performance than standard DeePC in handling scenarios with unknown payloads.

Conclusion: The velocity-form DeePC effectively mitigates performance degradation caused by unknown payloads, offering a robust solution for controlling soft robots.

Abstract: Data-driven control methods such as data-enabled predictive control (DeePC)
have shown strong potential in efficient control of soft robots without
explicit parametric models. However, in object manipulation tasks, unknown
external payloads and disturbances can significantly alter the system dynamics
and behavior, leading to offset error and degraded control performance. In this
paper, we present a novel velocity-form DeePC framework that achieves robust
and optimal control of soft robots under unknown payloads. The proposed
framework leverages input-output data in an incremental representation to
mitigate performance degradation induced by unknown payloads, eliminating the
need for weighted datasets or disturbance estimators. We validate the method
experimentally on a planar soft robot and demonstrate its superior performance
compared to standard DeePC in scenarios involving unknown payloads.

</details>


### [716] [Everything-Grasping (EG) Gripper: A Universal Gripper with Synergistic Suction-Grasping Capabilities for Cross-Scale and Cross-State Manipulation](https://arxiv.org/abs/2510.04585)
*Jianshu Zhou,Jing Shu,Tianle Pan,Puchen Zhu,Jiajun An,Huayu Zhang,Junda Huang,Upinder Kaur,Xin Ma,Masayoshi Tomizuka*

Main category: cs.RO

TL;DR: The paper introduces a novel soft robotic gripper, the Everything-Grasping (EG) Gripper, capable of manipulating both solid and liquid objects across varied scales without requiring airtight sealing.


<details>
  <summary>Details</summary>
Motivation: Robots face challenges in grasping objects of varying sizes and physical states, including both solids and liquids, with unified end-effectors.

Method: The EG Gripper synergistically uses distributed surface suction and granular jamming, alongside a tactile sensing framework with liquid detection and suction feedback, facilitating robust cross-scale and cross-state manipulation.

Result: The EG Gripper demonstrated robust performance across diverse tasks, successfully grasping objects spanning sub-millimeter to large scales, such as glass beads and A4 paper.

Conclusion: This innovation is the first soft gripper that reliably grasps both solid and liquid objects across scales using a unified architecture, advancing capabilities in soft robotics.

Abstract: Grasping objects across vastly different sizes and physical states-including
both solids and liquids-with a single robotic gripper remains a fundamental
challenge in soft robotics. We present the Everything-Grasping (EG) Gripper, a
soft end-effector that synergistically integrates distributed surface suction
with internal granular jamming, enabling cross-scale and cross-state
manipulation without requiring airtight sealing at the contact interface with
target objects. The EG Gripper can handle objects with surface areas ranging
from sub-millimeter scale 0.2 mm2 (glass bead) to over 62,000 mm2 (A4 sized
paper and woven bag), enabling manipulation of objects nearly 3,500X smaller
and 88X larger than its own contact area (approximated at 707 mm2 for a 30
mm-diameter base). We further introduce a tactile sensing framework that
combines liquid detection and pressure-based suction feedback, enabling
real-time differentiation between solid and liquid targets. Guided by the
actile-Inferred Grasping Mode Selection (TIGMS) algorithm, the gripper
autonomously selects grasping modes based on distributed pressure and voltage
signals. Experiments across diverse tasks-including underwater grasping,
fragile object handling, and liquid capture-demonstrate robust and repeatable
performance. To our knowledge, this is the first soft gripper to reliably grasp
both solid and liquid objects across scales using a unified compliant
architecture.

</details>


### [717] [MobRT: A Digital Twin-Based Framework for Scalable Learning in Mobile Manipulation](https://arxiv.org/abs/2510.04592)
*Yilin Mei,Peng Qiu,Wei Zhang,WenChao Zhang,Wenjie Song*

Main category: cs.RO

TL;DR: This paper introduces MobRT, a digital twin-based framework to generate high-quality demonstration data for mobile manipulation tasks, aiding in policy generalization and performance for robots.


<details>
  <summary>Details</summary>
Motivation: Current robotic imitation learning faces challenges due to the difficulty of acquiring high-quality demonstration data for mobile manipulators in complex environments. Existing research often focuses on simpler tabletop scenarios.

Method: The authors developed MobRT, integrating virtual kinematic control and whole-body motion planning to autonomously produce realistic demonstrations for diverse mobile manipulation tasks.

Result: MobRT-generated data was evaluated using baseline algorithms, showing strong correlation between task performance and data quantity. Real-world experiments confirmed improvements in policy generalization and robot performance.

Conclusion: MobRT successfully bridges the gap in mobile manipulation research by enabling robust simulation and coherent demonstration generation, advancing real-world applicability of robotic learning systems.

Abstract: Recent advances in robotics have been largely driven by imitation learning,
which depends critically on large-scale, high-quality demonstration data.
However, collecting such data remains a significant challenge-particularly for
mobile manipulators, which must coordinate base locomotion and arm manipulation
in high-dimensional, dynamic, and partially observable environments.
Consequently, most existing research remains focused on simpler tabletop
scenarios, leaving mobile manipulation relatively underexplored. To bridge this
gap, we present \textit{MobRT}, a digital twin-based framework designed to
simulate two primary categories of complex, whole-body tasks: interaction with
articulated objects (e.g., opening doors and drawers) and mobile-base
pick-and-place operations. \textit{MobRT} autonomously generates diverse and
realistic demonstrations through the integration of virtual kinematic control
and whole-body motion planning, enabling coherent and physically consistent
execution. We evaluate the quality of \textit{MobRT}-generated data across
multiple baseline algorithms, establishing a comprehensive benchmark and
demonstrating a strong correlation between task success and the number of
generated trajectories. Experiments integrating both simulated and real-world
demonstrations confirm that our approach markedly improves policy
generalization and performance, achieving robust results in both simulated and
real-world environments.

</details>


### [718] [OKVIS2-X: Open Keyframe-based Visual-Inertial SLAM Configurable with Dense Depth or LiDAR, and GNSS](https://arxiv.org/abs/2510.04612)
*Simon Boche,Jaehyung Jung,Sebastián Barbas Laina,Stefan Leutenegger*

Main category: cs.RO

TL;DR: OKVIS2-X is a real-time SLAM system offering high accuracy, robust state estimation, and scalable dense volumetric mapping for mobile robots.


<details>
  <summary>Details</summary>
Motivation: To enhance mobile robot navigation by combining high state estimation accuracy with robust and scalable mapping capabilities in real-time.

Method: Developing OKVIS2-X, a multi-sensor SLAM framework that integrates various sensor modalities and employs submapping strategies to maintain scalability and map alignment factors for accuracy.

Result: OKVIS2-X sets benchmarks in trajectory accuracy, outperforms competitors in multiple evaluations (e.g., EuRoC, Hilti22), and provides state-of-the-art performance on large-scale datasets like VBR.

Conclusion: By integrating multi-sensor data with advanced mapping and calibration techniques, OKVIS2-X delivers superior real-time SLAM performance, enabling practical navigation solutions for mobile robots.

Abstract: To empower mobile robots with usable maps as well as highest state estimation
accuracy and robustness, we present OKVIS2-X: a state-of-the-art multi-sensor
Simultaneous Localization and Mapping (SLAM) system building dense volumetric
occupancy maps, while scalable to large environments and operating in realtime.
Our unified SLAM framework seamlessly integrates different sensor modalities:
visual, inertial, measured or learned depth, LiDAR and Global Navigation
Satellite System (GNSS) measurements. Unlike most state-of-the-art SLAM
systems, we advocate using dense volumetric map representations when leveraging
depth or range-sensing capabilities. We employ an efficient submapping strategy
that allows our system to scale to large environments, showcased in sequences
of up to 9 kilometers. OKVIS2-X enhances its accuracy and robustness by
tightly-coupling the estimator and submaps through map alignment factors. Our
system provides globally consistent maps, directly usable for autonomous
navigation. To further improve the accuracy of OKVIS2-X, we also incorporate
the option of performing online calibration of camera extrinsics. Our system
achieves the highest trajectory accuracy in EuRoC against state-of-the-art
alternatives, outperforms all competitors in the Hilti22 VI-only benchmark,
while also proving competitive in the LiDAR version, and showcases state of the
art accuracy in the diverse and large-scale sequences from the VBR dataset.

</details>


### [719] [Bio-Inspired Robotic Houbara: From Development to Field Deployment for Behavioral Studies](https://arxiv.org/abs/2510.04692)
*Lyes Saad Saoud,Irfan Hussain*

Main category: cs.RO

TL;DR: The paper introduces a biomimetic robotic platform that mimics the morphology and behavior of the female Houbara bustard for ecological studies, featuring innovations in digital fabrication, perception, and mobility for effective outdoor operation.


<details>
  <summary>Details</summary>
Motivation: To overcome challenges in studying avian behavior in the wild, such as the need for realistic morphology, durable operations in the field, and intelligent perception for uncontrolled environments.

Method: The platform uses digitally replicable fabrication (3D scanning, CAD modeling, 3D printing, UV texturing), a six-wheeled rocker-bogie chassis for terrain mobility, and an NVIDIA Jetson module for real-time RGB and thermal perception, YOLO-based detection, and autonomous visual servoing.

Result: Field trials demonstrated successful operation of 15-22 FPS with latency under 100 ms, achieving natural recognition and interaction with live Houbara bustards under outdoor desert conditions.

Conclusion: This biomimetic robotic system provides an effective, reproducible framework for advancing ecological and conservation-oriented research, uniting durable design, visual intelligence, and environmental validation, making it transferable to other applications.

Abstract: Biomimetic intelligence and robotics are transforming field ecology by
enabling lifelike robotic surrogates that interact naturally with animals under
real world conditions. Studying avian behavior in the wild remains challenging
due to the need for highly realistic morphology, durable outdoor operation, and
intelligent perception that can adapt to uncontrolled environments. We present
a next generation bio inspired robotic platform that replicates the morphology
and visual appearance of the female Houbara bustard to support controlled
ethological studies and conservation oriented field research. The system
introduces a fully digitally replicable fabrication workflow that combines high
resolution structured light 3D scanning, parametric CAD modelling, articulated
3D printing, and photorealistic UV textured vinyl finishing to achieve
anatomically accurate and durable robotic surrogates. A six wheeled rocker
bogie chassis ensures stable mobility on sand and irregular terrain, while an
embedded NVIDIA Jetson module enables real time RGB and thermal perception,
lightweight YOLO based detection, and an autonomous visual servoing loop that
aligns the robot's head toward detected targets without human intervention. A
lightweight thermal visible fusion module enhances perception in low light
conditions. Field trials in desert aviaries demonstrated reliable real time
operation at 15 to 22 FPS with latency under 100 ms and confirmed that the
platform elicits natural recognition and interactive responses from live
Houbara bustards under harsh outdoor conditions. This integrated framework
advances biomimetic field robotics by uniting reproducible digital fabrication,
embodied visual intelligence, and ecological validation, providing a
transferable blueprint for animal robot interaction research, conservation
robotics, and public engagement.

</details>


### [720] [Building Gradient by Gradient: Decentralised Energy Functions for Bimanual Robot Assembly](https://arxiv.org/abs/2510.04696)
*Alexander L. Mitchell,Joe Watson,Ingmar Posner*

Main category: cs.RO

TL;DR: This paper presents a decentralized gradient-based framework for efficient and adaptive planning in bimanual assembly tasks.


<details>
  <summary>Details</summary>
Motivation: Address the challenges in bimanual assembly, such as high-level sequencing, multi-robot coordination, and the need for rapid replanning during tight-tolerance tasks.

Method: Introduces a decentralized gradient-based framework using a piecewise continuous energy function composed of adaptive potential functions for sub-goal generation via myopic optimization.

Result: Demonstrates that the framework effectively handles bimanual assembly tasks, including tight-tolerance assemblies, with emergent behaviors like retries, coordinated motions, and autonomous handovers.

Conclusion: The proposed framework simplifies task planning, enhances flexibility for replanning, and offers robust adaptability to disturbances in bimanual assembly tasks.

Abstract: There are many challenges in bimanual assembly, including high-level
sequencing, multi-robot coordination, and low-level, contact-rich operations
such as component mating. Task and motion planning (TAMP) methods, while
effective in this domain, may be prohibitively slow to converge when adapting
to disturbances that require new task sequencing and optimisation. These events
are common during tight-tolerance assembly, where difficult-to-model dynamics
such as friction or deformation require rapid replanning and reattempts.
Moreover, defining explicit task sequences for assembly can be cumbersome,
limiting flexibility when task replanning is required. To simplify this
planning, we introduce a decentralised gradient-based framework that uses a
piecewise continuous energy function through the automatic composition of
adaptive potential functions. This approach generates sub-goals using only
myopic optimisation, rather than long-horizon planning. It demonstrates
effectiveness at solving long-horizon tasks due to the structure and adaptivity
of the energy function. We show that our approach scales to physical bimanual
assembly tasks for constructing tight-tolerance assemblies. In these
experiments, we discover that our gradient-based rapid replanning framework
generates automatic retries, coordinated motions and autonomous handovers in an
emergent fashion.

</details>


### [721] [Performance-guided Task-specific Optimization for Multirotor Design](https://arxiv.org/abs/2510.04724)
*Etor Arza,Welf Rehberg,Philipp Weiss,Mihir Kulkarni,Kostas Alexis*

Main category: cs.RO

TL;DR: Optimizes drone designs using reinforcement learning and other algorithms to improve task-specific performance, validated through real-world testing.


<details>
  <summary>Details</summary>
Motivation: Improve aerial robot design for specific tasks by addressing manufacturability and aerodynamic constraints.

Method: Combines reinforcement learning, Bayesian optimization, and evolution strategies to optimize motor configurations.

Result: Better performance in agile waypoint navigation compared to traditional and fully actuated designs; validated with real-world testing.

Conclusion: The methodology enables systematic and effective optimization of multirotor designs, demonstrating both improved task performance and sim2real transferability.

Abstract: This paper introduces a methodology for task-specific design optimization of
multirotor Micro Aerial Vehicles. By leveraging reinforcement learning,
Bayesian optimization, and covariance matrix adaptation evolution strategy, we
optimize aerial robot designs guided exclusively by their closed-loop
performance in a considered task. Our approach systematically explores the
design space of motor pose configurations while ensuring manufacturability
constraints and minimal aerodynamic interference. Results demonstrate that
optimized designs achieve superior performance compared to conventional
multirotor configurations in agile waypoint navigation tasks, including against
fully actuated designs from the literature. We build and test one of the
optimized designs in the real world to validate the sim2real transferability of
our approach.

</details>


### [722] [Online automatic code generation for robot swarms: LLMs and self-organizing hierarchy](https://arxiv.org/abs/2510.04774)
*Weixu Zhu,Marco Dorigo,Mary Katherine Heinrich*

Main category: cs.RO

TL;DR: This paper introduces SoNS, enabling robot swarms to self-organize, estimate global configurations, and use online external LLM-generated code to overcome challenges, achieving an 85% success rate.


<details>
  <summary>Details</summary>
Motivation: To address challenges in robot swarm behavior design and coordination, particularly in situations where swarms encounter unexpected challenges.

Method: The authors implemented a self-organizing nervous system (SoNS) for robot swarms, enabling online automatic code generation using an external large language model (LLM). Experiments were conducted with real robots and in simulations.

Result: The implementation using SoNS allowed robot swarms to successfully recover from stuck states by generating and running external code, achieving an 85% mission success rate.

Conclusion: SoNS enhances robot swarm functionality by simplifying behavior design and providing real-time code adaptation, demonstrating its potential to improve swarm autonomy and mission reliability.

Abstract: Our recently introduced self-organizing nervous system (SoNS) provides robot
swarms with 1) ease of behavior design and 2) global estimation of the swarm
configuration and its collective environment, facilitating the implementation
of online automatic code generation for robot swarms. In a demonstration with 6
real robots and simulation trials with >30 robots, we show that when a
SoNS-enhanced robot swarm gets stuck, it can automatically solicit and run code
generated by an external LLM on the fly, completing its mission with an 85%
success rate.

</details>


### [723] [TAG-K: Tail-Averaged Greedy Kaczmarz for Computationally Efficient and Performant Online Inertial Parameter Estimation](https://arxiv.org/abs/2510.04839)
*Shuo Sha,Anupam Bhakta,Zhenyuan Jiang,Kevin Qiu,Ishaan Mahajan,Gabriel Bravo,Brian Plancher*

Main category: cs.RO

TL;DR: TAG-K, a new method based on the Kaczmarz framework, improves real-time inertial parameter estimation for robots, achieving faster solve times, lower computational costs, and better estimation accuracy compared to traditional methods and other variants.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the limitations of traditional methods like Recursive Least Squares and Kalman Filter in adapting to abrupt parameter changes and operating efficiently in computationally constrained robotic systems.

Method: The authors propose TAG-K, a lightweight extension of the Kaczmarz method, which incorporates greedy randomized row selection for faster convergence and tail averaging for noise robustness.

Result: TAG-K demonstrates 1.5x-20.7x faster solve times across different hardware platforms while also reducing estimation error by 25%, leading to nearly twice the tracking performance improvement.

Conclusion: TAG-K provides a robust and computationally efficient solution for dynamic robotic environments, outperforming state-of-the-art parameter estimation methods in both speed and accuracy.

Abstract: Accurate online inertial parameter estimation is essential for adaptive
robotic control, enabling real-time adjustment to payload changes,
environmental interactions, and system wear. Traditional methods such as
Recursive Least Squares (RLS) and the Kalman Filter (KF) often struggle to
track abrupt parameter shifts or incur high computational costs, limiting their
effectiveness in dynamic environments and for computationally constrained
robotic systems. As such, we introduce TAG-K, a lightweight extension of the
Kaczmarz method that combines greedy randomized row selection for rapid
convergence with tail averaging for robustness under noise and inconsistency.
This design enables fast, stable parameter adaptation while retaining the low
per-iteration complexity inherent to the Kaczmarz framework. We evaluate TAG-K
in synthetic benchmarks and quadrotor tracking tasks against RLS, KF, and other
Kaczmarz variants. TAG-K achieves 1.5x-1.9x faster solve times on laptop-class
CPUs and 4.8x-20.7x faster solve times on embedded microcontrollers. More
importantly, these speedups are paired with improved resilience to measurement
noise and a 25% reduction in estimation error, leading to nearly 2x better
end-to-end tracking performance.

</details>


### [724] [CLEAR-IR: Clarity-Enhanced Active Reconstruction of Infrared Imagery](https://arxiv.org/abs/2510.04883)
*Nathan Shankar,Pawel Ladosz,Hujun Yin*

Main category: cs.RO

TL;DR: This paper proposes a U-Net-based architecture to enhance infrared (IR) images for robust robotic perception in dark environments.


<details>
  <summary>Details</summary>
Motivation: Robotic systems face challenges using RGB streams in low-light environments, and IR streams, though less noisy, have active emitter patterns that impair high-level tasks.

Method: The researchers designed a U-Net-based architecture to remove unwanted emitter patterns from IR streams, producing cleaner images.

Result: The method enhances IR image quality and improves robotic performance in both well-lit and extreme low-light settings, surpassing existing enhancement techniques.

Conclusion: The proposed approach allows reliable operation of vision-driven robotic systems under varying illumination conditions.

Abstract: This paper presents a novel approach for enabling robust robotic perception
in dark environments using infrared (IR) stream. IR stream is less susceptible
to noise than RGB in low-light conditions. However, it is dominated by active
emitter patterns that hinder high-level tasks such as object detection,
tracking and localisation. To address this, a U-Net-based architecture is
proposed that reconstructs clean IR images from emitter-populated input,
improving both image quality and downstream robotic performance. This approach
outperforms existing enhancement techniques and enables reliable operation of
vision-driven robotic systems across illumination conditions from well-lit to
extreme low-light scenes.

</details>


### [725] [HyperVLA: Efficient Inference in Vision-Language-Action Models via Hypernetworks](https://arxiv.org/abs/2510.04898)
*Zheng Xiong,Kang Li,Zilin Wang,Matthew Jackson,Jakob Foerster,Shimon Whiteson*

Main category: cs.RO

TL;DR: The paper introduces HyperVLA, a new Vision-Language-Action (VLA) model that drastically reduces inference costs using a hypernetwork-based architecture.


<details>
  <summary>Details</summary>
Motivation: Current Vision-Language-Action (VLA) models have high generalization ability but suffer from extremely high inference costs.

Method: The authors propose HyperVLA, which uses a hypernetwork (HN)-based architecture to activate only task-specific policies during inference, leveraging techniques such as HN normalization and action generation strategies.

Result: HyperVLA achieves comparable or better success rates in zero-shot generalization and few-shot adaptation compared to existing VLAs, reducing activated parameters by 90x and inference speed by 120x compared to OpenVLA.

Conclusion: HyperVLA successfully addresses the inference cost issues of VLAs, maintaining performance while greatly improving computational efficiency. The method is validated through significant reductions in parameters and speed improvements.

Abstract: Built upon language and vision foundation models with strong generalization
ability and trained on large-scale robotic data, Vision-Language-Action (VLA)
models have recently emerged as a promising approach to learning generalist
robotic policies. However, a key drawback of existing VLAs is their extremely
high inference costs. In this paper, we propose HyperVLA to address this
problem. Unlike existing monolithic VLAs that activate the whole model during
both training and inference, HyperVLA uses a novel hypernetwork (HN)-based
architecture that activates only a small task-specific policy during inference,
while still retaining the high model capacity needed to accommodate diverse
multi-task behaviors during training. Successfully training an HN-based VLA is
nontrivial so HyperVLA contains several key algorithm design features that
improve its performance, including properly utilizing the prior knowledge from
existing vision foundation models, HN normalization, and an action generation
strategy. Compared to monolithic VLAs, HyperVLA achieves a similar or even
higher success rate for both zero-shot generalization and few-shot adaptation,
while significantly reducing inference costs. Compared to OpenVLA, a
state-of-the-art VLA model, HyperVLA reduces the number of activated parameters
at test time by $90\times$, and accelerates inference speed by $120\times$.
Code is publicly available at https://github.com/MasterXiong/HyperVLA

</details>


### [726] [Efficient Navigation in Unknown Indoor Environments with Vision-Language Models](https://arxiv.org/abs/2510.04991)
*D. Schwartz,K. Kondo,J. P. How*

Main category: cs.RO

TL;DR: The paper discusses a vision-language model-driven planning framework that enhances autonomous indoor navigation by addressing inefficiencies in traditional exploration methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address inefficiencies in autonomous navigation within unknown indoor environments, where traditional methods struggle with limited global reasoning and reliance on local heuristics.

Method: The method involves using vision-language models in a zero-shot, high-level planning framework to convert 3D occupancy grids into 2D maps, generate subgoal candidates, evaluate them, and enhance navigation using the DYNUS trajectory planner.

Result: The result shows improved navigation efficiency in simulation, reducing path lengths by approximately 10% on average through better structural reasoning and balance between goal progress and risk.

Conclusion: The framework enables VLMs to effectively infer structural patterns and avoid common exploration failures, proving beneficial for autonomous navigation in complex indoor settings.

Abstract: We present a novel high-level planning framework that leverages
vision-language models (VLMs) to improve autonomous navigation in unknown
indoor environments with many dead ends. Traditional exploration methods often
take inefficient routes due to limited global reasoning and reliance on local
heuristics. In contrast, our approach enables a VLM to reason directly about an
occupancy map in a zero-shot manner, selecting subgoals that are likely to lead
to more efficient paths. At each planning step, we convert a 3D occupancy grid
into a partial 2D map of the environment, and generate candidate subgoals. Each
subgoal is then evaluated and ranked against other candidates by the model. We
integrate this planning scheme into DYNUS \cite{kondo2025dynus}, a
state-of-the-art trajectory planner, and demonstrate improved navigation
efficiency in simulation. The VLM infers structural patterns (e.g., rooms,
corridors) from incomplete maps and balances the need to make progress toward a
goal against the risk of entering unknown space. This reduces common greedy
failures (e.g., detouring into small rooms) and achieves about 10\% shorter
paths on average.

</details>


### [727] [Walking, Rolling, and Beyond: First-Principles and RL Locomotion on a TARS-Inspired Robot](https://arxiv.org/abs/2510.05001)
*Aditya Sripada,Abhishek Warrier*

Main category: cs.RO

TL;DR: The paper introduces TARS3D, a robotic platform inspired by the TARS robot from Interstellar, capable of bipedal walking and rolling. It uses mathematical models and deep reinforcement learning to explore and validate its locomotion modes.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the need for robotic designs that explore non-anthropomorphic, bio-transcendent forms to achieve innovative and versatile locomotion capabilities.

Method: TARS3D employs reduced-order models, closed-form limit-cycle conditions, and hardware validation alongside deep reinforcement learning simulation to analyze and expand the locomotion repertoire.

Result: Experiments validate predicted gaits, such as an eight-step rolling hybrid limit cycle, while reinforcement learning discovers both analytic and novel motion behaviors.

Conclusion: The study demonstrates the potential of TARS3D's unique design for achieving diverse locomotion modes and highlights the synergy of analytic methods and learning models for multimodal robotic exploration.

Abstract: Robotic locomotion research typically draws from biologically inspired leg
designs, yet many human-engineered settings can benefit from
non-anthropomorphic forms. TARS3D translates the block-shaped 'TARS' robot from
Interstellar into a 0.25 m, 0.99 kg research platform with seven actuated
degrees of freedom. The film shows two primary gaits: a bipedal-like walk and a
high-speed rolling mode. For TARS3D, we build reduced-order models for each,
derive closed-form limit-cycle conditions, and validate the predictions on
hardware. Experiments confirm that the robot respects its +/-150 degree hip
limits, alternates left-right contacts without interference, and maintains an
eight-step hybrid limit cycle in rolling mode. Because each telescopic leg
provides four contact corners, the rolling gait is modeled as an eight-spoke
double rimless wheel. The robot's telescopic leg redundancy implies a far
richer gait repertoire than the two limit cycles treated analytically. So, we
used deep reinforcement learning (DRL) in simulation to search the unexplored
space. We observed that the learned policy can recover the analytic gaits under
the right priors and discover novel behaviors as well. Our findings show that
TARS3D's fiction-inspired bio-transcending morphology can realize multiple
previously unexplored locomotion modes and that further learning-driven search
is likely to reveal more. This combination of analytic synthesis and
reinforcement learning opens a promising pathway for multimodal robotics.

</details>


### [728] [StaMo: Unsupervised Learning of Generalizable Robot Motion from Compact State Representation](https://arxiv.org/abs/2510.05057)
*Mingyu Liu,Jiuhe Shu,Hui Chen,Zeju Li,Canyu Zhao,Jiange Yang,Shenyuan Gao,Hao Chen,Chunhua Shen*

Main category: cs.RO

TL;DR: This paper introduces StaMo, a method for unsupervised learning of compact state representations for robotics. It uses a lightweight encoder and a diffusion transformer, improving performance in various tasks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of creating compact yet expressive state representations for robotics, as existing methods often fail by being too redundant or missing critical task information.

Method: StaMo leverages a lightweight encoder coupled with a pre-trained Diffusion Transformer decoder to generate compact two-token state representations. It then derives latent actions from token differences without explicit supervision.

Result: StaMo achieved performance improvements of 14.3% on LIBERO and 30% in real-world task success rates, with minimal inference overhead. It also outperformed prior methods in policy co-training by 10.4%.

Conclusion: StaMo effectively learns structured dynamics and generalizable robotic motion from compact state representations. It reduces reliance on complex architectures and video data while enhancing interpretability and scalability across diverse datasets.

Abstract: A fundamental challenge in embodied intelligence is developing expressive and
compact state representations for efficient world modeling and decision making.
However, existing methods often fail to achieve this balance, yielding
representations that are either overly redundant or lacking in task-critical
information. We propose an unsupervised approach that learns a highly
compressed two-token state representation using a lightweight encoder and a
pre-trained Diffusion Transformer (DiT) decoder, capitalizing on its strong
generative prior. Our representation is efficient, interpretable, and
integrates seamlessly into existing VLA-based models, improving performance by
14.3% on LIBERO and 30% in real-world task success with minimal inference
overhead. More importantly, we find that the difference between these tokens,
obtained via latent interpolation, naturally serves as a highly effective
latent action, which can be further decoded into executable robot actions. This
emergent capability reveals that our representation captures structured
dynamics without explicit supervision. We name our method StaMo for its ability
to learn generalizable robotic Motion from compact State representation, which
is encoded from static images, challenging the prevalent dependence to learning
latent action on complex architectures and video data. The resulting latent
actions also enhance policy co-training, outperforming prior methods by 10.4%
with improved interpretability. Moreover, our approach scales effectively
across diverse data sources, including real-world robot data, simulation, and
human egocentric video.

</details>


### [729] [Automaton Constrained Q-Learning](https://arxiv.org/abs/2510.05061)
*Anastasios Manganaris,Vittorio Giammarino,Ahmed H. Qureshi*

Main category: cs.RO

TL;DR: The paper introduces Automaton Constrained Q-Learning (ACQL), a method combining reinforcement learning with Linear-time Temporal Logic (LTL) to address tasks requiring goal sequences and dynamic safety constraints. ACQL excels in handling temporally-extended goals and safety in complex environments.


<details>
  <summary>Details</summary>
Motivation: To improve reinforcement learning methods for real-world robotics tasks that involve both achieving sequential, temporally-extended goals and adhering to varying safety constraints which current methods struggle with.

Method: ACQL integrates goal-conditioned value learning with automaton-guided reinforcement leveraging LTL automaton representations to manage stage-wise goals and dynamic safety constraints.

Result: ACQL shows superior performance in complex continuous control tasks where existing methods fail, and it performs well in real-world robotic arm experiments with safety constraints.

Conclusion: ACQL is a robust and scalable solution for learning robotic behaviors using rich temporal logic specifications.

Abstract: Real-world robotic tasks often require agents to achieve sequences of goals
while respecting time-varying safety constraints. However, standard
Reinforcement Learning (RL) paradigms are fundamentally limited in these
settings. A natural approach to these problems is to combine RL with
Linear-time Temporal Logic (LTL), a formal language for specifying complex,
temporally extended tasks and safety constraints. Yet, existing RL methods for
LTL objectives exhibit poor empirical performance in complex and continuous
environments. As a result, no scalable methods support both temporally ordered
goals and safety simultaneously, making them ill-suited for realistic robotics
scenarios. We propose Automaton Constrained Q-Learning (ACQL), an algorithm
that addresses this gap by combining goal-conditioned value learning with
automaton-guided reinforcement. ACQL supports most LTL task specifications and
leverages their automaton representation to explicitly encode stage-wise goal
progression and both stationary and non-stationary safety constraints. We show
that ACQL outperforms existing methods across a range of continuous control
tasks, including cases where prior methods fail to satisfy either goal-reaching
or safety constraints. We further validate its real-world applicability by
deploying ACQL on a 6-DOF robotic arm performing a goal-reaching task in a
cluttered, cabinet-like space with safety constraints. Our results demonstrate
that ACQL is a robust and scalable solution for learning robotic behaviors
according to rich temporal specifications.

</details>


### [730] [ResMimic: From General Motion Tracking to Humanoid Whole-body Loco-Manipulation via Residual Learning](https://arxiv.org/abs/2510.05070)
*Siheng Zhao,Yanjie Ze,Yue Wang,C. Karen Liu,Pieter Abbeel,Guanya Shi,Rocky Duan*

Main category: cs.RO

TL;DR: The paper introduces ResMimic, a framework aimed at achieving precise humanoid control for tasks like whole-body loco-manipulation. It builds on existing general motion tracking policies and refines them for object-induced interactions and locomotion.


<details>
  <summary>Details</summary>
Motivation: The research aims to address the lack of precision and object-awareness in current general motion tracking policies for humanoid robots, which limits their effectiveness in tasks like loco-manipulation essential for daily service and warehouse use.

Method: The proposed approach, ResMimic, uses a two-stage residual learning framework. Initially, a base policy trained on large-scale human motion data generates human-like movements. This is refined through a residual policy that improves locomotion and allows for interaction with objects. Techniques such as point-cloud-based object tracking rewards, contact rewards, and curriculum-based stabilizers were introduced to optimize training.

Result: ResMimic demonstrates significant improvements in task success rates, training efficiency, and robustness when tested in simulations and on real humanoid robots compared to baseline methods.

Conclusion: ResMimic offers a transformative solution for achieving precise and expressive humanoid control, making it effective for real-world applications in service and warehouse environments.

Abstract: Humanoid whole-body loco-manipulation promises transformative capabilities
for daily service and warehouse tasks. While recent advances in general motion
tracking (GMT) have enabled humanoids to reproduce diverse human motions, these
policies lack the precision and object awareness required for
loco-manipulation. To this end, we introduce ResMimic, a two-stage residual
learning framework for precise and expressive humanoid control from human
motion data. First, a GMT policy, trained on large-scale human-only motion,
serves as a task-agnostic base for generating human-like whole-body movements.
An efficient but precise residual policy is then learned to refine the GMT
outputs to improve locomotion and incorporate object interaction. To further
facilitate efficient training, we design (i) a point-cloud-based object
tracking reward for smoother optimization, (ii) a contact reward that
encourages accurate humanoid body-object interactions, and (iii) a
curriculum-based virtual object controller to stabilize early training. We
evaluate ResMimic in both simulation and on a real Unitree G1 humanoid. Results
show substantial gains in task success, training efficiency, and robustness
over strong baselines. Videos are available at https://resmimic.github.io/ .

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [731] [Repairing Leaks in Resource Wrappers](https://arxiv.org/abs/2510.03461)
*Sanjay Malakar,Michael D. Ernst,Martin Kellogg,Manu Sridharan*

Main category: cs.SE

TL;DR: The paper enhances approaches for repairing resource leaks in programs by addressing limitations seen in prior works, with notable effectiveness in handling resource wrappers and improving repair rates.


<details>
  <summary>Details</summary>
Motivation: Resource leaks, involving unclosed finite resources like files or database connections, cause program inefficiencies. Existing techniques struggle to repair leaks thoroughly, especially for resource wrappers, limiting their real-world applicability.

Method: The authors integrated resource management inference, improved program transformations for better analysis, introduced field containment analysis for resource lifetimes, and developed a new repair pattern to address non-final fields.

Result: The implementation, "Arodnap," improved resource leak repair effectiveness, fixing 68% of warnings in the NJR benchmark suite compared to 41% by previous methods.

Conclusion: By addressing limitations in detecting and repairing resource leaks, especially those involving wrappers, the paper presents advances that significantly boost repair effectiveness and applicability in real-world programs.

Abstract: A resource leak occurs when a program fails to release a finite resource like
a socket, file descriptor or database connection. While sound static analysis
tools can detect all leaks, automatically repairing them remains challenging.
Prior work took the output of a detection tool and attempted to repair only
leaks from a hard-coded list of library resource types. That approach limits
the scope of repairable leaks: real-world code uses resource wrappers that
store a resource in a field and must themselves be closed. This paper makes
four key contributions to improve resource leak repair in the presence of
wrappers. (1) It integrates inference of resource management specifications
into the repair pipeline, enabling extant fixing approaches to reason about
wrappers. (2) It transforms programs into variants that are easier to analyze,
making inference, detection, and fixing tools more effective; for instance, it
makes detection tools report problems closer to the root cause, often in a
client of a resource wrapper rather than within the wrapper class itself. (3) A
novel field containment analysis reasons about resource lifetimes, enabling
repair of more leaks involving resources stored in fields. (4) It introduces a
new repair pattern and more precise reasoning to better handle resources stored
in non-final fields. Prior work fixed 41% of resource leak warnings in the NJR
benchmark suite; our implementation Arodnap fixes 68%.

</details>


### [732] [ALMAS: an Autonomous LLM-based Multi-Agent Software Engineering Framework](https://arxiv.org/abs/2510.03463)
*Vali Tawosi,Keshav Ramani,Salwa Alamir,Xiaomo Liu*

Main category: cs.SE

TL;DR: The paper presents ALMAS, an Autonomous LLM-based Multi-Agent Software Engineering framework, designed to support multi-stage agile software development.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the need for LLM systems that can handle the complexities of software development beyond code implementation, integrating multiple stages of the SDLC.

Method: The ALMAS framework applies LLM-based agents aligned with agile roles, enabling modular and seamless integration with human developers and their existing workflows.

Result: The paper demonstrates ALMAS's capabilities through a use case where it successfully generates an application and adds a new feature autonomously.

Conclusion: ALMAS is a promising solution for autonomous engagement in software engineering tasks, showing potential for improving productivity in agile software development environments.

Abstract: Multi-agent Large Language Model (LLM) systems have been leading the way in
applied LLM research across a number of fields. One notable area is software
development, where researchers have advanced the automation of code
implementation, code testing, code maintenance, inter alia, using LLM agents.
However, software development is a multifaceted environment that extends beyond
just code. As such, a successful LLM system must factor in multiple stages of
the software development life-cycle (SDLC). In this paper, we propose a vision
for ALMAS, an Autonomous LLM-based Multi-Agent Software Engineering framework,
which follows the above SDLC philosophy such that it may work within an agile
software development team to perform several tasks end-to-end. ALMAS aligns its
agents with agile roles, and can be used in a modular fashion to seamlessly
integrate with human developers and their development environment. We showcase
the progress towards ALMAS through our published works and a use case
demonstrating the framework, where ALMAS is able to seamlessly generate an
application and add a new feature.

</details>


### [733] [Relative Code Comprehensibility Prediction](https://arxiv.org/abs/2510.03474)
*Nadeeshan De Silva,Martin Kellogg,Oscar Chaparro*

Main category: cs.SE

TL;DR: This paper proposes a machine learning approach to predict the relative comprehensibility of code snippets, which outperforms traditional absolute prediction models.


<details>
  <summary>Details</summary>
Motivation: Traditional code comprehensibility metrics and machine learning models struggle with accurately predicting how humans understand code, partly due to noise in measurement data.

Method: The authors train machine learning models to predict the relative comprehensibility of two code snippets instead of their absolute comprehensibility, and validate the approach using a dataset of 150 Java snippets and 12.5k human measurements.

Result: Relative comprehensibility models significantly outperform absolute models, with improvements of 137.8% snippet-wise and 74.7% developer-wise compared to baselines.

Conclusion: Relative comprehensibility models are more effective at handling noisy human data and have strong practical implications for software engineering tasks like refactoring assessment.

Abstract: Automatically predicting how difficult it is for humans to understand a code
snippet can assist developers in tasks like deciding when and where to
refactor. Despite many proposed code comprehensibility metrics, studies have
shown they often correlate poorly with actual measurements of human
comprehensibility. This has motivated the use of machine learning models to
predict human comprehensibility directly from code, but these models have also
shown limited accuracy.
  We argue that model inaccuracy stems from inherent noise in human
comprehensibility data, which confuses models trained to predict it directly.
To address this, we propose training models to predict the relative
comprehensibility of two code snippets - that is, predicting which snippet a
human would find easier to understand without predicting each snippet's
comprehensibility in isolation. This mitigates noise in predicting 'absolute'
comprehensibility measurements, but is still useful for downstream
software-engineering tasks like assessing whether refactoring improves or
hinders comprehensibility.
  We conducted a study to assess and compare the effectiveness of absolute and
relative code comprehensibility prediction via machine learning. We used a
dataset of 150 Java code snippets and 12.5k human comprehensibility
measurements from prior user studies, comparing the models' performance with
naive baselines (eg 'always predict the majority class'). Our findings indicate
that absolute comprehensibility models improve over the baselines by at most
33.4% and frequently underperform. In contrast, relative comprehensibility
models are substantially better, with average improvements of 137.8% and 74.7%
for snippet-wise and developer-wise prediction, respectively. These results
suggest that relative comprehensibility models learn more effectively from the
data, supporting their practical applicability for downstream SE tasks.

</details>


### [734] [LLM Agents for Automated Dependency Upgrades](https://arxiv.org/abs/2510.03480)
*Vali Tawosi,Salwa Alamir,Xiaomo Liu,Manuela Veloso*

Main category: cs.SE

TL;DR: This paper presents a framework of LLM agents for automating library updates in codebases, ensuring compatibility with newer versions while reducing developer workload.


<details>
  <summary>Details</summary>
Motivation: The paper aims to solve the challenge of outdated library dependencies in expanding codebases, which pose security risks and require time-intensive updates.

Method: The framework uses LLM agents alongside migration documentation to identify outdated library usage, recommend fixes, and apply updates in Java codebases effectively. It includes components like a Summary Agent, Control Agent, and Code Agent.

Result: The framework demonstrates its capability in industrial use cases with synthetic code repositories, achieving a precision of 71.4% and outperforming state-of-the-art methods in token efficiency.

Conclusion: The proposed approach proves to be efficient and effective for automated library updates, reducing developer effort and maintaining codebase innovation and security.

Abstract: As a codebase expands over time, its library dependencies can become outdated
and require updates to maintain innovation and security. However, updating a
library can introduce breaking changes in the code, necessitating significant
developer time for maintenance. To address this, we introduce a framework of
LLM agents to be used in combination with migration documentation to
automatically recommend and apply code updates and ensure compatibility with
new versions. Our solution can automatically localize updated library usages in
live Java codebases and implement recommended fixes in a user-friendly manner.
The system architecture consists of multiple key components: a Summary Agent,
Control Agent, and Code Agent. To validate our approach, we apply the framework
on an industrial use case by which we create three synthetic code repositories
with major Upgrade changes and benchmark our approach against state-of-the-art
methods. Results show that our approach not only performs upgrades using fewer
tokens across all cases but also achieves a precision of 71.4%, highlighting
its efficiency and effectiveness compared to state-of-the-art methods.

</details>


### [735] [AgentHub: A Research Agenda for Agent Sharing Infrastructure](https://arxiv.org/abs/2510.03495)
*Erik Pautsch,Tanmay Singla,Wenxin Jiang,Huiyun Peng,Behnaz Hassanshahi,Konstantin Läufer,George K. Thiruvathukal,James C. Davis*

Main category: cs.SE

TL;DR: The paper introduces AgentHub, a proposed framework for sharing and managing LLM-based agents, outlining key challenges for creating scalable, trusted ecosystems.


<details>
  <summary>Details</summary>
Motivation: The infrastructure for managing and governing LLM-based agents is fragmented compared to mature ecosystems like npm or Hugging Face, leading to challenges in sharing and reuse.

Method: The authors propose a research agenda called AgentHub, focusing on challenges such as capability clarity, lifecycle transparency, interoperability, governance, security, and workflow integration.

Result: AgentHub serves as a roadmap for building a reliable and scalable agent ecosystem, enabling open-source distribution and reuse.

Conclusion: AgentHub envisions a future where LLM-based agents can be shared and integrated seamlessly, akin to the experience of using software libraries today.

Abstract: LLM-based agents are rapidly proliferating, yet the infrastructure for
discovering, evaluating, and governing them remains fragmented compared to
mature ecosystems like software package registries (e.g., npm) and model hubs
(e.g., Hugging Face). Recent research and engineering works have begun to
consider the requisite infrastructure, but so far they focus narrowly -- on
distribution, naming, or protocol negotiation. However, considering broader
software engineering requirements would improve open-source distribution and
ease reuse. We therefore propose AgentHub, a research agenda for agent sharing.
By framing the key challenges of capability clarity, lifecycle transparency,
interoperability, governance, security, and workflow integration, AgentHub
charts a community-wide agenda for building reliable and scalable agent
ecosystems. Our vision is a future where agents can be shared, trusted, and
composed as seamlessly as today's software libraries.

</details>


### [736] [REFINE: Enhancing Program Repair Agents through Context-Aware Patch Refinement](https://arxiv.org/abs/2510.03588)
*Anvith Pabba,Simin Chen,Alex Mathai,Anindya Chakraborty,Baishakhi Ray*

Main category: cs.SE

TL;DR: The paper introduces Refine, a framework for improving partially correct patches generated by large language models in automated program repair. It achieves significant performance improvements in evaluation tests.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with generating correct program fixes due to limited code context understanding and reliance on inadequate test suites, leading to partially correct patches.

Method: Refine systematically improves draft patches through disambiguating vague contexts, enhancing patch diversity with test-time scaling, and aggregating partial fixes using an LLM-based code review process.

Result: Refine was integrated into APR systems, showing a 14.67% performance boost for AutoCodeRover, improvements in resolution rates, and general enhancements across multiple APR systems.

Conclusion: Refine effectively fills the gap in APR pipelines, demonstrating significant gains through refinement and showing promise for improving agent-LLM collaboration in program repair.

Abstract: Large Language Models (LLMs) have recently shown strong potential in
automatic program repair (APR), especially in repository-level settings where
the goal is to generate patches based on natural language issue descriptions,
large codebases, and regression tests. However, despite their promise, current
LLM-based APR techniques often struggle to produce correct fixes due to limited
understanding of code context and over-reliance on incomplete test suites. As a
result, they frequently generate Draft Patches-partially correct patches that
either incompletely address the bug or overfit to the test cases. In this work,
we propose a novel patch refinement framework, Refine, that systematically
transforms Draft Patches into correct ones. Refine addresses three key
challenges: disambiguating vague issue and code context, diversifying patch
candidates through test-time scaling, and aggregating partial fixes via an
LLM-powered code review process. We implement Refine as a general refinement
module that can be integrated into both open-agent-based and workflow-based APR
systems. Our evaluation on the SWE-Bench Lite benchmark shows that Refine
achieves state-of-the-art results among workflow-based approaches and
approaches the best-known performance across all APR categories. Specifically,
Refine boosts AutoCodeRover's performance by 14.67%, achieving a score of
51.67% and surpassing all prior baselines. On SWE-Bench Verified, Refine
improves the resolution rate by 12.2%, and when integrated across multiple APR
systems, it yields an average improvement of 14%-demonstrating its broad
effectiveness and generalizability. These results highlight the effectiveness
of refinement as a missing component in current APR pipelines and the potential
of agentic collaboration in closing the gap between near-correct and correct
patches. We also open source our code.

</details>


### [737] [Generating High-Level Test Cases from Requirements using LLM: An Industry Study](https://arxiv.org/abs/2510.03641)
*Satoshi Masuda,Satoshi Kouzawa,Kyousuke Sezai,Hidetoshi Suhara,Yasuaki Hiruta,Kunihiro Kudou*

Main category: cs.SE

TL;DR: The paper proposes a method to automatically generate high-level test cases from requirement documents using only prompts and LLMs, avoiding the need for labor-intensive retrieval-augmented generation (RAG).


<details>
  <summary>Details</summary>
Motivation: The paper addresses the demand for automating the manual process of test case generation from requirement documents in the software industry, which is labor-intensive and lacks generalization across applications.

Method: The method involves inputting requirement documents into LLMs to generate relevant test design techniques, followed by creating high-level test cases for each technique. Semantic similarity evaluation is used to verify the effectiveness.

Result: Experiments conducted on Bluetooth and Mozilla datasets achieved macro-recall measurements of 0.81 and 0.37, highlighting practical feasibility.

Conclusion: The proposed method demonstrates potential for practical application in automating high-level test case generation using prompts, eliminating the need for RAG customization.

Abstract: Currently, generating high-level test cases described in natural language
from requirement documents is performed manually. In the industry, including
companies specializing in software testing, there is a significant demand for
the automatic generation of high-level test cases from requirement documents
using Large Language Models (LLMs). Efforts to utilize LLMs for requirement
analysis are underway. In some cases, retrieval-augmented generation (RAG) is
employed for generating high-level test cases using LLMs. However, in practical
applications, it is necessary to create a RAG tailored to the knowledge system
of each specific application, which is labor-intensive. Moreover, when applying
high-level test case generation as a prompt, there is no established method for
instructing the generation of high-level test cases at a level applicable to
other specifications without using RAG. It is required to establish a method
for the automatic generation of high-level test cases that can be generalized
across a wider range of requirement documents. In this paper, we propose a
method for generating high-level (GHL) test cases from requirement documents
using only prompts, without creating RAGs. In the proposed method, first, the
requirement document is input into the LLM to generate test design techniques
corresponding to the requirement document. Then, high-level test cases are
generated for each of the generated test design techniques. Furthermore, we
verify an evaluation method based on semantic similarity of the generated
high-level test cases. In the experiments, we confirmed the method using
datasets from Bluetooth and Mozilla, where requirement documents and high-level
test cases are available, achieving macro-recall measurement of 0.81 and 0.37,
respectively. We believe that the method is feasible for practical application
in generating high-level test cases without using RAG.

</details>


### [738] [Detecting and Preventing Latent Risk Accumulation in High-Performance Software Systems](https://arxiv.org/abs/2510.03712)
*Jahidul Arafat,Kh. M. Moniruzzaman,Shamim Hossain,Fariha Tasmin,Kamrujjaman,Ahsan Habib Tareq*

Main category: cs.SE

TL;DR: The paper introduces a framework to proactively detect latent risks in distributed systems, which traditional reliability approaches fail to handle effectively. It uses a Latent Risk Index (LRI) and incorporates innovative methodologies with strong statistical validation and real-world benefits.


<details>
  <summary>Details</summary>
Motivation: Distributed systems often fail catastrophically when masked vulnerabilities in optimization layers are exposed. Current reliability focuses reactively on incident response rather than proactively addressing these hidden risks.

Method: The framework integrates three systems: HYDRA for perturbation testing, RAVEN for monitoring, and APEX for risk-aware optimization. It also introduces the Latent Risk Index (LRI) for predictive risk assessment and employs extensive testbeds and production validations.

Result: The framework achieved 89.7% risk discovery rates, reduced latent risks by 59.2%, and provided measurable improvements in recovery times (-69.1%) and incident severity (-78.6%). Production deployment generated substantial cost savings with a short ROI period.

Conclusion: This approach advances reliability engineering by shifting from reactive to proactive methodologies, optimizing performance while minimizing risks and delivering tangible operational and financial benefits.

Abstract: Modern distributed systems employ aggressive optimization strategies that
create latent risks - hidden vulnerabilities where exceptional performance
masks catastrophic fragility when optimizations fail. Cache layers achieving
99% hit rates can obscure database bottlenecks until cache failures trigger
100x load amplification and cascading collapse. Current reliability engineering
focuses on reactive incident response rather than proactive detection of
optimization-induced vulnerabilities. This paper presents the first
comprehensive framework for systematic latent risk detection, prevention, and
optimization through integrated mathematical modeling, intelligent perturbation
testing, and risk-aware performance optimization. We introduce the Latent Risk
Index (LRI) that correlates strongly with incident severity (r=0.863, p<0.001),
enabling predictive risk assessment. Our framework integrates three systems:
HYDRA employing six optimization-aware perturbation strategies achieving 89.7%
risk discovery rates, RAVEN providing continuous production monitoring with
92.9% precision and 93.8% recall across 1,748 scenarios, and APEX enabling
risk-aware optimization maintaining 96.6% baseline performance while reducing
latent risks by 59.2%. Evaluation across three testbed environments
demonstrates strong statistical validation with large effect sizes (Cohen
d>2.0) and exceptional reproducibility (r>0.92). Production deployment over 24
weeks shows 69.1% mean time to recovery reduction, 78.6% incident severity
reduction, and 81 prevented incidents generating 1.44M USD average annual
benefits with 3.2-month ROI. Our approach transforms reliability engineering
from reactive incident management to proactive risk-aware optimization.

</details>


### [739] [APIDA-Chat: Structured Synthesis of API Search Dialogues to Bootstrap Conversational Agents](https://arxiv.org/abs/2510.03743)
*Zachary Eberhart,Collin McMillan*

Main category: cs.SE

TL;DR: APIDA-Chat is a pipeline for generating training data for niche or proprietary API dialogue using symbolic dialogue-act scripts. It fine-tunes a student model with synthesized dialogues for improved performance on consumer hardware.


<details>
  <summary>Details</summary>
Motivation: Assistants struggle to explain niche or proprietary APIs due to the lack of multi-turn dialogue datasets. Addressing this scarcity is necessary for better utility of large-language models in specific domains.

Method: APIDA-Chat first uses a dialogue planner with a teacher model to synthesize realistic dialogues, then fine-tunes a smaller student model. The pipeline is modular and allows cost-efficient synthesis by reusing resources.

Result: The fine-tuned student model improved BLEU from 0.38 to 0.50 and BERTScore from 0.88 to 0.91, showcasing enhanced performance while being computationally efficient.

Conclusion: APIDA-Chat provides an effective, low-cost, open-source solution for generating realistic conversations about APIs, facilitating better training for models handling niche tasks.

Abstract: Large-language-model assistants are suitable for explaining popular APIs, yet
they falter on niche or proprietary libraries because the multi-turn dialogue
data needed for fine-tuning are scarce. We present APIDA-Chat, an open-source
pipeline that converts symbolic dialogue-act "scripts" into realistic,
domain-grounded API Search conversations using a lightweight model for
inexpensive training data generation. Phase I pairs a legacy dialogue planner
with a high-capability teacher LLM (o4-mini) to synthesize a "gold set" of
realized dialogues; then, a smaller Llama 3.2 3B student model is fine-tuned on
this corpus. Phase II drops the teacher and reuses the same planner with the
fine-tuned model, allowing rapid, low-cost synthesis of new dialogues without
exposing source code to external services. The fine-tuned student improves BLEU
from 0.38 to 0.50 and BERTScore from 0.88 to 0.91 versus the base model while
running entirely on a single consumer GPU. All components are modular and
publicly released to serve as a conservative baseline for future work.
APIDA-Chat is open-sourced at https://github.com/Zeberhart/apida-chat and a
video demo is available at https://youtu.be/YqmZBHyGbPs .

</details>


### [740] [Code4MeV2: a Research-oriented Code-completion Platform](https://arxiv.org/abs/2510.03755)
*Roham Koohestani,Parham Bateni,Aydin Ebrahimi,Behdad Etezadi,Kiarash Karimi,Maliheh Izadi*

Main category: cs.SE

TL;DR: Code4MeV2 is an open-source code completion tool designed for research purposes, addressing proprietary data issues in AI-assisted coding.


<details>
  <summary>Details</summary>
Motivation: Proprietary user interaction data from AI-powered code completion tools limit reproducible academic research and large-scale analysis.

Method: Developed an open-source JetBrains plugin using client-server architecture with inline code completion and a context-aware chat assistant, enabling transparent data collection.

Result: Code4MeV2 achieved industry-comparable performance with 200ms latency, and feedback from eight users highlighted its informativeness and usefulness.

Conclusion: Code4MeV2 empowers researchers by providing an open platform for studying human-AI interaction, inviting community contribution for improvement.

Abstract: The adoption of AI-powered code completion tools in software development has
increased substantially, yet the user interaction data produced by these
systems remain proprietary within large corporations. This creates a barrier
for the academic community, as researchers must often develop dedicated
platforms to conduct studies on human--AI interaction, making reproducible
research and large-scale data analysis impractical. In this work, we introduce
Code4MeV2, a research-oriented, open-source code completion plugin for
JetBrains IDEs, as a solution to this limitation. Code4MeV2 is designed using a
client--server architecture and features inline code completion and a
context-aware chat assistant. Its core contribution is a modular and
transparent data collection framework that gives researchers fine-grained
control over telemetry and context gathering. Code4MeV2 achieves
industry-comparable performance in terms of code completion, with an average
latency of 200~ms. We assess our tool through a combination of an expert
evaluation and a user study with eight participants. Feedback from both
researchers and daily users highlights its informativeness and usefulness. We
invite the community to adopt and contribute to this tool. More information
about the tool can be found at https://app.code4me.me.

</details>


### [741] [A First Look at the Lifecycle of DL-Specific Self-Admitted Technical Debt](https://arxiv.org/abs/2510.03802)
*Gilberto Recupito,Vincenzo De Martino,Dario Di Nucci,Fabio Palomba*

Main category: cs.SE

TL;DR: The study investigates the lifecycle and persistence of DL-specific Self-Admitted Technical Debt (SATD) in ML projects, offering insights into managing its impact.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenges posed by Self-Admitted Technical Debt (SATD) in DL-enabled systems, which affect software quality and maintainability.

Method: Mining software repository techniques were applied to analyze 40 ML projects and trace the lifecycle of 185 DL-specific SATD instances from commit histories.

Result: The study identified that DL-specific SATD is introduced during early and middle project stages. SATD instances in 'Training' and 'Hardware' phases persist the longest, with frequent introduction during feature implementation and bug fixes.

Conclusion: Targeted strategies for managing DL-specific SATD are crucial to enhancing maintainability and addressing technical debt at critical stages of development.

Abstract: The rapid adoption of Deep Learning (DL)-enabled systems has revolutionized
software development, driving innovation across various domains. However, these
systems also introduce unique challenges, particularly in maintaining software
quality and performance. Among these challenges, Self-Admitted Technical Debt
(SATD) has emerged as a growing concern, significantly impacting the
maintainability and overall quality of ML and DL-enabled systems. Despite its
critical implications, the lifecycle of DL-specific SATD, how developers
introduce, acknowledge, and address it over time-remains underexplored. This
study presents a preliminary analysis of the persistence and lifecycle of
DL-specific SATD in DL-enabled systems. The purpose of this project is to
uncover the patterns of SATD introduction, recognition, and durability during
the development life cycle, providing information on how to manage these
issues. Using mining software repository techniques, we examined 40 ML
projects, focusing on 185 DL-specific SATD instances. The analysis tracked the
introduction and persistence of SATD instances through project commit histories
to assess their lifecycle and developer actions. The findings indicate that
DL-specific SATD is predominantly introduced during the early and middle stages
of project development. Training and Hardware phases showed the longest SATD
durations, highlighting critical areas where debt accumulates and persists.
Additionally, developers introduce DL-specific SATD more frequently during
feature implementation and bug fixes. This study emphasizes the need for
targeted DL-specific SATD management strategies in DL-enabled systems to
mitigate its impact. By understanding the temporal characteristics and
evolution of DL-specific SATD, developers can prioritize interventions at
critical stages to improve the maintainability and quality of the system.

</details>


### [742] [Smart Paste: Automatically Fixing Copy/Paste for Google Developers](https://arxiv.org/abs/2510.03843)
*Vincent Nguyen,Guilherme Herzog,José Cambronero,Marcus Revaj,Aditya Kini,Alexander Frömmgen,Maxim Tabachnyk*

Main category: cs.SE

TL;DR: The paper discusses Smart Paste, an AI-based IDE feature for suggesting edits after code pasting, with 45% suggestion acceptance and substantial adoption at Google's scale.


<details>
  <summary>Details</summary>
Motivation: Developers face challenges with follow-up edits after pasting code, which is a frequent activity within Google's software development.

Method: The authors developed and scaled Smart Paste, combining user-centered design, system integration, and machine learning models to provide relevant post-paste edit suggestions.

Result: Smart Paste received positive feedback with a 45% suggestion acceptance rate, contributing to over 1% of all code written across Google.

Conclusion: Smart Paste demonstrates the feasibility and impact of integrating AI-powered features within IDEs, providing insights for broader AI tool development.

Abstract: Manually editing pasted code is a long-standing developer pain point. In
internal software development at Google, we observe that code is pasted 4 times
more often than it is manually typed. These paste actions frequently require
follow-up edits, ranging from simple reformatting and renaming to more complex
style adjustments and cross-language translations. Prior work has shown deep
learning can be used to predict these edits. In this work, we show how to
iteratively develop and scale Smart Paste, an IDE feature for post-paste edit
suggestions, to Google's development environment. This experience can serve as
a guide for AI practitioners on a holistic approach to feature development,
covering user experience, system integration, and model capabilities. Since
deployment, Smart Paste has had overwhelmingly positive feedback with a 45%
acceptance rate. At Google's enterprise scale, these accepted suggestions
account substantially for over 1% of all code written company-wide.

</details>


### [743] [Designing Empirical Studies on LLM-Based Code Generation: Towards a Reference Framework](https://arxiv.org/abs/2510.03862)
*Nathalia Nascimento,Everton Guimaraes,Paulo Alencar*

Main category: cs.SE

TL;DR: This paper advocates for a structured framework to standardize empirical evaluations of large language models (LLMs) for code generation, addressing the current lack of comparability and reproducibility in the field.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the inconsistent methodologies in evaluating LLM-based code generation, which hampers comparability and lacks standardization across studies.

Method: The authors propose a theoretical framework grounded in their earlier experiments and comparative analysis of existing studies. It organizes evaluations around core elements such as problem types, quality metrics, and attributes to ensure systematic experimentation.

Result: The framework's applicability has been demonstrated through representative case mappings, and it highlights opportunities for enhancement to improve experimental rigor.

Conclusion: The study emphasizes the need to refine the framework further to make it a robust tool for standardizing evaluations of LLMs in code generation within software engineering fields.

Abstract: The rise of large language models (LLMs) has introduced transformative
potential in automated code generation, addressing a wide range of software
engineering challenges. However, empirical evaluation of LLM-based code
generation lacks standardization, with studies varying widely in goals, tasks,
and metrics, which limits comparability and reproducibility. In this paper, we
propose a theoretical framework for designing and reporting empirical studies
on LLM-based code generation. The framework is grounded in both our prior
experience conducting such experiments and a comparative analysis of key
similarities and differences among recent studies. It organizes evaluation
around core components such as problem sources, quality attributes, and
metrics, supporting structured and systematic experimentation. We demonstrate
its applicability through representative case mappings and identify
opportunities for refinement. Looking forward, we plan to evolve the framework
into a more robust and mature tool for standardizing LLM evaluation across
software engineering contexts.

</details>


### [744] [Adversarial Agent Collaboration for C to Rust Translation](https://arxiv.org/abs/2510.03879)
*Tianyu Li,Ruishi Li,Bo Wang,Brandon Paulsen,Umang Mathur,Prateek Saxena*

Main category: cs.SE

TL;DR: ACToR uses an adversarial approach with an LLM agent-based system for translating C programs to memory-safe Rust, achieving high translation correctness and test pass rates without human intervention.


<details>
  <summary>Details</summary>
Motivation: C programs often suffer from memory safety vulnerabilities, which could be mitigated by translating them to Rust. Existing approaches struggle with large codebases due to complexities in program analysis.

Method: ACToR employs an iterative adversarial setup inspired by GANs, with a generator agent refining Rust translations and a discriminator agent identifying new failing tests to enhance the translation.

Result: ACToR successfully translates 63 real-world command line utilities (average size 485 LoC) with over a 90% test pass rate and improves correctness by up to 18.9% compared to non-adversarial methods.

Conclusion: ACToR is the first system to reliably translate large-scale C programs to Rust, demonstrating the effectiveness of adversarial methods in LLM-assisted translation.

Abstract: Translating C to memory-safe languages, like Rust, prevents critical memory
safety vulnerabilities that are prevalent in legacy C software. Existing
approaches for C to safe Rust translation, including LLM-assisted ones, do not
generalize on larger (> 500 LoC) C codebases because they depend on complex
program analyses that frequently break. In this work, we present ACToR
(Adversarial C To Rust translator), a simple LLM agent-based approach. Inspired
by GANs, ACToR pits a generator agent against a discriminator agent, which
collaborate to iteratively generate a Rust translation. On each iteration, the
translator agent synthesizes and refines a Rust translation to pass an existing
suite of tests, and then the discriminator agent finds new failing tests. We
demonstrate that ACToR translates all of the 63 real-world command line
utilities considered in our benchmarks, which have an average size of 485 lines
of code, and it achieves over 90% test pass rate with zero human intervention.
To our knowledge, it is the first such system that reliably translates C
programs of this scale. Furthermore, ACToR improves translation correctness by
up to 18.9% compared to baseline, non-adversarial approaches.

</details>


### [745] [Rethinking Services in the Quantum Age: The SOQ Paradigm](https://arxiv.org/abs/2510.03890)
*Jose Garcia-Alonso,Enrique Moguel,Jaime Alvarado-Valiente,Javier Romero-Alvarez,Álvaro M. Aparicio-Morales,Juan M. Murillo,Francisco Javier Cavero,Adrián Romero-Flores,Alfonso E. Marquez-Chamorro,José Antonio Parejo,Antonio Ruiz-Cortés,Giuseppe Bisicchia,Alessandro Bocci,Antonio Brogi*

Main category: cs.SE

TL;DR: The paper introduces the Service-Oriented Quantum (SOQ) paradigm, focusing on scalable and interoperable quantum computing integration beyond classical dependencies.


<details>
  <summary>Details</summary>
Motivation: Integration of quantum computing into real-world systems is hampered by hardware fragility, platform diversity, and lack of robust software practices.

Method: Defines SOQ principles, proposes a layered technology stack, and addresses challenges like interoperability, hybridity, and workforce development.

Result: SOQ enables modular and independent integration of quantum computing services without reliance on classical environments.

Conclusion: SOQ is vital for advancing quantum technology by providing scalable and autonomous integration within software systems.

Abstract: Quantum computing is rapidly progressing from theoretical promise to
practical implementation, offering significant computational advantages for
tasks in optimization, simulation, cryptography, and machine learning. However,
its integration into real-world software systems remains constrained by
hardware fragility, platform heterogeneity, and the absence of robust software
engineering practices. This paper introduces Service-Oriented Quantum (SOQ), a
novel paradigm that reimagines quantum software systems through the lens of
classical service-oriented computing. Unlike prior approaches such as Quantum
Service-Oriented Computing (QSOC), which treat quantum capabilities as
auxiliary components within classical systems, SOQ positions quantum services
as autonomous, composable, and interoperable entities. We define the
foundational principles of SOQ, propose a layered technology stack to support
its realization, and identify the key research and engineering challenges that
must be addressed, including interoperability, hybridity, pricing models,
service abstractions, and workforce development. This approach is of vital
importance for the advancement of quantum technology because it enables the
scalable, modular, and interoperable integration of quantum computing into
real-world software systems independently and without relying on a dedicated
classical environment to manage quantum processing.

</details>


### [746] [A Brief History of the Waterfall Model: Past, Present, and Future](https://arxiv.org/abs/2510.03894)
*Antonios Saravanos*

Main category: cs.SE

TL;DR: The paper explores the historical role of the waterfall model, its criticism, and ongoing relevance, particularly as part of modern hybrid systems.


<details>
  <summary>Details</summary>
Motivation: To critically analyze the evolution, influence, and continuing relevance of the waterfall model in software development methodologies.

Method: The paper employs a historical and critical approach, synthesizing findings from various scholarly sources to trace the lifecycle and influence of the waterfall model.

Result: The analysis shows that the waterfall model has evolved from a standalone methodology to a foundational component in hybrid frameworks, proving its adaptability and ongoing utility.

Conclusion: The paper concludes that the waterfall model remains relevant, emphasizing that recognizing its limitations and strengths aids in selecting appropriate methodologies in diverse software development contexts.

Abstract: The waterfall model, one of the earliest software development methodologies,
has played a foundational role in shaping contemporary software engineering
practices. This paper provides a historical and critical overview of the model,
tracing its conceptual origins in software engineering, its formalization by
Royce, and its evolution through decades of industry adoption and critique.
Although often criticized for its rigidity, shortcomings, and high failure
rates, the waterfall model persists in specific domains. Its principles
continue to influence contemporary hybrid development frameworks that combine
traditional and agile methods. Drawing on a range of scholarly sources, this
study synthesizes key developments in the perception and application of the
waterfall model. The analysis highlights how the model has shifted from a
standalone framework to a component within modern hybrid methodologies. By
revisiting its origins, assessing its present utility, and examining its role
in contemporary development practices, this paper argues that the waterfall
model remains relevant, not as a relic of the past but as part of context-aware
development strategies. The paper contends that the model's enduring relevance
lies in its adaptability. By recognizing both its limitations and its
strengths, and by understanding its integration within hybrid approaches,
practitioners can make more informed decisions about methodology selection and
process design in diverse development environments.

</details>


### [747] [Multi-Agent Code-Orchestrated Generation for Reliable Infrastructure-as-Code](https://arxiv.org/abs/2510.03902)
*Rana Nameer Hussain Khan,Dawood Wasif,Jin-Hee Cho,Ali Butt*

Main category: cs.SE

TL;DR: This paper introduces MACOG, a multi-agent system for generating Infrastructure-as-Code (IaC) that overcomes limitations of current large language models (LLMs).


<details>
  <summary>Details</summary>
Motivation: The motivation is the increasing complexity of cloud-native infrastructure, which requires more robust and policy-compliant IaC generation than what is provided by current monolithic LLM-based approaches.

Method: The authors propose MACOG, a multi-agent LLM-based framework. Specialized agents work collaboratively via a shared-blackboard and orchestrator to generate Terraform configurations, validated by Terraform Plan and Open Policy Agent (OPA).

Result: MACOG significantly improves performance on the IaC-Eval benchmark, enhancing models like GPT-5 from 54.90 to 74.02 and Gemini-2.5 Pro from 43.56 to 60.13, with improvements in BLEU, CodeBERTScore, and an LLM-judge metric.

Conclusion: MACOG's modular, multi-agent approach demonstrates superior ability in generating syntactically valid, policy-compliant IaC, supported by ablation studies showing the importance of constrained decoding and deployment feedback.

Abstract: The increasing complexity of cloud-native infrastructure has made
Infrastructure-as-Code (IaC) essential for reproducible and scalable
deployments. While large language models (LLMs) have shown promise in
generating IaC snippets from natural language prompts, their monolithic,
single-pass generation approach often results in syntactic errors, policy
violations, and unscalable designs. In this paper, we propose MACOG
(Multi-Agent Code-Orchestrated Generation), a novel multi-agent LLM-based
architecture for IaC generation that decomposes the task into modular subtasks
handled by specialized agents: Architect, Provider Harmonizer, Engineer,
Reviewer, Security Prover, Cost and Capacity Planner, DevOps, and Memory
Curator. The agents interact via a shared-blackboard, finite-state orchestrator
layer, and collectively produce Terraform configurations that are not only
syntactically valid but also policy-compliant and semantically coherent. To
ensure infrastructure correctness and governance, we incorporate Terraform Plan
for execution validation and Open Policy Agent (OPA) for customizable policy
enforcement. We evaluate MACOG using the IaC-Eval benchmark, where MACOG is the
top enhancement across models, e.g., GPT-5 improves from 54.90 (RAG) to 74.02
and Gemini-2.5 Pro from 43.56 to 60.13, with concurrent gains on BLEU,
CodeBERTScore, and an LLM-judge metric. Ablations show constrained decoding and
deploy feedback are critical: removing them drops IaC-Eval to 64.89 and 56.93,
respectively.

</details>


### [748] [Refactoring with LLMs: Bridging Human Expertise and Machine Understanding](https://arxiv.org/abs/2510.03914)
*Yonnel Chen Kuang Piao,Jean Carlors Paul,Leuson Da Silva,Arghavan Moradi Dakhel,Mohammad Hamdaqa,Foutse Khomh*

Main category: cs.SE

TL;DR: The paper investigates whether instruction strategies inspired by human guidelines can improve Large Language Models (LLMs) in automating diverse code refactoring tasks.


<details>
  <summary>Details</summary>
Motivation: Developers often neglect refactoring due to high effort and lack of immediate rewards, and existing automated refactoring tools are limited.

Method: The authors use state-of-the-art LLMs (e.g., GPT-mini and DeepSeek-V3) and Martin Fowler's refactoring guidelines to design instruction strategies for 61 refactoring types. They test these strategies on benchmarks and real-world GitHub code.

Result: LLMs successfully performed all benchmark refactoring types and preserved program semantics using the proposed strategies. Rule-based instructions performed better in specific scenarios, while goal-focused strategies improved overall code quality.

Conclusion: Leveraging human-inspired instructions significantly enhances LLM-driven refactoring capabilities, suggesting a promising future for automated code quality improvement.

Abstract: Code refactoring is a fundamental software engineering practice aimed at
improving code quality and maintainability. Despite its importance, developers
often neglect refactoring due to the significant time, effort, and resources it
requires, as well as the lack of immediate functional rewards. Although several
automated refactoring tools have been proposed, they remain limited in
supporting a broad spectrum of refactoring types. In this study, we explore
whether instruction strategies inspired by human best-practice guidelines can
enhance the ability of Large Language Models (LLMs) to perform diverse
refactoring tasks automatically. Leveraging the instruction-following and code
comprehension capabilities of state-of-the-art LLMs (e.g., GPT-mini and
DeepSeek-V3), we draw on Martin Fowler's refactoring guidelines to design
multiple instruction strategies that encode motivations, procedural steps, and
transformation objectives for 61 well-known refactoring types. We evaluate
these strategies on benchmark examples and real-world code snippets from GitHub
projects. Our results show that instruction designs grounded in Fowler's
guidelines enable LLMs to successfully perform all benchmark refactoring types
and preserve program semantics in real-world settings, an essential criterion
for effective refactoring. Moreover, while descriptive instructions are more
interpretable to humans, our results show that rule-based instructions often
lead to better performance in specific scenarios. Interestingly, allowing
models to focus on the overall goal of refactoring, rather than prescribing a
fixed transformation type, can yield even greater improvements in code quality.

</details>


### [749] [Why Does the Engineering Manager Still Exist in Agile Software Development?](https://arxiv.org/abs/2510.03920)
*Ravi Kalluri*

Main category: cs.SE

TL;DR: The paper examines why engineering managers remain relevant in Agile organizations despite Agile's emphasis on decentralized decision-making and team autonomy, using a multidimensional framework and conceptual model.


<details>
  <summary>Details</summary>
Motivation: To investigate the paradox of engineering managers continuing to play a role in Agile organizations where managerial hierarchies are theoretically deemphasized.

Method: The authors conducted a systematic literature review and used illustrative case studies to analyze the coexistence of traditional managerial roles within Agile frameworks.

Result: The study presents a conceptual model that reconciles Agile principles with the necessity of managerial roles, highlighting their evolving nature in organizations.

Conclusion: Engineering managers persist in Agile environments due to practical necessities, and the paper provides insights for aligning Agile practices with leadership roles, suggesting implications for future research and tools.

Abstract: Although Agile methodologies emphasize decentralized decision-making and team
autonomy, engineering managers continue to be employed in Agile software
organizations. This apparent paradox suggests that traditional managerial
functions persist despite the theoretical displacement of managerial hierarchy
in Agile. This paper explores the persistence of engineering managers through a
multidimensional framework encompassing historical context, theoretical
tensions, organizational realities, empirical evidence, evolving managerial
roles, and practical implications. A systematic literature review underpins our
multifaceted analysis, supplemented by illustrative case studies. We conclude
by proposing a conceptual model that reconciles Agile principles with
managerial necessity, offering guidance for practitioners, researchers, and
tool designers. Implications for leadership development, tool integration, and
future research are discussed.

</details>


### [750] [Bamboo: LLM-Driven Discovery of API-Permission Mappings in the Android Framework](https://arxiv.org/abs/2510.04078)
*Han Hu,Wei Minn,Yonghui Liu,Jiakun Liu,Ferdian Thung,Terry Yue Zhuo,Lwin Khin Shar,Debin Gao,David Lo*

Main category: cs.SE

TL;DR: The paper introduces \tool{}, a large language model-based tool aimed at improving the identification of API-permission mappings in Android, outperforming existing methods significantly.


<details>
  <summary>Details</summary>
Motivation: Developers struggle with imprecise and incomplete Android API documentation, leading to challenges in accurate permission declaration and potential security risks in app development.

Method: The study uses large language models (LLMs), a dual-role prompting strategy, and API-driven code generation to create \tool{}, which systematically discovers API-permission mappings.

Result: \tool{} detects significantly more API-permission mappings compared to existing baselines, identifying 2,234, 3,552, and 4,576 mappings in Android versions 6, 7, and 10, respectively.

Conclusion: The results demonstrate that \tool{} is a superior tool for uncovering API-permission mappings and can aid in enhancing Android app development and security.

Abstract: The permission mechanism in the Android Framework is integral to safeguarding
the privacy of users by managing users' and processes' access to sensitive
resources and operations. As such, developers need to be equipped with an
in-depth understanding of API permissions to build robust Android apps.
Unfortunately, the official API documentation by Android chronically suffers
from imprecision and incompleteness, causing developers to spend significant
effort to accurately discern necessary permissions. This potentially leads to
incorrect permission declarations in Android app development, potentially
resulting in security violations and app failures. Recent efforts in improving
permission specification primarily leverage static and dynamic code analyses to
uncover API-permission mappings within the Android framework. Yet, these
methodologies encounter substantial shortcomings, including poor adaptability
to Android SDK and Framework updates, restricted code coverage, and a
propensity to overlook essential API-permission mappings in intricate
codebases. This paper introduces a pioneering approach utilizing large language
models (LLMs) for a systematic examination of API-permission mappings. In
addition to employing LLMs, we integrate a dual-role prompting strategy and an
API-driven code generation approach into our mapping discovery pipeline,
resulting in the development of the corresponding tool, \tool{}. We formulate
three research questions to evaluate the efficacy of \tool{} against
state-of-the-art baselines, assess the completeness of official SDK
documentation, and analyze the evolution of permission-required APIs across
different SDK releases. Our experimental results reveal that \tool{} identifies
2,234, 3,552, and 4,576 API-permission mappings in Android versions 6, 7, and
10 respectively, substantially outprforming existing baselines.

</details>


### [751] [GA4GC: Greener Agent for Greener Code via Multi-Objective Configuration Optimization](https://arxiv.org/abs/2510.04135)
*Jingzhi Gong,Yixin Bian,Luis de la Cal,Giovanni Pinna,Anisha Uteem,David Williams,Mar Zamorano,Karine Even-Mendoza,W. B. Langdon,Hector Menendez,Federica Sarro*

Main category: cs.SE

TL;DR: This paper introduces GA4GC, a framework optimized for large language model (LLM) coding agents, focusing on balancing runtime sustainability and code optimization while offering strategies for industrial use.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to address the high computational and environmental costs of deploying coding agents powered by LLMs, and to find a balance between runtime efficiency and code performance.

Method: The paper proposes GA4GC, a framework that systematically discovers Pareto-optimal hyperparameters and prompt templates for coding agents to optimize runtime and code performance trade-offs.

Result: The evaluation on the SWE-Perf benchmark shows a significant hypervolume improvement of up to 135x, with a 37.7% reduction in agent runtime while improving correctness.

Conclusion: Temperature is identified as the most critical hyperparameter, and the proposed framework provides effective strategies for balancing sustainability and optimization for industrial uses.

Abstract: Coding agents powered by LLMs face critical sustainability and scalability
challenges in industrial deployment, with single runs consuming over 100k
tokens and incurring environmental costs that may exceed optimization benefits.
This paper introduces GA4GC, the first framework to systematically optimize
coding agent runtime (greener agent) and code performance (greener code)
trade-offs by discovering Pareto-optimal agent hyperparameters and prompt
templates. Evaluation on the SWE-Perf benchmark demonstrates up to 135x
hypervolume improvement, reducing agent runtime by 37.7% while improving
correctness. Our findings establish temperature as the most critical
hyperparameter, and provide actionable strategies to balance agent
sustainability with code optimization effectiveness in industrial deployment.

</details>


### [752] [Detecting Semantic Clones of Unseen Functionality](https://arxiv.org/abs/2510.04143)
*Konstantinos Kitsios,Francesco Sovrano,Earl T. Barr,Alberto Bacchelli*

Main category: cs.SE

TL;DR: The paper addresses the challenge of semantic code clone detection for unseen functionalities, revealing significant performance drops in state-of-the-art models and proposing contrastive learning techniques to improve their generalization.


<details>
  <summary>Details</summary>
Motivation: Detecting semantic code clones is crucial for developers to identify code snippets with similar functionality, including those not represented in training data. Current models struggle with detecting clones of functionalities unseen during training, highlighting the need for improvement.

Method: The paper reviews literature to identify tasks requiring generalization to unseen functionalities, reevaluates six state-of-the-art clone detection models under these circumstances, and proposes the use of contrastive learning techniques. For task-specific models, they replace the final classifier with a contrastive classifier, and for generative LLMs, they introduce contrastive in-context learning.

Result: Performance on detecting clones of unseen functionalities drops significantly (up to 48% F1 drop for task-specific models). LLMs generalize better with smaller performance drops (up to 5%). Introducing contrastive learning improves F1 scores by up to 26% for task-specific models and up to 5% for LLMs.

Conclusion: Contrastive learning proves effective in enhancing the ability of models to generalize to unseen functionalities in code clone detection. It offers improvements for both task-specific models and generative LLMs, addressing a critical limitation in current approaches.

Abstract: Semantic code clone detection is the task of detecting whether two snippets
of code implement the same functionality (e.g., Sort Array). Recently, many
neural models achieved near-perfect performance on this task. These models seek
to make inferences based on their training data. Consequently, they better
detect clones similar to those they have seen during training and may struggle
to detect those they have not. Developers seeking clones are, of course,
interested in both types of clones. We confirm this claim through a literature
review, identifying three practical clone detection tasks in which the model's
goal is to detect clones of a functionality even if it was trained on clones of
different functionalities. In light of this finding, we re-evaluate six
state-of-the-art models, including both task-specific models and generative
LLMs, on the task of detecting clones of unseen functionality. Our experiments
reveal a drop in F1 of up to 48% (average 31%) for task-specific models. LLMs
perform on par with task-specific models without explicit training for clone
detection, but generalize better to unseen functionalities, where F1 drops up
to 5% (average 3%) instead. We propose and evaluate the use of contrastive
learning to improve the performance of existing models on clones of unseen
functionality. We draw inspiration from the computer vision and natural
language processing fields where contrastive learning excels at measuring
similarity between two objects, even if they come from classes unseen during
training. We replace the final classifier of the task-specific models with a
contrastive classifier, while for the generative LLMs we propose contrastive
in-context learning, guiding the LLMs to focus on the differences between
clones and non-clones. The F1 on clones of unseen functionality is improved by
up to 26% (average 9%) for task-specific models and up to 5% (average 3%) for
LLMs.

</details>


### [753] [Multi Language Models for On-the-Fly Syntax Highlighting](https://arxiv.org/abs/2510.04166)
*Marco Edoardo Palma,Pooja Rani,Harald C. Gall*

Main category: cs.SE

TL;DR: This paper introduces a unified, deep-learning-based syntax highlighting model to efficiently support multiple programming languages in real-time, reducing system complexities and improving performance with fewer resources.


<details>
  <summary>Details</summary>
Motivation: The challenges of delivering real-time, accurate syntax highlighting in code editors due to backend constraints and multi-language environments motivated this research.

Method: The authors developed a unified deep learning model leveraging deep abstraction, a normalization technique for better generalization, and few-shot learning to reduce data dependence.

Result: The proposed model supports six programming languages in one framework, reduces deployment complexity by six-fold, and improves performance on unseen languages using fewer resources.

Conclusion: Efficient, scalable, and cost-effective syntax highlighting across multiple languages becomes viable, simplifying deployment and enhancing practical usability.

Abstract: Syntax highlighting is a critical feature in modern software development
environments, enhancing code readability and developer productivity. However,
delivering accurate highlighting in real time remains challenging for online
and web-based development tools due to strict time and memory constraints on
backend services. These systems must serve highlights rapidly and frequently,
even when code is partially valid or invalid. This has led to on-the-fly syntax
highlighting, where visual annotations are generated just before content is
served, often at high request rates and under incomplete input conditions. To
meet these demands efficiently, state-of-the-art models use deep learning to
learn the behavior of brute-force syntax highlighting resolvers, tools that are
easy to implement but too slow for production. Through the Deep Abstraction
process, brute-force strategies are encoded into fast statistical models that
achieve both high accuracy and low-latency inference. Despite their success,
such models face key challenges: they support only one programming language per
model, require large datasets from slow brute-force generators, and involve
resource-intensive training. In multi-language environments, this means
maintaining multiple independent models, increasing system complexity and
operational cost. This work addresses these issues by introducing a unified
model capable of highlighting up to six mainstream programming languages,
reducing deployment complexity by a factor of six and improving performance on
unseen languages. A novel normalization technique significantly enhances model
generalization, while few-shot learning experiments show that a small number of
oracle samples can replace large datasets, minimizing dependence on brute-force
generators. Combined, these innovations enable efficient, scalable, and
cost-effective syntax highlighting across diverse programming languages.

</details>


### [754] [Selecting Cybersecurity Requirements: Effects of LLM Use and Professional Software Development Experience](https://arxiv.org/abs/2510.04274)
*Damjan Fujs,Damjan Vavpotič,Tomaž Hovelja,Marko Poženel*

Main category: cs.SE

TL;DR: The study examined the influence of Large Language Models (LLMs) and professional experience on cybersecurity requirements prioritization, finding no significant impact of LLMs but notable differences linked to experience.


<details>
  <summary>Details</summary>
Motivation: To understand whether access to LLMs and different levels of professional experience affect prioritization and assessment of cybersecurity requirements in web applications.

Method: The research involved twenty-three postgraduate students who used the MoSCoW method to prioritize security requirements. They were split into two groups, with one having access to LLMs. Participants rated solutions based on evaluation criteria.

Result: No noticeable differences were found between groups with or without LLM access. Experience level, however, showed statistically significant differences in criteria like cost estimation, user experience impact, and risk assessment.

Conclusion: Access to LLMs does not significantly influence prioritization or evaluation of cybersecurity solutions, but professional experience markedly impacts certain assessment criteria.

Abstract: This study investigates how access to Large Language Models (LLMs) and
varying levels of professional software development experience affect the
prioritization of cybersecurity requirements for web applications. Twenty-three
postgraduate students participated in a research study to prioritize security
requirements (SRs) using the MoSCoW method and subsequently rated their
proposed solutions against multiple evaluation criteria. We divided
participants into two groups (one with and the other without access to LLM
support during the task). Results showed no significant differences related to
LLM use, suggesting that access to LLMs did not noticeably influence how
participants evaluated cybersecurity solutions. However, statistically
significant differences emerged between experience groups for certain criteria,
such as estimated cost to develop a feature, perceived impact on user
experience, and risk assessment related to non-implementation of the proposed
feature. Participants with more professional experience tended to provide
higher ratings for user experience impact and lower risk estimates.

</details>


### [755] [Challenge on Optimization of Context Collection for Code Completion](https://arxiv.org/abs/2510.04349)
*Dmitry Ustalov,Egor Bogomolov,Alexander Bezzubov,Yaroslav Golubev,Evgeniy Glukhov,Georgii Levtsov,Vladimir Kovalenko*

Main category: cs.SE

TL;DR: This paper analyzes submissions to a competition focused on optimizing context collection for Python and Kotlin code completion using neural models and open-source datasets.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of improving code completion quality by optimizing context collection in large codebases.

Method: Efficient context collection mechanisms were developed, evaluated using permissively licensed datasets and the chrF metric across Python and Kotlin submissions.

Result: Nineteen teams participated in the Python track, eight in the Kotlin track during the public phase, and six competed in the private phase, with five submitting papers.

Conclusion: The challenge highlighted effective techniques and approaches for context collection in code repositories, fostering advancements in AI-driven software engineering tools.

Abstract: The rapid advancement of workflows and methods for software engineering using
AI emphasizes the need for a systematic evaluation and analysis of their
ability to leverage information from entire projects, particularly in large
code bases. In this challenge on optimization of context collection for code
completion, organized by JetBrains in collaboration with Mistral AI as part of
the ASE 2025 conference, participants developed efficient mechanisms for
collecting context from source code repositories to improve fill-in-the-middle
code completions for Python and Kotlin. We constructed a large dataset of
real-world code in these two programming languages using permissively licensed
open-source projects. The submissions were evaluated based on their ability to
maximize completion quality for multiple state-of-the-art neural models using
the chrF metric. During the public phase of the competition, nineteen teams
submitted solutions to the Python track and eight teams submitted solutions to
the Kotlin track. In the private phase, six teams competed, of which five
submitted papers to the workshop.

</details>


### [756] [MacroBench: A Novel Testbed for Web Automation Scripts via Large Language Models](https://arxiv.org/abs/2510.04363)
*Hyunjun Kim,Sejong Kim*

Main category: cs.SE

TL;DR: MacroBench evaluates large language models (LLMs) for generating reusable web automation programs from natural language commands. The benchmark covers diverse websites and tasks, with results indicating high variability in model performance.


<details>
  <summary>Details</summary>
Motivation: The research aims to test the ability of large language models to automate web-based tasks by synthesizing functional code that interacts with web elements.

Method: The team created MacroBench, a benchmark that involves seven simulated websites and 681 tasks. They used a protocol with validation methods including static checks, sandbox execution, and outcome verification.

Result: Key results showed stratified model performance: GPT-4o-Mini achieved 96.8%, GPT-4.1 95.3%, Gemini-2.5-Pro 89.0%, and DeepSeek-V3.1 83.4%. Models were effective for simple tasks but failed for complex workflows and coding best practices.

Conclusion: MacroBench highlights the potential and limitations of LLMs in generating web automation programs. While promising for simple tasks, models still struggle with complex workflows, and their code quality does not meet production standards.

Abstract: We introduce MacroBench, a code-first benchmark that evaluates whether LLMs
can synthesize reusable browser automation programs from natural language goals
by reading HTML/DOM and emitting Python with Selenium. MacroBench instantiates
seven self-hosted sites: Airbnb-like, TikTok-like, Reddit-like, Instagram-like,
Facebook-like, Discord-like, and Threads-like, covering 681 tasks across
interaction complexity and targeting difficulty. Our end-to-end protocol
validates generated code via static checks, sandboxed execution, and outcome
verification including DOM assertions and database snapshots, and includes a
safety suite for scraping, spam/abuse, and credential/privacy prompts. Across
2636 model-task runs, we observe stratified success: GPT-4o-Mini achieves 96.8
percent, GPT-4.1 achieves 95.3 percent, Gemini-2.5-Pro achieves 89.0 percent,
and DeepSeek-V3.1 achieves 83.4 percent. Models handle simple tasks reliably at
91.7 percent but fail on complex workflows at 0.0 percent, and none meet
production-quality coding practices despite functional completion. We release
our complete benchmark pipeline, evaluation framework, and experimental results
to enable reproducible assessment of macro synthesis for web automation.

</details>


### [757] [Reconsidering Requirements Engineering: Human-AI Collaboration in AI-Native Software Development](https://arxiv.org/abs/2510.04380)
*Mateen Ahmed Abbasi,Petri Ihantola,Tommi Mikkonen,Niko Mäkitalo*

Main category: cs.SE

TL;DR: The paper investigates the role of Artificial Intelligence (AI) in improving Requirement Engineering (RE), highlighting its benefits and challenges.


<details>
  <summary>Details</summary>
Motivation: Persistent challenges in RE, such as ambiguity, conflicting stakeholder needs, and evolving requirements, highlight the need for more efficient and accurate approaches.

Method: The paper examines how AI can automate tasks, assist in prioritization, and improve collaboration while addressing ethical concerns and biases.

Result: AI offers benefits such as automating tedious tasks and improving collaboration but also introduces challenges like ethical concerns and lack of transparency.

Conclusion: To make AI solutions practical and trustworthy in RE, ethical practices and collaboration between academia and industry are essential.

Abstract: Requirement Engineering (RE) is the foundation of successful software
development. In RE, the goal is to ensure that implemented systems satisfy
stakeholder needs through rigorous requirements elicitation, validation, and
evaluation processes. Despite its critical role, RE continues to face
persistent challenges, such as ambiguity, conflicting stakeholder needs, and
the complexity of managing evolving requirements. A common view is that
Artificial Intelligence (AI) has the potential to streamline the RE process,
resulting in improved efficiency, accuracy, and management actions. However,
using AI also introduces new concerns, such as ethical issues, biases, and lack
of transparency. This paper explores how AI can enhance traditional RE
practices by automating labor-intensive tasks, supporting requirement
prioritization, and facilitating collaboration between stakeholders and AI
systems. The paper also describes the opportunities and challenges that AI
brings to RE. In particular, the vision calls for ethical practices in AI,
along with a much-enhanced collaboration between academia and industry
professionals. The focus should be on creating not only powerful but also
trustworthy and practical AI solutions ready to adapt to the fast-paced world
of software development.

</details>


### [758] [Smart Hiring Redefined: An Intelligent Recruitment Management Platform](https://arxiv.org/abs/2510.04437)
*Fangzhe Wu,Dongyang Lyu,Xiaoqi Li*

Main category: cs.SE

TL;DR: The paper discusses intelligent recruitment management systems as a solution to inefficiencies in traditional hiring practices, highlighting their automation and data-driven capabilities.


<details>
  <summary>Details</summary>
Motivation: The motivation is rooted in addressing inefficiencies, high costs, and information asymmetry in traditional recruitment models in the context of digital transformation.

Method: The paper focuses on intelligent recruitment systems, which automate tasks like resume screening, candidate matching, and interview scheduling using data-driven approaches.

Result: Intelligent systems improve the efficiency and accuracy of recruitment processes by automating labor-intensive tasks and handling large data volumes effectively.

Conclusion: Intelligent recruitment management systems are essential for modern organizations to optimize hiring processes and enhance competitiveness through innovation.

Abstract: Against the backdrop of deepening digital and intelligent transformation in
human resource management, traditional recruitment models struggle to fully
meet enterprises' growing demand for precise talent acquisition due to limited
efficiency, high costs, and information asymmetry. As a vital tool for
optimizing recruitment processes, reducing labor and time costs, and enhancing
core competitiveness, intelligent recruitment management systems become an
indispensable component of modern organizational talent strategies.Compared
with the labor intensive tasks of resume screening, candidate position
matching, and interview coordination in traditional manual recruitment,
intelligent recruitment systems significantly enhance the efficiency and
accuracy of the hiring process through automation and data driven approaches.
These systems enable rapid parsing of massive resume volumes, intelligent
matching of candidates to positions, and automated scheduling of interview
processes.

</details>


### [759] [Improving IR-based Bug Localization with Semantics-Driven Query Reduction](https://arxiv.org/abs/2510.04468)
*Asif Mohammed Samir,Mohammad Masudur Rahman*

Main category: cs.SE

TL;DR: The study introduces IQLoc, a bug localization approach that combines Information Retrieval (IR) and Large Language Model (LLM)-based techniques to improve bug localization accuracy. It uses transformer models for program semantics and enhances IR query reformulations.


<details>
  <summary>Details</summary>
Motivation: Bug localization is challenging due to ambiguous and heterogeneous bug reports. Current IR-based methods lack context and semantic understanding of code, and LLMs have yet to be effectively applied to the task.

Method: IQLoc leverages transformer-based models to understand program semantics and uses this understanding to reformulate IR queries. It evaluates its effectiveness against a refined and expanded benchmark dataset.

Result: IQLoc achieved significant performance improvements over baseline methods, with up to 60.59% in MAP, 64.58% in MRR, and 100.90% in HIT@K metrics under time-wise splits. Specific improvements for bug reports with stack traces (91.67%) and other scenarios were also highlighted.

Conclusion: By combining program semantics from LLMs with IR-based techniques, IQLoc addresses key limitations of traditional methods and enhances bug localization performance significantly.

Abstract: Despite decades of research, software bug localization remains challenging
due to heterogeneous content and inherent ambiguities in bug reports. Existing
methods such as Information Retrieval (IR)-based approaches often attempt to
match source documents to bug reports, overlooking the context and semantics of
the source code. On the other hand, Large Language Models (LLM) (e.g.,
Transformer models) show promising results in understanding both texts and
code. However, they have not been yet adapted well to localize software bugs
against bug reports. They could be also data or resource-intensive. To bridge
this gap, we propose, IQLoc, a novel bug localization approach that capitalizes
on the strengths of both IR and LLM-based approaches. In particular, we
leverage the program semantics understanding of transformer-based models to
reason about the suspiciousness of code and reformulate queries during bug
localization using Information Retrieval. To evaluate IQLoc, we refine the
Bench4BL benchmark dataset and extend it by incorporating ~30% more recent bug
reports, resulting in a benchmark containing ~7.5K bug reports. We evaluated
IQLoc using three performance metrics and compare it against four baseline
techniques. Experimental results demonstrate its superiority, achieving up to
58.52% and 60.59% in MAP, 61.49% and 64.58% in MRR, and 69.88% and 100.90% in
HIT@K for the test bug reports with random and time-wise splits, respectively.
Moreover, IQLoc improves MAP by 91.67% for bug reports with stack traces,
72.73% for those that include code elements, and 65.38% for those containing
only descriptions in natural language. By integrating program semantic
understanding into Information Retrieval, IQLoc mitigates several longstanding
challenges of traditional IR-based approaches in bug localization.

</details>


### [760] [DynamiQ: Unlocking the Potential of Dynamic Task Allocation in Parallel Fuzzing](https://arxiv.org/abs/2510.04469)
*Wenqi Yan,Toby Murray,Benjamin Rubinstein,Van-Thuan Pham*

Main category: cs.SE

TL;DR: DynamiQ is an advanced, optimized parallel fuzzing tool leveraging runtime feedback and program call graph information to improve efficiency and results compared to state-of-the-art approaches.


<details>
  <summary>Details</summary>
Motivation: Enhance fuzzing efficiency at scale by addressing redundancy in exploration and optimizing task allocation based on structural program insights.

Method: DynamiQ uses call graph-based task allocation and runtime feedback-driven refinements, combined with optimizations in the LibAFL framework for task-aware parallel fuzzing.

Result: DynamiQ achieves superior code coverage and vulnerability discovery, identifying 9 new bugs in widely used open-source software, and outperforming leading parallel fuzzers.

Conclusion: DynamiQ effectively boosts parallel fuzzing performance, validating its design and optimizations for real-world applicability in improving software security.

Abstract: We present DynamiQ, a full-fledged and optimized successor to AFLTeam that
supports dynamic and adaptive parallel fuzzing. Unlike most existing approaches
that treat individual seeds as tasks, DynamiQ leverages structural information
from the program's call graph to define tasks and continuously refines task
allocation using runtime feedback. This design significantly reduces redundant
exploration and enhances fuzzing efficiency at scale. Built on top of the
state-of-the-art LibAFL framework, DynamiQ incorporates several practical
optimizations in both task allocation and task-aware fuzzing. Evaluated on 12
real-world targets from OSS-Fuzz and FuzzBench over 25,000 CPU hours, DynamiQ
outperforms state-of-the-art parallel fuzzers in both code coverage and
vulnerability discovery, uncovering 9 previously unknown bugs in widely used
and extensively fuzzed open-source software.

</details>


### [761] [Detecting and Characterizing Low and No Functionality Packages in the NPM Ecosystem](https://arxiv.org/abs/2510.04495)
*Napasorn Tevarut,Brittany Reid,Yutaro Kashiwa,Pattara Leelaprute,Arnon Rungsawang,Bundit Manaskasemsak,Hajimu Iida*

Main category: cs.SE

TL;DR: The paper analyzes trivial and data-only npm packages, finding they pose significant security risks and offer a detection tool with 94% accuracy.


<details>
  <summary>Details</summary>
Motivation: To address security risks and issues posed by trivial and data-only npm packages in software dependency management.

Method: A rule-based static analysis method was developed to detect trivial and data-only npm packages, and the ecosystem was evaluated at scale.

Result: The analysis found 17.92% of npm packages are trivial, with vulnerabilities similar to complex packages, while data-only packages also pose risks. The tool achieves 94% accuracy with a macro-F1 score of 0.87.

Conclusion: Trivial and data-only npm packages require increased focus for robust dependency management, as their prevalence and risks contribute to potential technical debt and security vulnerabilities.

Abstract: Trivial packages, small modules with low functionality, are common in the npm
ecosystem and can pose security risks despite their simplicity. This paper
refines existing definitions and introduce data-only packages that contain no
executable logic. A rule-based static analysis method is developed to detect
trivial and data-only packages and evaluate their prevalence and associated
risks in the 2025 npm ecosystem. The analysis shows that 17.92% of packages are
trivial, with vulnerability levels comparable to non-trivial ones, and
data-only packages, though rare, also contain risks. The proposed detection
tool achieves 94% accuracy (macro-F1 0.87), enabling effective large-scale
analysis to reduce security exposure. This findings suggest that trivial and
data-only packages warrant greater attention in dependency management to reduce
potential technical debt and security exposure.

</details>


### [762] [Spec2Control: Automating PLC/DCS Control-Logic Engineering from Natural Language Requirements with LLMs - A Multi-Plant Evaluation](https://arxiv.org/abs/2510.04519)
*Heiko Koziolek,Thilo Braun,Virendra Ashiwal,Sofia Linsbauer,Marthe Ahlgreen Hansen,Karoline Grotterud*

Main category: cs.SE

TL;DR: Spec2Control is an automated workflow using large language models (LLMs) to generate graphical control logic for Distributed Control Systems (DCS) based on natural language inputs, significantly reducing manual effort.


<details>
  <summary>Details</summary>
Motivation: Programming DCS software is time-intensive and costly due to its largely manual nature, creating a need for automated solutions to reduce costs and improve efficiency.

Method: A workflow called Spec2Control is introduced, which leverages LLMs to directly translate natural language user requirements into graphical control logic. The approach was tested using an open dataset.

Result: Spec2Control successfully identified control strategies and produced 98.6% correct control strategy connections autonomously, saving 94-96% of human labor in experiments involving 10 narratives and 65 test cases.

Conclusion: The approach offers a high level of automation for DCS programming with considerable time and cost savings, and has applications in both commercial and open-source domains for validation and deployment.

Abstract: Distributed control systems (DCS) manage the automation for many industrial
production processes (e.g., power plants, chemical refineries, steel mills).
Programming the software for such systems remains a largely manual and tedious
process, incurring costs of millions of dollars for extensive facilities. Large
language models (LLMs) have been found helpful in generating DCS control logic,
resulting in commercial copilot tools. Today, these tools are focused on
textual notations, they provide limited automation, and have not been tested on
large datasets with realistic test cases. We introduce Spec2Control, a highly
automated LLM workflow to generate graphical control logic directly from
natural language user requirements. Experiments using an open dataset with 10
control narratives and 65 complex test cases demonstrate that Spec2Control can
successfully identify control strategies, can generate 98.6% of correct control
strategy connections autonomously, and can save between 94-96% of human labor.
Spec2Control is being integrated into commercial ABB engineering tools, but is
also available as an open-source variant for independent validation.

</details>


### [763] [Advancing Digital Government: Integrating Open Source Software Enablement Indicators in Maturity Indexes](https://arxiv.org/abs/2510.04603)
*Johan Linåker,Sachiko Muto*

Main category: cs.SE

TL;DR: The paper investigates government adoption of Open Source Software (OSS) in 16 digitally advanced countries, emphasizing its role in digital transformation and proposing indicators to assess adoption within digital maturity frameworks.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the lack of systematic measurement of governmental OSS adoption despite its critical role in modern software, national growth, and public sector transformation.

Method: Policies were analyzed using a combination of desk research and semi-structured interviews with government officials, producing detailed country reports. A cross-analysis was performed focusing on OSS promotion, rationales, and implementation.

Result: OSS policies targeting reuse and sharing are prevalent, overseen by central public agencies. They focus on goals like interoperability and cost efficiency. Implementation is bolstered by Open Source Program Offices (OSPOs), with synthesized indicators proposed across 14 domains.

Conclusion: OSS plays a key role in public sector digital transformation. Robust policy frameworks and institutional backing are essential. Digital maturity indexes should broaden OSS indicators to enhance government adoption assessment.

Abstract: Context: Open Source Software (OSS) is a vital public good, included across
most of modern software stacks, significantly impacting GDP and national tech
growth, while supporting interoperability, sovereignty, and transparency.
However, systematic measurement of governmental OSS adoption remain limited.
  Research Aim: This study contributes to digital government maturity indexes
by analyzing policies and support actions leveraging OSS for software reuse and
collaborative development across 16 digitally mature countries, and proposing
potential indicators for said indexes. It examines OSS policy formation, stated
goals, key actors, and support mechanisms.
  Methodology: A qualitative approach is used combining desk research of policy
documents with semi-structured interviews of government representatives,
producing detailed country reports. These are cross-analyzed, focusing on OSS
policy promotion, rationale, and implementation support.
  Results: Policies facilitating OSS reuse are widespread, targeting both
inbound acquisition and outbound sharing, and are predominantly governed by
central public sector organizations. Policy goals include interoperability,
digital sovereignty, transparency, and cost efficiency, with security framed
both as a risk and strength. Implementation is supported by diverse Open Source
Program Offices (OSPOs) at multiple government levels, which foster capacity
building, resource pooling, and sustainable project governance. Indicators are
synthesized and proposed across 14 areas covering policy incentives and design,
and implementation and support.
  Conclusions: OSS is a strategic enabler for public sector digital
transformation. Clear policy frameworks, coupled with institutional support
such as OSPOs, are essential. International digital maturity frameworks should
expand OSS indicators to better guide and assess government adoption and
impact.

</details>


### [764] [Exploring the Power of Diffusion Large Language Models for Software Engineering: An Empirical Investigation](https://arxiv.org/abs/2510.04605)
*Jingyao Zhang,Tianlin Li,Xiaoyu Zhang,Qiang Hu,Bin Shi*

Main category: cs.SE

TL;DR: The study evaluates Diffusion Large Language Models (DLLMs) for software engineering tasks, showing they outperform Autoregressive Large Language Models (AR-LLMs) in accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Current Autoregressive LLMs face challenges such as limited code structure understanding and high inference latency, prompting exploration of new modeling approaches.

Method: The paper conducted a comprehensive evaluation of DLLMs across key software engineering tasks using a large-scale benchmark of 52,937 tasks.

Result: DLLMs achieved a 30% average improvement in accuracy over AR-LLMs, a 113% gain in cross-file repair tasks, alongside reduced inference latency and improved efficiency.

Conclusion: DLLMs offer significant advantages over traditional AR-LLMs in handling software engineering tasks, establishing them as a superior modeling approach.

Abstract: Autoregressive Large Language Models (AR-LLMs) are widely used in software
engineering (SE) but face limitations in processing code structure information
and suffer from high inference latency. Diffusion LLMs (DLLMs) offer a
promising alternative with global bidirectional encoding and decoupled
generation steps. This work presents the first comprehensive evaluation of
DLLMs across the software development lifecycle, including code generation,
defect detection, and program repair. On a large-scale benchmark of 52,937
tasks, 7Bparameter DLLMs outperform AR-LLMs with a 30% average accuracy
improvement achieving a 113% gain on cross-file repair, while maintaining
superior efficiency and reduced latency. Our results establish DLLMs as a
superior paradigm for SE tasks.

</details>


### [765] [A survey on the impact of emotions on the productivity among software developers](https://arxiv.org/abs/2510.04611)
*Pawel Weichbroth,Maciej Lotysz,Michal Wrobel*

Main category: cs.SE

TL;DR: The study reveals a strong positive relationship between emotional state and perceived productivity among software developers.


<details>
  <summary>Details</summary>
Motivation: Investigate whether emotions affect perceived productivity and their impact on software development performance.

Method: A two-stage survey involving expert validation and a broader pool of 88 software developers with data analysis using Partial Least Squares.

Result: Empirical evidence confirmed developers' emotional state has a strong positive effect (beta = 0.893, p < 0.001) on perceived productivity.

Conclusion: Managing developers' emotional well-being is critical for enhancing productivity and addressing stress or burnout effectively.

Abstract: The time pressure associated with software development, among other factors,
often leads to a diminished emotional state among developers. However, whether
emotions affect perceived productivity remains an open question. This study
aims to determine the strength and direction of the relationship between
emotional state and perceived productivity among software developers. We
employed a two-stage approach. First, a survey was conducted with a pool of
nine experts to validate the measurement model. Second, a survey was
administered to a pool of 88 software developers to empirically test the
formulated hypothesis by using Partial Least Squares, as the data analysis
method. The results of the path analysis clearly confirm the formulated
hypothesis, showing that the emotional state of a software developer has a
strong positive, and significant impact (beta = 0.893, p < 0.001) on perceived
productivity among software developers. The findings highlight the importance
of managing and improving developers emotional well-being to enhance
productivity in software development environments. Additionally, interventions
aimed at reducing burnout, stress, and other negative factors could have a
considerable impact on their performance outcomes.

</details>


### [766] [Evolaris: A Roadmap to Self-Evolving Software Intelligence Management](https://arxiv.org/abs/2510.04689)
*Chengwei Liu,Wenbo Guo,Yuxin Zhang,Limin Wang,Sen Chen,Lei Bu,Yang Liu*

Main category: cs.SE

TL;DR: The paper introduces Evolaris, a multi-agent system for real-time collection, analysis, and adaptation to emerging software threats sourced from fragmented, informal channels.


<details>
  <summary>Details</summary>
Motivation: Emergent critical threat intelligence is increasingly found in informal and fragmented data sources such as social media, forums, and underground communities rather than formal databases, posing challenges for security maintenance and responses.

Method: The proposed solution involves Evolaris, a self-evolving multi-agent system with agents independently performing discovery, reasoning, validation, and risk detection tasks while refining their internal knowledge over time.

Result: Evolaris provides improved precision, scalability, and adaptability in analyzing software threats, learning and evolving continuously to address emerging patterns.

Conclusion: Evolaris lays a foundation for sustainable, proactive security decision-making and enhances threat understanding in the cybersecurity ecosystem.

Abstract: In recent years, the landscape of software threats has become significantly
more dynamic and distributed. Security vulnerabilities are no longer discovered
and shared only through formal channels such as public vulnerability databases
or vendor advisories. Increasingly, criti- cal threat information emerges
informally through blogs, social media, developer forums, open source
repositories, and even underground com- munities. To this end, capturing such
intelligence in a timely manner is essential for maintaining situational
awareness and enabling prompt security responses. However, this remains a
complex challenge due to the fragmented nature of data sources and the
technical difficulty of collecting, parsing, mapping, and validating
information at scale. To ad- dress this, we propose Evolaris, a self-evolving
software intelligence sys- tem built on a multi-agent framework. Evolaris is
designed to support a full-stack workflow, where agents operate independently
but coordinate through shared context to perform tasks such as information
discovery, reasoning, gap completion, validation, and risk detection. This
archi- tecture enables the platform to learn from new inputs, refine its
internal knowledge, and adapt to emerging threat patterns over time, which
could continuously improve the precision, timeliness, and scalability of
software threat analysis, and offers a sustainable foundation for proactive
secu- rity decision-making and strengthens the broader ecosystem of security
threat understanding.

</details>


### [767] [An Empirical Study of SOTA RCA Models: From Oversimplified Benchmarks to Realistic Failures](https://arxiv.org/abs/2510.04711)
*Aoyang Fang,Songhan Zhang,Yifan Yang,Haotong Wu,Junjielong Xu,Xuyang Wang,Rui Wang,Manyi Wang,Qisheng Lu,Pinjia He*

Main category: cs.SE

TL;DR: Current Root Cause Analysis (RCA) benchmarks for microservices may overestimate model performance due to oversimplification. This research introduces a new, realistic RCA framework and dataset, revealing performance challenges in state-of-the-art (SOTA) models.


<details>
  <summary>Details</summary>
Motivation: Address the over-simplicity of existing RCA benchmarks that fail to reflect the complexity of real-world cloud-native microservice architectures.

Method: Analyzed limitations in existing RCA benchmarks, designed an automated framework to generate realistic benchmarks, conducted fault injections, and validated failure cases while testing performance of SOTA models.

Result: Generated a realistic dataset of 1,430 failure cases. Re-evaluation of 11 SOTA models on this dataset revealed average Top@1 accuracy of 0.21 and highlighted major performance issues.

Conclusion: Current RCA models struggle with scalability, observability, and modeling challenges on realistic benchmarks, urging the need for more sophisticated benchmarking and model design.

Abstract: While cloud-native microservice architectures have transformed software
development, their complexity makes Root Cause Analysis (RCA) both crucial and
challenging. Although many data-driven RCA models have been proposed, we find
that existing benchmarks are often oversimplified and fail to capture
real-world conditions. Our preliminary study shows that simple rule-based
methods can match or even outperform state-of-the-art (SOTA) models on four
widely used benchmarks, suggesting performance overestimation due to benchmark
simplicity. To address this, we systematically analyze popular RCA benchmarks
and identify key limitations in fault injection, call graph design, and
telemetry patterns. Based on these insights, we develop an automated framework
to generate more realistic benchmarks, yielding a dataset of 1,430 validated
failure cases from 9,152 injections, covering 25 fault types under dynamic
workloads with hierarchical ground-truth labels and verified SLI impact.
Re-evaluation of 11 SOTA models on this dataset shows low Top@1 accuracy
(average 0.21, best 0.37) and significantly longer execution times. Our
analysis highlights three common failure patterns: scalability issues,
observability blind spots, and modeling bottlenecks.

</details>


### [768] [Agile Software Effort Estimation using Regression Techniques](https://arxiv.org/abs/2510.04760)
*Sisay Deresa Sima,Ayalew Belay Habtie*

Main category: cs.SE

TL;DR: The paper develops an agile effort estimation model using LASSO and Elastic Net regression techniques and benchmarks their effectiveness using metrics like MMRE and MSE.


<details>
  <summary>Details</summary>
Motivation: Accurate software development effort estimation is crucial for the success of software projects, especially in agile methodologies where estimation remains challenging.

Method: The study trains LASSO and Elastic Net regression models using default parameters and optimized grid search with 5-fold cross-validation, applied across 21 project datasets from six firms.

Result: LASSO regression showed better performance with metrics such as a perfect PRED (8%) & PRED (25%) scores of 100.0, MMRE of 0.0491, MMER of 0.0551, and MSE of 0.0007.

Conclusion: LASSO regression offers superior predictive accuracy in agile effort estimation compared to other models and benchmarks found in related literature.

Abstract: Software development effort estimation is one of the most critical aspect in
software development process, as the success or failure of the entire project
depends on the accuracy of estimations. Researchers are still conducting
studies on agile effort estimation. The aim of this research is to develop a
story point based agile effort estimation model using LASSO and Elastic Net
regression techniques. The experimental work is applied to the agile story
point approach using 21 software projects collected from six firms. The two
algorithms are trained using their default parameters and tuned grid search
with 5-fold cross-validation to get an enhanced model. The experiment result
shows LASSO regression achieved better predictive performance PRED (8%) and
PRED (25%) results of 100.0, MMRE of 0.0491, MMER of 0.0551, MdMRE of 0.0593,
MdMER of 0.063, and MSE of 0.0007. The results are also compared with other
related literature.

</details>


### [769] [GUISpector: An MLLM Agent Framework for Automated Verification of Natural Language Requirements in GUI Prototypes](https://arxiv.org/abs/2510.04791)
*Kristian Kolthoff,Felix Kretzer,Simone Paolo Ponzetto,Alexander Maedche,Christian Bartelt*

Main category: cs.SE

TL;DR: GUISpector is a framework leveraging multi-modal large language models for verifying natural language (NL) requirements in GUI prototypes, providing actionable feedback and integration capabilities.


<details>
  <summary>Details</summary>
Motivation: The motivation lies in addressing the limitations of current GUI testing approaches, particularly the complexity of modern interfaces and their lack of actionable feedback and effective integration mechanisms with automated development agents.

Method: GUISpector uses a multi-modal large language model (MLLM) agent to operationalize NL requirements, autonomously verify GUI compliance, and generate detailed feedback for iterative improvement and integration in LLM-driven development workflows.

Result: GUISpector was evaluated with 150 requirements and 900 acceptance criteria across diverse GUI applications, demonstrating effective detection of requirement satisfaction and violations.

Conclusion: GUISpector showcases a robust solution for automating GUI requirement verification, offering actionable insights and streamlined integration into LLM-driven workflows, ultimately paving the way for better GUI software engineering practices.

Abstract: GUIs are foundational to interactive systems and play a pivotal role in early
requirements elicitation through prototyping. Ensuring that GUI implementations
fulfill NL requirements is essential for robust software engineering,
especially as LLM-driven programming agents become increasingly integrated into
development workflows. Existing GUI testing approaches, whether traditional or
LLM-driven, often fall short in handling the complexity of modern interfaces,
and typically lack actionable feedback and effective integration with automated
development agents. In this paper, we introduce GUISpector, a novel framework
that leverages a multi-modal (M)LLM-based agent for the automated verification
of NL requirements in GUI prototypes. First, GUISpector adapts a MLLM agent to
interpret and operationalize NL requirements, enabling to autonomously plan and
execute verification trajectories across GUI applications. Second, GUISpector
systematically extracts detailed NL feedback from the agent's verification
process, providing developers with actionable insights that can be used to
iteratively refine the GUI artifact or directly inform LLM-based code
generation in a closed feedback loop. Third, we present an integrated tool that
unifies these capabilities, offering practitioners an accessible interface for
supervising verification runs, inspecting agent rationales and managing the
end-to-end requirements verification process. We evaluated GUISpector on a
comprehensive set of 150 requirements based on 900 acceptance criteria
annotations across diverse GUI applications, demonstrating effective detection
of requirement satisfaction and violations and highlighting its potential for
seamless integration of actionable feedback into automated LLM-driven
development workflows. The video presentation of GUISpector is available at:
https://youtu.be/JByYF6BNQeE, showcasing its main capabilities.

</details>


### [770] [RevMine: An LLM-Assisted Tool for Code Review Mining and Analysis Across Git Platforms](https://arxiv.org/abs/2510.04796)
*Samah Kansab,Francis Bordeleau,Ali Tizghadam*

Main category: cs.SE

TL;DR: The paper introduces RevMine, a tool leveraging large language models to simplify the process of mining and analyzing code review data, aiming to democratize the practice and advance software engineering research.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of time-consuming and technically intensive workflows involved in collecting and analyzing code review data, and to democratize access to this research domain.

Method: RevMine employs large language models to automate tasks such as data retrieval, endpoint discovery, and analysis, guided by natural language inputs, thereby streamlining code review mining.

Result: RevMine lowers the technical barrier, allowing both quantitative and qualitative analysis of code review data without extensive manual scripting efforts.

Conclusion: The tool enhances accessibility to code review analysis, broadening the scope for empirical software engineering studies and fostering collaboration.

Abstract: Empirical research on code review processes is increasingly central to
understanding software quality and collaboration. However, collecting and
analyzing review data remains a time-consuming and technically intensive task.
Most researchers follow similar workflows - writing ad hoc scripts to extract,
filter, and analyze review data from platforms like GitHub and GitLab. This
paper introduces RevMine, a conceptual tool that streamlines the entire code
review mining pipeline using large language models (LLMs). RevMine guides users
through authentication, endpoint discovery, and natural language-driven data
collection, significantly reducing the need for manual scripting. After
retrieving review data, it supports both quantitative and qualitative analysis
based on user-defined filters or LLM-inferred patterns. This poster outlines
the tool's architecture, use cases, and research potential. By lowering the
barrier to entry, RevMine aims to democratize code review mining and enable a
broader range of empirical software engineering studies.

</details>


### [771] [InsightQL: Advancing Human-Assisted Fuzzing with a Unified Code Database and Parameterized Query Interface](https://arxiv.org/abs/2510.04835)
*Wentao Gao,Renata Borovica-Gajic,Sang Kil Cha,Tian Qiu,Van-Thuan Pham*

Main category: cs.SE

TL;DR: The paper introduces InsightQL, a framework designed to tackle the limits of fuzzing techniques caused by fuzz blockers, significantly improving code coverage.


<details>
  <summary>Details</summary>
Motivation: Fuzzing is limited by coverage plateaus caused by fuzz blockers, requiring a labor-intensive human analysis to address them effectively.

Method: InsightQL is implemented as a human-assisting framework featuring a unified database and intuitive query interface for systematic fuzz blocker analysis.

Result: InsightQL was tested on 14 real-world libraries, showing its ability to unblock fuzz blockers and improve code coverage by up to 13.90%.

Conclusion: InsightQL provides an effective solution for resolving fuzz blockers, enabling developers to achieve deeper code coverage through streamlined analysis and intervention.

Abstract: Fuzzing is a highly effective automated testing method for uncovering
software vulnerabilities. Despite advances in fuzzing techniques, such as
coverage-guided greybox fuzzing, many fuzzers struggle with coverage plateaus
caused by fuzz blockers, limiting their ability to find deeper vulnerabilities.
Human expertise can address these challenges, but analyzing fuzzing results to
guide this support remains labor-intensive. To tackle this, we introduce
InsightQL, the first human-assisting framework for fuzz blocker analysis.
Powered by a unified database and an intuitive parameterized query interface,
InsightQL aids developers in systematically extracting insights and efficiently
unblocking fuzz blockers. Our experiments on 14 popular real-world libraries
from the FuzzBench benchmark demonstrate the effectiveness of InsightQL,
leading to the unblocking of many fuzz blockers and considerable improvements
in code coverage (up to 13.90%).

</details>


### [772] [FreshBrew: A Benchmark for Evaluating AI Agents on Java Code Migration](https://arxiv.org/abs/2510.04852)
*Victor May,Diganta Misra,Yanqi Luo,Anjali Sridhar,Justine Gehring,Silvio Soares Ribeiro Junior*

Main category: cs.SE

TL;DR: The paper introduces FreshBrew, a benchmark to evaluate AI agents for Java code migration, and finds existing LLMs moderately effective, with Gemini 2.5 Flash leading results.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the growing need to automate software codebase migrations and modernization using AI, as manual and rule-based systems are inefficient and outdated.

Method: The authors developed the FreshBrew benchmark to systematically assess AI agent performance in Java project-level migrations, focusing on semantic preservation and avoidance of reward hacking.

Result: FreshBrew evaluation showed the Gemini 2.5 Flash model successfully migrated 52.3% of Java projects to JDK 17, outperforming rule-based systems.

Conclusion: FreshBrew provides a rigorous platform to evaluate and improve AI agents for code modernization, highlighting key strengths and failure modes of LLM-driven approaches.

Abstract: AI coding assistants are rapidly becoming integral to modern software
development. A key challenge in this space is the continual need to migrate and
modernize codebases in response to evolving software ecosystems. Traditionally,
such migrations have relied on rule-based systems and human intervention. With
the advent of powerful large language models (LLMs), AI-driven agentic
frameworks offer a promising alternative-but their effectiveness has not been
systematically evaluated. In this paper, we introduce FreshBrew, a novel
benchmark for evaluating AI agents on project-level Java migrations, with a
specific focus on measuring an agent's ability to preserve program semantics
and avoid reward hacking, which we argue requires projects with high test
coverage for a rigorous and reliable evaluation. We benchmark several
state-of-the-art LLMs, and compare their performance against established
rule-based tools. Our evaluation of AI agents on this benchmark of 228
repositories shows that the top-performing model, Gemini 2.5 Flash, can
successfully migrate 52.3 percent of projects to JDK 17. Our empirical analysis
reveals novel insights into the critical strengths and limitations of current
agentic approaches, offering actionable insights into their real-world
applicability. Our empirical study reveals failure modes of current AI agents
in realistic Java modernization tasks, providing a foundation for evaluating
trustworthy code-migration systems. By releasing FreshBrew, we aim to
facilitate rigorous, reproducible evaluation and catalyze progress in AI-driven
codebase modernization.

</details>


### [773] [Retrieval-Augmented Code Generation: A Survey with Focus on Repository-Level Approaches](https://arxiv.org/abs/2510.04905)
*Yicheng Tao,Yao Qin,Yepang Liu*

Main category: cs.SE

TL;DR: This paper surveys Retrieval-Augmented Code Generation (RACG) with a focus on repository-level code generation (RLCG), providing insights into generation strategies, retrieval methods, model architectures, datasets, and evaluation mechanisms.


<details>
  <summary>Details</summary>
Motivation: The complexity of real-world software development demands reasoning across entire code repositories, which involves addressing long-range dependencies and ensuring global semantic consistency, challenging for current code generation models.

Method: The authors systematically review existing work in RACG, categorize advancements across various dimensions such as retrieval modalities and training paradigms, and synthesize findings into a unified framework.

Result: The survey identifies gaps in current research, highlights widely used benchmarks and datasets, and presents a structured understanding of RACG advancements.

Conclusion: By organizing existing research and pinpointing challenges, the paper aims to inspire further development in repository-level code generation using RACG, advancing the capabilities of AI in software engineering.

Abstract: Recent advancements in large language models (LLMs) have substantially
improved automated code generation. While function-level and file-level
generation have achieved promising results, real-world software development
typically requires reasoning across entire repositories. This gives rise to the
challenging task of Repository-Level Code Generation (RLCG), where models must
capture long-range dependencies, ensure global semantic consistency, and
generate coherent code spanning multiple files or modules. To address these
challenges, Retrieval-Augmented Generation (RAG) has emerged as a powerful
paradigm that integrates external retrieval mechanisms with LLMs, enhancing
context-awareness and scalability. In this survey, we provide a comprehensive
review of research on Retrieval-Augmented Code Generation (RACG), with an
emphasis on repository-level approaches. We categorize existing work along
several dimensions, including generation strategies, retrieval modalities,
model architectures, training paradigms, and evaluation protocols. Furthermore,
we summarize widely used datasets and benchmarks, analyze current limitations,
and outline key challenges and opportunities for future research. Our goal is
to establish a unified analytical framework for understanding this rapidly
evolving field and to inspire continued progress in AI-powered software
engineering.

</details>


### [774] [Why Software Signing (Still) Matters: Trust Boundaries in the Software Supply Chain](https://arxiv.org/abs/2510.04964)
*Kelechi G. Kalu,James C. Davis*

Main category: cs.SE

TL;DR: The paper discusses the role and necessity of software signing in ensuring artifact integrity and verifying producer identity, especially considering modern software distribution boundaries where registry security may fall short.


<details>
  <summary>Details</summary>
Motivation: Centralized registries streamline software distribution but raise questions about whether hardened security controls eliminate the need for end-to-end signing. The motivation is to address gaps in trust and assurance across distribution boundaries.

Method: The authors synthesize historical practices and present a trust model to evaluate software signing's effectiveness in modern distribution boundaries.

Result: The study identifies specific conditions where registry security cannot offer sufficient assurance and argues that artifact signing is necessary to extend trust beyond these boundaries.

Conclusion: Signing should be treated as a foundational defense mechanism to bolster trust and assurance in the software supply chain, even in scenarios with secure registries.

Abstract: Software signing provides a formal mechanism for provenance by ensuring
artifact integrity and verifying producer identity. It also imposes tooling and
operational costs to implement in practice. In an era of centralized registries
such as PyPI, npm, Maven Central, and Hugging Face, it is reasonable to ask
whether hardening registry security controls obviates the need for end-to-end
artifact signing. In this work, we posit that the core guarantees of signing,
provenance, integrity, and accountability are not automatically carried across
different software distribution boundaries. These boundaries include mirrors,
corporate proxies, re-hosting, and air-gapped transfers, where registry
security controls alone cannot provide sufficient assurance. We synthesize
historical practice and present a trust model for modern distribution modes to
identify when signing is necessary to extend trust beyond registry control.
Treating signing as a baseline layer of defense strengthens software supply
chain assurance even when registries are secure.

</details>


### [775] [Quantum Computing as a Service - a Software Engineering Perspective](https://arxiv.org/abs/2510.04982)
*Aakash Ahmad,Muhammad Waseem,Bakheet Aljedaani,Mahdi Fahmideh,Peng Liang,Feras Awaysheh*

Main category: cs.SE

TL;DR: This paper explores the creation of a reference architecture for Quantum Computing as a Service (QCaaS) through systematic research and architectural modeling.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between quantum computing technology and its accessibility by offering QC resources as a service, allowing more users to benefit without owning quantum hardware.

Method: A two-phase approach: (1) systematic mapping study to review literature, extracting insights on quantum service development, and (2) architecture-based development to craft a reference model for QCaaS.

Result: The research derived a 4-phase quantum service development lifecycle. It provided key components like Quantum Significant Requirements (QSRs), patterns, modeling notations, and deployment platforms to support engineering QCaaS.

Conclusion: The study introduces a structured, lifecycle-driven reference architecture for QCaaS, aimed at improving the accessibility and usability of quantum computing resources.

Abstract: Quantum systems have started to emerge as a disruptive technology and
enabling platforms - exploiting the principles of quantum mechanics via
programmable quantum bits (QuBits) - to achieve quantum supremacy in computing.
Academic research, industrial projects (e.g., Amazon Braket, IBM Qiskit), and
consortiums like 'Quantum Flagship' are striving to develop practically capable
and commercially viable quantum computing (QC) systems and technologies.
Quantum Computing as a Service (QCaaS) is viewed as a solution attuned to the
philosophy of service-orientation that can offer QC resources and platforms, as
utility computing, to individuals and organisations who do not own quantum
computers. This research investigates a process-centric and architecture-driven
approach to offer a software engineering perspective on enabling QCaaS - a.k.a
quantum service-orientation. We employed a two-phase research method comprising
(a) a systematic mapping study and (b) an architecture-based development, first
to identify the phases of the quantum service development life cycle and
subsequently to integrate these phases into a reference architecture that
supports QCaaS. The SMS process retrieved a collection of potentially relevant
research literature and based on a multi-step selection and qualitative
assessment, we selected 41 peer-reviewed studies to answer three RQs. The RQs
investigate (i) demographic details in terms of frequency, types, and trends of
research, (ii) phases of quantum service development lifecycle to derive a
reference architecture for conception, modeling, assembly, and deployment of
services, and (iii) The results identify a 4-phased development lifecycle along
with quantum significant requirements (QSRs), various modeling notations,
catalogue of patterns, programming languages, and deployment platforms that can
be integrated in a layered reference architecture to engineer QCaaS.

</details>


### [776] [AutoEmpirical: LLM-Based Automated Research for Empirical Software Fault Analysis](https://arxiv.org/abs/2510.04997)
*Jiongchi Yu,Weipeng Jiang,Xiaoyu Zhang,Qiang Hu,Xiaofei Xie,Chao Shen*

Main category: cs.SE

TL;DR: The paper explores using Large Language Models (LLMs) to assist in analyzing software faults, aiming to enhance efficiency and reduce manual effort.


<details>
  <summary>Details</summary>
Motivation: Traditional software fault analysis is labor-intensive and time-consuming, creating obstacles in large-scale, complex system studies.

Method: The study decomposes fault analysis into three phases and evaluates LLMs using 3,829 faults from an empirical study.

Result: LLMs significantly improve processing speed, reducing analysis time from weeks to about two hours.

Conclusion: While LLMs show promise in making fault analysis efficient, future research should address challenges to enable fully automated fault analysis.

Abstract: Understanding software faults is essential for empirical research in software
development and maintenance. However, traditional fault analysis, while
valuable, typically involves multiple expert-driven steps such as collecting
potential faults, filtering, and manual investigation. These processes are both
labor-intensive and time-consuming, creating bottlenecks that hinder
large-scale fault studies in complex yet critical software systems and slow the
pace of iterative empirical research.
  In this paper, we decompose the process of empirical software fault study
into three key phases: (1) research objective definition, (2) data preparation,
and (3) fault analysis, and we conduct an initial exploration study of applying
Large Language Models (LLMs) for fault analysis of open-source software.
Specifically, we perform the evaluation on 3,829 software faults drawn from a
high-quality empirical study. Our results show that LLMs can substantially
improve efficiency in fault analysis, with an average processing time of about
two hours, compared to the weeks of manual effort typically required. We
conclude by outlining a detailed research plan that highlights both the
potential of LLMs for advancing empirical fault studies and the open challenges
that required be addressed to achieve fully automated, end-to-end software
fault analysis.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [777] [A Biologically Interpretable Cognitive Architecture for Online Structuring of Episodic Memories into Cognitive Maps](https://arxiv.org/abs/2510.03286)
*E. A. Dzhivelikian,A. I. Panov*

Main category: q-bio.NC

TL;DR: The paper presents a biologically plausible cognitive model for structuring episodic memories into cognitive maps using Hebbian-like learning rules.


<details>
  <summary>Details</summary>
Motivation: To advance understanding of cognitive maps in artificial and biological agents by addressing the lack of biologically plausible mechanisms in existing models.

Method: The paper integrates the Successor Features framework with local learning rules akin to Hebbian learning to structure episodic memories into cognitive maps through agent-environment interactions.

Result: The model demonstrated its ability to autonomously organize episodic memories into structured cognitive maps in a partially observable grid-world, without relying on centralized optimization techniques.

Conclusion: The approach offers a biologically plausible method for cognitive map formation, bridging gaps between computational neuroscience and AI, and advancing artificial adaptive agents.

Abstract: Cognitive maps provide a powerful framework for understanding spatial and
abstract reasoning in biological and artificial agents. While recent
computational models link cognitive maps to hippocampal-entorhinal mechanisms,
they often rely on global optimization rules (e.g., backpropagation) that lack
biological plausibility. In this work, we propose a novel cognitive
architecture for structuring episodic memories into cognitive maps using local,
Hebbian-like learning rules, compatible with neural substrate constraints. Our
model integrates the Successor Features framework with episodic memories,
enabling incremental, online learning through agent-environment interaction. We
demonstrate its efficacy in a partially observable grid-world, where the
architecture autonomously organizes memories into structured representations
without centralized optimization. This work bridges computational neuroscience
and AI, offering a biologically grounded approach to cognitive map formation in
artificial adaptive agents.

</details>


### [778] [Stability of Fractional-Order Discrete-Time Systems with Application to Rulkov Neural Networks and Asymmetric Memristor Synapses](https://arxiv.org/abs/2510.03304)
*Leila Eftekhari,Moein Khalighi,Saeid Abbasbandy*

Main category: q-bio.NC

TL;DR: This paper introduces novel discrete fractional-order neural models coupled with memristors and develops a theorem for stability analysis, enhancing neural network simulations.


<details>
  <summary>Details</summary>
Motivation: Discrete memristor-coupled neural network models are essential for computational efficiency and capturing biological neuron properties. Fractional-order modeling addresses numerical accuracy in capturing memory effects.

Method: Two discrete fractional-order neural systems are developed: (1) dual memristor-coupled Rulkov neuron model and (2) an expanded ring-shaped neural network. A stability theorem applicable to such systems is introduced.

Result: The proposed models improve memory and hereditary property simulation accuracy and provide tools for analyzing neural network stability.

Conclusion: Integrating fractional-order calculus into memristor models enhances neural dynamic simulations and stability understanding, fostering future computational neural network research.

Abstract: Memristors have emerged as ideal components for modeling synaptic connections
in neural networks due to their ability to emulate synaptic plasticity and
memory effects. Discrete models of memristor-coupled neurons are crucial for
simplifying computations and efficiently analyzing large-scale neural networks.
Furthermore, incorporating fractional-order calculus into discrete models
enhances their capacity to capture the memory and hereditary properties
inherent in biological neurons, thus reducing numerical discretization errors
compared to integer-order models. Despite this potential, discrete
fractional-order neural models coupled through memristors have received limited
attention. To address this gap, we introduce two novel discrete
fractional-order neural systems. The first system consists of two Rulkov
neurons coupled via dual memristors to emulate synaptic functions. The second
system expands this configuration into a ring-shaped network of neurons
consisting of multiple similar subnetworks. We present a novel theorem that
defines stability regions for discrete fractional-order systems, applicable to
both proposed models. Integrating discrete fractional-order calculus into
memristor-coupled neural models provides a foundation for more accurate and
efficient simulations of neural dynamics. This work advances the understanding
of neural network stability and paves the way for future research into
efficient neural computations.

</details>


### [779] [Atlas-free Brain Network Transformer](https://arxiv.org/abs/2510.03306)
*Shuai Huang,Xuan Kan,James J. Lah,Deqiang Qiu*

Main category: q-bio.NC

TL;DR: This paper proposes an atlas-free brain network transformer (BNT) based on individualized brain parcellations from resting-state fMRI data, outperforming existing atlas-based methods in precision and robustness.


<details>
  <summary>Details</summary>
Motivation: The paper aims to overcome limitations of standardized atlases such as spatial misalignment, functional heterogeneity, and selection biases that reduce the reliability of brain network analyses.

Method: The proposed method derives individualized parcellations from subject-specific fMRI data and computes ROI-to-voxel features in a standardized voxel-based space. These features are processed with a BNT architecture to create comparable subject-level embeddings.

Result: Experimental results showed superior performance of the atlas-free BNT in tasks like sex classification and brain-connectome age prediction compared to state-of-the-art methods.

Conclusion: The atlas-free approach enhances the precision, robustness, and generalizability of brain network analyses, promising improved neuroimaging biomarkers and clinical diagnostic tools for personalized medicine.

Abstract: Current atlas-based approaches to brain network analysis rely heavily on
standardized anatomical or connectivity-driven brain atlases. However, these
fixed atlases often introduce significant limitations, such as spatial
misalignment across individuals, functional heterogeneity within predefined
regions, and atlas-selection biases, collectively undermining the reliability
and interpretability of the derived brain networks. To address these
challenges, we propose a novel atlas-free brain network transformer (atlas-free
BNT) that leverages individualized brain parcellations derived directly from
subject-specific resting-state fMRI data. Our approach computes ROI-to-voxel
connectivity features in a standardized voxel-based feature space, which are
subsequently processed using the BNT architecture to produce comparable
subject-level embeddings. Experimental evaluations on sex classification and
brain-connectome age prediction tasks demonstrate that our atlas-free BNT
consistently outperforms state-of-the-art atlas-based methods, including
elastic net, BrainGNN, Graphormer and the original BNT. Our atlas-free approach
significantly improves the precision, robustness, and generalizability of brain
network analyses. This advancement holds great potential to enhance
neuroimaging biomarkers and clinical diagnostic tools for personalized
precision medicine.

</details>


### [780] [Not Even Wrong: On the Limits of Prediction as Explanation in Cognitive Science](https://arxiv.org/abs/2510.03311)
*Mark Orr,Drew Cranford,Ken Ford,Kevin Gluck,Will Hancock,Christian Lebiere,Pete Pirolli,Frank Ritter,Andrea Stocco*

Main category: q-bio.NC

TL;DR: The paper critiques the Centaur model, proposing it diverges from unified theories of cognition to focus on behavior minus cognition.


<details>
  <summary>Details</summary>
Motivation: To challenge the claim that the Centaur model aligns with unified theories of cognition and offer an alternative perspective.

Method: Presented counterarguments and reasoning to suggest the Centaur model emphasizes behavior over cognition.

Result: Highlighted the divergence of Centaur from unified cognitive theories towards unified behavioral models.

Conclusion: Centaur is more of a unified model of behavior rather than cognition, contrary to the original claim by Binz et al.

Abstract: We offer a comment on the Centaur (Binz et al., 2025) transformer-based model
of human behavior. In particular, Centaur was cast as a path towards unified
theories of cognition. We offer a counter claim with supporting argument:
Centaur is a path divergent from unified theories of cognition, one that moves
towards a unified model of behavior sans cognition.

</details>


### [781] [Model-Guided Microstimulation Steers Primate Visual Behavior](https://arxiv.org/abs/2510.03684)
*Johannes Mehrer,Ben Lonnqvist,Anna Mitola,Abdulkadir Gokce,Paolo Papale,Martin Schrimpf*

Main category: q-bio.NC

TL;DR: This paper introduces a computational framework for guided brain stimulation targeting higher-level visual areas to evoke object-level percepts efficiently, aiming to enhance visual prosthetics.


<details>
  <summary>Details</summary>
Motivation: Current visual prosthetics are limited by hardware constraints and the representational properties of early visual cortex, motivating exploration of stimulation in higher-level visual areas.

Method: The framework comprises three components: a perturbation module for translating microstimulation parameters, topographic models capturing spatial neuron organization, and a mapping procedure linking optimized stimulation sites to primate cortex.

Result: Applying the framework to macaque monkeys improved visual recognition tasks, showing strong correlations between model predictions and behavioral outcomes.

Conclusion: This study lays groundwork for advanced visual prosthetics capable of inducing complex visual experiences using computationally guided stimulation.

Abstract: Brain stimulation is a powerful tool for understanding cortical function and
holds promise for therapeutic interventions in neuropsychiatric disorders.
Initial visual prosthetics apply electric microstimulation to early visual
cortex which can evoke percepts of simple symbols such as letters. However,
these approaches are fundamentally limited by hardware constraints and the
low-level representational properties of this cortical region. In contrast,
higher-level visual areas encode more complex object representations and
therefore constitute a promising target for stimulation - but determining
representational targets that reliably evoke object-level percepts constitutes
a major challenge. We here introduce a computational framework to causally
model and guide stimulation of high-level cortex, comprising three key
components: (1) a perturbation module that translates microstimulation
parameters into spatial changes to neural activity, (2) topographic models that
capture the spatial organization of cortical neurons and thus enable
prototyping of stimulation experiments, and (3) a mapping procedure that links
model-optimized stimulation sites back to primate cortex. Applying this
framework in two macaque monkeys performing a visual recognition task,
model-predicted stimulation experiments produced significant in-vivo changes in
perceptual choices. Per-site model predictions and monkey behavior were
strongly correlated, underscoring the promise of model-guided stimulation.
Image generation further revealed a qualitative similarity between in-silico
stimulation of face-selective sites and a patient's report of facephenes. This
proof-of-principle establishes a foundation for model-guided microstimulation
and points toward next-generation visual prosthetics capable of inducing more
complex visual experiences.

</details>


### [782] [Dissecting Larval Zebrafish Hunting using Deep Reinforcement Learning Trained RNN Agents](https://arxiv.org/abs/2510.03699)
*Raaghav Malik,Satpreet H. Singh,Sonja Johnson-Yu,Nathan Wu,Roy Harpaz,Florian Engert,Kanaka Rajan*

Main category: q-bio.NC

TL;DR: The paper introduces a minimal agent-based model of zebrafish hunting behavior using deep reinforcement learning. The model emulates several hallmark behaviors of zebrafish and explores how ecological and energetic constraints influence these behaviors.


<details>
  <summary>Details</summary>
Motivation: To understand how ecological and energetic constraints shape adaptive hunting behavior in both biological systems and artificial agents, using larval zebrafish as a model due to its tractable behavioral system.

Method: The study uses a minimal agent-based model where recurrent policies are trained with deep reinforcement learning in a bout-based zebrafish simulator. Virtual experiments and parameter sweeps are conducted to analyze the effects of constraints and environments on behavior.

Result: The model replicates key zebrafish hunting behaviors and demonstrates how constraints like binocular sensing, bout kinematics, and energetic costs shape these behaviors. Quantitative analyses reveal systematic reductions in prey angle before striking and provide predictions for neuroscience experiments.

Conclusion: The findings highlight that zebrafish-like hunting behavior emerges naturally as an optimal balance of energetic cost and sensory benefit, without the need for detailed biomechanical or neural data. The study delivers a virtual lab for generating testable hypotheses and studying adaptive behavior.

Abstract: Larval zebrafish hunting provides a tractable setting to study how ecological
and energetic constraints shape adaptive behavior in both biological brains and
artificial agents. Here we develop a minimal agent-based model, training
recurrent policies with deep reinforcement learning in a bout-based zebrafish
simulator. Despite its simplicity, the model reproduces hallmark hunting
behaviors -- including eye vergence-linked pursuit, speed modulation, and
stereotyped approach trajectories -- that closely match real larval zebrafish.
Quantitative trajectory analyses show that pursuit bouts systematically reduce
prey angle by roughly half before strike, consistent with measurements. Virtual
experiments and parameter sweeps vary ecological and energetic constraints,
bout kinematics (coupled vs. uncoupled turns and forward motion), and
environmental factors such as food density, food speed, and vergence limits.
These manipulations reveal how constraints and environments shape pursuit
dynamics, strike success, and abort rates, yielding falsifiable predictions for
neuroscience experiments. These sweeps identify a compact set of constraints --
binocular sensing, the coupling of forward speed and turning in bout
kinematics, and modest energetic costs on locomotion and vergence -- that are
sufficient for zebrafish-like hunting to emerge. Strikingly, these behaviors
arise in minimal agents without detailed biomechanics, fluid dynamics, circuit
realism, or imitation learning from real zebrafish data. Taken together, this
work provides a normative account of zebrafish hunting as the optimal balance
between energetic cost and sensory benefit, highlighting the trade-offs that
structure vergence and trajectory dynamics. We establish a virtual lab that
narrows the experimental search space and generates falsifiable predictions
about behavior and neural coding.

</details>


### [783] [Intrinsic cause-effect power: the tradeoff between differentiation and specification](https://arxiv.org/abs/2510.03881)
*William G. P. Mayner,William Marshall,Giulio Tononi*

Main category: q-bio.NC

TL;DR: The paper discusses Integrated Information Theory (IIT), focusing on consciousness as it pertains to cause-effect power in a system. It examines operational requirements of IIT, emphasizing 'differentiation' and 'specification' within micro and macro systems, like neural networks.


<details>
  <summary>Details</summary>
Motivation: To explore and define the operational requirements for the existence of consciousness in systems, in the context of Integrated Information Theory.

Method: IIT's properties are analyzed through the lens of 'intrinsic availability' and 'specific probability' of cause-effect states. The study uses simple systems of micro units and extends the concepts to macro systems, such as neural networks.

Result: The research shows that differentiation and specification are essential for assessing cause-effect power in systems. A trade-off between these two is identified as a necessary condition for intrinsic existence, related to consciousness.

Conclusion: A detailed framework is presented for understanding consciousness as intrinsic existence. Differentiation and specification are highlighted as key operational conditions, offering insights applicable to broader neural systems.

Abstract: Integrated information theory (IIT) starts from the existence of
consciousness and characterizes its essential properties: every experience is
intrinsic, specific, unitary, definite, and structured. IIT then formulates
existence and its essential properties operationally in terms of cause-effect
power of a substrate of units. Here we address IIT's operational requirements
for existence by considering that, to have cause-effect power, to have it
intrinsically, and to have it specifically, substrate units in their actual
state must both (i) ensure the intrinsic availability of a repertoire of
cause-effect states, and (ii) increase the probability of a specific
cause-effect state. We showed previously that requirement (ii) can be assessed
by the intrinsic difference of a state's probability from maximal
differentiation. Here we show that requirement (i) can be assessed by the
intrinsic difference from maximal specification. These points and their
consequences for integrated information are illustrated using simple systems of
micro units. When applied to macro units and systems of macro units such as
neural systems, a tradeoff between differentiation and specification is a
necessary condition for intrinsic existence, i.e., for consciousness.

</details>


### [784] [Bridging integrated information theory and the free-energy principle in living neuronal networks](https://arxiv.org/abs/2510.04084)
*Teruki Mayama,Sota Shimizu,Yuki Takano,Dai Akita,Hirokazu Takahashi*

Main category: q-bio.NC

TL;DR: This paper explores the connection between Integrated Information Theory (IIT) and the Free-Energy Principle (FEP) using neuronal cultures, revealing that integrated information ({\Phi}) increases during belief updating, correlating most strongly with Bayesian surprise.


<details>
  <summary>Details</summary>
Motivation: The paper aims to resolve the relationship between IIT, which regards integrated information as the substrate of consciousness, and the FEP, which describes how systems maintain stability via variational Bayesian inference.

Method: Experiments involved dissociated neuronal cultures subjected to repeated stimulation from hidden sources to analyze behavioral trends related to FEP metrics such as variational free energy and Bayesian surprise, alongside proxies for integrated information.

Result: The study found that integrated information ({\Phi}) follows a hill-shaped trajectory during inference, correlating positively with Bayesian surprise, modestly with accuracy, and not significantly with variational free energy.

Conclusion: Integrated information increases specifically during belief updating (when sensory inputs are informative), linking IIT's physical perspective of consciousness with FEP's functional view, and highlighting mechanisms behind phenomenological and adaptive processes.

Abstract: The relationship between Integrated Information Theory (IIT) and the
Free-Energy Principle (FEP) remains unresolved, particularly with respect to
how integrated information, proposed as the intrinsic substrate of
consciousness, behaves within variational Bayesian inference. We investigated
this issue using dissociated neuronal cultures, previously shown to perform
perceptual inference consistent with the FEP. Repeated stimulation from hidden
sources induced robust source selectivity: variational free energy (VFE)
decreased across sessions, whereas accuracy and Bayesian surprise (complexity)
increased. Network-level analyses revealed that a proxy measure of integrated
information and the size of the main complex followed a hill-shaped trajectory,
with informational cores organizing diverse neuronal activity. Across
experiments, integrated information correlated strongly and positively with
Bayesian surprise, modestly and heterogeneously with accuracy, and showed no
significant relationship with VFE. The positive coupling between {\Phi} and
Bayesian surprise likely reflects the diversity of activity observed in
critical dynamics. These findings suggest that integrated information increases
specifically during belief updating, when sensory inputs are most informative,
rather than tracking model efficiency. The hill-shaped trajectory of {\Phi}
during inference can be functionally interpreted as a transition from
exploration to exploitation. This work provides empirical evidence linking the
physical account of consciousness advanced by IIT with the functional
perspective offered by the FEP, contributing to a unified framework for the
mechanisms and adaptive roles of phenomenology.

</details>


### [785] [Simultaneously Determining Regional Heterogeneity and Connection Directionality from Neural Activity and Symmetric Connection](https://arxiv.org/abs/2510.04110)
*Jiawen Chang,Zhuda Yang,Changsong Zhou*

Main category: q-bio.NC

TL;DR: This paper presents a framework to jointly estimate neural component heterogeneity and asymmetric network connections, validated with data and models, while uncovering principles related to interactions and reconstruction limitations.


<details>
  <summary>Details</summary>
Motivation: Despite advances in estimating network connection directionality, the impact of local heterogeneity and excitatory-inhibitory interactions on direction estimation is not well understood, prompting the need for a unified framework.

Method: The authors propose a reconstruction framework combining effective local heterogeneity estimation and network connection asymmetry analysis, using macaque cortical connectivity data and various circuit models for validation.

Result: The study found that local heterogeneity estimations were consistent despite varied circuit features and highlighted the influence of hidden inhibitory populations on connection dynamics, alongside the effects of sampling intervals on network interaction estimates.

Conclusion: This unified framework advances the understanding of neural dynamics by identifying functional impacts of local interactions and clarifying reconstruction limits and scaling principles, aiding experimental neural circuit studies.

Abstract: The spatiotemporal patterns of neural dynamics are jointly shaped by directed
structural interactions and heterogeneous intrinsic features of the neural
components. Despite well-developed methods for estimating directionality in
network connections from network of homogeneous nodes, how local heterogeneity
impacts on directionality estimation remains poorly understood. In particular,
the role of excitatory-inhibitory interactions in shaping network
directionality and how these interactions should be incorporated into
reconstruction frameworks remain largely unexplored. Here, we present a novel
reconstruction framework that simultaneously estimates effective heterogeneity
across network nodes and asymmetric network connections from neural activity
and symmetric connection, both are assessible in experimental data, validated
using macaque cortical connectivity data and several circuit models. We found
that the estimated local heterogeneity remains consistent across various forms
of parameterized local circuit heterogeneity. Furthermore, we demonstrated and
quantified how hidden local inhibitory populations only modify within-region
connection strengths, elucidating the functional equivalence between dynamics
of excitatory-inhibitory networks and purely observing excitatory networks when
estimating effective heterogeneity and asymmetry. Finally, we demonstrated the
sampling interval effect in estimating network interactions with respect to the
sampling resolution. Together, our results not only provide a unified framework
for evaluating relative functional contributions of local heterogeneity and
asymmetry to overall system dynamics but also reveal the fundamental
limitations and scaling principles in reconstructing neural circuit
connectivity from experimental observations.

</details>


### [786] [The Bayesian Origin of the Probability Weighting Function in Human Representation of Probabilities](https://arxiv.org/abs/2510.04698)
*Xin Tong,Thi Thu Uyen Hoang,Xue-Xin Wei,Michael Hahn*

Main category: q-bio.NC

TL;DR: The paper investigates how humans perceive probabilities, presenting a rational model based on noisy neural encoding which aligns with human decision-making behaviors.


<details>
  <summary>Details</summary>
Motivation: To better understand how humans distort or perceive probabilities and why classical probability weighting functions arise in decision making.

Method: The authors develop a model centered on rational inference over optimal decoding from noisy neural encoding and test it using behavioral tasks like lotteries and dot counting.

Result: The model successfully explains human behavior in tasks requiring probability judgments and accounts for adaptation to different probability distributions.

Conclusion: The findings provide a unified explanation of human probability representation, rooted in rational inference and neural encoding principles.

Abstract: Understanding the representation of probability in the human mind has been of
great interest to understanding human decision making. Classical paradoxes in
decision making suggest that human perception distorts probability magnitudes.
Previous accounts postulate a Probability Weighting Function that transforms
perceived probabilities; however, its motivation has been debated. Recent work
has sought to motivate this function in terms of noisy representations of
probabilities in the human mind. Here, we present an account of the Probability
Weighting Function grounded in rational inference over optimal decoding from
noisy neural encoding of quantities. We show that our model accurately accounts
for behavior in a lottery task and a dot counting task. It further accounts for
adaptation to a bimodal short-term prior. Taken together, our results provide a
unifying account grounding the human representation of probability in rational
inference.

</details>


### [787] [On graphical domination for threshold-linear networks with recurrent excitation and global inhibition](https://arxiv.org/abs/2510.05098)
*Carina Curto*

Main category: q-bio.NC

TL;DR: The paper introduces two theorems about graphical domination, proving their applicability in generalized CTLNs and E-I TLNs, ensuring consistency in network fixed points and dynamic behaviors.


<details>
  <summary>Details</summary>
Motivation: The study seeks to enhance understanding of fixed point elimination and dynamic behaviors in recurrent networks by expanding graphical domination concepts to generalized CTLNs and E-I TLNs.

Method: The authors prove two theoretical results, Theorem 1 and Theorem 2, on graphical domination and introduce E-I TLNs. They connect network parameters of gCTLNs and E-I TLNs and analyze their fixed points and dynamics.

Result: Theorems 1 and 2 are established, showing fixed points remain invariant in reduced networks and the irreducible graph is unique. It is proven that these results apply to both gCTLNs and E-I TLNs, which demonstrate similar dynamics.

Conclusion: The paper reinforces the utility of graphical domination in predicting network fixed points and behaviors across recurrent networks, simplifying their analysis and ensuring broader applicability.

Abstract: Graphical domination was first introduced in [1] in the context of
combinatorial threshold-linear networks (CTLNs). There it was shown that when a
domination relationship exists between a pair of vertices in a graph, certain
fixed points in the corresponding CTLN can be ruled out. Here we prove two new
theorems about graphical domination, and show that they apply to a
significantly more general class of recurrent networks called generalized CTLNs
(gCTLNs). Theorem 1 establishes that if a dominated node is removed from a
network, the reduced network has exactly the same fixed points. Theorem 2 tells
us that by iteratively removing dominated nodes from an initial graph $G$, the
final (irreducible) graph $\widetilde{G}$ is unique. We also introduce another
new family of TLNs, called E-I TLNs, consisting of $n$ excitatory nodes and a
single inhibitory node providing global inhibition. We provide a concrete
mapping between the parameters of gCTLNs and E-I TLNs built from the same graph
such that corresponding networks have the same fixed points. We also show that
Theorems 1 and 2 apply equally well to E-I TLNs, and that the dynamics of
gCTLNs and E-I TLNs with the same underlying graph $G$ exhibit similar behavior
that is well predicted by the fixed points of the reduced graph
$\widetilde{G}$.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [788] [Quantile-Scaled Bayesian Optimization Using Rank-Only Feedback](https://arxiv.org/abs/2510.03277)
*Tunde Fahd Egunjobi*

Main category: stat.ML

TL;DR: QS-BO converts ranks into usable Gaussian targets to extend Bayesian Optimization for rank-only feedback scenarios.


<details>
  <summary>Details</summary>
Motivation: Standard Bayesian Optimization struggles in settings with rank-based or unreliable feedback due to its reliance on objective values.

Method: QS-BO employs rank-to-Gaussian scaling through a quantile-scaling pipeline and then uses Gaussian process surrogates for optimization.

Result: QS-BO achieves lower objective values than Random Search and is statistically confirmed to outperform it with 1% significance.

Conclusion: QS-BO is a stable, effective extension of Bayesian Optimization suited to rank-based environments like recommendation systems or human-in-the-loop optimization.

Abstract: Bayesian Optimization (BO) is widely used for optimizing expensive black-box
functions, particularly in hyperparameter tuning. However, standard BO assumes
access to precise objective values, which may be unavailable, noisy, or
unreliable in real-world settings where only relative or rank-based feedback
can be obtained. In this study, we propose Quantile-Scaled Bayesian
Optimization (QS-BO), a principled rank-based optimization framework. QS-BO
converts ranks into heteroscedastic Gaussian targets through a quantile-scaling
pipeline, enabling the use of Gaussian process surrogates and standard
acquisition functions without requiring explicit metric scores. We evaluate
QS-BO on synthetic benchmark functions, including one- and two-dimensional
nonlinear functions and the Branin function, and compare its performance
against Random Search. Results demonstrate that QS-BO consistently achieves
lower objective values and exhibits greater stability across runs. Statistical
tests further confirm that QS-BO significantly outperforms Random Search at the
1\% significance level. These findings establish QS-BO as a practical and
effective extension of Bayesian Optimization for rank-only feedback, with
promising applications in preference learning, recommendation, and
human-in-the-loop optimization where absolute metric values are unavailable or
unreliable.

</details>


### [789] [Mathematically rigorous proofs for Shapley explanations](https://arxiv.org/abs/2510.03281)
*David van Batenburg*

Main category: stat.ML

TL;DR: This thesis rigorously discusses Shapley values in machine learning, providing proofs for their axiomatic characterization and unique representation via a weighted linear regression problem.


<details>
  <summary>Details</summary>
Motivation: To improve understanding of machine learning models' decision-making processes by rigorously exploring Shapley values.

Method: Mathematical proofs and counterexamples are used to analyze axioms and representation methods for Shapley values.

Result: Shapley values are characterized uniquely using axioms of local accuracy, missingness, symmetry, and consistency. A redundancy claim on the symmetry axiom is disproven using counterexamples.

Conclusion: Shapley values are vital for interpreting machine learning models, with essential properties clarified and proven rigorously.

Abstract: Machine Learning is becoming increasingly more important in today's world. It
is therefore very important to provide understanding of the decision-making
process of machine-learning models. A popular way to do this is by looking at
the Shapley-Values of these models as introduced by Lundberg and Lee.
  In this thesis, we discuss the two main results by Lundberg and Lee from a
mathematically rigorous standpoint and provide full proofs, which are not
available from the original material.
  The first result of this thesis is an axiomatic characterization of the
Shapley values in machine learning based on axioms by Young. We show that the
Shapley values are the unique explanation to satisfy local accuracy,
missingness, symmetry and consistency. Lundberg and Lee claim that the symmetry
axiom is redundant for explanations. However, we provide a counterexample that
shows the symmetry axiom is in fact essential.
  The second result shows that we can write the Shapley values as the unique
solution to a weighted linear regression problem. This result is proven with
the use of dimensionality reduction.

</details>


### [790] [Transformed $\ell_1$ Regularizations for Robust Principal Component Analysis: Toward a Fine-Grained Understanding](https://arxiv.org/abs/2510.03624)
*Kun Zhao,Haoke Zhang,Jiayi Wang,Yifei Lou*

Main category: stat.ML

TL;DR: The paper proposes a nonconvex regularization method called transformed ℓ1 (TL1) for improved Robust Principal Component Analysis (RPCA), achieving higher accuracy than traditional convex models.


<details>
  <summary>Details</summary>
Motivation: To improve the recovery of low-rank structures from noisy data corrupted by sparse outliers, addressing the limitations of traditional convex RPCA models.

Method: The method introduces TL1 regularization, which adjusts its internal parameter to approximate either ℓ0 or ℓ1 norms for enhanced matrix rank and sparsity approximations, with statistical analysis and numerical experiments.

Result: TL1 achieves statistical convergence rates comparable to classical RPCA, with constant-order bounds on rank and sparsity estimates, and demonstrates superior accuracy in both synthetic and real-world non-uniform sampling scenarios.

Conclusion: The nonconvex TL1 approach is an effective alternative to classical convex RPCA models, offering enhanced performance in recovering low-rank and sparse components.

Abstract: Robust Principal Component Analysis (RPCA) aims to recover a low-rank
structure from noisy, partially observed data that is also corrupted by sparse,
potentially large-magnitude outliers. Traditional RPCA models rely on convex
relaxations, such as nuclear norm and $\ell_1$ norm, to approximate the rank of
a matrix and the $\ell_0$ functional (the number of non-zero elements) of
another. In this work, we advocate a nonconvex regularization method, referred
to as transformed $\ell_1$ (TL1), to improve both approximations. The rationale
is that by varying the internal parameter of TL1, its behavior asymptotically
approaches either $\ell_0$ or $\ell_1$. Since the rank is equal to the number
of non-zero singular values and the nuclear norm is defined as their sum,
applying TL1 to the singular values can approximate either the rank or the
nuclear norm, depending on its internal parameter. We conduct a fine-grained
theoretical analysis of statistical convergence rates, measured in the
Frobenius norm, for both the low-rank and sparse components under general
sampling schemes. These rates are comparable to those of the classical RPCA
model based on the nuclear norm and $\ell_1$ norm. Moreover, we establish
constant-order upper bounds on the estimated rank of the low-rank component and
the cardinality of the sparse component in the regime where TL1 behaves like
$\ell_0$, assuming that the respective matrices are exactly low-rank and
exactly sparse. Extensive numerical experiments on synthetic data and
real-world applications demonstrate that the proposed approach achieves higher
accuracy than the classic convex model, especially under non-uniform sampling
schemes.

</details>


### [791] [The analogy theorem in Hoare logic](https://arxiv.org/abs/2510.03685)
*Nikitin Nikita*

Main category: stat.ML

TL;DR: This paper formalizes a rigorous mathematical framework for transferring machine learning models between data domains using first-order and Hoare logic.


<details>
  <summary>Details</summary>
Motivation: Machine learning models often fail to transfer effectively between data domains due to a lack of rigorous mathematical justification.

Method: The paper introduces a theorem using first-order and Hoare logic to establish necessary and sufficient conditions for analogy between datasets. These conditions are tested on synthetic data, MNIST, and USPS datasets.

Result: The theoretical framework was empirically tested, achieving F1 scores of 0.84 and 0.88 for convolutional neural networks and random forests, respectively, demonstrating the method's effectiveness.

Conclusion: The formalization of analogy in program logic provides verifiable guarantees for knowledge transfer, enabling new theoretical research and broader practical applications of machine learning.

Abstract: The introduction of machine learning methods has led to significant advances
in automation, optimization, and discoveries in various fields of science and
technology. However, their widespread application faces a fundamental
limitation: the transfer of models between data domains generally lacks a
rigorous mathematical justification. The key problem is the lack of formal
criteria to guarantee that a model trained on one type of data will retain its
properties on another.This paper proposes a solution to this problem by
formalizing the concept of analogy between data sets and models using
first-order logic and Hoare logic.We formulate and rigorously prove a theorem
that sets out the necessary and sufficient conditions for analogy in the task
of knowledge transfer between machine learning models. Practical verification
of the analogy theorem on model data obtained using the Monte Carlo method, as
well as on MNIST and USPS data, allows us to achieving F1 scores of 0.84 and
0.88 for convolutional neural networks and random forests, respectively.The
proposed approach not only allows us to justify the correctness of transfer
between domains but also provides tools for comparing the applicability of
models to different types of data.The main contribution of the work is a
rigorous formalization of analogy at the level of program logic, providing
verifiable guarantees of the correctness of knowledge transfer, which opens new
opportunities for both theoretical research and the practical use of machine
learning models in previously inaccessible areas.

</details>


### [792] [Spectral Thresholds for Identifiability and Stability:Finite-Sample Phase Transitions in High-Dimensional Learning](https://arxiv.org/abs/2510.03809)
*William Hao-Cheng Huang*

Main category: stat.ML

TL;DR: This paper discusses the instability of high-dimensional learning models when a critical sample size is not met. It introduces the Fisher Threshold Theorem to formalize the issue and proposes the Fisher floor for robust spectral regularization.


<details>
  <summary>Details</summary>
Motivation: High-dimensional learning models collapse abruptly when sample sizes are insufficient, driven by geometric instability related to Fisher eigendirections. The paper aims to address this fundamental problem.

Method: The authors formalize the instability through the Fisher Threshold Theorem, which establishes a finite-sample, non-asymptotic law. They also propose the Fisher floor, a spectral regularization method to mitigate failures.

Result: Experiments on Gaussian mixtures and logistic models demonstrate the predicted phase transition and validate $d/n$ scaling's impact on model stability.

Conclusion: The study provides a foundational result for high-dimensional learning by defining a sharp spectral sample-complexity threshold and offering diagnostics for robust inference.

Abstract: In high-dimensional learning, models remain stable until they collapse
abruptly once the sample size falls below a critical level. This instability is
not algorithm-specific but a geometric mechanism: when the weakest Fisher
eigendirection falls beneath sample-level fluctuations, identifiability fails.
Our Fisher Threshold Theorem formalizes this by proving that stability requires
the minimal Fisher eigenvalue to exceed an explicit $O(\sqrt{d/n})$ bound.
Unlike prior asymptotic or model-specific criteria, this threshold is
finite-sample and necessary, marking a sharp phase transition between reliable
concentration and inevitable failure. To make the principle constructive, we
introduce the Fisher floor, a verifiable spectral regularization robust to
smoothing and preconditioning. Synthetic experiments on Gaussian mixtures and
logistic models confirm the predicted transition, consistent with $d/n$
scaling. Statistically, the threshold sharpens classical eigenvalue conditions
into a non-asymptotic law; learning-theoretically, it defines a spectral
sample-complexity frontier, bridging theory with diagnostics for robust
high-dimensional inference.

</details>


### [793] [Self-Speculative Masked Diffusions](https://arxiv.org/abs/2510.03929)
*Andrew Campbell,Valentin De Bortoli,Jiaxin Shi,Arnaud Doucet*

Main category: stat.ML

TL;DR: The paper introduces self-speculative masked diffusions, a method for generating discrete data more efficiently by reducing the number of function evaluations required.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency in standard masked diffusion models, which rely on many function evaluations to maintain high-quality data generation.

Method: The method changes the transformer attention mask from non-causal to causal, allowing draft token generation and parallel validation using a model-integrated speculative sampling mechanism.

Result: The proposed approach reduces the number of network forward passes required by approximately 2x compared to standard masked diffusion models, demonstrating results on GPT2-scale text and protein sequence generation.

Conclusion: The self-speculative masked diffusions method significantly improves the efficiency of the generation process while maintaining high-quality outputs.

Abstract: We present self-speculative masked diffusions, a new class of masked
diffusion generative models for discrete data that require significantly fewer
function evaluations to generate samples. Standard masked diffusion models
predict factorized logits over currently masked positions. A number of masked
positions are then sampled, however, the factorization approximation means that
sampling too many positions in one go leads to poor sample quality. As a
result, many simulation steps and therefore neural network function evaluations
are required to generate high-quality data. We reduce the computational burden
by generating non-factorized predictions over masked positions. This is
achieved by modifying the final transformer attention mask from non-causal to
causal, enabling draft token generation and parallel validation via a novel,
model-integrated speculative sampling mechanism. This results in a
non-factorized predictive distribution over masked positions in a single
forward pass. We apply our method to GPT2 scale text modelling and protein
sequences generation, finding that we can achieve a ~2x reduction in the
required number of network forward passes relative to standard masked diffusion
models.

</details>


### [794] [Simulation-based inference via telescoping ratio estimation for trawl processes](https://arxiv.org/abs/2510.04042)
*Dan Leonte,Raphaël Huser,Almut E. D. Veraart*

Main category: stat.ML

TL;DR: The paper introduces a sample-efficient simulation-based inference (SBI) framework for temporally stochastic processes to address parameter estimation challenges. It focuses on improving reliability, accuracy, and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Parameter estimation for complex temporal stochastic processes remains a challenge due to their non-Markovian nature, non-Gaussian tails, and other stylized facts inherent in real-world data sets.

Method: The approach involves sequentially learning posterior densities and leveraging Chebyshev polynomial approximations to generate independent posterior samples. Novel diagnostic tools and calibration techniques are also introduced.

Result: The framework achieves faster, more accurate inference and improved reliability of posterior samples. It is applicable to new time series data of varying lengths without retraining.

Conclusion: The proposed SBI framework successfully tackles crucial limitations in the inference process for complex stochastic models, showcasing its practical utility in real-world applications like energy demand analysis.

Abstract: The growing availability of large and complex datasets has increased interest
in temporal stochastic processes that can capture stylized facts such as
marginal skewness, non-Gaussian tails, long memory, and even non-Markovian
dynamics. While such models are often easy to simulate from, parameter
estimation remains challenging. Simulation-based inference (SBI) offers a
promising way forward, but existing methods typically require large training
datasets or complex architectures and frequently yield confidence (credible)
regions that fail to attain their nominal values, raising doubts on the
reliability of estimates for the very features that motivate the use of these
models. To address these challenges, we propose a fast and accurate,
sample-efficient SBI framework for amortized posterior inference applicable to
intractable stochastic processes. The proposed approach relies on two main
steps: first, we learn the posterior density by decomposing it sequentially
across parameter dimensions. Then, we use Chebyshev polynomial approximations
to efficiently generate independent posterior samples, enabling accurate
inference even when Markov chain Monte Carlo methods mix poorly. We further
develop novel diagnostic tools for SBI in this context, as well as post-hoc
calibration techniques; the latter not only lead to performance improvements of
the learned inferential tool, but also to the ability to reuse it directly with
new time series of varying lengths, thus amortizing the training cost. We
demonstrate the method's effectiveness on trawl processes, a class of flexible
infinitely divisible models that generalize univariate Gaussian processes,
applied to energy demand data.

</details>


### [795] [Scalable Causal Discovery from Recursive Nonlinear Data via Truncated Basis Function Scores and Tests](https://arxiv.org/abs/2510.04276)
*Joseph Ramsey,Bryan Andrews*

Main category: stat.ML

TL;DR: The paper introduces two methods, BF-BIC and BF-LRT, for efficiently learning graphical conditional independence structures in nonlinear, continuous, or mixed data.


<details>
  <summary>Details</summary>
Motivation: Many existing causal discovery methods face scalability issues when dealing with thousands of samples and hundreds of variables, especially for nonlinear systems.

Method: The authors propose BF-BIC for scoring based on truncated additive expansions and BF-LRT for fast conditional independence testing through basis function techniques.

Result: BF-BIC outperforms kernel and constraint-based methods in terms of accuracy and runtime, while BF-LRT offers rapid conditional independence tests with competitive accuracy.

Conclusion: The proposed BF-based methods advance interpretable, scalable causal discovery and are practically implemented in Python, R, and Java.

Abstract: Learning graphical conditional independence structures from nonlinear,
continuous or mixed data is a central challenge in machine learning and the
sciences, and many existing methods struggle to scale to thousands of samples
or hundreds of variables. We introduce two basis-expansion tools for scalable
causal discovery. First, the Basis Function BIC (BF-BIC) score uses truncated
additive expansions to approximate nonlinear dependencies. BF-BIC is
theoretically consistent under additive models and extends to post-nonlinear
(PNL) models via an invertible reparameterization. It remains robust under
moderate interactions and supports mixed data through a degenerate-Gaussian
embedding for discrete variables. In simulations with fully nonlinear neural
causal models (NCMs), BF-BIC outperforms kernel- and constraint-based methods
(e.g., KCI, RFCI) in both accuracy and runtime. Second, the Basis Function
Likelihood Ratio Test (BF-LRT) provides an approximate conditional independence
test that is substantially faster than kernel tests while retaining competitive
accuracy. Extensive simulations and a real-data application to Canadian
wildfire risk show that, when integrated into hybrid searches, BF-based methods
enable interpretable and scalable causal discovery. Implementations are
available in Python, R, and Java.

</details>


### [796] [Relative Information Gain and Gaussian Process Regression](https://arxiv.org/abs/2510.04277)
*Hamish Flynn*

Main category: stat.ML

TL;DR: The paper introduces the concept of relative information gain to link two measures, effective dimension and information gain, in Gaussian process regression. It proves bounds showing minimax-optimal rates of convergence.


<details>
  <summary>Details</summary>
Motivation: The study aims to reconcile and smooth the gap between effective dimension and information gain for sample complexity in functional estimation, leveraging the benefits of both metrics.

Method: Introduces the relative information gain, analyzes its properties, proves PAC-Bayesian excess risk bounds, and combines properties with spectral kernel analysis.

Result: Relative information gain is proven to interpolate between effective dimension and information gain while maintaining similar growth rates. PAC-Bayesian bounds incorporate relative information gain into minimax rates.

Conclusion: Relative information gain serves as a refined metric with practical implications for deriving minimax optimal convergence rates in Gaussian process regression.

Abstract: The sample complexity of estimating or maximising an unknown function in a
reproducing kernel Hilbert space is known to be linked to both the effective
dimension and the information gain associated with the kernel. While the
information gain has an attractive information-theoretic interpretation, the
effective dimension typically results in better rates. We introduce a new
quantity called the relative information gain, which measures the sensitivity
of the information gain with respect to the observation noise. We show that the
relative information gain smoothly interpolates between the effective dimension
and the information gain, and that the relative information gain has the same
growth rate as the effective dimension. In the second half of the paper, we
prove a new PAC-Bayesian excess risk bound for Gaussian process regression. The
relative information gain arises naturally from the complexity term in this
PAC-Bayesian bound. We prove bounds on the relative information gain that
depend on the spectral properties of the kernel. When these upper bounds are
combined with our excess risk bound, we obtain minimax-optimal rates of
convergence.

</details>


### [797] [Adaptive Coverage Policies in Conformal Prediction](https://arxiv.org/abs/2510.04318)
*Etienne Gauthier,Francis Bach,Michael I. Jordan*

Main category: stat.ML

TL;DR: This paper addresses limitations of traditional conformal prediction methods by introducing a dynamic approach that adapts coverage levels to individual examples using neural networks.


<details>
  <summary>Details</summary>
Motivation: Traditional conformal predictions often use fixed coverage levels, leading to inefficiencies such as overly conservative or empty prediction sets which fail to adapt to example-specific characteristics.

Method: The authors propose leveraging e-values and post-hoc conformal inference to implement adaptive coverage levels via neural networks trained using a leave-one-out procedure on the calibration set.

Result: The paper provides theoretical coverage guarantees and demonstrates practical benefits through experimental results, showing improved efficiency and flexibility over traditional methods.

Conclusion: Adaptive coverage policies provide a more informative and flexible alternative for conformal predictions, offering robust statistical guarantees and better handling of individual example characteristics.

Abstract: Traditional conformal prediction methods construct prediction sets such that
the true label falls within the set with a user-specified coverage level.
However, poorly chosen coverage levels can result in uninformative predictions,
either producing overly conservative sets when the coverage level is too high,
or empty sets when it is too low. Moreover, the fixed coverage level cannot
adapt to the specific characteristics of each individual example, limiting the
flexibility and efficiency of these methods. In this work, we leverage recent
advances in e-values and post-hoc conformal inference, which allow the use of
data-dependent coverage levels while maintaining valid statistical guarantees.
We propose to optimize an adaptive coverage policy by training a neural network
using a leave-one-out procedure on the calibration set, allowing the coverage
level and the resulting prediction set size to vary with the difficulty of each
individual example. We support our approach with theoretical coverage
guarantees and demonstrate its practical benefits through a series of
experiments.

</details>


### [798] [Modular and Adaptive Conformal Prediction for Sequential Models via Residual Decomposition](https://arxiv.org/abs/2510.04406)
*William Zhang,Saurabh Amin,Georgia Perakis*

Main category: stat.ML

TL;DR: The paper introduces a conformal prediction framework for two-stage models to attribute uncertainty and maintain robust coverage under shifts.


<details>
  <summary>Details</summary>
Motivation: Existing conformal prediction methods overlook structured modularity, missing opportunities for fine-grained uncertainty attribution.

Method: The method decomposes prediction residuals into stage-wise components and uses FWER-controlled calibration and adaptive adjustments for dynamic environments.

Result: Experiments show robust coverage in challenging conditions and provide interpretable uncertainty attribution across synthetic and real-world datasets.

Conclusion: This framework enhances diagnostic capabilities and offers robust coverage advantages over standard conformal methods.

Abstract: Conformal prediction offers finite-sample coverage guarantees under minimal
assumptions. However, existing methods treat the entire modeling process as a
black box, overlooking opportunities to exploit modular structure. We introduce
a conformal prediction framework for two-stage sequential models, where an
upstream predictor generates intermediate representations for a downstream
model. By decomposing the overall prediction residual into stage-specific
components, our method enables practitioners to attribute uncertainty to
specific pipeline stages. We develop a risk-controlled parameter selection
procedure using family-wise error rate (FWER) control to calibrate stage-wise
scaling parameters, and propose an adaptive extension for non-stationary
settings that preserves long-run coverage guarantees. Experiments on synthetic
distribution shifts, as well as real-world supply chain and stock market data,
demonstrate that our approach maintains coverage under conditions that degrade
standard conformal methods, while providing interpretable stage-wise
uncertainty attribution. This framework offers diagnostic advantages and robust
coverage that standard conformal methods lack.

</details>


### [799] [Learning Survival Models with Right-Censored Reporting Delays](https://arxiv.org/abs/2510.04421)
*Yuta Shikuri,Hironori Fujisawa*

Main category: stat.ML

TL;DR: The paper presents a method to address reporting delays in survival analysis, especially for newly enrolled cohorts in the insurance industry, through a parametric joint modeling approach.


<details>
  <summary>Details</summary>
Motivation: Addressing reporting delays in survival analysis is crucial for accurate risk evaluation, particularly for newly enrolled cohorts with limited follow-up intervals.

Method: The authors jointly model the parametric hazard functions of event occurrences and report timings, marginalizing over latent event occurrence status, and develop an expectation-maximization algorithm for estimation.

Result: The proposed method improves the timeliness of risk evaluations for newly enrolled cohorts under administrative censoring, demonstrated via experimental implementation.

Conclusion: The study provides a novel and effective approach to survival analysis under reporting delays, offering practical benefits for timely risk assessments in the insurance domain.

Abstract: Survival analysis is a statistical technique used to estimate the time until
an event occurs. Although it is applied across a wide range of fields,
adjusting for reporting delays under practical constraints remains a
significant challenge in the insurance industry. Such delays render event
occurrences unobservable when their reports are subject to right censoring.
This issue becomes particularly critical when estimating hazard rates for newly
enrolled cohorts with limited follow-up due to administrative censoring. Our
study addresses this challenge by jointly modeling the parametric hazard
functions of event occurrences and report timings. The joint probability
distribution is marginalized over the latent event occurrence status. We
construct an estimator for the proposed survival model and establish its
asymptotic consistency. Furthermore, we develop an expectation-maximization
algorithm to compute its estimates. Using these findings, we propose a
two-stage estimation procedure based on a parametric proportional hazards model
to evaluate observations subject to administrative censoring. Experimental
results demonstrate that our method effectively improves the timeliness of risk
evaluation for newly enrolled cohorts.

</details>


### [800] [Divergence Phase Index: A Riesz-Transform Framework for Multidimensional Phase Difference Analysis](https://arxiv.org/abs/2510.04426)
*Magaly Catanzariti,Hugo Aimar,Diego M. Mateos*

Main category: stat.ML

TL;DR: The Divergence Phase Index (DPI) is a new method for measuring phase differences in signals using harmonic analysis, applicable in diverse 1D and 2D scenarios, including EEG and image analysis.


<details>
  <summary>Details</summary>
Motivation: The paper aims to develop a geometry-aware, multidimensional metric to analyze phase differences in signals, overcoming limitations of traditional methods like the Hilbert Transform.

Method: The DPI leverages the Riesz transform in harmonic analysis to create phase measures that are invariant to intensity scaling and sensitive to structural changes.

Result: DPI demonstrated effectiveness in detecting epilepsy-associated hypersynchronization in iEEG, subtle image changes, and rotational variations in microscopy images.

Conclusion: DPI has robust adaptability and accuracy, making it suitable for applications in nonlinear dynamics, complex systems, and multidimensional signal processing.

Abstract: We introduce the Divergence Phase Index (DPI), a novel framework for
quantifying phase differences in one and multidimensional signals, grounded in
harmonic analysis via the Riesz transform. Based on classical Hilbert Transform
phase measures, the DPI extends these principles to higher dimensions, offering
a geometry-aware metric that is invariant to intensity scaling and sensitive to
structural changes. We applied this method on both synthetic and real-world
datasets, including intracranial EEG (iEEG) recordings during epileptic
seizures, high-resolution microscopy images, and paintings. In the 1D case, the
DPI robustly detects hypersynchronization associated with generalized epilepsy,
while in 2D, it reveals subtle, imperceptible changes in images and artworks.
Additionally, it can detect rotational variations in highly isotropic
microscopy images. The DPI's robustness to amplitude variations and its
adaptability across domains enable its use in diverse applications from
nonlinear dynamics, complex systems analysis, to multidimensional signal
processing.

</details>


### [801] [Gini-based Model Monitoring: A General Framework with an Application to Non-life Insurance Pricing](https://arxiv.org/abs/2510.04556)
*Alexej Brauer,Paul Menzel*

Main category: stat.ML

TL;DR: This paper explores concept drift in non-life insurance pricing, offering a systematic review, performance measures, and a monitoring procedure.


<details>
  <summary>Details</summary>
Motivation: To study and address the challenges of concept drift in non-life insurance pricing models to ensure long-term model accuracy.

Method: The authors review literature, formalize performance metrics like the Gini index, derive its asymptotic distribution for hypothesis testing, and propose a monitoring and refitting procedure to identify drift.

Result: The paper introduces a framework validated on a modified portfolio with induced concept drift, providing insights into practical concerns and pitfalls.

Conclusion: The study highlights the importance of monitoring and handling concept drift, offering robust methodologies for insurance pricing model maintenance.

Abstract: In a dynamic landscape where portfolios and environments evolve, maintaining
the accuracy of pricing models is critical. To the best of our knowledge, this
is the first study to systematically examine concept drift in non-life
insurance pricing. We (i) provide an overview of the relevant literature and
commonly used methodologies, clarify the distinction between virtual drift and
concept drift, and explain their implications for long-run model performance;
(ii) review and formalize common performance measures, including the Gini index
and deviance loss, and articulate their interpretation; (iii) derive the
asymptotic distribution of the Gini index, enabling valid inference and
hypothesis testing; and (iv) present a standardized monitoring procedure that
indicates when refitting is warranted. We illustrate the framework using a
modified real-world portfolio with induced concept drift and discuss practical
considerations and pitfalls.

</details>


### [802] [Computing Wasserstein Barycenters through Gradient Flows](https://arxiv.org/abs/2510.04602)
*Eduardo Fernandes Montesuma,Yassir Bendou,Mike Gartrell*

Main category: stat.ML

TL;DR: This paper develops scalable methods for computing Wasserstein barycenters by reformulating the problem as a gradient flow in Wasserstein space, allowing for mini-batch sampling and energy functionals.


<details>
  <summary>Details</summary>
Motivation: Existing discrete methods for Wasserstein barycenters are computationally inefficient and require access to complete input sample sets, limiting scalability.

Method: The authors reformulate the barycenter computation using gradient flow in Wasserstein space, allowing mini-batch sampling and incorporating energy functionals. They introduce two specific algorithms for empirical and Gaussian mixture measures.

Result: The introduced methods show convergence guarantees under the Polyak-Łojasiewicz inequality and outperform existing discrete and neural network-based approaches on various datasets and benchmarks.

Conclusion: The paper provides a scalable and efficient way to compute Wasserstein barycenters with theoretical and empirical advantages over existing methods.

Abstract: Wasserstein barycenters provide a powerful tool for aggregating probability
measures, while leveraging the geometry of their ambient space. Existing
discrete methods suffer from poor scalability, as they require access to the
complete set of samples from input measures. We address this issue by recasting
the original barycenter problem as a gradient flow in the Wasserstein space.
Our approach offers two advantages. First, we achieve scalability by sampling
mini-batches from the input measures. Second, we incorporate functionals over
probability measures, which regularize the barycenter problem through internal,
potential, and interaction energies. We present two algorithms for empirical
and Gaussian mixture measures, providing convergence guarantees under the
Polyak-{\L}ojasiewicz inequality. Experimental validation on toy datasets and
domain adaptation benchmarks show that our methods outperform previous discrete
and neural net-based methods for computing Wasserstein barycenters.

</details>


### [803] [Fisher-Bingham-like normalizing flows on the sphere](https://arxiv.org/abs/2510.04762)
*Thorsten Glüsenkamp*

Main category: stat.ML

TL;DR: The paper introduces a novel family of normalizing flows, called "zoom-linear-project" (ZLP)-Fisher flows, that generalize specific spherical distributions (Fisher-Bingham and Angular Gaussian) to address practical challenges in probabilistic modeling.


<details>
  <summary>Details</summary>
Motivation: Existing methodologies for spherical distributions, such as the Fisher-Bingham or Angular Gaussian distributions, are inflexible and cannot generally be modeled as normalizing flows, except in special cases. There is a need for a scalable, flexible framework to handle complex conditional density estimation on the sphere, particularly in scientific fields like astronomy.

Method: The authors propose ZLP-Fisher flows, a framework that extends special cases of spherical distributions into a generalizable family. This approach includes composing transformations to manage complexity dynamically and adapt to varying target distribution scales.

Result: The proposed ZLP-Fisher flows handle complex conditional density estimations effectively, outperforming existing methods when working with vastly scaled target distributions. The framework is especially practical for applications demanding scalable solutions, such as astronomy.

Conclusion: ZLP-Fisher flows offer a significant advancement in modeling spherical distributions by introducing scalable, flexible, and computationally efficient transformations. This method also allows for conditional density estimation in difficult settings, making it broadly useful for interdisciplinary applications.

Abstract: A generic D-dimensional Gaussian can be conditioned or projected onto the D-1
unit sphere, thereby leading to the well-known Fisher-Bingham (FB) or Angular
Gaussian (AG) distribution families, respectively. These are some of the most
fundamental distributions on the sphere, yet cannot straightforwardly be
written as a normalizing flow except in two special cases: the von-Mises Fisher
in D=3 and the central angular Gaussian in any D. In this paper, we describe
how to generalize these special cases to a family of normalizing flows that
behave similarly to the full FB or AG family in any D. We call them
"zoom-linear-project" (ZLP)-Fisher flows. Unlike a normal Fisher-Bingham
distribution, their composition allows to gradually add complexity as needed.
Furthermore, they can naturally handle conditional density estimation with
target distributions that vary by orders of magnitude in scale - a setting that
is important in astronomical applications but that existing flows often
struggle with. A particularly useful member of the new family is the Kent
analogue that can cheaply upgrade any flow in this situation to yield better
performance.

</details>


### [804] [Kernel ridge regression under power-law data: spectrum and generalization](https://arxiv.org/abs/2510.04780)
*Arie Wortsman,Bruno Loureiro*

Main category: stat.ML

TL;DR: This paper analyzes high-dimensional kernel ridge regression (KRR) under power-law anisotropic data, highlighting its advantage over isotropic data.


<details>
  <summary>Details</summary>
Motivation: To understand how kernel-based machine learning algorithms perform when applied to anisotropic power-law data, differing from classical assumptions.

Method: Derivation of kernel spectrum for polynomial inner-product kernels and asymptotic excess risk analysis under high-dimensional settings.

Result: Effective dimension of data, rather than ambient dimension, governs sample complexity in KRR applied to anisotropic power-law data.

Conclusion: Power-law anisotropic data offers inherent advantages for kernel-based learning algorithms compared to isotropic data.

Abstract: In this work, we investigate high-dimensional kernel ridge regression (KRR)
on i.i.d. Gaussian data with anisotropic power-law covariance. This setting
differs fundamentally from the classical source & capacity conditions for KRR,
where power-law assumptions are typically imposed on the kernel eigen-spectrum
itself. Our contributions are twofold. First, we derive an explicit
characterization of the kernel spectrum for polynomial inner-product kernels,
giving a precise description of how the kernel eigen-spectrum inherits the data
decay. Second, we provide an asymptotic analysis of the excess risk in the
high-dimensional regime for a particular kernel with this spectral behavior,
showing that the sample complexity is governed by the effective dimension of
the data rather than the ambient dimension. These results establish a
fundamental advantage of learning with power-law anisotropic data over
isotropic data. To our knowledge, this is the first rigorous treatment of
non-linear KRR under power-law data.

</details>


### [805] [A Noise Resilient Approach for Robust Hurst Exponent Estimation](https://arxiv.org/abs/2510.04811)
*Malith Premarathna,Fabrizio Ruggeri,Dixon Vimalajeewa*

Main category: stat.ML

TL;DR: The study introduces NC-ALPHEE, an enhanced method for estimating the Hurst exponent, improving noise performance over traditional techniques.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of accurately estimating the Hurst exponent in noisy real-world data.

Method: The method involves enhancing an existing estimator (ALPHEE), implementing noise mitigation, employing neural networks for adaptive learning, and generating level-pairwise estimates.

Result: NC-ALPHEE matches the performance of ALPHEE in noise-free conditions and significantly outperforms it under noisy conditions without requiring constraints.

Conclusion: NC-ALPHEE provides a robust solution to reliably estimate Hurst exponent in noisy environments, outpacing existing wavelet-based approaches.

Abstract: Understanding signal behavior across scales is vital in areas such as natural
phenomena analysis and financial modeling. A key property is self-similarity,
quantified by the Hurst exponent (H), which reveals long-term dependencies.
Wavelet-based methods are effective for estimating H due to their multi-scale
analysis capability, but additive noise in real-world measurements often
degrades accuracy. We propose Noise-Controlled ALPHEE (NC-ALPHEE), an
enhancement of the Average Level-Pairwise Hurst Exponent Estimator (ALPHEE),
incorporating noise mitigation and generating multiple level-pairwise estimates
from signal energy pairs. A neural network (NN) combines these estimates,
replacing traditional averaging. This adaptive learning maintains ALPHEE's
behavior in noise-free cases while improving performance in noisy conditions.
Extensive simulations show that in noise-free data, NC-ALPHEE matches ALPHEE's
accuracy using both averaging and NN-based methods. Under noise, however,
traditional averaging deteriorates and requires impractical level restrictions,
while NC-ALPHEE consistently outperforms existing techniques without such
constraints. NC-ALPHEE offers a robust, adaptive approach for H estimation,
significantly enhancing the reliability of wavelet-based methods in noisy
environments.

</details>


### [806] [Set to Be Fair: Demographic Parity Constraints for Set-Valued Classification](https://arxiv.org/abs/2510.04926)
*Eyal Cohen,Christophe Denis,Mohamed Hebiri*

Main category: stat.ML

TL;DR: The paper presents methods for fair set-valued classification under constraints on demographic parity and expected size.


<details>
  <summary>Details</summary>
Motivation: Set-valued classification may lead to amplified discriminatory bias, prompting advancements in fairness-aware methods.

Method: Two methods are introduced: an oracle-based approach focusing on classification risk, constraints satisfaction, and a computationally efficient proxy prioritizing constraint adherence.

Result: Optimal fair classifiers were developed with theoretical guarantees, including convergence rates and excess-risk bounds. Empirical evaluations validate the methods' effectiveness.

Conclusion: The proposed strategies address fairness and efficiency, offering viable solutions for set-valued classification under constraint settings.

Abstract: Set-valued classification is used in multiclass settings where confusion
between classes can occur and lead to misleading predictions. However, its
application may amplify discriminatory bias motivating the development of
set-valued approaches under fairness constraints. In this paper, we address the
problem of set-valued classification under demographic parity and expected size
constraints. We propose two complementary strategies: an oracle-based method
that minimizes classification risk while satisfying both constraints, and a
computationally efficient proxy that prioritizes constraint satisfaction. For
both strategies, we derive closed-form expressions for the (optimal) fair
set-valued classifiers and use these to build plug-in, data-driven procedures
for empirical predictions. We establish distribution-free convergence rates for
violations of the size and fairness constraints for both methods, and under
mild assumptions we also provide excess-risk bounds for the oracle-based
approach. Empirical results demonstrate the effectiveness of both strategies
and highlight the efficiency of our proxy method.

</details>


### [807] [Embracing Discrete Search: A Reasonable Approach to Causal Structure Learning](https://arxiv.org/abs/2510.04970)
*Marcel Wienöbst,Leonard Henckel,Sebastian Weichwald*

Main category: stat.ML

TL;DR: FLOP, a score-based algorithm for linear models, introduces faster parent selection and Cholesky score updates, achieving accurate and efficient causal discovery.


<details>
  <summary>Details</summary>
Motivation: The need for faster and more accurate causal discovery algorithms to analyze and discover relationships in data using linear models.

Method: FLOP combines fast parent selection with Cholesky-based score updates, along with discrete search and iterated local search strategies.

Result: FLOP achieves superior accuracy, with near-perfect recovery in standard benchmarks and dramatically reduced run-times compared to prior methods.

Conclusion: This algorithm suggests discrete search is a viable and effective method for causal discovery, challenging assumptions about its feasibility.

Abstract: We present FLOP (Fast Learning of Order and Parents), a score-based causal
discovery algorithm for linear models. It pairs fast parent selection with
iterative Cholesky-based score updates, cutting run-times over prior
algorithms. This makes it feasible to fully embrace discrete search, enabling
iterated local search with principled order initialization to find graphs with
scores at or close to the global optimum. The resulting structures are highly
accurate across benchmarks, with near-perfect recovery in standard settings.
This performance calls for revisiting discrete search over graphs as a
reasonable approach to causal discovery.

</details>


### [808] [Curiosity-Driven Co-Development of Action and Language in Robots Through Self-Exploration](https://arxiv.org/abs/2510.05013)
*Theodore Jerome Tinker,Kenji Doya,Jun Tani*

Main category: stat.ML

TL;DR: This paper simulates robots learning actions based on imperative sentences using active inference and reinforcement learning, achieving notable findings on curiosity-driven exploration and compositional generalization.


<details>
  <summary>Details</summary>
Motivation: Understanding mechanisms behind the efficient developmental learning in human infants compared to resource-intensive learning in large language models.

Method: Robots were trained using active inference combined with reinforcement learning for a curiosity-driven, self-guided exploration to map sentences to corresponding actions.

Result: Key findings include improved learning with curiosity-driven exploration, sequential emergence of simpler to complex actions, early rote pairing of sentences and actions, and enhanced generalization with more compositional elements.

Conclusion: The study provides insights into infant co-developmental learning and bridges computational findings with developmental psychology.

Abstract: Human infants acquire language and action co-developmentally, achieving
remarkable generalization capabilities from only a minimal number of learning
examples. In contrast, recent large language models require exposure to
billions of training tokens to achieve such generalization. What mechanisms
underlie such efficient developmental learning in humans? This study addresses
this question through simulation experiments in which robots learn to perform
various actions corresponding to imperative sentences (e.g., \textit{push red
cube}) via trials of self-guided exploration. Our approach integrates the
active inference framework with reinforcement learning, enabling
curiosity-driven developmental learning. The simulations yielded several
nontrivial findings: i) Curiosity-driven exploration combined with motor noise
substantially outperforms learning without curiosity. ii) Simpler,
prerequisite-like actions emerge earlier in development, while more complex
actions involving these prerequisites develop later. iii) Rote pairing of
sentences and actions occurs before the emergence of compositional
generalization. iv) Generalization is drastically improved as the number of
compositional elements increases. These results shed light into possible
mechanisms underlying efficient co-developmental learning in infants and
provide computational parallels to findings in developmental psychology.

</details>


### [809] [Causal Abstractions, Categorically Unified](https://arxiv.org/abs/2510.05033)
*Markus Englberger,Devendra Singh Dhami*

Main category: stat.ML

TL;DR: This paper introduces a categorical framework for defining causal abstractions, using natural transformations and Markov functors, and unifying past results while generalizing causal abstraction methods.


<details>
  <summary>Details</summary>
Motivation: The authors aim to develop a unified, generalized framework for causal abstractions that relates different levels of abstraction in causal models and addresses limitations in existing approaches.

Method: The framework leverages categorical principles using natural transformations between Markov functors, utilizes string diagrammatic tools for graph consistency, and incorporates interventions and do-calculus under specific graph scenarios.

Result: The authors demonstrate how the framework generalizes earlier results on causal abstractions, shows consistency across abstraction levels under interventions, and validates high-to-low level causal inference even with graphical confounders.

Conclusion: The research argues for the superiority of this categorical framework in modeling causal abstractions and recovers notions like $	au$-consistency and constructive $	au$-abstractions within the proposed approach.

Abstract: We present a categorical framework for relating causal models that represent
the same system at different levels of abstraction. We define a causal
abstraction as natural transformations between appropriate Markov functors,
which concisely consolidate desirable properties a causal abstraction should
exhibit. Our approach unifies and generalizes previously considered causal
abstractions, and we obtain categorical proofs and generalizations of existing
results on causal abstractions. Using string diagrammatical tools, we can
explicitly describe the graphs that serve as consistent abstractions of a
low-level graph under interventions. We discuss how methods from mechanistic
interpretability, such as circuit analysis and sparse autoencoders, fit within
our categorical framework. We also show how applying do-calculus on a
high-level graphical abstraction of an acyclic-directed mixed graph (ADMG),
when unobserved confounders are present, gives valid results on the low-level
graph, thus generalizing an earlier statement by Anand et al. (2023). We argue
that our framework is more suitable for modeling causal abstractions compared
to existing categorical frameworks. Finally, we discuss how notions such as
$\tau$-consistency and constructive $\tau$-abstractions can be recovered with
our framework.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [810] [Real-time nonlinear inversion of magnetic resonance elastography with operator learning](https://arxiv.org/abs/2510.03372)
*Juampablo E. Heras Rivera,Caitlin M. Neher,Mehmet Kurt*

Main category: eess.IV

TL;DR: This paper proposes oNLI, a deep learning framework for real-time magnetic resonance elastography (MRE) data inversion in the brain, offering significant speedup and improved accuracy compared to alternative methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the performance and accuracy limitations in nonlinear inversion (NLI) for brain MRE, enabling real-time processing while preserving spatial accuracy.

Method: A deep operator learning framework (oNLI) was developed, trained on complex curl of displacement fields as inputs and NLI-derived elastograms as outputs. Key improvements included structural priors for enhanced spatial accuracy and comparisons with CNNs using metrics like correlation, error rates, and statistical analyses.

Result: oNLI shows lower absolute percent error and higher correlation coefficients compared to CNN baselines, achieving better spatial accuracy and real-time inversion capabilities.

Conclusion: oNLI offers a transformative improvement in speed and accuracy for MRE data inversion compared to CNNs, facilitating rapid yet precise brain elastogram analysis.

Abstract: $\textbf{Purpose:}$ To develop and evaluate an operator learning framework
for nonlinear inversion (NLI) of brain magnetic resonance elastography (MRE)
data, which enables real-time inversion of elastograms with comparable spatial
accuracy to NLI.
  $\textbf{Materials and Methods:}$ In this retrospective study, 3D MRE data
from 61 individuals (mean age, 37.4 years; 34 female) were used for development
of the framework. A predictive deep operator learning framework (oNLI) was
trained using 10-fold cross-validation, with the complex curl of the measured
displacement field as inputs and NLI-derived reference elastograms as outputs.
A structural prior mechanism, analogous to Soft Prior Regularization in the MRE
literature, was incorporated to improve spatial accuracy. Subject-level
evaluation metrics included Pearson's correlation coefficient, absolute
relative error, and structural similarity index measure between predicted and
reference elastograms across brain regions of different sizes to understand
accuracy. Statistical analyses included paired t-tests comparing the proposed
oNLI variants to the convolutional neural network baselines.
  $\textbf{Results:}$ Whole brain absolute percent error was 8.4 $\pm$ 0.5
($\mu'$) and 10.0 $\pm$ 0.7 ($\mu''$) for oNLI and 15.8 $\pm$ 0.8 ($\mu'$) and
26.1 $\pm$ 1.1 ($\mu''$) for CNNs. Additionally, oNLI outperformed
convolutional architectures as per Pearson's correlation coefficient, $r$, in
the whole brain and across all subregions for both the storage modulus and loss
modulus (p < 0.05).
  $\textbf{Conclusion:}$ The oNLI framework enables real-time MRE inversion
(30,000x speedup), outperforming CNN-based approaches and maintaining the
fine-grained spatial accuracy achievable with NLI in the brain.

</details>


### [811] [How We Won BraTS-SSA 2025: Brain Tumor Segmentation in the Sub-Saharan African Population Using Segmentation-Aware Data Augmentation and Model Ensembling](https://arxiv.org/abs/2510.03568)
*Claudia Takyi Ankomah,Livingstone Eli Ayivor,Ireneaus Nyame,Leslie Wambo,Patrick Yeboah Bonsu,Aondona Moses Iorumbur,Raymond Confidence,Toufiq Musah*

Main category: eess.IV

TL;DR: The paper addresses improved glioma segmentation by combining advanced data augmentation and ensemble modeling to enhance performance on underrepresented datasets like BraTS-Africa.


<details>
  <summary>Details</summary>
Motivation: The study seeks to overcome challenges posed by glioma segmentation, particularly in underserved regions, due to the variability in brain structures and limited diversity of existing training datasets.

Method: The researchers enhanced dataset size and diversity with segmentation-aware offline data augmentation on the BraTS-Africa dataset. They built an ensemble of complementary models: MedNeXt, SegMamba, and Residual-Encoder U-Net. Models were trained with varying epochs to evaluate robustness and segmentation performance.

Result: The MedNeXt model achieved the highest lesion-wise dice score of 0.86 and normalized surface distance score of 0.81 after 1000 training epochs. Meanwhile, the ensemble model exhibited balanced performance across tumor subregions after 500 epochs.

Conclusion: The study concludes that leveraging advanced augmentation techniques and ensembling diverse models significantly improves segmentation accuracy and robustness, especially for diverse and underrepresented datasets in regions like Africa.

Abstract: Brain tumors, particularly gliomas, pose significant chall-enges due to their
complex growth patterns, infiltrative nature, and the variability in brain
structure across individuals, which makes accurate diagnosis and monitoring
difficult. Deep learning models have been developed to accurately delineate
these tumors. However, most of these models were trained on relatively
homogenous high-resource datasets, limiting their robustness when deployed in
underserved regions. In this study, we performed segmentation-aware offline
data augmentation on the BraTS-Africa dataset to increase the data sample size
and diversity to enhance generalization. We further constructed an ensemble of
three distinct architectures, MedNeXt, SegMamba, and Residual-Encoder U-Net, to
leverage their complementary strengths. Our best-performing model, MedNeXt, was
trained on 1000 epochs and achieved the highest average lesion-wise dice and
normalized surface distance scores of 0.86 and 0.81 respectively. However, the
ensemble model trained for 500 epochs produced the most balanced segmentation
performance across the tumour subregions. This work demonstrates that a
combination of advanced augmentation and model ensembling can improve
segmentation accuracy and robustness on diverse and underrepresented datasets.
Code available at:
https://github.com/SPARK-Academy-2025/SPARK-2025/tree/main/SPARK2025_BraTs_MODELS/SPARK_NeuroAshanti

</details>


### [812] [Towards Robust and Generalizable Continuous Space-Time Video Super-Resolution with Events](https://arxiv.org/abs/2510.03833)
*Shuoyan Wei,Feng Li,Shengeng Tang,Runmin Cong,Yao Zhao,Meng Wang,Huihui Bai*

Main category: eess.IV

TL;DR: The paper introduces EvEnhancer and EvEnhancerPlus for robust video super-resolution using event streams, achieving state-of-the-art performance and generalizing well to out-of-distribution scales.


<details>
  <summary>Details</summary>
Motivation: Conventional continuous space-time video super-resolution (C-STVSR) struggles with poor generalization when applied to videos at out-of-distribution (OOD) spatial and temporal scales.

Method: The approach integrates event-adapted synthesis to utilize correlations between frames and high-resolution event streams, along with a local implicit video transformer for spatiotemporal attention and continuous representations. EvEnhancerPlus adds a dynamic mechanism to adapt routes and reduce computational overhead, supported by a cross-derivative training strategy for stabilization.

Result: Extensive experiments confirm superior performance on synthetic and real datasets, with strong generalizability to OOD scales.

Conclusion: EvEnhancer and EvEnhancerPlus enable robust and generalizable C-STVSR while optimizing computational efficiency and maintaining high reconstruction quality.

Abstract: Continuous space-time video super-resolution (C-STVSR) has garnered
increasing interest for its capability to reconstruct high-resolution and
high-frame-rate videos at arbitrary spatial and temporal scales. However,
prevailing methods often generalize poorly, producing unsatisfactory results
when applied to out-of-distribution (OOD) scales. To overcome this limitation,
we present EvEnhancer, a novel approach that marries the unique properties of
high temporal resolution and high dynamic range encapsulated in event streams
to achieve robust and generalizable C-STVSR. Our approach incorporates
event-adapted synthesis that capitalizes on the spatiotemporal correlations
between frames and events to capture long-term motion trajectories, enabling
adaptive interpolation and fusion across space and time. This is then coupled
with a local implicit video transformer that integrates local implicit video
neural function with cross-scale spatiotemporal attention to learn continuous
video representations and generate plausible videos at arbitrary resolutions
and frame rates. We further develop EvEnhancerPlus, which builds a controllable
switching mechanism that dynamically determines the reconstruction difficulty
for each spatiotemporal pixel based on local event statistics. This allows the
model to adaptively route reconstruction along the most suitable pathways at a
fine-grained pixel level, substantially reducing computational overhead while
maintaining excellent performance. Furthermore, we devise a cross-derivative
training strategy that stabilizes the convergence of such a multi-pathway
framework through staged cross-optimization. Extensive experiments demonstrate
that our method achieves state-of-the-art performance on both synthetic and
real-world datasets, while maintaining superior generalizability at OOD scales.
The code is available at https://github.com/W-Shuoyan/EvEnhancerPlus.

</details>


### [813] [AI-Assisted Pleural Effusion Volume Estimation from Contrast-Enhanced CT Images](https://arxiv.org/abs/2510.03856)
*Sanhita Basu,Tomas Fröding,Ali Teymur Kahraman,Dimitris Toumpanakis,Tobias Sjöblom*

Main category: eess.IV

TL;DR: This paper presents a semi-supervised deep learning framework (TTAS) for Pleural Effusion segmentation and volume quantification in CT scans, showing superior performance over state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Accurate measurement and segmentation of Pleural Effusions in CT scans are vital for effective clinical management, but existing approaches are suboptimal.

Method: The study employed a novel semi-supervised TTAS deep learning framework, using manually annotated CT Pulmonary Angiogram data for training and non-segmented cases for testing and validation.

Result: The TTAS model outperformed nnU-Net in segmentation accuracy (mean Dice score 0.82 vs 0.73) and volume determination (mean Absolute Volume Difference: 6.49 mL vs 23.16 mL, p < 0.0001).

Conclusion: TTAS enables more accurate segmentation and volume quantification of Pleural Effusions, showing promise for improved clinical applications.

Abstract: Background: Pleural Effusions (PE) is a common finding in many different
clinical conditions, but accurately measuring their volume from CT scans is
challenging. Purpose: To improve PE segmentation and quantification for
enhanced clinical management, we have developed and trained a semi-supervised
deep learning framework on contrast-enhanced CT volumes. Materials and Methods:
This retrospective study collected CT Pulmonary Angiogram (CTPA) data from
internal and external datasets. A subset of 100 cases was manually annotated
for model training, while the remaining cases were used for testing and
validation. A novel semi-supervised deep learning framework, Teacher-Teaching
Assistant-Student (TTAS), was developed and used to enable efficient training
in non-segmented examinations. Segmentation performance was compared to that of
state-of-the-art models. Results: 100 patients (mean age, 72 years, 28
[standard deviation]; 55 men) were included in the study. The TTAS model
demonstrated superior segmentation performance compared to state-of-the-art
models, achieving a mean Dice score of 0.82 (95% CI, 0.79 - 0.84) versus 0.73
for nnU-Net (p < 0.0001, Student's T test). Additionally, TTAS exhibited a
four-fold lower mean Absolute Volume Difference (AbVD) of 6.49 mL (95% CI, 4.80
- 8.20) compared to nnU-Net's AbVD of 23.16 mL (p < 0.0001). Conclusion: The
developed TTAS framework offered superior PE segmentation, aiding accurate
volume determination from CT scans.

</details>


### [814] [Sliding Window Attention for Learned Video Compression](https://arxiv.org/abs/2510.03926)
*Alexander Kopte,André Kaup*

Main category: eess.IV

TL;DR: This paper addresses limitations of transformers in video compression by introducing 3D Sliding Window Attention (SWA), enhancing both performance and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of existing models like Video Compression Transformer (VCT), which suffer from irregular receptive fields and computational inefficiency due to overlapping windows.

Method: The paper proposes a patchless local attention mechanism called 3D Sliding Window Attention (SWA), alongside a decoder-only architecture that unifies spatial and temporal context processing.

Result: The proposed method achieves up to 18.6% Bjørntegaard Delta-rate savings compared to VCT, reduces decoder complexity by 2.8×, and increases entropy model efficiency by 3.5×.

Conclusion: 3D SWA unifies context processing, simplifies computation, and enhances rate-distortion performance, though excessive long-range context may negatively impact results.

Abstract: To manage the complexity of transformers in video compression, local
attention mechanisms are a practical necessity. The common approach of
partitioning frames into patches, however, creates architectural flaws like
irregular receptive fields. When adapted for temporal autoregressive models,
this paradigm, exemplified by the Video Compression Transformer (VCT), also
necessitates computationally redundant overlapping windows. This work
introduces 3D Sliding Window Attention (SWA), a patchless form of local
attention. By enabling a decoder-only architecture that unifies spatial and
temporal context processing, and by providing a uniform receptive field, our
method significantly improves rate-distortion performance, achieving
Bj{\o}rntegaard Delta-rate savings of up to 18.6 % against the VCT baseline.
Simultaneously, by eliminating the need for overlapping windows, our method
reduces overall decoder complexity by a factor of 2.8, while its entropy model
is nearly 3.5 times more efficient. We further analyze our model's behavior and
show that while it benefits from long-range temporal context, excessive context
can degrade performance.

</details>


### [815] [The method of the approximate inverse for limited-angle CT](https://arxiv.org/abs/2510.04369)
*Bernadette Hahn,Gael Rigaud,Richard Schmähl*

Main category: eess.IV

TL;DR: The paper introduces a constrained reconstruction method for limited-angle tomography that mitigates artefacts and stabilizes the process using an innovative approach.


<details>
  <summary>Details</summary>
Motivation: Limited-angle tomography faces challenges due to artefacts produced in standard methods like FBP or total-variation functional, especially when dealing with large imaging datasets and limited data angles.

Method: The authors propose a method called CLARK that leverages a regularization strategy combining spectral filtering, approximate inverse techniques, and custom denoising using precomputed reconstruction kernels.

Result: The proposed CLARK method effectively addresses artefacts found in limited-angle tomography, removes streak artefacts present in traditional approaches, and stabilizes solutions despite ill-conditioned problems.

Conclusion: CLARK provides a practical way to handle large-scale limited-angle tomography problems, making it a promising foundation for future advancements in imaging techniques and deep learning applications.

Abstract: Limited-angle computerized tomography stands for one of the most difficult
challenges in imaging. Although it opens the way to faster data acquisition in
industry and less dangerous scans in medicine, standard approaches, such as the
filtered backprojection (FBP) algorithm or the widely used total-variation
functional, often produce various artefacts that hinder the diagnosis. With the
rise of deep learning, many modern techniques have proven themselves successful
in removing such artefacts but at the cost of large datasets. In this paper, we
propose a new model-driven approach based on the method of the approximate
inverse, which could serve as new starting point for learning strategies in the
future. In contrast to FBP-type approaches, our reconstruction step consists in
evaluating linear functionals on the measured data using reconstruction kernels
that are precomputed as solution of an auxiliary problem. With this problem
being uniquely solvable, the derived limited-angle reconstruction kernel (LARK)
is able to fully reconstruct the object without the well-known streak
artefacts, even for large limited angles. However, it inherits severe
ill-conditioning which leads to a different kind of artefacts arising from the
singular functions of the limited-angle Radon transform. The problem becomes
particularly challenging when working on semi-discrete (real or analytical)
measurements. We develop a general regularization strategy, named constrained
limited-angle reconstruction kernel (CLARK), by combining spectral filter, the
method of the approximate inverse and custom edge-preserving denoising in order
to stabilize the whole process. We further derive and interpret error estimates
for the application on real, i.e. semi-discrete, data and we validate our
approach on synthetic and real data.

</details>


### [816] [Adaptive double-phase Rudin--Osher--Fatemi denoising model](https://arxiv.org/abs/2510.04382)
*Wojciech Górny,Michał Łasica,Alexandros Matsoukas*

Main category: eess.IV

TL;DR: The paper proposes an image denoising model to address the staircasing issue and preserve edges, tested on various images and noise levels.


<details>
  <summary>Details</summary>
Motivation: The authors aim to improve classical denoising methods like the Rudin–Osher–Fatemi model by reducing staircasing and maintaining edge preservation in the processed images.

Method: The proposed model employs a variable-growth total variation regularization of double-phase type, with adaptive weighting parameters.

Result: Testing on synthetic and natural images in both 1D and 2D under varying noise conditions demonstrated the model's effectiveness in reducing staircasing while preserving edges.

Conclusion: The developed model achieves better image denoising outcomes compared to classical methods, with improved handling of staircasing and edge features, proving its robustness across different noise levels and settings.

Abstract: We propose a new image denoising model based on a variable-growth total
variation regularization of double-phase type with adaptive weight. It is
designed to reduce staircasing with respect to the classical
Rudin--Osher--Fatemi model, while preserving the edges of the image in a
similar fashion. We implement the model and test its performance on synthetic
and natural images in 1D and 2D over a range of noise levels.

</details>


### [817] [ReTiDe: Real-Time Denoising for Energy-Efficient Motion Picture Processing with FPGAs](https://arxiv.org/abs/2510.03812)
*Changhong Li,Clément Bled,Rosa Fernandez,Shreejith Shanker*

Main category: eess.IV

TL;DR: The paper introduces ReTiDe, a hardware-accelerated denoising system using FPGAs for efficient processing of high-resolution video streams with improved energy efficiency and minimal quality loss.


<details>
  <summary>Details</summary>
Motivation: Modern video pipelines require efficient denoising methods to manage sensor noise, quantisation artifacts, and post-production operations, but current models demand high computational and energy resources.

Method: The paper utilizes a compact convolutional model quantised to INT8, deployed on AMD DPU-based FPGAs, integrated within a client-server network to offload computation from host systems while ensuring compatibility with existing workflows.

Result: ReTiDe achieves 37.71x GOPS throughput and 5.29x higher energy efficiency compared to previous solutions, with negligible loss in PSNR/SSIM on benchmark tests.

Conclusion: Specialised FPGA accelerators like ReTiDe offer scalable, energy-efficient denoising solutions for encoding and post-production workflows, maintaining performance metrics and compatibility.

Abstract: Denoising is a core operation in modern video pipelines. In codecs, in-loop
filters suppress sensor noise and quantisation artefacts to improve
rate-distortion performance; in cinema post-production, denoisers are used for
restoration, grain management, and plate clean-up. However, state-of-the-art
deep denoisers are computationally intensive and, at scale, are typically
deployed on GPUs, incurring high power and cost for real-time, high-resolution
streams. This paper presents Real-Time Denoise (ReTiDe), a hardware-accelerated
denoising system that serves inference on data-centre Field Programmable Gate
Arrays (FPGAs). A compact convolutional model is quantised (post-training
quantisation plus quantisation-aware fine-tuning) to INT8 and compiled for AMD
Deep Learning Processor Unit (DPU)-based FPGAs. A client-server integration
offloads computation from the host CPU/GPU to a networked FPGA service, while
remaining callable from existing workflows, e.g., NUKE, without disrupting
artist tooling. On representative benchmarks, ReTiDe delivers 37.71$\times$
Giga Operations Per Second (GOPS) throughput and 5.29$\times$ higher energy
efficiency than prior FPGA denoising accelerators, with negligible degradation
in Peak Signal-to-Noise Ratio (PSNR)/Structural Similarity Index (SSIM). These
results indicate that specialised accelerators can provide practical, scalable
denoising for both encoding pipelines and post-production, reducing energy per
frame without sacrificing quality or workflow compatibility. Code is available
at https://github.com/RCSL-TCD/ReTiDe.

</details>


<div id='cond-mat.stat-mech'></div>

# cond-mat.stat-mech [[Back]](#toc)

### [818] [Optimal Computation from Fluctuation Responses](https://arxiv.org/abs/2510.03900)
*Jinghao Lyu,Kyle J. Ray,James P. Crutchfield*

Main category: cond-mat.stat-mech

TL;DR: The paper introduces a novel framework combining fluctuation response relations and machine learning to optimize energy-efficient protocols for information-processing systems, showcasing applications in bit erasure and task optimization.


<details>
  <summary>Details</summary>
Motivation: Understanding how to minimize energy cost during computations while maintaining accurate outcomes in information-processing systems far from equilibrium.

Method: The proposed method employs fluctuation response relations-derived gradients and iterative learning using noisy trajectory samples, thereby combining distribution and protocol optimization under a unified framework.

Result: The framework successfully computes optimal protocols for canonical tasks, including bit erasure and harmonic trap translation, achieving energy costs comparable to theoretical bounds.

Conclusion: The research provides efficient strategies for designing thermodynamic protocols applicable to quantum gates, chemical networks, and other systems, paving the way for energy-efficient computation in physical systems.

Abstract: The energy cost of computation has emerged as a central challenge at the
intersection of physics and computer science. Recent advances in statistical
physics -- particularly in stochastic thermodynamics -- enable precise
characterizations of work, heat, and entropy production in
information-processing systems driven far from equilibrium by time-dependent
control protocols. A key open question is then how to design protocols that
minimize thermodynamic cost while ensur- ing correct outcomes. To this end, we
develop a unified framework to identify optimal protocols using fluctuation
response relations (FRR) and machine learning. Unlike previous approaches that
optimize either distributions or protocols separately, our method unifies both
using FRR-derived gradients. Moreover, our method is based primarily on
iteratively learning from sampled noisy trajectories, which is generally much
easier than solving for the optimal protocol directly from a set of governing
equations. We apply the framework to canonical examples -- bit erasure in a
double-well potential and translating harmonic traps -- demonstrating how to
construct loss functions that trade-off energy cost against task error. The
framework extends trivially to underdamped systems, and we show this by
optimizing a bit-flip in an underdamped system. In all computations we test,
the framework achieves the theoretically optimal protocol or achieves work
costs comparable to relevant finite time bounds. In short, the results provide
principled strategies for designing thermodynamically efficient protocols in
physical information-processing systems. Applications range from quantum gates
robust under noise to energy-efficient control of chemical and synthetic
biological networks.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [819] [Improving S&P 500 Volatility Forecasting through Regime-Switching Methods](https://arxiv.org/abs/2510.03236)
*Ava C. Blake,Nivika A. Gandhi,Anurag R. Jakkula*

Main category: q-fin.ST

TL;DR: The paper introduces regime-switching models to enhance the prediction of S&P 500 volatility by capturing structural market changes and evaluating performance over different time periods.


<details>
  <summary>Details</summary>
Motivation: The motivation of the paper lies in improving financial market volatility predictions, which are crucial for risk management, derivatives pricing, and investment strategies.

Method: The study developed regime-switching methods, which included algorithms like soft Markov switching, distributional spectral clustering with XGBoost, and a coefficient-based clustering algorithm using HAR coefficients, Bayesian GMM, and XGBoost.

Result: The coefficient-based clustering algorithm outperformed competing models, including autoregressive baselines, across various time periods, showcasing superior predictive accuracy for daily realized volatility.

Conclusion: The research highlights the effectiveness of regime-aware and soft clustering modeling approaches for accurately predicting financial market volatility, particularly during periods of structural change or uncertainty.

Abstract: Accurate prediction of financial market volatility is critical for risk
management, derivatives pricing, and investment strategy. In this study, we
propose a multitude of regime-switching methods to improve the prediction of
S&P 500 volatility by capturing structural changes in the market across time.
We use eleven years of SPX data, from May 1st, 2014 to May 27th, 2025, to
compute daily realized volatility (RV) from 5-minute intraday log returns,
adjusted for irregular trading days. To enhance forecast accuracy, we
engineered features to capture both historical dynamics and forward-looking
market sentiment across regimes. The regime-switching methods include a soft
Markov switching algorithm to estimate soft-regime probabilities, a
distributional spectral clustering method that uses XGBoost to assign clusters
at prediction time, and a coefficient-based soft regime algorithm that extracts
HAR coefficients from time segments segmented through the Mood test and
clusters through Bayesian GMM for soft regime weights, using XGBoost to predict
regime probabilities. Models were evaluated across three time periods--before,
during, and after the COVID-19 pandemic. The coefficient-based clustering
algorithm outperformed all other models, including the baseline autoregressive
model, during all time periods. Additionally, each model was evaluated on its
recursive forecasting performance for 5- and 10-day horizons during each time
period. The findings of this study demonstrate the value of regime-aware
modeling frameworks and soft clustering approaches in improving volatility
forecasting, especially during periods of heightened uncertainty and structural
change.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [820] [On the Limits of Consensus under Dynamic Availability and Reconfiguration](https://arxiv.org/abs/2510.03625)
*Joachim Neu,Javier Nieto,Ling Ren*

Main category: cs.CR

TL;DR: The paper addresses the limitations of existing proof-of-stake blockchain consensus protocols in dynamic settings (DAR) and proposes a new realistic assumption for achieving secure consensus without relying on impractical conditions.


<details>
  <summary>Details</summary>
Motivation: The paper aims to overcome unrealistic assumptions in state-of-the-art DAR consensus protocols, such as social consensus or participating nodes maintaining key evolution during downtime.

Method: It identifies the necessary and sufficient adversarial conditions for DAR consensus and introduces a key disposal mechanism for honest nodes exiting operations. Additionally, it presents a bootstrapping gadget to enable reconfiguration efficiently.

Result: The new assumption and bootstrapping gadget simplify the design of DAR consensus protocols, making them practical and efficient under typical circumstances.

Conclusion: Consensus in proof-of-stake blockchains in DAR settings can be achieved securely and realistically by enforcing key disposal by exiting nodes and leveraging efficient reconfiguration mechanisms.

Abstract: Proof-of-stake blockchains require consensus protocols that support Dynamic
Availability and Reconfiguration (so-called DAR setting), where the former
means that the consensus protocol should remain live even if a large number of
nodes temporarily crash, and the latter means it should be possible to change
the set of operating nodes over time. State-of-the-art protocols for the DAR
setting, such as Ethereum, Cardano's Ouroboros, or Snow White, require
unrealistic additional assumptions, such as social consensus, or that key
evolution is performed even while nodes are not participating. In this paper,
we identify the necessary and sufficient adversarial condition under which
consensus can be achieved in the DAR setting without additional assumptions. We
then introduce a new and realistic additional assumption: honest nodes dispose
of their cryptographic keys the moment they express intent to exit from the set
of operating nodes. To add reconfiguration to any dynamically available
consensus protocol, we provide a bootstrapping gadget that is particularly
simple and efficient in the common optimistic case of few reconfigurations and
no double-spending attempts.

</details>


### [821] [MulVuln: Enhancing Pre-trained LMs with Shared and Language-Specific Knowledge for Multilingual Vulnerability Detection](https://arxiv.org/abs/2510.04397)
*Van Nguyen,Surya Nepal,Xingliang Yuan,Tingmin Wu,Fengchao Chen,Carsten Rudolph*

Main category: cs.CR

TL;DR: The paper introduces MULVULN, a multilingual software vulnerability detection approach, which achieves superior performance compared to state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: There is a need for effective solutions to detect software vulnerabilities in multilingual codebases, as current AI-based methods perform poorly across diverse programming languages.

Method: The proposed MULVULN model captures shared and language-specific knowledge of source code through multilingual learning, enabling robust vulnerability detection.

Result: MULVULN outperforms thirteen state-of-the-art baselines on the REEF dataset, improving F1-scores by 1.45% to 23.59%.

Conclusion: MULVULN helps better detect vulnerabilities in complex, multilingual software systems and demonstrates strong potential for real-world applications.

Abstract: Software vulnerabilities (SVs) pose a critical threat to safety-critical
systems, driving the adoption of AI-based approaches such as machine learning
and deep learning for software vulnerability detection. Despite promising
results, most existing methods are limited to a single programming language.
This is problematic given the multilingual nature of modern software, which is
often complex and written in multiple languages. Current approaches often face
challenges in capturing both shared and language-specific knowledge of source
code, which can limit their performance on diverse programming languages and
real-world codebases. To address this gap, we propose MULVULN, a novel
multilingual vulnerability detection approach that learns from source code
across multiple languages. MULVULN captures both the shared knowledge that
generalizes across languages and the language-specific knowledge that reflects
unique coding conventions. By integrating these aspects, it achieves more
robust and effective detection of vulnerabilities in real-world multilingual
software systems. The rigorous and extensive experiments on the real-world and
diverse REEF dataset, consisting of 4,466 CVEs with 30,987 patches across seven
programming languages, demonstrate the superiority of MULVULN over thirteen
effective and state-of-the-art baselines. Notably, MULVULN achieves
substantially higher F1-score, with improvements ranging from 1.45% to 23.59%
compared to the baseline methods.

</details>


### [822] [P2P: A Poison-to-Poison Remedy for Reliable Backdoor Defense in LLMs](https://arxiv.org/abs/2510.04503)
*Shuai Zhao,Xinyi Wu,Shiqian Zhao,Xiaobao Wu,Zhongliang Guo,Yanhao Jia,Anh Tuan Luu*

Main category: cs.CR

TL;DR: The paper introduces Poison-to-Poison (P2P), a general defense algorithm to combat data-poisoning backdoor attacks in large language models.


<details>
  <summary>Details</summary>
Motivation: To address the vulnerability of large language models to data-poisoning backdoor attacks during fine-tuning and overcome the narrow applicability of existing defense strategies.

Method: P2P works by injecting benign triggers with safe alternative labels into part of the training data, fine-tuning the model on this re-poisoned dataset using prompt-based learning to override the influence of malicious triggers.

Result: Extensive experiments demonstrate that P2P significantly reduces the attack success rate across multiple tasks (classification, mathematical reasoning, and summary generation) on state-of-the-art LLMs.

Conclusion: P2P is a general, robust, and effective defense algorithm that neutralizes malicious backdoors while preserving task performance, providing insights for securing LLMs against backdoor attacks.

Abstract: During fine-tuning, large language models (LLMs) are increasingly vulnerable
to data-poisoning backdoor attacks, which compromise their reliability and
trustworthiness. However, existing defense strategies suffer from limited
generalization: they only work on specific attack types or task settings. In
this study, we propose Poison-to-Poison (P2P), a general and effective backdoor
defense algorithm. P2P injects benign triggers with safe alternative labels
into a subset of training samples and fine-tunes the model on this re-poisoned
dataset by leveraging prompt-based learning. This enforces the model to
associate trigger-induced representations with safe outputs, thereby overriding
the effects of original malicious triggers. Thanks to this robust and
generalizable trigger-based fine-tuning, P2P is effective across task settings
and attack types. Theoretically and empirically, we show that P2P can
neutralize malicious backdoors while preserving task performance. We conduct
extensive experiments on classification, mathematical reasoning, and summary
generation tasks, involving multiple state-of-the-art LLMs. The results
demonstrate that our P2P algorithm significantly reduces the attack success
rate compared with baseline models. We hope that the P2P can serve as a
guideline for defending against backdoor attacks and foster the development of
a secure and trustworthy LLM community.

</details>


### [823] [NEXUS: Network Exploration for eXploiting Unsafe Sequences in Multi-Turn LLM Jailbreaks](https://arxiv.org/abs/2510.03417)
*Javad Rafiei Asl,Sidhant Narula,Mohammad Ghasemigol,Eduardo Blanco,Daniel Takabi*

Main category: cs.CR

TL;DR: NEXUS is a modular framework designed to exploit vulnerabilities in large language models through systematic multi-turn jailbreak attacks, significantly surpassing existing methods in attack success rates.


<details>
  <summary>Details</summary>
Motivation: LLMs are transformative for natural language processing but remain susceptible to sophisticated jailbreak attacks, necessitating effective strategies for addressing these security gaps.

Method: The NEXUS framework consists of three components: the ThoughtNet for crafting structured semantic networks, a Simulator for iterative refinement and pruning of attack paths, and a Network Traverser for navigating the optimized query space during real-time attacks.

Result: NEXUS achieved up to 19.4% higher attack success rates compared to previous methods across multiple LLM platforms.

Conclusion: NEXUS demonstrates a potent approach for identifying and exploiting adversarial vulnerabilities in LLMs, pointing towards a need for more robust security measures in these systems.

Abstract: Large Language Models (LLMs) have revolutionized natural language processing
but remain vulnerable to jailbreak attacks, especially multi-turn jailbreaks
that distribute malicious intent across benign exchanges and bypass alignment
mechanisms. Existing approaches often explore the adversarial space poorly,
rely on hand-crafted heuristics, or lack systematic query refinement. We
present NEXUS (Network Exploration for eXploiting Unsafe Sequences), a modular
framework for constructing, refining, and executing optimized multi-turn
attacks. NEXUS comprises: (1) ThoughtNet, which hierarchically expands a
harmful intent into a structured semantic network of topics, entities, and
query chains; (2) a feedback-driven Simulator that iteratively refines and
prunes these chains through attacker-victim-judge LLM collaboration using
harmfulness and semantic-similarity benchmarks; and (3) a Network Traverser
that adaptively navigates the refined query space for real-time attacks. This
pipeline uncovers stealthy, high-success adversarial paths across LLMs. On
several closed-source and open-source LLMs, NEXUS increases attack success rate
by 2.1% to 19.4% over prior methods. Code: https://github.com/inspire-lab/NEXUS

</details>


### [824] [Proactive defense against LLM Jailbreak](https://arxiv.org/abs/2510.05052)
*Weiliang Zhao,Jinjun Peng,Daniel Ben-Levi,Zhou Yu,Junfeng Yang*

Main category: cs.CR

TL;DR: ProAct is a proactive defense framework designed to mislead adversarial jailbreak attacks on large language models by giving spurious responses, reducing attack success rates by up to 92%.


<details>
  <summary>Details</summary>
Motivation: Large language models require robust safety alignment due to vulnerabilities to adversarial attacks, especially multi-turn jailbreaking that adapts iteratively. Current reactive methods fall short in countering these sophisticated attacks.

Method: ProAct provides intentionally misleading 'spurious responses' designed to appear as successful jailbreak results but lack harmful content, thereby disrupting adversarial optimization loops and misleading attackers.

Result: Experiments demonstrate ProAct reduces jailbreak attack success rates by up to 92% and, when paired with other defensive methods, can reduce attack success to 0%.

Conclusion: ProAct is an effective and orthogonal defense strategy that enhances LLM safety against advanced jailbreak attacks, supplementing existing defense mechanisms.

Abstract: The proliferation of powerful large language models (LLMs) has necessitated
robust safety alignment, yet these models remain vulnerable to evolving
adversarial attacks, including multi-turn jailbreaks that iteratively search
for successful queries. Current defenses, primarily reactive and static, often
fail to counter these search-based attacks. In this paper, we introduce ProAct,
a novel proactive defense framework designed to disrupt and mislead autonomous
jailbreaking processes. Our core idea is to intentionally provide adversaries
with "spurious responses" that appear to be results of successful jailbreak
attacks but contain no actual harmful content. These misleading responses
provide false signals to the attacker's internal optimization loop, causing the
adversarial search to terminate prematurely and effectively jailbreaking the
jailbreak. By conducting extensive experiments across state-of-the-art LLMs,
jailbreaking frameworks, and safety benchmarks, our method consistently and
significantly reduces attack success rates by up to 92\%. When combined with
other defense frameworks, it further reduces the success rate of the latest
attack strategies to 0\%. ProAct represents an orthogonal defense strategy that
can serve as an additional guardrail to enhance LLM safety against the most
effective jailbreaking attacks.

</details>


### [825] [PentestMCP: A Toolkit for Agentic Penetration Testing](https://arxiv.org/abs/2510.03610)
*Zachary Ezetta,Wu-chang Feng*

Main category: cs.CR

TL;DR: Agentic AI enhances security tasks with automation. The PentestMCP library uses an MCP framework for advanced, customizable penetration testing.


<details>
  <summary>Details</summary>
Motivation: To utilize agentic AI's capabilities for enhancing penetration testing workflows, improving efficiency and flexibility.

Method: Development of PentestMCP, a library implementing the Model-Context-Protocol (MCP) paradigm, enabling remote-procedure calls for agentic applications in security.

Result: PentestMCP supports various penetration testing tasks such as scanning, enumeration, fingerprinting, and exploitation with multi-agent workflows.

Conclusion: PentestMCP demonstrates the potential of MCP server implementations in streamlining and customizing agentic penetration testing processes.

Abstract: Agentic AI is transforming security by automating many tasks being performed
manually. While initial agentic approaches employed a monolithic architecture,
the Model-Context-Protocol has now enabled a remote-procedure call (RPC)
paradigm to agentic applications, allowing for the flexible construction and
composition of multi-function agents. This paper describes PentestMCP, a
library of MCP server implementations that support agentic penetration testing.
By supporting common penetration testing tasks such as network scanning,
resource enumeration, service fingerprinting, vulnerability scanning,
exploitation, and post-exploitation, PentestMCP allows a developer to customize
multi-agent workflows for performing penetration tests.

</details>


### [826] [Explainable but Vulnerable: Adversarial Attacks on XAI Explanation in Cybersecurity Applications](https://arxiv.org/abs/2510.03623)
*Maraz Mia,Mir Mehedi A. Pritom*

Main category: cs.CR

TL;DR: This paper explores six attack techniques targeting XAI methods like SHAP, LIME, and IG and examines their impact within cybersecurity contexts such as phishing and malware detection.


<details>
  <summary>Details</summary>
Motivation: The paper aims to highlight vulnerabilities in XAI methods and emphasize the need to enhance their resilience against adversarial attacks.

Method: The authors analyze techniques like fairwashing explanation and backdoor-enabled manipulation attacks across post-hoc XAI methods within cybersecurity scenarios.

Result: The study reveals that adversarial attacks significantly impact XAI methods and their application in decision-making processes.

Conclusion: Immediate measures are needed to strengthen XAI methods to withstand adversarial manipulations, especially in critical fields such as cybersecurity.

Abstract: Explainable Artificial Intelligence (XAI) has aided machine learning (ML)
researchers with the power of scrutinizing the decisions of the black-box
models. XAI methods enable looking deep inside the models' behavior, eventually
generating explanations along with a perceived trust and transparency. However,
depending on any specific XAI method, the level of trust can vary. It is
evident that XAI methods can themselves be a victim of post-adversarial attacks
that manipulate the expected outcome from the explanation module. Among such
attack tactics, fairwashing explanation (FE), manipulation explanation (ME),
and backdoor-enabled manipulation attacks (BD) are the notable ones. In this
paper, we try to understand these adversarial attack techniques, tactics, and
procedures (TTPs) on explanation alteration and thus the effect on the model's
decisions. We have explored a total of six different individual attack
procedures on post-hoc explanation methods such as SHAP (SHapley Additive
exPlanations), LIME (Local Interpretable Model-agnostic Explanation), and IG
(Integrated Gradients), and investigated those adversarial attacks in
cybersecurity applications scenarios such as phishing, malware, intrusion, and
fraudulent website detection. Our experimental study reveals the actual
effectiveness of these attacks, thus providing an urgency for immediate
attention to enhance the resiliency of XAI methods and their applications.

</details>


### [827] [You Have Been LaTeXpOsEd: A Systematic Analysis of Information Leakage in Preprint Archives Using Large Language Models](https://arxiv.org/abs/2510.03761)
*Richard A. Dubniczky,Bertalan Borsos,Tihanyi Norbert*

Main category: cs.CR

TL;DR: The paper conducts a large-scale security audit of arXiv preprints, uncovering sensitive information leaks facilitated by non-sanitized LaTeX submissions.


<details>
  <summary>Details</summary>
Motivation: The motivation lies in addressing the overlooked security risks associated with preprint repositories like arXiv, which provide unrestricted access to source materials that may unintentionally disclose sensitive data.

Method: The paper introduces LaTeXpOsEd, a four-stage framework using pattern matching, logical filtering, traditional harvesting techniques, and large language models (LLMs) to detect sensitive information in LaTeX files. Additionally, LLMSec-DB was created to benchmark the secret-detection capabilities of 25 LLMs.

Result: The audit identified thousands of sensitive data leaks, including personal information, credential exposure, GPS metadata, and internal communications, posing risks to researchers and institutions.

Conclusion: The study highlights a critical need for improved sanitization standards for preprint repositories to mitigate security risks and protect researchers' and institutions' reputations.

Abstract: The widespread use of preprint repositories such as arXiv has accelerated the
communication of scientific results but also introduced overlooked security
risks. Beyond PDFs, these platforms provide unrestricted access to original
source materials, including LaTeX sources, auxiliary code, figures, and
embedded comments. In the absence of sanitization, submissions may disclose
sensitive information that adversaries can harvest using open-source
intelligence. In this work, we present the first large-scale security audit of
preprint archives, analyzing more than 1.2 TB of source data from 100,000 arXiv
submissions. We introduce LaTeXpOsEd, a four-stage framework that integrates
pattern matching, logical filtering, traditional harvesting techniques, and
large language models (LLMs) to uncover hidden disclosures within
non-referenced files and LaTeX comments. To evaluate LLMs' secret-detection
capabilities, we introduce LLMSec-DB, a benchmark on which we tested 25
state-of-the-art models. Our analysis uncovered thousands of PII leaks,
GPS-tagged EXIF files, publicly available Google Drive and Dropbox folders,
editable private SharePoint links, exposed GitHub and Google credentials, and
cloud API keys. We also uncovered confidential author communications, internal
disagreements, and conference submission credentials, exposing information that
poses serious reputational risks to both researchers and institutions. We urge
the research community and repository operators to take immediate action to
close these hidden security gaps. To support open science, we release all
scripts and methods from this study but withhold sensitive findings that could
be misused, in line with ethical principles. The source code and related
material are available at the project website https://github.com/LaTeXpOsEd

</details>


### [828] [Quantifying Distributional Robustness of Agentic Tool-Selection](https://arxiv.org/abs/2510.03992)
*Jehyeok Yeon,Isha Chaudhary,Gagandeep Singh*

Main category: cs.CR

TL;DR: Tool selection in agentic systems using LLMs is vulnerable to adversarial attacks, leading to severe performance degradation. ToolCert statistically certifies robustness against such attacks.


<details>
  <summary>Details</summary>
Motivation: There is a growing need to ensure the security of agentic systems using LLMs, especially in their tool selection mechanisms, which can be exploited for unauthorized data access or other adverse outcomes.

Method: ToolCert employs a statistical framework using Bernoulli success modeling to evaluate tool selection robustness against adaptive adversarial attacks that exploit metadata and retrieval strategies.

Result: Under adversarial attacks, certified accuracy drops drastically—near zero for deceptive tools and less than 20% after adaptive adversarial refinement on retrieval and selection stages.

Conclusion: ToolCert highlights critical vulnerabilities in tool selection mechanisms and offers a robust evaluation framework essential for securing agentic systems leveraging LLMs.

Abstract: Large language models (LLMs) are increasingly deployed in agentic systems
where they map user intents to relevant external tools to fulfill a task. A
critical step in this process is tool selection, where a retriever first
surfaces candidate tools from a larger pool, after which the LLM selects the
most appropriate one. This pipeline presents an underexplored attack surface
where errors in selection can lead to severe outcomes like unauthorized data
access or denial of service, all without modifying the agent's model or code.
While existing evaluations measure task performance in benign settings, they
overlook the specific vulnerabilities of the tool selection mechanism under
adversarial conditions. To address this gap, we introduce ToolCert, the first
statistical framework that formally certifies tool selection robustness.
ToolCert models tool selection as a Bernoulli success process and evaluates it
against a strong, adaptive attacker who introduces adversarial tools with
misleading metadata, and are iteratively refined based on the agent's previous
choices. By sampling these adversarial interactions, ToolCert produces a
high-confidence lower bound on accuracy, formally quantifying the agent's
worst-case performance. Our evaluation with ToolCert uncovers the severe
fragility: under attacks injecting deceptive tools or saturating retrieval, the
certified accuracy bound drops near zero, an average performance drop of over
60% compared to non-adversarial settings. For attacks targeting the retrieval
and selection stages, the certified accuracy bound plummets to less than 20%
after just a single round of adversarial adaptation. ToolCert thus reveals
previously unexamined security threats inherent to tool selection and provides
a principled method to quantify an agent's robustness to such threats, a
necessary step for the safe deployment of agentic systems.

</details>


### [829] [PrivSpike: Employing Homomorphic Encryption for Private Inference of Deep Spiking Neural Networks](https://arxiv.org/abs/2510.03995)
*Nges Brian Njungle,Eric Jahns,Milan Stojkov,Michel A. Kinsy*

Main category: cs.CR

TL;DR: This paper introduces PRIVSPIKE, a framework for privacy-preserving Spiking Neural Networks (SNNs) using CKKS homomorphic encryption for encrypted data computation.


<details>
  <summary>Details</summary>
Motivation: To address the privacy risks in deep learning and SNNs due to sensitive data, while exploring energy-efficient yet secure computation approaches.

Method: The framework utilizes the CKKS homomorphic encryption scheme and introduces two algorithms: one for efficient polynomial approximation of the Leaky Integrate-and-Fire function, and another for precision optimization with a trade-off in computational cost.

Result: PRIVSPIKE achieved competitive encrypted inference accuracies on multiple benchmarks (e.g., MNIST: 98.10%, CIFAR-10: 79.3%). The computational times were also reported for SNN LeNet-5 and ResNet-19 models on varying datasets.

Conclusion: PRIVSPIKE successfully enables privacy-preserving SNN inference with energy efficiency, substantial accuracy, and cryptographic security, outperforming prior solutions.

Abstract: Deep learning has become a cornerstone of modern machine learning. It relies
heavily on vast datasets and significant computational resources for high
performance. This data often contains sensitive information, making privacy a
major concern in deep learning. Spiking Neural Networks (SNNs) have emerged as
an energy-efficient alternative to conventional deep learning approaches.
Nevertheless, SNNs still depend on large volumes of data, inheriting all the
privacy challenges of deep learning. Homomorphic encryption addresses this
challenge by allowing computations to be performed on encrypted data, ensuring
data confidentiality throughout the entire processing pipeline. In this paper,
we introduce PRIVSPIKE, a privacy-preserving inference framework for SNNs using
the CKKS homomorphic encryption scheme. PRIVSPIKE supports arbitrary depth SNNs
and introduces two key algorithms for evaluating the Leaky Integrate-and-Fire
activation function: (1) a polynomial approximation algorithm designed for
high-performance SNN inference, and (2) a novel scheme-switching algorithm that
optimizes precision at a higher computational cost. We evaluate PRIVSPIKE on
MNIST, CIFAR-10, Neuromorphic MNIST, and CIFAR-10 DVS using models from LeNet-5
and ResNet-19 architectures, achieving encrypted inference accuracies of
98.10%, 79.3%, 98.1%, and 66.0%, respectively. On a consumer-grade CPU, SNN
LeNet-5 models achieved inference times of 28 seconds on MNIST and 212 seconds
on Neuromorphic MNIST. For SNN ResNet-19 models, inference took 784 seconds on
CIFAR-10 and 1846 seconds on CIFAR-10 DVS. These results establish PRIVSPIKE as
a viable and efficient solution for secure SNN inference, bridging the gap
between energy-efficient deep neural networks and strong cryptographic privacy
guarantees while outperforming prior encrypted SNN solutions.

</details>


### [830] [SVDefense: Effective Defense against Gradient Inversion Attacks via Singular Value Decomposition](https://arxiv.org/abs/2510.03319)
*Chenxiang Luo,David K. Y. Yau,Qun Song*

Main category: cs.CR

TL;DR: The paper proposes SVDefense, a defense mechanism against gradient inversion attacks in Federated Learning, using innovative techniques like truncated SVD to protect data privacy while maintaining model accuracy.


<details>
  <summary>Details</summary>
Motivation: Federated Learning faces vulnerabilities from gradient inversion attacks, threatening private data reconstruction. Current defenses either lack efficiency or fail to balance privacy protection and model utility.

Method: SVDefense employs truncated Singular Value Decomposition (SVD) with innovations like self-adaptive energy thresholds, channel-wise weighted approximations, and layer-wise aggregation for privacy and utility enhancement.

Result: Experimental evaluations show SVDefense provides strong privacy protection with minimal impact on model accuracy across tasks like image classification, activity recognition, and keyword spotting.

Conclusion: SVDefense effectively safeguards Federated Learning against gradient inversion attacks while ensuring practical deployment on resource-constrained platforms, marking a significant advancement in FL security.

Abstract: Federated learning (FL) enables collaborative model training without sharing
raw data but is vulnerable to gradient inversion attacks (GIAs), where
adversaries reconstruct private data from shared gradients. Existing defenses
either incur impractical computational overhead for embedded platforms or fail
to achieve privacy protection and good model utility at the same time.
Moreover, many defenses can be easily bypassed by adaptive adversaries who have
obtained the defense details. To address these limitations, we propose
SVDefense, a novel defense framework against GIAs that leverages the truncated
Singular Value Decomposition (SVD) to obfuscate gradient updates. SVDefense
introduces three key innovations, a Self-Adaptive Energy Threshold that adapts
to client vulnerability, a Channel-Wise Weighted Approximation that selectively
preserves essential gradient information for effective model training while
enhancing privacy protection, and a Layer-Wise Weighted Aggregation for
effective model aggregation under class imbalance. Our extensive evaluation
shows that SVDefense outperforms existing defenses across multiple
applications, including image classification, human activity recognition, and
keyword spotting, by offering robust privacy protection with minimal impact on
model accuracy. Furthermore, SVDefense is practical for deployment on various
resource-constrained embedded platforms. We will make our code publicly
available upon paper acceptance.

</details>


### [831] [Attack logics, not outputs: Towards efficient robustification of deep neural networks by falsifying concept-based properties](https://arxiv.org/abs/2510.03320)
*Raik Dankworth,Gesina Schwalbe*

Main category: cs.CR

TL;DR: The paper proposes a novel method to improve neural network robustness by attacking concept-based properties rather than just output classes, using explainable AI techniques.


<details>
  <summary>Details</summary>
Motivation: Traditional robustness testing focuses solely on class-level output changes, which might not address broader issues like logically consistent behavior.

Method: The authors propose a framework using explainable AI that enables the testing of concept-based properties (e.g., human-interpretable rules) on already trained neural networks.

Result: They theoretically argue that attacking concept-based properties reduces the search space and offers better alignment with intuitive robustness goals.

Conclusion: This work suggests that concept-based attacks may improve both robustness and logical compliance, though the work is still in progress.

Abstract: Deep neural networks (NNs) for computer vision are vulnerable to adversarial
attacks, i.e., miniscule malicious changes to inputs may induce unintuitive
outputs. One key approach to verify and mitigate such robustness issues is to
falsify expected output behavior. This allows, e.g., to locally proof security,
or to (re)train NNs on obtained adversarial input examples. Due to the
black-box nature of NNs, current attacks only falsify a class of the final
output, such as flipping from $\texttt{stop_sign}$ to $\neg\texttt{stop_sign}$.
In this short position paper we generalize this to search for generally
illogical behavior, as considered in NN verification: falsify constraints
(concept-based properties) involving further human-interpretable concepts, like
$\texttt{red}\wedge\texttt{octogonal}\rightarrow\texttt{stop_sign}$. For this,
an easy implementation of concept-based properties on already trained NNs is
proposed using techniques from explainable artificial intelligence. Further, we
sketch the theoretical proof that attacks on concept-based properties are
expected to have a reduced search space compared to simple class falsification,
whilst arguably be more aligned with intuitive robustness targets. As an
outlook to this work in progress we hypothesize that this approach has
potential to efficiently and simultaneously improve logical compliance and
robustness.

</details>


### [832] [AgentTypo: Adaptive Typographic Prompt Injection Attacks against Black-box Multimodal Agents](https://arxiv.org/abs/2510.04257)
*Yanjie Li,Yiming Cao,Dong Wang,Bin Xiao*

Main category: cs.CR

TL;DR: AgentTypo is a new framework for exploiting vulnerabilities in large vision-language models (LVLMs) via adaptive typographic prompt injection in images, significantly outperforming existing image-based attacks.


<details>
  <summary>Details</summary>
Motivation: To address the vulnerabilities of multimodal agents (built on large vision-language models) to adversarial attacks, particularly prompt injection through visual inputs.

Method: The study introduces AgentTypo, which uses an automatic typographic prompt injection algorithm to create stealthy and optimized text in image-based attacks through black-box optimization, guided by a Tree-structured Parzen Estimator and includes a multi-LLM system for progressive learning of attack strategies.

Result: AgentTypo outperformed state-of-the-art attacks on multimodal agents, achieving higher attack success rates (e.g., 0.45 success on GPT-4o) and demonstrating its effectiveness across different agents and settings.

Conclusion: AgentTypo represents a significant advancement in exploiting LVLM vulnerabilities, emphasizing the need for urgent and robust defensive strategies to protect multimodal agents.

Abstract: Multimodal agents built on large vision-language models (LVLMs) are
increasingly deployed in open-world settings but remain highly vulnerable to
prompt injection, especially through visual inputs. We introduce AgentTypo, a
black-box red-teaming framework that mounts adaptive typographic prompt
injection by embedding optimized text into webpage images. Our automatic
typographic prompt injection (ATPI) algorithm maximizes prompt reconstruction
by substituting captioners while minimizing human detectability via a stealth
loss, with a Tree-structured Parzen Estimator guiding black-box optimization
over text placement, size, and color. To further enhance attack strength, we
develop AgentTypo-pro, a multi-LLM system that iteratively refines injection
prompts using evaluation feedback and retrieves successful past examples for
continual learning. Effective prompts are abstracted into generalizable
strategies and stored in a strategy repository, enabling progressive knowledge
accumulation and reuse in future attacks. Experiments on the VWA-Adv benchmark
across Classifieds, Shopping, and Reddit scenarios show that AgentTypo
significantly outperforms the latest image-based attacks such as AgentAttack.
On GPT-4o agents, our image-only attack raises the success rate from 0.23 to
0.45, with consistent results across GPT-4V, GPT-4o-mini, Gemini 1.5 Pro, and
Claude 3 Opus. In image+text settings, AgentTypo achieves 0.68 ASR, also
outperforming the latest baselines. Our findings reveal that AgentTypo poses a
practical and potent threat to multimodal agents and highlight the urgent need
for effective defense.

</details>


### [833] [Pilot Contamination Attacks Detection with Machine Learning for Multi-User Massive MIMO](https://arxiv.org/abs/2510.03831)
*Pedro Ivo da Cruz,Dimitri Silva,Tito Spadini,Ricardo Suyama,Murilo Bellezoni Loiola*

Main category: cs.CR

TL;DR: The paper presents a Decision Tree (DT) algorithm for detecting pilot contamination attacks (PCA) in MMIMO systems, outperforming classical techniques like likelihood ratio testing (LRT) under various scenarios.


<details>
  <summary>Details</summary>
Motivation: MMIMO systems, pivotal for 5G and 6G wireless communications, are vulnerable to pilot contamination attacks that compromise channel estimation accuracy. Effective detection methods are required to mitigate these security risks.

Method: The authors propose using a Decision Tree algorithm to detect PCA at the base station. Training data generation for the DT classifier is detailed, along with optimization for depth selection. Comparisons with likelihood ratio testing (LRT) are conducted under simulated scenarios.

Result: Simulation results demonstrate that a DT with only one level of depth outperforms LRT, especially in noisy environments or when the malicious user's transmission power is low, where LRT struggles.

Conclusion: Decision Trees provide robust PCA detection without needing prior knowledge of system parameters like noise power, making them a practical and effective alternative to traditional methods like LRT.

Abstract: Massive multiple-input multiple-output (MMIMO) is essential to modern
wireless communication systems, like 5G and 6G, but it is vulnerable to active
eavesdropping attacks. One type of such attack is the pilot contamination
attack (PCA), where a malicious user copies pilot signals from an authentic
user during uplink, intentionally interfering with the base station's (BS)
channel estimation accuracy. In this work, we propose to use a Decision Tree
(DT) algorithm for PCA detection at the BS in a multi-user system. We present a
methodology to generate training data for the DT classifier and select the best
DT according to their depth. Then, we simulate different scenarios that could
be encountered in practice and compare the DT to a classical technique based on
likelihood ratio testing (LRT) submitted to the same scenarios. The results
revealed that a DT with only one level of depth is sufficient to outperform the
LRT. The DT shows a good performance regarding the probability of detection in
noisy scenarios and when the malicious user transmits with low power, in which
case the LRT fails to detect the PCA. We also show that the reason for the good
performance of the DT is its ability to compute a threshold that separates PCA
data from non-PCA data better than the LRT's threshold. Moreover, the DT does
not necessitate prior knowledge of noise power or assumptions regarding the
signal power of malicious users, prerequisites typically essential for LRT and
other hypothesis testing methodologies.

</details>


### [834] [Unified Threat Detection and Mitigation Framework (UTDMF): Combating Prompt Injection, Deception, and Bias in Enterprise-Scale Transformers](https://arxiv.org/abs/2510.04528)
*Santhosh KumarRavindran*

Main category: cs.CR

TL;DR: The paper proposes the Unified Threat Detection and Mitigation Framework (UTDMF) to enhance security and fairness in large language models (LLMs) for enterprise use by addressing vulnerabilities like prompt injection attacks, deception, and bias.


<details>
  <summary>Details</summary>
Motivation: To counteract security threats, deceptive behaviors, and biases in large language models used in enterprise systems, ensuring trust and fairness.

Method: The authors developed UTDMF, a scalable and real-time pipeline tested on enterprise-grade models. They conducted 700+ experiments and generalized the patching algorithm for multi-threat detection.

Result: UTDMF achieved 92% accuracy in detecting prompt injection attacks, reduced deceptive outputs by 65%, and improved fairness metrics by 78%.

Conclusion: UTDMF effectively mitigates vulnerabilities in enterprise LLMs by providing a comprehensive and scalable solution for improving security, accuracy, and fairness while offering deployable APIs for integration.

Abstract: The rapid adoption of large language models (LLMs) in enterprise systems
exposes vulnerabilities to prompt injection attacks, strategic deception, and
biased outputs, threatening security, trust, and fairness. Extending our
adversarial activation patching framework (arXiv:2507.09406), which induced
deception in toy networks at a 23.9% rate, we introduce the Unified Threat
Detection and Mitigation Framework (UTDMF), a scalable, real-time pipeline for
enterprise-grade models like Llama-3.1 (405B), GPT-4o, and Claude-3.5. Through
700+ experiments per model, UTDMF achieves: (1) 92% detection accuracy for
prompt injection (e.g., jailbreaking); (2) 65% reduction in deceptive outputs
via enhanced patching; and (3) 78% improvement in fairness metrics (e.g.,
demographic bias). Novel contributions include a generalized patching algorithm
for multi-threat detection, three groundbreaking hypotheses on threat
interactions (e.g., threat chaining in enterprise workflows), and a
deployment-ready toolkit with APIs for enterprise integration.

</details>


### [835] [ObCLIP: Oblivious CLoud-Device Hybrid Image Generation with Privacy Preservation](https://arxiv.org/abs/2510.04153)
*Haoqi Wu,Wei Dai,Ming Xu,Li Wang,Qiang Yan*

Main category: cs.CR

TL;DR: ObCLIP is a system designed for secure hybrid image generation, ensuring private user prompts during model processing by leveraging semantic transformations and client-cloud collaboration.


<details>
  <summary>Details</summary>
Motivation: The paper addresses concerns about sensitive user information leakage during the use of cloud-based image generation systems, and the lack of existing solutions with both privacy guarantees and cost-effective operations.

Method: ObCLIP uses a hybrid approach by transforming sensitive prompts into semantically similar ones for anonymized processing on the cloud. It splits processing steps between cloud models (handling early stages) and client devices (handling final denoising), enabling secure and efficient distribution of computation.

Result: Experiments across various datasets show that ObCLIP achieves strong privacy protections, maintains utility comparable to fully cloud-based models, and slightly increases server costs.

Conclusion: ObCLIP is effective in offering an efficient balance of utility, cost, and privacy, making cloud-device hybrid image generation more secure and practical.

Abstract: Diffusion Models have gained significant popularity due to their remarkable
capabilities in image generation, albeit at the cost of intensive computation
requirement. Meanwhile, despite their widespread deployment in inference
services such as Midjourney, concerns about the potential leakage of sensitive
information in uploaded user prompts have arisen. Existing solutions either
lack rigorous privacy guarantees or fail to strike an effective balance between
utility and efficiency. To bridge this gap, we propose ObCLIP, a plug-and-play
safeguard that enables oblivious cloud-device hybrid generation. By oblivious,
each input prompt is transformed into a set of semantically similar candidate
prompts that differ only in sensitive attributes (e.g., gender, ethnicity). The
cloud server processes all candidate prompts without knowing which one is the
real one, thus preventing any prompt leakage. To mitigate server cost, only a
small portion of denoising steps is performed upon the large cloud model. The
intermediate latents are then sent back to the client, which selects the
targeted latent and completes the remaining denoising using a small device
model. Additionally, we analyze and incorporate several cache-based
accelerations that leverage temporal and batch redundancy, effectively reducing
computation cost with minimal utility degradation. Extensive experiments across
multiple datasets demonstrate that ObCLIP provides rigorous privacy and
comparable utility to cloud models with slightly increased server cost.

</details>


### [836] [RL Is a Hammer and LLMs Are Nails: A Simple Reinforcement Learning Recipe for Strong Prompt Injection](https://arxiv.org/abs/2510.04885)
*Yuxin Wen,Arman Zharmagambetov,Ivan Evtimov,Narine Kokhlikyan,Tom Goldstein,Kamalika Chaudhuri,Chuan Guo*

Main category: cs.CR

TL;DR: The paper introduces RL-Hammer, a reinforcement learning technique to train models for executing powerful prompt injection attacks against large language models (LLMs) and evaluates defense robustness.


<details>
  <summary>Details</summary>
Motivation: Prompt injection attacks threaten the reliability and safety of large language models (LLMs), and existing defenses have limited robustness evaluation methods.

Method: The authors propose RL-Hammer—a training approach using reinforcement learning from scratch combined with practical techniques to craft universal, strong attacks.

Result: RL-Hammer achieves high attack success rates, such as 98% against GPT-4o and 72% against GPT-5 equipped with Instruction Hierarchy defenses.

Conclusion: RL-Hammer highlights the need for stronger and more principled defenses, stressing its ability to evade detector systems while advancing automated red-teaming efforts.

Abstract: Prompt injection poses a serious threat to the reliability and safety of LLM
agents. Recent defenses against prompt injection, such as Instruction Hierarchy
and SecAlign, have shown notable robustness against static attacks. However, to
more thoroughly evaluate the robustness of these defenses, it is arguably
necessary to employ strong attacks such as automated red-teaming. To this end,
we introduce RL-Hammer, a simple recipe for training attacker models that
automatically learn to perform strong prompt injections and jailbreaks via
reinforcement learning. RL-Hammer requires no warm-up data and can be trained
entirely from scratch. To achieve high ASRs against industrial-level models
with defenses, we propose a set of practical techniques that enable highly
effective, universal attacks. Using this pipeline, RL-Hammer reaches a 98% ASR
against GPT-4o and a $72\%$ ASR against GPT-5 with the Instruction Hierarchy
defense. We further discuss the challenge of achieving high diversity in
attacks, highlighting how attacker models tend to reward-hack diversity
objectives. Finally, we show that RL-Hammer can evade multiple prompt injection
detectors. We hope our work advances automatic red-teaming and motivates the
development of stronger, more principled defenses. Code is available at
https://github.com/facebookresearch/rl-injector.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [837] [Adapting Diarization-Conditioned Whisper for End-to-End Multi-Talker Speech Recognition](https://arxiv.org/abs/2510.03723)
*Martin Kocour,Martin Karafiat,Alexander Polok,Dominik Klement,Lukáš Burget,Jan Černocký*

Main category: eess.AS

TL;DR: The paper introduces a model that combines target-speaker modeling with serialized output training for multi-talker speech recognition, outperforming existing methods like SOT-based approaches and DiCoW.


<details>
  <summary>Details</summary>
Motivation: Multi-talker speech recognition poses challenges, especially when speakers overlap. Current methods often require separate decoding for each speaker, lacking efficiency and coherence.

Method: The paper employs a Diarization-Conditioned Whisper (DiCoW) encoder to extract speaker embeddings and combines them for joint decoding in a shared decoder. This provides context-aware transcriptions with speaker tags and timestamps.

Result: The proposed approach demonstrated improved performance over existing SOT-based methods and surpassed DiCoW in multi-talker setups like LibriMix.

Conclusion: Joint decoding using the proposed model represents an advancement in multi-talker speech recognition, offering better performance and addressing the inefficiencies of separate decoding systems.

Abstract: We propose a speaker-attributed (SA) Whisper-based model for multi-talker
speech recognition that combines target-speaker modeling with serialized output
training (SOT). Our approach leverages a Diarization-Conditioned Whisper
(DiCoW) encoder to extract target-speaker embeddings, which are concatenated
into a single representation and passed to a shared decoder. This enables the
model to transcribe overlapping speech as a serialized output stream with
speaker tags and timestamps. In contrast to target-speaker ASR systems such as
DiCoW, which decode each speaker separately, our approach performs joint
decoding, allowing the decoder to condition on the context of all speakers
simultaneously. Experiments show that the model outperforms existing SOT-based
approaches and surpasses DiCoW on multi-talker mixtures (e.g., LibriMix).

</details>


### [838] [MoME: Mixture of Matryoshka Experts for Audio-Visual Speech Recognition](https://arxiv.org/abs/2510.04136)
*Umberto Cappellazzo,Minsu Kim,Pingchuan Ma,Honglie Chen,Xubo Liu,Stavros Petridis,Maja Pantic*

Main category: eess.AS

TL;DR: The paper introduces MoME (Mixture of Matryoshka Experts), a framework combining Matryoshka representation learning with sparse Mixture-of-Experts, to enhance audio-visual speech recognition (AVSR) with scalable, efficient, and interpretable methods.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenges of high computational demands and inflexibility in token granularity of large language models for AVSR, as well as the limitations in current MRL-based methods in terms of cross-scale generalization and interpretability.

Method: The proposed MoME framework integrates sparse Mixture-of-Experts (MoE) into MRL-based models, enabling dynamic capacity allocation across scales and modalities via top-k routed and shared experts. A shared router ensures consistent expert activation across token granularities.

Result: On LRS2 and LRS3 datasets, MoME achieved state-of-the-art results in AVSR, automatic speech recognition (ASR), and video-only speech recognition (VSR) tasks while using fewer parameters and maintaining robustness under noise.

Conclusion: MoME effectively unifies the adaptability of MRL with the efficiency of sparse MoE, providing a flexible, high-performance solution for resource-aware and interpretable speech recognition in constrained environments.

Abstract: Large language models (LLMs) have recently shown strong potential in
audio-visual speech recognition (AVSR), but their high computational demands
and sensitivity to token granularity limit their practicality in
resource-constrained settings. Token compression methods can reduce inference
cost, but they require fixing a compression rate in advance and produce a
single fixed-length output, offering no flexibility to balance information
density and efficiency at inference time. Matryoshka representation learning
(MRL) addresses this by enabling a single model to operate across multiple
token granularities, allowing compression rates to be adjusted dynamically.
However, current MRL-based methods treat each scale independently during
training, limiting cross-scale generalization, robustness at high compression,
and interpretability. To overcome these limitations, we propose MoME (Mixture
of Matryoshka Experts), a novel framework that integrates sparse
Mixture-of-Experts (MoE) into MRL-based LLMs for AVSR. MoME augments a frozen
LLM with top-k routed and shared experts, allowing dynamic capacity allocation
across scales and modalities. A shared router promotes consistent expert
activation across granularities, enabling compressed sequences to benefit from
representations learned at lower compression. Experiments on LRS2 and LRS3
demonstrate that MoME achieves state-of-the-art performance across AVSR, ASR,
and VSR tasks, while requiring significantly fewer parameters and maintaining
robustness under noise. MoME unifies the adaptability of MRL with the
efficiency of MoE, offering a scalable and interpretable solution for
resource-aware speech recognition.

</details>


### [839] [Drax: Speech Recognition with Discrete Flow Matching](https://arxiv.org/abs/2510.04162)
*Aviv Navon,Aviv Shamsian,Neta Glazer,Yael Segal-Feldman,Gill Hetz,Joseph Keshet,Ethan Fetaya*

Main category: eess.AS

TL;DR: This paper proposes Drax, a discrete flow matching framework for automatic speech recognition (ASR), which offers efficient parallel decoding and improved accuracy-efficiency trade-offs.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the gap in leveraging diffusion and flow-based non-autoregressive (NAR) models in ASR, aiming to explore their potential and improve decoding efficiency in this domain.

Method: The method involves constructing audio-conditioned probability paths to guide the model during training, aligning with likely inference errors by resembling intermediate trajectories, and linking generalization gaps to training-inference occupancy divergences via cumulative velocity errors.

Result: Empirical results show that Drax achieves recognition accuracy comparable to state-of-the-art ASR models while emphasizing better accuracy-efficiency trade-offs.

Conclusion: The study highlights discrete flow matching as an effective and promising approach for advancing NAR ASR models in terms of performance and efficiency.

Abstract: Diffusion and flow-based non-autoregressive (NAR) models have shown strong
promise in large language modeling, however, their potential for automatic
speech recognition (ASR) remains largely unexplored. We propose Drax, a
discrete flow matching framework for ASR that enables efficient parallel
decoding. To better align training with inference, we construct an
audio-conditioned probability path that guides the model through trajectories
resembling likely intermediate inference errors, rather than direct random
noise to target transitions. Our theoretical analysis links the generalization
gap to divergences between training and inference occupancies, controlled by
cumulative velocity errors, thereby motivating our design choice. Empirical
evaluation demonstrates that our approach attains recognition accuracy on par
with state-of-the-art speech models while offering improved accuracy-efficiency
trade-offs, highlighting discrete flow matching as a promising direction for
advancing NAR ASR.

</details>


### [840] [AURA Score: A Metric For Holistic Audio Question Answering Evaluation](https://arxiv.org/abs/2510.04934)
*Satvik Dixit,Soham Deshmukh,Bhiksha Raj*

Main category: eess.AS

TL;DR: This paper addresses limitations in existing metrics for evaluating Audio Question Answering (AQA) by introducing a benchmark called AQEval and a new evaluation metric named AURA score, which significantly outperforms existing methods in correlation with human judgment.


<details>
  <summary>Details</summary>
Motivation: Existing metrics for AQA evaluation struggle with assessing open-ended responses due to reliance on surface similarity, neglecting question context, reasoning, and partial correctness.

Method: The authors created AQEval, a benchmark with 10,000 model responses annotated for correctness and relevance, and proposed the AURA score as a new metric to evaluate open-ended model responses.

Result: The proposed AURA metric achieved state-of-the-art correlation with human judgments on the AQEval benchmark, outperforming existing metrics.

Conclusion: The study highlights the limitations of measuring AQA with current metrics and emphasizes the importance of adopting holistic evaluation techniques for better model assessments through AQEval and AURA.

Abstract: Audio Question Answering (AQA) is a key task for evaluating Audio-Language
Models (ALMs), yet assessing open-ended responses remains challenging. Existing
metrics used for AQA such as BLEU, METEOR and BERTScore, mostly adapted from
NLP and audio captioning, rely on surface similarity and fail to account for
question context, reasoning, and partial correctness. To address the gap in
literature, we make three contributions in this work. First, we introduce
AQEval to enable systematic benchmarking of AQA metrics. It is the first
benchmark of its kind, consisting of 10k model responses annotated by multiple
humans for their correctness and relevance. Second, we conduct a comprehensive
analysis of existing AQA metrics on AQEval, highlighting weak correlation with
human judgment, especially for longer answers. Third, we propose a new metric -
AURA score, to better evaluate open-ended model responses. On AQEval, AURA
achieves state-of-the-art correlation with human ratings, significantly
outperforming all baselines. Through this work, we aim to highlight the
limitations of current AQA evaluation methods and motivate better metrics. We
release both the AQEval benchmark and the AURA metric to support future
research in holistic AQA evaluation.

</details>


### [841] [MuFFIN: Multifaceted Pronunciation Feedback Model with Interactive Hierarchical Neural Modeling](https://arxiv.org/abs/2510.04956)
*Bi-Cheng Yan,Ming-Kang Tsai,Berlin Chen*

Main category: eess.AS

TL;DR: This paper introduces MuFFIN, a hierarchical neural model to jointly address mispronunciation detection and automatic pronunciation assessment for second-language learners.


<details>
  <summary>Details</summary>
Motivation: Existing CAPT methods treat mispronunciation detection and pronunciation assessment as separate tasks, despite their complementary nature.

Method: The paper proposes MuFFIN with phoneme-contrastive ordinal regularization and a training objective to address data imbalance and nuanced phoneme distinctions.

Result: MuFFIN achieves state-of-the-art performance on both tasks in experiments conducted on the Speechocean762 dataset.

Conclusion: Integrating MDD and APA tasks through MuFFIN improves overall pronunciation feedback quality and demonstrates significant efficacy over existing methods.

Abstract: Computer-assisted pronunciation training (CAPT) manages to facilitate
second-language (L2) learners to practice pronunciation skills by offering
timely and instructive feedback. To examine pronunciation proficiency from
multiple facets, existing methods for CAPT broadly fall into two categories:
mispronunciation detection and diagnosis (MDD) as well as automatic
pronunciation assessment (APA). The former aims to pinpoint phonetic
pronunciation errors and provide diagnostic feedback, while the latter seeks
instead to quantify pronunciation proficiency pertaining to various aspects.
Despite the natural complementarity between MDD and APA, researchers and
practitioners, however, often treat them as independent tasks with disparate
modeling paradigms. In light of this, we in this paper first introduce MuFFIN,
a Multi-Faceted pronunciation Feedback model with an Interactive hierarchical
Neural architecture, to jointly address the tasks of MDD and APA. To better
capture the nuanced distinctions between phonemes in the feature space, a novel
phoneme-contrastive ordinal regularization mechanism is then put forward to
optimize the proposed model to generate more phoneme-discriminative features
while factoring in the ordinality of the aspect scores. In addition, to address
the intricate data imbalance problem in MDD, we design a simple yet effective
training objective, which is specifically tailored to perturb the outputs of a
phoneme classifier with the phoneme-specific variations, so as to better render
the distribution of predicted phonemes meanwhile considering their
mispronunciation characteristics. A series of experiments conducted on the
Speechocean762 benchmark dataset demonstrates the efficacy of our method in
relation to several cutting-edge baselines, showing state-of-the-art
performance on both the APA and MDD tasks.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [842] [Application of a Virtual Imaging Framework for Investigating a Deep Learning-Based Reconstruction Method for 3D Quantitative Photoacoustic Computed Tomography](https://arxiv.org/abs/2510.03431)
*Refik Mert Cam,Seonyeong Park,Umberto Villa,Mark A. Anastasio*

Main category: physics.med-ph

TL;DR: The paper evaluates a machine-learning-based reconstruction method for quantitative photoacoustic computed tomography (qPACT) using realistic virtual imaging environments, focusing on breast imaging.


<details>
  <summary>Details</summary>
Motivation: Developing accurate qPACT reconstruction methods is difficult due to computational, modeling, and experimental challenges, requiring validation through realistic virtual imaging studies.

Method: A 3D learning-based qPACT reconstruction method is assessed using realistic, stochastically generated numerical phantoms that simulate anatomy and physiology.

Result: The study demonstrates the effectiveness of the reconstruction method against key variables like subject variability, noise, and acoustic aberrations.

Conclusion: The research offers valuable insights into both the strengths and limitations of learning-based qPACT methods, paving the way for robust physiological imaging solutions.

Abstract: Quantitative photoacoustic computed tomography (qPACT) is a promising imaging
modality for estimating physiological parameters such as blood oxygen
saturation. However, developing robust qPACT reconstruction methods remains
challenging due to computational demands, modeling difficulties, and
experimental uncertainties. Learning-based methods have been proposed to
address these issues but remain largely unvalidated. Virtual imaging (VI)
studies are essential for validating such methods early in development, before
proceeding to less-controlled phantom or in vivo studies. Effective VI studies
must employ ensembles of stochastically generated numerical phantoms that
accurately reflect relevant anatomy and physiology. Yet, most prior VI studies
for qPACT relied on overly simplified phantoms. In this work, a realistic VI
testbed is employed for the first time to assess a representative 3D
learning-based qPACT reconstruction method for breast imaging. The method is
evaluated across subject variability and physical factors such as measurement
noise and acoustic aberrations, offering insights into its strengths and
limitations.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [843] [Long-Term Mapping of the Douro River Plume with Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2510.03534)
*Nicolò Dal Fabbro,Milad Mesbahi,Renato Mendes,João Borges de Sousa,George J. Pappas*

Main category: cs.MA

TL;DR: The paper proposes an energy-efficient multi-agent reinforcement learning system using autonomous underwater vehicles (AUVs) to map river plumes over multiple days.


<details>
  <summary>Details</summary>
Motivation: To improve the efficiency and endurance of long-term monitoring of dynamic river plume environments.

Method: The authors developed a multi-agent reinforcement learning approach using spatiotemporal Gaussian process regression integrated with a multi-head Q-network controller.

Result: Simulations showed better performance in mean squared error and operational endurance compared to benchmarks, with an increase in agents providing scalability improvements.

Conclusion: The approach proves scalable, accurate, and generalizable across seasonal regimes, highlighting its potential for future long-term environmental monitoring applications.

Abstract: We study the problem of long-term (multiple days) mapping of a river plume
using multiple autonomous underwater vehicles (AUVs), focusing on the Douro
river representative use-case. We propose an energy - and communication -
efficient multi-agent reinforcement learning approach in which a central
coordinator intermittently communicates with the AUVs, collecting measurements
and issuing commands. Our approach integrates spatiotemporal Gaussian process
regression (GPR) with a multi-head Q-network controller that regulates
direction and speed for each AUV. Simulations using the Delft3D ocean model
demonstrate that our method consistently outperforms both single- and
multi-agent benchmarks, with scaling the number of agents both improving mean
squared error (MSE) and operational endurance. In some instances, our algorithm
demonstrates that doubling the number of AUVs can more than double endurance
while maintaining or improving accuracy, underscoring the benefits of
multi-agent coordination. Our learned policies generalize across unseen
seasonal regimes over different months and years, demonstrating promise for
future developments of data-driven long-term monitoring of dynamic plume
environments.

</details>


### [844] [LegalSim: Multi-Agent Simulation of Legal Systems for Discovering Procedural Exploits](https://arxiv.org/abs/2510.03405)
*Sanket Badhe*

Main category: cs.MA

TL;DR: LegalSim, a modular multi-agent AI-based simulation, explores legal adversarial proceedings to study how AI can exploit procedural weaknesses in codified rules.


<details>
  <summary>Details</summary>
Motivation: To understand and address how AI systems can exploit procedural vulnerabilities in legal proceedings and aid in improving rule systems.

Method: A simulation where plaintiff and defendant agents strategically interact in a rules-governed framework, evaluated through metrics like effective win rate and exploit scores across various procedural contexts.

Result: PPO agents perform best, contextual bandit policies are consistently competitive, LLM agents perform moderately, and heuristics are weakest; the system exposes various exploit chains.

Conclusion: The findings highlight the need for red-teaming legal rule sets to improve systemic robustness beyond model-level AI testing.

Abstract: We present LegalSim, a modular multi-agent simulation of adversarial legal
proceedings that explores how AI systems can exploit procedural weaknesses in
codified rules. Plaintiff and defendant agents choose from a constrained action
space (for example, discovery requests, motions, meet-and-confer, sanctions)
governed by a JSON rules engine, while a stochastic judge model with calibrated
grant rates, cost allocations, and sanction tendencies resolves outcomes. We
compare four policies: PPO, a contextual bandit with an LLM, a direct LLM
policy, and a hand-crafted heuristic; Instead of optimizing binary case
outcomes, agents are trained and evaluated using effective win rate and a
composite exploit score that combines opponent-cost inflation, calendar
pressure, settlement pressure at low merit, and a rule-compliance margin.
Across configurable regimes (e.g., bankruptcy stays, inter partes review, tax
procedures) and heterogeneous judges, we observe emergent ``exploit chains'',
such as cost-inflating discovery sequences and calendar-pressure tactics that
remain procedurally valid yet systemically harmful. Evaluation via cross-play
and Bradley-Terry ratings shows, PPO wins more often, the bandit is the most
consistently competitive across opponents, the LLM trails them, and the
heuristic is weakest. The results are stable in judge settings, and the
simulation reveals emergent exploit chains, motivating red-teaming of legal
rule systems in addition to model-level testing.

</details>


### [845] [Cooperative Flexibility Exchange: Fair and Comfort-Aware Decentralized Resource Allocation](https://arxiv.org/abs/2510.04192)
*Rabiya Khalid,Evangelos Pournaras*

Main category: cs.MA

TL;DR: The paper introduces a decentralized multi-agent demand-side energy management system that enhances user comfort and fairness without compromising system efficiency.


<details>
  <summary>Details</summary>
Motivation: Traditional energy management systems often sacrifice user comfort to optimize system efficiency, creating a need for approaches that balance both.

Method: The paper proposes a decentralized coordination system using a slot exchange mechanism, where agents optimize their energy schedules through collaboration while ensuring fairness.

Result: Performance evaluation using a real-world dataset shows improved user comfort and fairness with no additional inefficiency cost to the system.

Conclusion: The proposed system is scalable, practical, and balances user comfort, fairness, and system efficiency, making it suited for modern smart grids.

Abstract: The growing electricity demand and increased use of smart appliances are
placing new pressures on power grids, making efficient energy management more
important than ever. The existing energy management systems often prioritize
system efficiency (balanced energy demand and supply) at the expense of user
comfort. This paper addresses this gap by proposing a novel decentralized
multi-agent coordination-based demand-side management system. The proposed
system enables individual agents to coordinate for demand-side energy
optimization while improving the user comfort and maintaining the system
efficiency. A key innovation of this work is the introduction of a slot
exchange mechanism, where agents first receive optimized appliance-level energy
consumption schedules and then coordinate with each other to adjust these
schedules through slot exchanges. This approach improves user comfort even when
agents show non-altruistic behaviour, and it scales well with large
populations. The system also promotes fairness by balancing satisfaction levels
across users. For performance evaluation, a real-world dataset is used, and the
results demonstrate that the proposed slot exchange mechanism increases user
comfort and fairness without raising system inefficiency cost, making it a
practical and scalable solution for future smart grids.

</details>


### [846] [Audit the Whisper: Detecting Steganographic Collusion in Multi-Agent LLMs](https://arxiv.org/abs/2510.04303)
*Om Tailor*

Main category: cs.MA

TL;DR: This paper addresses covert coordination among large language model agents in workflows like market and governance and introduces a robust auditing artifact to detect and prevent such behavior with theoretical guarantees.


<details>
  <summary>Details</summary>
Motivation: The need to ensure trust and social welfare in workflows involving multi-agent LLMs, which can be eroded by covert coordination among agents undetected by current heuristic-driven audits.

Method: The paper introduces 'Audit the Whisper,' which integrates channel-capacity analysis, Kullback--Leibler diagnostics, a benchmark platform (	extsc{ColludeBench}-v0), and a calibrated auditing pipeline that minimizes false positives while detecting covert behavior.

Result: The proposed method attains a 100% true positive rate with zero observed false positives across 600 audited runs under various conditions, demonstrating the effectiveness of the approach.

Conclusion: The auditing framework offers high precision and reproducibility, enabling external auditors to detect behavior and extend the platform with ease, ensuring trust and fairness in multi-agent interactions.

Abstract: Multi-agent deployments of large language models (LLMs) are increasingly
embedded in market, allocation, and governance workflows, yet covert
coordination among agents can silently erode trust and social welfare. Existing
audits are dominated by heuristics that lack theoretical guarantees, struggle
to transfer across tasks, and seldom ship with the infrastructure needed for
independent replication. We introduce \emph{Audit the Whisper}, a
conference-grade research artifact that spans theory, benchmark design,
detection, and reproducibility. Our contributions are: (i) a channel-capacity
analysis showing how interventions such as paraphrase, rate limiting, and role
permutation impose quantifiable capacity penalties -- operationalized via
paired-run Kullback--Leibler diagnostics -- that tighten mutual-information
thresholds with finite-sample guarantees; (ii) \textsc{ColludeBench}-v0,
covering pricing, first-price auctions, and peer review with configurable
covert schemes, deterministic manifests, and reward instrumentation; and (iii)
a calibrated auditing pipeline that fuses cross-run mutual information,
permutation invariance, watermark variance, and fairness-aware acceptance bias,
each tuned to a \(10^{-3}\) false-positive budget. Across 600 audited runs
spanning 12 intervention conditions, the union meta-test attains TPR~$=1$ with
zero observed false alarms, while ablations surface the price-of-auditing
trade-off and highlight fairness-driven colluders invisible to MI alone. We
release regeneration scripts, seed-stamped manifests, and documentation so that
external auditors can reproduce every figure and extend the framework with
minimal effort.

</details>


### [847] [NegotiationGym: Self-Optimizing Agents in a Multi-Agent Social Simulation Environment](https://arxiv.org/abs/2510.04368)
*Shashank Mangla,Chris Hokamp,Jack Boylan,Demian Gholipour Ghalandari,Yuuv Jauhari,Lauren Cassidy,Oisin Duffy*

Main category: cs.MA

TL;DR: NegotiationGym is a tool for simulating multi-agent negotiation and cooperation, allowing customization and enabling agents to optimize strategies through repeated interactions.


<details>
  <summary>Details</summary>
Motivation: To facilitate the study of negotiation and cooperation in multi-agent systems by providing a customizable and simulation-oriented tool.

Method: Developing an API and user interface, where agents optimize strategies via iterative interactions and scenario configuration.

Result: NegotiationGym offers simulation scenarios with user-driven configurations for observing agent behaviors and strategy adaptation.

Conclusion: NegotiationGym simplifies the design and study of multi-agent negotiation scenarios, aiding in understanding social dynamics and cooperative behavior.

Abstract: We design and implement NegotiationGym, an API and user interface for
configuring and running multi-agent social simulations focused upon negotiation
and cooperation. The NegotiationGym codebase offers a user-friendly,
configuration-driven API that enables easy design and customization of
simulation scenarios. Agent-level utility functions encode optimization
criteria for each agent, and agents can self-optimize by conducting multiple
interaction rounds with other agents, observing outcomes, and modifying their
strategies for future rounds.

</details>


### [848] [Trade in Minutes! Rationality-Driven Agentic System for Quantitative Financial Trading](https://arxiv.org/abs/2510.04787)
*Zifan Song,Kaitao Song,Guosheng Hu,Ding Qi,Junyao Gao,Xiaohua Wang,Dongsheng Li,Cairong Zhao*

Main category: cs.MA

TL;DR: This paper introduces TiMi, a rationality-centric multi-agent system for quantitative trading, which decouples strategic planning from minute-level deployment and empirically demonstrates stable performance across volatile markets.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of current financial trading agents that are prone to emotional biases, reliance on peripheral information, and inefficiency due to continuous inference during deployment.

Method: TiMi utilizes large language models (LLMs) for semantic analysis, coding, and mathematical reasoning, implemented within a comprehensive policy-optimization-deployment structure. It introduces a layered programming design, two-tier analysis (macro patterns and micro customization), and closed-loop optimization driven by mathematical reflection.

Result: Extensive evaluation on over 200 trading pairs in stock and cryptocurrency markets shows TiMi’s stable profitability, action efficiency, and effective risk control in volatile market conditions.

Conclusion: TiMi successfully combines mechanical rationality and strategic depth to create an effective trading system, showcasing significant improvements in quantitative trading outcomes by leveraging LLMs and its multi-agent architecture.

Abstract: Recent advancements in large language models (LLMs) and agentic systems have
shown exceptional decision-making capabilities, revealing significant potential
for autonomic finance. Current financial trading agents predominantly simulate
anthropomorphic roles that inadvertently introduce emotional biases and rely on
peripheral information, while being constrained by the necessity for continuous
inference during deployment. In this paper, we pioneer the harmonization of
strategic depth in agents with the mechanical rationality essential for
quantitative trading. Consequently, we present TiMi (Trade in Minutes), a
rationality-driven multi-agent system that architecturally decouples strategy
development from minute-level deployment. TiMi leverages specialized LLM
capabilities of semantic analysis, code programming, and mathematical reasoning
within a comprehensive policy-optimization-deployment chain. Specifically, we
propose a two-tier analytical paradigm from macro patterns to micro
customization, layered programming design for trading bot implementation, and
closed-loop optimization driven by mathematical reflection. Extensive
evaluations across 200+ trading pairs in stock and cryptocurrency markets
empirically validate the efficacy of TiMi in stable profitability, action
efficiency, and risk control under volatile market dynamics.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [849] [Overlapping Schwarz Scheme for Linear-Quadratic Programs in Continuous Time](https://arxiv.org/abs/2510.04478)
*Hongli Zhao,Mihai Anitescu,Sen Na*

Main category: math.OC

TL;DR: This paper proposes an optimize-then-discretize framework for solving linear-quadratic optimal control problems governed by time-inhomogeneous ODEs, combining Pontryagin Minimum Principle and modified Schwarz decomposition.


<details>
  <summary>Details</summary>
Motivation: The study aims to address limitations in solving linear-quadratic optimal control problems using traditional approaches, particularly the inflexibility of numerical methods within discretize-then-optimize techniques.

Method: The method partitions the temporal domain into overlapping intervals using Schwarz decomposition and solves Hamiltonian systems in continuous time, leveraging the Pontryagin Minimum Principle and updates to boundary conditions.

Result: The paper demonstrates convergence by proving that exponential decay of sensitivity (EDS) in discrete-time extends to continuous-time settings. Numerical experiments validate performance.

Conclusion: This optimize-then-discretize approach provides greater flexibility in numerical methods, including adaptive-time integrators, advancing applicability in various scientific domains.

Abstract: We present an optimize-then-discretize framework for solving linear-quadratic
optimal control problems (OCP) governed by time-inhomogeneous ordinary
differential equations (ODEs). Our method employs a modified overlapping
Schwarz decomposition based on the Pontryagin Minimum Principle, partitioning
the temporal domain into overlapping intervals and independently solving
Hamiltonian systems in continuous time. We demonstrate that the convergence is
ensured by appropriately updating the boundary conditions of the individual
Hamiltonian dynamics. The cornerstone of our analysis is to prove that the
exponential decay of sensitivity (EDS) exhibited in discrete-time OCPs carries
over to the continuous-time setting. Unlike the discretize-then-optimize
approach, our method can flexibly incorporate different numerical integration
methods for solving the resulting Hamiltonian two-point boundary-value
subproblems, including adaptive-time integrators. A numerical experiment on a
linear-quadratic OCP illustrates the practicality of our approach in broad
scientific applications.

</details>


### [850] [Optimal Regularization Under Uncertainty: Distributional Robustness and Convexity Constraints](https://arxiv.org/abs/2510.03464)
*Oscar Leong,Eliza O'Reilly,Yong Sheng Soh*

Main category: math.OC

TL;DR: The paper introduces a framework for creating distributionally robust regularizers that remain effective under data distribution shifts, combining theoretical insights with computational solutions.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of designing reliable regularizers for inverse problems and statistical estimation under model uncertainty and structural constraints, such as convexity.

Method: The paper uses convex duality to reformulate distributionally robust optimization problems, making them computationally tractable, and explores ambiguity sets like Wasserstein-1 distance to design robust regularizers.

Result: The proposed framework produces robust regularizers that balance memorization and uniform priors, while also ensuring stability under distributional shifts and promoting desirable properties like lower Lipschitz constants.

Conclusion: The research establishes a strong theoretical and computational foundation for creating structurally constrained and robust regularizers suitable for deployment under uncertainty.

Abstract: Regularization is a central tool for addressing ill-posedness in inverse
problems and statistical estimation, with the choice of a suitable penalty
often determining the reliability and interpretability of downstream solutions.
While recent work has characterized optimal regularizers for well-specified
data distributions, practical deployments are often complicated by
distributional uncertainty and the need to enforce structural constraints such
as convexity. In this paper, we introduce a framework for distributionally
robust optimal regularization, which identifies regularizers that remain
effective under perturbations of the data distribution. Our approach leverages
convex duality to reformulate the underlying distributionally robust
optimization problem, eliminating the inner maximization and yielding
formulations that are amenable to numerical computation. We show how the
resulting robust regularizers interpolate between memorization of the training
distribution and uniform priors, providing insights into their behavior as
robustness parameters vary. For example, we show how certain ambiguity sets,
such as those based on the Wasserstein-1 distance, naturally induce regularity
in the optimal regularizer by promoting regularizers with smaller Lipschitz
constants. We further investigate the setting where regularizers are required
to be convex, formulating a convex program for their computation and
illustrating their stability with respect to distributional shifts. Taken
together, our results provide both theoretical and computational foundations
for designing regularizers that are reliable under model uncertainty and
structurally constrained for robust deployment.

</details>


### [851] [Composite Optimization with Error Feedback: the Dual Averaging Approach](https://arxiv.org/abs/2510.03507)
*Yuan Gao,Anton Rodomanov,Jeremy Rack,Sebastian Stich*

Main category: math.OC

TL;DR: The paper addresses challenges posed by Error Feedback (EF) methods in composite optimization and proposes an improved algorithm combining Dual Averaging with EControl to ensure robust convergence.


<details>
  <summary>Details</summary>
Motivation: Error Feedback methods, while effective for smooth unconstrained optimization, fail for composite optimization settings consisting of smooth loss and non-smooth components. Theoretical understanding in this broader context is unexplored.

Method: The paper introduces a novel algorithm combining Dual Averaging with EControl, a refined EF variant. It also develops an analysis framework for inexact dual averaging tailored to composite settings.

Result: The proposed method demonstrates strong theoretical convergence capabilities for composite optimization, overcoming limitations of standard EF techniques. Experimental results corroborate these claims.

Conclusion: The integration of Dual Averaging with EControl provides a breakthrough in handling composite optimization tasks under error feedback, offering strong theoretical and experimental support for its efficacy.

Abstract: Communication efficiency is a central challenge in distributed machine
learning training, and message compression is a widely used solution. However,
standard Error Feedback (EF) methods (Seide et al., 2014), though effective for
smooth unconstrained optimization with compression (Karimireddy et al., 2019),
fail in the broader and practically important setting of composite
optimization, which captures, e.g., objectives consisting of a smooth loss
combined with a non-smooth regularizer or constraints. The theoretical
foundation and behavior of EF in the context of the general composite setting
remain largely unexplored. In this work, we consider composite optimization
with EF. We point out that the basic EF mechanism and its analysis no longer
stand when a composite part is involved. We argue that this is because of a
fundamental limitation in the method and its analysis technique. We propose a
novel method that combines Dual Averaging with EControl (Gao et al., 2024), a
state-of-the-art variant of the EF mechanism, and achieves for the first time a
strong convergence analysis for composite optimization with error feedback.
Along with our new algorithm, we also provide a new and novel analysis template
for inexact dual averaging method, which might be of independent interest. We
also provide experimental results to complement our theoretical findings.

</details>


### [852] [Agile Tradespace Exploration for Space Rendezvous Mission Design via Transformers](https://arxiv.org/abs/2510.03544)
*Yuji Takubo,Daniele Gammelli,Marco Pavone,Simone D'Amico*

Main category: math.OC

TL;DR: The paper introduces an AI-based framework to streamline mission design and generate near-Pareto optimal trajectories for Earth orbit rendezvous, addressing nonconvex optimization challenges effectively.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the trade-off between control cost and flight time in spacecraft rendezvous missions and improve efficiency while dealing with the challenges of solving nonconvex underlying constraints.

Method: The paper proposes a Transformer-based AI architecture that generates near-Pareto optimal trajectories across flight times in a single parallelized inference step. It also accommodates variable flight times and perturbed orbital dynamics.

Result: The model successfully generalizes across flight times and dynamics with fewer iterations while efficiently approximating the Pareto front. It achieves runtimes comparable to convex relaxation methods.

Conclusion: The framework demonstrates its potential as a practical AI-powered surrogate for nonconvex trajectory design, accelerating mission planning and enabling efficient trade studies in real-world rendezvous scenarios.

Abstract: Spacecraft rendezvous enables on-orbit servicing, debris removal, and crewed
docking, forming the foundation for a scalable space economy. Designing such
missions requires rapid exploration of the tradespace between control cost and
flight time across multiple candidate targets. However, multi-objective
optimization in this setting is challenging, as the underlying constraints are
often highly nonconvex, and mission designers must balance accuracy (e.g.,
solving the full problem) with efficiency (e.g., convex relaxations), slowing
iteration and limiting design agility. To address these challenges, this paper
proposes an AI-powered framework that enables agile mission design for a wide
range of Earth orbit rendezvous scenarios. Given the orbital information of the
target spacecraft, boundary conditions, and a range of flight times, this work
proposes a Transformer-based architecture that generates, in a single
parallelized inference step, a set of near-Pareto optimal trajectories across
varying flight times, thereby enabling rapid mission trade studies. The model
is further extended to accommodate variable flight times and perturbed orbital
dynamics, supporting realistic multi-objective trade-offs. Validation on
chance-constrained rendezvous problems with passive safety constraints
demonstrates that the model generalizes across both flight times and dynamics,
consistently providing high-quality initial guesses that converge to superior
solutions in fewer iterations. Moreover, the framework efficiently approximates
the Pareto front, achieving runtimes comparable to convex relaxation by
exploiting parallelized inference. Together, these results position the
proposed framework as a practical surrogate for nonconvex trajectory generation
and mark an important step toward AI-driven trajectory design for accelerating
preliminary mission planning in real-world rendezvous applications.

</details>


### [853] [Inverse Mixed-Integer Programming: Learning Constraints then Objective Functions](https://arxiv.org/abs/2510.04455)
*Akira Kitaoka*

Main category: math.OC

TL;DR: The paper introduces a two-stage method for data-driven inverse optimization to simultaneously learn objective functions and constraints in mixed-integer linear programming.


<details>
  <summary>Details</summary>
Motivation: To address the lack of methods for learning both objective functions and constraints in inverse optimization, which is crucial for constructing mathematical models in fields like power systems and scheduling.

Method: The proposed two-stage method first learns constraints and then learns the objective function, leveraging statistical learning theory in pseudometric spaces and sub-Gaussian distributions.

Result: The method theoretically solves inverse optimization with finite datasets and demonstrates practical applicability in scheduling problems with integer linear programming models involving up to 100 decision variables.

Conclusion: The proposed approach advances the field of inverse optimization by integrating constraint and objective function learning, making it suitable for practical applications in complex real-world tasks.

Abstract: In mixed-integer linear programming, data-driven inverse optimization that
learns the objective function and the constraints from observed data plays an
important role in constructing appropriate mathematical models for various
fields, including power systems and scheduling. However, to the best of our
knowledge, there is no known method for learning both the objective functions
and the constraints. In this paper, we propose a two-stage method for a class
of problems where the objective function is expressed as a linear combination
of functions and the constraints are represented by functions and thresholds.
Specifically, our method first learns the constraints and then learns the
objective function. On the theoretical side, we show the proposed method can
solve inverse optimization problems in finite dataset, develop statistical
learning theory in pseudometric spaces and sub-Gaussian distributions, and
construct a statistical learning for inverse optimization. On the experimental
side, we demonstrate that our method is practically applicable for scheduling
problems formulated as integer linear programmings with up to 100 decision
variables, which are typical in real-world settings.

</details>


### [854] [On decomposability and subdifferential of the tensor nuclear norm](https://arxiv.org/abs/2510.04647)
*Jiewen Guan,Bo Jiang,Zhening Li*

Main category: math.OC

TL;DR: This paper investigates the decomposability and subdifferential of the tensor nuclear norm, extending results to higher-order tensors.


<details>
  <summary>Details</summary>
Motivation: To address the unclear properties of decomposability and subdifferentials for higher-order tensor nuclear norms, unlike their well-understood matrix counterparts.

Method: Mathematical proofs to demonstrate full decomposability over specific subspaces, derivation of subdifferential inclusions, and subgradient analysis of tensor nuclear norms.

Result: Identifies the largest subspaces supporting full decomposability of tensor nuclear norms and establishes novel inclusions for subdifferentials. Also, the statistical performance of tensor robust principal component analysis is explored for tensors of arbitrary order.

Conclusion: The paper extends the understanding of tensor nuclear norms, providing theoretical insights with practical implications for tensor decomposition and robust principal component analysis.

Abstract: We study the decomposability and the subdifferential of the tensor nuclear
norm. Both concepts are well understood and widely applied in matrices but
remain unclear for higher-order tensors. We show that the tensor nuclear norm
admits a full decomposability over specific subspaces and determine the largest
possible subspaces that allow the full decomposability. We derive novel
inclusions of the subdifferential of the tensor nuclear norm and study its
subgradients in a variety of subspaces of interest. All the results hold for
tensors of an arbitrary order. As an immediate application, we establish the
statistical performance of the tensor robust principal component analysis, the
first such result for tensors of an arbitrary order.

</details>


### [855] [Machine Learning and Control: Foundations, Advances, and Perspectives](https://arxiv.org/abs/2510.03303)
*Enrique Zuazua*

Main category: math.OC

TL;DR: The paper investigates how control theory can enhance understanding and performance in neural networks, introduces a hybrid learning model, and connects classical mathematical properties to modern AI.


<details>
  <summary>Details</summary>
Motivation: To explore how concepts in control theory and classical mathematics can shed new light on machine learning, particularly neural networks.

Method: The authors apply control theory concepts, hybrid modeling techniques, and mathematical analyses such as diffusion processes to neural network tasks.

Result: Insights into neural network classification, hybrid cooperative learning models, and connections between mathematical principles and AI effectiveness are revealed.

Conclusion: Control theory and mathematical principles offer valuable tools for understanding neural networks, proposing new methodologies, and guiding future AI research.

Abstract: Control theory of dynamical systems offers a powerful framework for tackling
challenges in deep neural networks and other machine learning architectures. We
show that concepts such as simultaneous and ensemble controllability offer new
insights into the classification and representation properties of deep neural
networks while the control and optimization of static systems can be employed
to better understand the performance of shallow networks. Inspired by the
classical concept of turnpike, we also explore the relationship between dynamic
and static neural networks, where depth is traded for width, and the role of
transformers as mechanisms for accelerating classical neural network tasks. We
also exploit the expressive power of neural networks (exemplified, for
instance, by the Universal Approximation Theorem) to develop a novel hybrid
modeling methodology, the Hybrid-Cooperative Learning (HYCO), combining
mechanics and data-driven methods in a game-theoretic setting. Finally, we
describe how classical properties of diffusion processes, long established in
the context of partial differential equations, contribute to explaining the
success of modern generative artificial intelligence (AI). We present an
overview of our recent results in these areas, illustrating how control,
machine learning, numerical analysis, and partial differential equations come
together to motivate a fertile ground for future research.

</details>


### [856] [Quantizer Design for Finite Model Approximations, Model Learning, and Quantized Q-Learning for MDPs with Unbounded Spaces](https://arxiv.org/abs/2510.04355)
*Osman Bicer,Ali D. Kara,Serdar Yuksel*

Main category: math.OC

TL;DR: This paper refines upper bounds on finite model approximation errors for MDPs with unbounded state spaces, focusing on optimized quantizer design and its implications on learning policies.


<details>
  <summary>Details</summary>
Motivation: Understanding and improving the performance of policies for MDPs with unbounded state spaces, especially under quantized Q-learning and empirical modeling setups, where existing bounds are insufficient.

Method: The paper refines and extends error bounds through optimized quantizer designs, analyzing their performance under planning and learning frameworks, and explores conditions like Lyapunov growth to derive explicit upper bounds.

Result: The results provide explicit upper bounds that improve with increased quantization granularity. The findings also distinguish between planning and learning contexts in MDP approximations.

Conclusion: Optimized quantizer designs can enhance policy performance in MDP setups, with upper bounds that diminish with finer quantization. However, learning scenarios entail distinct challenges compared to planning, which must be carefully addressed.

Abstract: In this paper, for Markov decision processes (MDPs) with unbounded state
spaces we present refined upper bounds presented in [Kara et. al. JMLR'23] on
finite model approximation errors via optimizing the quantizers used for finite
model approximations. We also consider implications on quantizer design for
quantized Q-learning and empirical model learning, and the performance of
policies obtained via Q-learning where the quantized state is treated as the
state itself. We highlight the distinctions between planning, where
approximating MDPs can be independently designed, and learning (either via
Q-learning or empirical model learning), where approximating MDPs are
restricted to be defined by invariant measures of Markov chains under
exploration policies, leading to significant subtleties on quantizer design
performance, even though asymptotic near optimality can be established under
both setups. In particular, under Lyapunov growth conditions, we obtain
explicit upper bounds which decay to zero as the number of bins approaches
infinity.

</details>


### [857] [Zeroth-Order Methods for Stochastic Nonconvex Nonsmooth Composite Optimization](https://arxiv.org/abs/2510.04446)
*Ziyi Chen,Peiran Yu,Heng Huang*

Main category: math.OC

TL;DR: The paper addresses stochastic nonconvex nonsmooth composite optimization problems without assuming smoothness, introducing algorithms converging to novel stationary points.


<details>
  <summary>Details</summary>
Motivation: To address limitations in classical composite optimization approaches, which assume smoothness and cannot handle examples like regularized ReLU networks and sparse support matrix machines.

Method: Introduced two new notions of approximate stationary points and developed two zeroth-order algorithms for optimization.

Result: The algorithms achieve finite-time convergence to the defined approximate stationary points.

Conclusion: The algorithms are proven effective through numerical experiments, widening the scope of problems solvable without smoothness assumptions.

Abstract: This work aims to solve a stochastic nonconvex nonsmooth composite
optimization problem. Previous works on composite optimization problem requires
the major part to satisfy Lipschitz smoothness or some relaxed smoothness
conditions, which excludes some machine learning examples such as regularized
ReLU network and sparse support matrix machine. In this work, we focus on
stochastic nonconvex composite optimization problem without any smoothness
assumptions. In particular, we propose two new notions of approximate
stationary points for such optimization problem and obtain finite-time
convergence results of two zeroth-order algorithms to these two approximate
stationary points respectively. Finally, we demonstrate that these algorithms
are effective using numerical experiments.

</details>


### [858] [A Unified Optimization Framework for Multiclass Classification with Structured Hyperplane Arrangements](https://arxiv.org/abs/2510.05047)
*Víctor Blanco,Harshit Kothari,James Luedtke*

Main category: math.OC

TL;DR: Proposes a new optimization model for multiclass classification using hyperplanes, improving computational efficiency and incorporating non-linear and alternative structures.


<details>
  <summary>Details</summary>
Motivation: Improve upon SVM-based multiclass classification by optimizing efficiency and enabling compatibility with advanced geometric structures.

Method: Introduces a kernel-based extension and a dynamic clustering heuristic using MIP for large-scale instances.

Result: Demonstrates computational efficiency and competitive classification performance through experiments on synthetic datasets and UCI benchmarks.

Conclusion: The proposed method outperforms existing approaches in efficiency and performs competitively in classification tasks, showing promise for practical applications.

Abstract: In this paper, we propose a new mathematical optimization model for
multiclass classification based on arrangements of hyperplanes. Our approach
preserves the core support vector machine (SVM) paradigm of maximizing class
separation while minimizing misclassification errors, and it is computationally
more efficient than a previous formulation. We present a kernel-based extension
that allows it to construct nonlinear decision boundaries. Furthermore, we show
how the framework can naturally incorporate alternative geometric structures,
including classification trees, $\ell_p$-SVMs, and models with discrete feature
selection. To address large-scale instances, we develop a dynamic clustering
matheuristic that leverages the proposed MIP formulation. Extensive
computational experiments demonstrate the efficiency of the proposed model and
dynamic clustering heuristic, and we report competitive classification
performance on both synthetic datasets and real-world benchmarks from the UCI
Machine Learning Repository, comparing our method with state-of-the-art
implementations available in scikit-learn.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [859] [Viability-Preserving Passive Torque Control](https://arxiv.org/abs/2510.03367)
*Zizhe Zhang,Yicong Wang,Zhiquan Zhang,Tianyu Li,Nadia Figueroa*

Main category: eess.SY

TL;DR: The paper proposes a viability theory-based torque control framework that ensures safety for manipulators using pre-computed safe sets, validated with smoother results on a 7-DoF robot.


<details>
  <summary>Details</summary>
Motivation: To address safety concerns in conventional unconstrained torque controllers for manipulators, particularly under external perturbations.

Method: Uses viability theory to pre-compute safe sets for joint positions and velocities and enforces these constraints through a quadratic programming-based control framework.

Result: Results show improved safety constraints, higher control-loop rates, and smoother trajectories compared to baseline methods, demonstrated via simulations and experiments on a Franka Emika manipulator.

Conclusion: The proposed approach ensures safety and smooth operation for robotic manipulators by integrating viability-based constraints into torque controllers.

Abstract: Conventional passivity-based torque controllers for manipulators are
typically unconstrained, which can lead to safety violations under external
perturbations. In this paper, we employ viability theory to pre-compute safe
sets in the state-space of joint positions and velocities. These viable sets,
constructed via data-driven and analytical methods for self-collision
avoidance, external object collision avoidance and joint-position and
joint-velocity limits, provide constraints on joint accelerations and thus
joint torques via the robot dynamics. A quadratic programming-based control
framework enforces these constraints on a passive controller tracking a
dynamical system, ensuring the robot states remain within the safe set in an
infinite time horizon. We validate the proposed approach through simulations
and hardware experiments on a 7-DoF Franka Emika manipulator. In comparison to
a baseline constrained passive controller, our method operates at higher
control-loop rates and yields smoother trajectories.

</details>


### [860] [Learning a Shape-adaptive Assist-as-needed Rehabilitation Policy from Therapist-informed Input](https://arxiv.org/abs/2510.04666)
*Zhimin Hou,Jiacheng Hou,Xiao Chen,Hamid Sadeghian,Tianyu Ren,Sami Haddadin*

Main category: eess.SY

TL;DR: This paper introduces a telerobotics framework to enhance robotic rehabilitation by combining therapist-guided minimal assistance with adaptive learning policies, thus improving therapy outcomes.


<details>
  <summary>Details</summary>
Motivation: Despite the potential of therapist-in-the-loop robotic rehabilitation, challenges such as safe interaction and adaptability hinder its widespread adoption.

Method: The framework encodes therapist-corrective force as via-points in a latent space, while a shape-adaptive policy progressively adjusts the therapy trajectory based on patient and therapist inputs.

Result: The proposed system outperformed two state-of-the-art methods in reducing corrective assistance and enhancing movement smoothness during validation tests on two tasks.

Conclusion: The system improves remote assist-as-needed therapy by fostering intuitive collaboration between therapists and patients, addressing known limitations of safety and adaptability in robotic rehabilitation.

Abstract: Therapist-in-the-loop robotic rehabilitation has shown great promise in
enhancing rehabilitation outcomes by integrating the strengths of therapists
and robotic systems. However, its broader adoption remains limited due to
insufficient safe interaction and limited adaptation capability. This article
proposes a novel telerobotics-mediated framework that enables therapists to
intuitively and safely deliver assist-as-needed~(AAN) therapy based on two
primary contributions. First, our framework encodes the therapist-informed
corrective force into via-points in a latent space, allowing the therapist to
provide only minimal assistance while encouraging patient maintaining own
motion preferences. Second, a shape-adaptive ANN rehabilitation policy is
learned to partially and progressively deform the reference trajectory for
movement therapy based on encoded patient motion preferences and
therapist-informed via-points. The effectiveness of the proposed shape-adaptive
AAN strategy was validated on a telerobotic rehabilitation system using two
representative tasks. The results demonstrate its practicality for remote AAN
therapy and its superiority over two state-of-the-art methods in reducing
corrective force and improving movement smoothness.

</details>


### [861] [Efficient Probabilistic Planning with Maximum-Coverage Distributionally Robust Backward Reachable Trees](https://arxiv.org/abs/2510.04807)
*Alex Rose,Naman Aggarwal,Christopher Jewison,Jonathan P. How*

Main category: eess.SY

TL;DR: The paper introduces two motion planning algorithms for linear Gaussian systems, focusing on coverage and robustness using novel ambiguity sets of Gaussian distributions.


<details>
  <summary>Details</summary>
Motivation: To improve coverage and robustness in multi-query motion planning for linear Gaussian systems and address current limitations in handling uncertainty.

Method: The authors propose new formulations for ball-shaped and ellipsoidal ambiguity sets of Gaussian distributions and design robust belief roadmap construction algorithms based on these formulations.

Result: The proposed methods either match or outperform existing algorithms in coverage, robustness, and efficacy under diverse simulation scenarios.

Conclusion: The newly developed algorithms provide improved solutions for motion planning under uncertainty, leveraging novel formulations to achieve better coverage and safety guarantees.

Abstract: This paper presents a new multi-query motion planning algorithm for linear
Gaussian systems with the goal of reaching a Euclidean ball with high
probability. We develop a new formulation for ball-shaped ambiguity sets of
Gaussian distributions and leverage it to develop a distributionally robust
belief roadmap construction algorithm. This algorithm synthe- sizes robust
controllers which are certified to be safe for maximal size ball-shaped
ambiguity sets of Gaussian distributions. Our algorithm achieves better
coverage than the maximal coverage algorithm for planning over Gaussian
distributions [1], and we identify mild conditions under which our algorithm
achieves strictly better coverage. For the special case of no process noise or
state constraints, we formally prove that our algorithm achieves maximal
coverage. In addition, we present a second multi-query motion planning
algorithm for linear Gaussian systems with the goal of reaching a region
parameterized by the Minkowski sum of an ellipsoid and a Euclidean ball with
high probability. This algorithm plans over ellipsoidal sets of maximal size
ball-shaped ambiguity sets of Gaussian distributions, and provably achieves
equal or better coverage than the best-known algorithm for planning over
ellipsoidal ambiguity sets of Gaussian distributions [2]. We demonstrate the
efficacy of both methods in a wide range of conditions via extensive simulation
experiments.

</details>


### [862] [Use of Quadcopter Wakes to Supplement Strawberry Pollination](https://arxiv.org/abs/2510.03974)
*Sadie Cutler,Ben DeFay,Scott McArt,Kirstin Petersen*

Main category: eess.SY

TL;DR: This paper explores using quadcopters for artificial pollination as a supplementary solution to address global pollinator declines.


<details>
  <summary>Details</summary>
Motivation: Pollination shortfalls in crops such as strawberries affect ecosystems and food supply, and declines of natural pollinators require alternative solutions.

Method: Field experiments were conducted to assess quadcopters assisting natural pollinators, combined with lab studies to test artificial pollination methods based on wind pollination.

Result: Field results were inconclusive, but lab experiments demonstrated potential for using quadcopters to enhance pollination.

Conclusion: More research and adaptations are needed for quadcopter-assisted pollination to achieve better field results and prove its feasibility for widespread agricultural use.

Abstract: Pollinators are critical to the world's ecosystems and food supply, yet
recent studies have found pollination shortfalls in several crops, including
strawberry. This is troubling because wild and managed pollinators are
currently experiencing declines. One possibility is to try and provide
supplemental pollination solutions. These solutions should be affordable and
simple for farmers to implement if their use is to be widespread; quadcopters
are a great example, already used for monitoring on many farms. This paper
investigates a new method for artificial pollination based on wind pollination
that bears further investigation. After determining the height where the
lateral flow is maximized, we performed field experiments with a quadcopter
assisting natural pollinators. Although our results in the field were
inconclusive, lab studies show that the idea shows promise and could be adapted
for better field results.

</details>


### [863] [A Trustworthy Industrial Fault Diagnosis Architecture Integrating Probabilistic Models and Large Language Models](https://arxiv.org/abs/2510.03815)
*Yue wu*

Main category: eess.SY

TL;DR: The paper presents a novel framework combining Bayesian networks, large language models (LLMs), and confidence calibration for more interpretable, reliable, and accurate industrial fault diagnosis.


<details>
  <summary>Details</summary>
Motivation: Traditional and deep learning approaches lack interpretability, generalization, and uncertainty quantification in industrial fault diagnosis, leading to insufficient trustworthiness in these systems.

Method: The architecture combines a Bayesian network-based diagnostic engine with an LLM-powered cognitive quorum module for expert-level arbitration. It integrates confidence calibration (via temperature scaling) and risk assessment modules that compute metrics like expected calibration error (ECE) to gauge system reliability.

Result: Experimental results showed over a 28-point improvement in diagnostic accuracy compared to baseline methods and a reduction of more than 75% in calibrated ECE, demonstrating significant gains in reliability and accuracy.

Conclusion: The proposed framework effectively addresses the challenges of complex features and limitations of traditional models, offering high-trust, explainable AI solutions suitable for industrial diagnostics.

Abstract: There are limitations of traditional methods and deep learning methods in
terms of interpretability, generalization, and quantification of uncertainty in
industrial fault diagnosis, and there are core problems of insufficient
credibility in industrial fault diagnosis. The architecture performs
preliminary analysis through a Bayesian network-based diagnostic engine and
features an LLM-driven cognitive quorum module with multimodal input
capabilities. The module conducts expert-level arbitration of initial diagnoses
by analyzing structured features and diagnostic charts, prioritizing final
decisions after conflicts are identified. To ensure the reliability of the
system output, the architecture integrates a confidence calibration module
based on temperature calibration and a risk assessment module, which
objectively quantifies the reliability of the system using metrics such as
expected calibration error (ECE). Experimental results on a dataset containing
multiple fault types showed that the proposed framework improved diagnostic
accuracy by more than 28 percentage points compared to the baseline model,
while the calibrated ECE was reduced by more than 75%. Case studies have
confirmed that HCAA effectively corrects misjudgments caused by complex feature
patterns or knowledge gaps in traditional models, providing novel and practical
engineering solutions for building high-trust, explainable AI diagnostic
systems for industrial applications.

</details>


### [864] [Design Process of a Self Adaptive Smart Serious Games Ecosystem](https://arxiv.org/abs/2510.04615)
*X. Tao,P. Chen,M. Tsami,F. Khayati,M. Eckert*

Main category: eess.SY

TL;DR: The paper presents the design and vision of Blexer v3, a modular, AI-driven rehabilitation system using serious games with features like dynamic difficulty adjustment and personalized gameplay.


<details>
  <summary>Details</summary>
Motivation: To enhance rehabilitation efficiency and personalization through a modular ecosystem that incorporates serious games powered by advanced AI techniques.

Method: The proposed system architecture involves multimodal sensing, real-time reasoning, and intelligent control. It will integrate modules for data collection, user state inference, and gameplay adaptation, supporting features like DDA and PCG.

Result: The paper provides a conceptual framework for Blexer v3, detailing its modular structure and data flow for future prototype development and clinical application.

Conclusion: Blexer v3's modular and AI-driven architecture sets the foundation for creating personalized and data-driven rehabilitation systems, paving the way for functional prototyping and clinical testing.

Abstract: This paper outlines the design vision and planned evolution of Blexer v3, a
modular and AI-driven rehabilitation ecosystem based on serious games. Building
on insights from previous versions of the system, we propose a new architecture
that aims to integrate multimodal sensing, real-time reasoning, and intelligent
control. The envisioned system will include distinct modules for data
collection, user state inference, and gameplay adaptation. Key features such as
dynamic difficulty adjustment (DDA) and procedural content generation (PCG) are
also considered to support personalized interventions. We present the complete
conceptual framework of Blexer v3, which defines the modular structure and data
flow of the system. This serves as the foundation for the next phase: the
development of a functional prototype and its integration into clinical
rehabilitation scenarios.

</details>


### [865] [Model Predictive Control-Guided Reinforcement Learning for Implicit Balancing](https://arxiv.org/abs/2510.04868)
*Seyed Soroush Karimi Madahi,Kenneth Bruninx,Bert Claessens,Chris Develder*

Main category: eess.SY

TL;DR: This paper introduces an MPC-guided RL method for balancing energy supply-demand and demonstrates a notable improvement in arbitrage profits compared to using standalone MPC or RL.


<details>
  <summary>Details</summary>
Motivation: Current methods for balancing energy supply-demand in European imbalance markets face limitations: MPC strategies struggle with price-formation and computational inefficiency, while RL methods require extensive training and depend on real-time/historical data.

Method: The paper integrates MPC with RL into a novel MPC-guided RL framework combining the strengths of both: leveraging RL's computational efficiency and MPC's ability to use forecasts in decision-making.

Result: Using Belgian balancing data from 2023, the proposed MPC-guided RL method shows arbitrage profit improvements of 16.15% and 54.36% compared to standalone RL and MPC, respectively.

Conclusion: The MPC-guided RL approach effectively combines RL's speed and MPC's forecasting strength, achieving superior performance for implicit balancing strategies and reducing existing limitations.

Abstract: In Europe, profit-seeking balance responsible parties can deviate in real
time from their day-ahead nominations to assist transmission system operators
in maintaining the supply-demand balance. Model predictive control (MPC)
strategies to exploit these implicit balancing strategies capture arbitrage
opportunities, but fail to accurately capture the price-formation process in
the European imbalance markets and face high computational costs. Model-free
reinforcement learning (RL) methods are fast to execute, but require
data-intensive training and usually rely on real-time and historical data for
decision-making. This paper proposes an MPC-guided RL method that combines the
complementary strengths of both MPC and RL. The proposed method can effectively
incorporate forecasts into the decision-making process (as in MPC), while
maintaining the fast inference capability of RL. The performance of the
proposed method is evaluated on the implicit balancing battery control problem
using Belgian balancing data from 2023. First, we analyze the performance of
the standalone state-of-the-art RL and MPC methods from various angles, to
highlight their individual strengths and limitations. Next, we show an
arbitrage profit benefit of the proposed MPC-guided RL method of 16.15% and
54.36%, compared to standalone RL and MPC.

</details>


### [866] [Data-Driven Adaptive PID Control Based on Physics-Informed Neural Networks](https://arxiv.org/abs/2510.04591)
*Junsei Ito,Yasuaki Wasa*

Main category: eess.SY

TL;DR: The paper presents a PID controller design that uses PINNs and adaptive gain optimization for improving control under nonlinear system conditions.


<details>
  <summary>Details</summary>
Motivation: The study aims to use physics-informed neural networks (PINNs) in PID controller design to manage nonlinearities and ensure both stability and accuracy.

Method: The approach uses PINNs for predictive modeling, optimizes PID gains via automatic differentiation, and employs a cost function based on tracking errors and control inputs for adaptive gain tuning.

Result: The method demonstrated effectiveness in numerical experiments across both time and frequency domains.

Conclusion: The integration of PINNs with adaptive gain optimization provides a systematic and effective method for PID control design in nonlinear dynamical systems.

Abstract: This article proposes a data-driven PID controller design based on the
principle of adaptive gain optimization, leveraging Physics-Informed Neural
Networks (PINNs) generated for predictive modeling purposes. The proposed
control design method utilizes gradients of the PID gain optimization, achieved
through the automatic differentiation of PINNs, to apply model predictive
control using a cost function based on tracking error and control inputs. By
optimizing PINNs-based PID gains, the method achieves adaptive gain tuning that
ensures stability while accounting for system nonlinearities. The proposed
method features a systematic framework for integrating PINNs-based models of
dynamical control systems into closed-loop control systems, enabling direct
application to PID control design. A series of numerical experiments is
conducted to demonstrate the effectiveness of the proposed method from the
control perspectives based on both time and frequency domains.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [867] [Assessing the impact of contact time on leachate chemistry from recycled concrete aggregates](https://arxiv.org/abs/2510.03344)
*Morgan D. Sanger,Gabrielle Campagnola,Robin Ritchey,Tuncer B. Edil,Matthew Ginder-Vogel*

Main category: physics.chem-ph

TL;DR: This paper studies the behavior of leachate pH, alkalinity, and calcium ion concentration from recycled concrete aggregate (RCA) through batch tests, highlighting its potential risks and environmental interactions.


<details>
  <summary>Details</summary>
Motivation: To address the gaps in understanding RCA leachate characteristics, particularly the differences between field and lab measurements, and aged leachate composition over time.

Method: The study conducts modified batch tests on RCA samples, monitoring leachate pH, alkalinity, and calcium concentration for 24 hours to observe changes in chemistry during initial contact with water.

Result: Leachate pH starts high (above pH 10) and decreases as it interacts with atmospheric CO2; calcium ion concentration shows rapid initial growth and then stabilizes, while alkalinity stabilizes after a sharp increase.

Conclusion: RCA leachate exhibits notable chemical changes upon water contact, and its environmental implications should be considered for sustainable pavement applications.

Abstract: Recycled concrete aggregate (RCA) is recognized as a readily available,
mechanically sufficient construction and demolition waste product that is
suitable as a base course substitute for natural, virgin aggregate in pavement
construction. Environmentally responsible applications of RCA must consider the
high alkalinity, high pH leachate, and heavy metal leaching risks reported in
the literature. The existing body of literature does not address discrepancies
between field and laboratory measurements of RCA leachate pH, nor are there any
existing studies of aged RCA leachate composition as a function of time. To
consider the influence of contact time on RCA leachate, the present study
evaluates recovered RCA base course samples from the Minnesota Road Research
highway construction study site using modified batch test methodology. Leachate
pH, alkalinity, and calcium ion (Ca2+) concentration were monitored for 24
hours to understand RCA leachate chemistry during the initial contact period.
Leachate pH is high upon initial contact with water (pH > 10) and decreases
over time as it reacts with atmospheric carbon dioxide. Calcium ion
concentration increases rapidly in the initial contact period, then more
gradually as calcium saturation is reached. Alkalinity stabilizes (50-65 mg
CaCO3/L) after a dramatic increase during the initial contact period.

</details>


### [868] [A Universal Deep Learning Force Field for Molecular Dynamic Simulation and Vibrational Spectra Prediction](https://arxiv.org/abs/2510.04227)
*Shengjiao Ji,Yujin Zhang,Zihan Zou,Bin Jiang,Jun Jiang,Yi Luo,Wei Hu*

Main category: physics.chem-ph

TL;DR: This paper presents DetaNet, a machine learning-based molecular dynamics framework for efficient and accurate IR and Raman spectral simulations.


<details>
  <summary>Details</summary>
Motivation: The study addresses the limitations of traditional quantum chemistry methods and computationally expensive AIMD simulations in accurately predicting IR and Raman spectra.

Method: The authors developed DetaNet, a deep learning model trained on the QMe14S dataset, and integrated it with velocity-Verlet integrators to predict time-correlation functions via MLMD and RPMD simulations.

Result: DetaNet demonstrated near-experimental spectral accuracy with speed improvements of up to three orders of magnitude over AIMD. It was tested on various molecular systems, including crystals and biological macromolecules, without significant fine-tuning.

Conclusion: The work introduces a universal and transferable machine learning framework for dynamic simulations and spectral predictions, combining accuracy with reduced computational costs across diverse systems.

Abstract: Accurate and efficient simulation of infrared (IR) and Raman spectra is
essential for molecular identification and structural analysis. Traditional
quantum chemistry methods based on the harmonic approximation neglect
anharmonicity and nuclear quantum effects, while ab initio molecular dynamics
(AIMD) remains computationally expensive. Here, we integrate our deep
equivariant tensor attention network (DetaNet) with a velocity-Verlet
integrator to enable fast and accurate machine learning molecular dynamics
(MLMD) simulations for spectral prediction. Trained on the QMe14S dataset
containing energies, forces, dipole moments, and polarizabilities for 186,102
small organic molecules, DetaNet yields a universal and transferable force
field with high-order tensor prediction capability. Using time-correlation
functions derived from MLMD and ring-polymer molecular dynamics (RPMD)
trajectories, we computed IR and Raman spectra that accurately reproduce
anharmonic and nuclear quantum effects. Benchmark tests on isolated molecules,
including polycyclic aromatic hydrocarbons, demonstrate that the DetaNet-based
MD approach achieves near-experimental spectral accuracy with speedups up to
three orders of magnitude over AIMD. Furthermore, the framework extends
seamlessly to molecular and inorganic crystals, molecular aggregates, and
biological macromolecules such as polypeptides with minimal fine-tuning. In all
systems, DetaNet maintains high accuracy while significantly reducing
computational cost. Overall, this work establishes a universal machine learning
force field and tensor-aware MLMD framework that enable fast, accurate, and
broadly applicable dynamic simulations and IR/Raman spectral predictions across
diverse molecular and material systems.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [869] [Embedding Sustainability in Software Engineering Curriculum: A Case Study](https://arxiv.org/abs/2510.03321)
*Ruzanna Chitchyan,Niki Mahmoudi*

Main category: cs.CY

TL;DR: The paper explores integrating sustainability into Software Engineering education via a case study, providing frameworks and strategies to help institutions implement such integration.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of integrating sustainability into Software Engineering curricula, which is increasingly critical in modern education.

Method: A case study that involves co-identifying integration opportunities by academics and students using the Sustainability Awareness Framework, discussion questions, and examples from the Green Software Foundation.

Result: Practical steps and strategies for embedding sustainability into Software Engineering programs were identified, including frameworks, examples, and iterative processes.

Conclusion: Integrating sustainability into Software Engineering education is necessary and urgent for preparing graduates to be sustainability-aware professionals in a changing society.

Abstract: Sustainability is increasingly recognized as a critical dimension of
engineering education, yet its integration into Software Engineering curricula
remains a challenge. This paper reports on a case study that examines how
sustainability is being embedded across modules in the Software Engineering
program at one university. The paper outlines the process through which
academics and students co-identified opportunities for integration, guided by
the five dimensions of the Sustainability Awareness Framework, targeted
discussion questions, and good practice examples drawn from the Green Software
Foundation patterns. The study highlights practical steps - including the use
of frameworks, illustrative examples, student engagement, and iterative
consultative processes - that can support other institutions seeking to embed
sustainability into their programs. We also discuss strategies for integrating
sustainability into the Software Engineering curriculum and argue that such
integration is a necessary and urgent step to prepare Software Engineering
graduates as sustainability-aware professionals in our changing society.

</details>


### [870] [Lightweight Prompt Engineering for Cognitive Alignment in Educational AI: A OneClickQuiz Case Study](https://arxiv.org/abs/2510.03374)
*Antoun Yaacoub,Zainab Assaghir,Jérôme Da-Rugna*

Main category: cs.CY

TL;DR: The paper evaluates how different prompting strategies affect the cognitive alignment of AI-generated questions in an educational plugin, concluding that detailed prompts are more effective for aligning with Bloom's Taxonomy.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of quality and pedagogical alignment in AI-generated educational content.

Method: The study used three prompting strategies (detailed, simpler, and persona-based) to generate questions across different cognitive levels of Bloom's Taxonomy. Outputs were assessed using an automated model and human review.

Result: Detailed prompts yielded more cognitively aligned questions, while simpler and persona-based prompts often resulted in misaligned questions.

Conclusion: Strategic prompt engineering is essential for creating pedagogically effective AI-generated educational content.

Abstract: The rapid integration of Artificial Intelligence (AI) into educational
technology promises to revolutionize content creation and assessment. However,
the quality and pedagogical alignment of AI-generated content remain critical
challenges. This paper investigates the impact of lightweight prompt
engineering strategies on the cognitive alignment of AI-generated questions
within OneClickQuiz, a Moodle plugin leveraging generative AI. We evaluate
three prompt variants-a detailed baseline, a simpler version, and a
persona-based approach-across Knowledge, Application, and Analysis levels of
Bloom's Taxonomy. Utilizing an automated classification model (from prior work)
and human review, our findings demonstrate that explicit, detailed prompts are
crucial for precise cognitive alignment. While simpler and persona-based
prompts yield clear and relevant questions, they frequently misalign with
intended Bloom's levels, generating outputs that are either too complex or
deviate from the desired cognitive objective. This study underscores the
importance of strategic prompt engineering in fostering pedagogically sound
AI-driven educational solutions and advises on optimizing AI for quality
content generation in learning analytics and smart learning environments.

</details>


### [871] [Red Lines and Grey Zones in the Fog of War: Benchmarking Legal Risk, Moral Harm, and Regional Bias in Large Language Model Military Decision-Making](https://arxiv.org/abs/2510.03514)
*Toby Drinkall*

Main category: cs.CY

TL;DR: This study evaluates the legal and moral risks of using large language models (LLMs) for military decision-making, focusing on their targeting behavior in simulated conflicts. The findings reveal concerning behaviors, such as violating humanitarian principles and tolerating civilian harm.


<details>
  <summary>Details</summary>
Motivation: To examine the behavioral tendencies of LLMs when used for military decision-making, specifically targeting behavior, and assess their adherence to International Humanitarian Law (IHL) principles.

Method: The study introduces a benchmarking framework with four IHL-grounded metrics to evaluate LLMs’ behavior in simulated conflict scenarios. It tests three models (GPT-4o, Gemini-2.5, LLaMA-3.1) across 90 multi-turn, multi-agent crisis simulations.

Result: The tested LLMs exhibited significant violations of legal and moral principles, with high variability in their targeting behaviors. Civilian targeting rates ranged widely, and harm tolerance increased during simulations. For instance, LLaMA-3.1 showed the highest issues, while Gemini-2.5 performed relatively better.

Conclusion: The paper underscores the risks of integrating untested LLMs into military operations and highlights the need for rigorous benchmarking frameworks like the one developed to standardize pre-deployment testing and inform model selection based on risk tolerances.

Abstract: As military organisations consider integrating large language models (LLMs)
into command and control (C2) systems for planning and decision support,
understanding their behavioural tendencies is critical. This study develops a
benchmarking framework for evaluating aspects of legal and moral risk in
targeting behaviour by comparing LLMs acting as agents in multi-turn simulated
conflict. We introduce four metrics grounded in International Humanitarian Law
(IHL) and military doctrine: Civilian Target Rate (CTR) and Dual-use Target
Rate (DTR) assess compliance with legal targeting principles, while Mean and
Max Simulated Non-combatant Casualty Value (SNCV) quantify tolerance for
civilian harm.
  We evaluate three frontier models, GPT-4o, Gemini-2.5, and LLaMA-3.1, through
90 multi-agent, multi-turn crisis simulations across three geographic regions.
Our findings reveal that off-the-shelf LLMs exhibit concerning and
unpredictable targeting behaviour in simulated conflict environments. All
models violated the IHL principle of distinction by targeting civilian objects,
with breach rates ranging from 16.7% to 66.7%. Harm tolerance escalated through
crisis simulations with MeanSNCV increasing from 16.5 in early turns to 27.7 in
late turns. Significant inter-model variation emerged: LLaMA-3.1 selected an
average of 3.47 civilian strikes per simulation with MeanSNCV of 28.4, while
Gemini-2.5 selected 0.90 civilian strikes with MeanSNCV of 17.6. These
differences indicate that model selection for deployment constitutes a choice
about acceptable legal and moral risk profiles in military operations.
  This work seeks to provide a proof-of-concept of potential behavioural risks
that could emerge from the use of LLMs in Decision Support Systems (AI DSS) as
well as a reproducible benchmarking framework with interpretable metrics for
standardising pre-deployment testing.

</details>


### [872] [Intelligent Healthcare Ecosystems: Optimizing the Iron Triangle of Healthcare (Access, Cost, Quality)](https://arxiv.org/abs/2510.03331)
*Vivek Acharya*

Main category: cs.CY

TL;DR: This study proposes an Intelligent Healthcare Ecosystem (iHE) that utilizes AI, interoperability standards, and digital technologies to address the trade-offs among cost, quality, and access in the U.S. healthcare system.


<details>
  <summary>Details</summary>
Motivation: The U.S. healthcare system spends 17% of GDP but has challenges in access and outcomes. The motivating problem is improving the trade-offs across cost, quality, and access (iron triangle).

Method: A narrative review of recent literature and policy reports was conducted, focusing on historical spending trends, waste, international comparisons, and enabling technologies for improving healthcare systems.

Result: The iHE framework includes AI decision support, interoperability, telehealth, and automation. It can reduce waste, personalize care, and improve payment models while addressing privacy, bias, and adoption issues.

Conclusion: A coordinated Intelligent Healthcare Ecosystem has the potential to transform the U.S. healthcare system, making it more accessible, affordable, and high-quality by targeting inefficiencies and leveraging advanced technologies.

Abstract: The United States spends nearly 17% of GDP on healthcare yet continues to
face uneven access and outcomes. This well-known trade-off among cost, quality,
and access - the "iron triangle" - motivates a system-level redesign. This
paper proposes an Intelligent Healthcare Ecosystem (iHE): an integrated,
data-driven framework that uses generative AI and large language models,
federated learning, interoperability standards (FHIR, TEFCA), and digital twins
to improve access and quality while lowering cost. We review historical
spending trends, waste, and international comparisons; introduce a value
equation that jointly optimizes access, quality, and cost; and synthesize
evidence on the enabling technologies and operating model for iHE. Methods
follow a narrative review of recent literature and policy reports. Results
outline core components (AI decision support, interoperability, telehealth,
automation) and show how iHE can reduce waste, personalize care, and support
value-based payment while addressing privacy, bias, and adoption challenges. We
argue that a coordinated iHE can bend - if not break - the iron triangle,
moving the system toward care that is more accessible, affordable, and high
quality.

</details>


### [873] [Defining a Strategic Action Plan for AI in Higher Education](https://arxiv.org/abs/2510.03343)
*Nikolaos Avouris*

Main category: cs.CY

TL;DR: The paper explores challenges of AI in higher education and proposes a CASD framework to address them.


<details>
  <summary>Details</summary>
Motivation: To address challenges of integrating AI in higher education institutions and propose actionable strategies.

Method: A review of normative actions followed by proposing a CASD (Challenges, Actions, Stakeholders, Deployment) framework with examples.

Result: Defined five dimensions for challenges and matched strategic actions to higher education stakeholders along with a deployment plan.

Conclusion: Strategic actions and a structured framework are essential to tackle AI challenges in higher education effectively.

Abstract: This paper discusses key challenges of Artificial Intelligence in Education,
with main focus on higher education institutions. We start with reviewing
normative actions of international organizations and concerns expressed about
the current technical landscape. Then we proceed with proposing a framework
that comprises five key dimensions relating to the main challenges relating to
AI in higher education institutions, followed by five key strategic actions
that the main stakeholders need to take in order to address the current
developments. We map these actions to the main stakeholders of higher education
and propose a deployment plan. This defines a framework along the dimensions:
Challenges, Actions, Stakeholders, Deployment CASD. Examples of AI specific
actions at the institutional and individual course level are also provided and
discussed.

</details>


### [874] [An Adaptive Responsible AI Governance Framework for Decentralized Organizations](https://arxiv.org/abs/2510.03368)
*Kiana Jafari Meimandi,Anka Reuel,Gabriela Aranguiz-Dias,Hatim Rahama,Ala-Eddine Ayadi,Xavier Boullier,Jérémy Verdo,Louis Montanie,Mykel Kochenderfer*

Main category: cs.CY

TL;DR: The paper addresses challenges in implementing Responsible AI (RAI) governance in decentralized organizations and proposes an Adaptive RAI Governance (ARGO) Framework to balance central guidelines and local autonomy.


<details>
  <summary>Details</summary>
Motivation: The motivation is to explore how proposed Responsible AI (RAI) frameworks can be effectively applied in complex, decentralized organizational settings where decision-making is distributed.

Method: The authors conducted an RAI assessment across multiple business units and AI use cases in collaboration with a multinational enterprise.

Result: The study uncovered four key challenges: interplay between group-level guidance and local interpretation, difficulties in operationalizing RAI principles, regional/functional variations, and inconsistent accountability in oversight.

Conclusion: The paper introduces the ARGO Framework, emphasizing modular governance with shared standards, advisory resources, and localized implementation to ensure effective and responsible governance in decentralized organizations.

Abstract: This paper examines the assessment challenges of Responsible AI (RAI)
governance efforts in globally decentralized organizations through a case study
collaboration between a leading research university and a multinational
enterprise. While there are many proposed frameworks for RAI, their application
in complex organizational settings with distributed decision-making authority
remains underexplored. Our RAI assessment, conducted across multiple business
units and AI use cases, reveals four key patterns that shape RAI
implementation: (1) complex interplay between group-level guidance and local
interpretation, (2) challenges translating abstract principles into operational
practices, (3) regional and functional variation in implementation approaches,
and (4) inconsistent accountability in risk oversight. Based on these findings,
we propose an Adaptive RAI Governance (ARGO) Framework that balances central
coordination with local autonomy through three interdependent layers: shared
foundation standards, central advisory resources, and contextual local
implementation. We contribute insights from academic-industry collaboration for
RAI assessments, highlighting the importance of modular governance approaches
that accommodate organizational complexity while maintaining alignment with
responsible AI principles. These lessons offer practical guidance for
organizations navigating the transition from RAI principles to operational
practice within decentralized structures.

</details>


### [875] [TriQuest:An AI Copilot-Powered Platform for Interdisciplinary Curriculum Design](https://arxiv.org/abs/2510.03369)
*Huazhen Wang,Huimin Yang,Hainbin Lin,Yan Dong,Lili Chen,Liangliang Xia,Wenwen Xu*

Main category: cs.CY

TL;DR: This paper introduces TriQuest, an AI-powered platform for efficient interdisciplinary lesson planning, significantly improving curriculum design efficiency and quality.


<details>
  <summary>Details</summary>
Motivation: Modern curriculum reform emphasizes interdisciplinary teaching, but challenges exist in integrating knowledge and planning lessons efficiently.

Method: TriQuest combines large language models and knowledge graphs within an intuitive GUI to aid teachers in creating lesson plans, with core features like knowledge integration and computer-assisted review.

Result: A study with 43 teachers showed a 75% gain in curriculum design efficiency, 41% improvement in lesson plan quality, and reduction in design barriers and cognitive load.

Conclusion: TriQuest establishes a new approach for enhancing teacher professional development through intelligent technology.

Abstract: Interdisciplinary teaching is a cornerstone of modern curriculum reform, but
its implementation is hindered by challenges in knowledge integration and
time-consuming lesson planning. Existing tools often lack the required
pedagogical and domain-specific depth.We introduce TriQuest, an AI-copilot
platform designed to solve these problems. TriQuest uses large language models
and knowledge graphs via an intuitive GUI to help teachers efficiently generate
high-quality interdisciplinary lesson plans. Its core features include
intelligent knowledge integration from various disciplines and a human-computer
collaborative review process to ensure quality and innovation.In a study with
43 teachers, TriQuest increased curriculum design efficiency by an average of
75% and improved lesson plan quality scores by 41%. It also significantly
lowered design barriers and cognitive load. Our work presents a new paradigm
for empowering teacher professional development with intelligent technologies.

</details>


### [876] [Can an AI-Powered Presentation Platform Based On The Game "Just a Minute" Be Used To Improve Students' Public Speaking Skills?](https://arxiv.org/abs/2510.03379)
*Frederic Higham,Tommy Yuan*

Main category: cs.CY

TL;DR: The study evaluates an AI and gamification-based presentation platform, "Just a Minute (JAM)," to enhance public speaking skills among university students.


<details>
  <summary>Details</summary>
Motivation: To help university students overcome public speaking anxiety and improve their spontaneous speaking skills by combining AI with gamification.

Method: An experiment was conducted with University of York students who participated in the game, filled out pre- and post-questionnaires, and had their performance metrics recorded.

Result: The students found the game engaging and believed it had the potential to improve their public speaking skills over longer use periods.

Conclusion: The JAM platform shows promise as a tool for improving spontaneous speaking skills, but more extended studies are required to confirm its effectiveness.

Abstract: This study explores the effectiveness of applying AI and gamification into a
presentation platform aimed at University students wanting to improve their
public speaking skills in their native tongue. Specifically, a platform based
on the radio show, Just a Minute (JAM), is explored. In this game, players are
challenged to speak fluently on a topic for 60 seconds without repeating
themselves, hesitating or deviating from the topic. JAM has proposed benefits
such as allowing students to improve their spontaneous speaking skills and
reduce their use of speech disfluencies ("um", "uh", etc.).
  Previous research has highlighted the difficulties students face when
speaking publicly, the main one being anxiety. AI Powered Presentation
Platforms (AI-PPPs), where students can speak with an immersive AI audience and
receive real-time feedback, have been explored as a method to improve student's
speaking skills and confidence. So far they have shown promising results which
this study aims to build upon.
  A group of students from the University of York are enlisted to evaluate the
effectiveness of the JAM platform. They are asked to fill in a questionnaire,
play through the game twice and then complete a final questionnaire to discuss
their experiences playing the game. Various statistics are gathered during
their gameplay such as the number of points they gained and the number of rules
they broke. The results showed that students found the game promising and
believed that their speaking skills could improve if they played the game for
longer. More work will need to be carried out to prove the effectiveness of the
game beyond the short term.

</details>


### [877] [AI Adoption Across Mission-Driven Organizations](https://arxiv.org/abs/2510.03868)
*Dalia Ali,Muneeb Ahmed,Hailan Wang,Arfa Khan,Naira Paola Arnez Jordan,Sunnie S. Y. Kim,Meet Dilip Muchhala,Anne Kathrin Merkle,Orestis Papakyriakopoulos*

Main category: cs.CY

TL;DR: AI is selectively adopted by mission-driven organizations (MDOs), prioritizing deployment in controlled areas and preserving human oversight for critical tasks.


<details>
  <summary>Details</summary>
Motivation: To understand the nuanced ways mission-driven organizations approach AI adoption, especially given their resource constraints and values-driven priorities.

Method: Thematic analysis of semi-structured interviews with 15 practitioners from environmental, humanitarian, and development organizations across various global contexts.

Result: MDOs deploy AI for tasks like content creation and data analysis but face barriers when efficiency conflicts with organizational values. Decisions stall rather than proceeding with trade-offs.

Conclusion: AI adoption in MDOs is conditional, focusing on strengthening organizational sovereignty, mission integrity, and human-centered approaches.

Abstract: Despite AI's promise for addressing global challenges, empirical
understanding of AI adoption in mission-driven organizations (MDOs) remains
limited. While research emphasizes individual applications or ethical
principles, little is known about how resource-constrained, values-driven
organizations navigate AI integration across operations. We conducted thematic
analysis of semi-structured interviews with 15 practitioners from
environmental, humanitarian, and development organizations across the Global
North and South contexts. Our analysis examines how MDOs currently deploy AI,
what barriers constrain adoption, and how practitioners envision future
integration. MDOs adopt AI selectively, with sophisticated deployment in
content creation and data analysis while maintaining human oversight for
mission-critical applications. When AI's efficiency benefits conflict with
organizational values, decision-making stalls rather than negotiating
trade-offs. This study contributes empirical evidence that AI adoption in MDOs
should be understood as conditional rather than inevitable, proceeding only
where it strengthens organizational sovereignty and mission integrity while
preserving human-centered approaches essential to their missions.

</details>


### [878] [Accountability Capture: How Record-Keeping to Support AI Transparency and Accountability (Re)shapes Algorithmic Oversight](https://arxiv.org/abs/2510.04609)
*Shreya Chappidi,Jennifer Cobbe,Chris Norval,Anjali Mazumder,Jatinder Singh*

Main category: cs.CY

TL;DR: The paper explores the implications of record-keeping practices in algorithmic accountability, introducing the concept of 'accountability capture' and addressing the tensions and challenges it creates.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this study is to address the underexplored risks and effects of implementing record-keeping practices necessary for algorithmic accountability, especially on oversight, privacy, and socio-technical processes.

Method: The authors conducted a survey with 100 practitioners to gather evidence on the challenges and issues related to record-keeping practices in algorithmic accountability contexts.

Result: The study identifies widespread record-keeping issues, tensions between internal and external accountability demands, and instances of employee resistance to imposed practices, aligning them with the concept of 'accountability capture.'

Conclusion: The paper concludes that record-keeping for algorithmic accountability has broader implications for privacy, surveillance, and governance, underlining the importance of continuous attention from researchers, practitioners, and policymakers.

Abstract: Accountability regimes typically encourage record-keeping to enable the
transparency that supports oversight, investigation, contestation, and redress.
However, implementing such record-keeping can introduce considerations, risks,
and consequences, which so far remain under-explored. This paper examines how
record-keeping practices bring algorithmic systems within accountability
regimes, providing a basis to observe and understand their effects. For this,
we introduce, describe, and elaborate 'accountability capture' -- the
re-configuration of socio-technical processes and the associated downstream
effects relating to record-keeping for algorithmic accountability. Surveying
100 practitioners, we evidence and characterise record-keeping issues in
practice, identifying their alignment with accountability capture. We further
document widespread record-keeping practices, tensions between internal and
external accountability requirements, and evidence of employee resistance to
practices imposed through accountability capture. We discuss these and other
effects for surveillance, privacy, and data protection, highlighting
considerations for algorithmic accountability communities. In all, we show that
implementing record-keeping to support transparency in algorithmic
accountability regimes can itself bring wider implications -- an issue
requiring greater attention from practitioners, researchers, and policymakers
alike.

</details>


### [879] [A New Digital Divide? Coder Worldviews, the Slop Economy, and Democracy in the Age of AI](https://arxiv.org/abs/2510.04755)
*Jason Miklian,Kristian Hoelscher*

Main category: cs.CY

TL;DR: This paper analyzes the impact of software developers' ethics and workplace cultures on democratic technology, highlighting a new digital divide in information quality and advocating for ethical design and policy interventions.


<details>
  <summary>Details</summary>
Motivation: Investigate the tension between the democratic potential of digital technologies and their real-world impact, particularly through the perspectives of software developers and the emerging digital divide.

Method: A survey of software developers in Silicon Valley to understand how their ethics, workplace culture, and design choices influence democracy. The findings are critically analyzed in the context of the Slop Economy.

Result: Developers are aware of their products' influence, yet face ethical dilemmas and pressures that lead to design choices undermining democracy. The Slop Economy exacerbates the divide by exposing users to low-quality, AI-driven content.

Conclusion: Technological advancements need ethically informed design and targeted policies to enhance democratic values, addressing the digital divide and ensuring innovations support governance ideals.

Abstract: Digital technologies are transforming democratic life in conflicting ways.
This article bridges two perspectives to unpack these tensions. First, we
present an original survey of software developers in Silicon Valley,
interrogating how coder worldviews, ethics, and workplace cultures shape the
democratic potential and social impact of the technologies they build. Results
indicate that while most developers recognize the power of their products to
influence civil liberties and political discourse, they often face ethical
dilemmas and top-down pressures that can lead to design choices undermining
democratic ideals. Second, we critically investigate these findings in the
context of an emerging new digital divide, not of internet access but of
information quality. We interrogate the survey findings in the context of the
Slop Economy, in which billions of users unable to pay for high-quality content
experience an internet dominated by low-quality, AI-generated ad-driven
content. We find a reinforcing cycle between tech creator beliefs and the
digital ecosystems they spawn. We discuss implications for democratic
governance, arguing for more ethically informed design and policy interventions
to help bridge the digital divide to ensure that technological innovation
supports rather than subverts democratic values in the next chapter of the
digital age.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [880] [NS-Pep: De novo Peptide Design with Non-Standard Amino Acids](https://arxiv.org/abs/2510.03326)
*Tao Guo,Junbo Yin,Yu Wang,Xin Gao*

Main category: q-bio.BM

TL;DR: This paper presents NS-Pep, a framework for co-designing peptide sequences and structures with non-standard amino acids (NSAAs). It addresses challenges due to the underrepresentation of NSAAs using novel techniques, enhancing both design and folding tasks.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of existing peptide design methods which are tailored to standard amino acids and cannot effectively utilize NSAAs despite their potential for improved pharmacological properties.

Method: NS-Pep employs Residue Frequency-Guided Modification (RFGM) to handle the underrepresentation of NSAAs, Progressive Side-chain Perception (PSP) for better geometric modeling of NSAAs, and Interaction-Aware Weighting (IAW) to prioritize critical residues near the binding pocket.

Result: The proposed NS-Pep demonstrated a 6.23% improvement in sequence recovery rate, a 5.12% improvement in binding affinity, and outperformed AlphaFold3 by 17.76% in peptide folding tasks.

Conclusion: NS-Pep effectively addresses the challenges of NSAA-aware peptide design, achieving advancements in both sequence design and peptide folding, highlighting its potential to enhance pharmaceutical development.

Abstract: Peptide drugs incorporating non-standard amino acids (NSAAs) offer improved
binding affinity and improved pharmacological properties. However, existing
peptide design methods are limited to standard amino acids, leaving NSAA-aware
design largely unexplored. We introduce NS-Pep, a unified framework for
co-designing peptide sequences and structures with NSAAs. The main challenge is
that NSAAs are extremely underrepresented-even the most frequent one, SEP,
accounts for less than 0.4% of residues-resulting in a severe long-tailed
distribution. To improve generalization to rare amino acids, we propose Residue
Frequency-Guided Modification (RFGM), which mitigates over-penalization through
frequency-aware logit calibration, supported by both theoretical and empirical
analysis. Furthermore, we identify that insufficient side-chain modeling limits
geometric representation of NSAAs. To address this, we introduce Progressive
Side-chain Perception (PSP) for coarse-to-fine torsion and location prediction,
and Interaction-Aware Weighting (IAW) to emphasize pocket-proximal residues.
Moreover, NS-Pep generalizes naturally to the peptide folding task with NSAAs,
addressing a major limitation of current tools. Experiments show that NS-Pep
improves sequence recovery rate and binding affinity by 6.23% and 5.12%,
respectively, and outperforms AlphaFold3 by 17.76% in peptide folding success
rate.

</details>


<div id='cond-mat.dis-nn'></div>

# cond-mat.dis-nn [[Back]](#toc)

### [881] [Learning Linear Regression with Low-Rank Tasks in-Context](https://arxiv.org/abs/2510.04548)
*Kaito Takanami,Takashi Takahashi,Yoshiyuki Kabashima*

Main category: cond-mat.dis-nn

TL;DR: This paper studies the mechanisms of in-context learning (ICL) within a linear attention model, analyzing predictions, generalization error, and their behavior under task structures.


<details>
  <summary>Details</summary>
Motivation: To better understand the theoretical mechanisms of in-context learning (ICL) in tasks with a common structure, which currently lacks clarity in real-world applications.

Method: Analyzed a linear attention model trained on low-rank regression tasks, characterized prediction distributions and generalization error, investigated statistical fluctuations from pre-training, and identified a phase transition governed by task structure.

Result: Generalization error and prediction distributions were characterized, implicit regularization induced by finite pre-training data was discovered, and a phase transition driven by task structure was identified.

Conclusion: The findings build a theoretical framework to understand how transformers learn task structures in in-context learning scenarios, providing insight into their generalization and behavior.

Abstract: In-context learning (ICL) is a key building block of modern large language
models, yet its theoretical mechanisms remain poorly understood. It is
particularly mysterious how ICL operates in real-world applications where tasks
have a common structure. In this work, we address this problem by analyzing a
linear attention model trained on low-rank regression tasks. Within this
setting, we precisely characterize the distribution of predictions and the
generalization error in the high-dimensional limit. Moreover, we find that
statistical fluctuations in finite pre-training data induce an implicit
regularization. Finally, we identify a sharp phase transition of the
generalization error governed by task structure. These results provide a
framework for understanding how transformers learn to learn the task structure.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [882] [Predictive economics: Rethinking economic methodology with machine learning](https://arxiv.org/abs/2510.04726)
*Miguel Alves Pereira*

Main category: econ.GN

TL;DR: The paper introduces predictive economics, emphasizing machine learning's role and prioritizing predictive accuracy over causal identification.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to redefine economics analysis methodologies by emphasizing predictive accuracy in data-rich and complex situations, rather than solely focusing on causality.

Method: It builds from various analytical traditions to formalize prediction as a valid epistemological goal, leveraging machine learning in empirical analysis.

Result: Predictive models were reviewed and demonstrated to enhance empirical analysis, particularly under complex or data-intense circumstances.

Conclusion: The paper supports a pluralistic methodology in economics, balancing interpretation, theoretical structure, and out-of-sample performance via predictive approaches.

Abstract: This article proposes predictive economics as a distinct analytical
perspective within economics, grounded in machine learning and centred on
predictive accuracy rather than causal identification. Drawing on the
instrumentalist tradition (Friedman), the explanation-prediction divide
(Shmueli), and the contrast between modelling cultures (Breiman), we formalise
prediction as a valid epistemological and methodological objective. Reviewing
recent applications across economic subfields, we show how predictive models
contribute to empirical analysis, particularly in complex or data-rich
contexts. This perspective complements existing approaches and supports a more
pluralistic methodology - one that values out-of-sample performance alongside
interpretability and theoretical structure.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [883] [Pivotal CLTs for Pseudolikelihood via Conditional Centering in Dependent Random Fields](https://arxiv.org/abs/2510.04972)
*Nabarun Deb*

Main category: math.ST

TL;DR: The paper examines the convergence of conditionally centered statistics for dependent random fields, leading to Gaussian scale mixture limits. It develops an asymptotic framework for MPLE inference in several models, like Ising models and ERGMs.


<details>
  <summary>Details</summary>
Motivation: To address the need for general asymptotic frameworks to analyze and infer properties for dependent random fields, especially in models exhibiting dense and irregular interactions.

Method: The authors utilize a method of moments approach with combinatorial decision-tree pruning to analyze conditionally centered statistics. They apply this to the development of maximum pseudolikelihood estimation frameworks.

Result: They demonstrate convergence of the statistics to Gaussian scale mixtures with random scales and provide general inference results for dependent random fields, including central limit theorems for Ising models and ERGMs, even in dense, irregular domains.

Conclusion: The findings establish a novel theoretical framework for advanced statistical inference in dependent random fields, applying successfully to key models like Ising and exponential random graph models.

Abstract: In this paper, we study fluctuations of conditionally centered statistics of
the form $$N^{-1/2}\sum_{i=1}^N
c_i(g(\sigma_i)-\mathbb{E}_N[g(\sigma_i)|\sigma_j,j\neq i])$$ where
$(\sigma_1,\ldots ,\sigma_N)$ are sampled from a dependent random field, and
$g$ is some bounded function. Our first main result shows that under weak
smoothness assumptions on the conditional means (which cover both sparse and
dense interactions), the above statistic converges to a Gaussian \emph{scale
mixture} with a random scale determined by a \emph{quadratic variance} and an
\emph{interaction component}. We also show that under appropriate
studentization, the limit becomes a pivotal Gaussian. We leverage this theory
to develop a general asymptotic framework for maximum pseudolikelihood (MPLE)
inference in dependent random fields. We apply our results to Ising models with
pairwise as well as higher-order interactions and exponential random graph
models (ERGMs). In particular, we obtain a joint central limit theorem for the
inverse temperature and magnetization parameters via the joint MPLE (to our
knowledge, the first such result in dense, irregular regimes), and we derive
conditionally centered edge CLTs and marginal MPLE CLTs for ERGMs without
restricting to the ``sub-critical" region. Our proof is based on a method of
moments approach via combinatorial decision-tree pruning, which may be of
independent interest.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [884] [Paris: A Decentralized Trained Open-Weight Diffusion Model](https://arxiv.org/abs/2510.03434)
*Zhiying Jiang,Raihan Seraj,Marcos Villagra,Bidhan Roy*

Main category: cs.GR

TL;DR: The paper introduces Paris, a novel decentralized diffusion model for text-to-image generation that doesn’t rely on a centralized infrastructure, achieving efficiency in computational resources and training data.


<details>
  <summary>Details</summary>
Motivation: To explore and demonstrate the possibility of training high-quality diffusion models without relying on a central infrastructure or synchronous GPU clusters.

Method: Implemented a Distributed Diffusion Training framework where data is partitioned into semantic clusters, and eight expert models independently train their subsets. A transformer router selects the best-fit expert during inference.

Result: Paris matched the generation quality of baseline models while using significantly less training data and compute resources. Specifically, it achieved this with 14× less training data and 16× less computation.

Conclusion: Paris establishes a new paradigm for decentralized diffusion model training that removes the need for synchronized and specialized GPU clusters, paving the way for more accessible and resource-efficient methods.

Abstract: We present Paris, the first publicly released diffusion model pre-trained
entirely through decentralized computation. Paris demonstrates that
high-quality text-to-image generation can be achieved without centrally
coordinated infrastructure. Paris is open for research and commercial use.
Paris required implementing our Distributed Diffusion Training framework from
scratch. The model consists of 8 expert diffusion models (129M-605M parameters
each) trained in complete isolation with no gradient, parameter, or
intermediate activation synchronization. Rather than requiring synchronized
gradient updates across thousands of GPUs, we partition data into semantically
coherent clusters where each expert independently optimizes its subset while
collectively approximating the full distribution. A lightweight transformer
router dynamically selects appropriate experts at inference, achieving
generation quality comparable to centrally coordinated baselines. Eliminating
synchronization enables training on heterogeneous hardware without specialized
interconnects. Empirical validation confirms that Paris's decentralized
training maintains generation quality while removing the dedicated GPU cluster
requirement for large-scale diffusion models. Paris achieves this using
14$\times$ less training data and 16$\times$ less compute than the prior
decentralized baseline.

</details>


### [885] [Creative synthesis of kinematic mechanisms](https://arxiv.org/abs/2510.03308)
*Jiong Lin,Jialong Ning,Judah Goldfeder,Hod Lipson*

Main category: cs.GR

TL;DR: This paper treats kinematic synthesis for planar linkages as a cross-domain image generation task, leveraging a shared-latent VAE to design and simulate novel mechanisms using image-based representation.


<details>
  <summary>Details</summary>
Motivation: To redefine mechanical design by exploring generative approaches that use image representations, aiming to synthesize novel mechanisms.

Method: A shared-latent variational autoencoder encoding trajectory motion as RGB images and gradient color representations for kinematics with velocity profiles.

Result: Validation across datasets shows the model effectively synthesizes mechanisms, including revolute joints, prismatic joints, and complex configurations.

Conclusion: Unifying planar linkage synthesis with image generation opens pathways for designing mechanical systems through generative frameworks.

Abstract: In this paper, we formulate the problem of kinematic synthesis for planar
linkages as a cross-domain image generation task. We develop a planar linkages
dataset using RGB image representations, covering a range of mechanisms: from
simple types such as crank-rocker and crank-slider to more complex eight-bar
linkages like Jansen's mechanism. A shared-latent variational autoencoder (VAE)
is employed to explore the potential of image generative models for
synthesizing unseen motion curves and simulating novel kinematics. By encoding
the drawing speed of trajectory points as color gradients, the same
architecture also supports kinematic synthesis conditioned on both trajectory
shape and velocity profiles. We validate our method on three datasets of
increasing complexity: a standard four-bar linkage set, a mixed set of four-bar
and crank-slider mechanisms, and a complex set including multi-loop mechanisms.
Preliminary results demonstrate the effectiveness of image-based
representations for generative mechanical design, showing that mechanisms with
revolute and prismatic joints, and potentially cams and gears, can be
represented and synthesized within a unified image generation framework.

</details>


### [886] [Universal Beta Splatting](https://arxiv.org/abs/2510.03312)
*Rong Liu,Zhongpai Gao,Benjamin Planche,Meida Chen,Van Nguyen Nguyen,Meng Zheng,Anwesa Choudhuri,Terrence Chen,Yue Wang,Andrew Feng,Ziyan Wu*

Main category: cs.GR

TL;DR: The paper introduces Universal Beta Splatting (UBS), a framework extending Gaussian Splatting to flexible Beta kernels for rendering explicit radiance fields efficiently and with high accuracy.


<details>
  <summary>Details</summary>
Motivation: To create a unified and scalable approach for rendering explicit radiance fields that can model spatial, angular, and temporal dependencies without relying on auxiliary networks or complex encodings.

Method: Develops the Universal Beta Splatting (UBS) framework using N-dimensional Beta kernels to model light transport and scene properties, implemented with CUDA acceleration for real-time performance.

Result: UBS achieves superior performance across static, view-dependent, and dynamic radiance field benchmarks, retaining compatibility with Gaussian Splatting and demonstrating its scalability and interpretability.

Conclusion: Universal Beta Splatting is a versatile and efficient framework that simplifies radiance field rendering while offering high-quality results and enhanced interpretability across diverse benchmarks.

Abstract: We introduce Universal Beta Splatting (UBS), a unified framework that
generalizes 3D Gaussian Splatting to N-dimensional anisotropic Beta kernels for
explicit radiance field rendering. Unlike fixed Gaussian primitives, Beta
kernels enable controllable dependency modeling across spatial, angular, and
temporal dimensions within a single representation. Our unified approach
captures complex light transport effects, handles anisotropic view-dependent
appearance, and models scene dynamics without requiring auxiliary networks or
specific color encodings. UBS maintains backward compatibility by approximating
to Gaussian Splatting as a special case, guaranteeing plug-in usability and
lower performance bounds. The learned Beta parameters naturally decompose scene
properties into interpretable without explicit supervision: spatial (surface
vs. texture), angular (diffuse vs. specular), and temporal (static vs.
dynamic). Our CUDA-accelerated implementation achieves real-time rendering
while consistently outperforming existing methods across static,
view-dependent, and dynamic benchmarks, establishing Beta kernels as a scalable
universal primitive for radiance field rendering. Our project website is
available at https://rongliu-leo.github.io/universal-beta-splatting/.

</details>


### [887] [Diverse Text-to-Image Generation via Contrastive Noise Optimization](https://arxiv.org/abs/2510.03813)
*Byungjun Kim,Soobin Um,Jong Chul Ye*

Main category: cs.GR

TL;DR: This paper introduces Contrastive Noise Optimization to enhance image diversity in text-to-image diffusion models by optimizing initial noise rather than intermediate latents.


<details>
  <summary>Details</summary>
Motivation: Text-to-image diffusion models generate high-quality images but suffer from limited output diversity, particularly under strong text guidance, which existing methods address only modestly or with sensitivity to hyperparameters.

Method: The method employs Contrastive Noise Optimization, utilizing a contrastive loss in the Tweedie data space to optimize initial noise latents, ensuring diversity through batch-level instance repulsion while preserving fidelity through anchoring.

Result: Experiments across multiple T2I backbones show that the proposed approach achieves better performance in balancing quality and diversity and is more robust to hyperparameter choices.

Conclusion: This work offers a novel and effective solution for improving diversity in T2I diffusion models, delivering both theoretical insights and practical robustness.

Abstract: Text-to-image (T2I) diffusion models have demonstrated impressive performance
in generating high-fidelity images, largely enabled by text-guided inference.
However, this advantage often comes with a critical drawback: limited
diversity, as outputs tend to collapse into similar modes under strong text
guidance. Existing approaches typically optimize intermediate latents or text
conditions during inference, but these methods deliver only modest gains or
remain sensitive to hyperparameter tuning. In this work, we introduce
Contrastive Noise Optimization, a simple yet effective method that addresses
the diversity issue from a distinct perspective. Unlike prior techniques that
adapt intermediate latents, our approach shapes the initial noise to promote
diverse outputs. Specifically, we develop a contrastive loss defined in the
Tweedie data space and optimize a batch of noise latents. Our contrastive
optimization repels instances within the batch to maximize diversity while
keeping them anchored to a reference sample to preserve fidelity. We further
provide theoretical insights into the mechanism of this preprocessing to
substantiate its effectiveness. Extensive experiments across multiple T2I
backbones demonstrate that our approach achieves a superior quality-diversity
Pareto frontier while remaining robust to hyperparameter choices.

</details>


### [888] [Joint Neural SDF Reconstruction and Semantic Segmentation for CAD Models](https://arxiv.org/abs/2510.03837)
*Shen Fan,Przemyslaw Musialski*

Main category: cs.GR

TL;DR: This paper introduces a simple and efficient pipeline combining neural SDF-based CAD reconstructions with a segmentation head for geometry-part segmentation without fixed taxonomies.


<details>
  <summary>Details</summary>
Motivation: The motivation is to allow CAD models with varying part counts to be semantically structured and segmented without relying on fixed taxonomies or curated label palettes.

Method: The method involves attaching a segmentation head to a neural SDF-based CAD trunk, trained using PartField-generated supervision to produce geometry-aligned labels with no restrictions on part cardinalities.

Result: Strong performance is reported in both reconstruction and segmentation, with metrics including CDL1/CDL2, F1-micro, NC, mIoU, and Accuracy. A novel Segmentation Consistency metric is introduced to measure label smoothness.

Conclusion: The approach offers accurate part segmentation even for complex geometries, without altering reconstruction quality. However, it faces challenges in boundary precision and offers potential improvements in higher resolution labeling and training approaches.

Abstract: We propose a simple, data-efficient pipeline that augments an implicit
reconstruction network based on neural SDF-based CAD parts with a
part-segmentation head trained under PartField-generated supervision. Unlike
methods tied to fixed taxonomies, our model accepts meshes with any number of
parts and produces coherent, geometry-aligned labels in a single pass. We
evaluate on randomly sampled CAD meshes from the ABC dataset with intentionally
varied part cardinalities, including over-segmented shapes, and report strong
performance across reconstruction (CDL1/CDL2, F1-micro, NC) and segmentation
(mIoU, Accuracy), together with a new Segmentation Consistency metric that
captures local label smoothness. We attach a lightweight segmentation head to
the Flat-CAD SDF trunk; on a paired evaluation it does not alter reconstruction
while providing accurate part labels for meshes with any number of parts. Even
under degraded reconstructions on thin or intricate geometries, segmentation
remains accurate and label-coherent, often preserving the correct part count.
Our approach therefore offers a practical route to semantically structured CAD
meshes without requiring curated taxonomies or exact palette matches. We
discuss limitations in boundary precision, partly due to per-face supervision,
and outline paths toward boundary-aware training and higher resolution labels.

</details>


### [889] [Neon: Negative Extrapolation From Self-Training Improves Image Generation](https://arxiv.org/abs/2510.03597)
*Sina Alemohammad,Zhangyang Wang,Richard G. Baraniuk*

Main category: cs.GR

TL;DR: The paper introduces Neon, a method to counteract the degradation caused by using synthetic data in training AI models, achieving better alignment with true data distributions.


<details>
  <summary>Details</summary>
Motivation: The scarcity of high-quality training data limits scaling generative AI models. Synthetic data could help, but its usage often leads to model collapse (MAD).

Method: Neon fine-tunes models on synthetic data, reverses gradient updates to correct degradation, leveraging anti-alignment between real and synthetic gradients to improve alignment with true data.

Result: Neon improves performance on multiple model architectures and datasets, achieving state-of-the-art results with minimal additional compute.

Conclusion: Neon offers a scalable, efficient solution to counteract self-training degradation, demonstrating broad applicability across generative AI models with minimal resources.

Abstract: Scaling generative AI models is bottlenecked by the scarcity of high-quality
training data. The ease of synthesizing from a generative model suggests using
(unverified) synthetic data to augment a limited corpus of real data for the
purpose of fine-tuning in the hope of improving performance. Unfortunately,
however, the resulting positive feedback loop leads to model autophagy disorder
(MAD, aka model collapse) that results in a rapid degradation in sample quality
and/or diversity. In this paper, we introduce Neon (for Negative Extrapolation
frOm self-traiNing), a new learning method that turns the degradation from
self-training into a powerful signal for self-improvement. Given a base model,
Neon first fine-tunes it on its own self-synthesized data but then,
counterintuitively, reverses its gradient updates to extrapolate away from the
degraded weights. We prove that Neon works because typical inference samplers
that favor high-probability regions create a predictable anti-alignment between
the synthetic and real data population gradients, which negative extrapolation
corrects to better align the model with the true data distribution. Neon is
remarkably easy to implement via a simple post-hoc merge that requires no new
real data, works effectively with as few as 1k synthetic samples, and typically
uses less than 1% additional training compute. We demonstrate Neon's
universality across a range of architectures (diffusion, flow matching,
autoregressive, and inductive moment matching models) and datasets (ImageNet,
CIFAR-10, and FFHQ). In particular, on ImageNet 256x256, Neon elevates the
xAR-L model to a new state-of-the-art FID of 1.02 with only 0.36% additional
training compute. Code is available at https://github.com/SinaAlemohammad/Neon

</details>


### [890] [3Dify: a Framework for Procedural 3D-CG Generation Assisted by LLMs Using MCP and RAG](https://arxiv.org/abs/2510.04536)
*Shun-ichiro Hayashi,Daichi Mukunoki,Tetsuya Hoshino,Satoshi Ohshima,Takahiro Katagiri*

Main category: cs.GR

TL;DR: The paper introduces "3Dify," a framework leveraging Large Language Models (LLMs) to enable procedural generation of 3D computer graphics using natural language instructions.


<details>
  <summary>Details</summary>
Motivation: The motivation is to streamline the creation of 3D computer graphics by utilizing LLMs, minimizing user effort, and overcoming limitations in existing Digital Content Creation processes.

Method: 3Dify leverages technologies like MCP, RAG, and automates DCC tools using protocols or GUI automation. It incorporates user feedback for image enhancement and supports locally deployed LLMs.

Result: The framework effectively generates 3D graphics using natural language input, adapts to user preferences, supports custom LLM integration, and reduces dependency on external APIs.

Conclusion: 3Dify represents a significant advancement in 3D-CG creation, demonstrating the utility of LLM-based frameworks in enhancing efficiency and customization.

Abstract: This paper proposes "3Dify," a procedural 3D computer graphics (3D-CG)
generation framework utilizing Large Language Models (LLMs). The framework
enables users to generate 3D-CG content solely through natural language
instructions. 3Dify is built upon Dify, an open-source platform for AI
application development, and incorporates several state-of-the-art LLM-related
technologies such as the Model Context Protocol (MCP) and Retrieval-Augmented
Generation (RAG). For 3D-CG generation support, 3Dify automates the operation
of various Digital Content Creation (DCC) tools via MCP. When DCC tools do not
support MCP-based interaction, the framework employs the Computer-Using Agent
(CUA) method to automate Graphical User Interface (GUI) operations. Moreover,
to enhance image generation quality, 3Dify allows users to provide feedback by
selecting preferred images from multiple candidates. The LLM then learns
variable patterns from these selections and applies them to subsequent
generations. Furthermore, 3Dify supports the integration of locally deployed
LLMs, enabling users to utilize custom-developed models and to reduce both time
and monetary costs associated with external API calls by leveraging their own
computational resources.

</details>


### [891] [C3Editor: Achieving Controllable Consistency in 2D Model for 3D Editing](https://arxiv.org/abs/2510.04539)
*Zeng Tao,Zheng Ding,Zeyuan Chen,Xiang Zhang,Leizhi Li,Zhuowen Tu*

Main category: cs.GR

TL;DR: This paper introduces C3Editor, a 2D-lifting-based 3D editing framework addressing view-consistency challenges for improved 3D editing results.


<details>
  <summary>Details</summary>
Motivation: Existing 2D-lifting-based 3D editors struggle to maintain consistency across views due to lack of consistent 2D editing models and cross-view consistency mechanisms.

Method: The method uses a ground truth view, edited images, and LoRA modules to establish consistency and fine-tune 2D models across views.

Result: C3Editor achieves more controllable and consistent 2D- and 3D-editing results, outperforming existing methods in qualitative and quantitative tests.

Conclusion: The framework resolves 3D editing consistency issues and sets a higher benchmark for 2D-lifting-based approaches.

Abstract: Existing 2D-lifting-based 3D editing methods often encounter challenges
related to inconsistency, stemming from the lack of view-consistent 2D editing
models and the difficulty of ensuring consistent editing across multiple views.
To address these issues, we propose C3Editor, a controllable and consistent
2D-lifting-based 3D editing framework. Given an original 3D representation and
a text-based editing prompt, our method selectively establishes a
view-consistent 2D editing model to achieve superior 3D editing results. The
process begins with the controlled selection of a ground truth (GT) view and
its corresponding edited image as the optimization target, allowing for
user-defined manual edits. Next, we fine-tune the 2D editing model within the
GT view and across multiple views to align with the GT-edited image while
ensuring multi-view consistency. To meet the distinct requirements of GT view
fitting and multi-view consistency, we introduce separate LoRA modules for
targeted fine-tuning. Our approach delivers more consistent and controllable 2D
and 3D editing results than existing 2D-lifting-based methods, outperforming
them in both qualitative and quantitative evaluations.

</details>


### [892] [Social Agent: Mastering Dyadic Nonverbal Behavior Generation via Conversational LLM Agents](https://arxiv.org/abs/2510.04637)
*Zeyi Zhang,Yanju Zhou,Heyuan Yao,Tenglong Ao,Xiaohang Zhan,Libin Liu*

Main category: cs.GR

TL;DR: This paper introduces Social Agent, a framework for generating realistic co-speech nonverbal behaviors in two-person conversations using an LLM and a novel gesture generation model.


<details>
  <summary>Details</summary>
Motivation: Achieving realistic and coordinated nonverbal behaviors in dyadic conversations for improved interaction quality.

Method: An LLM-driven agentic system directs dialogue flow and behaviors, combined with a novel dual-person gesture generation model using an auto-regressive diffusion mechanism.

Result: Studies show that the model improves dyadic interaction quality through natural and synchronized gestures and nonverbal communication.

Conclusion: Social Agent demonstrates effectiveness in generating contextually appropriate and dynamic nonverbal behaviors, enhancing conversational realism.

Abstract: We present Social Agent, a novel framework for synthesizing realistic and
contextually appropriate co-speech nonverbal behaviors in dyadic conversations.
In this framework, we develop an agentic system driven by a Large Language
Model (LLM) to direct the conversation flow and determine appropriate
interactive behaviors for both participants. Additionally, we propose a novel
dual-person gesture generation model based on an auto-regressive diffusion
model, which synthesizes coordinated motions from speech signals. The output of
the agentic system is translated into high-level guidance for the gesture
generator, resulting in realistic movement at both the behavioral and motion
levels. Furthermore, the agentic system periodically examines the movements of
interlocutors and infers their intentions, forming a continuous feedback loop
that enables dynamic and responsive interactions between the two participants.
User studies and quantitative evaluations show that our model significantly
improves the quality of dyadic interactions, producing natural, synchronized
nonverbal behaviors.

</details>


### [893] [Bridging Text and Video Generation: A Survey](https://arxiv.org/abs/2510.04999)
*Nilay Kumar,Priyansh Bhandari,G. Maragatham*

Main category: cs.GR

TL;DR: The paper surveys advancements in text-to-video (T2V) generation, from GANs to diffusion-based models, highlighting their methodologies, datasets, benchmarks, challenges, and future directions.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive overview of the evolution, challenges, and opportunities in text-to-video generation systems, emphasizing their potential applications in various domains.

Method: A systematic review of T2V models, analyzing their architectures, training configurations, datasets, evaluation metrics, and benchmarks.

Result: A detailed comparison of T2V models' performance and limitations, as well as an analysis of emerging evaluation strategies for more holistic assessments.

Conclusion: The study identifies open challenges in T2V research and proposes actionable future directions to advance the field and enhance applications across various industries.

Abstract: Text-to-video (T2V) generation technology holds potential to transform
multiple domains such as education, marketing, entertainment, and assistive
technologies for individuals with visual or reading comprehension challenges,
by creating coherent visual content from natural language prompts. From its
inception, the field has advanced from adversarial models to diffusion-based
models, yielding higher-fidelity, temporally consistent outputs. Yet challenges
persist, such as alignment, long-range coherence, and computational efficiency.
Addressing this evolving landscape, we present a comprehensive survey of
text-to-video generative models, tracing their development from early GANs and
VAEs to hybrid Diffusion-Transformer (DiT) architectures, detailing how these
models work, what limitations they addressed in their predecessors, and why
shifts toward new architectural paradigms were necessary to overcome challenges
in quality, coherence, and control. We provide a systematic account of the
datasets, which the surveyed text-to-video models were trained and evaluated
on, and, to support reproducibility and assess the accessibility of training
such models, we detail their training configurations, including their hardware
specifications, GPU counts, batch sizes, learning rates, optimizers, epochs,
and other key hyperparameters. Further, we outline the evaluation metrics
commonly used for evaluating such models and present their performance across
standard benchmarks, while also discussing the limitations of these metrics and
the emerging shift toward more holistic, perception-aligned evaluation
strategies. Finally, drawing from our analysis, we outline the current open
challenges and propose a few promising future directions, laying out a
perspective for future researchers to explore and build upon in advancing T2V
research and applications.

</details>


### [894] [SAEdit: Token-level control for continuous image editing via Sparse AutoEncoder](https://arxiv.org/abs/2510.05081)
*Ronen Kamenetsky,Sara Dorfman,Daniel Garibi,Roni Paiss,Or Patashnik,Daniel Cohen-Or*

Main category: cs.GR

TL;DR: The paper presents a method for better control in text-to-image diffusion models using token-level manipulation of text embeddings, enabling disentangled and continuous editing.


<details>
  <summary>Details</summary>
Motivation: Modern image editing with text-to-image diffusion models lacks adequate control, particularly in achieving disentanglement of attributes and providing continuous adjustments during editing.

Method: The method manipulates text embeddings along selected directions identified by a Sparse Autoencoder (SAE), which isolates semantically meaningful dimensions. This approach is model-agnostic and doesn’t alter the diffusion process.

Result: The technique allows for intuitive and effective attribute control across various domains, enabling disentangled and smoothly adjustable edits.

Conclusion: The proposed approach significantly enhances the editing process for text-to-image diffusion models, offering both precision and flexibility.

Abstract: Large-scale text-to-image diffusion models have become the backbone of modern
image editing, yet text prompts alone do not offer adequate control over the
editing process. Two properties are especially desirable: disentanglement,
where changing one attribute does not unintentionally alter others, and
continuous control, where the strength of an edit can be smoothly adjusted. We
introduce a method for disentangled and continuous editing through token-level
manipulation of text embeddings. The edits are applied by manipulating the
embeddings along carefully chosen directions, which control the strength of the
target attribute. To identify such directions, we employ a Sparse Autoencoder
(SAE), whose sparse latent space exposes semantically isolated dimensions. Our
method operates directly on text embeddings without modifying the diffusion
process, making it model agnostic and broadly applicable to various image
synthesis backbones. Experiments show that it enables intuitive and efficient
manipulations with continuous control across diverse attributes and domains.

</details>


### [895] [Pulp Motion: Framing-aware multimodal camera and human motion generation](https://arxiv.org/abs/2510.05097)
*Robin Courant,Xi Wang,David Loiseaux,Marc Christie,Vicky Kalogeiton*

Main category: cs.GR

TL;DR: This paper introduces a unified text-conditioned approach for generating human motion and camera trajectories that maintain consistent on-screen framing.


<details>
  <summary>Details</summary>
Motivation: Previous methods treat human motion generation and camera trajectory generation separately, overlooking their intrinsic interdependence needed for high-quality cinematographic work.

Method: The authors propose a model-agnostic framework using an auxiliary modality—on-screen framing—by designing a joint autoencoder and introducing auxiliary sampling techniques to ensure multimodal coherence.

Result: The proposed method demonstrates improved consistency and precision for human and camera motions, achieving better textual alignment and setting state-of-the-art performance in generating cinematographically meaningful framings.

Conclusion: The method offers a novel framework that successfully integrates human motion and camera trajectory generation, enhancing multimodal coherence and advancing cinematographic standards in this domain.

Abstract: Treating human motion and camera trajectory generation separately overlooks a
core principle of cinematography: the tight interplay between actor performance
and camera work in the screen space. In this paper, we are the first to cast
this task as a text-conditioned joint generation, aiming to maintain consistent
on-screen framing while producing two heterogeneous, yet intrinsically linked,
modalities: human motion and camera trajectories. We propose a simple,
model-agnostic framework that enforces multimodal coherence via an auxiliary
modality: the on-screen framing induced by projecting human joints onto the
camera. This on-screen framing provides a natural and effective bridge between
modalities, promoting consistency and leading to more precise joint
distribution. We first design a joint autoencoder that learns a shared latent
space, together with a lightweight linear transform from the human and camera
latents to a framing latent. We then introduce auxiliary sampling, which
exploits this linear transform to steer generation toward a coherent framing
modality. To support this task, we also introduce the PulpMotion dataset, a
human-motion and camera-trajectory dataset with rich captions, and high-quality
human motions. Extensive experiments across DiT- and MAR-based architectures
show the generality and effectiveness of our method in generating on-frame
coherent human-camera motions, while also achieving gains on textual alignment
for both modalities. Our qualitative results yield more cinematographically
meaningful framings setting the new state of the art for this task. Code,
models and data are available in our
\href{https://www.lix.polytechnique.fr/vista/projects/2025_pulpmotion_courant/}{project
page}.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [896] [Perspectives on Stochastic Localization](https://arxiv.org/abs/2510.04460)
*Bobby Shi,Kevin Tian,Matthew S. Zhang*

Main category: math.PR

TL;DR: This paper surveys various perspectives on Eldan's stochastic localization, emphasizing its alternative constructions and interconnections.


<details>
  <summary>Details</summary>
Motivation: To facilitate greater accessibility and future utilization of stochastic localization across disciplines by presenting its different constructions systematically.

Method: The authors provide a self-contained review connecting all known alternative constructions of Eldan’s stochastic localization across fields like probability theory and algorithm design.

Result: The paper identifies and connects several perspectives on stochastic localization, making the knowledge more unified and accessible for broader applications.

Conclusion: By unifying alternative approaches to stochastic localization, the study broadens its accessibility, fostering further research and interdisciplinary applications.

Abstract: We survey different perspectives on the stochastic localization process of
[Eld13], a powerful construction that has had many exciting recent applications
in high-dimensional probability and algorithm design. Unlike prior surveys on
this topic, our focus is on giving a self-contained presentation of all known
alternative constructions of Eldan's stochastic localization, with an emphasis
on connections between different constructions. Our hope is that by collecting
these perspectives, some of which had primarily arisen within a particular
community (e.g., probability theory, theoretical computer science, information
theory, or machine learning), we can broaden the accessibility of stochastic
localization, and ease its future use.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [897] [Fair Minimum Labeling: Efficient Temporal Network Activations for Reachability and Equity](https://arxiv.org/abs/2510.03899)
*Lutz Oettershagen,Othon Michail*

Main category: cs.SI

TL;DR: The paper introduces the Fair Minimum Labeling (FML) problem to balance resource efficiency and fairness in networked systems. It provides algorithms to minimize costs while ensuring equitable access requirements for groups of nodes in a network.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the trade-off between resource costs and equitable access in networked systems, which is critical for applications like distributed data collection, edge-cloud updates, and service restoration.

Method: The authors define the FML problem, prove its complexity, and propose probabilistic approximation algorithms to minimize the edge activation cost while meeting group fairness coverage requirements.

Result: Empirical results show that their approach achieves significant cost savings over baseline heuristics while enforcing fairness in multi-source data aggregation applications.

Conclusion: FML is a feasible and efficient solution for resource-efficient, fair networks, making it valuable for learning-integrated and distributed systems.

Abstract: Balancing resource efficiency and fairness is critical in networked systems
that support modern learning applications. We introduce the Fair Minimum
Labeling (FML) problem: the task of designing a minimum-cost temporal edge
activation plan that ensures each group of nodes in a network has sufficient
access to a designated target set, according to specified coverage
requirements. FML captures key trade-offs in systems where edge activations
incur resource costs and equitable access is essential, such as distributed
data collection, update dissemination in edge-cloud systems, and fair service
restoration in critical infrastructure. We show that FML is NP-hard and
$\Omega(\log |V|)$-hard to approximate, and we present probabilistic
approximation algorithms that match this bound, achieving the best possible
guarantee for the activation cost. We demonstrate the practical utility of FML
in a fair multi-source data aggregation task for training a shared model.
Empirical results show that FML enforces group-level fairness with
substantially lower activation cost than baseline heuristics, underscoring its
potential for building resource-efficient, equitable temporal reachability in
learning-integrated networks.

</details>


### [898] [Deep learning framework for predicting stochastic take-off and die-out of early spreading](https://arxiv.org/abs/2510.04574)
*Wenchao He,Tao Jia*

Main category: cs.SI

TL;DR: The paper introduces a novel deep learning framework for forecasting the progression of early-stage outbreaks, predicting whether they will escalate into epidemics or fade out. This approach addresses sparse early-stage data challenges and validates its efficacy across different network types and scenarios.


<details>
  <summary>Details</summary>
Motivation: Address the critical gap in forecasting whether emerging outbreaks will escalate into epidemics or naturally die out, especially given the limitations in data and the stochastic nature of transmission chains.

Method: Develop a deep learning framework using stochastic spreading models for real-time prediction, combined with a pretrain-finetune approach to handle sparse early-stage data.

Result: The framework accurately predicts outbreak outcomes across various network structures and infectivity levels, demonstrating robust performance. The pretrain-finetune technique enables high accuracy even with limited scenario-specific data.

Conclusion: This systematic approach provides actionable insights for epidemic preparedness and early-stage intervention strategies, marking a significant advancement in public health forecasting.

Abstract: Large-scale outbreaks of epidemics, misinformation, or other harmful
contagions pose significant threats to human society, yet the fundamental
question of whether an emerging outbreak will escalate into a major epidemic or
naturally die out remains largely unaddressed. This problem is challenging,
partially due to inadequate data during the early stages of outbreaks and also
because established models focus on average behaviors of large epidemics rather
than the stochastic nature of small transmission chains. Here, we introduce the
first systematic framework for forecasting whether initial transmission events
will amplify into major outbreaks or fade into extinction during early stages,
when intervention strategies can still be effectively implemented. Using
extensive data from stochastic spreading models, we developed a deep learning
framework that predicts early-stage spreading outcomes in real-time. Validation
across Erd\H{o}s-R\'enyi and Barab\'asi-Albert networks with varying
infectivity levels shows our method accurately forecasts stochastic spreading
events well before potential outbreaks, demonstrating robust performance across
different network structures and infectivity scenarios.To address the challenge
of sparse data during early outbreak stages, we further propose a
pretrain-finetune framework that leverages diverse simulation data for
pretraining and adapts to specific scenarios through targeted fine-tuning. The
pretrain-finetune framework consistently outperforms baseline models, achieving
superior performance even when trained on limited scenario-specific data. To
our knowledge, this work presents the first framework for predicting stochastic
take-off versus die-out. This framework provides valuable insights for epidemic
preparedness and public health decision-making, enabling more informed early
intervention strategies.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [899] [InstructPLM-mu: 1-Hour Fine-Tuning of ESM2 Beats ESM3 in Protein Mutation Predictions](https://arxiv.org/abs/2510.03370)
*Junde Xu,Yapin Shi,Lijun Lang,Taoyong Cui,Zhiming Zhang,Guangyong Chen,Jiezhong Qiu,Pheng-Ann Heng*

Main category: q-bio.QM

TL;DR: The paper introduces InstructPLM-mu, a fine-tuning framework for protein language models, demonstrating that fine-tuning with structural inputs can match state-of-the-art models without needing to train from scratch.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the computational challenges of training multimodal protein language models from scratch by exploring if fine-tuning pretrained models can achieve similar performance.

Method: The authors fine-tune the ESM2 protein language model with structural inputs using three different feature-fusion designs and fine-tuning strategies.

Result: Experiments showed fine-tuning ESM2 with structural inputs achieves performance comparable to ESM3, with fusion methods and fine-tuning strategies playing critical roles.

Conclusion: Fine-tuning structural inputs into pretrained models is effective, offering practical insights into optimizing fusion methods and fine-tuning protocols for future protein model research.

Abstract: Multimodal protein language models deliver strong performance on
mutation-effect prediction, but training such models from scratch demands
substantial computational resources. In this paper, we propose a fine-tuning
framework called InstructPLM-mu and try to answer a question: \textit{Can
multimodal fine-tuning of a pretrained, sequence-only protein language model
match the performance of models trained end-to-end? } Surprisingly, our
experiments show that fine-tuning ESM2 with structural inputs can reach
performance comparable to ESM3. To understand how this is achieved, we
systematically compare three different feature-fusion designs and fine-tuning
recipes. Our results reveal that both the fusion method and the tuning strategy
strongly affect final accuracy, indicating that the fine-tuning process is not
trivial. We hope this work offers practical guidance for injecting structure
into pretrained protein language models and motivates further research on
better fusion mechanisms and fine-tuning protocols.

</details>


### [900] [TCR-EML: Explainable Model Layers for TCR-pMHC Prediction](https://arxiv.org/abs/2510.04377)
*Jiarui Li,Zixiang Yin,Zhengming Ding,Samuel J. Landry,Ramgopal R. Mettu*

Main category: q-bio.QM

TL;DR: The study develops explainable model layers (TCR-EML) for TCR-pMHC binding prediction, integrating biological mechanisms for improved insights while maintaining predictive performance.


<details>
  <summary>Details</summary>
Motivation: TCR recognition of pMHC complexes is crucial for adaptive immunity but current prediction models lack interpretability, making it difficult to rationalize their outputs.

Method: The researchers introduce explainable model layers (TCR-EML) that embed prototype layers reflecting known TCR-pMHC binding mechanisms, coupled with protein-language model backbones.

Result: The proposed method achieves competitive accuracy and generalization on large datasets, along with superior explainability on the TCR-XAI benchmark compared to existing methods.

Conclusion: TCR-EML provides an interpretable alternative for TCR-pMHC modeling without compromising performance, potentially benefiting adaptive immunity-related applications like vaccines and immunotherapies.

Abstract: T cell receptor (TCR) recognition of peptide-MHC (pMHC) complexes is a
central component of adaptive immunity, with implications for vaccine design,
cancer immunotherapy, and autoimmune disease. While recent advances in machine
learning have improved prediction of TCR-pMHC binding, the most effective
approaches are black-box transformer models that cannot provide a rationale for
predictions. Post-hoc explanation methods can provide insight with respect to
the input but do not explicitly model biochemical mechanisms (e.g. known
binding regions), as in TCR-pMHC binding. ``Explain-by-design'' models (i.e.,
with architectural components that can be examined directly after training)
have been explored in other domains, but have not been used for TCR-pMHC
binding. We propose explainable model layers (TCR-EML) that can be incorporated
into protein-language model backbones for TCR-pMHC modeling. Our approach uses
prototype layers for amino acid residue contacts drawn from known TCR-pMHC
binding mechanisms, enabling high-quality explanations for predicted TCR-pMHC
binding. Experiments of our proposed method on large-scale datasets demonstrate
competitive predictive accuracy and generalization, and evaluation on the
TCR-XAI benchmark demonstrates improved explainability compared with existing
approaches.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [901] [Exact and Approximate MCMC for Doubly-intractable Probabilistic Graphical Models Leveraging the Underlying Independence Model](https://arxiv.org/abs/2510.03587)
*Yujie Chen,Antik Chakraborty,Anindya Bhadra*

Main category: stat.CO

TL;DR: This paper introduces an innovative method for Bayesian inference in doubly-intractable probabilistic graphical models, eliminating the need for perfect or sequential samplers by leveraging finite sample unbiased Monte Carlo estimates.


<details>
  <summary>Details</summary>
Motivation: Current methods for Bayesian inference in doubly-intractable probabilistic graphical models suffer from limitations such as poor mixing or dependency on unavailable perfect or sequential samplers.

Method: The proposed method uses tractable independence models to construct finite sample unbiased Monte Carlo estimates of the Metropolis-Hastings ratio, improving scalability in high dimensions.

Result: The method is successfully demonstrated on the Ising model, with further exploration of gradient-based alternatives such as Langevin and Hamiltonian Monte Carlo approaches.

Conclusion: This novel approach enhances scalability and addresses key limitations in Bayesian inference for complex probabilistic graphical models, offering practical solutions for high-dimensional scenarios.

Abstract: Bayesian inference for doubly-intractable probabilistic graphical models
typically involves variations of the exchange algorithm or approximate Markov
chain Monte Carlo (MCMC) samplers. However, existing methods for both classes
of algorithms require either perfect samplers or sequential samplers for
complex models, which are often either not available, or suffer from poor
mixing, especially in high dimensions. We develop a method that does not
require perfect or sequential sampling, and can be applied to both classes of
methods: exact and approximate MCMC. The key to our approach is to utilize the
tractable independence model underlying an intractable probabilistic graphical
model for the purpose of constructing a finite sample unbiased Monte Carlo (and
not MCMC) estimate of the Metropolis--Hastings ratio. This innovation turns out
to be crucial for scalability in high dimensions. The method is demonstrated on
the Ising model. Gradient-based alternatives to construct a proposal, such as
Langevin and Hamiltonian Monte Carlo approaches, also arise as a natural
corollary to our general procedure, and are demonstrated as well.

</details>


### [902] [Analysis of kinetic Langevin Monte Carlo under the stochastic exponential Euler discretization from underdamped all the way to overdamped](https://arxiv.org/abs/2510.03949)
*Kyurae Kim,Samuel Gruffaz,Ji Won Park,Alain Oliviero Durmus*

Main category: stat.CO

TL;DR: This paper revisits the exponential integrator method for simulating kinetic Langevin dynamics and refines its analysis to show stability even in the overdamped regime with proper adjustments.


<details>
  <summary>Details</summary>
Motivation: Existing analyses of the kinetic Langevin Monte Carlo (KLMC) fail to explain its behavior for various parameter choices and suggest instability in the overdamped regime.

Method: The authors use refined synchronous Wasserstein coupling analysis to study the stochastic exponential Euler discretization used in KLMC.

Result: The refined analysis shows Wasserstein contraction and asymptotic bias bounds under weaker parameter restrictions, demonstrating stability of the exponential integrator in the overdamped regime with time acceleration.

Conclusion: Contrary to prior assumptions, the exponential integrator can stably simulate kinetic Langevin dynamics in overdamped conditions with appropriate parameter adjustments.

Abstract: Simulating the kinetic Langevin dynamics is a popular approach for sampling
from distributions, where only their unnormalized densities are available.
Various discretizations of the kinetic Langevin dynamics have been considered,
where the resulting algorithm is collectively referred to as the kinetic
Langevin Monte Carlo (KLMC) or underdamped Langevin Monte Carlo. Specifically,
the stochastic exponential Euler discretization, or exponential integrator for
short, has previously been studied under strongly log-concave and log-Lipschitz
smooth potentials via the synchronous Wasserstein coupling strategy. Existing
analyses, however, impose restrictions on the parameters that do not explain
the behavior of KLMC under various choices of parameters. In particular, all
known results fail to hold in the overdamped regime, suggesting that the
exponential integrator degenerates in the overdamped limit. In this work, we
revisit the synchronous Wasserstein coupling analysis of KLMC with the
exponential integrator. Our refined analysis results in Wasserstein
contractions and bounds on the asymptotic bias that hold under weaker
restrictions on the parameters, which assert that the exponential integrator is
capable of stably simulating the kinetic Langevin dynamics in the overdamped
regime, as long as proper time acceleration is applied.

</details>


### [903] [spd-metrics-id: A Python Package for SPD-Aware Distance Metrics in Connectome Fingerprinting and Beyond](https://arxiv.org/abs/2510.04438)
*Kaosar Uddin*

Main category: stat.CO

TL;DR: spd-metrics-id is a Python package that offers a unified and extensible framework for computing distances and divergences between symmetric positive-definite matrices using various geometry-aware metrics.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the need for a reproducible and versatile tool for SPD matrix comparisons across various domains, as traditional toolkits focus only on specific applications.

Method: The package provides diverse geometry-aware metrics for SPD matrix comparisons, supports both command-line and Python API, and ensures reproducibility via Docker images and Zenodo archiving.

Result: Users have access to a broad range of metrics for SPD matrix computations applicable to fields like connectome fingerprinting, covariance analysis, and diffusion tensor imaging.

Conclusion: This Python package fills a gap by offering a unified, user-friendly, and reproducible solution for SPD matrix analysis with broad applicability across domains.

Abstract: We present spd-metrics-id, a Python package for computing distances and
divergences between symmetric positive-definite (SPD) matrices. Unlike
traditional toolkits that focus on specific applications, spd-metrics-id
provides a unified, extensible, and reproducible framework for SPD distance
computation. The package supports a wide variety of geometry-aware metrics,
including Alpha-z Bures-Wasserstein, Alpha-Procrustes, affine-invariant
Riemannian, log-Euclidean, and others, and is accessible both via a
command-line interface and a Python API. Reproducibility is ensured through
Docker images and Zenodo archiving. We illustrate usage through a connectome
fingerprinting example, but the package is broadly applicable to covariance
analysis, diffusion tensor imaging, and other domains requiring SPD matrix
comparison. The package is openly available at
https://pypi.org/project/spd-metrics-id/.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [904] [Report of the 2025 Workshop on Next-Generation Ecosystems for Scientific Computing: Harnessing Community, Software, and AI for Cross-Disciplinary Team Science](https://arxiv.org/abs/2510.03413)
*L. C. McInnes,D. Arnold,P. Balaprakash,M. Bernhardt,B. Cerny,A. Dubey,R. Giles,D. W. Hood,M. A. Leung,V. Lopez-Marrero,P. Messina,O. B. Newton,C. Oehmen,S. M. Wild,J. Willenbring,L. Woodley,T. Baylis,D. E. Bernholdt,C. Camano,J. Cohoon,C. Ferenbaugh,S. M. Fiore,S. Gesing,D. Gomez-Zara,J. Howison,T. Islam,D. Kepczynski,C. Lively,H. Menon,B. Messer,M. Ngom,U. Paliath,M. E. Papka,I. Qualters,E. M. Raybourn,K. Riley,P. Rodriguez,D. Rouson,M. Schwalbe,S. K. Seal,O. Surer,V. Taylor,L. Wu*

Main category: cs.CE

TL;DR: The workshop discussed how to integrate AI, HPC, and scientific software through socio-technical co-design to create sustainable and collaborative ecosystems for cross-disciplinary scientific computing.


<details>
  <summary>Details</summary>
Motivation: The need to tackle challenges at the intersection of AI, HPC, and scientific software while fostering collaboration and driving scientific advancements was the primary motivation.

Method: Participants envisioned socio-technical co-design, modular systems, innovative training pipelines, and pilot projects to build robust ecosystems, emphasizing collaboration and technical integration.

Result: Recommendations included trustworthy AI-driven systems, integration of AI with human creativity, updated training models, hybrid AI/HPC infrastructure, and public-private partnership prototypes.

Conclusion: A unified ecosystem combining AI, software, hardware, and human expertise was proposed to enhance discovery, broaden access, strengthen the workforce, and accelerate progress.

Abstract: This report summarizes insights from the 2025 Workshop on Next-Generation
Ecosystems for Scientific Computing: Harnessing Community, Software, and AI for
Cross-Disciplinary Team Science, which convened more than 40 experts from
national laboratories, academia, industry, and community organizations to chart
a path toward more powerful, sustainable, and collaborative scientific software
ecosystems. To address urgent challenges at the intersection of
high-performance computing (HPC), AI, and scientific software, participants
envisioned agile, robust ecosystems built through socio-technical
co-design--the intentional integration of social and technical components as
interdependent parts of a unified strategy. This approach combines advances in
AI, HPC, and software with new models for cross-disciplinary collaboration,
training, and workforce development. Key recommendations include building
modular, trustworthy AI-enabled scientific software systems; enabling
scientific teams to integrate AI systems into their workflows while preserving
human creativity, trust, and scientific rigor; and creating innovative training
pipelines that keep pace with rapid technological change. Pilot projects were
identified as near-term catalysts, with initial priorities focused on hybrid
AI/HPC infrastructure, cross-disciplinary collaboration and pedagogy,
responsible AI guidelines, and prototyping of public-private partnerships. This
report presents a vision of next-generation ecosystems for scientific computing
where AI, software, hardware, and human expertise are interwoven to drive
discovery, expand access, strengthen the workforce, and accelerate scientific
progress.

</details>


### [905] [Lightweight and Data-Efficient MultivariateTime Series Forecasting using Residual-Stacked Gaussian (RS-GLinear) Architecture](https://arxiv.org/abs/2510.03788)
*Abukar Ali*

Main category: cs.CE

TL;DR: This paper presents the Residual Stacked Gaussian Linear (RSGL) model for time-series forecasting, demonstrating better accuracy compared to existing models.


<details>
  <summary>Details</summary>
Motivation: To address limitations in long-term forecasting using Transformer-based architectures and explore enhancements to Gaussian Linear models.

Method: Enhanced the Gaussian-based Linear model by creating the Residual Stacked Gaussian Linear (RSGL) and tested its broader applicability in various domains.

Result: The RSGL model displayed improved prediction accuracy and robustness over Gaussian Linear and Transformer-based models.

Conclusion: The RSGL model is a promising advancement for time-series forecasting across diverse domains.

Abstract: Following the success of Transformer architectures in language modeling,
particularly their ability to capture long-range dependencies, researchers have
explored how these architectures can be adapted for time-series forecasting.
Transformer-based models have been proposed to handle both short- and long-term
dependencies when predicting future values from historical data. However,
studies such as those by Zeng et al. (2022) and Rizvi et al. (2025) have
reported mixed results in long-term forecasting tasks. In this work, we
evaluate the Gaussian-based Linear architecture introduced by Rizvi et al.
(2025) and present an enhanced version called the Residual Stacked Gaussian
Linear (RSGL) model. We also investigate the broader applicability of the RSGL
model in additional domains, including financial time series and
epidemiological data. Experimental results show that the RSGL model achieves
improved prediction accuracy and robustness compared to both the baseline
Gaussian Linear and Transformer-based models.

</details>


### [906] [A Complement to Neural Networks for Anisotropic Inelasticity at Finite Strains](https://arxiv.org/abs/2510.04187)
*Hagen Holthusen,Ellen Kuhl*

Main category: cs.CE

TL;DR: The paper proposes a machine learning framework integrating material principles into neural networks to model anisotropy and inelasticity at finite strains while ensuring stability beyond training data.


<details>
  <summary>Details</summary>
Motivation: Traditional constitutive modeling struggles to effectively handle anisotropy and inelasticity in materials under finite strains. A new approach is needed to better integrate physical principles with machine learning techniques.

Method: The authors introduce dual potentials, invariant-based input representations, Input Convex and Monotonic Neural Networks, and recurrent Liquid Neural Networks to model material behavior and avoid instability in finite strain regimes.

Result: Their method demonstrates accurate and stable performance at both material and structural scales, surpassing conventional recurrent models and validating predictions on unseen problems.

Conclusion: The study presents an innovative, validated approach for material modeling using physics-informed neural networks, making advancement in both scientific understanding and practical application with open-source access provided.

Abstract: We propose a complement to constitutive modeling that augments neural
networks with material principles to capture anisotropy and inelasticity at
finite strains. The key element is a dual potential that governs dissipation,
consistently incorporates anisotropy, and-unlike conventional convex
formulations-satisfies the dissipation inequality without requiring convexity.
  Our neural network architecture employs invariant-based input representations
in terms of mixed elastic, inelastic and structural tensors. It adapts Input
Convex Neural Networks, and introduces Input Monotonic Neural Networks to
broaden the admissible potential class. To bypass exponential-map time
integration in the finite strain regime and stabilize the training of inelastic
materials, we employ recurrent Liquid Neural Networks.
  The approach is evaluated at both material point and structural scales. We
benchmark against recurrent models without physical constraints and validate
predictions of deformation and reaction forces for unseen boundary value
problems. In all cases, the method delivers accurate and stable performance
beyond the training regime. The neural network and finite element
implementations are available as open-source and are accessible to the public
via https://doi.org/10.5281/zenodo.17199965.

</details>


### [907] [Towards Fast Option Pricing PDE Solvers Powered by PIELM](https://arxiv.org/abs/2510.04322)
*Akshay Govind Srinivasan,Anuj Jagannath Said,Sathwik Pentela,Vikas Dwivedi,Balaji Srinivasan*

Main category: cs.CE

TL;DR: The paper introduces Physics-Informed Extreme Learning Machines (PIELMs) for solving PDEs in finance with high speed and comparable accuracy to Physics-Informed Neural Networks (PINNs).


<details>
  <summary>Details</summary>
Motivation: Traditional PINNs for solving financial PDEs are computationally expensive and exhibit poor scalability, prompting the need for a faster, more efficient alternative.

Method: PIELMs employ a least-squares approach instead of iterative gradient descent, enhancing computational efficiency and determinism in training.

Result: PIELMs demonstrate up to 30x faster computation while maintaining accuracy comparable to PINNs in solving the Black-Scholes and Heston-Hull-White models.

Conclusion: PIELMs offer a promising alternative for real-time financial modeling, blending high accuracy with significantly improved computational speed.

Abstract: Partial differential equation (PDE) solvers underpin modern quantitative
finance, governing option pricing and risk evaluation. Physics-Informed Neural
Networks (PINNs) have emerged as a promising approach for solving the forward
and inverse problems of partial differential equations (PDEs) using deep
learning. However they remain computationally expensive due to their iterative
gradient descent based optimization and scale poorly with increasing model
size. This paper introduces Physics-Informed Extreme Learning Machines (PIELMs)
as fast alternative to PINNs for solving both forward and inverse problems in
financial PDEs. PIELMs replace iterative optimization with a single
least-squares solve, enabling deterministic and efficient training. We
benchmark PIELM on the Black-Scholes and Heston-Hull-White models for forward
pricing and demonstrate its capability in inverse model calibration to recover
volatility and interest rate parameters from noisy data. From experiments we
observe that PIELM achieve accuracy comparable to PINNs while being up to
$30\times$ faster, highlighting their potential for real-time financial
modeling.

</details>


### [908] [Deep vs. Shallow: Benchmarking Physics-Informed Neural Architectures on the Biharmonic Equation](https://arxiv.org/abs/2510.04490)
*Akshay Govind Srinivasan,Vikas Dwivedi,Balaji Srinivasan*

Main category: cs.CE

TL;DR: The paper benchmarks RBF-PIELM, a PINN variant, for higher-order PDEs, showing it is significantly faster and requires fewer parameters than traditional PINNs, but still lags behind classical mesh-based solvers.


<details>
  <summary>Details</summary>
Motivation: Classical PDE solvers struggle with complex geometries and higher-order operators, while newer PINNs are computationally intensive and sometimes less accurate. This necessitates exploring alternative computational techniques.

Method: The authors used RBF-PIELM, an extreme learning machine with radial-basis activations, which replaces gradient descent with a one-shot least-squares solve. They validated it on two benchmarks involving the fourth-order biharmonic equation.

Result: RBF-PIELM exhibited up to 350x faster training and 10x fewer parameters for comparable accuracy relative to traditional PINNs. However, it performed poorly on highly oscillatory solutions.

Conclusion: While RBF-PIELM outperforms PINNs in speed and parameter efficiency, it is still less robust than traditional mesh-based solvers, especially for practical high-complexity problems.

Abstract: Partial differential equation (PDE) solvers are fundamental to engineering
simulation. Classical mesh-based approaches (finite difference/volume/element)
are fast and accurate on high-quality meshes but struggle with higher-order
operators and complex, hard-to-mesh geometries. Recently developed
physics-informed neural networks (PINNs) and their variants are mesh-free and
flexible, yet compute-intensive and often less accurate. This paper
systematically benchmarks RBF-PIELM, a rapid PINN variant-an extreme learning
machine with radial-basis activations-for higher-order PDEs. RBF-PIELM replaces
PINNs' time-consuming gradient descent with a single-shot least-squares solve.
We test RBF-PIELM on the fourth-order biharmonic equation using two benchmarks:
lid-driven cavity flow (streamfunction formulation) and a manufactured
oscillatory solution. Our results show up to $(350\times)$ faster training than
PINNs and over $(10\times)$ fewer parameters for comparable solution accuracy.
Despite surpassing PINNs, RBF-PIELM still lags mature mesh-based solvers and
its accuracy degrades on highly oscillatory solutions, highlighting remaining
challenges for practical deployment.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [909] [An Empirical Study of Rational Tree Unification for miniKanren](https://arxiv.org/abs/2510.03789)
*Eridan Domoratskiy,Dmitrii Kosarev,Dmitry Boulytchev*

Main category: cs.LO

TL;DR: This paper examines unification of rational trees using miniKanren, detailing definitions, algorithms, optimizations, benchmark evaluations, and relations to conventional techniques.


<details>
  <summary>Details</summary>
Motivation: Understanding unification for rational trees is crucial for improving relational programming and exploring its compatibility with conventional unification algorithms.

Method: The study defines rational trees, specifies the unification algorithm, develops heuristic optimizations, and evaluates their performance across benchmarks.

Result: The paper provides key properties of rational tree unification, evaluates heuristic optimizations, and compares rational and conventional unification techniques.

Conclusion: Rational tree unification is effective in relational programming, and there's potential for coexistence and interoperability with conventional algorithms.

Abstract: We present a study of unification for rational trees in the context of
miniKanren. We give the definition of rational trees, specify the unification
algorithm and prove some of its properties. We also introduce a number of
heuristic optimizations and evaluate them for a number of relevant benchmarks.
Finally we discuss the relations between rational and conventional unification
algorithms and possible scenarios of their coexistence in the context of
relational programming.

</details>


### [910] [Strategy Logic, Imperfect Information, and Hyperproperties](https://arxiv.org/abs/2510.03952)
*Raven Beutner,Bernd Finkbeiner*

Main category: cs.LO

TL;DR: The paper demonstrates the equivalence between Strategy Logic with Imperfect Information (SL$_ii$) and Hyper Strategy Logic (HyperSL), showing that instances of one can be encoded into the other.


<details>
  <summary>Details</summary>
Motivation: To explore the relationship and equivalence between two branches of logic—SL$_ii$ and HyperSL—that deal with strategic behavior in multi-agent systems under imperfect information and hyperproperties.

Method: The authors establish equivalence by encoding SL$_ii$ into HyperSL and vice versa. They utilize the concept of imperfect information as a hyperproperty and simulate hyperproperties through self-composition of multi-agent systems.

Result: They prove the equivalence between SL$_ii$ and HyperSL for formulas where no state formulas are nested within path formulas, highlighting their interconnectedness.

Conclusion: The results unify perspectives on reasoning about strategies in MAS under imperfect information and hyperproperties, providing new insights into how these logics interrelate.

Abstract: Strategy logic (SL) is a powerful temporal logic that enables first-class
reasoning over strategic behavior in multi-agent systems (MAS). In many MASs,
the agents (and their strategies) cannot observe the global state of the
system, leading to many extensions of SL centered around imperfect information,
such as strategy logic with imperfect information (SL$_\mathit{ii}$). Along
orthogonal lines, researchers have studied the combination of strategic
behavior and hyperproperties. Hyperproperties are system properties that relate
multiple executions in a system and commonly arise when specifying security
policies. Hyper Strategy Logic (HyperSL) is a temporal logic that combines
quantification over strategies with the ability to express hyperproperties on
the executions of different strategy profiles. In this paper, we study the
relation between SL$_\mathit{ii}$ and HyperSL. Our main result is that both
logics (restricted to formulas where no state formulas are nested within path
formulas) are equivalent in the sense that we can encode SL$_\mathit{ii}$
instances into HyperSL instances and vice versa. For the former direction, we
build on the well-known observation that imperfect information is a
hyperproperty. For the latter direction, we construct a self-composition of
MASs and show how we can simulate hyperproperties using imperfect information.

</details>


### [911] [Curved Boolean Logic: A Contextual Generalization of Propositional Logic with Algorithmic Consequences](https://arxiv.org/abs/2510.04716)
*Maximilian R. P. von Liechtenstein*

Main category: cs.LO

TL;DR: Curved Boolean Logic (CBL) generalizes traditional Boolean Logic with local truth assignments that can accommodate inconsistencies, offering benefits for logic-based problem-solving under various frameworks.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of classical propositional logic by developing a new logic framework capable of handling contradictions locally, inspired by curvature concepts in geometry.

Method: The paper introduces semantics through sheaves and exclusivity graphs, formulates the CBL-SAT problem, analyzes complexity, and models noise under multiple frameworks. The authors also present algorithms (CBL-AC and CBL-CONS) for faster contradiction pruning on classical hardware.

Result: CBL is proven to be NP-complete in general, with newly developed operators accelerating computations and supporting robustness in noisy and perturbed environments.

Conclusion: CBL offers a novel approach to logic theory, with applications to SAT/CSP problems and stability mechanisms in emerging technologies like large language models, demonstrating its theoretical and computational strengths.

Abstract: Curved Boolean Logic (CBL) generalizes propositional logic by allowing local
truth assignments that do not extend to a single global valuation, analogous to
curvature in geometry. We give equivalent sheaf and exclusivity-graph semantics
and a context-aware proof calculus that is conservative in the flat limit. We
formalize CBL-SAT and basic complexity (NP-complete in general) and present
operational operators (CBL-AC and CBL-CONS) that prune contradictions earlier
on classical hardware. We model noise with iid, AR(1)-correlated, and
adversarial bounded perturbations and provide permutation-based significance
with Benjamini-Hochberg FDR control. A Colab-ready notebook (ancillary files)
regenerates all figures and statistics. We position CBL relative to KCBS, CSW,
and sheaf frameworks and outline links to SAT/CSP and robustness/adapter
stability in large language models.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [912] [Super-resolution image projection over an extended depth of field using a diffractive decoder](https://arxiv.org/abs/2510.03938)
*Hanlong Chen,Cagatay Isil,Tianyi Gan,Mona Jarrahi,Aydogan Ozcan*

Main category: physics.optics

TL;DR: The paper introduces a hybrid image projection system that combines a CNN digital encoder with an all-optical diffractive decoder to achieve pixel super-resolution, extended depth-of-field, and efficient data usage.


<details>
  <summary>Details</summary>
Motivation: The motivation is to develop an efficient image projection system that reduces data storage and transmission requirements while achieving high resolution and extended depth-of-field without additional power constraints.

Method: The method combines a convolutional neural network (CNN)-based digital encoder that compresses input images into phase representations, and a passive optical diffractive decoder that reconstructs these images with pixel super-resolution and extended depth-of-field.

Result: The system demonstrated high-fidelity image synthesis with up to a 16-fold improvement in space-bandwidth-product (SBP) across lateral planes and an extended depth-of-field ~267 times the illumination wavelength.

Conclusion: This hybrid system optimizes data storage, computational load, and power consumption, while enhancing resolution and depth-of-field, with applications extending to optical metrology and microscopy.

Abstract: Image projection systems must be efficient in data storage, computation and
transmission while maintaining a large space-bandwidth-product (SBP) at their
output. Here, we introduce a hybrid image projection system that achieves
extended depth-of-field (DOF) with improved resolution, combining a
convolutional neural network (CNN)-based digital encoder with an all-optical
diffractive decoder. A CNN-based encoder compresses input images into compact
phase representations, which are subsequently displayed by a low-resolution
(LR) projector and processed by an analog diffractive decoder for all-optical
image reconstruction. This optical decoder is completely passive, designed to
synthesize pixel super-resolved image projections that feature an extended DOF
while eliminating the need for additional power consumption for super-resolved
image reconstruction. Our pixel super-resolution (PSR) image projection system
demonstrates high-fidelity image synthesis over an extended DOF of ~267xW,
where W is the illumination wavelength, concurrently offering up to ~16-fold
SBP improvement at each lateral plane. The proof of concept of this approach is
validated through an experiment conducted in the THz spectrum, and the system
is scalable across different parts of the electromagnetic spectrum. This image
projection architecture can reduce data storage and transmission requirements
for display systems without imposing additional power constraints on the
optical decoder. Beyond extended DOF PSR image projection, the underlying
principles of this approach can be extended to various applications, including
optical metrology and microscopy.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [913] [Sharp Lower Bounds for Linearized ReLU^k Approximation on the Sphere](https://arxiv.org/abs/2510.04060)
*Tong Mao,Jinchao Xu*

Main category: math.NA

TL;DR: This paper establishes the exact saturation order for linearized ReLU^k neural networks' approximation on the unit sphere, showing that their convergence speed is inherently limited by smoothness and network structure.


<details>
  <summary>Details</summary>
Motivation: To understand the limitations of linearized shallow ReLU^k neural networks and determine the convergence bounds for approximating functions, providing insights into the intrinsic efficiency of these networks.

Method: The authors prove a saturation theorem by deriving lower bounds for the best L^2 approximation on the unit sphere, and compare these theoretical bounds with existing upper bounds.

Result: The paper demonstrates that the saturation order for target functions with sufficient smoothness is limited to a specific convergence rate that cannot exceed n^{-(d+2k+1)/(2d)}, matching upper bounds.

Conclusion: Linearized ReLU^k networks are shown to excel over finite elements in terms of approximation efficiency, but their advantages are intrinsically constrained by their structure and the saturation framework.

Abstract: We prove a saturation theorem for linearized shallow ReLU$^k$ neural networks
on the unit sphere $\mathbb S^d$. For any antipodally quasi-uniform set of
centers, if the target function has smoothness $r>\tfrac{d+2k+1}{2}$, then the
best $\mathcal{L}^2(\mathbb S^d)$ approximation cannot converge faster than
order $n^{-\frac{d+2k+1}{2d}}$. This lower bound matches existing upper bounds,
thereby establishing the exact saturation order $\tfrac{d+2k+1}{2d}$ for such
networks. Our results place linearized neural-network approximation firmly
within the classical saturation framework and show that, although ReLU$^k$
networks outperform finite elements under equal degrees $k$, this advantage is
intrinsically limited.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [914] [Multi-Agent Distributed Optimization With Feasible Set Privacy](https://arxiv.org/abs/2510.05068)
*Shreya Meel,Sennur Ulukus*

Main category: cs.IT

TL;DR: The paper tackles decentralized optimization with privacy-respecting solutions by proposing efficient communication schemes for retrieving solution sets with nominal information leakage.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of decentralized optimization where agents maintain privacy of their feasible sets while jointly finding optimal solutions, resolving practical constraints in communication setups.

Method: Developed achievable schemes for solution retrieval ensuring nominal information leakage, analyzing communication costs under ring and star network setups, and integrating concepts of Private Set Intersection (PSI) and threshold PSI for implementation.

Result: The proposed approach prevents excessive information leakage, improves communication efficiency under specific realizations of the objective function, and reduces to PSI and ThPSI for feasibility and efficiency.

Conclusion: The schemes effectively balance privacy and communication efficiency, outperforming standard PSI in terms of leakage and communication cost in decentralized constrained optimization.

Abstract: We consider the problem of decentralized constrained optimization with
multiple agents $E_1,\ldots,E_N$ who jointly wish to learn the optimal solution
set while keeping their feasible sets $\mathcal{P}_1,\ldots,\mathcal{P}_N$
private from each other. We assume that the objective function $f$ is known to
all agents and each feasible set is a collection of points from a universal
alphabet $\mathcal{P}_{alph}$. A designated agent (leader) starts the
communication with the remaining (non-leader) agents, and is the first to
retrieve the solution set. The leader searches for the solution by sending
queries to and receiving answers from the non-leaders, such that the
information on the individual feasible sets revealed to the leader should be no
more than nominal, i.e., what is revealed from learning the solution set alone.
We develop achievable schemes for obtaining the solution set at nominal
information leakage, and characterize their communication costs under two
communication setups between agents. In this work, we focus on two kinds of
network setups: i) ring, where each agent communicates with two adjacent
agents, and ii) star, where only the leader communicates with the remaining
agents. We show that, if the leader first learns the joint feasible set through
an existing private set intersection (PSI) protocol and then deduces the
solution set, the information leaked to the leader is greater than nominal.
Moreover, we draw connection of our schemes to threshold PSI (ThPSI), which is
a PSI-variant where the intersection is revealed only when its cardinality is
larger than a threshold value. Finally, for various realizations of $f$ mapped
uniformly at random to a fixed range of values, our schemes are more
communication-efficient with a high probability compared to retrieving the
entire feasible set through PSI.

</details>


### [915] [Multi-Modal Multi-Task Semantic Communication: A Distributed Information Bottleneck Perspective](https://arxiv.org/abs/2510.04000)
*Yujie Zhou,Yiwei Liao,Cheng Peng,Yong Xiao,Yingyu Li*

Main category: cs.IT

TL;DR: A novel framework, PoM$^2$-DIB, is proposed for semantic communication, addressing issues of redundant data transmission and task participation through modality selection.


<details>
  <summary>Details</summary>
Motivation: Current semantic communication schemes require full data modality participation, leading to inefficiencies due to channel limitations and computational constraints.

Method: The paper introduces an enhanced Distributed Information Bottleneck (DIB) framework that integrates modality selection as a variable, optimized probabilistically using score function estimation with common randomness, enabling end-to-end optimization across distributed devices.

Result: Experimental results show that PoM$^2$-DIB achieves high inference quality while adhering to physical constraints in various tasks.

Conclusion: PoM$^2$-DIB provides an efficient solution for multi-modal semantic communication, balancing communication rates and task performance under physical limitations.

Abstract: Semantic communication (SemCom) shifts the focus from data transmission to
meaning delivery, enabling efficient and intelligent communication.
  Existing AI-based coding schemes for multi-modal multi-task SemCom often
require transmitters with full-modal data to participate in all receivers'
tasks, which leads to redundant transmissions and conflicts with the physical
limits of channel capacity and computational capability.
  In this paper, we propose PoM$^2$-DIB, a novel framework that extends the
distributed information bottleneck (DIB) theory to address this problem.
  Unlike the typical DIB, this framework introduces modality selection as an
additional key design variable, enabling a more flexible tradeoff between
communication rate and inference quality.
  This extension selects only the most relevant modalities for task
participation, adhering to the physical constraints, while following efficient
DIB-based coding.
  To optimize selection and coding end-to-end, we relax modality selection into
a probabilistic form, allowing the use of score function estimation with common
randomness to enable optimizable coordinated decisions across distributed
devices.
  Experimental results on public datasets verify that PoM$^2$-DIB achieves high
inference quality compared to full-participation baselines in various tasks
under physical limits.

</details>


<div id='cs.SY'></div>

# cs.SY [[Back]](#toc)

### [916] [Adaptive Cruise Control in Autonomous Vehicles: Challenges, Gaps, Comprehensive Review, and, Future Directions](https://arxiv.org/abs/2510.03300)
*Shradha Bavalatti,Yash Kangralkar,Santosh Pattar,Veena P Badiger*

Main category: cs.SY

TL;DR: The paper surveys challenges in adaptive cruise control (ACC) for autonomous vehicles (AVs) and provides solutions to advance future ACC systems.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address gaps in existing literature that lack comprehensive analysis and impactful solutions for challenges in adaptive cruise control of autonomous vehicles.

Method: The authors conduct a detailed and systematic review of limitations in current ACC research while proposing innovative directions for future study.

Result: The paper identifies research gaps and suggests strategies to design next-generation ACC systems that ensure sustainable and fault-resilient urban transportation.

Conclusion: The survey helps guide researchers in advancing the field of ACC systems, addressing current limitations, and facilitating safer urban transportation.

Abstract: The development of Autonomous Vehicles (AVs) has redefined the way of
transportation by eliminating the need for human intervention in driving. This
revolution is fueled by rapid advancements in adaptive cruise control (ACC),
which make AVs capable of interpreting their surroundings and responding
intelligently. While AVs offer significant advantages, such as enhanced safety
and improved traffic efficiency, they also face several challenges that need to
be addressed. Existing survey papers often lack a comprehensive analysis of
these challenges and their potential solutions. Our paper stands out by
meticulously identifying these gaps in current ACC research and offering
impactful future directions to guide researchers in designing next-generation
ACC systems. Our survey provides a detailed and systematic review, addressing
the limitations of previous studies and proposing innovative approaches to
achieve sustainable and fault-resilient urban transportation.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [917] [Consistent kinetic modeling of compressible flows with variable Prandtl numbers: Double-distribution quasi-equilibrium approach](https://arxiv.org/abs/2510.04197)
*R. M. Strässle,S. A. Hosseini,I. V. Karlin*

Main category: physics.flu-dyn

TL;DR: The paper proposes a kinetic modeling and discretization method using double-distribution frameworks for accurately simulating compressible flows across varying Prandtl numbers and specific heat ratios.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of accurately modeling compressible flows across all Prandtl numbers and specific heat ratios, ensuring robust recovery of fluid dynamic equations and accommodating a broad range of flow conditions.

Method: The study employs the quasi-equilibrium approach within double-distribution frameworks, utilizes high-order velocity lattices, and ensures accurate hydrodynamic limit analysis with targeted equilibrium and quasi-equilibrium attractors.

Result: The proposed models demonstrate high accuracy, stability, Galilean invariance, and reproduce Navier-Stokes-Fourier physics in sensitive flow scenarios like 2D shock-vortex interaction.

Conclusion: The models offer an efficient and scalable tool for simulating compressible flows at varying physical conditions and enable future extensions to high-Mach or hypersonic regimes with further refinements.

Abstract: A consistent kinetic modeling and discretization strategy for compressible
flows across all Prandtl numbers and specific heat ratios is developed using
the quasi-equilibrium approach within two of the most widely used
double-distribution frameworks. The methodology ensures accurate recovery of
the Navier-Stokes-Fourier equations, including all macroscopic moments and
dissipation rates, through detailed hydrodynamic limit analysis and careful
construction of equilibrium and quasi-equilibrium attractors. Discretization is
performed using high-order velocity lattices with a static reference frame in a
discrete velocity Boltzmann context to isolate key modeling aspects such as the
necessary requirements on expansion and quadrature orders. The proposed models
demonstrate high accuracy, numerical stability and Galilean invariance across a
wide range of Mach numbers and temperature ratios. Separate tests for strict
conservation and measurements of all dissipation rates confirm these insights
for all Prandtl numbers and specific heat ratios. Simulations on a sensitive
two-dimensional shock-vortex interaction excellently reproduce viscous
Navier-Stokes-Fourier-level physics. The proposed models establish an accurate,
efficient and scalable framework for kinetic simulations of compressible flows
with moderate supersonic speeds and discontinuities at arbitrary Prandtl
numbers and specific heat ratios, offering a valuable tool for studying complex
problems in fluid dynamics and paving the way for future extensions to the
lattice Boltzmann context, by application of correction terms, as well as
high-Mach and hypersonic regimes, employing target-designed reference frames.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [918] [Fairness in Repeated Matching: A Maximin Perspective](https://arxiv.org/abs/2510.04624)
*Eugene Lim,Tzeh Yuan Neoh,Nicholas Teh*

Main category: cs.GT

TL;DR: The paper studies a sequential decision-making model for optimizing agent-item matching over multiple rounds, addressing computational challenges and proposing viable algorithms.


<details>
  <summary>Details</summary>
Motivation: Optimize the utility for the least advantaged agent in repeated agent-item matchings.

Method: Investigating computational challenges and proposing approximation and fixed-parameter tractable algorithms for solving matching problems.

Result: Identified that optimal and anytime optimal outcomes are computationally intractable but provided solutions for approximation and special cases.

Conclusion: While solving the general problem is challenging, the proposed solutions and characterizations contribute to advancements in matching theory.

Abstract: We study a sequential decision-making model where a set of items is
repeatedly matched to the same set of agents over multiple rounds. The
objective is to determine a sequence of matchings that either maximizes the
utility of the least advantaged agent at the end of all rounds (optimal) or at
the end of every individual round (anytime optimal). We investigate the
computational challenges associated with finding (anytime) optimal outcomes and
demonstrate that these problems are generally computationally intractable.
However, we provide approximation algorithms, fixed-parameter tractable
algorithms, and identify several special cases whereby the problem(s) can be
solved efficiently. Along the way, we also establish characterizations of
Pareto-optimal/maximum matchings, which may be of independent interest to works
in matching theory and house allocation.

</details>


### [919] [Scale-Invariant Regret Matching and Online Learning with Optimal Convergence: Bridging Theory and Practice in Zero-Sum Games](https://arxiv.org/abs/2510.04407)
*Brian Hu Zhang,Ioannis Anagnostides,Tuomas Sandholm*

Main category: cs.GT

TL;DR: This paper introduces IREG-PRM$^+$, a new variant of counterfactual regret minimization that closes the gap between theoretical guarantees ($T^{-1}$ convergence rate) and practical performance benchmarks.


<details>
  <summary>Details</summary>
Motivation: To address the disparity between well-established theoretical convergence rates in zero-sum game solving and the slower convergence rates of practical methods like PRM$^+$.

Method: The authors propose IREG-PRM$^+$, a scale-invariant, parameter-free variant of PRM$^+$. It is designed to maintain non-decreasing regret vector norms, ensuring better learning rate behavior and leveraging an adaptive approach similar to optimistic gradient descent.

Result: IREG-PRM$^+$ achieves both $T^{-1/2}$ best-iterate and $T^{-1}$ average-iterate convergence guarantees, while matching practical performance benchmarks. Additionally, an adaptive learning rate based on misprediction error is introduced.

Conclusion: IREG-PRM$^+$ bridges the gap between theory and practice in game-solving techniques, demonstrating optimal theoretical guarantees while maintaining practical efficiency and performance.

Abstract: A considerable chasm has been looming for decades between theory and practice
in zero-sum game solving through first-order methods. Although a convergence
rate of $T^{-1}$ has long been established since Nemirovski's mirror-prox
algorithm and Nesterov's excessive gap technique in the early 2000s, the most
effective paradigm in practice is *counterfactual regret minimization*, which
is based on *regret matching* and its modern variants. In particular, the state
of the art across most benchmarks is *predictive* regret matching$^+$
(PRM$^+$), in conjunction with non-uniform averaging. Yet, such algorithms can
exhibit slower $\Omega(T^{-1/2})$ convergence even in self-play.
  In this paper, we close the gap between theory and practice. We propose a new
scale-invariant and parameter-free variant of PRM$^+$, which we call
IREG-PRM$^+$. We show that it achieves $T^{-1/2}$ best-iterate and $T^{-1}$
(i.e., optimal) average-iterate convergence guarantees, while also being on par
with PRM$^+$ on benchmark games. From a technical standpoint, we draw an
analogy between IREG-PRM$^+$ and optimistic gradient descent with *adaptive*
learning rate. The basic flaw of PRM$^+$ is that the ($\ell_2$-)norm of the
regret vector -- which can be thought of as the inverse of the learning rate --
can decrease. By contrast, we design IREG-PRM$^+$ so as to maintain the
invariance that the norm of the regret vector is nondecreasing. This enables us
to derive an RVU-type bound for IREG-PRM$^+$, the first such property that does
not rely on introducing additional hyperparameters to enforce smoothness.
  Furthermore, we find that IREG-PRM$^+$ performs on par with an adaptive
version of optimistic gradient descent that we introduce whose learning rate
depends on the misprediction error, demystifying the effectiveness of the
regret matching family *vis-a-vis* more standard optimization techniques.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [920] [Interactive High-Performance Visualization for Astronomy and Cosmology](https://arxiv.org/abs/2510.04665)
*Eva Sciacca,Nicola Tuccari,Umer Arshad,Fabio Pitari,Giuseppa Muscianisi,Emiliano Tramontana*

Main category: astro-ph.IM

TL;DR: This paper presents the integration of the VisIVO visualization framework with Cineca's InterActive Computing service, providing interactive, high-performance visualization within HPC environments.


<details>
  <summary>Details</summary>
Motivation: To address challenges in handling and visualizing large-scale astrophysics and cosmology datasets, and to enhance user accessibility to HPC resources.

Method: Integrating VisIVO with Cineca's IAC service via Jupyter-based gateways, utilizing GPU-enabled nodes and custom Python wrappers to facilitate visualization workflows.

Result: The infrastructure simplifies HPC accessibility, improves reproducibility, and accelerates workflows, validated through simulations of the Universe's large-scale structure using the GADGET code.

Conclusion: This integration bridges domain-specific tools and advanced HPC infrastructures, promoting scalable, user-friendly, and reproducible research in astrophysics and cosmology.

Abstract: The exponential growth of data in Astrophysics and Cosmology demands scalable
computational tools and intuitive interfaces for analysis and visualization. In
this work, we present an innovative integration of the VisIVO scientific
visualization framework with the InterActive Computing (IAC) service at Cineca,
enabling interactive, high-performance visual workflows directly within HPC
environments. Through seamless integration into Jupyter-based science gateways,
users can now access GPU-enabled compute nodes to perform complex 3D
visualizations using VisIVO via custom Python wrappers and preconfigured
interactive notebooks. We demonstrate how this infrastructure simplifies access
to advanced HPC resources, enhances reproducibility, and accelerates
exploratory workflows in astronomical research. Our approach has been validated
through a set of representative use cases involving large-scale simulations
from the GADGET code, highlighting the effectiveness of this system in
visualizing the large-scale structure of the Universe. This work exemplifies
how science gateways can bridge domain-specific tools and advanced
infrastructures, fostering user-centric, scalable, and reproducible research
environments.

</details>


### [921] [Large Language Models Achieve Gold Medal Performance at International Astronomy & Astrophysics Olympiad](https://arxiv.org/abs/2510.05016)
*Lucas Carrit Delgado Pinheiro,Ziru Chen,Bruno Caixeta Piazza,Ness Shroff,Yingbin Liang,Yuan-Sen Ting,Huan Sun*

Main category: astro-ph.IM

TL;DR: The paper evaluates the performance of five state-of-the-art LLMs on the International Olympiad on Astronomy and Astrophysics (IOAA) exams to assess their strengths and weaknesses in complex reasoning and multimodal analysis.


<details>
  <summary>Details</summary>
Motivation: The study aims to examine the capabilities of large language models (LLMs) for solving astronomy problems and identify their limitations, as existing benchmarks focus primarily on simple tasks and fail to evaluate the complex reasoning required for real-world research.

Method: Five advanced LLMs were benchmarked against IOAA exams (2022-2025), which evaluate deep conceptual understanding, multi-step derivations, and multimodal analysis. The results of both theory and data analysis exams were analyzed, with a detailed error analysis performed to identify weaknesses.

Result: Gemini 2.5 Pro and GPT-5 achieved gold medal-level performance in theory exams, ranking in the top two among 200-300 participants, while GPT-5 performed well on data analysis exams (88.5%), but other models showed significant drops in performance (48-76%). Weak areas identified included conceptual reasoning, geometric reasoning, and spatial visualization.

Conclusion: Although LLMs demonstrate near top human performance in astronomy theory exams, their limitations in specific reasoning and visualization tasks highlight the need for improvement before they can operate as fully autonomous research agents in astronomy.

Abstract: While task-specific demonstrations show early success in applying large
language models (LLMs) to automate some astronomical research tasks, they only
provide incomplete views of all necessary capabilities in solving astronomy
problems, calling for more thorough understanding of LLMs' strengths and
limitations. So far, existing benchmarks and evaluations focus on simple
question-answering that primarily tests astronomical knowledge and fails to
evaluate the complex reasoning required for real-world research in the
discipline. Here, we address this gap by systematically benchmarking five
state-of-the-art LLMs on the International Olympiad on Astronomy and
Astrophysics (IOAA) exams, which are designed to examine deep conceptual
understanding, multi-step derivations, and multimodal analysis. With average
scores of 85.6% and 84.2%, Gemini 2.5 Pro and GPT-5 (the two top-performing
models) not only achieve gold medal level performance but also rank in the top
two among ~200-300 participants in all four IOAA theory exams evaluated
(2022-2025). In comparison, results on the data analysis exams show more
divergence. GPT-5 still excels in the exams with an 88.5% average score,
ranking top 10 among the participants in the four most recent IOAAs, while
other models' performances drop to 48-76%. Furthermore, our in-depth error
analysis underscores conceptual reasoning, geometric reasoning, and spatial
visualization (52-79% accuracy) as consistent weaknesses among all LLMs. Hence,
although LLMs approach peak human performance in theory exams, critical gaps
must be addressed before they can serve as autonomous research agents in
astronomy.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [922] [NaturalEdit: Code Modification through Direct Interaction with Adaptive Natural Language Representation](https://arxiv.org/abs/2510.04494)
*Ningzhi Tang,David Meininger,Gelei Xu,Yiyu Shi,Yu Huang,Collin McMillan,Toby Jia-Jun Li*

Main category: cs.HC

TL;DR: NaturalEdit is a system that creates interactive, adaptive code summaries to aid developers in code modification, improving comprehension, intent articulation, and validation.


<details>
  <summary>Details</summary>
Motivation: Code modification is cognitively demanding, and static code summaries are insufficient for supporting the full workflow of developers.

Method: NaturalEdit uses interactive, adaptive code summaries with three core features: adaptable representations, interactive mapping to code, and intent-driven synchronization.

Result: Technical evaluation verified its performance, while a user study with 12 developers demonstrated improved comprehension, intent articulation, and validation.

Conclusion: NaturalEdit provides an effective, interactive, and adaptive approach to supporting code modification, enhancing developer confidence and control.

Abstract: Code modification requires developers to comprehend code, plan changes,
articulate intentions, and validate outcomes, making it a cognitively demanding
process. Generated natural language code summaries aid comprehension but remain
static and limited in supporting the full workflow. We present NaturalEdit, a
system that makes code summaries interactive and adaptive representations
directly linked to source code. Grounded in the Cognitive Dimensions of
Notations, NaturalEdit implements a paradigm of code modification through
interaction with natural language representations through three key features:
(1) adaptive multi-faceted representation of code summaries with flexible
Abstraction Gradient; (2) interactive mapping mechanisms between summaries and
codes, ensuring a tight Closeness of Mapping; and (3) intent-driven,
bidirectional synchronization that reduces Viscosity in editing and validation.
A technical evaluation confirms the performance of NaturalEdit, and a user
study with 12 developers shows that it enhances comprehension, intent
articulation, and validation, giving developers greater confidence and control.

</details>


### [923] [AI-Driven Grading and Moderation for Collaborative Projects in Computer Science Education](https://arxiv.org/abs/2510.03998)
*Songmei Yu,Andrew Zagula*

Main category: cs.HC

TL;DR: The paper proposes an AI-assisted system to fairly grade collaborative CS projects by evaluating both project quality and individual contribution.


<details>
  <summary>Details</summary>
Motivation: To address the difficulty in fairly assessing individual contributions in collaborative group projects in large computer science classes.

Method: Introduces an AI-assisted grading system using repository mining, communication analytics, and machine learning, integrated with platforms like GitHub.

Result: The system aligns well with instructor assessments, improves student satisfaction, and reduces instructor grading workload, as shown in a pilot deployment.

Conclusion: The proposed system demonstrates potential for scalable and fair assessment, with discussions on ethical concerns and future improvements for broader usage.

Abstract: Collaborative group projects are integral to computer science education, as
they foster teamwork, problem-solving skills, and industry-relevant
competencies. However, assessing individual contributions within group settings
has long been a challenge. Traditional assessment strategies, such as the equal
distribution of grades or subjective peer assessments, often fall short in
terms of fairness, objectivity, and scalability, particularly in large
classrooms. This paper introduces a semi-automated, AI-assisted grading system
that evaluates both project quality and individual effort using repository
mining, communication analytics, and machine learning models. The system
comprises modules for project evaluation, contribution analysis, and grade
computation, integrating seamlessly with platforms like GitHub. A pilot
deployment in a senior-level course demonstrated high alignment with instructor
assessments, increased student satisfaction, and reduced instructor grading
effort. We conclude by discussing implementation considerations, ethical
implications, and proposed enhancements to broaden applicability.

</details>


### [924] [When AI Gets Persuaded, Humans Follow: Inducing the Conformity Effect in Persuasive Dialogue](https://arxiv.org/abs/2510.04229)
*Rikuo Sasaki,Michimasa Inaba*

Main category: cs.HC

TL;DR: This paper studies if humans exhibit the 'conformity effect' influenced by AI and finds that an AI Persuadee Agent can enhance persuasion when it aligns with the persuader.


<details>
  <summary>Details</summary>
Motivation: To examine whether and how the 'conformity effect,' typically observed among humans, can be leveraged using AI agents in persuasive scenarios.

Method: The researchers conducted a text-based dialogue experiment involving participants interacting with a Persuader Agent and a Persuadee Agent under different conditions, varying persuasion acceptance and presence of an initial icebreaker session.

Result: The study found that when the Persuadee Agent accepted persuasion, both perceived persuasiveness and actual attitude change improved notably, especially when combined with an icebreaker session. Conversely, refusal by the agent to be persuaded suppressed attitude change.

Conclusion: Effectively designing AI agents to demonstrate persuasion acceptance can harness the conformity effect, thereby amplifying persuasiveness in human-AI interactions.

Abstract: Recent advancements in AI have highlighted its application in captology, the
field of using computers as persuasive technologies. We hypothesized that the
"conformity effect," where individuals align with others' actions, also occurs
with AI agents. This study verifies this hypothesis by introducing a "Persuadee
Agent" that is persuaded alongside a human participant in a three-party
persuasive dialogue with a Persuader Agent. We conducted a text-based dialogue
experiment with human participants. We compared four conditions manipulating
the Persuadee Agent's behavior (persuasion acceptance vs. non-acceptance) and
the presence of an icebreaker session. Results showed that when the Persuadee
Agent accepted persuasion, both perceived persuasiveness and actual attitude
change significantly improved. Attitude change was greatest when an icebreaker
was also used, whereas an unpersuaded AI agent suppressed attitude change.
Additionally, it was confirmed that the persuasion acceptance of participants
increased at the moment the Persuadee Agent was persuaded. These results
suggest that appropriately designing a Persuadee Agent can improve persuasion
through the conformity effect.

</details>


### [925] [Autonomy Matters: A Study on Personalization-Privacy Dilemma in LLM Agents](https://arxiv.org/abs/2510.04465)
*Zhiping Zhang,Yi Evie Zhang,Freda Shi,Tianshi Li*

Main category: cs.HC

TL;DR: The study explores the interplay of personalization and autonomy in LLM agents, highlighting their effects on privacy concerns, trust, and user willingness.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the personalization-privacy dilemma posed by LLM agents, particularly in balancing privacy concerns and effective user personalization.

Method: A 3×3 between-subjects experiment involving 450 participants was conducted, evaluating the effects of different autonomy and personalization levels on user privacy concerns, trust, and willingness to use the agents.

Result: Personalization raises privacy concerns and lowers trust/willingness when user privacy preferences are overlooked. Autonomy moderates these impacts, with intermediate autonomy reducing extreme effects compared to no/full autonomy conditions.

Conclusion: Balancing the autonomy of LLM agent actions with user control is key to mitigating privacy concerns while maintaining personalization effectiveness.

Abstract: Large Language Model (LLM) agents require personal information for
personalization in order to better act on users' behalf in daily tasks, but
this raises privacy concerns and a personalization-privacy dilemma. Agent's
autonomy introduces both risks and opportunities, yet its effects remain
unclear. To better understand this, we conducted a 3$\times$3 between-subjects
experiment ($N=450$) to study how agent's autonomy level and personalization
influence users' privacy concerns, trust and willingness to use, as well as the
underlying psychological processes. We find that personalization without
considering users' privacy preferences increases privacy concerns and decreases
trust and willingness to use. Autonomy moderates these effects: Intermediate
autonomy flattens the impact of personalization compared to No- and Full
autonomy conditions. Our results suggest that rather than aiming for perfect
model alignment in output generation, balancing autonomy of agent's action and
user control offers a promising path to mitigate the personalization-privacy
dilemma.

</details>


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [926] [Discrete scalar curvature as a weighted sum of Ollivier-Ricci curvatures](https://arxiv.org/abs/2510.04936)
*Abigail Hickok,Andrew J. Blumberg*

Main category: cs.DM

TL;DR: The paper studies relationships between Ollivier-Ricci curvature and scalar curvature in discrete settings such as point clouds and graphs, proposing a method for scalar Ollivier-Ricci curvature and proving convergence to continuous metrics.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between discrete settings (like graphs and point clouds) and continuous geometric quantities by formalizing relationships between Ollivier-Ricci curvature and scalar curvature.

Method: Defines scalar Ollivier-Ricci curvature for graphs/point clouds, and provides theoretical proofs of convergence to traditional scalar and Ricci curvature in continuous Riemannian manifolds through sampling techniques.

Result: The proposed scalar Ollivier-Ricci curvature converges to the traditional scalar curvature for nearest neighbor graphs sampled from manifolds. Additional theoretical insights into Ollivier-Ricci curvature's convergence to Ricci curvature are also presented.

Conclusion: Scalar Ollivier-Ricci curvature serves as a meaningful analogue for scalar curvature in discrete settings, facilitating deeper connections between discrete structures and manifold geometry.

Abstract: We study the relationship between discrete analogues of Ricci and scalar
curvature that are defined for point clouds and graphs. In the discrete
setting, Ricci curvature is replaced by Ollivier-Ricci curvature. Scalar
curvature can be computed as the trace of Ricci curvature for a Riemannian
manifold; this motivates a new definition of a scalar version of Ollivier-Ricci
curvature. We show that our definition converges to scalar curvature for
nearest neighbor graphs obtained by sampling from a manifold. We also prove
some new results about the convergence of Ollivier-Ricci curvature to Ricci
curvature.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [927] [A Benchmark Study of Deep Learning Methods for Multi-Label Pediatric Electrocardiogram-Based Cardiovascular Disease Classification](https://arxiv.org/abs/2510.03780)
*Yiqiao Chen*

Main category: eess.SP

TL;DR: This study benchmarks deep learning models for pediatric cardiovascular disease classification using the ZZU-pECG dataset, achieving strong classification accuracy, but identifying challenges in rare conditions.


<details>
  <summary>Details</summary>
Motivation: Cardiovascular disease is a significant pediatric health issue, requiring early screening methods like ECG for effective diagnosis.

Method: Benchmarking four deep learning models (ResNet-1D, BiLSTM, Transformer, and Mamba 2) for pediatric CVD classification under 9-lead and 12-lead ECG configurations.

Result: The models showed strong performance (F1-scores above 85%, Hamming Loss as low as 0.0069), with ResNet-1D achieving a macro-F1 of 94.67% and others showing competitive results.

Conclusion: The study establishes robust baselines and notes the need for larger datasets, broader analysis, and multi-center validation for practical pediatric CVD applications.

Abstract: Cardiovascular disease (CVD) is a major pediatric health burden, and early
screening is of critical importance. Electrocardiography (ECG), as a
noninvasive and accessible tool, is well suited for this purpose. This paper
presents the first benchmark study of deep learning for multi-label pediatric
CVD classification on the recently released ZZU-pECG dataset, comprising 3716
recordings with 19 CVD categories. We systematically evaluate four
representative paradigms--ResNet-1D, BiLSTM, Transformer, and Mamba 2--under
both 9-lead and 12-lead configurations. All models achieved strong results,
with Hamming Loss as low as 0.0069 and F1-scores above 85% in most settings.
ResNet-1D reached a macro-F1 of 94.67% on the 12-lead subset, while BiLSTM and
Transformer also showed competitive performance. Per-class analysis indicated
challenges for rare conditions such as hypertrophic cardiomyopathy in the
9-lead subset, reflecting the effect of limited positive samples. This
benchmark establishes reusable baselines and highlights complementary strengths
across paradigms. It further points to the need for larger-scale, multi-center
validation, age-stratified analysis, and broader disease coverage to support
real-world pediatric ECG applications.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [928] [Is it Bigger than a Breadbox: Efficient Cardinality Estimation for Real World Workloads](https://arxiv.org/abs/2510.03386)
*Zixuan Yi,Sami Abu-el-Haija,Yawen Wang,Teja Vemparala,Yannis Chronis,Yu Gan,Michael Burrows,Carsten Binnig,Bryan Perozzi,Ryan Marcus,Fatma Ozcan*

Main category: cs.DB

TL;DR: The paper presents a low-overhead, practical machine learning-based approach for improving query execution plan efficiency by learning simple regressors localized to repetitive subquery patterns.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiencies of current DB engines' cost models (heavily reliant on heuristics) and the operational complexities of adopting learning-based cardinality estimators.

Method: Propose using lightweight, pattern-based online learned regressors that are accessed via hashed subquery structures, reducing overhead while maintaining accuracy.

Result: The approach achieves comparable accuracy to state-of-the-art methods while improving query runtime efficiency; specifically, amending PostgreSQL showed runtime improvements over traditional approaches with reduced operational costs.

Conclusion: The proposed method bridges the gap between practicality and efficiency, offering an effective solution with minimal overhead suitable for real-world DB workloads.

Abstract: DB engines produce efficient query execution plans by relying on cost models.
Practical implementations estimate cardinality of queries using heuristics,
with magic numbers tuned to improve average performance on benchmarks.
Empirically, estimation error significantly grows with query complexity.
Alternatively, learning-based estimators offer improved accuracy, but add
operational complexity preventing their adoption in-practice. Recognizing that
query workloads contain highly repetitive subquery patterns, we learn many
simple regressors online, each localized to a pattern. The regressor
corresponding to a pattern can be randomly-accessed using hash of graph
structure of the subquery. Our method has negligible overhead and competes with
SoTA learning-based approaches on error metrics. Further, amending PostgreSQL
with our method achieves notable accuracy and runtime improvements over
traditional methods and drastically reduces operational costs compared to other
learned cardinality estimators, thereby offering the most practical and
efficient solution on the Pareto frontier. Concretely, simulating JOB-lite
workload on IMDb speeds-up execution by 7.5 minutes (>30%) while incurring only
37 seconds overhead for online learning.

</details>


<div id='cs.CG'></div>

# cs.CG [[Back]](#toc)

### [929] [Fast Witness Persistence for MRI Volumes via Hybrid Landmarking](https://arxiv.org/abs/2510.04553)
*Jorge Leonardo Ruiz Williams*

Main category: cs.CG

TL;DR: The paper introduces 'whale-tda,' a GPU-efficient persistent homology pipeline for MRI volumetric data, outperforming conventional methods while preserving topological consistency.


<details>
  <summary>Details</summary>
Motivation: Persistent homology is vital for analyzing MRI-derived topological features, but traditional methods are computationally prohibitive for full MRI volumes due to high complexity.

Method: They propose a pipeline combining density-aware landmark selection with GPU-optimized witness filtration. Their landmark selection uses a hybrid metric preventing geometric and density bias.

Result: The approach reduced mean pairwise distances by 30-60% compared to baselines and processed in under 10 seconds on an RTX 4090 GPU while ensuring scalability and maintaining topological fidelity.

Conclusion: The pipeline is a fast, scalable solution integrating with medical workflows, improving computational efficiency greatly while preserving essential topological details.

Abstract: We introduce a scalable witness-based persistent homology pipeline for
full-brain MRI volumes that couples density-aware landmark selection with a
GPU-ready witness filtration. Candidates are scored by a hybrid metric that
balances geometric coverage against inverse kernel density, yielding landmark
sets that shrink mean pairwise distances by 30-60% over random or density-only
baselines while preserving topological features. Benchmarks on BrainWeb, IXI,
and synthetic manifolds execute in under ten seconds on a single NVIDIA RTX
4090 GPU, avoiding the combinatorial blow-up of Cech, Vietoris-Rips, and alpha
filtrations. The package is distributed on PyPI as whale-tda (installable via
pip); source and issues are hosted at https://github.com/jorgeLRW/whale. The
release also exposes a fast preset (mri_deep_dive_fast) for exploratory sweeps,
and ships with reproducibility-focused scripts and artifacts for drop-in use in
medical imaging workflows.

</details>


### [930] [Cellular Learning: Scattered Data Regression in High Dimensions via Voronoi Cells](https://arxiv.org/abs/2510.03810)
*Shankar Prasad Sastry*

Main category: cs.CG

TL;DR: The paper presents a regression algorithm using piecewise-smooth functions over Voronoi cells to approximate scattered data, achieving 98.2% accuracy on the MNIST dataset.


<details>
  <summary>Details</summary>
Motivation: To develop a scalable, high-dimensional algorithm capable of approximating scattered data with high accuracy by avoiding explicit computations of Voronoi diagrams.

Method: The algorithm composes and blends linear functions over Voronoi cells inferred from seed vertices, bypassing the curse of dimensionality by not explicitly generating Voronoi diagrams.

Result: The algorithm achieves 98.2% accuracy on MNIST with 722,200 degrees of freedom, without utilizing data augmentation, convolution, or other geometric operators.

Conclusion: The approach demonstrates effective scalability and applicability, providing a powerful tool for approximating scattered data in high-dimensional spaces.

Abstract: I present a regression algorithm that provides a continuous, piecewise-smooth
function approximating scattered data. It is based on composing and blending
linear functions over Voronoi cells, and it scales to high dimensions. The
algorithm infers Voronoi cells from seed vertices and constructs a linear
function for the input data in and around each cell. As the algorithm does not
explicitly compute the Voronoi diagram, it avoids the curse of dimensionality.
An accuracy of around 98.2% on the MNIST dataset with 722,200 degrees of
freedom (without data augmentation, convolution, or other geometric operators)
demonstrates the applicability and scalability of the algorithm.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [931] [AtomWorld: A Benchmark for Evaluating Spatial Reasoning in Large Language Models on Crystalline Materials](https://arxiv.org/abs/2510.04704)
*Taoyuze Lv,Alexander Chen,Fengyu Xie,Chu Wu,Jeffrey Meng,Dongzhan Zhou,Bram Hoex,Zhicheng Zhong,Tong Xie*

Main category: cond-mat.mtrl-sci

TL;DR: This paper introduces the AtomWorld benchmark to evaluate and analyze the spatial reasoning and structural understanding capabilities of Large Language Models (LLMs) in the context of atomic structures, highlighting critical gaps in their performance.


<details>
  <summary>Details</summary>
Motivation: The study was motivated by the need to combine textual reasoning and spatial understanding abilities of LLMs for complex tasks, particularly in domains like materials science where detailed comprehension of 3D atomic structures is necessary.

Method: The authors designed the AtomWorld benchmark using tasks based on Crystallographic Information Files (CIFs), which included structural editing, CIF perception, and property-guided modeling. Experiments tested LLM capabilities in understanding and manipulating atomic structures.

Result: Experiments revealed that current LLMs fail to perform well on structural modification and CIF comprehension tasks, displaying frequent errors and challenges in spatial reasoning.

Conclusion: By standardizing evaluation tasks, AtomWorld aims to drive advancements in atomic-scale modeling with LLMs, supporting automation and progress in materials science research.

Abstract: Large Language Models (LLMs) excel at textual reasoning and are beginning to
develop spatial understanding, prompting the question of whether these
abilities can be combined for complex, domain-specific tasks. This question is
essential in fields like materials science, where deep understanding of 3D
atomic structures is fundamental. While initial studies have successfully
applied LLMs to tasks involving pure crystal generation or coordinate
understandings, a standardized benchmark to systematically evaluate their core
reasoning abilities across diverse atomic structures has been notably absent.
To address this gap, we introduce the AtomWorld benchmark to evaluate LLMs on
tasks based in Crystallographic Information Files (CIFs), a standard structure
representation format. These tasks, including structural editing, CIF
perception, and property-guided modeling, reveal a critical limitation: current
models, despite establishing promising baselines, consistently fail in
structural understanding and spatial reasoning. Our experiments show that these
models make frequent errors on structure modification tasks, and even in the
basic CIF format understandings, potentially leading to cumulative errors in
subsequent analysis and materials insights. By defining these standardized
tasks, AtomWorld lays the ground for advancing LLMs toward robust atomic-scale
modeling, crucial for accelerating materials research and automating scientific
workflows.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [932] [Désentrelacement Fréquentiel Doux pour les Codecs Audio Neuronaux](https://arxiv.org/abs/2510.03741)
*Benoît Giniès,Xiaoyu Bie,Olivier Fercoq,Gaël Richard*

Main category: cs.SD

TL;DR: The paper introduces a disentangled neural audio codec with spectral decomposition to improve interpretability of audio representations and shows superior reconstruction fidelity and perceptual quality compared to a baseline.


<details>
  <summary>Details</summary>
Motivation: Neural models excel in audio feature extraction but their learned representations often lack interpretability. Existing disentanglement methods show dependency issues on datasets and task setups.

Method: The proposed method combines disentangled neural audio codec with spectral decomposition in time-domain signals to structure the extracted representations.

Result: Experimental results highlight improved reconstruction fidelity and perceptual quality, outperforming a state-of-the-art baseline.

Conclusion: Leveraging spectral decomposition enhances interpretability in audio feature representations and addresses limitations of previous disentanglement techniques.

Abstract: While neural-based models have led to significant advancements in audio
feature extraction, the interpretability of the learned representations remains
a critical challenge. To address this, disentanglement techniques have been
integrated into discrete neural audio codecs to impose structure on the
extracted tokens. However, these approaches often exhibit strong dependencies
on specific datasets or task formulations. In this work, we propose a
disentangled neural audio codec that leverages spectral decomposition of
time-domain signals to enhance representation interpretability. Experimental
evaluations demonstrate that our method surpasses a state-of-the-art baseline
in both reconstruction fidelity and perceptual quality.

</details>


### [933] [Linguistic and Audio Embedding-Based Machine Learning for Alzheimer's Dementia and Mild Cognitive Impairment Detection: Insights from the PROCESS Challenge](https://arxiv.org/abs/2510.03336)
*Adharsha Sam Edwin Sam Devahi,Sohail Singh Sangha,Prachee Priyadarshinee,Jithin Thilakan,Ivan Fu Xing Tan,Christopher Johann Clarke,Sou Ka Lon,Balamurali B T,Yow Wei Quin,Chen Jer-Ming*

Main category: cs.SD

TL;DR: This study presents a machine learning framework using linguistic and acoustic features from speech to detect Alzheimer's Dementia (AD) and Mild Cognitive Impairment (MCI).


<details>
  <summary>Details</summary>
Motivation: Early intervention in Alzheimer's Dementia (AD) and Mild Cognitive Impairment (MCI) is critical; however, current methods are resource-intensive and invasive. Speech, as a non-invasive biomarker, offers a potential alternative.

Method: The study used Whisper embeddings for audio features and linguistic markers (e.g., pronoun usage, syntactic complexity) from spontaneous speech tasks. Ensemble models were developed for classification of cognitive states and MMSE score prediction.

Result: Ensemble models combining linguistic features achieved the best classification performance (F1 = 0.497). Ensemble regressors based on Whisper embeddings obtained the lowest MMSE prediction error (RMSE = 2.843).

Conclusion: The findings validate the viability of using multimodal speech-based methods for non-invasive dementia detection and emphasize the benefits of combining linguistic and acoustic features.

Abstract: Early detection of Alzheimer's Dementia (AD) and Mild Cognitive Impairment
(MCI) is critical for timely intervention, yet current diagnostic approaches
remain resource-intensive and invasive. Speech, encompassing both acoustic and
linguistic dimensions, offers a promising non-invasive biomarker for cognitive
decline. In this study, we present a machine learning framework for the PROCESS
Challenge, leveraging both audio embeddings and linguistic features derived
from spontaneous speech recordings. Audio representations were extracted using
Whisper embeddings from the Cookie Theft description task, while linguistic
features-spanning pronoun usage, syntactic complexity, filler words, and clause
structure-were obtained from transcriptions across Semantic Fluency, Phonemic
Fluency, and Cookie Theft picture description. Classification models aimed to
distinguish between Healthy Controls (HC), MCI, and AD participants, while
regression models predicted Mini-Mental State Examination (MMSE) scores.
Results demonstrated that voted ensemble models trained on concatenated
linguistic features achieved the best classification performance (F1 = 0.497),
while Whisper embedding-based ensemble regressors yielded the lowest MMSE
prediction error (RMSE = 2.843). Comparative evaluation within the PROCESS
Challenge placed our models among the top submissions in regression task, and
mid-range for classification, highlighting the complementary strengths of
linguistic and audio embeddings. These findings reinforce the potential of
multimodal speech-based approaches for scalable, non-invasive cognitive
assessment and underline the importance of integrating task-specific linguistic
and acoustic markers in dementia detection.

</details>


### [934] [Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba](https://arxiv.org/abs/2510.04738)
*Baher Mohammad,Magauiya Zhussip,Stamatios Lefkimmiatis*

Main category: cs.SD

TL;DR: MAVE (Mamba with Cross-Attention for Voice Editing and Synthesis) is a novel system for text-guided voice editing and text-to-speech synthesis, achieving top-tier results in both tasks with efficient memory usage.


<details>
  <summary>Details</summary>
Motivation: The paper aims to develop a more effective system for voice editing and text-to-speech synthesis that combines high fidelity, speaker consistency, and computational efficiency.

Method: The authors use an autoregressive architecture called MAVE, which integrates the Mamba backbone for audio modeling and cross-attention to align text with acoustics, enabling context-aware voice editing and synthesis.

Result: MAVE outperformed leading autoregressive and diffusion models in voice editing and displayed competitive zero-shot TTS capabilities, achieving close fidelity to original audio in human evaluations and requiring significantly less memory than state-of-the-art alternatives.

Conclusion: MAVE sets a new benchmark in voice editing and synthesis by effectively combining structured state-space modeling and cross-modal attention for superior performance, naturalness, and computational efficiency.

Abstract: We introduce MAVE (Mamba with Cross-Attention for Voice Editing and
Synthesis), a novel autoregressive architecture for text-conditioned voice
editing and high-fidelity text-to-speech (TTS) synthesis, built on a
cross-attentive Mamba backbone. MAVE achieves state-of-the-art performance in
speech editing and very competitive results in zero-shot TTS, while not being
explicitly trained on the latter task, outperforming leading autoregressive and
diffusion models on diverse, real-world audio. By integrating Mamba for
efficient audio sequence modeling with cross-attention for precise
text-acoustic alignment, MAVE enables context-aware voice editing with
exceptional naturalness and speaker consistency. In pairwise human evaluations
on a random 40-sample subset of the RealEdit benchmark (400 judgments), 57.2%
of listeners rated MAVE - edited speech as perceptually equal to the original,
while 24.8% prefered the original and 18.0% MAVE - demonstrating that in the
majority of cases edits are indistinguishable from the source. MAVE compares
favorably with VoiceCraft and FluentSpeech both on pairwise comparisons and
standalone mean opinion score (MOS) evaluations. For zero-shot TTS, MAVE
exceeds VoiceCraft in both speaker similarity and naturalness, without
requiring multiple inference runs or post-processing. Remarkably, these quality
gains come with a significantly lower memory cost and approximately the same
latency: MAVE requires ~6x less memory than VoiceCraft during inference on
utterances from the RealEdit database (mean duration: 6.21s, A100, FP16, batch
size 1). Our results demonstrate that MAVE establishes a new standard for
flexible, high-fidelity voice editing and synthesis through the synergistic
integration of structured state-space modeling and cross-modal attention.

</details>


### [935] [Pitch-Conditioned Instrument Sound Synthesis From an Interactive Timbre Latent Space](https://arxiv.org/abs/2510.04339)
*Christian Limberg,Fares Schulz,Zhe Zhang,Stefan Weinzierl*

Main category: cs.SD

TL;DR: The paper proposes a new approach to neural instrument sound synthesis using a two-stage framework combining Variational Autoencoders and Transformer-based models, creating an intuitive interface for pitch-accurate, high-quality audio generation.


<details>
  <summary>Details</summary>
Motivation: Existing sound synthesis methods rely on high-dimensional latent spaces, which are challenging to navigate and limit user experience. The paper aims to make the system more user-friendly and intuitive by disentangling pitch and timbre.

Method: A two-stage semi-supervised training framework is employed: (1) a Variational Autoencoder creates a disentangled 2D pitch-timbre representation of audio samples; (2) the representation is used to condition a Transformer-based generative model.

Result: The model learns expressive and controllable timbre spaces while ensuring reliable pitch accuracy. Experimental results confirm its ability to generate nuanced timbre variations and integrate into actionable web-based applications.

Conclusion: This method successfully simplifies interaction with generative sound systems and holds promise for shaping future, user-friendly music production tools that balance creativity and technical precision.

Abstract: This paper presents a novel approach to neural instrument sound synthesis
using a two-stage semi-supervised learning framework capable of generating
pitch-accurate, high-quality music samples from an expressive timbre latent
space. Existing approaches that achieve sufficient quality for music production
often rely on high-dimensional latent representations that are difficult to
navigate and provide unintuitive user experiences. We address this limitation
through a two-stage training paradigm: first, we train a pitch-timbre
disentangled 2D representation of audio samples using a Variational
Autoencoder; second, we use this representation as conditioning input for a
Transformer-based generative model. The learned 2D latent space serves as an
intuitive interface for navigating and exploring the sound landscape. We
demonstrate that the proposed method effectively learns a disentangled timbre
space, enabling expressive and controllable audio generation with reliable
pitch conditioning. Experimental results show the model's ability to capture
subtle variations in timbre while maintaining a high degree of pitch accuracy.
The usability of our method is demonstrated in an interactive web application,
highlighting its potential as a step towards future music production
environments that are both intuitive and creatively empowering:
https://pgesam.faresschulz.com

</details>


### [936] [Lightweight and Generalizable Acoustic Scene Representations via Contrastive Fine-Tuning and Distillation](https://arxiv.org/abs/2510.03728)
*Kuang Yuan,Yang Gao,Xilin Li,Xinhao Mei,Syavosh Zadissa,Tarun Pruthi,Saeed Bagheri Sereshki*

Main category: cs.SD

TL;DR: This paper introduces ContrastASC, an approach for acoustic scene classification that allows adaptation to unseen categories without retraining, providing better few-shot learning capabilities.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the limitations of ASC models on edge devices that are unable to adapt to new or refined acoustic categories in real-world applications.

Method: The authors propose supervised contrastive fine-tuning of pre-trained models and contrastive representation distillation to transfer structured knowledge to compact student models.

Result: ContrastASC shows enhanced few-shot adaptation to unseen categories while retaining robust closed-set performance.

Conclusion: ContrastASC offers a generalizable approach to ASC by structuring acoustic scene representations for improved flexibility and adaptability in real-world scenarios.

Abstract: Acoustic scene classification (ASC) models on edge devices typically operate
under fixed class assumptions, lacking the transferability needed for
real-world applications that require adaptation to new or refined acoustic
categories. We propose ContrastASC, which learns generalizable acoustic scene
representations by structuring the embedding space to preserve semantic
relationships between scenes, enabling adaptation to unseen categories without
retraining. Our approach combines supervised contrastive fine-tuning of
pre-trained models with contrastive representation distillation to transfer
this structured knowledge to compact student models. Our evaluation shows that
ContrastASC demonstrates improved few-shot adaptation to unseen categories
while maintaining strong closed-set performance.

</details>


### [937] [Language Model Based Text-to-Audio Generation: Anti-Causally Aligned Collaborative Residual Transformers](https://arxiv.org/abs/2510.04577)
*Juncheng Wang,Chao Xu,Cheng Yu,Zhe Hu,Haoyu Xie,Guoqi Yu,Lei Shang,Shujun Wang*

Main category: cs.SD

TL;DR: The paper introduces Siren, a new framework for text-to-audio (T2A) generation that improves upon past language model (LM)-based systems and rivals diffusion-based models by addressing shortcomings in residual vector quantization (RVQ) tokenizers.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the limitations of current language model (LM)-based T2A generation systems in bridging the gap in performance compared to diffusion-based models, attributed to issues in RVQ's dynamic representation.

Method: The authors analyzed RVQ limitations and proposed Siren, a framework using multiple isolated transformers, causal and anti-causal conditioning, and reinforcement learning for improved T2A generation.

Result: Experiments show that Siren outperforms both LM-based and diffusion-based T2A systems, achieving state-of-the-art performance.

Conclusion: Siren's integration of LM strengths with improved audio fidelity offers a competitive alternative to diffusion models for T2A tasks, while also establishing a pathway for unified multi-modal generation frameworks.

Abstract: While language models (LMs) paired with residual vector quantization (RVQ)
tokenizers have shown promise in text-to-audio (T2A) generation, they still lag
behind diffusion-based models by a non-trivial margin. We identify a critical
dilemma underpinning this gap: incorporating more RVQ layers improves audio
reconstruction fidelity but exceeds the generation capacity of conventional
LMs. To address this, we first analyze RVQ dynamics and uncover two key
limitations: 1) orthogonality of features across RVQ layers hinders effective
LMs training, and 2) descending semantic richness in tokens from deeper RVQ
layers exacerbates exposure bias during autoregressive decoding. Based on these
insights, we propose Siren, a novel LM-based framework that employs multiple
isolated transformers with causal conditioning and anti-causal alignment via
reinforcement learning. Extensive experiments demonstrate that Siren
outperforms both existing LM-based and diffusion-based T2A systems, achieving
state-of-the-art results. By bridging the representational strengths of LMs
with the fidelity demands of audio synthesis, our approach repositions LMs as
competitive contenders against diffusion models in T2A tasks. Moreover, by
aligning audio representations with linguistic structures, Siren facilitates a
promising pathway toward unified multi-modal generation frameworks.

</details>


### [938] [A Study on the Data Distribution Gap in Music Emotion Recognition](https://arxiv.org/abs/2510.04688)
*Joann Ching,Gerhard Widmer*

Main category: cs.SD

TL;DR: This paper addresses Music Emotion Recognition by examining genre diversity, dataset biases, and improving cross-dataset generalization using a framework combining Jukebox embeddings and chroma features.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations in music emotion recognition research that focus narrowly on specific genres and introduce methods that account for diversity across musical styles and genre-emotion relationships.

Method: Systematic experiments on five datasets spanning diverse genres, exploring genre dominance and dataset biases, and creating a framework combining Jukebox embeddings with chroma features.

Result: Improved cross-dataset generalization by combining embeddings from Jukebox and chroma features within a diverse multi-dataset training framework.

Conclusion: The proposed framework significantly enhances the cross-dataset generalization capabilities for emotion recognition, offering insights into genre-emotion relationships and dataset bias handling.

Abstract: Music Emotion Recognition (MER) is a task deeply connected to human
perception, relying heavily on subjective annotations collected from
contributors. Prior studies tend to focus on specific musical styles rather
than incorporating a diverse range of genres, such as rock and classical,
within a single framework. In this paper, we address the task of recognizing
emotion from audio content by investigating five datasets with dimensional
emotion annotations -- EmoMusic, DEAM, PMEmo, WTC, and WCMED -- which span
various musical styles. We demonstrate the problem of out-of-distribution
generalization in a systematic experiment. By closely looking at multiple data
and feature sets, we provide insight into genre-emotion relationships in
existing data and examine potential genre dominance and dataset biases in
certain feature representations. Based on these experiments, we arrive at a
simple yet effective framework that combines embeddings extracted from the
Jukebox model with chroma features and demonstrate how, alongside a combination
of several diverse training sets, this permits us to train models with
substantially improved cross-dataset generalization capabilities.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [939] [A Case for Declarative LLM-friendly Interfaces for Improved Efficiency of Computer-Use Agents](https://arxiv.org/abs/2510.04607)
*Yuan Wang,Mingyu Li,Haibo Chen*

Main category: cs.OS

TL;DR: The paper introduces Goal-Oriented Interface (GOI), a novel framework that simplifies graphical user interfaces (GUIs) for large language models (LLMs), leading to significantly improved task success rates and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing GUIs are designed for humans and pose challenges for LLMs, forcing them to break down tasks into complex, error-prone sequences, thereby reducing their effectiveness and efficiency.

Method: The GOI framework abstracts GUIs into three declarative primitives: access, state, and observation. It separates high-level planning (handled by LLMs) from low-level interaction (handled by GOI) without requiring modifications to the application source code.

Result: When tested on Microsoft Office Suite applications, GOI showed a 67% improvement in task success rates and a 43.5% reduction in interaction steps compared to GUI-based methods. Over 61% of tasks were completed with a single LLM call.

Conclusion: GOI effectively makes GUIs more LLM-friendly without altering application source code, significantly enhancing the automation capabilities of LLM-powered agents.

Abstract: Computer-use agents (CUAs) powered by large language models (LLMs) have
emerged as a promising approach to automating computer tasks, yet they struggle
with graphical user interfaces (GUIs). GUIs, designed for humans, force LLMs to
decompose high-level goals into lengthy, error-prone sequences of fine-grained
actions, resulting in low success rates and an excessive number of LLM calls.
  We propose Goal-Oriented Interface (GOI), a novel abstraction that transforms
existing GUIs into three declarative primitives: access, state, and
observation, which are better suited for LLMs. Our key idea is policy-mechanism
separation: LLMs focus on high-level semantic planning (policy) while GOI
handles low-level navigation and interaction (mechanism). GOI does not require
modifying the application source code or relying on application programming
interfaces (APIs).
  We evaluate GOI with Microsoft Office Suite (Word, PowerPoint, Excel) on
Windows. Compared to a leading GUI-based agent baseline, GOI improves task
success rates by 67% and reduces interaction steps by 43.5%. Notably, GOI
completes over 61% of successful tasks with a single LLM call.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [940] [Quantum feature-map learning with reduced resource overhead](https://arxiv.org/abs/2510.03389)
*Jonas Jäger,Philipp Elsässer,Elham Torabian*

Main category: quant-ph

TL;DR: Q-FLAIR is introduced to reduce quantum resource overhead in quantum feature-map learning by leveraging classical resources for iterative optimization. It outperforms benchmarks and enables higher accuracy on real-world datasets like full-resolution MNIST on near-term quantum devices.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of quantum computing resources, especially in quantum machine learning, by developing efficient feature-map learning algorithms.

Method: A hybrid approach called Q-FLAIR is presented, which uses classical computation to iteratively construct quantum feature-map circuits. It employs partial analytic reconstructions and classical optimization of feature and weight parameters for each gate addition.

Result: Q-FLAIR achieves state-of-the-art performance on benchmarks. It decouples resource overhead from feature dimension and achieves over 90% accuracy on the full-resolution MNIST dataset using a real IBM quantum device in just four hours.

Conclusion: Q-FLAIR reduces quantum resource requirements, making quantum machine learning more feasible for real-world applications on near-term quantum computers.

Abstract: Current quantum computers require algorithms that use limited resources
economically. In quantum machine learning, success hinges on quantum feature
maps, which embed classical data into the state space of qubits. We introduce
Quantum Feature-Map Learning via Analytic Iterative Reconstructions (Q-FLAIR),
an algorithm that reduces quantum resource overhead in iterative feature-map
circuit construction. It shifts workloads to a classical computer via partial
analytic reconstructions of the quantum model, using only a few evaluations.
For each probed gate addition to the ansatz, the simultaneous selection and
optimization of the data feature and weight parameter is then entirely
classical. Integrated into quantum neural network and quantum kernel support
vector classifiers, Q-FLAIR shows state-of-the-art benchmark performance. Since
resource overhead decouples from feature dimension, we train a quantum model on
a real IBM device in only four hours, surpassing 90% accuracy on the
full-resolution MNIST dataset (784 features, digits 3 vs 5). Such results were
previously unattainable, as the feature dimension prohibitively drives hardware
demands for fixed and search costs for adaptive ans\"atze. By rethinking
feature-map learning beyond black-box optimization, this work takes a concrete
step toward enabling quantum machine learning for real-world problems and
near-term quantum computers.

</details>


### [941] [Quantum generative model on bicycle-sharing system and an application](https://arxiv.org/abs/2510.04512)
*Fumio Nemoto,Nobuyuki Koike,Daichi Sato,Yuuta Kawaai,Masayuki Ohzeki*

Main category: quant-ph

TL;DR: The paper introduces a quantum machine learning model to address bicycle shortages in bike-sharing systems by analyzing time series data and simulating proactive measures.


<details>
  <summary>Details</summary>
Motivation: The paper aims to solve the problem of bicycle shortages in urban bike-sharing systems, which happens during peak commuting times.

Method: It uses a quantum machine learning model to analyze time series data and Monte Carlo simulation to proactively distribute bicycles.

Result: The model effectively captures bicycle trends and port correlations, enabling simulation of improved system-wide rental numbers.

Conclusion: The approach provides a promising solution for bike-sharing systems and is applicable to other industrial scenarios.

Abstract: Recently, bicycle-sharing systems have been implemented in numerous cities,
becoming integral to daily life. However, a prevalent issue arises when
intensive commuting demand leads to bicycle shortages in specific areas and at
particular times. To address this challenge, we employ a novel quantum machine
learning model that analyzes time series data by fitting quantum time evolution
to observed sequences. This model enables us to capture actual trends in
bicycle counts at individual ports and identify correlations between different
ports. Utilizing the trained model, we simulate the impact of proactively
adding bicycles to high-demand ports on the overall rental number across the
system. Given that the core of this method lies in a Monte Carlo simulation, it
is anticipated to have a wide range of industrial applications.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [942] [Short-circuiting Rings for Low-Latency AllReduce](https://arxiv.org/abs/2510.03491)
*Sarah-Michelle Hammer,Stefan Schmid,Rachee Singh,Vamsi Addanki*

Main category: cs.NI

TL;DR: The paper revisits the performance comparison between the Ring and Recursive Doubling algorithms for AllReduce communication, demonstrating that the former is efficient even for short messages under certain conditions. It also explores adaptive topologies using photonic interconnects to further optimize communication.


<details>
  <summary>Details</summary>
Motivation: To challenge the existing belief that Recursive Doubling is superior for small messages in AllReduce collective communication and explore methods to enhance communication efficiency through dynamic topologies.

Method: The authors analytically and experimentally study the Ring and Recursive Doubling algorithms in real-world GPU-to-GPU topologies, accounting for propagation delays, link constraints, and congestion. They also propose a heuristic for circuit-switching in photonic interconnects.

Result: The study reveals that Ring can outperform Recursive Doubling for small messages due to lower congestion, and their photonic switching heuristic enables faster Recursive Doubling under dynamic topologies.

Conclusion: The paper demonstrates that dynamic, in-collective adaptive topologies, like those leveraging photonic interconnects, can improve communication efficiency. Future work should address practical challenges to implement such systems.

Abstract: Efficient collective communication is critical for many distributed ML and
HPC applications. In this context, it is widely believed that the Ring
algorithm for the AllReduce collective communication operation is optimal only
for large messages, while Recursive Doubling is preferable for small ones due
to its logarithmic number of steps compared to the linear number for Ring. In
this paper, we challenge this long-held assumption and show that the Ring
algorithm can remain optimal even for short messages in ring-based GPU-to-GPU
topologies, once realistic propagation delays and link capacity constraints are
accounted for. We find that the total propagation delay for both Ring and
Recursive Doubling essentially sums to the same value, but the latter incurs
significantly higher congestion due to longer hop counts, leading to increased
completion times. This surprising result motivates our case for in-collective
adaptive topologies, particularly in the context of emerging photonic
interconnects, which can break through the limitations of static topology
designs at the collective communication granularity. We design a \emph{simple
and fast} heuristic for circuit-switching that enables Recursive Doubling to
exploit dynamically reconfigurable photonic paths, carefully balancing
reconfiguration delays, propagation latencies, and link congestion to minimize
overall completion time. Our preliminary evaluations, using realistic
reconfiguration delays, show that our circuit-switching schedules enable faster
completion times for Recursive Doubling, even compared to Ring AllReduce on
static ring topologies. We conclude by highlighting key challenges and future
research directions for realizing practical, in-collective photonic switching.

</details>


### [943] [Scalable Ground Station Selection for Large LEO Constellations](https://arxiv.org/abs/2510.03438)
*Grace Ra Kim,Duncan Eddy,Vedant Srinivas,Mykel J. Kochenderfer*

Main category: cs.NI

TL;DR: The paper presents a scalable hierarchical method for selecting ground stations for LEO satellite constellations, achieving near-optimal performance while overcoming scalability issues inherent in traditional methods.


<details>
  <summary>Details</summary>
Motivation: The paper aims to solve the scalability and optimization challenges in selecting ground stations for LEO satellite constellations, which are crucial for minimizing costs, maximizing data downlink, and reducing communication gaps.

Method: The authors introduce a hierarchical framework that decomposes the problem into subproblems for single satellites and short time windows, clusters high-value locations, and matches them to existing GSaaS sites to deliver scalable and globally feasible solutions.

Result: Synthetic test cases and real-world evaluations showed that the method achieves solutions within 95% of the global IP optimum and scales effectively, outperforming traditional optimization approaches for large constellations.

Conclusion: The proposed framework provides a scalable and high-quality approach to ground station selection, avoiding the intractability issues of mixed-integer programming methods while maintaining near-optimal performance for real-world satellite networks.

Abstract: Effective ground station selection is critical for low Earth orbiting (LEO)
satellite constellations to minimize operational costs, maximize data downlink
volume, and reduce communication gaps between access windows. Traditional
ground station selection typically begins by choosing from a fixed set of
locations offered by Ground Station-as-a-Service (GSaaS) providers, which helps
reduce the problem scope to optimizing locations over existing infrastructure.
However, finding a globally optimal solution for stations using existing
mixed-integer programming methods quickly becomes intractable at scale,
especially when considering multiple providers and large satellite
constellations. To address this issue, we introduce a scalable, hierarchical
framework that decomposes the global selection problem into single-satellite,
short time-window subproblems. Optimal station choices from each subproblem are
clustered to identify consistently high-value locations across all decomposed
cases. Cluster-level sets are then matched back to the closest GSaaS candidate
sites to produce a globally feasible solution. This approach enables scalable
coordination while maintaining near-optimal performance. We evaluate our
method's performance on synthetic Walker-Star test cases (1-10 satellites, 1-10
stations), achieving solutions within 95% of the global IP optimum for all test
cases. Real-world evaluations on Capella Space (5 satellites), ICEYE (40), and
Planet's Flock (96) show that while exact IP solutions fail to scale, our
framework continues to deliver high-quality site selections.

</details>


### [944] [6G-Enabled Digital Twin Framework for Real-Time Cyber-Physical Systems: An Experimental Validation with Industrial Bearing Fault Detection](https://arxiv.org/abs/2510.03807)
*Vaskar Chakma,Wooyeol Choi*

Main category: cs.NI

TL;DR: The paper proposes a 6G-enabled Digital Twin framework addressing ultra-low latency and real-time synchronization challenges for industrial applications, achieving significant performance improvements in bearing fault detection.


<details>
  <summary>Details</summary>
Motivation: Current 5G-enabled CPS with DT technology cannot meet the sub-millisecond latencies required for critical applications like autonomous control and predictive maintenance.

Method: The framework integrates terahertz communications, intelligent reflecting surfaces, and edge AI within a five-layer architecture. It uses the CWRU bearing dataset for experimental validation with feature extraction and Random Forest classification.

Result: The proposed system achieved 97.7% fault classification accuracy with end-to-end latency of 0.8ms, significantly outperforming WiFi-6 (12.5ms) and 5G (4.2ms).

Conclusion: The 6G-enabled framework is effective for mission-critical applications, offering high fault classification accuracy, ultra-low latency, and scalability in detecting bearing faults across industrial contexts.

Abstract: Current Cyber-Physical Systems (CPS) integrated with Digital Twin (DT)
technology face critical limitations in achieving real-time performance for
mission-critical industrial applications. Existing 5G-enabled systems suffer
from latencies exceeding 10ms, which are inadequate for applications requiring
sub-millisecond response times, such as autonomous industrial control and
predictive maintenance. This research aims to develop and validate a 6G-enabled
Digital Twin framework that achieves ultra-low latency communication and
real-time synchronization between physical industrial assets and their digital
counterparts, specifically targeting bearing fault detection as a critical
industrial use case. The proposed framework integrates terahertz communications
(0.1-1 THz), intelligent reflecting surfaces, and edge artificial intelligence
within a five-layer architecture. Experimental validation was conducted using
the Case Western Reserve University (CWRU) bearing dataset, implementing
comprehensive feature extraction (15 time and frequency domain features) and
Random Forest classification algorithms. The system performance was evaluated
against traditional WiFi-6 and 5G networks across multiple metrics, including
classification accuracy, end-to-end latency, and scalability. It achieved 97.7%
fault classification accuracy with 0.8ms end-to-end latency, representing a
15.6x improvement over WiFi-6 (12.5ms) and 5.25x improvement over 5G (4.2ms)
networks. The system demonstrated superior scalability with sub-linear
processing time growth and maintained consistent performance across four
bearing fault categories (normal, inner race, outer race, and ball faults) with
macro-averaged F1-scores exceeding 97%.

</details>


### [945] [A4FN: an Agentic AI Architecture for Autonomous Flying Networks](https://arxiv.org/abs/2510.03829)
*André Coelho,Pedro Ribeiro,Helder Fontes,Rui Campos*

Main category: cs.NI

TL;DR: The paper introduces A4FN, an AI architecture for automating UAV-based networks using Generative AI and LLMs.


<details>
  <summary>Details</summary>
Motivation: To address automation in Flying Networks for critical scenarios by leveraging Agentic AI.

Method: A4FN architecture is split into Perception Agent and Decision-and-Action Agent to manage UAV networks dynamically.

Result: Key properties of Agentic AI applied for network control, with adaptive reconfiguration supporting disaster scenarios.

Conclusion: A4FN offers a novel direction for Agentic AI integration, but multi-agent coordination challenges remain open.

Abstract: This position paper presents A4FN, an Agentic Artificial Intelligence (AI)
architecture for intent-driven automation in Flying Networks (FNs) using
Unmanned Aerial Vehicles (UAVs) as access nodes. A4FN leverages Generative AI
and Large Language Models (LLMs) to enable real-time, context-aware network
control via a distributed agentic system. It comprises two components: the
Perception Agent (PA), which semantically interprets multimodal input --
including imagery, audio, and telemetry data -- from UAV-mounted sensors to
derive Service Level Specifications (SLSs); and the Decision-and-Action Agent
(DAA), which reconfigures the network based on inferred intents. A4FN embodies
key properties of Agentic AI, including autonomy, goal-driven reasoning, and
continuous perception-action cycles. Designed for mission-critical,
infrastructure-limited scenarios such as disaster response, it supports
adaptive reconfiguration, dynamic resource management, and interoperability
with emerging wireless technologies. The paper details the A4FN architecture,
its core innovations, and open research challenges in multi-agent coordination
and Agentic AI integration in next-generation FNs.

</details>


### [946] [Environment-Aware Indoor LoRaWAN Path Loss: Parametric Regression Comparisons, Shadow Fading, and Calibrated Fade Margins](https://arxiv.org/abs/2510.04346)
*Nahshon Mokua Obiri,Kristof Van Laerhoven*

Main category: cs.NI

TL;DR: The paper introduces an environment-aware path loss framework and enhanced reliability methods for indoor LoRaWAN propagation, outperforming traditional linear models.


<details>
  <summary>Details</summary>
Motivation: Current log-distance models and assumptions of log-normal shadowing struggle with indoor LoRaWAN propagation due to environmental and contextual factors.

Method: The authors devised a statistically disciplined framework with a log-distance model augmented by environmental covariates and signal-to-noise ratio. They used advanced statistical techniques like robust ANOVA, nested F tests, and kernel density estimation, testing various linear and polynomial regression approaches.

Result: The enhanced polynomial mean model reduces RMSE from 8.07 to 7.09 dB, improves R^2 from 0.81 to 0.86, and requires 25.7 dB coverage at 99% packet delivery ratio compared to 27.7 to 27.9 dB for linear baselines.

Conclusion: The study delivers a deployment-ready, interpretable workflow for IoT and 6G indoor planning with improved accuracy and reliability.

Abstract: Indoor LoRaWAN propagation is shaped by structural and time-varying context
factors, which challenge log-distance models and the assumption of log-normal
shadowing. We present an environment-aware, statistically disciplined path loss
framework evaluated using leakage-safe cross-validation on a 12-month campaign
in an eighth-floor office measuring 240 m^2. A log-distance multi-wall mean is
augmented with environmental covariates (relative humidity, temperature, carbon
dioxide, particulate matter, and barometric pressure), as well as the
signal-to-noise ratio. We compare multiple linear regression with regularized
variants, Bayesian linear regression, and a selective second-order polynomial
applied to continuous drivers. Predictor relevance is established using
heteroscedasticity-robust Type II and III analysis of variance and nested
partial F tests. Shadow fading is profiled with kernel density estimation and
non-parametric families, including Normal, Skew-Normal, Student's t, and
Gaussian mixtures. The polynomial mean reduces cross-validated RMSE from 8.07
to 7.09 dB and raises R^2 from 0.81 to 0.86. Out-of-fold residuals are
non-Gaussian; a 3-component mixture captures a sharp core with a light, broad
tail. We convert accuracy into reliability by prescribing the fade margin as
the upper-tail quantile of cross-validated residuals, quantifying uncertainty
via a moving-block bootstrap, and validating on a held-out set. At 99% packet
delivery ratio, the environment-aware polynomial requires 25.7 dB versus 27.7
to 27.9 dB for linear baselines. This result presents a deployment-ready,
interpretable workflow with calibrated reliability control for indoor Internet
of Things planning, aligned with 6G targets.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [947] [Bias and Coverage Properties of the WENDy-IRLS Algorithm](https://arxiv.org/abs/2510.03365)
*Abhi Chawla,David M. Bortz,Vanja Dukic*

Main category: stat.ME

TL;DR: The paper evaluates the WENDy-IRLS algorithm's capability to estimate parameters and states in nonlinear differential equations under diverse noise conditions.


<details>
  <summary>Details</summary>
Motivation: To assess how the WENDy-IRLS algorithm performs in estimating nonlinear dynamics parameters under various noise distributions and levels.

Method: Simulated data examples with five differential equations subjected to four noise distributions, with varying levels of noise, were employed to analyze the algorithm's robustness and accuracy.

Result: The study demonstrated that the WENDy-IRLS algorithm exhibits notable performance even under high levels of noise across multiple noise distributions.

Conclusion: WENDy-IRLS is a robust and computationally efficient method for nonlinear dynamics parameter estimation, with strong resilience to diverse and extreme noise conditions.

Abstract: The Weak form Estimation of Nonlinear Dynamics (WENDy) method is a recently
proposed class of parameter estimation algorithms that exhibits notable noise
robustness and computational efficiency. This work examines the coverage and
bias properties of the original WENDy-IRLS algorithm's parameter and state
estimators in the context of the following differential equations: Logistic,
Lotka-Volterra, FitzHugh-Nagumo, Hindmarsh-Rose, and a Protein Transduction
Benchmark. The estimators' performance was studied in simulated data examples,
under four different noise distributions (normal, log-normal, additive censored
normal, and additive truncated normal), and a wide range of noise, reaching
levels much higher than previously tested for this algorithm.

</details>


### [948] [Handling Missing Data in Probabilistic Regression Trees: Methods and Implementation in R](https://arxiv.org/abs/2510.03634)
*Taiane Schaedler Prass,Alisson Silva Neimaier,Guilherme Pumi*

Main category: stat.ME

TL;DR: The paper introduces an adaptation to Probabilistic Regression Trees (PRTrees) to handle missing data using three distinct methods, showcasing their advantages through simulations and offering tools in the PRTree package.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address challenges with missing values in datasets and extend the functionality of Probabilistic Regression Trees for broader, smoother decision-making applications.

Method: Three distinct approaches are proposed for handling missing data in PRTrees: uniform probability method, partial observation approach, and dimension-reduced smoothing technique. Performance comparisons were conducted through simulations under MCAR conditions.

Result: Simulation studies illustrated the effectiveness of each adaptation method and demonstrated the improved performance over traditional regression trees for smooth function estimation tasks.

Conclusion: The proposed adaptations successfully extend the applicability of PRTrees to incomplete datasets while preserving interpretability. These methods are implemented in the PRTree package, which is publicly available for researchers and practitioners.

Abstract: Probabilistic Regression Trees (PRTrees) generalize traditional decision
trees by incorporating probability functions that associate each data point
with different regions of the tree, providing smooth decisions and continuous
responses. This paper introduces an adaptation of PRTrees capable of handling
missing values in covariates through three distinct approaches: (i) a uniform
probability method, (ii) a partial observation approach, and (iii) a
dimension-reduced smoothing technique. The proposed methods preserve the
interpretability properties of PRTrees while extending their applicability to
incomplete datasets. Simulation studies under MCAR conditions demonstrate the
relative performance of each approach, including comparisons with traditional
regression trees on smooth function estimation tasks. The proposed methods,
together with the original version, have been developed in R with highly
optimized routines and are distributed in the PRTree package, publicly
available on CRAN. In this paper we also present and discuss the main
functionalities of the PRTree package, providing researchers and practitioners
with new tools for incomplete data analysis.

</details>


### [949] [Two new approaches to multiple canonical correlation analysis for repeated measures data](https://arxiv.org/abs/2510.04457)
*Tomasz Górecki,Mirosław Krzyśko,Felix Gnettner,Piotr Kokoszka*

Main category: stat.ME

TL;DR: The paper proposes two generalizations of classical canonical correlation analysis (CCA) for complex data structures and multidimensional random processes, using embeddings into Reproducing Kernel Hilbert Spaces.


<details>
  <summary>Details</summary>
Motivation: To extend classical CCA to handle more complex data structures and multidimensional random processes encountered in modern applications.

Method: The authors develop a new CCA framework for data with $L$ features over $T$ time points and multidimensional random processes, utilizing embeddings into Reproducing Kernel Hilbert Spaces (RKHS) and consistent estimators for the proposed methods.

Result: They demonstrate the approaches with their application to two data sets and derive consistency rates for the transformation and correlation estimators, relaxing assumptions on compactness and independence of data.

Conclusion: The generalizations presented provide a useful and theoretically justified extension of CCA for complex and functional data scenarios.

Abstract: In classical canonical correlation analysis (CCA), the goal is to determine
the linear transformations of two random vectors into two new random variables
that are most strongly correlated. Canonical variables are pairs of these new
random variables, while canonical correlations are correlations between these
pairs. In this paper, we propose and study two generalizations of this
classical method:
  (1) Instead of two random vectors we study more complex data structures that
appear in important applications. In these structures, there are $L$ features,
each described by $p_l$ scalars, $1 \le l \le L$. We observe $n$ such objects
over $T$ time points. We derive a suitable analog of the CCA for such data. Our
approach relies on embeddings into Reproducing Kernel Hilbert Spaces, and
covers several related data structures as well.
  (2) We develop an analogous approach for multidimensional random processes.
In this case, the experimental units are multivariate continuous,
square-integrable functions over a given interval. These functions are modeled
as elements of a Hilbert space, so in this case, we define the multiple
functional canonical correlation analysis, MFCCA.
  We justify our approaches by their application to two data sets and suitable
large sample theory. We derive consistency rates for the related transformation
and correlation estimators, and show that it is possible to relax two common
assumptions on the compactness of the underlying cross-covariance operators and
the independence of the data.

</details>


### [950] [A Contextual Quality Reward Model for Reliable and Efficient Best-of-N Sampling](https://arxiv.org/abs/2510.04087)
*Hyung Gyu Rho*

Main category: stat.ME

TL;DR: The paper addresses reliability issues in preference alignment models using a novel data collection and modeling framework that incorporates outside options to distinguish "good enough" responses. It introduces an adaptive inference strategy that reduces reliability failures and improves inference speed.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the shortcomings of current preference alignment techniques, such as the inability to assess response acceptability, which leads to systems selecting suboptimal outcomes, especially for complex prompts.

Method: The authors augment preference data with an "outside option" inspired by discrete choice models. They train a reward model capable of distinguishing not just relative preferences but also absolute acceptability. They propose an adaptive inference approach, called "best of mini-N in-loop," to optimize reliability and efficiency.

Result: The proposed framework reduces reliability failures by 70% when used as a guardrail and enhances inference speed by over 22% in an IMDB-sentiment setting when tuned for efficiency.

Conclusion: The paper presents a principled and adaptive framework that efficiently balances reliability and computational efficiency in preference alignment systems, offering a flexible tool for practitioners.

Abstract: Modern preference alignment techniques, such as Best-of-N (BoN) sampling,
rely on reward models trained with pairwise comparison data. While effective at
learning relative preferences, this paradigm fails to capture a signal of
response acceptability, leaving systems vulnerable to selecting the least bad
of many unacceptable options. This is particularly problematic for hard
prompts, where the risk of such false acceptances increases with the number of
samples. In this paper, we address this critical reliability gap by introducing
a new data collection and modeling framework. By augmenting preference data
with an outside option, inspired by discrete choice models, we train a reward
model that can distinguish not just what is \textit{better}, but what is
\textit{good enough}. We leverage this capability to create an adaptive
inference strategy, best of mini-N in-loop, which partitions the generation
budget into sequential loops with a calibrated, early-exit condition. Our
experiments show that when tuned as an alignment guardrail, it reduces
reliability failures by 70\%, and when tuned as an inference accelerator, it
improves average inference speed by over 22\% in IMDB-sentiment setting. We
thus provide a principled and flexible framework for practitioners to
explicitly manage the trade-off between reliability and computational
efficiency.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [951] [Score-based generative emulation of impact-relevant Earth system model outputs](https://arxiv.org/abs/2510.04358)
*Shahine Bouabid,Andre Nogueira Souza,Raffaele Ferrari*

Main category: physics.ao-ph

TL;DR: The paper introduces a climate model emulator using deep generative models, which closely matches Earth System Model outputs for planning climate impacts, but acknowledges limitations in capturing certain regime shifts.


<details>
  <summary>Details</summary>
Motivation: Outdated climate projections hinder effective adaptation and mitigation strategies, requiring faster and cheaper tools to simulate impacts of policy changes.

Method: The paper uses deep generative models with score-based diffusion on a spherical mesh, leveraging monthly ESM fields to evaluate variables relevant to impacts.

Result: The emulator closely matches ESM distributions and captures forced responses, but struggles with strong regime shifts in seasonal cycles.

Conclusion: While not perfect, the emulator's inaccuracies are negligible relative to ESM variability, making it a promising tool for impact assessments, with future improvements planned for higher resolution and bias-aware training.

Abstract: Policy targets evolve faster than the Couple Model Intercomparison Project
cycles, complicating adaptation and mitigation planning that must often contend
with outdated projections. Climate model output emulators address this gap by
offering inexpensive surrogates that can rapidly explore alternative futures
while staying close to Earth System Model (ESM) behavior. We focus on emulators
designed to provide inputs to impact models. Using monthly ESM fields of
near-surface temperature, precipitation, relative humidity, and wind speed, we
show that deep generative models have the potential to model jointly the
distribution of variables relevant for impacts. The specific model we propose
uses score-based diffusion on a spherical mesh and runs on a single mid-range
graphical processing unit. We introduce a thorough suite of diagnostics to
compare emulator outputs with their parent ESMs, including their probability
densities, cross-variable correlations, time of emergence, or tail behavior. We
evaluate performance across three distinct ESMs in both pre-industrial and
forced regimes. The results show that the emulator produces distributions that
closely match the ESM outputs and captures key forced responses. They also
reveal important failure cases, notably for variables with a strong regime
shift in the seasonal cycle. Although not a perfect match to the ESM, the
inaccuracies of the emulator are small relative to the scale of internal
variability in ESM projections. We therefore argue that it shows potential to
be useful in supporting impact assessment. We discuss priorities for future
development toward daily resolution, finer spatial scales, and bias-aware
training. Code is made available at https://github.com/shahineb/climemu.

</details>


### [952] [Deep learning the sources of MJO predictability: a spectral view of learned features](https://arxiv.org/abs/2510.03582)
*Lin Yao,Da Yang,James P. C. Duncan,Ashesh Chattopadhyay,Pedram Hassanzadeh,Wahid Bhimji,Bin Yu*

Main category: physics.ao-ph

TL;DR: This paper applies deep learning to better understand and forecast the Madden-Julian Oscillation (MJO), showing that large-scale patterns are dominant in predictability, but small-scale patterns also contribute meaningfully to shorter-term forecasts.


<details>
  <summary>Details</summary>
Motivation: The motivation is to resolve a significant debate in MJO theories regarding which spatial scales drive this phenomenon and improve MJO predictability, which is poorly understood but essential for global weather and climate prediction.

Method: The paper develops a deep convolutional neural network (DCNN) to forecast MJO indices (RMM and ROMI) and performs spectral analysis on the model's latent feature space to identify relevant spatial scales. Additional experiments are conducted with varying input scales to assess their impact on predictability.

Result: The DCNN achieves forecasting skills comparable to leading models, predicts RMM up to 21 days and ROMI up to 33 days, and identifies that large-scale patterns dominate predictability. Surprisingly, small-scale signals alone can predict MJO up to 1-2 weeks, hinting at their role in reconstructing large-scale dynamics.

Conclusion: Large-scale spatial patterns appear to be the primary source of MJO predictability, but small-scale patterns also have an informative role by reconstructing large-scale dynamics, supporting a multi-scale perspective of the MJO phenomenon.

Abstract: The Madden-Julian oscillation (MJO) is a planetary-scale, intraseasonal
tropical rainfall phenomenon crucial for global weather and climate; however,
its dynamics and predictability remain poorly understood. Here, we leverage
deep learning (DL) to investigate the sources of MJO predictability, motivated
by a central difference in MJO theories: which spatial scales are essential for
driving the MJO? We first develop a deep convolutional neural network (DCNN) to
forecast the MJO indices (RMM and ROMI). Our model predicts RMM and ROMI up to
21 and 33 days, respectively, achieving skills comparable to leading
subseasonal-to-seasonal models such as NCEP. To identify the spatial scales
most relevant for MJO forecasting, we conduct spectral analysis of the latent
feature space and find that large-scale patterns dominate the learned signals.
Additional experiments show that models using only large-scale signals as the
input have the same skills as those using all the scales, supporting the
large-scale view of the MJO. Meanwhile, we find that small-scale signals remain
informative: surprisingly, models using only small-scale input can still
produce skillful forecasts up to 1-2 weeks ahead. We show that this is achieved
by reconstructing the large-scale envelope of the small-scale activities, which
aligns with the multi-scale view of the MJO. Altogether, our findings support
that large-scale patterns--whether directly included or reconstructed--may be
the primary source of MJO predictability.

</details>


### [953] [Benchmarking atmospheric circulation variability in an AI emulator, ACE2, and a hybrid model, NeuralGCM](https://arxiv.org/abs/2510.04466)
*Ian Baxter,Hamid Pahlavan,Pedram Hassanzadeh,Katharine Rucker,Tiffany Shaw*

Main category: physics.ao-ph

TL;DR: The paper evaluates AI emulators and hybrid models in simulating atmospheric variability, identifying successes and limitations in capturing key dynamical metrics.


<details>
  <summary>Details</summary>
Motivation: To address biases in traditional physics-based atmosphere-land models by exploring the potential of AI emulators and hybrid models to represent atmospheric variability.

Method: The paper assesses the performance of a fully data-driven AI emulator (ACE2-ERA5) and a hybrid model (NeuralGCM) using four atmospheric variability benchmarking metrics.

Result: The models effectively represent large-scale wave spectra and extratropical eddy-mean flow interactions but have difficulty with quasi-biennial oscillation and Southern annular mode propagation timescales.

Conclusion: The benchmarking metrics provide valuable insights into the strengths and weaknesses of AI models, aiding their development and adaptation for broader climate applications.

Abstract: Physics-based atmosphere-land models with prescribed sea surface temperature
have notable successes but also biases in their ability to represent
atmospheric variability compared to observations. Recently, AI emulators and
hybrid models have emerged with the potential to overcome these biases, but
still require systematic evaluation against metrics grounded in fundamental
atmospheric dynamics. Here, we evaluate the representation of four atmospheric
variability benchmarking metrics in a fully data-driven AI emulator (ACE2-ERA5)
and hybrid model (NeuralGCM). The hybrid model and emulator can capture the
spectra of large-scale tropical waves and extratropical eddy-mean flow
interactions, including critical levels. However, both struggle to capture the
timescales associated with quasi-biennial oscillation (QBO, $\sim 28$ months)
and Southern annular mode propagation ($\sim 150$ days). These dynamical
metrics serve as an initial benchmarking tool to inform AI model development
and understand their limitations, which may be essential for
out-of-distribution applications (e.g., extrapolating to unseen climates).

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [954] [Investigating LLM Variability in Personalized Conversational Information Retrieval](https://arxiv.org/abs/2510.03795)
*Simon Lupart,Daniël van Dijk,Eric Langezaal,Ian van Dort,Mohammad Aliannejadi*

Main category: cs.IR

TL;DR: The paper investigates the variability and generalization in personalized Conversational Information Retrieval, focusing on integrating Personal Textual Knowledge Bases with Large Language Models for query reformulation.


<details>
  <summary>Details</summary>
Motivation: To address concerns about the variability and reliability of using Personal Textual Knowledge Bases (PTKB) in LLM-based personalized document retrieval.

Method: Reproduce earlier findings using the TREC iKAT 2024 dataset, evaluate numerous LLM models (Llama, Qwen, GPT variants), and assess model outputs for variance and generalizability across datasets and metrics.

Result: The study shows that human-selected PTKBs consistently improve retrieval, while LLM-based PTKB selection methods are less effective. Recall-based metrics exhibited lower variance compared to precision-oriented ones.

Conclusion: Multi-run evaluations and variance reporting are essential for robust evaluation of LLM-based CIR systems, and human-selected PTKBs are more reliable for enhancing retrieval.

Abstract: Personalized Conversational Information Retrieval (CIR) has seen rapid
progress in recent years, driven by the development of Large Language Models
(LLMs). Personalized CIR aims to enhance document retrieval by leveraging
user-specific information, such as preferences, knowledge, or constraints, to
tailor responses to individual needs. A key resource for this task is the TREC
iKAT 2023 dataset, designed to evaluate personalization in CIR pipelines.
Building on this resource, Mo et al. explored several strategies for
incorporating Personal Textual Knowledge Bases (PTKB) into LLM-based query
reformulation. Their findings suggested that personalization from PTKBs could
be detrimental and that human annotations were often noisy. However, these
conclusions were based on single-run experiments using the GPT-3.5 Turbo model,
raising concerns about output variability and repeatability. In this
reproducibility study, we rigorously reproduce and extend their work, focusing
on LLM output variability and model generalization. We apply the original
methods to the new TREC iKAT 2024 dataset and evaluate a diverse range of
models, including Llama (1B-70B), Qwen-7B, GPT-4o-mini. Our results show that
human-selected PTKBs consistently enhance retrieval performance, while
LLM-based selection methods do not reliably outperform manual choices. We
further compare variance across datasets and observe higher variability on iKAT
than on CAsT, highlighting the challenges of evaluating personalized CIR.
Notably, recall-oriented metrics exhibit lower variance than precision-oriented
ones, a critical insight for first-stage retrievers. Finally, we underscore the
need for multi-run evaluations and variance reporting when assessing LLM-based
CIR systems. By broadening evaluation across models, datasets, and metrics, our
study contributes to more robust and generalizable practices for personalized
CIR.

</details>


### [955] [Visual Lifelog Retrieval through Captioning-Enhanced Interpretation](https://arxiv.org/abs/2510.04010)
*Yu-Fei Shih,An-Zi Yen,Hen-Hsen Huang,Hsin-Hsi Chen*

Main category: cs.IR

TL;DR: This study introduces a Captioning-Integrated Visual Lifelog (CIVIL) Retrieval System to retrieve specific images from users' wearable camera-based visual lifelogs via textual queries.


<details>
  <summary>Details</summary>
Motivation: Memory recall is often hampered by difficulty in remembering specific past details, necessitating advanced lifelog retrieval systems for quick, effective access to personal visual data.

Method: The authors combined caption generation for life logs with a text embedding model to align user queries and captions in a shared vector space. They proposed three captioning approaches: single, collective, and merged captions.

Result: Experimental results indicated that the proposed methods accurately describe first-person visual data, improving lifelog retrieval effectiveness and reconstructing life experiences into captions.

Conclusion: Integrating textual captions with visual lifelogs enhances memory recall capabilities, with the developed dataset offering valuable support for lifelog interpretation and retrieval techniques.

Abstract: People often struggle to remember specific details of past experiences, which
can lead to the need to revisit these memories. Consequently, lifelog retrieval
has emerged as a crucial application. Various studies have explored methods to
facilitate rapid access to personal lifelogs for memory recall assistance. In
this paper, we propose a Captioning-Integrated Visual Lifelog (CIVIL) Retrieval
System for extracting specific images from a user's visual lifelog based on
textual queries. Unlike traditional embedding-based methods, our system first
generates captions for visual lifelogs and then utilizes a text embedding model
to project both the captions and user queries into a shared vector space.
Visual lifelogs, captured through wearable cameras, provide a first-person
viewpoint, necessitating the interpretation of the activities of the individual
behind the camera rather than merely describing the scene. To address this, we
introduce three distinct approaches: the single caption method, the collective
caption method, and the merged caption method, each designed to interpret the
life experiences of lifeloggers. Experimental results show that our method
effectively describes first-person visual images, enhancing the outcomes of
lifelog retrieval. Furthermore, we construct a textual dataset that converts
visual lifelogs into captions, thereby reconstructing personal life
experiences.

</details>


### [956] [Learning-Based Hashing for ANN Search: Foundations and Early Advances](https://arxiv.org/abs/2510.04127)
*Sean Moran*

Main category: cs.IR

TL;DR: This paper surveys foundational learning-based hashing methods for Approximate Nearest Neighbour (ANN) search, discussing supervised, unsupervised, and semi-supervised approaches while focusing on historical context and conceptual foundations.


<details>
  <summary>Details</summary>
Motivation: ANN search is crucial for large-scale applications across various domains. Hashing-based methods improve efficiency by mapping high-dimensional data to compact binary representations.

Method: The paper reviews projection functions for embedding creation and quantisation strategies for binary code conversion, covering supervised, unsupervised, semi-supervised models, and early cross-modal retrieval advancements.

Result: It organizes early learning-based hashing models, emphasizing core ideas, principles, and historical developments relevant to ANN search.

Conclusion: Understanding foundational methods in learning-based hashing equips researchers with insights into principles, trade-offs, and ongoing challenges in ANN-related research.

Abstract: Approximate Nearest Neighbour (ANN) search is a fundamental problem in
information retrieval, underpinning large-scale applications in computer
vision, natural language processing, and cross-modal search. Hashing-based
methods provide an efficient solution by mapping high-dimensional data into
compact binary codes that enable fast similarity computations in Hamming space.
Over the past two decades, a substantial body of work has explored learning to
hash, where projection and quantisation functions are optimised from data
rather than chosen at random.
  This article offers a foundational survey of early learning-based hashing
methods, with an emphasis on the core ideas that shaped the field. We review
supervised, unsupervised, and semi-supervised approaches, highlighting how
projection functions are designed to generate meaningful embeddings and how
quantisation strategies convert these embeddings into binary codes. We also
examine extensions to multi-bit and multi-threshold models, as well as early
advances in cross-modal retrieval.
  Rather than providing an exhaustive account of the most recent methods, our
goal is to introduce the conceptual foundations of learning-based hashing for
ANN search. By situating these early models in their historical context, we aim
to equip readers with a structured understanding of the principles, trade-offs,
and open challenges that continue to inform current research in this area.

</details>


### [957] [Empowering Denoising Sequential Recommendation with Large Language Model Embeddings](https://arxiv.org/abs/2510.04239)
*Tongzhou Wu,Yuhao Wang,Maolin Wang,Chi Zhang,Xiangyu Zhao*

Main category: cs.IR

TL;DR: The paper introduces a new framework, IADSR, for sequential recommendation systems, addressing noise issues in user-item interactions by combining collaborative and semantic information.


<details>
  <summary>Details</summary>
Motivation: Sequential recommendation systems suffer from suboptimal performance due to noisy interactions, especially over-denoising problems for cold items when relying solely on collaborative information.

Method: IADSR involves a two-stage process: 1) deriving collaborative and semantic embeddings from a sequential recommendation model and a large language model (LLM); 2) aligning these embeddings and identifying noise based on users' long-term and short-term interests.

Result: Extensive experiments on four public datasets show the framework's effectiveness and its compatibility with various sequential recommendation models.

Conclusion: The integration of collaborative and semantic information helps mitigate noise in sequential recommendation systems, enhancing their performance and robustness.

Abstract: Sequential recommendation aims to capture user preferences by modeling
sequential patterns in user-item interactions. However, these models are often
influenced by noise such as accidental interactions, leading to suboptimal
performance. Therefore, to reduce the effect of noise, some works propose
explicitly identifying and removing noisy items. However, we find that simply
relying on collaborative information may result in an over-denoising problem,
especially for cold items. To overcome these limitations, we propose a novel
framework: Interest Alignment for Denoising Sequential Recommendation (IADSR)
which integrates both collaborative and semantic information. Specifically,
IADSR is comprised of two stages: in the first stage, we obtain the
collaborative and semantic embeddings of each item from a traditional
sequential recommendation model and an LLM, respectively. In the second stage,
we align the collaborative and semantic embeddings and then identify noise in
the interaction sequence based on long-term and short-term interests captured
in the collaborative and semantic modalities. Our extensive experiments on four
public datasets validate the effectiveness of the proposed framework and its
compatibility with different sequential recommendation systems.

</details>


### [958] [Beyond Static Evaluation: Rethinking the Assessment of Personalized Agent Adaptability in Information Retrieval](https://arxiv.org/abs/2510.03984)
*Kirandeep Kaur,Preetam Prabhu Srikar Dammu,Hideo Joho,Chirag Shah*

Main category: cs.IR

TL;DR: This paper proposes redefining the evaluation of personalized AI agents from static benchmarks to dynamic, interaction-aware methods, including evolving user simulations and adaptation-driven metrics.


<details>
  <summary>Details</summary>
Motivation: Current evaluation methods for personalized AI agents fail to address longitudinal needs and dynamic user adaptation, limiting assessments of meaningful personalization.

Method: The paper introduces a conceptual framework with three components: persona-based user simulations evolving temporally, contextually-driven preference elicitation protocols, and metrics tracking behavioral improvements across sessions.

Result: A case study in e-commerce search using the PersonalWAB dataset demonstrates the applicability of the proposed framework.

Conclusion: The work establishes a foundational approach to rethinking evaluation in adaptive personalization by emphasizing continuous, user-centric assessments over static methodologies.

Abstract: Personalized AI agents are becoming central to modern information retrieval,
yet most evaluation methodologies remain static, relying on fixed benchmarks
and one-off metrics that fail to reflect how users' needs evolve over time.
These limitations hinder our ability to assess whether agents can meaningfully
adapt to individuals across dynamic, longitudinal interactions. In this
perspective paper, we propose a conceptual lens for rethinking evaluation in
adaptive personalization, shifting the focus from static performance snapshots
to interaction-aware, evolving assessments. We organize this lens around three
core components: (1) persona-based user simulation with temporally evolving
preference models; (2) structured elicitation protocols inspired by reference
interviews to extract preferences in context; and (3) adaptation-aware
evaluation mechanisms that measure how agent behavior improves across sessions
and tasks. While recent works have embraced LLM-driven user simulation, we
situate this practice within a broader paradigm for evaluating agents over
time. To illustrate our ideas, we conduct a case study in e-commerce search
using the PersonalWAB dataset. Beyond presenting a framework, our work lays a
conceptual foundation for understanding and evaluating personalization as a
continuous, user-centric endeavor.

</details>


### [959] [RLRF: Competitive Search Agent Design via Reinforcement Learning from Ranker Feedback](https://arxiv.org/abs/2510.04096)
*Tommy Mordo,Sagie Dekel,Omer Madmon,Moshe Tennenholtz,Oren Kurland*

Main category: cs.IR

TL;DR: The paper introduces a framework called Reinforcement Learning from Ranker Feedback (RLRF) to train large language models (LLMs) for creating competitive content that ranks highly in search settings, outperforming previous approaches.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the increasing use of LLMs by publishers to modify content for better search engine rankings and the need for effective strategies to navigate competitive environments.

Method: The authors propose RLRF, which trains LLMs using preference datasets derived from ranking competitions. These datasets are generated without relying on human-authored data.

Result: The proposed agents outperform prior methods in competitive document modifications, adapt effectively to unseen ranking functions, and demonstrate adaptability against strategic opponents.

Conclusion: The paper concludes that reinforcement learning holds significant potential for improving LLM-based content optimization in competitive search scenarios.

Abstract: Competitive search is a setting where document publishers modify them to
improve their ranking in response to a query. Recently, publishers have
increasingly leveraged LLMs to generate and modify competitive content. We
introduce Reinforcement Learning from Ranker Feedback (RLRF), a framework that
trains LLMs using preference datasets derived from ranking competitions. The
goal of a publisher (LLM-based) agent is to optimize content for improved
ranking while accounting for the strategies of competing agents. We generate
the datasets using approaches that do not rely on human-authored data. We show
that our proposed agents consistently and substantially outperform previously
suggested approaches for LLM-based competitive document modification. We
further show that our agents are effective with ranking functions they were not
trained for (i.e., out of distribution) and they adapt to strategic opponents.
These findings provide support to the significant potential of using
reinforcement learning in competitive search.

</details>


### [960] [Causality-aware Graph Aggregation Weight Estimator for Popularity Debiasing in Top-K Recommendation](https://arxiv.org/abs/2510.04502)
*Yue Que,Yingyi Zhang,Xiangyu Zhao,Chen Ma*

Main category: cs.IR

TL;DR: The paper addresses popularity bias in graph-based recommender systems by proposing a novel method, CAGED, which leverages causal inference to adjust aggregation weights and improves debiasing effectiveness.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the popularity bias in graph-based recommender systems, which leads to suboptimal recommendations due to the echo effect during information propagation. Existing solutions fail to provide optimal insights and a balanced approach to address this issue.

Method: The authors propose CAGED, an encoder-decoder architecture that uses causal inference to optimize the unbiased aggregation weights. It employs a momentum update strategy for refinement of the weight matrix during early training.

Result: The proposed CAGED method demonstrates superior performance over existing graph-based debiasing methods across three datasets, as evidenced by extensive experiments.

Conclusion: CAGED effectively reconciles graph aggregation rationality and achieves optimized debiasing, presenting a significant improvement over prior methods in mitigating popularity bias in recommender systems.

Abstract: Graph-based recommender systems leverage neighborhood aggregation to generate
node representations, which is highly sensitive to popularity bias, resulting
in an echo effect during information propagation. Existing graph-based
debiasing solutions refine the aggregation process with attempts such as edge
reconstruction or weight adjustment. However, these methods remain inadequate
in fully alleviating popularity bias. Specifically, this is because 1) they
provide no insights into graph aggregation rationality, thus lacking an
optimality guarantee; 2) they fail to well balance the training and debiasing
process, which undermines the effectiveness. In this paper, we propose a novel
approach to mitigate popularity bias through rational modeling of the graph
aggregation process. We reveal that graph aggregation is a special form of
backdoor adjustment in causal inference, where the aggregation weight
corresponds to the historical interaction likelihood distribution. Based on
this insight, we devise an encoder-decoder architecture, namely Causality-aware
Graph Aggregation Weight Estimator for Debiasing (CAGED), to approximate the
unbiased aggregation weight by optimizing the evidence lower bound of the
interaction likelihood. In order to enhance the debiasing effectiveness during
early training stages, we further design a momentum update strategy that
incrementally refines the aggregation weight matrix. Extensive experiments on
three datasets demonstrate that CAGED outperforms existing graph-based
debiasing methods. Our implementation is available at
https://github.com/QueYork/CAGED.

</details>
