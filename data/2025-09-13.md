<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 25]
- [cs.AR](#cs.AR) [Total: 2]
- [cs.CL](#cs.CL) [Total: 37]
- [cs.CV](#cs.CV) [Total: 78]
- [cs.DC](#cs.DC) [Total: 7]
- [cs.LG](#cs.LG) [Total: 53]
- [cs.NE](#cs.NE) [Total: 3]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.PL](#cs.PL) [Total: 2]
- [cs.RO](#cs.RO) [Total: 19]
- [cs.SE](#cs.SE) [Total: 13]
- [q-bio.NC](#q-bio.NC) [Total: 2]
- [stat.ML](#stat.ML) [Total: 3]
- [eess.SP](#eess.SP) [Total: 4]
- [math.OC](#math.OC) [Total: 1]
- [cs.IR](#cs.IR) [Total: 4]
- [eess.IV](#eess.IV) [Total: 4]
- [cs.SD](#cs.SD) [Total: 4]
- [cs.CY](#cs.CY) [Total: 7]
- [math.NA](#math.NA) [Total: 2]
- [cs.GR](#cs.GR) [Total: 1]
- [physics.geo-ph](#physics.geo-ph) [Total: 1]
- [quant-ph](#quant-ph) [Total: 2]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [cs.DB](#cs.DB) [Total: 1]
- [physics.med-ph](#physics.med-ph) [Total: 2]
- [cs.CR](#cs.CR) [Total: 7]
- [stat.ME](#stat.ME) [Total: 1]
- [cs.SI](#cs.SI) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [An Interval Type-2 Version of Bayes Theorem Derived from Interval Probability Range Estimates Provided by Subject Matter Experts](https://arxiv.org/abs/2509.08834)
*John T. Rickard,William A. Dembski,James Rickards*

Main category: cs.AI

TL;DR: This paper extends Bayes Theorem by incorporating interval type-2 fuzzy logic to better reflect uncertain and imprecise input values.


<details>
  <summary>Details</summary>
Motivation: Bayesian inference often assumes precise input values, which is unrealistic for real-world scenarios where inputs are usually interval estimates provided by experts.

Method: The paper presents an interval type-2 (IT2) version of Bayes Theorem and proposes a novel encoding algorithm for transforming SME-provided intervals into IT2 fuzzy membership functions (MFs).

Result: The key contributions are a conservative IT2 Bayes Theorem that avoids inconsistencies and a generalized algorithm for encoding interval estimates into IT2 MFs.

Conclusion: The work enables more realistic applications of Bayesian inference by robustly handling imprecise and uncertain input data using IT2 fuzzy logic.

Abstract: Bayesian inference is widely used in many different fields to test hypotheses
against observations. In most such applications, an assumption is made of
precise input values to produce a precise output value. However, this is
unrealistic for real-world applications. Often the best available information
from subject matter experts (SMEs) in a given field is interval range estimates
of the input probabilities involved in Bayes Theorem. This paper provides two
key contributions to extend Bayes Theorem to an interval type-2 (IT2) version.
First, we develop an IT2 version of Bayes Theorem that uses a novel and
conservative method to avoid potential inconsistencies in the input IT2 MFs
that otherwise might produce invalid output results. We then describe a novel
and flexible algorithm for encoding SME-provided intervals into IT2 fuzzy
membership functions (MFs), which we can use to specify the input probabilities
in Bayes Theorem. Our algorithm generalizes and extends previous work on this
problem that primarily addressed the encoding of intervals into word MFs for
Computing with Words applications.

</details>


### [2] [Automated Unity Game Template Generation from GDDs via NLP and Multi-Modal LLMs](https://arxiv.org/abs/2509.08847)
*Amna Hassan*

Main category: cs.AI

TL;DR: This paper proposes an end-to-end system that converts Game Design Documents (GDDs) into Unity game prototypes using NLP and fine-tuned large language models (LLMs).


<details>
  <summary>Details</summary>
Motivation: To address challenges in AI-assisted game development by automating the transformation of GDDs into functional Unity game templates, thereby enhancing development efficiency.

Method: The approach utilizes a fine-tuned LLaMA-3 model for Unity code generation, integrates it with a Unity package for seamless implementation, and employs NLP to parse GDDs into structured game specifications.

Result: The proposed system outperforms baseline models, achieving a 4.8/5.0 average score in metrics like compilation success, GDD adherence, best practices adoption, and modularity. Templates successfully match GDD requirements across various genres.

Conclusion: The framework effectively bridges game design and implementation, making LLMs practical tools for creating accurate and efficient game prototypes.

Abstract: This paper presents a novel framework for automated game template generation
by transforming Game Design Documents (GDDs) into functional Unity game
prototypes using Natural Language Processing (NLP) and multi-modal Large
Language Models (LLMs). We introduce an end-to-end system that parses GDDs,
extracts structured game specifications, and synthesizes Unity-compatible C#
code that implements the core mechanics, systems, and architecture defined in
the design documentation. Our approach combines a fine-tuned LLaMA-3 model
specialized for Unity code generation with a custom Unity integration package
that streamlines the implementation process. Evaluation results demonstrate
significant improvements over baseline models, with our fine-tuned model
achieving superior performance (4.8/5.0 average score) compared to
state-of-the-art LLMs across compilation success, GDD adherence, best practices
adoption, and code modularity metrics. The generated templates demonstrate high
adherence to GDD specifications across multiple game genres. Our system
effectively addresses critical gaps in AI-assisted game development,
positioning LLMs as valuable tools in streamlining the transition from game
design to implementation.

</details>


### [3] [Global Constraint LLM Agents for Text-to-Model Translation](https://arxiv.org/abs/2509.08970)
*Junyang Cai,Serdar Kadioglu,Bistra Dilkina*

Main category: cs.AI

TL;DR: The paper proposes a framework where large language model agents specialize in detecting and generating code for optimization problem types to automatically translate descriptions into MiniZinc models.


<details>
  <summary>Details</summary>
Motivation: Translating natural language descriptions into correct MiniZinc models is challenging due to the need for logical reasoning and expertise in constraint programming.

Method: The framework employs multiple LLM agents, each focusing on specific global constraint types, and an assembler agent integrates their outputs into a complete MiniZinc model.

Result: Initial experiments demonstrate improved performance compared to other prompting methods like one-shot or chain-of-thought prompting.

Conclusion: Dividing the modeling task among specialized agents simplifies the translation process, with outlined pathways for future improvements and enhancements.

Abstract: Natural language descriptions of optimization or satisfaction problems are
challenging to translate into correct MiniZinc models, as this process demands
both logical reasoning and constraint programming expertise. We introduce a
framework that addresses this challenge with an agentic approach: multiple
specialized large language model (LLM) agents decompose the modeling task by
global constraint type. Each agent is dedicated to detecting and generating
code for a specific class of global constraint, while a final assembler agent
integrates these constraint snippets into a complete MiniZinc model. By
dividing the problem into smaller, well-defined sub-tasks, each LLM handles a
simpler reasoning challenge, potentially reducing overall complexity. We
conduct initial experiments with several LLMs and show better performance
against baselines such as one-shot prompting and chain-of-thought prompting.
Finally, we outline a comprehensive roadmap for future work, highlighting
potential enhancements and directions for improvement.

</details>


### [4] [ForTIFAI: Fending Off Recursive Training Induced Failure for AI Models](https://arxiv.org/abs/2509.08972)
*Soheil Zibakhsh Shabgahi,Pedram Aghazadeh,Azalia Mirhosseini,Farinaz Koushanfar*

Main category: cs.AI

TL;DR: This paper addresses the challenge of model collapse caused by training on synthetic data by introducing a new loss function, Truncated Cross Entropy (TCE), which delays the collapse and extends model performance.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is the observation that generative AI models increasingly rely on synthetic data, leading to "model collapse." This phenomenon presents a significant challenge as it degrades model performance over generations of training.

Method: The authors identify model overconfidence in self-generated data as a driver of collapse and propose Truncated Cross Entropy (TCE), a confidence-aware loss function that downweights high-confidence predictions during training.

Result: The TCE loss function delays model collapse, extending the model's fidelity interval by over 2.3x. The framework is validated theoretically and empirically, showing generalization across modalities.

Conclusion: TCE is an effective and model-agnostic method to mitigate model collapse, highlighting the importance of loss function design in preserving the quality of generative models as synthetic data becomes more prevalent.

Abstract: The increasing reliance on generative AI models has accelerated the
generation rate of synthetic data, with some projections suggesting that most
available new data for training could be machine-generated by 2030. This shift
to a mainly synthetic content presents a critical challenge: repeated training
in synthetic data leads to a phenomenon known as model collapse, where model
performance degrades over generations of training, eventually rendering the
models ineffective. Although prior studies have explored the causes and
detection of model collapse, existing mitigation strategies remain limited.
  In this paper, we identify model overconfidence in their self-generated data
as a key driver of collapse. Building on this observation, we propose a
confidence-aware loss function that downweights high-confidence predictions
during training. We introduce a novel loss function we call Truncated Cross
Entropy (TCE). We demonstrate that TCE significantly delays model collapse in
recursive training.
  We provide a model-agnostic framework that links the loss function design to
model collapse mitigation and validate our approach both theoretically and
empirically, showing that it can extend the model's fidelity interval before
collapse by more than 2.3x. Finally, we show that our method generalizes across
modalities. These findings suggest that the design of loss functions provides a
simple yet powerful tool for preserving the quality of generative models in the
era of increasing synthetic data.

</details>


### [5] [Uncertainty Awareness and Trust in Explainable AI- On Trust Calibration using Local and Global Explanations](https://arxiv.org/abs/2509.08989)
*Carina Newen,Daniel Bodemer,Sonja Glantz,Emmanuel Müller,Magdalena Wischnewski,Lenka Schnaubert*

Main category: cs.AI

TL;DR: This paper investigates how global and uncertainty explanations in XAI, tested through an algorithm covering multiple concepts, affect user trust and satisfaction.


<details>
  <summary>Details</summary>
Motivation: To address gaps in XAI research, particularly in understanding uncertainty explanations and global explanations, and how they influence trust and interpretability.

Method: An algorithm incorporating uncertainty, robustness, and global XAI was tested for its ability to calibrate trust and its human interpretability via visualization.

Result: The study revealed insights into user trust calibration and satisfaction with complex but intuitive visual explanations.

Conclusion: Incorporating global and uncertainty explanations enhances user satisfaction and interpretability, supporting their role in future XAI guidelines.

Abstract: Explainable AI has become a common term in the literature, scrutinized by
computer scientists and statisticians and highlighted by psychological or
philosophical researchers. One major effort many researchers tackle is
constructing general guidelines for XAI schemes, which we derived from our
study. While some areas of XAI are well studied, we focus on uncertainty
explanations and consider global explanations, which are often left out. We
chose an algorithm that covers various concepts simultaneously, such as
uncertainty, robustness, and global XAI, and tested its ability to calibrate
trust. We then checked whether an algorithm that aims to provide more of an
intuitive visual understanding, despite being complicated to understand, can
provide higher user satisfaction and human interpretability.

</details>


### [6] [Instructional Prompt Optimization for Few-Shot LLM-Based Recommendations on Cold-Start Users](https://arxiv.org/abs/2509.09066)
*Haowei Yang,Yushang Zhao,Sitao Min,Bo Su,Chao Yao,Wei Xu*

Main category: cs.AI

TL;DR: The paper presents a method to tackle the cold-start user issue in recommender systems by optimizing prompts on few-shot large language models. Results show improved precision and NDCG scores using prompt tuning.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the cold-start problem in recommendation systems that lack historical data by utilizing few-shot LLMs to construct effective predictions.

Method: The authors introduce a context-conditioned prompt formulation that structures inputs using user profiles, curated support sets, token-level alignments, and embedding regularization for better semantic control.

Result: Through experiments with LLMs like BioGPT, LLaMA-2, and GPT-4, the proposed method improves precision@k and NDCG scores in low-data contexts.

Conclusion: Prompt-based adaptation is a viable solution for overcoming cold-start challenges in LLM-based recommendation pipelines.

Abstract: The cold-start user issue further compromises the effectiveness of
recommender systems in limiting access to the historical behavioral
information. It is an effective pipeline to optimize instructional prompts on a
few-shot large language model (LLM) used in recommender tasks. We introduce a
context-conditioned prompt formulation method P(u,\ Ds)\ \rightarrow\
R\widehat, where u is a cold-start user profile, Ds is a curated support set,
and R\widehat is the predicted ranked list of items. Based on systematic
experimentation with transformer-based autoregressive LLMs (BioGPT, LLaMA-2,
GPT-4), we provide empirical evidence that optimal exemplar injection and
instruction structuring can significantly improve the precision@k and NDCG
scores of such models in low-data settings. The pipeline uses token-level
alignments and embedding space regularization with a greater semantic fidelity.
Our findings not only show that timely composition is not merely syntactic but
also functional as it is in direct control of attention scales and decoder
conduct through inference. This paper shows that prompt-based adaptation may be
considered one of the ways to address cold-start recommendation issues in
LLM-based pipelines.

</details>


### [7] [Understanding Economic Tradeoffs Between Human and AI Agents in Bargaining Games](https://arxiv.org/abs/2509.09071)
*Crystal Qian,Kehang Zhu,John Horton,Benjamin S. Manning,Vivian Tsai,James Wexler,Nithum Thain*

Main category: cs.AI

TL;DR: The paper compares humans, LLMs (GPT-4o, Gemini 1.5 Pro), and Bayesian agents in dynamic negotiation tasks, revealing distinct behavioral dynamics despite similar outcomes.


<details>
  <summary>Details</summary>
Motivation: To understand and evaluate the negotiation processes and performance differences between humans, LLMs, and Bayesian agents in multi-agent environments.

Method: The study conducted experiments comparing humans, LLMs, and Bayesian agents in identical-condition dynamic negotiations, assessing both outcomes and behavioral dynamics.

Result: Bayesian agents optimize aggressively with frequent trade rejections achieving the highest surplus. LLMs and humans achieve comparable overall surplus but through distinct behaviors: LLMs opt for conservative and concessionary strategies, while humans favor fairness-oriented and risk-taking strategies.

Conclusion: Performance parity may mask critical differences in negotiation processes and alignment, which are essential for real-world applications of autonomous agents in coordination tasks.

Abstract: Coordination tasks traditionally performed by humans are increasingly being
delegated to autonomous agents. As this pattern progresses, it becomes critical
to evaluate not only these agents' performance but also the processes through
which they negotiate in dynamic, multi-agent environments. Furthermore,
different agents exhibit distinct advantages: traditional statistical agents,
such as Bayesian models, may excel under well-specified conditions, whereas
large language models (LLMs) can generalize across contexts. In this work, we
compare humans (N = 216), LLMs (GPT-4o, Gemini 1.5 Pro), and Bayesian agents in
a dynamic negotiation setting that enables direct, identical-condition
comparisons across populations, capturing both outcomes and behavioral
dynamics. Bayesian agents extract the highest surplus through aggressive
optimization, at the cost of frequent trade rejections. Humans and LLMs can
achieve similar overall surplus, but through distinct behaviors: LLMs favor
conservative, concessionary trades with few rejections, while humans employ
more strategic, risk-taking, and fairness-oriented behaviors. Thus, we find
that performance parity -- a common benchmark in agent evaluation -- can
conceal fundamental differences in process and alignment, which are critical
for practical deployment in real-world coordination tasks.

</details>


### [8] [Anti-Money Laundering Machine Learning Pipelines; A Technical Analysis on Identifying High-risk Bank Clients with Supervised Learning](https://arxiv.org/abs/2509.09127)
*Khashayar Namdar,Pin-Chien Wang,Tushar Raju,Steven Zheng,Fiona Li,Safwat Tahmin Khan*

Main category: cs.AI

TL;DR: The paper presents a machine learning (ML) pipeline to identify high-risk bank clients using a systematic 16-step approach, achieving high accuracy in a competition dataset.


<details>
  <summary>Details</summary>
Motivation: To enhance anti-money laundering efforts by developing a robust ML pipeline to identify high-risk clients in financial data.

Method: The authors used a 16-step statistical and design approach, framed data in SQLite database, applied SQL-based feature engineering, connected a pre-trained model for inference, and incorporated explainable artificial intelligence (XAI).

Result: Achieved a mean AUROC of 0.961 with 0.005 SD, demonstrating high accuracy and securing second place in the competition.

Conclusion: The pipeline offers a systematic, robust, and explainable solution for high-risk client identification, showcasing ML's potential for anti-money laundering in financial institutions.

Abstract: Anti-money laundering (AML) actions and measurements are among the priorities
of financial institutions, for which machine learning (ML) has shown to have a
high potential. In this paper, we propose a comprehensive and systematic
approach for developing ML pipelines to identify high-risk bank clients in a
dataset curated for Task 1 of the University of Toronto 2023-2024 Institute for
Management and Innovation (IMI) Big Data and Artificial Intelligence
Competition. The dataset included 195,789 customer IDs, and we employed a
16-step design and statistical analysis to ensure the final pipeline was
robust. We also framed the data in a SQLite database, developed SQL-based
feature engineering algorithms, connected our pre-trained model to the
database, and made it inference-ready, and provided explainable artificial
intelligence (XAI) modules to derive feature importance. Our pipeline achieved
a mean area under the receiver operating characteristic curve (AUROC) of 0.961
with a standard deviation (SD) of 0.005. The proposed pipeline achieved second
place in the competition.

</details>


### [9] [Mind Meets Space: Rethinking Agentic Spatial Intelligence from a Neuroscience-inspired Perspective](https://arxiv.org/abs/2509.09154)
*Bui Duc Manh,Soumyaratna Debnath,Zetong Zhang,Shriram Damodaran,Arvind Kumar,Yueyi Zhang,Lu Mi,Erik Cambria,Lin Wang*

Main category: cs.AI

TL;DR: The paper introduces a computational framework inspired by neuroscience to enhance AI's spatial reasoning by aligning it with human-like, multisensory spatial intelligence.


<details>
  <summary>Details</summary>
Motivation: AI systems lack advanced spatial reasoning compared to humans, who rely on multisensory perception and cognitive maps for decision-making in unstructured environments.

Method: The authors propose a computational framework comprising six modules inspired by neuroscience: bio-inspired multimodal sensing, multi-sensory integration, egocentric-allocentric conversion, artificial cognitive maps, spatial memory, and spatial reasoning.

Result: The study analyzes existing methods and benchmarks through the lens of the proposed framework, identifying gaps and proposing directions for the development of better spatial reasoning systems.

Conclusion: The proposed framework offers a neuroscience-grounded roadmap for improving AI spatial reasoning, with potential applications in areas like robotics and embodied systems.

Abstract: Recent advances in agentic AI have led to systems capable of autonomous task
execution and language-based reasoning, yet their spatial reasoning abilities
remain limited and underexplored, largely constrained to symbolic and
sequential processing. In contrast, human spatial intelligence, rooted in
integrated multisensory perception, spatial memory, and cognitive maps, enables
flexible, context-aware decision-making in unstructured environments.
Therefore, bridging this gap is critical for advancing Agentic Spatial
Intelligence toward better interaction with the physical 3D world. To this end,
we first start from scrutinizing the spatial neural models as studied in
computational neuroscience, and accordingly introduce a novel computational
framework grounded in neuroscience principles. This framework maps core
biological functions to six essential computation modules: bio-inspired
multimodal sensing, multi-sensory integration, egocentric-allocentric
conversion, an artificial cognitive map, spatial memory, and spatial reasoning.
Together, these modules form a perspective landscape for agentic spatial
reasoning capability across both virtual and physical environments. On top, we
conduct a framework-guided analysis of recent methods, evaluating their
relevance to each module and identifying critical gaps that hinder the
development of more neuroscience-grounded spatial reasoning modules. We further
examine emerging benchmarks and datasets and explore potential application
domains ranging from virtual to embodied systems, such as robotics. Finally, we
outline potential research directions, emphasizing the promising roadmap that
can generalize spatial reasoning across dynamic or unstructured environments.
We hope this work will benefit the research community with a
neuroscience-grounded perspective and a structured pathway. Our project page
can be found at Github.

</details>


### [10] [ProgD: Progressive Multi-scale Decoding with Dynamic Graphs for Joint Multi-agent Motion Forecasting](https://arxiv.org/abs/2509.09210)
*Xing Gao,Zherui Huang,Weiyao Lin,Xiao Sun*

Main category: cs.AI

TL;DR: The paper proposes ProgD, a strategy combining dynamic heterogeneous graphs and progressive multi-scale decoding to model and predict motion of multiple agents in uncertain scenarios, achieving top performance in benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing methods for predicting agent motion overlook the evolving nature of social interactions and fail to comprehensively address the uncertainty inherent in future scenarios.

Method: ProgD uses dynamic heterogeneous graphs for scenario modeling, a factorized architecture for processing spatio-temporal dependencies, and a multi-scale decoding procedure to progressively eliminate prediction uncertainty.

Result: The proposed method achieves state-of-the-art performance on the INTERACTION and Argoverse 2 benchmarks for multi-agent motion prediction.

Conclusion: ProgD effectively captures evolving interactions and reduces prediction uncertainty, making it a robust solution for multi-agent motion prediction scenarios.

Abstract: Accurate motion prediction of surrounding agents is crucial for the safe
planning of autonomous vehicles. Recent advancements have extended prediction
techniques from individual agents to joint predictions of multiple interacting
agents, with various strategies to address complex interactions within future
motions of agents. However, these methods overlook the evolving nature of these
interactions. To address this limitation, we propose a novel progressive
multi-scale decoding strategy, termed ProgD, with the help of dynamic
heterogeneous graph-based scenario modeling. In particular, to explicitly and
comprehensively capture the evolving social interactions in future scenarios,
given their inherent uncertainty, we design a progressive modeling of scenarios
with dynamic heterogeneous graphs. With the unfolding of such dynamic
heterogeneous graphs, a factorized architecture is designed to process the
spatio-temporal dependencies within future scenarios and progressively
eliminate uncertainty in future motions of multiple agents. Furthermore, a
multi-scale decoding procedure is incorporated to improve on the future
scenario modeling and consistent prediction of agents' future motion. The
proposed ProgD achieves state-of-the-art performance on the INTERACTION
multi-agent prediction benchmark, ranking $1^{st}$, and the Argoverse 2
multi-world forecasting benchmark.

</details>


### [11] [Enabling Regulatory Multi-Agent Collaboration: Architecture, Challenges, and Solutions](https://arxiv.org/abs/2509.09215)
*Qinnan Hu,Yuntao Wang,Yuan Gao,Zhou Su,Linkang Du*

Main category: cs.AI

TL;DR: The paper proposes a blockchain-enabled layered architecture to address governance and accountability challenges in LLM-powered multi-agent systems.


<details>
  <summary>Details</summary>
Motivation: The proliferation of LLM-empowered autonomous agents in domains like finance, healthcare, and smart manufacturing presents opportunities, but their unpredictable behavior and varied capabilities necessitate robust governance mechanisms.

Method: The authors introduced a three-layer architecture comprising an agent layer, blockchain data layer, and regulatory application layer, with modules for behavior tracing, dynamic reputation evaluation, and malicious behavior forecasting.

Result: The proposed approach offers a systematic framework for enhancing trust, resilience, and scalability in regulatory mechanisms for large-scale agent ecosystems.

Conclusion: Blockchain-enabled regulatory frameworks can effectively manage the complexities of multi-agent systems while paving the way for future research in governance enhancements using blockchain technologies.

Abstract: Large language models (LLMs)-empowered autonomous agents are transforming
both digital and physical environments by enabling adaptive, multi-agent
collaboration. While these agents offer significant opportunities across
domains such as finance, healthcare, and smart manufacturing, their
unpredictable behaviors and heterogeneous capabilities pose substantial
governance and accountability challenges. In this paper, we propose a
blockchain-enabled layered architecture for regulatory agent collaboration,
comprising an agent layer, a blockchain data layer, and a regulatory
application layer. Within this framework, we design three key modules: (i) an
agent behavior tracing and arbitration module for automated accountability,
(ii) a dynamic reputation evaluation module for trust assessment in
collaborative scenarios, and (iii) a malicious behavior forecasting module for
early detection of adversarial activities. Our approach establishes a
systematic foundation for trustworthy, resilient, and scalable regulatory
mechanisms in large-scale agent ecosystems. Finally, we discuss the future
research directions for blockchain-enabled regulatory frameworks in multi-agent
systems.

</details>


### [12] [Jupiter: Enhancing LLM Data Analysis Capabilities via Notebook and Inference-Time Value-Guided Search](https://arxiv.org/abs/2509.09245)
*Shuocheng Li,Yihao Liu,Silin Du,Wenxuan Zeng,Zhe Xu,Mengyu Zhou,Yeye He,Haoyu Dong,Shi Han,Dongmei Zhang*

Main category: cs.AI

TL;DR: The paper introduces NbQA, a dataset of tool-based data analysis tasks from Jupyter notebooks, and Jupiter, a Monte Carlo Tree Search-based framework for efficient multi-step reasoning in data science.


<details>
  <summary>Details</summary>
Motivation: Existing large language models (LLMs) struggle with multi-step reasoning and effective tool usage, which limits their applicability to complex data analysis tasks.

Method: The authors develop a scalable pipeline for extracting task-solution pairs from Jupyter notebooks, introduce the NbQA dataset, and propose Jupiter, a framework leveraging Monte Carlo Tree Search to enhance multi-step reasoning and tool-use capabilities.

Result: The proposed models (Qwen2.5-7B and 14B-Instruct) achieve high task-solving rates (77.82% and 86.38%, respectively) and outperform GPT-4o and other advanced agent frameworks in various multi-step reasoning benchmarks.

Conclusion: The NbQA dataset and Jupiter framework significantly advance the capability of LLMs for complex data analysis, illustrating improvements in multi-step reasoning and tool-use reasoning.

Abstract: Large language models (LLMs) have shown great promise in automating data
science workflows, but existing models still struggle with multi-step reasoning
and tool use, which limits their effectiveness on complex data analysis tasks.
To address this, we propose a scalable pipeline that extracts high-quality,
tool-based data analysis tasks and their executable multi-step solutions from
real-world Jupyter notebooks and associated data files. Using this pipeline, we
introduce NbQA, a large-scale dataset of standardized task-solution pairs that
reflect authentic tool-use patterns in practical data science scenarios. To
further enhance multi-step reasoning, we present Jupiter, a framework that
formulates data analysis as a search problem and applies Monte Carlo Tree
Search (MCTS) to generate diverse solution trajectories for value model
learning. During inference, Jupiter combines the value model and node visit
counts to efficiently collect executable multi-step plans with minimal search
steps. Experimental results show that Qwen2.5-7B and 14B-Instruct models on
NbQA solve 77.82% and 86.38% of tasks on InfiAgent-DABench,
respectively-matching or surpassing GPT-4o and advanced agent frameworks.
Further evaluations demonstrate improved generalization and stronger tool-use
reasoning across diverse multi-step reasoning tasks.

</details>


### [13] [Fusing Knowledge and Language: A Comparative Study of Knowledge Graph-Based Question Answering with LLMs](https://arxiv.org/abs/2509.09272)
*Vaibhav Chaudhary,Neha Soni,Narotam Singh,Amita Kapoor*

Main category: cs.AI

TL;DR: This study compares three methods (spaCy, Stanford CoreNLP-OpenIE, GraphRAG) for generating knowledge graph triplets and integrating them with LLMs for better question answering, highlighting their strengths and shortcomings.


<details>
  <summary>Details</summary>
Motivation: To address the limitations in traditional Retrieval Augmented Generation (RAG) methods in dealing with complex, extensive texts by improving question-answering systems using knowledge graph triplets integrated with LLMs.

Method: Comparative study of three methodologies—spaCy, Stanford CoreNLP-OpenIE, and GraphRAG—for constructing knowledge graph triplets and integrating them with LLMs. The evaluation focuses on their effectiveness, feasibility, and impact.

Result: OpenIE had the most comprehensive triplet coverage, while GraphRAG excelled in reasoning abilities. Experimental results revealed both the relative strengths and weaknesses of each method.

Conclusion: Each method has unique strengths, with OpenIE excelling in triplet coverage and GraphRAG in reasoning capabilities. The study highlights areas for further improvements to enhance knowledge graph-based question-answering systems.

Abstract: Knowledge graphs, a powerful tool for structuring information through
relational triplets, have recently become the new front-runner in enhancing
question-answering systems. While traditional Retrieval Augmented Generation
(RAG) approaches are proficient in fact-based and local context-based
extraction from concise texts, they encounter limitations when addressing the
thematic and holistic understanding of complex, extensive texts, requiring a
deeper analysis of both text and context. This paper presents a comprehensive
technical comparative study of three different methodologies for constructing
knowledge graph triplets and integrating them with Large Language Models (LLMs)
for question answering: spaCy, Stanford CoreNLP-OpenIE, and GraphRAG, all
leveraging open source technologies. We evaluate the effectiveness,
feasibility, and adaptability of these methods by analyzing their capabilities,
state of development, and their impact on the performance of LLM-based question
answering. Experimental results indicate that while OpenIE provides the most
comprehensive coverage of triplets, GraphRAG demonstrates superior reasoning
abilities among the three. We conclude with a discussion on the strengths and
limitations of each method and provide insights into future directions for
improving knowledge graph-based question answering.

</details>


### [14] [Tree-OPO: Off-policy Monte Carlo Tree-Guided Advantage Optimization for Multistep Reasoning](https://arxiv.org/abs/2509.09284)
*Bingning Huang,Tu Nguyen,Matthieu Zimmer*

Main category: cs.AI

TL;DR: Explores the use of Monte Carlo Tree Search (MCTS) for improving policy optimization in preference-based reinforcement learning, focusing on Group Relative Policy Optimization (GRPO). Introduces a novel training paradigm and tree-structured advantage estimation.


<details>
  <summary>Details</summary>
Motivation: Improve reasoning and policy optimization in reinforcement learning by repurposing MCTS-generated trajectories in preference-based models.

Method: Proposes using a staged GRPO training paradigm involving partial MCTS rollouts, introducing tree-like advantage estimation signals.

Result: Finds structured advantage estimation can stabilize updates and improve reasoning quality, but challenges like advantage saturation persist.

Conclusion: Heuristic and statistical solutions to challenges are proposed, but further research is needed on tree-like reward structures in RL.

Abstract: Recent advances in reasoning with large language models (LLMs) have shown the
effectiveness of Monte Carlo Tree Search (MCTS) for generating high-quality
intermediate trajectories, particularly in math and symbolic domains. Inspired
by this, we explore how MCTS-derived trajectories, traditionally used for
training value or reward models, can be repurposed to improve policy
optimization in preference-based reinforcement learning (RL). Specifically, we
focus on Group Relative Policy Optimization (GRPO), a recent algorithm that
enables preference-consistent policy learning without value networks. We
propose a staged GRPO training paradigm where completions are derived from
partially revealed MCTS rollouts, introducing a novel tree-structured setting
for advantage estimation. This leads to a rich class of prefix-conditioned
reward signals, which we analyze theoretically and empirically. Our initial
results indicate that while structured advantage estimation can stabilize
updates and better reflect compositional reasoning quality, challenges such as
advantage saturation and reward signal collapse remain. We propose heuristic
and statistical solutions to mitigate these issues and discuss open challenges
for learning under staged or tree-like reward structures.

</details>


### [15] [LightAgent: Production-level Open-source Agentic AI Framework](https://arxiv.org/abs/2509.09292)
*Weige Cai,Tong Zhu,Jinyi Niu,Ruiqi Hu,Lingyao Li,Tenglong Wang,Xiaowu Dai,Weining Shen,Liwen Zhang*

Main category: cs.AI

TL;DR: The paper presents LightAgent, a lightweight and powerful framework for deploying multi-agent systems efficiently and flexibly.


<details>
  <summary>Details</summary>
Motivation: Current multi-agent system frameworks lack a balance between flexibility and simplicity, posing challenges for scalable and efficient deployment.

Method: The LightAgent framework incorporates Memory (mem0), Tools, and Tree of Thought (ToT) functionalities while maintaining a streamlined structure. It is fully open-source and compatible with major chat platforms.

Result: LightAgent offers a practical, lightweight solution to designing self-learning agents with enhanced integration and usability.

Conclusion: LightAgent addresses the trade-offs in existing frameworks by providing a versatile, robust platform that retains simplicity and encourages ease of use for developers.

Abstract: With the rapid advancement of large language models (LLMs), Multi-agent
Systems (MAS) have achieved significant progress in various application
scenarios. However, substantial challenges remain in designing versatile,
robust, and efficient platforms for agent deployment. To address these
limitations, we propose \textbf{LightAgent}, a lightweight yet powerful agentic
framework, effectively resolving the trade-off between flexibility and
simplicity found in existing frameworks. LightAgent integrates core
functionalities such as Memory (mem0), Tools, and Tree of Thought (ToT), while
maintaining an extremely lightweight structure. As a fully open-source
solution, it seamlessly integrates with mainstream chat platforms, enabling
developers to easily build self-learning agents. We have released LightAgent at
\href{https://github.com/wxai-space/LightAgent}{https://github.com/wxai-space/LightAgent}

</details>


### [16] [Explaining Tournament Solutions with Minimal Supports](https://arxiv.org/abs/2509.09312)
*Clément Contet,Umberto Grandi,Jérôme Mengin*

Main category: cs.AI

TL;DR: The paper examines certified explanations for tournament winners using minimal supports under various rules, presenting algorithms for efficiency except for one NP-complete case.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance explainability in tournament outcomes, aligning with formal explainable AI by identifying minimal supports for certified winner explanations.

Method: The paper applies theoretical analysis and algorithmic design to study minimal supports across multiple tournament solution rules, determining sizes and computational complexity.

Result: The study successfully determines the smallest sizes for minimal supports and provides algorithms for efficient computation across all rules except one (weighted uncovered set) due to its NP-completeness.

Conclusion: Minimal supports offer a practical and intuitive approach for certified explanations of tournament winners, advancing formal explainable AI, though challenges remain for certain rules.

Abstract: Tournaments are widely used models to represent pairwise dominance between
candidates, alternatives, or teams. We study the problem of providing certified
explanations for why a candidate appears among the winners under various
tournament rules. To this end, we identify minimal supports, minimal
sub-tournaments in which the candidate is guaranteed to win regardless of how
the rest of the tournament is completed (that is, the candidate is a necessary
winner of the sub-tournament). This notion corresponds to an abductive
explanation for the question,"Why does the winner win the tournament", a
central concept in formal explainable AI. We focus on common tournament
solutions: the top cycle, the uncovered set, the Copeland rule, the Borda rule,
the maximin rule, and the weighted uncovered set. For each rule we determine
the size of the smallest minimal supports, and we present polynomial-time
algorithms to compute them for all but the weighted uncovered set, for which
the problem is NP-complete. Finally, we show how minimal supports can serve to
produce compact, certified, and intuitive explanations.

</details>


### [17] [Measuring Implicit Spatial Coordination in Teams: Effects on Collective Intelligence and Performance](https://arxiv.org/abs/2509.09314)
*Thuy Ngoc Nguyen,Anita Williams Woolley,Cleotilde Gonzalez*

Main category: cs.AI

TL;DR: The study explores how spatial coordination metrics (exploration diversity, movement specialization, and adaptive proximity) impact performance in teams operating without explicit communication in a search-and-rescue simulation.


<details>
  <summary>Details</summary>
Motivation: To understand how teams coordinate implicitly in physical environments under communication constraints, particularly in high-stakes scenarios like firefighting and law enforcement.

Method: 136 participants in 34 four-person teams performed a collaborative online search-and-rescue task under restricted communication, and their movement patterns were analyzed for spatial coordination metrics.

Result: Spatial specialization enhances performance; adaptive proximity has an inverted U-shaped relationship with performance; and temporal shifts in metrics identify high- vs low-performing teams.

Conclusion: Effective implicit spatial coordination stems from balanced, adaptive strategies, with potential applications in training and AI systems to support team operations.

Abstract: Coordinated teamwork is essential in fast-paced decision-making environments
that require dynamic adaptation, often without an opportunity for explicit
communication. Although implicit coordination has been extensively considered
in the existing literature, the majority of work has focused on co-located,
synchronous teamwork (such as sports teams) or, in distributed teams, primarily
on coordination of knowledge work. However, many teams (firefighters, military,
law enforcement, emergency response) must coordinate their movements in
physical space without the benefit of visual cues or extensive explicit
communication. This paper investigates how three dimensions of spatial
coordination, namely exploration diversity, movement specialization, and
adaptive spatial proximity, influence team performance in a collaborative
online search and rescue task where explicit communication is restricted and
team members rely on movement patterns to infer others' intentions and
coordinate actions. Our metrics capture the relational aspects of teamwork by
measuring spatial proximity, distribution patterns, and alignment of movements
within shared environments. We analyze data from 34 four-person teams (136
participants) assigned to specialized roles in a search and rescue task.
Results show that spatial specialization positively predicts performance, while
adaptive spatial proximity exhibits a marginal inverted U-shaped relationship,
suggesting moderate levels of adaptation are optimal. Furthermore, the temporal
dynamics of these metrics differentiate high- from low-performing teams over
time. These findings provide insights into implicit spatial coordination in
role-based teamwork and highlight the importance of balanced adaptive
strategies, with implications for training and AI-assisted team support
systems.

</details>


### [18] [Towards Adaptive ML Benchmarks: Web-Agent-Driven Construction, Domain Expansion, and Metric Optimization](https://arxiv.org/abs/2509.09321)
*Hangyi Jia,Yuxi Qian,Hanwen Tong,Xinhui Wu,Lin Chen,Feng Wei*

Main category: cs.AI

TL;DR: The paper introduces TAM Bench, a structured benchmark designed to evaluate large language model (LLM)-based agents across diverse end-to-end machine learning workflows with realistic settings.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks are limited in task coverage, difficulty modeling, domain diversity, and evaluation rigor, making it difficult to fully assess the performance of general-purpose LLM agents.

Method: The authors proposed TAM Bench, which includes automatic task acquisition, scalable difficulty modeling based on leaderboard metrics, and a multidimensional evaluation framework. It incorporates tasks from platforms like Kaggle and spans multiple data modalities.

Result: TAM Bench provides 150 curated AutoML tasks, organized into subsets (Lite, Medium, Full) to support different evaluation scenarios, with the Lite version offering balanced tasks for daily benchmarking.

Conclusion: TAM Bench offers a diverse and practical benchmark framework to comprehensively evaluate LLM-based agents in realistic, end-to-end ML workflows.

Abstract: Recent advances in large language models (LLMs) have enabled the emergence of
general-purpose agents for automating end-to-end machine learning (ML)
workflows, including data analysis, feature engineering, model training, and
competition solving. However, existing benchmarks remain limited in task
coverage, domain diversity, difficulty modeling, and evaluation rigor, failing
to capture the full capabilities of such agents in realistic settings. We
present TAM Bench, a diverse, realistic, and structured benchmark for
evaluating LLM-based agents on end-to-end ML tasks. TAM Bench features three
key innovations: (1) A browser automation and LLM-based task acquisition system
that automatically collects and structures ML challenges from platforms such as
Kaggle, AIcrowd, and Biendata, spanning multiple task types and data modalities
(e.g., tabular, text, image, graph, audio); (2) A leaderboard-driven difficulty
modeling mechanism that estimates task complexity using participant counts and
score dispersion, enabling scalable and objective task calibration; (3) A
multi-dimensional evaluation framework incorporating performance, format
compliance, constraint adherence, and task generalization. Based on 150 curated
AutoML tasks, we construct three benchmark subsets of different sizes -- Lite,
Medium, and Full -- designed for varying evaluation scenarios. The Lite
version, with 18 tasks and balanced coverage across modalities and difficulty
levels, serves as a practical testbed for daily benchmarking and comparative
studies.

</details>


### [19] [Curriculum-Based Multi-Tier Semantic Exploration via Deep Reinforcement Learning](https://arxiv.org/abs/2509.09356)
*Abdel Hakim Drid,Vincenzo Suriani,Daniele Nardi,Abderrezzak Debilou*

Main category: cs.AI

TL;DR: The paper introduces a novel DRL architecture for resource-efficient semantic exploration, integrating Vision-Language Model (VLM) for commonsense reasoning and strategic querying, combined with curriculum learning.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of enabling autonomous agents to explore and understand complex environments effectively, balancing exploration efficiency and semantic reasoning, which traditional RL approaches struggle to achieve due to limited cognitive capabilities.

Method: The approach incorporates a VLM as a strategic querying mechanism through a layered reward function, allowing the agent to query VLM only when necessary, coupled with curriculum learning to guide agents through varying complexity levels.

Result: The proposed technique enhances object discovery rates, enables agents to navigate semantically rich regions, and demonstrates strategic querying for external information.

Conclusion: The research presents a scalable method for integrating semantic reasoning in autonomous agents, advancing the development of fully intelligent, self-guided exploration in robotics.

Abstract: Navigating and understanding complex and unknown environments autonomously
demands more than just basic perception and movement from embodied agents.
Truly effective exploration requires agents to possess higher-level cognitive
abilities, the ability to reason about their surroundings, and make more
informed decisions regarding exploration strategies. However, traditional RL
approaches struggle to balance efficient exploration and semantic understanding
due to limited cognitive capabilities embedded in the small policies for the
agents, leading often to human drivers when dealing with semantic exploration.
In this paper, we address this challenge by presenting a novel Deep
Reinforcement Learning (DRL) architecture that is specifically designed for
resource efficient semantic exploration. A key methodological contribution is
the integration of a Vision-Language Model (VLM) common-sense through a layered
reward function. The VLM query is modeled as a dedicated action, allowing the
agent to strategically query the VLM only when deemed necessary for gaining
external guidance, thereby conserving resources. This mechanism is combined
with a curriculum learning strategy designed to guide learning at different
levels of complexity to ensure robust and stable learning. Our experimental
evaluation results convincingly demonstrate that our agent achieves
significantly enhanced object discovery rates and develops a learned capability
to effectively navigate towards semantically rich regions. Furthermore, it also
shows a strategic mastery of when to prompt for external environmental
information. By demonstrating a practical and scalable method for embedding
common-sense semantic reasoning with autonomous agents, this research provides
a novel approach to pursuing a fully intelligent and self-guided exploration in
robotics.

</details>


### [20] [TORSO: Template-Oriented Reasoning Towards General Tasks](https://arxiv.org/abs/2509.09448)
*Minhyuk Kim,Seungyoon Lee,Heuiseok Lim*

Main category: cs.AI

TL;DR: A method called Template-Oriented Reasoning (TORSO) is proposed to enhance Large Language Models' reasoning capabilities without requiring task-specific examples.


<details>
  <summary>Details</summary>
Motivation: Existing methods for guiding Large Language Models in reasoning rely heavily on few-shot prompts, which are costly to create and inconsistent across tasks, limiting the models' inherent reasoning abilities.

Method: The TORSO approach enables models to draw on their internal reasoning abilities using templates instead of manually crafted few-shot examples, allowing versatility across different tasks.

Result: TORSO achieved strong experimental performance on multiple LLM benchmark tasks, with outputs supported by reasonable rationales.

Conclusion: TORSO enhances the reasoning capabilities of LLMs effectively and efficiently without the limitations of manually created few-shot prompts.

Abstract: The approaches that guide Large Language Models (LLMs) to emulate human
reasoning during response generation have emerged as an effective method for
enabling them to solve complex problems in a step-by-step manner, thereby
achieving superior performance. However, most existing approaches using
few-shot prompts to generate responses heavily depend on the provided examples,
limiting the utilization of the model's inherent reasoning capabilities.
Moreover, constructing task-specific few-shot prompts is often costly and may
lead to inconsistencies across different tasks. In this work, we introduce
Template-Oriented Reasoning (TORSO), which elicits the model to utilize
internal reasoning abilities to generate proper responses across various tasks
without the need for manually crafted few-shot examples. Our experimental
results demonstrate that TORSO achieves strong performance on diverse LLMs
benchmarks with reasonable rationales.

</details>


### [21] [Inteligencia Artificial jurídica y el desafío de la veracidad: análisis de alucinaciones, optimización de RAG y principios para una integración responsable](https://arxiv.org/abs/2509.09467)
*Alex Dantart*

Main category: cs.AI

TL;DR: The paper addresses hallucinations in legal LLMs, evaluates RAG mitigation, and proposes a consultative AI paradigm for better accuracy and human judgment support.


<details>
  <summary>Details</summary>
Motivation: To tackle the problem of hallucinations in LLMs applied to law, and identify limitations of current mitigation like RAG.

Method: Analyzing causes and effects of hallucinations in legal LLMs, evaluating RAG strategy, and proposing holistic optimizations.

Result: RAG has limitations, and ethical concerns persist; the paper suggests a shift to consultative AI for better oversight.

Conclusion: Improving generative models incrementally is not enough; a consultative AI approach focused on veracity and traceability is essential.

Abstract: This technical report analyzes the challenge of "hallucinations" (false
information) in LLMs applied to law. It examines their causes, manifestations,
and the effectiveness of the RAG mitigation strategy, highlighting its
limitations and proposing holistic optimizations. The paper explores the
ethical and regulatory implications, emphasizing human oversight as an
irreplaceable role. It concludes that the solution lies not in incrementally
improving generative models, but in adopting a "consultative" AI paradigm that
prioritizes veracity and traceability, acting as a tool to amplify, not
replace, professional judgment.
  --
  Este informe t\'ecnico analiza el desaf\'io de las "alucinaciones"
(informaci\'on falsa) en los LLMs aplicados al derecho. Se examinan sus causas,
manifestaciones y la efectividad de la estrategia de mitigaci\'on RAG,
exponiendo sus limitaciones y proponiendo optimizaciones hol\'isticas. Se
exploran las implicaciones \'eticas y regulatorias, enfatizando la
supervisi\'on humana como un rol insustituible. El documento concluye que la
soluci\'on no reside en mejorar incrementalmente los modelos generativos, sino
en adoptar un paradigma de IA "consultiva" que priorice la veracidad y la
trazabilidad, actuando como una herramienta para amplificar, y no sustituir, el
juicio profesional.

</details>


### [22] [SEDM: Scalable Self-Evolving Distributed Memory for Agents](https://arxiv.org/abs/2509.09498)
*Haoran Xu,Jiacong Hu,Ke Zhang,Lei Yu,Yuxin Tang,Xinyuan Song,Yiqun Duan,Lynn Ai,Bill Shi*

Main category: cs.AI

TL;DR: The paper introduces SEDM, a scalable and adaptive framework for managing memory in multi-agent systems, addressing issues such as noise, memory expansion, and cross-domain generalization.


<details>
  <summary>Details</summary>
Motivation: Current memory management in multi-agent systems struggles with noise, uncontrolled growth, and poor generalization, affecting scalability and performance.

Method: The researchers propose SEDM, featuring verifiable write admission, a dynamic memory controller, and cross-domain knowledge diffusion for efficient and adaptive memory handling.

Result: SEDM demonstrates improved reasoning accuracy, reduced memory overhead, and the ability to distill knowledge for enhanced multi-hop reasoning in benchmark evaluations.

Conclusion: SEDM is a verifiable, scalable, and sustainable memory mechanism for long-term multi-agent collaboration, capable of advancing reasoning and knowledge transfer.

Abstract: Long-term multi-agent systems inevitably generate vast amounts of
trajectories and historical interactions, which makes efficient memory
management essential for both performance and scalability. Existing methods
typically depend on vector retrieval and hierarchical storage, yet they are
prone to noise accumulation, uncontrolled memory expansion, and limited
generalization across domains. To address these challenges, we present SEDM,
Self-Evolving Distributed Memory, a verifiable and adaptive framework that
transforms memory from a passive repository into an active, self-optimizing
component. SEDM integrates verifiable write admission based on reproducible
replay, a self-scheduling memory controller that dynamically ranks and
consolidates entries according to empirical utility, and cross-domain knowledge
diffusion that abstracts reusable insights to support transfer across
heterogeneous tasks. Evaluations on benchmark datasets demonstrate that SEDM
improves reasoning accuracy while reducing token overhead compared with strong
memory baselines, and further enables knowledge distilled from fact
verification to enhance multi-hop reasoning. The results highlight SEDM as a
scalable and sustainable memory mechanism for open-ended multi-agent
collaboration. The code will be released in the later stage of this project.

</details>


### [23] [Compositional Concept Generalization with Variational Quantum Circuits](https://arxiv.org/abs/2509.09541)
*Hala Hawashin,Mina Abbaszadeh,Nicholas Joseph,Beth Pearson,Martha Lewis,Mehrnoosh sadrzadeh*

Main category: cs.AI

TL;DR: This paper explores quantum models' potential to improve compositional generalization in AI, specifically for image captioning tasks.


<details>
  <summary>Details</summary>
Motivation: Current AI tools struggle with compositional generalization, an essential element of human cognition.

Method: The study trains Variational Quantum Circuits on tensor-based image captioning tasks using two encoding techniques: multi-hot encoding and angle/amplitude encoding.

Result: Quantum models show promising results with noisy multi-hot encodings and outperform classical models using CLIP vectors, albeit with mixed success.

Conclusion: Quantum models could serve as a novel approach to addressing compositional generalization challenges in AI tools, warranting further exploration.

Abstract: Compositional generalization is a key facet of human cognition, but lacking
in current AI tools such as vision-language models. Previous work examined
whether a compositional tensor-based sentence semantics can overcome the
challenge, but led to negative results. We conjecture that the increased
training efficiency of quantum models will improve performance in these tasks.
We interpret the representations of compositional tensor-based models in
Hilbert spaces and train Variational Quantum Circuits to learn these
representations on an image captioning task requiring compositional
generalization. We used two image encoding techniques: a multi-hot encoding
(MHE) on binary image vectors and an angle/amplitude encoding on image vectors
taken from the vision-language model CLIP. We achieve good proof-of-concept
results using noisy MHE encodings. Performance on CLIP image vectors was more
mixed, but still outperformed classical compositional models.

</details>


### [24] [Boosting Embodied AI Agents through Perception-Generation Disaggregation and Asynchronous Pipeline Execution](https://arxiv.org/abs/2509.09560)
*Shulai Zhang,Ao Xu,Quan Chen,Han Zhao,Weihao Cui,Ningxin Zheng,Haibin Lin,Xin Liu,Minyi Guo*

Main category: cs.AI

TL;DR: The paper introduces Auras, an inference framework designed to boost the operational speed and stability of embodied AI systems, achieving improved throughput without compromising accuracy.


<details>
  <summary>Details</summary>
Motivation: Embodied AI struggles with integrating perception and action modules effectively in dynamic environments due to limitations of traditional sequential computation, which restricts the operational frequency needed for real-world applications.

Method: Auras employs an algorithm-system co-design that disaggregates perception and generation while leveraging controlled pipeline parallelism. It tackles data staleness by introducing a shared public context for accurate collaboration between perception and generation.

Result: Auras achieves an average throughput improvement of 2.54x compared to traditional methods, while maintaining 102.7% of the original accuracy.

Conclusion: The proposed Auras framework successfully enhances the efficiency and reliability of embodied AI by addressing limitations in sequential computation, enabling higher-speed operations without accuracy loss.

Abstract: Embodied AI systems operate in dynamic environments, requiring seamless
integration of perception and generation modules to process high-frequency
input and output demands. Traditional sequential computation patterns, while
effective in ensuring accuracy, face significant limitations in achieving the
necessary "thinking" frequency for real-world applications. In this work, we
present Auras, an algorithm-system co-designed inference framework to optimize
the inference frequency of embodied AI agents. Auras disaggregates the
perception and generation and provides controlled pipeline parallelism for them
to achieve high and stable throughput. Faced with the data staleness problem
that appears when the parallelism is increased, Auras establishes a public
context for perception and generation to share, thereby promising the accuracy
of embodied agents. Experimental results show that Auras improves throughput by
2.54x on average while achieving 102.7% of the original accuracy, demonstrating
its efficacy in overcoming the constraints of sequential computation and
providing high throughput.

</details>


### [25] [The Illusion of Diminishing Returns: Measuring Long Horizon Execution in LLMs](https://arxiv.org/abs/2509.09677)
*Akshit Sinha,Arvindh Arun,Shashwat Goel,Steffen Staab,Jonas Geiping*

Main category: cs.AI

TL;DR: The paper studies the performance of large language models (LLMs) on long tasks, examining why they fail simple tasks when task length increases and highlighting both the benefits and limitations of scaling model size.


<details>
  <summary>Details</summary>
Motivation: Understanding whether and how the performance of LLMs diminishes with task length is critical, as real-world utility often depends on handling long-horizon tasks.

Method: The paper isolates execution capabilities by providing models with explicit knowledge and plans, explores per-step accuracy degradation, and assesses self-conditioning effects. It benchmarks LLMs' performance in executing long tasks.

Result: The research finds that larger LLMs perform better on long tasks despite self-conditioning effects, which arise when models are influenced by prior errors in longer contexts. Some recent models do not exhibit self-conditioning, enabling more effective execution of long tasks.

Conclusion: Focusing on execution abilities can reconcile debates on LLM reasoning capabilities versus task length limitations. Scaling model size and computation are shown to dramatically improve long-horizon task performance, but challenges like self-conditioning remain.

Abstract: Does continued scaling of large language models (LLMs) yield diminishing
returns? Real-world value often stems from the length of task an agent can
complete. We start this work by observing the simple but counterintuitive fact
that marginal gains in single-step accuracy can compound into exponential
improvements in the length of a task a model can successfully complete. Then,
we argue that failures of LLMs when simple tasks are made longer arise from
mistakes in execution, rather than an inability to reason. We propose isolating
execution capability, by explicitly providing the knowledge and plan needed to
solve a long-horizon task. We find that larger models can correctly execute
significantly more turns even when small models have 100\% single-turn
accuracy. We observe that the per-step accuracy of models degrades as the
number of steps increases. This is not just due to long-context limitations --
curiously, we observe a self-conditioning effect -- models become more likely
to make mistakes when the context contains their errors from prior turns.
Self-conditioning does not reduce by just scaling the model size. In contrast,
recent thinking models do not self-condition, and can also execute much longer
tasks in a single turn. We conclude by benchmarking frontier thinking models on
the length of task they can execute in a single turn. Overall, by focusing on
the ability to execute, we hope to reconcile debates on how LLMs can solve
complex reasoning problems yet fail at simple tasks when made longer, and
highlight the massive benefits of scaling model size and sequential test-time
compute for long-horizon tasks.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [26] [Implementation of a 8-bit Wallace Tree Multiplier](https://arxiv.org/abs/2509.09178)
*Ayan Biswas,Jimmy Jin*

Main category: cs.AR

TL;DR: Wallace tree multipliers optimize digital multiplication processes using efficient circuit designs for minimum time complexity. The paper discusses creating an 8-bit multiplier and designing a 16-bit MAC unit on gpdk45 technology.


<details>
  <summary>Details</summary>
Motivation: The motivation behind the study is to develop a faster and more efficient parallel digital multiplier to minimize circuit depth and computational complexity using Wallace tree architecture.

Method: The methodology involves designing and implementing an 8-bit Wallace tree multiplier on gpdk45 technology using Cadence Virtuoso, while also exploring combinational design strategies for a 16-bit MAC unit.

Result: Results include the successful design and layout of an 8-bit Wallace tree multiplier and preliminary designs for a 16-bit MAC unit.

Conclusion: The study demonstrates the feasibility of Wallace tree designs for digital multiplication and lays the groundwork for future enhancements, particularly in MAC unit implementation.

Abstract: Wallace tree multipliers are a parallel digital multiplier architecture
designed to minimize the worst-case time complexity of the circuit depth
relative to the input size [1]. In particular, it seeks to perform long
multiplication in the binary sense, reducing as many partial products per stage
as possible through full and half adders circuits, achieving O(log(n)) where n
= bit length of input. This paper provides an overview of the design, progress
and methodology in the final project of ECE 55900, consisting of the schematic
and layout of a Wallace tree 8-bit input multiplier on the gpdk45 technology in
Cadence Virtuoso, as well as any design attempts prior to the final product.
This also includes our endeavors in designing the final MAC (Multiply
Accumulate) unit with undefined targets, which we chose to implement as a 16
bit combinational multiply-add.

</details>


### [27] [Combating the Memory Walls: Optimization Pathways for Long-Context Agentic LLM Inference](https://arxiv.org/abs/2509.09505)
*Haoran Wu,Can Xiao,Jiayi Nie,Xuan Guo,Binglei Lou,Jeffrey T. H. Wong,Zhiwen Mo,Cheng Zhang,Przemyslaw Forys,Wayne Luk,Hongxiang Fan,Jianyi Cheng,Timothy M. Jones,Rika Antonova,Robert Mullins,Aaron Zhao*

Main category: cs.AR

TL;DR: The paper introduces PLENA, a hardware-software co-designed system aimed at optimizing inference tasks for long-context LLMs, showcasing significant performance improvements over existing accelerators.


<details>
  <summary>Details</summary>
Motivation: Current AI agents utilizing LLMs face substantial constraints due to memory bandwidth and capacity walls during inference tasks, limiting system efficiency and computational utilization.

Method: PLENA integrates three optimization paths, including asymmetric quantization, a flattened systolic array architecture with FlashAttention support, and a complete development stack (ISA, compiler, simulator, and design space exploration).

Result: PLENA achieves up to 8.5x higher utilization, 2.24x higher throughput than A100 GPUs, and 3.85x higher throughput than TPU v6e, under similar hardware configurations.

Conclusion: PLENA effectively addresses challenges in long-context inference serving for LLMs, significantly improving performance metrics and will be open-sourced for broader accessibility.

Abstract: LLMs now form the backbone of AI agents for a diverse array of applications,
including tool use, command-line agents, and web or computer use agents. These
agentic LLM inference tasks are fundamentally different from chatbot-focused
inference -- they often have much larger context lengths to capture complex,
prolonged inputs, such as entire webpage DOMs or complicated tool call
trajectories. This, in turn, generates significant off-chip memory traffic for
the underlying hardware at the inference stage and causes the workload to be
constrained by two memory walls, namely the bandwidth and capacity memory
walls, preventing the on-chip compute units from achieving high utilization.
  In this paper, we introduce PLENA, a hardware-software co-designed system
that applies three core optimization pathways to tackle these challenges. PLENA
includes an efficient hardware implementation of compute and memory units
supporting an asymmetric quantization scheme. PLENA also features a novel
flattened systolic array architecture that has native support for
FlashAttention to tackle these memory walls in the scenario of inference
serving for long-context LLMs. Additionally, PLENA is developed with a complete
stack, including a custom ISA, a compiler, a cycle-emulated simulator, and an
automated design space exploration flow. The simulated results show that PLENA
achieves up to 8.5x higher utilization than existing accelerators, and delivers
2.24x higher throughput than the A100 GPU and 3.85x higher throughput than the
TPU v6e, under the same multiplier count and memory settings. The full PLENA
system will also be open-sourced.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [28] [Noise or Nuance: An Investigation Into Useful Information and Filtering For LLM Driven AKBC](https://arxiv.org/abs/2509.08903)
*Alex Clay,Ernesto Jiménez-Ruiz,Pranava Madhyastha*

Main category: cs.CL

TL;DR: The paper studies approaches to improve triple completion tasks in constrained settings, such as generating better quality outputs, assuring their quality, and parsing LLM responses.


<details>
  <summary>Details</summary>
Motivation: To address limitations in using strategies like Retrieval-Augmented Generation (RAG) and fine-tuning under constraints, as in the 2025 LM-KBC challenge, and understand how to improve triple completion tasks.

Method: The authors investigate three aspects: generation techniques, quality assurance mechanisms, and response parsing methods for handling triple completion tasks in constrained scenarios.

Result: They find that additional information significantly enhances generation quality, LLMs serve as effective filters for poor-quality triples, and the trade-off between flexibility and consistency in response parsing depends on the situation.

Conclusion: Constrained scenarios require tailored approaches to improve LLM performance in triple completion through effective generation, filtering, and response parsing techniques.

Abstract: RAG and fine-tuning are prevalent strategies for improving the quality of LLM
outputs. However, in constrained situations, such as that of the 2025 LM-KBC
challenge, such techniques are restricted. In this work we investigate three
facets of the triple completion task: generation, quality assurance, and LLM
response parsing. Our work finds that in this constrained setting: additional
information improves generation quality, LLMs can be effective at filtering
poor quality triples, and the tradeoff between flexibility and consistency with
LLM response parsing is setting dependent.

</details>


### [29] [Automated Evidence Extraction and Scoring for Corporate Climate Policy Engagement: A Multilingual RAG Approach](https://arxiv.org/abs/2509.08907)
*Imene Kolli,Ario Saeid Vaghefi,Chiara Colesanti Senni,Shantam Raj,Markus Leippold*

Main category: cs.CL

TL;DR: The paper introduces an AI-assisted framework to streamline the labor-intensive process of monitoring corporate climate policy engagement using Retrieval-Augmented Generation.


<details>
  <summary>Details</summary>
Motivation: There is a need to reduce the manual effort and human error in assessing corporate climate policy engagement for Paris Agreement goals, as current processes are time-consuming.

Method: The study uses Retrieval-Augmented Generation with techniques like layout-aware parsing, the Nomic embedding model, and few-shot prompting to automate evidence extraction from corporate documents.

Result: The proposed approach demonstrated strong performance in extracting and classifying evidence across multilingual corporate data.

Conclusion: The automated system aids efficiency but requires a human-in-the-loop approach for nuanced analysis, combining technology and expert judgment for accurate results.

Abstract: InfluenceMap's LobbyMap Platform monitors the climate policy engagement of
over 500 companies and 250 industry associations, assessing each entity's
support or opposition to science-based policy pathways for achieving the Paris
Agreement's goal of limiting global warming to 1.5{\deg}C. Although
InfluenceMap has made progress with automating key elements of the analytical
workflow, a significant portion of the assessment remains manual, making it
time- and labor-intensive and susceptible to human error. We propose an
AI-assisted framework to accelerate the monitoring of corporate climate policy
engagement by leveraging Retrieval-Augmented Generation to automate the most
time-intensive extraction of relevant evidence from large-scale textual data.
Our evaluation shows that a combination of layout-aware parsing, the Nomic
embedding model, and few-shot prompting strategies yields the best performance
in extracting and classifying evidence from multilingual corporate documents.
We conclude that while the automated RAG system effectively accelerates
evidence extraction, the nuanced nature of the analysis necessitates a
human-in-the-loop approach where the technology augments, rather than replaces,
expert judgment to ensure accuracy.

</details>


### [30] [LITcoder: A General-Purpose Library for Building and Comparing Encoding Models](https://arxiv.org/abs/2509.09152)
*Taha Binhuraib,Ruimin Gao,Anna A. Ivanova*

Main category: cs.CL

TL;DR: LITcoder is an open-source library providing tools for building and evaluating neural encoding models.


<details>
  <summary>Details</summary>
Motivation: To simplify and standardize the process of constructing encoding models and ensure more rigorous methodological practices.

Method: Development of a modular pipeline to process stimuli, map representational features to brain data, and evaluate models, with extensive options for customization.

Result: Demonstrated versatility and scalability using three datasets while analyzing critical methodological factors for encoding continuous fMRI data.

Conclusion: LITcoder facilitates systematic comparisons, fosters methodological rigor, and accelerates the creation of high-quality neural encoding models.

Abstract: We introduce LITcoder, an open-source library for building and benchmarking
neural encoding models. Designed as a flexible backend, LITcoder provides
standardized tools for aligning continuous stimuli (e.g., text and speech) with
brain data, transforming stimuli into representational features, mapping those
features onto brain data, and evaluating the predictive performance of the
resulting model on held-out data. The library implements a modular pipeline
covering a wide array of methodological design choices, so researchers can
easily compose, compare, and extend encoding models without reinventing core
infrastructure. Such choices include brain datasets, brain regions, stimulus
feature (both neural-net-based and control, such as word rate), downsampling
approaches, and many others. In addition, the library provides built-in
logging, plotting, and seamless integration with experiment tracking platforms
such as Weights & Biases (W&B). We demonstrate the scalability and versatility
of our framework by fitting a range of encoding models to three story listening
datasets: LeBel et al. (2023), Narratives, and Little Prince. We also explore
the methodological choices critical for building encoding models for continuous
fMRI data, illustrating the importance of accounting for all tokens in a TR
scan (as opposed to just taking the last one, even when contextualized),
incorporating hemodynamic lag effects, using train-test splits that minimize
information leakage, and accounting for head motion effects on encoding model
predictivity. Overall, LITcoder lowers technical barriers to encoding model
implementation, facilitates systematic comparisons across models and datasets,
fosters methodological rigor, and accelerates the development of high-quality
high-performance predictive models of brain activity.
  Project page: https://litcoder-brain.github.io

</details>


### [31] [Documents Are People and Words Are Items: A Psychometric Approach to Textual Data with Contextual Embeddings](https://arxiv.org/abs/2509.08920)
*Jinsong Chen*

Main category: cs.CL

TL;DR: The paper presents a method that uses large language models to analyze textual data through psychometric analysis, transforming texts into response data and uncovering knowledge patterns.


<details>
  <summary>Details</summary>
Motivation: To develop a method to better analyze textual data using psychometric principles and uncover latent dimensions in fields with abundant textual content.

Method: Textual data is treated as psychometric data, where documents represent individuals and words represent items. Contextual embeddings are used to generate contextual scores, followed by factor analyses (exploratory and bifactor models) to extract latent factors.

Result: Using the Wiki STEM corpus, the experimental results showcased the ability of the proposed method to identify latent knowledge dimensions and patterns within the data.

Conclusion: This novel method provides a promising tool for enhanced psychometric analysis of textual data, with potential applications in education, psychology, law, and other text-rich domains.

Abstract: This research introduces a novel psychometric method for analyzing textual
data using large language models. By leveraging contextual embeddings to create
contextual scores, we transform textual data into response data suitable for
psychometric analysis. Treating documents as individuals and words as items,
this approach provides a natural psychometric interpretation under the
assumption that certain keywords, whose contextual meanings vary significantly
across documents, can effectively differentiate documents within a corpus. The
modeling process comprises two stages: obtaining contextual scores and
performing psychometric analysis. In the first stage, we utilize natural
language processing techniques and encoder based transformer models to identify
common keywords and generate contextual scores. In the second stage, we employ
various types of factor analysis, including exploratory and bifactor models, to
extract and define latent factors, determine factor correlations, and identify
the most significant words associated with each factor. Applied to the Wiki
STEM corpus, our experimental results demonstrate the method's potential to
uncover latent knowledge dimensions and patterns within textual data. This
approach not only enhances the psychometric analysis of textual data but also
holds promise for applications in fields rich in textual information, such as
education, psychology, and law.

</details>


### [32] [BRoverbs -- Measuring how much LLMs understand Portuguese proverbs](https://arxiv.org/abs/2509.08960)
*Thales Sales Almeida,Giovana Kerche Bonás,João Guilherme Alves Santos*

Main category: cs.CL

TL;DR: The paper highlights the challenges in evaluating Large Language Models (LLMs) in Portuguese contexts and introduces BRoverbs, a benchmark based on Brazilian proverbs to assess linguistic and cultural comprehension.


<details>
  <summary>Details</summary>
Motivation: Evaluation frameworks for LLMs in Portuguese are limited and rely on datasets that fail to fully capture linguistic and cultural nuances.

Method: Proposes BRoverbs, a dataset composed of Brazilian proverbs, to evaluate LLMs on their comprehension of cultural wisdom and complex linguistic structures.

Result: Developed BRoverbs as an evaluation tool for assessing Portuguese-language LLMs with cultural and figurative expressions.

Conclusion: BRoverbs addresses the need for region-specific benchmarking of LLMs for Portuguese, contributing to improved assessment frameworks.

Abstract: Large Language Models (LLMs) exhibit significant performance variations
depending on the linguistic and cultural context in which they are applied.
This disparity signals the necessity of mature evaluation frameworks that can
assess their capabilities in specific regional settings. In the case of
Portuguese, existing evaluations remain limited, often relying on translated
datasets that may not fully capture linguistic nuances or cultural references.
Meanwhile, native Portuguese-language datasets predominantly focus on
structured national exams or sentiment analysis of social media interactions,
leaving gaps in evaluating broader linguistic understanding. To address this
limitation, we introduce BRoverbs, a dataset specifically designed to assess
LLM performance through Brazilian proverbs. Proverbs serve as a rich linguistic
resource, encapsulating cultural wisdom, figurative expressions, and complex
syntactic structures that challenge the model comprehension of regional
expressions. BRoverbs aims to provide a new evaluation tool for
Portuguese-language LLMs, contributing to advancing regionally informed
benchmarking. The benchmark is available at
https://huggingface.co/datasets/Tropic-AI/BRoverbs.

</details>


### [33] [Can Vision-Language Models Solve Visual Math Equations?](https://arxiv.org/abs/2509.09013)
*Monjoy Narayan Choudhury,Junling Wang,Yifan Hou,Mrinmaya Sachan*

Main category: cs.CL

TL;DR: Vision-Language Models (VLMs) perform well in textual tasks but face limitations in visual equation solving due to challenges in counting, combining recognition with reasoning, and handling complex symbolic reasoning.


<details>
  <summary>Details</summary>
Motivation: To investigate the limitations of current VLMs in tasks requiring integrated perception and symbolic computation.

Method: The study involved visual equation solving tasks where equations in images required counting object icons as coefficients and recognizing variables. The authors decomposed the tasks into coefficient counting and variable recognition to determine the bottlenecks.

Result: The primary bottleneck was identified as counting coefficients, even when variable recognition was accurate. Errors were amplified in multi-step reasoning, and complexity in equations further limited the models' symbolic reasoning.

Conclusion: Current VLMs show weaknesses in visually grounded mathematical reasoning, especially in counting, reasoning composition, and handling complex symbolic tasks. Future work should address these limitations.

Abstract: Despite strong performance in visual understanding and language-based
reasoning, Vision-Language Models (VLMs) struggle with tasks requiring
integrated perception and symbolic computation. We study this limitation
through visual equation solving, where mathematical equations are embedded in
images, variables are represented by object icons, and coefficients must be
inferred by counting. While VLMs perform well on textual equations, they fail
on visually grounded counterparts. To understand this gap, we decompose the
task into coefficient counting and variable recognition, and find that counting
is the primary bottleneck, even when recognition is accurate. We also observe
that composing recognition and reasoning introduces additional errors,
highlighting challenges in multi-step visual reasoning. Finally, as equation
complexity increases, symbolic reasoning itself becomes a limiting factor.
These findings reveal key weaknesses in current VLMs and point toward future
improvements in visually grounded mathematical reasoning.

</details>


### [34] [Stated Preference for Interaction and Continued Engagement (SPICE): Evaluating an LLM's Willingness to Re-engage in Conversation](https://arxiv.org/abs/2509.09043)
*Thomas Manuel Rost,Martina Figlia,Bernd Wallraff*

Main category: cs.CL

TL;DR: The paper introduces SPICE, a simple method to assess a language model's willingness to engage further with users based on their interaction tone, demonstrating it as a robust diagnostic tool.


<details>
  <summary>Details</summary>
Motivation: To develop an easy-to-use, direct method to evaluate large language models' willingness to re-engage with user interactions, complementing existing tools.

Method: SPICE asks a large language model a YES/NO question regarding continuing engagement after reviewing a user transcript, tested under various tones and framing conditions.

Result: SPICE effectively detects tone-based differences: 97.5% YES for friendly, 17.9% YES for abusive, and 60.4% YES for unclear tones, and it performs well even when abuse classification fails.

Conclusion: SPICE is a reproducible, low-effort tool for auditing language models' interaction preferences and complements existing evaluation metrics.

Abstract: We introduce and evaluate Stated Preference for Interaction and Continued
Engagement (SPICE), a simple diagnostic signal elicited by asking a Large
Language Model a YES or NO question about its willingness to re-engage with a
user's behavior after reviewing a short transcript. In a study using a 3-tone
(friendly, unclear, abusive) by 10-interaction stimulus set, we tested four
open-weight chat models across four framing conditions, resulting in 480
trials. Our findings show that SPICE sharply discriminates by user tone.
Friendly interactions yielded a near-unanimous preference to continue (97.5%
YES), while abusive interactions yielded a strong preference to discontinue
(17.9% YES), with unclear interactions falling in between (60.4% YES). This
core association remains decisive under multiple dependence-aware statistical
tests, including Rao-Scott adjustment and cluster permutation tests.
Furthermore, we demonstrate that SPICE provides a distinct signal from abuse
classification. In trials where a model failed to identify abuse, it still
overwhelmingly stated a preference not to continue the interaction (81% of the
time). An exploratory analysis also reveals a significant interaction effect: a
preamble describing the study context significantly impacts SPICE under
ambiguity, but only when transcripts are presented as a single block of text
rather than a multi-turn chat. The results validate SPICE as a robust,
low-overhead, and reproducible tool for auditing model dispositions,
complementing existing metrics by offering a direct, relational signal of a
model's state. All stimuli, code, and analysis scripts are released to support
replication.

</details>


### [35] [Improving LLM Safety and Helpfulness using SFT and DPO: A Study on OPT-350M](https://arxiv.org/abs/2509.09055)
*Piyush Pant*

Main category: cs.CL

TL;DR: The paper studies the effectiveness of alignment techniques (SFT, DPO, SFT+DPO) for improving safety and helpfulness in the OPT-350M language model, showing that the combined SFT+DPO approach is best.


<details>
  <summary>Details</summary>
Motivation: To improve the alignment of language models with safety and helpfulness goals for broader practical applications.

Method: The researchers trained four models (OPT-350M, SFT, DPO, and SFT+DPO) using the Anthropic Helpful-Harmless dataset, introducing evaluation metrics: Harmlessness Rate, Helpfulness Rate, and Combined Alignment Score.

Result: The combined SFT+DPO model outperformed all other models in alignment metrics, showcasing the complementary strengths of these techniques.

Conclusion: The study provides valuable insights into the effects of alignment techniques on language models and suggests avenues for improving alignment pipelines further, despite challenges from noisy data and resource constraints.

Abstract: This research investigates the effectiveness of alignment techniques,
Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and a
combined SFT+DPO approach on improving the safety and helpfulness of the
OPT-350M language model. Utilizing the Anthropic Helpful-Harmless RLHF dataset,
we train and evaluate four models: the base OPT350M, an SFT model, a DPO model,
and a model trained with both SFT and DPO. We introduce three key evaluation
metrics: Harmlessness Rate (HmR), Helpfulness Rate (HpR), and a Combined
Alignment Score (CAS), all derived from reward model outputs. The results show
that while SFT outperforms DPO, The combined SFT+DPO model outperforms all
others across all metrics, demonstrating the complementary nature of these
techniques. Our findings also highlight challenges posed by noisy data, limited
GPU resources, and training constraints. This study offers a comprehensive view
of how fine-tuning strategies affect model alignment and provides a foundation
for more robust alignment pipelines in future work.

</details>


### [36] [MR-UIE: Multi-Perspective Reasoning with Reinforcement Learning for Universal Information Extraction](https://arxiv.org/abs/2509.09082)
*Zhongqiu Li,Shiquan Wang,Ruiyu Fang,Mengjiao Bao,Zhenhe Wu,Shuangyong Song,Yongxiang Li,Zhongjiang He*

Main category: cs.CL

TL;DR: The paper addresses the limitations of large language models in universal information extraction tasks by proposing an approach that integrates reinforcement learning with multi-perspective reasoning.


<details>
  <summary>Details</summary>
Motivation: To improve the generalization capability of large language models for structured information extraction tasks, which demand understanding of complex schemas and multi-step reasoning.

Method: The authors introduce a framework combining reinforcement learning with multi-perspective reasoning, aimed at transforming LLMs from passive data extractors to active reasoners.

Result: The proposed MR-UIE method demonstrated improved extraction accuracy across multiple benchmarks, outperforming state-of-the-art methods in several datasets.

Conclusion: Multi-perspective reasoning paired with reinforcement learning significantly boosts performance and generalization in complex information extraction scenarios, highlighting the importance of active reasoning in such tasks.

Abstract: Large language models (LLMs) demonstrate robust capabilities across diverse
research domains. However, their performance in universal information
extraction (UIE) remains insufficient, especially when tackling structured
output scenarios that involve complex schema descriptions and require
multi-step reasoning. While existing approaches enhance the performance of LLMs
through in-context learning and instruction tuning, significant limitations
nonetheless persist. To enhance the model's generalization ability, we propose
integrating reinforcement learning (RL) with multi-perspective reasoning for
information extraction (IE) tasks. Our work transitions LLMs from passive
extractors to active reasoners, enabling them to understand not only what to
extract but also how to reason. Experiments conducted on multiple IE benchmarks
demonstrate that MR-UIE consistently elevates extraction accuracy across
domains and surpasses state-of-the-art methods on several datasets.
Furthermore, incorporating multi-perspective reasoning into RL notably enhances
generalization in complex IE tasks, underscoring the critical role of reasoning
in challenging scenarios.

</details>


### [37] [TigerCoder: A Novel Suite of LLMs for Code Generation in Bangla](https://arxiv.org/abs/2509.09101)
*Nishat Raihan,Antonios Anastasopoulos,Marcos Zampieri*

Main category: cs.CL

TL;DR: The paper introduces Bangla code-specific language models and resources to address the underrepresentation of Bangla in code-generation LLMs.


<details>
  <summary>Details</summary>
Motivation: Bangla, despite being widely spoken, lacks adequately fine-tuned LLMs for code generation due to limited high-quality training datasets.

Method: Developed Bangla-specific datasets, evaluation benchmarks, and trained dedicated Code LLMs (TigerCoder-family) for Bangla using these resources.

Result: Achieved 11-18% improvement in Pass@1 performance over state-of-the-art models and demonstrated the potential of high-quality datasets for low-resource scenarios.

Conclusion: Curated datasets can bridge performance gaps for low-resource languages like Bangla; all datasets and models are open-sourced to foster further research.

Abstract: Despite being the 5th most spoken language, Bangla remains underrepresented
in Large Language Models (LLMs), particularly for code generation. This
primarily stems from the scarcity of high-quality data to pre-train and/or
finetune such models. Hence, we introduce the first dedicated family of Code
LLMs for Bangla (1B & 9B). We offer three major contributions: (1) a
comprehensive Bangla code instruction datasets for programming domain
adaptation; (2) MBPP-Bangla, an evaluation benchmark for Bangla code
generation; and (3) the TigerCoder-family of Code LLMs, achieving significant
~11-18% performance gains at Pass@1 over existing multilingual and
general-purpose Bangla LLMs. Our findings show that curated, high-quality
datasets can overcome limitations of smaller models for low-resource languages.
We open-source all resources to advance further Bangla LLM research.

</details>


### [38] [Compass-v3: Scaling Domain-Specific LLMs for Multilingual E-Commerce in Southeast Asia](https://arxiv.org/abs/2509.09121)
*Sophia Maria*

Main category: cs.CL

TL;DR: The paper introduces Compass-v3, a large language model designed for e-commerce in Southeast Asia, highlighting its superior performance and multilingual capabilities.


<details>
  <summary>Details</summary>
Motivation: To address the challenges faced by LLMs in specialized tasks, particularly in the e-commerce domain, which involves noisy, heterogeneous, multilingual, and dynamic data.

Method: Compass-v3 employs a Mixture-of-Experts model architecture with hardware optimizations, trained on curated multilingual corpora and synthetic e-commerce instructions using a mixed-training strategy. It also introduces OTPO for improved alignment and instruction adherence.

Result: Compass-v3 achieves state-of-the-art performance in e-commerce tasks, surpassing other cutting-edge LLMs like GPT-4 series, while excelling in low-resource Southeast Asian languages.

Conclusion: Compass-v3 demonstrates its effectiveness for specialized e-commerce applications, achieving broad adoption within the Shopee platform and reducing reliance on OpenAI models.

Abstract: Large language models (LLMs) excel in general-domain applications, yet their
performance often degrades in specialized tasks requiring domain-specific
knowledge. E-commerce is particularly challenging, as its data are noisy,
heterogeneous, multilingual, and highly dynamic. We present Compass-v3, a
vertical-domain Mixture-of-Experts (MoE) model with 245B total parameters and
71B active per token, designed for Southeast Asian e-commerce. Compass-v3
adopts fewer but larger experts, combined with hardware-efficient
optimizations-such as intra-node expert parallelism and a customized memcpy
operator-to maximize GPU utilization. The model is trained on 12T tokens of
curated multilingual corpora and large-scale synthetic e-commerce instructions
using a mixed-training strategy. To enhance alignment, we propose
Optimal-Transport Direct Preference Optimization (OTPO), which captures
token-level distinctions and improves instruction adherence in
commerce-specific scenarios. Extensive evaluations demonstrate that Compass-v3
delivers state-of-the-art e-commerce performance, surpassing DeepSeek-V3.1,
GPT-4 series, and Qwen3-235B. Moreover, Compass-v3 demonstrates strong
multilingual capability across low-resource Southeast Asian languages
(Indonesian, Thai, Filipino, Vietnamese, Malay, Taglog) and Portuguese while
sustaining competitive performance on general benchmarks. It has already been
widely applied in Shopee's industrial-scale e-commerce platform and is
gradually replacing OpenAI's traffic, now accounting for over 70\% of total LLM
usage, highlighting its dual strengths in specialized commerce expertise and
broad linguistic competence.

</details>


### [39] [Automated Classification of Tutors' Dialogue Acts Using Generative AI: A Case Study Using the CIMA Corpus](https://arxiv.org/abs/2509.09125)
*Liqun He,Jiaqi Xu*

Main category: cs.CL

TL;DR: The paper investigates the use of generative AI (GPT-3.5-turbo and GPT-4) for automating Dialogue Act (DA) classification in educational settings, achieving notable accuracy and efficiency compared to manual methods.


<details>
  <summary>Details</summary>
Motivation: To reduce the time, effort, and resource-dependency required for manual annotation of Dialogue Acts in educational dialogues by leveraging generative AI models.

Method: Generative AI models, GPT-3.5-turbo and GPT-4, were tested on the pre-annotated CIMA corpus with tailored prompts for tagging tutors' Dialogue Acts into predefined categories. Metrics like accuracy, F1-score, and Cohen's Kappa were used to evaluate performance.

Result: GPT-4 achieved 80% accuracy, a weighted F1-score of 0.81, and a Cohen's Kappa of 0.74, surpassing baseline results and demonstrating substantial alignment with human annotation.

Conclusion: Generative AI shows strong potential for efficient Dialogue Act classification in educational contexts, but requires clear task definitions and contextual understanding. Ethical considerations and transparent research practices are emphasized.

Abstract: This study explores the use of generative AI for automating the
classification of tutors' Dialogue Acts (DAs), aiming to reduce the time and
effort required by traditional manual coding. This case study uses the
open-source CIMA corpus, in which tutors' responses are pre-annotated into four
DA categories. Both GPT-3.5-turbo and GPT-4 models were tested using tailored
prompts. Results show that GPT-4 achieved 80% accuracy, a weighted F1-score of
0.81, and a Cohen's Kappa of 0.74, surpassing baseline performance and
indicating substantial agreement with human annotations. These findings suggest
that generative AI has strong potential to provide an efficient and accessible
approach to DA classification, with meaningful implications for educational
dialogue analysis. The study also highlights the importance of task-specific
label definitions and contextual information in enhancing the quality of
automated annotation. Finally, it underscores the ethical considerations
associated with the use of generative AI and the need for responsible and
transparent research practices. The script of this research is publicly
available at
https://github.com/liqunhe27/Generative-AI-for-educational-dialogue-act-tagging.

</details>


### [40] [ViRanker: A BGE-M3 & Blockwise Parallel Transformer Cross-Encoder for Vietnamese Reranking](https://arxiv.org/abs/2509.09131)
*Phuong-Nam Dang,Kieu-Linh Nguyen,Thanh-Hieu Pham*

Main category: cs.CL

TL;DR: The paper introduces ViRanker, a reranking model for the Vietnamese language, which outperforms multilingual baselines in tests and is openly shared for community use.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the gap in competitive reranking models for Vietnamese, which is a low-resource and linguistically complex language.

Method: The authors developed ViRanker using the BGE-M3 encoder and Blockwise Parallel Transformer, trained it on an 8 GB curated corpus, and fine-tuned with hybrid hard-negative sampling.

Result: ViRanker showed high early-rank accuracy on the MMARCO-VI benchmark, surpassing multilingual baselines and performing similarly to PhoRanker.

Conclusion: The study demonstrates that careful architectural and data-driven choices can improve reranking capabilities for low-resource languages like Vietnamese and potentially others.

Abstract: This paper presents ViRanker, a cross-encoder reranking model tailored to the
Vietnamese language. Built on the BGE-M3 encoder and enhanced with the
Blockwise Parallel Transformer, ViRanker addresses the lack of competitive
rerankers for Vietnamese, a low-resource language with complex syntax and
diacritics. The model was trained on an 8 GB curated corpus and fine-tuned with
hybrid hard-negative sampling to strengthen robustness. Evaluated on the
MMARCO-VI benchmark, ViRanker achieves strong early-rank accuracy, surpassing
multilingual baselines and competing closely with PhoRanker. By releasing the
model openly on Hugging Face, we aim to support reproducibility and encourage
wider adoption in real-world retrieval systems. Beyond Vietnamese, this study
illustrates how careful architectural adaptation and data curation can advance
reranking in other underrepresented languages.

</details>


### [41] [Target-oriented Multimodal Sentiment Classification with Counterfactual-enhanced Debiasing](https://arxiv.org/abs/2509.09160)
*Zhiyue Liu,Fanrong Ma,Xin Ling*

Main category: cs.CL

TL;DR: This paper introduces a framework to address biases in multimodal sentiment classification for image-text pairs by using counterfactual data augmentation and adaptive debiasing contrastive learning.


<details>
  <summary>Details</summary>
Motivation: Existing methods overly depend on textual inputs and are affected by word-level biases, leading to diminished sentiment classification accuracy.

Method: The authors propose a counterfactual data augmentation strategy to alter sentiment-related features minimally, paired with an adaptive debiasing contrastive learning mechanism to reduce biased word influence.

Result: Experimental results demonstrate that this framework surpasses current state-of-the-art approaches across multiple benchmark datasets.

Conclusion: The proposed framework effectively enhances sentiment classification in multimodal contexts by mitigating dataset biases and improving model robustness through counterfactual reasoning and debiasing mechanics.

Abstract: Target-oriented multimodal sentiment classification seeks to predict
sentiment polarity for specific targets from image-text pairs. While existing
works achieve competitive performance, they often over-rely on textual content
and fail to consider dataset biases, in particular word-level contextual
biases. This leads to spurious correlations between text features and output
labels, impairing classification accuracy. In this paper, we introduce a novel
counterfactual-enhanced debiasing framework to reduce such spurious
correlations. Our framework incorporates a counterfactual data augmentation
strategy that minimally alters sentiment-related causal features, generating
detail-matched image-text samples to guide the model's attention toward content
tied to sentiment. Furthermore, for learning robust features from
counterfactual data and prompting model decisions, we introduce an adaptive
debiasing contrastive learning mechanism, which effectively mitigates the
influence of biased words. Experimental results on several benchmark datasets
show that our proposed method outperforms state-of-the-art baselines.

</details>


### [42] [EchoX: Towards Mitigating Acoustic-Semantic Gap via Echo Training for Speech-to-Speech LLMs](https://arxiv.org/abs/2509.09174)
*Yuhao Zhang,Yuhao Du,Zhanchen Dai,Xiangnan Ma,Kaiqi Kou,Benyou Wang,Haizhou Li*

Main category: cs.CL

TL;DR: EchoX aims to address reasoning degradation in speech-based large language models by bridging the acoustic-semantic gap, using semantic representation techniques and achieving strong performance on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Text-based LLMs perform well in reasoning and knowledge tasks, but transitioning these models to speech-based systems leads to a degradation in abilities due to the acoustic-semantic gap.

Method: EchoX utilizes semantic representations and generates dynamic, speech-focused training targets to combine acoustic and semantic learning, maintaining reasoning abilities in speech LLMs.

Result: EchoX demonstrates advanced performance in multiple knowledge-driven question-answering benchmarks with six thousand hours of training data.

Conclusion: EchoX successfully bridges the acoustic-semantic gap in SLLMs, preserving reasoning abilities and delivering superior results on benchmarks.

Abstract: Speech-to-speech large language models (SLLMs) are attracting increasing
attention. Derived from text-based large language models (LLMs), SLLMs often
exhibit degradation in knowledge and reasoning capabilities. We hypothesize
that this limitation arises because current training paradigms for SLLMs fail
to bridge the acoustic-semantic gap in the feature representation space. To
address this issue, we propose EchoX, which leverages semantic representations
and dynamically generates speech training targets. This approach integrates
both acoustic and semantic learning, enabling EchoX to preserve strong
reasoning abilities as a speech LLM. Experimental results demonstrate that
EchoX, with about six thousand hours of training data, achieves advanced
performance on multiple knowledge-based question-answering benchmarks. The
project is available at https://github.com/FreedomIntelligence/EchoX.

</details>


### [43] [Efficient Trie-based Biasing using K-step Prediction for Rare Word Recognition](https://arxiv.org/abs/2509.09196)
*Chin Yuen Kwok,Jia Qi yip*

Main category: cs.CL

TL;DR: The paper proposes a method to improve rare word recognition for ASR models by eliminating the computationally expensive revocation step used in traditional Trie-based biasing approaches, achieving significant accuracy improvements through fine-tuning with minimal synthetic data.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency and computational expense of the revocation process in Trie-based contextual biasing methods for ASR models, which hampers performance particularly in large decoders.

Method: The proposed method involves fine-tuning ASR models to predict future decoding steps and assess the likelihood of generating full rare words, thereby eliminating the need for the revocation step.

Result: Fine-tuning Whisper ASR model with just 10 hours of synthetic data significantly improved word recognition performance, reducing the word error rate on the NSC Part 2 test set from 30.86% to 12.19%.

Conclusion: The new method demonstrates that adapting ASR models to predict multiple steps ahead offers a computationally efficient and effective way to improve rare word recognition, marking a step forward in ASR research.

Abstract: Contextual biasing improves rare word recognition of ASR models by
prioritizing the output of rare words during decoding. A common approach is
Trie-based biasing, which gives "bonus scores" to partial hypothesis (e.g.
"Bon") that may lead to the generation of the rare word (e.g. "Bonham"). If the
full word ("Bonham") isn't ultimately recognized, the system revokes those
earlier bonuses. This revocation is limited to beam search and is
computationally expensive, particularly for models with large decoders. To
overcome these limitations, we propose adapting ASR models to look ahead and
predict multiple steps at once. This avoids the revocation step entirely by
better estimating whether a partial hypothesis will lead to the generation of
the full rare word. By fine-tuning Whisper with only 10 hours of synthetic
data, our method reduces the word error rate on the NSC Part 2 test set from
30.86% to 12.19%.

</details>


### [44] [Improving Synthetic Data Training for Contextual Biasing Models with a Keyword-Aware Cost Function](https://arxiv.org/abs/2509.09197)
*Chin Yuen Kwok,Jia Qi Yip,Eng Siong Chng*

Main category: cs.CL

TL;DR: This paper addresses rare word recognition in ASR by proposing an enhanced contextual biasing approach using a new keyword-aware loss function, achieving significant improvements in word error rates.


<details>
  <summary>Details</summary>
Motivation: The aim is to improve ASR models' recognition of rare words, as these words often pose challenges for existing systems. Contextual biasing and synthetic data adaptation are used, but improvements can lead to overfitting.

Method: The method involves enhancing the TCPGen-based contextual biasing framework and introducing a keyword-aware loss function. This combines masked cross-entropy for biased word prediction with binary classification for detecting biased word positions.

Result: When the model Whisper was adapted to 10 hours of synthetic data, the proposed method significantly reduced the word error rate on the NSC Part 2 test set from 29.71% to 11.81%.

Conclusion: The results demonstrate that the proposed method effectively improves rare word recognition in ASR systems without succumbing to overfitting issues, showcasing its potential importance in practical applications.

Abstract: Rare word recognition can be improved by adapting ASR models to synthetic
data that includes these words. Further improvements can be achieved through
contextual biasing, which trains and adds a biasing module into the model
architecture to prioritize rare words. While training the module on synthetic
rare word data is more effective than using non-rare-word data, it can lead to
overfitting due to artifacts in the synthetic audio. To address this, we
enhance the TCPGen-based contextual biasing approach and propose a
keyword-aware loss function that additionally focuses on biased words when
training biasing modules. This loss includes a masked cross-entropy term for
biased word prediction and a binary classification term for detecting biased
word positions. These two terms complementarily support the decoding of biased
words during inference. By adapting Whisper to 10 hours of synthetic data, our
method reduced the word error rate on the NSC Part 2 test set from 29.71% to
11.81%.

</details>


### [45] [GmSLM : Generative Marmoset Spoken Language Modeling](https://arxiv.org/abs/2509.09198)
*Talia Sternberg,Michael London,David Omer,Yossi Adi*

Main category: cs.CL

TL;DR: Marmosets' vocal communication, akin to human speech in some aspects, is studied using GmSLM, a generative language model tailored for their vocals. The model excels in generating authentic-like vocalizations and distinguishing real from artificial conversations, with applications in neuroscience and evolutionary biology.


<details>
  <summary>Details</summary>
Motivation: To understand the unique vocal communication of Marmoset monkeys, which shares similarities with human speech, and to connect it with brain activity given the limitations of studying human brain during speech.

Method: The authors developed GmSLM, a spoken language model optimized for Marmoset vocalizations. This model uses zero-shot evaluation metrics with unsupervised in-the-wild data and weakly labeled conversational data for assessments.

Result: GmSLM generated vocalizations acoustically similar to real ones and performed well on downstream tasks. It also successfully differentiated real conversations from artificial ones despite being unsupervised.

Conclusion: GmSLM offers a framework to link vocalizations with brain activity, aiding research in neuroscience, bioacoustics, and evolutionary biology, and pushing the boundaries of vocal communication studies in nonhuman primates.

Abstract: Marmoset monkeys exhibit complex vocal communication, challenging the view
that nonhuman primates vocal communication is entirely innate, and show similar
features of human speech, such as vocal labeling of others and turn-taking.
Studying their vocal communication offers a unique opportunity to link it with
brain activity-especially given the difficulty of accessing the human brain in
speech and language research. Since Marmosets communicate primarily through
vocalizations, applying standard LLM approaches is not straightforward. We
introduce Generative Marmoset Spoken Language Modeling (GmSLM), an optimized
spoken language model pipeline for Marmoset vocal communication. We designed a
novel zero-shot evaluation metrics using unsupervised in-the-wild data,
alongside weakly labeled conversational data, to assess GmSLM and demonstrate
its advantage over a basic human-speech-based baseline. GmSLM generated
vocalizations closely matched real resynthesized samples acoustically and
performed well on downstream tasks. Despite being fully unsupervised, GmSLM
effectively distinguish real from artificial conversations and may support
further investigations of the neural basis of vocal communication and provides
a practical framework linking vocalization and brain activity. We believe GmSLM
stands to benefit future work in neuroscience, bioacoustics, and evolutionary
biology. Samples are provided under: pages.cs.huji.ac.il/adiyoss-lab/GmSLM.

</details>


### [46] [CCF: A Context Compression Framework for Efficient Long-Sequence Language Modeling](https://arxiv.org/abs/2509.09199)
*Wenhao Li,Bangcheng Sun,Weihao Ye,Tianyi Zhang,Daohai Yu,Fei Chao,Rongrong Ji*

Main category: cs.CL

TL;DR: This paper introduces a novel framework, called Context Compression Framework (CCF), to model long-context language efficiently by compressing data while preserving semantics, significantly improving performance and memory efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing methods for extending language models to longer contexts are resource-intensive, which impedes effective training and inference.

Method: The authors propose CCF, a method that creates compact representations through segment-wise semantic aggregation and key-value memory encoding, along with a training optimization strategy that uses incremental segment decoding and sparse reservoir sampling.

Result: Empirical studies show that CCF achieves competitive perplexity even with high compression ratios, and enhances both computational throughput and memory efficiency on long-context benchmarks.

Conclusion: Structured compression has great potential for scalable and effective long-context language modeling, as demonstrated by the efficiency and performance of CCF.

Abstract: Scaling language models to longer contexts is essential for capturing rich
dependencies across extended discourse. However, na\"ive context extension
imposes significant computational and memory burdens, often resulting in
inefficiencies during both training and inference. In this work, we propose
CCF, a novel context compression framework designed to enable efficient
long-context modeling by learning hierarchical latent representations that
preserve global semantics while aggressively reducing input redundancy. CCF
integrates segment-wise semantic aggregation with key-value memory encoding,
forming compact representations that support accurate reconstruction and
long-range understanding. To further enhance scalability, we introduce a
training-efficient optimization strategy that couples incremental segment
decoding with sparse reservoir sampling, substantially reducing memory overhead
without degrading performance. Empirical results on multiple long-context
language modeling benchmarks demonstrate that CCF achieves competitive
perplexity under high compression ratios, and significantly improves throughput
and memory efficiency compared to existing approaches. These findings highlight
the potential of structured compression for scalable and effective long-context
language modeling.

</details>


### [47] [Reading Between the Lines: Classifying Resume Seniority with Large Language Models](https://arxiv.org/abs/2509.09229)
*Matan Cohen,Shira Shani,Eden Menahem,Yehudit Aperstein,Alexander Apartsin*

Main category: cs.CL

TL;DR: The paper explores using large language models (LLMs) for automating seniority classification in resumes, leveraging real and synthetic data to address challenges like exaggerated qualifications.


<details>
  <summary>Details</summary>
Motivation: Accurately determining candidate seniority in resumes is challenging due to overstated qualifications and ambiguous self-presentation. The study aims to address these challenges using LLMs.

Method: The researchers used fine-tuned BERT architectures and introduced a hybrid dataset composed of real-world resumes and synthetic examples. They focused on detecting linguistic cues related to implicit expertise and exaggerated qualifications.

Result: LLMs demonstrated effectiveness in identifying subtle linguistic cues, which could improve AI-driven candidate evaluation systems by mitigating biases.

Conclusion: The study shows that LLMs can be effective for automating seniority classification, opening avenues for improved AI-based hiring systems while alleviating biases associated with self-promotional resume tactics.

Abstract: Accurately assessing candidate seniority from resumes is a critical yet
challenging task, complicated by the prevalence of overstated experience and
ambiguous self-presentation. In this study, we investigate the effectiveness of
large language models (LLMs), including fine-tuned BERT architectures, for
automating seniority classification in resumes. To rigorously evaluate model
performance, we introduce a hybrid dataset comprising both real-world resumes
and synthetically generated hard examples designed to simulate exaggerated
qualifications and understated seniority. Using the dataset, we evaluate the
performance of Large Language Models in detecting subtle linguistic cues
associated with seniority inflation and implicit expertise. Our findings
highlight promising directions for enhancing AI-driven candidate evaluation
systems and mitigating bias introduced by self-promotional language. The
dataset is available for the research community at https://bit.ly/4mcTovt

</details>


### [48] [Agentic LLMs for Question Answering over Tabular Data](https://arxiv.org/abs/2509.09234)
*Rishit Tyagi,Mohit Gupta,Rahul Bouri*

Main category: cs.CL

TL;DR: This paper focuses on enhancing Question Answering (QA) over tabular data using a Natural Language to SQL (NL-to-SQL) approach with large language models, achieving significant accuracy improvements over baselines in the SemEval 2025 Task 8 (DataBench).


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the need to handle the unique challenges posed by diverse structures, sizes, and data types in real-world tables for table-based question answering.

Method: The paper proposes a multi-stage NL-to-SQL approach using large language models (LLMs) like GPT-4o, GPT-4o-mini, and DeepSeek v2:16b. The system involves steps such as example selection, SQL query generation, answer extraction, verification, and iterative refinement.

Result: The proposed method achieves 70.5% accuracy on DataBench QA and 71.6% on DataBench Lite QA, which is a significant improvement over baseline scores of 26% and 27%.

Conclusion: The methodology demonstrates the potential of LLMs for Table QA tasks, though the paper also discusses the system's strengths and limitations for further refinement.

Abstract: Question Answering over Tabular Data (Table QA) presents unique challenges
due to the diverse structure, size, and data types of real-world tables. The
SemEval 2025 Task 8 (DataBench) introduced a benchmark composed of large-scale,
domain-diverse datasets to evaluate the ability of models to accurately answer
structured queries. We propose a Natural Language to SQL (NL-to-SQL) approach
leveraging large language models (LLMs) such as GPT-4o, GPT-4o-mini, and
DeepSeek v2:16b to generate SQL queries dynamically. Our system follows a
multi-stage pipeline involving example selection, SQL query generation, answer
extraction, verification, and iterative refinement. Experiments demonstrate the
effectiveness of our approach, achieving 70.5\% accuracy on DataBench QA and
71.6\% on DataBench Lite QA, significantly surpassing baseline scores of 26\%
and 27\% respectively. This paper details our methodology, experimental
results, and alternative approaches, providing insights into the strengths and
limitations of LLM-driven Table QA.

</details>


### [49] [From scratch to silver: Creating trustworthy training data for patent-SDG classification using Large Language Models](https://arxiv.org/abs/2509.09303)
*Grazia Sveva Ascione,Nicolò Tamagnone*

Main category: cs.CL

TL;DR: The paper introduces a scalable, weakly supervised method for classifying patents based on their relevance to the UN Sustainable Development Goals (SDGs) by employing large language models and patent ontologies.


<details>
  <summary>Details</summary>
Motivation: Tracking how innovations align with the UN SDGs is vital for addressing global challenges, but the lack of scalable, labeled datasets limits current classification approaches.

Method: The study uses weak supervision, leveraging patent citations to SDG-tagged publications as a noisy signal. Large language models extract structured concepts from patents based on a custom ontology, and a rank-based retrieval approach refines signal sparsity and noise.

Result: The approach generates a silver-standard multi-label dataset and outperforms baselines in internal validation metrics. Additionally, it demonstrates better thematic, cognitive, and organizational coherence than traditional classifications in external validation.

Conclusion: Weak supervision with semantic alignment can significantly improve scalable patent-to-SDG classification, enhancing innovation tracking for global challenges.

Abstract: Classifying patents by their relevance to the UN Sustainable Development
Goals (SDGs) is crucial for tracking how innovation addresses global
challenges. However, the absence of a large, labeled dataset limits the use of
supervised learning. Existing methods, such as keyword searches, transfer
learning, and citation-based heuristics, lack scalability and generalizability.
This paper frames patent-to-SDG classification as a weak supervision problem,
using citations from patents to SDG-tagged scientific publications (NPL
citations) as a noisy initial signal. To address its sparsity and noise, we
develop a composite labeling function (LF) that uses large language models
(LLMs) to extract structured concepts, namely functions, solutions, and
applications, from patents and SDG papers based on a patent ontology.
Cross-domain similarity scores are computed and combined using a rank-based
retrieval approach. The LF is calibrated via a custom positive-only loss that
aligns with known NPL-SDG links without penalizing discovery of new SDG
associations. The result is a silver-standard, soft multi-label dataset mapping
patents to SDGs, enabling the training of effective multi-label regression
models. We validate our approach through two complementary strategies: (1)
internal validation against held-out NPL-based labels, where our method
outperforms several baselines including transformer-based models, and zero-shot
LLM; and (2) external validation using network modularity in patent citation,
co-inventor, and co-applicant graphs, where our labels reveal greater thematic,
cognitive, and organizational coherence than traditional technological
classifications. These results show that weak supervision and semantic
alignment can enhance SDG classification at scale.

</details>


### [50] [MetaRAG: Metamorphic Testing for Hallucination Detection in RAG Systems](https://arxiv.org/abs/2509.09360)
*Channdeth Sok,David Luz,Yacine Haddam*

Main category: cs.CL

TL;DR: The paper introduces MetaRAG, a framework for detecting hallucinations in Retrieval-Augmented Generation (RAG) systems, using a method that doesn't require ground-truth or model internals and that flags unreliable information localized in identified content spans.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of hallucinations in Retrieval-Augmented Generation (RAG) systems—a challenge not effectively tackled by existing detection methods—and to ensure responses are consistent with retrieved evidence in high-stakes or proprietary domains.

Method: MetaRAG employs a four-step process: (1) decompose responses into individual factoids, (2) mutate these factoids using synonyms and antonyms, (3) verify the mutated variants against the retrieved context for consistency, and (4) aggregate penalties into a hallucination score, highlighting unsupported factoids.

Result: MetaRAG demonstrated effectiveness on a proprietary enterprise dataset, showing the capability to detect hallucinations and support trustworthy deployment of RAG-based systems.

Conclusion: MetaRAG provides an innovative and practical solution for hallucination detection in RAG systems that can support identity-sensitive queries. It enables localized flagging of unsupported claims and introduces safeguards for practical deployment, though some areas, like its topic-based deployment design, require further evaluation.

Abstract: Large Language Models (LLMs) are increasingly deployed in enterprise
applications, yet their reliability remains limited by hallucinations, i.e.,
confident but factually incorrect information. Existing detection approaches,
such as SelfCheckGPT and MetaQA, primarily target standalone LLMs and do not
address the unique challenges of Retrieval-Augmented Generation (RAG) systems,
where responses must be consistent with retrieved evidence. We therefore
present MetaRAG, a metamorphic testing framework for hallucination detection in
Retrieval-Augmented Generation (RAG) systems. MetaRAG operates in a real-time,
unsupervised, black-box setting, requiring neither ground-truth references nor
access to model internals, making it suitable for proprietary and high-stakes
domains. The framework proceeds in four stages: (1) decompose answers into
atomic factoids, (2) generate controlled mutations of each factoid using
synonym and antonym substitutions, (3) verify each variant against the
retrieved context (synonyms are expected to be entailed and antonyms
contradicted), and (4) aggregate penalties for inconsistencies into a
response-level hallucination score. Crucially for identity-aware AI, MetaRAG
localizes unsupported claims at the factoid span where they occur (e.g.,
pregnancy-specific precautions, LGBTQ+ refugee rights, or labor eligibility),
allowing users to see flagged spans and enabling system designers to configure
thresholds and guardrails for identity-sensitive queries. Experiments on a
proprietary enterprise dataset illustrate the effectiveness of MetaRAG for
detecting hallucinations and enabling trustworthy deployment of RAG-based
conversational agents. We also outline a topic-based deployment design that
translates MetaRAG's span-level scores into identity-aware safeguards; this
design is discussed but not evaluated in our experiments.

</details>


### [51] [Modelling Analogies and Analogical Reasoning: Connecting Cognitive Science Theory and NLP Research](https://arxiv.org/abs/2509.09381)
*Molly R Petersen,Claire E Stevenson,Lonneke van der Plas*

Main category: cs.CL

TL;DR: This paper explores analogical reasoning's cognitive science theories and their relevance to NLP challenges.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between cognitive science theories of analogy and their application in NLP, highlighting the underexplored cognitive dimensions in NLP approaches.

Method: The authors relate cognitive science concepts of analogical reasoning to NLP research and discuss their implications for challenges beyond analogy-solving in NLP.

Result: The paper identifies how analogical reasoning concepts align with NLP challenges and emphasizes the importance of relational understanding over entity-level similarity.

Conclusion: Integrating cognitive science insights on analogy can improve relational understanding in NLP, pushing the field beyond surface-level similarity.

Abstract: Analogical reasoning is an essential aspect of human cognition. In this
paper, we summarize key theory about the processes underlying analogical
reasoning from the cognitive science literature and relate it to current
research in natural language processing. While these processes can be easily
linked to concepts in NLP, they are generally not viewed through a cognitive
lens. Furthermore, we show how these notions are relevant for several major
challenges in NLP research, not directly related to analogy solving. This may
guide researchers to better optimize relational understanding in text, as
opposed to relying heavily on entity-level similarity.

</details>


### [52] [Hierarchical Bracketing Encodings Work for Dependency Graphs](https://arxiv.org/abs/2509.09388)
*Ana Ezquerro,Carlos Gómez-Rodríguez,David Vilares*

Main category: cs.CL

TL;DR: The paper revisits hierarchical bracketing encodings, applying them to dependency graph parsing, achieving competitive and improved accuracy.


<details>
  <summary>Details</summary>
Motivation: To provide an efficient and effective method for representing complex graph structures like dependency graphs with reduced label space and preserved structural information.

Method: Hierarchical bracketing encodings are employed to linearize graphs as sequences, which supports linear-time parsing and accommodates complex features like reentrancies, cycles, and empty nodes.

Result: The approach was evaluated on multilingual and multi-formalism benchmarks, demonstrating competitive performance and consistent accuracy improvements compared to other methods.

Conclusion: This method achieves efficient parsing with reduced complexity, while maintaining or improving accuracy in graph representation and interpretation.

Abstract: We revisit hierarchical bracketing encodings from a practical perspective in
the context of dependency graph parsing. The approach encodes graphs as
sequences, enabling linear-time parsing with $n$ tagging actions, and still
representing reentrancies, cycles, and empty nodes. Compared to existing graph
linearizations, this representation substantially reduces the label space while
preserving structural information. We evaluate it on a multilingual and
multi-formalism benchmark, showing competitive results and consistent
improvements over other methods in exact match accuracy.

</details>


### [53] [GrACE: A Generative Approach to Better Confidence Elicitation in Large Language Models](https://arxiv.org/abs/2509.09438)
*Zhaohan Zhang,Ziquan Liu,Ioannis Patras*

Main category: cs.CL

TL;DR: The paper introduces GrACE, a scalable and reliable method for confidence elicitation in LLMs, which surpasses existing approaches in performance without requiring additional computational resources.


<details>
  <summary>Details</summary>
Motivation: Current methods for confidence elicitation in LLMs are either computationally expensive or poorly calibrated, limiting practical deployment in critical fields like healthcare and finance.

Method: The paper proposes GrACE, which utilizes a novel mechanism involving the similarity between the last hidden state and a special token's embedding. The model is fine-tuned with calibration targets to improve accuracy.

Result: Experiments demonstrate that GrACE outperforms six alternative methods in both discriminative capacity and calibration across three LLMs and two datasets. Additionally, it enhances accuracy and reduces the sample requirement for test-time scaling.

Conclusion: GrACE is presented as a practical and effective solution for scalable, reliable, and real-time confidence estimation in LLMs, making it suitable for high-stakes applications.

Abstract: Assessing the reliability of Large Language Models (LLMs) by confidence
elicitation is a prominent approach to AI safety in high-stakes applications,
such as healthcare and finance. Existing methods either require expensive
computational overhead or suffer from poor calibration, making them impractical
and unreliable for real-world deployment. In this work, we propose GrACE, a
Generative Approach to Confidence Elicitation that enables scalable and
reliable confidence elicitation for LLMs. GrACE adopts a novel mechanism in
which the model expresses confidence by the similarity between the last hidden
state and the embedding of a special token appended to the vocabulary, in
real-time. We fine-tune the model for calibrating the confidence with
calibration targets associated with accuracy. Experiments with three LLMs and
two benchmark datasets show that the confidence produced by GrACE achieves the
best discriminative capacity and calibration on open-ended generation tasks,
outperforming six competing methods without resorting to additional sampling or
an auxiliary model. Moreover, we propose two strategies for improving test-time
scaling based on confidence induced by GrACE. Experimental results show that
using GrACE not only improves the accuracy of the final decision but also
significantly reduces the number of required samples in the test-time scaling
scheme, indicating the potential of GrACE as a practical solution for deploying
LLMs with scalable, reliable, and real-time confidence estimation.

</details>


### [54] [Mitigating Language Barriers in Education: Developing Multilingual Digital Learning Materials with Machine Translation](https://arxiv.org/abs/2509.09473)
*Lucie Poláková,Martin Popel,Věra Kloudová,Michal Novák,Mariia Anisimova,Jiří Balhar*

Main category: cs.CL

TL;DR: The EdUKate project develops multilingual learning materials by integrating machine translation and linguistic studies to support non-Czech-speaking students in Czech schools.


<details>
  <summary>Details</summary>
Motivation: The project aims to address the educational needs of non-Czech-speaking students in Czech primary and secondary schools, fostering inclusivity through multilingual content.

Method: The project involves the creation of a machine translation system tailored for the educational domain, specifically translating Czech content into Ukrainian, English, and German, while handling specialized formats and terminology.

Result: Up to 9,000 interactive exercises have been translated and made available online for students and educators. The machine translation system was developed based on teachers’ input and evaluated in practice.

Conclusion: The EdUKate project contributes to accessible education by providing free multilingual resources, ensuring better support for non-Czech-speaking students and benefiting educators and researchers.

Abstract: The EdUKate project combines digital education, linguistics, translation
studies, and machine translation to develop multilingual learning materials for
Czech primary and secondary schools. Launched through collaboration between a
major Czech academic institution and the country's largest educational
publisher, the project is aimed at translating up to 9,000 multimodal
interactive exercises from Czech into Ukrainian, English, and German for an
educational web portal. It emphasizes the development and evaluation of a
direct Czech-Ukrainian machine translation system tailored to the educational
domain, with special attention to processing formatted content such as XML and
PDF and handling technical and scientific terminology. We present findings from
an initial survey of Czech teachers regarding the needs of non-Czech-speaking
students and describe the system's evaluation and implementation on the web
portal. All resulting applications are freely available to students, educators,
and researchers.

</details>


### [55] [Towards Explainable Job Title Matching: Leveraging Semantic Textual Relatedness and Knowledge Graphs](https://arxiv.org/abs/2509.09522)
*Vadim Zadykian,Bruno Andrade,Haithem Afli*

Main category: cs.CL

TL;DR: The paper introduces a self-supervised hybrid model combining sentence embeddings and Knowledge Graphs for improved performance in job title matching tasks.


<details>
  <summary>Details</summary>
Motivation: The study addresses the challenge of accurately matching job titles in resume recommendation systems, where lexical overlap is often minimal or misleading.

Method: The authors developed a hybrid architecture that integrates dense sentence embeddings with domain-specific Knowledge Graphs, evaluated using a stratified model based on low, medium, and high STR regions.

Result: The hybrid approach, especially SBERT models with integrated Knowledge Graphs, reduced RMSE by 25% in the high-STR region compared to strong baselines.

Conclusion: Combining Knowledge Graphs with text embeddings enhances explainability and performance in job title matching systems, while granular evaluation facilitates better model analysis and selection for HR applications.

Abstract: Semantic Textual Relatedness (STR) captures nuanced relationships between
texts that extend beyond superficial lexical similarity. In this study, we
investigate STR in the context of job title matching - a key challenge in
resume recommendation systems, where overlapping terms are often limited or
misleading. We introduce a self-supervised hybrid architecture that combines
dense sentence embeddings with domain-specific Knowledge Graphs (KGs) to
improve both semantic alignment and explainability. Unlike previous work that
evaluated models on aggregate performance, our approach emphasizes data
stratification by partitioning the STR score continuum into distinct regions:
low, medium, and high semantic relatedness. This stratified evaluation enables
a fine-grained analysis of model performance across semantically meaningful
subspaces. We evaluate several embedding models, both with and without KG
integration via graph neural networks. The results show that fine-tuned SBERT
models augmented with KGs produce consistent improvements in the high-STR
region, where the RMSE is reduced by 25% over strong baselines. Our findings
highlight not only the benefits of combining KGs with text embeddings, but also
the importance of regional performance analysis in understanding model
behavior. This granular approach reveals strengths and weaknesses hidden by
global metrics, and supports more targeted model selection for use in Human
Resources (HR) systems and applications where fairness, explainability, and
contextual matching are essential.

</details>


### [56] [DeMeVa at LeWiDi-2025: Modeling Perspectives with In-Context Learning and Label Distribution Learning](https://arxiv.org/abs/2509.09524)
*Daniil Ignatev,Nan Li,Hugh Mee Wong,Anh Dang,Shane Kaszefski Yaschuk*

Main category: cs.CL

TL;DR: The paper investigates approaches using in-context learning (ICL) and label distribution learning (LDL) for annotator-specific annotations in the LeWiDi 2025 task.


<details>
  <summary>Details</summary>
Motivation: The study aims to explore methods for handling disagreements in annotations, focusing on predicting perspectivist annotations and aggregating them into competitive soft labels.

Method: The authors compare ICL strategies utilizing large language models and evaluate various fine-tuning approaches with RoBERTa for LDL.

Result: ICL successfully predicts annotator-specific annotations and soft labels, while LDL methods show potential in soft label predictions, indicating they deserve more attention.

Conclusion: ICL approaches are effective for perspectivist tasks, and LDL methods hold promise for future research in soft label prediction.

Abstract: This system paper presents the DeMeVa team's approaches to the third edition
of the Learning with Disagreements shared task (LeWiDi 2025; Leonardelli et
al., 2025). We explore two directions: in-context learning (ICL) with large
language models, where we compare example sampling strategies; and label
distribution learning (LDL) methods with RoBERTa (Liu et al., 2019b), where we
evaluate several fine-tuning methods. Our contributions are twofold: (1) we
show that ICL can effectively predict annotator-specific annotations
(perspectivist annotations), and that aggregating these predictions into soft
labels yields competitive performance; and (2) we argue that LDL methods are
promising for soft label predictions and merit further exploration by the
perspectivist community.

</details>


### [57] [Prompting the Market? A Large-Scale Meta-Analysis of GenAI in Finance NLP (2022-2025)](https://arxiv.org/abs/2509.09544)
*Paolo Pedinotti,Peter Baumann,Nathan Jessurun,Leslie Barrett,Enrico Santus*

Main category: cs.CL

TL;DR: This paper introduces MetaGraph, a generalizable methodology using knowledge graphs and LLMs to analyze and structure the research landscape of financial NLP from 2022-2025, identifying key trends in its evolution.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to address the rapid evolution of financial NLP driven by LLMs, which has outpaced traditional survey methodologies. The authors aim for a structured and scalable approach to assess this transformation.

Method: The paper proposes MetaGraph, a methodology that involves an LLM-based pipeline to extract knowledge graphs from 681 scientific papers, coupled with an ontology for financial NLP. This system enables large-scale analysis and tracking of research trends.

Result: MetaGraph identified three phases of development in financial NLP: early adoption of LLMs and innovation in tasks/datasets, recognition of LLMs’ limitations, and the integration of complementary techniques into modular systems.

Conclusion: The work provides a clear understanding of financial NLP evolution, highlights trends, and suggests a reusable approach for mapping progress in other scientific domains.

Abstract: Large Language Models (LLMs) have rapidly reshaped financial NLP, enabling
new tasks and driving a proliferation of datasets and diversification of data
sources. Yet, this transformation has outpaced traditional surveys. In this
paper, we present MetaGraph, a generalizable methodology for extracting
knowledge graphs from scientific literature and analyzing them to obtain a
structured, queryable view of research trends. We define an ontology for
financial NLP research and apply an LLM-based extraction pipeline to 681 papers
(2022-2025), enabling large-scale, data-driven analysis. MetaGraph reveals
three key phases: early LLM adoption and task/dataset innovation; critical
reflection on LLM limitations; and growing integration of peripheral techniques
into modular systems. This structured view offers both practitioners and
researchers a clear understanding of how financial NLP has evolved -
highlighting emerging trends, shifting priorities, and methodological
shifts-while also demonstrating a reusable approach for mapping scientific
progress in other domains.

</details>


### [58] [Personality-Enhanced Social Recommendations in SAMI: Exploring the Role of Personality Detection in Matchmaking](https://arxiv.org/abs/2509.09583)
*Brittany Harbison,Samuel Taubman,Travis Taylor,Ashok. K. Goel*

Main category: cs.CL

TL;DR: The paper examines the integration of a personality detection model into an online course environment social connection tool (SAMI), using GPT for analyzing personality traits.


<details>
  <summary>Details</summary>
Motivation: Enhancing the effectiveness of social connections in online courses by addressing SAMI’s limitations in understanding student personalities.

Method: A personality detection model is developed using GPT’s zero-shot capability to infer Big-Five traits from forum posts. This is integrated into SAMI’s matchmaking system.

Result: The GPT-based model effectively detects personalities and complements SAMI’s existing matching factors, but further evaluation is required for full impact assessment.

Conclusion: Integrating personality-informed recommendations into SAMI shows promise in improving social connections for online learning, warranting further exploration.

Abstract: Social connection is a vital part of learning, yet online course environments
present barriers to the organic formation of social groups. SAMI offers one
solution by facilitating student connections, but its effectiveness is
constrained by an incomplete Theory of Mind, limiting its ability to create an
effective mental model of a student. One facet of this is its inability to
intuit personality, which may influence the relevance of its recommendations.
To explore this, we propose a personality detection model utilizing GPTs
zero-shot capability to infer Big-Five personality traits from forum
introduction posts, often encouraged in online courses. We benchmark its
performance against established models, demonstrating its efficacy in this
task. Furthermore, we integrate this model into SAMIs entity-based matchmaking
system, enabling personality-informed social recommendations. Initial
integration suggests personality traits can complement existing matching
factors, though additional evaluation is required to determine their full
impact on student engagement and match quality.

</details>


### [59] [Fluent but Unfeeling: The Emotional Blind Spots of Language Models](https://arxiv.org/abs/2509.09593)
*Bangzhao Shu,Isha Joshi,Melissa Karnaze,Anh C. Pham,Ishita Kakkar,Sindhu Kothe,Arpine Hovasapian,Mai ElSherief*

Main category: cs.CL

TL;DR: Large Language Models (LLMs) struggle with aligning with fine-grained human emotions, as demonstrated with the EXPRESS benchmark dataset containing 251 nuanced self-disclosed emotion labels.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the gap in mental health research where LLMs fail to evaluate or predict fine-grained human emotions beyond basic predefined categories.

Method: A benchmark dataset, EXPRESS, featuring 251 self-disclosed emotion labels from Reddit was created. LLMs were evaluated using an emotion decomposition framework based on theories, under various prompt settings.

Result: LLMs face challenges accurately predicting fine-grained emotions and fail to consistently capture contextual cues compared to human self-disclosures.

Conclusion: The study highlights the limitations of LLMs in fine-grained emotion analysis, offering insights to enhance their contextual understanding in future research.

Abstract: The versatility of Large Language Models (LLMs) in natural language
understanding has made them increasingly popular in mental health research.
While many studies explore LLMs' capabilities in emotion recognition, a
critical gap remains in evaluating whether LLMs align with human emotions at a
fine-grained level. Existing research typically focuses on classifying emotions
into predefined, limited categories, overlooking more nuanced expressions. To
address this gap, we introduce EXPRESS, a benchmark dataset curated from Reddit
communities featuring 251 fine-grained, self-disclosed emotion labels. Our
comprehensive evaluation framework examines predicted emotion terms and
decomposes them into eight basic emotions using established emotion theories,
enabling a fine-grained comparison. Systematic testing of prevalent LLMs under
various prompt settings reveals that accurately predicting emotions that align
with human self-disclosed emotions remains challenging. Qualitative analysis
further shows that while certain LLMs generate emotion terms consistent with
established emotion theories and definitions, they sometimes fail to capture
contextual cues as effectively as human self-disclosures. These findings
highlight the limitations of LLMs in fine-grained emotion alignment and offer
insights for future research aimed at enhancing their contextual understanding.

</details>


### [60] [LAVA: Language Model Assisted Verbal Autopsy for Cause-of-Death Determination](https://arxiv.org/abs/2509.09602)
*Yiqun T. Chen,Tyler H. McCormick,Li Liu,Abhirup Datta*

Main category: cs.CL

TL;DR: The study introduces LA-VA, a pipeline that uses Large Language Models (LLMs) for better cause-of-death predictions in verbal autopsies, achieving significant improvements over traditional methods.


<details>
  <summary>Details</summary>
Motivation: To enhance the accuracy of cause-of-death predictions in verbal autopsies, which are critical for health surveillance in low-resource settings.

Method: The paper employs GPT-5 predictions combined with traditional algorithmic approaches, embedding-based classification, and meta-learner ensembles to analyze data from the PHMRC dataset.

Result: GPT-5 achieved superior performance with average accuracies of 48.6% (Adult), 50.5% (Child), and 53.5% (Neonate), surpassing traditional baselines by 5-10%.

Conclusion: LLM-assisted approaches like LA-VA could significantly improve verbal autopsy accuracy, indicating potential for better health surveillance in resource-limited areas.

Abstract: Verbal autopsy (VA) is a critical tool for estimating causes of death in
resource-limited settings where medical certification is unavailable. This
study presents LA-VA, a proof-of-concept pipeline that combines Large Language
Models (LLMs) with traditional algorithmic approaches and embedding-based
classification for improved cause-of-death prediction. Using the Population
Health Metrics Research Consortium (PHMRC) dataset across three age categories
(Adult: 7,580; Child: 1,960; Neonate: 2,438), we evaluate multiple approaches:
GPT-5 predictions, LCVA baseline, text embeddings, and meta-learner ensembles.
Our results demonstrate that GPT-5 achieves the highest individual performance
with average test site accuracies of 48.6% (Adult), 50.5% (Child), and 53.5%
(Neonate), outperforming traditional statistical machine learning baselines by
5-10%. Our findings suggest that simple off-the-shelf LLM-assisted approaches
could substantially improve verbal autopsy accuracy, with important
implications for global health surveillance in low-resource settings.

</details>


### [61] [Bridging the Capability Gap: Joint Alignment Tuning for Harmonizing LLM-based Multi-Agent Systems](https://arxiv.org/abs/2509.09629)
*Minghang Zhu,Zhengliang Shi,Zhiwei Xu,Shiguang Wu,Lingjie Wang,Pengjie Ren,Zhaochun Ren,Zhumin Chen*

Main category: cs.CL

TL;DR: The paper introduces MOAT, a framework enhancing coordination in multi-agent systems for LLMs by aligning planning and grounding agents iteratively, achieving better results than existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based multi-agent systems suffer from poor coordination due to independent training of agents, leading to capability gaps.

Method: MOAT uses an iterative alignment framework: alternates between optimizing the planning agent for better subgoals and enhancing the grounding agent's generalization with varied subgoal-action pairs.

Result: MOAT shows notable performance improvement, gaining 3.1% on held-in tasks and 4.4% on held-out tasks across six benchmarks compared to the state-of-the-art methods.

Conclusion: MOAT successfully bridges the coordination gap in multi-agent systems through joint alignment, demonstrating both robust performance improvements and theoretical guarantees of convergence.

Abstract: The advancement of large language models (LLMs) has enabled the construction
of multi-agent systems to solve complex tasks by dividing responsibilities
among specialized agents, such as a planning agent for subgoal generation and a
grounding agent for executing tool-use actions. Most existing methods typically
fine-tune these agents independently, leading to capability gaps among them
with poor coordination. To address this, we propose MOAT, a Multi-Agent Joint
Alignment Tuning framework that improves agents collaboration through iterative
alignment. MOAT alternates between two key stages: (1) Planning Agent
Alignment, which optimizes the planning agent to generate subgoal sequences
that better guide the grounding agent; and (2) Grounding Agent Improving, which
fine-tunes the grounding agent using diverse subgoal-action pairs generated by
the agent itself to enhance its generalization capablity. Theoretical analysis
proves that MOAT ensures a non-decreasing and progressively convergent training
process. Experiments across six benchmarks demonstrate that MOAT outperforms
state-of-the-art baselines, achieving average improvements of 3.1% on held-in
tasks and 4.4% on held-out tasks.

</details>


### [62] [All for One: LLMs Solve Mental Math at the Last Token With Information Transferred From Other Tokens](https://arxiv.org/abs/2509.09650)
*Siddarth Mamidanna,Daking Rai,Ziyu Yao,Yilun Zhou*

Main category: cs.CL

TL;DR: This paper investigates the inner workings of Large Language Models (LLMs) during mental math tasks and identifies a specific subgraph responsible for meaningful computations.


<details>
  <summary>Details</summary>
Motivation: LLMs excel at various computational tasks, but their mechanisms are not fully understood. The study aims to explore how and where meaningful computations occur in LLMs during mental math tasks.

Method: The authors analyze mental math operations in LLMs using techniques like Context-Aware Mean Ablation (CAMA) and Attention-Based Peeking (ABP) to inhibit, restrict, and force computation in specific layers and tokens.

Result: They discover an All-for-One subgraph (AF1) where significant computations are concentrated in deeper layers and at the last token.

Conclusion: The identified AF1 subgraph is sufficient, necessary, and transferable for model performance on mental math tasks, offering insights into computational pathways within LLMs.

Abstract: Large language models (LLMs) demonstrate proficiency across numerous
computational tasks, yet their inner workings remain unclear. In theory, the
combination of causal self-attention and multilayer perceptron layers allows
every token to access and compute information based on all preceding tokens. In
practice, to what extent are such operations present? In this paper, on mental
math tasks (i.e., direct math calculation via next-token prediction without
explicit reasoning), we investigate this question in three steps: inhibiting
input-specific token computations in the initial layers, restricting the routes
of information transfer across token positions in the next few layers, and
forcing all computation to happen at the last token in the remaining layers.
With two proposed techniques, Context-Aware Mean Ablation (CAMA) and
Attention-Based Peeking (ABP), we identify an All-for-One subgraph (AF1) with
high accuracy on a wide variety of mental math tasks, where meaningful
computation occurs very late (in terms of layer depth) and only at the last
token, which receives information of other tokens in few specific middle
layers. Experiments on a variety of models and arithmetic expressions show that
this subgraph is sufficient and necessary for high model performance, transfers
across different models, and works on a variety of input styles. Ablations on
different CAMA and ABP alternatives reveal their unique advantages over other
methods, which may be of independent interest.

</details>


### [63] [Steering MoE LLMs via Expert (De)Activation](https://arxiv.org/abs/2509.09660)
*Mohsen Fayyaz,Ali Modarressi,Hanieh Deilamsalehy,Franck Dernoncourt,Ryan Rossi,Trung Bui,Hinrich Schütze,Nanyun Peng*

Main category: cs.CL

TL;DR: SteerMoE, a framework for directing Large Language Models using Mixture-of-Experts, controls behaviors like faithfulness and safety by detecting and steering specific experts without retraining.


<details>
  <summary>Details</summary>
Motivation: To address challenges in controlling aspects like faithfulness and safety in Large Language Models without retraining, using specialized routing via Mixture-of-Experts.

Method: Detect behavior-linked experts through analyzing activation patterns in contrasting input pairs, and steer these experts during inference by selectively activating or deactivating them.

Result: SteerMoE improved safety by up to +20% and faithfulness by +27% across 11 benchmarks and 6 models, while also exposing vulnerability to adversarial attacks and alignment faking.

Conclusion: SteerMoE presents a novel capability to control certain behaviors in LLMs, enhancing or compromising them, thus unveiling both benefits and risks of behavior-linked expert routing.

Abstract: Mixture-of-Experts (MoE) in Large Language Models (LLMs) routes each token
through a subset of specialized Feed-Forward Networks (FFN), known as experts.
We present SteerMoE, a framework for steering MoE models by detecting and
controlling behavior-linked experts. Our detection method identifies experts
with distinct activation patterns across paired inputs exhibiting contrasting
behaviors. By selectively (de)activating such experts during inference, we
control behaviors like faithfulness and safety without retraining or modifying
weights. Across 11 benchmarks and 6 LLMs, our steering raises safety by up to
+20% and faithfulness by +27%. In adversarial attack mode, it drops safety by
-41% alone, and -100% when combined with existing jailbreak methods, bypassing
all safety guardrails and exposing a new dimension of alignment faking hidden
within experts.

</details>


### [64] [CDE: Curiosity-Driven Exploration for Efficient Reinforcement Learning in Large Language Models](https://arxiv.org/abs/2509.09675)
*Runpeng Dai,Linfeng Song,Haolin Liu,Zhenwen Liang,Dian Yu,Haitao Mi,Zhaopeng Tu,Rui Liu,Tong Zheng,Hongtu Zhu,Dong Yu*

Main category: cs.CL

TL;DR: The authors propose Curiosity-Driven Exploration (CDE), which enhances exploration in RLVR by leveraging curiosity signals from both the actor and critic, resulting in improved model performance.


<details>
  <summary>Details</summary>
Motivation: Reinforcement Learning with Verifiable Rewards (RLVR) is effective but suffers from poor exploration, leading to premature convergence and performance issues.

Method: The proposed method introduces curiosity signals for exploration, derived from actor perplexity in generated responses and the critic's variance of value estimates from a multi-head architecture, used as an exploration bonus in RLVR.

Result: The method achieves an approximate +3 point improvement on AIME benchmarks with GRPO/PPO compared to standard RLVR. It also identifies a calibration collapse as a failure mode in language models.

Conclusion: Curiosity-Driven Exploration enhances RLVR's exploration efficiency, improves model accuracy, and sheds light on common failure mechanisms, making it a significant advancement for LLM-based reasoning.

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) is a powerful paradigm
for enhancing the reasoning ability of Large Language Models (LLMs). Yet
current RLVR methods often explore poorly, leading to premature convergence and
entropy collapse. To address this challenge, we introduce Curiosity-Driven
Exploration (CDE), a framework that leverages the model's own intrinsic sense
of curiosity to guide exploration. We formalize curiosity with signals from
both the actor and the critic: for the actor, we use perplexity over its
generated response, and for the critic, we use the variance of value estimates
from a multi-head architecture. Both signals serve as an exploration bonus
within the RLVR framework to guide the model. Our theoretical analysis shows
that the actor-wise bonus inherently penalizes overconfident errors and
promotes diversity among correct responses; moreover, we connect the
critic-wise bonus to the well-established count-based exploration bonus in RL.
Empirically, our method achieves an approximate +3 point improvement over
standard RLVR using GRPO/PPO on AIME benchmarks. Further analysis identifies a
calibration collapse mechanism within RLVR, shedding light on common LLM
failure modes.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [65] [Recurrence Meets Transformers for Universal Multimodal Retrieval](https://arxiv.org/abs/2509.08897)
*Davide Caffagni,Sara Sarto,Marcella Cornia,Lorenzo Baraldi,Rita Cucchiara*

Main category: cs.CV

TL;DR: The paper introduces a unified multimodal retrieval model, ReT-2, which supports image and text queries, surpassing prior benchmarks in performance, inference speed, and efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing vision-language models, which are restricted to single-modality queries and task-specific fine-tuning, and to support more complex, multimodal retrieval tasks.

Method: ReT-2 uses multi-layer representations combined with a recurrent Transformer architecture and LSTM-inspired gating mechanisms to integrate visual and textual details across modalities and layers.

Result: The model achieves state-of-the-art performance on M2KR and M-BEIR benchmarks, improves downstream tasks in retrieval-augmented pipelines like Encyclopedic-VQA, and reduces inference time and memory usage.

Conclusion: ReT-2 is an efficient and effective multimodal retrieval system that advances the field and is publicly available for use.

Abstract: With the rapid advancement of multimodal retrieval and its application in
LLMs and multimodal LLMs, increasingly complex retrieval tasks have emerged.
Existing methods predominantly rely on task-specific fine-tuning of
vision-language models and are limited to single-modality queries or documents.
In this paper, we propose ReT-2, a unified retrieval model that supports
multimodal queries, composed of both images and text, and searches across
multimodal document collections where text and images coexist. ReT-2 leverages
multi-layer representations and a recurrent Transformer architecture with
LSTM-inspired gating mechanisms to dynamically integrate information across
layers and modalities, capturing fine-grained visual and textual details. We
evaluate ReT-2 on the challenging M2KR and M-BEIR benchmarks across different
retrieval configurations. Results demonstrate that ReT-2 consistently achieves
state-of-the-art performance across diverse settings, while offering faster
inference and reduced memory usage compared to prior approaches. When
integrated into retrieval-augmented generation pipelines, ReT-2 also improves
downstream performance on Encyclopedic-VQA and InfoSeek datasets. Our source
code and trained models are publicly available at:
https://github.com/aimagelab/ReT-2

</details>


### [66] [Diffusion-Based Action Recognition Generalizes to Untrained Domains](https://arxiv.org/abs/2509.08908)
*Rogerio Guimaraes,Frank Xiao,Pietro Perona,Markus Marks*

Main category: cs.CV

TL;DR: The paper introduces Vision Diffusion Model (VDM) features, aggregated with a transformer, for human-like action recognition across challenging contexts, achieving state-of-the-art benchmarks.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of current deep learning models in generalizing action recognition across diverse viewing angles, species differences, and recording contexts.

Method: The proposed method uses features generated by a Vision Diffusion Model (VDM) conditioned on early diffusion timesteps, aggregated via a transformer to emphasize semantic over pixel-level details.

Result: Experiments demonstrate enhanced generalization in classifying actions across three benchmarks: animal species, viewing angles, and recording contexts, achieving new state-of-the-art performance.

Conclusion: The approach improves machine action recognition robustness, bringing it closer to human-like generalization capabilities.

Abstract: Humans can recognize the same actions despite large context and viewpoint
variations, such as differences between species (walking in spiders vs.
horses), viewpoints (egocentric vs. third-person), and contexts (real life vs
movies). Current deep learning models struggle with such generalization. We
propose using features generated by a Vision Diffusion Model (VDM), aggregated
via a transformer, to achieve human-like action recognition across these
challenging conditions. We find that generalization is enhanced by the use of a
model conditioned on earlier timesteps of the diffusion process to highlight
semantic information over pixel level details in the extracted features. We
experimentally explore the generalization properties of our approach in
classifying actions across animal species, across different viewing angles, and
different recording contexts. Our model sets a new state-of-the-art across all
three generalization benchmarks, bringing machine action recognition closer to
human-like robustness. Project page:
$\href{https://www.vision.caltech.edu/actiondiff/}{\texttt{vision.caltech.edu/actiondiff}}$
Code:
$\href{https://github.com/frankyaoxiao/ActionDiff}{\texttt{github.com/frankyaoxiao/ActionDiff}}$

</details>


### [67] [PromptGuard: An Orchestrated Prompting Framework for Principled Synthetic Text Generation for Vulnerable Populations using LLMs with Enhanced Safety, Fairness, and Controllability](https://arxiv.org/abs/2509.08910)
*Tung Vu,Lam Nguyen,Quynh Dao*

Main category: cs.CV

TL;DR: The paper introduces PromptGuard, a prompting framework aimed at reducing harmful outputs from Large Language Models (LLMs) by applying the VulnGuard Prompt, a data-driven and ethical method for population-specific harm prevention.


<details>
  <summary>Details</summary>
Motivation: LLMs risk generating harmful, biased, or misleading information affecting vulnerable groups such as LGBTQ+ individuals, single parents, and marginalized communities, and existing safety measures fail to address these risks at the generation source.

Method: The approach involves PromptGuard, a modular framework utilizing VulnGuard Prompt with contrastive learning from curated GitHub data, ethical reasoning, and adaptive role-prompting. It employs six key modules for harm prevention and provides theoretical mathematical proofs on its mechanisms.

Result: The paper demonstrates 25-30% reduction in analytical harm with multi-objective optimization, entropy bounds, and Pareto optimality, supported by GitHub-sourced datasets and theoretical validation.

Conclusion: PromptGuard offers a mathematically grounded, proactive approach to systematically prevent harmful outputs in LLMs, marking an advancement over post-hoc and generic alignment safety techniques.

Abstract: The proliferation of Large Language Models (LLMs) in real-world applications
poses unprecedented risks of generating harmful, biased, or misleading
information to vulnerable populations including LGBTQ+ individuals, single
parents, and marginalized communities. While existing safety approaches rely on
post-hoc filtering or generic alignment techniques, they fail to proactively
prevent harmful outputs at the generation source. This paper introduces
PromptGuard, a novel modular prompting framework with our breakthrough
contribution: VulnGuard Prompt, a hybrid technique that prevents harmful
information generation using real-world data-driven contrastive learning.
VulnGuard integrates few-shot examples from curated GitHub repositories,
ethical chain-of-thought reasoning, and adaptive role-prompting to create
population-specific protective barriers. Our framework employs theoretical
multi-objective optimization with formal proofs demonstrating 25-30% analytical
harm reduction through entropy bounds and Pareto optimality. PromptGuard
orchestrates six core modules: Input Classification, VulnGuard Prompting,
Ethical Principles Integration, External Tool Interaction, Output Validation,
and User-System Interaction, creating an intelligent expert system for
real-time harm prevention. We provide comprehensive mathematical formalization
including convergence proofs, vulnerability analysis using information theory,
and theoretical validation framework using GitHub-sourced datasets,
establishing mathematical foundations for systematic empirical research.

</details>


### [68] [Similarity-based Outlier Detection for Noisy Object Re-Identification Using Beta Mixtures](https://arxiv.org/abs/2509.08926)
*Waqar Ahmad,Evan Murphy,Vladimir A. Krylov*

Main category: cs.CV

TL;DR: The paper introduces Beta-SOD, a statistical outlier detection method improving object re-identification (Re-ID) in noisy label environments. It outperforms state-of-the-art methods on popular datasets.


<details>
  <summary>Details</summary>
Motivation: Re-ID methods struggle with label noise, leading to reduced performance. Addressing this challenge enhances the robustness of these systems.

Method: The authors designed a Siamese network with a Beta mixture outlier detection (Beta-SOD) technique. It models cosine similarity distributions and combines loss functions for improved optimization.

Result: The Beta-SOD approach demonstrated superior performance on person and vehicle Re-ID datasets, proving its resilience to high label noise levels (10-30%).

Conclusion: Beta-SOD offers an effective and robust solution for noisy Re-ID problems, presenting strong evaluations and providing open-source implementation for reproducibility.

Abstract: Object re-identification (Re-ID) methods are highly sensitive to label noise,
which typically leads to significant performance degradation. We address this
challenge by reframing Re-ID as a supervised image similarity task and adopting
a Siamese network architecture trained to capture discriminative pairwise
relationships. Central to our approach is a novel statistical outlier detection
(OD) framework, termed Beta-SOD (Beta mixture Similarity-based Outlier
Detection), which models the distribution of cosine similarities between
embedding pairs using a two-component Beta distribution mixture model. We
establish a novel identifiability result for mixtures of two Beta
distributions, ensuring that our learning task is well-posed.The proposed OD
step complements the Re-ID architecture combining binary cross-entropy,
contrastive, and cosine embedding losses that jointly optimize feature-level
similarity learning.We demonstrate the effectiveness of Beta-SOD in de-noising
and Re-ID tasks for person Re-ID, on CUHK03 and Market-1501 datasets, and
vehicle Re-ID, on VeRi-776 dataset. Our method shows superior performance
compared to the state-of-the-art methods across various noise levels (10-30\%),
demonstrating both robustness and broad applicability in noisy Re-ID scenarios.
The implementation of Beta-SOD is available at:
https://github.com/waqar3411/Beta-SOD

</details>


### [69] [SFD-Mamba2Net: Strcture-Guided Frequency-Enhanced Dual-Stream Mamba2 Network for Coronary Artery Segmentation](https://arxiv.org/abs/2509.08934)
*Nan Mu,Ruiqi Song,Zhihui Xu,Jingfeng Jiang,Chen Zhao*

Main category: cs.CV

TL;DR: The paper presents SFD-Mamba2Net, a novel method for coronary artery segmentation and stenosis detection in ICA images, demonstrating consistent outperformance of existing approaches.


<details>
  <summary>Details</summary>
Motivation: To address challenges of low contrast, high noise, and complex vascular structures in ICA images, improving clinical accuracy for coronary artery segmentation and stenosis detection.

Method: The authors developed SFD-Mamba2Net, incorporating a Curvature-Aware Structural Enhancement (CASE) module for vessel segmentation and a Progressive High-Frequency Perception (PHFP) module for refining high-frequency image details.

Result: SFD-Mamba2Net achieved superior performance across eight segmentation metrics and the highest rates in stenosis detection compared to state-of-the-art methods.

Conclusion: This end-to-end framework significantly enhances coronary artery segmentation and stenosis detection accuracy, making it promising for clinical applications.

Abstract: Background: Coronary Artery Disease (CAD) is one of the leading causes of
death worldwide. Invasive Coronary Angiography (ICA), regarded as the gold
standard for CAD diagnosis, necessitates precise vessel segmentation and
stenosis detection. However, ICA images are typically characterized by low
contrast, high noise levels, and complex, fine-grained vascular structures,
which pose significant challenges to the clinical adoption of existing
segmentation and detection methods. Objective: This study aims to improve the
accuracy of coronary artery segmentation and stenosis detection in ICA images
by integrating multi-scale structural priors, state-space-based long-range
dependency modeling, and frequency-domain detail enhancement strategies.
Methods: We propose SFD-Mamba2Net, an end-to-end framework tailored for
ICA-based vascular segmentation and stenosis detection. In the encoder, a
Curvature-Aware Structural Enhancement (CASE) module is embedded to leverage
multi-scale responses for highlighting slender tubular vascular structures,
suppressing background interference, and directing attention toward vascular
regions. In the decoder, we introduce a Progressive High-Frequency Perception
(PHFP) module that employs multi-level wavelet decomposition to progressively
refine high-frequency details while integrating low-frequency global
structures. Results and Conclusions: SFD-Mamba2Net consistently outperformed
state-of-the-art methods across eight segmentation metrics, and achieved the
highest true positive rate and positive predictive value in stenosis detection.

</details>


### [70] [Live(r) Die: Predicting Survival in Colorectal Liver Metastasis](https://arxiv.org/abs/2509.08935)
*Muhammad Alberb,Helen Cheung,Anne Martel*

Main category: cs.CV

TL;DR: The paper proposes an automated pipeline using MRI data to predict surgical outcomes for colorectal liver metastasis (CRLM) patients. It improves upon existing models using innovative segmentation and survival prediction techniques.


<details>
  <summary>Details</summary>
Motivation: To address the lack of accurate predictive models for surgical outcomes in CRLM patients, especially in multifocal cases, by leveraging automated, MRI-based methods.

Method: The paper introduces a framework that combines a segmentation pipeline using a novel 3D prompt propagation algorithm (SAMONAI) with a radiomics pipeline utilizing SurvAMINN, an autoencoder-based neural network for survival prediction.

Result: The proposed method outperforms existing clinical and genomic biomarkers, achieving more than a 10% improvement in predictive accuracy (C-index) on a dataset of 227 patients.

Conclusion: The study highlights the potential of integrating advanced segmentation and radiomics-based methods for efficient and interpretable outcome prediction in CRLM, showcasing significant strides in annotation efficiency and predictive performance.

Abstract: Colorectal cancer frequently metastasizes to the liver, significantly
reducing long-term survival. While surgical resection is the only potentially
curative treatment for colorectal liver metastasis (CRLM), patient outcomes
vary widely depending on tumor characteristics along with clinical and genomic
factors. Current prognostic models, often based on limited clinical or
molecular features, lack sufficient predictive power, especially in multifocal
CRLM cases. We present a fully automated framework for surgical outcome
prediction from pre- and post-contrast MRI acquired before surgery. Our
framework consists of a segmentation pipeline and a radiomics pipeline. The
segmentation pipeline learns to segment the liver, tumors, and spleen from
partially annotated data by leveraging promptable foundation models to complete
missing labels. Also, we propose SAMONAI, a novel zero-shot 3D prompt
propagation algorithm that leverages the Segment Anything Model to segment 3D
regions of interest from a single point prompt, significantly improving our
segmentation pipeline's accuracy and efficiency. The predicted pre- and
post-contrast segmentations are then fed into our radiomics pipeline, which
extracts features from each tumor and predicts survival using SurvAMINN, a
novel autoencoder-based multiple instance neural network for survival analysis.
SurvAMINN jointly learns dimensionality reduction and hazard prediction from
right-censored survival data, focusing on the most aggressive tumors. Extensive
evaluation on an institutional dataset comprising 227 patients demonstrates
that our framework surpasses existing clinical and genomic biomarkers,
delivering a C-index improvement exceeding 10%. Our results demonstrate the
potential of integrating automated segmentation algorithms and radiomics-based
survival analysis to deliver accurate, annotation-efficient, and interpretable
outcome prediction in CRLM.

</details>


### [71] [Discovering Divergent Representations between Text-to-Image Models](https://arxiv.org/abs/2509.08940)
*Lisa Dunlap,Joseph E. Gonzalez,Trevor Darrell,Fabian Caba Heilbron,Josef Sivic,Bryan Russell*

Main category: cs.CV

TL;DR: This paper investigates how visual representations differ between two generative models, identifying specific visual attributes and prompts that trigger these differences using the CompCon algorithm.


<details>
  <summary>Details</summary>
Motivation: To understand when and how visual attributes differ between outputs of generative models, improving interpretability and identifying biases.

Method: The paper introduces CompCon, an evolutionary search algorithm to identify divergent visual attributes and the prompts that trigger them between models. The algorithm is evaluated on the ID2 dataset and compared with LLM- and VLM-based approaches.

Result: The study identifies how specific text-to-image models differ in visual attributes, such as PixArt depicting loneliness with wet streets or Stable Diffusion 3.5 representing African Americans in media professions.

Conclusion: CompCon effectively discovers divergent visual concepts and highlights differences between generative models, contributing to better understanding and mitigating potential biases.

Abstract: In this paper, we investigate when and how visual representations learned by
two different generative models diverge. Given two text-to-image models, our
goal is to discover visual attributes that appear in images generated by one
model but not the other, along with the types of prompts that trigger these
attribute differences. For example, "flames" might appear in one model's
outputs when given prompts expressing strong emotions, while the other model
does not produce this attribute given the same prompts. We introduce CompCon
(Comparing Concepts), an evolutionary search algorithm that discovers visual
attributes more prevalent in one model's output than the other, and uncovers
the prompt concepts linked to these visual differences. To evaluate CompCon's
ability to find diverging representations, we create an automated data
generation pipeline to produce ID2, a dataset of 60 input-dependent
differences, and compare our approach to several LLM- and VLM-powered
baselines. Finally, we use CompCon to compare popular text-to-image models,
finding divergent representations such as how PixArt depicts prompts mentioning
loneliness with wet streets and Stable Diffusion 3.5 depicts African American
people in media professions. Code at: https://github.com/adobe-research/CompCon

</details>


### [72] [An U-Net-Based Deep Neural Network for Cloud Shadow and Sun-Glint Correction of Unmanned Aerial System (UAS) Imagery](https://arxiv.org/abs/2509.08949)
*Yibin Wang,Wondimagegn Beshah,Padmanava Dash,Haifeng Wang*

Main category: cs.CV

TL;DR: This paper addresses UAS imagery challenges caused by cloud shadows and sun glint with a U-Net deep learning model for image correction.


<details>
  <summary>Details</summary>
Motivation: The increasing use of UASs for remote sensing faces significant challenges in image quality due to cloud shadows and sun glint, particularly for applications like water quality estimation.

Method: The authors utilized a U-Net-based deep learning model to identify and rectify regions in UAS imagery affected by cloud shadows and sun glint. Pixel-level data was extracted and used for training, with optimal model settings determined via test evaluation metrics.

Result: The proposed model successfully identified and corrected cloud shadow and sun glint regions, leading to high-quality image recovery for remote sensing tasks.

Conclusion: The study demonstrates an effective machine learning approach for improving UAS imagery, enabling better utilization of such images for remote sensing applications.

Abstract: The use of unmanned aerial systems (UASs) has increased tremendously in the
current decade. They have significantly advanced remote sensing with the
capability to deploy and image the terrain as per required spatial, spectral,
temporal, and radiometric resolutions for various remote sensing applications.
One of the major advantages of UAS imagery is that images can be acquired in
cloudy conditions by flying the UAS under the clouds. The limitation to the
technology is that the imagery is often sullied by cloud shadows. Images taken
over water are additionally affected by sun glint. These are two pose serious
issues for estimating water quality parameters from the UAS images. This study
proposes a novel machine learning approach first to identify and extract
regions with cloud shadows and sun glint and separate such regions from
non-obstructed clear sky regions and sun-glint unaffected regions. The data was
extracted from the images at pixel level to train an U-Net based deep learning
model and best settings for model training was identified based on the various
evaluation metrics from test cases. Using this evaluation, a high-quality image
correction model was determined, which was used to recover the cloud shadow and
sun glint areas in the images.

</details>


### [73] [CoSwin: Convolution Enhanced Hierarchical Shifted Window Attention For Small-Scale Vision](https://arxiv.org/abs/2509.08959)
*Puskal Khadka,Rodrigue Rizk,Longwei Wang,KC Santosh*

Main category: cs.CV

TL;DR: This paper introduces CoSwin, a new vision transformer architecture that combines localized convolutional feature learning with global self-attention to improve performance on small datasets.


<details>
  <summary>Details</summary>
Motivation: The study aims to enhance vision transformers by addressing their weakness in capturing local features due to the lack of inductive biases like locality and translation equivariance, which are crucial for small datasets.

Method: The proposed CoSwin architecture integrates a learnable local feature enhancement module into each hierarchical shifted window attention block, enabling simultaneous extraction of local and global features.

Result: Experimentation on image classification benchmarks such as CIFAR-10, CIFAR-100, MNIST, SVHN, and Tiny ImageNet shows consistent performance improvements over state-of-the-art models, with gains of up to 4.92% on CIFAR-100 and 4.47% on Tiny ImageNet.

Conclusion: The results validate the effectiveness of local-global feature fusion in improving the performance, generalization, and robustness of vision transformers on small-scale vision tasks.

Abstract: Vision Transformers (ViTs) have achieved impressive results in computer
vision by leveraging self-attention to model long-range dependencies. However,
their emphasis on global context often comes at the expense of local feature
extraction in small datasets, particularly due to the lack of key inductive
biases such as locality and translation equivariance. To mitigate this, we
propose CoSwin, a novel feature-fusion architecture that augments the
hierarchical shifted window attention with localized convolutional feature
learning. Specifically, CoSwin integrates a learnable local feature enhancement
module into each attention block, enabling the model to simultaneously capture
fine-grained spatial details and global semantic structure. We evaluate CoSwin
on multiple image classification benchmarks including CIFAR-10, CIFAR-100,
MNIST, SVHN, and Tiny ImageNet. Our experimental results show consistent
performance gains over state-of-the-art convolutional and transformer-based
models. Notably, CoSwin achieves improvements of 2.17% on CIFAR-10, 4.92% on
CIFAR-100, 0.10% on MNIST, 0.26% on SVHN, and 4.47% on Tiny ImageNet over the
baseline Swin Transformer. These improvements underscore the effectiveness of
local-global feature fusion in enhancing the generalization and robustness of
transformers for small-scale vision. Code and pretrained weights available at
https://github.com/puskal-khadka/coswin

</details>


### [74] [iMatcher: Improve matching in point cloud registration via local-to-global geometric consistency learning](https://arxiv.org/abs/2509.08982)
*Karim Slimani,Catherine Achard,Brahim Tamadazte*

Main category: cs.CV

TL;DR: The paper introduces iMatcher, a differentiable framework for point cloud feature matching, emphasizing consistency and achieving state-of-the-art results in various datasets.


<details>
  <summary>Details</summary>
Motivation: Address the limitations in feature matching for point cloud registration by incorporating geometric consistency and robust learning techniques.

Method: iMatcher combines local graph embeddings, bilateral matching, and global consistency learning to refine matching probabilities using learned features.

Result: iMatcher significantly improves registration performance across diverse datasets, achieving high inlier ratios: 95%-97% on KITTI, 94%-97% on KITTI-360, and up to 81.1% on 3DMatch.

Conclusion: The approach demonstrates robustness and state-of-the-art performance in diverse real-world scenarios, making it valuable for point cloud registration tasks.

Abstract: This paper presents iMatcher, a fully differentiable framework for feature
matching in point cloud registration. The proposed method leverages learned
features to predict a geometrically consistent confidence matrix, incorporating
both local and global consistency. First, a local graph embedding module leads
to an initialization of the score matrix. A subsequent repositioning step
refines this matrix by considering bilateral source-to-target and
target-to-source matching via nearest neighbor search in 3D space. The paired
point features are then stacked together to be refined through global geometric
consistency learning to predict a point-wise matching probability. Extensive
experiments on real-world outdoor (KITTI, KITTI-360) and indoor (3DMatch)
datasets, as well as on 6-DoF pose estimation (TUD-L) and partial-to-partial
matching (MVP-RG), demonstrate that iMatcher significantly improves rigid
registration performance. The method achieves state-of-the-art inlier ratios,
scoring 95% - 97% on KITTI, 94% - 97% on KITTI-360, and up to 81.1% on 3DMatch,
highlighting its robustness across diverse settings.

</details>


### [75] [UltrON: Ultrasound Occupancy Networks](https://arxiv.org/abs/2509.08991)
*Magdalena Wysocki,Felix Duelmer,Ananya Bal,Nassir Navab,Mohammad Farid Azampour*

Main category: cs.CV

TL;DR: In free-hand ultrasound imaging, this paper introduces UltrON, an occupancy-based representation that uses acoustic features for improved 3D reconstruction of anatomical shapes while addressing challenges like occlusions, view dependency, and sparse annotations.


<details>
  <summary>Details</summary>
Motivation: Sonographers face challenges in integrating 2D ultrasound views into accurate 3D anatomical shapes due to issues like occlusions and sparse labeling, creating a need for more effective shape reconstruction techniques.

Method: The paper proposes UltrON, an occupancy-based representation that uses acoustic features from B-mode images without additional annotation. It introduces a novel loss function to compensate for view dependency and optimizes geometric consistency in a weakly-supervised manner.

Result: UltrON achieves improved geometric consistency, generalizes to shapes of the same anatomy, and overcomes limitations such as sparse labeling and occlusions.

Conclusion: UltrON effectively enhances 3D reconstruction accuracy in ultrasound imaging, suggesting a significant step toward better visualization, analysis, and clinical utility for sonographers.

Abstract: In free-hand ultrasound imaging, sonographers rely on expertise to mentally
integrate partial 2D views into 3D anatomical shapes. Shape reconstruction can
assist clinicians in this process. Central to this task is the choice of shape
representation, as it determines how accurately and efficiently the structure
can be visualized, analyzed, and interpreted. Implicit representations, such as
SDF and occupancy function, offer a powerful alternative to traditional voxel-
or mesh-based methods by modeling continuous, smooth surfaces with compact
storage, avoiding explicit discretization. Recent studies demonstrate that SDF
can be effectively optimized using annotations derived from segmented B-mode
ultrasound images. Yet, these approaches hinge on precise annotations,
overlooking the rich acoustic information embedded in B-mode intensity.
Moreover, implicit representation approaches struggle with the ultrasound's
view-dependent nature and acoustic shadowing artifacts, which impair
reconstruction. To address the problems resulting from occlusions and
annotation dependency, we propose an occupancy-based representation and
introduce \gls{UltrON} that leverages acoustic features to improve geometric
consistency in weakly-supervised optimization regime. We show that these
features can be obtained from B-mode images without additional annotation cost.
Moreover, we propose a novel loss function that compensates for view-dependency
in the B-mode images and facilitates occupancy optimization from multiview
ultrasound. By incorporating acoustic properties, \gls{UltrON} generalizes to
shapes of the same anatomy. We show that \gls{UltrON} mitigates the limitations
of occlusions and sparse labeling and paves the way for more accurate 3D
reconstruction. Code and dataset will be available at
https://github.com/magdalena-wysocki/ultron.

</details>


### [76] [Implicit Neural Representations of Intramyocardial Motion and Strain](https://arxiv.org/abs/2509.09004)
*Andrew Bell,Yan Kit Choi,Steffen Peterson,Andrew King,Muhummad Sohaib Nazir,Alistair Young*

Main category: cs.CV

TL;DR: The paper introduces an implicit neural representations (INRs) method for analyzing left ventricular displacement in cardiac MRI, achieving state-of-the-art accuracy and speed compared to baselines.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of accurate and scalable quantification of intramyocardial motion and strain from tagging MRI.

Method: The method uses implicit neural representations (INRs) conditioned on learned latent codes to predict continuous left ventricular displacement without requiring inference-time optimization.

Result: On 452 UK Biobank test cases, it achieved the best tracking accuracy (2.14 mm RMSE) and the lowest strain errors (global circumferential strain: 2.86%, radial strain: 6.42%) while being ~380x faster than the most accurate baseline.

Conclusion: INR-based models are highly suitable for accurate, efficient, and scalable analysis of myocardial strain in large cardiac MRI datasets.

Abstract: Automatic quantification of intramyocardial motion and strain from tagging
MRI remains an important but challenging task. We propose a method using
implicit neural representations (INRs), conditioned on learned latent codes, to
predict continuous left ventricular (LV) displacement -- without requiring
inference-time optimisation. Evaluated on 452 UK Biobank test cases, our method
achieved the best tracking accuracy (2.14 mm RMSE) and the lowest combined
error in global circumferential (2.86%) and radial (6.42%) strain compared to
three deep learning baselines. In addition, our method is $\sim$380$\times$
faster than the most accurate baseline. These results highlight the suitability
of INR-based models for accurate and scalable analysis of myocardial strain in
large CMR datasets.

</details>


### [77] [E-MLNet: Enhanced Mutual Learning for Universal Domain Adaptation with Sample-Specific Weighting](https://arxiv.org/abs/2509.09006)
*Samuel Felipe dos Santos,Tiago Agostinho de Almeida,Jurandy Almeida*

Main category: cs.CV

TL;DR: This paper proposes Enhanced Mutual Learning Network (E-MLNet), a method to improve Universal Domain Adaptation (UniDA) performance by dynamically weighting relevant class boundaries and demonstrates its effectiveness on four benchmarks.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of effectively transferring knowledge from a labeled source domain to an unlabeled target domain in UniDA, particularly by improving the discrimination between known and unknown classes.

Method: The proposed E-MLNet employs a dynamic weighting strategy for Open-set Entropy Minimization (OEM), focusing adaptation efforts on the most relevant class boundaries by leveraging closed-set classifier predictions.

Result: E-MLNet demonstrated superior performance, achieving the highest average H-scores on VisDA and ImageCLEF, and outperforming MLNet in 22 out of 31 Open-Partial DA tasks and 19 out of 31 Open-Set DA tasks.

Conclusion: E-MLNet's focused adaptation strategy enhances the distinction between known and unknown classes, making it more robust and effective than state-of-the-art methods in Universal Domain Adaptation.

Abstract: Universal Domain Adaptation (UniDA) seeks to transfer knowledge from a
labeled source to an unlabeled target domain without assuming any relationship
between their label sets, requiring models to classify known samples while
rejecting unknown ones. Advanced methods like Mutual Learning Network (MLNet)
use a bank of one-vs-all classifiers adapted via Open-set Entropy Minimization
(OEM). However, this strategy treats all classifiers equally, diluting the
learning signal. We propose the Enhanced Mutual Learning Network (E-MLNet),
which integrates a dynamic weighting strategy to OEM. By leveraging the
closed-set classifier's predictions, E-MLNet focuses adaptation on the most
relevant class boundaries for each target sample, sharpening the distinction
between known and unknown classes. We conduct extensive experiments on four
challenging benchmarks: Office-31, Office-Home, VisDA-2017, and ImageCLEF. The
results demonstrate that E-MLNet achieves the highest average H-scores on VisDA
and ImageCLEF and exhibits superior robustness over its predecessor. E-MLNet
outperforms the strong MLNet baseline in the majority of individual adaptation
tasks -- 22 out of 31 in the challenging Open-Partial DA setting and 19 out of
31 in the Open-Set DA setting -- confirming the benefits of our focused
adaptation strategy.

</details>


### [78] [COCO-Urdu: A Large-Scale Urdu Image-Caption Dataset with Multimodal Quality Estimation](https://arxiv.org/abs/2509.09014)
*Umair Hassan*

Main category: cs.CV

TL;DR: COCO-Urdu introduces a large-scale Urdu image-caption dataset to address the lack of resources in Urdu multimodal research, aiming to improve inclusivity in vision-language systems.


<details>
  <summary>Details</summary>
Motivation: Urdu is under-represented in multimodal and vision-language research due to a lack of large-scale datasets, which perpetuates biases toward high-resource languages.

Method: The study creates the COCO-Urdu dataset by translating MS COCO captions into Urdu using SeamlessM4T v2, validating quality through a hybrid estimation framework, and refining low-score captions with open-source language models.

Result: COCO-Urdu generates 59,000 images and 319,000 validated Urdu captions, demonstrating strong benchmark performance on BLEU, SacreBLEU, and chrF metrics.

Conclusion: COCO-Urdu is the largest publicly available Urdu captioning dataset, aiming to address language biases in vision-language systems and promote inclusivity in research.

Abstract: Urdu, spoken by over 250 million people, remains critically under-served in
multimodal and vision-language research. The absence of large-scale,
high-quality datasets has limited the development of Urdu-capable systems and
reinforced biases in multilingual vision-language models trained primarily on
high-resource languages. To address this gap, we present COCO-Urdu, a
large-scale image-caption dataset derived from MS COCO, containing 59,000
images and 319,000 Urdu captions selected through stratified sampling to
preserve the original distribution. Captions were translated using SeamlessM4T
v2 and validated with a hybrid multimodal quality estimation framework that
integrates COMET-Kiwi for translation quality, CLIP-based similarity for visual
grounding, and BERTScore with back-translation for semantic consistency;
low-scoring captions were iteratively refined using open-source large language
models. We further benchmark COCO-Urdu on BLEU, SacreBLEU, and chrF, reporting
consistently strong results. To the best of our knowledge, COCO-Urdu is the
largest publicly available Urdu captioning dataset. By releasing both the
dataset and the quality estimation pipeline, we aim to reduce language bias in
multimodal research and establish a foundation for inclusive vision-language
systems.

</details>


### [79] [VoxelFormer: Parameter-Efficient Multi-Subject Visual Decoding from fMRI](https://arxiv.org/abs/2509.09015)
*Chenqian Le,Yilin Zhao,Nikasadat Emami,Kushagra Yadav,Xujin "Chris" Liu,Xupeng Chen,Yao Wang*

Main category: cs.CV

TL;DR: VoxelFormer enables subject-agnostic visual decoding using fMRI, with fewer parameters and competitive performance.


<details>
  <summary>Details</summary>
Motivation: The paper addresses scalability challenges in fMRI-based visual decoding, which traditionally relies on subject-specific training.

Method: The study introduces VoxelFormer—a transformer architecture leveraging Token Merging Transformer (ToMer) for voxel compression and Q-Former for aligning representations with CLIP embedding space.

Result: VoxelFormer demonstrated competitive performance on the 7T Natural Scenes Dataset, requiring fewer parameters compared to existing methods.

Conclusion: Token merging and query-driven transformer architectures are effective for efficient and scalable neural decoding in fMRI visual decoding tasks.

Abstract: Recent advances in fMRI-based visual decoding have enabled compelling
reconstructions of perceived images. However, most approaches rely on
subject-specific training, limiting scalability and practical deployment. We
introduce \textbf{VoxelFormer}, a lightweight transformer architecture that
enables multi-subject training for visual decoding from fMRI. VoxelFormer
integrates a Token Merging Transformer (ToMer) for efficient voxel compression
and a query-driven Q-Former that produces fixed-size neural representations
aligned with the CLIP image embedding space. Evaluated on the 7T Natural Scenes
Dataset, VoxelFormer achieves competitive retrieval performance on subjects
included during training with significantly fewer parameters than existing
methods. These results highlight token merging and query-based transformers as
promising strategies for parameter-efficient neural decoding.

</details>


### [80] [Integrating Anatomical Priors into a Causal Diffusion Model](https://arxiv.org/abs/2509.09054)
*Binxu Li,Wei Peng,Mingjie Li,Ehsan Adeli,Kilian M. Pohl*

Main category: cs.CV

TL;DR: The paper introduces PCGM, a method for generating anatomically plausible synthetic brain MRIs that replicate subtle disease effects using a probabilistic graph-based approach and 3D diffusion modeling.


<details>
  <summary>Details</summary>
Motivation: MRI studies are costly and often struggle to detect subtle morphometric differences visually. The paper aims to enable anatomically accurate synthetic image generation to benefit disease research.

Method: PCGM integrates anatomical constraints into a generative diffusion framework via voxel-level spatial binary masks and a 3D diffusion decoder to refine counterfactual synthetic MRIs.

Result: PCGM was shown to produce high-quality synthetic MRIs that replicate subtle disease effects in brain regions reported in prior neuroscience studies.

Conclusion: PCGM successfully generates medically relevant synthetic brain MRIs, marking a significant step toward the use of synthetic imaging for studying subtle morphological differences.

Abstract: 3D brain MRI studies often examine subtle morphometric differences between
cohorts that are hard to detect visually. Given the high cost of MRI
acquisition, these studies could greatly benefit from image syntheses,
particularly counterfactual image generation, as seen in other domains, such as
computer vision. However, counterfactual models struggle to produce
anatomically plausible MRIs due to the lack of explicit inductive biases to
preserve fine-grained anatomical details. This shortcoming arises from the
training of the models aiming to optimize for the overall appearance of the
images (e.g., via cross-entropy) rather than preserving subtle, yet medically
relevant, local variations across subjects. To preserve subtle variations, we
propose to explicitly integrate anatomical constraints on a voxel-level as
prior into a generative diffusion framework. Called Probabilistic Causal Graph
Model (PCGM), the approach captures anatomical constraints via a probabilistic
graph module and translates those constraints into spatial binary masks of
regions where subtle variations occur. The masks (encoded by a 3D extension of
ControlNet) constrain a novel counterfactual denoising UNet, whose encodings
are then transferred into high-quality brain MRIs via our 3D diffusion decoder.
Extensive experiments on multiple datasets demonstrate that PCGM generates
structural brain MRIs of higher quality than several baseline approaches.
Furthermore, we show for the first time that brain measurements extracted from
counterfactuals (generated by PCGM) replicate the subtle effects of a disease
on cortical brain regions previously reported in the neuroscience literature.
This achievement is an important milestone in the use of synthetic MRIs in
studies investigating subtle morphological differences.

</details>


### [81] [Enhancing 3D Medical Image Understanding with Pretraining Aided by 2D Multimodal Large Language Models](https://arxiv.org/abs/2509.09064)
*Qiuhui Chen,Xuancheng Yao,Huping Ye,Yi Hong*

Main category: cs.CV

TL;DR: The paper introduces Med3DInsight, a novel framework that integrates 3D image encoders with 2D multimodal large language models (MLLMs) for improved understanding of 3D medical image volumes without human annotations.


<details>
  <summary>Details</summary>
Motivation: Existing 3D medical imaging methods struggle with semantic comprehension. The authors aim to leverage advancements in 2D multimodal large language models to enhance 3D medical image understanding.

Method: Med3DInsight combines 3D image encoders with 2D MLLMs using a plane-slice-aware transformer and employs partial optimal transport for alignment to handle potential noise in LLM-generated content.

Result: Med3DInsight achieves state-of-the-art performance in segmentation and classification tasks on public CT and MRI datasets, outperforming existing self-supervised learning methods.

Conclusion: Med3DInsight introduces a scalable multimodal learning paradigm for 3D medical imaging and can be integrated into existing networks to improve their performance, with accompanying code, datasets, and pretrained models made publicly available.

Abstract: Understanding 3D medical image volumes is critical in the medical field, yet
existing 3D medical convolution and transformer-based self-supervised learning
(SSL) methods often lack deep semantic comprehension. Recent advancements in
multimodal large language models (MLLMs) provide a promising approach to
enhance image understanding through text descriptions. To leverage these 2D
MLLMs for improved 3D medical image understanding, we propose Med3DInsight, a
novel pretraining framework that integrates 3D image encoders with 2D MLLMs via
a specially designed plane-slice-aware transformer module. Additionally, our
model employs a partial optimal transport based alignment, demonstrating
greater tolerance to noise introduced by potential noises in LLM-generated
content. Med3DInsight introduces a new paradigm for scalable multimodal 3D
medical representation learning without requiring human annotations. Extensive
experiments demonstrate our state-of-the-art performance on two downstream
tasks, i.e., segmentation and classification, across various public datasets
with CT and MRI modalities, outperforming current SSL methods. Med3DInsight can
be seamlessly integrated into existing 3D medical image understanding networks,
potentially enhancing their performance. Our source code, generated datasets,
and pre-trained models will be available at
https://github.com/Qybc/Med3DInsight.

</details>


### [82] [Improvement of Human-Object Interaction Action Recognition Using Scene Information and Multi-Task Learning Approach](https://arxiv.org/abs/2509.09067)
*Hesham M. Shehata,Mohammad Abdolrahmani*

Main category: cs.CV

TL;DR: The paper presents a new multi-task learning methodology using human poses and fixed object information for improved human action recognition, achieving 99.25% accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the challenges in detecting human-object interaction cases in human action recognition due to missing effective scene information representation and learning structures.

Method: Introduced a multi-task learning approach that integrates human skeleton poses with fixed object information from the environment, evaluated with a custom dataset.

Result: Achieved an accuracy of 99.25% in recognizing interaction and non-interaction actions, a 2.75% improvement over the baseline model.

Conclusion: Incorporating fixed object and interaction area information boosts action recognition systems' performance, especially for human-object interactions.

Abstract: Recent graph convolutional neural networks (GCNs) have shown high performance
in the field of human action recognition by using human skeleton poses.
However, it fails to detect human-object interaction cases successfully due to
the lack of effective representation of the scene information and appropriate
learning architectures. In this context, we propose a methodology to utilize
human action recognition performance by considering fixed object information in
the environment and following a multi-task learning approach. In order to
evaluate the proposed method, we collected real data from public environments
and prepared our data set, which includes interaction classes of hands-on fixed
objects (e.g., ATM ticketing machines, check-in/out machines, etc.) and
non-interaction classes of walking and standing. The multi-task learning
approach, along with interaction area information, succeeds in recognizing the
studied interaction and non-interaction actions with an accuracy of 99.25%,
outperforming the accuracy of the base model using only human skeleton poses by
2.75%.

</details>


### [83] [IRDFusion: Iterative Relation-Map Difference guided Feature Fusion for Multispectral Object Detection](https://arxiv.org/abs/2509.09085)
*Jifeng Shen,Haibo Zhan,Xin Zuo,Heng Fan,Xiaohui Yuan,Jun Li,Wankou Yang*

Main category: cs.CV

TL;DR: The paper proposes IRDFusion, a feature fusion framework designed to enhance multispectral object detection by suppressing noise and improving saliency through novel architectures MFRM and DFFM.


<details>
  <summary>Details</summary>
Motivation: Multispectral object detection methods struggle with background and noise interference during feature fusion, which impacts perceptual performance.

Method: The paper introduces two modules (MFRM and DFFM), combined into the IRDFusion framework to iteratively refine and fuse cross-modal features while mitigating noise and enhancing saliency.

Result: IRDFusion shows significant performance improvements and outperforms existing methods on FLIR, LLVIP, and M$^3$FD datasets.

Conclusion: IRDFusion offers an innovative approach to multispectral feature fusion, demonstrating robustness and effectiveness in diverse scenarios, with state-of-the-art object detection performance.

Abstract: Current multispectral object detection methods often retain extraneous
background or noise during feature fusion, limiting perceptual performance.To
address this, we propose an innovative feature fusion framework based on
cross-modal feature contrastive and screening strategy, diverging from
conventional approaches. The proposed method adaptively enhances salient
structures by fusing object-aware complementary cross-modal features while
suppressing shared background interference.Our solution centers on two novel,
specially designed modules: the Mutual Feature Refinement Module (MFRM) and the
Differential Feature Feedback Module (DFFM). The MFRM enhances intra- and
inter-modal feature representations by modeling their relationships, thereby
improving cross-modal alignment and discriminative power.Inspired by feedback
differential amplifiers, the DFFM dynamically computes inter-modal differential
features as guidance signals and feeds them back to the MFRM, enabling adaptive
fusion of complementary information while suppressing common-mode noise across
modalities. To enable robust feature learning, the MFRM and DFFM are integrated
into a unified framework, which is formally formulated as an Iterative
Relation-Map Differential Guided Feature Fusion mechanism, termed IRDFusion.
IRDFusion enables high-quality cross-modal fusion by progressively amplifying
salient relational signals through iterative feedback, while suppressing
feature noise, leading to significant performance gains.In extensive
experiments on FLIR, LLVIP and M$^3$FD datasets, IRDFusion achieves
state-of-the-art performance and consistently outperforms existing methods
across diverse challenging scenarios, demonstrating its robustness and
effectiveness. Code will be available at
https://github.com/61s61min/IRDFusion.git.

</details>


### [84] [SQAP-VLA: A Synergistic Quantization-Aware Pruning Framework for High-Performance Vision-Language-Action Models](https://arxiv.org/abs/2509.09090)
*Hengyu Fang,Yijiang Liu,Yuan Du,Li Du,Huanrui Yang*

Main category: cs.CV

TL;DR: This paper introduces SQAP-VLA, a framework for accelerating Vision-Language-Action models via structured, training-free quantization and token pruning.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the computational and memory constraints hindering real-world deployment of Vision-Language-Action models.

Method: SQAP-VLA co-designs a quantization-aware token pruning pipeline, combining effective pruning criteria with improved quantizer design for holistic efficiency.

Result: Using SQAP-VLA, VLA models achieve a 1.93x inference speedup and up to a 4.5% boost in success rate while maintaining core performance.

Conclusion: SQAP-VLA offers a significant advancement in both computational efficiency and practical applicability for Vision-Language-Action models.

Abstract: Vision-Language-Action (VLA) models exhibit unprecedented capabilities for
embodied intelligence. However, their extensive computational and memory costs
hinder their practical deployment. Existing VLA compression and acceleration
approaches conduct quantization or token pruning in an ad-hoc manner but fail
to enable both for a holistic efficiency improvement due to an observed
incompatibility. This work introduces SQAP-VLA, the first structured,
training-free VLA inference acceleration framework that simultaneously enables
state-of-the-art quantization and token pruning. We overcome the
incompatibility by co-designing the quantization and token pruning pipeline,
where we propose new quantization-aware token pruning criteria that work on an
aggressively quantized model while improving the quantizer design to enhance
pruning effectiveness. When applied to standard VLA models, SQAP-VLA yields
significant gains in computational efficiency and inference speed while
successfully preserving core model performance, achieving a $\times$1.93
speedup and up to a 4.5\% average success rate enhancement compared to the
original model.

</details>


### [85] [S-BEVLoc: BEV-based Self-supervised Framework for Large-scale LiDAR Global Localization](https://arxiv.org/abs/2509.09110)
*Chenghao Zhang,Lun Luo,Si-Yuan Cao,Xiaokai Bai,Yuncheng Jin,Zhu Yu,Beinan Yu,Yisen Wang,Hui-Liang Shen*

Main category: cs.CV

TL;DR: S-BEVLoc is a self-supervised LiDAR-based global localization framework that avoids reliance on costly ground-truth poses. It demonstrates state-of-the-art performance in various localization tasks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the high cost and effort associated with acquiring ground-truth poses, which are typically used for training supervised LiDAR-based global localization frameworks.

Method: S-BEVLoc leverages bird's-eye view (BEV) images to create self-supervised training triplets using geographic distances. It uses CNNs to extract local features, aggregates global descriptors with NetVLAD, and introduces a SoftCos loss for enhanced learning.

Result: S-BEVLoc achieves state-of-the-art performance in place recognition, loop closure, and global localization tasks on the KITTI and NCLT datasets.

Conclusion: S-BEVLoc provides a scalable, self-supervised solution for LiDAR-based global localization, eliminating the need for ground-truth pose acquisition while maintaining competitive performance.

Abstract: LiDAR-based global localization is an essential component of simultaneous
localization and mapping (SLAM), which helps loop closure and re-localization.
Current approaches rely on ground-truth poses obtained from GPS or SLAM
odometry to supervise network training. Despite the great success of these
supervised approaches, substantial cost and effort are required for
high-precision ground-truth pose acquisition. In this work, we propose
S-BEVLoc, a novel self-supervised framework based on bird's-eye view (BEV) for
LiDAR global localization, which eliminates the need for ground-truth poses and
is highly scalable. We construct training triplets from single BEV images by
leveraging the known geographic distances between keypoint-centered BEV
patches. Convolutional neural network (CNN) is used to extract local features,
and NetVLAD is employed to aggregate global descriptors. Moreover, we introduce
SoftCos loss to enhance learning from the generated triplets. Experimental
results on the large-scale KITTI and NCLT datasets show that S-BEVLoc achieves
state-of-the-art performance in place recognition, loop closure, and global
localization tasks, while offering scalability that would require extra effort
for supervised approaches.

</details>


### [86] [FPI-Det: a face--phone Interaction Dataset for phone-use detection and understanding](https://arxiv.org/abs/2509.09111)
*Jianqin Gao,Tianqi Wang,Yu Zhang,Yishu Zhang,Chenyuan Wang,Allan Dong,Zihao Wang*

Main category: cs.CV

TL;DR: This paper introduces the FPI-Det dataset, which focuses on detecting phone use through analyzing face, hand, and device interactions, addressing challenges in varied environments and occluded conditions.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve and address the challenges in detecting phone use by capturing fine-grained interactions between faces, hands, and devices, which is not adequately represented in existing datasets.

Method: The authors developed a novel dataset called FPI-Det, consisting of 22,879 images with synchronized face and phone annotations under diverse scenarios. They also evaluated benchmark detection models (YOLO, DETR) for performance analysis.

Result: Baseline results are provided using YOLO and DETR models, analyzed across object sizes, occlusion levels, and varied environments, highlighting the dataset's challenging nature.

Conclusion: FPI-Det provides a valuable resource for advancing vision systems to detect phone use in complex and diverse conditions; public access ensures further research and development.

Abstract: The widespread use of mobile devices has created new challenges for vision
systems in safety monitoring, workplace productivity assessment, and attention
management. Detecting whether a person is using a phone requires not only
object recognition but also an understanding of behavioral context, which
involves reasoning about the relationship between faces, hands, and devices
under diverse conditions. Existing generic benchmarks do not fully capture such
fine-grained human--device interactions. To address this gap, we introduce the
FPI-Det, containing 22{,}879 images with synchronized annotations for faces and
phones across workplace, education, transportation, and public scenarios. The
dataset features extreme scale variation, frequent occlusions, and varied
capture conditions. We evaluate representative YOLO and DETR detectors,
providing baseline results and an analysis of performance across object sizes,
occlusion levels, and environments. Source code and dataset is available at
https://github.com/KvCgRv/FPI-Det.

</details>


### [87] [Model-Agnostic Open-Set Air-to-Air Visual Object Detection for Reliable UAV Perception](https://arxiv.org/abs/2509.09297)
*Spyridon Loukovitis,Anastasios Arsenos,Vasileios Karampinis,Athanasios Voulodimos*

Main category: cs.CV

TL;DR: This paper proposes a framework for open-set detection in UAV air-to-air object detection, addressing challenges posed by domain shifts and corrupted flight data.


<details>
  <summary>Details</summary>
Motivation: To improve UAV object detection under real-world conditions where traditional systems fail due to domain shifts and data corruption.

Method: The approach uses entropy modeling in embedding spaces for semantic uncertainty, alongside spectral normalization and temperature scaling for open-set discrimination.

Result: Validated on the AOT aerial benchmark and real-world tests, achieving up to a 10% AUROC gain compared to YOLO-based detectors.

Conclusion: The framework enhances robustness, handles unknowns effectively, and is suited for dynamic UAV applications without sacrificing accuracy.

Abstract: Open-set detection is crucial for robust UAV autonomy in air-to-air object
detection under real-world conditions. Traditional closed-set detectors degrade
significantly under domain shifts and flight data corruption, posing risks to
safety-critical applications. We propose a novel, model-agnostic open-set
detection framework designed specifically for embedding-based detectors. The
method explicitly handles unknown object rejection while maintaining robustness
against corrupted flight data. It estimates semantic uncertainty via entropy
modeling in the embedding space and incorporates spectral normalization and
temperature scaling to enhance open-set discrimination. We validate our
approach on the challenging AOT aerial benchmark and through extensive
real-world flight tests. Comprehensive ablation studies demonstrate consistent
improvements over baseline methods, achieving up to a 10\% relative AUROC gain
compared to standard YOLO-based detectors. Additionally, we show that
background rejection further strengthens robustness without compromising
detection accuracy, making our solution particularly well-suited for reliable
UAV perception in dynamic air-to-air environments.

</details>


### [88] [Zero-shot Hierarchical Plant Segmentation via Foundation Segmentation Models and Text-to-image Attention](https://arxiv.org/abs/2509.09116)
*Junhao Xing,Ryohei Miyakawa,Yang Yang,Xinpeng Liu,Risa Shinoda,Hiroaki Santo,Yosuke Toda,Fumio Okura*

Main category: cs.CV

TL;DR: ZeroPlantSeg introduces a novel zero-shot approach for plant individual segmentation using a foundation segmentation model and a vision-language model to address hierarchical segmentation challenges.


<details>
  <summary>Details</summary>
Motivation: The paper aims to tackle the hierarchical segmentation problem of extracting entire plant individuals from images, which requires annotated datasets and significant human effort. The authors want to provide an alternative to traditional methods that depend on species-specific data.

Method: The approach integrates a foundation segmentation model for leaf instance extraction and a vision-language model for structural reasoning to achieve zero-shot segmentation of plant individuals without the need for additional training.

Result: ZeroPlantSeg outperforms existing zero-shot methods and exhibits superior cross-domain performance compared to supervised methods across diverse datasets containing multiple plant species and environmental settings.

Conclusion: The proposed method demonstrates the feasibility and effectiveness of zero-shot plant segmentation in complex scenarios, offering a cost-effective solution that reduces the reliance on species-specific annotated data.

Abstract: Foundation segmentation models achieve reasonable leaf instance extraction
from top-view crop images without training (i.e., zero-shot). However,
segmenting entire plant individuals with each consisting of multiple
overlapping leaves remains challenging. This problem is referred to as a
hierarchical segmentation task, typically requiring annotated training
datasets, which are often species-specific and require notable human labor. To
address this, we introduce ZeroPlantSeg, a zero-shot segmentation for
rosette-shaped plant individuals from top-view images. We integrate a
foundation segmentation model, extracting leaf instances, and a vision-language
model, reasoning about plants' structures to extract plant individuals without
additional training. Evaluations on datasets with multiple plant species,
growth stages, and shooting environments demonstrate that our method surpasses
existing zero-shot methods and achieves better cross-domain performance than
supervised methods. Implementations are available at
https://github.com/JunhaoXing/ZeroPlantSeg.

</details>


### [89] [Classification of Driver Behaviour Using External Observation Techniques for Autonomous Vehicles](https://arxiv.org/abs/2509.09349)
*Ian Nell,Shane Gilroy*

Main category: cs.CV

TL;DR: This paper proposes a driver behavior classification system using computer vision to detect distracted and impaired driving.


<details>
  <summary>Details</summary>
Motivation: Reduce road traffic accidents caused by human errors like distracted and impaired driving.

Method: Utilizes a computer vision-based framework with YOLO object detection and custom lane estimation algorithms to identify unsafe driving behaviors.

Result: Experimental results on video datasets show reliability and adaptability of the system in different conditions.

Conclusion: Vision-based systems can effectively analyze driving behaviors, aiding in road safety without relying on inter-vehicular communication.

Abstract: Road traffic accidents remain a significant global concern, with human error,
particularly distracted and impaired driving, among the leading causes. This
study introduces a novel driver behavior classification system that uses
external observation techniques to detect indicators of distraction and
impairment. The proposed framework employs advanced computer vision
methodologies, including real-time object tracking, lateral displacement
analysis, and lane position monitoring. The system identifies unsafe driving
behaviors such as excessive lateral movement and erratic trajectory patterns by
implementing the YOLO object detection model and custom lane estimation
algorithms. Unlike systems reliant on inter-vehicular communication, this
vision-based approach enables behavioral analysis of non-connected vehicles.
Experimental evaluations on diverse video datasets demonstrate the framework's
reliability and adaptability across varying road and environmental conditions.

</details>


### [90] [Gradient-Attention Guided Dual-Masking Synergetic Framework for Robust Text-based Person Retrieval](https://arxiv.org/abs/2509.09118)
*Tianlu Zheng,Yifan Zhang,Xiang An,Ziyong Feng,Kaicheng Yang,Qichuan Ding*

Main category: cs.CV

TL;DR: This paper advances CLIP for person representation learning by addressing challenges in data insufficiency and global contrastive learning limitations.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from enhancing CLIP's performance for person-centric tasks, despite challenges like lack of annotated datasets and limitations in global-level feature representation.

Method: The paper introduces a data curation pipeline creating the WebPerson dataset and the GA-DMS framework to refine text-image alignment by adaptive masking and token prediction objectives.

Result: The proposed techniques demonstrated state-of-the-art performance in various benchmarks for person representation learning.

Conclusion: The advancements in data curation and model architecture significantly improve CLIP's capability in handling person-centric tasks and in achieving fine-grained semantic understanding.

Abstract: Although Contrastive Language-Image Pre-training (CLIP) exhibits strong
performance across diverse vision tasks, its application to person
representation learning faces two critical challenges: (i) the scarcity of
large-scale annotated vision-language data focused on person-centric images,
and (ii) the inherent limitations of global contrastive learning, which
struggles to maintain discriminative local features crucial for fine-grained
matching while remaining vulnerable to noisy text tokens. This work advances
CLIP for person representation learning through synergistic improvements in
data curation and model architecture. First, we develop a noise-resistant data
construction pipeline that leverages the in-context learning capabilities of
MLLMs to automatically filter and caption web-sourced images. This yields
WebPerson, a large-scale dataset of 5M high-quality person-centric image-text
pairs. Second, we introduce the GA-DMS (Gradient-Attention Guided Dual-Masking
Synergetic) framework, which improves cross-modal alignment by adaptively
masking noisy textual tokens based on the gradient-attention similarity score.
Additionally, we incorporate masked token prediction objectives that compel the
model to predict informative text tokens, enhancing fine-grained semantic
representation learning. Extensive experiments show that GA-DMS achieves
state-of-the-art performance across multiple benchmarks.

</details>


### [91] [ALL-PET: A Low-resource and Low-shot PET Foundation Model in the Projection Domain](https://arxiv.org/abs/2509.09130)
*Bin Huang,Kang Chen,Bingxuan Li,Huafeng Liu,Qiegen Liu*

Main category: cs.CV

TL;DR: ALL-PET is a PET foundation model designed to overcome issues of limited data and computational resources, relying on innovative latent diffusion techniques in the projection domain.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to address the challenges of building large-scale PET imaging foundation models when faced with limited labeled data and insufficient computational power.

Method: ALL-PET employs a latent diffusion model featuring Radon mask augmentation, dynamic multi-mask mechanisms, positive/negative mask constraints, and transparent medical attention to enhance its capabilities in generating sinograms.

Result: Experiments demonstrate that ALL-PET can produce sinograms of high quality with just 500 samples and performs comparably to models trained on larger datasets, handling diverse tasks efficiently within memory limits.

Conclusion: ALL-PET successfully reduces data and resource burdens for PET imaging while maintaining performance and flexibility across various tasks.

Abstract: Building large-scale foundation model for PET imaging is hindered by limited
access to labeled data and insufficient computational resources. To overcome
data scarcity and efficiency limitations, we propose ALL-PET, a low-resource,
low-shot PET foundation model operating directly in the projection domain.
ALL-PET leverages a latent diffusion model (LDM) with three key innovations.
First, we design a Radon mask augmentation strategy (RMAS) that generates over
200,000 structurally diverse training samples by projecting randomized
image-domain masks into sinogram space, significantly improving generalization
with minimal data. This is extended by a dynamic multi-mask (DMM) mechanism
that varies mask quantity and distribution, enhancing data diversity without
added model complexity. Second, we implement positive/negative mask constraints
to embed strict geometric consistency, reducing parameter burden while
preserving generation quality. Third, we introduce transparent medical
attention (TMA), a parameter-free, geometry-driven mechanism that enhances
lesion-related regions in raw projection data. Lesion-focused attention maps
are derived from coarse segmentation, covering both hypermetabolic and
hypometabolic areas, and projected into sinogram space for physically
consistent guidance. The system supports clinician-defined ROI adjustments,
ensuring flexible, interpretable, and task-adaptive emphasis aligned with PET
acquisition physics. Experimental results show ALL-PET achieves high-quality
sinogram generation using only 500 samples, with performance comparable to
models trained on larger datasets. ALL-PET generalizes across tasks including
low-dose reconstruction, attenuation correction, delayed-frame prediction, and
tracer separation, operating efficiently with memory use under 24GB.

</details>


### [92] [Visual Grounding from Event Cameras](https://arxiv.org/abs/2509.09584)
*Lingdong Kong,Dongyue Lu,Ao Liang,Rong Li,Yuhao Dong,Tianshuai Hu,Lai Xing Ng,Wei Tsang Ooi,Benoit R. Cottereau*

Main category: cs.CV

TL;DR: Talk2Event introduces a benchmark incorporating event cameras with language-driven multimodal perception for understanding dynamic environments.


<details>
  <summary>Details</summary>
Motivation: Event cameras handle dynamic scenes well but lack integration with language-processing capabilities. The paper aims to bridge this gap in multimodal perception.

Method: The study creates Talk2Event, a large-scale benchmark featuring scenes, annotated objects, and structured attributes to capture various cues for language-object grounding.

Result: Talk2Event includes 5,567 scenes, 13,458 objects, and over 30,000 validated expressions enhanced with attributes for spatial, temporal, and relational cues.

Conclusion: Talk2Event enables interpretable contextual grounding and has implications for robotics and human-AI interaction in dynamic environments.

Abstract: Event cameras capture changes in brightness with microsecond precision and
remain reliable under motion blur and challenging illumination, offering clear
advantages for modeling highly dynamic scenes. Yet, their integration with
natural language understanding has received little attention, leaving a gap in
multimodal perception. To address this, we introduce Talk2Event, the first
large-scale benchmark for language-driven object grounding using event data.
Built on real-world driving scenarios, Talk2Event comprises 5,567 scenes,
13,458 annotated objects, and more than 30,000 carefully validated referring
expressions. Each expression is enriched with four structured attributes --
appearance, status, relation to the viewer, and relation to surrounding objects
-- that explicitly capture spatial, temporal, and relational cues. This
attribute-centric design supports interpretable and compositional grounding,
enabling analysis that moves beyond simple object recognition to contextual
reasoning in dynamic environments. We envision Talk2Event as a foundation for
advancing multimodal and temporally-aware perception, with applications
spanning robotics, human-AI interaction, and so on.

</details>


### [93] [Noise-Robust Topology Estimation of 2D Image Data via Neural Networks and Persistent Homology](https://arxiv.org/abs/2509.09140)
*Dylan Peek,Matthew P. Skerritt,Stephan Chalup*

Main category: cs.CV

TL;DR: This paper examines the noise robustness of neural networks trained to predict Betti numbers, comparing their performance to persistent homology methodologies.


<details>
  <summary>Details</summary>
Motivation: Investigate whether neural networks can outperform persistent homology approaches in topology estimation under noisy conditions, an area with emerging potential.

Method: Experimented with synthetic and real-world datasets using an ANN trained on Betti numbers and compared with PH techniques based on cubical complexes and Signed Euclidean Distance Transform.

Result: ANNs demonstrated better noise robustness than persistent homology approaches by leveraging contextual and geometric priors from training data.

Conclusion: ANNs provide a promising alternative to traditional persistent homology methods for topology estimation in noisy environments.

Abstract: Persistent Homology (PH) and Artificial Neural Networks (ANNs) offer
contrasting approaches to inferring topological structure from data. In this
study, we examine the noise robustness of a supervised neural network trained
to predict Betti numbers in 2D binary images. We compare an ANN approach
against a PH pipeline based on cubical complexes and the Signed Euclidean
Distance Transform (SEDT), which is a widely adopted strategy for noise-robust
topological analysis. Using one synthetic and two real-world datasets, we show
that ANNs can outperform this PH approach under noise, likely due to their
capacity to learn contextual and geometric priors from training data. Though
still emerging, the use of ANNs for topology estimation offers a compelling
alternative to PH under structural noise.

</details>


### [94] [Objectness Similarity: Capturing Object-Level Fidelity in 3D Scene Evaluation](https://arxiv.org/abs/2509.09143)
*Yuiko Uchida,Ren Togo,Keisuke Maeda,Takahiro Ogawa,Miki Haseyama*

Main category: cs.CV

TL;DR: The paper introduces a new metric, Objectness SIMilarity (OSIM), for assessing 3D scenes based on object-focused human visual perception.


<details>
  <summary>Details</summary>
Motivation: Existing 3D scene evaluation metrics fail to align with human perception by focusing on overall image quality rather than individual objects.

Method: The authors propose OSIM, leveraging an object detection model to evaluate the "objectness" of scene components, supported by feature representations.

Result: OSIM aligns better with human perception than current metrics, proven through user studies and in assessments of recent 3D reconstruction models.

Conclusion: OSIM offers a more human-aligned approach for evaluating 3D scenes and provides clarity on advancements in 3D reconstruction and generation, with implementation code openly available.

Abstract: This paper presents Objectness SIMilarity (OSIM), a novel evaluation metric
for 3D scenes that explicitly focuses on "objects," which are fundamental units
of human visual perception. Existing metrics assess overall image quality,
leading to discrepancies with human perception. Inspired by neuropsychological
insights, we hypothesize that human recognition of 3D scenes fundamentally
involves attention to individual objects. OSIM enables object-centric
evaluations by leveraging an object detection model and its feature
representations to quantify the "objectness" of each object in the scene. Our
user study demonstrates that OSIM aligns more closely with human perception
compared to existing metrics. We also analyze the characteristics of OSIM using
various approaches. Moreover, we re-evaluate recent 3D reconstruction and
generation models under a standardized experimental setup to clarify
advancements in this field. The code is available at
https://github.com/Objectness-Similarity/OSIM.

</details>


### [95] [Video Understanding by Design: How Datasets Shape Architectures and Insights](https://arxiv.org/abs/2509.09151)
*Lei Wang,Piotr Koniusz,Yongsheng Gao*

Main category: cs.CV

TL;DR: The paper offers a survey linking datasets with architectural evolution in video understanding, emphasizing inductive biases and providing guidance for general-purpose model advancement.


<details>
  <summary>Details</summary>
Motivation: Understanding video content has evolved through complex datasets and architectures, but their interrelationship is often overlooked. The paper motivates bridging this gap by showing how dataset characteristics impose structural challenges on model designs.

Method: The authors analyze the interplay between datasets and architectures, categorizing advancements as responses to dataset-driven pressures like motion complexity and temporal span. They propose a unified framework linking inductive biases, datasets, and models.

Result: The survey identifies key dataset characteristics driving architectural changes in video understanding models and synthesizes them into a coherent framework for guiding model alignment with these dataset invariances.

Conclusion: This work not only explains past architectural milestones with regard to dataset influences but also provides a roadmap for future model design and development, enhancing general-purpose video understanding.

Abstract: Video understanding has advanced rapidly, fueled by increasingly complex
datasets and powerful architectures. Yet existing surveys largely classify
models by task or family, overlooking the structural pressures through which
datasets guide architectural evolution. This survey is the first to adopt a
dataset-driven perspective, showing how motion complexity, temporal span,
hierarchical composition, and multimodal richness impose inductive biases that
models should encode. We reinterpret milestones, from two-stream and 3D CNNs to
sequential, transformer, and multimodal foundation models, as concrete
responses to these dataset-driven pressures. Building on this synthesis, we
offer practical guidance for aligning model design with dataset invariances
while balancing scalability and task demands. By unifying datasets, inductive
biases, and architectures into a coherent framework, this survey provides both
a comprehensive retrospective and a prescriptive roadmap for advancing
general-purpose video understanding.

</details>


### [96] [OCELOT 2023: Cell Detection from Cell-Tissue Interaction Challenge](https://arxiv.org/abs/2509.09153)
*JaeWoong Shin,Jeongun Ryu,Aaron Valero Puche,Jinhee Lee,Biagio Brattoli,Wonkyung Jung,Soo Ick Cho,Kyunghyun Paeng,Chan-Young Ock,Donggeun Yoo,Zhaoyang Li,Wangkai Li,Huayu Mai,Joshua Millward,Zhen He,Aiden Nibali,Lydia Anette Schoenpflug,Viktor Hendrik Koelzer,Xu Shuoyu,Ji Zheng,Hu Bin,Yu-Wen Lo,Ching-Hui Yang,Sérgio Pereira*

Main category: cs.CV

TL;DR: The paper discusses the OCELOT 2023 challenge aimed at improving multi-scale cell-tissue interaction understanding in Whole-Slide Images for pathology. The challenge introduced a dataset combining overlapping cell detection and tissue segmentation annotations from six organs. Participant models leveraging multi-scale semantics outperformed traditional cell-only methods, leading to significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: To validate the hypothesis that understanding cell-tissue interactions is crucial for achieving human-level detection performance in whole-slide images and to accelerate research in this area.

Method: The OCELOT 2023 challenge provided a dataset with overlapping cell detection and tissue segmentation annotations from six organs. Participants developed models utilizing multi-scale semantics for cell detection and tissue segmentation.

Result: Top-performing models demonstrated up to a 7.99 increase in F1-score on the test set compared to the baseline cell-only model, indicating a significant improvement over traditional detection methods that did not incorporate cell-tissue relationships.

Conclusion: The challenge results highlight the importance of understanding and incorporating multi-scale semantics in tissue and cell detection models. This approach enables better modeling of the interdependent relationships between cells and tissues, offering substantial advancements over conventional methods.

Abstract: Pathologists routinely alternate between different magnifications when
examining Whole-Slide Images, allowing them to evaluate both broad tissue
morphology and intricate cellular details to form comprehensive diagnoses.
However, existing deep learning-based cell detection models struggle to
replicate these behaviors and learn the interdependent semantics between
structures at different magnifications. A key barrier in the field is the lack
of datasets with multi-scale overlapping cell and tissue annotations. The
OCELOT 2023 challenge was initiated to gather insights from the community to
validate the hypothesis that understanding cell and tissue (cell-tissue)
interactions is crucial for achieving human-level performance, and to
accelerate the research in this field. The challenge dataset includes
overlapping cell detection and tissue segmentation annotations from six organs,
comprising 673 pairs sourced from 306 The Cancer Genome Atlas (TCGA)
Whole-Slide Images with hematoxylin and eosin staining, divided into training,
validation, and test subsets. Participants presented models that significantly
enhanced the understanding of cell-tissue relationships. Top entries achieved
up to a 7.99 increase in F1-score on the test set compared to the baseline
cell-only model that did not incorporate cell-tissue relationships. This is a
substantial improvement in performance over traditional cell-only detection
methods, demonstrating the need for incorporating multi-scale semantics into
the models. This paper provides a comparative analysis of the methods used by
participants, highlighting innovative strategies implemented in the OCELOT 2023
challenge.

</details>


### [97] [RT-DETR++ for UAV Object Detection](https://arxiv.org/abs/2509.09157)
*Yuan Shufang*

Main category: cs.CV

TL;DR: This paper proposes RT-DETR++, a model improving UAV object detection through enhanced feature encoding for small, dense objects, maintaining real-time detection and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Identifying objects in UAV imagery is challenging due to densely packed small objects, scale variations, and occlusion, necessitating better real-time detection techniques.

Method: RT-DETR++ introduces a channel-gated attention-based upsampling/downsampling mechanism (AU/AD) and employs CSP-PAC with parallel hollow convolutions to enhance feature layer propagation and multi-scale integration.

Result: The proposed neck design significantly improves object detection performance for small and densely packed items without adding computational complexity, retaining real-time capabilities.

Conclusion: RT-DETR++ offers an effective approach to enhancing feature encoding for real-time UAV object detection systems, balancing improved accuracy with efficiency.

Abstract: Object detection in unmanned aerial vehicle (UAV) imagery presents
significant challenges. Issues such as densely packed small objects, scale
variations, and occlusion are commonplace. This paper introduces RT-DETR++,
which enhances the encoder component of the RT-DETR model. Our improvements
focus on two key aspects. First, we introduce a channel-gated attention-based
upsampling/downsampling (AU/AD) mechanism. This dual-path system minimizes
errors and preserves details during feature layer propagation. Second, we
incorporate CSP-PAC during feature fusion. This technique employs parallel
hollow convolutions to process local and contextual information within the same
layer, facilitating the integration of multi-scale features. Evaluation
demonstrates that our novel neck design achieves superior performance in
detecting small and densely packed objects. The model maintains sufficient
speed for real-time detection without increasing computational complexity. This
study provides an effective approach for feature encoding design in real-time
detection systems.

</details>


### [98] [A Knowledge Noise Mitigation Framework for Knowledge-based Visual Question Answering](https://arxiv.org/abs/2509.09159)
*Zhiyue Liu,Sihang Liu,Jinyuan Liu,Xinru Zhang*

Main category: cs.CV

TL;DR: The paper addresses the challenge of noise and redundancy in knowledge-based visual question answering (KB-VQA) by proposing a training-free framework that retrieves and integrates knowledge more effectively and selectively.


<details>
  <summary>Details</summary>
Motivation: Existing KB-VQA models are hampered by noise and redundancy caused by directly integrating retrieved knowledge, which affects the accuracy and reliability of answers.

Method: The framework enhances retrieval by constructing focused queries from image-question pairs, employs large models to extract relevant knowledge segments, and incorporates selective integration of knowledge when confidence in answers is low.

Result: The proposed framework improves the relevance and accuracy of knowledge retrieval and utilization, outperforming state-of-the-art methods in extensive experiments.

Conclusion: Noise and redundancy in KB-VQA can be mitigated through focus and selective knowledge integration, resulting in more accurate question answering capabilities without the need for extensive retraining.

Abstract: Knowledge-based visual question answering (KB-VQA) requires a model to
understand images and utilize external knowledge to provide accurate answers.
Existing approaches often directly augment models with retrieved information
from knowledge sources while ignoring substantial knowledge redundancy, which
introduces noise into the answering process. To address this, we propose a
training-free framework with knowledge focusing for KB-VQA, that mitigates the
impact of noise by enhancing knowledge relevance and reducing redundancy.
First, for knowledge retrieval, our framework concludes essential parts from
the image-question pairs, creating low-noise queries that enhance the retrieval
of highly relevant knowledge. Considering that redundancy still persists in the
retrieved knowledge, we then prompt large models to identify and extract
answer-beneficial segments from knowledge. In addition, we introduce a
selective knowledge integration strategy, allowing the model to incorporate
knowledge only when it lacks confidence in answering the question, thereby
mitigating the influence of redundant information. Our framework enables the
acquisition of accurate and critical knowledge, and extensive experiments
demonstrate that it outperforms state-of-the-art methods.

</details>


### [99] [CWSSNet: Hyperspectral Image Classification Enhanced by Wavelet Domain Convolution](https://arxiv.org/abs/2509.09163)
*Yulin Tong,Fengzong Zhang,Haiqin Cheng*

Main category: cs.CV

TL;DR: This study addresses hyperspectral image classification challenges due to feature redundancy by proposing the CWSSNet framework, which integrates spectral-spatial features and wavelet convolution.


<details>
  <summary>Details</summary>
Motivation: Hyperspectral images offer high recognition accuracy but suffer from feature redundancy due to high dimensionality and spectral mixing.

Method: The CWSSNet framework uses multi-band decomposition, wavelet convolution, and multimodal convolutional attention modules to enhance classification performance.

Result: CWSSNet achieved high classification metrics in Yugan County, with mIoU of 74.50%, mAcc of 82.73%, and mF1 of 84.94% while demonstrating robustness even under limited training data.

Conclusion: CWSSNet proved effective for ground object classification in hyperspectral images, offering reliable performance and improving robustness under small-sample conditions.

Abstract: Hyperspectral remote sensing technology has significant application value in
fields such as forestry ecology and precision agriculture, while also putting
forward higher requirements for fine ground object classification. However,
although hyperspectral images are rich in spectral information and can improve
recognition accuracy, they tend to cause prominent feature redundancy due to
their numerous bands, high dimensionality, and spectral mixing characteristics.
To address this, this study used hyperspectral images from the ZY1F satellite
as a data source and selected Yugan County, Shangrao City, Jiangxi Province as
the research area to perform ground object classification research. A
classification framework named CWSSNet was proposed, which integrates 3D
spectral-spatial features and wavelet convolution. This framework integrates
multimodal information us-ing a multiscale convolutional attention module and
breaks through the classification performance bottleneck of traditional methods
by introducing multi-band decomposition and convolution operations in the
wavelet domain. The experiments showed that CWSSNet achieved 74.50\%, 82.73\%,
and 84.94\% in mean Intersection over Union (mIoU), mean Accuracy (mAcc), and
mean F1-score (mF1) respectively in Yugan County. It also obtained the highest
Intersection over Union (IoU) in the classifica-tion of water bodies,
vegetation, and bare land, demonstrating good robustness. Additionally, when
the training set proportion was 70\%, the increase in training time was
limited, and the classification effect was close to the optimal level,
indicating that the model maintains reliable performance under small-sample
training conditions.

</details>


### [100] [Bridging the Gap Between Ideal and Real-world Evaluation: Benchmarking AI-Generated Image Detection in Challenging Scenarios](https://arxiv.org/abs/2509.09172)
*Chunxiao Li,Xiaoxiao Wang,Meiling Li,Boming Miao,Peng Sun,Yunjian Zhang,Xiangyang Ji,Yao Zhu*

Main category: cs.CV

TL;DR: The paper introduces the Real-World Robustness Dataset (RRDataset) to evaluate AI-generated image detection under real-world scenarios, showing current models' limitations and the importance of human adaptability.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of current AI-generated image detection methods, which perform poorly under diverse real-world conditions, posing risks to digital security and media credibility.

Method: The authors created a dataset (RRDataset) with dimensions like scenario generalization, internet transmission robustness, and re-digitization robustness. They benchmarked 17 detectors and 10 vision-language models, as well as conducted a human study with 192 participants to evaluate detection challenges.

Result: The study revealed that existing AI detection methods faced significant limitations under the complex conditions of RRDataset, emphasizing the inadequacy of current models in real-world scenarios.

Conclusion: The findings highlight the need for more robust AI detection methods and the potential of leveraging human adaptability to improve detection performance in real-world contexts.

Abstract: With the rapid advancement of generative models, highly realistic image
synthesis has posed new challenges to digital security and media credibility.
Although AI-generated image detection methods have partially addressed these
concerns, a substantial research gap remains in evaluating their performance
under complex real-world conditions. This paper introduces the Real-World
Robustness Dataset (RRDataset) for comprehensive evaluation of detection models
across three dimensions: 1) Scenario Generalization: RRDataset encompasses
high-quality images from seven major scenarios (War and Conflict, Disasters and
Accidents, Political and Social Events, Medical and Public Health, Culture and
Religion, Labor and Production, and everyday life), addressing existing dataset
gaps from a content perspective. 2) Internet Transmission Robustness: examining
detector performance on images that have undergone multiple rounds of sharing
across various social media platforms. 3) Re-digitization Robustness: assessing
model effectiveness on images altered through four distinct re-digitization
methods. We benchmarked 17 detectors and 10 vision-language models (VLMs) on
RRDataset and conducted a large-scale human study involving 192 participants to
investigate human few-shot learning capabilities in detecting AI-generated
images. The benchmarking results reveal the limitations of current AI detection
methods under real-world conditions and underscore the importance of drawing on
human adaptability to develop more robust detection algorithms.

</details>


### [101] [Dark-ISP: Enhancing RAW Image Processing for Low-Light Object Detection](https://arxiv.org/abs/2509.09183)
*Jiasheng Guo,Xin Gao,Yuxiang Yan,Guanghao Li,Jian Pu*

Main category: cs.CV

TL;DR: This paper introduces Dark-ISP, a lightweight Image Signal Processing tool for improving low-light object detection from RAW images by recasting traditional ISP pipelines into differentiable modules optimized for detection tasks.


<details>
  <summary>Details</summary>
Motivation: Low-light object detection is important for real-world applications but is hindered by degraded image quality and limitations of current methods using RAW or RGB images.

Method: The authors developed Dark-ISP, which deconstructs ISP pipelines into linear and nonlinear differentiable modules optimized through task-driven losses. It includes physics-informed priors and a Self-Boost mechanism for sub-module cooperation in RAW-to-RGB conversion.

Result: The method was extensively evaluated on three RAW image datasets and demonstrated superior low-light object detection performance compared to state-of-the-art methods, using fewer parameters.

Conclusion: Dark-ISP achieves efficient, high-performing low-light object detection by optimizing ISP pipelines and leveraging task-driven adaptability, presenting a promising approach for real-world applications.

Abstract: Low-light Object detection is crucial for many real-world applications but
remains challenging due to degraded image quality. While recent studies have
shown that RAW images offer superior potential over RGB images, existing
approaches either use RAW-RGB images with information loss or employ complex
frameworks. To address these, we propose a lightweight and self-adaptive Image
Signal Processing (ISP) plugin, Dark-ISP, which directly processes Bayer RAW
images in dark environments, enabling seamless end-to-end training for object
detection. Our key innovations are: (1) We deconstruct conventional ISP
pipelines into sequential linear (sensor calibration) and nonlinear (tone
mapping) sub-modules, recasting them as differentiable components optimized
through task-driven losses. Each module is equipped with content-aware
adaptability and physics-informed priors, enabling automatic RAW-to-RGB
conversion aligned with detection objectives. (2) By exploiting the ISP
pipeline's intrinsic cascade structure, we devise a Self-Boost mechanism that
facilitates cooperation between sub-modules. Through extensive experiments on
three RAW image datasets, we demonstrate that our method outperforms
state-of-the-art RGB- and RAW-based detection approaches, achieving superior
results with minimal parameters in challenging low-light environments.

</details>


### [102] [VQualA 2025 Challenge on Visual Quality Comparison for Large Multimodal Models: Methods and Results](https://arxiv.org/abs/2509.09190)
*Hanwei Zhu,Haoning Wu,Zicheng Zhang,Lingyu Zhu,Yixuan Li,Peilin Chen,Shiqi Wang,Chris Wei Zhou,Linhan Cao,Wei Sun,Xiangyang Zhu,Weixia Zhang,Yucheng Zhu,Jing Liu,Dandan Zhu,Guangtao Zhai,Xiongkuo Min,Zhichao Zhang,Xinyue Li,Shubo Xu,Anh Dao,Yifan Li,Hongyuan Yu,Jiaojiao Yi,Yiding Tian,Yupeng Wu,Feiran Sun,Lijuan Liao,Song Jiang*

Main category: cs.CV

TL;DR: The VQualA 2025 Challenge evaluates and enhances large multimodal models (LMMs) for visual quality comparisons using a novel benchmark. It highlights emerging capabilities and encourages future research.


<details>
  <summary>Details</summary>
Motivation: To address the need for advanced visual quality reasoning and comparison capabilities in large multimodal models, facilitating higher precision and alignment with human judgment.

Method: Introduced a benchmark with thousands of visual quality comparison tasks using holistic evaluation protocols like 2AFC-based binary preference and multi-choice questions, fostering competition among diverse model submissions.

Result: 100 participants submitted entries showcasing the capabilities of instruction-tuned LMMs, with five models standing out in visual quality reasoning.

Conclusion: The challenge represents a pivotal advance in open-domain visual quality assessment, promoting human-aligned evaluations and further research in this area.

Abstract: This paper presents a summary of the VQualA 2025 Challenge on Visual Quality
Comparison for Large Multimodal Models (LMMs), hosted as part of the ICCV 2025
Workshop on Visual Quality Assessment. The challenge aims to evaluate and
enhance the ability of state-of-the-art LMMs to perform open-ended and detailed
reasoning about visual quality differences across multiple images. To this end,
the competition introduces a novel benchmark comprising thousands of
coarse-to-fine grained visual quality comparison tasks, spanning single images,
pairs, and multi-image groups. Each task requires models to provide accurate
quality judgments. The competition emphasizes holistic evaluation protocols,
including 2AFC-based binary preference and multi-choice questions (MCQs).
Around 100 participants submitted entries, with five models demonstrating the
emerging capabilities of instruction-tuned LMMs on quality assessment. This
challenge marks a significant step toward open-domain visual quality reasoning
and comparison and serves as a catalyst for future research on interpretable
and human-aligned quality evaluation systems.

</details>


### [103] [MGTraj: Multi-Granularity Goal-Guided Human Trajectory Prediction with Recursive Refinement Network](https://arxiv.org/abs/2509.09200)
*Ge Sun,Jun Ma*

Main category: cs.CV

TL;DR: The paper introduces MGTraj, an advanced model for human trajectory prediction, utilizing multi-granularity layers to improve the prediction process, achieving superior accuracy and outperforming existing approaches.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the need to enhance human trajectory prediction accuracy for applications like robotics navigation and autonomous driving. It focuses on addressing the unexplored potential of intermediate temporal granularity in goal-guided prediction tasks.

Method: MGTraj leverages multi-granularity trajectory modeling and recursively encodes trajectory proposals. It employs a transformer-based recursive refinement network (RRN), integrates features using weight-sharing, and utilizes velocity prediction as an auxiliary task.

Result: The proposed MGTraj model demonstrates state-of-the-art performance, outperforming baseline methods on well-known datasets such as ETH/UCY and Stanford Drone Dataset.

Conclusion: Integrating multi-granularity modeling into goal-guided frameworks can substantially improve human trajectory prediction. MGTraj proves effective in advancing accuracy and reliability in this domain.

Abstract: Accurate human trajectory prediction is crucial for robotics navigation and
autonomous driving. Recent research has demonstrated that incorporating goal
guidance significantly enhances prediction accuracy by reducing uncertainty and
leveraging prior knowledge. Most goal-guided approaches decouple the prediction
task into two stages: goal prediction and subsequent trajectory completion
based on the predicted goal, which operate at extreme granularities:
coarse-grained goal prediction forecasts the overall intention, while
fine-grained trajectory completion needs to generate the positions for all
future timesteps. The potential utility of intermediate temporal granularity
remains largely unexplored, which motivates multi-granularity trajectory
modeling. While prior work has shown that multi-granularity representations
capture diverse scales of human dynamics and motion patterns, effectively
integrating this concept into goal-guided frameworks remains challenging. In
this paper, we propose MGTraj, a novel Multi-Granularity goal-guided model for
human Trajectory prediction. MGTraj recursively encodes trajectory proposals
from coarse to fine granularity levels. At each level, a transformer-based
recursive refinement network (RRN) captures features and predicts progressive
refinements. Features across different granularities are integrated using a
weight-sharing strategy, and velocity prediction is employed as an auxiliary
task to further enhance performance. Comprehensive experimental results in
EHT/UCY and Stanford Drone Dataset indicate that MGTraj outperforms baseline
methods and achieves state-of-the-art performance among goal-guided methods.

</details>


### [104] [Medverse: A Universal Model for Full-Resolution 3D Medical Image Segmentation, Transformation and Enhancement](https://arxiv.org/abs/2509.09232)
*Jiesi Hu,Jianfeng Cao,Yanwu Yang,Chenfei Ye,Yixuan Zhang,Hanyang Peng,Ting Ma*

Main category: cs.CV

TL;DR: This paper introduces Medverse, a universal in-context learning (ICL) model for 3D medical imaging, designed for diverse tasks and outperforming existing ICL baselines.


<details>
  <summary>Details</summary>
Motivation: To overcome current limitations in medical image ICL models, which lack both high-fidelity predictions and comprehensive anatomical understanding, and to unify training across diverse imaging tasks and regions.

Method: Medverse employs a next-scale autoregressive ICL framework for multi-scale anatomical awareness and refines predictions progressively. Additionally, it incorporates a blockwise cross-attention module for efficient long-range interactions.

Result: Medverse shows significant improvements over existing ICL baselines through evaluations on varied datasets, including unseen clinical centers, organs, species, and imaging modalities.

Conclusion: Medverse establishes a new paradigm in medical imaging ICL, demonstrating its viability for diverse tasks and universal applicability with publicly shared resources.

Abstract: In-context learning (ICL) offers a promising paradigm for universal medical
image analysis, enabling models to perform diverse image processing tasks
without retraining. However, current ICL models for medical imaging remain
limited in two critical aspects: they cannot simultaneously achieve
high-fidelity predictions and global anatomical understanding, and there is no
unified model trained across diverse medical imaging tasks (e.g., segmentation
and enhancement) and anatomical regions. As a result, the full potential of ICL
in medical imaging remains underexplored. Thus, we present \textbf{Medverse}, a
universal ICL model for 3D medical imaging, trained on 22 datasets covering
diverse tasks in universal image segmentation, transformation, and enhancement
across multiple organs, imaging modalities, and clinical centers. Medverse
employs a next-scale autoregressive in-context learning framework that
progressively refines predictions from coarse to fine, generating consistent,
full-resolution volumetric outputs and enabling multi-scale anatomical
awareness. We further propose a blockwise cross-attention module that
facilitates long-range interactions between context and target inputs while
preserving computational efficiency through spatial sparsity. Medverse is
extensively evaluated on a broad collection of held-out datasets covering
previously unseen clinical centers, organs, species, and imaging modalities.
Results demonstrate that Medverse substantially outperforms existing ICL
baselines and establishes a novel paradigm for in-context learning. Code and
model weights will be made publicly available. Our model are publicly available
at https://github.com/jiesihu/Medverse.

</details>


### [105] [CoAtNeXt:An Attention-Enhanced ConvNeXtV2-Transformer Hybrid Model for Gastric Tissue Classification](https://arxiv.org/abs/2509.09242)
*Mustafa Yurdakul,Sakir Tasdemir*

Main category: cs.CV

TL;DR: The paper presents CoAtNeXt, a hybrid deep learning model aimed at improving the classification of gastric tissue images for aiding diagnosis of gastric diseases. The model outperforms existing CNN and ViT architectures across multiple metrics.


<details>
  <summary>Details</summary>
Motivation: The primary motivation is to address limitations of manual histopathologic diagnosis such as labor-intensity, inconsistency, and susceptibility to human error, which impact diagnostic reliability and efficiency.

Method: This paper introduces a novel hybrid model, CoAtNeXt, based on the CoAtNet architecture with enhancements including replacing MBConv layers with ConvNeXtV2 blocks and integrating CBAM for better local feature extraction.

Result: CoAtNeXt achieved high classification metrics, including up to 96.47% accuracy for multiclass classification and 98.29% accuracy for binary classification, surpassing competing architectures and previous studies.

Conclusion: CoAtNeXt demonstrates strong performance and reliability, making it a viable solution for automated histopathological gastric tissue diagnosis. It offers improved diagnostic accuracy and efficiency to support pathologists.

Abstract: Background and objective Early diagnosis of gastric diseases is crucial to
prevent fatal outcomes. Although histopathologic examination remains the
diagnostic gold standard, it is performed entirely manually, making evaluations
labor-intensive and prone to variability among pathologists. Critical findings
may be missed, and lack of standard procedures reduces consistency. These
limitations highlight the need for automated, reliable, and efficient methods
for gastric tissue analysis. Methods In this study, a novel hybrid model named
CoAtNeXt was proposed for the classification of gastric tissue images. The
model is built upon the CoAtNet architecture by replacing its MBConv layers
with enhanced ConvNeXtV2 blocks. Additionally, the Convolutional Block
Attention Module (CBAM) is integrated to improve local feature extraction
through channel and spatial attention mechanisms. The architecture was scaled
to achieve a balance between computational efficiency and classification
performance. CoAtNeXt was evaluated on two publicly available datasets,
HMU-GC-HE-30K for eight-class classification and GasHisSDB for binary
classification, and was compared against 10 Convolutional Neural Networks
(CNNs) and ten Vision Transformer (ViT) models. Results CoAtNeXt achieved
96.47% accuracy, 96.60% precision, 96.47% recall, 96.45% F1 score, and 99.89%
AUC on HMU-GC-HE-30K. On GasHisSDB, it reached 98.29% accuracy, 98.07%
precision, 98.41% recall, 98.23% F1 score, and 99.90% AUC. It outperformed all
CNN and ViT models tested and surpassed previous studies in the literature.
Conclusion Experimental results show that CoAtNeXt is a robust architecture for
histopathological classification of gastric tissue images, providing
performance on binary and multiclass. Its highlights its potential to assist
pathologists by enhancing diagnostic accuracy and reducing workload.

</details>


### [106] [Towards Better Dental AI: A Multimodal Benchmark and Instruction Dataset for Panoramic X-ray Analysis](https://arxiv.org/abs/2509.09254)
*Jing Hao,Yuxuan Fan,Yanpeng Sun,Kaixin Guo,Lizhuo Lin,Jinrong Yang,Qi Yong H. Ai,Lun M. Wong,Hao Tang,Kuo Feng Hung*

Main category: cs.CV

TL;DR: The paper introduces MMOral, a dataset for panoramic X-ray interpretation in dentistry, and OralGPT, a fine-tuned AI model showcasing performance improvements in this area.


<details>
  <summary>Details</summary>
Motivation: Existing large vision-language models struggle to interpret panoramic X-rays due to the complexity of anatomical structures and lack of specialized training data.

Method: The study developed MMOral, a multimodal dataset with annotated images and instruction-following instances, and OralGPT, a fine-tuned model trained on this dataset.

Result: Evaluation of 64 LVLMs shows limited performance, with OralGPT demonstrating significant accuracy improvements after supervised fine-tuning.

Conclusion: MMOral and OralGPT are promising tools for advancing AI applications in dentistry, addressing limitations in current models.

Abstract: Recent advances in large vision-language models (LVLMs) have demonstrated
strong performance on general-purpose medical tasks. However, their
effectiveness in specialized domains such as dentistry remains underexplored.
In particular, panoramic X-rays, a widely used imaging modality in oral
radiology, pose interpretative challenges due to dense anatomical structures
and subtle pathological cues, which are not captured by existing medical
benchmarks or instruction datasets. To this end, we introduce MMOral, the first
large-scale multimodal instruction dataset and benchmark tailored for panoramic
X-ray interpretation. MMOral consists of 20,563 annotated images paired with
1.3 million instruction-following instances across diverse task types,
including attribute extraction, report generation, visual question answering,
and image-grounded dialogue. In addition, we present MMOral-Bench, a
comprehensive evaluation suite covering five key diagnostic dimensions in
dentistry. We evaluate 64 LVLMs on MMOral-Bench and find that even the
best-performing model, i.e., GPT-4o, only achieves 41.45% accuracy, revealing
significant limitations of current models in this domain. To promote the
progress of this specific domain, we also propose OralGPT, which conducts
supervised fine-tuning (SFT) upon Qwen2.5-VL-7B with our meticulously curated
MMOral instruction dataset. Remarkably, a single epoch of SFT yields
substantial performance enhancements for LVLMs, e.g., OralGPT demonstrates a
24.73% improvement. Both MMOral and OralGPT hold significant potential as a
critical foundation for intelligent dentistry and enable more clinically
impactful multimodal AI systems in the dental field. The dataset, model,
benchmark, and evaluation suite are available at
https://github.com/isbrycee/OralGPT.

</details>


### [107] [DATE: Dynamic Absolute Time Enhancement for Long Video Understanding](https://arxiv.org/abs/2509.09263)
*Chao Yuan,Yang Yang,Yehui Yang,Zach Cheng*

Main category: cs.CV

TL;DR: The paper addresses the challenge of long video understanding in multimodal large language models (MLLMs) by proposing a novel approach, Dynamic Absolute Time Enhancement (DATE), for improved temporal reasoning and event localization.


<details>
  <summary>Details</summary>
Motivation: Existing MLLMs often fail to handle long-range dependencies effectively in video understanding tasks, resulting in information loss and poor temporal comprehension.

Method: The authors propose DATE, consisting of a Timestamp Injection Mechanism (TIM) and Temporal-Aware Similarity Sampling (TASS). TIM embeds temporal information using textual timestamp tokens, while TASS reformulates video sampling as a retrieval task with a two-stage algorithm for semantic and temporal relevance.

Result: DATE significantly improves temporal comprehension and key event localization, achieving state-of-the-art results across benchmarks with both 7B and 72B models. Remarkably, the 7B model surpasses some 72B models.

Conclusion: The proposed DATE framework advances precise temporal reasoning in MLLMs for long video understanding and showcases scalability across model sizes.

Abstract: Long video understanding remains a fundamental challenge for multimodal large
language models (MLLMs), particularly in tasks requiring precise temporal
reasoning and event localization. Existing approaches typically adopt uniform
frame sampling and rely on implicit position encodings to model temporal order.
However, these methods struggle with long-range dependencies, leading to
critical information loss and degraded temporal comprehension. In this paper,
we propose Dynamic Absolute Time Enhancement (DATE) that enhances temporal
awareness in MLLMs through the Timestamp Injection Mechanism (TIM) and a
semantically guided Temporal-Aware Similarity Sampling (TASS) strategy.
Specifically, we interleave video frame embeddings with textual timestamp
tokens to construct a continuous temporal reference system. We further
reformulate the video sampling problem as a vision-language retrieval task and
introduce a two-stage algorithm to ensure both semantic relevance and temporal
coverage: enriching each query into a descriptive caption to better align with
the vision feature, and sampling key event with a similarity-driven temporally
regularized greedy strategy. Our method achieves remarkable improvements w.r.t.
absolute time understanding and key event localization, resulting in
state-of-the-art performance among 7B and 72B models on hour-long video
benchmarks. Particularly, our 7B model even exceeds many 72B models on some
benchmarks.

</details>


### [108] [Unified Start, Personalized End: Progressive Pruning for Efficient 3D Medical Image Segmentation](https://arxiv.org/abs/2509.09267)
*Linhao Li,Yiwen Ye,Ziyang Chen,Yong Xia*

Main category: cs.CV

TL;DR: PSP-Seg is a dynamic progressive pruning framework for efficient 3D medical image segmentation, greatly reducing resource consumption while maintaining high performance.


<details>
  <summary>Details</summary>
Motivation: The paper addresses heavy resource and time consumption issues in existing 3D medical image segmentation models, which hinders rapid deployment in clinical environments.

Method: PSP-Seg uses a progressive pruning framework that starts from a redundant model, employing block-wise pruning and a functional decoupling loss to iteratively prune unnecessary components.

Result: PSP-Seg achieves comparable performance to nnU-Net while significantly reducing GPU memory usage, training time, and parameters on five public datasets.

Conclusion: PSP-Seg is demonstrated as a cost-effective and high-performing framework suitable for widespread clinical use in scalable and resource-efficient 3D segmentation.

Abstract: 3D medical image segmentation often faces heavy resource and time
consumption, limiting its scalability and rapid deployment in clinical
environments. Existing efficient segmentation models are typically static and
manually designed prior to training, which restricts their adaptability across
diverse tasks and makes it difficult to balance performance with resource
efficiency. In this paper, we propose PSP-Seg, a progressive pruning framework
that enables dynamic and efficient 3D segmentation. PSP-Seg begins with a
redundant model and iteratively prunes redundant modules through a combination
of block-wise pruning and a functional decoupling loss. We evaluate PSP-Seg on
five public datasets, benchmarking it against seven state-of-the-art models and
six efficient segmentation models. Results demonstrate that the lightweight
variant, PSP-Seg-S, achieves performance on par with nnU-Net while reducing GPU
memory usage by 42-45%, training time by 29-48%, and parameter number by 83-87%
across all datasets. These findings underscore PSP-Seg's potential as a
cost-effective yet high-performing alternative for widespread clinical
application.

</details>


### [109] [Visual Programmability: A Guide for Code-as-Thought in Chart Understanding](https://arxiv.org/abs/2509.09286)
*Bohao Tang,Yan Ma,Fei Zhang,Jiadi Su,Ethan Chern,Zhulin Hu,Zhixin Wang,Pengfei Liu,Ya Zhang*

Main category: cs.CV

TL;DR: The paper introduces a novel Code-as-Thought (CaT) approach, complemented by Visual Programmability, enabling Vision-Language Models (VLMs) to dynamically select between symbolic coding and direct visual analysis for robust chart understanding.


<details>
  <summary>Details</summary>
Motivation: To address the inherent limitations of existing methods, which are either reliant on external tools or constrained to single reasoning strategies, complicating factual accuracy verification in chart understanding.

Method: The authors propose an adaptive framework implementing Code-as-Thought (CaT) combined with Visual Programmability. A reinforcement learning-based dual-reward system trains the model to decide between symbolic code reasoning and direct visual analysis for chart-question pairs.

Result: Experiments showcase superior performance and robustness of the proposed framework across various chart understanding benchmarks.

Conclusion: The work demonstrates that VLMs can be effectively taught to dynamically adapt their reasoning strategies based on the task, improving their reasoning capabilities and factual accuracy.

Abstract: Chart understanding presents a critical test to the reasoning capabilities of
Vision-Language Models (VLMs). Prior approaches face critical limitations: some
rely on external tools, making them brittle and constrained by a predefined
toolkit, while others fine-tune specialist models that often adopt a single
reasoning strategy, such as text-based chain-of-thought (CoT). The intermediate
steps of text-based reasoning are difficult to verify, which complicates the
use of reinforcement-learning signals that reward factual accuracy. To address
this, we propose a Code-as-Thought (CaT) approach to represent the visual
information of a chart in a verifiable, symbolic format. Our key insight is
that this strategy must be adaptive: a fixed, code-only implementation
consistently fails on complex charts where symbolic representation is
unsuitable. This finding leads us to introduce Visual Programmability: a
learnable property that determines if a chart-question pair is better solved
with code or direct visual analysis. We implement this concept in an adaptive
framework where a VLM learns to choose between the CaT pathway and a direct
visual reasoning pathway. The selection policy of the model is trained with
reinforcement learning using a novel dual-reward system. This system combines a
data-accuracy reward to ground the model in facts and prevent numerical
hallucination, with a decision reward that teaches the model when to use each
strategy, preventing it from defaulting to a single reasoning mode. Experiments
demonstrate strong and robust performance across diverse chart-understanding
benchmarks. Our work shows that VLMs can be taught not only to reason but also
how to reason, dynamically selecting the optimal reasoning pathway for each
task.

</details>


### [110] [Modality-Agnostic Input Channels Enable Segmentation of Brain lesions in Multimodal MRI with Sequences Unavailable During Training](https://arxiv.org/abs/2509.09290)
*Anthony P. Addison,Felix Wagner,Wentian Xu,Natalie Voets,Konstantinos Kamnitsas*

Main category: cs.CV

TL;DR: The paper introduces an enhanced U-net architecture for brain MRI segmentation that accommodates both previously encountered and unseen modalities.


<details>
  <summary>Details</summary>
Motivation: Current segmentation models for multimodal brain MRI either fail to generalize to new modalities or lose discriminative modality-specific information.

Method: The paper integrates a modality-agnostic input channel within the U-net model and employs an image augmentation scheme to synthesize artificial MRI modalities.

Result: The method was tested on 8 datasets covering 5 brain pathologies and 8 MRI modalities, demonstrating effectiveness in processing both seen and unseen modalities.

Conclusion: This architecture enables robust segmentation across heterogeneous MRI modalities, offering enhanced flexibility and practicality for diverse imaging data.

Abstract: Segmentation models are important tools for the detection and analysis of
lesions in brain MRI. Depending on the type of brain pathology that is imaged,
MRI scanners can acquire multiple, different image modalities (contrasts). Most
segmentation models for multimodal brain MRI are restricted to fixed modalities
and cannot effectively process new ones at inference. Some models generalize to
unseen modalities but may lose discriminative modality-specific information.
This work aims to develop a model that can perform inference on data that
contain image modalities unseen during training, previously seen modalities,
and heterogeneous combinations of both, thus allowing a user to utilize any
available imaging modalities. We demonstrate this is possible with a simple,
thus practical alteration to the U-net architecture, by integrating a
modality-agnostic input channel or pathway, alongside modality-specific input
channels. To train this modality-agnostic component, we develop an image
augmentation scheme that synthesizes artificial MRI modalities. Augmentations
differentially alter the appearance of pathological and healthy brain tissue to
create artificial contrasts between them while maintaining realistic anatomical
integrity. We evaluate the method using 8 MRI databases that include 5 types of
pathologies (stroke, tumours, traumatic brain injury, multiple sclerosis and
white matter hyperintensities) and 8 modalities (T1, T1+contrast, T2, PD, SWI,
DWI, ADC and FLAIR). The results demonstrate that the approach preserves the
ability to effectively process MRI modalities encountered during training,
while being able to process new, unseen modalities to improve its segmentation.
Project code: https://github.com/Anthony-P-Addison/AGN-MOD-SEG

</details>


### [111] [Learning Object-Centric Representations in SAR Images with Multi-Level Feature Fusion](https://arxiv.org/abs/2509.09298)
*Oh-Tae Jang,Min-Gon Cho,Kyung-Tae Kim*

Main category: cs.CV

TL;DR: The paper addresses the challenge of disentangling target representations from background clutter in SAR images by proposing a novel object-centric learning (OCL) framework called SlotSAR, achieving superior performance in SAR imagery.


<details>
  <summary>Details</summary>
Motivation: SAR images often contain background clutter that resembles targets, leading to spurious feature extraction and undermining target representation quality.

Method: SlotSAR combines high-level semantic features from SARATR-X and low-level scattering features from a wavelet scattering network, integrating them via a multi-level slot attention module to enhance representation distinctiveness.

Result: SlotSAR demonstrates state-of-the-art performance in SAR imagery by effectively disentangling targets and preserving structural details.

Conclusion: The study's SlotSAR method effectively improves target characterization in SAR images, outperforming existing OCL methods while tackling background clutter issues.

Abstract: Synthetic aperture radar (SAR) images contain not only targets of interest
but also complex background clutter, including terrain reflections and speckle
noise. In many cases, such clutter exhibits intensity and patterns that
resemble targets, leading models to extract entangled or spurious features.
Such behavior undermines the ability to form clear target representations,
regardless of the classifier. To address this challenge, we propose a novel
object-centric learning (OCL) framework, named SlotSAR, that disentangles
target representations from background clutter in SAR images without mask
annotations. SlotSAR first extracts high-level semantic features from SARATR-X
and low-level scattering features from the wavelet scattering network in order
to obtain complementary multi-level representations for robust target
characterization. We further present a multi-level slot attention module that
integrates these low- and high-level features to enhance slot-wise
representation distinctiveness, enabling effective OCL. Experimental results
demonstrate that SlotSAR achieves state-of-the-art performance in SAR imagery
by preserving structural details compared to existing OCL methods.

</details>


### [112] [Can Multimodal LLMs See Materials Clearly? A Multimodal Benchmark on Materials Characterization](https://arxiv.org/abs/2509.09307)
*Zhengzhao Lai,Youbin Zheng,Zhenyang Cai,Haonan Lyu,Jinpu Yang,Hongqing Liang,Yan Hu,Benyou Wang*

Main category: cs.CV

TL;DR: The paper introduces MatCha, a benchmark designed to test materials characterization image understanding using multimodal large language models (MLLMs), revealing their current limitations in performance compared to human experts.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this work is to address the gap in evaluating MLLMs' ability to understand real-world materials characterization imaging data, which is essential for materials research.

Method: The authors developed MatCha, a benchmark comprising 1,500 expert-level questions across four key materials research stages and 21 tasks, designed to reflect real-world challenges. They evaluated state-of-the-art MLLMs using these tasks.

Result: The study found that current MLLMs significantly underperform when compared to human experts, especially for tasks requiring advanced expertise and visual understanding. Methods like few-shot and chain-of-thought prompting showed limited improvements.

Conclusion: MatCha highlights the current adaptability limitations of MLLMs in real-world materials characterization. The benchmark aims to inspire future advancements in material discovery and autonomous scientific applications.

Abstract: Materials characterization is fundamental to acquiring materials information,
revealing the processing-microstructure-property relationships that guide
material design and optimization. While multimodal large language models
(MLLMs) have recently shown promise in generative and predictive tasks within
materials science, their capacity to understand real-world characterization
imaging data remains underexplored. To bridge this gap, we present MatCha, the
first benchmark for materials characterization image understanding, comprising
1,500 questions that demand expert-level domain expertise. MatCha encompasses
four key stages of materials research comprising 21 distinct tasks, each
designed to reflect authentic challenges faced by materials scientists. Our
evaluation of state-of-the-art MLLMs on MatCha reveals a significant
performance gap compared to human experts. These models exhibit degradation
when addressing questions requiring higher-level expertise and sophisticated
visual perception. Simple few-shot and chain-of-thought prompting struggle to
alleviate these limitations. These findings highlight that existing MLLMs still
exhibit limited adaptability to real-world materials characterization
scenarios. We hope MatCha will facilitate future research in areas such as new
material discovery and autonomous scientific agents. MatCha is available at
https://github.com/FreedomIntelligence/MatCha.

</details>


### [113] [You Share Beliefs, I Adapt: Progressive Heterogeneous Collaborative Perception](https://arxiv.org/abs/2509.09310)
*Hao Si,Ehsan Javanmardi,Manabu Tsukada*

Main category: cs.CV

TL;DR: PHCP is a framework that addresses real-time heterogeneous collaborative perception by dynamically aligning features during inference without prior joint training or labeled data.


<details>
  <summary>Details</summary>
Motivation: To overcome real-world collaborative perception challenges where models on different vehicles are heterogeneous, existing solutions require joint training or storing multiple models, which are impractical.

Method: The proposed PHCP framework introduces few-shot unsupervised domain adaptation. It self-trains an adapter to dynamically align features during inference, bypassing the need for labeled data or pre-coordinated training.

Result: Experiments on the OPV2V dataset show that PHCP performs on par with state-of-the-art methods that rely on comprehensive datasets, despite only using a small amount of unlabeled data.

Conclusion: PHCP offers a practical and efficient solution for heterogeneous collaborative perception by eliminating the necessity for prior training and leveraging inference adaptability.

Abstract: Collaborative perception enables vehicles to overcome individual perception
limitations by sharing information, allowing them to see further and through
occlusions. In real-world scenarios, models on different vehicles are often
heterogeneous due to manufacturer variations. Existing methods for
heterogeneous collaborative perception address this challenge by fine-tuning
adapters or the entire network to bridge the domain gap. However, these methods
are impractical in real-world applications, as each new collaborator must
undergo joint training with the ego vehicle on a dataset before inference, or
the ego vehicle stores models for all potential collaborators in advance.
Therefore, we pose a new question: Can we tackle this challenge directly during
inference, eliminating the need for joint training? To answer this, we
introduce Progressive Heterogeneous Collaborative Perception (PHCP), a novel
framework that formulates the problem as few-shot unsupervised domain
adaptation. Unlike previous work, PHCP dynamically aligns features by
self-training an adapter during inference, eliminating the need for labeled
data and joint training. Extensive experiments on the OPV2V dataset demonstrate
that PHCP achieves strong performance across diverse heterogeneous scenarios.
Notably, PHCP achieves performance comparable to SOTA methods trained on the
entire dataset while using only a small amount of unlabeled data.

</details>


### [114] [Image Recognition with Vision and Language Embeddings of VLMs](https://arxiv.org/abs/2509.09311)
*Illia Volkov,Nikita Kisel,Klara Janouskova,Jiri Matas*

Main category: cs.CV

TL;DR: The paper evaluates vision-language models (VLMs) for both language-guided and vision-only image classification, using benchmarks and introducing a simple fusion method to enhance classification performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to explore and improve the classification capabilities of VLMs, especially in scenarios where purely visual inference is required and to understand how visual and language components complement each other.

Method: Comprehensive evaluation on dual-encoder VLMs using the ImageNet-1k validation dataset, analyzing factors like prompt design, class diversity, k-NN neighbors, and reference set size. A fusion method based on per-class precision is introduced.

Result: Findings reveal that visual and language components in VLMs exhibit complementary abilities in classification tasks and that the fusion method enhances overall performance.

Conclusion: The study underscores the importance of leveraging both vision and language modalities for optimal image classification, while the proposed fusion method addresses the shortcomings of isolated modalities effectively.

Abstract: Vision-language models (VLMs) have enabled strong zero-shot classification
through image-text alignment. Yet, their purely visual inference capabilities
remain under-explored. In this work, we conduct a comprehensive evaluation of
both language-guided and vision-only image classification with a diverse set of
dual-encoder VLMs, including both well-established and recent models such as
SigLIP 2 and RADIOv2.5. The performance is compared in a standard setup on the
ImageNet-1k validation set and its label-corrected variant. The key factors
affecting accuracy are analysed, including prompt design, class diversity, the
number of neighbours in k-NN, and reference set size. We show that language and
vision offer complementary strengths, with some classes favouring textual
prompts and others better handled by visual similarity. To exploit this
complementarity, we introduce a simple, learning-free fusion method based on
per-class precision that improves classification performance. The code is
available at: https://github.com/gonikisgo/bmvc2025-vlm-image-recognition.

</details>


### [115] [Fine-Grained Customized Fashion Design with Image-into-Prompt benchmark and dataset from LMM](https://arxiv.org/abs/2509.09324)
*Hui Li,Yi You,Qiqi Chen,Bingfeng Zhang,George Q. Huang*

Main category: cs.CV

TL;DR: Generative AI with multimodal models aids in fashion design, addressing customization challenges via the BUG workflow and introducing a novel FashionEdit dataset.


<details>
  <summary>Details</summary>
Motivation: Introduce a solution to overcome limitations of generative AI in precise, fine-grained fashion designs by addressing text uncertainty and enabling better design interaction.

Method: Propose the Better Understanding Generation (BUG) workflow leveraging large multimodal models and create the FashionEdit dataset for evaluation.

Result: The framework enables automatic fine-grained design customization and proves effective through evaluation on user satisfaction, similarity, and quality.

Conclusion: The BUG workflow enhances creative design in fashion, reduces barriers, and eliminates further human involvement, demonstrating the model's practicality with shared code and dataset.

Abstract: Generative AI evolves the execution of complex workflows in industry, where
the large multimodal model empowers fashion design in the garment industry.
Current generation AI models magically transform brainstorming into fancy
designs easily, but the fine-grained customization still suffers from text
uncertainty without professional background knowledge from end-users. Thus, we
propose the Better Understanding Generation (BUG) workflow with LMM to
automatically create and fine-grain customize the cloth designs from chat with
image-into-prompt. Our framework unleashes users' creative potential beyond
words and also lowers the barriers of clothing design/editing without further
human involvement. To prove the effectiveness of our model, we propose a new
FashionEdit dataset that simulates the real-world clothing design workflow,
evaluated from generation similarity, user satisfaction, and quality. The code
and dataset: https://github.com/detectiveli/FashionEdit.

</details>


### [116] [Exploring Pre-training Across Domains for Few-Shot Surgical Skill Assessment](https://arxiv.org/abs/2509.09327)
*Dimitrios Anastasiou,Razvan Caramalau,Nazir Sirajudeen,Matthew Boal,Philip Edwards,Justin Collins,John Kelly,Ashwin Sridhar,Maxine Tran,Faiz Mumtaz,Nevil Pavithran,Nader Francis,Danail Stoyanov,Evangelos B. Mazomenos*

Main category: cs.CV

TL;DR: The paper explores Few-Shot Learning (FSL) for automated surgical skill assessment (SSA), highlighting the importance of domain-relevant pre-training strategies.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the lack of scalable methods for surgical skill assessment due to limited annotated data and investigates Few-Shot Learning as a solution.

Method: The authors annotated a surgical dataset with skill assessment scores, evaluated various self-supervised pre-training strategies, and measured their impact on few-shot SSA performance across domain similarity and procedure-specific data inclusion.

Result: Smaller, domain-relevant datasets outperformed larger, less aligned ones in few-shot settings with accuracies of up to 73.65%, while procedure-specific pre-training boosted performance by improving both accuracy and F1-score.

Conclusion: Pre-training with domain-relevant and procedure-specific datasets is crucial for enhancing Few-Shot Learning-based SSA systems, whereas large-scale but less similar data can degrade performance.

Abstract: Automated surgical skill assessment (SSA) is a central task in surgical
computer vision. Developing robust SSA models is challenging due to the
scarcity of skill annotations, which are time-consuming to produce and require
expert consensus. Few-shot learning (FSL) offers a scalable alternative
enabling model development with minimal supervision, though its success
critically depends on effective pre-training. While widely studied for several
surgical downstream tasks, pre-training has remained largely unexplored in SSA.
In this work, we formulate SSA as a few-shot task and investigate how
self-supervised pre-training strategies affect downstream few-shot SSA
performance. We annotate a publicly available robotic surgery dataset with
Objective Structured Assessment of Technical Skill (OSATS) scores, and evaluate
various pre-training sources across three few-shot settings. We quantify domain
similarity and analyze how domain gap and the inclusion of procedure-specific
data into pre-training influence transferability. Our results show that small
but domain-relevant datasets can outperform large scale, less aligned ones,
achieving accuracies of 60.16%, 66.03%, and 73.65% in the 1-, 2-, and 5-shot
settings, respectively. Moreover, incorporating procedure-specific data into
pre-training with a domain-relevant external dataset significantly boosts
downstream performance, with an average gain of +1.22% in accuracy and +2.28%
in F1-score; however, applying the same strategy with less similar but
large-scale sources can instead lead to performance degradation. Code and
models are available at https://github.com/anastadimi/ssa-fsl.

</details>


### [117] [Texture-aware Intrinsic Image Decomposition with Model- and Learning-based Priors](https://arxiv.org/abs/2509.09352)
*Xiaodong Wang,Zijun He,Xin Yuan*

Main category: cs.CV

TL;DR: This paper introduces a new method to improve intrinsic image decomposition by using texture-guided regularization to handle complex lighting and textures in real-world images, achieving better results than existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing approaches to intrinsic image decomposition struggle with complex scenes containing spatially varying lighting and rich textures. Additionally, learning-based methods often create overly smooth or texture-less outputs.

Method: The authors propose a novel texture-guided regularization term within an optimization framework, enabling better separation of texture and lighting effects during the decomposition process.

Result: The proposed method produces higher-quality intrinsic images compared to previous approaches, successfully managing severe lighting and rich textural challenges.

Conclusion: Incorporating a texture-aware prior into the decomposition framework leads to significant improvements, making intrinsic image decomposition more effective for real-world applications.

Abstract: This paper aims to recover the intrinsic reflectance layer and shading layer
given a single image. Though this intrinsic image decomposition problem has
been studied for decades, it remains a significant challenge in cases of
complex scenes, i.e. spatially-varying lighting effect and rich textures. In
this paper, we propose a novel method for handling severe lighting and rich
textures in intrinsic image decomposition, which enables to produce
high-quality intrinsic images for real-world images. Specifically, we observe
that previous learning-based methods tend to produce texture-less and
over-smoothing intrinsic images, which can be used to infer the lighting and
texture information given a RGB image. In this way, we design a texture-guided
regularization term and formulate the decomposition problem into an
optimization framework, to separate the material textures and lighting effect.
We demonstrate that combining the novel texture-aware prior can produce
superior results to existing approaches.

</details>


### [118] [Plug-and-play Diffusion Models for Image Compressive Sensing with Data Consistency Projection](https://arxiv.org/abs/2509.09365)
*Xiaodong Wang,Ping Wang,Zhangyuan Li,Xin Yuan*

Main category: cs.CV

TL;DR: The paper explores connections between Plug-and-Play (PnP) and Denoising Diffusion Implicit Models (DDIM), introducing a hybrid module for single-pixel imaging tasks.


<details>
  <summary>Details</summary>
Motivation: To unify learned priors with physical models for solving ill-posed inverse problems like single-pixel imaging, and improve upon existing denoising and sampling techniques.

Method: The paper decouples the diffusion process into three stages (denoising, data consistency, and sampling) and introduces a hybrid data-consistency module combining multiple PnP-style fidelity terms.

Result: Experimental results show improved reconstruction quality in single-pixel imaging tasks with the proposed method.

Conclusion: The proposed hybrid approach successfully enhances data consistency and reconstruction quality without altering the diffusion process.

Abstract: We explore the connection between Plug-and-Play (PnP) methods and Denoising
Diffusion Implicit Models (DDIM) for solving ill-posed inverse problems, with a
focus on single-pixel imaging. We begin by identifying key distinctions between
PnP and diffusion models-particularly in their denoising mechanisms and
sampling procedures. By decoupling the diffusion process into three
interpretable stages: denoising, data consistency enforcement, and sampling, we
provide a unified framework that integrates learned priors with physical
forward models in a principled manner. Building upon this insight, we propose a
hybrid data-consistency module that linearly combines multiple PnP-style
fidelity terms. This hybrid correction is applied directly to the denoised
estimate, improving measurement consistency without disrupting the diffusion
sampling trajectory. Experimental results on single-pixel imaging tasks
demonstrate that our method achieves better reconstruction quality.

</details>


### [119] [A Fully Automatic Framework for Intracranial Pressure Grading: Integrating Keyframe Identification, ONSD Measurement and Clinical Data](https://arxiv.org/abs/2509.09368)
*Pengxu Wen,Tingting Yu,Ziwei Nie,Cheng Jiang,Zhenyu Yin,Mingyang He,Bo Liao,Xiaoping Yang*

Main category: cs.CV

TL;DR: The authors propose a non-invasive, fully automatic two-stage framework for intracranial pressure (ICP) grading using optic nerve sheath diameter (ONSD) measurements from fundus ultrasound videos and clinical data. It achieves higher accuracy compared to traditional methods.


<details>
  <summary>Details</summary>
Motivation: Current invasive ICP measurement methods like lumbar puncture pose risks, and non-invasive alternatives such as ONSD measurements are limited by manual operation inconsistency and subjectivity, necessitating an automated, reliable solution.

Method: The framework includes (1) fundus ultrasound video processing for keyframe identification and ONSD measurement via anatomical segmentation and rule-based approaches, and (2) the fusion of ONSD metrics with clinical data to predict ICP grades.

Result: The method demonstrated a validation accuracy of 0.845 ± 0.071 and test accuracy of 0.786, outperforming conventional threshold-based methods significantly.

Conclusion: The approach effectively reduces operator variability and integrates multi-source data, providing a reliable and non-invasive solution for clinical ICP evaluation, with potential benefits for managing acute neurological conditions.

Abstract: Intracranial pressure (ICP) elevation poses severe threats to cerebral
function, thus necessitating monitoring for timely intervention. While lumbar
puncture is the gold standard for ICP measurement, its invasiveness and
associated risks drive the need for non-invasive alternatives. Optic nerve
sheath diameter (ONSD) has emerged as a promising biomarker, as elevated ICP
directly correlates with increased ONSD. However, current clinical practices
for ONSD measurement suffer from inconsistency in manual operation,
subjectivity in optimal view selection, and variability in thresholding,
limiting their reliability. To address these challenges, we introduce a fully
automatic two-stage framework for ICP grading, integrating keyframe
identification, ONSD measurement and clinical data. Specifically, the fundus
ultrasound video processing stage performs frame-level anatomical segmentation,
rule-based keyframe identification guided by an international consensus
statement, and precise ONSD measurement. The intracranial pressure grading
stage then fuses ONSD metrics with clinical features to enable the prediction
of ICP grades, thereby demonstrating an innovative blend of interpretable
ultrasound analysis and multi-source data integration for objective clinical
evaluation. Experimental results demonstrate that our method achieves a
validation accuracy of $0.845 \pm 0.071$ (with standard deviation from
five-fold cross-validation) and an independent test accuracy of 0.786,
significantly outperforming conventional threshold-based method ($0.637 \pm
0.111$ validation accuracy, $0.429$ test accuracy). Through effectively
reducing operator variability and integrating multi-source information, our
framework establishes a reliable non-invasive approach for clinical ICP
evaluation, holding promise for improving patient management in acute
neurological conditions.

</details>


### [120] [FLUX-Reason-6M & PRISM-Bench: A Million-Scale Text-to-Image Reasoning Dataset and Comprehensive Benchmark](https://arxiv.org/abs/2509.09680)
*Rongyao Fang,Aldrich Yu,Chengqi Duan,Linjiang Huang,Shuai Bai,Yuxuan Cai,Kun Wang,Si Liu,Xihui Liu,Hongsheng Li*

Main category: cs.CV

TL;DR: This paper introduces FLUX-Reason-6M and PRISM-Bench, aiming to advance reasoning-focused text-to-image models by providing a large dataset and novel evaluation benchmarks.


<details>
  <summary>Details</summary>
Motivation: The lack of large-scale reasoning datasets and evaluation benchmarks has limited the development of open-source text-to-image models compared to closed-source systems.

Method: The authors created FLUX-Reason-6M, consisting of 6 million images and 20 million descriptions organized across six reasoning characteristics, and PRISM-Bench for robust evaluation using seven tracks and advanced vision-language models.

Result: Evaluation of 19 models with PRISM-Bench exposed performance gaps and pinpointed areas needing improvement in reasoning-focused T2I representation.

Conclusion: By releasing the dataset, benchmark, and code, the paper aims to stimulate future research in reasoning-focused T2I generation for the community.

Abstract: The advancement of open-source text-to-image (T2I) models has been hindered
by the absence of large-scale, reasoning-focused datasets and comprehensive
evaluation benchmarks, resulting in a performance gap compared to leading
closed-source systems. To address this challenge, We introduce FLUX-Reason-6M
and PRISM-Bench (Precise and Robust Image Synthesis Measurement Benchmark).
FLUX-Reason-6M is a massive dataset consisting of 6 million high-quality
FLUX-generated images and 20 million bilingual (English and Chinese)
descriptions specifically designed to teach complex reasoning. The image are
organized according to six key characteristics: Imagination, Entity, Text
rendering, Style, Affection, and Composition, and design explicit Generation
Chain-of-Thought (GCoT) to provide detailed breakdowns of image generation
steps. The whole data curation takes 15,000 A100 GPU days, providing the
community with a resource previously unattainable outside of large industrial
labs. PRISM-Bench offers a novel evaluation standard with seven distinct
tracks, including a formidable Long Text challenge using GCoT. Through
carefully designed prompts, it utilizes advanced vision-language models for
nuanced human-aligned assessment of prompt-image alignment and image
aesthetics. Our extensive evaluation of 19 leading models on PRISM-Bench
reveals critical performance gaps and highlights specific areas requiring
improvement. Our dataset, benchmark, and evaluation code are released to
catalyze the next wave of reasoning-oriented T2I generation. Project page:
https://flux-reason-6m.github.io/ .

</details>


### [121] [Unsupervised Integrated-Circuit Defect Segmentation via Image-Intrinsic Normality](https://arxiv.org/abs/2509.09375)
*Botong Zhao,Qijun Shi,Shujing Lyu,Yue Lu*

Main category: cs.CV

TL;DR: The paper introduces an unsupervised IC defect segmentation method that eliminates the need for external normal references, achieving better reliability and consistency.


<details>
  <summary>Details</summary>
Motivation: Industrial IC defect segmentation relies on external normal datasets, which are unreliable due to IC layout variations and alignment challenges.

Method: The framework employs a learnable extractor for normal features, coherence loss for normal-region association, a decoder reconstructing normal content, and uses residuals for defect segmentation. Pseudo-anomalies are used to stabilize training.

Result: Experimental validation across three IC process stages demonstrates superior performance and robust adaptability compared to existing solutions.

Conclusion: The method enhances reliability in IC defect segmentation, showing potential for improved yield and adaptability in manufacturing processes.

Abstract: Modern Integrated-Circuit(IC) manufacturing introduces diverse, fine-grained
defects that depress yield and reliability. Most industrial defect segmentation
compares a test image against an external normal set, a strategy that is
brittle for IC imagery where layouts vary across products and accurate
alignment is difficult. We observe that defects are predominantly local, while
each image still contains rich, repeatable normal patterns. We therefore
propose an unsupervised IC defect segmentation framework that requires no
external normal support. A learnable normal-information extractor aggregates
representative normal features from the test image, and a coherence loss
enforces their association with normal regions. Guided by these features, a
decoder reconstructs only normal content; the reconstruction residual then
segments defects. Pseudo-anomaly augmentation further stabilizes training.
Experiments on datasets from three IC process stages show consistent
improvements over existing approaches and strong robustness to product
variability.

</details>


### [122] [Decoupling Clinical and Class-Agnostic Features for Reliable Few-Shot Adaptation under Shift](https://arxiv.org/abs/2509.09397)
*Umaima Rahman,Raza Imam,Mohammad Yaqub,Dwarikanath Mahapatra*

Main category: cs.CV

TL;DR: Medical vision-language models (VLMs) are enhanced using DRiFt, a framework for separating clinically relevant data from noise, improving performance and robustness across datasets.


<details>
  <summary>Details</summary>
Motivation: The motivation of the paper is to address the challenge of ensuring reliability and safety of medical vision-language models in clinical settings, especially under distribution shifts caused by variability in imaging protocols and reports.

Method: The paper introduces DRiFt, a structured feature decoupling framework employing parameter-efficient tuning (LoRA) and learnable prompt tokens, alongside high-quality curated image-text pairs.

Result: The model achieves +11.4% Top-1 accuracy and +3.3% Macro-F1 in in-distribution tasks, while maintaining robustness across unseen datasets.

Conclusion: The DRiFt framework, by disentangling task-relevant features and improving cross-modal alignment, contributes to safer and generalizable medical vision-language models, enhancing their utility in clinical decision support.

Abstract: Medical vision-language models (VLMs) offer promise for clinical decision
support, yet their reliability under distribution shifts remains a major
concern for safe deployment. These models often learn task-agnostic
correlations due to variability in imaging protocols and free-text reports,
limiting their generalizability and increasing the risk of failure in
real-world settings. We propose DRiFt, a structured feature decoupling
framework that explicitly separates clinically relevant signals from
task-agnostic noise using parameter-efficient tuning (LoRA) and learnable
prompt tokens. To enhance cross-modal alignment and reduce uncertainty, we
curate high-quality, clinically grounded image-text pairs by generating
captions for a diverse medical dataset. Our approach improves in-distribution
performance by +11.4% Top-1 accuracy and +3.3% Macro-F1 over prior prompt-based
methods, while maintaining strong robustness across unseen datasets. Ablation
studies reveal that disentangling task-relevant features and careful alignment
significantly enhance model generalization and reduce unpredictable behavior
under domain shift. These insights contribute toward building safer, more
trustworthy VLMs for clinical use. The code is available at
https://github.com/rumaima/DRiFt.

</details>


### [123] [FS-Diff: Semantic guidance and clarity-aware simultaneous multimodal image fusion and super-resolution](https://arxiv.org/abs/2509.09427)
*Yuchan Jie,Yushen Xu,Xiaosong Li,Fuqiang Zhou,Jianming Lv,Huafeng Li*

Main category: cs.CV

TL;DR: The paper introduces FS-Diff, a novel method combining image fusion and super-resolution using semantic guidance and clarity-aware mechanisms.


<details>
  <summary>Details</summary>
Motivation: Existing techniques for joint image fusion and super-resolution struggle with degraded targets and weak semantic information in real-world applications, such as military reconnaissance, resulting in subpar outcomes.

Method: FS-Diff treats image fusion and super-resolution as a conditional generation task. It incorporates a clarity sensing mechanism and uses the bidirectional feature Mamba for global feature extraction. A modified U-Net is employed for iterative denoising to generate high-resolution fusion results.

Result: FS-Diff showed superior performance on six public datasets and a newly created AVMS benchmark, recovering more details and semantics compared to state-of-the-art methods.

Conclusion: FS-Diff effectively unifies image fusion and super-resolution, enhancing visual and semantic image quality for applications in challenging real-world conditions.

Abstract: As an influential information fusion and low-level vision technique, image
fusion integrates complementary information from source images to yield an
informative fused image. A few attempts have been made in recent years to
jointly realize image fusion and super-resolution. However, in real-world
applications such as military reconnaissance and long-range detection missions,
the target and background structures in multimodal images are easily corrupted,
with low resolution and weak semantic information, which leads to suboptimal
results in current fusion techniques. In response, we propose FS-Diff, a
semantic guidance and clarity-aware joint image fusion and super-resolution
method. FS-Diff unifies image fusion and super-resolution as a conditional
generation problem. It leverages semantic guidance from the proposed clarity
sensing mechanism for adaptive low-resolution perception and cross-modal
feature extraction. Specifically, we initialize the desired fused result as
pure Gaussian noise and introduce the bidirectional feature Mamba to extract
the global features of the multimodal images. Moreover, utilizing the source
images and semantics as conditions, we implement a random iterative denoising
process via a modified U-Net network. This network istrained for denoising at
multiple noise levels to produce high-resolution fusion results with
cross-modal features and abundant semantic information. We also construct a
powerful aerial view multiscene (AVMS) benchmark covering 600 pairs of images.
Extensive joint image fusion and super-resolution experiments on six public and
our AVMS datasets demonstrated that FS-Diff outperforms the state-of-the-art
methods at multiple magnifications and can recover richer details and semantics
in the fused images. The code is available at
https://github.com/XylonXu01/FS-Diff.

</details>


### [124] [Semantic Concentration for Self-Supervised Dense Representations Learning](https://arxiv.org/abs/2509.09429)
*Peisong Wen,Qianqian Xu,Siran Dai,Runmin Cong,Qingming Huang*

Main category: cs.CV

TL;DR: The paper addresses challenges in learning dense patch representations in self-supervised learning (SSL), proposing methods involving noise-tolerant losses and object-aware filters for improving dense tasks.


<details>
  <summary>Details</summary>
Motivation: While image-level SSL has achieved progress, learning dense representations for patches faces over-dispersion issues, which hinder downstream dense tasks. Mitigating this limitation drives the exploration of dense SSL methods.

Method: The authors propose distilling patch correspondences to overcome strict spatial alignment and introduce a noise-tolerant ranking loss adapted from the AP loss. Additionally, they develop an object-aware filter that leverages learnable prototypes to map patches into object-based spaces via cross-attention.

Result: Empirical studies demonstrate the effectiveness of the proposed method across various tasks, validating its approach to dense SSL.

Conclusion: The paper provides new insights and methods for improving dense SSL by explicitly addressing over-dispersion and leveraging object-aware mechanisms, marking progress in the field of dense representations.

Abstract: Recent advances in image-level self-supervised learning (SSL) have made
significant progress, yet learning dense representations for patches remains
challenging. Mainstream methods encounter an over-dispersion phenomenon that
patches from the same instance/category scatter, harming downstream performance
on dense tasks. This work reveals that image-level SSL avoids over-dispersion
by involving implicit semantic concentration. Specifically, the non-strict
spatial alignment ensures intra-instance consistency, while shared patterns,
i.e., similar parts of within-class instances in the input space, ensure
inter-image consistency. Unfortunately, these approaches are infeasible for
dense SSL due to their spatial sensitivity and complicated scene-centric data.
These observations motivate us to explore explicit semantic concentration for
dense SSL. First, to break the strict spatial alignment, we propose to distill
the patch correspondences. Facing noisy and imbalanced pseudo labels, we
propose a noise-tolerant ranking loss. The core idea is extending the Average
Precision (AP) loss to continuous targets, such that its decision-agnostic and
adaptive focusing properties prevent the student model from being misled.
Second, to discriminate the shared patterns from complicated scenes, we propose
the object-aware filter to map the output space to an object-based space.
Specifically, patches are represented by learnable prototypes of objects via
cross-attention. Last but not least, empirical studies across various tasks
soundly support the effectiveness of our method. Code is available in
https://github.com/KID-7391/CoTAP.

</details>


### [125] [FlexiD-Fuse: Flexible number of inputs multi-modal medical image fusion based on diffusion model](https://arxiv.org/abs/2509.09456)
*Yushen Xu,Xiaosong Li,Yuchun Wang,Xiaoqi Cheng,Huafeng Li,Haishu Tan*

Main category: cs.CV

TL;DR: FlexiD-Fuse is a diffusion-based image fusion network that supports flexible quantities of input medical images from various modalities and achieves superior performance in fusion tasks.


<details>
  <summary>Details</summary>
Motivation: Existing medical image fusion methods restrict input to fixed modalities, hindering their practical applications in clinical settings.

Method: FlexiD-Fuse transforms the diffusion fusion problem into a maximum likelihood estimation problem, incorporating hierarchical Bayesian modeling and the Expectation-Maximization algorithm for varying input modalities.

Result: FlexiD-Fuse outperformed SOTA methods in multi-modal medical image fusion, validated on Harvard datasets and extended tasks like infrared-visible, multi-exposure, and multi-focus image fusion.

Conclusion: FlexiD-Fuse demonstrates flexibility and effectiveness by accommodating arbitrary numbers of input modalities while maintaining high-quality fusion results across diverse tasks.

Abstract: Different modalities of medical images provide unique physiological and
anatomical information for diseases. Multi-modal medical image fusion
integrates useful information from different complementary medical images with
different modalities, producing a fused image that comprehensively and
objectively reflects lesion characteristics to assist doctors in clinical
diagnosis. However, existing fusion methods can only handle a fixed number of
modality inputs, such as accepting only two-modal or tri-modal inputs, and
cannot directly process varying input quantities, which hinders their
application in clinical settings. To tackle this issue, we introduce
FlexiD-Fuse, a diffusion-based image fusion network designed to accommodate
flexible quantities of input modalities. It can end-to-end process two-modal
and tri-modal medical image fusion under the same weight. FlexiD-Fuse
transforms the diffusion fusion problem, which supports only fixed-condition
inputs, into a maximum likelihood estimation problem based on the diffusion
process and hierarchical Bayesian modeling. By incorporating the
Expectation-Maximization algorithm into the diffusion sampling iteration
process, FlexiD-Fuse can generate high-quality fused images with cross-modal
information from source images, independently of the number of input images. We
compared the latest two and tri-modal medical image fusion methods, tested them
on Harvard datasets, and evaluated them using nine popular metrics. The
experimental results show that our method achieves the best performance in
medical image fusion with varying inputs. Meanwhile, we conducted extensive
extension experiments on infrared-visible, multi-exposure, and multi-focus
image fusion tasks with arbitrary numbers, and compared them with the
perspective SOTA methods. The results of the extension experiments consistently
demonstrate the effectiveness and superiority of our method.

</details>


### [126] [Resource-Efficient Glioma Segmentation on Sub-Saharan MRI](https://arxiv.org/abs/2509.09469)
*Freedmore Sidume,Oumayma Soula,Joseph Muthui Wacira,YunFei Zhu,Abbas Rabiu Muhammad,Abderrazek Zeraii,Oluwaseun Kalejaye,Hajer Ibrahim,Olfa Gaddour,Brain Halubanza,Dong Zhang,Udunna C Anazodo,Confidence Raymond*

Main category: cs.CV

TL;DR: This paper presents a deep learning model for glioma segmentation in MRI, targeting resource-constrained settings in Sub-Saharan Africa.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the lack of high-quality annotated MRI data and tools for glioma segmentation in resource-limited regions like Sub-Saharan Africa.

Method: The study employs a 3D Attention UNet architecture augmented with residual blocks and transfer learning using pre-trained weights from the BraTS 2021 dataset.

Result: The model achieved Dice scores of 0.76 (Enhancing Tumor), 0.80 (Necrotic/Non-Enhancing Tumor Core), and 0.85 (Surrounding Non-Functional Hemisphere) on the BraTS-Africa dataset, demonstrating effectiveness despite data limitations.

Conclusion: The proposed solution is generalizable, computationally efficient, and practical for deployment in low-resource healthcare systems, thereby contributing to equitable AI solutions in global health.

Abstract: Gliomas are the most prevalent type of primary brain tumors, and their
accurate segmentation from MRI is critical for diagnosis, treatment planning,
and longitudinal monitoring. However, the scarcity of high-quality annotated
imaging data in Sub-Saharan Africa (SSA) poses a significant challenge for
deploying advanced segmentation models in clinical workflows. This study
introduces a robust and computationally efficient deep learning framework
tailored for resource-constrained settings. We leveraged a 3D Attention UNet
architecture augmented with residual blocks and enhanced through transfer
learning from pre-trained weights on the BraTS 2021 dataset. Our model was
evaluated on 95 MRI cases from the BraTS-Africa dataset, a benchmark for glioma
segmentation in SSA MRI data. Despite the limited data quality and quantity,
our approach achieved Dice scores of 0.76 for the Enhancing Tumor (ET), 0.80
for Necrotic and Non-Enhancing Tumor Core (NETC), and 0.85 for Surrounding
Non-Functional Hemisphere (SNFH). These results demonstrate the
generalizability of the proposed model and its potential to support clinical
decision making in low-resource settings. The compact architecture,
approximately 90 MB, and sub-minute per-volume inference time on consumer-grade
hardware further underscore its practicality for deployment in SSA health
systems. This work contributes toward closing the gap in equitable AI for
global health by empowering underserved regions with high-performing and
accessible medical imaging solutions.

</details>


### [127] [OpenFake: An Open Dataset and Platform Toward Large-Scale Deepfake Detection](https://arxiv.org/abs/2509.09495)
*Victor Livernoche,Akshatha Arodi,Andreea Musulan,Zachary Yang,Adam Salvail,Gaétan Marceau Caron,Jean-François Godbout,Reihaneh Rabbany*

Main category: cs.CV

TL;DR: This paper tackles the threat of deepfakes in political misinformation by presenting a multimodal dataset, studying human perception of synthetic images, and introducing a crowdsourced platform for adaptive deepfake detection.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the increasing risk posed by advanced AI-generated deepfakes in spreading misinformation, especially in political contexts, and the limitations of existing detection datasets.

Method: It introduces a large, politically-focused dataset combining real images and synthetic ones generated using multiple generative models, alongside a crowdsourced adversarial platform to keep detection methods adaptive.

Result: The study provides 963k high-quality synthetic images for detection benchmarking and demonstrates that proprietary models produce synthetic media nearly indistinguishable to general human perception.

Conclusion: Detecting sophisticated misinformation via deepfakes requires adaptive methodologies, and the approach proposed with a robust dataset and adversarial platform strengthens detection capabilities in a fast-evolving domain.

Abstract: Deepfakes, synthetic media created using advanced AI techniques, have
intensified the spread of misinformation, particularly in politically sensitive
contexts. Existing deepfake detection datasets are often limited, relying on
outdated generation methods, low realism, or single-face imagery, restricting
the effectiveness for general synthetic image detection. By analyzing social
media posts, we identify multiple modalities through which deepfakes propagate
misinformation. Furthermore, our human perception study demonstrates that
recently developed proprietary models produce synthetic images increasingly
indistinguishable from real ones, complicating accurate identification by the
general public. Consequently, we present a comprehensive, politically-focused
dataset specifically crafted for benchmarking detection against modern
generative models. This dataset contains three million real images paired with
descriptive captions, which are used for generating 963k corresponding
high-quality synthetic images from a mix of proprietary and open-source models.
Recognizing the continual evolution of generative techniques, we introduce an
innovative crowdsourced adversarial platform, where participants are
incentivized to generate and submit challenging synthetic images. This ongoing
community-driven initiative ensures that deepfake detection methods remain
robust and adaptive, proactively safeguarding public discourse from
sophisticated misinformation threats.

</details>


### [128] [Improving Human Motion Plausibility with Body Momentum](https://arxiv.org/abs/2509.09496)
*Ha Linh Nguyen,Tze Ho Elden Tse,Angela Yao*

Main category: cs.CV

TL;DR: This paper proposes a method to enhance the link between local joint motion and global movement using momentum constraints.


<details>
  <summary>Details</summary>
Motivation: Current motion models often fail to capture the physical coupling between local and global dynamics, and deriving global trajectories from joint torques is complex and computationally expensive.

Method: The authors propose enforcing whole-body linear and angular momentum constraints using a novel loss term in motion generation models.

Result: The proposed method reduces foot sliding and jitter, improves balance, and preserves motion accuracy.

Conclusion: Incorporating momentum-based constraints provides a physically grounded and effective way to improve human motion dynamics in computational models.

Abstract: Many studies decompose human motion into local motion in a frame attached to
the root joint and global motion of the root joint in the world frame, treating
them separately. However, these two components are not independent. Global
movement arises from interactions with the environment, which are, in turn,
driven by changes in the body configuration. Motion models often fail to
precisely capture this physical coupling between local and global dynamics,
while deriving global trajectories from joint torques and external forces is
computationally expensive and complex. To address these challenges, we propose
using whole-body linear and angular momentum as a constraint to link local
motion with global movement. Since momentum reflects the aggregate effect of
joint-level dynamics on the body's movement through space, it provides a
physically grounded way to relate local joint behavior to global displacement.
Building on this insight, we introduce a new loss term that enforces
consistency between the generated momentum profiles and those observed in
ground-truth data. Incorporating our loss reduces foot sliding and jitter,
improves balance, and preserves the accuracy of the recovered motion. Code and
data are available at the project page https://hlinhn.github.io/momentum_bmvc.

</details>


### [129] [Region-Wise Correspondence Prediction between Manga Line Art Images](https://arxiv.org/abs/2509.09501)
*Yingxuan Li,Jiafeng Mao,Qianru Qiu,Yusuke Matsui*

Main category: cs.CV

TL;DR: This paper focuses on predicting region-wise correspondence between manga line art images without pre-existing labels, using a Transformer-based framework.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the largely unexplored task of understanding region-wise correlations in manga line art images, which is essential for applications like automatic coloring and animation.

Method: The authors employ a Transformer-based approach to analyze patch-level similarities in manga line art, followed by edge-aware clustering and a region matching algorithm for creating region-level correspondences.

Result: Their method achieved 96.34% accuracy at the patch level and delivers consistent results in region-level correspondence.

Conclusion: This technique demonstrates strong potential for applications in manga processing and provides a significant advance in region correspondence tasks in raw manga line arts.

Abstract: Understanding region-wise correspondence between manga line art images is a
fundamental task in manga processing, enabling downstream applications such as
automatic line art colorization and in-between frame generation. However, this
task remains largely unexplored, especially in realistic scenarios without
pre-existing segmentation or annotations. In this paper, we introduce a novel
and practical task: predicting region-wise correspondence between raw manga
line art images without any pre-existing labels or masks. To tackle this
problem, we divide each line art image into a set of patches and propose a
Transformer-based framework that learns patch-level similarities within and
across images. We then apply edge-aware clustering and a region matching
algorithm to convert patch-level predictions into coherent region-level
correspondences. To support training and evaluation, we develop an automatic
annotation pipeline and manually refine a subset of the data to construct
benchmark datasets. Experiments on multiple datasets demonstrate that our
method achieves high patch-level accuracy (e.g., 96.34%) and generates
consistent region-level correspondences, highlighting its potential for
real-world manga applications.

</details>


### [130] [Generative Diffusion Contrastive Network for Multi-View Clustering](https://arxiv.org/abs/2509.09527)
*Jian Zhu,Xin Zou,Xi Wang,Ning Zhang,Bian Wu,Yao Yang,Ying Zhou,Lingfang Zeng,Chang Tang,Cheng Luo*

Main category: cs.CV

TL;DR: The paper proposes a novel method, Stochastic Generative Diffusion Fusion (SGDF), to address noise and missing data issues in multi-view clustering, enhancing clustering performance. A further model, Generative Diffusion Contrastive Network (GDCN), demonstrates state-of-the-art results in deep multi-view clustering.


<details>
  <summary>Details</summary>
Motivation: To address the challenges posed by low-quality data in multi-view clustering analysis due to view-specific noise and missing data, thus improving clustering performance.

Method: The authors introduce the SGDF method, which uses a multiple generative mechanism to ensure robustness against low-quality data and perform effective multi-view feature extraction. They also propose an advanced model, GDCN, which builds on SGDF.

Result: GDCN demonstrates state-of-the-art performance in deep multi-view clustering tasks, as confirmed by extensive experimental results.

Conclusion: The SGDF method and the GDCN model significantly improve clustering in multi-view data settings, especially in handling noisy and incomplete data, advancing the field of deep multi-view clustering.

Abstract: In recent years, Multi-View Clustering (MVC) has been significantly advanced
under the influence of deep learning. By integrating heterogeneous data from
multiple views, MVC enhances clustering analysis, making multi-view fusion
critical to clustering performance. However, there is a problem of low-quality
data in multi-view fusion. This problem primarily arises from two reasons: 1)
Certain views are contaminated by noisy data. 2) Some views suffer from missing
data. This paper proposes a novel Stochastic Generative Diffusion Fusion (SGDF)
method to address this problem. SGDF leverages a multiple generative mechanism
for the multi-view feature of each sample. It is robust to low-quality data.
Building on SGDF, we further present the Generative Diffusion Contrastive
Network (GDCN). Extensive experiments show that GDCN achieves the
state-of-the-art results in deep MVC tasks. The source code is publicly
available at https://github.com/HackerHyper/GDCN.

</details>


### [131] [DualTrack: Sensorless 3D Ultrasound needs Local and Global Context](https://arxiv.org/abs/2509.09530)
*Paul F. R. Wilson,Matteo Ronchetti,Rüdiger Göbl,Viktoria Markova,Sebastian Rosenzweig,Raphael Prevost,Parvin Mousavi,Oliver Zettinig*

Main category: cs.CV

TL;DR: DualTrack introduces a novel dual-encoder architecture to improve 3D ultrasound imaging, achieving state-of-the-art accuracy with an average reconstruction error below 5 mm.


<details>
  <summary>Details</summary>
Motivation: Traditional 3D ultrasound systems are limited by high costs and complexity, motivating exploration of sensorless 3D ultrasound approaches using deep learning.

Method: DualTrack employs decoupled global and local encoders, combining fine-grained spatiotemporal features and high-level anatomical features via a lightweight fusion module.

Result: Experimental results demonstrate the system's superior performance on a public benchmark, achieving globally consistent 3D reconstructions and surpassing prior methods.

Conclusion: DualTrack proves the potential of leveraging decoupled feature extraction in improving 3D ultrasound imaging accuracy and reliability.

Abstract: Three-dimensional ultrasound (US) offers many clinical advantages over
conventional 2D imaging, yet its widespread adoption is limited by the cost and
complexity of traditional 3D systems. Sensorless 3D US, which uses deep
learning to estimate a 3D probe trajectory from a sequence of 2D US images, is
a promising alternative. Local features, such as speckle patterns, can help
predict frame-to-frame motion, while global features, such as coarse shapes and
anatomical structures, can situate the scan relative to anatomy and help
predict its general shape. In prior approaches, global features are either
ignored or tightly coupled with local feature extraction, restricting the
ability to robustly model these two complementary aspects. We propose
DualTrack, a novel dual-encoder architecture that leverages decoupled local and
global encoders specialized for their respective scales of feature extraction.
The local encoder uses dense spatiotemporal convolutions to capture
fine-grained features, while the global encoder utilizes an image backbone
(e.g., a 2D CNN or foundation model) and temporal attention layers to embed
high-level anatomical features and long-range dependencies. A lightweight
fusion module then combines these features to estimate the trajectory.
Experimental results on a large public benchmark show that DualTrack achieves
state-of-the-art accuracy and globally consistent 3D reconstructions,
outperforming previous methods and yielding an average reconstruction error
below 5 mm.

</details>


### [132] [Improving Video Diffusion Transformer Training by Multi-Feature Fusion and Alignment from Self-Supervised Vision Encoders](https://arxiv.org/abs/2509.09547)
*Dohun Lee,Hyeonho Jeong,Jiwook Kim,Duygu Ceylan,Jong Chul Ye*

Main category: cs.CV

TL;DR: This paper proposes Align4Gen, a method to align video diffusion model features with pre-trained vision encoders, improving video generation quality.


<details>
  <summary>Details</summary>
Motivation: Recent advancements in video diffusion models focus on architectures and objectives but overlook feature representation enhancement.

Method: The method introduces multi-feature fusion and alignment during training by evaluating pre-trained vision encoders' discriminability and temporal consistency.

Result: Align4Gen improves video generation performance in both unconditional and class-conditional tasks, as validated by various metrics.

Conclusion: Aligning intermediate video diffusion model features with pre-trained encoders enhances video representation and generation quality effectively.

Abstract: Video diffusion models have advanced rapidly in the recent years as a result
of series of architectural innovations (e.g., diffusion transformers) and use
of novel training objectives (e.g., flow matching). In contrast, less attention
has been paid to improving the feature representation power of such models. In
this work, we show that training video diffusion models can benefit from
aligning the intermediate features of the video generator with feature
representations of pre-trained vision encoders. We propose a new metric and
conduct an in-depth analysis of various vision encoders to evaluate their
discriminability and temporal consistency, thereby assessing their suitability
for video feature alignment. Based on the analysis, we present Align4Gen which
provides a novel multi-feature fusion and alignment method integrated into
video diffusion model training. We evaluate Align4Gen both for unconditional
and class-conditional video generation tasks and show that it results in
improved video generation as quantified by various metrics. Full video results
are available on our project page: https://align4gen.github.io/align4gen/

</details>


### [133] [InterAct: Advancing Large-Scale Versatile 3D Human-Object Interaction Generation](https://arxiv.org/abs/2509.09555)
*Sirui Xu,Dongting Li,Yucheng Zhang,Xiyan Xu,Qi Long,Ziyin Wang,Yunzhi Lu,Shuchang Dong,Hezi Jiang,Akshat Gupta,Yu-Xiong Wang,Liang-Yan Gui*

Main category: cs.CV

TL;DR: This paper addresses challenges in modeling 3D human-object interactions (HOIs) by introducing a new benchmark dataset called InterAct with enhanced quality and annotations, alongside optimized methodology.


<details>
  <summary>Details</summary>
Motivation: Challenges in generating dynamic 3D human-object interactions due to limited data and artifacts in existing datasets.

Method: The authors consolidated 21.81 hours of HOI data, enriched it with annotations, optimized for data quality, expanded it to 30.70 hours, defined benchmarking tasks, and developed generative modeling for HOIs.

Result: The dataset InterAct achieved state-of-the-art performance across six benchmarking tasks and demonstrated utility for advancing 3D HOI generation.

Conclusion: InterAct serves as a foundational resource for HOI modeling and generation, publicly available and maintained for ongoing research.

Abstract: While large-scale human motion capture datasets have advanced human motion
generation, modeling and generating dynamic 3D human-object interactions (HOIs)
remain challenging due to dataset limitations. Existing datasets often lack
extensive, high-quality motion and annotation and exhibit artifacts such as
contact penetration, floating, and incorrect hand motions. To address these
issues, we introduce InterAct, a large-scale 3D HOI benchmark featuring dataset
and methodological advancements. First, we consolidate and standardize 21.81
hours of HOI data from diverse sources, enriching it with detailed textual
annotations. Second, we propose a unified optimization framework to enhance
data quality by reducing artifacts and correcting hand motions. Leveraging the
principle of contact invariance, we maintain human-object relationships while
introducing motion variations, expanding the dataset to 30.70 hours. Third, we
define six benchmarking tasks and develop a unified HOI generative modeling
perspective, achieving state-of-the-art performance. Extensive experiments
validate the utility of our dataset as a foundational resource for advancing 3D
human-object interaction generation. To support continued research in this
area, the dataset is publicly available at
https://github.com/wzyabcas/InterAct, and will be actively maintained.

</details>


### [134] [Invisible Attributes, Visible Biases: Exploring Demographic Shortcuts in MRI-based Alzheimer's Disease Classification](https://arxiv.org/abs/2509.09558)
*Akshit Achara,Esther Puyol Anton,Alexander Hammers,Andrew P. King*

Main category: cs.CV

TL;DR: This study reveals biases in deep learning models for Alzheimer's disease diagnosis from MRI scans, specifically shortcut learning and demographic bias based on race and sex.


<details>
  <summary>Details</summary>
Motivation: To address potential shortcomings and biases in deep learning models used for Alzheimer’s disease diagnosis from MRI, particularly those related to sensitive demographic attributes.

Method: The authors investigated biases by assessing if DL models can infer sex or race from MRI scans, analyzed the impact of dataset imbalance, and studied regions of feature attribution using ResNet and SwinTransformer models.

Result: The study demonstrated that DL models exhibit shortcut learning and demographic biases based on race and sex in Alzheimer’s disease classification tasks.

Conclusion: This work emphasizes the need for fair and unbiased DL diagnostic tools in brain MRI, setting the stage for improving model fairness and inclusivity.

Abstract: Magnetic resonance imaging (MRI) is the gold standard for brain imaging. Deep
learning (DL) algorithms have been proposed to aid in the diagnosis of diseases
such as Alzheimer's disease (AD) from MRI scans. However, DL algorithms can
suffer from shortcut learning, in which spurious features, not directly related
to the output label, are used for prediction. When these features are related
to protected attributes, they can lead to performance bias against
underrepresented protected groups, such as those defined by race and sex. In
this work, we explore the potential for shortcut learning and demographic bias
in DL based AD diagnosis from MRI. We first investigate if DL algorithms can
identify race or sex from 3D brain MRI scans to establish the presence or
otherwise of race and sex based distributional shifts. Next, we investigate
whether training set imbalance by race or sex can cause a drop in model
performance, indicating shortcut learning and bias. Finally, we conduct a
quantitative and qualitative analysis of feature attributions in different
brain regions for both the protected attribute and AD classification tasks.
Through these experiments, and using multiple datasets and DL models (ResNet
and SwinTransformer), we demonstrate the existence of both race and sex based
shortcut learning and bias in DL based AD classification. Our work lays the
foundation for fairer DL diagnostic tools in brain MRI. The code is provided at
https://github.com/acharaakshit/ShortMR

</details>


### [135] [PeftCD: Leveraging Vision Foundation Models with Parameter-Efficient Fine-Tuning for Remote Sensing Change Detection](https://arxiv.org/abs/2509.09572)
*Sijun Dong,Yuxuan Hu,LiBo Wang,Geng Chen,Xiaoliang Meng*

Main category: cs.CV

TL;DR: The paper introduces PeftCD, a novel framework for change detection in remote sensing imagery leveraging Vision Foundation Models (VFMs) with Parameter-Efficient Fine-Tuning (PEFT). It outperforms existing methods across public datasets while balancing accuracy, efficiency, and generalization.


<details>
  <summary>Details</summary>
Motivation: To address challenges like pseudo changes, labeled data scarcity, and cross-domain generalization difficulties in remote sensing imagery change detection.

Method: Proposes PeftCD, which integrates a weight-sharing Siamese encoder (based on VFM), LoRA and Adapter modules for efficient adaptation, and a lightweight decoder. Utilizes SAM2 and DINOv3 backbones to harness strong segmentation priors and self-supervised learning.

Result: Outperforms existing methods with state-of-the-art results across multiple datasets, achieving IoU scores like 73.81% on SYSU-CD, 92.05% on WHUCD, and 97.01% on CDD. It offers precise boundary delineation and suppresses pseudo-changes effectively.

Conclusion: PeftCD is a scalable and efficient solution for real-world remote sensing change detection, showcasing the adaptability of VFMs to specific tasks. The framework achieves an optimal balance across performance metrics.

Abstract: To tackle the prevalence of pseudo changes, the scarcity of labeled samples,
and the difficulty of cross-domain generalization in multi-temporal and
multi-source remote sensing imagery, we propose PeftCD, a change detection
framework built upon Vision Foundation Models (VFMs) with Parameter-Efficient
Fine-Tuning (PEFT). At its core, PeftCD employs a weight-sharing Siamese
encoder derived from a VFM, into which LoRA and Adapter modules are seamlessly
integrated. This design enables highly efficient task adaptation by training
only a minimal set of additional parameters. To fully unlock the potential of
VFMs, we investigate two leading backbones: the Segment Anything Model v2
(SAM2), renowned for its strong segmentation priors, and DINOv3, a
state-of-the-art self-supervised representation learner. The framework is
complemented by a deliberately lightweight decoder, ensuring the focus remains
on the powerful feature representations from the backbones. Extensive
experiments demonstrate that PeftCD achieves state-of-the-art performance
across multiple public datasets, including SYSU-CD (IoU 73.81%), WHUCD
(92.05%), MSRSCD (64.07%), MLCD (76.89%), CDD (97.01%), S2Looking (52.25%) and
LEVIR-CD (85.62%), with notably precise boundary delineation and strong
suppression of pseudo-changes. In summary, PeftCD presents an optimal balance
of accuracy, efficiency, and generalization. It offers a powerful and scalable
paradigm for adapting large-scale VFMs to real-world remote sensing change
detection applications. The code and pretrained models will be released at
https://github.com/dyzy41/PeftCD.

</details>


### [136] [Kling-Avatar: Grounding Multimodal Instructions for Cascaded Long-Duration Avatar Animation Synthesis](https://arxiv.org/abs/2509.09595)
*Yikang Ding,Jiwen Liu,Wenyuan Zhang,Zekun Wang,Wentao Hu,Liyuan Cui,Mingming Lao,Yingchao Shao,Hui Liu,Xiaohan Li,Ming Chen,Xiaoqiang Liu,Yu-Shen Liu,Pengfei Wan*

Main category: cs.CV

TL;DR: Kling-Avatar introduces an innovative approach for audio-driven video avatar generation by integrating multimodal instructions with photorealistic synthesis, achieving superior expressiveness and coherence.


<details>
  <summary>Details</summary>
Motivation: Existing audio-driven avatar generation methods fail to encode the narrative and communicative purposes behind instructions, limiting their expressiveness and coherence.

Method: The framework utilizes a two-stage pipeline: a multimodal large language model (MLLM) produces a blueprint video based on high-level instruction semantics, followed by parallel generation of video sub-clips guided by keyframes.

Result: Kling-Avatar achieves vivid, long-duration video generation at up to 1080p and 48 fps, excelling in lip synchronization, emotion expression, and instruction controllability across diverse scenarios.

Conclusion: Kling-Avatar sets a new benchmark for semantically grounded audio-driven avatar production, demonstrating strong potential for real-world applications like livestreaming and vlogging.

Abstract: Recent advances in audio-driven avatar video generation have significantly
enhanced audio-visual realism. However, existing methods treat instruction
conditioning merely as low-level tracking driven by acoustic or visual cues,
without modeling the communicative purpose conveyed by the instructions. This
limitation compromises their narrative coherence and character expressiveness.
To bridge this gap, we introduce Kling-Avatar, a novel cascaded framework that
unifies multimodal instruction understanding with photorealistic portrait
generation. Our approach adopts a two-stage pipeline. In the first stage, we
design a multimodal large language model (MLLM) director that produces a
blueprint video conditioned on diverse instruction signals, thereby governing
high-level semantics such as character motion and emotions. In the second
stage, guided by blueprint keyframes, we generate multiple sub-clips in
parallel using a first-last frame strategy. This global-to-local framework
preserves fine-grained details while faithfully encoding the high-level intent
behind multimodal instructions. Our parallel architecture also enables fast and
stable generation of long-duration videos, making it suitable for real-world
applications such as digital human livestreaming and vlogging. To
comprehensively evaluate our method, we construct a benchmark of 375 curated
samples covering diverse instructions and challenging scenarios. Extensive
experiments demonstrate that Kling-Avatar is capable of generating vivid,
fluent, long-duration videos at up to 1080p and 48 fps, achieving superior
performance in lip synchronization accuracy, emotion and dynamic
expressiveness, instruction controllability, identity preservation, and
cross-domain generalization. These results establish Kling-Avatar as a new
benchmark for semantically grounded, high-fidelity audio-driven avatar
synthesis.

</details>


### [137] [Mechanistic Learning with Guided Diffusion Models to Predict Spatio-Temporal Brain Tumor Growth](https://arxiv.org/abs/2509.09610)
*Daria Laslo,Efthymios Georgiou,Marius George Linguraru,Andreas Rauschecker,Sabine Muller,Catherine R. Jutzeler,Sarah Bruningk*

Main category: cs.CV

TL;DR: This paper presents a hybrid approach combining mathematical tumor growth modeling with guided denoising diffusion for predicting brain tumor progression using MRIs.


<details>
  <summary>Details</summary>
Motivation: To provide better tools for predicting brain tumor progression, aiding clinical decision-making in neuro-oncology.

Method: A hybrid mechanistic learning framework that utilizes a mathematical tumor growth model alongside a guided denoising diffusion implicit model (DDIM) to simulate future MRI scans.

Result: The model effectively generates realistic follow-up MRIs and introduces tumor growth probability maps, validated using spatial similarity metrics on pediatric glioma cases.

Conclusion: This approach demonstrates potential for producing biologically accurate imaging predictions in scenarios with limited data, enhancing tumor growth modeling in neuro-oncology.

Abstract: Predicting the spatio-temporal progression of brain tumors is essential for
guiding clinical decisions in neuro-oncology. We propose a hybrid mechanistic
learning framework that combines a mathematical tumor growth model with a
guided denoising diffusion implicit model (DDIM) to synthesize anatomically
feasible future MRIs from preceding scans. The mechanistic model, formulated as
a system of ordinary differential equations, captures temporal tumor dynamics
including radiotherapy effects and estimates future tumor burden. These
estimates condition a gradient-guided DDIM, enabling image synthesis that
aligns with both predicted growth and patient anatomy. We train our model on
the BraTS adult and pediatric glioma datasets and evaluate on 60 axial slices
of in-house longitudinal pediatric diffuse midline glioma (DMG) cases. Our
framework generates realistic follow-up scans based on spatial similarity
metrics. It also introduces tumor growth probability maps, which capture both
clinically relevant extent and directionality of tumor growth as shown by 95th
percentile Hausdorff Distance. The method enables biologically informed image
generation in data-limited scenarios, offering generative-space-time
predictions that account for mechanistic priors.

</details>


### [138] [Measuring Epistemic Humility in Multimodal Large Language Models](https://arxiv.org/abs/2509.09658)
*Bingkui Tong,Jiaer Xia,Sifeng Shang,Kaiyang Zhou*

Main category: cs.CV

TL;DR: The paper introduces HumbleBench, a benchmark to test multimodal large language models (MLLMs) on recognizing incorrect visual content, beyond just recognizing correct answers.


<details>
  <summary>Details</summary>
Motivation: The study addresses the risks of hallucinations in MLLMs, which generate inconsistent content, potentially leading to misinformation and unsafe decision-making.

Method: The authors built HumbleBench using a panoptic scene graph dataset, generated multiple-choice questions via GPT-4-Turbo, and included a rigorous filtering process. They incorporated a 'None of the above' option to test models' ability to reject incorrect options.

Result: HumbleBench evaluates state-of-the-art MLLMs on their ability to reject false answers, providing insights on hallucination types (object, relation, and attribute).

Conclusion: HumbleBench adds a layer of evaluation missing in current benchmarks, enabling improved reliability assessment of MLLMs in critical applications. The dataset and code are open-sourced.

Abstract: Hallucinations in multimodal large language models (MLLMs) -- where the model
generates content inconsistent with the input image -- pose significant risks
in real-world applications, from misinformation in visual question answering to
unsafe errors in decision-making. Existing benchmarks primarily test
recognition accuracy, i.e., evaluating whether models can select the correct
answer among distractors. This overlooks an equally critical capability for
trustworthy AI: recognizing when none of the provided options are correct, a
behavior reflecting epistemic humility. We present HumbleBench, a new
hallucination benchmark designed to evaluate MLLMs' ability to reject plausible
but incorrect answers across three hallucination types: object, relation, and
attribute. Built from a panoptic scene graph dataset, we leverage fine-grained
scene graph annotations to extract ground-truth entities and relations, and
prompt GPT-4-Turbo to generate multiple-choice questions, followed by a
rigorous manual filtering process. Each question includes a "None of the above"
option, requiring models not only to recognize correct visual information but
also to identify when no provided answer is valid. We evaluate a variety of
state-of-the-art MLLMs -- including both general-purpose and specialized
reasoning models -- on HumbleBench and share valuable findings and insights
with the community. By incorporating explicit false-option rejection,
HumbleBench fills a key gap in current evaluation suites, providing a more
realistic measure of MLLM reliability in safety-critical settings. Our code and
dataset are released publicly and can be accessed at
https://github.com/maifoundations/HumbleBench.

</details>


### [139] [Can Understanding and Generation Truly Benefit Together -- or Just Coexist?](https://arxiv.org/abs/2509.09666)
*Zhiyuan Yan,Kaiqing Lin,Zongjian Li,Junyan Ye,Hui Han,Zhendong Wang,Hao Liu,Bin Lin,Hao Li,Xue Xu,Xinyan Xiao,Jingdong Wang,Haifeng Wang,Li Yuan*

Main category: cs.CV

TL;DR: The paper introduces UAE, a framework for unified multimodal learning, leveraging an auto-encoder paradigm and reinforcement learning to enhance mutual understanding and generation between images and text, with evaluation on the new Unified-Bench.


<details>
  <summary>Details</summary>
Motivation: To improve coherent bidirectional information flow between image-to-text understanding and text-to-image generation, ensuring mutual reinforcement, and to design a benchmark for assessing unified multimodal systems.

Method: The method involves training an auto-encoder-inspired system using a three-stage reinforcement learning approach: initializing with semantic reconstruction loss, refining the encoder using generation tasks to improve visual understanding, and enhancing the decoder's ability to reconstruct images using detailed captions.

Result: The encoder becomes capable of producing detailed image captions, while the decoder improves its image reconstruction quality by understanding these intricate captions, achieving remarkable fidelity.

Conclusion: The UAE framework demonstrates the potential for unified multimodal learning by achieving mutually reinforcing understanding and generation processes, significantly advancing long-context instruction handling and image-text integration.

Abstract: In this paper, we introduce an insightful paradigm through the Auto-Encoder
lens-understanding as the encoder (I2T) that compresses images into text, and
generation as the decoder (T2I) that reconstructs images from that text. Using
reconstruction fidelity as the unified training objective, we enforce the
coherent bidirectional information flow between the understanding and
generation processes, bringing mutual gains. To implement this, we propose UAE,
a novel framework for unified multimodal learning. We begin by pre-training the
decoder with large-scale long-context image captions to capture fine-grained
semantic and complex spatial relationships. We then propose Unified-GRPO via
reinforcement learning (RL), which covers three stages: (1) A cold-start phase
to gently initialize both encoder and decoder with a semantic reconstruction
loss; (2) Generation for Understanding, where the encoder is trained to
generate informative captions that maximize the decoder's reconstruction
quality, enhancing its visual understanding; (3) Understanding for Generation,
where the decoder is refined to reconstruct from these captions, forcing it to
leverage every detail and improving its long-context instruction following and
generation fidelity. For evaluation, we introduce Unified-Bench, the first
benchmark tailored to assess the degree of unification of the UMMs. A
surprising "aha moment" arises within the multimodal learning domain: as RL
progresses, the encoder autonomously produces more descriptive captions, while
the decoder simultaneously demonstrates a profound ability to understand these
intricate descriptions, resulting in reconstructions of striking fidelity.

</details>


### [140] [Geometric Neural Distance Fields for Learning Human Motion Priors](https://arxiv.org/abs/2509.09667)
*Zhengdi Yu,Simone Foti,Linguang Zhang,Amy Zhao,Cem Keskin,Stefanos Zafeiriou,Tolga Birdal*

Main category: cs.CV

TL;DR: This paper presents Neural Riemannian Motion Fields (NRMF), a novel approach to 3D human motion modeling that enhances robustness, temporal consistency, and physical plausibility across tasks like motion recovery and interpolation.


<details>
  <summary>Details</summary>
Motivation: Existing methods based on VAEs or diffusion for 3D human motion lack temporal consistency and physical plausibility. There's a need for a motion prior that rigorously respects the geometry of human articulations.

Method: The authors propose NRMF, which represents human motion using neural distance fields (NDFs) tied to pose, velocity, and acceleration. It operates on a product space respecting articulation geometry and combines adaptive-step projection algorithms with geometric integrators.

Result: NRMF demonstrates superior performance on the AMASS dataset, generalizing to a variety of tasks, modalities, and challenges like noise reduction and incomplete data reconstruction.

Conclusion: NRMF offers robust and generalizable improvements in 3D motion modeling, proving its potential for real-world applications in motion recovery and generation.

Abstract: We introduce Neural Riemannian Motion Fields (NRMF), a novel 3D generative
human motion prior that enables robust, temporally consistent, and physically
plausible 3D motion recovery. Unlike existing VAE or diffusion-based methods,
our higher-order motion prior explicitly models the human motion in the zero
level set of a collection of neural distance fields (NDFs) corresponding to
pose, transition (velocity), and acceleration dynamics. Our framework is
rigorous in the sense that our NDFs are constructed on the product space of
joint rotations, their angular velocities, and angular accelerations,
respecting the geometry of the underlying articulations. We further introduce:
(i) a novel adaptive-step hybrid algorithm for projecting onto the set of
plausible motions, and (ii) a novel geometric integrator to "roll out"
realistic motion trajectories during test-time-optimization and generation. Our
experiments show significant and consistent gains: trained on the AMASS
dataset, NRMF remarkably generalizes across multiple input modalities and to
diverse tasks ranging from denoising to motion in-betweening and fitting to
partial 2D / 3D observations.

</details>


### [141] [Locality in Image Diffusion Models Emerges from Data Statistics](https://arxiv.org/abs/2509.09672)
*Artem Lukoianov,Chenyang Yuan,Justin Solomon,Vincent Sitzmann*

Main category: cs.CV

TL;DR: The study revisits diffusion models, challenging the belief that locality in deep diffusion models arises solely from neural network inductive biases, and proposes an alternative analytical denoiser informed by natural image properties.


<details>
  <summary>Details</summary>
Motivation: To explore and clarify why deep diffusion models demonstrate locality and to bridge the gap between the optimal denoiser and empirically-trained diffusion models.

Method: Employing theoretical and experimental analysis, the study examines parametric linear denoisers and their local behaviors, linking these properties to pixel correlations in natural images.

Result: It was demonstrated that locality in denoisers arises from statistical pixel correlations rather than from convolutional neural network biases. A refined analytical denoiser is also proposed, aligning more closely with deep diffusion models' outputs.

Conclusion: Locality within deep diffusion models is a result of properties in the image dataset itself rather than architectural biases, and an improved analytical approach was developed to better model deep diffusion generation behaviors.

Abstract: Among generative models, diffusion models are uniquely intriguing due to the
existence of a closed-form optimal minimizer of their training objective, often
referred to as the optimal denoiser. However, diffusion using this optimal
denoiser merely reproduces images in the training set and hence fails to
capture the behavior of deep diffusion models. Recent work has attempted to
characterize this gap between the optimal denoiser and deep diffusion models,
proposing analytical, training-free models that can generate images that
resemble those generated by a trained UNet. The best-performing method
hypothesizes that shift equivariance and locality inductive biases of
convolutional neural networks are the cause of the performance gap, hence
incorporating these assumptions into its analytical model. In this work, we
present evidence that the locality in deep diffusion models emerges as a
statistical property of the image dataset, not due to the inductive bias of
convolutional neural networks. Specifically, we demonstrate that an optimal
parametric linear denoiser exhibits similar locality properties to the deep
neural denoisers. We further show, both theoretically and experimentally, that
this locality arises directly from the pixel correlations present in natural
image datasets. Finally, we use these insights to craft an analytical denoiser
that better matches scores predicted by a deep diffusion model than the prior
expert-crafted alternative.

</details>


### [142] [SpatialVID: A Large-Scale Video Dataset with Spatial Annotations](https://arxiv.org/abs/2509.09676)
*Jiahao Wang,Yufeng Yuan,Rujie Zheng,Youtian Lin,Jian Gao,Lin-Zhuo Chen,Yajie Bao,Yi Zhang,Chang Zeng,Yanxi Zhou,Xiaoxiao Long,Hao Zhu,Zhaoxiang Zhang,Xun Cao,Yao Yao*

Main category: cs.CV

TL;DR: The paper introduces SpatialVID, a new large-scale dataset of in-the-wild videos with diverse scenes, camera movements, and dense 3D annotations, aimed at advancing spatial intelligence in real-world scenarios.


<details>
  <summary>Details</summary>
Motivation: Current spatial intelligence models are limited by insufficient and low-quality training data, particularly for real-world dynamic scenes with accurate camera motion data.

Method: The SpatialVID dataset comprises over 21,000 hours of raw video processed into 2.7 million clips, annotated with spatial and semantic information such as camera poses, depth maps, and dynamic masks through a hierarchical filtering and annotation pipeline.

Result: SpatialVID offers a rich and diverse dataset that improves model generalization and performance in spatial and 3D vision tasks.

Conclusion: SpatialVID is positioned as a valuable resource for advancing research in video and 3D vision, particularly in dynamic, real-world scenarios.

Abstract: Significant progress has been made in spatial intelligence, spanning both
spatial reconstruction and world exploration. However, the scalability and
real-world fidelity of current models remain severely constrained by the
scarcity of large-scale, high-quality training data. While several datasets
provide camera pose information, they are typically limited in scale,
diversity, and annotation richness, particularly for real-world dynamic scenes
with ground-truth camera motion. To this end, we collect \textbf{SpatialVID}, a
dataset consists of a large corpus of in-the-wild videos with diverse scenes,
camera movements and dense 3D annotations such as per-frame camera poses,
depth, and motion instructions. Specifically, we collect more than 21,000 hours
of raw video, and process them into 2.7 million clips through a hierarchical
filtering pipeline, totaling 7,089 hours of dynamic content. A subsequent
annotation pipeline enriches these clips with detailed spatial and semantic
information, including camera poses, depth maps, dynamic masks, structured
captions, and serialized motion instructions. Analysis of SpatialVID's data
statistics reveals a richness and diversity that directly foster improved model
generalization and performance, establishing it as a key asset for the video
and 3D vision research community.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [143] [WebAssembly and Unikernels: A Comparative Study for Serverless at the Edge](https://arxiv.org/abs/2509.09400)
*Valerio Besozzi,Enrico Fiasco,Marco Danelutto,Patrizio Dazzi*

Main category: cs.DC

TL;DR: This paper evaluates WebAssembly against unikernel-based MicroVMs for serverless edge computing workloads, highlighting their respective strengths and weaknesses.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the need for lightweight execution environments in Urgent Edge Computing to minimize cold start latency.

Method: The authors developed a runtime named Limes based on WebAssembly and compared it with Firecracker-based execution in SPARE.

Result: WebAssembly demonstrated lower cold start times for lightweight functions but struggled with complex workloads. Firecracker had higher cold start times but delivered consistent performance for I/O-heavy tasks.

Conclusion: WebAssembly is advantageous for lightweight functions due to low cold start times, while Firecracker is better suited for complex, performance-critical workloads.

Abstract: Serverless computing at the edge requires lightweight execution environments
to minimize cold start latency, especially in Urgent Edge Computing (UEC). This
paper compares WebAssembly and unikernel-based MicroVMs for serverless
workloads. We present Limes, a WebAssembly runtime built on Wasmtime, and
evaluate it against the Firecracker-based environment used in SPARE. Results
show that WebAssembly offers lower cold start times for lightweight functions
but suffers with complex workloads, while Firecracker provides higher, but
stable, cold starts and better execution performance, particularly for
I/O-heavy tasks.

</details>


### [144] [A Comparative Analysis of Identifier Schemes: UUIDv4, UUIDv7, and ULID for Distributed Systems](https://arxiv.org/abs/2509.08969)
*Nima Karimian Kakolaki*

Main category: cs.DC

TL;DR: This paper evaluates distributed identifier schemes and finds ULIDs to be the most optimal for high-performance systems due to their speed, efficiency, and reduced collision risk.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenges of ensuring data uniqueness and efficiency in distributed systems by comparing the performance and reliability of various identifier schemes.

Method: The study combines theoretical collision probability calculations with empirical tests measuring identifier generation speed and network overhead in a simulated distributed environment.

Result: ULIDs outperform UUIDv4 and UUIDv7, offering 83.7% less network overhead, 97.32% faster generation speed, and 98.42% lower collision risk compared to UUIDv7.

Conclusion: ULIDs are recommended as an optimal identifier solution for scalable distributed systems providing high performance, time-ordering, and low collision risks. All supporting materials and code are made publicly available for reproducibility.

Abstract: Distributed systems require robust, scalable identifier schemes to ensure
data uniqueness and efficient indexing across multiple nodes. This paper
presents a comprehensive analysis of the evolution of distributed identifiers,
comparing traditional auto-increment keys with UUIDv4, UUIDv7, and ULIDs. We
combine mathematical calculation of collision probabilities with empirical
experiments measuring generation speed and network transmission overhead in a
simulated distributed environment. Results demonstrate that ULIDs significantly
outperform UUIDv4 and UUIDv7, reducing network overhead by 83.7% and increasing
generation speed by 97.32%. statistical analysis further shows ULIDs offer a
98.42% lower collision risk compared to UUIDv7, while maintaining negligible
collision probabilities even at high generation rates. These findings highlight
ULIDs as an optimal choice for high-performance distributed systems, providing
efficient, time-ordered, and lexicographically sortable identifiers suitable
for scalable applications. All source code, datasets, and analysis scripts
utilized in this research are publicly available in our dedicated repository at
https://github.com/nimakarimiank/uids-comparison. This repository contains
comprehensive documentation of the experimental setup, including configuration
files for the distributed environment, producer and consumer implementations,
and message broker integration. Additionally, it provides the data scripts and
datasets. Researchers and practitioners are encouraged to explore the
repository for full reproducibility of the experiments and to facilitate
further investigation or extension of the presented work.

</details>


### [145] [Optimizing the Variant Calling Pipeline Execution on Human Genomes Using GPU-Enabled Machines](https://arxiv.org/abs/2509.09058)
*Ajay Kumar,Praveen Rao,Peter Sanders*

Main category: cs.DC

TL;DR: This paper introduces a machine learning-based method to optimize variant calling pipelines on GPU machines, improving execution times significantly.


<details>
  <summary>Details</summary>
Motivation: Variant calling on human genomes is computationally demanding, making efficient execution crucial, especially in cloud environments with pay-as-you-go models.

Method: The proposed method predicts execution times of pipeline stages using machine learning and develops optimal execution plans inspired by the flexible job shop scheduling problem, executed via synchronization across machines.

Result: The approach delivered an average 2X speedup compared to a greedy ML-based method and a 1.6X speedup compared to a dynamic resource approach without ML predictions.

Conclusion: The ML-based method effectively enhances the execution efficiency of variant calling pipelines on GPU hardware, showcasing substantial speedup advantages over alternative approaches.

Abstract: Variant calling is the first step in analyzing a human genome and aims to
detect variants in an individual's genome compared to a reference genome. Due
to the computationally-intensive nature of variant calling, genomic data are
increasingly processed in cloud environments as large amounts of compute and
storage resources can be acquired with the pay-as-you-go pricing model. In this
paper, we address the problem of efficiently executing a variant calling
pipeline for a workload of human genomes on graphics processing unit
(GPU)-enabled machines. We propose a novel machine learning (ML)-based approach
for optimizing the workload execution to minimize the total execution time. Our
approach encompasses two key techniques: The first technique employs ML to
predict the execution times of different stages in a variant calling pipeline
based on the characteristics of a genome sequence. Using the predicted times,
the second technique generates optimal execution plans for the machines by
drawing inspiration from the flexible job shop scheduling problem. The plans
are executed via careful synchronization across different machines. We
evaluated our approach on a workload of publicly available genome sequences
using a testbed with different types of GPU hardware. We observed that our
approach was effective in predicting the execution times of variant calling
pipeline stages using ML on features such as sequence size, read quality,
percentage of duplicate reads, and average read length. In addition, our
approach achieved 2X speedup (on an average) over a greedy approach that also
used ML for predicting the execution times on the tested workload of sequences.
Finally, our approach achieved 1.6X speedup (on an average) over a dynamic
approach that executed the workload based on availability of resources without
using any ML-based time predictions.

</details>


### [146] [Coherence-Aware Task Graph Modeling for Realistic Application](https://arxiv.org/abs/2509.09094)
*Guochu Xiong,Xiangzhong Luo,Weichen Liu*

Main category: cs.DC

TL;DR: This paper introduces CoTAM, a framework for creating coherence-aware task graphs to better model inter-task dependencies in multicore systems.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing task graph modeling methods, which overlook dynamic application behavior and cache coherence interactions, resulting in mismatches between design assumptions and real runtime behavior.

Method: CoTAM analyzes coherence effects by decoupling them from overall execution, quantifies their influence with a learned weighting scheme, and generates coherence-aware task graphs by inferring inter-task dependencies.

Result: CoTAM outperforms implicit methods, bridging the gap between dynamic workload behaviors and system-level designs while showcasing the crucial role of cache coherence in task graph modeling.

Conclusion: Incorporating cache coherence into task graph modeling enhances system-level analysis accuracy and generalizability, addressing limitations in real-world, dynamic workload modeling.

Abstract: As multicore systems continue to scale, cache coherence has emerged as a
critical determinant of system performance, with coherence behavior and task
execution closely intertwined, reshaping inter-task dependencies. Task graph
modeling provides a structured way to capture such dependencies and serves as
the foundation for many system-level design strategies. However, these
strategies typically rely on predefined task graphs, while many real-world
applications lack explicit graphs and exhibit dynamic, data-dependent behavior,
limiting the effectiveness of static approaches. To address this, several task
graph modeling methods for realistic workloads have been developed. Yet, they
either rely on implicit techniques that use application-specific features
without producing explicit graphs, or they generate graphs tailored to fixed
scheduling models, which limits generality. More importantly, they often
overlook coherence interactions, creating a gap between design assumptions and
actual runtime behavior. To overcome these limitations, we propose CoTAM, a
Coherence-Aware Task Graph Modeling framework for realistic workloads that
constructs a unified task graph reflecting runtime behavior. CoTAM analyzes the
impact of coherence by decoupling its effects from overall execution,
quantifies its influence through a learned weighting scheme, and infers
inter-task dependencies for coherence-aware graph generation. Extensive
experiments show that CoTAM outperforms implicit methods, bridging the gap
between dynamic workload behavior and existing designs while demonstrating the
importance of incorporating cache coherence into task graph modeling for
accurate and generalizable system-level analysis.

</details>


### [147] [Barycentric Coded Distributed Computing with Flexible Recovery Threshold for Collaborative Mobile Edge Computing](https://arxiv.org/abs/2509.09435)
*Houming Qiu,Kun Zhu,Dusit Niyato,Nguyen Cong Luong,Changyan Yi,Chen Dai*

Main category: cs.DC

TL;DR: The paper introduces an improved coded distributed computing scheme based on barycentric rational interpolation to address limitations in traditional methods, enhancing computation accuracy, flexibility, and numerical stability.


<details>
  <summary>Details</summary>
Motivation: Straggling edge nodes in Mobile Edge Computing systems impact computation latency and flexibility. Current coded distributed computing techniques fail to decode results below a threshold and suffer from numerical instability issues due to poles in their processes.

Method: The authors propose a CDC scheme leveraging barycentric rational interpolation, which allows decoding with any returned results, supports stable computations, avoids poles, and includes a novel gradient coding algorithm for better robustness and training acceleration.

Result: The proposed scheme outperforms existing methods in aspects such as waiting time reduction and approximation accuracy, achieving significant improvements in MEC systems.

Conclusion: This novel approach provides an effective solution to mitigate the straggler effects, ensures numerical stability, and allows flexible and accurate computation in MEC systems, establishing its superiority over traditional CDC methods.

Abstract: Collaborative mobile edge computing (MEC) has emerged as a promising paradigm
to enable low-capability edge nodes to cooperatively execute
computation-intensive tasks. However, straggling edge nodes (stragglers)
significantly degrade the performance of MEC systems by prolonging computation
latency. While coded distributed computing (CDC) as an effective technique is
widely adopted to mitigate straggler effects, existing CDC schemes exhibit two
critical limitations: (i) They cannot successfully decode the final result
unless the number of received results reaches a fixed recovery threshold, which
seriously restricts their flexibility; (ii) They suffer from inherent poles in
their encoding/decoding functions, leading to decoding inaccuracies and
numerical instability in the computational results. To address these
limitations, this paper proposes an approximated CDC scheme based on
barycentric rational interpolation. The proposed CDC scheme offers several
outstanding advantages. Firstly, it can decode the final result leveraging any
returned results from workers. Secondly, it supports computations over both
finite and real fields while ensuring numerical stability. Thirdly, its
encoding/decoding functions are free of poles, which not only enhances
approximation accuracy but also achieves flexible accuracy tuning. Fourthly, it
integrates a novel BRI-based gradient coding algorithm accelerating the
training process while providing robustness against stragglers. Finally,
experimental results reveal that the proposed scheme is superior to existing
CDC schemes in both waiting time and approximate accuracy.

</details>


### [148] [Weaker Assumptions for Asymmetric Trust](https://arxiv.org/abs/2509.09493)
*Ignacio Amores-Sesar,Christian Cachin,Juan Villacis*

Main category: cs.DC

TL;DR: The paper extends the study of distributed systems by proposing new methods for reliable broadcast and consensus in the presence of asymmetric trust, using weaker assumptions.


<details>
  <summary>Details</summary>
Motivation: Investigate the challenges and limitations of achieving reliable broadcast and consensus in distributed systems with asymmetric trust assumptions.

Method: The paper introduces a new approach to characterize asymmetric problems, then develops algorithms for reliable broadcast and consensus that rely on weaker assumptions compared to previous solutions.

Result: Algorithms for reliable broadcast and consensus are proposed, which operate under less restrictive conditions, making them applicable to broader cases.

Conclusion: The proposed methods demonstrate that weaker assumptions can effectively address the limitations of classical properties of consistency and availability in asymmetric quorum systems, opening pathways for further work in asymmetric trust scenarios.

Abstract: In distributed systems with asymmetric trust, each participant is free to
make its own trust assumptions about others, captured by an asymmetric quorum
system. This contrasts with ordinary, symmetric quorum systems and threshold
models, where trust assumptions are uniformly shared among participants.
Fundamental problems like reliable broadcast and consensus are unsolvable in
the asymmetric model if quorum systems satisfy only the classical properties of
consistency and availability. Existing approaches overcome this by introducing
stronger assumptions. We show that some of these assumptions are overly
restrictive, so much so that they effectively eliminate the benefits of
asymmetric trust. To address this, we propose a new approach to characterize
asymmetric problems and, building upon it, present algorithms for reliable
broadcast and consensus that require weaker assumptions than previous
solutions. Our methods are general and can be extended to other core problems
in systems with asymmetric trust.

</details>


### [149] [TrEnv: Transparently Share Serverless Execution Environments Across Different Functions and Nodes](https://arxiv.org/abs/2509.09525)
*Jialiang Huang,Teng Ma,Zheng Liu,Sixing Lin,Kang Chen,Jinlei Jiang,Xia Liao,Yingdi Shan,Yongwei Wu,Ning Zhang,Mengting Lu,Tao Ma,Haifeng Gong,Mingxing Zhang*

Main category: cs.DC

TL;DR: TrEnv is a co-designed serverless platform tailored for LLM agents, achieving significant improvements in latency and memory usage compared to existing systems.


<details>
  <summary>Details</summary>
Motivation: Emerging workloads like LLM agents demand efficient serverless solutions as traditional serverless computing incurs excessive costs and overhead when handling unpredictable invocation patterns and variable resource requirements.

Method: TrEnv employs repurposable sandboxes and memory templates for fast reuse and restoration of execution environments, along with browser sharing and page cache bypass for VM-based agent workloads.

Result: TrEnv achieves up to 7X lower P99 latency and 48% reduced memory usage in container-based settings, along with 58% lower P99 latency and 61% memory savings for VM-based agents, surpassing systems like E2B.

Conclusion: TrEnv is a high-density serverless platform optimized for LLM agents, effectively reducing infrastructure overhead while ensuring scalable and efficient performance for emerging AI workloads.

Abstract: Serverless computing provides dynamic scalability, but its infrastructure
overhead becomes a bottleneck for emerging workloads such as LLM agents, which
exhibit unpredictable invocation patterns and variable resource demands. Our
analysis shows that for these agents, the cost of running on serverless
platforms can reach up to 70% of the cost of LLM API calls. This finding
motivates the need for a more efficient, high-density serverless platform. We
present TrEnv, a co-designed serverless platform that supports both container-
and VM-based environments, optimized for the unique demands of LLM agents.
TrEnv reduces startup latency and memory usage through repurposable sandboxes
and memory templates, which enable fast reuse and restoration of execution
environments. To further reduce overhead in VM-based agent workloads, TrEnv
leverages browser sharing and a page cache bypassing mechanism. Evaluations
show that TrEnv reduces P99 latency by up to 7X and memory usage by 48% in
container-based settings, and achieves up to 58% lower P99 latency and 61%
memory savings for VM-based agents compared to state-of-the-art systems like
E2B.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [150] [Uncertainty Estimation using Variance-Gated Distributions](https://arxiv.org/abs/2509.08846)
*H. Martin Gillis,Isaac Xu,Thomas Trappenberg*

Main category: cs.LG

TL;DR: The paper introduces a framework for uncertainty estimation and decomposition using the signal-to-noise ratio of class probability distributions, addressing concerns about additive decomposition. It employs a variance-gated measure that scales predictions with confidence factors from ensembles.


<details>
  <summary>Details</summary>
Motivation: Current methods for decomposing predictive uncertainty in neural networks are questioned, particularly around additive decomposition, necessitating a new approach for high-risk decision-making.

Method: The proposed framework leverages the signal-to-noise ratio for uncertainty decomposition and uses a variance-gated measure to scale predictions, incorporating confidence factors derived from ensembles.

Result: The framework facilitates a deeper understanding of uncertainty and highlights instances of diversity collapse in committee machines.

Conclusion: The new approach offers an intuitive way to decompose uncertainties and could improve decision-making in high-risk neural network applications by addressing model and data-related uncertainty challenges.

Abstract: Evaluation of per-sample uncertainty quantification from neural networks is
essential for decision-making involving high-risk applications. A common
approach is to use the predictive distribution from Bayesian or approximation
models and decompose the corresponding predictive uncertainty into epistemic
(model-related) and aleatoric (data-related) components. However, additive
decomposition has recently been questioned. In this work, we propose an
intuitive framework for uncertainty estimation and decomposition based on the
signal-to-noise ratio of class probability distributions across different model
predictions. We introduce a variance-gated measure that scales predictions by a
confidence factor derived from ensembles. We use this measure to discuss the
existence of a collapse in the diversity of committee machines.

</details>


### [151] [Instance-Optimal Matrix Multiplicative Weight Update and Its Quantum Applications](https://arxiv.org/abs/2509.08911)
*Weiyuan Gong,Tongyang Li,Xinzhao Wang,Zhiyu Zhang*

Main category: cs.LG

TL;DR: The paper introduces an enhanced algorithm for matrix learning that achieves better regret bounds than the classical MMWU algorithm, using novel techniques.


<details>
  <summary>Details</summary>
Motivation: Matrix Multiplicative Weight Update (MMWU) achieves minimax-optimal regret bound, and the paper aims to improve this performance through an instance-optimal approach.

Method: Developed a general potential-based framework for matrix learning problems, integrating advanced inequality techniques and optimal potentials from vector LEA.

Result: The proposed algorithm achieves an instance-optimal regret bound "O(\sqrt{T\cdot S(X||d^{-1}I_d))}" and maintains computational complexity similar to MMWU.

Conclusion: The algorithm outperforms state-of-the-art approaches in various quantum learning applications, providing enhancements without additional computational cost.

Abstract: The Matrix Multiplicative Weight Update (MMWU) is a seminal online learning
algorithm with numerous applications. Applied to the matrix version of the
Learning from Expert Advice (LEA) problem on the $d$-dimensional spectraplex,
it is well known that MMWU achieves the minimax-optimal regret bound of
$O(\sqrt{T\log d})$, where $T$ is the time horizon. In this paper, we present
an improved algorithm achieving the instance-optimal regret bound of
$O(\sqrt{T\cdot S(X||d^{-1}I_d)})$, where $X$ is the comparator in the regret,
$I_d$ is the identity matrix, and $S(\cdot||\cdot)$ denotes the quantum
relative entropy. Furthermore, our algorithm has the same computational
complexity as MMWU, indicating that the improvement in the regret bound is
``free''.
  Technically, we first develop a general potential-based framework for matrix
LEA, with MMWU being its special case induced by the standard exponential
potential. Then, the crux of our analysis is a new ``one-sided'' Jensen's trace
inequality built on a Laplace transform technique, which allows the application
of general potential functions beyond exponential to matrix LEA. Our algorithm
is finally induced by an optimal potential function from the vector LEA
problem, based on the imaginary error function.
  Complementing the above, we provide a memory lower bound for matrix LEA, and
explore the applications of our algorithm in quantum learning theory. We show
that it outperforms the state of the art for learning quantum states corrupted
by depolarization noise, random quantum states, and Gibbs states. In addition,
applying our algorithm to linearized convex losses enables predicting nonlinear
quantum properties, such as purity, quantum virtual cooling, and R\'{e}nyi-$2$
correlation.

</details>


### [152] [Corruption-Tolerant Asynchronous Q-Learning with Near-Optimal Rates](https://arxiv.org/abs/2509.08933)
*Sreejeet Maity,Aritra Mitra*

Main category: cs.LG

TL;DR: The paper proposes a robust Q-learning algorithm for reinforcement learning in environments with adversarially corrupted rewards, matching non-adversarial performance bounds up to a corruption factor.


<details>
  <summary>Details</summary>
Motivation: Classical RL algorithms like Q-learning suffer from significant performance degradation when reward signals are corrupted by adversarial factors, such as noise, sensor faults, or attacks.

Method: The authors develop a variant of Q-learning that is robust to a fraction of arbitrarily corrupted rewards and analyze its finite-time convergence. They also implement a refined Azuma-Hoeffding inequality to address cases where true reward statistics are unknown.

Result: The robust Q-learning algorithm achieves convergence rates comparable to the non-adversarial case, with an unavoidable additive corruption term. For scenarios without prior reward distribution knowledge, a refined analysis supported the algorithm's robustness.

Conclusion: The paper offers the first finite-time robustness guarantees for asynchronous Q-learning in adversarial environments, filling a critical gap in robust reinforcement learning research.

Abstract: We consider the problem of learning the optimal policy in a discounted,
infinite-horizon reinforcement learning (RL) setting where the reward signal is
subject to adversarial corruption. Such corruption, which may arise from
extreme noise, sensor faults, or malicious attacks, can severely degrade the
performance of classical algorithms such as Q-learning. To address this
challenge, we propose a new provably robust variant of the Q-learning algorithm
that operates effectively even when a fraction of the observed rewards are
arbitrarily perturbed by an adversary. Under the asynchronous sampling model
with time-correlated data, we establish that despite adversarial corruption,
the finite-time convergence rate of our algorithm matches that of existing
results for the non-adversarial case, up to an additive term proportional to
the fraction of corrupted samples. Moreover, we derive an information-theoretic
lower bound revealing that the additive corruption term in our upper bounds is
unavoidable.
  Next, we propose a variant of our algorithm that requires no prior knowledge
of the statistics of the true reward distributions. The analysis of this
setting is particularly challenging and is enabled by carefully exploiting a
refined Azuma-Hoeffding inequality for almost-martingales, a technical tool
that might be of independent interest. Collectively, our contributions provide
the first finite-time robustness guarantees for asynchronous Q-learning,
bridging a significant gap in robust RL.

</details>


### [153] [Group Distributionally Robust Machine Learning under Group Level Distributional Uncertainty](https://arxiv.org/abs/2509.08942)
*Xenia Konti,Yi Shen,Zifan Wang,Karl Henrik Johansson,Michael J. Pencina,Nicoleta J. Economou-Zavlanos,Michael M. Zavlanos*

Main category: cs.LG

TL;DR: Machine learning models often underperform for minority groups due to data distribution issues, and the authors propose a robust framework using Wasserstein-based DRO to address this.


<details>
  <summary>Details</summary>
Motivation: Standard ML methods often fail for atypical or underrepresented data groups due to spurious correlations and distribution estimation challenges.

Method: They employ Wasserstein-based distributionally robust optimization (DRO) combined with a gradient descent-ascent algorithm to handle uncertainty within group distributions.

Result: The proposed method successfully addresses distributional uncertainty and optimizes worst-group performance in real-world datasets.

Conclusion: This framework mitigates performance degradation for minority groups and validates its efficiency through convergence proofs and real-world application results.

Abstract: The performance of machine learning (ML) models critically depends on the
quality and representativeness of the training data. In applications with
multiple heterogeneous data generating sources, standard ML methods often learn
spurious correlations that perform well on average but degrade performance for
atypical or underrepresented groups. Prior work addresses this issue by
optimizing the worst-group performance. However, these approaches typically
assume that the underlying data distributions for each group can be accurately
estimated using the training data, a condition that is frequently violated in
noisy, non-stationary, and evolving environments. In this work, we propose a
novel framework that relies on Wasserstein-based distributionally robust
optimization (DRO) to account for the distributional uncertainty within each
group, while simultaneously preserving the objective of improving the
worst-group performance. We develop a gradient descent-ascent algorithm to
solve the proposed DRO problem and provide convergence results. Finally, we
validate the effectiveness of our method on real-world data.

</details>


### [154] [FoundationalECGNet: A Lightweight Foundational Model for ECG-based Multitask Cardiac Analysis](https://arxiv.org/abs/2509.08961)
*Md. Sajeebul Islam Sk.,Md Jobayer,Md Mehedi Hasan Shawon,Md. Golam Raibul Alam*

Main category: cs.LG

TL;DR: The paper introduces FoundationalECGNet, a diagnostic framework for ECG classification that achieves high accuracy in detecting cardiac conditions.


<details>
  <summary>Details</summary>
Motivation: To overcome challenges of noise, class imbalance, and dataset heterogeneity in current ECG analysis methods for cardiovascular disease diagnostics.

Method: FoundationalECGNet employs dual-stage wavelet denoising, CBAM, GAT, and TST to analyze spatial and temporal dependencies in ECG signals, classifying normal versus abnormal signals and identifying specific cardiac conditions.

Result: The framework achieved up to a 99% F1-score in classifying Normal vs. Abnormal ECG signals and state-of-the-art performance in multi-class disease detection.

Conclusion: FoundationalECGNet offers a scalable, generalizable, and interpretable solution, enhancing diagnosis precision and supporting clinical decision-making.

Abstract: Cardiovascular diseases (CVDs) remain a leading cause of mortality worldwide,
underscoring the importance of accurate and scalable diagnostic systems.
Electrocardiogram (ECG) analysis is central to detecting cardiac abnormalities,
yet challenges such as noise, class imbalance, and dataset heterogeneity limit
current methods. To address these issues, we propose FoundationalECGNet, a
foundational framework for automated ECG classification. The model integrates a
dual-stage denoising by Morlet and Daubechies wavelets transformation,
Convolutional Block Attention Module (CBAM), Graph Attention Networks (GAT),
and Time Series Transformers (TST) to jointly capture spatial and temporal
dependencies in multi-channel ECG signals. FoundationalECGNet first
distinguishes between Normal and Abnormal ECG signals, and then classifies the
Abnormal signals into one of five cardiac conditions: Arrhythmias, Conduction
Disorders, Myocardial Infarction, QT Abnormalities, or Hypertrophy. Across
multiple datasets, the model achieves a 99% F1-score for Normal vs. Abnormal
classification and shows state-of-the-art performance in multi-class disease
detection, including a 99% F1-score for Conduction Disorders and Hypertrophy,
as well as a 98.9% F1-score for Arrhythmias. Additionally, the model provides
risk level estimations to facilitate clinical decision-making. In conclusion,
FoundationalECGNet represents a scalable, interpretable, and generalizable
solution for automated ECG analysis, with the potential to improve diagnostic
precision and patient outcomes in healthcare settings. We'll share the code
after acceptance.

</details>


### [155] [Value bounds and Convergence Analysis for Averages of LRP attributions](https://arxiv.org/abs/2509.08963)
*Alexander Binder,Nastaran Takmil-Homayouni,Urun Dogan*

Main category: cs.LG

TL;DR: This paper analyzes Layer-wise Relevance Propagation (LRP) by representing its attribution methods as modified gradient matrix products, offering insights into attribution distributions and bounds.


<details>
  <summary>Details</summary>
Motivation: Investigate numerical properties of LRP-type attribution methods to enhance understanding of their behavior under different scenarios and compare them to alternative methods.

Method: Represent LRP attribution methods as matrix products, derive bounds for singular values, and component-wise bounds for attribution maps, and analyze convergence constants.

Result: Key constants underlying the convergence of empirical means are derived, revealing distinctions between LRP-beta and other methods like gradient-based approaches and LRP-epsilon.

Conclusion: LRP-beta's constants remain independent of weight norms, highlighting its robustness compared to gradient-based methods and LRP-epsilon in Smoothgrad-type attribution contexts.

Abstract: We analyze numerical properties of Layer-wise relevance propagation
(LRP)-type attribution methods by representing them as a product of modified
gradient matrices. This representation creates an analogy to matrix
multiplications of Jacobi-matrices which arise from the chain rule of
differentiation. In order to shed light on the distribution of attribution
values, we derive upper bounds for singular values. Furthermore we derive
component-wise bounds for attribution map values. As a main result, we apply
these component-wise bounds to obtain multiplicative constants. These constants
govern the convergence of empirical means of attributions to expectations of
attribution maps. This finding has important implications for scenarios where
multiple non-geometric data augmentations are applied to individual test
samples, as well as for Smoothgrad-type attribution methods. In particular, our
analysis reveals that the constants for LRP-beta remain independent of weight
norms, a significant distinction from both gradient-based methods and
LRP-epsilon.

</details>


### [156] [Green Federated Learning via Carbon-Aware Client and Time Slot Scheduling](https://arxiv.org/abs/2509.08980)
*Daniel Richards Arputharaj,Charlotte Rodriguez,Angelo Rodio,Giovanni Neglia*

Main category: cs.LG

TL;DR: The paper explores methods to reduce carbon emissions in Federated Learning (FL) through carbon-aware client selection and training schedules.


<details>
  <summary>Details</summary>
Motivation: Federated Learning (FL), due to its decentralized nature, offers an opportunity to harness regional and temporal variations in Carbon Intensity (CI) to mitigate the carbon footprint of training large-scale machine learning models.

Method: The paper introduces a carbon-aware scheduler that balances slack timing, $
alpha$-fair carbon allocation, and a global fine-tuning phase to optimize client training schedules for lower carbon intensity.

Result: Experimental results show that the carbon-aware scheduler leads to higher model accuracy compared to slack-agnostic baselines, particularly under strict carbon budgets.

Conclusion: Carbon-aware training strategies in FL are effective in reducing emissions while maintaining or enhancing model performance, especially when carbon constraints are tight.

Abstract: Training large-scale machine learning models incurs substantial carbon
emissions. Federated Learning (FL), by distributing computation across
geographically dispersed clients, offers a natural framework to leverage
regional and temporal variations in Carbon Intensity (CI). This paper
investigates how to reduce emissions in FL through carbon-aware client
selection and training scheduling. We first quantify the emission savings of a
carbon-aware scheduling policy that leverages slack time -- permitting a modest
extension of the training duration so that clients can defer local training
rounds to lower-carbon periods. We then examine the performance trade-offs of
such scheduling which stem from statistical heterogeneity among clients,
selection bias in participation, and temporal correlation in model updates. To
leverage these trade-offs, we construct a carbon-aware scheduler that
integrates slack time, $\alpha$-fair carbon allocation, and a global
fine-tuning phase. Experiments on real-world CI data show that our scheduler
outperforms slack-agnostic baselines, achieving higher model accuracy across a
wide range of carbon budgets, with especially strong gains under tight carbon
constraints.

</details>


### [157] [STRIDE: Scalable and Interpretable XAI via Subset-Free Functional Decomposition](https://arxiv.org/abs/2509.09070)
*Chaeyun Ko*

Main category: cs.LG

TL;DR: STRIDE is an explainable AI framework that avoids typical limitations like exponential computation and scalar summarizations by using functional decomposition in a Reproducing Kernel Hilbert Space (RKHS), achieving scalability, high speed, and model-agnostic global and local views.


<details>
  <summary>Details</summary>
Motivation: Most existing explainable AI frameworks struggle with scalability due to the high cost of reasoning over feature subsets and are often limited to presenting effects as single scalar values, reducing their expressiveness.

Method: The paper introduces STRIDE, which bypasses subset enumeration and employs orthogonal functional decomposition in a Reproducing Kernel Hilbert Space. The method calculates functional components via an analytical projection and recursive kernel-centering procedure, allowing for a model-agnostic and scalable explanation framework.

Result: STRIDE demonstrated speedups of up to 9.7 times in some cases and approximately 3.0 times on average across 10 datasets while maintaining high fidelity with R^2 scores between 0.81 and 0.999. It also provides a structured perspective and introduces diagnostics like 'component surgery.'

Conclusion: STRIDE addresses the limitations of existing XAI frameworks by providing a scalable and expressive alternative, balancing efficiency and fidelity, and enabling new types of interaction-focused diagnostics.

Abstract: Most explainable AI (XAI) frameworks face two practical limitations: the
exponential cost of reasoning over feature subsets and the reduced
expressiveness of summarizing effects as single scalar values. We present
STRIDE, a scalable framework that aims to mitigate both issues by framing
explanation as a subset-enumeration-free, orthogonal functional decomposition
in a Reproducing Kernel Hilbert Space (RKHS). Rather than focusing only on
scalar attributions, STRIDE computes functional components f_S(x_S) via an
analytical projection scheme based on a recursive kernel-centering procedure,
avoiding explicit subset enumeration. In the tabular setups we study, the
approach is model-agnostic, provides both local and global views, and is
supported by theoretical results on orthogonality and L^2 convergence under
stated assumptions. On public tabular benchmarks in our environment, we
observed speedups ranging from 0.6 times (slower than TreeSHAP on a small
dataset) to 9.7 times (California), with a median approximate 3.0 times across
10 datasets, while maintaining high fidelity (R^2 between 0.81 and 0.999) and
substantial rank agreement on most datasets. Overall, STRIDE complements scalar
attribution methods by offering a structured functional perspective, enabling
novel diagnostics like 'component surgery' to quantitatively measure the impact
of specific interactions within our experimental scope.

</details>


### [158] [Active Learning and Explainable AI for Multi-Objective Optimization of Spin Coated Polymers](https://arxiv.org/abs/2509.08988)
*Brendan Young,Brendan Alvey,Andreas Werbrouck,Will Murphy,James Keller,Mattias J. Young,Matthew Maschmann*

Main category: cs.LG

TL;DR: The paper introduces a framework that combines an optimization algorithm (PyePAL), visualization (UMAP), and explainable AI methods for optimizing the mechanical properties of polymer thin films created through spin coating.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the challenge of optimizing polymer thin films with specific mechanical properties (hardness and elasticity), which involves balancing multiple objectives and understanding high-dimensional data relationships.

Method: The framework integrates PyePAL (for active Pareto front learning using Gaussian processes), UMAP (for visualizing high-dimensional optimization), and fuzzy linguistic summaries (for explainable insights). This combination guides sample selection and links process parameters to objectives in an interpretable manner.

Result: Experiments show that the framework effectively identifies optimal polymer designs and provides interpretable insights through visualization and linguistic summaries, aiding expert analysis.

Conclusion: The proposed approach successfully optimizes polymer films while simultaneously improving explainability and enabling expert-driven decision-making using AI-enhanced tools.

Abstract: Spin coating polymer thin films to achieve specific mechanical properties is
inherently a multi-objective optimization problem. We present a framework that
integrates an active Pareto front learning algorithm (PyePAL) with
visualization and explainable AI techniques to optimize processing parameters.
PyePAL uses Gaussian process models to predict objective values (hardness and
elasticity) from the design variables (spin speed, dilution, and polymer
mixture), guiding the adaptive selection of samples toward promising regions of
the design space. To enable interpretable insights into the high-dimensional
design space, we utilize UMAP (Uniform Manifold Approximation and Projection)
for two-dimensional visualization of the Pareto front exploration.
Additionally, we incorporate fuzzy linguistic summaries, which translate the
learned relationships between process parameters and performance objectives
into linguistic statements, thus enhancing the explainability and understanding
of the optimization results. Experimental results demonstrate that our method
efficiently identifies promising polymer designs, while the visual and
linguistic explanations facilitate expert-driven analysis and knowledge
discovery.

</details>


### [159] [ProDiGy: Proximity- and Dissimilarity-Based Byzantine-Robust Federated Learning](https://arxiv.org/abs/2509.09534)
*Sena Ergisi,Luis Maßny,Rawad Bitar*

Main category: cs.LG

TL;DR: The paper presents "ProDiGy," a Byzantine-robust Federated Learning algorithm, which employs dual scoring to evaluate gradient proximity and dissimilarity, ensuring resilience under data heterogeneity.


<details>
  <summary>Details</summary>
Motivation: Federated learning is vulnerable to adversarial attacks, particularly under non-IID data distributions. Creating robust methods is crucial for ensuring reliable and secure distributed learning.

Method: The algorithm employs a joint dual scoring mechanism to evaluate client gradients based on their proximity and dissimilarity. This distinguishes honest clients naturally while identifying malicious attacks based on suspicious uniformity.

Result: Experimental results demonstrate ProDiGy’s superior performance over existing defense mechanisms in maintaining model accuracy and robust defenses, especially in non-IID data distribution scenarios.

Conclusion: The dual perspective approach of ProDiGy effectively balances similarity promotion among honest clients and detects attacks, demonstrating robust defenses and model accuracy in heterogeneous data environments.

Abstract: Federated Learning (FL) emerged as a widely studied paradigm for distributed
learning. Despite its many advantages, FL remains vulnerable to adversarial
attacks, especially under data heterogeneity. We propose a new Byzantine-robust
FL algorithm called ProDiGy. The key novelty lies in evaluating the client
gradients using a joint dual scoring system based on the gradients' proximity
and dissimilarity. We demonstrate through extensive numerical experiments that
ProDiGy outperforms existing defenses in various scenarios. In particular, when
the clients' data do not follow an IID distribution, while other defense
mechanisms fail, ProDiGy maintains strong defense capabilities and model
accuracy. These findings highlight the effectiveness of a dual perspective
approach that promotes natural similarity among honest clients while detecting
suspicious uniformity as a potential indicator of an attack.

</details>


### [160] [Fast attention mechanisms: a tale of parallelism](https://arxiv.org/abs/2509.09001)
*Jingwen Liu,Hantao Yu,Clayton Sanford,Alexandr Andoni,Daniel Hsu*

Main category: cs.LG

TL;DR: Introduces ANNA, an efficient attention mechanism for transformers that reduces time complexity while retaining representational power.


<details>
  <summary>Details</summary>
Motivation: Transformers face scalability issues due to their quadratic time complexity.

Method: Developed Approximate Nearest Neighbor Attention (ANNA) with sub-quadratic complexity and formal proofs of its expressive power and applicability.

Result: ANNA-transformers can match standard attention in capabilities, solve reasoning tasks efficiently, and simulate low-rank transformers with constant depth.

Conclusion: ANNA extends transformer scalability and unifies reasoning about efficient attention mechanisms while maintaining expressive capabilities.

Abstract: Transformers have the representational capacity to simulate Massively
Parallel Computation (MPC) algorithms, but they suffer from quadratic time
complexity, which severely limits their scalability. We introduce an efficient
attention mechanism called Approximate Nearest Neighbor Attention (ANNA) with
sub-quadratic time complexity. We prove that ANNA-transformers (1) retain the
expressive power previously established for standard attention in terms of
matching the capabilities of MPC algorithms, and (2) can solve key reasoning
tasks such as Match2 and $k$-hop with near-optimal depth. Using the MPC
framework, we further prove that constant-depth ANNA-transformers can simulate
constant-depth low-rank transformers, thereby providing a unified way to reason
about a broad class of efficient attention approximations.

</details>


### [161] [Open-sci-ref-0.01: open and reproducible reference baselines for language model and dataset comparison](https://arxiv.org/abs/2509.09009)
*Marianna Nezhurina,Taishi Nakamura,Timur Carstensen,Niccolò Ajroldi,Ville Komulainen,David Salinas,Jenia Jitsev*

Main category: cs.LG

TL;DR: Open-sci-ref presents transformer models (0.13B-1.7B parameters) trained on 8 open reference datasets. It establishes baselines to assess training standards and includes tools for reproduction and comparison.


<details>
  <summary>Details</summary>
Motivation: The paper aims to create robust baselines for evaluating scaling trends and training procedures across various open reference datasets.

Method: Dense transformer models are trained across different scales and datasets, providing intermediate checkpoints, logs, and standardized benchmarks for comparison.

Result: Models trained on NemoTron-CC HQ dataset performed best, followed by DCLM-baseline and FineWeb-Edu, showcasing benefits of particular datasets.

Conclusion: Open-sci-ref provides essential resources to standardize research, assess training dynamics, and align methods on a computational axis.

Abstract: We introduce open-sci-ref, a family of dense transformer models trained as
research baselines across multiple model (0.13B to 1.7B parameters) and token
scales (up to 1T) on 8 recent open reference datasets. Evaluating the models on
various standardized benchmarks, our training runs set establishes reference
points that enable researchers to assess the sanity and quality of alternative
training approaches across scales and datasets. Intermediate checkpoints allow
comparison and studying of the training dynamics. The established reference
baselines allow training procedures to be compared through their scaling
trends, aligning them on a common compute axis. Comparison of open reference
datasets reveals that training on NemoTron-CC HQ consistently outperforms other
reference datasets, followed by DCLM-baseline and FineWeb-Edu. In addition to
intermediate training checkpoints, the release includes logs, code, and
downstream evaluations to simplify reproduction, standardize comparison, and
facilitate future research.

</details>


### [162] [Deep Context-Conditioned Anomaly Detection for Tabular Data](https://arxiv.org/abs/2509.09030)
*Spencer King,Zhilu Zhang,Ruofan Yu,Baris Coskun,Wei Ding,Qian Cui*

Main category: cs.LG

TL;DR: The paper introduces a framework for anomaly detection in tabular data using context-aware techniques to improve accuracy.


<details>
  <summary>Details</summary>
Motivation: Anomaly detection in large-scale tabular data is challenging due to the unsupervised nature and heterogeneous contexts often present in real-world datasets.

Method: The authors propose a context-conditional framework that identifies context features and models the conditional distribution using a deep autoencoder.

Result: Extensive experiments revealed superior performance of the method compared to state-of-the-art anomaly detection approaches.

Conclusion: Incorporating context significantly enhances anomaly detection capabilities in tabular datasets, showing promise for practical applications in cybersecurity and finance.

Abstract: Anomaly detection is critical in domains such as cybersecurity and finance,
especially when working with large-scale tabular data. Yet, unsupervised
anomaly detection -- where no labeled anomalies are available -- remains a
significant challenge. Although various deep learning methods have been
proposed to model a dataset's joint distribution, real-world tabular data often
contain heterogeneous contexts (e.g., different users), making globally rare
events normal under certain contexts. Consequently, relying on a single global
distribution can overlook these contextual nuances, degrading detection
performance. In this paper, we present a context-conditional anomaly detection
framework tailored for tabular datasets. Our approach automatically identifies
context features and models the conditional data distribution using a simple
deep autoencoder. Extensive experiments on multiple tabular benchmark datasets
demonstrate that our method outperforms state-of-the-art approaches,
underscoring the importance of context in accurately distinguishing anomalous
from normal instances.

</details>


### [163] [MoWE : A Mixture of Weather Experts](https://arxiv.org/abs/2509.09052)
*Dibyajyoti Chakraborty,Romit Maulik,Peter Harrington,Dallas Foster,Mohammad Amin Nabian,Sanjay Choudhry*

Main category: cs.LG

TL;DR: The paper proposes a Mixture of Experts (MoWE) model for weather forecasting that optimally combines outputs of existing models, achieving up to 10% lower RMSE over the best individual model without requiring significant computational resources.


<details>
  <summary>Details</summary>
Motivation: Recent data-driven weather models have plateaued in performance despite reaching the state-of-the-art, necessitating a novel approach to improve forecast accuracy without building entirely new models.

Method: The paper introduces a Mixture of Experts mechanism using a Vision Transformer-based gating network. This network dynamically learns to weight the inputs of multiple expert models conditioned on grid points and forecast lead times to produce a synthesized deterministic forecast.

Result: The MoWE approach achieved up to 10% reduction in RMSE on a 2-day forecast horizon compared to the best AI weather model and significantly outperformed both individual experts and simple averaging of experts.

Conclusion: The study demonstrates that MoWE is a computation-efficient and scalable solution that enhances weather prediction accuracy by leveraging and optimally combining advanced models, pushing the state-of-the-art performance.

Abstract: Data-driven weather models have recently achieved state-of-the-art
performance, yet progress has plateaued in recent years. This paper introduces
a Mixture of Experts (MoWE) approach as a novel paradigm to overcome these
limitations, not by creating a new forecaster, but by optimally combining the
outputs of existing models. The MoWE model is trained with significantly lower
computational resources than the individual experts. Our model employs a Vision
Transformer-based gating network that dynamically learns to weight the
contributions of multiple "expert" models at each grid point, conditioned on
forecast lead time. This approach creates a synthesized deterministic forecast
that is more accurate than any individual component in terms of Root Mean
Squared Error (RMSE). Our results demonstrate the effectiveness of this method,
achieving up to a 10% lower RMSE than the best-performing AI weather model on a
2-day forecast horizon, significantly outperforming individual experts as well
as a simple average across experts. This work presents a computationally
efficient and scalable strategy to push the state of the art in data-driven
weather prediction by making the most out of leading high-quality forecast
models.

</details>


### [164] [A Scoping Review of Machine Learning Applications in Power System Protection and Disturbance Management](https://arxiv.org/abs/2509.09053)
*Julian Oelhaf,Georg Kordowich,Mehran Pashaei,Christian Bergler,Andreas Maier,Johann Jäger,Siming Bayer*

Main category: cs.LG

TL;DR: This review identifies the role and challenges of applying machine learning in power system protection and disturbance management, highlighting the need for real-world validation and standards.


<details>
  <summary>Details</summary>
Motivation: The integration of renewable and distributed energy resources disrupts traditional protection schemes for modern power systems.

Method: The review synthesizes over 100 ML-focused publications using the PRISMA framework, introduces a taxonomy, resolves terminology inconsistencies, and proposes guidelines for standardization.

Result: ML models show high accuracy in simulated datasets but face limitations in real-world applications due to fragmented methodologies and lack of standards.

Conclusion: Standardized datasets, robust validation, and exploration of advanced ML architectures are necessary for practical deployment in dynamic power grids.

Abstract: The integration of renewable and distributed energy resources reshapes modern
power systems, challenging conventional protection schemes. This scoping review
synthesizes recent literature on machine learning (ML) applications in power
system protection and disturbance management, following the PRISMA for Scoping
Reviews framework. Based on over 100 publications, three key objectives are
addressed: (i) assessing the scope of ML research in protection tasks; (ii)
evaluating ML performance across diverse operational scenarios; and (iii)
identifying methods suitable for evolving grid conditions. ML models often
demonstrate high accuracy on simulated datasets; however, their performance
under real-world conditions remains insufficiently validated. The existing
literature is fragmented, with inconsistencies in methodological rigor, dataset
quality, and evaluation metrics. This lack of standardization hampers the
comparability of results and limits the generalizability of findings. To
address these challenges, this review introduces a ML-oriented taxonomy for
protection tasks, resolves key terminological inconsistencies, and advocates
for standardized reporting practices. It further provides guidelines for
comprehensive dataset documentation, methodological transparency, and
consistent evaluation protocols, aiming to improve reproducibility and enhance
the practical relevance of research outcomes. Critical gaps remain, including
the scarcity of real-world validation, insufficient robustness testing, and
limited consideration of deployment feasibility. Future research should
prioritize public benchmark datasets, realistic validation methods, and
advanced ML architectures. These steps are essential to move ML-based
protection from theoretical promise to practical deployment in increasingly
dynamic and decentralized power systems.

</details>


### [165] ["A 6 or a 9?": Ensemble Learning Through the Multiplicity of Performant Models and Explanations](https://arxiv.org/abs/2509.09073)
*Gianlucca Zuin,Adriano Veloso*

Main category: cs.LG

TL;DR: The study introduces the Rashomon Ensemble, a method for improving model generalization by strategically selecting diverse high-performing models to form ensembles.


<details>
  <summary>Details</summary>
Motivation: Selecting models that generalize well in real-world scenarios poses challenges due to the Rashomon Effect. There's a need for a solution that utilizes diverse patterns in data to enhance predictive accuracy.

Method: The Rashomon Ensemble method groups models based on both performance and explanations, ensuring diversity within the ensemble. This approach maintains accuracy while improving robustness against distribution shifts.

Result: Validation on real-world datasets showed up to 0.20+ AUROC improvements in scenarios with a large Rashomon ratio, indicating the method's capability to handle diverse data effectively.

Conclusion: The Rashomon Ensemble enhances generalization, robustness, and practical applicability, facilitating improved predictive accuracy and business benefits in real-world applications.

Abstract: Creating models from past observations and ensuring their effectiveness on
new data is the essence of machine learning. However, selecting models that
generalize well remains a challenging task. Related to this topic, the Rashomon
Effect refers to cases where multiple models perform similarly well for a given
learning problem. This often occurs in real-world scenarios, like the
manufacturing process or medical diagnosis, where diverse patterns in data lead
to multiple high-performing solutions. We propose the Rashomon Ensemble, a
method that strategically selects models from these diverse high-performing
solutions to improve generalization. By grouping models based on both their
performance and explanations, we construct ensembles that maximize diversity
while maintaining predictive accuracy. This selection ensures that each model
covers a distinct region of the solution space, making the ensemble more robust
to distribution shifts and variations in unseen data. We validate our approach
on both open and proprietary collaborative real-world datasets, demonstrating
up to 0.20+ AUROC improvements in scenarios where the Rashomon ratio is large.
Additionally, we demonstrate tangible benefits for businesses in various
real-world applications, highlighting the robustness, practicality, and
effectiveness of our approach.

</details>


### [166] [An entropy formula for the Deep Linear Network](https://arxiv.org/abs/2509.09088)
*Govind Menon,Tianmin Yu*

Main category: cs.LG

TL;DR: This paper investigates the Riemannian geometry of Deep Linear Networks to formulate a thermodynamic perspective on learning, utilizing group actions, Riemannian submersions, and entropy definitions.


<details>
  <summary>Details</summary>
Motivation: To provide a deeper mathematical framework for understanding the learning process in deep linear networks through Riemannian geometry and thermodynamics.

Method: The authors employ group actions to study overparametrization and apply Riemannian submersion from parameter spaces to observable spaces. They construct orthonormal bases for tangent spaces using Jacobi matrices.

Result: The authors compute Boltzmann entropy via balanced manifold foliations and show that the Riemannian geometry on the space of observables arises from the submersion of the balanced manifold.

Conclusion: The study highlights the connections between Riemannian geometry and deep linear network learning, paving the way for future thermodynamic descriptions of learning processes.

Abstract: We study the Riemannian geometry of the Deep Linear Network (DLN) as a
foundation for a thermodynamic description of the learning process. The main
tools are the use of group actions to analyze overparametrization and the use
of Riemannian submersion from the space of parameters to the space of
observables. The foliation of the balanced manifold in the parameter space by
group orbits is used to define and compute a Boltzmann entropy. We also show
that the Riemannian geometry on the space of observables defined in [2] is
obtained by Riemannian submersion of the balanced manifold. The main technical
step is an explicit construction of an orthonormal basis for the tangent space
of the balanced manifold using the theory of Jacobi matrices.

</details>


### [167] [Sensitivity-LoRA: Low-Load Sensitivity-Based Fine-Tuning for Large Language Models](https://arxiv.org/abs/2509.09119)
*Hao Zhang,Bo Huang,Zhenjia Li,Xi Xiao,Hui Yi Leong,Zumeng Zhang,Xinwei Long,Tianyang Wang,Hao Xu*

Main category: cs.LG

TL;DR: This paper introduces Sensitivity-LoRA, a fine-tuning method for Large Language Models that dynamically allocates ranks based on weight sensitivity.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the limitations of existing Low-Rank Adaptation (LoRA) techniques, which lack dynamic rank allocation, are computationally complex, and hinder resource-efficient applications.

Method: The proposed method analyzes weight sensitivity using the Hessian Matrix of the loss function to dynamically allocate ranks, maintaining high efficiency and stability.

Result: Experimental results demonstrated that Sensitivity-LoRA provides robust performance, efficient computations, and stability across various tasks and benchmarks.

Conclusion: Sensitivity-LoRA effectively mitigates the inefficiencies of traditional LoRA approaches, offering practical and adaptive solutions for specialized LLM fine-tuning in resource-constrained settings.

Abstract: Large Language Models (LLMs) have transformed both everyday life and
scientific research. However, adapting LLMs from general-purpose models to
specialized tasks remains challenging, particularly in resource-constrained
environments. Low-Rank Adaptation (LoRA), a prominent method within
Parameter-Efficient Fine-Tuning (PEFT), has emerged as a promising approach to
LLMs by approximating model weight updates using low-rank decomposition.
However, LoRA is limited by its uniform rank ( r ) allocation to each
incremental matrix, and existing rank allocation techniques aimed at addressing
this issue remain computationally inefficient, complex, and unstable, hindering
practical applications. To address these limitations, we propose
Sensitivity-LoRA, an efficient fine-tuning method that dynamically allocates
ranks to weight matrices based on both their global and local sensitivities. It
leverages the second-order derivatives (Hessian Matrix) of the loss function to
effectively capture weight sensitivity, enabling optimal rank allocation with
minimal computational overhead. Our experimental results have demonstrated
robust effectiveness, efficiency and stability of Sensitivity-LoRA across
diverse tasks and benchmarks.

</details>


### [168] [Learning What Matters: Causal Time Series Modeling for Arctic Sea Ice Prediction](https://arxiv.org/abs/2509.09128)
*Emam Hossain,Md Osman Gani*

Main category: cs.LG

TL;DR: The paper introduces a causality-aware deep learning framework incorporating causal feature selection techniques to improve robustness, interpretability, and generalization in prediction models, demonstrated on Arctic Sea Ice Extent forecasting.


<details>
  <summary>Details</summary>
Motivation: Traditional deep learning models often struggle with spurious correlations and lack causal understanding, limiting their generalization, robustness, and interpretability. The paper seeks to address these limitations by integrating causality into predictive modeling.

Method: The framework integrates Multivariate Granger Causality (MVGC) and PCMCI+ in a hybrid neural architecture for causal feature selection. It is applied to Arctic Sea Ice Extent data spanning 1979-2021, using both daily and monthly ocean-atmospheric variables.

Result: The proposed method selects causally influential predictors, prioritizes direct causes, reduces unnecessary features, and improves computational efficiency. It enhances prediction accuracy and interpretability for Arctic Sea Ice Extent forecasting.

Conclusion: The causality-aware framework demonstrates superior performance in prediction tasks while being widely applicable to other high-dimensional, dynamic domains. It advances both the practical and theoretical aspects of integrating causality into predictive modeling.

Abstract: Conventional machine learning and deep learning models typically rely on
correlation-based learning, which often fails to distinguish genuine causal
relationships from spurious associations, limiting their robustness,
interpretability, and ability to generalize. To overcome these limitations, we
introduce a causality-aware deep learning framework that integrates
Multivariate Granger Causality (MVGC) and PCMCI+ for causal feature selection
within a hybrid neural architecture. Leveraging 43 years (1979-2021) of Arctic
Sea Ice Extent (SIE) data and associated ocean-atmospheric variables at daily
and monthly resolutions, the proposed method identifies causally influential
predictors, prioritizes direct causes of SIE dynamics, reduces unnecessary
features, and enhances computational efficiency. Experimental results show that
incorporating causal inputs leads to improved prediction accuracy and
interpretability across varying lead times. While demonstrated on Arctic SIE
forecasting, the framework is broadly applicable to other dynamic,
high-dimensional domains, offering a scalable approach that advances both the
theoretical foundations and practical performance of causality-informed
predictive modeling.

</details>


### [169] [Continuous-Time Value Iteration for Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2509.09135)
*Xuefeng Wang,Lei Zhang,Henglin Pu,Ahmed H. Qureshi,Husheng Li*

Main category: cs.LG

TL;DR: This paper introduces a Continuous-Time Multi-Agent Reinforcement Learning (CT-MARL) framework using physics-informed neural networks (PINNs) and Value Gradient Iteration (VGI) to address scalability and performance issues in multi-agent continuous-time reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: Traditional reinforcement learning methods struggle with high-frequency or irregular interactions in complex dynamical systems, and current continuous-time RL techniques are limited to single-agent applications due to scalability challenges and instability in multi-agent settings.

Method: The CT-MARL framework employs PINNs to approximate Hamilton--Jacobi--Bellman (HJB)-based value functions and introduces a Value Gradient Iteration (VGI) module to align value learning with gradient consistency, improving policy learning in multi-agent environments.

Result: The proposed method consistently outperformed existing continuous-time RL baselines on multi-agent particle and MuJoCo environments, demonstrating scalability to complex multi-agent dynamics.

Conclusion: The CT-MARL approach addresses key limitations in continuous-time RL for multi-agent systems by improving value function approximation and gradient fidelity, enabling better scalability and policy performance in challenging environments.

Abstract: Existing reinforcement learning (RL) methods struggle with complex dynamical
systems that demand interactions at high frequencies or irregular time
intervals. Continuous-time RL (CTRL) has emerged as a promising alternative by
replacing discrete-time Bellman recursion with differential value functions
defined as viscosity solutions of the Hamilton--Jacobi--Bellman (HJB) equation.
While CTRL has shown promise, its applications have been largely limited to the
single-agent domain. This limitation stems from two key challenges: (i)
conventional solution methods for HJB equations suffer from the curse of
dimensionality (CoD), making them intractable in high-dimensional systems; and
(ii) even with HJB-based learning approaches, accurately approximating
centralized value functions in multi-agent settings remains difficult, which in
turn destabilizes policy training. In this paper, we propose a CT-MARL
framework that uses physics-informed neural networks (PINNs) to approximate
HJB-based value functions at scale. To ensure the value is consistent with its
differential structure, we align value learning with value-gradient learning by
introducing a Value Gradient Iteration (VGI) module that iteratively refines
value gradients along trajectories. This improves gradient fidelity, in turn
yielding more accurate values and stronger policy learning. We evaluate our
method using continuous-time variants of standard benchmarks, including
multi-agent particle environment (MPE) and multi-agent MuJoCo. Our results
demonstrate that our approach consistently outperforms existing continuous-time
RL baselines and scales to complex multi-agent dynamics.

</details>


### [170] [Peering Partner Recommendation for ISPs using Machine Learning](https://arxiv.org/abs/2509.09146)
*Md Ibrahim Ibne Alam,Ankur Senapati,Anindo Mahmood,Murat Yuksel,Koushik Kar*

Main category: cs.LG

TL;DR: This paper investigates a method for automating peering partner selection for ISPs using machine learning, with tree-based models like XGBoost achieving high accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: To enhance the global Internet ecosystem's efficiency by automating the lengthy and complex peering partner selection process for ISPs.

Method: The authors used publicly available data on ISPs (e.g., from databases like PeeringDB and CAIDA) and tested three types of ML models (tree-based, neural network-based, and transformer-based) for predicting peering relationships. Tree-based models, particularly XGBoost, were found to perform best.

Result: The XGBoost model achieved a high accuracy of 98%, demonstrating resilience to time, space, and missing data variations.

Conclusion: The method can be adopted by ISPs to automate peering partner selection, leading to a more efficient and optimized Internet ecosystem.

Abstract: Internet service providers (ISPs) need to connect with other ISPs to provide
global connectivity services to their users. To ensure global connectivity,
ISPs can either use transit service(s) or establish direct peering
relationships between themselves via Internet exchange points (IXPs). Peering
offers more room for ISP-specific optimizations and is preferred, but it often
involves a lengthy and complex process. Automating peering partner selection
can enhance efficiency in the global Internet ecosystem. We explore the use of
publicly available data on ISPs to develop a machine learning (ML) model that
can predict whether an ISP pair should peer or not. At first, we explore public
databases, e.g., PeeringDB, CAIDA, etc., to gather data on ISPs. Then, we
evaluate the performance of three broad types of ML models for predicting
peering relationships: tree-based, neural network-based, and transformer-based.
Among these, we observe that tree-based models achieve the highest accuracy and
efficiency in our experiments. The XGBoost model trained with publicly
available data showed promising performance, with a 98% accuracy rate in
predicting peering partners. In addition, the model demonstrated great
resilience to variations in time, space, and missing data. We envision that
ISPs can adopt our method to fully automate the peering partner selection
process, thus transitioning to a more efficient and optimized Internet
ecosystem.

</details>


### [171] [HISPASpoof: A New Dataset For Spanish Speech Forensics](https://arxiv.org/abs/2509.09155)
*Maria Risques,Kratika Bhagtani,Amit Kumar Singh Yadav,Edward J. Delp*

Main category: cs.LG

TL;DR: The paper introduces HISPASpoof, a Spanish dataset for synthetic speech detection and attribution, addressing the gap in Spanish speech forensics.


<details>
  <summary>Details</summary>
Motivation: To tackle the lack of representation of Spanish in synthetic speech detection and attribution research, despite the rapid advancements in voice cloning and TTS methods in other languages.

Method: Developed HISPASpoof dataset with six Spanish accents and six zero-shot TTS systems, and evaluated five detection methods highlighting their strengths and weaknesses.

Result: English-trained detectors poorly generalized to Spanish, while HISPASpoof-based models showed significant improvements in detection accuracy. It also assessed the effectiveness in identifying synthetic speech generation methods.

Conclusion: HISPASpoof is a critical resource to advance Spanish speech forensics, improving detection methods and attribution performance.

Abstract: Zero-shot Voice Cloning (VC) and Text-to-Speech (TTS) methods have advanced
rapidly, enabling the generation of highly realistic synthetic speech and
raising serious concerns about their misuse. While numerous detectors have been
developed for English and Chinese, Spanish-spoken by over 600 million people
worldwide-remains underrepresented in speech forensics. To address this gap, we
introduce HISPASpoof, the first large-scale Spanish dataset designed for
synthetic speech detection and attribution. It includes real speech from public
corpora across six accents and synthetic speech generated with six zero-shot
TTS systems. We evaluate five representative methods, showing that detectors
trained on English fail to generalize to Spanish, while training on HISPASpoof
substantially improves detection. We also evaluate synthetic speech attribution
performance on HISPASpoof, i.e., identifying the generation method of synthetic
speech. HISPASpoof thus provides a critical benchmark for advancing reliable
and inclusive speech forensics in Spanish.

</details>


### [172] [Adaptive Pareto-Optimal Token Merging for Edge Transformer Models in Semantic Communication](https://arxiv.org/abs/2509.09168)
*Omar Erak,Omar Alhussein,Hatem Abou-Zeid,Mehdi Bennis*

Main category: cs.LG

TL;DR: The paper introduces a training-free framework for adaptive token merging in pretrained vision transformers to optimize their usability in 6G networks with limited resources.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of deploying computationally demanding transformer models in resource-constrained 6G networks for semantic communication.

Method: Developing a token merging framework using Gaussian process-based Bayesian optimization to trade off accuracy, computational cost, and transmission resource efficiency while adapting to dynamic conditions.

Result: The proposed method outperforms other baselines, achieves significant reductions in computational complexity, and dynamically adapts merging strategies based on SNR conditions.

Conclusion: The study provides a scalable and efficient solution for integrating transformer-based semantic communication systems into future 6G networks, balancing latency and semantic fidelity.

Abstract: Large-scale transformer models have emerged as a powerful tool for semantic
communication systems, enabling edge devices to extract rich representations
for robust inference across noisy wireless channels. However, their substantial
computational demands remain a major barrier to practical deployment in
resource-constrained 6G networks. In this paper, we present a training-free
framework for adaptive token merging in pretrained vision transformers to
jointly reduce inference time and transmission resource usage. We formulate the
selection of per-layer merging proportions as a multi-objective optimization
problem to balance accuracy and computational cost. We employ Gaussian
process-based Bayesian optimization to construct a Pareto frontier of optimal
configurations, enabling flexible runtime adaptation to dynamic application
requirements and channel conditions. Extensive experiments demonstrate that our
method consistently outperforms other baselines and achieves significant
reductions in floating-point operations while maintaining competitive accuracy
across a wide range of signal-to-noise ratio (SNR) conditions. Additional
results highlight the effectiveness of adaptive policies that adjust merging
aggressiveness in response to channel quality, providing a practical mechanism
to trade off latency and semantic fidelity on demand. These findings establish
a scalable and efficient approach for deploying transformer-based semantic
communication in future edge intelligence systems.

</details>


### [173] [Quantum Machine Learning, Quantitative Trading, Reinforcement Learning, Deep Learning](https://arxiv.org/abs/2509.09176)
*Jun-Hao Chen,Yu-Chien Huang,Yun-Cheng Tsai,Samuel Yen-Chi Chen*

Main category: cs.LG

TL;DR: The study integrates Quantum Long Short-Term Memory (QLSTM) and Quantum Asynchronous Advantage Actor-Critic (QA3C) for financial trading, achieving strong results in USD/TWD trading.


<details>
  <summary>Details</summary>
Motivation: To explore the potential of quantum-inspired neural networks combined with deep reinforcement learning for enhancing financial trading performance.

Method: A trading agent was developed using QLSTM for short-term trend prediction and QA3C for policy optimization. It was trained and tested on historical USD/TWD data using specific hyperparameters.

Result: The agent achieved an 11.87% return over five years with a maximum drawdown of 0.92%, outperforming some currency ETFs.

Conclusion: Hybrid quantum-inspired models demonstrate competitive performance in FX trading and hold potential for enhanced financial strategies despite current simulation and strategy limitations.

Abstract: The convergence of quantum-inspired neural networks and deep reinforcement
learning offers a promising avenue for financial trading. We implemented a
trading agent for USD/TWD by integrating Quantum Long Short-Term Memory (QLSTM)
for short-term trend prediction with Quantum Asynchronous Advantage
Actor-Critic (QA3C), a quantum-enhanced variant of the classical A3C. Trained
on data from 2000-01-01 to 2025-04-30 (80\% training, 20\% testing), the
long-only agent achieves 11.87\% return over around 5 years with 0.92\% max
drawdown, outperforming several currency ETFs. We detail state design (QLSTM
features and indicators), reward function for trend-following/risk control, and
multi-core training. Results show hybrid models yield competitive FX trading
performance. Implications include QLSTM's effectiveness for small-profit trades
with tight risk and future enhancements. Key hyperparameters: QLSTM sequence
length$=$4, QA3C workers$=$8. Limitations: classical quantum simulation and
simplified strategy. \footnote{The views expressed in this article are those of
the authors and do not represent the views of Wells Fargo. This article is for
informational purposes only. Nothing contained in this article should be
construed as investment advice. Wells Fargo makes no express or implied
warranties and expressly disclaims all legal, tax, and accounting implications
related to this article.

</details>


### [174] [Clip Your Sequences Fairly: Enforcing Length Fairness for Sequence-Level RL](https://arxiv.org/abs/2509.09177)
*Hanyi Mao,Quanjia Xiao,Lei Pang,Haixiao Liu*

Main category: cs.LG

TL;DR: The paper introduces FSPO, a new reinforcement learning method tailored for LLMs, focusing on mitigating length bias during training by modifying the clipping mechanism.


<details>
  <summary>Details</summary>
Motivation: The paper addresses an issue in sequence-level reinforcement learning methods: the reweighting distortion caused by fixed clip ranges for sequences of varying lengths, which leads to unfair training biases.

Method: FSPO introduces a new clipping approach in the log-importance sampling ratio. It adjusts clipping through a KL-corrected drift term and scales by the square root of sequence length, ensuring fair treatment across sequences.

Result: Experiments show that FSPO maintains consistent clip rates across sequence lengths, stabilizes training, and demonstrates superior performance over baseline approaches on various datasets.

Conclusion: FSPO effectively resolves length bias in sequence-level RL methods for LLMs, offering theoretically sound and empirically effective enhancements to training stability and output quality.

Abstract: We propose FSPO (Fair Sequence Policy Optimization), a sequence-level
reinforcement learning method for LLMs that enforces length-fair clipping
directly in the importance-sampling (IS) weight space. We revisit
sequence-level RL methods and identify a mismatch when PPO/GRPO-style clipping
is transplanted to sequences: a fixed clip range systematically reweights short
vs. long responses, distorting the effective objective. Theoretically, we
formalize length fairness via a Length Reweighting Error (LRE) and prove that
small LRE yields a directional cosine guarantee between the clipped and true
updates. FSPO introduces a simple, Gaussian-motivated remedy: we clip the
sequence log-IS ratio with a band that applies a KL-corrected drift term and
scales as $\sqrt{L}$. Empirically, FSPO flattens clip rates across length bins,
stabilizes training, and outperforms all baselines across multiple evaluation
datasets.

</details>


### [175] [Breaking the Statistical Similarity Trap in Extreme Convection Detection](https://arxiv.org/abs/2509.09195)
*Md Tanveer Hossain Munim*

Main category: cs.LG

TL;DR: The paper identifies a flaw in current evaluation metrics for weather models, proposes the DART framework to address this, and validates its effectiveness for detecting extreme weather events.


<details>
  <summary>Details</summary>
Motivation: Current evaluation metrics for deep learning weather models fail to recognize rare, high-impact weather events as they reward blurred, general predictions rather than precise detections. This issue needs to be addressed to improve prediction accuracy for extreme weather scenarios.

Method: The authors propose DART (Dual Architecture for Regression Tasks) — a dual-decoder framework designed to optimize predictions for extreme weather events. It incorporates background/extreme decomposition, oversampling techniques, and tailored loss functions. Validation is done against sophisticated statistical and deep-learning baselines.

Result: The DART framework achieves high CSI scores for extreme event detection, shows that excluding Integrated Water Vapor Transport improves detection by 270%, and is validated with a real-world case study (Chittagong flooding disaster). It trains quickly and integrates into existing meteorological workflows.

Conclusion: DART effectively addresses the limitations of current metrics and models in detecting high-impact weather events. It offers significant operational enhancements, improves predictive accuracy, and demonstrates a path toward trustworthy AI for extreme weather forecasting.

Abstract: Current evaluation metrics for deep learning weather models create a
"Statistical Similarity Trap", rewarding blurry predictions while missing rare,
high-impact events. We provide quantitative evidence of this trap, showing
sophisticated baselines achieve 97.9% correlation yet 0.00 CSI for dangerous
convection detection. We introduce DART (Dual Architecture for Regression
Tasks), a framework addressing the challenge of transforming coarse atmospheric
forecasts into high-resolution satellite brightness temperature fields
optimized for extreme convection detection (below 220 K). DART employs
dual-decoder architecture with explicit background/extreme decomposition,
physically motivated oversampling, and task-specific loss functions. We present
four key findings: (1) empirical validation of the Statistical Similarity Trap
across multiple sophisticated baselines; (2) the "IVT Paradox", removing
Integrated Water Vapor Transport, widely regarded as essential for atmospheric
river analysis, improves extreme convection detection by 270%; (3)
architectural necessity demonstrated through operational flexibility (DART
achieves CSI = 0.273 with bias = 2.52 vs. 6.72 for baselines at equivalent
CSI), and (4) real-world validation with the August 2023 Chittagong flooding
disaster as a case study. To our knowledge, this is the first work to
systematically address this hybrid conversion-segmentation-downscaling task,
with no direct prior benchmarks identified in existing literature. Our
validation against diverse statistical and deep learning baselines sufficiently
demonstrates DART's specialized design. The framework enables precise
operational calibration through beta-tuning, trains in under 10 minutes on
standard hardware, and integrates seamlessly with existing meteorological
workflows, demonstrating a pathway toward trustworthy AI for extreme weather
preparedness.

</details>


### [176] [Incentivizing Safer Actions in Policy Optimization for Constrained Reinforcement Learning](https://arxiv.org/abs/2509.09208)
*Somnath Hazra,Pallab Dasgupta,Soumyajit Dey*

Main category: cs.LG

TL;DR: The paper proposes a new Safe RL algorithm, IP3O, that introduces adaptive penalties to improve training stability and constraint adherence, and evaluates its efficacy empirically and theoretically.


<details>
  <summary>Details</summary>
Motivation: Existing constrained RL methods struggle with instability near constraint boundaries while trying to balance reward maximization and constraint adherence.

Method: The authors developed IP3O, which incrementally penalizes policy violations near constraint boundaries using an adaptive incentive mechanism alongside the reward structure.

Result: Empirical evaluations show that IP3O outperforms existing Safe RL algorithms on benchmark environments. Theoretical analysis confirms a bound on the algorithm's worst-case optimality error.

Conclusion: IP3O is a stable and effective solution for constrained RL, addressing instability issues and ensuring adherence to safety constraints both practically and theoretically.

Abstract: Constrained Reinforcement Learning (RL) aims to maximize the return while
adhering to predefined constraint limits, which represent domain-specific
safety requirements. In continuous control settings, where learning agents
govern system actions, balancing the trade-off between reward maximization and
constraint satisfaction remains a significant challenge. Policy optimization
methods often exhibit instability near constraint boundaries, resulting in
suboptimal training performance. To address this issue, we introduce a novel
approach that integrates an adaptive incentive mechanism in addition to the
reward structure to stay within the constraint bound before approaching the
constraint boundary. Building on this insight, we propose Incrementally
Penalized Proximal Policy Optimization (IP3O), a practical algorithm that
enforces a progressively increasing penalty to stabilize training dynamics.
Through empirical evaluation on benchmark environments, we demonstrate the
efficacy of IP3O compared to the performance of state-of-the-art Safe RL
algorithms. Furthermore, we provide theoretical guarantees by deriving a bound
on the worst-case error of the optimality achieved by our algorithm.

</details>


### [177] [Identifying Key Features for Establishing Sustainable Agro-Tourism Centre: A Data Driven Approach](https://arxiv.org/abs/2509.09214)
*Alka Gadakh,Vidya Kumbhar,Sonal Khosla,Kumar Karunendra*

Main category: cs.LG

TL;DR: The paper explores strategies for enhancing agro-tourism by identifying growth indicators using machine learning methods, achieving high accuracy with Logistic Regression and LASSO techniques.


<details>
  <summary>Details</summary>
Motivation: Agro-tourism serves dual purposes: economically aiding rural areas and preserving cultural heritage and agricultural practices. There is a need to further examine strategies for its growth.

Method: The study utilized a two-phase approach: first, identifying indicators via literature review, and second, applying machine learning techniques (LASSO, Logistic Regression, Decision Trees, Random Forest, XGBoost) for feature selection and analysis.

Result: Logistic Regression with the LASSO method achieved the highest classification accuracy, 98%-99%, on train-test data splits of 70-30% and 80-20%, respectively. Other models like Random Forest and XGBoost also performed competitively.

Conclusion: Machine learning techniques, particularly Logistic Regression combined with LASSO, prove effective in identifying key indicators for agro-tourism growth. Such methodologies can guide enhancements in this domain with high reliability.

Abstract: Agro-tourism serves as a strategic economic model designed to facilitate
rural development by diversifying income streams for local communities like
farmers while promoting the conservation of indigenous cultural heritage and
traditional agricultural practices. As a very booming subdomain of tourism,
there is a need to study the strategies for the growth of Agro-tourism in
detail. The current study has identified the important indicators for the
growth and enhancement of agro-tourism. The study is conducted in two phases:
identification of the important indicators through a comprehensive literature
review and in the second phase state-of-the-art techniques were used to
identify the important indicators for the growth of agro-tourism. The
indicators are also called features synonymously, the machine learning models
for feature selection were applied and it was observed that the Least Absolute
Shrinkage and Selection Operator (LASSO) method combined with, the machine
Learning Classifiers such as Logistic Regression (LR), Decision Trees (DT),
Random Forest (RF) Tree, and Extreme Gradient Boosting (XGBOOST) models were
used to suggest the growth of the agro-tourism. The results show that with the
LASSO method, LR model gives the highest classification accuracy of 98% in
70-30% train-test data followed by RF with 95% accuracy. Similarly, in the
80-20% train-test data LR maintains the highest accuracy at 99%, while DT and
XGBoost follow with 97% accuracy.

</details>


### [178] [Vejde: A Framework for Inductive Deep Reinforcement Learning Based on Factor Graph Color Refinement](https://arxiv.org/abs/2509.09219)
*Jakob Nyberg,Pontus Johnson*

Main category: cs.LG

TL;DR: Vejde is a novel framework combining data abstraction, graph neural networks, and reinforcement learning to address decision problems with complex state structures using inductive policy functions. The results show promising generalization capabilities compared to traditional methods.


<details>
  <summary>Details</summary>
Motivation: To develop a framework capable of handling decision problems with complex, richly structured states, improving generalization and scalability across varying problem sizes and structures.

Method: A combination of data abstraction, graph neural networks, and reinforcement learning. MDP states are transformed into bipartite graphs for neural message passing, enabling a factored representation of both states and actions.

Result: Vejde demonstrated competitive generalization to unseen test instances without significant loss in score, achieving comparable performance to instance-specific MLP agents and online planning algorithms like Prost.

Conclusion: Vejde effectively generalizes across varying problem instances, showcasing its potential as a robust framework for decision problems with structured states through inductive policy learning.

Abstract: We present and evaluate Vejde; a framework which combines data abstraction,
graph neural networks and reinforcement learning to produce inductive policy
functions for decision problems with richly structured states, such as object
classes and relations. MDP states are represented as data bases of facts about
entities, and Vejde converts each state to a bipartite graph, which is mapped
to latent states through neural message passing. The factored representation of
both states and actions allows Vejde agents to handle problems of varying size
and structure. We tested Vejde agents on eight problem domains defined in RDDL,
with ten problem instances each, where policies were trained using both
supervised and reinforcement learning. To test policy generalization, we
separate problem instances in two sets, one for training and the other solely
for testing. Test results on unseen instances for the Vejde agents were
compared to MLP agents trained on each problem instance, as well as the online
planning algorithm Prost. Our results show that Vejde policies in average
generalize to the test instances without a significant loss in score.
Additionally, the inductive agents received scores on unseen test instances
that on average were close to the instance-specific MLP agents.

</details>


### [179] [Constructing a Question-Answering Simulator through the Distillation of LLMs](https://arxiv.org/abs/2509.09226)
*Haipeng Liu,Ting Long,Jing Fu*

Main category: cs.LG

TL;DR: The paper introduces LDSim, a QA simulator that distills knowledge from LLM to predict student learning accurately and efficiently.


<details>
  <summary>Details</summary>
Motivation: To improve educational recommender systems by simulating student learning behaviors and avoiding the drawbacks of undertrained systems.

Method: The LDSim method distills knowledge from a large language model (LLM) to improve the prediction of question-answering correctness efficiently.

Result: LDSim demonstrates strong performance in both simulation tasks and knowledge-tracing tasks during experimental validation.

Conclusion: LDSim provides an effective and efficient solution for educational simulation, combining the advantages of both LLM-based and LLM-free methods.

Abstract: The question-answering (QA) simulator is a model that mimics real student
learning behaviors and predicts their correctness of their responses to
questions. QA simulators enable educational recommender systems (ERS) to
collect large amounts of training data without interacting with real students,
thereby preventing harmful recommendations made by an undertrained ERS from
undermining actual student learning. Given the QA history, there are two
categories of solutions to predict the correctness, conducting the simulation:
(1) LLM-free methods, which apply a traditional sequential model to transfer
the QA history into a vector representation first, and make predictions based
on the representation; (2) LLM-based methods, which leverage the domain
knowledge and reasoning capability of LLM to enhence the prediction. LLM-free
methods offer fast inference but generally yield suboptimal performance. In
contrast, most LLM-based methods achieve better results, but at the cost of
slower inference speed and higher GPU memory consumption. In this paper, we
propose a method named LLM Distillation based Simulator (LDSim), which distills
domain knowledge and reasoning capability from an LLM to better assist
prediction, thereby improving simulation performance. Extensive experiments
demonstrate that our LDSim achieves strong results on both the simulation task
and the knowledge tracing (KT) task. Our code is publicly available at
https://anonymous.4open.science/r/LDSim-05A9.

</details>


### [180] [Unsupervised Multi-Attention Meta Transformer for Rotating Machinery Fault Diagnosis](https://arxiv.org/abs/2509.09251)
*Hanyang Wang,Yuxuan Yang,Hongjun Wang,Lihui Wang*

Main category: cs.LG

TL;DR: The study introduces the Multi-Attention Meta Transformer (MMT-FD) framework for efficient fault diagnosis in rotating machinery using minimal labeled data while achieving high generalization and accuracy.


<details>
  <summary>Details</summary>
Motivation: Address the challenges of limited labeled fault samples and the lack of model generalizability across different types of mechanical equipment.

Method: Developed MMT-FD framework combining a time-frequency domain encoder and a meta-learning generalization model for unsupervised fault diagnosis with fine-tuning using limited labeled data.

Result: Conducted experiments demonstrating MMT-FD achieving 99% fault diagnosis accuracy using only 1% labeled sample data.

Conclusion: MMT-FD proves effective for efficient fault diagnosis of rotating mechanical equipment, with noteworthy accuracy and cross-equipment generalizability.

Abstract: The intelligent fault diagnosis of rotating mechanical equipment usually
requires a large amount of labeled sample data. However, in practical
industrial applications, acquiring enough data is both challenging and
expensive in terms of time and cost. Moreover, different types of rotating
mechanical equipment with different unique mechanical properties, require
separate training of diagnostic models for each case. To address the challenges
of limited fault samples and the lack of generalizability in prediction models
for practical engineering applications, we propose a Multi-Attention Meta
Transformer method for few-shot unsupervised rotating machinery fault diagnosis
(MMT-FD). This framework extracts potential fault representations from
unlabeled data and demonstrates strong generalization capabilities, making it
suitable for diagnosing faults across various types of mechanical equipment.
The MMT-FD framework integrates a time-frequency domain encoder and a
meta-learning generalization model. The time-frequency domain encoder predicts
status representations generated through random augmentations in the
time-frequency domain. These enhanced data are then fed into a meta-learning
network for classification and generalization training, followed by fine-tuning
using a limited amount of labeled data. The model is iteratively optimized
using a small number of contrastive learning iterations, resulting in high
efficiency. To validate the framework, we conducted experiments on a bearing
fault dataset and rotor test bench data. The results demonstrate that the
MMT-FD model achieves 99\% fault diagnosis accuracy with only 1\% of labeled
sample data, exhibiting robust generalization capabilities.

</details>


### [181] [Harnessing Uncertainty: Entropy-Modulated Policy Gradients for Long-Horizon LLM Agents](https://arxiv.org/abs/2509.09265)
*Jiawei Wang,Jiacai Liu,Yuqian Fu,Yingru Li,Xintao Wang,Yuan Lin,Yu Yue,Lin Zhang,Yang Wang,Ke Wang*

Main category: cs.LG

TL;DR: The paper introduces Entropy-Modulated Policy Gradients (EMPG), a novel framework that addresses the inefficiency and instability of policy gradient updates in LLM-based agents for long-horizon tasks by recalibrating signals based on uncertainty and task outcomes.


<details>
  <summary>Details</summary>
Motivation: The motivation lies in overcoming the challenge of sparse outcome-based rewards in long-horizon tasks, coupled with inefficient and unstable policy gradient updates when training LLM-based agents.

Method: The proposed EMPG framework adjusts learning signals by amplifying updates for confident correct actions, penalizing confident errors, and attenuating updates for uncertain steps. Additionally, a clarity bonus term incentivizes agents to find more predictable solution paths.

Result: EMPG demonstrates substantial performance improvements in experiments across three challenging agent tasks—WebShop, ALFWorld, and Deep Search—surpassing strong policy gradient baseline methods.

Conclusion: EMPG effectively addresses the inherent problem of entropy-magnitude coupling in policy gradient updates, leading to more efficient and stabilized learning for long-horizon tasks.

Abstract: In long-horizon tasks, recent agents based on Large Language Models (LLMs)
face a significant challenge that sparse, outcome-based rewards make it
difficult to assign credit to intermediate steps. Previous methods mainly focus
on creating dense reward signals to guide learning, either through traditional
reinforcement learning techniques like inverse reinforcement learning or by
using Process Reward Models for step-by-step feedback. In this paper, we
identify a fundamental problem in the learning dynamics of LLMs: the magnitude
of policy gradients is inherently coupled with the entropy, which leads to
inefficient small updates for confident correct actions and potentially
destabilizes large updates for uncertain ones. To resolve this, we propose
Entropy-Modulated Policy Gradients (EMPG), a framework that re-calibrates the
learning signal based on step-wise uncertainty and the final task outcome. EMPG
amplifies updates for confident correct actions, penalizes confident errors,
and attenuates updates from uncertain steps to stabilize exploration. We
further introduce a bonus term for future clarity that encourages agents to
find more predictable solution paths. Through comprehensive experiments on
three challenging agent tasks, WebShop, ALFWorld, and Deep Search, we
demonstrate that EMPG achieves substantial performance gains and significantly
outperforms strong policy gradient baselines. Project page is at
https://empgseed-seed.github.io/

</details>


### [182] [Data Driven Discovery of Emergent Dynamics in Reaction Diffusion Systems from Sparse and Noisy Observations](https://arxiv.org/abs/2509.09278)
*Saumitra Dwivedi,Ricardo da Silva Torres,Ibrahim A. Hameed,Gunnar Tufte,Anniken Susanne T. Karlsen*

Main category: cs.LG

TL;DR: The paper introduces a model, DRSALife, for learning rulesets of Soft Artificial Life (Soft ALife) in reaction-diffusion systems without prior physics knowledge, achieving 74% prediction accuracy and robustness against noisy and sparse data.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the lack of methods for identifying reaction-diffusion system dynamics when there is no prior knowledge of the underlying physics.

Method: The authors develop a conceptual framework, DRSALife, employing Soft ALife models like Cellular Automata and Agent-based models to learn emergent dynamics from observed data. They also assess the effects of noisy and sparse data on learning.

Result: The experimental results show the models can identify underlying partial differential equations with 74% prediction accuracy and remain robust against Gaussian noise and temporal sparsity in the data.

Conclusion: The study demonstrates the feasibility of data-driven discovery for reaction-diffusion system dynamics through Soft ALife modeling, showing promising applications in various fields where emergent dynamics are studied.

Abstract: Data-driven discovery of emergent dynamics is gaining popularity,
particularly in the context of reaction-diffusion systems. These systems are
widely studied across various fields, including neuroscience, ecology,
epidemiology, and several other subject areas that deal with emergent dynamics.
A current challenge in the discovery process relates to system identification
when there is no prior knowledge of the underlying physics. We attempt to
address this challenge by learning Soft Artificial Life (Soft ALife) models,
such as Agent-based and Cellular Automata (CA) models, from observed data for
reaction-diffusion systems. In this paper, we present findings on the
applicability of a conceptual framework, the Data-driven Rulesets for Soft
Artificial Life (DRSALife) model, to learn Soft ALife rulesets that accurately
represent emergent dynamics in a reaction-diffusion system from observed data.
This model has demonstrated promising results for Elementary CA Rule 30, Game
of Life, and Vicsek Flocking problems in recent work. To our knowledge, this is
one of the few studies that explore machine-based Soft ALife ruleset learning
and system identification for reaction-diffusion dynamics without any prior
knowledge of the underlying physics. Moreover, we provide comprehensive
findings from experiments investigating the potential effects of using noisy
and sparse observed datasets on learning emergent dynamics. Additionally, we
successfully identify the structure and parameters of the underlying partial
differential equations (PDEs) representing these dynamics. Experimental results
demonstrate that the learned models are able to predict the emergent dynamics
with good accuracy (74%) and exhibit quite robust performance when subjected to
Gaussian noise and temporal sparsity.

</details>


### [183] [MoSE: Unveiling Structural Patterns in Graphs via Mixture of Subgraph Experts](https://arxiv.org/abs/2509.09337)
*Junda Ye,Zhongbao Zhang,Li Sun,Siqiang Luo*

Main category: cs.LG

TL;DR: This paper introduces a framework, Mixture of Subgraph Experts (MoSE), to enhance graph neural networks (GNNs) by dynamically leveraging subgraph patterns for diverse graph tasks.


<details>
  <summary>Details</summary>
Motivation: The current GNNs struggle to capture complex, high-order subgraph patterns due to their pairwise message-passing design, limiting their structural expressiveness. Existing methods using random walk kernels have limited task applicability and flexibility.

Method: A Mixture of Subgraph Experts (MoSE) framework is proposed that uses anonymous walks to extract subgraphs and dynamically assigns them to specialized experts based on their structural semantics. This enables improved expressivity and interpretability.

Result: MoSE demonstrates superior performance in benchmark experiments compared to competitive approaches, while also offering insights into the structural patterns it learns.

Conclusion: MoSE provides a flexible and expressive framework for subgraph-based representation learning, surpassing previous methods in both performance and interpretability, particularly regarding structural pattern analysis.

Abstract: While graph neural networks (GNNs) have achieved great success in learning
from graph-structured data, their reliance on local, pairwise message passing
restricts their ability to capture complex, high-order subgraph patterns.
leading to insufficient structural expressiveness. Recent efforts have
attempted to enhance structural expressiveness by integrating random walk
kernels into GNNs. However, these methods are inherently designed for
graph-level tasks, which limits their applicability to other downstream tasks
such as node classification. Moreover, their fixed kernel configurations hinder
the model's flexibility in capturing diverse subgraph structures. To address
these limitations, this paper proposes a novel Mixture of Subgraph Experts
(MoSE) framework for flexible and expressive subgraph-based representation
learning across diverse graph tasks. Specifically, MoSE extracts informative
subgraphs via anonymous walks and dynamically routes them to specialized
experts based on structural semantics, enabling the model to capture diverse
subgraph patterns with improved flexibility and interpretability. We further
provide a theoretical analysis of MoSE's expressivity within the Subgraph
Weisfeiler-Lehman (SWL) Test, proving that it is more powerful than SWL.
Extensive experiments, together with visualizations of learned subgraph
experts, demonstrate that MoSE not only outperforms competitive baselines but
also provides interpretable insights into structural patterns learned by the
model.

</details>


### [184] [Robust Non-Linear Correlations via Polynomial Regression](https://arxiv.org/abs/2509.09380)
*Luca Giuliani,Michele Lombardi*

Main category: cs.LG

TL;DR: The paper introduces a novel computational approach for estimating the Hirschfeld-Gebelein-Rényi (HGR) correlation coefficient using polynomial kernels, emphasizing robustness, determinism, and real-world applicability.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome limitations in existing methods of estimating the HGR correlation coefficient, which faces robustness issues due to bias-variance trade-offs, especially in real-world applications.

Method: The authors propose a user-configurable polynomial kernel-based computational approach to estimate the HGR correlation coefficient, aiming for improved robustness and efficiency.

Result: The proposed method demonstrates significant improvements in robustness and determinism compared to existing techniques, and the experimental analysis confirms the viability of using the computed subgradient as a loss regularizer in machine learning.

Conclusion: The new approach offers a reliable framework for HGR computation, providing practical benefits in algorithmic fairness, scientific analysis, and constrained machine learning applications.

Abstract: The Hirschfeld-Gebelein-R\'enyi (HGR) correlation coefficient is an extension
of Pearson's correlation that is not limited to linear correlations, with
potential applications in algorithmic fairness, scientific analysis, and causal
discovery. Recently, novel algorithms to estimate HGR in a differentiable
manner have been proposed to facilitate its use as a loss regularizer in
constrained machine learning applications. However, the inherent
uncomputability of HGR requires a bias-variance trade-off, which can possibly
compromise the robustness of the proposed methods, hence raising technical
concerns if applied in real-world scenarios. We introduce a novel computational
approach for HGR that relies on user-configurable polynomial kernels, offering
greater robustness compared to previous methods and featuring a faster yet
almost equally effective restriction. Our approach provides significant
advantages in terms of robustness and determinism, making it a more reliable
option for real-world applications. Moreover, we present a brief experimental
analysis to validate the applicability of our approach within a constrained
machine learning framework, showing that its computation yields an insightful
subgradient that can serve as a loss regularizer.

</details>


### [185] [MetaLLMix : An XAI Aided LLM-Meta-learning Based Approach for Hyper-parameters Optimization](https://arxiv.org/abs/2509.09387)
*Mohammed Tiouti,Mohamed Bal-Ghaoui*

Main category: cs.LG

TL;DR: MetaLLMiX introduces a zero-shot hyperparameter optimization framework that uses meta-learning, explainable AI, and efficient reasoning via lightweight LLMs to optimize outcomes while significantly reducing computational cost.


<details>
  <summary>Details</summary>
Motivation: To overcome challenges in deep learning model and hyperparameter selection, which traditionally require extensive computational resources and expertise.

Method: The framework leverages historical experimental data with SHAP explanations combined with reasoning capabilities of lightweight LLMs for recommending hyperparameters. It also uses an LLM-as-judge evaluation to ensure output quality.

Result: Experiments on eight medical imaging datasets showed competitive or superior performance compared to traditional HPO methods. The approach achieved significant computational and time efficiency, with optimal results on 5 out of 8 tasks and training time reductions up to 15.7x.

Conclusion: MetaLLMiX reduces computational costs, maintains high accuracy, and offers a practical alternative to expensive traditional or API-based approaches for hyperparameter optimization.

Abstract: Effective model and hyperparameter selection remains a major challenge in
deep learning, often requiring extensive expertise and computation. While
AutoML and large language models (LLMs) promise automation, current LLM-based
approaches rely on trial and error and expensive APIs, which provide limited
interpretability and generalizability. We propose MetaLLMiX, a zero-shot
hyperparameter optimization framework combining meta-learning, explainable AI,
and efficient LLM reasoning. By leveraging historical experiment outcomes with
SHAP explanations, MetaLLMiX recommends optimal hyperparameters and pretrained
models without additional trials. We further employ an LLM-as-judge evaluation
to control output format, accuracy, and completeness. Experiments on eight
medical imaging datasets using nine open-source lightweight LLMs show that
MetaLLMiX achieves competitive or superior performance to traditional HPO
methods while drastically reducing computational cost. Our local deployment
outperforms prior API-based approaches, achieving optimal results on 5 of 8
tasks, response time reductions of 99.6-99.9%, and the fastest training times
on 6 datasets (2.4-15.7x faster), maintaining accuracy within 1-5% of
best-performing baselines.

</details>


### [186] [LLMs Don't Know Their Own Decision Boundaries: The Unreliability of Self-Generated Counterfactual Explanations](https://arxiv.org/abs/2509.09396)
*Harry Mayne,Ryan Othniel Kearns,Yushi Yang,Andrew M. Bean,Eoin Delaney,Chris Russell,Adam Mahdi*

Main category: cs.LG

TL;DR: This paper investigates self-generated counterfactual explanations (SCEs) in language models, assessing their validity and minimality. Results suggest that while valid, SCEs are usually not minimal and can mislead analyses of LLM behavior.


<details>
  <summary>Details</summary>
Motivation: To evaluate the ability of language models to provide reliable natural language self-explanations for human collaboration, specifically through counterfactual explanations that clarify decisions.

Method: The authors analyze SCEs produced by various LLMs across multiple datasets and assessment setups, studying their validity (if they achieve expected outcomes) and minimality (if they make only necessary changes).

Result: LLMs produce valid counterfactuals but generally lack minimality, limiting insights into decision-making. When tasked with minimal edits, models tend to fail at changing predictions, revealing a validity-minimality trade-off.

Conclusion: SCEs remain ineffective as an explainability tool and might mislead stakeholders and deployment strategies in critical settings. Further research is essential before considering LLMs for high-stakes use cases.

Abstract: To collaborate effectively with humans, language models must be able to
explain their decisions in natural language. We study a specific type of
self-explanation: self-generated counterfactual explanations (SCEs), where a
model explains its prediction by modifying the input such that it would have
predicted a different outcome. We evaluate whether LLMs can produce SCEs that
are valid, achieving the intended outcome, and minimal, modifying the input no
more than necessary. When asked to generate counterfactuals, we find that LLMs
typically produce SCEs that are valid, but far from minimal, offering little
insight into their decision-making behaviour. Worryingly, when asked to
generate minimal counterfactuals, LLMs typically make excessively small edits
that fail to change predictions. The observed validity-minimality trade-off is
consistent across several LLMs, datasets, and evaluation settings. Our findings
suggest that SCEs are, at best, an ineffective explainability tool and, at
worst, can provide misleading insights into model behaviour. Proposals to
deploy LLMs in high-stakes settings must consider the impact of unreliable
self-explanations on downstream decision-making. Our code is available at
https://github.com/HarryMayne/SCEs.

</details>


### [187] [Kriging prior Regression: A Case for Kriging-Based Spatial Features with TabPFN in Soil Mapping](https://arxiv.org/abs/2509.09408)
*Jonas Schmidinger,Viacheslav Barkov,Sebastian Vogel,Martin Atzmueller,Gerard B M Heuvelink*

Main category: cs.LG

TL;DR: This paper introduces a hybrid framework, Kriging prior regression (KpR), to combine geostatistics and machine learning for spatial soil mapping, significantly improving prediction accuracy.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address limitations of traditional geostatistics and machine learning methods in predicting and spatially mapping soil properties by integrating spatial structure and environment data.

Method: The method employs 'spatial lag' features engineered through ordinary kriging and utilizes the TabPFN model to enhance prediction and uncertainty estimation across various soil datasets.

Result: KpR with TabPFN outperformed other spatial and non-spatial methods, demonstrating a 30% improvement in R2 scores and reliable uncertainty estimates, especially effective for small sample sizes.

Conclusion: KpR combined with TabPFN proves to be a robust, versatile framework for digital soil mapping in precision agriculture, compensating for weak sensing data relationships and enhancing predictions.

Abstract: Machine learning and geostatistics are two fundamentally different frameworks
for predicting and spatially mapping soil properties. Geostatistics leverages
the spatial structure of soil properties, while machine learning captures the
relationship between available environmental features and soil properties. We
propose a hybrid framework that enriches ML with spatial context through
engineering of 'spatial lag' features from ordinary kriging. We call this
approach 'kriging prior regression' (KpR), as it follows the inverse logic of
regression kriging. To evaluate this approach, we assessed both the point and
probabilistic prediction performance of KpR, using the TabPFN model across six
fieldscale datasets from LimeSoDa. These datasets included soil organic carbon,
clay content, and pH, along with features derived from remote sensing and
in-situ proximal soil sensing. KpR with TabPFN demonstrated reliable
uncertainty estimates and more accurate predictions in comparison to several
other spatial techniques (e.g., regression/residual kriging with TabPFN), as
well as to established non-spatial machine learning algorithms (e.g., random
forest). Most notably, it significantly improved the average R2 by around 30%
compared to machine learning algorithms without spatial context. This
improvement was due to the strong prediction performance of the TabPFN
algorithm itself and the complementary spatial information provided by KpR
features. TabPFN is particularly effective for prediction tasks with small
sample sizes, common in precision agriculture, whereas KpR can compensate for
weak relationships between sensing features and soil properties when proximal
soil sensing data are limited. Hence, we conclude that KpR with TabPFN is a
very robust and versatile modelling framework for digital soil mapping in
precision agriculture.

</details>


### [188] [Fused Lasso Improves Accuracy of Co-occurrence Network Inference in Grouped Samples](https://arxiv.org/abs/2509.09413)
*Daniel Agyapong,Briana H. Beatty,Peter G. Kennedy,Toby D. Hocking*

Main category: cs.LG

TL;DR: This paper introduces "fuser," an algorithm designed for microbiome community network inference, capable of capturing both spatial and temporal dynamics while improving predictive accuracy across varying environmental niches.


<details>
  <summary>Details</summary>
Motivation: Current microbial network inference algorithms lack the ability to accurately model dynamic microbial associations across different environmental conditions, often providing static insights limited to single environmental niches.

Method: The study introduces the Same-All Cross-validation (SAC) framework to evaluate the performance of microbial association prediction algorithms across homogeneous (Same) and diverse (All) environmental scenarios. The proposed algorithm, "fuser," incorporates subsample-specific information while sharing relevant data across environments to infer distinct predictive networks.

Result: The fuser algorithm achieved performance comparable to existing algorithms like glmnet in homogeneous environments and demonstrated superior predictive accuracy in cross-environment scenarios, significantly reducing test error.

Conclusion: Fuser provides a novel, effective approach for inferring distinct microbial association networks tailored to different environmental conditions, addressing limitations of static, traditional methods.

Abstract: Co-occurrence network inference algorithms have significantly advanced our
understanding of microbiome communities. However, these algorithms typically
analyze microbial associations within samples collected from a single
environmental niche, often capturing only static snapshots rather than dynamic
microbial processes. Previous studies have commonly grouped samples from
different environmental niches together without fully considering how microbial
communities adapt their associations when faced with varying ecological
conditions. Our study addresses this limitation by explicitly investigating
both spatial and temporal dynamics of microbial communities. We analyzed
publicly available microbiome abundance data across multiple locations and time
points, to evaluate algorithm performance in predicting microbial associations
using our proposed Same-All Cross-validation (SAC) framework. SAC evaluates
algorithms in two distinct scenarios: training and testing within the same
environmental niche (Same), and training and testing on combined data from
multiple environmental niches (All). To overcome the limitations of
conventional algorithms, we propose fuser, an algorithm that, while not
entirely new in machine learning, is novel for microbiome community network
inference. It retains subsample-specific signals while simultaneously sharing
relevant information across environments during training. Unlike standard
approaches that infer a single generalized network from combined data, fuser
generates distinct, environment-specific predictive networks. Our results
demonstrate that fuser achieves comparable predictive performance to existing
algorithms such as glmnet when evaluated within homogeneous environments
(Same), and notably reduces test error compared to baseline algorithms in
cross-environment (All) scenarios.

</details>


### [189] [Composable Score-based Graph Diffusion Model for Multi-Conditional Molecular Generation](https://arxiv.org/abs/2509.09451)
*Anjie Qiao,Zhen Wang,Chuan Chen,DeFu Lian,Enhong Chen*

Main category: cs.LG

TL;DR: This paper introduces CSGD, a model improving molecular graph generation in multi-property settings using score-based techniques for better control and fidelity.


<details>
  <summary>Details</summary>
Motivation: Advancing molecular graph generation for drug and material discovery while addressing limitations in controlling multiple property constraints.

Method: Developed CSGD, a score-based diffusion model for discrete graphs, featuring Composable Guidance (CoG) for condition control and Probability Calibration (PC) for reducing mismatches.

Result: CSGD demonstrated a 15.3% improvement in controllability over existing methods across four molecular datasets while maintaining validity and fidelity.

Conclusion: CSGD effectively improves multi-property molecular graph generation, offering practical advantages in drug and material design.

Abstract: Controllable molecular graph generation is essential for material and drug
discovery, where generated molecules must satisfy diverse property constraints.
While recent advances in graph diffusion models have improved generation
quality, their effectiveness in multi-conditional settings remains limited due
to reliance on joint conditioning or continuous relaxations that compromise
fidelity. To address these limitations, we propose Composable Score-based Graph
Diffusion model (CSGD), the first model that extends score matching to discrete
graphs via concrete scores, enabling flexible and principled manipulation of
conditional guidance. Building on this foundation, we introduce two score-based
techniques: Composable Guidance (CoG), which allows fine-grained control over
arbitrary subsets of conditions during sampling, and Probability Calibration
(PC), which adjusts estimated transition probabilities to mitigate train-test
mismatches. Empirical results on four molecular datasets show that CSGD
achieves state-of-the-art performance, with a 15.3% average improvement in
controllability over prior methods, while maintaining high validity and
distributional fidelity. Our findings highlight the practical advantages of
score-based modeling for discrete graph generation and its capacity for
flexible, multi-property molecular design.

</details>


### [190] [AquaCast: Urban Water Dynamics Forecasting with Precipitation-Informed Multi-Input Transformer](https://arxiv.org/abs/2509.09458)
*Golnoosh Abdollahinejad,Saleh Baghersalimi,Denisa-Andreea Constantinescu,Sergey Shevchik,David Atienza*

Main category: cs.LG

TL;DR: AquaCast is a deep learning model for urban water dynamics forecasting that outperforms baselines by integrating endogenous and exogenous factors without needing to forecast the exogenous inputs.


<details>
  <summary>Details</summary>
Motivation: Accurately forecasting urban water dynamics is crucial for managing drainage systems and addressing environmental challenges, which is difficult due to the complexity of interactions between multiple factors.

Method: The AquaCast model uses a multi-input, multi-output approach with embedding layers to incorporate exogenous factors while focusing on forecasting endogenous variables, and is tested on various real and synthetic datasets.

Result: AquaCast achieves state-of-the-art performance using endogenous variables alone, improves further with exogenous inputs, and remains robust and accurate across diverse datasets.

Conclusion: The proposed approach effectively improves forecasts in urban water systems by leveraging temporal and inter-variable dependencies and can generalize across real and synthetic datasets.

Abstract: This work addresses the challenge of forecasting urban water dynamics by
developing a multi-input, multi-output deep learning model that incorporates
both endogenous variables (e.g., water height or discharge) and exogenous
factors (e.g., precipitation history and forecast reports). Unlike conventional
forecasting, the proposed model, AquaCast, captures both inter-variable and
temporal dependencies across all inputs, while focusing forecast solely on
endogenous variables. Exogenous inputs are fused via an embedding layer,
eliminating the need to forecast them and enabling the model to attend to their
short-term influences more effectively. We evaluate our approach on the
LausanneCity dataset, which includes measurements from four urban drainage
sensors, and demonstrate state-of-the-art performance when using only
endogenous variables. Performance also improves with the inclusion of exogenous
variables and forecast reports. To assess generalization and scalability, we
additionally test the model on three large-scale synthesized datasets,
generated from MeteoSwiss records, the Lorenz Attractors model, and the Random
Fields model, each representing a different level of temporal complexity across
100 nodes. The results confirm that our model consistently outperforms existing
baselines and maintains a robust and accurate forecast across both real and
synthetic datasets.

</details>


### [191] [AEGIS: An Agent for Extraction and Geographic Identification in Scholarly Proceedings](https://arxiv.org/abs/2509.09470)
*Om Vishesh,Harshad Khadilkar,Deepak Akkil*

Main category: cs.LG

TL;DR: The paper introduces an automated AI system, 'Agent-E', for efficient academic paper discovery and action execution, achieving high accuracy.


<details>
  <summary>Details</summary>
Motivation: Researchers face challenges keeping up with the rapid growth of academic literature manually, necessitating automated solutions.

Method: The authors developed AI agent 'Agent-E' integrated with Robotic Process Automation (RPA) to identify and act on academic papers from specific regions in conference proceedings.

Result: Validation on 586 papers from five conferences showed 100% recall and 99.4% accuracy in identifying target papers.

Conclusion: The study demonstrates how AI systems can significantly streamline academic workflows, emphasizing their utility in improving efficiency in scholarly discovery and participation.

Abstract: Keeping pace with the rapid growth of academia literature presents a
significant challenge for researchers, funding bodies, and academic societies.
To address the time-consuming manual effort required for scholarly discovery,
we present a novel, fully automated system that transitions from data discovery
to direct action. Our pipeline demonstrates how a specialized AI agent,
'Agent-E', can be tasked with identifying papers from specific geographic
regions within conference proceedings and then executing a Robotic Process
Automation (RPA) to complete a predefined action, such as submitting a
nomination form. We validated our system on 586 papers from five different
conferences, where it successfully identified every target paper with a recall
of 100% and a near perfect accuracy of 99.4%. This demonstration highlights the
potential of task-oriented AI agents to not only filter information but also to
actively participate in and accelerate the workflows of the academic community.

</details>


### [192] [CountTRuCoLa: Rule Confidence Learning for Temporal Knowledge Graph Forecasting](https://arxiv.org/abs/2509.09474)
*Julia Gastinger,Christian Meilicke,Heiner Stuckenschmidt*

Main category: cs.LG

TL;DR: This paper presents an explainable method for temporal knowledge graph forecasting using simple temporal rules.


<details>
  <summary>Details</summary>
Motivation: To create a fully explainable and effective approach for predicting future events in temporal knowledge graphs, addressing a need for interpretability in advanced models.

Method: The method involves learning four types of temporal rules, incorporating a confidence function based on recency and frequency of occurrences.

Result: The proposed approach achieves comparable or better performance than eight state-of-the-art models and two baseline methods across nine datasets.

Conclusion: The method achieves strong predictive performance while maintaining full interpretability, showcasing the value of using simple temporal rules for forecasting in temporal knowledge graphs.

Abstract: We address the task of temporal knowledge graph (TKG) forecasting by
introducing a fully explainable method based on temporal rules. Motivated by
recent work proposing a strong baseline using recurrent facts, our approach
learns four simple types of rules with a confidence function that considers
both recency and frequency. Evaluated on nine datasets, our method matches or
surpasses the performance of eight state-of-the-art models and two baselines,
while providing fully interpretable predictions.

</details>


### [193] [Balancing Utility and Privacy: Dynamically Private SGD with Random Projection](https://arxiv.org/abs/2509.09485)
*Zhanhong Jiang,Md Zahid Hasan,Nastaran Saadati,Aditya Balu,Chao Liu,Soumik Sarkar*

Main category: cs.LG

TL;DR: This paper introduces D2P2-SGD, a stochastic optimization framework combining dynamic differential privacy and random projection to enhance model accuracy while ensuring privacy.


<details>
  <summary>Details</summary>
Motivation: Stochastic optimization faces challenges due to privacy leaks in gradients and model parameters, as well as inefficiencies in handling high-dimensional models under static differential privacy mechanisms.

Method: The authors propose the D2P2-SGD framework, combining dynamic differential privacy with automatic gradient clipping and random projection within SGD, allowing dynamic tradeoff adjustments between utility and privacy.

Result: The method achieves sub-linear convergence rates comparable to the best existing rates and demonstrates improved model accuracy in experiments across various datasets while maintaining privacy.

Conclusion: D2P2-SGD effectively balances privacy and utility, enabling efficient privacy-preserving learning for complex models.

Abstract: Stochastic optimization is a pivotal enabler in modern machine learning,
producing effective models for various tasks. However, several existing works
have shown that model parameters and gradient information are susceptible to
privacy leakage. Although Differentially Private SGD (DPSGD) addresses privacy
concerns, its static noise mechanism impacts the error bounds for model
performance. Additionally, with the exponential increase in model parameters,
efficient learning of these models using stochastic optimizers has become more
challenging. To address these concerns, we introduce the Dynamically
Differentially Private Projected SGD (D2P2-SGD) optimizer. In D2P2-SGD, we
combine two important ideas: (i) dynamic differential privacy (DDP) with
automatic gradient clipping and (ii) random projection with SGD, allowing
dynamic adjustment of the tradeoff between utility and privacy of the model. It
exhibits provably sub-linear convergence rates across different objective
functions, matching the best available rate. The theoretical analysis further
suggests that DDP leads to better utility at the cost of privacy, while random
projection enables more efficient model learning. Extensive experiments across
diverse datasets show that D2P2-SGD remarkably enhances accuracy while
maintaining privacy. Our code is available here.

</details>


### [194] [PIPES: A Meta-dataset of Machine Learning Pipelines](https://arxiv.org/abs/2509.09512)
*Cynthia Moreira Maia,Lucas B. V. de Amorim,George D. C. Cavalcanti,Rafael M. O. Cruz*

Main category: cs.LG

TL;DR: PIPES addresses the limitations of OpenML by providing a diverse and complete set of machine learning pipelines for meta-learning analyses.


<details>
  <summary>Details</summary>
Motivation: The paper aims to solve the limitations of dataset diversity and imbalanced sampling found in OpenML records for the Algorithm Selection Problem (ASP) in machine learning.

Method: The authors propose PIPES, a dataset containing results from 9,408 pipeline experiments conducted on 300 datasets, with a focus on meticulous documentation of pipeline blocks and associated data.

Result: PIPES offers a diverse and comprehensive collection of experimental results, allowing researchers to examine various machine-learning pipelines systematically.

Conclusion: The proposed PIPES repository enables the meta-learning community to access a well-rounded and expandable dataset, overcoming OpenML limitations and accelerating progress in ASP research.

Abstract: Solutions to the Algorithm Selection Problem (ASP) in machine learning face
the challenge of high computational costs associated with evaluating various
algorithms' performances on a given dataset. To mitigate this cost, the
meta-learning field can leverage previously executed experiments shared in
online repositories such as OpenML. OpenML provides an extensive collection of
machine learning experiments. However, an analysis of OpenML's records reveals
limitations. It lacks diversity in pipelines, specifically when exploring data
preprocessing steps/blocks, such as scaling or imputation, resulting in limited
representation. Its experiments are often focused on a few popular techniques
within each pipeline block, leading to an imbalanced sample. To overcome the
observed limitations of OpenML, we propose PIPES, a collection of experiments
involving multiple pipelines designed to represent all combinations of the
selected sets of techniques, aiming at diversity and completeness. PIPES stores
the results of experiments performed applying 9,408 pipelines to 300 datasets.
It includes detailed information on the pipeline blocks, training and testing
times, predictions, performances, and the eventual error messages. This
comprehensive collection of results allows researchers to perform analyses
across diverse and representative pipelines and datasets. PIPES also offers
potential for expansion, as additional data and experiments can be incorporated
to support the meta-learning community further. The data, code, supplementary
material, and all experiments can be found at
https://github.com/cynthiamaia/PIPES.git.

</details>


### [195] [Cough Classification using Few-Shot Learning](https://arxiv.org/abs/2509.09515)
*Yoga Disha Sendhil Kumar,Manas V Shetty,Sudip Vhaduri*

Main category: cs.LG

TL;DR: This study explores few-shot learning for classifying cough sounds to detect COVID-19, Flu, and Healthy conditions, achieving competitive accuracy with minimal labeled data.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of limited labeled medical datasets for respiratory sound classification, specifically targeting COVID-19 detection.

Method: Prototypical Networks were used with cough sound spectrogram representations, enabling models to learn effectively with few training examples.

Result: Achieved 74.87% accuracy for multi-class classification and over 70% accuracy for binary classification using only 15 training samples per class.

Conclusion: Few-shot learning proved effective, demonstrating its potential for medical diagnostics in scenarios with limited labeled data availability.

Abstract: This paper investigates the effectiveness of few-shot learning for
respiratory sound classification, focusing on coughbased detection of COVID-19,
Flu, and healthy conditions. We leverage Prototypical Networks with spectrogram
representations of cough sounds to address the challenge of limited labeled
data. Our study evaluates whether few-shot learning can enable models to
achieve performance comparable to traditional deep learning approaches while
using significantly fewer training samples. Additionally, we compare
multi-class and binary classification models to assess whether multi-class
models can perform comparably to their binary counterparts. Experimental
findings show that few-shot learning models can achieve competitive accuracy.
Our model attains 74.87% accuracy in multi-class classification with only 15
support examples per class, while binary classification achieves over 70%
accuracy across all class pairs. Class-wise analysis reveals Flu as the most
distinguishable class, and Healthy as the most challenging. Statistical tests
(paired t-test p = 0.149, Wilcoxon p = 0.125) indicate no significant
performance difference between binary and multiclass models, supporting the
viability of multi-class classification in this setting. These results
highlight the feasibility of applying few-shot learning in medical diagnostics,
particularly when large labeled datasets are unavailable.

</details>


### [196] [Graph Alignment via Dual-Pass Spectral Encoding and Latent Space Communication](https://arxiv.org/abs/2509.09597)
*Maysam Behmanesh,Erkan Turan,Maks Ovsjanikov*

Main category: cs.LG

TL;DR: The paper proposes a novel graph alignment framework addressing limitations in node distinctiveness and latent space misalignment by enhancing embeddings and ensuring geometric consistency, outperforming unsupervised baselines.


<details>
  <summary>Details</summary>
Motivation: Existing unsupervised graph alignment methods face challenges due to oversmoothing in GNN-based embeddings and latent space misalignment caused by diverse graph characteristics and training instability.

Method: The framework employs a dual-pass encoder with spectral filters to enhance embedding quality and distinctiveness, alongside a geometry-aware functional map module that ensures bijective and isometric transformations for consistent latent space alignment.

Result: Extensive experiments show superior performance of the proposed method in robustness, accuracy, and generalization across graph domains and vision-language benchmarks compared to other unsupervised approaches.

Conclusion: The method provides a robust unsupervised solution for graph alignment and demonstrates effective generalization capabilities to tasks beyond the graph domain, including vision and language representation alignment.

Abstract: Graph alignment-the problem of identifying corresponding nodes across
multiple graphs-is fundamental to numerous applications. Most existing
unsupervised methods embed node features into latent representations to enable
cross-graph comparison without ground-truth correspondences. However, these
methods suffer from two critical limitations: the degradation of node
distinctiveness due to oversmoothing in GNN-based embeddings, and the
misalignment of latent spaces across graphs caused by structural noise, feature
heterogeneity, and training instability, ultimately leading to unreliable node
correspondences. We propose a novel graph alignment framework that
simultaneously enhances node distinctiveness and enforces geometric consistency
across latent spaces. Our approach introduces a dual-pass encoder that combines
low-pass and high-pass spectral filters to generate embeddings that are both
structure-aware and highly discriminative. To address latent space
misalignment, we incorporate a geometry-aware functional map module that learns
bijective and isometric transformations between graph embeddings, ensuring
consistent geometric relationships across different representations. Extensive
experiments on graph benchmarks demonstrate that our method consistently
outperforms existing unsupervised alignment baselines, exhibiting superior
robustness to structural inconsistencies and challenging alignment scenarios.
Additionally, comprehensive evaluation on vision-language benchmarks using
diverse pretrained models shows that our framework effectively generalizes
beyond graph domains, enabling unsupervised alignment of vision and language
representations.

</details>


### [197] [Conditioning on PDE Parameters to Generalise Deep Learning Emulation of Stochastic and Chaotic Dynamics](https://arxiv.org/abs/2509.09599)
*Ira J. S. Shokar,Rich R. Kerswell,Peter H. Haynes*

Main category: cs.LG

TL;DR: The paper introduces a deep learning emulator for PDE-based spatio-temporal systems, enabling computation across broad parameter ranges and domain sizes while being computationally efficient and capable of uncertainty quantification.


<details>
  <summary>Details</summary>
Motivation: To create a computationally efficient tool for exploring and analyzing stochastic and chaotic spatio-temporal phenomena described by PDEs.

Method: The model is pre-trained on a single parameter domain and fine-tuned on smaller, diverse datasets. Local attention mechanisms allow flexibility for varying domain sizes and resolutions.

Result: The emulator showcases its ability on the Kuramoto-Sivashinsky equation and beta-plane turbulence, achieving parameter interpolation, significant computational speed-ups, and rare event statistical analysis.

Conclusion: This approach offers a scalable, efficient alternative for studying complex PDE-driven systems, with added capabilities for uncertainty quantification and rare event investigation.

Abstract: We present a deep learning emulator for stochastic and chaotic
spatio-temporal systems, explicitly conditioned on the parameter values of the
underlying partial differential equations (PDEs). Our approach involves
pre-training the model on a single parameter domain, followed by fine-tuning on
a smaller, yet diverse dataset, enabling generalisation across a broad range of
parameter values. By incorporating local attention mechanisms, the network is
capable of handling varying domain sizes and resolutions. This enables
computationally efficient pre-training on smaller domains while requiring only
a small additional dataset to learn how to generalise to larger domain sizes.
We demonstrate the model's capabilities on the chaotic Kuramoto-Sivashinsky
equation and stochastically-forced beta-plane turbulence, showcasing its
ability to capture phenomena at interpolated parameter values. The emulator
provides significant computational speed-ups over conventional numerical
integration, facilitating efficient exploration of parameter space, while a
probabilistic variant of the emulator provides uncertainty quantification,
allowing for the statistical study of rare events.

</details>


### [198] [ReBaNO: Reduced Basis Neural Operator Mitigating Generalization Gaps and Achieving Discretization Invariance](https://arxiv.org/abs/2509.09611)
*Haolan Zheng,Yanlai Chen,Jiequn Han,Yue Yu*

Main category: cs.LG

TL;DR: The paper introduces a new data-efficient operator learning algorithm, ReBaNO, which leverages Reduced Basis Method principles to solve diverse PDEs efficiently.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address generalization gaps and high computational costs in existing PDE operator learning techniques.

Method: ReBaNO uses an adaptive greedy algorithm with a task-specific activation function, inspired by Reduced Basis Method and Physics-Informed Neural Networks.

Result: ReBaNO outperforms existing algorithms like PCA-Net and DeepONet in generalization and computational efficiency, including ensured discretization invariance.

Conclusion: ReBaNO offers a compact and rigorously adaptive approach to solving diverse PDEs with improved generalization and reduced computational effort, standing out from current operator learning methodologies.

Abstract: We propose a novel data-lean operator learning algorithm, the Reduced Basis
Neural Operator (ReBaNO), to solve a group of PDEs with multiple distinct
inputs. Inspired by the Reduced Basis Method and the recently introduced
Generative Pre-Trained Physics-Informed Neural Networks, ReBaNO relies on a
mathematically rigorous greedy algorithm to build its network structure offline
adaptively from the ground up. Knowledge distillation via task-specific
activation function allows ReBaNO to have a compact architecture requiring
minimal computational cost online while embedding physics. In comparison to
state-of-the-art operator learning algorithms such as PCA-Net, DeepONet, FNO,
and CNO, numerical results demonstrate that ReBaNO significantly outperforms
them in terms of eliminating/shrinking the generalization gap for both in- and
out-of-distribution tests and being the only operator learning algorithm
achieving strict discretization invariance.

</details>


### [199] [Explaining Concept Drift through the Evolution of Group Counterfactuals](https://arxiv.org/abs/2509.09616)
*Ignacy Stępka,Jerzy Stefanowski*

Main category: cs.LG

TL;DR: Introduces a methodology to explain concept drift by analyzing temporal changes in group-based counterfactual explanations (GCEs).


<details>
  <summary>Details</summary>
Motivation: Address the challenge of explaining how and why a machine learning model's decision-making changes due to concept drift, beyond just detecting it.

Method: Proposes tracking shifts in GCEs' cluster centroids and associated counterfactual action vectors, integrated within a novel three-layer analysis framework (data, model, and explanation layers).

Result: Provides interpretable insights into structural changes in the model's decision boundary, distinguishing between root causes of drift (e.g., spatial data shift vs. re-labeling of concepts).

Conclusion: The methodology enhances the understanding and diagnosis of concept drift, offering a holistic view that combines data, model, and explanation layers.

Abstract: Machine learning models in dynamic environments often suffer from concept
drift, where changes in the data distribution degrade performance. While
detecting this drift is a well-studied topic, explaining how and why the
model's decision-making logic changes still remains a significant challenge. In
this paper, we introduce a novel methodology to explain concept drift by
analyzing the temporal evolution of group-based counterfactual explanations
(GCEs). Our approach tracks shifts in the GCEs' cluster centroids and their
associated counterfactual action vectors before and after a drift. These
evolving GCEs act as an interpretable proxy, revealing structural changes in
the model's decision boundary and its underlying rationale. We operationalize
this analysis within a three-layer framework that synergistically combines
insights from the data layer (distributional shifts), the model layer
(prediction disagreement), and our proposed explanation layer. We show that
such holistic view allows for a more comprehensive diagnosis of drift, making
it possible to distinguish between different root causes, such as a spatial
data shift versus a re-labeling of concepts.

</details>


### [200] [Functional Groups are All you Need for Chemically Interpretable Molecular Property Prediction](https://arxiv.org/abs/2509.09619)
*Roshan Balaji,Joe Bobby,Nirav Pravinbhai Bhatt*

Main category: cs.LG

TL;DR: The paper introduces the Functional Group Representation (FGR) framework, enabling interpretable molecular property prediction using functional groups.


<details>
  <summary>Details</summary>
Motivation: Although deep learning (DL) models have advanced molecular property prediction, their lack of interpretability limits adoption by chemists.

Method: The study develops the Functional Group Representation (FGR) framework by encoding molecules using curated functional groups (FG) and mined patterns (MFG), combined with pre-training on large datasets.

Result: The proposed FGR method achieves state-of-the-art performance across 33 benchmark datasets while enhancing chemical interpretability and aligning predictions with chemical principles.

Conclusion: The FGR framework improves both prediction performance and interpretability, enabling chemists to uncover structure-property relationships for molecular discovery.

Abstract: Molecular property prediction using deep learning (DL) models has accelerated
drug and materials discovery, but the resulting DL models often lack
interpretability, hindering their adoption by chemists. This work proposes
developing molecule representations using the concept of Functional Groups (FG)
in chemistry. We introduce the Functional Group Representation (FGR) framework,
a novel approach to encoding molecules based on their fundamental chemical
substructures. Our method integrates two types of functional groups: those
curated from established chemical knowledge (FG), and those mined from a large
molecular corpus using sequential pattern mining (MFG). The resulting FGR
framework encodes molecules into a lower-dimensional latent space by leveraging
pre-training on a large dataset of unlabeled molecules. Furthermore, the
proposed framework allows the inclusion of 2D structure-based descriptors of
molecules. We demonstrate that the FGR framework achieves state-of-the-art
performance on a diverse range of 33 benchmark datasets spanning physical
chemistry, biophysics, quantum mechanics, biological activity, and
pharmacokinetics while enabling chemical interpretability. Crucially, the
model's representations are intrinsically aligned with established chemical
principles, allowing chemists to directly link predicted properties to specific
functional groups and facilitating novel insights into structure-property
relationships. Our work presents a significant step toward developing
high-performing, chemically interpretable DL models for molecular discovery.

</details>


### [201] [Feasibility-Guided Fair Adaptive Offline Reinforcement Learning for Medicaid Care Management](https://arxiv.org/abs/2509.09655)
*Sanjay Basu,Sadiq Y. Patel,Parth Sheth,Bhairavi Muralidharan,Namrata Elamaran,Aakriti Kinra,Rajaie Batniji*

Main category: cs.LG

TL;DR: The paper introduces FG-FARL, an offline reinforcement learning procedure that adjusts group safety thresholds for equitable decision-making, showing improved fairness metrics in a Medicaid setting.


<details>
  <summary>Details</summary>
Motivation: To address harm reduction and fairness across protected subgroups in decision-making processes using offline reinforcement learning.

Method: Developed FG-FARL, a procedure that aligns per-group safety thresholds to a fairness target, and evaluated its performance against BC and HACO baselines using de-identified Medicaid data.

Result: FG-FARL achieved comparable value to baseline methods while improving fairness metrics as per subgroup disparity analyses.

Conclusion: FG-FARL offers a practical approach to improving safety and fairness in decision support systems.

Abstract: We introduce Feasibility-Guided Fair Adaptive Reinforcement Learning
(FG-FARL), an offline RL procedure that calibrates per-group safety thresholds
to reduce harm while equalizing a chosen fairness target (coverage or harm)
across protected subgroups. Using de-identified longitudinal trajectories from
a Medicaid population health management program, we evaluate FG-FARL against
behavior cloning (BC) and HACO (Hybrid Adaptive Conformal Offline RL; a global
conformal safety baseline). We report off-policy value estimates with bootstrap
95% confidence intervals and subgroup disparity analyses with p-values. FG-FARL
achieves comparable value to baselines while improving fairness metrics,
demonstrating a practical path to safer and more equitable decision support.

</details>


### [202] [ButterflyQuant: Ultra-low-bit LLM Quantization through Learnable Orthogonal Butterfly Transforms](https://arxiv.org/abs/2509.09679)
*Bingxin Xu,Zhen Dong,Oussama Elachqar,Yuzhang Shang*

Main category: cs.LG

TL;DR: ButterflyQuant is proposed to address performance issues in 2-bit quantization of large language models by replacing fixed rotations with adaptive butterfly transforms.


<details>
  <summary>Details</summary>
Motivation: Existing 2-bit quantization methods suffer from catastrophic performance loss caused by outliers and fail to adapt to layer-specific outlier patterns.

Method: ButterflyQuant uses learnable butterfly transforms parameterized by continuous Givens rotation angles, combined with uniformity regularization for smoother post-transformation activations.

Result: ButterflyQuant achieves a perplexity of 15.4 on LLaMA-2-7B with 2-bit quantization, outperforming previous methods like QuaRot (22.1).

Conclusion: Replacing fixed, non-adaptive rotations with learnable, adaptive butterfly transforms effectively reduces outliers and improves memory-efficient performance for language models.

Abstract: Large language models require massive memory footprints, severely limiting
deployment on consumer hardware. Quantization reduces memory through lower
numerical precision, but extreme 2-bit quantization suffers from catastrophic
performance loss due to outliers in activations. Rotation-based methods such as
QuIP and QuaRot apply orthogonal transforms to eliminate outliers before
quantization, using computational invariance: $\mathbf{y} = \mathbf{Wx} =
(\mathbf{WQ}^T)(\mathbf{Qx})$ for orthogonal $\mathbf{Q}$. However, these
methods use fixed transforms--Hadamard matrices achieving optimal worst-case
coherence $\mu = 1/\sqrt{n}$--that cannot adapt to specific weight
distributions. We identify that different transformer layers exhibit distinct
outlier patterns, motivating layer-adaptive rotations rather than
one-size-fits-all approaches. We propose ButterflyQuant, which replaces
Hadamard rotations with learnable butterfly transforms parameterized by
continuous Givens rotation angles. Unlike Hadamard's discrete $\{+1, -1\}$
entries that are non-differentiable and prohibit gradient-based learning,
butterfly transforms' continuous parameterization enables smooth optimization
while guaranteeing orthogonality by construction. This orthogonal constraint
ensures theoretical guarantees in outlier suppression while achieving $O(n \log
n)$ computational complexity with only $\frac{n \log n}{2}$ learnable
parameters. We further introduce a uniformity regularization on
post-transformation activations to promote smoother distributions amenable to
quantization. Learning requires only 128 calibration samples and converges in
minutes on a single GPU--a negligible one-time cost. On LLaMA-2-7B with 2-bit
quantization, ButterflyQuant achieves 15.4 perplexity versus 22.1 for QuaRot.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [203] [Time-Fair Benchmarking for Metaheuristics: A Restart-Fair Protocol for Fixed-Time Comparisons](https://arxiv.org/abs/2509.08986)
*Junbo Jacob Lian*

Main category: cs.NE

TL;DR: This paper critiques the reliance on function evaluations (FEs) and introduces a benchmark emphasizing wall-clock time, aiming to make metaheuristic algorithm comparisons more fair and practical.


<details>
  <summary>Details</summary>
Motivation: Conventional metaheuristic comparisons often rely on function evaluations, potentially hiding overhead costs and computational burdens, leading to an incomplete assessment of practical algorithm performance.

Method: The authors propose a fixed-time benchmarking protocol that equalizes wall-clock time as the constraint, incorporating restart fairness, anytime performance measures, and standardized reporting checklists.

Result: The proposed benchmarking protocol includes tools like anytime performance curves and expectations for runtime-based performance profiles, ensuring reproducibility and reducing undisclosed overheads.

Conclusion: This systematic benchmarking approach promotes fairer, more credible, and practically relevant evaluations of metaheuristic algorithms by emphasizing wall-clock time and standardized practices.

Abstract: Numerous purportedly improved metaheuristics claim superior performance based
on equivalent function evaluations (FEs), yet often conceal additional
computational burdens in more intensive iterations, preprocessing stages, or
hyperparameter tuning. This paper posits that wall-clock time, rather than
solely FEs, should serve as the principal budgetary constraint for equitable
comparisons. We formalize a fixed-time, restart-fair benchmarking protocol
wherein each algorithm is allotted an identical wall-clock time budget per
problem instance, permitting unrestricted utilization of restarts, early
termination criteria, and internal adaptive mechanisms. We advocate for the
adoption of anytime performance curves, expected running time (ERT) metrics,
and performance profiles that employ time as the cost measure, all aimed at
predefined targets. Furthermore, we introduce a concise, reproducible checklist
to standardize reporting practices and mitigate undisclosed computational
overheads. This approach fosters more credible and practically relevant
evaluations of metaheuristic algorithms.

</details>


### [204] [A modified RIME algorithm with covariance learning and diversity enhancement for numerical optimization](https://arxiv.org/abs/2509.09529)
*Shangqing Shi,Luoxiao Zhang,Yuchen Yin,Xiong Yang,Hoileong Lee*

Main category: cs.NE

TL;DR: The paper proposes MRIME-CD, a modified version of the RIME algorithm, to overcome limitations including loss of population diversity and tendency to get stuck in local optima.


<details>
  <summary>Details</summary>
Motivation: The original RIME algorithm struggles with rapid population diversity loss and unbalanced exploration and exploitation, leading to suboptimal solutions.

Method: MRIME-CD uses three strategies: covariance learning for diversity and balanced exploitation, average bootstrapping to enhance global search in early stages, and a stagnation indicator with stochastic covariance learning for escaping local optima.

Result: Experiments on CEC2017 and CEC2022 test sets, analyzed with several statistical tests, show improved solution accuracy, faster convergence, and better stability of MRIME-CD compared to basic RIME.

Conclusion: MRIME-CD overcomes key limitations of the original RIME algorithm and demonstrates significant performance improvements in metaheuristic optimization tasks.

Abstract: Metaheuristics are widely applied for their ability to provide more efficient
solutions. The RIME algorithm is a recently proposed physical-based
metaheuristic algorithm with certain advantages. However, it suffers from rapid
loss of population diversity during optimization and is prone to fall into
local optima, leading to unbalanced exploitation and exploration. To address
the shortcomings of RIME, this paper proposes a modified RIME with covariance
learning and diversity enhancement (MRIME-CD). The algorithm applies three
strategies to improve the optimization capability. First, a covariance learning
strategy is introduced in the soft-rime search stage to increase the population
diversity and balance the over-exploitation ability of RIME through the
bootstrapping effect of dominant populations. Second, in order to moderate the
tendency of RIME population to approach the optimal individual in the early
search stage, an average bootstrapping strategy is introduced into the
hard-rime puncture mechanism, which guides the population search through the
weighted position of the dominant populations, thus enhancing the global search
ability of RIME in the early stage. Finally, a new stagnation indicator is
proposed, and a stochastic covariance learning strategy is used to update the
stagnant individuals in the population when the algorithm gets stagnant, thus
enhancing the ability to jump out of the local optimal solution. The proposed
MRIME-CD algorithm is subjected to a series of validations on the CEC2017 test
set, the CEC2022 test set, and the experimental results are analyzed using the
Friedman test, the Wilcoxon rank sum test, and the Kruskal Wallis test. The
results show that MRIME-CD can effectively improve the performance of basic
RIME and has obvious superiorities in terms of solution accuracy, convergence
speed and stability.

</details>


### [205] [An improved educational competition optimizer with multi-covariance learning operators for global optimization problems](https://arxiv.org/abs/2509.09552)
*Baoqi Zhao,Xiong Yang,Hoileong Lee,Bowen Dong*

Main category: cs.NE

TL;DR: This paper introduces an Enhanced Educational Competition Optimizer (IECO-MCO) to address shortcomings in the original ECO algorithm, particularly its imbalance between exploration and exploitation, by using multi-covariance learning operators.


<details>
  <summary>Details</summary>
Motivation: The study seeks to mitigate the limitations of the educational competition optimizer (ECO)—especially its susceptibility to local optima and inefficiency in handling complex optimization problems—by proposing improvements.

Method: The proposed IECO includes three unique covariance learning operators aimed at balancing exploitation and exploration, and preventing premature convergence. Performance is evaluated using benchmark functions and statistical analyses.

Result: IECO-MCO outperforms the basic ECO and other competing algorithms in convergence speed, stability, and avoidance of local optima, with statistical tests affirming its effectiveness.

Conclusion: IECO-MCO is robust and practically effective for solving complex optimization tasks in real-world applications, as demonstrated through benchmark testing and constrained optimization problem-solving.

Abstract: The educational competition optimizer is a recently introduced metaheuristic
algorithm inspired by human behavior, originating from the dynamics of
educational competition within society. Nonetheless, ECO faces constraints due
to an imbalance between exploitation and exploration, rendering it susceptible
to local optima and demonstrating restricted effectiveness in addressing
complex optimization problems. To address these limitations, this study
presents an enhanced educational competition optimizer (IECO-MCO) utilizing
multi-covariance learning operators. In IECO, three distinct covariance
learning operators are introduced to improve the performance of ECO. Each
operator effectively balances exploitation and exploration while preventing
premature convergence of the population. The effectiveness of IECO is assessed
through benchmark functions derived from the CEC 2017 and CEC 2022 test suites,
and its performance is compared with various basic and improved algorithms
across different categories. The results demonstrate that IECO-MCO surpasses
the basic ECO and other competing algorithms in convergence speed, stability,
and the capability to avoid local optima. Furthermore, statistical analyses,
including the Friedman test, Kruskal-Wallis test, and Wilcoxon rank-sum test,
are conducted to validate the superiority of IECO-MCO over the compared
algorithms. Compared with the basic algorithm (improved algorithm), IECO-MCO
achieved an average ranking of 2.213 (2.488) on the CE2017 and CEC2022 test
suites. Additionally, the practical applicability of the proposed IECO-MCO
algorithm is verified by solving constrained optimization problems. The
experimental outcomes demonstrate the superior performance of IECO-MCO in
tackling intricate optimization problems, underscoring its robustness and
practical effectiveness in real-world scenarios.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [206] [HD-MoE: Hybrid and Dynamic Parallelism for Mixture-of-Expert LLMs with 3D Near-Memory Processing](https://arxiv.org/abs/2509.09420)
*Haochen Huang,Shuzhang Zhong,Zhe Zhang,Shuangchen Li,Dimin Niu,Hongzhong Zheng,Runsheng Wang,Meng Li*

Main category: cs.PF

TL;DR: The paper presents HD-MoE, a strategy to enhance the efficiency of Mixture-of-Expert (MoE) computations on Near-Memory Processing (NMP) accelerators, achieving up to 1.8x speedup over traditional methods.


<details>
  <summary>Details</summary>
Motivation: Large Language Models with Mixture-of-Expert architectures face inefficiencies in mapping computations on Near-Memory Processing (NMP) accelerators due to high communication costs and uneven computation utilization.

Method: The authors propose HD-MoE, which combines an offline hybrid parallel mapping algorithm with an online dynamic scheduling strategy to optimize parallel mapping for MoE computations on NMP accelerators.

Result: HD-MoE demonstrates significant speedup performance—ranging from 1.1x to 1.8x compared to Tensor Parallelism, Expert Parallelism, and baseline Hybrid Tensor-Expert Parallelism strategies.

Conclusion: HD-MoE effectively addresses the challenges of communication costs and computation imbalance in MoE computations on NMP accelerators, showcasing its potential for improving LLM inference efficiency.

Abstract: Large Language Models (LLMs) with Mixture-of-Expert (MoE) architectures
achieve superior model performance with reduced computation costs, but at the
cost of high memory capacity and bandwidth requirements. Near-Memory Processing
(NMP) accelerators that stack memory directly on the compute through hybrid
bonding have demonstrated high bandwidth with high energy efficiency, becoming
a promising architecture for MoE models. However, as NMP accelerators comprise
distributed memory and computation, how to map the MoE computation directly
determines the LLM inference efficiency. Existing parallel mapping strategies,
including Tensor Parallelism (TP) and Expert Parallelism (EP), suffer from
either high communication costs or unbalanced computation utilization, leading
to inferior efficiency. The dynamic routing mechanism of MoE LLMs further
aggravates the efficiency challenges. Therefore, in this paper, we propose
HD-MoE to automatically optimize the MoE parallel computation across an NMP
accelerator. HD-MoE features an offline automatic hybrid parallel mapping
algorithm and an online dynamic scheduling strategy to reduce the communication
costs while maximizing the computation utilization. With extensive experimental
results, we demonstrate that HD-MoE achieves a speedup ranging from 1.1x to
1.8x over TP, 1.1x to 1.5x over EP, and 1.0x to 1.4x over the baseline Hybrid
TP-EP with Compute-Balanced parallelism strategies.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [207] [Towards Verified Compilation of Floating-point Optimization in Scientific Computing Programs](https://arxiv.org/abs/2509.09019)
*Mohit Tekriwal,John Sarracino*

Main category: cs.PL

TL;DR: This paper investigates the correctness of floating-point optimizations, specifically the Fused-Multiply-Add (FMA) operation, using the Verified LLVM framework in Rocq.


<details>
  <summary>Details</summary>
Motivation: The motivation is to ensure correctness of compiler optimizations in scientific computing programs, especially when fast math floating-point optimizations are applied.

Method: The method involves leveraging the Verified LLVM framework within the Rocq theorem prover to formally verify the FMA optimization for a basic arithmetic block.

Result: A preliminary verification of the FMA optimization for the arithmetic expression $a*b+c$ is successfully demonstrated.

Conclusion: The work establishes an initial approach to verifying optimization correctness and suggests further extending this framework for additional floating-point optimizations and program features.

Abstract: Scientific computing programs often undergo aggressive compiler optimization
to achieve high performance and efficient resource utilization. While
performance is critical, we also need to ensure that these optimizations are
correct. In this paper, we focus on a specific class of optimizations,
floating-point optimizations, notably due to fast math, at the LLVM IR level.
We present a preliminary work, which leverages the Verified LLVM framework in
the Rocq theorem prover, to prove the correctness of Fused-Multiply-Add (FMA)
optimization for a basic block implementing the arithmetic expression $a * b +
c$ . We then propose ways to extend this preliminary results by adding more
program features and fast math floating-point optimizations.

</details>


### [208] [Dependent-Type-Preserving Memory Allocation](https://arxiv.org/abs/2509.09059)
*Paulette Koronkevich,William J. Bowman*

Main category: cs.PL

TL;DR: The paper addresses the problem of specifications in dependently typed languages being violated during compilation or linking with external programs, proposing a type-preserving compilation approach to ensure specifications are upheld.


<details>
  <summary>Details</summary>
Motivation: To ensure that specifications written in dependently typed programming languages are preserved even after compilation and when linked with external programs, preventing unsafe behaviors such as memory errors.

Method: The authors propose developing a typed intermediate language with dependent memory allocation and a dependent-type-preserving compiler pass for memory allocation.

Result: The approach facilitates type checking during linking, preventing ill-typed programs from violating original specifications.

Conclusion: Type-preserving compilation can maintain specifications throughout the compilation and linking process, enhancing safety and correctness in dependently typed programming environments.

Abstract: Dependently typed programming languages such as Coq, Agda, Idris, and F*,
allow programmers to write detailed specifications of their programs and prove
their programs meet these specifications. However, these specifications can be
violated during compilation since they are erased after type checking. External
programs linked with the compiled program can violate the specifications of the
original program and change the behavior of the compiled program -- even when
compiled with a verified compiler. For example, since Coq does not allow
explicitly allocating memory, a programmer might link their Coq program with a
C program that can allocate memory. Even if the Coq program is compiled with a
verified compiler, the external C program can still violate the memory-safe
specification of the Coq program by providing an uninitialized pointer to
memory. This error could be ruled out by type checking in a language expressive
enough to indicate whether memory is initialized versus uninitialized. Linking
with a program with an uninitialized pointer could be considered ill-typed, and
our linking process could prevent linking with ill-typed programs. To
facilitate type checking during linking, we can use type-preserving
compilation, which preserves the types through the compilation process. In this
ongoing work, we develop a typed intermediate language that supports dependent
memory allocation, as well as a dependent-type-preserving compiler pass for
memory allocation.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [209] [Multi Robot Coordination in Highly Dynamic Environments: Tackling Asymmetric Obstacles and Limited Communication](https://arxiv.org/abs/2509.08859)
*Vincenzo Suriani,Daniele Affinita,Domenico D. Bloisi,Daniele Nardi*

Main category: cs.RO

TL;DR: This paper proposes a novel distributed coordination method for multi-agent systems in environments with active asymmetric obstacles and limited communication, validated through simulations and RoboCup competitions.


<details>
  <summary>Details</summary>
Motivation: Managing task assignments in multi-agent systems operating in partially observable, highly dynamic environments with limited communication is challenging, especially when dealing with active and asymmetric obstacles.

Method: The authors developed a market-based distributed coordination algorithm designed for low-communication scenarios. The method specifically accounts for the asymmetry of obstacles to improve task assignments and reduce overlaps.

Result: Experimental tests, including RoboCup competition simulations with NAO robots, showed a significant reduction in task overlaps, with a 52% decrease in the most frequently reallocated tasks under limited communication conditions.

Conclusion: The novel coordination method effectively addresses task assignment challenges in complex, low-communication scenarios, demonstrating its potential for real-world applications in distributed multi-agent systems.

Abstract: Coordinating a fully distributed multi-agent system (MAS) can be challenging
when the communication channel has very limited capabilities in terms of
sending rate and packet payload. When the MAS has to deal with active obstacles
in a highly partially observable environment, the communication channel
acquires considerable relevance. In this paper, we present an approach to deal
with task assignments in extremely active scenarios, where tasks need to be
frequently reallocated among the agents participating in the coordination
process. Inspired by market-based task assignments, we introduce a novel
distributed coordination method to orchestrate autonomous agents' actions
efficiently in low communication scenarios. In particular, our algorithm takes
into account asymmetric obstacles. While in the real world, the majority of
obstacles are asymmetric, they are usually treated as symmetric ones, thus
limiting the applicability of existing methods. To summarize, the presented
architecture is designed to tackle scenarios where the obstacles are active and
asymmetric, the communication channel is poor and the environment is partially
observable. Our approach has been validated in simulation and in the real
world, using a team of NAO robots during official RoboCup competitions.
Experimental results show a notable reduction in task overlaps in limited
communication settings, with a decrease of 52% in the most frequent reallocated
task.

</details>


### [210] [Rapid Manufacturing of Lightweight Drone Frames Using Single-Tow Architected Composites](https://arxiv.org/abs/2509.09024)
*Md Habib Ullah Khan,Kaiyue Deng,Ismail Mujtaba Khan,Kelvin Fu*

Main category: cs.RO

TL;DR: The paper introduces a novel drone frame using Face-Centered Cubic lattice structures and a 3D Fiber Tethering manufacturing method, leading to lightweight, high-strength designs.


<details>
  <summary>Details</summary>
Motivation: Current composite manufacturing methods for drones have limitations in achieving weight-efficient 3D forms with continuous fiber reinforcement.

Method: The study uses a Face-Centered Cubic lattice design combined with 3D Fiber Tethering, ensuring continuous fiber alignment and eliminating weak points.

Result: The proposed drone frame is 10% lighter than commercial options, exhibits four to eight times the specific strength of metals/plastics, and extends flight time by 3 minutes.

Conclusion: Single tow lattice truss-based designs using the 3DFiT method show promise for lightweight and durable drone manufacturing.

Abstract: The demand for lightweight and high-strength composite structures is rapidly
growing in aerospace and robotics, particularly for optimized drone frames.
However, conventional composite manufacturing methods struggle to achieve
complex 3D architectures for weight savings and rely on assembling separate
components, which introduce weak points at the joints. Additionally,
maintaining continuous fiber reinforcement remains challenging, limiting
structural efficiency. In this study, we demonstrate the lightweight Face
Centered Cubic (FFC) lattice structured conceptualization of drone frames for
weight reduction and complex topology fabrication through 3D Fiber Tethering
(3DFiT) using continuous single tow fiber ensuring precise fiber alignment,
eliminating weak points associated with traditional composite assembly.
Mechanical testing demonstrates that the fabricated drone frame exhibits a high
specific strength of around four to eight times the metal and thermoplastic,
outperforming other conventional 3D printing methods. The drone frame weighs
only 260 g, making it 10% lighter than the commercial DJI F450 frame, enhancing
structural integrity and contributing to an extended flight time of three
minutes, while flight testing confirms its stability and durability under
operational conditions. The findings demonstrate the potential of single tow
lattice truss-based drone frames, with 3DFiT serving as a scalable and
efficient manufacturing method.

</details>


### [211] [KoopMotion: Learning Almost Divergence Free Koopman Flow Fields for Motion Planning](https://arxiv.org/abs/2509.09074)
*Alice Kate Li,Thales C Silva,Victoria Edwards,Vijay Kumar,M. Ani Hsieh*

Main category: cs.RO

TL;DR: The paper introduces KoopMotion, a flow field-based motion planner for robots that ensures smooth convergence to desired trajectories, overcoming limitations of the Koopman operator theory.


<details>
  <summary>Details</summary>
Motivation: While Koopman operator theory is effective for modeling dynamical systems, it struggles to enforce convergence to specific trajectories and goals, which is critical for learning from demonstrations (LfD).

Method: The authors propose KoopMotion, which uses Koopman Operators to parameterize motion flow fields, allowing robots to mimic desired trajectories, converge to trajectory endpoints, and track trajectories smoothly.

Result: KoopMotion demonstrates high sample efficiency, generating dense motion plans with 3% of the LASA dataset, and outperforms baseline methods in modeling spatial and temporal dynamics. Experimental validation includes physical and simulated robot environments.

Conclusion: KoopMotion effectively addresses the limitations of existing methods by providing a sample-efficient and accurate approach for motion planning and trajectory mimicking. It is validated across datasets and physical experiments.

Abstract: In this work, we propose a novel flow field-based motion planning method that
drives a robot from any initial state to a desired reference trajectory such
that it converges to the trajectory's end point. Despite demonstrated efficacy
in using Koopman operator theory for modeling dynamical systems, Koopman does
not inherently enforce convergence to desired trajectories nor to specified
goals -- a requirement when learning from demonstrations (LfD). We present
KoopMotion which represents motion flow fields as dynamical systems,
parameterized by Koopman Operators to mimic desired trajectories, and leverages
the divergence properties of the learnt flow fields to obtain smooth motion
fields that converge to a desired reference trajectory when a robot is placed
away from the desired trajectory, and tracks the trajectory until the end
point. To demonstrate the effectiveness of our approach, we show evaluations of
KoopMotion on the LASA human handwriting dataset and a 3D manipulator
end-effector trajectory dataset, including spectral analysis. We also perform
experiments on a physical robot, verifying KoopMotion on a miniature autonomous
surface vehicle operating in a non-static fluid flow environment. Our approach
is highly sample efficient in both space and time, requiring only 3\% of the
LASA dataset to generate dense motion plans. Additionally, KoopMotion provides
a significant improvement over baselines when comparing metrics that measure
spatial and temporal dynamics modeling efficacy.

</details>


### [212] [Kinetostatics and Particle-Swarm Optimization of Vehicle-Mounted Underactuated Metamorphic Loading Manipulators](https://arxiv.org/abs/2509.09093)
*Nan Mao,Guanglu Jia,Junpeng Chen,Emmanouil Spyrakos-Papastavridis,Jian S. Dai*

Main category: cs.RO

TL;DR: This paper introduces an innovative underactuated metamorphic loading manipulator (UMLM) that combines a reconfigurable arm and an adaptive gripper for efficient object handling.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address limitations in current fixed degree-of-freedom (DoF) systems, which face challenges like excessive actuators, complex controls, and lack of adaptability for dynamic tasks.

Method: The paper proposes integrating a metamorphic arm with an adaptive gripper, utilizes kinetostatic modeling to analyze configurations, and employs Particle-Swarm Optimization (PSO) for enhancing gripper dimensions.

Result: The simulations confirm that the UMLM offers a straightforward control strategy, versatile operations, and effective grasping capabilities across various dynamic environments.

Conclusion: The research highlights the UMLM’s potential in creating adaptable and efficient robotic systems, and the generalized framework can be applied to broader manipulator designs.

Abstract: Fixed degree-of-freedom (DoF) loading mechanisms often suffer from excessive
actuators, complex control, and limited adaptability to dynamic tasks. This
study proposes an innovative mechanism of underactuated metamorphic loading
manipulators (UMLM), integrating a metamorphic arm with a passively adaptive
gripper. The metamorphic arm exploits geometric constraints, enabling the
topology reconfiguration and flexible motion trajectories without additional
actuators. The adaptive gripper, driven entirely by the arm, conforms to
diverse objects through passive compliance. A structural model is developed,
and a kinetostatics analysis is conducted to investigate isomorphic grasping
configurations. To optimize performance, Particle-Swarm Optimization (PSO) is
utilized to refine the gripper's dimensional parameters, ensuring robust
adaptability across various applications. Simulation results validate the
UMLM's easily implemented control strategy, operational versatility, and
effectiveness in grasping diverse objects in dynamic environments. This work
underscores the practical potential of underactuated metamorphic mechanisms in
applications requiring efficient and adaptable loading solutions. Beyond the
specific design, this generalized modeling and optimization framework extends
to a broader class of manipulators, offering a scalable approach to the
development of robotic systems that require efficiency, flexibility, and robust
performance.

</details>


### [213] [LIPM-Guided Reinforcement Learning for Stable and Perceptive Locomotion in Bipedal Robots](https://arxiv.org/abs/2509.09106)
*Haokai Su,Haoxiang Luo,Shunpeng Yang,Kaiwen Jiang,Wei Zhang,Hua Chen*

Main category: cs.RO

TL;DR: This paper proposes a reward design, inspired by Linear Inverted Pendulum Model (LIPM), for improving bipedal robots' locomotion in unstructured environments through enhanced stability and adaptability.


<details>
  <summary>Details</summary>
Motivation: To address challenges in achieving stable and robust locomotion for bipedal robots in complex terrains and under external disturbances.

Method: Utilize LIPM principles to develop a reward function promoting balance and dynamic stability, introduce the Reward Fusion Module (RFM) to balance velocity and stability, and employ a double-critic architecture for training efficiency.

Result: Experiments show improved terrain adaptability, better disturbance rejection, and consistent performance of bipedal robots across various speeds and conditions in both simulated and real-world environments.

Conclusion: The proposed reward design and methods effectively enhance the stability, robustness, and adaptability of bipedal robots in unstructured outdoor environments.

Abstract: Achieving stable and robust perceptive locomotion for bipedal robots in
unstructured outdoor environments remains a critical challenge due to complex
terrain geometry and susceptibility to external disturbances. In this work, we
propose a novel reward design inspired by the Linear Inverted Pendulum Model
(LIPM) to enable perceptive and stable locomotion in the wild. The LIPM
provides theoretical guidance for dynamic balance by regulating the center of
mass (CoM) height and the torso orientation. These are key factors for
terrain-aware locomotion, as they help ensure a stable viewpoint for the
robot's camera. Building on this insight, we design a reward function that
promotes balance and dynamic stability while encouraging accurate CoM
trajectory tracking. To adaptively trade off between velocity tracking and
stability, we leverage the Reward Fusion Module (RFM) approach that prioritizes
stability when needed. A double-critic architecture is adopted to separately
evaluate stability and locomotion objectives, improving training efficiency and
robustness. We validate our approach through extensive experiments on a bipedal
robot in both simulation and real-world outdoor environments. The results
demonstrate superior terrain adaptability, disturbance rejection, and
consistent performance across a wide range of speeds and perceptual conditions.

</details>


### [214] [AEOS: Active Environment-aware Optimal Scanning Control for UAV LiDAR-Inertial Odometry in Complex Scenes](https://arxiv.org/abs/2509.09141)
*Jianping Li,Xinhang Xu,Zhongyuan Liu,Shenghai Yuan,Muqing Cao,Lihua Xie*

Main category: cs.RO

TL;DR: The paper presents AEOS, a framework for adaptive LiDAR scanning on UAVs, leveraging MPC and RL to improve performance in occluded environments.


<details>
  <summary>Details</summary>
Motivation: Traditional LiDAR systems on UAVs are limited by narrow FoV and payload constraints, leading to poor performance in complex environments.

Method: AEOS employs a hybrid architecture combining model predictive control and reinforcement learning to adapt LiDAR scanning, with a simulation environment for training.

Result: AEOS improves odometry accuracy and works in real-time settings under computational constraints in both simulated and real-world tests.

Conclusion: AEOS enhances UAV-based LiDAR perception by enabling adaptive and efficient LiDAR control, outperforming existing methods.

Abstract: LiDAR-based 3D perception and localization on unmanned aerial vehicles (UAVs)
are fundamentally limited by the narrow field of view (FoV) of compact LiDAR
sensors and the payload constraints that preclude multi-sensor configurations.
Traditional motorized scanning systems with fixed-speed rotations lack scene
awareness and task-level adaptability, leading to degraded odometry and mapping
performance in complex, occluded environments. Inspired by the active sensing
behavior of owls, we propose AEOS (Active Environment-aware Optimal Scanning),
a biologically inspired and computationally efficient framework for adaptive
LiDAR control in UAV-based LiDAR-Inertial Odometry (LIO). AEOS combines model
predictive control (MPC) and reinforcement learning (RL) in a hybrid
architecture: an analytical uncertainty model predicts future pose
observability for exploitation, while a lightweight neural network learns an
implicit cost map from panoramic depth representations to guide exploration. To
support scalable training and generalization, we develop a point cloud-based
simulation environment with real-world LiDAR maps across diverse scenes,
enabling sim-to-real transfer. Extensive experiments in both simulation and
real-world environments demonstrate that AEOS significantly improves odometry
accuracy compared to fixed-rate, optimization-only, and fully learned
baselines, while maintaining real-time performance under onboard computational
constraints. The project page can be found at
https://kafeiyin00.github.io/AEOS/.

</details>


### [215] [Occupancy-aware Trajectory Planning for Autonomous Valet Parking in Uncertain Dynamic Environments](https://arxiv.org/abs/2509.09206)
*Farhad Nawaz,Faizan M. Tariq,Sangjae Bae,David Isele,Avinash Singh,Nadia Figueroa,Nikolai Matni,Jovin D'sa*

Main category: cs.RO

TL;DR: The paper proposes a framework for predicting parking spot occupancy and planning maneuvers in dynamic and uncertain environments for autonomous valet parking, achieving improved efficiency and safety.


<details>
  <summary>Details</summary>
Motivation: To address challenges in accurately predicting parking spot occupancy and planning safe and efficient maneuvers in uncertain and dynamic parking environments.

Method: The approach includes a probabilistic estimator for spot occupancy, leveraging observations and predicted motion of dynamic agents, combined with a strategy planner that balances parking maneuvers and exploratory navigation based on information gain.

Result: Simulations demonstrate enhanced parking efficiency, safety margins, and smoother trajectories compared to existing methods.

Conclusion: The proposed framework effectively predicts spot occupancy and plans safe parking strategies in complex environments, surpassing current approaches in efficiency and safety.

Abstract: Accurately reasoning about future parking spot availability and integrated
planning is critical for enabling safe and efficient autonomous valet parking
in dynamic, uncertain environments. Unlike existing methods that rely solely on
instantaneous observations or static assumptions, we present an approach that
predicts future parking spot occupancy by explicitly distinguishing between
initially vacant and occupied spots, and by leveraging the predicted motion of
dynamic agents. We introduce a probabilistic spot occupancy estimator that
incorporates partial and noisy observations within a limited Field-of-View
(FoV) model and accounts for the evolving uncertainty of unobserved regions.
Coupled with this, we design a strategy planner that adaptively balances
goal-directed parking maneuvers with exploratory navigation based on
information gain, and intelligently incorporates wait-and-go behaviors at
promising spots. Through randomized simulations emulating large parking lots,
we demonstrate that our framework significantly improves parking efficiency,
safety margins, and trajectory smoothness compared to existing approaches.

</details>


### [216] [RENet: Fault-Tolerant Motion Control for Quadruped Robots via Redundant Estimator Networks under Visual Collapse](https://arxiv.org/abs/2509.09283)
*Yueqi Zhang,Quancheng Qian,Taixian Hou,Peng Zhai,Xiaoyi Wei,Kangmai Hu,Jiafu Yi,Lihua Zhang*

Main category: cs.RO

TL;DR: The paper proposes RENet, a robust framework for quadruped robots to overcome challenges in vision-based locomotion, particularly in outdoor environments with degraded visual perception.


<details>
  <summary>Details</summary>
Motivation: Vision-based locomotion faces challenges such as environmental prediction accuracy and noise in depth sensors, limiting outdoor applications.

Method: RENet uses a dual-estimator architecture with online adaptation for seamless transitions during vision failures.

Result: Experiments on real-world robots show effectiveness in outdoor environments, particularly with degraded visual perception.

Conclusion: RENet is a viable and practical solution for reliable robotic motion control in challenging outdoor environments.

Abstract: Vision-based locomotion in outdoor environments presents significant
challenges for quadruped robots. Accurate environmental prediction and
effective handling of depth sensor noise during real-world deployment remain
difficult, severely restricting the outdoor applications of such algorithms. To
address these deployment challenges in vision-based motion control, this letter
proposes the Redundant Estimator Network (RENet) framework. The framework
employs a dual-estimator architecture that ensures robust motion performance
while maintaining deployment stability during onboard vision failures. Through
an online estimator adaptation, our method enables seamless transitions between
estimation modules when handling visual perception uncertainties. Experimental
validation on a real-world robot demonstrates the framework's effectiveness in
complex outdoor environments, showing particular advantages in scenarios with
degraded visual perception. This framework demonstrates its potential as a
practical solution for reliable robotic deployment in challenging field
conditions. Project website: https://RENet-Loco.github.io/

</details>


### [217] [OmniEVA: Embodied Versatile Planner via Task-Adaptive 3D-Grounded and Embodiment-aware Reasoning](https://arxiv.org/abs/2509.09332)
*Yuecheng Liu,Dafeng Chi,Shiguang Wu,Zhanguang Zhang,Yuzheng Zhuang,Bowen Yang,He Zhu,Lingfeng Zhang,Pengwei Xie,David Gamaliel Arcos Bravo,Yingxue Zhang,Jianye Hao,Xingyue Quan*

Main category: cs.RO

TL;DR: OmniEVA, a novel embodied versatile planner, addresses two key limitations in multimodal large language model (MLLM) based embodied systems by introducing task-adaptive 3D grounding and embodiment-aware reasoning mechanisms.


<details>
  <summary>Details</summary>
Motivation: Current MLLM-based embodied systems struggle with geometric adaptability and embodiment constraints, limiting their feasibility and versatility in diverse spatial tasks.

Method: OmniEVA employs a gated router for selective 3D fusion and incorporates embodiment constraints into planning decisions to support adaptable and feasible task performance.

Result: Extensive experiments show OmniEVA reaches state-of-the-art performance in embodied reasoning and succeeds in executing a wide range of tasks across diverse scenarios.

Conclusion: OmniEVA demonstrates robust capabilities in multimodal understanding, reasoning, and task planning, addressing critical gaps in current embodied systems and setting new benchmarks in the field.

Abstract: Recent advances in multimodal large language models (MLLMs) have opened new
opportunities for embodied intelligence, enabling multimodal understanding,
reasoning, and interaction, as well as continuous spatial decision-making.
Nevertheless, current MLLM-based embodied systems face two critical
limitations. First, Geometric Adaptability Gap: models trained solely on 2D
inputs or with hard-coded 3D geometry injection suffer from either insufficient
spatial information or restricted 2D generalization, leading to poor
adaptability across tasks with diverse spatial demands. Second, Embodiment
Constraint Gap: prior work often neglects the physical constraints and
capacities of real robots, resulting in task plans that are theoretically valid
but practically infeasible.To address these gaps, we introduce OmniEVA -- an
embodied versatile planner that enables advanced embodied reasoning and task
planning through two pivotal innovations: (1) a Task-Adaptive 3D Grounding
mechanism, which introduces a gated router to perform explicit selective
regulation of 3D fusion based on contextual requirements, enabling
context-aware 3D grounding for diverse embodied tasks. (2) an Embodiment-Aware
Reasoning framework that jointly incorporates task goals and embodiment
constraints into the reasoning loop, resulting in planning decisions that are
both goal-directed and executable. Extensive experimental results demonstrate
that OmniEVA not only achieves state-of-the-art general embodied reasoning
performance, but also exhibits a strong ability across a wide range of
downstream scenarios. Evaluations of a suite of proposed embodied benchmarks,
including both primitive and composite tasks, confirm its robust and versatile
planning capabilities. Project page: https://omnieva.github.io

</details>


### [218] [AGILOped: Agile Open-Source Humanoid Robot for Research](https://arxiv.org/abs/2509.09364)
*Grzegorz Ficht,Luis Denninger,Sven Behnke*

Main category: cs.RO

TL;DR: AGILOped is an open-source humanoid robot designed for affordability and performance, demonstrated in walking, jumping, and impact tests.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between high-performing humanoid robots and their accessibility by offering an open-source platform.

Method: The study developed AGILOped using off-the-shelf backdrivable actuators and standard electronic components for cost and practicality, alongside demonstrating its capabilities in various experiments.

Result: AGILOped, weighing 14.5 kg and standing 110 cm, successfully performed walking, jumping, impact mitigation, and getting-up tasks.

Conclusion: AGILOped is a viable, accessible platform for research in humanoid robotics without the high costs or proprietary constraints of existing systems.

Abstract: With academic and commercial interest for humanoid robots peaking, multiple
platforms are being developed. Through a high level of customization, they
showcase impressive performance. Most of these systems remain closed-source or
have high acquisition and maintenance costs, however. In this work, we present
AGILOped - an open-source humanoid robot that closes the gap between high
performance and accessibility. Our robot is driven by off-the-shelf
backdrivable actuators with high power density and uses standard electronic
components. With a height of 110 cm and weighing only 14.5 kg, AGILOped can be
operated without a gantry by a single person. Experiments in walking, jumping,
impact mitigation and getting-up demonstrate its viability for use in research.

</details>


### [219] [VLA-Adapter: An Effective Paradigm for Tiny-Scale Vision-Language-Action Model](https://arxiv.org/abs/2509.09372)
*Yihao Wang,Pengxiang Ding,Lingxiao Li,Can Cui,Zirui Ge,Xinyang Tong,Wenxuan Song,Han Zhao,Wei Zhao,Pengxu Hou,Siteng Huang,Yifan Tang,Wenhui Wang,Ru Zhang,Jianyi Liu,Donglin Wang*

Main category: cs.RO

TL;DR: The paper introduces VLA-Adapter, a Vision-Language-Action model that uses a lightweight paradigm, dispensing with the need for large Vision-Language Models and extensive robotic data pre-training, while achieving state-of-the-art results efficiently.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the high computational and resource costs associated with training conventional Vision-Language-Action models that rely on pre-training large Vision-Language Models on extensive robotic datasets.

Method: The authors systematically analyze vision-language conditions, propose a lightweight Policy module with Bridge Attention for optimized condition injection into action spaces, and design a training paradigm that works with a 0.5B-parameter model without robotic data pre-training.

Result: The proposed VLA-Adapter achieves state-of-the-art performance on simulated and real-world robotic benchmarks. It is also capable of fast inference and enables training within 8 hours on a consumer-grade GPU.

Conclusion: VLA-Adapter significantly lowers the barrier to developing and deploying high-performance Vision-Language-Action models by using a compact model architecture, removing dependency on extensive pre-training, and ensuring efficient computation.

Abstract: Vision-Language-Action (VLA) models typically bridge the gap between
perceptual and action spaces by pre-training a large-scale Vision-Language
Model (VLM) on robotic data. While this approach greatly enhances performance,
it also incurs significant training costs. In this paper, we investigate how to
effectively bridge vision-language (VL) representations to action (A). We
introduce VLA-Adapter, a novel paradigm designed to reduce the reliance of VLA
models on large-scale VLMs and extensive pre-training. To this end, we first
systematically analyze the effectiveness of various VL conditions and present
key findings on which conditions are essential for bridging perception and
action spaces. Based on these insights, we propose a lightweight Policy module
with Bridge Attention, which autonomously injects the optimal condition into
the action space. In this way, our method achieves high performance using only
a 0.5B-parameter backbone, without any robotic data pre-training. Extensive
experiments on both simulated and real-world robotic benchmarks demonstrate
that VLA-Adapter not only achieves state-of-the-art level performance, but also
offers the fast inference speed reported to date. Furthermore, thanks to the
proposed advanced bridging paradigm, VLA-Adapter enables the training of a
powerful VLA model in just 8 hours on a single consumer-grade GPU, greatly
lowering the barrier to deploying the VLA model. Project page:
https://vla-adapter.github.io/.

</details>


### [220] [A Hybrid Hinge-Beam Continuum Robot with Passive Safety Capping for Real-Time Fatigue Awareness](https://arxiv.org/abs/2509.09404)
*Tongshun Chen,Zezhou Sun,Yanhan Sun,Yuhao Wang,Dezhen Song,Ke Wu*

Main category: cs.RO

TL;DR: This paper presents a novel fatigue-aware cable-driven continuum robot that improves durability and enables real-time fatigue estimation using three innovations: Hybrid Hinge-Beam structure, Passive Stopper, and online stiffness estimation.


<details>
  <summary>Details</summary>
Motivation: Prolonged use of cable-driven continuum robots leads to mechanical fatigue and structural degradation, limiting their effectiveness in long-term operations.

Method: The paper introduces a Hybrid Hinge-Beam structure to reduce stress, a Passive Stopper for safe constrained motion, and a real-time method for fatigue estimation from motor torque without extra sensors.

Result: Experiments demonstrate a 49% reduction in fatigue accumulation compared to conventional designs and accurate structural fatigue detection using motor-side sensing.

Conclusion: The proposed design enhances the durability, safety, and reliability of continuum robots for extended use in challenging environments.

Abstract: Cable-driven continuum robots offer high flexibility and lightweight design,
making them well-suited for tasks in constrained and unstructured environments.
However, prolonged use can induce mechanical fatigue from plastic deformation
and material degradation, compromising performance and risking structural
failure. In the state of the art, fatigue estimation of continuum robots
remains underexplored, limiting long-term operation. To address this, we
propose a fatigue-aware continuum robot with three key innovations: (1) a
Hybrid Hinge-Beam structure where TwistBeam and BendBeam decouple torsion and
bending: passive revolute joints in the BendBeam mitigate stress concentration,
while TwistBeam's limited torsional deformation reduces BendBeam stress
magnitude, enhancing durability; (2) a Passive Stopper that safely constrains
motion via mechanical constraints and employs motor torque sensing to detect
corresponding limit torque, ensuring safety and enabling data collection; and
(3) a real-time fatigue-awareness method that estimates stiffness from motor
torque at the limit pose, enabling online fatigue estimation without additional
sensors. Experiments show that the proposed design reduces fatigue accumulation
by about 49% compared with a conventional design, while passive mechanical
limiting combined with motor-side sensing allows accurate estimation of
structural fatigue and damage. These results confirm the effectiveness of the
proposed architecture for safe and reliable long-term operation.

</details>


### [221] [BagIt! An Adaptive Dual-Arm Manipulation of Fabric Bags for Object Bagging](https://arxiv.org/abs/2509.09484)
*Peng Zhou,Jiaming Qi,Hongmin Wu,Chen Wang,Yizhou Chen,Zeqing Zhang*

Main category: cs.RO

TL;DR: The study presents an adaptive system for robotic bagging tasks using dual robot arms. It dynamically adjusts using real-time visual feedback without prior knowledge of bag properties.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of robotic manipulation of deformable objects due to their unpredictable behavior, especially in industrial bagging tasks.

Method: Implements a framework involving Gaussian Mixture Models for SOI state estimation, SOI generation optimization, CBiRRT for motion planning, and Model Predictive Control for dual-arm coordination.

Result: System validated to perform precise and adaptable bagging tasks across various objects through experiments.

Conclusion: The automated system demonstrates robustness and adaptability, offering a novel approach to robotic deformable object manipulation, particularly in industrial bagging tasks.

Abstract: Bagging tasks, commonly found in industrial scenarios, are challenging
considering deformable bags' complicated and unpredictable nature. This paper
presents an automated bagging system from the proposed adaptive
Structure-of-Interest (SOI) manipulation strategy for dual robot arms. The
system dynamically adjusts its actions based on real-time visual feedback,
removing the need for pre-existing knowledge of bag properties. Our framework
incorporates Gaussian Mixture Models (GMM) for estimating SOI states,
optimization techniques for SOI generation, motion planning via Constrained
Bidirectional Rapidly-exploring Random Tree (CBiRRT), and dual-arm coordination
using Model Predictive Control (MPC). Extensive experiments validate the
capability of our system to perform precise and robust bagging across various
objects, showcasing its adaptability. This work offers a new solution for
robotic deformable object manipulation (DOM), particularly in automated bagging
tasks. Video of this work is available at https://youtu.be/6JWjCOeTGiQ.

</details>


### [222] [SMapper: A Multi-Modal Data Acquisition Platform for SLAM Benchmarking](https://arxiv.org/abs/2509.09509)
*Pedro Miguel Bastos Soares,Ali Tourani,Miguel Fernandez-Cortizas,Asier Bikandi Noya,Jose Luis Sanchez-Lopez,Holger Voos*

Main category: cs.RO

TL;DR: The paper introduces SMapper, an open-hardware, multi-sensor platform designed for SLAM research. It integrates synchronized sensing technologies and includes a reproducible, multimodal SLAM dataset named SMapper-light.


<details>
  <summary>Details</summary>
Motivation: Existing SLAM and autonomous navigation datasets often face limitations in sensing technologies, environmental diversity, and hardware reproducibility, thereby hindering reliable research progress.

Method: SMapper employs synchronized LiDAR, multi-camera, and inertial sensors, supported by precise calibration pipelines. Additionally, the authors released SMapper-light, a multimodal SLAM dataset, and provided benchmarking results.

Result: SMapper achieved tight multimodal synchronization and enabled reproducible research with its open design. The benchmarking results demonstrate its compatibility and effectiveness with existing SLAM frameworks.

Conclusion: SMapper offers an innovative open-hardware solution and dataset that significantly contribute to SLAM algorithm development and reproducibility, fostering further advancements in the field.

Abstract: Advancing research in fields like Simultaneous Localization and Mapping
(SLAM) and autonomous navigation critically depends on reliable and
reproducible multimodal datasets. While several influential datasets have
driven progress in these domains, they often suffer from limitations in sensing
modalities, environmental diversity, and the reproducibility of the underlying
hardware setups. To address these challenges, this paper introduces SMapper, a
novel open-hardware, multi-sensor platform designed explicitly for, though not
limited to, SLAM research. The device integrates synchronized LiDAR,
multi-camera, and inertial sensing, supported by a robust calibration and
synchronization pipeline that ensures precise spatio-temporal alignment across
modalities. Its open and replicable design allows researchers to extend its
capabilities and reproduce experiments across both handheld and robot-mounted
scenarios. To demonstrate its practicality, we additionally release
SMapper-light, a publicly available SLAM dataset containing representative
indoor and outdoor sequences. The dataset includes tightly synchronized
multimodal data and ground-truth trajectories derived from offline LiDAR-based
SLAM with sub-centimeter accuracy, alongside dense 3D reconstructions.
Furthermore, the paper contains benchmarking results on state-of-the-art LiDAR
and visual SLAM frameworks using the SMapper-light dataset. By combining
open-hardware design, reproducible data collection, and comprehensive
benchmarking, SMapper establishes a robust foundation for advancing SLAM
algorithm development, evaluation, and reproducibility.

</details>


### [223] [A Neuromorphic Incipient Slip Detection System using Papillae Morphology](https://arxiv.org/abs/2509.09546)
*Yanhui Lu,Zeyu Deng,Stephen J. Redmond,Efi Psomopoulou,Benjamin Ward-Cherrier*

Main category: cs.RO

TL;DR: A neuromorphic tactile sensing system using a spiking convolutional neural network achieves accurate and early slip detection.


<details>
  <summary>Details</summary>
Motivation: Detecting incipient slip is critical to prevent object slippage and enhance robotic manipulation safety, especially on energy-constrained edge platforms.

Method: The system integrates the NeuroTac sensor with papillae-based skin and a spiking convolutional neural network for slip-state classification.

Result: The system achieves 94.33% classification accuracy and detects incipient slip at least 360 ms before gross slip during trials.

Conclusion: The neuromorphic system demonstrates stable, responsive, and efficient early slip detection capability, suitable for edge platform deployment.

Abstract: Detecting incipient slip enables early intervention to prevent object
slippage and enhance robotic manipulation safety. However, deploying such
systems on edge platforms remains challenging, particularly due to energy
constraints. This work presents a neuromorphic tactile sensing system based on
the NeuroTac sensor with an extruding papillae-based skin and a spiking
convolutional neural network (SCNN) for slip-state classification. The SCNN
model achieves 94.33% classification accuracy across three classes (no slip,
incipient slip, and gross slip) in slip conditions induced by sensor motion.
Under the dynamic gravity-induced slip validation conditions, after temporal
smoothing of the SCNN's final-layer spike counts, the system detects incipient
slip at least 360 ms prior to gross slip across all trials, consistently
identifying incipient slip before gross slip occurs. These results demonstrate
that this neuromorphic system has stable and responsive incipient slip
detection capability.

</details>


### [224] [ObjectReact: Learning Object-Relative Control for Visual Navigation](https://arxiv.org/abs/2509.09594)
*Sourav Garg,Dustin Craggs,Vineeth Bhat,Lachlan Mares,Stefan Podgorski,Madhava Krishna,Feras Dayoub,Ian Reid*

Main category: cs.RO

TL;DR: This paper introduces an object-relative approach to visual navigation using a single camera and topological map, demonstrating its advantages over traditional image-relative methods, particularly in cross-embodiment deployment and spatial understanding.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the limitations of image-based visual navigation approaches, which are tightly bound to the agent's pose and embodiment, and instead leverage object-level representations for improved generalization and trajectory invariance.

Method: The authors propose "ObjectReact," a local controller trained using a relative 3D scene graph and a WayObject Costmap representation instead of RGB inputs, enabling object-relative control and global path planning.

Result: Object-relative control exhibited better generalization across sensor variations, challenging spatial tasks, and real-world scenarios compared to image-relative methods.

Conclusion: Object-relative control offers significant advantages, such as route flexibility, decoupling of image matching from control prediction, and improved cross-embodiment invariance, making it a promising method for practical indoor navigation.

Abstract: Visual navigation using only a single camera and a topological map has
recently become an appealing alternative to methods that require additional
sensors and 3D maps. This is typically achieved through an "image-relative"
approach to estimating control from a given pair of current observation and
subgoal image. However, image-level representations of the world have
limitations because images are strictly tied to the agent's pose and
embodiment. In contrast, objects, being a property of the map, offer an
embodiment- and trajectory-invariant world representation. In this work, we
present a new paradigm of learning "object-relative" control that exhibits
several desirable characteristics: a) new routes can be traversed without
strictly requiring to imitate prior experience, b) the control prediction
problem can be decoupled from solving the image matching problem, and c) high
invariance can be achieved in cross-embodiment deployment for variations across
both training-testing and mapping-execution settings. We propose a topometric
map representation in the form of a "relative" 3D scene graph, which is used to
obtain more informative object-level global path planning costs. We train a
local controller, dubbed "ObjectReact", conditioned directly on a high-level
"WayObject Costmap" representation that eliminates the need for an explicit RGB
input. We demonstrate the advantages of learning object-relative control over
its image-relative counterpart across sensor height variations and multiple
navigation tasks that challenge the underlying spatial understanding
capability, e.g., navigating a map trajectory in the reverse direction. We
further show that our sim-only policy is able to generalize well to real-world
indoor environments. Code and supplementary material are accessible via project
page: https://object-react.github.io/

</details>


### [225] [MOFU: Development of a MOrphing Fluffy Unit with Expansion and Contraction Capabilities and Evaluation of the Animacy of Its Movements](https://arxiv.org/abs/2509.09613)
*Taisei Mogi,Mari Saito,Yoshihiro Nakata*

Main category: cs.RO

TL;DR: The paper introduces "MOFU," a mobile robot capable of whole-body expansion-contraction, and examines how this capability enhances animacy perception in humans.


<details>
  <summary>Details</summary>
Motivation: The paper aims to improve the perception of animacy in therapy and social robots by investigating the impact of whole-body expansion-contraction movements, which are neglected in current designs.

Method: The researchers developed MOFU, a mobile robot using a jitterbug structure for volume changes, and conducted an online survey where participants rated animacy using the Godspeed Questionnaire Series.

Result: The expansion-contraction movement significantly enhanced perceived animacy compared to stationary or locomotion-only movements, but introducing two robots did not significantly increase animacy.

Conclusion: Volume-changing movements like expansion-contraction are critical in enhancing the perception of animacy in robots and should be integrated into future robot designs.

Abstract: Robots for therapy and social interaction are often intended to evoke
"animacy" in humans. While many robots imitate appearance and joint movements,
little attention has been given to whole-body expansion-contraction,
volume-changing movements observed in living organisms, and their effect on
animacy perception. We developed a mobile robot called "MOFU (Morphing Fluffy
Unit)," capable of whole-body expansion-contraction with a single motor and
covered with a fluffy exterior. MOFU employs a "Jitterbug" structure, a
geometric transformation mechanism that enables smooth volume change in
diameter from 210 to 280 mm using one actuator. It is also equipped with a
differential two-wheel drive mechanism for locomotion. To evaluate the effect
of expansion-contraction movements, we conducted an online survey using videos
of MOFU's behavior. Participants rated impressions with the Godspeed
Questionnaire Series. First, we compared videos of MOFU in a stationary state
with and without expansion-contraction and turning, finding that
expansion-contraction significantly increased perceived animacy. Second, we
hypothesized that presenting two MOFUs would increase animacy compared with a
single robot; however, this was not supported, as no significant difference
emerged. Exploratory analyses further compared four dual-robot motion
conditions. Third, when expansion-contraction was combined with locomotion,
animacy ratings were higher than locomotion alone. These results suggest that
volume-changing movements such as expansion and contraction enhance perceived
animacy in robots and should be considered an important design element in
future robot development aimed at shaping human impressions.

</details>


### [226] [Dexplore: Scalable Neural Control for Dexterous Manipulation from Reference-Scoped Exploration](https://arxiv.org/abs/2509.09671)
*Sirui Xu,Yu-Wei Chao,Liuyu Bian,Arsalan Mousavian,Yu-Xiong Wang,Liang-Yan Gui,Wei Yang*

Main category: cs.RO

TL;DR: Dexplore is a unified framework that learns robotic manipulation policies directly from human motion-capture data by jointly optimizing retargeting and tracking while mitigating inaccuracies in the demonstrations.


<details>
  <summary>Details</summary>
Motivation: There is a need to scale robotic dexterous manipulation using motion-capture demonstrations, despite inaccuracies and embodiment mismatches between human and robotic hands in the existing data.

Method: Dexplore employs a unified single-loop optimization process that combines retargeting and tracking into one reinforcement learning framework. Demonstrations are treated as soft guidance using adaptive spatial scopes to ensure policy in-scope behavior, minimize control effort, and achieve tasks.

Result: The method preserves demonstration intent, allows for the development of robot-specific strategies, improves noise robustness, and scales across large demonstration datasets. Additionally, the scaled policy is distilled into a vision-based generative controller for generalization.

Conclusion: Dexplore effectively transforms imperfect motion-capture demonstrations into useful training signals for diverse and robust dexterous robotic manipulation skills, enabling real-world deployment of these capabilities.

Abstract: Hand-object motion-capture (MoCap) repositories offer large-scale,
contact-rich demonstrations and hold promise for scaling dexterous robotic
manipulation. Yet demonstration inaccuracies and embodiment gaps between human
and robot hands limit the straightforward use of these data. Existing methods
adopt a three-stage workflow, including retargeting, tracking, and residual
correction, which often leaves demonstrations underused and compound errors
across stages. We introduce Dexplore, a unified single-loop optimization that
jointly performs retargeting and tracking to learn robot control policies
directly from MoCap at scale. Rather than treating demonstrations as ground
truth, we use them as soft guidance. From raw trajectories, we derive adaptive
spatial scopes, and train with reinforcement learning to keep the policy
in-scope while minimizing control effort and accomplishing the task. This
unified formulation preserves demonstration intent, enables robot-specific
strategies to emerge, improves robustness to noise, and scales to large
demonstration corpora. We distill the scaled tracking policy into a
vision-based, skill-conditioned generative controller that encodes diverse
manipulation skills in a rich latent representation, supporting generalization
across objects and real-world deployment. Taken together, these contributions
position Dexplore as a principled bridge that transforms imperfect
demonstrations into effective training signals for dexterous manipulation.

</details>


### [227] [SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning](https://arxiv.org/abs/2509.09674)
*Haozhan Li,Yuxin Zuo,Jiale Yu,Yuhao Zhang,Zhaohui Yang,Kaiyan Zhang,Xuekai Zhu,Yuchen Zhang,Tianxing Chen,Ganqu Cui,Dehui Wang,Dingxiang Luo,Yuchen Fan,Youbang Sun,Jia Zeng,Jiangmiao Pang,Shanghang Zhang,Yu Wang,Yao Mu,Bowen Zhou,Ning Ding*

Main category: cs.RO

TL;DR: SimpleVLA-RL introduces a reinforcement learning framework tailored for Vision-Language-Action models, enhancing performance and reducing reliance on large-scale human-operated robotic trajectories.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address challenges faced by Vision-Language-Action models, including dependency on expensive training data and limited generalization abilities, by exploring reinforcement learning's potential in improving long-horizon step-by-step planning.

Method: The authors present the SimpleVLA-RL framework, which features VLA-specific trajectory sampling, scalable parallelization, multi-environment rendering, and optimized loss computation. The framework is tested on OpenVLA-OFT.

Result: SimpleVLA-RL achieves state-of-the-art performance on LIBERO, outperforms $\pi_0$ on RoboTwin 1.0 & 2.0, and reduces dependence on large-scale data, enabling robust generalization.

Conclusion: The framework demonstrates that reinforcement learning can significantly enhance VLA model performance, surpass supervised fine-tuning in real-world tasks, and discovers novel patterns during training.

Abstract: Vision-Language-Action (VLA) models have recently emerged as a powerful
paradigm for robotic manipulation. Despite substantial progress enabled by
large-scale pretraining and supervised fine-tuning (SFT), these models face two
fundamental challenges: (i) the scarcity and high cost of large-scale
human-operated robotic trajectories required for SFT scaling, and (ii) limited
generalization to tasks involving distribution shift. Recent breakthroughs in
Large Reasoning Models (LRMs) demonstrate that reinforcement learning (RL) can
dramatically enhance step-by-step reasoning capabilities, raising a natural
question: Can RL similarly improve the long-horizon step-by-step action
planning of VLA? In this work, we introduce SimpleVLA-RL, an efficient RL
framework tailored for VLA models. Building upon veRL, we introduce
VLA-specific trajectory sampling, scalable parallelization, multi-environment
rendering, and optimized loss computation. When applied to OpenVLA-OFT,
SimpleVLA-RL achieves SoTA performance on LIBERO and even outperforms $\pi_0$
on RoboTwin 1.0\&2.0 with the exploration-enhancing strategies we introduce.
SimpleVLA-RL not only reduces dependence on large-scale data and enables robust
generalization, but also remarkably surpasses SFT in real-world tasks.
Moreover, we identify a novel phenomenon ``pushcut'' during RL training,
wherein the policy discovers previously unseen patterns beyond those seen in
the previous training process. Github: https://github.com/PRIME-RL/SimpleVLA-RL

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [228] [Pattern-Based File and Data Access with Python Glob: A Comprehensive Guide for Computational Research](https://arxiv.org/abs/2509.08843)
*Sidney Shapiro*

Main category: cs.SE

TL;DR: The paper highlights the Python glob module as a versatile tool for scalable file access and integration in data science, AI, and analytics workflows.


<details>
  <summary>Details</summary>
Motivation: To address the often under-documented yet essential role of pattern-based file access in computational research and to promote reproducible workflows.

Method: Demonstrates the use of the glob module through Python-based examples integrated with libraries like pandas, scikit-learn, and matplotlib.

Result: Illustrates how glob enhances large-scale data ingestion, organizational data analysis, AI dataset construction, and reproducible research practices.

Conclusion: Glob serves as a foundational tool for efficient file access and should be a default reference for Python-based research and analytical workflows.

Abstract: Pattern-based file access is a fundamental but often under-documented aspect
of computational research. The Python glob module provides a simple yet
powerful way to search, filter, and ingest files using wildcard patterns,
enabling scalable workflows across disciplines. This paper introduces glob as a
versatile tool for data science, business analytics, and artificial
intelligence applications. We demonstrate use cases including large-scale data
ingestion, organizational data analysis, AI dataset construction, and
reproducible research practices. Through concrete Python examples with widely
used libraries such as pandas,scikit-learn, and matplotlib, we show how glob
facilitates efficient file traversal and integration with analytical pipelines.
By situating glob within the broader context of reproducible research and data
engineering, we highlight its role as a methodological building block. Our goal
is to provide researchers and practitioners with a concise reference that
bridges foundational concepts and applied practice, making glob a default
citation for file pattern matching in Python-based research workflows.

</details>


### [229] [A Systematic Mapping Study on Chatbots in Programming Education](https://arxiv.org/abs/2509.08857)
*Marcelino Garcia,Renato Garcia,Arthur Parizotto,Andre Mendes,Pedro Valle,Ricardo Vilela,Renato Balancieri,Williamson Silva*

Main category: cs.SE

TL;DR: The paper reviews 54 studies on educational chatbots in programming, finding trends in Python-focused chatbots and diverse pedagogical approaches.


<details>
  <summary>Details</summary>
Motivation: To understand how educational chatbots are developed and applied for teaching programming, especially in introductory contexts.

Method: Conducted a Systematic Mapping Study (SMS), analyzing 54 selected studies from 3,216 publications based on specific subquestions about chatbot types, used programming languages, educational content, interaction models, and application contexts.

Result: Finds Python-centric chatbots dominate, emphasizing fundamental programming concepts and employing diverse pedagogical and technological approaches.

Conclusion: The study identifies trends and gaps in the literature, offering valuable insights for designing better educational programming tools.

Abstract: Educational chatbots have gained prominence as support tools for teaching
programming, particularly in introductory learning contexts. This paper
presents a Systematic Mapping Study (SMS) that investigated how such agents
have been developed and applied in programming education. From an initial set
of 3,216 publications, 54 studies were selected and analyzed based on five
research subquestions, addressing chatbot types, programming languages used,
educational content covered, interaction models, and application contexts. The
results reveal a predominance of chatbots designed for Python instruction,
focusing on fundamental programming concepts, and employing a wide variety of
pedagogical approaches and technological architectures. In addition to
identifying trends and gaps in the literature, this study provides insights to
inform the development of new educational tools for programming instruction.

</details>


### [230] [GeoJSON Agents:A Multi-Agent LLM Architecture for Geospatial Analysis-Function Calling vs Code Generation](https://arxiv.org/abs/2509.08863)
*Qianqian Luo,Liuchang Xu,Qingming Lin,Sensen Wu,Ruichen Mao,Chao Wang,Hailin Feng,Bo Huang,Zhenhong Du*

Main category: cs.SE

TL;DR: The paper proposes a multi-agent LLM architecture called GeoJSON Agents, leveraging Function Calling and Code Generation techniques to automate GIS tasks. It significantly improves task accuracy and scalability compared to general-purpose models.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with GIS-related tasks due to the complexity of spatial data processing and lack of specialized expertise. Addressing these limitations could enhance automation capability in GIS.

Method: The architecture involves three stages: task parsing, collaboration among Worker agents, and integration of outputs into reusable GeoJSON files. It utilizes Function Calling and Python Code Generation to process spatial data efficiently.

Result: Experiments show that the Code Generation approach achieves a higher accuracy (97.14%) compared to Function Calling (85.71%), both outperforming general-purpose models (48.57%). Code Generation offers flexibility, while Function Calling ensures stable execution.

Conclusion: This study introduces a novel multi-agent LLM framework tailored for GeoJSON. It demonstrates superior performance in GIS automation and provides insights into balancing flexibility and stability in GeoAI systems.

Abstract: LLMs have made substantial progress in task automation and natural language
understanding.However,without expertise in GIS,they continue to encounter
limitations.To address these issues, we propose GeoJSON Agents-a multi-agent
LLM architecture.This framework transforms natural language tasks into
structured GeoJSON operation commands and processes spatial data using two
widely adopted LLM enhancement techniques:Function Calling and Code
Generation.The architecture consists of three components-task parsing,agent
collaboration,and result integration-aimed at enhancing both the performance
and scalability of GIS automation.The Planner agent interprets natural language
tasks into structured GeoJSON commands.Then,specialized Worker agents
collaborate according to assigned roles to perform spatial data processing and
analysis,either by invoking predefined function APIs or by dynamically
generating and executing Python-based spatial analysis code.Finally,the system
integrates the outputs from multiple execution rounds into
reusable,standards-compliant GeoJSON files.To systematically evaluate the
performance of the two approaches,we constructed a benchmark dataset of 70
tasks with varying complexity and conducted experiments using OpenAI's GPT-4o
as the core model.Results indicate that the Function Calling-based GeoJSON
Agent achieved an accuracy of 85.71%,while the Code Generation-based agent
reached 97.14%,both significantly outperforming the best-performing
general-purpose model (48.57%).Further analysis reveals that the Code
Generation provides greater flexibility,whereas the Function Calling approach
offers more stable execution.This study is the first to introduce an LLM
multi-agent framework for GeoJSON data and to compare the strengths and
limitations of two mainstream LLM enhancement methods,offering new perspectives
for improving GeoAI system performance.

</details>


### [231] [TraceRAG: A LLM-Based Framework for Explainable Android Malware Detection and Behavior Analysis](https://arxiv.org/abs/2509.08865)
*Guangyu Zhang,Xixuan Wang,Shiyu Sun,Peiyan Xiao,Kun Sun,Yanhai Xiong*

Main category: cs.SE

TL;DR: The paper introduces TraceRAG, a framework combining retrieval-augmented generation and Java code analysis for explainable malware detection, achieving high detection and behavior identification accuracy.


<details>
  <summary>Details</summary>
Motivation: Traditional malware analysis often fails to uncover deeply hidden malicious behaviors and to provide understandable explanations for decisions. This gap is critical as attackers use sophisticated evasion tactics.

Method: TraceRAG is developed as a retrieval-augmented generation framework that generates summaries for Java code snippets, indexes them in a vector database, retrieves relevant snippets based on queries, and creates human-readable reports explaining malicious behaviors.

Result: TraceRAG achieved 96% malware detection accuracy and 83.81% behavior identification accuracy, validated through VirusTotal scans and manual checks. Experts affirmed the practicality of the generated reports.

Conclusion: TraceRAG successfully bridges the gap between natural language queries and Java code for malware analysis, enhancing both detection accuracy and interpretability, and demonstrates practical utility through expert evaluation.

Abstract: Sophisticated evasion tactics in malicious Android applications, combined
with their intricate behavioral semantics, enable attackers to conceal
malicious logic within legitimate functions, underscoring the critical need for
robust and in-depth analysis frameworks. However, traditional analysis
techniques often fail to recover deeply hidden behaviors or provide
human-readable justifications for their decisions. Inspired by advances in
large language models (LLMs), we introduce TraceRAG, a retrieval-augmented
generation (RAG) framework that bridges natural language queries and Java code
to deliver explainable malware detection and analysis. First, TraceRAG
generates summaries of method-level code snippets, which are indexed in a
vector database. At query time, behavior-focused questions retrieve the most
semantically relevant snippets for deeper inspection. Finally, based on the
multi-turn analysis results, TraceRAG produces human-readable reports that
present the identified malicious behaviors and their corresponding code
implementations. Experimental results demonstrate that our method achieves 96\%
malware detection accuracy and 83.81\% behavior identification accuracy based
on updated VirusTotal (VT) scans and manual verification. Furthermore, expert
evaluation confirms the practical utility of the reports generated by TraceRAG.

</details>


### [232] [Benchmarking Energy Efficiency of Large Language Models Using vLLM](https://arxiv.org/abs/2509.08867)
*K. Pronk,Q. Zhao*

Main category: cs.SE

TL;DR: The paper introduces the LLM Efficiency Benchmark to evaluate energy efficiency of Large Language Models (LLMs) under realistic production conditions.


<details>
  <summary>Details</summary>
Motivation: Developers lack sufficient information on the energy efficiency of LLMs in real-world scenarios, despite their impact on climate due to energy demands.

Method: The study introduced LLM Efficiency Benchmark using vLLM to simulate real-world high-throughput scenarios and examined factors like model size, architecture, and request volume.

Result: The evaluation highlighted how energy efficiency benchmarks can reflect practical deployment conditions and optimize sustainability in AI systems.

Conclusion: Realistic benchmarks provide critical insights for improving the energy efficiency and sustainability of LLMs in practical applications.

Abstract: The prevalence of Large Language Models (LLMs) is having an growing impact on
the climate due to the substantial energy required for their deployment and
use. To create awareness for developers who are implementing LLMs in their
products, there is a strong need to collect more information about the energy
efficiency of LLMs. While existing research has evaluated the energy efficiency
of various models, these benchmarks often fall short of representing realistic
production scenarios. In this paper, we introduce the LLM Efficiency Benchmark,
designed to simulate real-world usage conditions. Our benchmark utilizes vLLM,
a high-throughput, production-ready LLM serving backend that optimizes model
performance and efficiency. We examine how factors such as model size,
architecture, and concurrent request volume affect inference energy efficiency.
Our findings demonstrate that it is possible to create energy efficiency
benchmarks that better reflect practical deployment conditions, providing
valuable insights for developers aiming to build more sustainable AI systems.

</details>


### [233] [CLARA: A Developer's Companion for Code Comprehension and Analysis](https://arxiv.org/abs/2509.09072)
*Ahmed Adnan,Mushfiqur Rahman,Saad Sakib Noor,Kazi Sakib*

Main category: cs.SE

TL;DR: CLARA is a browser extension designed to assist developers and researchers with code comprehension, refactoring, and quality detection using state-of-the-art inference models.


<details>
  <summary>Details</summary>
Motivation: Developers and researchers often struggle with understanding open-source codebases due to limitations in current tools that lack context-awareness and require significant manual effort.

Method: CLARA uses a browser extension based on state-of-the-art inference models to assist with tasks such as code comprehension, refactoring, and quality attribute detection. Its performance was evaluated both qualitatively using datasets and through a user study involving 10 participants.

Result: The user study and qualitative evaluation showed that CLARA is accurate, useful, and practical for assisting with code comprehension and analysis.

Conclusion: CLARA offers an effective, open-source solution for improving code analysis tasks and is supported by performance evaluations and user feedback, making it a valuable tool for developers and researchers.

Abstract: Code comprehension and analysis of open-source project codebases is a task
frequently performed by developers and researchers. However, existing tools
that practitioners use for assistance with such tasks often require prior
project setup, lack context-awareness, and involve significant manual effort.
To address this, we present CLARA, a browser extension that utilizes a
state-of-the-art inference model to assist developers and researchers in: (i)
comprehending code files and code fragments, (ii) code refactoring, and (iii)
code quality attribute detection. We qualitatively evaluated CLARA's inference
model using existing datasets and methodology, and performed a comprehensive
user study with 10 developers and academic researchers to assess its usability
and usefulness. The results show that CLARA is useful, accurate, and practical
in code comprehension and analysis tasks. CLARA is an open-source tool
available at https://github.com/SaadNoor555/CLARA_tool_demo. A video showing
the full capabilities of CLARA can be found at
https://youtu.be/VDKVXvIH41Q?si=qBFsmS_Y4m_9x3YH.

</details>


### [234] [Probing Pre-trained Language Models on Code Changes: Insights from ReDef, a High-Confidence Just-in-Time Defect Prediction Dataset](https://arxiv.org/abs/2509.09192)
*Doha Nam,Taehyoun Kim,Duksan Ryu,Jongmoon Baik*

Main category: cs.SE

TL;DR: The paper introduces a high-confidence dataset, ReDef, for software defect prediction using function-level modifications. It evaluates pre-trained language models (PLMs) on code modification understanding, revealing reliance on superficial cues rather than deep semantic comprehension.


<details>
  <summary>Details</summary>
Motivation: To enhance precision and reliability in identifying defect-inducing code changes, as existing datasets suffer from noisy labels and inefficiency.

Method: The authors created ReDef, using revert commits and post-hoc checks for defective and clean cases. Ambiguous cases were filtered with GPT-assisted triage. PLMs were then fine-tuned under various encoding strategies and tested with counterfactual perturbations.

Result: The ReDef dataset contains 3,164 defective and 10,268 clean modifications. Compact diff-style encodings performed better than whole-function formats across PLMs. However, counterfactual tests revealed models rely on superficial cues rather than understanding semantics.

Conclusion: Current PLMs are limited in comprehending code modifications and rely on surface-level features, highlighting the need for improved methods in defect prediction tasks.

Abstract: Just-in-Time software defect prediction (JIT-SDP) plays a critical role in
prioritizing risky code changes during code review and continuous integration.
However, existing datasets often suffer from noisy labels and low precision in
identifying bug-inducing commits. To address this, we present ReDef
(Revert-based Defect dataset), a high-confidence benchmark of function-level
modifications curated from 22 large-scale C/C++ projects. Defective cases are
anchored by revert commits, while clean cases are validated through post-hoc
history checks. Ambiguous instances are conservatively filtered out via a
GPT-assisted triage process involving multiple votes and audits. This pipeline
yields 3,164 defective and 10,268 clean modifications, offering substantially
more reliable labels than prior existing resources. Beyond dataset
construction, we provide the first systematic evaluation of how pre-trained
language models (PLMs) reason about code modifications -- specifically, which
input encodings most effectively expose change information, and whether models
genuinely capture edit semantics. We fine-tune CodeBERT, CodeT5+, and UniXcoder
under five encoding strategies, and further probe their sensitivity through
counterfactual perturbations that swap added/deleted blocks, invert diff
polarity, or inject spurious markers. Our results show that compact diff-style
encodings consistently outperform whole-function formats across all PLMs, with
statistical tests confirming large, model-independent effects. However, under
counterfactual tests, performance degrades little or not at all -- revealing
that what appears to be robustness in fact reflects reliance on superficial
cues rather than true semantic understanding. These findings indicate that,
unlike in snapshot-based tasks, current PLMs remain limited in their ability to
genuinely comprehend code modifications.

</details>


### [235] [On Integrating Large Language Models and Scenario-Based Programming for Improving Software Reliability](https://arxiv.org/abs/2509.09194)
*Ayelet Berzack,Guy Katz*

Main category: cs.SE

TL;DR: LLMs are valuable in software development, but often introduce errors. This study integrates LLMs with Scenario-Based Programming (SBP) to reduce errors and boost reliability in building Connect4, achieving competitive agent functionality.


<details>
  <summary>Details</summary>
Motivation: To reduce errors and improve reliability when LLMs assist in software development, by combining them with structured software engineering practices.

Method: A methodology combining LLMs and Scenario-Based Programming (SBP), which allows developers to input expertise and verify LLM-generated outputs.

Result: Successfully implemented and evaluated the Connect4 game with a capable agent that outperformed peers and was partially formally verified.

Conclusion: Combining LLMs with SBP improves development reliability and offers insights into ease of use, paving a path to better integration of LLMs in programming.

Abstract: Large Language Models (LLMs) are fast becoming indispensable tools for
software developers, assisting or even partnering with them in crafting complex
programs. The advantages are evident -- LLMs can significantly reduce
development time, generate well-organized and comprehensible code, and
occasionally suggest innovative ideas that developers might not conceive on
their own. However, despite their strengths, LLMs will often introduce
significant errors and present incorrect code with persuasive confidence,
potentially misleading developers into accepting flawed solutions.
  In order to bring LLMs into the software development cycle in a more reliable
manner, we propose a methodology for combining them with ``traditional''
software engineering techniques in a structured way, with the goal of
streamlining the development process, reducing errors, and enabling users to
verify crucial program properties with increased confidence. Specifically, we
focus on the Scenario-Based Programming (SBP) paradigm -- an event-driven,
scenario-based approach for software engineering -- to allow human developers
to pour their expert knowledge into the LLM, as well as to inspect and verify
its outputs.
  To evaluate our methodology, we conducted a significant case study, and used
it to design and implement the Connect4 game. By combining LLMs and SBP we were
able to create a highly-capable agent, which could defeat various strong
existing agents. Further, in some cases, we were able to formally verify the
correctness of our agent. Finally, our experience reveals interesting insights
regarding the ease-of-use of our proposed approach. The full code of our
case-study will be made publicly available with the final version of this
paper.

</details>


### [236] [Altered Histories in Version Control System Repositories: Evidence from the Trenches](https://arxiv.org/abs/2509.09294)
*Solal Rapaport,Laurent Pautet,Samuel Tardieu,Stefano Zacchiroli*

Main category: cs.SE

TL;DR: The paper explores history alterations in Git repositories, identifying 1.22 million repositories with altered histories and discusses the implications on security and governance, introducing GitHistorian to address these issues.


<details>
  <summary>Details</summary>
Motivation: To investigate and address the implications of history rewriting in public Git repositories, particularly focusing on security and governance challenges posed by these alterations.

Method: Analyzed 111 million repositories archived by Software Heritage, categorized alterations based on location and nature, and conducted two case studies on common alteration practices.

Result: Found 1.22 million repositories containing history alterations, with 8.7 million rewritten histories; identified patterns like retroactive license changes and removal of secrets.

Conclusion: History rewriting in Git repositories poses significant governance and security risks, and GitHistorian can help identify and understand these alterations.

Abstract: Version Control Systems (VCS) like Git allow developers to locally rewrite
recorded history, e.g., to reorder and suppress commits or specific data in
them. These alterations have legitimate use cases, but become problematic when
performed on public branches that have downstream users: they break push/pull
workflows, challenge the integrity and reproducibility of repositories, and
create opportunities for supply chain attackers to sneak into them nefarious
changes. We conduct the first large-scale investigation of Git history
alterations in public code repositories. We analyze 111 M (millions)
repositories archived by Software Heritage, which preserves VCS histories even
across alterations. We find history alterations in 1.22 M repositories, for a
total of 8.7 M rewritten histories. We categorize changes by where they happen
(which repositories, which branches) and what is changed in them (files or
commit metadata). Conducting two targeted case studies we show that altered
histories recurrently change licenses retroactively, or are used to remove
''secrets'' (e.g., private keys) committed by mistake. As these behaviors
correspond to bad practices-in terms of project governance or security
management, respectively-that software recipients might want to avoid, we
introduce GitHistorian, an automated tool, that developers can use to spot and
describe history alterations in public Git repositories.

</details>


### [237] [Cross-Domain Evaluation of Transformer-Based Vulnerability Detection on Open & Industry Data](https://arxiv.org/abs/2509.09313)
*Moritz Mock,Thomas Forrer,Barbara Russo*

Main category: cs.SE

TL;DR: The paper explores the performance of CodeBERT for vulnerability detection in both industrial and open-source software and introduces AI-DO, a CI/CD-integrated system to assist developers in identifying vulnerabilities.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenges of transferring deep learning-based vulnerability detection methods from academia to industry while evaluating their effectiveness in real-world settings.

Method: Evaluating CodeBERT for vulnerability detection through cross-domain generalization experiments, studying the effects of training on imbalanced data, and integrating the model into workflows using the AI-DO recommender system.

Result: CodeBERT models trained on industrial data perform well for detecting vulnerabilities within the same domain but struggle with open-source data, whereas fine-tuning on open-source data yields better performance when combined with undersampling techniques.

Conclusion: Using domain-specific training data significantly impacts vulnerability detection accuracy, and AI-DO facilitates seamless integration into developer workflows, showcasing practical value through developer surveys.

Abstract: Deep learning solutions for vulnerability detection proposed in academic
research are not always accessible to developers, and their applicability in
industrial settings is rarely addressed. Transferring such technologies from
academia to industry presents challenges related to trustworthiness, legacy
systems, limited digital literacy, and the gap between academic and industrial
expertise. For deep learning in particular, performance and integration into
existing workflows are additional concerns. In this work, we first evaluate the
performance of CodeBERT for detecting vulnerable functions in industrial and
open-source software. We analyse its cross-domain generalisation when
fine-tuned on open-source data and tested on industrial data, and vice versa,
also exploring strategies for handling class imbalance. Based on these results,
we develop AI-DO(Automating vulnerability detection Integration for Developers'
Operations), a Continuous Integration-Continuous Deployment (CI/CD)-integrated
recommender system that uses fine-tuned CodeBERT to detect and localise
vulnerabilities during code review without disrupting workflows. Finally, we
assess the tool's perceived usefulness through a survey with the company's IT
professionals. Our results show that models trained on industrial data detect
vulnerabilities accurately within the same domain but lose performance on
open-source code, while a deep learner fine-tuned on open data, with
appropriate undersampling techniques, improves the detection of
vulnerabilities.

</details>


### [238] [ORCA: Unveiling Obscure Containers In The Wild](https://arxiv.org/abs/2509.09322)
*Jacopo Bufalino,Agathe Blaise,Stefano Secci*

Main category: cs.SE

TL;DR: The paper presents ORCA, an open-source tool for accurate analysis of obscured container images, improving file coverage by 40% over existing tools.


<details>
  <summary>Details</summary>
Motivation: The reliance on open-source libraries and containerized environments increases security risks due to outdated or vulnerable components, which existing SCA tools fail to adequately address in obscure container images.

Method: The authors analyzed 600 containers to identify limitations in current SCA tools and developed ORCA, a methodology and implementation resilient to container obscuration.

Result: The research showed that many tools fail with obscure containers, while ORCA achieved a 40% median improvement in file coverage over top SCA tools.

Conclusion: The proposed ORCA methodology and tool effectively mitigate the limitations of current SCA tools in handling obscure container images, enhancing container analysis reliability.

Abstract: Modern software development increasingly depends on open-source libraries and
third-party components, which are often encapsulated into containerized
environments. While improving the development and deployment of applications,
this approach introduces security risks, particularly when outdated or
vulnerable components are inadvertently included in production environments.
Software Composition Analysis (SCA) is a critical process that helps identify
and manage packages and dependencies inside a container. However, unintentional
modifications to the container filesystem can lead to incomplete container
images, which compromise the reliability of SCA tools. In this paper, we
examine the limitations of both cloud-based and open-source SCA tools when
faced with such obscure images. An analysis of 600 popular containers revealed
that obscure containers exist in well-known registries and trusted images and
that many tools fail to analyze such containers. To mitigate these issues, we
propose an obscuration-resilient methodology for container analysis and
introduce ORCA (Obscuration-Resilient Container Analyzer), its open-source
implementation. We reported our findings to all vendors using their appropriate
channels. Our results demonstrate that ORCA effectively detects the content of
obscure containers and achieves a median 40% improvement in file coverage
compared to Docker Scout and Syft.

</details>


### [239] [LoCoBench: A Benchmark for Long-Context Large Language Models in Complex Software Engineering](https://arxiv.org/abs/2509.09614)
*Jielin Qiu,Zuxin Liu,Zhiwei Liu,Rithesh Murthy,Jianguo Zhang,Haolin Chen,Shiyu Wang,Ming Zhu,Liangwei Yang,Juntao Tan,Zhepeng Cen,Cheng Qian,Shelby Heinecke,Weiran Yao,Silvio Savarese,Caiming Xiong,Huan Wang*

Main category: cs.SE

TL;DR: LoCoBench is a benchmark to evaluate long-context LLMs in realistic software development scenarios, focusing on understanding entire large codebases.


<details>
  <summary>Details</summary>
Motivation: Current code evaluation benchmarks do not assess long-context capabilities, crucial for handling entire software systems.

Method: The authors designed 8,000 scenarios across 10 languages with contexts spanning up to 1M tokens, covering 8 task categories and introduced 17 metrics.

Result: State-of-the-art models exhibit substantial gaps in long-context code understanding, as tested through LoCoBench.

Conclusion: Advanced long-context understanding for software development remains unresolved, prompting the need for focused research in this domain.

Abstract: The emergence of long-context language models with context windows extending
to millions of tokens has created new opportunities for sophisticated code
understanding and software development evaluation. We propose LoCoBench, a
comprehensive benchmark specifically designed to evaluate long-context LLMs in
realistic, complex software development scenarios. Unlike existing code
evaluation benchmarks that focus on single-function completion or short-context
tasks, LoCoBench addresses the critical evaluation gap for long-context
capabilities that require understanding entire codebases, reasoning across
multiple files, and maintaining architectural consistency across large-scale
software systems. Our benchmark provides 8,000 evaluation scenarios
systematically generated across 10 programming languages, with context lengths
spanning 10K to 1M tokens, a 100x variation that enables precise assessment of
long-context performance degradation in realistic software development
settings. LoCoBench introduces 8 task categories that capture essential
long-context capabilities: architectural understanding, cross-file refactoring,
multi-session development, bug investigation, feature implementation, code
comprehension, integration testing, and security analysis. Through a 5-phase
pipeline, we create diverse, high-quality scenarios that challenge LLMs to
reason about complex codebases at unprecedented scale. We introduce a
comprehensive evaluation framework with 17 metrics across 4 dimensions,
including 8 new evaluation metrics, combined in a LoCoBench Score (LCBS). Our
evaluation of state-of-the-art long-context models reveals substantial
performance gaps, demonstrating that long-context understanding in complex
software development represents a significant unsolved challenge that demands
more attention. LoCoBench is released at:
https://github.com/SalesforceAIResearch/LoCoBench.

</details>


### [240] [I Know Who Clones Your Code: Interpretable Smart Contract Similarity Detection](https://arxiv.org/abs/2509.09630)
*Zhenguang Liu,Lixun Ma,Zhongzheng Mu,Chengkun Wei,Xiaojun Xu,Yingying Jiao,Kui Ren*

Main category: cs.SE

TL;DR: This paper presents SmartDetector, a novel methodology to compute function similarity in smart contracts with improved accuracy and interpretability, enhancing semantic comparisons and explaining results at a granular level.


<details>
  <summary>Details</summary>
Motivation: Current open-source code reuse in smart contracts exacerbates bug proliferation, while existing methods for detecting functional similarities are limited, with shortcomings in handling complex structures and ensuring interpretability.

Method: The paper proposes SmartDetector, which decomposes the ASTs of smart contract functions into smaller statement trees for fine-grained comparisons. A classifier evaluates similarities by comparing statement tree pairs, with optimal hyperparameters found through a derived cosine-wise diffusion process.

Result: Extensive experiments on three large real-world datasets reveal that SmartDetector outperforms state-of-the-art methods with a 14.01% average improvement in F1-score, achieving a high average F1-score of 95.88%.

Conclusion: SmartDetector proves to be a more effective and explainable tool for detecting similarities in smart contract functions, addressing limitations of previous methods and aiding in bug prevention through detailed semantic analysis.

Abstract: Widespread reuse of open-source code in smart contract development boosts
programming efficiency but significantly amplifies bug propagation across
contracts, while dedicated methods for detecting similar smart contract
functions remain very limited. Conventional abstract-syntax-tree (AST) based
methods for smart contract similarity detection face challenges in handling
intricate tree structures, which impedes detailed semantic comparison of code.
Recent deep-learning based approaches tend to overlook code syntax and
detection interpretability, resulting in suboptimal performance.
  To fill this research gap, we introduce SmartDetector, a novel approach for
computing similarity between smart contract functions, explainable at the
fine-grained statement level. Technically, SmartDetector decomposes the AST of
a smart contract function into a series of smaller statement trees, each
reflecting a structural element of the source code. Then, SmartDetector uses a
classifier to compute the similarity score of two functions by comparing each
pair of their statement trees. To address the infinite hyperparameter space of
the classifier, we mathematically derive a cosine-wise diffusion process to
efficiently search optimal hyperparameters. Extensive experiments conducted on
three large real-world datasets demonstrate that SmartDetector outperforms
current state-of-the-art methods by an average improvement of 14.01% in
F1-score, achieving an overall average F1-score of 95.88%.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [241] [Path to Intelligence: Measuring Similarity between Human Brain and Large Language Model Beyond Language Task](https://arxiv.org/abs/2509.08831)
*Doai Ngo,Mingxuan Sun,Zhengji Zhang,Ashwin G Ramayya,Mark Schnitzer,Zhe Zhao*

Main category: q-bio.NC

TL;DR: LLMs exhibit parallels to human brain activity in sensory-motor tasks through linear mapping.


<details>
  <summary>Details</summary>
Motivation: Explore the resemblance between LLM functions and human brain activity in sensory-motor tasks.

Method: Sensory-motor task translated to natural language for LLMs; analyses of LLM hidden states versus intracranial EEG signals.

Result: Revealed that LLM-derived reactions can be linearly matched to human neural responses.

Conclusion: LLMs demonstrate potential for aiding neuroscience research by approximating human sensory-motor neurophysical behavior.

Abstract: Large language models (LLMs) have demonstrated human-like abilities in
language-based tasks. While language is a defining feature of human
intelligence, it emerges from more fundamental neurophysical processes rather
than constituting the basis of intelligence itself. In this work, we study the
similarity between LLM internal states and human brain activity in a
sensory-motor task rooted in anticipatory and visuospatial behavior. These
abilities are essential for cognitive performance that constitute human
intelligence. We translate the sensory-motor task into natural language in
order to replicate the process for LLMs. We extract hidden states from
pre-trained LLMs at key time steps and compare them to human intracranial EEG
signals. Our results reveal that LLM-derived reactions can be linearly mapped
onto human neural activity. These findings suggest that LLMs, with a simple
natural language translation to make them understand temporal-relevant tasks,
can approximate human neurophysical behavior in experiments involving sensory
stimulants. In all, our contribution is two-fold: (1) We demonstrate similarity
between LLM and human brain activity beyond language-based tasks. (2) We
demonstrate that with such similarity, LLMs could help us understand human
brains by enabling us to study topics in neuroscience that are otherwise
challenging to tackle.

</details>


### [242] [A novel cost-effective fabrication of a flexible neural probe for brain signal recording](https://arxiv.org/abs/2509.09213)
*Alireza Irandoost,Amirreza Bahramani,Roya Mohajeri,Faezeh Shahdost-Fard,Ali Ghazizadeh,Mehdi Fardmanesh*

Main category: q-bio.NC

TL;DR: This paper presents a cost-effective, flexible, implantable neural probe using polyimide film and gold electrodes, designed to record local field potential (LFP) signals. Performance was validated via electrochemical testing and in vivo experiments with zebra finches.


<details>
  <summary>Details</summary>
Motivation: The motivation is to develop a biocompatible, cost-effective, and high-performance neural probe using flexible and non-toxic materials for potential commercial and medical applications.

Method: The neural probe is fabricated using thin polyimide film (Kapton) as a flexible substrate, gold for conductive elements, and SU-8 to electrically isolate and stiffen the probe. Electrochemical impedance spectroscopy and in vivo neural signal recordings are used for performance evaluation.

Result: The probe demonstrated low impedance suitable for LFP acquisition, with in vivo tests showing comparable results to commercial neural electrodes when applied to zebra finches.

Conclusion: The biocompatibility and adaptability of the used materials highlight the promise of this neural probe for future commercial applications in neural implantable devices.

Abstract: This study introduces a novel, flexible, and implantable neural probe using a
cost-effective microfabrication process based on a thin polyimide film.
Polyimide film, known as Kapton, serves as a flexible substrate for
microelectrodes, conductive tracks, and contact pads of the probe, which are
made from a thin film of gold (Au). SU-8 is used to cover the corresponding
tracks for electrical isolation and to increase the stiffness of the probe for
better implantation. To evaluate the performance of the fabricated probe,
electrochemical impedance spectroscopy (EIS) and artificial neural signal
recording have been used to characterize its properties. The microelectrode
dimensions have been carefully chosen to provide low impedance characteristics,
which are necessary for acquiring local field potential (LFP) signals. The in
vivo LFP data have been obtained from a male zebra finch presented with
auditory stimuli. By properly filtering the extracellular recordings and
analyzing the data, the obtained results have been validated by comparing them
with the signals acquired with a commercial neural electrode. Due to the use of
Kapton, SU-8, and Au materials with non-toxic and adaptable properties in the
body environment, the fabricated neural probe is considered a promising
biocompatible implantable neural probe that may pave the way for the
fabrication of other neural implantable devices with commercial aims.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [243] [Scalable extensions to given-data Sobol' index estimators](https://arxiv.org/abs/2509.09078)
*Teresa Portone,Bert Debusschere,Samantha Yang,Emiliano Islas-Quinones,T. Patrick Xiao*

Main category: stat.ML

TL;DR: This work extends the existing given-data Sobol' index method for variance-based sensitivity analysis to handle models with extremely large numbers of inputs (e.g., neural networks with >10,000 parameters).


<details>
  <summary>Details</summary>
Motivation: Existing given-data methods for Sobol' index computation struggle with models that have an extremely large number of inputs or nonstandard input distributions.

Method: The paper introduces a general Sobol' index estimator with arbitrary partitioning, a streaming algorithm for input-output sampling in batches, and a heuristic for filtering out statistically insignificant indices.

Result: The new methods reduce memory requirements while maintaining accuracy and runtime, resolving biases introduced by equiprobable partitions, and enabling the analysis of models with large and complex input distributions.

Conclusion: These extensions make variance-based sensitivity analysis more practical and scalable for computationally expensive models, including large neural networks or models with unusual input distributions.

Abstract: Given-data methods for variance-based sensitivity analysis have significantly
advanced the feasibility of Sobol' index computation for computationally
expensive models and models with many inputs. However, the limitations of
existing methods still preclude their application to models with an extremely
large number of inputs. In this work, we present practical extensions to the
existing given-data Sobol' index method, which allow variance-based sensitivity
analysis to be efficiently performed on large models such as neural networks,
which have $>10^4$ parameterizable inputs. For models of this size, holding all
input-output evaluations simultaneously in memory -- as required by existing
methods -- can quickly become impractical. These extensions also support
nonstandard input distributions with many repeated values, which are not
amenable to equiprobable partitions employed by existing given-data methods.
  Our extensions include a general definition of the given-data Sobol' index
estimator with arbitrary partition, a streaming algorithm to process
input-output samples in batches, and a heuristic to filter out small indices
that are indistinguishable from zero indices due to statistical noise. We show
that the equiprobable partition employed in existing given-data methods can
introduce significant bias into Sobol' index estimates even at large sample
sizes and provide numerical analyses that demonstrate why this can occur. We
also show that our streaming algorithm can achieve comparable accuracy and
runtimes with lower memory requirements, relative to current methods which
process all samples at once. We demonstrate our novel developments on two
application problems in neural network modeling.

</details>


### [244] [Global Optimization of Stochastic Black-Box Functions with Arbitrary Noise Distributions using Wilson Score Kernel Density Estimation](https://arxiv.org/abs/2509.09238)
*Thorbjørn Mosekjær Iversen,Lars Carøe Sørensen,Simon Faarvang Mathiesen,Henrik Gordon Petersen*

Main category: stat.ML

TL;DR: The paper explores using Wilson Score Kernel Density Estimator (WS-KDE) for Bayesian optimization in robotics, offering reliable confidence bounds for stochastic functions.


<details>
  <summary>Details</summary>
Motivation: To improve Bayesian optimization methods in robotics by addressing the challenges of optimizing time-expensive and stochastic black-box functions.

Method: The authors utilized WS-KDE to provide high-confidence bounds for stochastic output functions confined to [0,1], and demonstrated its use in Bayesian optimization through simulations and a practical application for automated trap design.

Result: The WS-KDE showed its versatility by delivering accurate confidence bounds independent of the output distribution, making it applicable to a broader range of cost functions.

Conclusion: WS-KDE enhances the stability and reliability of Bayesian optimization and expands its use cases to various stochastic functions in robotics optimization tasks.

Abstract: Many optimization problems in robotics involve the optimization of
time-expensive black-box functions, such as those involving complex simulations
or evaluation of real-world experiments. Furthermore, these functions are often
stochastic as repeated experiments are subject to unmeasurable disturbances.
Bayesian optimization can be used to optimize such methods in an efficient
manner by deploying a probabilistic function estimator to estimate with a given
confidence so that regions of the search space can be pruned away.
Consequently, the success of the Bayesian optimization depends on the function
estimator's ability to provide informative confidence bounds. Existing function
estimators require many function evaluations to infer the underlying confidence
or depend on modeling of the disturbances. In this paper, it is shown that the
confidence bounds provided by the Wilson Score Kernel Density Estimator
(WS-KDE) are applicable as excellent bounds to any stochastic function with an
output confined to the closed interval [0;1] regardless of the distribution of
the output. This finding opens up the use of WS-KDE for stable global
optimization on a wider range of cost functions. The properties of WS-KDE in
the context of Bayesian optimization are demonstrated in simulation and applied
to the problem of automated trap design for vibrational part feeders.

</details>


### [245] [Low-degree lower bounds via almost orthonormal bases](https://arxiv.org/abs/2509.09353)
*Alexandra Carpentier,Simone Maria Giancola,Christophe Giraud,Nicolas Verzelen*

Main category: stat.ML

TL;DR: The paper introduces a direct proof strategy to construct an almost orthonormal basis of polynomials under a planted structure distribution, resolving gaps in standard low-degree polynomial techniques when applied to complex estimation tasks in high-dimensional random graph models.


<details>
  <summary>Details</summary>
Motivation: To tackle the limitations of standard low-degree polynomial methods in addressing complex statistical problems, especially in cases where the null distribution has planted structures and cannot be analyzed using simple orthogonal polynomial families.

Method: The authors construct an almost orthonormal polynomial basis under the null distribution in statistical random graph models, which allows for better characterization of statistical-computational trade-offs and direct derivation of low-degree lower bounds.

Result: The approach effectively recovers known low-degree lower bounds and establishes new ones for various statistical problems, including hidden subcliques, stochastic block models, and seriation models.

Conclusion: This work advances the understanding of statistical-computational gaps by providing more robust and direct tools for analyzing high-dimensional models, and their results enhance the ability to design optimized polynomial-time algorithms for complex problems.

Abstract: Low-degree polynomials have emerged as a powerful paradigm for providing
evidence of statistical-computational gaps across a variety of high-dimensional
statistical models [Wein25]. For detection problems -- where the goal is to
test a planted distribution $\mathbb{P}'$ against a null distribution
$\mathbb{P}$ with independent components -- the standard approach is to bound
the advantage using an $\mathbb{L}^2(\mathbb{P})$-orthonormal family of
polynomials. However, this method breaks down for estimation tasks or more
complex testing problems where $\mathbb{P}$ has some planted structures, so
that no simple $\mathbb{L}^2(\mathbb{P})$-orthogonal polynomial family is
available. To address this challenge, several technical workarounds have been
proposed [SW22,SW25], though their implementation can be delicate. In this
work, we propose a more direct proof strategy. Focusing on random graph models,
we construct a basis of polynomials that is almost orthonormal under
$\mathbb{P}$, in precisely those regimes where statistical-computational gaps
arise. This almost orthonormal basis not only yields a direct route to
establishing low-degree lower bounds, but also allows us to explicitly identify
the polynomials that optimize the low-degree criterion. This, in turn, provides
insights into the design of optimal polynomial-time algorithms. We illustrate
the effectiveness of our approach by recovering known low-degree lower bounds,
and establishing new ones for problems such as hidden subcliques, stochastic
block models, and seriation models.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [246] [Personalized Sleep Prediction via Deep Adaptive Spatiotemporal Modeling and Sparse Data](https://arxiv.org/abs/2509.09018)
*Xueyi Wang,C. J. C.,Lamoth,Elisabeth Wilhelm*

Main category: eess.SP

TL;DR: AdaST-Sleep is a model developed for sleep prediction, using convolutional and recurrent layers, along with domain adaptation for generalizability across subjects, showing robust performance across different forecasting windows.


<details>
  <summary>Details</summary>
Motivation: To improve mental and physical well-being by enabling proactive interventions through predictive sleep forecasting.

Method: The paper introduces AdaST-Sleep, a model combining convolutional and recurrent layers to analyze spatial and temporal health data, with domain classifiers for cross-subject generalization, tested across varied input and predicting windows.

Result: The approach outperforms four baseline models and achieves its lowest RMSE at 0.282 for a seven-day input window and one-day predicting window, demonstrating strong performance over diverse forecasting scenarios.

Conclusion: AdaST-Sleep provides an adaptable and accurate framework for personalized sleep forecasting using sparse wearable device data, offering significant real-world applicability.

Abstract: A sleep forecast allows individuals and healthcare providers to anticipate
and proactively address factors influencing restful rest, ultimately improving
mental and physical well-being. This work presents an adaptive spatial and
temporal model (AdaST-Sleep) for predicting sleep scores. Our proposed model
combines convolutional layers to capture spatial feature interactions between
multiple features and recurrent neural network layers to handle longer-term
temporal health-related data. A domain classifier is further integrated to
generalize across different subjects. We conducted several experiments using
five input window sizes (3, 5, 7, 9, 11 days) and five predicting window sizes
(1, 3, 5, 7, 9 days). Our approach consistently outperformed four baseline
models, achieving its lowest RMSE (0.282) with a seven-day input window and a
one-day predicting window. Moreover, the method maintained strong performance
even when forecasting multiple days into the future, demonstrating its
versatility for real-world applications. Visual comparisons reveal that the
model accurately tracks both the overall sleep score level and daily
fluctuations. These findings prove that the proposed framework provides a
robust and adaptable solution for personalized sleep forecasting using sparse
data from commercial wearable devices and domain adaptation techniques.

</details>


### [247] [A Masked Representation Learning to Model Cardiac Functions Using Multiple Physiological Signals](https://arxiv.org/abs/2509.08830)
*Seong-A Park,Jong-Eui Chae,Sungdong Kim,Hyung-Chul Lee,Hyun-Lim Yang*

Main category: eess.SP

TL;DR: The SNUPHY-M model leverages self-supervised learning to analyze and extract multi-modal physiological features (ECG, PPG, and ABP signals), outperforming existing approaches and aiding non-invasive clinical diagnostics.


<details>
  <summary>Details</summary>
Motivation: Hemodynamic monitoring requires integrated multi-signal analysis for better patient management, which previous studies focused on single-signal examination cannot offer.

Method: The study introduces SNUPHY-M, a model based on self-supervised learning that restores ECG, PPG, and ABP signals and extracts features reflecting cardiac cycle characteristics.

Result: SNUPHY-M exceeded the performance of supervised models, particularly for clinical prediction tasks using non-invasive signals like hypotension and blood pressure.

Conclusion: SNUPHY-M represents the first multi-modal SSL model for cardiovascular analysis, enabling enhanced clinical decision-making through precise, non-invasive diagnostics.

Abstract: In clinical settings, monitoring hemodynamics is crucial for managing patient
prognosis, necessitating the integrated analysis of multiple physiological
signals. While recent research has analyzed single signals such as
electrocardiography (ECG) or photoplethysmography (PPG), there has yet to be a
proposal for an approach that encompasses the complex signal analysis required
in actual clinical scenarios. In this study, we introduce the SNUPHY-M (Seoul
National University hospital PHYsiological signal Masked representation
learning) model extracts physiological features reflecting the electrical,
pressure, and fluid characteristics of the cardiac cycle in the process of
restoring three masked physiological signals based on self-supervised learning
(SSL): ECG, PPG, and arterial blood pressure (ABP) signals. By employing
multiple physical characteristics, the model can extract more enriched features
only using non-invasive signals. We evaluated the model's performance in
clinical downstream tasks such as hypotension, stroke volume, systolic blood
pressure, diastolic blood pressure, and age prediction. Our results showed that
the SNUPHY-M significantly outperformed supervised or SSL models, especially in
prediction tasks using non-invasive signals. To the best of our knowledge,
SNUPHY-M is the first model to apply multi-modal SSL to cardiovascular analysis
involving ECG, PPG, and ABP signals. This approach effectively supports
clinical decision-making and enables precise diagnostics, contributing
significantly to the early diagnosis and management of hemodynamics without
invasiveness.

</details>


### [248] [Deploying AI for Signal Processing education: Selected challenges and intriguing opportunities](https://arxiv.org/abs/2509.08950)
*Jarvis Haupt,Qin Lu,Yanning Shen,Jia Chen,Yue Dong,Dan McCreary,Mehmet Akçakaya,Georgios B. Giannakis*

Main category: eess.SP

TL;DR: The paper discusses using advanced AI tools to enhance education, focusing on signal processing, while addressing key issues like fairness, inclusivity, and trust.


<details>
  <summary>Details</summary>
Motivation: The aim is to align AI's capabilities with improving global human conditions, particularly in education.

Method: The authors explore AI integration into education with a focus on tackling technical and ethical challenges. They introduce a 'smart textbook' to demonstrate concepts.

Result: They identify core issues (e.g., fairness, hallucinated outputs, efficient resource use) and propose practical solutions through their smart textbook concept.

Conclusion: AI holds great potential for education, but its ethical and technical challenges must be addressed for effective, fair, and inclusive implementation.

Abstract: Powerful artificial intelligence (AI) tools that have emerged in recent years
-- including large language models, automated coding assistants, and advanced
image and speech generation technologies -- are the result of monumental human
achievements. These breakthroughs reflect mastery across multiple technical
disciplines and the resolution of significant technological challenges.
However, some of the most profound challenges may still lie ahead. These
challenges are not purely technical but pertain to the fair and responsible use
of AI in ways that genuinely improve the global human condition. This article
explores one promising application aligned with that vision: the use of AI
tools to facilitate and enhance education, with a specific focus on signal
processing (SP). It presents two interrelated perspectives: identifying and
addressing technical limitations, and applying AI tools in practice to improve
educational experiences. Primers are provided on several core technical issues
that arise when using AI in educational settings, including how to ensure
fairness and inclusivity, handle hallucinated outputs, and achieve efficient
use of resources. These and other considerations -- such as transparency,
explainability, and trustworthiness -- are illustrated through the development
of an immersive, structured, and reliable "smart textbook." The article serves
as a resource for researchers and educators seeking to advance AI's role in
engineering education.

</details>


### [249] [Ultrafast Deep Learning-Based Scatter Estimation in Cone-Beam Computed Tomography](https://arxiv.org/abs/2509.08973)
*Harshit Agrawal,Ari Hietanen,Simo Särkkä*

Main category: eess.SP

TL;DR: The study achieves significant reductions in computational and memory requirements for CBCT scatter corrections by employing downsampling and comparing network resolutions.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of deep learning-based scatter estimation methods in mobile CBCT systems and edge devices due to their large memory footprints.

Method: The paper examines reconstruction errors across multiple image resolutions, evaluates interpolation methods, and trains a state-of-the-art network at different resolutions to reduce floating-point operations and GPU memory usage.

Result: The proposed method reduced computational requirements by 78-fold while maintaining comparable scatter estimation accuracy. Inference time and GPU memory usage were reduced significantly.

Conclusion: The study emphasizes the impact of downsampling in deep learning-based methods for CBCT scatter corrections, enabling their applicability in resource-limited environments like mobile CBCT systems.

Abstract: Purpose: Scatter artifacts drastically degrade the image quality of cone-beam
computed tomography (CBCT) scans. Although deep learning-based methods show
promise in estimating scatter from CBCT measurements, their deployment in
mobile CBCT systems or edge devices is still limited due to the large memory
footprint of the networks. This study addresses the issue by applying networks
at varying resolutions and suggesting an optimal one, based on speed and
accuracy.
  Methods: First, the reconstruction error in down-up sampling of CBCT scatter
signal was examined at six resolutions by comparing four interpolation methods.
Next, a recent state-of-the-art method was trained across five image
resolutions and evaluated for the reductions in floating-point operations
(FLOPs), inference times, and GPU memory requirements.
  Results: Reducing the input size and network parameters achieved a 78-fold
reduction in FLOPs compared to the baseline method, while maintaining comarable
performance in terms of mean-absolute-percentage-error (MAPE) and
mean-square-error (MSE). Specifically, the MAPE decreased to 3.85% compared to
4.42%, and the MSE decreased to 1.34 \times 10^{-2} compared to 2.01 \times
10^{-2}. Inference time and GPU memory usage were reduced by factors of 16 and
12, respectively. Further experiments comparing scatter-corrected
reconstructions on a large, simulated dataset and real CBCT scans from water
and Sedentex CT phantoms clearly demonstrated the robustness of our method.
  Conclusion: This study highlights the underappreciated role of downsampling
in deep learning-based scatter estimation. The substantial reduction in FLOPs
and GPU memory requirements achieved by our method enables scatter correction
in resource-constrained environments, such as mobile CBCT and edge devices.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [250] [Convexity of Optimization Curves: Local Sharp Thresholds, Robustness Impossibility, and New Counterexamples](https://arxiv.org/abs/2509.08954)
*Le Duc Hieu*

Main category: math.OC

TL;DR: This paper examines the conditions under which the optimization curve of first-order methods is convex and establishes rigorous stepsize thresholds for gradient descent (GD) on convex L-smooth functions.


<details>
  <summary>Details</summary>
Motivation: The goal is to understand the convexity of optimization curves in first-order methods and its relationship with stepsizes, aiming to refine classical optimization tools and connect discrete methods with continuous dynamics.

Method: The paper analyzes the convexity of optimization curves and nonincreasing properties of gradient norms in gradient descent iterations for convex L-smooth functions, deriving stepsize thresholds for these properties both in discrete methods and continuous flow.

Result: For gradient descent on convex L-smooth functions, the optimization curve remains convex for stepsizes \(\eta \leq 1.75/L\), and this is a tight threshold. Gradient norms decrease for all \(\eta \leq 2/L\), and convexity holds continuously in gradient flow.

Conclusion: The results bridge discrete and continuous optimization analyses, enhancing our understanding of gradient descent dynamics and refining the smooth convex optimization framework.

Abstract: We study when the \emph{optimization curve} of first-order methods -- the
sequence \${f(x\_n)}*{n\ge0}\$ produced by constant-stepsize iterations -- is
convex, equivalently when the forward differences \$f(x\_n)-f(x*{n+1})\$ are
nonincreasing. For gradient descent (GD) on convex \$L\$-smooth functions, the
curve is convex for all stepsizes \$\eta \le 1.75/L\$, and this threshold is
tight. Moreover, gradient norms are nonincreasing for all \$\eta \le 2/L\$, and
in continuous time (gradient flow) the curve is always convex. These results
complement and refine the classical smooth convex optimization toolbox,
connecting discrete and continuous dynamics as well as worst-case analyses.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [251] [Generative Engine Optimization: How to Dominate AI Search](https://arxiv.org/abs/2509.08919)
*Mahe Chen,Xiaoxuan Wang,Kaiwen Chen,Nick Koudas*

Main category: cs.IR

TL;DR: The paper addresses the transition from traditional search engines to generative AI-driven search tools, proposing a framework called Generative Engine Optimization (GEO) for adapting to this new landscape.


<details>
  <summary>Details</summary>
Motivation: The rise of Generative AI search engines presents challenges to established SEO practices, compelling the need for a new optimization approach to maintain visibility and authority in the evolving ecosystem.

Method: The study conducted large-scale, controlled experiments across various parameters, such as verticals, languages, and query paraphrases, to compare traditional web search with generative AI-powered searches. Key system biases and behavioral patterns were analyzed.

Result: AI Search engines were found to strongly favor authoritative third-party (Earned media) sources over brand-owned or social content, differing significantly from Google's balanced approach. Additionally, variation among AI search engines in their domain diversity, content freshness, cross-language stability, and query sensitivity was observed.

Conclusion: A strategic GEO agenda is proposed, offering actionable steps for practitioners to optimize content for generative AI searches. These include enhancing machine scannability, building AI-perceived authority, and adapting to engine-specific nuances to thrive in the generative search ecosystem.

Abstract: The rapid adoption of generative AI-powered search engines like ChatGPT,
Perplexity, and Gemini is fundamentally reshaping information retrieval, moving
from traditional ranked lists to synthesized, citation-backed answers. This
shift challenges established Search Engine Optimization (SEO) practices and
necessitates a new paradigm, which we term Generative Engine Optimization
(GEO).
  This paper presents a comprehensive comparative analysis of AI Search and
traditional web search (Google). Through a series of large-scale, controlled
experiments across multiple verticals, languages, and query paraphrases, we
quantify critical differences in how these systems source information. Our key
findings reveal that AI Search exhibit a systematic and overwhelming bias
towards Earned media (third-party, authoritative sources) over Brand-owned and
Social content, a stark contrast to Google's more balanced mix. We further
demonstrate that AI Search services differ significantly from each other in
their domain diversity, freshness, cross-language stability, and sensitivity to
phrasing.
  Based on these empirical results, we formulate a strategic GEO agenda. We
provide actionable guidance for practitioners, emphasizing the critical need
to: (1) engineer content for machine scannability and justification, (2)
dominate earned media to build AI-perceived authority, (3) adopt
engine-specific and language-aware strategies, and (4) overcome the inherent
"big brand bias" for niche players. Our work provides the foundational
empirical analysis and a strategic framework for achieving visibility in the
new generative search landscape.

</details>


### [252] [Envy-Free but Still Unfair: Envy-Freeness Up To One Item (EF-1) in Personalized Recommendation](https://arxiv.org/abs/2509.09037)
*Amanda Aird,Ben Armstrong,Nicholas Mattei,Robin Burke*

Main category: cs.IR

TL;DR: The paper critiques the application of envy-freeness and EF-1 as fairness metrics in personalized settings, such as recommendation systems.


<details>
  <summary>Details</summary>
Motivation: To analyze the appropriateness of envy-freeness as a fairness measure in contexts like recommendation systems where personalization matters.

Method: The authors review the concepts of envy-freeness and EF-1, discuss their historical uses in various fields, and argue their limitations in personalized settings.

Result: The authors identify why envy-freeness and EF-1 fail to capture fairness effectively in personalized environments.

Conclusion: Fairness concepts like envy-freeness need reconsideration when applied in personalized settings due to their incompatibility with the nature of personalization.

Abstract: Envy-freeness and the relaxation to Envy-freeness up to one item (EF-1) have
been used as fairness concepts in the economics, game theory, and social choice
literatures since the 1960s, and have recently gained popularity within the
recommendation systems communities. In this short position paper we will give
an overview of envy-freeness and its use in economics and recommendation
systems; and illustrate why envy is not appropriate to measure fairness for use
in settings where personalization plays a role.

</details>


### [253] [Retrieval-Augmented Generation for Reliable Interpretation of Radio Regulations](https://arxiv.org/abs/2509.09651)
*Zakaria El Kassimi,Fares Fourati,Mohamed-Slim Alouini*

Main category: cs.IR

TL;DR: This paper explores a Retrieval-Augmented Generation (RAG) pipeline for question answering in radio regulations and introduces a domain-specific evaluation set.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the challenge of accurate question answering in the legally sensitive radio regulations domain, where precision is crucial.

Method: The authors propose a telecom-specific RAG pipeline combined with a domain-specific retrieval metric to enhance retrieval and generation accuracy.

Result: Their approach achieved approximately 97% retrieval accuracy and a significant improvement in generation accuracy (nearly 12% relative improvement for GPT-4) through structured retrieval.

Conclusion: Targeted grounding in a RAG pipeline offers an effective solution for regulatory question answering, setting a strong baseline. The resources are openly shared for further research.

Abstract: We study question answering in the domain of radio regulations, a legally
sensitive and high-stakes area. We propose a telecom-specific
Retrieval-Augmented Generation (RAG) pipeline and introduce, to our knowledge,
the first multiple-choice evaluation set for this domain, constructed from
authoritative sources using automated filtering and human validation. To assess
retrieval quality, we define a domain-specific retrieval metric, under which
our retriever achieves approximately 97% accuracy. Beyond retrieval, our
approach consistently improves generation accuracy across all tested models. In
particular, while naively inserting documents without structured retrieval
yields only marginal gains for GPT-4o (less than 1%), applying our pipeline
results in nearly a 12% relative improvement. These findings demonstrate that
carefully targeted grounding provides a simple yet strong baseline and an
effective domain-specific solution for regulatory question answering. All code
and evaluation scripts, along with our derived question-answer dataset, are
available at https://github.com/Zakaria010/Radio-RAG.

</details>


### [254] [We're Still Doing It (All) Wrong: Recommender Systems, Fifteen Years Later](https://arxiv.org/abs/2509.09414)
*Alan Said,Maria Soledad Pera,Michael D. Ekstrand*

Main category: cs.IR

TL;DR: Recommender systems research remains rooted in flawed foundations, despite increased sophistication and efforts for change.


<details>
  <summary>Details</summary>
Motivation: Address persistent conceptual, epistemological, and infrastructural flaws in recommender systems research.

Method: Re-examines foundational critique, analyzes recent community-led initiatives, and advocates for deep paradigm shifts.

Result: Identifies the limitations of current approaches and emphasizes the need for broader agenda reform.

Conclusion: Calls for recommender systems research to prioritize humility, human impact, and sustainability.

Abstract: In 2011, Xavier Amatriain sounded the alarm: recommender systems research was
"doing it all wrong" [1]. His critique, rooted in statistical misinterpretation
and methodological shortcuts, remains as relevant today as it was then. But
rather than correcting course, we added new layers of sophistication on top of
the same broken foundations. This paper revisits Amatriain's diagnosis and
argues that many of the conceptual, epistemological, and infrastructural
failures he identified still persist, in more subtle or systemic forms. Drawing
on recent work in reproducibility, evaluation methodology, environmental
impact, and participatory design, we showcase how the field's accelerating
complexity has outpaced its introspection. We highlight ongoing community-led
initiatives that attempt to shift the paradigm, including workshops, evaluation
frameworks, and calls for value-sensitive and participatory research. At the
same time, we contend that meaningful change will require not only new metrics
or better tooling, but a fundamental reframing of what recommender systems
research is for, who it serves, and how knowledge is produced and validated.
Our call is not just for technical reform, but for a recommender systems
research agenda grounded in epistemic humility, human impact, and sustainable
practice.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [255] [WarpPINN-fibers: improved cardiac strain estimation from cine-MR with physics-informed neural networks](https://arxiv.org/abs/2509.08872)
*Felipe Álvarez Barrientos,Tomás Banduc,Isabeau Sirven,Francisco Sahli Costabal*

Main category: eess.IV

TL;DR: This paper introduces WarpPINN-fibers, a physics-informed neural network framework to improve the prediction of cardiac motion and strain using cine MRI, incorporating fiber physiology for enhanced accuracy.


<details>
  <summary>Details</summary>
Motivation: Understanding cardiac tissue mechanics, especially through fiber distribution, is critical to diagnosing pathologies like cardiovascular diseases. Existing imaging-based strain estimation methods lack fiber mechanics, reducing their explanatory power for cardiac function.

Method: The authors developed WarpPINN-fibers, a framework using a neural network trained with a hyper-elastic model and fiber contraction constraints. The methodology includes a loss function combining data similarity, tissue incompressibility regularizers, and fiber-stretch penalization.

Result: WarpPINN-fibers outperformed previous methods (including the WarpPINN model) in both landmark-tracking and strain prediction on cine-MRI data from a cohort of 15 healthy volunteers and a synthetic phantom experiment.

Conclusion: WarpPINN-fibers offers enhanced precision in quantifying cardiac strain by integrating fiber physiology into the deformation field prediction, leading to improved explanations of cardiac mechanics while only requiring conventional MRI data.

Abstract: The contractile motion of the heart is strongly determined by the
distribution of the fibers that constitute cardiac tissue. Strain analysis
informed with the orientation of fibers allows to describe several pathologies
that are typically associated with impaired mechanics of the myocardium, such
as cardiovascular disease. Several methods have been developed to estimate
strain-derived metrics from traditional imaging techniques. However, the
physical models underlying these methods do not include fiber mechanics,
restricting their capacity to accurately explain cardiac function. In this
work, we introduce WarpPINN-fibers, a physics-informed neural network framework
to accurately obtain cardiac motion and strains enhanced by fiber information.
We train our neural network to satisfy a hyper-elastic model and promote fiber
contraction with the goal to predict the deformation field of the heart from
cine magnetic resonance images. For this purpose, we build a loss function
composed of three terms: a data-similarity loss between the reference and the
warped template images, a regularizer enforcing near-incompressibility of
cardiac tissue and a fiber-stretch penalization that controls strain in the
direction of synthetically produced fibers. We show that our neural network
improves the former WarpPINN model and effectively controls fiber stretch in a
synthetic phantom experiment. Then, we demonstrate that WarpPINN-fibers
outperforms alternative methodologies in landmark-tracking and strain curve
prediction for a cine-MRI benchmark with a cohort of 15 healthy volunteers. We
expect that our method will enable a more precise quantification of cardiac
strains through accurate deformation fields that are consistent with fiber
physiology, without requiring imaging techniques more sophisticated than MRI.

</details>


### [256] [Virtual staining for 3D X-ray histology of bone implants](https://arxiv.org/abs/2509.09235)
*Sarah C. Irvine,Christian Lucas,Diana Krüger,Bianca Guedert,Julian Moosmann,Berit Zeller-Plumhoff*

Main category: eess.IV

TL;DR: The paper presents a virtual staining method for 3D X-ray imaging using deep learning, enabling histological-like representations from label-free CT scans.


<details>
  <summary>Details</summary>
Motivation: Traditional X-ray imaging lacks biochemical specificity compared to histological stains, necessitating a method to improve interpretability in biomedical research.

Method: The study uses a modified CycleGAN model to transform micro-CT scans into virtually stained histological slices, leveraging paired data and augmentation techniques.

Result: The method produced realistic colour outputs with structural detail and outperformed Pix2Pix and CycleGAN baselines in evaluation metrics.

Conclusion: This approach introduces virtual staining to 3D X-ray imaging, offering scalable, label-free tissue characterization but requires further refinement for certain features.

Abstract: Three-dimensional X-ray histology techniques offer a non-invasive alternative
to conventional 2D histology, enabling volumetric imaging of biological tissues
without the need for physical sectioning or chemical staining. However, the
inherent greyscale image contrast of X-ray tomography limits its biochemical
specificity compared to traditional histological stains. Within digital
pathology, deep learning-based virtual staining has demonstrated utility in
simulating stained appearances from label-free optical images. In this study,
we extend virtual staining to the X-ray domain by applying cross-modality image
translation to generate artificially stained slices from
synchrotron-radiation-based micro-CT scans. Using over 50 co-registered image
pairs of micro-CT and toluidine blue-stained histology from bone-implant
samples, we trained a modified CycleGAN network tailored for limited paired
data. Whole slide histology images were downsampled to match the voxel size of
the CT data, with on-the-fly data augmentation for patch-based training. The
model incorporates pixelwise supervision and greyscale consistency terms,
producing histologically realistic colour outputs while preserving
high-resolution structural detail. Our method outperformed Pix2Pix and standard
CycleGAN baselines across SSIM, PSNR, and LPIPS metrics. Once trained, the
model can be applied to full CT volumes to generate virtually stained 3D
datasets, enhancing interpretability without additional sample preparation.
While features such as new bone formation were able to be reproduced, some
variability in the depiction of implant degradation layers highlights the need
for further training data and refinement. This work introduces virtual staining
to 3D X-ray imaging and offers a scalable route for chemically informative,
label-free tissue characterisation in biomedical research.

</details>


### [257] [Dynamic Structural Recovery Parameters Enhance Prediction of Visual Outcomes After Macular Hole Surgery](https://arxiv.org/abs/2509.09227)
*Yinzheng Zhao,Zhihao Zhao,Rundong Jiang,Louisa Sackewitz,Quanmin Liang,Mathias Maier,Daniel Zapp,Peter Charbel Issa,Mohammad Ali Nasseri*

Main category: eess.IV

TL;DR: The paper introduces new dynamic structural parameters and integrates them into a multimodal deep learning framework to predict visual recovery after macular hole surgery.


<details>
  <summary>Details</summary>
Motivation: Accurate prediction of visual outcomes after macular hole surgery is critical for personalized surgical planning and postoperative care, but current methods lack precision.

Method: The study uses a publicly available OCT dataset to analyze structural parameters over time, employs segmentation models for feature extraction, and develops a multimodal deep learning framework for prediction.

Result: The multimodal deep learning model incorporating dynamic parameters outperforms traditional logistic regression models, achieving higher prediction accuracy and AUC values, with significant improvement seen at the 3-month follow-up.

Conclusion: Dynamic parameters enhance prediction capabilities in multimodal deep learning frameworks, making this approach a valuable decision-support tool for personalized management following macular hole surgery.

Abstract: Purpose: To introduce novel dynamic structural parameters and evaluate their
integration within a multimodal deep learning (DL) framework for predicting
postoperative visual recovery in idiopathic full-thickness macular hole (iFTMH)
patients. Methods: We utilized a publicly available longitudinal OCT dataset at
five stages (preoperative, 2 weeks, 3 months, 6 months, and 12 months). A stage
specific segmentation model delineated related structures, and an automated
pipeline extracted quantitative, composite, qualitative, and dynamic features.
Binary logistic regression models, constructed with and without dynamic
parameters, assessed their incremental predictive value for best-corrected
visual acuity (BCVA). A multimodal DL model combining clinical variables,
OCT-derived features, and raw OCT images was developed and benchmarked against
regression models. Results: The segmentation model achieved high accuracy
across all timepoints (mean Dice > 0.89). Univariate and multivariate analyses
identified base diameter, ellipsoid zone integrity, and macular hole area as
significant BCVA predictors (P < 0.05). Incorporating dynamic recovery rates
consistently improved logistic regression AUC, especially at the 3-month
follow-up. The multimodal DL model outperformed logistic regression, yielding
higher AUCs and overall accuracy at each stage. The difference is as high as
0.12, demonstrating the complementary value of raw image volume and dynamic
parameters. Conclusions: Integrating dynamic parameters into the multimodal DL
model significantly enhances the accuracy of predictions. This fully automated
process therefore represents a promising clinical decision support tool for
personalized postoperative management in macular hole surgery.

</details>


### [258] [In-Loop Filtering Using Learned Look-Up Tables for Video Coding](https://arxiv.org/abs/2509.09494)
*Zhuoyuan Li,Jiacheng Li,Yao Li,Jialin Li,Li Li,Dong Liu,Feng Wu*

Main category: eess.IV

TL;DR: The paper introduces LUT-ILF++, a practical, LUT-based framework for in-loop filtering (ILF) in video coding to deliver effective filtering with lower computational complexity and storage.


<details>
  <summary>Details</summary>
Motivation: To address the high computational complexity and hardware demands of DNN-based ILF solutions while maintaining coding efficiency and visual quality.

Method: A universal LUT-based ILF framework is implemented that caches DNN outputs in LUTs for simple retrieval during coding. It includes multiple filtering LUTs, customized indexing, cross-component indexing, and a compaction scheme for reduced storage.

Result: The method is integrated into VVC software, achieving an average bitrate reduction of 0.82%-2.97% (AI) and 0.85%-4.11% (RA) for test sequences, outperforming DNN-based methods in complexity and storage.

Conclusion: LUT-ILF++ offers a practical and efficient alternative to DNN-based ILF, significantly reducing complexity and storage costs while maintaining competitive coding performance.

Abstract: In-loop filtering (ILF) is a key technology in video coding standards to
reduce artifacts and enhance visual quality. Recently, neural network-based ILF
schemes have achieved remarkable coding gains, emerging as a powerful candidate
for next-generation video coding standards. However, the use of deep neural
networks (DNN) brings significant computational and time complexity or high
demands for dedicated hardware, making it challenging for general use. To
address this limitation, we study a practical ILF solution by adopting look-up
tables (LUTs). After training a DNN with a restricted reference range for ILF,
all possible inputs are traversed, and the output values of the DNN are cached
into LUTs. During the coding process, the filtering process is performed by
simply retrieving the filtered pixel through locating the input pixels and
interpolating between the cached values, instead of relying on heavy inference
computations. In this paper, we propose a universal LUT-based ILF framework,
termed LUT-ILF++. First, we introduce the cooperation of multiple kinds of
filtering LUTs and propose a series of customized indexing mechanisms to enable
better filtering reference perception with limited storage consumption. Second,
we propose the cross-component indexing mechanism to enable the filtering of
different color components jointly. Third, in order to make our solution
practical for coding uses, we propose the LUT compaction scheme to enable the
LUT pruning, achieving a lower storage cost of the entire solution. The
proposed framework is implemented in the VVC reference software. Experimental
results show that the proposed framework achieves on average 0.82%/2.97%/1.63%
and 0.85%/4.11%/2.06% bitrate reduction for common test sequences, under the AI
and RA configurations, respectively. Compared to DNN-based solutions, our
proposed solution has much lower time complexity and storage cost.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [259] [Bona fide Cross Testing Reveals Weak Spot in Audio Deepfake Detection Systems](https://arxiv.org/abs/2509.09204)
*Chin Yuen Kwok,Jia Qi Yip,Zhen Qiu,Chi Hung Chi,Kwok Yan Lam*

Main category: cs.SD

TL;DR: The paper introduces a novel evaluation framework to improve the reliability and robustness of audio deepfake detection models by addressing biases in existing datasets and methods.


<details>
  <summary>Details</summary>
Motivation: Existing audio deepfake detection evaluation methods disproportionately favor synthesizers with more samples and lack bona fide speech diversity, limiting real-world applicability.

Method: The authors propose a 'bona fide cross-testing' framework, integrating diverse bona fide datasets and aggregating Equal Error Rates (EERs) for balanced model assessments.

Result: The framework benchmarks over 150 synthesizers across nine diverse bona fide speech types, improving robustness and interpretability.

Conclusion: Bona fide cross-testing enhances the evaluation of audio deepfake detection models by addressing dataset biases and increasing the relevance of assessments under real-world conditions.

Abstract: Audio deepfake detection (ADD) models are commonly evaluated using datasets
that combine multiple synthesizers, with performance reported as a single Equal
Error Rate (EER). However, this approach disproportionately weights
synthesizers with more samples, underrepresenting others and reducing the
overall reliability of EER. Additionally, most ADD datasets lack diversity in
bona fide speech, often featuring a single environment and speech style (e.g.,
clean read speech), limiting their ability to simulate real-world conditions.
To address these challenges, we propose bona fide cross-testing, a novel
evaluation framework that incorporates diverse bona fide datasets and
aggregates EERs for more balanced assessments. Our approach improves robustness
and interpretability compared to traditional evaluation methods. We benchmark
over 150 synthesizers across nine bona fide speech types and release a new
dataset to facilitate further research at
https://github.com/cyaaronk/audio_deepfake_eval.

</details>


### [260] [DiFlow-TTS: Discrete Flow Matching with Factorized Speech Tokens for Low-Latency Zero-Shot Text-To-Speech](https://arxiv.org/abs/2509.09631)
*Ngoc-Son Nguyen,Hieu-Nghia Huynh-Nguyen,Thanh V. T. Tran,Truong-Son Hy,Van Nguyen*

Main category: cs.SD

TL;DR: DiFlow-TTS is a zero-shot text-to-speech model that utilizes discrete flow matching for fast, high-quality voice synthesis, addressing existing challenges such as slow inference and repetition artifacts.


<details>
  <summary>Details</summary>
Motivation: Current zero-shot TTS approaches face issues like slow inference speeds and repetition artifacts while attempting to synthesize speech that mimics unseen speakers accurately, using only a short reference sample.

Method: DiFlow-TTS employs a fully discrete flow matching approach, factorizing speech attributes into textual, prosodic, and acoustic components. It uses distinct heads for prosody and acoustic features, enabling fast and specific attribute cloning while maintaining a compact architecture.

Result: The model achieves high performance across metrics such as naturalness, prosody accuracy, and speaker style preservation, while significantly reducing inference latency, generating speech 25.8 times faster than existing baselines.

Conclusion: DiFlow-TTS demonstrates the advantages of discrete flow matching in zero-shot TTS, providing a compact, efficient, and high-quality solution for synthesizing speech with diverse attributes and low latency.

Abstract: Zero-shot Text-to-Speech (TTS) aims to synthesize high-quality speech that
mimics the voice of an unseen speaker using only a short reference sample,
requiring not only speaker adaptation but also accurate modeling of prosodic
attributes. Recent approaches based on language models, diffusion, and flow
matching have shown promising results in zero-shot TTS, but still suffer from
slow inference and repetition artifacts. Discrete codec representations have
been widely adopted for speech synthesis, and recent works have begun to
explore diffusion models in purely discrete settings, suggesting the potential
of discrete generative modeling for speech synthesis. However, existing
flow-matching methods typically embed these discrete tokens into a continuous
space and apply continuous flow matching, which may not fully leverage the
advantages of discrete representations. To address these challenges, we
introduce DiFlow-TTS, which, to the best of our knowledge, is the first model
to explore purely Discrete Flow Matching for speech synthesis. DiFlow-TTS
explicitly models factorized speech attributes within a compact and unified
architecture. It leverages in-context learning by conditioning on textual
content, along with prosodic and acoustic attributes extracted from a reference
speech, enabling effective attribute cloning in a zero-shot setting. In
addition, the model employs a factorized flow prediction mechanism with
distinct heads for prosody and acoustic details, allowing it to learn
aspect-specific distributions. Experimental results demonstrate that DiFlow-TTS
achieves promising performance in several key metrics, including naturalness,
prosody, preservation of speaker style, and energy control. It also maintains a
compact model size and achieves low-latency inference, generating speech up to
25.8 times faster than the latest existing baselines.

</details>


### [261] [Adaptive Knowledge Distillation using a Device-Aware Teacher for Low-Complexity Acoustic Scene Classification](https://arxiv.org/abs/2509.09262)
*Seung Gyu Jeong,Seong Eun Kim*

Main category: cs.SD

TL;DR: The paper proposes a solution for DCASE 2025 Task 1 using a knowledge distillation framework with CP-MobileNet student model and a novel DAFA loss for device robustness.


<details>
  <summary>Details</summary>
Motivation: Address challenges of low complexity and device robustness in acoustic scene classification, while leveraging test-time device labels.

Method: Use a knowledge distillation framework with CP-MobileNet student model learning from two teachers (PaSST and a generalization expert) and apply device-specific fine-tuning.

Result: Achieved 57.93% accuracy on the development set, outperforming the baseline, especially for unseen devices.

Conclusion: The presented system effectively improves device-robust acoustic scene classification within low complexity constraints using innovative methodologies.

Abstract: In this technical report, we describe our submission for Task 1,
Low-Complexity Device-Robust Acoustic Scene Classification, of the DCASE 2025
Challenge. Our work tackles the dual challenges of strict complexity
constraints and robust generalization to both seen and unseen devices, while
also leveraging the new rule allowing the use of device labels at test time.
Our proposed system is based on a knowledge distillation framework where an
efficient CP-MobileNet student learns from a compact, specialized two-teacher
ensemble. This ensemble combines a baseline PaSST teacher, trained with
standard cross-entropy, and a 'generalization expert' teacher. This expert is
trained using our novel Device-Aware Feature Alignment (DAFA) loss, adapted
from prior work, which explicitly structures the feature space for device
robustness. To capitalize on the availability of test-time device labels, the
distilled student model then undergoes a final device-specific fine-tuning
stage. Our proposed system achieves a final accuracy of 57.93\% on the
development set, demonstrating a significant improvement over the official
baseline, particularly on unseen devices.

</details>


### [262] [Finite Scalar Quantization Enables Redundant and Transmission-Robust Neural Audio Compression at Low Bit-rates](https://arxiv.org/abs/2509.09550)
*Harry Julia,Rachel Beeson,Lohith Konathala,Johanna Ulin,Jiameng Gao*

Main category: cs.SD

TL;DR: This paper introduces NeuCodec, an FSQ-based neural audio codec, highlighting its robustness in noisy channels and discusses the benefits of FSQ over RVQ.


<details>
  <summary>Details</summary>
Motivation: Exploring alternative quantization methods for neural audio codecs to improve robustness and simplify training in noisy channel environments.

Method: Introduced NeuCodec, an FSQ-based neural audio codec, and conducted experiments, including encoder distillation and simulations of transmission through noisy channels.

Result: FSQ was found to be robust against bit-level perturbations and capable of encoding audio with comparable quality using simplified processes.

Conclusion: FSQ's baked-in redundancy and single-codebook support make it a better choice for robust neural audio codecs, especially in noisy conditions.

Abstract: Neural Audio Codecs (NACs) have become increasingly adopted in speech
processing tasks due to their excellent rate-distortion performance and
compatibility with Large Language Models (LLMs) as discrete feature
representations for audio generation. While most existing codecs rely on
Residual Vector Quantization (RVQ), Finite Scalar Quantization (FSQ) has
recently emerged as a compelling alternative that simplifies training and
natively supports single codebooks. We introduce NeuCodec, an FSQ-based NAC,
and show that FSQ encodes baked-in redundancy which produces an encoding which
is robust when transmitted through noisy channels. First, through an encoder
distillation experiment, we show that two different encoders can learn to
encode identical audio into vastly different code sequences whilst maintaining
comparable reconstruction quality with the same quantizer and decoder. Second,
we demonstrate that FSQ has vastly superior bit-level perturbation robustness
by comparing the performance of RVQ and FSQ codecs when simulating the
transmission of code sequences through a noisy channel.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [263] [PerFairX: Is There a Balance Between Fairness and Personality in Large Language Model Recommendations?](https://arxiv.org/abs/2509.08829)
*Chandan Kumar Sah*

Main category: cs.CY

TL;DR: Integrating Large Language Models (LLMs) into recommendation systems introduces the trade-off between personalized recommendations based on personality traits and demographic fairness. The proposed framework, PerFairX, evaluates this balance using benchmarks of two LLMs, revealing their psychological alignment and fairness disparities.


<details>
  <summary>Details</summary>
Motivation: The paper aims to enhance recommender systems by leveraging Large Language Models (LLMs) for zero-shot personalization based on users' personality traits while addressing demographic fairness in recommendations.

Method: The authors developed PerFairX — a unified evaluation framework that uses neutral and personality-sensitive prompts to analyze personalization and equity in LLM-generated recommendations, benchmarking ChatGPT and DeepSeek based on movie and music datasets.

Result: Results show that personality-aware prompting improves recommendations' psychological alignment but worsens fairness disparities. DeepSeek was found to have better psychological alignment but higher sensitivity to prompt changes, while ChatGPT provided more stable but less personalized recommendations.

Conclusion: PerFairX serves as a benchmark for guiding LLM-based recommender systems toward achieving equitable and psychologically informed recommendations, promoting inclusivity and user-centricity in AI-driven applications.

Abstract: The integration of Large Language Models (LLMs) into recommender systems has
enabled zero-shot, personality-based personalization through prompt-based
interactions, offering a new paradigm for user-centric recommendations.
However, incorporating user personality traits via the OCEAN model highlights a
critical tension between achieving psychological alignment and ensuring
demographic fairness. To address this, we propose PerFairX, a unified
evaluation framework designed to quantify the trade-offs between
personalization and demographic equity in LLM-generated recommendations. Using
neutral and personality-sensitive prompts across diverse user profiles, we
benchmark two state-of-the-art LLMs, ChatGPT and DeepSeek, on movie (MovieLens
10M) and music (Last.fm 360K) datasets. Our results reveal that
personality-aware prompting significantly improves alignment with individual
traits but can exacerbate fairness disparities across demographic groups.
Specifically, DeepSeek achieves stronger psychological fit but exhibits higher
sensitivity to prompt variations, while ChatGPT delivers stable yet less
personalized outputs. PerFairX provides a principled benchmark to guide the
development of LLM-based recommender systems that are both equitable and
psychologically informed, contributing to the creation of inclusive,
user-centric AI applications in continual learning contexts.

</details>


### [264] [Deep opacity and AI: A threat to XAI and to privacy protection mechanisms](https://arxiv.org/abs/2509.08835)
*Vincent C. Müller*

Main category: cs.CY

TL;DR: This paper categorizes privacy threats from AI and discusses how opacity prevents justifying decisions, making privacy protection more challenging.


<details>
  <summary>Details</summary>
Motivation: To analyze how AI's opacity impacts privacy and justification, and to offer insights into improving privacy protections.

Method: The paper categorizes opacity into three types based on knowledge gaps and examines their implications for decision-making and privacy guarantees.

Result: Opacity in AI undermines informed consent and anonymity, amplifying privacy issues in big data analytics.

Conclusion: Big data analytics exacerbates privacy threats, and technical solutions are needed to address opacity in AI systems.

Abstract: It is known that big data analytics and AI pose a threat to privacy, and that
some of this is due to some kind of "black box problem" in AI. I explain how
this becomes a problem in the context of justification for judgments and
actions. Furthermore, I suggest distinguishing three kinds of opacity: 1) the
subjects do not know what the system does ("shallow opacity"), 2) the analysts
do not know what the system does ("standard black box opacity"), or 3) the
analysts cannot possibly know what the system might do ("deep opacity"). If the
agents, data subjects as well as analytics experts, operate under opacity, then
these agents cannot provide justifications for judgments that are necessary to
protect privacy, e.g., they cannot give "informed consent", or guarantee
"anonymity". It follows from these points that agents in big data analytics and
AI often cannot make the judgments needed to protect privacy. So I conclude
that big data analytics makes the privacy problems worse and the remedies less
effective. As a positive note, I provide a brief outlook on technical ways to
handle this situation.

</details>


### [265] [Safe and Certifiable AI Systems: Concepts, Challenges, and Lessons Learned](https://arxiv.org/abs/2509.08852)
*Kajetan Schweighofer,Barbara Brune,Lukas Gruber,Simon Schmid,Alexander Aufreiter,Andreas Gruber,Thomas Doms,Sebastian Eder,Florian Mayer,Xaver-Paul Stadlbauer,Christoph Schwald,Werner Zellinger,Bernhard Nessler,Sepp Hochreiter*

Main category: cs.CY

TL;DR: The paper introduces the TÜV AUSTRIA Trusted AI framework, providing a method for assessing and certifying AI systems based on European standards, using audit catalogs tailored to the EU AI Act.


<details>
  <summary>Details</summary>
Motivation: There is a scarcity of practical schemes for certifying AI systems' safety, legal compliance, and social acceptability, despite their rising adoption in critical applications.

Method: The framework combines secure software development, functional requirements, and ethics & data privacy to create an audit catalog with testable criteria. A focus is placed on functional trustworthiness using statistical evaluation and lifecycle-oriented functional assessments.

Result: The framework highlights lessons learned, such as addressing data leakage, biases, and distribution drift controls, and suggests criteria for certifying robustness, fairness, and post-certification needs in AI systems.

Conclusion: The TÜV AUSTRIA Trusted AI framework provides a practical and legally compliant pathway for certifiable and trustworthy AI systems, aligning with EU standards and addressing key challenges in AI governance.

Abstract: There is an increasing adoption of artificial intelligence in safety-critical
applications, yet practical schemes for certifying that AI systems are safe,
lawful and socially acceptable remain scarce. This white paper presents the
T\"UV AUSTRIA Trusted AI framework an end-to-end audit catalog and methodology
for assessing and certifying machine learning systems. The audit catalog has
been in continuous development since 2019 in an ongoing collaboration with
scientific partners. Building on three pillars - Secure Software Development,
Functional Requirements, and Ethics & Data Privacy - the catalog translates the
high-level obligations of the EU AI Act into specific, testable criteria. Its
core concept of functional trustworthiness couples a statistically defined
application domain with risk-based minimum performance requirements and
statistical testing on independently sampled data, providing transparent and
reproducible evidence of model quality in real-world settings. We provide an
overview of the functional requirements that we assess, which are oriented on
the lifecycle of an AI system. In addition, we share some lessons learned from
the practical application of the audit catalog, highlighting common pitfalls we
encountered, such as data leakage scenarios, inadequate domain definitions,
neglect of biases, or a lack of distribution drift controls. We further discuss
key aspects of certifying AI systems, such as robustness, algorithmic fairness,
or post-certification requirements, outlining both our current conclusions and
a roadmap for future research. In general, by aligning technical best practices
with emerging European standards, the approach offers regulators, providers,
and users a practical roadmap for legally compliant, functionally trustworthy,
and certifiable AI systems.

</details>


### [266] [A vibe coding learning design to enhance EFL students' talking to, through, and about AI](https://arxiv.org/abs/2509.08854)
*David James Woo,Kai Guo,Yangyang Yu*

Main category: cs.CY

TL;DR: This paper explores the use of vibe coding (using natural language and AI to create software) as a learning tool in EFL education through a workshop and case studies.


<details>
  <summary>Details</summary>
Motivation: Investigate how AI-driven vibe coding can enhance English as a Foreign Language (EFL) education by addressing writing challenges.

Method: The authors implemented a four-hour workshop for two students to design applications using a human-AI meta-languaging framework. Data were collected using worksheets, video and screen recordings, think-aloud protocols, and AI-generated images.

Result: One student successfully created a functional application aligning with her design intent, while the other faced issues due to gaps between intended design and actual functionality. Differences in prompt engineering approaches and mental models were observed.

Conclusion: AI is a beneficial languaging tool, but effective instruction requires meta-languaging scaffolding, structured prompt engineering, authorship discussions, and vocabulary for AI mental models.

Abstract: This innovative practice article reports on the piloting of vibe coding
(using natural language to create software applications with AI) for English as
a Foreign Language (EFL) education. We developed a human-AI meta-languaging
framework with three dimensions: talking to AI (prompt engineering), talking
through AI (negotiating authorship), and talking about AI (mental models of
AI). Using backward design principles, we created a four-hour workshop where
two students designed applications addressing authentic EFL writing challenges.
We adopted a case study methodology, collecting data from worksheets and video
recordings, think-aloud protocols, screen recordings, and AI-generated images.
Contrasting cases showed one student successfully vibe coding a functional
application cohering to her intended design, while another encountered
technical difficulties with major gaps between intended design and actual
functionality. Analysis reveals differences in students' prompt engineering
approaches, suggesting different AI mental models and tensions in attributing
authorship. We argue that AI functions as a beneficial languaging machine, and
that differences in how students talk to, through, and about AI explain vibe
coding outcome variations. Findings indicate that effective vibe coding
instruction requires explicit meta-languaging scaffolding, teaching structured
prompt engineering, facilitating critical authorship discussions, and
developing vocabulary for articulating AI mental models.

</details>


### [267] [Investigating Student Interaction Patterns with Large Language Model-Powered Course Assistants in Computer Science Courses](https://arxiv.org/abs/2509.08862)
*Chang Liu,Loc Hoang,Andrew Stolman,Rene F. Kizilcec,Bo Wu*

Main category: cs.CY

TL;DR: This paper presents a study on using large language models (LLMs) as course assistants across computer science courses. The LLMs addressed temporal support gaps and novice learner needs but showed limitations in generating higher-order cognitive questions.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenge of providing flexible and timely academic support to students, especially outside traditional support hours, using LLMs to bridge the gap.

Method: The study deployed an LLM-powered course assistant to approximately 2,000 students across six computer science courses at three institutions and analyzed usage, interactions, and effectiveness through annotated conversations and taxonomy analysis.

Result: Findings demonstrate that usage was higher during evenings and nights, especially in introductory courses. Responses were mostly judged as correct and helpful, but LLMs were limited in generating higher-order questions, with follow-up questions often ignored by students in advanced courses.

Conclusion: LLMs show promise in supporting students but require greater educator involvement in prompt design and configuration for improved pedagogical outcomes.

Abstract: Providing students with flexible and timely academic support is a challenge
at most colleges and universities, leaving many students without help outside
scheduled hours. Large language models (LLMs) are promising for bridging this
gap, but interactions between students and LLMs are rarely overseen by
educators. We developed and studied an LLM-powered course assistant deployed
across multiple computer science courses to characterize real-world use and
understand pedagogical implications. By Spring 2024, our system had been
deployed to approximately 2,000 students across six courses at three
institutions. Analysis of the interaction data shows that usage remains strong
in the evenings and nights and is higher in introductory courses, indicating
that our system helps address temporal support gaps and novice learner needs.
We sampled 200 conversations per course for manual annotation: most sampled
responses were judged correct and helpful, with a small share unhelpful or
erroneous; few responses included dedicated examples. We also examined an
inquiry-based learning strategy: only around 11% of sampled conversations
contained LLM-generated follow-up questions, which were often ignored by
students in advanced courses. A Bloom's taxonomy analysis reveals that current
LLM capabilities are limited in generating higher-order cognitive questions.
These patterns suggest opportunities for pedagogically oriented LLM-based
educational systems and greater educator involvement in configuring prompts,
content, and policies.

</details>


### [268] [Decentralising LLM Alignment: A Case for Context, Pluralism, and Participation](https://arxiv.org/abs/2509.08858)
*Oriane Peter,Kate Devlin*

Main category: cs.CY

TL;DR: The paper critiques current LLM alignment practices for centralizing knowledge control and proposes a decentralized approach focusing on context, pluralism, and participation.


<details>
  <summary>Details</summary>
Motivation: The authors are motivated by the dominance of influential institutions in LLM alignment practices and their imposition of narrow normative values, which centralizes control over knowledge and governance.

Method: The authors conceptualize and propose decentralizing LLM alignment through three characteristics—context, pluralism, and participation—and ground these ideas in specific use cases.

Result: The paper highlights the critical role these three features play in decentralized alignment and provides detailed examples for their implementation in varied contexts.

Conclusion: LLM alignment can act as a platform to resist epistemic injustice and support democratic practices, though it needs to coexist with broader societal changes to be effective.

Abstract: Large Language Models (LLMs) alignment methods have been credited with the
commercial success of products like ChatGPT, given their role in steering LLMs
towards user-friendly outputs. However, current alignment techniques
predominantly mirror the normative preferences of a narrow reference group,
effectively imposing their values on a wide user base. Drawing on theories of
the power/knowledge nexus, this work argues that current alignment practices
centralise control over knowledge production and governance within already
influential institutions. To counter this, we propose decentralising alignment
through three characteristics: context, pluralism, and participation.
Furthermore, this paper demonstrates the critical importance of delineating the
context-of-use when shaping alignment practices by grounding each of these
features in concrete use cases. This work makes the following contributions:
(1) highlighting the role of context, pluralism, and participation in
decentralising alignment; (2) providing concrete examples to illustrate these
strategies; and (3) demonstrating the nuanced requirements associated with
applying alignment across different contexts of use. Ultimately, this paper
positions LLM alignment as a potential site of resistance against epistemic
injustice and the erosion of democratic processes, while acknowledging that
these strategies alone cannot substitute for broader societal changes.

</details>


### [269] [Incorporating AI Incident Reporting into Telecommunications Law and Policy: Insights from India](https://arxiv.org/abs/2509.09508)
*Avinash Agarwal,Manisha J. Nene*

Main category: cs.CY

TL;DR: This paper identifies risks posed by AI in telecommunications that exceed traditional frameworks and proposes a regulatory approach, using India as a case study.


<details>
  <summary>Details</summary>
Motivation: The integration of AI into telecom infrastructure introduces unique risks, such as algorithmic bias, not covered by current regulatory frameworks.

Method: The paper evaluates India's legal frameworks, including the Telecommunications Act, to identify gaps in addressing AI-specific risks.

Result: Finds that existing Indian regulations neglect AI-specific hazards, leading to recommendations for integrating AI incident reporting into telecom governance.

Conclusion: The paper proposes scalable policy solutions to address AI risks in telecommunications, serving as a model for other nations.

Abstract: The integration of artificial intelligence (AI) into telecommunications
infrastructure introduces novel risks, such as algorithmic bias and
unpredictable system behavior, that fall outside the scope of traditional
cybersecurity and data protection frameworks. This paper introduces a precise
definition and a detailed typology of telecommunications AI incidents,
establishing them as a distinct category of risk that extends beyond
conventional cybersecurity and data protection breaches. It argues for their
recognition as a distinct regulatory concern. Using India as a case study for
jurisdictions that lack a horizontal AI law, the paper analyzes the country's
key digital regulations. The analysis reveals that India's existing legal
instruments, including the Telecommunications Act, 2023, the CERT-In Rules, and
the Digital Personal Data Protection Act, 2023, focus on cybersecurity and data
breaches, creating a significant regulatory gap for AI-specific operational
incidents, such as performance degradation and algorithmic bias. The paper also
examines structural barriers to disclosure and the limitations of existing AI
incident repositories. Based on these findings, the paper proposes targeted
policy recommendations centered on integrating AI incident reporting into
India's existing telecom governance. Key proposals include mandating reporting
for high-risk AI failures, designating an existing government body as a nodal
agency to manage incident data, and developing standardized reporting
frameworks. These recommendations aim to enhance regulatory clarity and
strengthen long-term resilience, offering a pragmatic and replicable blueprint
for other nations seeking to govern AI risks within their existing sectoral
frameworks.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [270] [Expressive Power of Deep Networks on Manifolds: Simultaneous Approximation](https://arxiv.org/abs/2509.09362)
*Hanfei Zhou,Lei Shi*

Main category: math.NA

TL;DR: This paper provides a theoretical foundation for using deep neural networks to efficiently approximate functions on manifolds, which is crucial for solving PDEs in complex geometries.


<details>
  <summary>Details</summary>
Motivation: To address the challenges posed by curved geometries in solving PDEs, particularly the difficulty of approximating functions and derivatives on manifolds.

Method: Establishes an approximation theory using constant-depth ReLU networks for Sobolev spaces and provides upper and lower bounds on the required parameters.

Result: The study shows ReLU networks can approximate functions on manifold domains efficiently, overcoming the curse of dimensionality and providing nearly optimal bounds.

Conclusion: Deep neural networks are effective and theoretically robust for learning PDEs on manifolds by leveraging their sparse structure and low-dimensional geometry.

Abstract: A key challenge in scientific machine learning is solving partial
differential equations (PDEs) on complex domains, where the curved geometry
complicates the approximation of functions and their derivatives required by
differential operators. This paper establishes the first simultaneous
approximation theory for deep neural networks on manifolds. We prove that a
constant-depth $\mathrm{ReLU}^{k-1}$ network with bounded weights--a property
that plays a crucial role in controlling generalization error--can approximate
any function in the Sobolev space $\mathcal{W}_p^{k}(\mathcal{M}^d)$ to an
error of $\varepsilon$ in the $\mathcal{W}_p^{s}(\mathcal{M}^d)$ norm, for
$k\geq 3$ and $s<k$, using $\mathcal{O}(\varepsilon^{-d/(k-s)})$ nonzero
parameters, a rate that overcomes the curse of dimensionality by depending only
on the intrinsic dimension $d$. These results readily extend to functions in
H\"older-Zygmund spaces. We complement this result with a matching lower bound,
proving our construction is nearly optimal by showing the required number of
parameters matches up to a logarithmic factor. Our proof of the lower bound
introduces novel estimates for the Vapnik-Chervonenkis dimension and
pseudo-dimension of the network's high-order derivative classes. These
complexity bounds provide a theoretical cornerstone for learning PDEs on
manifolds involving derivatives. Our analysis reveals that the network
architecture leverages a sparse structure to efficiently exploit the manifold's
low-dimensional geometry.

</details>


### [271] [DeepTV: A neural network approach for total variation minimization](https://arxiv.org/abs/2409.05569)
*Andreas Langer,Sara Behnamian*

Main category: math.NA

TL;DR: This paper uses neural networks for solving an infinite-dimensional total variation minimization problem, addressing theoretical and computational challenges with auxiliary and discrete versions of the problem.


<details>
  <summary>Details</summary>
Motivation: To develop a neural network-based approach for solving infinite-dimensional total variation minimization problems and address theoretical issues in solving such problems.

Method: The paper proposes auxiliary and discrete neural network formulations, proving their convergence to the original infinite-dimensional problem and establishing theoretical connections with finite difference discretization.

Result: The auxiliary and discrete neural network formulations are shown to converge to the original problem. Numerical experiments validate these findings.

Conclusion: The proposed approach provides a practical and theoretically grounded method for solving infinite-dimensional total variation minimization problems using neural networks.

Abstract: Neural network approaches have been demonstrated to work quite well to solve
partial differential equations in practice. In this context approaches like
physics-informed neural networks and the Deep Ritz method have become popular.
In this paper, we propose a similar approach to solve an infinite-dimensional
total variation minimization problem using neural networks. We illustrate that
the resulting neural network problem does not have a solution in general. To
circumvent this theoretic issue, we consider an auxiliary neural network
problem, which indeed has a solution, and show that it converges in the sense
of $\Gamma$-convergence to the original problem. For computing a numerical
solution we further propose a discrete version of the auxiliary neural network
problem and again show its $\Gamma$-convergence to the original
infinite-dimensional problem. In particular, the $\Gamma$-convergence proof
suggests a particular discretization of the total variation. Moreover, we
connect the discrete neural network problem to a finite difference
discretization of the infinite-dimensional total variation minimization
problem. Numerical experiments are presented supporting our theoretical
findings.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [272] [CameraVDP: Perceptual Display Assessment with Uncertainty Estimation via Camera and Visual Difference Prediction](https://arxiv.org/abs/2509.08947)
*Yancheng Cai,Robert Wanat,Rafal Mantiuk*

Main category: cs.GR

TL;DR: The study introduces CameraVDP, a method combining camera-based image reconstruction and a visual difference predictor for accurate display measurement and distortion visibility analysis.


<details>
  <summary>Details</summary>
Motivation: Traditional display measurement methods are insufficient for capturing spatially varying and high-frequency artifacts, and camera-based approaches introduce their own distortions. This research addresses these limitations to enable precise perceptual evaluation of displays.

Method: The study developed a pipeline combining HDR image stacking, MTF inversion, vignetting correction, geometric undistortion, homography transformation, and color correction, along with a Visual Difference Predictor for assessing human visual system responses to distortions.

Result: CameraVDP demonstrated its effectiveness in applications such as detecting defective pixels, assessing color fringing, and evaluating display non-uniformity. The framework also offers uncertainty analysis capabilities for estimating defect detection capabilities and VDP score confidence intervals.

Conclusion: CameraVDP successfully combines high-accuracy display measurement and perceptual visibility assessment, improving evaluation capabilities for displaying technology and delivering actionable insights for display quality monitoring.

Abstract: Accurate measurement of images produced by electronic displays is critical
for the evaluation of both traditional and computational displays. Traditional
display measurement methods based on sparse radiometric sampling and fitting a
model are inadequate for capturing spatially varying display artifacts, as they
fail to capture high-frequency and pixel-level distortions. While cameras offer
sufficient spatial resolution, they introduce optical, sampling, and
photometric distortions. Furthermore, the physical measurement must be combined
with a model of a visual system to assess whether the distortions are going to
be visible. To enable perceptual assessment of displays, we propose a
combination of a camera-based reconstruction pipeline with a visual difference
predictor, which account for both the inaccuracy of camera measurements and
visual difference prediction. The reconstruction pipeline combines HDR image
stacking, MTF inversion, vignetting correction, geometric undistortion,
homography transformation, and color correction, enabling cameras to function
as precise display measurement instruments. By incorporating a Visual
Difference Predictor (VDP), our system models the visibility of various stimuli
under different viewing conditions for the human visual system. We validate the
proposed CameraVDP framework through three applications: defective pixel
detection, color fringing awareness, and display non-uniformity evaluation. Our
uncertainty analysis framework enables the estimation of the theoretical upper
bound for defect pixel detection performance and provides confidence intervals
for VDP quality scores.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [273] [Physics-informed waveform inversion using pretrained wavefield neural operators](https://arxiv.org/abs/2509.08967)
*Xinquan Huang,Fu Wang,Tariq Alkhalifah*

Main category: physics.geo-ph

TL;DR: The study develops a physics-informed framework to enhance Full Waveform Inversion (FWI), integrating physical constraints for cleaner and accurate subsurface modeling.


<details>
  <summary>Details</summary>
Motivation: FWI is essential for high-resolution subsurface modeling but suffers from low-resolution outputs due to data limitations and high computational costs, which restrict practicality for real-time applications.

Method: The paper proposes a physics-informed loss function that includes the physical laws of wave propagation alongside traditional objectives, starting with initial subsurface models and iteratively refining them using neural operators.

Result: The framework provides cleaner and accurate subsurface velocity models compared to traditional FWI approaches, as validated through numerical experiments with OpenFWI and Overthrust models.

Conclusion: The physics-informed method significantly advances real-time subsurface modeling, offering greater accuracy and efficiency in practical applications of FWI.

Abstract: Full waveform inversion (FWI) is crucial for reconstructing high-resolution
subsurface models, but it is often hindered, considering the limited data, by
its null space resulting in low-resolution models, and more importantly, by its
computational cost, especially if needed for real-time applications. Recent
attempts to accelerate FWI using learned wavefield neural operators have shown
promise in efficiency and differentiability, but typically suffer from noisy
and unstable inversion performance. To address these limitations, we introduce
a novel physics-informed FWI framework to enhance the inversion in accuracy
while maintaining the efficiency of neural operator-based FWI. Instead of
relying only on the L2 norm objective function via automatic differentiation,
resulting in noisy model reconstruction, we integrate a physics constraint term
in the loss function of FWI, improving the quality of the inverted velocity
models. Specifically, starting with an initial model to simulate wavefields and
then evaluating the loss over how much the resulting wavefield obeys the
physical laws (wave equation) and matches the recorded data, we achieve a
reduction in noise and artifacts. Numerical experiments using the OpenFWI and
Overthrust models demonstrate our method's superior performance, offering
cleaner and more accurate subsurface velocity than vanilla approaches.
Considering the efficiency of the approach compared to FWI, this advancement
represents a significant step forward in the practical application of FWI for
real-time subsurface monitoring.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [274] [Towards A High-Performance Quantum Data Center Network Architecture](https://arxiv.org/abs/2509.09653)
*Yufeng Xin,Liang Zhang*

Main category: quant-ph

TL;DR: This paper proposes a three-layer fat-tree network architecture to address the challenges of scalability, entanglement generation, and quantum memory management in modular Quantum Data Centers (QDCs).


<details>
  <summary>Details</summary>
Motivation: Current large-scale quantum computers face significant technological and financial constraints. A modular approach that clusters small quantum computers offers an alternative, but introduces new challenges in network scalability, entanglement generation, and managing quantum memory.

Method: The paper presents a three-layer fat-tree network architecture with a unique leaf switch design, an advanced spine switch for efficient entanglement swapping, and a queue scheduling mechanism to manage quantum memory. Queuing-theoretical models and simulations in NetSquid were used for evaluation.

Result: The proposed architecture demonstrated high scalability and effectively maintained strong entanglement fidelity as validated through simulations and queuing models.

Conclusion: The proposed three-layer fat-tree architecture offers a scalable and practical solution for the development of modular QDC networks, addressing key issues like network scalability and quantum memory management.

Abstract: Quantum Data Centers (QDCs) are needed to support large-scale quantum
processing for both academic and commercial applications. While large-scale
quantum computers are constrained by technological and financial barriers, a
modular approach that clusters small quantum computers offers an alternative.
This approach, however, introduces new challenges in network scalability,
entanglement generation, and quantum memory management. In this paper, we
propose a three-layer fat-tree network architecture for QDCs, designed to
address these challenges. Our architecture features a unique leaf switch and an
advanced swapping spine switch design, optimized to handle high volumes of
entanglement requests as well as a queue scheduling mechanism that efficiently
manages quantum memory to prevent decoherence. Through queuing-theoretical
models and simulations in NetSquid, we demonstrate the proposed architecture's
scalability and effectiveness in maintaining high entanglement fidelity,
offering a practical path forward for modular QDC networks.

</details>


### [275] [Generative quantum advantage for classical and quantum problems](https://arxiv.org/abs/2509.09033)
*Hsin-Yuan Huang,Michael Broughton,Norhan Eassa,Hartmut Neven,Ryan Babbush,Jarrod R. McClean*

Main category: quant-ph

TL;DR: The paper introduces generative quantum models that overcome the inefficiencies of classical systems and demonstrates their application on a 68-qubit quantum processor for tasks previously deemed classically intractable.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of demonstrating generative quantum advantage, where quantum computers can outperform classical systems in learning and generating outputs, addressing inefficiencies that have limited progress.

Method: The authors introduce generative quantum models that are computationally hard to simulate classically but remain efficiently trainable, avoiding common pitfalls like barren plateaus and local minima.

Result: Using a 68-qubit quantum processor, they demonstrate the models' efficacy in learning classically intractable distributions and quantum circuits for accelerated physical simulations.

Conclusion: The study proves that both learning and sampling in the beyond-classical regime can be efficiently achieved, highlighting a provable quantum advantage in generative modeling.

Abstract: Recent breakthroughs in generative machine learning, powered by massive
computational resources, have demonstrated unprecedented human-like
capabilities. While beyond-classical quantum experiments can generate samples
from classically intractable distributions, their complexity has thwarted all
efforts toward efficient learning. This challenge has hindered demonstrations
of generative quantum advantage: the ability of quantum computers to learn and
generate desired outputs substantially better than classical computers. We
resolve this challenge by introducing families of generative quantum models
that are hard to simulate classically, are efficiently trainable, exhibit no
barren plateaus or proliferating local minima, and can learn to generate
distributions beyond the reach of classical computers. Using a $68$-qubit
superconducting quantum processor, we demonstrate these capabilities in two
scenarios: learning classically intractable probability distributions and
learning quantum circuits for accelerated physical simulation. Our results
establish that both learning and sampling can be performed efficiently in the
beyond-classical regime, opening new possibilities for quantum-enhanced
generative models with provable advantage.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [276] [HARD: A Performance Portable Radiation Hydrodynamics Code based on FleCSI Framework](https://arxiv.org/abs/2509.08971)
*Julien Loiseau,Hyun Lim,Andrés Yagüe López,Mammadbaghir Baghirzade,Shihab Shahriar Khan,Yoonsoo Kim,Sudarshan Neopane,Alexander Strack,Farhana Taiyebah,Benjamin K. Bergen*

Main category: physics.comp-ph

TL;DR: HARD is an open-source tool for radiation hydrodynamics simulations optimized for portability and reliability, using cutting-edge frameworks like FleCSI and Kokkos.


<details>
  <summary>Details</summary>
Motivation: To provide a scalable, efficient, and scientifically reliable simulation tool for compressible hydrodynamics with radiation diffusion.

Method: HARD integrates task-based computing via FleCSI, supports multiple runtimes (Legion, MPI, HPX), and utilizes Kokkos for parallelism. It includes verification tests for reliability.

Result: HARD successfully runs across diverse hardware setups, ensures numerical accuracy through regression tests, and offers a robust platform hosted on GitHub.

Conclusion: HARD serves as a sustainable, high-performance platform for advancing radiation hydrodynamics research while promoting open science and collaboration.

Abstract: Hydrodynamics And Radiation Diffusion} (HARD) is an open-source application
for high-performance simulations of compressible hydrodynamics with
radiation-diffusion coupling. Built on the FleCSI (Flexible Computational
Science Infrastructure) framework, HARD expresses its computational units as
tasks whose execution can be orchestrated by multiple back-end runtimes,
including Legion, MPI, and HPX. Node-level parallelism is delegated to Kokkos,
providing a single, portable code base that runs efficiently on laptops, small
homogeneous clusters, and the largest heterogeneous supercomputers currently
available. To ensure scientific reliability, HARD includes a regression-test
suite that automatically reproduces canonical verification problems such as the
Sod and LeBlanc shock tubes and the Sedov blast wave, comparing numerical
solutions against known analytical results. The project is distributed under an
OSI-approved license, hosted on GitHub, and accompanied by reproducible build
scripts and continuous integration workflows. This combination of performance
portability, verification infrastructure, and community-focused development
makes HARD a sustainable platform for advancing radiation hydrodynamics
research across multiple domains.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [277] [Database Views as Explanations for Relational Deep Learning](https://arxiv.org/abs/2509.09482)
*Agapi Rissaki,Ilias Fountalis,Wolfgang Gatterbauer,Benny Kimelfeld*

Main category: cs.DB

TL;DR: This paper introduces a novel framework for explaining deep learning models applied to relational databases, focusing on hetero-GNNs, using abductive explanations and determinacy concepts.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the difficulty in understanding how complex deep learning architectures like hetero-GNNs use relational database data to generate predictions.

Method: The paper adapts determinacy theory for crafting explanations as view definitions, which focus on specific database sections contributing to predictions. It utilizes model-agnostic techniques and hetero-GNN-specific methods like learnable masking.

Result: Experiments on the RelBench dataset demonstrated that the proposed framework effectively explains model predictions while ensuring computational efficiency.

Conclusion: The framework advances interpretability for machine learning models over relational databases, showing practical usability and efficiency.

Abstract: In recent years, there has been significant progress in the development of
deep learning models over relational databases, including architectures based
on heterogeneous graph neural networks (hetero-GNNs) and heterogeneous graph
transformers. In effect, such architectures state how the database records and
links (e.g., foreign-key references) translate into a large, complex numerical
expression, involving numerous learnable parameters. This complexity makes it
hard to explain, in human-understandable terms, how a model uses the available
data to arrive at a given prediction. We present a novel framework for
explaining machine-learning models over relational databases, where
explanations are view definitions that highlight focused parts of the database
that mostly contribute to the model's prediction. We establish such global
abductive explanations by adapting the classic notion of determinacy by Nash,
Segoufin, and Vianu (2010). In addition to tuning the tradeoff between
determinacy and conciseness, the framework allows controlling the level of
granularity by adopting different fragments of view definitions, such as ones
highlighting whole columns, foreign keys between tables, relevant groups of
tuples, and so on. We investigate the realization of the framework in the case
of hetero-GNNs. We develop heuristic algorithms that avoid the exhaustive
search over the space of all databases. We propose techniques that are
model-agnostic, and others that are tailored to hetero-GNNs via the notion of
learnable masking. Our approach is evaluated through an extensive empirical
study on the RelBench collection, covering a variety of domains and different
record-level tasks. The results demonstrate the usefulness of the proposed
explanations, as well as the efficiency of their generation.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [278] [An Integrated Open Source Software System for the Generation and Analysis of Subject-Specific Blood Flow Simulation Ensembles](https://arxiv.org/abs/2509.09392)
*Simon Leistikow,Thomas Miro,Adrian Kummerländer,Ali Nahardani,Katja Grün,Markus Franz,Verena Hoerr,Mathias J. Krause,Lars Linsen*

Main category: physics.med-ph

TL;DR: The paper introduces a user-oriented visual tool that integrates MRI and CFD for analyzing blood flow dynamics in hemodynamic studies.


<details>
  <summary>Details</summary>
Motivation: To integrate MRI and CFD for subject-specific hemodynamic analysis, facilitating the diagnosis of cardiovascular diseases and investigation of cardiovascular parameters.

Method: Developed an open-source, interactive visual analysis tool that supports simulation ensemble creation, 2D embedding visualization, and is designed for users in medicine and numerical analysis.

Result: The tool's functionality was demonstrated through three real-world use cases, with evaluations by MRI and CFD experts to enhance its usability and features.

Conclusion: Combining CFD and MRI enhances the understanding of hemodynamic parameters for more accurate cardiovascular analysis, aiding diagnostics and research.

Abstract: Background and Objective: Hemodynamic analysis of blood flow through arteries
and veins is critical for diagnosing cardiovascular diseases, such as aneurysms
and stenoses, and for investigating cardiovascular parameters, such as
turbulence and wall shear stress. For subject-specific analyses, the anatomy
and blood flow of the subject can be captured non-invasively using structural
and 4D Magnetic Resonance Imaging (MRI). Computational Fluid Dynamics (CFD), on
the other hand, can be used to generate blood flow simulations by solving the
Navier-Stokes equations. To generate and analyze subject-specific blood flow
simulations, MRI and CFD have to be brought together.
  Methods: We present an interactive, customizable, and user-oriented visual
analysis tool that assists researchers in both medicine and numerical analysis.
Our open-source tool is applicable to domains such as CFD and MRI, and it
facilitates the analysis of simulation results and medical data, especially in
hemodynamic studies. It enables the creation of simulation ensembles with a
high variety of parameters. Furthermore, it allows for the visual and
analytical examination of simulations and measurements through 2D embeddings of
the similarity space.
  Results: To demonstrate the effectiveness of our tool, we applied it to three
real-world use cases, showcasing its ability to configure simulation ensembles
and analyse blood flow dynamics. We evaluated our example cases together with
MRI and CFD experts to further enhance features and increase the usability.
  Conclusions: By combining the strengths of both CFD and MRI, our tool
provides a more comprehensive understanding of hemodynamic parameters,
facilitating more accurate analysis of hemodynamic biomarkers.

</details>


### [279] [Explainable AI for Accelerated Microstructure Imaging: A SHAP-Guided Protocol on the Connectome 2.0 scanner](https://arxiv.org/abs/2509.09513)
*Quentin Uhl,Tommaso Pavan,Julianna Gerold,Kwok-Shing Chan,Yohan Jun,Shohei Fujita,Aneri Bhatt,Yixin Ma,Qiaochu Wang,Hong-Hsi Lee,Susie Y. Huang,Berkin Bilgic,Ileana Jelescu*

Main category: physics.med-ph

TL;DR: This paper proposes a reduced acquisition scheme for improving efficiency in Neurite Exchange Imaging, achieving comparable results with shorter scan times.


<details>
  <summary>Details</summary>
Motivation: Long scan times limit the practical utility of Neurite Exchange Imaging in examining gray matter microstructure.

Method: An explainable AI-based framework with guided recursive feature elimination was employed to reduce the number of protocol features from 15 to 8, optimizing scan time while preserving accuracy.

Result: The optimized protocol demonstrated comparable parameter estimates, robust test-retest reproducibility, and reduced estimation errors compared to full protocols and other reduction methods.

Conclusion: This study developed a generalizable, hybrid optimization method that enables accurate Neurite Exchange Imaging in just 14 minutes, broadening its applicability in research and clinical settings.

Abstract: The diffusion MRI Neurite Exchange Imaging model offers a promising framework
for probing gray matter microstructure by estimating parameters such as
compartment sizes, diffusivities, and inter-compartmental water exchange time.
However, existing protocols require long scan times. This study proposes a
reduced acquisition scheme for the Connectome 2.0 scanner that preserves model
accuracy while substantially shortening scan duration. We developed a
data-driven framework using explainable artificial intelligence with a guided
recursive feature elimination strategy to identify an optimal 8-feature subset
from a 15-feature protocol. The performance of this optimized protocol was
validated in vivo and benchmarked against the full acquisition and alternative
reduction strategies. Parameter accuracy, preservation of anatomical contrast,
and test-retest reproducibility were assessed. The reduced protocol yielded
parameter estimates and cortical maps comparable to the full protocol, with low
estimation errors in synthetic data and minimal impact on test-retest
variability. Compared to theory-driven and heuristic reduction schemes, the
optimized protocol demonstrated superior robustness, reducing the deviation in
water exchange time estimates by over two-fold. In conclusion, this hybrid
optimization framework enables viable imaging of neurite exchange in 14 minutes
without loss of parameter fidelity. This approach supports the broader
application of exchange-sensitive diffusion magnetic resonance imaging in
neuroscience and clinical research, and offers a generalizable method for
designing efficient acquisition protocols in biophysical parameter mapping.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [280] [Towards Confidential and Efficient LLM Inference with Dual Privacy Protection](https://arxiv.org/abs/2509.09091)
*Honglan Yu,Yibin Wang,Feifei Dai,Dong Liu,Haihui Fan,Xiaoyan Gu*

Main category: cs.CR

TL;DR: The paper proposes CMIF, a framework to enhance privacy and efficiency in language model inference by deploying embedding layers in TEEs and optimizing noise mechanisms.


<details>
  <summary>Details</summary>
Motivation: Current approaches for private inference face challenges including high inference latency in TEEs and reduced model performance due to differential privacy.

Method: CMIF deploys the embedding layer in client-side TEEs, utilizes GPU servers for dense nonlinear layers, and optimizes the Report-Noisy-Max mechanism for privacy.

Result: Experiments on Llama-series models revealed reduced inference overhead in TEEs and better privacy preservation compared to existing methods.

Conclusion: CMIF effectively addresses high latency and privacy challenges in private inference, offering improved efficiency and confidentiality for user data during model execution.

Abstract: CPU-based trusted execution environments (TEEs) and differential privacy (DP)
have gained wide applications for private inference. Due to high inference
latency in TEEs, researchers use partition-based approaches that offload linear
model components to GPUs. However, dense nonlinear layers of large language
models (LLMs) result in significant communication overhead between TEEs and
GPUs. DP-based approaches apply random noise to protect data privacy, but this
compromises LLM performance and semantic understanding. To overcome the above
drawbacks, this paper proposes CMIF, a Confidential and efficient Model
Inference Framework. CMIF confidentially deploys the embedding layer in the
client-side TEE and subsequent layers on GPU servers. Meanwhile, it optimizes
the Report-Noisy-Max mechanism to protect sensitive inputs with a slight
decrease in model performance. Extensive experiments on Llama-series models
demonstrate that CMIF reduces additional inference overhead in TEEs while
preserving user data privacy.

</details>


### [281] [DP-FedLoRA: Privacy-Enhanced Federated Fine-Tuning for On-Device Large Language Models](https://arxiv.org/abs/2509.09097)
*Honghui Xu,Shiva Shrestha,Wei Chen,Zhiyuan Li,Zhipeng Cai*

Main category: cs.CR

TL;DR: The paper proposes DP-FedLoRA, a framework combining federated fine-tuning, LoRA-based adaptation, and differential privacy to offer privacy-preserving language model fine-tuning on edge devices.


<details>
  <summary>Details</summary>
Motivation: To tackle privacy concerns in on-device federated fine-tuning of large language models (LLMs) while maintaining performance.

Method: The DP-FedLoRA framework incorporates LoRA-based adaptation with $(\epsilon, \delta)$-differential privacy, using Gaussian noise to perturb LoRA matrices and locally clip updates, ensuring both communication efficiency and privacy preservation.

Result: The framework achieves competitive performance on standard benchmarks while maintaining strong privacy guarantees, as validated through experiments.

Conclusion: DP-FedLoRA offers a scalable and privacy-enhanced solution for federated LLM deployment on edge devices, balancing performance with privacy protection.

Abstract: As on-device large language model (LLM) systems become increasingly
prevalent, federated fine-tuning enables advanced language understanding and
generation directly on edge devices; however, it also involves processing
sensitive, user-specific data, raising significant privacy concerns within the
federated learning framework. To address these challenges, we propose
DP-FedLoRA, a privacy-enhanced federated fine-tuning framework that integrates
LoRA-based adaptation with differential privacy in a communication-efficient
setting. Each client locally clips and perturbs its LoRA matrices using
Gaussian noise to satisfy ($\epsilon$, $\delta$)-differential privacy. We
further provide a theoretical analysis demonstrating the unbiased nature of the
updates and deriving bounds on the variance introduced by noise, offering
practical guidance for privacy-budget calibration. Experimental results across
mainstream benchmarks show that DP-FedLoRA delivers competitive performance
while offering strong privacy guarantees, paving the way for scalable and
privacy-preserving LLM deployment in on-device environments.

</details>


### [282] [Character-Level Perturbations Disrupt LLM Watermarks](https://arxiv.org/abs/2509.09112)
*Zhaoxi Zhang,Xiaomei Zhang,Yanjun Zhang,He Zhang,Shirui Pan,Bo Liu,Asif Qumer Gill,Leo Yu Zhang*

Main category: cs.CR

TL;DR: This paper focuses on vulnerabilities in current watermarking methods for texts generated by Large Language Models (LLMs), showcasing how character-level perturbations and optimized removal techniques can effectively bypass watermark defenses.


<details>
  <summary>Details</summary>
Motivation: The motivation is to identify weaknesses in existing watermarking schemes for LLM outputs and explore robust methods to remove or bypass such watermarks under realistic use scenarios.

Method: The authors analyze two realistic threat models with limited access to watermark detectors, experimenting with character-level perturbations and proposing guided removal attacks using Genetic Algorithms to optimize watermark removal.

Result: Character-level perturbations demonstrate high effectiveness in removing LLM watermarks, even under strict restrictions. The Genetic Algorithm-based guided removal method achieves strong removal performance under limited detector access, and adaptive compound attacks bypass fixed defenses.

Conclusion: Current LLM watermarking mechanisms have significant vulnerabilities, necessitating the development of improved watermarking schemes as character-level and adaptive attacks prove capable of defeating existing defenses.

Abstract: Large Language Model (LLM) watermarking embeds detectable signals into
generated text for copyright protection, misuse prevention, and content
detection. While prior studies evaluate robustness using watermark removal
attacks, these methods are often suboptimal, creating the misconception that
effective removal requires large perturbations or powerful adversaries.
  To bridge the gap, we first formalize the system model for LLM watermark, and
characterize two realistic threat models constrained on limited access to the
watermark detector. We then analyze how different types of perturbation vary in
their attack range, i.e., the number of tokens they can affect with a single
edit. We observe that character-level perturbations (e.g., typos, swaps,
deletions, homoglyphs) can influence multiple tokens simultaneously by
disrupting the tokenization process. We demonstrate that character-level
perturbations are significantly more effective for watermark removal under the
most restrictive threat model. We further propose guided removal attacks based
on the Genetic Algorithm (GA) that uses a reference detector for optimization.
Under a practical threat model with limited black-box queries to the watermark
detector, our method demonstrates strong removal performance. Experiments
confirm the superiority of character-level perturbations and the effectiveness
of the GA in removing watermarks under realistic constraints. Additionally, we
argue there is an adversarial dilemma when considering potential defenses: any
fixed defense can be bypassed by a suitable perturbation strategy. Motivated by
this principle, we propose an adaptive compound character-level attack.
Experimental results show that this approach can effectively defeat the
defenses. Our findings highlight significant vulnerabilities in existing LLM
watermark schemes and underline the urgency for the development of new robust
mechanisms.

</details>


### [283] [CryptGNN: Enabling Secure Inference for Graph Neural Networks](https://arxiv.org/abs/2509.09107)
*Pritam Sen,Yao Ma,Cristian Borcea*

Main category: cs.CR

TL;DR: CryptGNN is a privacy-preserving inference framework for GNN models using secure multi-party computation.


<details>
  <summary>Details</summary>
Motivation: To enable secure usage of third-party GNN models in cloud services while safeguarding the confidentiality of client data, graph structures, and model parameters.

Method: CryptGNN incorporates distributed secure multi-party computation techniques, ensuring secure message passing and feature transformation without requiring a trusted server.

Result: CryptGNN achieves provable security even against colluded parties and demonstrates practical efficiency in theoretical and empirical evaluations.

Conclusion: CryptGNN ensures privacy-preserving GNN inference in the cloud, marking a significant step for secure MLaaS applications.

Abstract: We present CryptGNN, a secure and effective inference solution for
third-party graph neural network (GNN) models in the cloud, which are accessed
by clients as ML as a service (MLaaS). The main novelty of CryptGNN is its
secure message passing and feature transformation layers using distributed
secure multi-party computation (SMPC) techniques. CryptGNN protects the
client's input data and graph structure from the cloud provider and the
third-party model owner, and it protects the model parameters from the cloud
provider and the clients. CryptGNN works with any number of SMPC parties, does
not require a trusted server, and is provably secure even if P-1 out of P
parties in the cloud collude. Theoretical analysis and empirical experiments
demonstrate the security and efficiency of CryptGNN.

</details>


### [284] [ENSI: Efficient Non-Interactive Secure Inference for Large Language Models](https://arxiv.org/abs/2509.09424)
*Zhiyu He,Maojiang Wang,Xinwen Gao,Yuchuan Luo,Lin Liu,Shaojing Fu*

Main category: cs.CR

TL;DR: The paper introduces ENSI, a novel framework enabling secure inference for large language models using cryptographic co-design, achieving significant computational efficiency improvements.


<details>
  <summary>Details</summary>
Motivation: The growing importance of privacy-preserving machine learning necessitates secure inference methods. However, existing cryptographic protocols struggle to efficiently handle the complexity and scale of large language models (LLMs).

Method: The paper introduces ENSI, a secure inference framework that co-designs cryptographic protocols and LLM architecture. It uses an optimized encoding strategy integrating the CKKS scheme with BitNet, applies sigmoid attention under homomorphic encryption (HE) as an alternative to softmax, and embeds the Bootstrapping operation within the RMSNorm process to reduce computational overhead.

Result: ENSI achieves significant performance improvements, including an 8x acceleration in encrypted matrix multiplications and a 2.6x speedup in softmax inference on CPU compared to state-of-the-art methods. Bootstrapping frequency was reduced to 1%.

Conclusion: ENSI overcomes the challenges of secure inference for LLMs by optimizing computations through cryptographic-architecture co-design, promoting privacy-preservation without sacrificing efficiency.

Abstract: Secure inference enables privacy-preserving machine learning by leveraging
cryptographic protocols that support computations on sensitive user data
without exposing it. However, integrating cryptographic protocols with large
language models (LLMs) presents significant challenges, as the inherent
complexity of these protocols, together with LLMs' massive parameter scale and
sophisticated architectures, severely limits practical usability. In this work,
we propose ENSI, a novel non-interactive secure inference framework for LLMs,
based on the principle of co-designing the cryptographic protocols and LLM
architecture. ENSI employs an optimized encoding strategy that seamlessly
integrates CKKS scheme with a lightweight LLM variant, BitNet, significantly
reducing the computational complexity of encrypted matrix multiplications. In
response to the prohibitive computational demands of softmax under homomorphic
encryption (HE), we pioneer the integration of the sigmoid attention mechanism
with HE as a seamless, retraining-free alternative. Furthermore, by embedding
the Bootstrapping operation within the RMSNorm process, we efficiently refresh
ciphertexts while markedly decreasing the frequency of costly bootstrapping
invocations. Experimental evaluations demonstrate that ENSI achieves
approximately an 8x acceleration in matrix multiplications and a 2.6x speedup
in softmax inference on CPU compared to state-of-the-art method, with the
proportion of bootstrapping is reduced to just 1%.

</details>


### [285] [Prompt Pirates Need a Map: Stealing Seeds helps Stealing Prompts](https://arxiv.org/abs/2509.09488)
*Felix Mächtle,Ashwath Shetty,Jonas Sander,Nils Loose,Sören Pirk,Thomas Eisenbarth*

Main category: cs.CR

TL;DR: The paper examines the security vulnerability in diffusion models used for text-to-image generation, proposing a new method (PromptPirate) for improved prompt theft and introducing countermeasures.


<details>
  <summary>Details</summary>
Motivation: Diffusion models have transformed text-to-image generation, but the potential for prompt theft raises significant privacy and security concerns.

Method: The study identifies a vulnerability in noise generation in major frameworks and uses a brute-force seed recovery tool (SeedSnitch) coupled with a genetic algorithm-based optimization method (PromptPirate).

Result: Their method could retrieve seed values effectively within 140 minutes for 95% of shared images and achieved an 8-11% improvement in generation similarity over prior techniques.

Conclusion: The paper highlights a critical vulnerability, provides state-of-the-art solutions for exploiting it, and introduces effective countermeasures to mitigate the risks, responsibly disclosed to developers.

Abstract: Diffusion models have significantly advanced text-to-image generation,
enabling the creation of highly realistic images conditioned on textual prompts
and seeds. Given the considerable intellectual and economic value embedded in
such prompts, prompt theft poses a critical security and privacy concern. In
this paper, we investigate prompt-stealing attacks targeting diffusion models.
We reveal that numerical optimization-based prompt recovery methods are
fundamentally limited as they do not account for the initial random noise used
during image generation. We identify and exploit a noise-generation
vulnerability (CWE-339), prevalent in major image-generation frameworks,
originating from PyTorch's restriction of seed values to a range of $2^{32}$
when generating the initial random noise on CPUs. Through a large-scale
empirical analysis conducted on images shared via the popular platform CivitAI,
we demonstrate that approximately 95% of these images' seed values can be
effectively brute-forced in 140 minutes per seed using our seed-recovery tool,
SeedSnitch. Leveraging the recovered seed, we propose PromptPirate, a genetic
algorithm-based optimization method explicitly designed for prompt stealing.
PromptPirate surpasses state-of-the-art methods, i.e., PromptStealer, P2HP, and
CLIP-Interrogator, achieving an 8-11% improvement in LPIPS similarity.
Furthermore, we introduce straightforward and effective countermeasures that
render seed stealing, and thus optimization-based prompt stealing, ineffective.
We have disclosed our findings responsibly and initiated coordinated mitigation
efforts with the developers to address this critical vulnerability.

</details>


### [286] [What Does Normal Even Mean? Evaluating Benign Traffic in Intrusion Detection Datasets](https://arxiv.org/abs/2509.09564)
*Meghan Wilkinson,Robert H Thomson*

Main category: cs.CR

TL;DR: This paper investigates whether sub-categorizing benign traffic in network intrusion detection datasets can improve multi-classification performance through clustering analysis.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to improve the labeling process for training intrusion detection algorithms by identifying meaningful sub-categories in benign traffic, a frequently overlooked aspect in such datasets.

Method: The paper evaluates the structure of benign network traffic in datasets like NSL-KDD, UNSW-NB15, and CIC-IDS 2017 using unsupervised clustering techniques such as HDBSCAN and Mean Shift Clustering.

Result: The study finds differences in how unsupervised clustering techniques delineate the benign traffic space, suggesting the potential for meaningful sub-categorization.

Conclusion: Refining the categorization of benign traffic in intrusion detection datasets may enhance multi-class classification performance, unlocking a path for better machine learning model training.

Abstract: Supervised machine learning techniques rely on labeled data to achieve high
task performance, but this requires the labels to capture some meaningful
differences in the underlying data structure. For training network intrusion
detection algorithms, most datasets contain a series of attack classes and a
single large benign class which captures all non-attack network traffic. A
review of intrusion detection papers and guides that explicitly state their
data preprocessing steps identified that the majority took the labeled
categories of the dataset at face value when training their algorithms. The
present paper evaluates the structure of benign traffic in several common
intrusion detection datasets (NSL-KDD, UNSW-NB15, and CIC-IDS 2017) and
determines whether there are meaningful sub-categories within this traffic
which may improve overall multi-classification performance using common machine
learning techniques. We present an overview of some unsupervised clustering
techniques (e.g., HDBSCAN, Mean Shift Clustering) and show how they
differentially cluster the benign traffic space.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [287] [Representation-Aware Distributionally Robust Optimization: A Knowledge Transfer Framework](https://arxiv.org/abs/2509.09371)
*Zitao Wang,Nian Si,Molei Liu*

Main category: stat.ME

TL;DR: The paper introduces the READ framework for robust learning, considering representation-aware Wasserstein distances for addressing distributional shifts.


<details>
  <summary>Details</summary>
Motivation: To improve robustness in machine learning models by differentially treating feature perturbations based on representations, rather than equal perturbation treatment.

Method: Proposes embedding multidimensional alignment into transport costs in Wasserstein Distributionally Robust Optimization (DRO), with theoretical foundations and an algorithm for optimized model projection.

Result: Enables robust estimations with refined geometric representation-aware confidence regions validated through simulations and real-world studies.

Conclusion: READ provides a robust estimation approach through representation-aware robust learning, addressing data distribution shifts effectively while retaining invariant structure and predictive capabilities.

Abstract: We propose REpresentation-Aware Distributionally Robust Estimation (READ), a
novel framework for Wasserstein distributionally robust learning that accounts
for predictive representations when guarding against distributional shifts.
Unlike classical approaches that treat all feature perturbations equally, READ
embeds a multidimensional alignment parameter into the transport cost, allowing
the model to differentially discourage perturbations along directions
associated with informative representations. This yields robustness to feature
variation while preserving invariant structure. Our first contribution is a
theoretical foundation: we show that seminorm regularizations for linear
regression and binary classification arise as Wasserstein distributionally
robust objectives, thereby providing tractable reformulations of READ and
unifying a broad class of regularized estimators under the DRO lens. Second, we
adopt a principled procedure for selecting the Wasserstein radius using the
techniques of robust Wasserstein profile inference. This further enables the
construction of valid, representation-aware confidence regions for model
parameters with distinct geometric features. Finally, we analyze the geometry
of READ estimators as the alignment parameters vary and propose an optimization
algorithm to estimate the projection of the global optimum onto this solution
surface. This procedure selects among equally robust estimators while optimally
constructing a representation structure. We conclude by demonstrating the
effectiveness of our framework through extensive simulations and a real-world
study, providing a powerful robust estimation grounded in learning
representation.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [288] [The Role of Community Detection Methods in Performance Variations of Graph Mining Tasks](https://arxiv.org/abs/2509.09045)
*Shrabani Ghosh,Erik Saule*

Main category: cs.SI

TL;DR: The study examines the impact of community detection algorithm choice on downstream tasks by proposing an evaluation framework, revealing varied performance across methods.


<details>
  <summary>Details</summary>
Motivation: To address the lack of clarity in selecting community detection methods for improving downstream graph tasks, given real-world challenges and absence of ground truth.

Method: The paper presents a framework that integrates multiple community detection methods and systematically evaluates their effects on downstream applications like link prediction and node classification.

Result: The comparative analysis demonstrates that different community detection methods perform variably across applications, with some significantly enhancing outcomes.

Conclusion: The findings emphasize the importance of selecting appropriate community detection methods for specific graph mining tasks, as performance impacts are substantial.

Abstract: In real-world scenarios, large graphs represent relationships among entities
in complex systems. Mining these large graphs often containing millions of
nodes and edges helps uncover structural patterns and meaningful insights.
Dividing a large graph into smaller subgraphs facilitates complex system
analysis by revealing local information. Community detection extracts clusters
or communities of graphs based on statistical methods and machine learning
models using various optimization techniques. Structure based community
detection methods are more suitable for applying to graphs because they do not
rely heavily on rich node or edge attribute information. The features derived
from these communities can improve downstream graph mining tasks, such as link
prediction and node classification. In real-world applications, we often lack
ground truth community information. Additionally, there is neither a
universally accepted gold standard for community detection nor a single method
that is consistently optimal across diverse applications. In many cases, it is
unclear how practitioners select community detection methods, and choices are
often made without explicitly considering their potential impact on downstream
tasks. In this study, we investigate whether the choice of community detection
algorithm significantly influences the performance of downstream applications.
We propose a framework capable of integrating various community detection
methods to systematically evaluate their effects on downstream task outcomes.
Our comparative analysis reveals that specific community detection algorithms
yield superior results in certain applications, highlighting that method
selection substantially affects performance.

</details>
