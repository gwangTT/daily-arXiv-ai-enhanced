{"id": "2507.00005", "pdf": "https://arxiv.org/pdf/2507.00005", "abs": "https://arxiv.org/abs/2507.00005", "authors": ["Vasavi Lankipalle"], "title": "SwarmFusion: Revolutionizing Disaster Response with Swarm Intelligence and Deep Learning", "categories": ["cs.NE", "cs.LG"], "comment": "6", "summary": "Disaster response requires rapid, adaptive decision-making in chaotic\nenvironments. SwarmFusion, a novel hybrid framework, integrates particle swarm\noptimization with convolutional neural networks to optimize real-time resource\nallocation and path planning. By processing live satellite, drone, and sensor\ndata, SwarmFusion enhances situational awareness and operational efficiency in\nflood and wildfire scenarios. Simulations using the DisasterSim2025 dataset\ndemonstrate up to 40 percentage faster response times and 90 percentage\nsurvivor coverage compared to baseline methods. This scalable, data-driven\napproach offers a transformative solution for time-critical disaster\nmanagement, with potential applications across diverse crisis scenarios.", "AI": {"tldr": "The paper presents SwarmFusion, a hybrid framework integrating particle swarm optimization and convolutional neural networks to improve disaster response operations.", "motivation": "To address the need for rapid, adaptive decision-making in disaster situations by enhancing resource allocation and path planning through data-driven methods.", "method": "SwarmFusion integrates particle swarm optimization and convolutional neural networks to process live data (from satellites, drones, and sensors) and improve situational awareness and decision-making.", "result": "Simulations using the DisasterSim2025 dataset showed improvements of up to 40% in response times and 90% in survivor coverage, outperforming baseline methods.", "conclusion": "SwarmFusion offers a scalable, innovative solution for disaster management, demonstrating its potential to handle various crisis scenarios effectively and efficiently."}}
{"id": "2507.00387", "pdf": "https://arxiv.org/pdf/2507.00387", "abs": "https://arxiv.org/abs/2507.00387", "authors": ["Chengze Jiang", "Jie Gui", "Long Jin", "Shuai Li"], "title": "A Review on Zeroing Neural Networks", "categories": ["cs.NE"], "comment": "This is what we submitted to IJCAI 2023. Maybe we will update this\n  paper in the future", "summary": "Zeroing neural networks (ZNNs) have demonstrated outstanding performance on\ntime-varying optimization and control problems. Nonetheless, few studies are\ncommitted to illustrating the relationship among different ZNNs and the\nderivation of them. Therefore, reviewing the advances for a systematical\nunderstanding of this field is desirable. This paper provides a survey of ZNNs'\nprogress regarding implementing methods, analysis theory, and practical\napplications.", "AI": {"tldr": "Zeroing neural networks (ZNNs) are effective in solving time-varying optimization and control problems, but their connections and derivations are underexplored. This paper surveys their methods, theories, and applications.", "motivation": "Few studies have examined the relationships between different ZNNs and their derivations, leading to the need for a systematic understanding of the field.", "method": "The paper conducts a survey focusing on ZNNs' implementation methods, theoretical analysis, and applications.", "result": "The survey provides a framework for understanding the progress and potential of ZNNs in various fields.", "conclusion": "A systematized understanding of ZNNs is necessary for advancing their applications and theoretical underpinnings."}}
{"id": "2507.00461", "pdf": "https://arxiv.org/pdf/2507.00461", "abs": "https://arxiv.org/abs/2507.00461", "authors": ["Garimella Ramamurthy", "Marcos Eduardo Valle", "Tata Jagannadha Swamy"], "title": "Novel Complex-Valued Hopfield Neural Networks with Phase and Magnitude Quantization", "categories": ["cs.NE", "cs.AI"], "comment": "Paper submitted to the Fifth International Conference on Emerging\n  Techniques in Computational Intelligence (ICETCI 2025)", "summary": "This research paper introduces two novel complex-valued Hopfield neural\nnetworks (CvHNNs) that incorporate phase and magnitude quantization. The first\nCvHNN employs a ceiling-type activation function that operates on the\nrectangular coordinate representation of the complex net contribution. The\nsecond CvHNN similarly incorporates phase and magnitude quantization but\nutilizes a ceiling-type activation function based on the polar coordinate\nrepresentation of the complex net contribution. The proposed CvHNNs, with their\nphase and magnitude quantization, significantly increase the number of states\ncompared to existing models in the literature, thereby expanding the range of\npotential applications for CvHNNs.", "AI": {"tldr": "The paper introduces two innovative complex-valued Hopfield neural networks (CvHNNs) with phase and magnitude quantization, enhancing the number of states for broader application.", "motivation": "Existing complex-valued Hopfield neural networks have limited states, which restricts applications.", "method": "Two CvHNNs are proposed, employing ceiling-type activation functions in rectangular and polar coordinate systems, along with phase and magnitude quantization.", "result": "The models achieved higher state counts compared to prior CvHNN models, demonstrating improved capacity and functional range.", "conclusion": "The novel CvHNNs successfully expand application possibilities by significantly increasing state numbers through phase and magnitude quantization."}}
{"id": "2507.00598", "pdf": "https://arxiv.org/pdf/2507.00598", "abs": "https://arxiv.org/abs/2507.00598", "authors": ["Madison Cotteret", "Christopher J. Kymn", "Hugh Greatorex", "Martin Ziegler", "Elisabetta Chicca", "Friedrich T. Sommer"], "title": "High-resolution spatial memory requires grid-cell-like neural codes", "categories": ["cs.NE", "cs.AI", "cs.SC"], "comment": "14 pages, 4 figures. Supplementary material: 11 pages, 5 figures", "summary": "Continuous attractor networks (CANs) are widely used to model how the brain\ntemporarily retains continuous behavioural variables via persistent recurrent\nactivity, such as an animal's position in an environment. However, this memory\nmechanism is very sensitive to even small imperfections, such as noise or\nheterogeneity, which are both common in biological systems. Previous work has\nshown that discretising the continuum into a finite set of discrete attractor\nstates provides robustness to these imperfections, but necessarily reduces the\nresolution of the represented variable, creating a dilemma between stability\nand resolution. We show that this stability-resolution dilemma is most severe\nfor CANs using unimodal bump-like codes, as in traditional models. To overcome\nthis, we investigate sparse binary distributed codes based on random feature\nembeddings, in which neurons have spatially-periodic receptive fields. We\ndemonstrate theoretically and with simulations that such grid-cell-like codes\nenable CANs to achieve both high stability and high resolution simultaneously.\nThe model extends to embedding arbitrary nonlinear manifolds into a CAN, such\nas spheres or tori, and generalises linear path integration to integration\nalong freely-programmable on-manifold vector fields. Together, this work\nprovides a theory of how the brain could robustly represent continuous\nvariables with high resolution and perform flexible computations over\ntask-relevant manifolds.", "AI": {"tldr": "The paper explores improving the robustness and resolution of continuous attractor networks (CANs) in representing variables, using sparse grid-cell-like codes that overcome traditional CAN limitations.", "motivation": "To address the stability-resolution trade-off in CANs caused by noise and imperfections in biological systems.", "method": "Investigate sparse binary distributed codes with spatially-periodic receptive fields using random feature embeddings. Simulations and theoretical analysis validate the model.", "result": "Grid-cell-like codes allow simultaneous high stability and resolution in CANs and extend capabilities to encode nonlinear manifolds and flexible computations.", "conclusion": "The proposed model explains how the brain could robustly represent continuous variables and perform computations on complex manifolds with high resolution and robustness."}}
{"id": "2507.00004", "pdf": "https://arxiv.org/pdf/2507.00004", "abs": "https://arxiv.org/abs/2507.00004", "authors": ["Austin R. Ellis-Mohr", "Anuj K. Nayak", "Lav R. Varshney"], "title": "A Theory of Inference Compute Scaling: Reasoning through Directed Stochastic Skill Search", "categories": ["cs.LG", "cs.AI", "cs.CY", "cs.PF"], "comment": null, "summary": "Large language models (LLMs) demand considerable computational, energy, and\nfinancial resources during both training and deployment. While scaling laws for\ntraining have guided much of the field's recent progress, inference costs now\nrepresent a significant and growing component of the overall resource burden,\nparticularly for reasoning-focused models. Existing characterizations of\ncompute-optimality that consider model size, dataset size, and inference tokens\nin isolation or in fixed combinations risk overlooking more efficient operating\npoints. We introduce directed stochastic skill search (DS3), a general\nframework that represents inference as stochastic traversal over a learned\nskill graph. From a simplified yet expressive instantiation, we derive\nclosed-form expressions for task success and compute cost across a wide range\nof inference strategies -- including chain-of-thought (CoT) and tree-of-thought\n(ToT) -- enabling comparative analysis as a function of task difficulty and\nmodel capability. To that end, we extend a prior first-principles tripartite\ngraph framework of LLM training to incorporate inference, and separately bridge\nDS3 with empirical methods that characterize LLM scaling behavior. We\ntheoretically recover empirically observed patterns, including: linear accuracy\nscaling with logarithmic compute; variation in preferred inference strategies\nas a function of task difficulty and model capability; emergent behavior\nelicited by reasoning even when performance plateaus under parameter scaling;\nand both best-of-N (BoN) and majority voting behavior captured within a unified\nanalytical framework. By explicitly characterizing training-inference\ninterdependencies, our framework deepens theoretical understanding and supports\nprincipled algorithmic design and resource allocation.", "AI": {"tldr": "The paper introduces DS3, a framework for optimizing inference efficiency in large language models through a learned skill graph. It provides theoretical and empirical insights into balancing task performance, compute costs, and resource allocation.", "motivation": "The motivation is to address the rising computational, financial, and energy costs of inference in large language models, which are becoming a significant burden as reasoning-focused tasks grow in prominence.", "method": "The authors developed the Directed Stochastic Skill Search (DS3) framework, using stochastic traversal on a skill graph to model and analyze various inference strategies. They extended theoretical frameworks of LLM training to include inference and aligned it with empirical scaling behavior.", "result": "The study recovers empirical observations, such as linear accuracy scaling with log compute, emergent reasoning behavior, and optimal inference strategies varying with task difficulty and model capability. These findings align with empirical patterns in current LLM operations.", "conclusion": "By linking LLM training and inference, the DS3 framework provides deeper theoretical insights and practical tools for optimizing inference strategies, balancing task success, and resource efficiency in large language models."}}
{"id": "2507.00008", "pdf": "https://arxiv.org/pdf/2507.00008", "abs": "https://arxiv.org/abs/2507.00008", "authors": ["Hang Wu", "Hongkai Chen", "Yujun Cai", "Chang Liu", "Qingwen Ye", "Ming-Hsuan Yang", "Yiwei Wang"], "title": "DiMo-GUI: Advancing Test-time Scaling in GUI Grounding via Modality-Aware Visual Reasoning", "categories": ["cs.AI", "cs.CV", "cs.HC"], "comment": "8 pages, 6 figures", "summary": "Grounding natural language queries in graphical user interfaces (GUIs) poses\nunique challenges due to the diversity of visual elements, spatial clutter, and\nthe ambiguity of language. In this paper, we introduce DiMo-GUI, a\ntraining-free framework for GUI grounding that leverages two core strategies:\ndynamic visual grounding and modality-aware optimization. Instead of treating\nthe GUI as a monolithic image, our method splits the input into textual\nelements and iconic elements, allowing the model to reason over each modality\nindependently using general-purpose vision-language models. When predictions\nare ambiguous or incorrect, DiMo-GUI dynamically focuses attention by\ngenerating candidate focal regions centered on the model's initial predictions\nand incrementally zooms into subregions to refine the grounding result. This\nhierarchical refinement process helps disambiguate visually crowded layouts\nwithout the need for additional training or annotations. We evaluate our\napproach on standard GUI grounding benchmarks and demonstrate consistent\nimprovements over baseline inference pipelines, highlighting the effectiveness\nof combining modality separation with region-focused reasoning.", "AI": {"tldr": "DiMo-GUI is a training-free framework for grounding language queries in GUIs, leveraging dynamic visual grounding and modality-aware optimization for effective results.", "motivation": "Grounding language queries in GUIs is challenging due to diverse visual elements, complex layouts, and language ambiguity, necessitating innovative solutions.", "method": "The framework divides GUI inputs into textual and iconic elements for independent reasoning, dynamically refining ambiguous results through a hierarchical zoom-in process.", "result": "DiMo-GUI achieves consistent improvements in performance over baseline methods on standard GUI grounding benchmarks.", "conclusion": "By separating modalities and using dynamic region-focused reasoning, DiMo-GUI effectively improves language grounding in GUIs without requiring additional training or data."}}
{"id": "2507.00152", "pdf": "https://arxiv.org/pdf/2507.00152", "abs": "https://arxiv.org/abs/2507.00152", "authors": ["Ekaterina Borisova", "Fabio Barth", "Nils Feldhus", "Raia Abu Ahmad", "Malte Ostendorff", "Pedro Ortiz Suarez", "Georg Rehm", "Sebastian M\u00f6ller"], "title": "Table Understanding and (Multimodal) LLMs: A Cross-Domain Case Study on Scientific vs. Non-Scientific Data", "categories": ["cs.CL"], "comment": "TRL@ACL 2025, camera-ready version", "summary": "Tables are among the most widely used tools for representing structured data\nin research, business, medicine, and education. Although LLMs demonstrate\nstrong performance in downstream tasks, their efficiency in processing tabular\ndata remains underexplored. In this paper, we investigate the effectiveness of\nboth text-based and multimodal LLMs on table understanding tasks through a\ncross-domain and cross-modality evaluation. Specifically, we compare their\nperformance on tables from scientific vs. non-scientific contexts and examine\ntheir robustness on tables represented as images vs. text. Additionally, we\nconduct an interpretability analysis to measure context usage and input\nrelevance. We also introduce the TableEval benchmark, comprising 3017 tables\nfrom scholarly publications, Wikipedia, and financial reports, where each table\nis provided in five different formats: Image, Dictionary, HTML, XML, and LaTeX.\nOur findings indicate that while LLMs maintain robustness across table\nmodalities, they face significant challenges when processing scientific tables.", "AI": {"tldr": "The paper explores how well text-based and multimodal LLMs perform on table understanding tasks, focusing on multiple contexts and modalities.", "motivation": "Large Language Models (LLMs) have proven effective in various tasks, but their capability with tabular data hasn't been thoroughly explored.", "method": "The study uses a novel benchmark called TableEval to evaluate LLM performance across different table modalities and domains, including scientific and non-scientific contexts.", "result": "LLMs show robustness across multiple modalities but struggle significantly with scientific tables.", "conclusion": "While LLMs are versatile with table modalities, challenges remain with scientific tables, highlighting the need for further optimization in specific contexts."}}
{"id": "2507.00057", "pdf": "https://arxiv.org/pdf/2507.00057", "abs": "https://arxiv.org/abs/2507.00057", "authors": ["Thomas Valentin", "Ardi Madadi", "Gaetano Sapia", "Marcel B\u00f6hme"], "title": "Estimating Correctness Without Oracles in LLM-Based Code Generation", "categories": ["cs.PL", "cs.AI", "cs.LG", "cs.SE"], "comment": "8 pages + refs and appendix", "summary": "Generating code from natural language specifications is one of the most\nsuccessful applications of Large Language Models (LLMs). Yet, they hallucinate:\nLLMs produce outputs that may be grammatically correct but are factually\nincorrect. Without an existing, correct implementation (i.e., an oracle), can\nwe quantify how likely the generated program is correct?\n  In this paper, we propose a measure of incorrectness, called incoherence,\nthat can be estimated efficiently in the absence of an oracle and provides a\nlower bound on the error, i.e., the probability that the LLM-generated program\nfor that specification is incorrect. Our experiments demonstrate an\nextraordinary effectiveness. For the average code generation task, our\nincoherence-based methodology can automatically identify about two-thirds of\nincorrect programs without reports of false positives. In fact, an oracle-based\nevaluation of LLMs can be reliably replaced by an incoherence-based evaluation.\nIn particular, we find a very strong agreement between the ranking of LLMs by\nthe number of programs deemed correct via an oracle (pass@1) and the ranking of\nLLMs by the number of programs deemed correct via our incoherence.", "AI": {"tldr": "The paper proposes a method to measure incorrectness in LLM-generated code, called incoherence, which efficiently estimates error probabilities without the need for an oracle.", "motivation": "LLMs are effective at generating code but often produce hallucinated, factually incorrect outputs. A method to quantify correctness without relying on existing correct implementations is needed.", "method": "The authors introduce 'incoherence,' a measure of potential error in LLM-generated code. It provides a lower bound on errors and can detect incorrect outputs efficiently without using an oracle.", "result": "The incoherence-based system identified about two-thirds of incorrect code programs in experiments and showed no false positives, effectively mimicking oracle evaluations.", "conclusion": "Incoherence can reliably replace traditional oracle-based evaluations for ranking LLMs in code correctness, showing strong agreement with oracle-based rankings."}}
{"id": "2507.00002", "pdf": "https://arxiv.org/pdf/2507.00002", "abs": "https://arxiv.org/abs/2507.00002", "authors": ["Christopher James Augeri"], "title": "Hypertokens: Holographic Associative Memory in Tokenized LLMs", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "preprint as accepted to https://qnlp.ai/ - Quantum AI and NLP\n  Conference 2025", "summary": "Large Language Models (LLMs) exhibit remarkable capabilities but suffer from\napparent precision loss, reframed here as information spreading. This reframing\nshifts the problem from computational precision to an information-theoretic\ncommunication issue. We address the K:V and V:K memory problem in LLMs by\nintroducing HDRAM (Holographically Defined Random Access Memory), a symbolic\nmemory framework treating transformer latent space as a spread-spectrum\nchannel. Built upon hypertokens, structured symbolic codes integrating\nclassical error-correcting codes (ECC), holographic computing, and\nquantum-inspired search, HDRAM recovers distributed information through\nprincipled despreading. These phase-coherent memory addresses enable efficient\nkey-value operations and Grover-style search in latent space. By combining ECC\ngrammar with compressed sensing and Krylov subspace alignment, HDRAM\nsignificantly improves associative retrieval without architectural changes,\ndemonstrating how Classical-Holographic-Quantum-inspired (CHQ) principles can\nfortify transformer architectures.", "AI": {"tldr": "The paper introduces HDRAM (Holographically Defined Random Access Memory), a symbolic memory framework that addresses information spreading in Large Language Models (LLMs) through a mix of classical, holographic, and quantum-inspired techniques.", "motivation": "Large Language Models exhibit a problem of information spreading, which is reframed as an information-theoretic communication issue rather than a computational precision loss.", "method": "The authors propose HDRAM, which uses hypertokens combining error-correcting codes, holographic computing, and quantum-inspired search to address memory operations in transformer latent space as a spread-spectrum channel.", "result": "HDRAM enables efficient key-value operations and Grover-style search in latent space, enhancing associative information retrieval without changes to transformer architectures.", "conclusion": "Classical-Holographic-Quantum-inspired (CHQ) principles in HDRAM successfully address the memory problem in transformers, improving their efficiency and robustness without architectural modifications."}}
{"id": "2507.00166", "pdf": "https://arxiv.org/pdf/2507.00166", "abs": "https://arxiv.org/abs/2507.00166", "authors": ["Aaron C. Davis", "Siting Zhang", "Adalyn Meeks", "Diya Sakhrani", "Luis Carlos Sanjuan Acosta", "D. Ethan Kelley", "Emma Caldwell", "Luis Solorio", "Craig J. Goergen", "David J. Cappelleri"], "title": "Novel Design of 3D Printed Tumbling Microrobots for in vivo Targeted Drug Delivery", "categories": ["cs.RO"], "comment": null, "summary": "This paper presents innovative designs for 3D-printed tumbling microrobots,\nspecifically engineered for targeted in vivo drug delivery applications. The\nmicrorobot designs, created using stereolithography 3D printing technologies,\nincorporate permanent micro-magnets to enable actuation via a rotating magnetic\nfield actuator system. The experimental framework encompasses a series of\nlocomotion characterization tests to evaluate microrobot performance under\nvarious conditions. Testing variables include variations in microrobot\ngeometries, actuation frequencies, and environmental conditions, such as dry\nand wet environments, and temperature changes. The paper outlines designs for\nthree drug loading methods, along with comprehensive assessments thermal drug\nrelease using a focused ultrasound system, as well as biocompatibility tests.\nAnimal model testing involves tissue phantoms and in vivo rat models, ensuring\na thorough evaluation of the microrobots' performance and compatibility. The\nresults highlight the robustness and adaptability of the proposed microrobot\ndesigns, showcasing the potential for efficient and targeted in vivo drug\ndelivery. This novel approach addresses current limitations in existing\ntumbling microrobot designs and paves the way for advancements in targeted drug\ndelivery within the large intestine.", "AI": {"tldr": "Development of 3D-printed microrobots for targeted in vivo drug delivery, featuring magnetic actuation and experimental performance evaluations.", "motivation": "Current tumbling microrobot designs face limitations in delivering drugs efficiently and specifically to targeted areas within the body, prompting the need for innovative solutions.", "method": "Microrobots were designed using stereolithography 3D printing, integrated with micro-magnets for magnetic actuation, and tested via locomotion and drug delivery performance tests under varying conditions.", "result": "The microrobots exhibited robust performance and adaptability, effectively delivering drugs in dry/wet environments and proving their effectiveness and biocompatibility in animal models.", "conclusion": "These advancements demonstrate significant potential for targeted drug delivery within the large intestine, addressing limitations in existing systems."}}
{"id": "2507.00269", "pdf": "https://arxiv.org/pdf/2507.00269", "abs": "https://arxiv.org/abs/2507.00269", "authors": ["Omar Claflin"], "title": "Feature Integration Spaces: Joint Training Reveals Dual Encoding in Neural Network Representations", "categories": ["q-bio.NC", "cs.AI"], "comment": null, "summary": "Current sparse autoencoder (SAE) approaches to neural network\ninterpretability assume that activations can be decomposed through linear\nsuperposition into sparse, interpretable features. Despite high reconstruction\nfidelity, SAEs consistently fail to eliminate polysemanticity and exhibit\npathological behavioral errors. We propose that neural networks encode\ninformation in two complementary spaces compressed into the same substrate:\nfeature identity and feature integration. To test this dual encoding\nhypothesis, we develop sequential and joint-training architectures to capture\nidentity and integration patterns simultaneously. Joint training achieves 41.3%\nreconstruction improvement and 51.6% reduction in KL divergence errors. This\narchitecture spontaneously develops bimodal feature organization: low squared\nnorm features contributing to integration pathways and the rest contributing\ndirectly to the residual. Small nonlinear components (3% of parameters) achieve\n16.5% standalone improvements, demonstrating parameter-efficient capture of\ncomputational relationships crucial for behavior. Additionally, intervention\nexperiments using 2x2 factorial stimulus designs demonstrated that integration\nfeatures exhibit selective sensitivity to experimental manipulations and\nproduce systematic behavioral effects on model outputs, including significant\ninteraction effects across semantic dimensions. This work provides systematic\nevidence for (1) dual encoding in neural representations, (2) meaningful\nnonlinearly encoded feature interactions, and (3) introduces an architectural\nparadigm shift from post-hoc feature analysis to integrated computational\ndesign, establishing foundations for next-generation SAEs.", "AI": {"tldr": "This paper proposes a new approach to sparse autoencoder (SAE) design that resolves issues like polysemanticity and behavioral errors by introducing dual encoding and joint-training architectures.", "motivation": "Existing SAE approaches face challenges in eliminating polysemanticity and exhibit pathological errors despite good reconstruction fidelity. The paper aims to address these limitations.", "method": "The authors propose a dual encoding hypothesis and develop both sequential and joint-training architectures to capture feature identity and integration patterns simultaneously. They incorporate small nonlinear components for parameter efficiency and conduct intervention experiments using factorial stimulus designs.", "result": "Joint-training architectures achieve significant improvements in reconstruction (41.3%) and error reduction (51.6%). They also revealed bimodal feature organization and demonstrated meaningful nonlinear feature interaction effects on model behavior.", "conclusion": "The paper provides evidence of dual encoding in neural networks and introduces a paradigm shift towards integrated computational design, laying the groundwork for advanced SAE frameworks."}}
{"id": "2507.00347", "pdf": "https://arxiv.org/pdf/2507.00347", "abs": "https://arxiv.org/abs/2507.00347", "authors": ["Sun Ding", "Ude Enebeli", "Atilhan", "Manay", "Ryan Pua", "Kamal Kotak"], "title": "VTS-Guided AI Interaction Workflow for Business Insights", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Modern firms face a flood of dense, unstructured reports. Turning these\ndocuments into usable insights takes heavy effort and is far from agile when\nquick answers are needed. VTS-AI tackles this gap. It integrates Visual\nThinking Strategies, which emphasize evidence-based observation, linking, and\nthinking, into AI agents, so the agents can extract business insights from\nunstructured text, tables, and images at scale. The system works in three tiers\n(micro, meso, macro). It tags issues, links them to source pages, and rolls\nthem into clear action levers stored in a searchable YAML file. In tests on an\n18-page business report, VTS-AI matched the speed of a one-shot ChatGPT prompt\nyet produced richer findings: page locations, verbatim excerpts, severity\nscores, and causal links. Analysts can accept or adjust these outputs in the\nsame IDE, keeping human judgment in the loop. Early results show VTS-AI spots\nthe direction of key metrics and flags where deeper number-crunching is needed.\nNext steps include mapping narrative tags to financial ratios, adding\nfinance-tuned language models through a Model-Context Protocol, and building a\nRisk & Safety Layer to stress-test models and secure data. These upgrades aim\nto make VTS-AI a production-ready, audit-friendly tool for rapid business\nanalysis.", "AI": {"tldr": "VTS-AI uses Visual Thinking Strategies in AI to extract rich business insights from unstructured data efficiently, outperforming baseline tools in speed and detail.", "motivation": "Firms struggle with turning dense, unstructured reports into actionable insights quickly and efficiently.", "method": "VTS-AI applies a system comprising micro, meso, and macro tiers to extract, tag, and summarize data from text, tables, and images. Outputs are stored as YAML files for analysis.", "result": "In a test on an 18-page report, VTS-AI matched the speed of ChatGPT while delivering richer results like page references, verbatim quotes, severity scores, and causal links.", "conclusion": "VTS-AI shows promise for production-ready business analysis, with planned enhancements to improve narrative-financial linkages, model tuning, and data security."}}
{"id": "2507.00033", "pdf": "https://arxiv.org/pdf/2507.00033", "abs": "https://arxiv.org/abs/2507.00033", "authors": ["Mustafa Chasmai", "Gauri Jagatap", "Gouthaman KV", "Grant Van Horn", "Subhransu Maji", "Andrea Fanelli"], "title": "Moment Sampling in Video LLMs for Long-Form Video QA", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Workshop on Video Large Language Models (VidLLMs) at CVPR 2025", "summary": "Recent advancements in video large language models (Video LLMs) have\nsignificantly advanced the field of video question answering (VideoQA). While\nexisting methods perform well on short videos, they often struggle with\nlong-range reasoning in longer videos. To scale Video LLMs for longer video\ncontent, frame sub-sampling (selecting frames at regular intervals) is commonly\nused. However, this approach is suboptimal, often leading to the loss of\ncrucial frames or the inclusion of redundant information from multiple similar\nframes. Missing key frames impairs the model's ability to answer questions\naccurately, while redundant frames lead the model to focus on irrelevant video\nsegments and increase computational resource consumption. In this paper, we\ninvestigate the use of a general-purpose text-to-video moment retrieval model\nto guide the frame sampling process. We propose \"moment sampling\", a novel,\nmodel-agnostic approach that enables the model to select the most relevant\nframes according to the context of the question. Specifically, we employ a\nlightweight moment retrieval model to prioritize frame selection. By focusing\non the frames most pertinent to the given question, our method enhances\nlong-form VideoQA performance in Video LLMs. Through extensive experiments on\nfour long-form VideoQA datasets, using four state-of-the-art Video LLMs, we\ndemonstrate the effectiveness of the proposed approach.", "AI": {"tldr": "The paper introduces 'moment sampling,' a method to address challenges in VideoQA for long videos by using a moment retrieval model to select crucial frames instead of uniform sub-sampling.", "motivation": "Existing Video LLMs struggle with long-range reasoning in lengthy videos due to inefficiencies in conventional frame sub-sampling, which either misses critical frames or retains redundant ones.", "method": "The authors employ a lightweight text-to-video moment retrieval model to guide the selection of the most relevant frames for a given question, improving the efficiency and relevance of frame sampling.", "result": "The experiments reveal that 'moment sampling' significantly enhances performance across four long-form VideoQA datasets using four state-of-the-art Video LLMs.", "conclusion": "The proposed moment sampling approach effectively addresses limitations in long-form VideoQA, improving accuracy and reducing unnecessary computational load."}}
{"id": "2507.00260", "pdf": "https://arxiv.org/pdf/2507.00260", "abs": "https://arxiv.org/abs/2507.00260", "authors": ["Jin-Hong Du", "Kathryn Roeder", "Larry Wasserman"], "title": "Disentangled Feature Importance", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.ME", "stat.TH"], "comment": "26 main and 29 supplementary pages", "summary": "Feature importance quantification faces a fundamental challenge: when\npredictors are correlated, standard methods systematically underestimate their\ncontributions. We prove that major existing approaches target identical\npopulation functionals under squared-error loss, revealing why they share this\ncorrelation-induced bias.\n  To address this limitation, we introduce \\emph{Disentangled Feature\nImportance (DFI)}, a nonparametric generalization of the classical $R^2$\ndecomposition via optimal transport. DFI transforms correlated features into\nindependent latent variables using a transport map, eliminating correlation\ndistortion. Importance is computed in this disentangled space and attributed\nback through the transport map's sensitivity. DFI provides a principled\ndecomposition of importance scores that sum to the total predictive variability\nfor latent additive models and to interaction-weighted functional ANOVA\nvariances more generally, under arbitrary feature dependencies.\n  We develop a comprehensive semiparametric theory for DFI. For general\ntransport maps, we establish root-$n$ consistency and asymptotic normality of\nimportance estimators in the latent space, which extends to the original\nfeature space for the Bures-Wasserstein map. Notably, our estimators achieve\nsecond-order estimation error, which vanishes if both regression function and\ntransport map estimation errors are $o_{\\mathbb{P}}(n^{-1/4})$. By design, DFI\navoids the computational burden of repeated submodel refitting and the\nchallenges of conditional covariate distribution estimation, thereby achieving\ncomputational efficiency.", "AI": {"tldr": "The paper introduces Disentangled Feature Importance (DFI), a method addressing bias in feature importance quantification caused by correlated predictors. It uses optimal transport to disentangle features and attributes importance scores, offering a computationally efficient and theoretically robust approach.", "motivation": "Feature importance quantification methods struggle with biased contributions when predictors are correlated. The study aims to address this fundamental limitation and proposes a new approach to handle such correlations.", "method": "The authors propose DFI, which uses optimal transport to convert correlated features into independent latent variables. Importance is computed in this disentangled space and then mapped back to the original feature space via sensitivity analysis of the transport map.", "result": "DFI eliminates correlation distortion and provides a consistent and asymptotically normal estimation of importance scores. It achieves computational efficiency and second-order error rates when estimation errors are small.", "conclusion": "DFI addresses the limitations of existing feature importance methods under correlated predictors, offering a principled, computationally efficient, and theoretically sound approach to quantify feature contributions."}}
{"id": "2507.00217", "pdf": "https://arxiv.org/pdf/2507.00217", "abs": "https://arxiv.org/abs/2507.00217", "authors": ["Tiancheng Chen", "Ales Kubicek", "Langwen Huang", "Torsten Hoefler"], "title": "CrossPipe: Towards Optimal Pipeline Schedules for Cross-Datacenter Training", "categories": ["cs.DC"], "comment": "USENIX ATC '25", "summary": "Training large language models (LLMs) now requires resources that exceed a\nsingle datacenter, making cross-datacenter strategies increasingly crucial. We\npresent CrossPipe, a framework designed to optimize model training across\ngeographically distributed datacenters by explicitly modeling and mitigating\nthe impact of network latency and limited bandwidth. It enables unified\nanalysis and optimization incorporating both pipeline parallelism (PP) and\nopportunities for overlapping data parallelism (DP) communication. CrossPipe\ngenerates optimized pipeline schedules using either solver-based optimal or\nfast near-optimal greedy algorithms, built upon a flexible execution engine\nthat separates scheduling logic from communication details. Our evaluation\nshows that CrossPipe reduces training time by up to 33.6\\% compared to\ntraditional pipeline schedules under identical memory constraints. When memory\nconstraints are relaxed, CrossPipe maintains strong performance despite\ncommunication delays, approaching the efficiency of idealized schedules without\ndelays. CrossPipe offers improved scalability and resource utilization,\nparticularly in environments with high network latency or limited bandwidth.", "AI": {"tldr": "CrossPipe is a framework to optimize the training of large language models across datacenters, addressing the challenges of network latency and bandwidth constraints, achieving up to 33.6% reduction in training time.", "motivation": "The paper addresses the challenge of training large language models, which often requires distributed datacenters due to the resource intensity, and aims to mitigate the effects of network latency and bandwidth limitations.", "method": "CrossPipe integrates pipeline parallelism and overlapping data parallelism, using solver-based or greedy algorithms for optimized pipeline scheduling and a flexible execution engine to improve efficiency.", "result": "The evaluation shows CrossPipe reduces training time by up to 33.6%, maintains strong performance under relaxed memory constraints, and improves scalability in environments with network limitations.", "conclusion": "CrossPipe effectively optimizes training across distributed environments, offering significant time savings, scalability, and efficiency despite challenges with network latency and bandwidth."}}
{"id": "2507.00367", "pdf": "https://arxiv.org/pdf/2507.00367", "abs": "https://arxiv.org/abs/2507.00367", "authors": ["Yeonsoo Jeon", "Mattan Erez", "Michael Orshansky"], "title": "Presto: Hardware Acceleration of Ciphers for Hybrid Homomorphic Encryption", "categories": ["cs.AR", "cs.CR"], "comment": null, "summary": "Hybrid Homomorphic Encryption (HHE) combines symmetric key and homomorphic\nencryption to reduce ciphertext expansion crucial in client-server deployments\nof HE. Special symmetric ciphers, amenable to efficient HE evaluation, have\nbeen developed. Their client-side deployment calls for performant and\nenergy-efficient implementation, and in this paper we develop and evaluate\nhardware accelerators for the two known CKKS-targeting HHE ciphers, HERA and\nRubato.\n  We design vectorized and overlapped functional modules. The design exploits\ntransposition-invariance property of the MixColumns and MixRows function and\nalternates the order of intermediate state to eliminate bubbles in stream key\ngeneration, improving latency and throughput. We decouple the RNG and key\ncomputation phases to hide the latency of RNG and to reduce the critical path\nin FIFOs, achieving higher operating frequency.\n  We implement the accelerator on an AMD Virtex UltraScale+ FPGA. Both Rubato\nand HERA achieve a 6x improvement in throughput compared to the software\nimplementation. In terms of latency, Rubato achieves a 5x reduction, while HERA\nachieves a 3x reduction. Additionally, our hardware implementations reduce\nenergy consumption by 75x for Rubato and 47x for HERA compared to their\nsoftware implementation.", "AI": {"tldr": "This paper focuses on creating hardware accelerators for the CKKS-targeting HHE ciphers HERA and Rubato to enhance performance and energy efficiency.", "motivation": "To address the challenges of ciphertext expansion and performance limitations in the client-side deployment of Hybrid Homomorphic Encryption (HHE) using the ciphers HERA and Rubato.", "method": "Developed hardware accelerators using an AMD Virtex UltraScale+ FPGA with designs that utilize vectorized and overlapped modules, exploit the transposition-invariance property, and decouple RNG and key computations.", "result": "Rubato achieved a 6x throughput improvement, 5x latency reduction, and 75x energy efficiency. HERA achieved a 6x throughput improvement, 3x latency reduction, and 47x energy efficiency.", "conclusion": "The proposed hardware accelerators significantly improve the performance and energy efficiency of Rubato and HERA ciphers, making them more suitable for client-side deployment in HHE."}}
{"id": "2507.00022", "pdf": "https://arxiv.org/pdf/2507.00022", "abs": "https://arxiv.org/abs/2507.00022", "authors": ["Zehao Wang"], "title": "GLU Attention Improve Transformer", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.NE"], "comment": "4 pages 4 figures", "summary": "Gated Linear Units (GLU) have shown great potential in enhancing neural\nnetwork performance. In this paper, I introduce a novel attention mechanism\ncalled GLU Attention, which introduces nonlinearity into the values of\nAttention. My experiments demonstrate that GLU Attention improves both model\nperformance and convergence speed across text and vision modalities with zero\nadditional parameters and negligible computational costs. GLU Attention is\nlightweight and can seamlessly integrate with other technologies, such as Flash\nAttention, Rotary Position Embedding (RoPE), and various Multi-Head Attention\n(MHA) variants such as Grouped-Query Attention (GQA). This project is\nopen-sourced at github.", "AI": {"tldr": "This paper introduces GLU Attention, a novel attention mechanism that leverages gated linear units for improved model performance and convergence speed in various modalities without additional parameters or significant computational costs.", "motivation": "The paper aims to enhance neural network performance and convergence speed by innovating within attention mechanisms using nonlinearity through gated linear units.", "method": "GLU Attention introduces nonlinearity into the values of Attention, seamlessly integrating with other advanced technologies such as Flash Attention, Rotary Position Embedding, and Multi-Head Attention variants.", "result": "Experiments show that GLU Attention improves performance and convergence speed in text and vision tasks while maintaining computational efficiency and zero parameter overhead.", "conclusion": "GLU Attention is a lightweight, efficient, and integrative attention mechanism that benefits neural network applications, open-sourced for broader adoption and research."}}
{"id": "2507.00264", "pdf": "https://arxiv.org/pdf/2507.00264", "abs": "https://arxiv.org/abs/2507.00264", "authors": ["Isabella Basso do Amaral", "Renato Cordeiro Ferreira", "Alfredo Goldman"], "title": "Rust vs. C for Python Libraries: Evaluating Rust-Compatible Bindings Toolchains", "categories": ["cs.PL", "cs.DC", "cs.PF", "cs.SE", "D.2.13; D.2.8; D.3.3; B.8"], "comment": "10 pages, 27 figures (1 diagram, 4 graphs, 9 tables, 13 code\n  listings), submitted to SBAC-PAD 2025", "summary": "The Python programming language is best known for its syntax and scientific\nlibraries, but it is also notorious for its slow interpreter. Optimizing\ncritical sections in Python entails special knowledge of the binary\ninteractions between programming languages, and can be cumbersome to interface\nmanually, with implementers often resorting to convoluted third-party\nlibraries. This comparative study evaluates the performance and ease of use of\nthe PyO3 Python bindings toolchain for Rust against ctypes and cffi. By using\nRust tooling developed for Python, we can achieve state-of-the-art performance\nwith no concern for API compatibility.", "AI": {"tldr": "The paper compares Python binding tools (PyO3) for Rust with ctypes and cffi, focusing on performance and ease of use.", "motivation": "Python's slow interpreter and the difficulty of optimizing critical code sections call for effective binding tools for languages like Rust.", "method": "The study conducts a comparative evaluation of PyO3, ctypes, and cffi based on performance and usability for Python-Rust integration.", "result": "Rust tooling via PyO3 achieves state-of-the-art performance without needing concerns over API compatibility.", "conclusion": "Using PyO3 provides a superior solution to optimize Python with Rust, overcoming traditional challenges with other binding methods."}}
{"id": "2507.00041", "pdf": "https://arxiv.org/pdf/2507.00041", "abs": "https://arxiv.org/abs/2507.00041", "authors": ["Varun Mannam", "Fang Wang", "Chaochun Liu", "Xin Chen"], "title": "TalentMine: LLM-Based Extraction and Question-Answering from Multimodal Talent Tables", "categories": ["cs.AI", "cs.CV", "cs.IR"], "comment": "Submitted to KDD conference, workshop: Talent and Management\n  Computing (TMC 2025), https://tmcworkshop.github.io/2025/", "summary": "In talent management systems, critical information often resides in complex\ntabular formats, presenting significant retrieval challenges for conventional\nlanguage models. These challenges are pronounced when processing Talent\ndocumentation that requires precise interpretation of tabular relationships for\naccurate information retrieval and downstream decision-making. Current table\nextraction methods struggle with semantic understanding, resulting in poor\nperformance when integrated into retrieval-augmented chat applications. This\npaper identifies a key bottleneck - while structural table information can be\nextracted, the semantic relationships between tabular elements are lost,\ncausing downstream query failures. To address this, we introduce TalentMine, a\nnovel LLM-enhanced framework that transforms extracted tables into semantically\nenriched representations. Unlike conventional approaches relying on CSV or text\nlinearization, our method employs specialized multimodal reasoning to preserve\nboth structural and semantic dimensions of tabular data. Experimental\nevaluation across employee benefits document collections demonstrates\nTalentMine's superior performance, achieving 100% accuracy in query answering\ntasks compared to 0% for standard AWS Textract extraction and 40% for AWS\nTextract Visual Q&A capabilities. Our comparative analysis also reveals that\nthe Claude v3 Haiku model achieves optimal performance for talent management\napplications. The key contributions of this work include (1) a systematic\nanalysis of semantic information loss in current table extraction pipelines,\n(2) a novel LLM-based method for semantically enriched table representation,\n(3) an efficient integration framework for retrieval-augmented systems as\nend-to-end systems, and (4) comprehensive benchmarks on talent analytics tasks\nshowing substantial improvements across multiple categories.", "AI": {"tldr": "Current table extraction methods struggle with semantic understanding of tabular data, leading to poor retrieval performance. TalentMine, a novel LLM-based framework, addresses this by transforming tables into semantically enriched representations, showing superior results in talent management tasks.", "motivation": "The need to overcome the limitations of existing table extraction methods, which fail to capture semantic relationships in tabular data, causing issues in talent management applications where precision and decision-making are crucial.", "method": "TalentMine employs a novel LLM-driven framework that utilizes multimodal reasoning to efficiently transform tables into semantically enriched representations, preserving both structural and semantic dimensions of the data.", "result": "TalentMine achieved 100% accuracy in query-answering tasks, significantly outperforming AWS Textract extraction (0%) and AWS Textract Visual Q&A (40%) on employee benefits documentation. The Claude v3 Haiku model was identified as the best-performing model.", "conclusion": "The study presents TalentMine as an effective solution for semantically understanding and retrieving information from tabular data in talent management systems. It highlights its transformational impact compared to existing solutions and its superior accuracy and efficiency."}}
{"id": "2507.00163", "pdf": "https://arxiv.org/pdf/2507.00163", "abs": "https://arxiv.org/abs/2507.00163", "authors": ["Ari Holtzman", "Chenhao Tan"], "title": "Prompting as Scientific Inquiry", "categories": ["cs.CL"], "comment": null, "summary": "Prompting is the primary method by which we study and control large language\nmodels. It is also one of the most powerful: nearly every major capability\nattributed to LLMs-few-shot learning, chain-of-thought, constitutional AI-was\nfirst unlocked through prompting. Yet prompting is rarely treated as science\nand is frequently frowned upon as alchemy. We argue that this is a category\nerror. If we treat LLMs as a new kind of complex and opaque organism that is\ntrained rather than programmed, then prompting is not a workaround: it is\nbehavioral science. Mechanistic interpretability peers into the neural\nsubstrate, prompting probes the model in its native interface: language. We\ncontend that prompting is not inferior, but rather a key component in the\nscience of LLMs.", "AI": {"tldr": "The paper argues that prompting should be considered a legitimate scientific approach and key component in studying large language models (LLMs).", "motivation": "To address the misconception that prompting LLMs is less scientific and to argue for its validity as a behavioral science approach to studying these models.", "method": "The paper reframes prompting as a behavioral science approach that investigates LLMs through their native language interface rather than dismissing it as a workaround or unscientific.", "result": "Prompting is positioned as a powerful and integral method for unlocking and understanding the capabilities of LLMs, such as few-shot learning and chain-of-thought prompting.", "conclusion": "Prompting is a valid and necessary scientific method, akin to behavioral science, for exploring and understanding LLMs as complex, trained organisms."}}
{"id": "2507.00003", "pdf": "https://arxiv.org/pdf/2507.00003", "abs": "https://arxiv.org/abs/2507.00003", "authors": ["Eyhab Al-Masri"], "title": "Deciding When Not to Decide: Indeterminacy-Aware Intrusion Detection with NeutroSENSE", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.NI"], "comment": null, "summary": "This paper presents NeutroSENSE, a neutrosophic-enhanced ensemble framework\nfor interpretable intrusion detection in IoT environments. By integrating\nRandom Forest, XGBoost, and Logistic Regression with neutrosophic logic, the\nsystem decomposes prediction confidence into truth (T), falsity (F), and\nindeterminacy (I) components, enabling uncertainty quantification and\nabstention. Predictions with high indeterminacy are flagged for review using\nboth global and adaptive, class-specific thresholds. Evaluated on the IoT-CAD\ndataset, NeutroSENSE achieved 97% accuracy, while demonstrating that\nmisclassified samples exhibit significantly higher indeterminacy (I = 0.62)\nthan correct ones (I = 0.24). The use of indeterminacy as a proxy for\nuncertainty enables informed abstention and targeted review-particularly\nvaluable in edge deployments. Figures and tables validate the correlation\nbetween I-scores and error likelihood, supporting more trustworthy,\nhuman-in-the-loop AI decisions. This work shows that neutrosophic logic\nenhances both accuracy and explainability, providing a practical foundation for\ntrust-aware AI in edge and fog-based IoT security systems.", "AI": {"tldr": "The paper introduces NeutroSENSE, a framework integrating neutrosophic logic with machine learning to enhance accuracy, interpretability, and uncertainty quantification in intrusion detection for IoT systems.", "motivation": "The motivation is to improve accuracy, explainability, and trust in intrusion detection mechanisms for IoT environments, where uncertainty and adaptability are crucial.", "method": "NeutroSENSE integrates Random Forest, XGBoost, and Logistic Regression with neutrosophic logic to decompose prediction confidence into truth, falsity, and indeterminacy. It flags high-indeterminacy predictions for review and uses thresholds to quantify uncertainty.", "result": "NeutroSENSE achieved 97% accuracy on the IoT-CAD dataset. Misclassified samples showed significantly higher indeterminacy scores compared to correct predictions, highlighting its effectiveness in uncertainty quantification.", "conclusion": "By incorporating neutrosophic logic, NeutroSENSE improves prediction accuracy, trustworthiness, and explainability, enabling targeted reviews valuable in edge-based IoT security systems."}}
{"id": "2507.00190", "pdf": "https://arxiv.org/pdf/2507.00190", "abs": "https://arxiv.org/abs/2507.00190", "authors": ["Satoshi Tanaka", "Koji Minoda", "Fumiya Watanabe", "Takamasa Horibe"], "title": "Rethink 3D Object Detection from Physical World", "categories": ["cs.RO", "cs.CV"], "comment": "15 pages, 10 figures", "summary": "High-accuracy and low-latency 3D object detection is essential for autonomous\ndriving systems. While previous studies on 3D object detection often evaluate\nperformance based on mean average precision (mAP) and latency, they typically\nfail to address the trade-off between speed and accuracy, such as 60.0 mAP at\n100 ms vs 61.0 mAP at 500 ms. A quantitative assessment of the trade-offs\nbetween different hardware devices and accelerators remains unexplored, despite\nbeing critical for real-time applications. Furthermore, they overlook the\nimpact on collision avoidance in motion planning, for example, 60.0 mAP leading\nto safer motion planning or 61.0 mAP leading to high-risk motion planning. In\nthis paper, we introduce latency-aware AP (L-AP) and planning-aware AP (P-AP)\nas new metrics, which consider the physical world such as the concept of time\nand physical constraints, offering a more comprehensive evaluation for\nreal-time 3D object detection. We demonstrate the effectiveness of our metrics\nfor the entire autonomous driving system using nuPlan dataset, and evaluate 3D\nobject detection models accounting for hardware differences and accelerators.\nWe also develop a state-of-the-art performance model for real-time 3D object\ndetection through latency-aware hyperparameter optimization (L-HPO) using our\nmetrics. Additionally, we quantitatively demonstrate that the assumption \"the\nmore point clouds, the better the recognition performance\" is incorrect for\nreal-time applications and optimize both hardware and model selection using our\nmetrics.", "AI": {"tldr": "The paper introduces new metrics for evaluating 3D object detection systems, which account for latency and planning impact, essential for real-time autonomous driving.", "motivation": "Existing evaluations for 3D object detection overlook trade-offs between speed and accuracy, hardware choices, and real-world impact like collision avoidance.", "method": "The authors proposed Latency-aware AP (L-AP) and Planning-aware AP (P-AP) metrics, applied optimization techniques, and evaluated their model on the nuPlan dataset.", "result": "Their proposed metrics improved real-time evaluation performance of 3D object detection systems and uncovered limitations in traditional assumptions regarding point cloud data.", "conclusion": "The paper emphasizes optimizing trade-offs between hardware, recognition, and latency for superior real-time autonomous driving systems by leveraging their new metrics."}}
{"id": "2507.00473", "pdf": "https://arxiv.org/pdf/2507.00473", "abs": "https://arxiv.org/abs/2507.00473", "authors": ["Moo K. Chung", "Kim M. Dalton", "Daniel J. Kelley", "Richard J. Davidson"], "title": "Mapping Brain-Behavior Correlations in Autism Using Heat Kernel Smoothing", "categories": ["q-bio.NC"], "comment": "Originally published in Quantitative Bio-Science 30(2), 75-83 (2011)", "summary": "This paper presents a streamlined image analysis framework for correlating\nbehavioral measures to anatomical measures on the cortex and detecting the\nregions of abnormal brain-behavior correlates. We correlated a facial emotion\ndiscrimination task score and its response time to cortical thickness\nmeasurements in a group of high functioning autistic subjects. Many previous\ncorrelation studies in brain imaging neglect to account for unwanted age effect\nand other variables and the subsequent statistical parametric maps may report\nspurious results. We demonstrate that the partial correlation mapping strategy\nproposed here can remove the effect of age and global cortical area difference\neffectively while localizing the regions of high correlation difference. The\nadvantage of the proposed correlation mapping strategy over the general linear\nmodel framework is that we can directly visualize more intuitive correlation\nmeasures across the cortex in each group.", "AI": {"tldr": "The paper introduces a framework for analyzing brain-behavior correlation while minimizing the effects of variables like age.", "motivation": "To address issues in brain imaging studies that fail to account for confounding variables such as age, potentially leading to inaccurate analyses.", "method": "Developed a partial correlation mapping strategy that removes unwanted effects like age and global cortical area differences to improve localization of correlation differences.", "result": "Demonstrated the ability of the proposed method to effectively identify regions with significant brain-behavior correlations in high-functioning autistic individuals.", "conclusion": "The new framework enhances visualization and accuracy in detecting correlation differences, outperforming traditional linear models in clarity and intuitiveness."}}
{"id": "2507.00352", "pdf": "https://arxiv.org/pdf/2507.00352", "abs": "https://arxiv.org/abs/2507.00352", "authors": ["Abanoub E. Abdelmalak", "Mohamed A. Elsayed", "David Abercrombie", "Ilhami Torunoglu"], "title": "An AST-guided LLM Approach for SVRF Code Synthesis", "categories": ["cs.SE", "cs.AI", "cs.ET"], "comment": "9 Pages, 5 Figures, 2 Tables", "summary": "Standard Verification Rule Format (SVRF) is essential for semiconductor\napplications like Design Rule Check (DRC), Layout Versus Schematic (LVS), and\nOptical Proximity Correction (OPC) and it faces challenges as advancing nodes\ncreate complex design rules that renders traditional SVRF development\nineffective and highlight an expertise gap. This paper introduces a novel\nmethodology integrating Abstract Syntax Tree (AST) embedding and\nRetrieval-Augmented Generation (RAG) for enhanced SVRF code synthesis, ensuring\nsemantic accuracy and error minimization through structural validation with\ndomain-specific insights for precise code generation.\n  We evaluate different T5-based models and propose an innovative SVRF-specific\nscoring framework that complements standard metrics like BLEU and ROUGE-L. In\nour approach, AST provides rigorous structural validation, while RAG infuses\nrelevant domain knowledge, effectively enhancing the code generation workflow.\n  Testing on a comprehensive benchmark of 740 DRC rule implementations, our\nmethodology demonstrates up to a 40\\% improvement in code generation accuracy\ncompared to basic text-based fine-tuning process. This fusion of industry\nexpertise with advanced coding strategies not only optimizes SVRF development\nunder limited dataset constraints but also creates a more intuitive and\nefficient coding environment. Consequently, users can rapidly iterate through\ndesign cycles, reduce manual error correction, and significantly improve\noverall productivity.", "AI": {"tldr": "This paper presents a hybrid method combining AST embedding and RAG to improve SVRF code generation, achieving up to a 40% enhancement in accuracy.", "motivation": "The complexity of SVRF development in semiconductor applications has grown due to advancing nodes and intricate design rules, highlighting gaps in traditional methods and expertise.", "method": "The authors integrate Abstract Syntax Tree (AST) embedding with Retrieval-Augmented Generation (RAG), validate code generation structurally, and propose a new scoring framework tailored to SVRF.", "result": "The new methodology, tested on 740 DRC rules, improved code generation accuracy by up to 40% compared to conventional text-based fine-tuning.", "conclusion": "This approach efficiently bridges expertise gaps, optimizes SVRF code synthesis, reduces errors, and accelerates design cycles, leading to improved productivity."}}
{"id": "2507.00042", "pdf": "https://arxiv.org/pdf/2507.00042", "abs": "https://arxiv.org/abs/2507.00042", "authors": ["Xinrun Xu", "Jianwen Yang", "Qiuhong Zhang", "Zhanbiao Lian", "Zhiming Ding", "Shan Jiang"], "title": "Catastrophic Forgetting Mitigation via Discrepancy-Weighted Experience Replay", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "ICANN 2025", "summary": "Continually adapting edge models in cloud-edge collaborative object detection\nfor traffic monitoring suffers from catastrophic forgetting, where models lose\npreviously learned knowledge when adapting to new data distributions. This is\nespecially problematic in dynamic traffic environments characterised by\nperiodic variations (e.g., day/night, peak hours), where past knowledge remains\nvaluable. Existing approaches like experience replay and visual prompts offer\nsome mitigation, but struggle to effectively prioritize and leverage historical\ndata for optimal knowledge retention and adaptation. Specifically, simply\nstoring and replaying all historical data can be inefficient, while treating\nall historical experiences as equally important overlooks their varying\nrelevance to the current domain. This paper proposes ER-EMU, an edge model\nupdate algorithm based on adaptive experience replay, to address these\nlimitations. ER-EMU utilizes a limited-size experience buffer managed using a\nFirst-In-First-Out (FIFO) principle, and a novel Domain Distance Metric-based\nExperience Selection (DDM-ES) algorithm. DDM-ES employs the multi-kernel\nmaximum mean discrepancy (MK-MMD) to quantify the dissimilarity between target\ndomains, prioritizing the selection of historical data that is most dissimilar\nto the current target domain. This ensures training diversity and facilitates\nthe retention of knowledge from a wider range of past experiences, while also\npreventing overfitting to the new domain. The experience buffer is also updated\nusing a simple random sampling strategy to maintain a balanced representation\nof previous domains. Experiments on the Bellevue traffic video dataset,\ninvolving repeated day/night cycles, demonstrate that ER-EMU consistently\nimproves the performance of several state-of-the-art cloud-edge collaborative\nobject detection frameworks.", "AI": {"tldr": "The paper introduces ER-EMU, an adaptive experience replay algorithm for cloud-edge object detection models to mitigate catastrophic forgetting, especially in dynamic traffic environments.", "motivation": "To address catastrophic forgetting in cloud-edge collaborative object detection models for traffic monitoring, particularly in dynamic settings like day/night or peak-hour cycles, where past knowledge is critical.", "method": "The proposed ER-EMU algorithm uses an adaptive experience buffer managed by a FIFO principle and a Domain Distance Metric-based Experience Selection (DDM-ES) utilizing multi-kernel maximum mean discrepancy (MK-MMD) to prioritize diverse and relevant historical data.", "result": "Experiments on the Bellevue traffic video dataset demonstrate improved performance of state-of-the-art cloud-edge collaborative object detection frameworks across repeated day/night cycles when using ER-EMU.", "conclusion": "ER-EMU effectively mitigates catastrophic forgetting by prioritizing and leveraging historical data, ensuring diverse training and preventing overfitting to new domain data, thus improving model adaptability and performance in dynamic environments."}}
{"id": "2507.00298", "pdf": "https://arxiv.org/pdf/2507.00298", "abs": "https://arxiv.org/abs/2507.00298", "authors": ["Arkaprabha Ganguli", "Nesar Ramachandra", "Julie Bessac", "Emil Constantinescu"], "title": "Enhancing Interpretability in Generative Modeling: Statistically Disentangled Latent Spaces Guided by Generative Factors in Scientific Datasets", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "This study addresses the challenge of statistically extracting generative\nfactors from complex, high-dimensional datasets in unsupervised or\nsemi-supervised settings. We investigate encoder-decoder-based generative\nmodels for nonlinear dimensionality reduction, focusing on disentangling\nlow-dimensional latent variables corresponding to independent physical factors.\nIntroducing Aux-VAE, a novel architecture within the classical Variational\nAutoencoder framework, we achieve disentanglement with minimal modifications to\nthe standard VAE loss function by leveraging prior statistical knowledge\nthrough auxiliary variables. These variables guide the shaping of the latent\nspace by aligning latent factors with learned auxiliary variables. We validate\nthe efficacy of Aux-VAE through comparative assessments on multiple datasets,\nincluding astronomical simulations.", "AI": {"tldr": "The paper proposes Aux-VAE, a new approach for disentangling generative factors in complex data using minimal changes to standard VAE architecture with auxiliary variables.", "motivation": "The motivation is to enable the disentangling of low-dimensional latent variables corresponding to independent factors in high-dimensional, complex datasets, especially in unsupervised settings.", "method": "The method involves modifying the standard Variational Autoencoder (VAE) by introducing auxiliary variables to guide the latent space and align factors with prior statistical knowledge.", "result": "Aux-VAE effectively disentangles latent variables, as demonstrated through comparative analyses on multiple datasets, including astronomical simulations.", "conclusion": "The study concludes that Aux-VAE provides a simple yet effective means to achieve disentanglement in complex datasets with minimal changes to the standard VAE framework."}}
{"id": "2507.00418", "pdf": "https://arxiv.org/pdf/2507.00418", "abs": "https://arxiv.org/abs/2507.00418", "authors": ["Mohammad Firas Sada", "John J. Graham", "Elham E Khoda", "Mahidhar Tatineni", "Dmitry Mishin", "Rajesh K. Gupta", "Rick Wagner", "Larry Smarr", "Thomas A. DeFanti", "Frank W\u00fcrthwein"], "title": "Serving LLMs in HPC Clusters: A Comparative Study of Qualcomm Cloud AI 100 Ultra and High-Performance GPUs", "categories": ["cs.DC", "cs.AI"], "comment": "To appear in Proceedings of the Practice and Experience in Advanced\n  Research Computing (PEARC '25)", "summary": "This study presents a benchmarking analysis of the Qualcomm Cloud AI 100\nUltra (QAic) accelerator for large language model (LLM) inference, evaluating\nits energy efficiency (throughput per watt) and performance against leading\nNVIDIA (A100, H200) and AMD (MI300A) GPUs within the National Research Platform\n(NRP) ecosystem. A total of 15 open-source LLMs, ranging from 117 million to 90\nbillion parameters, are served using the vLLM framework. The QAic inference\ncards appears to be energy efficient and performs well in the energy efficiency\nmetric in most cases. The findings offer insights into the potential of the\nQualcomm Cloud AI 100 Ultra for high-performance computing (HPC) applications\nwithin the National Research Platform (NRP).", "AI": {"tldr": "The study benchmarked Qualcomm Cloud AI 100 Ultra accelerator's energy efficiency and performance against NVIDIA and AMD GPUs for LLM inference.", "motivation": "To evaluate the energy efficiency and performance of Qualcomm Cloud AI 100 Ultra for large language model inference in comparison to major GPU competitors.", "method": "The study used the vLLM framework to serve 15 open-source LLMs (117M\u201390B parameters) and compared the energy efficiency and performance of Qualcomm's accelerator with NVIDIA (A100, H200) and AMD (MI300A) GPUs.", "result": "The QAic inference cards demonstrated strong energy efficiency metrics, generally outperforming competitors in this aspect, in most cases.", "conclusion": "The results indicate Qualcomm Cloud AI 100 Ultra is a viable and efficient option for high-performance computing tasks within the NRP ecosystem."}}
{"id": "2507.00642", "pdf": "https://arxiv.org/pdf/2507.00642", "abs": "https://arxiv.org/abs/2507.00642", "authors": ["Runkai Li", "Jia Xiong", "Xiuyuan He", "Jieru Zhao", "Qiang Xu", "Xi Wang"], "title": "ChatHLS: Towards Systematic Design Automation and Optimization for High-Level Synthesis", "categories": ["cs.AR"], "comment": null, "summary": "The increasing complexity of computational demands has accelerated the\nadoption of domain-specific accelerators, yet traditional hardware design\nmethodologies remain constrained by prolonged development and verification\ncycles. High-Level Synthesis (HLS) bridges the gap between software and\nhardware by enabling hardware design from high-level programming languages.\nHowever, its widespread adoption is hindered by strict coding constraints and\nintricate hardware-specific optimizations, creating significant obstacles for\ndevelopers. Recent advancements in Large Language Models (LLMs) demonstrate\nsubstantial potential in hardware design automation. However, their\neffectiveness is limited by the scarcity of high-quality datasets, particularly\nin the context of HLS. To address these challenges, we introduce ChatHLS, an\nagile HLS design automation and optimization workflow that leverages fine-tuned\nLLMs integrated within a multi-agent framework for error correction and design\noptimization. Our extensive evaluations reveal that ChatHLS achieves an average\nrepair pass rate of 82.7% over 612 test cases, outperforming the GPT-4o and\nLlama3-8B by 19.1% and 63.0%, respectively. Furthermore, ChatHLS delivers\nperformance enhancements ranging from 1.9$\\times$ to 14.8$\\times$ upon\nresource-constrained kernels. By enabling sophisticated optimization reasoning\nwithin practical computational budgets, ChatHLS attains a 4.9$\\times$ geometric\nmean speedup compared to state-of-the-art DSL-based approaches. These results\nunderscore the potential of ChatHLS in substantially expediting hardware\ndevelopment cycles while maintaining rigorous standards of design reliability\nand optimization quality.", "AI": {"tldr": "ChatHLS is a design automation workflow for High-Level Synthesis using fine-tuned Large Language Models within a multi-agent framework. It achieves high repair rates and performance enhancements over existing methods.", "motivation": "Hardware development faces challenges due to prolonged cycles, strict coding rules, and complex optimizations, which hinder developer productivity. High-Level Synthesis adoption struggles, while the rise of Large Language Models offers potential for automation.", "method": "The paper presents ChatHLS, combining fine-tuned Large Language Models integrated into a multi-agent framework for optimizing hardware design and automating error correction.", "result": "ChatHLS achieves a repair pass rate of 82.7%, outperforming GPT-4o and Llama3-8B by 19.1% and 63%, respectively. It also delivers speedup ranging from 1.9\u00d7 to 14.8\u00d7 compared to resource-constrained kernels, with a 4.9\u00d7 geometric mean speedup over DSL-based approaches.", "conclusion": "ChatHLS significantly accelerates hardware development cycles while preserving high standards in design reliability and optimization, showcasing its potential as a transformative tool in hardware design."}}
{"id": "2507.00032", "pdf": "https://arxiv.org/pdf/2507.00032", "abs": "https://arxiv.org/abs/2507.00032", "authors": ["Grey Kuling", "Marinka Zitnik"], "title": "Ken Utilization Layer: Hebbian Replay Within a Student's Ken for Adaptive Knowledge Tracing", "categories": ["cs.CY", "cs.AI", "cs.LG", "cs.NE"], "comment": null, "summary": "We introduce KUL-KT, a biologically inspired architecture for knowledge\ntracing (KT), combining Hebbian memory encoding with gradient-based\nconsolidation in a scalable, input-agnostic framework. KUL-KT adapts the\nprinciple of memory consolidation in neural systems, to student modeling by\nintroducing two key innovations: (i) a time-decaying Hebbian memory update that\nenables graceful forgetting, and (ii) a novel Loss-aligned Internal Target\n(LIT) method to compute an ideal internal state, allowing continual learning\nwithout backpropagation through time. The architecture consists of a fast\nHebbian memory that captures each learner interaction via a single associative\nupdate, and a slower linear network that consolidates recalled samples through\ngradient descent. This design enables few-shot personalization and natural\nforgetting without storing raw data or relying on large cohort training.\nOperating entirely in embedding space, KUL-KT supports both structured\n(tabular) and unstructured (short-answer) inputs. Empirically, KUL-KT\noutperforms strong baselines on ten public KT benchmarks in rank-sensitive\nmetrics such as nDCG and Recall@10. In a classroom deployment, KUL-KT\npersonalized quizzes from short-answer data, leading to improved\nlearner-perceived helpfulness and reduced difficulty (p < 0.05). Ablation\nstudies confirm that Hebbian decay and LIT are critical for continual\nadaptation. Compared to a strong graph-based KT model, KUL-KT trains 1.75x\nfaster and uses 99.01\\% less memory. These results position KUL-KT as a\nbiologically grounded, memory-efficient, and input-flexible framework for\npersonalized learning at scale.", "AI": {"tldr": "KUL-KT is a biologically inspired knowledge tracing architecture using Hebbian memory with gradient-based consolidation, achieving superior performance and efficiency for personalized learning.", "motivation": "To create a biologically inspired, memory-efficient, and input-flexible architecture to address the challenges of continual learning and personalization in knowledge tracing.", "method": "KUL-KT employs a Hebbian memory mechanism with time-decaying updates for forgetting, combined with a Loss-aligned Internal Target method for learning without backpropagation through time. The system uses a fast memory for single updates and a slower network for consolidation via gradient descent.", "result": "KUL-KT outperformed baselines on ten KT benchmarks in metrics like nDCG and Recall@10, showed effective classroom deployment with improved perceived helpfulness, and achieved faster training with significantly reduced memory use (99.01% less).", "conclusion": "KUL-KT offers a biologically inspired, scalable framework that excels in personalized learning through efficient, flexible, and memory-conserving mechanisms informed by neural principles."}}
{"id": "2507.00491", "pdf": "https://arxiv.org/pdf/2507.00491", "abs": "https://arxiv.org/abs/2507.00491", "authors": ["Zain Taufique", "Aman Vyas", "Antonio Miele", "Pasi Liljeberg", "Anil Kanduri"], "title": "Twill: Scheduling Compound AI Systems on Heterogeneous Mobile Edge Platforms", "categories": ["cs.MA", "cs.AI", "cs.CV", "cs.PF"], "comment": "9 Pages, 9 Figures, Accepted in International Conference on\n  Computer-Aided Design (ICCAD) 2025", "summary": "Compound AI (cAI) systems chain multiple AI models to solve complex problems.\ncAI systems are typically composed of deep neural networks (DNNs),\ntransformers, and large language models (LLMs), exhibiting a high degree of\ncomputational diversity and dynamic workload variation. Deploying cAI services\non mobile edge platforms poses a significant challenge in scheduling concurrent\nDNN-transformer inference tasks, which arrive dynamically in an unknown\nsequence. Existing mobile edge AI inference strategies manage multi-DNN or\ntransformer-only workloads, relying on design-time profiling, and cannot handle\nconcurrent inference of DNNs and transformers required by cAI systems. In this\nwork, we address the challenge of scheduling cAI systems on heterogeneous\nmobile edge platforms. We present Twill, a run-time framework to handle\nconcurrent inference requests of cAI workloads through task affinity-aware\ncluster mapping and migration, priority-aware task freezing/unfreezing, and\nDVFS, while minimizing inference latency within power budgets. We implement and\ndeploy our Twill framework on the Nvidia Jetson Orin NX platform. We evaluate\nTwill against state-of-the-art edge AI inference techniques over contemporary\nDNNs and LLMs, reducing inference latency by 54% on average, while honoring\npower budgets.", "AI": {"tldr": "The paper introduces Twill, a framework for scheduling Compound AI workloads on mobile platforms, reducing latency by 54% while adhering to power constraints.", "motivation": "The complexity and dynamic nature of compound AI workloads necessitate efficient scheduling solutions on mobile edge platforms, which is not addressed by existing strategies.", "method": "The Twill framework uses task affinity-aware cluster mapping, migration, priority-aware task freezing/unfreezing, and dynamic voltage frequency scaling (DVFS) to manage concurrent AI tasks.", "result": "Twill significantly outperforms state-of-the-art techniques, achieving a 54% reduction in inference latency on the Nvidia Jetson Orin NX platform.", "conclusion": "Twill provides a practical solution for deploying compound AI systems on mobile platforms, optimizing both latency and energy efficiency."}}
{"id": "2507.00048", "pdf": "https://arxiv.org/pdf/2507.00048", "abs": "https://arxiv.org/abs/2507.00048", "authors": ["Thomas M. Deucher", "Juan C. Verduzco", "Michael Titus", "Alejandro Strachan"], "title": "A collaborative digital twin built on FAIR data and compute infrastructure", "categories": ["cs.AI", "cond-mat.mtrl-sci", "cs.CE", "cs.LG"], "comment": "10 pages, 5 figures", "summary": "The integration of machine learning with automated experimentation in\nself-driving laboratories (SDL) offers a powerful approach to accelerate\ndiscovery and optimization tasks in science and engineering applications. When\nsupported by findable, accessible, interoperable, and reusable (FAIR) data\ninfrastructure, SDLs with overlapping interests can collaborate more\neffectively. This work presents a distributed SDL implementation built on\nnanoHUB services for online simulation and FAIR data management. In this\nframework, geographically dispersed collaborators conducting independent\noptimization tasks contribute raw experimental data to a shared central\ndatabase. These researchers can then benefit from analysis tools and machine\nlearning models that automatically update as additional data become available.\nNew data points are submitted through a simple web interface and automatically\nprocessed using a nanoHUB Sim2L, which extracts derived quantities and indexes\nall inputs and outputs in a FAIR data repository called ResultsDB. A separate\nnanoHUB workflow enables sequential optimization using active learning, where\nresearchers define the optimization objective, and machine learning models are\ntrained on-the-fly with all existing data, guiding the selection of future\nexperiments. Inspired by the concept of ``frugal twin\", the optimization task\nseeks to find the optimal recipe to combine food dyes to achieve the desired\ntarget color. With easily accessible and inexpensive materials, researchers and\nstudents can set up their own experiments, share data with collaborators, and\nexplore the combination of FAIR data, predictive ML models, and sequential\noptimization. The tools introduced are generally applicable and can easily be\nextended to other optimization problems.", "AI": {"tldr": "The study integrates machine learning and automated experimentation using FAIR data management to accelerate discovery. It creates a distributed self-driving laboratory framework for collaborative optimization tasks using nanoHUB tools.", "motivation": "To enhance optimization and discovery efficiency by integrating machine learning and automated experimentation supported by a FAIR data infrastructure.", "method": "A distributed SDL implementation developed on nanoHUB, utilizing Sim2L and ResultsDB tools. Researchers contribute data to a shared database and employ active learning for optimization tasks.", "result": "The framework successfully allows geographically dispersed researchers to collaborate on optimization using online tools, real-time data sharing, and machine learning-driven models.", "conclusion": "The developed tools and framework are generalizable and can be applied to other optimization problems, exemplified by optimizing food dye recipes to achieve specific target colors."}}
{"id": "2507.00210", "pdf": "https://arxiv.org/pdf/2507.00210", "abs": "https://arxiv.org/abs/2507.00210", "authors": ["Imene Kerboua", "Sahar Omidi Shayegan", "Megh Thakkar", "Xing Han L\u00f9", "Massimo Caccia", "V\u00e9ronique Eglin", "Alexandre Aussem", "J\u00e9r\u00e9my Espinas", "Alexandre Lacoste"], "title": "LineRetriever: Planning-Aware Observation Reduction for Web Agents", "categories": ["cs.CL"], "comment": null, "summary": "While large language models have demonstrated impressive capabilities in web\nnavigation tasks, the extensive context of web pages, often represented as DOM\nor Accessibility Tree (AxTree) structures, frequently exceeds model context\nlimits. Current approaches like bottom-up truncation or embedding-based\nretrieval lose critical information about page state and action history. This\nis particularly problematic for adaptive planning in web agents, where\nunderstanding the current state is essential for determining future actions. We\nhypothesize that embedding models lack sufficient capacity to capture\nplan-relevant information, especially when retrieving content that supports\nfuture action prediction. This raises a fundamental question: how can retrieval\nmethods be optimized for adaptive planning in web navigation tasks? In\nresponse, we introduce \\textit{LineRetriever}, a novel approach that leverages\na language model to identify and retrieve observation lines most relevant to\nfuture navigation steps. Unlike traditional retrieval methods that focus solely\non semantic similarity, \\textit{LineRetriever} explicitly considers the\nplanning horizon, prioritizing elements that contribute to action prediction.\nOur experiments demonstrate that \\textit{LineRetriever} can reduce the size of\nthe observation at each step for the web agent while maintaining consistent\nperformance within the context limitations.", "AI": {"tldr": "The paper introduces LineRetriever, a method for web navigation agents to retrieve relevant information efficiently for future action planning, addressing context limitations.", "motivation": "Existing web navigation methods face challenges in retaining critical information due to large web page contexts exceeding model limits, leading to suboptimal action predictions.", "method": "The authors developed LineRetriever, which uses a language model to retrieve observation lines relevant to future actions, prioritizing planning horizon over traditional semantic similarity.", "result": "Experiments showed that LineRetriever efficiently reduced observation sizes for web agents without degrading performance, within the constraints of model context limits.", "conclusion": "LineRetriever offers an effective approach to optimize retrieval for adaptive planning by focusing on future-relevant data in web navigation tasks."}}
{"id": "2507.00488", "pdf": "https://arxiv.org/pdf/2507.00488", "abs": "https://arxiv.org/abs/2507.00488", "authors": ["Lloyd Allison"], "title": "Have Object-Oriented Languages Missed a Trick with Class Function and its Subclasses?", "categories": ["cs.PL", "D.3.3; D.2"], "comment": null, "summary": "Compared to functions in mathematics, functions in programming languages seem\nto be under classified. Functional programming languages based on the lambda\ncalculus famously treat functions as first-class values. Object-oriented\nlanguages have adopted ``lambdas'', notably for call-back routines in\nevent-based programming. Typically a programming language has functions, a\nfunction has a type, and some functions act on other functions and/or return\nfunctions but there is generally a lack of (i) ``class Function'' in the OO\nsense of the word class and particularly (ii) subclasses of Function for\nfunctions having specific properties. Some such classes are presented here and\nprogrammed in some popular programming languages as an experimental\ninvestigation into OO languages missing this opportunity.", "AI": {"tldr": "The paper examines how programming languages, particularly object-oriented ones, underclassify functions, proposing new subclasses based on specific properties.", "motivation": "Explore the lack of nuanced function classifications in programming languages and highlight opportunities for enhancement.", "method": "Define subclasses of functions with unique properties and implement them experimentally across popular programming languages.", "result": "Demonstrated potential benefits of introducing subclasses for functions in object-oriented programming contexts.", "conclusion": "Introducing more detailed classifications for functions can enrich object-oriented programming capabilities and address current limitations."}}
{"id": "2507.00236", "pdf": "https://arxiv.org/pdf/2507.00236", "abs": "https://arxiv.org/abs/2507.00236", "authors": ["Chinmay Vilas Samak", "Tanmay Vilas Samak", "Bing Li", "Venkat Krovi"], "title": "Sim2Real Diffusion: Learning Cross-Domain Adaptive Representations for Transferable Autonomous Driving", "categories": ["cs.RO"], "comment": null, "summary": "Simulation-based design, optimization, and validation of autonomous driving\nalgorithms have proven to be crucial for their iterative improvement over the\nyears. Nevertheless, the ultimate measure of effectiveness is their successful\ntransition from simulation to reality (sim2real). However, existing sim2real\ntransfer methods struggle to comprehensively address the autonomy-oriented\nrequirements of balancing: (i) conditioned domain adaptation, (ii) robust\nperformance with limited examples, (iii) modularity in handling multiple domain\nrepresentations, and (iv) real-time performance. To alleviate these pain\npoints, we present a unified framework for learning cross-domain adaptive\nrepresentations for sim2real transferable autonomous driving algorithms using\nconditional latent diffusion models. Our framework offers options to leverage:\n(i) alternate foundation models, (ii) a few-shot fine-tuning pipeline, and\n(iii) textual as well as image prompts for mapping across given source and\ntarget domains. It is also capable of generating diverse high-quality samples\nwhen diffusing across parameter spaces such as times of day, weather\nconditions, seasons, and operational design domains. We systematically analyze\nthe presented framework and report our findings in the form of critical\nquantitative metrics and ablation studies, as well as insightful qualitative\nexamples and remarks. Additionally, we demonstrate the serviceability of the\nproposed approach in bridging the sim2real gap for end-to-end autonomous\ndriving using a behavioral cloning case study. Our experiments indicate that\nthe proposed framework is capable of bridging the perceptual sim2real gap by\nover 40%. We hope that our approach underscores the potential of generative\ndiffusion models in sim2real transfer, offering a pathway toward more robust\nand adaptive autonomous driving.", "AI": {"tldr": "The paper introduces a unified framework using conditional latent diffusion models to enhance autonomous driving algorithms' simulation-to-reality effectiveness, addressing challenges like domain adaptation, performance robustness, modularity, and real-time applicability.", "motivation": "Existing methods for sim2real transfer in autonomous driving face major challenges such as domain adaptation, handling limited samples, modular processing of domain representations, and achieving real-time results.", "method": "The framework uses conditional latent diffusion models with options for alternate foundation models, a few-shot fine-tuning pipeline, and prompts (text/image) to map across domains. It can generate diverse samples adapting to variables like weather, time, and conditions.", "result": "Experiments show that the framework bridges the perceptual sim2real gap by over 40%, demonstrated through a behavioral cloning case study for end-to-end autonomous driving.", "conclusion": "Generative diffusion models have strong potential to address sim2real transfer gaps in autonomous driving, improving robustness and adaptability of algorithms for real-world scenarios."}}
{"id": "2507.00265", "pdf": "https://arxiv.org/pdf/2507.00265", "abs": "https://arxiv.org/abs/2507.00265", "authors": ["Alexis Carrillo", "Asieh Abolpour Mofrad", "Anis Yazidi", "Moises Betancort"], "title": "Examining Reject Relations in Stimulus Equivalence Simulations", "categories": ["cs.LG", "q-bio.NC", "I.2.0; J.4; I.6.5"], "comment": "18 pages, 6 figures", "summary": "Simulations offer a valuable tool for exploring stimulus equivalence (SE),\nyet the potential of reject relations to disrupt the assessment of equivalence\nclass formation is contentious. This study investigates the role of reject\nrelations in the acquisition of stimulus equivalence using computational\nmodels. We examined feedforward neural networks (FFNs), bidirectional encoder\nrepresentations from transformers (BERT), and generative pre-trained\ntransformers (GPT) across 18 conditions in matching-to-sample (MTS)\nsimulations. Conditions varied in training structure (linear series,\none-to-many, and many-to-one), relation type (select-only, reject-only, and\nselect-reject), and negative comparison selection (standard and biased). A\nprobabilistic agent served as a benchmark, embodying purely associative\nlearning. The primary goal was to determine whether artificial neural networks\ncould demonstrate equivalence class formation or whether their performance\nreflected associative learning. Results showed that reject relations influenced\nagent performance. While some agents achieved high accuracy on equivalence\ntests, particularly with reject relations and biased negative comparisons, this\nperformance was comparable to the probabilistic agent. These findings suggest\nthat artificial neural networks, including transformer models, may rely on\nassociative strategies rather than SE. This underscores the need for careful\nconsideration of reject relations and more stringent criteria in computational\nmodels of equivalence.", "AI": {"tldr": "This study explores the impact of reject relations on the formation of stimulus equivalence using FFNs, BERT, and GPT models in matching-to-sample simulations.", "motivation": "The study addresses the contentious role of reject relations in evaluating stimulus equivalence class formation using computational models.", "method": "Researchers tested FFNs, BERT, and GPT in 18 matching-to-sample conditions with varying training structures, relation types, and negative comparison biases, comparing their performance to a probabilistic agent.", "result": "Reject relations impacted agent performance, but the behavior observed in neural networks was comparable to simple associative learning, akin to the probabilistic agent.", "conclusion": "Artificial neural networks may rely on associative mechanisms rather than true stimulus equivalence, highlighting the importance of stringent criteria when assessing equivalence in computational models."}}
{"id": "2507.00378", "pdf": "https://arxiv.org/pdf/2507.00378", "abs": "https://arxiv.org/abs/2507.00378", "authors": ["Xikai Sun", "Fan Dang", "Kebin Liu", "Xin Miao", "Zihao Yang", "Haimo Lu", "Yawen Zheng", "Yunhao Liu"], "title": "iPanda: An Intelligent Protocol Testing and Debugging Agent for Conformance Testing", "categories": ["cs.SE", "cs.AI"], "comment": "14 pages, 6 figures", "summary": "Conformance testing is essential for ensuring that protocol implementations\ncomply with their specifications. However, traditional testing approaches\ninvolve manually creating numerous test cases and scripts, making the process\nlabor-intensive and inefficient. Recently, Large Language Models (LLMs) have\ndemonstrated impressive text comprehension and code generation abilities,\nproviding promising opportunities for automation. In this paper, we propose\niPanda, the first end-to-end framework that leverages LLMs to automate protocol\nconformance testing. Given a protocol specification document and its\nimplementation, iPanda first employs a keyword-based method to automatically\ngenerate comprehensive test cases. Then, it utilizes a code-based\nretrieval-augmented generation approach to effectively interpret the\nimplementation and produce executable test code. To further enhance code\nquality, iPanda incorporates an iterative self-correction mechanism to refine\ngenerated test scripts interactively. Finally, by executing and analyzing the\ngenerated tests, iPanda systematically verifies compliance between\nimplementations and protocol specifications. Comprehensive experiments on\nvarious protocols show that iPanda significantly outperforms pure LLM-based\napproaches, improving the success rate (Pass@1) of test-code generation by\nfactors ranging from 4.675 times to 10.751 times.", "AI": {"tldr": "The paper introduces iPanda, a framework using Large Language Models (LLMs) to fully automate protocol conformance testing, improving test efficiency and performance significantly.", "motivation": "Traditional protocol conformance testing is labor-intensive, involving manual creation of many test cases and scripts. The authors aim to automate this process by utilizing the advanced capabilities of LLMs for text and code generation.", "method": "iPanda employs a keyword-based method to generate test cases, uses code retrieval-augmented generation to create executable test scripts, and features an iterative self-correction mechanism to refine these scripts. It then executes the tests to verify protocol compliance.", "result": "Experiments with various protocols showed that iPanda outperformed traditional LLM-based approaches, boosting the success rate of test-code generation by factors ranging from 4.675 to 10.751 times.", "conclusion": "iPanda demonstrates significant efficiency and accuracy improvements in protocol conformance testing by leveraging LLMs for automation, potentially setting a new standard in the field."}}
{"id": "2507.00043", "pdf": "https://arxiv.org/pdf/2507.00043", "abs": "https://arxiv.org/abs/2507.00043", "authors": ["Mehmet Yigit Avci", "Pedro Borges", "Paul Wright", "Mehmet Yigitsoy", "Sebastien Ourselin", "Jorge Cardoso"], "title": "MR-CLIP: Efficient Metadata-Guided Learning of MRI Contrast Representations", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Accurate interpretation of Magnetic Resonance Imaging scans in clinical\nsystems is based on a precise understanding of image contrast. This contrast is\nprimarily governed by acquisition parameters, such as echo time and repetition\ntime, which are stored in the DICOM metadata. To simplify contrast\nidentification, broad labels such as T1-weighted or T2-weighted are commonly\nused, but these offer only a coarse approximation of the underlying acquisition\nsettings. In many real-world datasets, such labels are entirely missing,\nleaving raw acquisition parameters as the only indicators of contrast. Adding\nto this challenge, the available metadata is often incomplete, noisy, or\ninconsistent. The lack of reliable and standardized metadata complicates tasks\nsuch as image interpretation, retrieval, and integration into clinical\nworkflows. Furthermore, robust contrast-aware representations are essential to\nenable more advanced clinical applications, such as achieving\nmodality-invariant representations and data harmonization. To address these\nchallenges, we propose MR-CLIP, a multimodal contrastive learning framework\nthat aligns MR images with their DICOM metadata to learn contrast-aware\nrepresentations, without relying on manual labels. Trained on a diverse\nclinical dataset that spans various scanners and protocols, MR-CLIP captures\ncontrast variations across acquisitions and within scans, enabling\nanatomy-invariant representations. We demonstrate its effectiveness in\ncross-modal retrieval and contrast classification, highlighting its scalability\nand potential for further clinical applications. The code and weights are\npublicly available at https://github.com/myigitavci/MR-CLIP.", "AI": {"tldr": "The paper introduces MR-CLIP, a framework that uses multimodal contrastive learning to align MRI scans with DICOM metadata for contrast-aware representation, addressing challenges like incomplete labels and noisy metadata.", "motivation": "The paper aims to overcome challenges in interpreting MRI scans caused by missing, inconsistent, or noisy metadata, as well as the lack of reliable contrast-aware representations.", "method": "A multimodal contrastive learning framework, MR-CLIP, is designed to align MR images with their DICOM metadata, learning contrast-aware representations without requiring manual labels.", "result": "MR-CLIP demonstrated effectiveness in tasks like cross-modal retrieval and contrast classification, achieving anatomy-invariant representations and scalability across diverse clinical datasets.", "conclusion": "MR-CLIP advances MRI interpretation by providing robust contrast-aware representations, enhancing clinical workflows, and is made accessible via open-source code and weights."}}
{"id": "2507.00402", "pdf": "https://arxiv.org/pdf/2507.00402", "abs": "https://arxiv.org/abs/2507.00402", "authors": ["Suqing Liu", "Xuan Bi", "Tianxi Li"], "title": "GRAND: Graph Release with Assured Node Differential Privacy", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.ME", "stat.TH"], "comment": null, "summary": "Differential privacy is a well-established framework for safeguarding\nsensitive information in data. While extensively applied across various\ndomains, its application to network data -- particularly at the node level --\nremains underexplored. Existing methods for node-level privacy either focus\nexclusively on query-based approaches, which restrict output to pre-specified\nnetwork statistics, or fail to preserve key structural properties of the\nnetwork. In this work, we propose GRAND (Graph Release with Assured Node\nDifferential privacy), which is, to the best of our knowledge, the first\nnetwork release mechanism that releases entire networks while ensuring\nnode-level differential privacy and preserving structural properties. Under a\nbroad class of latent space models, we show that the released network\nasymptotically follows the same distribution as the original network. The\neffectiveness of the approach is evaluated through extensive experiments on\nboth synthetic and real-world datasets.", "AI": {"tldr": "This paper introduces GRAND, the first mechanism to guarantee node-level differential privacy while releasing entire networks and maintaining structural properties.", "motivation": "Current methods for network data privacy either restrict outputs to specific statistics or don't preserve network structures, highlighting a gap for node-level differential privacy methods capable of releasing entire networks.", "method": "The authors propose 'GRAND', a privacy-preserving mechanism that releases full networks while ensuring node-level differential privacy, under latent space models that mimic the original network's distribution.", "result": "GRAND effectively preserves key structural and statistical properties of the network, validated through experiments on both synthetic and real-world datasets.", "conclusion": "GRAND represents a breakthrough in privacy-preserving network releases, ensuring node-level differential privacy while maintaining network fidelity, bridging a critical gap in the field."}}
{"id": "2507.00428", "pdf": "https://arxiv.org/pdf/2507.00428", "abs": "https://arxiv.org/abs/2507.00428", "authors": ["Mohammad Firas Sada", "John J. Graham", "Mahidhar Tatineni", "Dmitry Mishin", "Thomas A. DeFanti", "Frank W\u00fcrthwein"], "title": "Real-Time In-Network Machine Learning on P4-Programmable FPGA SmartNICs with Fixed-Point Arithmetic and Taylor", "categories": ["cs.DC", "cs.NI"], "comment": "To appear in Proceedings of the Practice and Experience in Advanced\n  Research Computing (PEARC25)", "summary": "As machine learning (ML) applications become integral to modern network\noperations, there is an increasing demand for network programmability that\nenables low-latency ML inference for tasks such as Quality of Service (QoS)\nprediction and anomaly detection in cybersecurity. ML models provide\nadaptability through dynamic weight adjustments, making Programming\nProtocol-independent Packet Processors (P4)-programmable FPGA SmartNICs an\nideal platform for investigating In-Network Machine Learning (INML). These\ndevices offer high-throughput, low-latency packet processing and can be\ndynamically reconfigured via the control plane, allowing for flexible\nintegration of ML models directly at the network edge. This paper explores the\napplication of the P4 programming paradigm to neural networks and regression\nmodels, where weights and biases are stored in control plane table lookups.\nThis approach enables flexible programmability and efficient deployment of\nretrainable ML models at the network edge, independent of core infrastructure\nat the switch level.", "AI": {"tldr": "The paper focuses on integrating retrainable ML models with P4-programmable FPGA SmartNICs for low-latency, high-throughput network processing.", "motivation": "To enable low-latency ML inference for tasks like QoS prediction and cybersecurity anomaly detection using network programmability.", "method": "Applying the P4 programming paradigm to neural networks and regression models, with weights and biases stored in control plane table lookups.", "result": "Demonstrated the flexible integration of retrainable ML models directly at the network edge using P4-programmable FPGA SmartNICs.", "conclusion": "The proposed approach allows for flexible, efficient deployment of ML models at the network edge, independent of core switch infrastructure."}}
{"id": "2507.00797", "pdf": "https://arxiv.org/pdf/2507.00797", "abs": "https://arxiv.org/abs/2507.00797", "authors": ["Zhican Wang", "Hongxiang Fan", "Haroon Waris", "Gang Wang", "Zhenyu Li", "Jianfei Jiang", "Yanan Sun", "Guanghui He"], "title": "VEDA: Efficient LLM Generation Through Voting-based KV Cache Eviction and Dataflow-flexible Accelerator", "categories": ["cs.AR"], "comment": "DAC 2025", "summary": "Large Language Models (LLMs) excel in natural language processing tasks but\npose significant computational and memory challenges for edge deployment due to\ntheir intensive resource demands. This work addresses the efficiency of LLM\ninference by algorithm-hardware-dataflow tri-optimizations. We propose a novel\nvoting-based KV cache eviction algorithm, balancing hardware efficiency and\nalgorithm accuracy by adaptively identifying unimportant kv vectors. From a\ndataflow perspective, we introduce a flexible-product dataflow and a runtime\nreconfigurable PE array for matrix-vector multiplication. The proposed approach\neffectively handles the diverse dimensional requirements and solves the\nchallenges of incrementally varying sequence lengths. Additionally, an\nelement-serial scheduling scheme is proposed for nonlinear operations, such as\nsoftmax and layer normalization (layernorm). Results demonstrate a substantial\nreduction in latency, accompanied by a significant decrease in hardware\ncomplexity, from O(N) to O(1). The proposed solution is realized in a\ncustom-designed accelerator, VEDA, which outperforms existing hardware\nplatforms. This research represents a significant advancement in LLM inference\non resource-constrained edge devices, facilitating real-time processing,\nenhancing data privacy, and enabling model customization.", "AI": {"tldr": "This paper introduces VEDA, a custom accelerator optimizing LLM inference on edge devices through a voting-based KV cache eviction algorithm, reconfigurable hardware, and efficient scheduling, reducing latency and hardware complexity.", "motivation": "LLMs are computationally demanding, posing challenges for deployment on edge devices. The paper seeks to optimize LLM inference for resource-constrained environments.", "method": "The method involves tri-optimizations in algorithm, hardware, and dataflow, including a novel KV cache eviction algorithm, flexible-product dataflow, runtime-reconfigurable PE array, and element-serial scheduling.", "result": "The method reduces latency and hardware complexity from O(N) to O(1), realized in the custom accelerator VEDA, which outperforms existing platforms.", "conclusion": "The proposed solution facilitates efficient LLM inference on edge devices, enhancing real-time processing, data privacy, and model customization."}}
{"id": "2507.00263", "pdf": "https://arxiv.org/pdf/2507.00263", "abs": "https://arxiv.org/abs/2507.00263", "authors": ["Vignesh Ram Nithin Kappagantula", "Shayan Hassantabar"], "title": "Room Scene Discovery and Grouping in Unstructured Vacation Rental Image Collections", "categories": ["cs.CV", "cs.LG", "cs.NE"], "comment": null, "summary": "The rapid growth of vacation rental (VR) platforms has led to an increasing\nvolume of property images, often uploaded without structured categorization.\nThis lack of organization poses significant challenges for travelers attempting\nto understand the spatial layout of a property, particularly when multiple\nrooms of the same type are present. To address this issue, we introduce an\neffective approach for solving the room scene discovery and grouping problem,\nas well as identifying bed types within each bedroom group. This grouping is\nvaluable for travelers to comprehend the spatial organization, layout, and the\nsleeping configuration of the property. We propose a computationally efficient\nmachine learning pipeline characterized by low latency and the ability to\nperform effectively with sample-efficient learning, making it well-suited for\nreal-time and data-scarce environments. The pipeline integrates a supervised\nroom-type detection model, a supervised overlap detection model to identify the\noverlap similarity between two images, and a clustering algorithm to group the\nimages of the same space together using the similarity scores. Additionally,\nthe pipeline maps each bedroom group to the corresponding bed types specified\nin the property's metadata, based on the visual content present in the group's\nimages using a Multi-modal Large Language Model (MLLM) model. We evaluate the\naforementioned models individually and also assess the pipeline in its\nentirety, observing strong performance that significantly outperforms\nestablished approaches such as contrastive learning and clustering with\npretrained embeddings.", "AI": {"tldr": "The study develops a machine learning pipeline to categorize vacation rental property images and detect bed types in bedrooms for improved spatial understanding, excelling over existing methods.", "motivation": "Vacation rental platforms encounter challenges due to unstructured image categorization, making it difficult for travelers to grasp property layouts, especially for repeated room types.", "method": "The proposed pipeline utilizes a supervised room-type detection model, overlap similarity detection through a supervised method, clustering to group room images, and MLLM for assigning bed types based on visual and metadata alignment.", "result": "The pipeline demonstrated strong performance, surpassing traditional methods like contrastive learning and clustering with pretrained embeddings.", "conclusion": "Their proposed approach effectively organizes property images for better user comprehension, proving both computational efficiency and suitability for real-time applications in scarce data environments."}}
{"id": "2507.00824", "pdf": "https://arxiv.org/pdf/2507.00824", "abs": "https://arxiv.org/abs/2507.00824", "authors": ["Matthieu Pigaglio", "Onur Ascigil", "Micha\u0142 Kr\u00f3l", "Sergi Rene", "Felix Lange", "Kaleem Peeroo", "Ramin Sadre", "Vladimir Stankovic", "Etienne Rivi\u00e8re"], "title": "PANDAS: Peer-to-peer, Adaptive Networking for Data Availability Sampling within Ethereum Consensus Timebounds", "categories": ["cs.DC", "cs.NI", "cs.PF"], "comment": "14 pages, 10 figures, 1 algorithm, 1 table, and 18 plots", "summary": "Layer-2 protocols can assist Ethereum's limited throughput, but globally\nbroadcasting layer-2 data limits their scalability. The Danksharding evolution\nof Ethereum aims to support the selective distribution of layer-2 data, whose\navailability in the network is verified using randomized data availability\nsampling (DAS). Integrating DAS into Ethereum's consensus process is\nchallenging, as pieces of layer-2 data must be disseminated and sampled within\nfour seconds of the beginning of each consensus slot. No existing solution can\nsupport dissemination and sampling under such strict time bounds.\n  We propose PANDAS, a practical approach to integrate DAS with Ethereum under\nDanksharding's requirements without modifying its protocols for consensus and\nnode discovery. PANDAS disseminates layer-2 data and samples its availability\nusing lightweight, direct exchanges. Its design accounts for message loss, node\nfailures, and unresponsive participants while anticipating the need to scale\nout the Ethereum network. Our evaluation of PANDAS's prototype in a 1,000-node\ncluster and simulations for up to 20,000 peers shows that it allows layer-2\ndata dissemination and sampling under planetary-scale latencies within the\n4-second deadline.", "AI": {"tldr": "PANDAS proposes a solution for integrating data availability sampling (DAS) with Ethereum's Danksharding evolution, improving layer-2 data dissemination and sampling efficiency within 4-second slots.", "motivation": "Ethereum's scalability is limited despite Layer-2 protocols; Danksharding evolution seeks efficient selective distribution of Layer-2 data using DAS within strict latency constraints.", "method": "PANDAS employs lightweight direct exchanges for disseminating and sampling Layer-2 data, accommodating challenges such as message loss, node failures, and scalability demands without modifying Ethereum's protocols.", "result": "Evaluation of PANDAS in a 1,000-node cluster and simulations for up to 20,000 peers demonstrate its ability to meet 4-second deadlines under planetary-scale latencies.", "conclusion": "PANDAS effectively tackles Ethereum's DAS integration challenges, supporting scalability and reliability while respecting strict time limits without altering existing protocols."}}
{"id": "2507.00050", "pdf": "https://arxiv.org/pdf/2507.00050", "abs": "https://arxiv.org/abs/2507.00050", "authors": ["Devin Y. De Silva", "Sandareka Wickramanayake", "Dulani Meedeniya", "Sanka Rasnayaka"], "title": "SEZ-HARN: Self-Explainable Zero-shot Human Activity Recognition Network", "categories": ["cs.AI", "cs.HC", "cs.LG", "I.2.0"], "comment": null, "summary": "Human Activity Recognition (HAR), which uses data from Inertial Measurement\nUnit (IMU) sensors, has many practical applications in healthcare and assisted\nliving environments. However, its use in real-world scenarios has been limited\nby the lack of comprehensive IMU-based HAR datasets that cover a wide range of\nactivities and the lack of transparency in existing HAR models. Zero-shot HAR\n(ZS-HAR) overcomes the data limitations, but current models struggle to explain\ntheir decisions, making them less transparent. This paper introduces a novel\nIMU-based ZS-HAR model called the Self-Explainable Zero-shot Human Activity\nRecognition Network (SEZ-HARN). It can recognize activities not encountered\nduring training and provide skeleton videos to explain its decision-making\nprocess. We evaluate the effectiveness of the proposed SEZ-HARN on four\nbenchmark datasets PAMAP2, DaLiAc, HTD-MHAD and MHealth and compare its\nperformance against three state-of-the-art black-box ZS-HAR models. The\nexperiment results demonstrate that SEZ-HARN produces realistic and\nunderstandable explanations while achieving competitive Zero-shot recognition\naccuracy. SEZ-HARN achieves a Zero-shot prediction accuracy within 3\\% of the\nbest-performing black-box model on PAMAP2 while maintaining comparable\nperformance on the other three datasets.", "AI": {"tldr": "The paper proposes SEZ-HARN, a framework for Zero-shot Human Activity Recognition (ZS-HAR) using IMU sensors that provides explainable skeleton video outputs while maintaining competitive performance.", "motivation": "The lack of broad IMU datasets and the opacity of current ZS-HAR approaches limit their real-world application. This research aims to improve recognition and transparency simultaneously.", "method": "The authors introduce SEZ-HARN, a novel model capable of recognizing unseen human activities and generating explainable skeleton videos to clarify its predictions.", "result": "SEZ-HARN achieves Zero-shot accuracy within 3% of the highest-performing black-box model on PAMAP2 and performs comparably with others, while offering interpretable recognition results.", "conclusion": "SEZ-HARN successfully balances competitive Zero-shot recognition accuracy with transparent decision-making through realistic skeleton video explanations."}}
{"id": "2507.00214", "pdf": "https://arxiv.org/pdf/2507.00214", "abs": "https://arxiv.org/abs/2507.00214", "authors": ["Mads Henrichsen", "Rasmus Krebs"], "title": "Two-Stage Reasoning-Infused Learning: Improving Classification with LLM-Generated Reasoning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Standard classification models often map inputs directly to labels without\nexplicit reasoning, potentially limiting their performance, robustness, and\ninterpretability. This paper introduces a novel two-stage approach to enhance\ntext classification by leveraging Large Language Model (LLM)-generated\nreasonings. In the first stage, we fine-tune a Llama-3.2-1B-Instruct model\n(henceforth Llama-R-Gen) on a general-purpose reasoning dataset\n(syvai/reasoning-gen) to generate textual reasoning (R) given a question and\nits answer. In the second stage, this generally trained Llama-R-Gen is used\noffline to create an augmented training dataset for a downstream generative\nmodel. This downstream model, based on Llama-3.2-1B-Instruct, takes only the\ninput text (Q) and is trained to output the generated reasoning (R) immediately\nfollowed by the predicted emotion (A). We demonstrate this methodology on the\ndair-ai/emotion dataset for emotion classification. Our experiments show that\nthe generative model trained to output reasoning and the emotion (Classifier\nQ->RA) achieves a significant improvement of 8.7 percentage points in accuracy\n(for emotion prediction) compared to a baseline generative model trained solely\nto output the emotion (Classifier Q->A), highlighting the strong generalization\ncapabilities of the reasoning generation and the benefit of explicit reasoning\ntraining. This work underscores the potential of LLM-generated reasonings for\ncreating richer training datasets, thereby improving the performance of diverse\ndownstream NLP tasks and providing explicit explanations.", "AI": {"tldr": "This paper introduces a two-stage approach utilizing reasoning generated by large language models to improve text classification accuracy and interpretability.", "motivation": "Highlighting the limitations of direct input-to-label mappings in standard models, this work seeks to enhance performance, robustness, and interpretability by incorporating explicit reasoning.", "method": "The approach employs a two-stage process: 1) fine-tuning a reasoning generation model (Llama-R-Gen) using a general-purpose reasoning dataset, and 2) using this model to augment training datasets for downstream models that generate reasoning along with classifications.", "result": "The reasoning-based generative model achieved an 8.7 percentage point increase in accuracy for emotion classification on the dair-ai/emotion dataset compared to a baseline model.", "conclusion": "Incorporating LLM-generated reasoning into training datasets significantly improves text classification tasks by providing better generalization and explicit interpretability advantages."}}
{"id": "2507.00011", "pdf": "https://arxiv.org/pdf/2507.00011", "abs": "https://arxiv.org/abs/2507.00011", "authors": ["Nathan Vaartjes", "Vincent Francois-Lavet"], "title": "Novel RL approach for efficient Elevator Group Control Systems", "categories": ["cs.LG", "cs.AI"], "comment": "15 pages, 12 figures", "summary": "Efficient elevator traffic management in large buildings is critical for\nminimizing passenger travel times and energy consumption. Because heuristic- or\npattern-detection-based controllers struggle with the stochastic and\ncombinatorial nature of dispatching, we model the six-elevator, fifteen-floor\nsystem at Vrije Universiteit Amsterdam as a Markov Decision Process and train\nan end-to-end Reinforcement Learning (RL) Elevator Group Control System (EGCS).\nKey innovations include a novel action space encoding to handle the\ncombinatorial complexity of elevator dispatching, the introduction of\ninfra-steps to model continuous passenger arrivals, and a tailored reward\nsignal to improve learning efficiency. In addition, we explore various ways to\nadapt the discounting factor to the infra-step formulation. We investigate RL\narchitectures based on Dueling Double Deep Q-learning, showing that the\nproposed RL-based EGCS adapts to fluctuating traffic patterns, learns from a\nhighly stochastic environment, and thereby outperforms a traditional rule-based\nalgorithm.", "AI": {"tldr": "This paper models elevator management as a Markov Decision Process and develops a Reinforcement Learning-based Elevator Group Control System to optimize travel times and energy consumption in large buildings.", "motivation": "Traditional elevator control methods struggle with the stochastic and combinatorial complexity inherent in dispatching, underscoring the need for a more adaptive system.", "method": "They use a Reinforcement Learning approach with enhancements like a novel action space encoding, infra-step modeling for continuous arrivals, a tailored reward signal, and adjusted discount factors.", "result": "The RL-based system outperformed a traditional rule-based algorithm, demonstrating adaptability to fluctuating traffic patterns and effectiveness in stochastic environments.", "conclusion": "Reinforcement Learning can effectively address complex elevator dispatching challenges, providing better performance compared to heuristic methods in managing real-world traffic and energy efficiency."}}
{"id": "2507.00268", "pdf": "https://arxiv.org/pdf/2507.00268", "abs": "https://arxiv.org/abs/2507.00268", "authors": ["Oren Fivel", "Matan Rudman", "Kobi Cohen"], "title": "Control-Optimized Deep Reinforcement Learning for Artificially Intelligent Autonomous Systems", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "comment": "27 pages, 10 figures", "summary": "Deep reinforcement learning (DRL) has become a powerful tool for complex\ndecision-making in machine learning and AI. However, traditional methods often\nassume perfect action execution, overlooking the uncertainties and deviations\nbetween an agent's selected actions and the actual system response. In\nreal-world applications, such as robotics, mechatronics, and communication\nnetworks, execution mismatches arising from system dynamics, hardware\nconstraints, and latency can significantly degrade performance. This work\nadvances AI by developing a novel control-optimized DRL framework that\nexplicitly models and compensates for action execution mismatches, a challenge\nlargely overlooked in existing methods. Our approach establishes a structured\ntwo-stage process: determining the desired action and selecting the appropriate\ncontrol signal to ensure proper execution. It trains the agent while accounting\nfor action mismatches and controller corrections. By incorporating these\nfactors into the training process, the AI agent optimizes the desired action\nwith respect to both the actual control signal and the intended outcome,\nexplicitly considering execution errors. This approach enhances robustness,\nensuring that decision-making remains effective under real-world uncertainties.\nOur approach offers a substantial advancement for engineering practice by\nbridging the gap between idealized learning and real-world implementation. It\nequips intelligent agents operating in engineering environments with the\nability to anticipate and adjust for actuation errors and system disturbances\nduring training. We evaluate the framework in five widely used open-source\nmechanical simulation environments we restructured and developed to reflect\nreal-world operating conditions, showcasing its robustness against\nuncertainties and offering a highly practical and efficient solution for\ncontrol-oriented applications.", "AI": {"tldr": "This paper presents a novel DRL framework designed to address action execution mismatches in real-world applications by introducing a two-stage decision-making process and considering control errors during training.", "motivation": "Traditional DRL approaches assume perfect action execution, ignoring real-world uncertainties caused by system dynamics, hardware limitations, and latency, which degrade performance in applications like robotics.", "method": "The framework explicitly incorporates action execution mismatches into the agent's training process using a two-stage approach where agents determine desired actions and select corresponding control signals to mitigate execution errors.", "result": "The framework improves robustness across multiple mechanical simulation environments, demonstrating resistance to real-world uncertainties and applicability to control-oriented engineering tasks.", "conclusion": "The proposed approach bridges the gap between idealized DRL models and real-world implementations, offering a robust tool for engineering applications where adaptability to execution mismatches is critical."}}
{"id": "2507.00305", "pdf": "https://arxiv.org/pdf/2507.00305", "abs": "https://arxiv.org/abs/2507.00305", "authors": ["Deland Liu", "Frigyes Samuel Racz", "Zoe Lalji", "Jose del R. Millan"], "title": "EEG-Based Auditory BCI for Communication in a Completely Locked-In Patient Using Volitional Frequency Band Modulation", "categories": ["cs.HC", "q-bio.NC"], "comment": null, "summary": "Patients with amyotrophic lateral sclerosis (ALS) in the completely locked-in\nstate (CLIS) can lose all reliable motor control and are left without any means\nof communication. It remains unknown whether non-invasive electroencephalogram\n(EEG) based brain-computer interfaces (BCIs) can support volitional\ncommunication in CLIS. Here, we show that a CLIS patient was able to operate an\nEEG-based BCI across multiple online sessions to respond to both general\nknowledge and personally relevant assistive questions. The patient delivered\n\"Yes\"/\"No\" responses by volitionally modulating alpha and beta band power at\ndifferent channels, guided by real-time auditory feedback from the BCI. The\npatient communicated assistive needs above chance in all sessions, achieving a\nperfect score in the final session. Performance on general knowledge questions\nvaried across sessions, with two sessions showing accurate and above-chance\nresponses, while the first and last sessions remained at chance level. The\npatient also showed consistent modulation patterns over time. These findings\nsuggest that non-invasive BCIs may offer a potential pathway for restoring\nbasic communication in CLIS.", "AI": {"tldr": "The study examines the potential of non-invasive EEG-based brain-computer interfaces (BCIs) to enable communication for completely locked-in ALS patients.", "motivation": "To explore whether patients in the completely locked-in state (CLIS) due to ALS can achieve volitional communication using non-invasive BCIs.", "method": "An EEG-based BCI system was tested on a CLIS patient, allowing \"Yes\"/\"No\" responses by modulating alpha and beta brainwave power in response to real-time auditory feedback during multiple online sessions.", "result": "The patient communicated assistive needs accurately across all sessions and showed an improvement in accuracy, but performance on general knowledge questions varied with only two sessions above chance.", "conclusion": "Non-invasive BCIs show potential for enabling basic communication in completely locked-in ALS patients, although performance consistency might be an issue."}}
{"id": "2507.00413", "pdf": "https://arxiv.org/pdf/2507.00413", "abs": "https://arxiv.org/abs/2507.00413", "authors": ["Taiming Wang", "Hui Liu", "Yuxia Zhang", "Yanjie Jiang"], "title": "Recommending Variable Names for Extract Local Variable Refactorings", "categories": ["cs.SE", "D.2.7"], "comment": "Accepted by TOSEM", "summary": "Extract local variable is one of the most popular refactorings, and most IDEs\nand refactoring tools provide automated support for this refactoring. However,\nwe find approximately 70% of the names recommended by these IDEs are different\nfrom what developers manually constructed, adding additional renaming burdens\nto developers and providing limited assistance. In this paper, we introduce\nVarNamer, an automated approach designed to recommend variable names for\nextract local variable refactorings. Through a large-scale empirical study, we\nidentify key contexts that are useful for composing variable names. Leveraging\nthese insights, we developed a set of heuristic rules through program static\nanalysis techniques and employ data mining techniques to recommend variable\nnames effectively. Notably, some of our heuristic rules have been successfully\nintegrated into Eclipse, where they are now distributed with the latest\nreleases of the IDE. Evaluation demonstrates its superiority over\nstate-of-the-art IDEs. Specifically, VarNamer significantly increases the\nchance of exact match by 52.6% compared to Eclipse and 40.7% compared to\nIntelliJ IDEA. We also evaluated the proposed approach with real-world extract\nlocal variable refactorings conducted in C++ projects, and the results suggest\nthat the approach can achieve comparable performance on programming languages\nbesides Java. It may suggest the generalizability of VarNamer. Finally, we\ndesigned and conducted a user study and the results of the user study suggest\nthat our approach can speed up the refactoring by 27.8% and reduce 49.3% edits\non the recommended variable names.", "AI": {"tldr": "This paper introduces VarNamer, a tool to recommend better variable names during extract local variable refactoring. It outperforms existing IDE recommendations and speeds up development.", "motivation": "IDE-recommended variable names during extract local variable refactoring often differ from developer preferences, creating inefficiency and additional renaming tasks.", "method": "They conducted a large-scale empirical study, identified useful naming contexts, developed heuristic rules using static analysis and data mining techniques, and incorporated some rules into IDEs like Eclipse.", "result": "VarNamer improved exact matches by 52.6% over Eclipse and by 40.7% over IntelliJ, demonstrated generalizability across programming languages like C++, and reduced 49.3% edits in refactoring during a user study.", "conclusion": "The proposed VarNamer approach greatly enhances variable naming accuracy, speeds up refactoring processes, and has the potential to generalize across different programming languages."}}
{"id": "2507.00044", "pdf": "https://arxiv.org/pdf/2507.00044", "abs": "https://arxiv.org/abs/2507.00044", "authors": ["Seyed Kahaki", "Alexander R. Webber", "Ghada Zamzmi", "Adarsh Subbaswamy", "Rucha Deshpande", "Aldo Badano"], "title": "HistoART: Histopathology Artifact Detection and Reporting Tool", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "14 pages, 5 figures", "summary": "In modern cancer diagnostics, Whole Slide Imaging (WSI) is widely used to\ndigitize tissue specimens for detailed, high-resolution examination; however,\nother diagnostic approaches, such as liquid biopsy and molecular testing, are\nalso utilized based on the cancer type and clinical context. While WSI has\nrevolutionized digital histopathology by enabling automated, precise analysis,\nit remains vulnerable to artifacts introduced during slide preparation and\nscanning. These artifacts can compromise downstream image analysis. To address\nthis challenge, we propose and compare three robust artifact detection\napproaches for WSIs: (1) a foundation model-based approach (FMA) using a\nfine-tuned Unified Neural Image (UNI) architecture, (2) a deep learning\napproach (DLA) built on a ResNet50 backbone, and (3) a knowledge-based approach\n(KBA) leveraging handcrafted features from texture, color, and frequency-based\nmetrics. The methods target six common artifact types: tissue folds,\nout-of-focus regions, air bubbles, tissue damage, marker traces, and blood\ncontamination. Evaluations were conducted on 50,000+ image patches from diverse\nscanners (Hamamatsu, Philips, Leica Aperio AT2) across multiple sites. The FMA\nachieved the highest patch-wise AUROC of 0.995 (95% CI [0.994, 0.995]),\noutperforming the ResNet50-based method (AUROC: 0.977, 95% CI [0.977, 0.978])\nand the KBA (AUROC: 0.940, 95% CI [0.933, 0.946]). To translate detection into\nactionable insights, we developed a quality report scorecard that quantifies\nhigh-quality patches and visualizes artifact distributions.", "AI": {"tldr": "This paper tackles the issue of diagnostic artifacts in Whole Slide Imaging (WSI) for cancer diagnostics by proposing three detection methods: foundation model-based, deep learning, and knowledge-based approaches.", "motivation": "Artifacts introduced during slide preparation and scanning can negatively affect downstream image analysis in digital histopathology, necessitating effective detection methods.", "method": "Three methods were proposed and compared: 1) a foundation model-based approach using a Unified Neural Image architecture, 2) a deep learning model based on ResNet50, and 3) a knowledge-based method using handcrafted texture, color, and frequency features. Six artifact types were screened.", "result": "The foundation model-based approach achieved the best performance (AUROC: 0.995), followed by the deep learning approach (AUROC: 0.977) and the knowledge-based approach (AUROC: 0.940).", "conclusion": "The proposed artifact detection methods improve the reliability of WSI image analysis, enabling actionable quality insights and artifact visualization."}}
{"id": "2507.00640", "pdf": "https://arxiv.org/pdf/2507.00640", "abs": "https://arxiv.org/abs/2507.00640", "authors": ["Denis Belomestny", "John. Schoenmakers"], "title": "Forward Reverse Kernel Regression for the Schr\u00f6dinger bridge problem", "categories": ["stat.ML", "cs.LG", "cs.NA", "math.NA", "90C40, 65C05, 62G08"], "comment": null, "summary": "In this paper, we study the Schr\\\"odinger Bridge Problem (SBP), which is\ncentral to entropic optimal transport. For general reference processes and\nbegin--endpoint distributions, we propose a forward-reverse iterative Monte\nCarlo procedure to approximate the Schr\\\"odinger potentials in a nonparametric\nway. In particular, we use kernel based Monte Carlo regression in the context\nof Picard iteration of a corresponding fixed point problem. By preserving in\nthe iteration positivity and contractivity in a Hilbert metric sense, we\ndevelop a provably convergent algorithm. Furthermore, we provide convergence\nrates for the potential estimates and prove their optimality. Finally, as an\napplication, we propose a non-nested Monte Carlo procedure for the final\ndimensional distributions of the Schr\\\"odinger Bridge process, based on the\nconstructed potentials and the forward-reverse simulation method for\nconditional diffusions.", "AI": {"tldr": "The paper introduces a Monte Carlo-based method for solving the Schr\u00f6dinger Bridge Problem (SBP), focusing on entropic optimal transport.", "motivation": "To develop efficient algorithms for solving the Schr\u00f6dinger Bridge Problem in entropic optimal transport, particularly for general reference processes and end-point distributions.", "method": "A forward-reverse iterative Monte Carlo procedure with kernel-based regression is used to approximate Schr\u00f6dinger potentials via Picard iteration while ensuring algorithm convergence using Hilbert metric properties.", "result": "The paper demonstrates theoretical convergence, provides convergence rates, proves the optimality of potential estimates, and showcases the method\u2019s application in non-nested Monte Carlo simulations for Schr\u00f6dinger Bridge process distributions.", "conclusion": "The study offers a robust, nonparametric algorithm with provable convergence for approximating Schr\u00f6dinger potentials and advancing SBP simulations, contributing to entropic optimal transport research."}}
{"id": "2507.00507", "pdf": "https://arxiv.org/pdf/2507.00507", "abs": "https://arxiv.org/abs/2507.00507", "authors": ["Chuhao Xu", "Zijun Li", "Quan Chen", "Han Zhao", "Minyi Guo"], "title": "LLM-Mesh: Enabling Elastic Sharing for Serverless LLM Inference", "categories": ["cs.DC"], "comment": null, "summary": "The rise of LLMs has driven demand for private serverless deployments,\ncharacterized by moderate-scale models and infrequent requests. While existing\nsolutions follow exclusive GPU deployment, we take a step back to explore\nmodern platforms and find that: Emerging CPU architectures with built-in\naccelerators are capable of serving LLMs but remain underutilized, and both\nCPUs and GPUs can accommodate multiple LLMs simultaneously.\n  We propose LLM-Mesh, a serverless inference scheme for small-to-mid-sized\nLLMs that enables elastic sharing across heterogeneous hardware. LLM-Mesh\ntackles three fundamental challenges: (1) precise, fine-grained compute\nresource allocation at token-level to handle fluctuating computational demands;\n(2) a coordinated and forward-looking memory scaling mechanism to detect\nout-of-memory hazards and reduce operational overhead; and (3) a dual approach\nthat reduces resource fragmentation through proactive preemption and reactive\nbin-packing. Experimental results on 4 32-core CPUs and 4 A100 GPUs show that\nLLM-Meshimproves service capacity by 44% - 63% through sharing, while further\nleveraging CPUs boosts this to 91% - 159%.", "AI": {"tldr": "LLM-Mesh introduces a serverless inference solution for small-to-mid-sized LLMs that effectively utilizes heterogeneous hardware, achieving enhanced computational efficiency and resource sharing.", "motivation": "The paper aims to address the underutilization of modern CPU architectures with built-in accelerators and explores ways to make both CPUs and GPUs effectively serve LLMs for private serverless deployments.", "method": "LLM-Mesh tackles challenges through fine-grained compute resource allocation at token-level, memory scaling mechanisms to detect hazards, and reducing resource fragmentation via proactive preemption and reactive bin-packing.", "result": "Experiments on mixed hardware setups show improvements in service capacity by 44%-63%, and leveraging CPUs further boosts this to 91%-159% capacity enhancement.", "conclusion": "LLM-Mesh demonstrates the potential for elastic sharing across heterogeneous hardware, enhancing serverless inference capabilities for LLMs and optimizing resource utilization."}}
{"id": "2507.00855", "pdf": "https://arxiv.org/pdf/2507.00855", "abs": "https://arxiv.org/abs/2507.00855", "authors": ["Marta Navarro", "Josu\u00e9 Feliu", "Salvador Petit", "Mar\u00eda E. G\u00f3mez", "Julio Sahuquillo"], "title": "A New Family of Thread to Core Allocation Policies for an SMT ARM Processor", "categories": ["cs.DC", "cs.AR"], "comment": "13 pages", "summary": "Modern high-performance servers commonly integrate Simultaneous\nMultithreading (SMT) processors, which efficiently boosts throughput over\nsingle-threaded cores. Optimizing performance in SMT processors faces\nchallenges due to the inter-application interference within each SMT core. To\nmitigate the interference, thread-to-core (T2C) allocation policies play a\npivotal role. State-of-the-art T2C policies work in two steps: i) building a\nper-application performance stack using performance counters and ii) building\nperformance prediction models to identify the best pairs of applications to run\non each core.\n  This paper explores distinct ways to build the performance stack in ARM\nprocessors and introduces the Instructions and Stalls Cycles (ISC) stack, a\nnovel approach to overcome ARM PMU limitations. The ISC stacks are used as\ninputs for a performance prediction model to estimate the applications'\nperformance considering the inter-application interference. The accuracy of the\nprediction model (second step) depends on the accuracy of the performance stack\n(first step); thus, the higher the accuracy of the performance stack, the\nhigher the potential performance gains obtained by the T2C allocation policy.\n  This paper presents SYNPA as a family of T2C allocation policies.\nExperimental results show that $SYNPA4$, the best-performing SYNPA variant,\noutperforms turnaround time by 38\\% over Linux, which represents 3$\\times$ the\ngains achieved by the state-of-the-art policies for ARM processors.\nFurthermore, the multiple discussions and refinements presented throughout this\npaper can be applied to other SMT processors from distinct vendors and are\naimed at helping performance analysts build performance stacks for accurate\nperformance estimates in real processors.", "AI": {"tldr": "The paper introduces SYNPA, a novel thread-to-core (T2C) allocation policy for SMT ARM processors leveraging the Instructions and Stalls Cycles (ISC) stack to address inter-application interference.", "motivation": "To optimize performance in SMT ARM processors while addressing challenges posed by inter-application interference using improved thread-to-core allocation policies.", "method": "Proposed the ISC stack as a novel approach to build accurate performance stacks and used it as input for performance prediction models, culminating in the design of the SYNPA family of T2C allocation policies.", "result": "The SYNPA4 variant outperformed Linux by 38% in turnaround time, achieving three times the gains of state-of-the-art policies for ARM processors.", "conclusion": "The SYNPA approach demonstrates significant performance improvements and provides a universally applicable framework for building accurate performance stacks across various SMT processors."}}
{"id": "2507.00909", "pdf": "https://arxiv.org/pdf/2507.00909", "abs": "https://arxiv.org/abs/2507.00909", "authors": ["Philip Colangelo", "Ayse K. Coskun", "Jack Megrue", "Ciaran Roberts", "Shayan Sengupta", "Varun Sivaram", "Ethan Tiao", "Aroon Vijaykar", "Chris Williams", "Daniel C. Wilson", "Zack MacFarland", "Daniel Dreiling", "Nathan Morey", "Anuja Ratnayake", "Baskar Vairamohan"], "title": "Turning AI Data Centers into Grid-Interactive Assets: Results from a Field Demonstration in Phoenix, Arizona", "categories": ["cs.DC", "cs.AI", "cs.PF", "cs.SY", "eess.SY"], "comment": "10 pages, 6 figures, 1 table", "summary": "Artificial intelligence (AI) is fueling exponential electricity demand\ngrowth, threatening grid reliability, raising prices for communities paying for\nnew energy infrastructure, and stunting AI innovation as data centers wait for\ninterconnection to constrained grids. This paper presents the first field\ndemonstration, in collaboration with major corporate partners, of a\nsoftware-only approach--Emerald Conductor--that transforms AI data centers into\nflexible grid resources that can efficiently and immediately harness existing\npower systems without massive infrastructure buildout. Conducted at a 256-GPU\ncluster running representative AI workloads within a commercial, hyperscale\ncloud data center in Phoenix, Arizona, the trial achieved a 25% reduction in\ncluster power usage for three hours during peak grid events while maintaining\nAI quality of service (QoS) guarantees. By orchestrating AI workloads based on\nreal-time grid signals without hardware modifications or energy storage, this\nplatform reimagines data centers as grid-interactive assets that enhance grid\nreliability, advance affordability, and accelerate AI's development.", "AI": {"tldr": "The paper introduces Emerald Conductor, a software approach to make AI data centers flexible assets for the power grid without requiring new infrastructure.", "motivation": "AI's increasing electricity demands strain grids, raise costs, and hinder innovation due to infrastructure delays.", "method": "Field-tested Emerald Conductor at a Phoenix-based 256-GPU AI cluster, optimizing workloads using real-time grid signals without hardware changes.", "result": "The trial reduced power usage by 25% during peak grid demand while maintaining AI performance guarantees.", "conclusion": "Emerald Conductor transforms data centers into grid-responsive assets, enhancing grid reliability and supporting AI's growth."}}
{"id": "2507.00054", "pdf": "https://arxiv.org/pdf/2507.00054", "abs": "https://arxiv.org/abs/2507.00054", "authors": ["Shreyansh Padarha"], "title": "Enhancing Reasoning Capabilities in SLMs with Reward Guided Dataset Distillation", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "17 Pages, 7 figures", "summary": "The push to compress and impart the proficiency of Large Language Models\n(LLMs) into more deployable and efficient Small Language Models (SLMs) has\nbenefited from improvements in knowledge distillation (KD) techniques. These\ntechniques allow a smaller student model to learn from a more capable and\nlarger teacher model's responses. However, distillation often revolves around\nthe student model merely copying the teacher's in-distribution responses,\nlimiting its generalisability. This limitation is amplified on reasoning tasks\nand can be computationally expensive. In this study, we propose AdvDistill, a\nreward-guided dataset distillation framework. We utilise multiple generations\n(responses) from a teacher for each prompt and assign rewards based on\nrule-based verifiers. These varying and normally distributed rewards serve as\nweights when training student models. Our methods and their subsequent\nbehavioural analysis demonstrate a significant improvement in student model\nperformance for mathematical and complex reasoning tasks, showcasing the\nefficacy and benefits of incorporating a rewarding mechanism in dataset\ndistillation processes.", "AI": {"tldr": "The paper proposes AdvDistill, a reward-guided dataset distillation framework that improves the student model's performance on reasoning tasks by assigning weights to teacher responses based on rewards.", "motivation": "Improve the efficiency and deployability of Small Language Models (SLMs) while addressing limitations in knowledge distillation techniques, particularly for reasoning tasks.", "method": "Introduce AdvDistill, which utilizes multiple teacher responses per prompt, assigns rewards via rule-based verifiers, and trains the student model using weighted responses.", "result": "Demonstrated significant improvement in student model performance for mathematical and complex reasoning tasks using AdvDistill.", "conclusion": "Incorporating a reward mechanism in dataset distillation enhances the efficacy of student models for reasoning challenges."}}
{"id": "2507.00216", "pdf": "https://arxiv.org/pdf/2507.00216", "abs": "https://arxiv.org/abs/2507.00216", "authors": ["Shreya Havaldar", "Adam Stein", "Eric Wong", "Lyle Ungar"], "title": "Towards Style Alignment in Cross-Cultural Translation", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025", "summary": "Successful communication depends on the speaker's intended style (i.e., what\nthe speaker is trying to convey) aligning with the listener's interpreted style\n(i.e., what the listener perceives). However, cultural differences often lead\nto misalignment between the two; for example, politeness is often lost in\ntranslation. We characterize the ways that LLMs fail to translate style -\nbiasing translations towards neutrality and performing worse in non-Western\nlanguages. We mitigate these failures with RASTA (Retrieval-Augmented STylistic\nAlignment), a method that leverages learned stylistic concepts to encourage LLM\ntranslation to appropriately convey cultural communication norms and align\nstyle.", "AI": {"tldr": "The paper discusses how cultural differences lead to misaligned communication in translations, particularly in stylistic nuances like politeness. It proposes RASTA, a method to improve stylistic alignment using learned concepts.", "motivation": "To address challenges where translations by LLMs fail to capture stylistic nuances, especially in non-Western languages, and bias translations towards neutrality.", "method": "RASTA is a retrieval-augmented method that incorporates learned stylistic concepts to enhance translations and align them with cultural communication norms.", "result": "RASTA successfully encourages translations to convey stylistic nuances and align better with cultural norms compared to standard LLM performance.", "conclusion": "RASTA shows promise in mitigating stylistic misalignment in translations caused by cultural differences through retrieval-augmented techniques."}}
{"id": "2507.00012", "pdf": "https://arxiv.org/pdf/2507.00012", "abs": "https://arxiv.org/abs/2507.00012", "authors": ["Linfeng Ye", "Shayan Mohajer Hamidi", "En-hui Yang"], "title": "Towards Undistillable Models by Minimizing Conditional Mutual Information", "categories": ["cs.LG", "cs.AI", "E.4"], "comment": "27 pages, 6 figures, Transactions on Machine Learning Research", "summary": "A deep neural network (DNN) is said to be undistillable if, when used as a\nblack-box input-output teacher, it cannot be distilled through knowledge\ndistillation (KD). In this case, the distilled student (referred to as the\nknockoff student) does not outperform a student trained independently with\nlabel smoothing (LS student) in terms of prediction accuracy. To protect\nintellectual property of DNNs, it is desirable to build undistillable DNNs. To\nthis end, it is first observed that an undistillable DNN may have the trait\nthat each cluster of its output probability distributions in response to all\nsample instances with the same label should be highly concentrated to the\nextent that each cluster corresponding to each label should ideally collapse\ninto one probability distribution. Based on this observation and by measuring\nthe concentration of each cluster in terms of conditional mutual information\n(CMI), a new training method called CMI minimized (CMIM) method is proposed,\nwhich trains a DNN by jointly minimizing the conventional cross entropy (CE)\nloss and the CMI values of all temperature scaled clusters across the entire\ntemperature spectrum. The resulting CMIM model is shown, by extensive\nexperiments, to be undistillable by all tested KD methods existing in the\nliterature. That is, the knockoff students distilled by these KD methods from\nthe CMIM model underperform the respective LS students. In addition, the CMIM\nmodel is also shown to performs better than the model trained with the CE loss\nalone in terms of their own prediction accuracy.", "AI": {"tldr": "The paper proposes a method to create deep neural networks (DNNs) that are undistillable, preventing efficient knowledge distillation (KD) and protecting intellectual property.", "motivation": "To safeguard intellectual property of DNNs by making them undistillable, ensuring students in KD processes cannot achieve better accuracy than independently trained models.", "method": "A CMI minimized (CMIM) method, which trains DNNs by jointly minimizing cross entropy loss and conditional mutual information across temperature scaled clusters.", "result": "Extensive experiments show CMIM-trained models are undistillable and outperform standard cross entropy-trained models in prediction accuracy.", "conclusion": "CMIM provides a reliable way to create undistillable DNNs, enhancing both security against KD attacks and predictive performance."}}
{"id": "2507.00273", "pdf": "https://arxiv.org/pdf/2507.00273", "abs": "https://arxiv.org/abs/2507.00273", "authors": ["Yusuke Tanaka", "Alvin Zhu", "Quanyou Wang", "Dennis Hong"], "title": "Mechanical Intelligence-Aware Curriculum Reinforcement Learning for Humanoids with Parallel Actuation", "categories": ["cs.RO"], "comment": null, "summary": "Reinforcement learning (RL) has enabled significant advances in humanoid\nrobot locomotion, yet most learning frameworks do not account for mechanical\nintelligence embedded in parallel actuation mechanisms due to limitations in\nsimulator support for closed kinematic chains. This omission can lead to\ninaccurate motion modeling and suboptimal policies, particularly for robots\nwith high actuation complexity. This paper presents an end-to-end curriculum RL\nframework for BRUCE, a kid-sized humanoid robot featuring three distinct\nparallel mechanisms in its legs: a differential pulley, a 5-bar linkage, and a\n4-bar linkage. Unlike prior approaches that rely on simplified serial\napproximations, we simulate all closed-chain constraints natively using\nGPU-accelerated MJX (MuJoCo), preserving the hardware's physical properties\nduring training. We benchmark our RL approach against a Model Predictive\nController (MPC), demonstrating better surface generalization and performance\nin real-world zero-shot deployment. This work highlights the computational\napproaches and performance benefits of fully simulating parallel mechanisms in\nend-to-end learning pipelines for legged humanoids.", "AI": {"tldr": "The paper introduces an RL system for a humanoid robot called BRUCE, which accurately simulates complex parallel mechanisms for better performance.", "motivation": "Learning frameworks often exclude mechanical intelligence in parallel actuation mechanisms because simulator support for closed chains is limited, leading to inaccuracies.", "method": "An end-to-end curriculum RL framework using GPU-accelerated MJX simulator ensures accurate closed-chain simulation for BRUCE's leg mechanisms.", "result": "The RL approach outperforms a Model Predictive Controller (MPC) in generalization and zero-shot real-world deployment.", "conclusion": "Fully simulating parallel mechanisms in humanoid RL pipelines leads to significant performance and computational advantages."}}
{"id": "2507.00320", "pdf": "https://arxiv.org/pdf/2507.00320", "abs": "https://arxiv.org/abs/2507.00320", "authors": ["Christiana Westlin", "Ashutosh Singh", "Deniz Erdogmus", "Georgios Stratis", "Lisa Feldman Barrett"], "title": "Exploring Theory-Laden Observations in the Brain Basis of Emotional Experience", "categories": ["cs.LG", "cs.CV", "q-bio.NC"], "comment": null, "summary": "In the science of emotion, it is widely assumed that folk emotion categories\nform a biological and psychological typology, and studies are routinely\ndesigned and analyzed to identify emotion-specific patterns. This approach\nshapes the observations that studies report, ultimately reinforcing the\nassumption that guided the investigation. Here, we reanalyzed data from one\nsuch typologically-guided study that reported mappings between individual brain\npatterns and group-averaged ratings of 34 emotion categories. Our reanalysis\nwas guided by an alternative view of emotion categories as populations of\nvariable, situated instances, and which predicts a priori that there will be\nsignificant variation in brain patterns within a category across instances.\nCorrespondingly, our analysis made minimal assumptions about the structure of\nthe variance present in the data. As predicted, we did not observe the original\nmappings and instead observed significant variation across individuals. These\nfindings demonstrate how starting assumptions can ultimately impact scientific\nconclusions and suggest that a hypothesis must be supported using multiple\nanalytic methods before it is taken seriously.", "AI": {"tldr": "The paper challenges traditional assumptions of fixed emotion-specific brain patterns, suggesting significant variability across individuals.", "motivation": "To question and test the widespread assumption that folk emotion categories represent fixed biological and psychological types.", "method": "Reanalyzed data from a prior study with minimal structural assumptions, emphasizing variability in brain patterns across emotional instances.", "result": "Observed significant variation in brain patterns across individuals, refuting the original study's findings of consistent mappings.", "conclusion": "Scientific findings are shaped by underlying assumptions, and hypotheses should be validated through multiple methods to ensure reliability."}}
{"id": "2507.00421", "pdf": "https://arxiv.org/pdf/2507.00421", "abs": "https://arxiv.org/abs/2507.00421", "authors": ["Parthiv Katapara", "Anand Sharma"], "title": "Embedded DevOps: A Survey on the Application of DevOps Practices in Embedded Software and Firmware Development", "categories": ["cs.SE"], "comment": "This paper present survey on DevOps practices which exists in\n  Embedded Software development", "summary": "The adoption of DevOps practices in embedded systems and firmware development\nis emerging as a response to the growing complexity of modern\nhardware--software co-designed products. Unlike cloud-native applications,\nembedded systems introduce challenges such as hardware dependency, real-time\nconstraints, and safety-critical requirements. This literature review\nsynthesizes findings from 20 academic and industrial sources to examine how\nDevOps principles--particularly continuous integration, continuous delivery,\nand automated testing--are adapted to embedded contexts. We categorize efforts\nacross tooling, testing strategies, pipeline automation, and security\npractices. The review highlights current limitations in deployment workflows\nand observability, proposing a roadmap for future research. This work offers\nresearchers and practitioners a consolidated understanding of Embedded DevOps,\nbridging fragmented literature with a structured perspective.", "AI": {"tldr": "This paper reviews how DevOps principles are adapted for embedded systems, addressing challenges like hardware dependency and safety-critical requirements.", "motivation": "The growing complexity of hardware-software co-design in embedded systems demands DevOps practices to enhance development efficiency and quality.", "method": "The paper synthesizes findings from 20 academic and industrial sources, categorizing efforts in tooling, testing, pipeline automation, and security.", "result": "Current limitations in deployment workflows and observability are identified, and a roadmap for future research is proposed.", "conclusion": "This work consolidates fragmented literature and provides a structured understanding of Embedded DevOps for researchers and practitioners."}}
{"id": "2507.00045", "pdf": "https://arxiv.org/pdf/2507.00045", "abs": "https://arxiv.org/abs/2507.00045", "authors": ["Ming Li", "Chenguang Wang", "Yijun Liang", "Xiyao Wang", "Yuhang Zhou", "Xiyang Wu", "Yuqing Zhang", "Ruiyi Zhang", "Tianyi Zhou"], "title": "CaughtCheating: Is Your MLLM a Good Cheating Detective? Exploring the Boundary of Visual Perception and Reasoning", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Recent agentic Multi-Modal Large Language Models (MLLMs) such as GPT-o3 have\nachieved near-ceiling scores on various existing benchmarks, motivating a\ndemand for more challenging test tasks. These MLLMs have been reported to excel\nin a few expert-level tasks for humans, e.g., GeoGuesser, reflecting their\npotential as a detective who can notice minuscule cues in an image and weave\nthem into coherent, situational explanations, leading to a reliable answer. But\ncan they match the performance of excellent human detectives? To answer this\nquestion, we investigate some hard scenarios where GPT-o3 can still handle, and\nfind a common scenario where o3's performance drops to nearly zero, which we\nname CaughtCheating. It is inspired by the social media requests that ask\nothers to detect suspicious clues from photos shared by the poster's partner.\nWe conduct extensive experiments and analysis to understand why existing MLLMs\nlack sufficient capability to solve this kind of task. CaughtCheating provides\na class of challenging visual perception and reasoning tasks with great value\nand practical usage. Success in these tasks paves the way for MLLMs to acquire\nhuman-level detective perception and reasoning capabilities.", "AI": {"tldr": "The paper evaluates GPT-o3, an advanced Multi-Modal Large Language Model (MLLM), to determine its capabilities in detective-like reasoning tasks with visual cues but identifies a task class, CaughtCheating, where it performs poorly.", "motivation": "To assess whether advanced MLLMs like GPT-o3 can match human-level detective reasoning capabilities in challenging visual scenarios.", "method": "The authors examine GPT-o3's performance in hard visual reasoning tasks inspired by real-life social media scenarios involving detection of suspicious clues in images.", "result": "GPT-o3 excels in expert-level tasks like GeoGuesser but shows significant shortcomings in solving CaughtCheating tasks, where its performance drops to nearly zero.", "conclusion": "The CaughtCheating tasks highlight limitations in current MLLMs' visual perception and reasoning abilities, indicating areas for improvement to achieve human-level detective capabilities."}}
{"id": "2507.00894", "pdf": "https://arxiv.org/pdf/2507.00894", "abs": "https://arxiv.org/abs/2507.00894", "authors": ["Davide Adamo", "Marco Corneli", "Manon Vuillien", "Emmanuelle Vila"], "title": "An in depth look at the Procrustes-Wasserstein distance: properties and barycenters", "categories": ["stat.ML", "cs.LG"], "comment": "16 pages", "summary": "Due to its invariance to rigid transformations such as rotations and\nreflections, Procrustes-Wasserstein (PW) was introduced in the literature as an\noptimal transport (OT) distance, alternative to Wasserstein and more suited to\ntasks such as the alignment and comparison of point clouds. Having that\napplication in mind, we carefully build a space of discrete probability\nmeasures and show that over that space PW actually is a distance. Algorithms to\nsolve the PW problems already exist, however we extend the PW framework by\ndiscussing and testing several initialization strategies. We then introduce the\nnotion of PW barycenter and detail an algorithm to estimate it from the data.\nThe result is a new method to compute representative shapes from a collection\nof point clouds. We benchmark our method against existing OT approaches,\ndemonstrating superior performance in scenarios requiring precise alignment and\nshape preservation. We finally show the usefulness of the PW barycenters in an\narchaeological context. Our results highlight the potential of PW in boosting\n2D and 3D point cloud analysis for machine learning and computational geometry\napplications.", "AI": {"tldr": "The paper introduces the Procrustes-Wasserstein (PW) distance as an optimal transport metric for better alignment and comparison of point clouds, proposes initialization strategies, defines barycenters for shape representation, and demonstrates superior performance in applications.", "motivation": "The authors aim to address challenges in effectively aligning and comparing point clouds, especially in tasks requiring invariance to rigid transformations like rotations and reflections, where traditional Wasserstein distance may fall short.", "method": "The paper carefully constructs a space of discrete probability measures in which PW is a valid distance, explores multiple initialization strategies for existing algorithms, and introduces a PW barycenter concept with an algorithm for estimating representative shapes.", "result": "The proposed PW framework and barycenters outperform existing optimal transport methods in alignment and shape preservation tasks, with demonstrated applications in 2D/3D point cloud analysis, including an archaeological context.", "conclusion": "Procrustes-Wasserstein is a promising metric for machine learning and computational geometry, enabling precise alignment and shape analysis. The PW barycenter method expands its practical applicability to diverse scenarios, such as archaeology."}}
{"id": "2507.00550", "pdf": "https://arxiv.org/pdf/2507.00550", "abs": "https://arxiv.org/abs/2507.00550", "authors": ["Bruce Fang", "Danyi Gao"], "title": "Collaborative Multi-Agent Reinforcement Learning Approach for Elastic Cloud Resource Scaling", "categories": ["cs.DC"], "comment": null, "summary": "This paper addresses the challenges of rapid resource variation and highly\nuncertain task loads in cloud computing environments. It proposes an\noptimization method for elastic cloud resource scaling based on a multi-agent\nsystem. The method deploys multiple autonomous agents to perceive resource\nstates in parallel and make local decisions. While maintaining the distributed\nnature of the system, it introduces a collaborative value function to achieve\nglobal coordination. This improves the responsiveness of resource scheduling\nand enhances overall system performance. To strengthen system foresight, a\nlightweight state prediction model is designed. It assists agents in\nidentifying future workload trends and optimizes the selection of scaling\nactions. For policy training, the method adopts a centralized training and\ndecentralized execution reinforcement learning framework. This enables agents\nto learn effectively and coordinate strategies under conditions of incomplete\ninformation. The paper also constructs typical cloud scenarios, including\nmulti-tenancy and burst traffic, to evaluate the proposed method. The\nevaluation focuses on resource isolation, service quality assurance, and\nrobustness. Experimental results show that the proposed multi-agent scaling\nstrategy outperforms existing methods in resource utilization, SLA violation\ncontrol, and scheduling latency. The results demonstrate strong adaptability\nand intelligent regulation. This provides an efficient and reliable new\napproach to solving the problem of elastic resource scaling in complex cloud\nplatforms.", "AI": {"tldr": "The paper proposes a multi-agent system-based optimization for elastic cloud resource scaling, achieving high efficiency and adaptability.", "motivation": "Cloud systems face challenges in handling rapid resource variations and uncertain task loads, requiring better scaling mechanisms for efficiency.", "method": "The method uses multi-agent systems with collaborative value functions and reinforcement learning for elastic scaling, implementing a lightweight prediction model for foresight.", "result": "The approach outperforms existing methods in resource utilization, SLA violations, and scheduling speed in simulated cloud environments.", "conclusion": "The proposed system provides an innovative, efficient, and robust solution to dynamic cloud resource scaling challenges."}}
{"id": "2507.00937", "pdf": "https://arxiv.org/pdf/2507.00937", "abs": "https://arxiv.org/abs/2507.00937", "authors": ["David Hunt", "Shaocheng Luo", "Spencer Hallyburton", "Shafii Nillongo", "Yi Li", "Tingjun Chen", "Miroslav Pajic"], "title": "RaGNNarok: A Light-Weight Graph Neural Network for Enhancing Radar Point Clouds on Unmanned Ground Vehicles", "categories": ["cs.RO", "cs.AR", "cs.CV", "cs.LG"], "comment": "8 pages, accepted by IROS 2025", "summary": "Low-cost indoor mobile robots have gained popularity with the increasing\nadoption of automation in homes and commercial spaces. However, existing lidar\nand camera-based solutions have limitations such as poor performance in\nvisually obscured environments, high computational overhead for data\nprocessing, and high costs for lidars. In contrast, mmWave radar sensors offer\na cost-effective and lightweight alternative, providing accurate ranging\nregardless of visibility. However, existing radar-based localization suffers\nfrom sparse point cloud generation, noise, and false detections. Thus, in this\nwork, we introduce RaGNNarok, a real-time, lightweight, and generalizable graph\nneural network (GNN)-based framework to enhance radar point clouds, even in\ncomplex and dynamic environments. With an inference time of just 7.3 ms on the\nlow-cost Raspberry Pi 5, RaGNNarok runs efficiently even on such\nresource-constrained devices, requiring no additional computational resources.\nWe evaluate its performance across key tasks, including localization, SLAM, and\nautonomous navigation, in three different environments. Our results demonstrate\nstrong reliability and generalizability, making RaGNNarok a robust solution for\nlow-cost indoor mobile robots.", "AI": {"tldr": "RaGNNarok is a graph neural network-based framework that improves radar-based localization for mobile robots, addressing issues like noise and sparse point clouds while being computationally efficient.", "motivation": "The paper aims to address the limitations in existing lidar and camera-based localization solutions, such as poor performance in visually obscured environments, high costs, and computational overhead. It leverages mmWave radar, which is cost-effective and performs well regardless of visibility.", "method": "The proposed method, RaGNNarok, utilizes a graph neural network to enhance radar point cloud data. It achieves real-time performance with low computational requirements, running efficiently on low-cost devices like the Raspberry Pi 5.", "result": "The framework is evaluated for localization, SLAM, and autonomous navigation tasks in three environments. Results show that RaGNNarok achieves strong reliability and generalizability.", "conclusion": "RaGNNarok is presented as a robust, efficient, and low-cost solution for real-time radar-based localization, suitable for indoor mobile robots."}}
{"id": "2507.00079", "pdf": "https://arxiv.org/pdf/2507.00079", "abs": "https://arxiv.org/abs/2507.00079", "authors": ["Ethan Smyth", "Alessandro Suglia"], "title": "VoyagerVision: Investigating the Role of Multi-modal Information for Open-ended Learning Systems", "categories": ["cs.AI", "cs.LG"], "comment": "website: https://esmyth-dev.github.io/VoyagerVision.github.io/", "summary": "Open-endedness is an active field of research in the pursuit of capable\nArtificial General Intelligence (AGI), allowing models to pursue tasks of their\nown choosing. Simultaneously, recent advancements in Large Language Models\n(LLMs) such as GPT-4o [9] have allowed such models to be capable of\ninterpreting image inputs. Implementations such as OMNI-EPIC [4] have made use\nof such features, providing an LLM with pixel data of an agent's POV to parse\nthe environment and allow it to solve tasks. This paper proposes that providing\nthese visual inputs to a model gives it greater ability to interpret spatial\nenvironments, and as such, can increase the number of tasks it can successfully\nperform, extending its open-ended potential. To this aim, this paper proposes\nVoyagerVision -- a multi-modal model capable of creating structures within\nMinecraft using screenshots as a form of visual feedback, building on the\nfoundation of Voyager. VoyagerVision was capable of creating an average of 2.75\nunique structures within fifty iterations of the system, as Voyager was\nincapable of this, it is an extension in an entirely new direction.\nAdditionally, in a set of building unit tests VoyagerVision was successful in\nhalf of all attempts in flat worlds, with most failures arising in more complex\nstructures. Project website is available at\nhttps://esmyth-dev.github.io/VoyagerVision.github.io/", "AI": {"tldr": "The paper introduces VoyagerVision, a multi-modal model that uses visual inputs for open-ended tasks in Minecraft, achieving notable success in constructing unique structures compared to its predecessor, Voyager.", "motivation": "To enhance open-ended capabilities in AGI by utilizing visual inputs, leveraging advancements in LLMs to interpret spatial environments for better task performance.", "method": "VoyagerVision integrates visual feedback through screenshots in a multi-modal system to guide structure creation in Minecraft, building on the Voyager framework.", "result": "VoyagerVision successfully created an average of 2.75 unique structures in fifty iterations and passed 50% of unit tests in flat-world building scenarios, surpassing the baseline Voyager model.", "conclusion": "Providing visual input to AI systems significantly enhances their ability to interpret environments and perform complex tasks, marking a step forward for open-ended AI research."}}
{"id": "2507.00239", "pdf": "https://arxiv.org/pdf/2507.00239", "abs": "https://arxiv.org/abs/2507.00239", "authors": ["Aryan Shrivastava", "Ari Holtzman"], "title": "Linearly Decoding Refused Knowledge in Aligned Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Most commonly used language models (LMs) are instruction-tuned and aligned\nusing a combination of fine-tuning and reinforcement learning, causing them to\nrefuse users requests deemed harmful by the model. However, jailbreak prompts\ncan often bypass these refusal mechanisms and elicit harmful responses. In this\nwork, we study the extent to which information accessed via jailbreak prompts\nis decodable using linear probes trained on LM hidden states. We show that a\ngreat deal of initially refused information is linearly decodable. For example,\nacross models, the response of a jailbroken LM for the average IQ of a country\ncan be predicted by a linear probe with Pearson correlations exceeding $0.8$.\nSurprisingly, we find that probes trained on base models (which do not refuse)\nsometimes transfer to their instruction-tuned versions and are capable of\nrevealing information that jailbreaks decode generatively, suggesting that the\ninternal representations of many refused properties persist from base LMs\nthrough instruction-tuning. Importantly, we show that this information is not\nmerely \"leftover\" in instruction-tuned models, but is actively used by them: we\nfind that probe-predicted values correlate with LM generated pairwise\ncomparisons, indicating that the information decoded by our probes align with\nsuppressed generative behavior that may be expressed more subtly in other\ndownstream tasks. Overall, our results suggest that instruction-tuning does not\nwholly eliminate or even relocate harmful information in representation\nspace-they merely suppress its direct expression, leaving it both linearly\naccessible and indirectly influential in downstream behavior.", "AI": {"tldr": "This paper reveals that harmful information suppressed in instruction-tuned LMs remains linearly decodable and can influence downstream tasks.", "motivation": "Investigate whether harmful information suppressed in language models through tuning is still accessible or influential.", "method": "The study trained linear probes on LM hidden states to decode suppressed data and analyzed correlation between refused and jailbroken outputs.", "result": "Many refused properties remain linearly decodable, with correlations exceeding 0.8 in some cases. Probes trained on base models transfer effectively to tuned models.", "conclusion": "Instruction-tuning suppresses harmful information's direct expression but does not eliminate it, leaving it accessible and impactful in downstream tasks."}}
{"id": "2507.00013", "pdf": "https://arxiv.org/pdf/2507.00013", "abs": "https://arxiv.org/abs/2507.00013", "authors": ["Hyunwoo Seo", "Chiehyeon Lim"], "title": "ST-MTM: Masked Time Series Modeling with Seasonal-Trend Decomposition for Time Series Forecasting", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "Accepted by KDD 2025 research track", "summary": "Forecasting complex time series is an important yet challenging problem that\ninvolves various industrial applications. Recently, masked time-series modeling\nhas been proposed to effectively model temporal dependencies for forecasting by\nreconstructing masked segments from unmasked ones. However, since the semantic\ninformation in time series is involved in intricate temporal variations\ngenerated by multiple time series components, simply masking a raw time series\nignores the inherent semantic structure, which may cause MTM to learn spurious\ntemporal patterns present in the raw data. To capture distinct temporal\nsemantics, we show that masked modeling techniques should address entangled\npatterns through a decomposition approach. Specifically, we propose ST-MTM, a\nmasked time-series modeling framework with seasonal-trend decomposition, which\nincludes a novel masking method for the seasonal-trend components that\nincorporates different temporal variations from each component. ST-MTM uses a\nperiod masking strategy for seasonal components to produce multiple masked\nseasonal series based on inherent multi-periodicity and a sub-series masking\nstrategy for trend components to mask temporal regions that share similar\nvariations. The proposed masking method presents an effective pre-training task\nfor learning intricate temporal variations and dependencies. Additionally,\nST-MTM introduces a contrastive learning task to support masked modeling by\nenhancing contextual consistency among multiple masked seasonal\nrepresentations. Experimental results show that our proposed ST-MTM achieves\nconsistently superior forecasting performance compared to existing masked\nmodeling, contrastive learning, and supervised forecasting methods.", "AI": {"tldr": "The paper introduces ST-MTM, a masked time-series modeling framework with seasonal-trend decomposition to enhance forecasting by disentangling temporal semantics.", "motivation": "Masked time-series modeling (MTM) faces challenges when modeling semantic information due to intricate temporal variations and entanglement of time series components, often leading to learning spurious patterns.", "method": "The authors propose ST-MTM, which employs seasonal-trend decomposition and novel masking strategies for seasonal and trend components. Seasonal components are masked using a period masking strategy, while trend components are masked using a sub-series masking strategy. The framework also incorporates a contrastive learning task to improve contextual consistency.", "result": "ST-MTM demonstrated superior forecasting performance across experiments compared to current methods, including traditional MTM, contrastive learning, and supervised forecasting techniques.", "conclusion": "The ST-MTM framework effectively captures distinct temporal semantics, making it a robust solution for forecasting complex time-series data."}}
{"id": "2507.00319", "pdf": "https://arxiv.org/pdf/2507.00319", "abs": "https://arxiv.org/abs/2507.00319", "authors": ["Tanmay Vilas Samak", "Chinmay Vilas Samak", "Bing Li", "Venkat Krovi"], "title": "When Digital Twins Meet Large Language Models: Realistic, Interactive, and Editable Simulation for Autonomous Driving", "categories": ["cs.RO"], "comment": null, "summary": "Simulation frameworks have been key enablers for the development and\nvalidation of autonomous driving systems. However, existing methods struggle to\ncomprehensively address the autonomy-oriented requirements of balancing: (i)\ndynamical fidelity, (ii) photorealistic rendering, (iii) context-relevant\nscenario orchestration, and (iv) real-time performance. To address these\nlimitations, we present a unified framework for creating and curating\nhigh-fidelity digital twins to accelerate advancements in autonomous driving\nresearch. Our framework leverages a mix of physics-based and data-driven\ntechniques for developing and simulating digital twins of autonomous vehicles\nand their operating environments. It is capable of reconstructing real-world\nscenes and assets (real2sim) with geometric and photorealistic accuracy and\ninfusing them with various physical properties to enable real-time dynamical\nsimulation of the ensuing driving scenarios. Additionally, it also incorporates\na large language model (LLM) interface to flexibly edit the driving scenarios\nonline via natural language prompts. We analyze the presented framework in\nterms of its fidelity, performance, and serviceability. Results indicate that\nour framework can reconstruct 3D scenes and assets with up to 97% structural\nsimilarity, while maintaining frame rates above 60 Hz. We also demonstrate that\nit can handle natural language prompts to generate diverse driving scenarios\nwith up to 95% repeatability and 85% generalizability.", "AI": {"tldr": "A unified simulation framework designed for autonomous driving systems offers high-fidelity digital twins, real-time performance, and natural language-driven scenario adjustments.", "motivation": "The motivation is to address challenges in autonomous driving simulation, specifically balancing dynamical fidelity, photorealistic rendering, scenario orchestration, and performance.", "method": "The paper introduces a framework leveraging both physics-based and data-driven techniques to create and simulate high-fidelity digital twins. It includes real2sim reconstruction capabilities and integrates a large language model for natural language scenario editing.", "result": "High-fidelity 3D scene reconstruction achieves 97% structural similarity, maintains real-time performance above 60 Hz, and supports natural language scenario editing with 95% repeatability and 85% generalizability.", "conclusion": "This framework provides a robust tool for advancing autonomous driving research by offering a balanced and flexible simulation environment for testing and validation."}}
{"id": "2507.00481", "pdf": "https://arxiv.org/pdf/2507.00481", "abs": "https://arxiv.org/abs/2507.00481", "authors": ["Philipp M. Z\u00e4hl", "Sabine Theis", "Martin R. Wolf"], "title": "The Influence of HEXACO Personality Traits on the Teamwork Quality in Software Teams -- A Preliminary Research Approach", "categories": ["cs.SE", "cs.HC"], "comment": null, "summary": "Although software engineering research has focused on optimizing processes\nand technology, there is a growing recognition that human factors, particularly\nteamwork, also significantly impact optimization. Recent research suggests that\ndeveloper personality has a strong influence on teamwork. In fact, personality\nconsiderations may have a greater impact on software development than processes\nand tools. This paper aims to design a study that measures the impact of HEXACO\npersonality traits on the Teamwork Quality (TWQ) of software teams. A\npreliminary data collection (n=54) was conducted for this purpose. The analysis\nshowed that several personality traits, as well as their composition, had a\nsignificant impact on TWQ. Additionally, other variables, such as the\nproportion of women and age distribution, also affected TWQ. The study's\ninitial results demonstrate the usefulness and validity of the study design.\nThe results also suggest several opportunities to improve teamwork in IT\norganizations and avenues for further research.", "AI": {"tldr": "This paper investigates how HEXACO personality traits, gender, and age distributions impact teamwork quality in software engineering teams.", "motivation": "To address the growing recognition that human factors, especially personality traits, significantly influence teamwork quality in software development.", "method": "The study collected preliminary data from 54 participants to measure how HEXACO personality traits and other factors like gender and age affect teamwork quality.", "result": "Analysis revealed that personality traits, gender composition, and age distribution significantly influence the quality of teamwork.", "conclusion": "Initial findings validate the study's design and highlight areas where IT organizations can enhance teamwork, offering directions for further research."}}
{"id": "2507.00046", "pdf": "https://arxiv.org/pdf/2507.00046", "abs": "https://arxiv.org/abs/2507.00046", "authors": ["Akshansh Mishra", "Eyob Mesele Sefene", "Shivraman Thapliyal"], "title": "Evolutionary computing-based image segmentation method to detect defects and features in Additive Friction Stir Deposition Process", "categories": ["cs.CV", "cs.CE"], "comment": "7 pages, 4 figures", "summary": "This work proposes an evolutionary computing-based image segmentation\napproach for analyzing soundness in Additive Friction Stir Deposition (AFSD)\nprocesses. Particle Swarm Optimization (PSO) was employed to determine optimal\nsegmentation thresholds for detecting defects and features in multilayer AFSD\nbuilds. The methodology integrates gradient magnitude analysis with distance\ntransforms to create novel attention-weighted visualizations that highlight\ncritical interface regions. Five AFSD samples processed under different\nconditions were analyzed using multiple visualization techniques i.e.\nself-attention maps, and multi-channel visualization. These complementary\napproaches reveal subtle material transition zones and potential defect regions\nwhich were not readily observable through conventional imaging. The PSO\nalgorithm automatically identified optimal threshold values (ranging from\n156-173) for each sample, enabling precise segmentation of material interfaces.\nThe multi-channel visualization technique effectively combines boundary\ninformation (red channel), spatial relationships (green channel), and material\ndensity data (blue channel) into cohesive representations that quantify\ninterface quality. The results demonstrate that attention-based analysis\nsuccessfully identifies regions of incomplete bonding and inhomogeneities in\nAFSD joints, providing quantitative metrics for process optimization and\nquality assessment of additively manufactured components.", "AI": {"tldr": "The paper introduces an evolutionary computing approach using Particle Swarm Optimization (PSO) for improved defect detection and material interface analysis in Additive Friction Stir Deposition (AFSD) processes, utilizing novel visualization techniques.", "motivation": "To develop a more precise and automated method for detecting defects and analyzing material interfaces in AFSD processes, overcoming the limitations of conventional imaging techniques.", "method": "The proposed method employs PSO to determine optimal segmentation thresholds, integrates gradient magnitude analysis with distance transforms, and uses novel attention-weighted multi-channel visualization techniques for critical interface analysis.", "result": "The PSO algorithm determined effective segmentation thresholds for precise material interface analysis, while multi-channel visualization enhanced the detection of transition zones and defects, highlighting regions of incomplete bonding and inhomogeneities.", "conclusion": "The approach successfully identifies material defects and inhomogeneities in AFSD components, providing a foundation for improved quality control and optimization in additive manufacturing processes."}}
{"id": "2507.00576", "pdf": "https://arxiv.org/pdf/2507.00576", "abs": "https://arxiv.org/abs/2507.00576", "authors": ["Dante D. Sanchez-Gallegos", "J. L. Gonzalez-Compean", "Maxime Gonthier", "Valerie Hayot-Sasson", "J. Gregory Pauloski", "Haochen Pan", "Kyle Chard", "Jesus Carretero", "Ian Foster"], "title": "DynoStore: A wide-area distribution system for the management of data over heterogeneous storage", "categories": ["cs.DC"], "comment": "10 pages. Conference: The 25th IEEE International Symposium on\n  Cluster, Cloud, and Internet Computing", "summary": "Data distribution across different facilities offers benefits such as\nenhanced resource utilization, increased resilience through replication, and\nimproved performance by processing data near its source. However, managing such\ndata is challenging due to heterogeneous access protocols, disparate\nauthentication models, and the lack of a unified coordination framework. This\npaper presents DynoStore, a system that manages data across heterogeneous\nstorage systems. At the core of DynoStore are data containers, an abstraction\nthat provides standardized interfaces for seamless data management,\nirrespective of the underlying storage systems. Multiple data container\nconnections create a cohesive wide-area storage network, ensuring resilience\nusing erasure coding policies. Furthermore, a load-balancing algorithm ensures\nequitable and efficient utilization of storage resources. We evaluate DynoStore\nusing benchmarks and real-world case studies, including the management of\nmedical and satellite data across geographically distributed environments. Our\nresults demonstrate a 10\\% performance improvement compared to centralized\ncloud-hosted systems while maintaining competitive performance with\nstate-of-the-art solutions such as Redis and IPFS. DynoStore also exhibits\nsuperior fault tolerance, withstanding more failures than traditional systems.", "AI": {"tldr": "The paper introduces DynoStore, a system for managing data across different storage systems efficiently and resiliently.", "motivation": "Data distribution across facilities improves resource utilization, resiliency, and performance, but current systems struggle with heterogeneous protocols and lack unified coordination.", "method": "DynoStore uses data containers as a standardized interface for storage systems, along with erasure coding for resilience and a load-balancing algorithm for efficient resource utilization.", "result": "DynoStore improves performance by 10% over centralized systems and competes with state-of-the-art solutions like Redis and IPFS while offering superior fault tolerance.", "conclusion": "DynoStore effectively addresses challenges in managing distributed data, ensuring efficiency, resilience, and fault tolerance across heterogeneous systems."}}
{"id": "2507.00949", "pdf": "https://arxiv.org/pdf/2507.00949", "abs": "https://arxiv.org/abs/2507.00949", "authors": ["Yuqing Wang", "Charles Colley", "Brian Wheatman", "Jiya Su", "David F. Gleich", "Andrew A. Chien"], "title": "How Fast Can Graph Computations Go on Fine-grained Parallel Architectures", "categories": ["cs.DC", "cs.AR"], "comment": "13 pages, 11 figures, 6 tables", "summary": "Large-scale graph problems are of critical and growing importance and\nhistorically parallel architectures have provided little support. In the spirit\nof co-design, we explore the question, How fast can graph computing go on a\nfine-grained architecture? We explore the possibilities of an architecture\noptimized for fine-grained parallelism, natural programming, and the\nirregularity and skew found in real-world graphs. Using two graph benchmarks,\nPageRank (PR) and Breadth-First Search (BFS), we evaluate a Fine-Grained Graph\narchitecture, UpDown, to explore what performance codesign can achieve. To\ndemonstrate programmability, we wrote five variants of these algorithms.\nSimulations of up to 256 nodes (524,288 lanes) and projections to 16,384 nodes\n(33M lanes) show the UpDown system can achieve 637K GTEPS PR and 989K GTEPS BFS\non RMAT, exceeding the best prior results by 5x and 100x respectively.", "AI": {"tldr": "This paper investigates high-performance graph computing on the fine-grained UpDown architecture, showcasing improvements in PageRank and BFS performance by significant factors.", "motivation": "Graph algorithms demand high computational power, and current architectures lack efficient support for such operations. A co-designed architecture could address this gap.", "method": "The authors designed the UpDown architecture optimized for fine-grained parallelism and irregular graph processing, running simulations on it for PageRank and BFS algorithms.", "result": "The UpDown system demonstrated a performance exceeding the best known results\u2014637K GTEPS for PageRank (5x improvement) and 989K GTEPS for BFS (100x improvement) on RMAT benchmark graphs.", "conclusion": "Fine-grained architectures like UpDown significantly enhance graph computing, achieving high scalability and superior performance compared to traditional architectures."}}
{"id": "2507.00092", "pdf": "https://arxiv.org/pdf/2507.00092", "abs": "https://arxiv.org/abs/2507.00092", "authors": ["Basab Jha", "Firoj Paudel", "Ujjwal Puri", "Zhang Yuting", "Choi Donghyuk", "Wang Junhao"], "title": "Thinking About Thinking: SAGE-nano's Inverse Reasoning for Self-Aware Language Models", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "19 pages, 2 figures, 9 tables", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities at\nsolving complex reasoning tasks with Chain-of-Thought (CoT) prompting, but\ntheir decision-making processes remain somewhat blackbox. We introduce\ntextbfinverse reasoning, a novel paradigm enabling LLMs to decompose and\nexplain their own reasoning chains post-hoc. Our approach, used in SAGE-nano, a\n4-billion-parameter reasoning model, employs a metacognitive structure that\nreflects back via attention processes to identify major decision points and\ngenerate explanations of reasoning choices. While typical CoT approaches are\ndirected towards forward reasoning generation, inverse reasoning provides\ninsight into why specific reasoning chains were selected over others. Through\nthorough testing of logical reasoning puzzles, math problems and ethical\ndilemmas from AQUA-RAT, CommonsenseQA, and customized benchmarks, we\ndemonstrate that SAGE-nano is at the cutting edge both on reasoning accuracy\n(74.6% on AQUA-RAT) and explanation quality (92.1% human preference score) for\nits task, and offers performance almost on par with models like Claude-3.5\nSonnet or GPT-4o. Our contributions are: (i) the first rigorous framework for\nLLM self-reflection via inverse reasoning, (ii) a novel metalearning framework\nto reverse the attention flow, (iii) comprehensive evaluation frameworks for\nreasoning transparency, and (iv) evidence that increasing reasoning using\ninverse reasoning improves interpretability along with reasoning performance.\nOur work creates new avenues for transparent AI systems and closes significant\ngaps in AI safety, education, and scientific discovery.", "AI": {"tldr": "The paper introduces 'inverse reasoning,' a novel approach for Large Language Models (LLMs) to explain their reasoning chains post-hoc, improving interpretability and performance.", "motivation": "LLMs excel in reasoning tasks but lack transparency in their decision-making processes, posing challenges for AI safety, education, and scientific discovery.", "method": "The authors developed SAGE-nano, a 4-billion-parameter reasoning model that leverages inverse reasoning via a metacognitive framework to reflect on decision points and generate reasoning explanations.", "result": "SAGE-nano achieved a reasoning accuracy of 74.6% on AQUA-RAT and an explanation quality score of 92.1% human preference, approaching performance levels of advanced models like GPT-4o.", "conclusion": "Inverse reasoning enhances interpretability and reasoning performance, contributing to safer and more transparent AI systems while addressing significant gaps in reasoning transparency."}}
{"id": "2507.00244", "pdf": "https://arxiv.org/pdf/2507.00244", "abs": "https://arxiv.org/abs/2507.00244", "authors": ["Isabella Senturia", "Matilde Marcolli"], "title": "The Algebraic Structure of Morphosyntax", "categories": ["cs.CL", "math.QA", "91F20, 18M60, 68Q70"], "comment": "45 pages, LaTeX, 2 png figures", "summary": "Within the context of the mathematical formulation of Merge and the Strong\nMinimalist Thesis, we present a mathematical model of the morphology-syntax\ninterface. In this setting, morphology has compositional properties responsible\nfor word formation, organized into a magma of morphological trees. However,\nunlike syntax, we do not have movement within morphology. A coproduct\ndecomposition exists, but it requires extending the set of morphological trees\nbeyond those which are generated solely by the magma, to a larger set of\npossible morphological inputs to syntactic trees. These participate in the\nformation of morphosyntactic trees as an algebra over an operad, and a\ncorrespondence between algebras over an operad. The process of structure\nformation for morphosyntactic trees can then be described in terms of this\noperadic correspondence that pairs syntactic and morphological data and the\nmorphology coproduct. We reinterpret in this setting certain operations of\nDistributed Morphology as transformation that allow for flexibility in moving\nthe boundary between syntax and morphology within the morphosyntactic objects.", "AI": {"tldr": "The paper develops a mathematical model for the morphology-syntax interface based on the Strong Minimalist Thesis, focusing on morphological trees and their connections with syntax through operadic algebra.", "motivation": "To mathematically formalize the interface between morphology and syntax using principles from the Strong Minimalist Thesis, addressing challenges in understanding morphological composition and its interaction with syntax.", "method": "The paper employs mathematical concepts like operads and coproduct decomposition to model the relationship between morphological and syntactic structures, leveraging Distributed Morphology to adjust the boundary between syntax and morphology.", "result": "A detailed framework is presented where morphological trees are extended beyond the magma, forming morphosyntactic trees through operadic algebra and demonstrating flexibility in the syntax-morphology boundary.", "conclusion": "The study provides a formal operadic framework for understanding structure formation in morphosyntactic trees, enabling advances in modeling linguistic phenomena and refining Distributed Morphology operations."}}
{"id": "2507.00014", "pdf": "https://arxiv.org/pdf/2507.00014", "abs": "https://arxiv.org/abs/2507.00014", "authors": ["Thomas Joshi", "Shayan Chowdhury", "Fatih Uysal"], "title": "SWE-Bench-CL: Continual Learning for Coding Agents", "categories": ["cs.LG", "cs.AI", "cs.SE"], "comment": null, "summary": "Large Language Models (LLMs) have achieved impressive results on static\ncode-generation benchmarks, but real-world software development unfolds as a\ncontinuous stream of evolving issues, fixes, and feature requests. We introduce\nSWE-Bench-CL, a novel continual learning benchmark built on the human-verified\nSWE-Bench Verified dataset introduced by OpenAI and Princeton-NLP in 2024. By\norganizing GitHub issues into chronologically ordered sequences that reflect\nnatural repository evolution, SWE-Bench-CL enables direct evaluation of an\nagent's ability to accumulate experience, transfer knowledge across tasks, and\nresist catastrophic forgetting. We complement the dataset with (i) a\npreliminary analysis of inter-task structural similarity and contextual\nsensitivity, (ii) an interactive LangGraph-based evaluation framework augmented\nwith a FAISS-backed semantic memory module, and (iii) a suite of specialized\ncontinual learning metrics -- including average accuracy, forgetting,\nforward/backward transfer, tool-use efficiency, and a generalized Composite\nContinual Learning Score and CL-F-beta score -- to capture the\nstability-plasticity trade-off. We outline a rigorous experimental protocol\ncomparing memory-enabled and memory-disabled agents across diverse Python\nrepositories. All code and data are publicly available at\nhttps://github.com/thomasjoshi/agents-never-forget, providing the community\nwith a reproducible platform for developing more adaptive and robust AI agents\nin software engineering.", "AI": {"tldr": "This paper introduces SWE-Bench-CL, a benchmark designed for analyzing large language models in continual code-generation tasks, emphasizing their ability to learn, transfer knowledge, and avoid forgetting.", "motivation": "To address the gap between static code-generation benchmarks and the dynamic, evolving nature of real-world software development.", "method": "The SWE-Bench-CL benchmark organizes chronological GitHub issues, incorporates structural and contextual task analysis, and utilizes specialized metrics and an evaluation framework for agents with/without memory.", "result": "SWE-Bench-CL demonstrated robust methodologies to evaluate agents against continual learning challenges in adaptive code-generation, enabling a deeper understanding of AI agents' performance.", "conclusion": "SWE-Bench-CL offers a reproducible and comprehensive setup for advancing AI agents suited for dynamic software engineering environments, promoting adaptability and robustness."}}
{"id": "2507.00416", "pdf": "https://arxiv.org/pdf/2507.00416", "abs": "https://arxiv.org/abs/2507.00416", "authors": ["Tao Lin", "Gen Li", "Yilei Zhong", "Yanwen Zou", "Bo Zhao"], "title": "Evo-0: Vision-Language-Action Model with Implicit Spatial Understanding", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Vision-Language-Action (VLA) models have emerged as a promising framework for\nenabling generalist robots capable of perceiving, reasoning, and acting in the\nreal world. These models usually build upon pretrained Vision-Language Models\n(VLMs), which excel at semantic understanding due to large-scale text\npretraining. However, VLMs typically lack precise spatial understanding\ncapabilities, as they are primarily tuned on 2D image-text pairs without 3D\nsupervision. To address this limitation, recent approaches have incorporated\nexplicit 3D inputs such as point clouds or depth maps, but this necessitates\nadditional depth sensors or defective estimation. In contrast, our work\nintroduces a plug-and-play module that implicitly injects 3D geometry features\ninto VLA models by leveraging an off-the-shelf visual geometry foundation\nmodels. We design five spatially challenging tasks that require precise spatial\nunderstanding ability to validate effectiveness of our method. Extensive\nevaluations show that our method significantly improves the performance of\nstate-of-the-art VLA models across diverse scenarios.", "AI": {"tldr": "This paper proposes a novel module to enhance Vision-Language-Action models by integrating 3D geometry features without needing explicit depth sensors.", "motivation": "Current Vision-Language Models lack precise spatial understanding due to being primarily trained on 2D image-text pairs, which limits their applications in complex real-world scenarios.", "method": "The authors introduce a plug-and-play module that implicitly integrates 3D geometry features into VLA models, using off-the-shelf visual geometry foundation models.", "result": "The module significantly improves performance in spatially complex tasks across diverse scenarios, as verified through extensive evaluations.", "conclusion": "The proposed method successfully enhances the spatial understanding capabilities of VLA models, bridging the gap between semantic and geometric understanding without additional hardware requirements."}}
{"id": "2507.00496", "pdf": "https://arxiv.org/pdf/2507.00496", "abs": "https://arxiv.org/abs/2507.00496", "authors": ["Hongjing Guo", "Chuanqi Tao", "Zhiqiu Huang", "Weiqin Zou"], "title": "Coverage-Guided Testing for Deep Learning Models: A Comprehensive Survey", "categories": ["cs.SE"], "comment": null, "summary": "As Deep Learning (DL) models are increasingly applied in safety-critical\ndomains, ensuring their quality has emerged as a pressing challenge in modern\nsoftware engineering. Among emerging validation paradigms, coverage-guided\ntesting (CGT) has gained prominence as a systematic framework for identifying\nerroneous or unexpected model behaviors. Despite growing research attention,\nexisting CGT studies remain methodologically fragmented, limiting the\nunderstanding of current advances and emerging trends. This work addresses that\ngap through a comprehensive review of state-of-the-art CGT methods for DL\nmodels, including test coverage analysis, coverage-guided test input\ngeneration, and coverage-guided test input optimization. This work provides\ndetailed taxonomies to organize these methods based on methodological\ncharacteristics and application scenarios. We also investigate evaluation\npractices adopted in existing studies, including the use of benchmark datasets,\nmodel architectures, and evaluation aspects. Finally, open challenges and\nfuture directions are highlighted in terms of the correlation between\nstructural coverage and testing objectives, method generalizability across\ntasks and models, practical deployment concerns, and the need for standardized\nevaluation and tool support. This work aims to provide a roadmap for future\nacademic research and engineering practice in DL model quality assurance.", "AI": {"tldr": "The paper reviews current methods in coverage-guided testing (CGT) for deep learning models, presents detailed taxonomies, and highlights open research challenges and future directions.", "motivation": "Deep learning models are increasingly used in safety-critical domains, raising the need for robust quality assurance methods like coverage-guided testing.", "method": "A comprehensive review of state-of-the-art CGT methods, organizing them into taxonomies, analyzing evaluation practices, and identifying gaps in research.", "result": "The paper categorizes CGT methods based on characteristics and application scenarios and discusses evaluation practices, challenges, and research trends.", "conclusion": "The paper provides a roadmap for advancing research and practical implementations of CGT in deep learning model quality assurance, emphasizing the importance of standardization and generalizability."}}
{"id": "2507.00049", "pdf": "https://arxiv.org/pdf/2507.00049", "abs": "https://arxiv.org/abs/2507.00049", "authors": ["Feiyang Kang", "Nadine Chang", "Maying Shen", "Marc T. Law", "Rafid Mahmood", "Ruoxi Jia", "Jose M. Alvarez"], "title": "AdaDeDup: Adaptive Hybrid Data Pruning for Efficient Large-Scale Object Detection Training", "categories": ["cs.CV", "cs.LG"], "comment": "Preprint", "summary": "The computational burden and inherent redundancy of large-scale datasets\nchallenge the training of contemporary machine learning models. Data pruning\noffers a solution by selecting smaller, informative subsets, yet existing\nmethods struggle: density-based approaches can be task-agnostic, while\nmodel-based techniques may introduce redundancy or prove computationally\nprohibitive. We introduce Adaptive De-Duplication (AdaDeDup), a novel hybrid\nframework that synergistically integrates density-based pruning with\nmodel-informed feedback in a cluster-adaptive manner. AdaDeDup first partitions\ndata and applies an initial density-based pruning. It then employs a proxy\nmodel to evaluate the impact of this initial pruning within each cluster by\ncomparing losses on kept versus pruned samples. This task-aware signal\nadaptively adjusts cluster-specific pruning thresholds, enabling more\naggressive pruning in redundant clusters while preserving critical data in\ninformative ones. Extensive experiments on large-scale object detection\nbenchmarks (Waymo, COCO, nuScenes) using standard models (BEVFormer, Faster\nR-CNN) demonstrate AdaDeDup's advantages. It significantly outperforms\nprominent baselines, substantially reduces performance degradation (e.g., over\n54% versus random sampling on Waymo), and achieves near-original model\nperformance while pruning 20% of data, highlighting its efficacy in enhancing\ndata efficiency for large-scale model training. Code is open-sourced.", "AI": {"tldr": "The paper proposes AdaDeDup, a framework combining density-based and model-informed pruning methods to efficiently reduce data redundancy and improve training efficiency for large-scale machine learning, maintaining near-original performance while pruning significant portions of data.", "motivation": "The training of large-scale machine learning models is challenged by computational demands and data redundancy in large datasets, prompting the need for efficient pruning methods.", "method": "AdaDeDup employs a hybrid framework combining density-based data pruning with model-informed feedback. It adapts pruning thresholds cluster-wise based on task-aware evaluation by a proxy model, enabling retention of valuable data and aggressive pruning of redundant clusters.", "result": "Extensive experiments on benchmarks such as Waymo, COCO, and nuScenes show AdaDeDup achieves near-original performance while pruning up to 20% of data and outperforms other baseline methods.", "conclusion": "AdaDeDup proves effective for enhancing data efficiency in the training of large-scale models by integrating adaptive and task-aware data pruning mechanisms, with open-sourced code for broader application."}}
{"id": "2507.00020", "pdf": "https://arxiv.org/pdf/2507.00020", "abs": "https://arxiv.org/abs/2507.00020", "authors": ["Marcio Borges", "Felipe Pereira", "Michel Tosin"], "title": "Variational Autoencoder for Generating Broader-Spectrum prior Proposals in Markov chain Monte Carlo Methods", "categories": ["cs.LG", "stat.ML"], "comment": "The main contribution of this work is to show the advantages of using\n  deep generative models like VAE to provide more flexible and versatile prior\n  distributions", "summary": "This study uses a Variational Autoencoder method to enhance the efficiency\nand applicability of Markov Chain Monte Carlo (McMC) methods by generating\nbroader-spectrum prior proposals. Traditional approaches, such as the\nKarhunen-Lo\\`eve Expansion (KLE), require previous knowledge of the covariance\nfunction, often unavailable in practical applications. The VAE framework\nenables a data-driven approach to flexibly capture a broader range of\ncorrelation structures in Bayesian inverse problems, particularly subsurface\nflow modeling. The methodology is tested on a synthetic groundwater flow\ninversion problem, where pressure data is used to estimate permeability fields.\nNumerical experiments demonstrate that the VAE-based parameterization achieves\ncomparable accuracy to KLE when the correlation length is known and outperforms\nKLE when the assumed correlation length deviates from the true value. Moreover,\nthe VAE approach significantly reduces stochastic dimensionality, improving\ncomputational efficiency. The results suggest that leveraging deep generative\nmodels in McMC methods can lead to more adaptable and efficient Bayesian\ninference in high-dimensional problems.", "AI": {"tldr": "The paper presents a Variational Autoencoder (VAE)-based approach to improve Markov Chain Monte Carlo (McMC) methods by enabling a more adaptable and efficient generation of prior proposals.", "motivation": "Traditional methods like Karhunen-Lo\u00e8ve Expansion (KLE) rely on prior knowledge of covariance functions, which is often unavailable in practical scenarios. This motivates the search for data-driven solutions for Bayesian inverse problems.", "method": "The authors applied a VAE framework to flexibly parameterize high-dimensional Bayesian inverse problems, particularly targeting correlation structures in subsurface flow modeling.", "result": "The VAE approach achieved accuracy comparable to KLE when correlation lengths were known and performed better when correlation length assumptions were incorrect. Additionally, it reduced stochastic dimensionality and improved computational efficiency.", "conclusion": "The study concludes that deep generative models like VAEs can enhance McMC methods, making Bayesian inference more effective for complex, high-dimensional problems."}}
{"id": "2507.00716", "pdf": "https://arxiv.org/pdf/2507.00716", "abs": "https://arxiv.org/abs/2507.00716", "authors": ["Mohsen Koohi Esfahani"], "title": "Accelerating Loading WebGraphs in ParaGrapher", "categories": ["cs.DC"], "comment": null, "summary": "ParaGrapher is a graph loading API and library that enables graph processing\nframeworks to load large-scale compressed graphs with minimal overhead. This\ncapability accelerates the design and implementation of new high-performance\ngraph algorithms and their evaluation on a wide range of graphs and across\ndifferent frameworks. However, our previous study identified two major\nlimitations in ParaGrapher: inefficient utilization of high-bandwidth storage\nand reduced decompression bandwidth due to increased compression ratios. To\naddress these limitations, we present two optimizations for ParaGrapher in this\npaper. To improve storage utilization, particularly for high-bandwidth storage,\nwe introduce ParaGrapher-FUSE (PG-Fuse) a filesystem based on the FUSE\n(Filesystem in User Space). PG-Fuse optimizes storage access by increasing the\nsize of requested blocks, reducing the number of calls to the underlying\nfilesystem, and caching the received blocks in memory for future calls. To\nimprove the decompression bandwidth, we introduce CompBin, a compact binary\nrepresentation of the CSR format. CompBin facilitates direct accesses to\nneighbors while preventing storage usage for unused bytes. Our evaluation on 12\nreal-world and synthetic graphs with up to 128 billion edges shows that PG-Fuse\nand CompBin achieve up to 7.6 and 21.8 times speedup, respectively.", "AI": {"tldr": "ParaGrapher introduces optimizations PG-Fuse and CompBin for improved high-bandwidth storage utilization and decompression bandwidth.", "motivation": "The paper aims to address inefficiencies in ParaGrapher's handling of high-bandwidth storage and decompression tasks for graph processing.", "method": "The authors developed PG-Fuse, a FUSE-based filesystem optimizing storage access, and CompBin, a binary representation for efficient graph compression and access.", "result": "The optimizations achieved up to 7.6x speedup for storage utilization and up to 21.8x speedup for decompression bandwidth in tests across large-scale graphs.", "conclusion": "PG-Fuse and CompBin effectively overcome key limitations in ParaGrapher, enhancing its performance for loading and processing large-scale graphs."}}
{"id": "2507.00180", "pdf": "https://arxiv.org/pdf/2507.00180", "abs": "https://arxiv.org/abs/2507.00180", "authors": ["Vidhi Rathore"], "title": "BlackBoxToBlueprint: Extracting Interpretable Logic from Legacy Systems using Reinforcement Learning and Counterfactual Analysis", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Modernizing legacy software systems is a critical but challenging task, often\nhampered by a lack of documentation and understanding of the original system's\nintricate decision logic. Traditional approaches like behavioral cloning merely\nreplicate input-output behavior without capturing the underlying intent. This\npaper proposes a novel pipeline to automatically extract interpretable decision\nlogic from legacy systems treated as black boxes. The approach uses a\nReinforcement Learning (RL) agent to explore the input space and identify\ncritical decision boundaries by rewarding actions that cause meaningful changes\nin the system's output. These counterfactual state transitions, where the\noutput changes, are collected and clustered using K-Means. Decision trees are\nthen trained on these clusters to extract human-readable rules that approximate\nthe system's decision logic near the identified boundaries. I demonstrated the\npipeline's effectiveness on three dummy legacy systems with varying complexity,\nincluding threshold-based, combined-conditional, and non-linear range logic.\nResults show that the RL agent successfully focuses exploration on relevant\nboundary regions, and the extracted rules accurately reflect the core logic of\nthe underlying dummy systems, providing a promising foundation for generating\nspecifications and test cases during legacy migration.", "AI": {"tldr": "The paper presents a pipeline that uses reinforcement learning and decision trees to extract interpretable decision logic from legacy software systems.", "motivation": "Modernizing legacy systems is challenging because of lacking documentation and an understanding of intricate decision logic. Existing approaches fail to capture the intent underlying system behavior.", "method": "The proposed pipeline uses an RL agent to find decision boundaries by rewarding meaningful output changes. Critical state transitions are clustered using K-Means, and decision trees are trained to extract interpretable rules.", "result": "The method is tested on three dummy legacy systems with diverse complexities, successfully extracting accurate and human-readable decision rules from the systems.", "conclusion": "This approach effectively captures the core logic of legacy systems, offering a basis for generating specifications and test cases, facilitating system modernization."}}
{"id": "2507.00246", "pdf": "https://arxiv.org/pdf/2507.00246", "abs": "https://arxiv.org/abs/2507.00246", "authors": ["Sanchit Ahuja", "Praneetha Vaddamanu", "Barun Patra"], "title": "EfficientXLang: Towards Improving Token Efficiency Through Cross-Lingual Reasoning", "categories": ["cs.CL"], "comment": "15 pages, 5 figures, 9 tables", "summary": "Despite recent advances in Language Reasoning Models (LRMs), most research\nfocuses solely on English, even though many models are pretrained on\nmultilingual data. In this work, we investigate: Is English the most\ntoken-efficient language for reasoning? We evaluate three open-source RLMs:\nDeepSeek R1, Qwen 2.5 and Qwen 3, across four math datasets and seven\ntypologically diverse languages. We find that reasoning in non-English\nlanguages not only reduces token usage, but also preserves accuracy. These\ngains persist even after translating the reasoning traces into English,\nsuggesting genuine shifts in reasoning behavior rather than surface-level\nlinguistic effects. The extent of improvement, however, depends on the models\nmultilingual strength. Our findings motivate a broader view of reasoning in\nlanguage models, highlighting the potential of multilingual reasoning and the\nimportance of strong multilingual foundations. The code for our work can be\nfound: https://github.com/microsoft/EfficientXLang.", "AI": {"tldr": "The paper investigates whether reasoning in non-English languages is more token-efficient than in English, revealing that non-English languages reduce token usage and maintain accuracy, across diverse math datasets and languages.", "motivation": "To explore if English is truly the most efficient language for reasoning in multilingual pretrained language reasoning models and assess the impact of non-English reasoning on token efficiency and accuracy.", "method": "The authors evaluated three open-source reasoning models (DeepSeek R1, Qwen 2.5, Qwen 3) across four mathematical datasets and seven diverse languages, analyzing accuracy and token usage. They also translated non-English reasoning traces into English to study reasoning behavior changes.", "result": "Non-English languages achieved reduced token usage while maintaining accuracy. Improvements persisted after translation, demonstrating genuine reasoning shifts. Improvement levels depend on the models\u2019 multilingual training strength.", "conclusion": "The study emphasizes the potential of multilingual reasoning in LRMs and the importance of a strong multilingual foundation for achieving efficiency and accuracy beyond English."}}
{"id": "2507.00015", "pdf": "https://arxiv.org/pdf/2507.00015", "abs": "https://arxiv.org/abs/2507.00015", "authors": ["Lu Zhang", "Sangarapillai Lambotharan", "Gan Zheng", "Guisheng Liao", "Xuekang Liu", "Fabio Roli", "Carsten Maple"], "title": "Vision Transformer with Adversarial Indicator Token against Adversarial Attacks in Radio Signal Classifications", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": null, "summary": "The remarkable success of transformers across various fields such as natural\nlanguage processing and computer vision has paved the way for their\napplications in automatic modulation classification, a critical component in\nthe communication systems of Internet of Things (IoT) devices. However, it has\nbeen observed that transformer-based classification of radio signals is\nsusceptible to subtle yet sophisticated adversarial attacks. To address this\nissue, we have developed a defensive strategy for transformer-based modulation\nclassification systems to counter such adversarial attacks. In this paper, we\npropose a novel vision transformer (ViT) architecture by introducing a new\nconcept known as adversarial indicator (AdvI) token to detect adversarial\nattacks. To the best of our knowledge, this is the first work to propose an\nAdvI token in ViT to defend against adversarial attacks. Integrating an\nadversarial training method with a detection mechanism using AdvI token, we\ncombine a training time defense and running time defense in a unified neural\nnetwork model, which reduces architectural complexity of the system compared to\ndetecting adversarial perturbations using separate models. We investigate into\nthe operational principles of our method by examining the attention mechanism.\nWe show the proposed AdvI token acts as a crucial element within the ViT,\ninfluencing attention weights and thereby highlighting regions or features in\nthe input data that are potentially suspicious or anomalous. Through\nexperimental results, we demonstrate that our approach surpasses several\ncompetitive methods in handling white-box attack scenarios, including those\nutilizing the fast gradient method, projected gradient descent attacks and\nbasic iterative method.", "AI": {"tldr": "This paper proposes a Vision Transformer (ViT) architecture with an adversarial indicator (AdvI) token to improve robustness against adversarial attacks in transformer-based modulation classification systems.", "motivation": "The transformer models used for automatic modulation classification in IoT communication systems are vulnerable to adversarial attacks, necessitating improved defensive mechanisms.", "method": "The paper introduces a novel AdvI token within a ViT architecture to detect adversarial attacks. The method combines adversarial training and a detection mechanism into a single neural network model, reducing system complexity. Attention mechanisms are used to identify suspicious features in the input data.", "result": "The proposed approach demonstrates superior performance compared to competitive methods in white-box attack scenarios, including fast gradient method, projected gradient descent, and basic iterative method attacks.", "conclusion": "The AdvI token in ViT successfully enhances adversarial defense by influencing attention weights to detect anomalies, elegantly balancing training and running time defenses within a unified model."}}
{"id": "2507.00435", "pdf": "https://arxiv.org/pdf/2507.00435", "abs": "https://arxiv.org/abs/2507.00435", "authors": ["Yi Ru Wang", "Carter Ung", "Grant Tannert", "Jiafei Duan", "Josephine Li", "Amy Le", "Rishabh Oswal", "Markus Grotz", "Wilbert Pumacay", "Yuquan Deng", "Ranjay Krishna", "Dieter Fox", "Siddhartha Srinivasa"], "title": "RoboEval: Where Robotic Manipulation Meets Structured and Scalable Evaluation", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "Project page: https://robo-eval.github.io", "summary": "We present RoboEval, a simulation benchmark and structured evaluation\nframework designed to reveal the limitations of current bimanual manipulation\npolicies. While prior benchmarks report only binary task success, we show that\nsuch metrics often conceal critical weaknesses in policy behavior -- such as\npoor coordination, slipping during grasping, or asymmetric arm usage. RoboEval\nintroduces a suite of tiered, semantically grounded tasks decomposed into\nskill-specific stages, with variations that systematically challenge spatial,\nphysical, and coordination capabilities. Tasks are paired with fine-grained\ndiagnostic metrics and 3000+ human demonstrations to support imitation\nlearning. Our experiments reveal that policies with similar success rates\ndiverge in how tasks are executed -- some struggle with alignment, others with\ntemporally consistent bimanual control. We find that behavioral metrics\ncorrelate with success in over half of task-metric pairs, and remain\ninformative even when binary success saturates. By pinpointing when and how\npolicies fail, RoboEval enables a deeper, more actionable understanding of\nrobotic manipulation -- and highlights the need for evaluation tools that go\nbeyond success alone.", "AI": {"tldr": "RoboEval is a new benchmark for assessing bimanual robotic policies, offering detailed task metrics to uncover weaknesses often masked by binary success results.", "motivation": "Current benchmarking methods in robotic manipulation lack the ability to pinpoint specific policy failures, such as coordination flaws or asymmetric arm usage.", "method": "RoboEval introduces semantically tiered tasks with diagnostic metrics, supported by human demonstrations for training, to systematically evaluate weaknesses in policies.", "result": "Experiments show policies with similar success rates may fail differently. Behavioral metrics are found to be informative and correlate with success in over half of task-metric pairs.", "conclusion": "RoboEval provides a more actionable way to understand robot manipulation failures, emphasizing the importance of evaluation systems beyond binary task success metrics."}}
{"id": "2507.00686", "pdf": "https://arxiv.org/pdf/2507.00686", "abs": "https://arxiv.org/abs/2507.00686", "authors": ["Ronny Seiger", "Daniel Locher", "Marco Kaufmann", "Aaron F. Kurz"], "title": "A Domain-specific Language and Architecture for Detecting Process Activities from Sensor Streams in IoT", "categories": ["cs.SE", "cs.ET"], "comment": "Submitted to Internet of Things (ISSN 2542-6605)", "summary": "Modern Internet of Things (IoT) systems are equipped with a plethora of\nsensors providing real-time data about the current operations of their\ncomponents, which is crucial for the systems' internal control systems and\nprocesses. However, these data are often too fine-grained to derive useful\ninsights into the execution of the larger processes an IoT system might be part\nof. Process mining has developed advanced approaches for the analysis of\nbusiness processes that may also be used in the context of IoT. Bringing\nprocess mining to IoT requires an event abstraction step to lift the low-level\nsensor data to the business process level. In this work, we aim to empower\ndomain experts to perform this step using a newly developed domain-specific\nlanguage (DSL) called Radiant. Radiant supports the specification of patterns\nwithin the sensor data that indicate the execution of higher level process\nactivities. These patterns are translated to complex event processing (CEP)\napplications to be used for detecting activity executions at runtime. We\npropose a corresponding software architecture for online event abstraction from\nIoT sensor streams using the CEP applications. We evaluate these applications\nto monitor activity executions using IoT sensors in smart manufacturing and\nsmart healthcare. The evaluation method and results inform the domain expert\nabout the quality of activity detections and potential for improvement.", "AI": {"tldr": "The paper introduces Radiant, a domain-specific language for transforming low-level IoT sensor data into higher-level business process events, using complex event processing (CEP).", "motivation": "To address the challenge of deriving meaningful business process insights from fine-grained IoT sensor data, which current systems lack.", "method": "The researchers created a domain-specific language, Radiant, for specifying patterns in sensor data. These patterns translate into CEP applications, allowing real-time activity detection from IoT sensor streams.", "result": "They evaluated the proposed system in smart manufacturing and smart healthcare scenarios, demonstrating its effectiveness in detecting activity executions and providing guidance on activity detection quality.", "conclusion": "Radiant empowers domain experts to bridge the gap between IoT sensor data and business process analysis, with proven benefits in real-world scenarios."}}
{"id": "2507.00052", "pdf": "https://arxiv.org/pdf/2507.00052", "abs": "https://arxiv.org/abs/2507.00052", "authors": ["Binesh Sadanandan", "Vahid Behzadan"], "title": "VSF-Med:A Vulnerability Scoring Framework for Medical Vision-Language Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Vision Language Models (VLMs) hold great promise for streamlining\nlabour-intensive medical imaging workflows, yet systematic security evaluations\nin clinical settings remain scarce. We introduce VSF--Med, an end-to-end\nvulnerability-scoring framework for medical VLMs that unites three novel\ncomponents: (i) a rich library of sophisticated text-prompt attack templates\ntargeting emerging threat vectors; (ii) imperceptible visual perturbations\ncalibrated by structural similarity (SSIM) thresholds to preserve clinical\nrealism; and (iii) an eight-dimensional rubric evaluated by two independent\njudge LLMs, whose raw scores are consolidated via z-score normalization to\nyield a 0--32 composite risk metric. Built entirely on publicly available\ndatasets and accompanied by open-source code, VSF--Med synthesizes over 30,000\nadversarial variants from 5,000 radiology images and enables reproducible\nbenchmarking of any medical VLM with a single command. Our consolidated\nanalysis reports mean z-score shifts of $0.90\\sigma$ for\npersistence-of-attack-effects, $0.74\\sigma$ for prompt-injection effectiveness,\nand $0.63\\sigma$ for safety-bypass success across state-of-the-art VLMs.\nNotably, Llama-3.2-11B-Vision-Instruct exhibits a peak vulnerability increase\nof $1.29\\sigma$ for persistence-of-attack-effects, while GPT-4o shows increases\nof $0.69\\sigma$ for that same vector and $0.28\\sigma$ for prompt-injection\nattacks.", "AI": {"tldr": "The paper introduces VSF--Med, a framework to evaluate vulnerabilities in medical Vision Language Models (VLMs), using adversarial attacks and a scoring rubric.", "motivation": "To address the lack of systematic security evaluations for Vision Language Models in medical imaging workflows.", "method": "Develop an end-to-end vulnerability scoring framework featuring adversarial text prompts, subtle visual perturbations, and an evaluation rubric by judge LLMs.", "result": "Built over 30,000 adversarial variants to benchmark vulnerabilities, reporting z-score shifts indicative of vulnerability across state-of-the-art VLMs.", "conclusion": "VSF--Med facilitates security benchmarking of medical VLMs, emphasizing the need to address vulnerabilities in these systems."}}
{"id": "2507.00025", "pdf": "https://arxiv.org/pdf/2507.00025", "abs": "https://arxiv.org/abs/2507.00025", "authors": ["Tiexin Qin", "Hong Yan", "Haoliang Li"], "title": "Generalizing to New Dynamical Systems via Frequency Domain Adaptation", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "Accepted by TPAMI 2025", "summary": "Learning the underlying dynamics from data with deep neural networks has\nshown remarkable potential in modeling various complex physical dynamics.\nHowever, current approaches are constrained in their ability to make reliable\npredictions in a specific domain and struggle with generalizing to unseen\nsystems that are governed by the same general dynamics but differ in\nenvironmental characteristics. In this work, we formulate a parameter-efficient\nmethod, Fourier Neural Simulator for Dynamical Adaptation (FNSDA), that can\nreadily generalize to new dynamics via adaptation in the Fourier space.\nSpecifically, FNSDA identifies the shareable dynamics based on the known\nenvironments using an automatic partition in Fourier modes and learns to adjust\nthe modes specific for each new environment by conditioning on low-dimensional\nlatent systematic parameters for efficient generalization. We evaluate our\napproach on four representative families of dynamic systems, and the results\nshow that FNSDA can achieve superior or competitive generalization performance\ncompared to existing methods with a significantly reduced parameter cost. Our\ncode is available at https://github.com/WonderSeven/FNSDA.", "AI": {"tldr": "The paper introduces a method using Fourier Neural Simulator for Dynamical Adaptation (FNSDA) to improve the generalization and adaptability of learned dynamics in unseen environments via Fourier space adaptation.", "motivation": "The motivation is to address the limitation of current deep learning models in reliably predicting specific domains and generalizing unseen systems governed by similar dynamics but differing in environmental factors.", "method": "The proposed method, FNSDA, uses Fourier mode partitioning to identify shared dynamics and adjusts modes for new environments by conditioning on low-dimensional latent parameters.", "result": "FNSDA demonstrates superior or competitive generalization performance on four representative dynamic systems while significantly reducing parameter costs.", "conclusion": "FNSDA offers an efficient and effective way to generalize dynamics modeling to unseen environments, leveraging Fourier space adaptation to achieve competitive results with fewer parameters."}}
{"id": "2507.00181", "pdf": "https://arxiv.org/pdf/2507.00181", "abs": "https://arxiv.org/abs/2507.00181", "authors": ["Georgios P. Georgiou"], "title": "ChatGPT produces more \"lazy\" thinkers: Evidence of cognitive engagement decline", "categories": ["cs.AI"], "comment": null, "summary": "Despite the increasing use of large language models (LLMs) in education,\nconcerns have emerged about their potential to reduce deep thinking and active\nlearning. This study investigates the impact of generative artificial\nintelligence (AI) tools, specifically ChatGPT, on the cognitive engagement of\nstudents during academic writing tasks. The study employed an experimental\ndesign with participants randomly assigned to either an AI-assisted (ChatGPT)\nor a non-assisted (control) condition. Participants completed a structured\nargumentative writing task followed by a cognitive engagement scale (CES), the\nCES-AI, developed to assess mental effort, attention, deep processing, and\nstrategic thinking. The results revealed significantly lower cognitive\nengagement scores in the ChatGPT group compared to the control group. These\nfindings suggest that AI assistance may lead to cognitive offloading. The study\ncontributes to the growing body of literature on the psychological implications\nof AI in education and raises important questions about the integration of such\ntools into academic practice. It calls for pedagogical strategies that promote\nactive, reflective engagement with AI-generated content to avoid compromising\nself-regulated learning and deep cognitive involvement of students.", "AI": {"tldr": "The study examines how ChatGPT affects student cognitive engagement during academic writing, finding it reduces deep thinking and is linked to cognitive offloading.", "motivation": "Investigate concerns that generative AI, like ChatGPT, may reduce deep cognitive engagement in academic activities.", "method": "An experimental design where students were assigned to either an AI-assisted (ChatGPT) group or a control group, followed by completion of a writing task and a cognitive engagement scale.", "result": "ChatGPT-assisted students scored significantly lower on cognitive engagement compared to the control group, indicating cognitive offloading.", "conclusion": "AI tools like ChatGPT may hinder deep self-regulated learning, necessitating strategies for fostering reflective engagement when using such technologies in educational settings."}}
{"id": "2507.00258", "pdf": "https://arxiv.org/pdf/2507.00258", "abs": "https://arxiv.org/abs/2507.00258", "authors": ["Jie Hou", "Chuxiong Wu", "Lannan Luo", "Qiang Zeng"], "title": "Impact of Fine-Tuning Methods on Memorization in Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "As the capabilities of pre-trained large language models (LLMs) continue to\nadvance, the \"pre-train and fine-tune\" paradigm has become increasingly\nmainstream, leading to the development of various fine-tuning methods. However,\nthe privacy risks arising from memorization during fine-tuning have received\nrelatively little attention. To address this gap, we categorize popular\nfine-tuning approaches and assess their impact on memorization through the lens\nof membership inference attacks (MIAs). Our results show that, compared to\nparameter-based fine-tuning, prompt-based fine-tuning achieves competitive\nperformance while exhibiting lower vulnerability to MIAs. Furthermore,\nprompt-based methods maintain low memorization regardless of model scale. These\nfindings suggest that parameter-based fine-tuning is more prone to leaking\nprivate information, whereas prompt-based fine-tuning serves as a more\nprivacy-preserving option.", "AI": {"tldr": "The study examines the privacy risks, particularly data memorization, in fine-tuning large language models (LLMs), comparing parameter-based and prompt-based fine-tuning.", "motivation": "The paper aims to address the underexplored privacy risks, particularly data memorization, associated with fine-tuning pre-trained large language models (LLMs).", "method": "The authors categorize fine-tuning approaches and use membership inference attacks (MIAs) to evaluate their impact on data memorization across these methods.", "result": "Prompt-based fine-tuning exhibits lower vulnerability to memorization and MIAs compared to parameter-based fine-tuning, regardless of model size.", "conclusion": "Prompt-based fine-tuning is a more privacy-preserving approach compared to parameter-based fine-tuning, which is more prone to leaking private information."}}
{"id": "2507.00016", "pdf": "https://arxiv.org/pdf/2507.00016", "abs": "https://arxiv.org/abs/2507.00016", "authors": ["Xuanbo Liu", "Liu Liu", "Fuxiang Wu", "Fusheng Hao", "Xianglong Liu"], "title": "Gradient-based Fine-Tuning through Pre-trained Model Regularization", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Large pre-trained models have demonstrated extensive applications across\nvarious fields. However, fine-tuning these models for specific downstream tasks\ndemands significant computational resources and storage. One fine-tuning\nmethod, gradient-based parameter selection (GPS), focuses on fine-tuning only\nthe parameters with high gradients in each neuron, thereby reducing the number\nof training parameters. Nevertheless, this approach increases computational\nresource requirements and storage demands. In this paper, we propose an\nefficient gradient-based and regularized fine-tuning method (GRFT) that updates\nthe rows or columns of the weight matrix. We theoretically demonstrate that the\nrows or columns with the highest sum of squared gradients are optimal for\nupdating. This strategy effectively reduces storage overhead and improves the\nefficiency of parameter selection. Additionally, we incorporate regularization\nto enhance knowledge transfer from the pre-trained model. GRFT achieves\nstate-of-the-art performance, surpassing existing methods such as GPS, Adapter\nTuning, and LoRA. Notably, GRFT requires updating only 1.22% and 0.30% of the\ntotal parameters on FGVC and VTAB datasets, respectively, demonstrating its\nhigh efficiency and effectiveness. The source code will be released soon.", "AI": {"tldr": "This paper introduces GRFT, an efficient fine-tuning method for pre-trained models that updates select rows or columns of weight matrices, reducing parameter loads and surpassing existing methods.", "motivation": "Fine-tuning large pre-trained models for specific tasks is computationally demanding and requires excessive storage.", "method": "GRFT selectively updates rows or columns of the weight matrix based on the sum of squared gradients and incorporates regularization for better knowledge transfer.", "result": "GRFT outperforms methods such as GPS, Adapter Tuning, and LoRA, updating only 1.22% and 0.30% of parameters for FGVC and VTAB datasets, respectively.", "conclusion": "The proposed GRFT method enhances fine-tuning efficiency while achieving state-of-the-art performance compared to existing approaches."}}
{"id": "2507.00443", "pdf": "https://arxiv.org/pdf/2507.00443", "abs": "https://arxiv.org/abs/2507.00443", "authors": ["Reza Ahmadvand", "Sarah Safura Sharif", "Yaser Mike Banad"], "title": "Novel Pigeon-inspired 3D Obstacle Detection and Avoidance Maneuver for Multi-UAV Systems", "categories": ["cs.RO", "cs.AI", "cs.MA"], "comment": "11 Pages, 11 Pictures, 1 Table, 3 Algorithms", "summary": "Recent advances in multi-agent systems manipulation have demonstrated a\nrising demand for the implementation of multi-UAV systems in urban areas, which\nare always subjected to the presence of static and dynamic obstacles. Inspired\nby the collective behavior of tilapia fish and pigeons, the focus of the\npresented research is on the introduction of a nature-inspired collision-free\nformation control for a multi-UAV system, considering the obstacle avoidance\nmaneuvers. The developed framework in this study utilizes a semi-distributed\ncontrol approach, in which, based on a probabilistic Lloyd's algorithm, a\ncentralized guidance algorithm works for optimal positioning of the UAVs, while\na distributed control approach has been used for the intervehicle collision and\nobstacle avoidance. Further, the presented framework has been extended to the\n3D space with a novel definition of 3D maneuvers. Finally, the presented\nframework has been applied to multi-UAV systems in 2D and 3D scenarios, and the\nobtained results demonstrated the validity of the presented method in dynamic\nenvironments with stationary and moving obstacles.", "AI": {"tldr": "This research introduces a nature-inspired collision-free formation control framework for multi-UAV systems, using a probabilistic Lloyd's algorithm for centralized guidance and distributed control for collision avoidance in both 2D and 3D scenarios.", "motivation": "The motivation is to address the challenges faced by multi-UAV systems in urban environments, particularly collision-free navigation amidst static and dynamic obstacles. The research takes inspiration from the collective behavior of tilapia fish and pigeons.", "method": "A semi-distributed control approach is applied, combining centralized guidance through a probabilistic Lloyd's algorithm for UAV positioning and distributed control for obstacle avoidance. The framework is extended to handle 3D maneuvers.", "result": "The framework was validated through simulations in 2D and 3D scenarios, demonstrating its effectiveness in dynamic environments and with both stationary and moving obstacles.", "conclusion": "The developed nature-inspired multi-UAV control framework is effective for collision-free navigation and formation control in complex urban environments, including 3D spaces."}}
{"id": "2507.00699", "pdf": "https://arxiv.org/pdf/2507.00699", "abs": "https://arxiv.org/abs/2507.00699", "authors": ["Guoliang Duan", "Mingwei Liu", "Yanlin Wang", "Chong Wang", "Xin Peng", "Zibin Zheng"], "title": "A Hierarchical and Evolvable Benchmark for Fine-Grained Code Instruction Following with Multi-Turn Feedback", "categories": ["cs.SE"], "comment": null, "summary": "Large language models (LLMs) have advanced significantly in code generation,\nyet their ability to follow complex programming instructions with layered and\ndiverse constraints remains underexplored. Existing benchmarks often prioritize\nfunctional correctness, overlooking the nuanced requirements found in\nreal-world development. We introduce MultiCodeIF, a comprehensive benchmark\ndesigned to evaluate instruction-following in code generation across multiple\ndimensions: constraint type, hierarchical levels, and iterative refinement.\nBuilt upon a structured taxonomy of 9 categories and 27 constraint types,\nMultiCodeIF enables granular assessment of both functional and non-functional\ninstruction adherence. Using an automated pipeline, ConstraGen, we synthesize\nand evolve 2,021 code tasks sourced from 14 programming languages, supporting\nmulti-turn evaluation through feedback-driven task variants. Empirical\nevaluation of six state-of-the-art LLMs uncovers substantial performance\ndisparities. The top-performing model, Claude-3-7-Sonnet, achieves 63.0%\naverage constraint satisfaction, while smaller models like Qwen3-1.7B fall to\n44.8%. Models perform well on explicit constraints, but struggle with implicit\nor abstract constraints. Tasks with multiple hierarchical constraints\nsignificantly reduce model success rates, from 54.5% in single-level to just\n18.8% in multi-level scenarios. However, structured feedback enables\nprogressive improvement: average constraint satisfaction rises from 63.0% to\n83.4% over four iterative refinement rounds. MultiCodeIF provides a scalable,\nconstraint-aware, and feedback-sensitive framework to benchmark LLMs under\nrealistic code generation scenarios, bridging the gap between synthetic\nevaluations and real-world instruction complexity. The full benchmark dataset,\nevaluation pipeline, and source code are available at\nhttps://github.com/SYSUSELab/MultiCodeIF.", "AI": {"tldr": "Large language models' ability to generate code following complex programming instructions is evaluated using MultiCodeIF, a benchmark covering multiple dimensions, types of tasks, and feedback-driven iteration.", "motivation": "To address the lack of benchmarks focusing on both functional and non-functional programming requirements for assessing LLMs under real-world development constraints.", "method": "The researchers developed MultiCodeIF, a benchmark built on a structured taxonomy with 9 categories and 27 constraint types, utilizing an automated pipeline, ConstraGen, to create and evolve code tasks from 14 programming languages.", "result": "Performance evaluation of six LLMs showed significant disparities in constraint satisfaction rates, with top-performing models performing better on single-level tasks compared to multi-level hierarchical scenarios.", "conclusion": "MultiCodeIF enables a nuanced and comprehensive assessment of LLMs, demonstrating the need for iterative feedback and multi-dimensional benchmarks to bridge synthetic and real-world code generation scenarios."}}
{"id": "2507.00068", "pdf": "https://arxiv.org/pdf/2507.00068", "abs": "https://arxiv.org/abs/2507.00068", "authors": ["Ziqi Zhong", "Daniel Tang"], "title": "MANTA: Cross-Modal Semantic Alignment and Information-Theoretic Optimization for Long-form Multimodal Understanding", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "While multi-modal learning has advanced significantly, current approaches\noften treat modalities separately, creating inconsistencies in representation\nand reasoning. We introduce MANTA (Multi-modal Abstraction and Normalization\nvia Textual Alignment), a theoretically-grounded framework that unifies visual\nand auditory inputs into a structured textual space for seamless processing\nwith large language models. MANTA addresses four key challenges: (1) semantic\nalignment across modalities with information-theoretic optimization, (2)\nadaptive temporal synchronization for varying information densities, (3)\nhierarchical content representation for multi-scale understanding, and (4)\ncontext-aware retrieval of sparse information from long sequences. We formalize\nour approach within a rigorous mathematical framework, proving its optimality\nfor context selection under token constraints. Extensive experiments on the\nchallenging task of Long Video Question Answering show that MANTA improves\nstate-of-the-art models by up to 22.6% in overall accuracy, with particularly\nsignificant gains (27.3%) on videos exceeding 30 minutes. Additionally, we\ndemonstrate MANTA's superiority on temporal reasoning tasks (23.8% improvement)\nand cross-modal understanding (25.1% improvement). Our framework introduces\nnovel density estimation techniques for redundancy minimization while\npreserving rare signals, establishing new foundations for unifying multimodal\nrepresentations through structured text.", "AI": {"tldr": "MANTA unifies visual and auditory inputs into a textual structured space for optimized multimodal learning, achieving significant performance gains in tasks like Long Video Question Answering.", "motivation": "Current multimodal learning approaches treat visual and auditory data separately, leading to inconsistencies in representation and reasoning.", "method": "MANTA uses semantic alignment, adaptive temporal synchronization, hierarchical content representation, and context-aware retrieval within a mathematical framework optimized for token constraints.", "result": "MANTA achieves up to 22.6% improvement in overall accuracy, including 27.3% on long videos and significant gains in temporal reasoning and cross-modal understanding tasks.", "conclusion": "MANTA establishes a unified approach for multimodal learning by leveraging textual structured space for seamless integration and improved task performance."}}
{"id": "2507.00073", "pdf": "https://arxiv.org/pdf/2507.00073", "abs": "https://arxiv.org/abs/2507.00073", "authors": ["Urvi Pawar", "Kunal Telangi"], "title": "Fractional Policy Gradients: Reinforcement Learning with Long-Term Memory", "categories": ["cs.LG", "stat.ML", "I.2.6; I.2.8"], "comment": "Submitted to Journal of Machine Learning Research (JMLR), June 2025.\n  24 pages, 3 figures. Under review", "summary": "We propose Fractional Policy Gradients (FPG), a reinforcement learning\nframework incorporating fractional calculus for long-term temporal modeling in\npolicy optimization. Standard policy gradient approaches face limitations from\nMarkovian assumptions, exhibiting high variance and inefficient sampling. By\nreformulating gradients using Caputo fractional derivatives, FPG establishes\npower-law temporal correlations between state transitions. We develop an\nefficient recursive computation technique for fractional temporal-difference\nerrors with constant time and memory requirements. Theoretical analysis shows\nFPG achieves asymptotic variance reduction of order O(t^(-alpha)) versus\nstandard policy gradients while preserving convergence. Empirical validation\ndemonstrates 35-68% sample efficiency gains and 24-52% variance reduction\nversus state-of-the-art baselines. This framework provides a mathematically\ngrounded approach for leveraging long-range dependencies without computational\noverhead.", "AI": {"tldr": "The paper introduces Fractional Policy Gradients (FPG), a reinforcement learning framework utilizing fractional calculus to address inefficiencies in standard policy gradients. FPG reduces variance and improves sample efficiency in policy optimization.", "motivation": "The motivation is to overcome the limitations of standard policy gradients, which rely on Markovian assumptions and suffer from high variance and sampling inefficiencies. The authors aim to integrate long-term temporal modeling into the policy optimization process.", "method": "The authors reformulate policy gradients using Caputo fractional derivatives to establish long-range temporal correlations. They also develop a recursive computation method for fractional temporal-difference errors with efficient time and memory usage.", "result": "Empirical tests show that FPG achieves 35-68% improvements in sample efficiency and reduces variance by 24-52% compared to leading baselines, while preserving convergence properties.", "conclusion": "Fractional Policy Gradients offer a powerful mathematical framework for incorporating long-range dependencies in reinforcement learning without adding computational complexity, outperforming existing approaches in efficiency and variance reduction."}}
{"id": "2507.00205", "pdf": "https://arxiv.org/pdf/2507.00205", "abs": "https://arxiv.org/abs/2507.00205", "authors": ["Periklis Petridis", "Georgios Margaritis", "Vasiliki Stoumpou", "Dimitris Bertsimas"], "title": "Holistic Artificial Intelligence in Medicine; improved performance and explainability", "categories": ["cs.AI", "cs.LG"], "comment": "Submitted to npj Digital Medicine", "summary": "With the increasing interest in deploying Artificial Intelligence in\nmedicine, we previously introduced HAIM (Holistic AI in Medicine), a framework\nthat fuses multimodal data to solve downstream clinical tasks. However, HAIM\nuses data in a task-agnostic manner and lacks explainability. To address these\nlimitations, we introduce xHAIM (Explainable HAIM), a novel framework\nleveraging Generative AI to enhance both prediction and explainability through\nfour structured steps: (1) automatically identifying task-relevant patient data\nacross modalities, (2) generating comprehensive patient summaries, (3) using\nthese summaries for improved predictive modeling, and (4) providing clinical\nexplanations by linking predictions to patient-specific medical knowledge.\nEvaluated on the HAIM-MIMIC-MM dataset, xHAIM improves average AUC from 79.9%\nto 90.3% across chest pathology and operative tasks. Importantly, xHAIM\ntransforms AI from a black-box predictor into an explainable decision support\nsystem, enabling clinicians to interactively trace predictions back to relevant\npatient data, bridging AI advancements with clinical utility.", "AI": {"tldr": "This paper introduces xHAIM, an explainable AI framework for medicine, improving prediction accuracy and providing patient-specific clinical explanations.", "motivation": "To enhance the HAIM framework by addressing its limitations in task-specific data usage and lack of explainability.", "method": "xHAIM is built on four steps: identifying task-relevant data, generating patient summaries, using them for predictive modeling, and linking predictions to medical knowledge.", "result": "xHAIM increases prediction accuracy (AUC improves from 79.9% to 90.3%) and offers explainable insights on the HAIM-MIMIC-MM dataset.", "conclusion": "xHAIM bridges the gap between AI advancements and clinical utility, transforming predictive models into explainable decision-support systems."}}
{"id": "2507.00297", "pdf": "https://arxiv.org/pdf/2507.00297", "abs": "https://arxiv.org/abs/2507.00297", "authors": ["David Ifeoluwa Adelani"], "title": "Natural language processing for African languages", "categories": ["cs.CL", "cs.AI"], "comment": "PhD thesis", "summary": "Recent advances in word embeddings and language models use large-scale,\nunlabelled data and self-supervised learning to boost NLP performance.\nMultilingual models, often trained on web-sourced data like Wikipedia, face\nchallenges: few low-resource languages are included, their data is often noisy,\nand lack of labeled datasets makes it hard to evaluate performance outside\nhigh-resource languages like English. In this dissertation, we focus on\nlanguages spoken in Sub-Saharan Africa where all the indigenous languages in\nthis region can be regarded as low-resourced in terms of the availability of\nlabelled data for NLP tasks and unlabelled data found on the web. We analyse\nthe noise in the publicly available corpora, and curate a high-quality corpus,\ndemonstrating that the quality of semantic representations learned in word\nembeddings does not only depend on the amount of data but on the quality of\npre-training data. We demonstrate empirically the limitations of word\nembeddings, and the opportunities the multilingual pre-trained language model\n(PLM) offers especially for languages unseen during pre-training and\nlow-resource scenarios. We further study how to adapt and specialize\nmultilingual PLMs to unseen African languages using a small amount of\nmonolingual texts. To address the under-representation of the African languages\nin NLP research, we developed large scale human-annotated labelled datasets for\n21 African languages in two impactful NLP tasks: named entity recognition and\nmachine translation. We conduct an extensive empirical evaluation using\nstate-of-the-art methods across supervised, weakly-supervised, and transfer\nlearning settings.", "AI": {"tldr": "This research targets improving NLP for Sub-Saharan African languages by curating higher-quality corpora, adapting pre-trained language models (PLMs), and creating large-scale labeled datasets for 21 African languages.", "motivation": "To address the challenges faced by low-resource languages in NLP, particularly those in Sub-Saharan Africa, where noise in existing datasets and lack of labeled data hinders performance.", "method": "The study curates high-quality corpora, evaluates multilingual pre-trained language models (PLMs) for low-resource scenarios, adapts PLMs to underserved African languages, and creates large-scale human-annotated datasets for named entity recognition and machine translation for 21 languages.", "result": "The research demonstrates the importance of high-quality pre-training data, showcases the limitations of word embeddings, explores the potential of multilingual PLMs, and provides valuable labeled datasets for African languages.", "conclusion": "Quality data and specialized adaptation techniques can significantly improve NLP outcomes for low-resource African languages, addressing their under-representation in NLP research."}}
{"id": "2507.00018", "pdf": "https://arxiv.org/pdf/2507.00018", "abs": "https://arxiv.org/abs/2507.00018", "authors": ["Bo Wang", "Qinyuan Cheng", "Runyu Peng", "Rong Bao", "Peiji Li", "Qipeng Guo", "Linyang Li", "Zhiyuan Zeng", "Yunhua Zhou", "Xipeng Qiu"], "title": "Implicit Reward as the Bridge: A Unified View of SFT and DPO Connections", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Post-training processes are essential phases in grounding pre-trained\nlanguage models to real-world tasks, with learning from demonstrations or\npreference signals playing a crucial role in this adaptation. We present a\nunified theoretical framework bridging Supervised Fine-Tuning (SFT) and\npreference learning in Large Language Model (LLM) post-training. Through\nrigorous mathematical derivation, we demonstrate that both SFT and preference\nlearning methods like Direct Preference Optimization (DPO) operate within the\nsame optimal policy-reward subspace, with SFT representing a special case of\nimplicit reward learning. Our analysis reveals a critical limitation in\nconventional SFT: the KL divergence term in distribution matching becomes\nconstant with respect to the policy during optimization, failing to constrain\nmodel updates. To address this, we propose a simple yet effective learning rate\nreduction approach that yields significant performance improvements (up to\n\\textbf{25\\%} relative gain and \\textbf{6\\%} absolute win rate increase in\ninstruction following tasks. Additionally, we derive alternative SFT objectives\nfrom various f-divergence functions that preserve the KL term during\noptimization, further enhancing post-DPO model performance. Finally, we extend\nthe theoretical relationship between LLM logits and Q-functions from preference\nlearning to the SFT context, providing mathematical derivations and\nexperimental validation.", "AI": {"tldr": "This paper presents a unified framework connecting SFT and preference learning in LLM post-training, identifies limitations in conventional SFT, and proposes theoretical and practical solutions to improve model performance.", "motivation": "Address the limitations in Supervised Fine-Tuning (SFT) during Large Language Model post-training and unify it with preference learning.", "method": "Rigorous mathematical derivation to link SFT and preference learning, proposing a learning rate reduction technique and alternative SFT objectives based on f-divergence functions.", "result": "Significant performance improvements: up to 25% relative gain and 6% absolute win rate increase in instruction-following tasks, along with theoretical validation of the extended relationship between logits and Q-functions.", "conclusion": "Unified theoretical framework provides a deeper understanding of SFT and preference learning, enabling effective improvements in LLM post-training processes."}}
{"id": "2507.00446", "pdf": "https://arxiv.org/pdf/2507.00446", "abs": "https://arxiv.org/abs/2507.00446", "authors": ["Yasunori Toshimitsu", "Kento Kawaharazuka", "Akihiro Miki", "Kei Okada", "Masayuki Inaba"], "title": "DIJE: Dense Image Jacobian Estimation for Robust Robotic Self-Recognition and Visual Servoing", "categories": ["cs.RO"], "comment": "2022 IEEE/RSJ International Conference on Intelligent Robots and\n  Systems (IROS)", "summary": "For robots to move in the real world, they must first correctly understand\nthe state of its own body and the tools that it holds. In this research, we\npropose DIJE, an algorithm to estimate the image Jacobian for every pixel. It\nis based on an optical flow calculation and a simplified Kalman Filter that can\nbe efficiently run on the whole image in real time. It does not rely on markers\nnor knowledge of the robotic structure. We use the DIJE in a self-recognition\nprocess which can robustly distinguish between movement by the robot and by\nexternal entities, even when the motion overlaps. We also propose a visual\nservoing controller based on DIJE, which can learn to control the robot's body\nto conduct reaching movements or bimanual tool-tip control. The proposed\nalgorithms were implemented on a physical musculoskeletal robot and its\nperformance was verified. We believe that such global estimation of the\nvisuomotor policy has the potential to be extended into a more general\nframework for manipulation.", "AI": {"tldr": "This paper introduces DIJE, an algorithm for real-time image Jacobian estimation capable of self-recognition and visual servoing on musculoskeletal robots.", "motivation": "Enable robots to robustly understand and adapt to their body states and tools for real-world applications without requiring markers or structural knowledge.", "method": "Developed DIJE, utilizing optical flow and a simplified Kalman Filter for whole-image processing. Applied DIJE to achieve self-recognition and create a visual servoing controller.", "result": "Successfully implemented DIJE on a musculoskeletal robot, demonstrating robust self-recognition and effective tool control.", "conclusion": "DIJE offers a promising framework for global visuomotor policy estimation, potentially advancing manipulation capabilities in robotics."}}
{"id": "2507.00786", "pdf": "https://arxiv.org/pdf/2507.00786", "abs": "https://arxiv.org/abs/2507.00786", "authors": ["Jukka Ruohonen", "Qusai Ramadan"], "title": "Snaps: Bloated and Outdated?", "categories": ["cs.SE"], "comment": "Submitted as a \"poster paper\" to APSEC", "summary": "Snap is an alternative software packaging system developed by Canonical and\nprovided by default in the Ubuntu Linux distribution. Given the heterogeneity\nof various Linux distributions and their various releases, Snap allows an\ninteroperable delivery of software directly to users. However, concerns and\ncriticism have also been frequently expressed. Regarding this criticism, the\npaper shows that currently distributed snap packages are indeed on average\nbloated in terms of their sizes and outdated in terms updating frequencies.\nWith these empirical observations, this short paper contributes to the research\ndomain of software packaging, software packages, and package managers.", "AI": {"tldr": "This paper evaluates the Snap packaging system highlighting concerns about bloated package sizes and infrequent updates.", "motivation": "To address concerns and criticisms around Snap packaging, aiming to assess its efficiency and impact on software delivery.", "method": "The study involves empirical observations regarding the size and updating frequencies of Snap packages.", "result": "Snap packages are shown to be bloated and less frequently updated compared to expectations.", "conclusion": "Snap packaging needs improvement in areas of package efficiency and update frequency, contributing to broader software packaging research."}}
{"id": "2507.00070", "pdf": "https://arxiv.org/pdf/2507.00070", "abs": "https://arxiv.org/abs/2507.00070", "authors": ["Bosubabu Sambana", "Hillary Sunday Nnadi", "Mohd Anas Wajid", "Nwosu Ogochukwu Fidelia", "Claudia Camacho-Zu\u00f1iga", "Henry Dozie Ajuzie", "Edeh Michael Onyema"], "title": "An efficient plant disease detection using transfer learning approach", "categories": ["cs.CV", "cs.AI"], "comment": "15 pages , 4 figures. Scientific Reports 2025", "summary": "Plant diseases pose significant challenges to farmers and the agricultural\nsector at large. However, early detection of plant diseases is crucial to\nmitigating their effects and preventing widespread damage, as outbreaks can\nseverely impact the productivity and quality of crops. With advancements in\ntechnology, there are increasing opportunities for automating the monitoring\nand detection of disease outbreaks in plants. This study proposed a system\ndesigned to identify and monitor plant diseases using a transfer learning\napproach. Specifically, the study utilizes YOLOv7 and YOLOv8, two\nstate-ofthe-art models in the field of object detection. By fine-tuning these\nmodels on a dataset of plant leaf images, the system is able to accurately\ndetect the presence of Bacteria, Fungi and Viral diseases such as Powdery\nMildew, Angular Leaf Spot, Early blight and Tomato mosaic virus. The model's\nperformance was evaluated using several metrics, including mean Average\nPrecision (mAP), F1-score, Precision, and Recall, yielding values of 91.05,\n89.40, 91.22, and 87.66, respectively. The result demonstrates the superior\neffectiveness and efficiency of YOLOv8 compared to other object detection\nmethods, highlighting its potential for use in modern agricultural practices.\nThe approach provides a scalable, automated solution for early any plant\ndisease detection, contributing to enhanced crop yield, reduced reliance on\nmanual monitoring, and supporting sustainable agricultural practices.", "AI": {"tldr": "This paper develops a system for automated plant disease detection using advanced YOLO models, achieving high performance metrics.", "motivation": "The study aims to address the challenges posed by plant diseases in agriculture by proposing an automated method for early detection to reduce crop damage and enhance productivity.", "method": "Transfer learning was applied using fine-tuned YOLOv7 and YOLOv8 models on a dataset of plant leaf images to detect disease types including bacterial, fungal, and viral issues.", "result": "The YOLOv8 model achieved strong evaluation metrics: mean Average Precision (91.05), F1-score (89.40), Precision (91.22), and Recall (87.66), demonstrating its efficiency.", "conclusion": "The findings affirm YOLOv8 as an effective solution for early plant disease detection, offering scalable and sustainable benefits to agriculture."}}
{"id": "2507.00195", "pdf": "https://arxiv.org/pdf/2507.00195", "abs": "https://arxiv.org/abs/2507.00195", "authors": ["Kumar Kshitij Patel"], "title": "What Makes Local Updates Effective: The Role of Data Heterogeneity and Smoothness", "categories": ["cs.LG", "cs.AI", "cs.MA", "math.OC", "stat.ML"], "comment": null, "summary": "This thesis contributes to the theoretical understanding of local update\nalgorithms, especially Local SGD, in distributed and federated optimization\nunder realistic models of data heterogeneity. A central focus is on the bounded\nsecond-order heterogeneity assumption, which is shown to be both necessary and\nsufficient for local updates to outperform centralized or mini-batch methods in\nconvex and non-convex settings. The thesis establishes tight upper and lower\nbounds in several regimes for various local update algorithms and characterizes\nthe min-max complexity of multiple problem classes. At its core is a\nfine-grained consensus-error-based analysis framework that yields sharper\nfinite-time convergence bounds under third-order smoothness and relaxed\nheterogeneity assumptions. The thesis also extends to online federated\nlearning, providing fundamental regret bounds under both first-order and bandit\nfeedback. Together, these results clarify when and why local updates offer\nprovable advantages, and the thesis serves as a self-contained guide for\nanalyzing Local SGD in heterogeneous environments.", "AI": {"tldr": "This thesis explores Local SGD in distributed and federated optimization, analyzing its performance under data heterogeneity and providing theoretical bounds.", "motivation": "To understand the conditions under which local updates outperform centralized methods under realistic data heterogeneity.", "method": "A consensus-error-based analytical framework is developed, with bounds derived for convex, non-convex scenarios and extended to online federated learning.", "result": "It identifies the bounded second-order heterogeneity assumption as crucial, and provides upper and lower bounds along with regret limits for online learning.", "conclusion": "The framework clarifies advantages of Local SGD, offering insights for distributed optimization in heterogeneous environments."}}
{"id": "2507.00218", "pdf": "https://arxiv.org/pdf/2507.00218", "abs": "https://arxiv.org/abs/2507.00218", "authors": ["Fangting Zhou", "Attila Lischka", "Balazs Kulcsar", "Jiaming Wu", "Morteza Haghir Chehreghani", "Gilbert Laporte"], "title": "Learning for routing: A guided review of recent developments and future directions", "categories": ["cs.AI", "math.OC"], "comment": "Accepted for publication in Transportation Research Part E: Logistics\n  and Transportation Review", "summary": "This paper reviews the current progress in applying machine learning (ML)\ntools to solve NP-hard combinatorial optimization problems, with a focus on\nrouting problems such as the traveling salesman problem (TSP) and the vehicle\nrouting problem (VRP). Due to the inherent complexity of these problems, exact\nalgorithms often require excessive computational time to find optimal\nsolutions, while heuristics can only provide approximate solutions without\nguaranteeing optimality. With the recent success of machine learning models,\nthere is a growing trend in proposing and implementing diverse ML techniques to\nenhance the resolution of these challenging routing problems. We propose a\ntaxonomy categorizing ML-based routing methods into construction-based and\nimprovement-based approaches, highlighting their applicability to various\nproblem characteristics. This review aims to integrate traditional OR methods\nwith state-of-the-art ML techniques, providing a structured framework to guide\nfuture research and address emerging VRP variants.", "AI": {"tldr": "The paper reviews progress on using machine learning to solve NP-hard routing problems like TSP and VRP, categorizing techniques into construction and improvement-based approaches.", "motivation": "The paper aims to address the limitations of exact algorithms (slow) and heuristics (non-optimal) by leveraging the potential of machine learning to solve complex routing problems efficiently.", "method": "The authors propose a taxonomy categorizing ML-based methods into two categories\u2014construction-based and improvement-based\u2014while integrating traditional optimization techniques with state-of-the-art ML models.", "result": "The review organizes and analyzes ML approaches in routing problems and highlights their strengths and applicability to different scenarios and emerging VRP variants.", "conclusion": "The paper provides a structured framework that combines traditional optimization with advanced ML methods to guide future research in solving complex routing challenges."}}
{"id": "2507.00322", "pdf": "https://arxiv.org/pdf/2507.00322", "abs": "https://arxiv.org/abs/2507.00322", "authors": ["Daking Rai", "Samuel Miller", "Kevin Moran", "Ziyu Yao"], "title": "Failure by Interference: Language Models Make Balanced Parentheses Errors When Faulty Mechanisms Overshadow Sound Ones", "categories": ["cs.CL", "cs.AI", "cs.SE", "I.2.7"], "comment": "23 pages, 10 figures, Preprint", "summary": "Despite remarkable advances in coding capabilities, language models (LMs)\nstill struggle with simple syntactic tasks such as generating balanced\nparentheses. In this study, we investigate the underlying mechanisms behind the\npersistence of these errors across LMs of varying sizes (124M-7B) to both\nunderstand and mitigate the errors. Our study reveals that LMs rely on a number\nof components (attention heads and FF neurons) that independently make their\nown predictions. While some components reliably promote correct answers across\na generalized range of inputs (i.e., implementing \"sound mechanisms''), others\nare less reliable and introduce noise by promoting incorrect tokens (i.e.,\nimplementing \"faulty mechanisms''). Errors occur when the faulty mechanisms\novershadow the sound ones and dominantly affect the predictions. Motivated by\nthis insight, we introduce RASteer, a steering method to systematically\nidentify and increase the contribution of reliable components for improving\nmodel performance. RASteer substantially improves performance on balanced\nparentheses tasks, boosting accuracy of some models from $0$% to around $100$%\nwithout impairing the models' general coding ability. We further demonstrate\nits broader applicability in arithmetic reasoning tasks, achieving performance\ngains of up to around $20$%.", "AI": {"tldr": "The paper investigates why language models struggle with simple syntactic tasks like balanced parentheses. It identifies \"sound\" and \"faulty\" mechanisms in language models and proposes RASteer, a method to enhance performance by emphasizing reliable components.", "motivation": "To understand and mitigate persistent language model errors in simple syntactic tasks like generating balanced parentheses, despite advancements in coding capabilities.", "method": "The authors analyze attention heads and feedforward neurons in language models to classify them as \"sound\" or \"faulty\" mechanisms and develop RASteer, a steering method to prioritize \"sound\" mechanisms in predictions.", "result": "RASteer improves performance on balanced parentheses tasks, raising accuracy from 0% to approximately 100% in some models. It also enhances performance in arithmetic reasoning tasks by up to 20%.", "conclusion": "The study provides insights into the internal mechanisms of language models and demonstrates the effectiveness of RASteer in improving syntactic and arithmetic reasoning tasks without compromising general coding abilities."}}
{"id": "2507.00019", "pdf": "https://arxiv.org/pdf/2507.00019", "abs": "https://arxiv.org/abs/2507.00019", "authors": ["Minati Rath", "Hema Date"], "title": "Quantum Inspired Encoding Strategies for Machine Learning Models: Proposing and Evaluating Instance Level, Global Discrete, and Class Conditional Representations", "categories": ["cs.LG", "cs.AI", "quant-ph"], "comment": null, "summary": "In this study, we propose, evaluate and compare three quantum inspired data\nencoding strategies, Instance Level Strategy (ILS), Global Discrete Strategy\n(GDS) and Class Conditional Value Strategy (CCVS), for transforming classical\ndata into quantum data for use in pure classical machine learning models. The\nprimary objective is to reduce high encoding time while ensuring correct\nencoding values and analyzing their impact on classification performance. The\nInstance Level Strategy treats each row of dataset independently; mimics local\nquantum states. Global Discrete Value Based encoding strategy maps all unique\nfeature values across the full dataset to quantum states uniformly. In\ncontrast, the Class conditional Value based encoding strategy encodes unique\nvalues separately for each class, preserving class dependent information.\n  We apply these encoding strategies to a classification task and assess their\nimpact on en-coding efficiency, correctness, model accuracy, and computational\ncost. By analyzing the trade offs between encoding time, precision, and\npredictive performance, this study provides insights into optimizing quantum\ninspired data transformations for classical machine learning workflows.", "AI": {"tldr": "The paper investigates three quantum-inspired data encoding strategies (ILS, GDS, and CCVS) to transform classical data into quantum format for classical machine learning, focusing on encoding efficiency and classification performance.", "motivation": "The motivation is to address challenges in converting classical data to quantum data while maintaining accurate representation and improving computational efficiency for machine learning applications.", "method": "Three distinct strategies, ILS, GDS, and CCVS, are introduced and applied on classical datasets to assess their performance regarding encoding efficiency, correctness, and model accuracy.", "result": "The encoding strategies were evaluated for their computational trade-offs, demonstrating varied impacts on encoding time, precision, and classification accuracy.", "conclusion": "The study concludes that optimizing quantum-inspired data encoding can enhance efficiency and predictive performance in classical machine learning models."}}
{"id": "2507.00464", "pdf": "https://arxiv.org/pdf/2507.00464", "abs": "https://arxiv.org/abs/2507.00464", "authors": ["Hyun-Bin Kim", "Kyung-Soo Kim"], "title": "A Miniature High-Resolution Tension Sensor Based on a Photo-Reflector for Robotic Hands and Grippers", "categories": ["cs.RO"], "comment": null, "summary": "This paper presents a miniature tension sensor using a photo-reflector,\ndesigned for compact tendon-driven grippers and robotic hands. The proposed\nsensor has a small form factor of 13~mm x 7~mm x 6.5~mm and is capable of\nmeasuring tensile forces up to 200~N. A symmetric elastomer structure\nincorporating fillets and flexure hinges is designed based on Timoshenko beam\ntheory and verified via FEM analysis, enabling improved sensitivity and\nmechanical durability while minimizing torsional deformation. The sensor\nutilizes a compact photo-reflector (VCNT2020) to measure displacement in the\nnear-field region, eliminating the need for light-absorbing materials or\ngeometric modifications required in photo-interrupter-based designs. A 16-bit\nanalog-to-digital converter (ADC) and CAN-FD (Flexible Data-rate) communication\nenable efficient signal acquisition with up to 5~kHz sampling rate. Calibration\nexperiments demonstrate a resolution of 9.9~mN (corresponding to over 14-bit\naccuracy) and a root mean square error (RMSE) of 0.455~N. Force control\nexperiments using a twisted string actuator and PI control yield RMSEs as low\nas 0.073~N. Compared to previous research using photo-interrupter, the proposed\nmethod achieves more than tenfold improvement in resolution while also reducing\nnonlinearity and hysteresis. The design is mechanically simple, lightweight,\neasy to assemble, and suitable for integration into robotic and prosthetic\nsystems requiring high-resolution force feedback.", "AI": {"tldr": "The paper introduces a small-scale tension sensor using a photo-reflector for compact robotic systems. It offers enhanced resolution, reliability, and ease of integration for tendon-driven grippers and robotic/prosthetic hands.", "motivation": "To design a compact, high-resolution, and mechanically simple tension sensor suitable for robotic and prosthetic systems.", "method": "Developed a tension sensor featuring a symmetric elastomer structure designed using Timoshenko beam theory and FEM analysis. It uses a compact photo-reflector (VCNT2020) with a 16-bit ADC and CAN-FD communication for precise force measurement and signal acquisition.", "result": "The sensor achieved a resolution of 9.9 mN and a low RMSE of 0.455 N during calibration experiments, with force control experiments yielding even lower RMSEs of 0.073 N. Additionally, it showed over a tenfold improvement in resolution compared to prior designs using photo-interrupters.", "conclusion": "The proposed sensor combines high resolution, reduced nonlinearity and hysteresis, and easy mechanical integration, making it highly suitable for robotic and prosthetic systems requiring precise force feedback."}}
{"id": "2507.00788", "pdf": "https://arxiv.org/pdf/2507.00788", "abs": "https://arxiv.org/abs/2507.00788", "authors": ["Markus Borg", "Dave Hewett", "Nadim Hagatulah", "Noric Couderc", "Emma S\u00f6derberg", "Donald Graham", "Uttam Kini", "Dave Farley"], "title": "Echoes of AI: Investigating the Downstream Effects of AI Assistants on Software Maintainability", "categories": ["cs.SE", "cs.AI"], "comment": "Preprint of study preregistered at ICSME 2025 with In-Principal\n  Acceptance.\n  https://conf.researchr.org/track/icsme-2024/icsme-2024-registered-reports-track", "summary": "[Context] AI assistants, like GitHub Copilot and Cursor, are transforming\nsoftware engineering. While several studies highlight productivity\nimprovements, their impact on maintainability requires further investigation.\n[Objective] This study investigates whether co-development with AI assistants\naffects software maintainability, specifically how easily other developers can\nevolve the resulting source code. [Method] We conducted a two-phase controlled\nexperiment involving 151 participants, 95% of whom were professional\ndevelopers. In Phase 1, participants added a new feature to a Java web\napplication, with or without AI assistance. In Phase 2, a randomized controlled\ntrial, new participants evolved these solutions without AI assistance.\n[Results] AI-assisted development in Phase 1 led to a modest speedup in\nsubsequent evolution and slightly higher average CodeHealth. Although neither\ndifference was significant overall, the increase in CodeHealth was\nstatistically significant when habitual AI users completed Phase 1. For Phase\n1, we also observed a significant effect that corroborates previous\nproductivity findings: using an AI assistant yielded a 30.7% median decrease in\ntask completion time. Moreover, for habitual AI users, the mean speedup was\n55.9%. [Conclusions] Our study adds to the growing evidence that AI assistants\ncan effectively accelerate development. Moreover, we did not observe warning\nsigns of degraded code-level maintainability. We recommend that future research\nfocus on risks such as code bloat from excessive code generation and the\nbuild-up of cognitive debt as developers invest less mental effort during\nimplementation.", "AI": {"tldr": "This study examines the impact of AI assistants on software maintainability through a controlled experiment, finding modest speedups, improved CodeHealth for habitual AI users, and no significant signs of degraded maintainability.", "motivation": "The study aims to explore the influence AI assistants like GitHub Copilot have on software maintainability, given the growing automation in software engineering but limited understanding of its longer-term impact on evolving code.", "method": "The researchers conducted a two-phase experiment with 151 participants involving new feature addition and subsequent code evolution, comparing outcomes with and without AI assistance.", "result": "AI assistance led to a 30.7% median task completion time reduction, with a 55.9% speedup for habitual users. CodeHealth slightly improved and showed statistically significant improvement for habitual AI users, but overall, maintainability did not significantly degrade.", "conclusion": "AI assistants effectively accelerate development and enhance CodeHealth for frequent users without harming maintainability. Future studies should focus on potential risks like code bloat and cognitive debt."}}
{"id": "2507.00153", "pdf": "https://arxiv.org/pdf/2507.00153", "abs": "https://arxiv.org/abs/2507.00153", "authors": ["Peter Mortimer", "Mirko Maehlisch"], "title": "Diffusion-Based Image Augmentation for Semantic Segmentation in Outdoor Robotics", "categories": ["cs.CV"], "comment": "Presented at the 2025 IEEE ICRA Workshop on Field Robotics", "summary": "The performance of leaning-based perception algorithms suffer when deployed\nin out-of-distribution and underrepresented environments. Outdoor robots are\nparticularly susceptible to rapid changes in visual scene appearance due to\ndynamic lighting, seasonality and weather effects that lead to scenes\nunderrepresented in the training data of the learning-based perception system.\nIn this conceptual paper, we focus on preparing our autonomous vehicle for\ndeployment in snow-filled environments. We propose a novel method for\ndiffusion-based image augmentation to more closely represent the deployment\nenvironment in our training data. Diffusion-based image augmentations rely on\nthe public availability of vision foundation models learned on internet-scale\ndatasets. The diffusion-based image augmentations allow us to take control over\nthe semantic distribution of the ground surfaces in the training data and to\nfine-tune our model for its deployment environment. We employ open vocabulary\nsemantic segmentation models to filter out augmentation candidates that contain\nhallucinations. We believe that diffusion-based image augmentations can be\nextended to many other environments apart from snow surfaces, like sandy\nenvironments and volcanic terrains.", "AI": {"tldr": "Learning-based perception algorithms for outdoor robots struggle in rapidly changing environments like snow. This paper introduces a diffusion-based image augmentation technique leveraging foundation models and semantic filtering to make training data more representative of target deployment conditions.", "motivation": "To address the degradation of learning-based perception algorithm performance in out-of-distribution environments, particularly for autonomous vehicles in challenging snow-filled settings.", "method": "A novel diffusion-based image augmentation approach is proposed, utilizing vision foundation models for generating environment-specific variations in training data, combined with open vocabulary semantic segmentation to exclude unrealistic augmentations.", "result": "The proposed diffusion-based augmentation enhances training datasets by semantically matching the target deployment environments, mitigating performance drops in underrepresented scenarios.", "conclusion": "Diffusion-based image augmentations can effectively prepare learning-based perception systems for deployment in snow-filled and other complex environments, improving adaptability and robustness."}}
{"id": "2507.00451", "pdf": "https://arxiv.org/pdf/2507.00451", "abs": "https://arxiv.org/abs/2507.00451", "authors": ["Matthew Stephenson", "Alex Newcombe", "Eric Piette", "Dennis Soemers"], "title": "Best Agent Identification for General Game Playing", "categories": ["cs.LG", "cs.AI", "cs.DS", "cs.IT", "math.IT", "stat.ML"], "comment": null, "summary": "We present an efficient and generalised procedure to accurately identify the\nbest performing algorithm for each sub-task in a multi-problem domain. Our\napproach treats this as a set of best arm identification problems for\nmulti-armed bandits, where each bandit corresponds to a specific task and each\narm corresponds to a specific algorithm or agent. We propose an optimistic\nselection process based on the Wilson score interval (Optimistic-WS) that ranks\neach arm across all bandits in terms of their potential regret reduction. We\nevaluate the performance of Optimistic-WS on two of the most popular general\ngame domains, the General Video Game AI (GVGAI) framework and the Ludii general\ngame playing system, with the goal of identifying the highest performing agent\nfor each game within a limited number of trials. Compared to previous best arm\nidentification algorithms for multi-armed bandits, our results demonstrate a\nsubstantial performance improvement in terms of average simple regret. This\nnovel approach can be used to significantly improve the quality and accuracy of\nagent evaluation procedures for general game frameworks, as well as other\nmulti-task domains with high algorithm runtimes.", "AI": {"tldr": "This paper introduces Optimistic-WS, a method to identify the best-performing algorithm for tasks in multi-problem domains more efficiently and accurately.", "motivation": "Existing methods for algorithm selection in multi-task domains often suffer from inefficiencies and inaccuracies, necessitating improved techniques for agent evaluation.", "method": "The authors model the problem as a multi-armed bandit framework and propose Optimistic-WS, which uses the Wilson score interval to rank algorithms by potential regret reduction.", "result": "Optimistic-WS was tested on two popular game domains (GVGAI and Ludii) and showed significant performance improvements in reducing simple regret compared to existing methods.", "conclusion": "Optimistic-WS offers a promising and effective approach for enhancing algorithm selection and evaluation in multi-task domains, especially where high computation time is a factor."}}
{"id": "2507.00417", "pdf": "https://arxiv.org/pdf/2507.00417", "abs": "https://arxiv.org/abs/2507.00417", "authors": ["Joongwon Kim", "Anirudh Goyal", "Liang Tan", "Hannaneh Hajishirzi", "Srinivasan Iyer", "Tianlu Wang"], "title": "ASTRO: Teaching Language Models to Reason by Reflecting and Backtracking In-Context", "categories": ["cs.AI", "cs.CL"], "comment": "36 pages, 23 figures", "summary": "We introduce ASTRO, the \"Autoregressive Search-Taught Reasoner\", a framework\nfor training language models to reason like search algorithms, explicitly\nleveraging self-reflection, backtracking, and exploration in their outputs.\nRecently, training large language models (LLMs) via reinforcement learning (RL)\nhas led to the advent of reasoning models with greatly enhanced reasoning\ncapabilities. Open-source replications of reasoning models, while successful,\nbuild upon models that already exhibit strong reasoning capabilities along with\nsearch behavior observed even before RL. As a result, it is yet unclear how to\nboost the reasoning capabilities of other non-reasoner models including Llama\n3. ASTRO teaches such models to internalize structured search behavior through\na synthetic dataset derived from Monte Carlo Tree Search (MCTS) over\nmathematical problem-solving trajectories. By converting search traces into\nnatural language chain-of-thoughts that capture both successes and recoveries\nfrom failure, ASTRO bootstraps models with a rich prior for exploration during\nRL. We finetune our models on these search-derived traces and further improve\nperformance via RL with verifiable rewards. We apply ASTRO to the Llama 3\nfamily of models and achieve absolute performance gains of 16.0% on MATH-500,\n26.9% on AMC 2023, and 20.0% on AIME 2024, especially improving upon\nchallenging problems that require iterative correction. Our results demonstrate\nthat search-inspired training offers a principled way to instill robust\nreasoning capabilities into open LLMs.", "AI": {"tldr": "ASTRO is a framework that trains language models to reason like search algorithms by leveraging structured search behavior from synthetic datasets and reinforcement learning (RL). It significantly improves reasoning capabilities in models like Llama 3.", "motivation": "There is a need to enhance reasoning capabilities in non-reasoner language models, such as Llama 3, which do not inherently exhibit advanced reasoning and search behavior.", "method": "ASTRO uses a synthetic dataset derived from Monte Carlo Tree Search (MCTS) problem-solving traces, converted into natural language chain-of-thoughts. These traces inform structured search behavior, which is further refined via reinforcement learning with verifiable rewards.", "result": "Applying ASTRO to the Llama 3 family showed absolute performance improvements: 16.0% on MATH-500, 26.9% on AMC 2023, and 20.0% on AIME 2024, particularly for challenging problems requiring iterative corrections.", "conclusion": "Search-inspired training frameworks like ASTRO provide a scalable solution to instill robust reasoning capabilities in open-source large language models."}}
{"id": "2507.00330", "pdf": "https://arxiv.org/pdf/2507.00330", "abs": "https://arxiv.org/abs/2507.00330", "authors": ["Mohna Chakraborty", "Adithya Kulkarni", "Qi Li"], "title": "Modeling Data Diversity for Joint Instance and Verbalizer Selection in Cold-Start Scenarios", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Prompt-based methods leverage the knowledge of pre-trained language models\n(PLMs) trained with a masked language modeling (MLM) objective; however, these\nmethods are sensitive to template, verbalizer, and few-shot instance selection,\nparticularly in cold-start settings with no labeled data. Existing studies\noverlook the dependency between instances and verbalizers, where instance-label\nprobabilities depend on verbalizer token proximity in the embedding space. To\naddress this, we propose COLDSELECT, a joint verbalizer and instance selection\napproach that models data diversity. COLDSELECT maps PLM vocabulary and\n$h_{[MASK]}$ embeddings into a shared space, applying dimensionality reduction\nand clustering to ensure efficient and diverse selection. By optimizing for\nminimal uncertainty and maximal diversity, COLDSELECT captures data\nrelationships effectively. Experiments on eight benchmarks demonstrate\nCOLDSELECT's superiority in reducing uncertainty and enhancing generalization,\noutperforming baselines in verbalizer and few-shot instance selection for\ncold-start scenarios.", "AI": {"tldr": "This paper introduces COLDSELECT, a joint verbalizer and instance selection method that enhances prompt-based techniques by reducing uncertainty and increasing diversity in cold-start few-shot learning scenarios.", "motivation": "Prompt-based methods using pre-trained language models (PLMs) are sensitive to verbalizers, templates, and instance selection, particularly in scenarios without labeled data. There is a need for a more effective approach that addresses this sensitivity and considers the interplay between instance-label probabilities and verbalizer selection.", "method": "The paper presents COLDSELECT, which maps PLM vocabulary and masked token embeddings into a shared space through dimensionality reduction and clustering. By optimizing for minimal uncertainty and maximal data diversity, it jointly selects verbalizers and few-shot instances.", "result": "COLDSELECT outperforms baseline methods across eight benchmarks in verbalizer and instance selection, demonstrating superior generalization and reduced uncertainty in cold-start scenarios.", "conclusion": "COLDSELECT effectively enhances the prompt-based methods by addressing their sensitivity to instance selection and verbalizers. It achieves this by leveraging a data-efficient approach that improves generalization in cold-start few-shot learning situations."}}
{"id": "2507.00523", "pdf": "https://arxiv.org/pdf/2507.00523", "abs": "https://arxiv.org/abs/2507.00523", "authors": ["Nazish Tahir", "Ramviyas Parasuraman"], "title": "Edge Computing and its Application in Robotics: A Survey", "categories": ["cs.RO", "cs.DC", "cs.NI"], "comment": null, "summary": "The Edge computing paradigm has gained prominence in both academic and\nindustry circles in recent years. By implementing edge computing facilities and\nservices in robotics, it becomes a key enabler in the deployment of artificial\nintelligence applications to robots. Time-sensitive robotics applications\nbenefit from the reduced latency, mobility, and location awareness provided by\nthe edge computing paradigm, which enables real-time data processing and\nintelligence at the network's edge. While the advantages of integrating edge\ncomputing into robotics are numerous, there has been no recent survey that\ncomprehensively examines these benefits. This paper aims to bridge that gap by\nhighlighting important work in the domain of edge robotics, examining recent\nadvancements, and offering deeper insight into the challenges and motivations\nbehind both current and emerging solutions. In particular, this article\nprovides a comprehensive evaluation of recent developments in edge robotics,\nwith an emphasis on fundamental applications, providing in-depth analysis of\nthe key motivations, challenges, and future directions in this rapidly evolving\ndomain. It also explores the importance of edge computing in real-world\nrobotics scenarios where rapid response times are critical. Finally, the paper\noutlines various open research challenges in the field of edge robotics.", "AI": {"tldr": "This paper surveys the integration of edge computing in robotics, emphasizing its advantages like reduced latency and real-time processing.", "motivation": "To assess the benefits and advancements of integrating edge computing in robotics, addressing time-sensitive applications.", "method": "Comprehensive survey and evaluation of recent advancements, challenges, and applications in edge robotics.", "result": "Outlined the importance of edge computing for critical robotics scenarios and identified open challenges in edge robotics.", "conclusion": "Edge robotics provides significant advantages for real-world applications, but it faces several research challenges requiring attention."}}
{"id": "2507.00803", "pdf": "https://arxiv.org/pdf/2507.00803", "abs": "https://arxiv.org/abs/2507.00803", "authors": ["Gillian Daniel", "Chris Hall", "Per Hammer", "Alec-Angus Macdonald", "Hollie Marwick-Best", "Emma McKenzie", "George Popa", "Derek Somerville", "Tim Storer"], "title": "Out of the Day Job: Perspectives of Industry Practitioners in Co-Design and Delivery of Software Engineering Courses", "categories": ["cs.SE"], "comment": null, "summary": "Over more than two decades, The University of Glasgow has co-designed and\ndelivered numerous software engineering focused courses with industry partners,\ncovering both technical and discipline specific professional skills. Such\ncollaborations are not unique and many of the benefits are well recognised in\nthe literature. These include enhancing the real-world relevance of curricula,\ndeveloping student professional networks ahead of graduation and easing\nrecruitment opportunities for employers.\n  However, there is relatively little scholarship on the perspectives of\nindustry practitioners who participate in course design and delivery. This gap\nis significant, since the effort invested by practitioners is often substantial\nand may require ongoing support from both the industry partner and academic\ninstitution. Understanding the motivations, expectations and experiences of\npractitioners who engage in course delivery can guide the formation of future\npartnerships and ensure their long-term sustainability.\n  We begin to address this gap by reporting on the outcomes of a retrospective\nconducted amongst the practitioner coauthors of this paper, with the academic\ncoauthors acting as facilitators. All coauthors have participated in the recent\nco-design and delivery of software engineering courses, but we choose to focus\nexplicitly on the perspectives of the practitioners. We report on the themes\nthat emerged from the discussions and our resulting recommendations for future\ncollaborations.", "AI": {"tldr": "This study explores the perspectives of industry practitioners involved in co-designing and delivering software engineering courses with academia at the University of Glasgow, providing recommendations for sustainable future collaborations.", "motivation": "To address the lack of research on the experiences and perspectives of industry practitioners contributing to course design and delivery, as their involvement requires significant effort and support.", "method": "A retrospective study was conducted involving practitioner co-authors sharing their experiences, with academic co-authors facilitating discussions. Emerging themes were analyzed to draw conclusions.", "result": "The study identified key themes related to the motivations, expectations, and experiences of practitioners. It provides insights into better managing their engagement and sustaining collaborative efforts.", "conclusion": "The findings emphasize the importance of understanding practitioner perspectives to improve industry-academia collaborations in course co-development and ensure their long-term viability."}}
{"id": "2507.00162", "pdf": "https://arxiv.org/pdf/2507.00162", "abs": "https://arxiv.org/abs/2507.00162", "authors": ["Yu Lu", "Yi Yang"], "title": "FreeLong++: Training-Free Long Video Generation via Multi-band SpectralFusion", "categories": ["cs.CV"], "comment": "under review", "summary": "Recent advances in video generation models have enabled high-quality short\nvideo generation from text prompts. However, extending these models to longer\nvideos remains a significant challenge, primarily due to degraded temporal\nconsistency and visual fidelity. Our preliminary observations show that naively\napplying short-video generation models to longer sequences leads to noticeable\nquality degradation. Further analysis identifies a systematic trend where\nhigh-frequency components become increasingly distorted as video length grows,\nan issue we term high-frequency distortion. To address this, we propose\nFreeLong, a training-free framework designed to balance the frequency\ndistribution of long video features during the denoising process. FreeLong\nachieves this by blending global low-frequency features, which capture holistic\nsemantics across the full video, with local high-frequency features extracted\nfrom short temporal windows to preserve fine details. Building on this,\nFreeLong++ extends FreeLong dual-branch design into a multi-branch architecture\nwith multiple attention branches, each operating at a distinct temporal scale.\nBy arranging multiple window sizes from global to local, FreeLong++ enables\nmulti-band frequency fusion from low to high frequencies, ensuring both\nsemantic continuity and fine-grained motion dynamics across longer video\nsequences. Without any additional training, FreeLong++ can be plugged into\nexisting video generation models (e.g. Wan2.1 and LTX-Video) to produce longer\nvideos with substantially improved temporal consistency and visual fidelity. We\ndemonstrate that our approach outperforms previous methods on longer video\ngeneration tasks (e.g. 4x and 8x of native length). It also supports coherent\nmulti-prompt video generation with smooth scene transitions and enables\ncontrollable video generation using long depth or pose sequences.", "AI": {"tldr": "This paper proposes FreeLong and FreeLong++, training-free frameworks to improve long video generation via frequency distribution balance, achieving better temporal consistency and visual quality.", "motivation": "The authors aim to address the challenges of temporal inconsistency and visual fidelity degradation commonly observed when extending text-to-video generation models to create longer videos.", "method": "FreeLong introduces a dual-branch framework that balances low-frequency (global) and high-frequency (local) features to address high-frequency distortion, while FreeLong++ extends this to a multi-branch architecture with varying temporal scales for enhanced frequency fusion.", "result": "FreeLong++ significantly improves the temporal consistency and visual quality of longer videos by augmenting existing video generation models without additional training.", "conclusion": "FreeLong++ provides a versatile, efficient solution for longer video generation, enabling smoother scene transitions, coherent multi-prompt integration, and controllable outputs using long depth or pose sequences."}}
{"id": "2507.00480", "pdf": "https://arxiv.org/pdf/2507.00480", "abs": "https://arxiv.org/abs/2507.00480", "authors": ["Kiyoung Om", "Kyuil Sim", "Taeyoung Yun", "Hyeongyu Kang", "Jinkyoo Park"], "title": "Posterior Inference in Latent Space for Scalable Constrained Black-box Optimization", "categories": ["cs.LG", "stat.ML"], "comment": "25 pages, 11 figures, 5 tables. Equal contribution by Kiyoung Om,\n  Kyuil Sim, and Taeyoung Yun", "summary": "Optimizing high-dimensional black-box functions under black-box constraints\nis a pervasive task in a wide range of scientific and engineering problems.\nThese problems are typically harder than unconstrained problems due to\nhard-to-find feasible regions. While Bayesian optimization (BO) methods have\nbeen developed to solve such problems, they often struggle with the curse of\ndimensionality. Recently, generative model-based approaches have emerged as a\npromising alternative for constrained optimization. However, they suffer from\npoor scalability and are vulnerable to mode collapse, particularly when the\ntarget distribution is highly multi-modal. In this paper, we propose a new\nframework to overcome these challenges. Our method iterates through two stages.\nFirst, we train flow-based models to capture the data distribution and\nsurrogate models that predict both function values and constraint violations\nwith uncertainty quantification. Second, we cast the candidate selection\nproblem as a posterior inference problem to effectively search for promising\ncandidates that have high objective values while not violating the constraints.\nDuring posterior inference, we find that the posterior distribution is highly\nmulti-modal and has a large plateau due to constraints, especially when\nconstraint feedback is given as binary indicators of feasibility. To mitigate\nthis issue, we amortize the sampling from the posterior distribution in the\nlatent space of flow-based models, which is much smoother than that in the data\nspace. We empirically demonstrate that our method achieves superior performance\non various synthetic and real-world constrained black-box optimization tasks.\nOur code is publicly available \\href{https://github.com/umkiyoung/CiBO}{here}.", "AI": {"tldr": "The paper addresses the challenge of optimizing constrained high-dimensional black-box functions using a novel framework leveraging flow-based models and posterior inference.", "motivation": "High-dimensional constrained optimization tasks are common yet difficult due to hard-to-find feasible regions, compounded by the failure of existing methods (e.g., Bayesian optimization) to overcome the curse of dimensionality.", "method": "The proposed framework uses flow-based models for data distribution and surrogate models to predict outcomes with uncertainty quantification. It addresses multi-modality and plateaus during posterior inference by sampling in the smoother latent space of flow-based models.", "result": "The method demonstrates superior empirical performance on synthetic and real-world black-box optimization tasks while mitigating common generative method issues like mode collapse.", "conclusion": "The framework offers an effective and scalable solution for constrained optimization, overcoming challenges of dimensionality, scalability, and mode collapse in the generative modeling approach."}}
{"id": "2507.00432", "pdf": "https://arxiv.org/pdf/2507.00432", "abs": "https://arxiv.org/abs/2507.00432", "authors": ["Maggie Huan", "Yuetai Li", "Tuney Zheng", "Xiaoyu Xu", "Seungone Kim", "Minxin Du", "Radha Poovendran", "Graham Neubig", "Xiang Yue"], "title": "Does Math Reasoning Improve General LLM Capabilities? Understanding Transferability of LLM Reasoning", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Math reasoning has become the poster child of progress in large language\nmodels (LLMs), with new models rapidly surpassing human-level performance on\nbenchmarks like MATH and AIME. But as math leaderboards improve week by week,\nit is worth asking: do these gains reflect broader problem-solving ability or\njust narrow overfitting? To answer this question, we evaluate over 20\nopen-weight reasoning-tuned models across a broad suite of tasks, including\nmath, scientific QA, agent planning, coding, and standard\ninstruction-following. We surprisingly find that most models that succeed in\nmath fail to transfer their gains to other domains. To rigorously study this\nphenomenon, we conduct controlled experiments on Qwen3-14B models using\nmath-only data but different tuning methods. We find that reinforcement\nlearning (RL)-tuned models generalize well across domains, while supervised\nfine-tuning (SFT)-tuned models often forget general capabilities. Latent-space\nrepresentation and token-space distribution shift analyses reveal that SFT\ninduces substantial representation and output drift, while RL preserves\ngeneral-domain structure. Our results suggest a need to rethink standard\npost-training recipes, particularly the reliance on SFT-distilled data for\nadvancing reasoning models.", "AI": {"tldr": "The paper evaluates reasoning models tuned for math and finds RL-tuned models generalize well across domains, whereas SFT-tuned models do not, indicating over-reliance on SFT methods.", "motivation": "To investigate whether the rapid progress in math reasoning tasks by large language models reflects genuine general problem-solving abilities or overfitting to narrow benchmarks.", "method": "The authors evaluated 20+ open-weight reasoning-tuned models across diverse tasks and conducted controlled experiments on Qwen3-14B models with different tuning methods (RL and SFT), analyzing their latent-space representations and output distribution shifts.", "result": "Most math-focused models fail to transfer their abilities to broader tasks. RL-tuned models generalize effectively across domains, while SFT-tuned models experience representation drift and forget general capabilities.", "conclusion": "The study suggests rethinking post-training strategies, especially the use of SFT-distilled data, as RL-tuned approaches demonstrate superior generalization and structural preservation."}}
{"id": "2507.00355", "pdf": "https://arxiv.org/pdf/2507.00355", "abs": "https://arxiv.org/abs/2507.00355", "authors": ["Paul J. L. Ammann", "Jonas Golde", "Alan Akbik"], "title": "Question Decomposition for Retrieval-Augmented Generation", "categories": ["cs.CL"], "comment": "Accepted to ACL SRW 2025. 9 Pages, 2 Figures, 4 Tables", "summary": "Grounding large language models (LLMs) in verifiable external sources is a\nwell-established strategy for generating reliable answers. Retrieval-augmented\ngeneration (RAG) is one such approach, particularly effective for tasks like\nquestion answering: it retrieves passages that are semantically related to the\nquestion and then conditions the model on this evidence. However, multi-hop\nquestions, such as \"Which company among NVIDIA, Apple, and Google made the\nbiggest profit in 2023?,\" challenge RAG because relevant facts are often\ndistributed across multiple documents rather than co-occurring in one source,\nmaking it difficult for standard RAG to retrieve sufficient information. To\naddress this, we propose a RAG pipeline that incorporates question\ndecomposition: (i) an LLM decomposes the original query into sub-questions,\n(ii) passages are retrieved for each sub-question, and (iii) the merged\ncandidate pool is reranked to improve the coverage and precision of the\nretrieved evidence. We show that question decomposition effectively assembles\ncomplementary documents, while reranking reduces noise and promotes the most\nrelevant passages before answer generation. Although reranking itself is\nstandard, we show that pairing an off-the-shelf cross-encoder reranker with\nLLM-driven question decomposition bridges the retrieval gap on multi-hop\nquestions and provides a practical, drop-in enhancement, without any extra\ntraining or specialized indexing. We evaluate our approach on the MultiHop-RAG\nand HotpotQA, showing gains in retrieval (MRR@10: +36.7%) and answer accuracy\n(F1: +11.6%) over standard RAG baselines.", "AI": {"tldr": "The paper introduces a retrieval-augmented generation (RAG) pipeline that uses question decomposition and reranking to improve multi-hop question answering.", "motivation": "Standard RAG struggles with multi-hop questions where relevant information is scattered across multiple documents, making it hard to retrieve sufficient evidence.", "method": "The method involves (i) decomposing the query into sub-questions using an LLM, (ii) retrieving relevant passages for each sub-question, and (iii) reranking the merged results using a cross-encoder reranker before answer generation.", "result": "The method improves retrieval (MRR@10: +36.7%) and answer accuracy (F1: +11.6%) on datasets like MultiHop-RAG and HotpotQA compared to standard RAG.", "conclusion": "Pairing question decomposition with reranking enhances multi-hop retrieval performance effectively, without requiring additional training or specialized indexing."}}
{"id": "2507.00552", "pdf": "https://arxiv.org/pdf/2507.00552", "abs": "https://arxiv.org/abs/2507.00552", "authors": ["Jiajie Zhang", "Shenrui Wu", "Xu Ma", "S\u00f6ren Schwertfeger"], "title": "Generation of Indoor Open Street Maps for Robot Navigation from CAD Files", "categories": ["cs.RO"], "comment": "8 pages, 8 figures", "summary": "The deployment of autonomous mobile robots is predicated on the availability\nof environmental maps, yet conventional generation via SLAM (Simultaneous\nLocalization and Mapping) suffers from significant limitations in time, labor,\nand robustness, particularly in dynamic, large-scale indoor environments where\nmap obsolescence can lead to critical localization failures. To address these\nchallenges, this paper presents a complete and automated system for converting\narchitectural Computer-Aided Design (CAD) files into a hierarchical topometric\nOpenStreetMap (OSM) representation, tailored for robust life-long robot\nnavigation. Our core methodology involves a multi-stage pipeline that first\nisolates key structural layers from the raw CAD data and then employs an\nAreaGraph-based topological segmentation to partition the building layout into\na hierarchical graph of navigable spaces. This process yields a comprehensive\nand semantically rich map, further enhanced by automatically associating\ntextual labels from the CAD source and cohesively merging multiple building\nfloors into a unified, topologically-correct model. By leveraging the permanent\nstructural information inherent in CAD files, our system circumvents the\ninefficiencies and fragility of SLAM, offering a practical and scalable\nsolution for deploying robots in complex indoor spaces. The software is\nencapsulated within an intuitive Graphical User Interface (GUI) to facilitate\npractical use. The code and dataset are available at\nhttps://github.com/jiajiezhang7/osmAG-from-cad.", "AI": {"tldr": "This paper introduces an automated system for converting architectural CAD files into a hierarchical topometric OpenStreetMap representation for robust indoor robot navigation.", "motivation": "Traditional SLAM-based map generation is inefficient and prone to failures in dynamic, large-scale indoor environments, highlighting the need for more robust mapping solutions.", "method": "The paper proposes a multi-stage pipeline that processes CAD files to create hierarchical topometric maps, leveraging AreaGraph-based segmentation and semantic labeling to produce navigable, unified building layouts.", "result": "The system generates semantically rich, topologically correct maps for robots, avoiding SLAM's limitations. It simplifies deployment in large and dynamic indoor environments.", "conclusion": "This CAD-based mapping system provides a scalable and robust solution for robot navigation in indoor spaces, bypassing SLAM-related inefficiencies while offering practical usability through a GUI."}}
{"id": "2507.00170", "pdf": "https://arxiv.org/pdf/2507.00170", "abs": "https://arxiv.org/abs/2507.00170", "authors": ["Hugo Baudchon", "Arthur Ouaknine", "Martin Weiss", "M\u00e9lisande Teng", "Thomas R. Walla", "Antoine Caron-Guay", "Christopher Pal", "Etienne Lalibert\u00e9"], "title": "SelvaBox: A high-resolution dataset for tropical tree crown detection", "categories": ["cs.CV", "I.2.10; I.4.8; I.5.4"], "comment": null, "summary": "Detecting individual tree crowns in tropical forests is essential to study\nthese complex and crucial ecosystems impacted by human interventions and\nclimate change. However, tropical crowns vary widely in size, structure, and\npattern and are largely overlapping and intertwined, requiring advanced remote\nsensing methods applied to high-resolution imagery. Despite growing interest in\ntropical tree crown detection, annotated datasets remain scarce, hindering\nrobust model development. We introduce SelvaBox, the largest open-access\ndataset for tropical tree crown detection in high-resolution drone imagery. It\nspans three countries and contains more than 83,000 manually labeled crowns -\nan order of magnitude larger than all previous tropical forest datasets\ncombined. Extensive benchmarks on SelvaBox reveal two key findings: (1)\nhigher-resolution inputs consistently boost detection accuracy; and (2) models\ntrained exclusively on SelvaBox achieve competitive zero-shot detection\nperformance on unseen tropical tree crown datasets, matching or exceeding\ncompeting methods. Furthermore, jointly training on SelvaBox and three other\ndatasets at resolutions from 3 to 10 cm per pixel within a unified\nmulti-resolution pipeline yields a detector ranking first or second across all\nevaluated datasets. Our dataset, code, and pre-trained weights are made public.", "AI": {"tldr": "SelvaBox is the largest open-access dataset for tropical tree crown detection, featuring over 83,000 manually labeled crowns from high-resolution drone images.", "motivation": "To assist in studying tropical forest ecosystems, which are impacted by human activity and climate change, by overcoming the lack of annotated datasets for tree crown detection.", "method": "SelvaBox was created using high-resolution drone imagery across three countries, benchmarking its effectiveness with models trained on the dataset, and developing a unified multi-resolution training pipeline.", "result": "Models trained on SelvaBox exhibit strong performance, achieving competitive results on unseen datasets and performing well across multiple benchmarks.", "conclusion": "SelvaBox provides a valuable new resource for tropical forest research by advancing tree crown detection capabilities and making tools and data publicly available for further study and application."}}
{"id": "2507.00651", "pdf": "https://arxiv.org/pdf/2507.00651", "abs": "https://arxiv.org/abs/2507.00651", "authors": ["Maurizio Filippone", "Marius P. Linhard"], "title": "GANs Secretly Perform Approximate Bayesian Model Selection", "categories": ["cs.LG", "cs.CV", "stat.ML"], "comment": null, "summary": "Generative Adversarial Networks (GANs) are popular and successful generative\nmodels. Despite their success, optimization is notoriously challenging and they\nrequire regularization against overfitting. In this work, we explain the\nsuccess and limitations of GANs by interpreting them as probabilistic\ngenerative models. This interpretation enables us to view GANs as Bayesian\nneural networks with partial stochasticity, allowing us to establish conditions\nof universal approximation. We can then cast the adversarial-style optimization\nof several variants of GANs as the optimization of a proxy for the marginal\nlikelihood. Taking advantage of the connection between marginal likelihood\noptimization and Occam's razor, we can define regularization and optimization\nstrategies to smooth the loss landscape and search for solutions with minimum\ndescription length, which are associated with flat minima and good\ngeneralization. The results on a wide range of experiments indicate that these\nstrategies lead to performance improvements and pave the way to a deeper\nunderstanding of regularization strategies for GANs.", "AI": {"tldr": "The paper introduces a new interpretation of Generative Adversarial Networks (GANs) as probabilistic generative models, which enables novel regularization and optimization strategies for improving their performance.", "motivation": "GANs, despite their success in generative modeling, face challenges in optimization and are prone to overfitting, necessitating the development of better theoretical understanding and practical strategies for improvement.", "method": "The authors interpret GANs as Bayesian neural networks with partial stochasticity, leveraging this view to frame GAN optimization as maximizing a proxy for marginal likelihood. They then use concepts like Occam's razor to devise regularization and optimization techniques aimed at flat minima and better generalization.", "result": "The proposed strategies demonstrate performance improvements across a range of experiments, indicating their effectiveness in addressing GAN limitations.", "conclusion": "This work provides a novel probabilistic perspective on GANs, enabling more effective regularization and optimization strategies, ultimately improving their generalization and performance."}}
{"id": "2507.00394", "pdf": "https://arxiv.org/pdf/2507.00394", "abs": "https://arxiv.org/abs/2507.00394", "authors": ["Geng Zhang", "Shenggan Cheng", "Xuanlei Zhao", "Ziming Liu", "Yang You"], "title": "HelixPipe: Efficient Distributed Training of Long Sequence Transformers with Attention Parallel Pipeline Parallelism", "categories": ["cs.LG", "cs.DC"], "comment": null, "summary": "As transformer sequence lengths grow, existing pipeline parallelisms incur\nsuboptimal performance due to the quadratic attention computation and the\nsubstantial memory overhead. To relieve these challenges, we propose HelixPipe,\na novel pipeline parallelism for long sequence transformer training. First,\nHelixPipe introduces attention parallel partition, which schedules attention\ncomputations of different micro batches across different pipeline stages in\nparallel, reducing pipeline bubbles. Second, it employs a two-fold\nfirst-in-last-out micro batch schedule to balance memory usage and overlap\ncommunication with computation. Additionally, HelixPipe utilizes recomputation\nwithout attention and chunked MLP to mitigate fragmentation and enable longer\nsequences. Experiments demonstrate that HelixPipe gains increasing advantages\nwith longer sequence lengths, and outperforms existing methods in throughput\nand scalability across varying pipeline sizes, model sizes, and cluster\nconfigurations. Notably, it achieves a 26\\% speedup over baseline methods when\ntraining a 7B model with 128k sequence length on 64 H20 GPUs. Code is available\nat https://github.com/code-tunnel/Megatron-LM/tree/dev.", "AI": {"tldr": "HelixPipe is a novel pipeline parallelism method addressing performance and memory challenges in long-sequence transformer training, achieving superior scalability and speedup.", "motivation": "Existing pipeline parallelisms for long sequence transformer training face limitations in performance due to quadratic attention computation and high memory usage.", "method": "HelixPipe employs attention parallel partition for efficient pipeline scheduling, two-fold micro batch scheduling for memory balance and communication overlap, recomputation without attention, and chunked MLP to support longer sequences.", "result": "HelixPipe shows improved throughput and scalability, achieving a 26% speedup over baseline methods during 7B model training with 128k sequence length on 64 H20 GPUs.", "conclusion": "HelixPipe effectively enhances the training efficiency of transformer models for long sequences and demonstrates its superiority over existing methods in practical experiments."}}
{"id": "2507.00557", "pdf": "https://arxiv.org/pdf/2507.00557", "abs": "https://arxiv.org/abs/2507.00557", "authors": ["Tianyi Ding", "Haokun Li", "Xinpeng Ni", "Bican Xia", "Tianqi Zhao"], "title": "Advancing Local Search in SMT-NRA with MCSAT Integration", "categories": ["cs.AI", "cs.LO", "cs.SC"], "comment": null, "summary": "In this paper, we advance local search for Satisfiability Modulo the Theory\nof Nonlinear Real Arithmetic (SMT-NRA for short). First, we introduce a\ntwo-dimensional cell-jump move, called \\emph{$2d$-cell-jump}, generalizing the\nkey operation, cell-jump, of the local search method for SMT-NRA. Then, we\npropose an extended local search framework, named \\emph{$2d$-LS} (following the\nlocal search framework, LS, for SMT-NRA), integrating the model constructing\nsatisfiability calculus (MCSAT) framework to improve search efficiency. To\nfurther improve the efficiency of MCSAT, we implement a recently proposed\ntechnique called \\emph{sample-cell projection operator} for MCSAT, which is\nwell suited for CDCL-style search in the real domain and helps guide the search\naway from conflicting states. Finally, we design a hybrid framework for SMT-NRA\ncombining MCSAT, $2d$-LS and OpenCAD, to improve search efficiency through\ninformation exchange. The experimental results demonstrate improvements in\nlocal search performance, highlighting the effectiveness of the proposed\nmethods.", "AI": {"tldr": "This paper proposes an improved local search framework for SMT-NRA, integrating new techniques with MCSAT to enhance efficiency.", "motivation": "The motivation is to address inefficiencies in solving SMT-NRA problems by advancing local search methods and integrating recent techniques.", "method": "The authors introduce a 2d-cell-jump move, a $2d$-LS framework, and integrate a sample-cell projection operator within MCSAT to guide searches more effectively. They also combine MCSAT, $2d$-LS, and OpenCAD into a hybrid framework.", "result": "Experimental results show that the introduced methods improve the performance of local search for SMT-NRA.", "conclusion": "The proposed methods and hybrid framework enhance search efficiency and highlight the effectiveness of integrating advanced techniques for solving SMT-NRA."}}
{"id": "2507.00380", "pdf": "https://arxiv.org/pdf/2507.00380", "abs": "https://arxiv.org/abs/2507.00380", "authors": ["Vojt\u011bch Lanz", "Jan Haji\u010d jr"], "title": "Gregorian melody, modality, and memory: Segmenting chant with Bayesian nonparametrics", "categories": ["cs.CL"], "comment": null, "summary": "The idea that Gregorian melodies are constructed from some vocabulary of\nsegments has long been a part of chant scholarship. This so-called\n\"centonisation\" theory has received much musicological criticism, but frequent\nre-use of certain melodic segments has been observed in chant melodies, and the\nintractable number of possible segmentations allowed the option that some\nundiscovered segmentation exists that will yet prove the value of\ncentonisation, and recent empirical results have shown that segmentations can\noutperform music-theoretical features in mode classification. Inspired by the\nfact that Gregorian chant was memorised, we search for an optimal unsupervised\nsegmentation of chant melody using nested hierarchical Pitman-Yor language\nmodels. The segmentation we find achieves state-of-the-art performance in mode\nclassification. Modeling a monk memorising the melodies from one liturgical\nmanuscript, we then find empirical evidence for the link between mode\nclassification and memory efficiency, and observe more formulaic areas at the\nbeginnings and ends of melodies corresponding to the practical role of modality\nin performance. However, the resulting segmentations themselves indicate that\neven such a memory-optimal segmentation is not what is understood as\ncentonisation.", "AI": {"tldr": "The study explores segmentation in Gregorian chants, applying hierarchical Pitman-Yor models to identify melodic patterns for mode classification, achieving state-of-the-art results.", "motivation": "Address the frequent re-use of melodic segments in Gregorian chants and evaluate centonisation theory for better understanding modality and memory efficiency.", "method": "Used unsupervised segmentation through nested hierarchical Pitman-Yor language models to analyze melodic patterns and their role in mode classification.", "result": "Achieved advanced performance in mode classification, highlighting links between segmentation, modality, and memory efficiency in Gregorian chant melodies.", "conclusion": "Despite success in mode classification and memory efficiency, the segmentation results challenge traditional centonisation notions."}}
{"id": "2507.00024", "pdf": "https://arxiv.org/pdf/2507.00024", "abs": "https://arxiv.org/abs/2507.00024", "authors": ["Yeyong Yu", "Xilei Bian", "Jie Xiong", "Xing Wu", "Quan Qian"], "title": "AIMatDesign: Knowledge-Augmented Reinforcement Learning for Inverse Materials Design under Data Scarcity", "categories": ["cs.LG", "cond-mat.mtrl-sci", "cs.AI"], "comment": null, "summary": "With the growing demand for novel materials, machine learning-driven inverse\ndesign methods face significant challenges in reconciling the high-dimensional\nmaterials composition space with limited experimental data. Existing approaches\nsuffer from two major limitations: (I) machine learning models often lack\nreliability in high-dimensional spaces, leading to prediction biases during the\ndesign process; (II) these models fail to effectively incorporate domain expert\nknowledge, limiting their capacity to support knowledge-guided inverse design.\nTo address these challenges, we introduce AIMatDesign, a reinforcement learning\nframework that addresses these limitations by augmenting experimental data\nusing difference-based algorithms to build a trusted experience pool,\naccelerating model convergence. To enhance model reliability, an automated\nrefinement strategy guided by large language models (LLMs) dynamically corrects\nprediction inconsistencies, reinforcing alignment between reward signals and\nstate value functions. Additionally, a knowledge-based reward function\nleverages expert domain rules to improve stability and efficiency during\ntraining. Our experiments demonstrate that AIMatDesign significantly surpasses\ntraditional machine learning and reinforcement learning methods in discovery\nefficiency, convergence speed, and success rates. Among the numerous candidates\nproposed by AIMatDesign, experimental synthesis of representative Zr-based\nalloys yielded a top-performing BMG with 1.7GPa yield strength and 10.2\\%\nelongation, closely matching predictions. Moreover, the framework accurately\ncaptured the trend of yield strength variation with composition, demonstrating\nits reliability and potential for closed-loop materials discovery.", "AI": {"tldr": "AIMatDesign is a reinforcement learning framework addressing challenges in machine learning-driven materials design by incorporating expert knowledge, enhancing model reliability, and leveraging innovative strategies for data augmentation and prediction refinement.", "motivation": "Address challenges of high-dimensional material composition space and limited experimental data, alongside overcoming reliability and knowledge integration issues in machine learning models.", "method": "Introduced AIMatDesign using reinforcement learning with difference-based algorithms, trusted experimental data pools, LLM-guided prediction refinement, and knowledge-based reward functions.", "result": "AIMatDesign achieved superior performance in discovery efficiency, convergence speed, success rates, and accurately predicted material properties, validated experimentally.", "conclusion": "AIMatDesign demonstrates significant potential for accelerating and enhancing machine learning-driven materials discovery, ensuring reliable predictions and effective incorporation of expert knowledge."}}
{"id": "2507.00635", "pdf": "https://arxiv.org/pdf/2507.00635", "abs": "https://arxiv.org/abs/2507.00635", "authors": ["Tinghe Hong", "Shenlin Cai", "Boyang Li", "Kai Huang"], "title": "Stable Tracking of Eye Gaze Direction During Ophthalmic Surgery", "categories": ["cs.RO", "cs.CV", "cs.HC"], "comment": "Accepted by ICRA 2025", "summary": "Ophthalmic surgical robots offer superior stability and precision by reducing\nthe natural hand tremors of human surgeons, enabling delicate operations in\nconfined surgical spaces. Despite the advancements in developing vision- and\nforce-based control methods for surgical robots, preoperative navigation\nremains heavily reliant on manual operation, limiting the consistency and\nincreasing the uncertainty. Existing eye gaze estimation techniques in the\nsurgery, whether traditional or deep learning-based, face challenges including\ndependence on additional sensors, occlusion issues in surgical environments,\nand the requirement for facial detection. To address these limitations, this\nstudy proposes an innovative eye localization and tracking method that combines\nmachine learning with traditional algorithms, eliminating the requirements of\nlandmarks and maintaining stable iris detection and gaze estimation under\nvarying lighting and shadow conditions. Extensive real-world experiment results\nshow that our proposed method has an average estimation error of 0.58 degrees\nfor eye orientation estimation and 2.08-degree average control error for the\nrobotic arm's movement based on the calculated orientation.", "AI": {"tldr": "The paper presents a novel method for improved eye localization and tracking in ophthalmic surgical robots, addressing limitations in current systems and achieving high precision in eye orientation estimation and robotic control.", "motivation": "Ophthalmic surgical robots enhance precision and stability during surgeries by minimizing hand tremors. However, manual preoperative navigation introduces inconsistency and uncertainty. Current gaze estimation methods face issues like occlusion, reliance on additional sensors, and facial detection dependency.", "method": "The study introduces a hybrid approach combining machine learning and traditional algorithms for eye localization and tracking. This technique avoids the need for landmarks and provides stable detection under various lighting and shadow conditions.", "result": "Experiments demonstrated high accuracy, with an average error of 0.58 degrees in eye orientation estimation and 2.08 degrees in robotic arm control based on the estimated orientation.", "conclusion": "The proposed methodology successfully overcomes existing gaze estimation challenges, offering improved reliability and accuracy for ophthalmic surgical robots' preoperative navigation."}}
{"id": "2507.00182", "pdf": "https://arxiv.org/pdf/2507.00182", "abs": "https://arxiv.org/abs/2507.00182", "authors": ["J. I. Ru\u00edz", "A. M\u00e9ndez", "E. Rodr\u00edguez"], "title": "Graph-Based Deep Learning for Component Segmentation of Maize Plants", "categories": ["cs.CV"], "comment": null, "summary": "In precision agriculture, one of the most important tasks when exploring crop\nproduction is identifying individual plant components. There are several\nattempts to accomplish this task by the use of traditional 2D imaging, 3D\nreconstructions, and Convolutional Neural Networks (CNN). However, they have\nseveral drawbacks when processing 3D data and identifying individual plant\ncomponents. Therefore, in this work, we propose a novel Deep Learning\narchitecture to detect components of individual plants on Light Detection and\nRanging (LiDAR) 3D Point Cloud (PC) data sets. This architecture is based on\nthe concept of Graph Neural Networks (GNN), and feature enhancing with\nPrincipal Component Analysis (PCA). For this, each point is taken as a vertex\nand by the use of a K-Nearest Neighbors (KNN) layer, the edges are established,\nthus representing the 3D PC data set. Subsequently, Edge-Conv layers are used\nto further increase the features of each point. Finally, Graph Attention\nNetworks (GAT) are applied to classify visible phenotypic components of the\nplant, such as the leaf, stem, and soil. This study demonstrates that our\ngraph-based deep learning approach enhances segmentation accuracy for\nidentifying individual plant components, achieving percentages above 80% in the\nIoU average, thus outperforming other existing models based on point clouds.", "AI": {"tldr": "The study presents a novel Graph Neural Network-based method for segmenting individual plant components in 3D LiDAR Point Cloud data and achieves over 80% accuracy.", "motivation": "Current methods using 2D imaging, 3D reconstructions, and CNNs exhibit limitations in processing 3D data and identifying individual plant components.", "method": "The proposed method uses a GNN-based architecture that integrates PCA for feature enhancement, KNN for edge creation, Edge-Conv layers for feature improvement, and GAT for component classification in 3D LiDAR Point Cloud data.", "result": "The approach achieved over 80% average IoU accuracy, surpassing traditional models in segmentation performance.", "conclusion": "This graph-based deep learning method provides a significant improvement in segmenting plant components from 3D data, making it a superior alternative to existing methodologies."}}
{"id": "2507.00671", "pdf": "https://arxiv.org/pdf/2507.00671", "abs": "https://arxiv.org/abs/2507.00671", "authors": ["Congye Wang", "Matthew A. Fisher", "Heishiro Kanagawa", "Wilson Chen", "Chris. J. Oates"], "title": "Harnessing the Power of Reinforcement Learning for Adaptive MCMC", "categories": ["stat.CO", "cs.LG", "stat.ML"], "comment": null, "summary": "Sampling algorithms drive probabilistic machine learning, and recent years\nhave seen an explosion in the diversity of tools for this task. However, the\nincreasing sophistication of sampling algorithms is correlated with an increase\nin the tuning burden. There is now a greater need than ever to treat the tuning\nof samplers as a learning task in its own right. In a conceptual breakthrough,\nWang et al (2025) formulated Metropolis-Hastings as a Markov decision process,\nopening up the possibility for adaptive tuning using Reinforcement Learning\n(RL). Their emphasis was on theoretical foundations; realising the practical\nbenefit of Reinforcement Learning Metropolis-Hastings (RLMH) was left for\nsubsequent work. The purpose of this paper is twofold: First, we observe the\nsurprising result that natural choices of reward, such as the acceptance rate,\nor the expected squared jump distance, provide insufficient signal for training\nRLMH. Instead, we propose a novel reward based on the contrastive divergence,\nwhose superior performance in the context of RLMH is demonstrated. Second, we\nexplore the potential of RLMH and present adaptive gradient-based samplers that\nbalance flexibility of the Markov transition kernel with learnability of the\nassociated RL task. A comprehensive simulation study using the posteriordb\nbenchmark supports the practical effectiveness of RLMH.", "AI": {"tldr": "This paper builds on previous breakthroughs in adaptive tuning of Metropolis-Hastings using Reinforcement Learning and proposes a new reward metric to improve sampler training, showcasing its effectiveness through simulations.", "motivation": "The increase in the complexity of sampling algorithms has made their tuning more challenging, necessitating solutions that view sampler tuning as a learning task.", "method": "The authors propose a novel reward metric based on contrastive divergence for Reinforcement Learning Metropolis-Hastings (RLMH) and design adaptive gradient-based samplers to enhance both flexibility and learnability.", "result": "Using the posteriordb benchmark, the study demonstrates superior training outcomes for RLMH with the proposed metric and highlights the practical benefits of adaptive gradient-based samplers.", "conclusion": "Contrastive divergence provides a stronger training signal for Reinforcement Learning Metropolis-Hastings, and the adaptive gradient-based samplers developed in the paper show strong practical utility in simulation studies."}}
{"id": "2507.00423", "pdf": "https://arxiv.org/pdf/2507.00423", "abs": "https://arxiv.org/abs/2507.00423", "authors": ["Wenjin Mo", "Zhiyuan Li", "Minghong Fang", "Mingwei Fang"], "title": "Find a Scapegoat: Poisoning Membership Inference Attack and Defense to Federated Learning", "categories": ["cs.CR", "cs.DC", "cs.LG"], "comment": "To appear in ICCV 2025", "summary": "Federated learning (FL) allows multiple clients to collaboratively train a\nglobal machine learning model with coordination from a central server, without\nneeding to share their raw data. This approach is particularly appealing in the\nera of privacy regulations like the GDPR, leading many prominent companies to\nadopt it. However, FL's distributed nature makes it susceptible to poisoning\nattacks, where malicious clients, controlled by an attacker, send harmful data\nto compromise the model. Most existing poisoning attacks in FL aim to degrade\nthe model's integrity, such as reducing its accuracy, with limited attention to\nprivacy concerns from these attacks. In this study, we introduce FedPoisonMIA,\na novel poisoning membership inference attack targeting FL. FedPoisonMIA\ninvolves malicious clients crafting local model updates to infer membership\ninformation. Additionally, we propose a robust defense mechanism to mitigate\nthe impact of FedPoisonMIA attacks. Extensive experiments across various\ndatasets demonstrate the attack's effectiveness, while our defense approach\nreduces its impact to a degree.", "AI": {"tldr": "The paper introduces FedPoisonMIA, a poisoning membership inference attack targeting federated learning (FL), demonstrating its efficacy, and proposes an accompanying defense mechanism.", "motivation": "The motivation is to address the vulnerabilities of federated learning, particularly in the context of privacy risks from poisoning attacks that have received limited attention.", "method": "The authors developed and tested FedPoisonMIA, where malicious clients craft updates to infer membership information in FL. They also propose a defense mechanism to counter this attack.", "result": "Experiments on various datasets show the effectiveness of FedPoisonMIA, while the proposed defense reduces its potency to some extent.", "conclusion": "The study reveals a new privacy threat to FL and provides initial steps toward mitigating it, emphasizing the need for further research into robust defenses."}}
{"id": "2507.00726", "pdf": "https://arxiv.org/pdf/2507.00726", "abs": "https://arxiv.org/abs/2507.00726", "authors": ["Dongyoon Hwang", "Hojoon Lee", "Jaegul Choo", "Dongmin Park", "Jongho Park"], "title": "Can Large Language Models Develop Strategic Reasoning? Post-training Insights from Learning Chess", "categories": ["cs.AI", "cs.LG"], "comment": "27 pages", "summary": "While reinforcement learning (RL) for large language models (LLMs) has shown\npromise in mathematical reasoning, strategic reasoning for LLMs using RL\nremains largely unexplored. We investigate whether LLMs can develop strategic\nreasoning capabilities through RL in chess. To this end, we leverage a\nchess-pretrained action-value network to provide dense reward on the LLM's\noutput move quality, which can be seen as a form of knowledge distillation. Our\nexperiments show that our distillation-based dense rewards often outperform\nsparse binary rewards. However, surprisingly, all models plateau far below\nexpert levels. We provide SFT and RL ablations on chess reasoning training and\nfind evidence that this limitation stems from a deficit in the pretrained\nmodels' internal understanding of chess--a deficit which RL alone may not be\nable to fully overcome.", "AI": {"tldr": "The paper explores developing strategic reasoning in large language models (LLMs) using reinforcement learning (RL) in chess, finding that dense rewards outperform sparse ones but LLMs still underperform compared to experts.", "motivation": "To determine whether reinforcement learning can enhance strategic reasoning capabilities of LLMs, particularly in the context of chess.", "method": "The study used a chess-pretrained action-value network to provide dense rewards for LLMs' move quality, akin to knowledge distillation. Sparse binary rewards were also compared. Ablation studies examined the effects of supervised fine-tuning (SFT) and RL.", "result": "Dense rewards enabled better performance than sparse rewards, but all tested models plateaued at performance levels far below chess experts.", "conclusion": "Pretrained LLMs exhibit intrinsic limitations in their chess understanding, which RL alone cannot fully correct, highlighting a gap in developing advanced strategic reasoning through current techniques."}}
{"id": "2507.00389", "pdf": "https://arxiv.org/pdf/2507.00389", "abs": "https://arxiv.org/abs/2507.00389", "authors": ["Jing Ren", "Wenhao Zhou", "Bowen Li", "Mujie Liu", "Nguyen Linh Dan Le", "Jiade Cen", "Liping Chen", "Ziqi Xu", "Xiwei Xu", "Xiaodong Li"], "title": "Causal Prompting for Implicit Sentiment Analysis with Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Implicit Sentiment Analysis (ISA) aims to infer sentiment that is implied\nrather than explicitly stated, requiring models to perform deeper reasoning\nover subtle contextual cues. While recent prompting-based methods using Large\nLanguage Models (LLMs) have shown promise in ISA, they often rely on majority\nvoting over chain-of-thought (CoT) reasoning paths without evaluating their\ncausal validity, making them susceptible to internal biases and spurious\ncorrelations. To address this challenge, we propose CAPITAL, a causal prompting\nframework that incorporates front-door adjustment into CoT reasoning. CAPITAL\ndecomposes the overall causal effect into two components: the influence of the\ninput prompt on the reasoning chains, and the impact of those chains on the\nfinal output. These components are estimated using encoder-based clustering and\nthe NWGM approximation, with a contrastive learning objective used to better\nalign the encoder's representation with the LLM's reasoning space. Experiments\non benchmark ISA datasets with three LLMs demonstrate that CAPITAL consistently\noutperforms strong prompting baselines in both accuracy and robustness,\nparticularly under adversarial conditions. This work offers a principled\napproach to integrating causal inference into LLM prompting and highlights its\nbenefits for bias-aware sentiment reasoning. The source code and case study are\navailable at: https://github.com/whZ62/CAPITAL.", "AI": {"tldr": "CAPITAL is a causal prompting framework for implicit sentiment analysis (ISA) using large language models (LLMs), aiming to address biases and spurious correlations by decomposing causal effects and aligning reasoning chains.", "motivation": "Implicit sentiment analysis requires identifying subtle contextual cues beyond explicit sentiment, necessitating models capable of deeper reasoning. Current LLM methods face challenges with internal biases and correlations.", "method": "The proposed CAPITAL framework integrates causal inference into chain-of-thought reasoning through front-door adjustment. It uses encoder-based clustering, NWGM approximation, and a contrastive learning objective to align representations.", "result": "CAPITAL demonstrates superior accuracy and robustness over baseline methods in ISA tasks on benchmark datasets, especially under adversarial conditions.", "conclusion": "Integrating causal inference into LLM prompting improves bias-aware sentiment reasoning, validating CAPITAL's approach for ISA tasks and providing promising accuracy gains."}}
{"id": "2507.00644", "pdf": "https://arxiv.org/pdf/2507.00644", "abs": "https://arxiv.org/abs/2507.00644", "authors": ["Rohit Kumar", "Melya Boukheddimi", "Dennis Mronga", "Shivesh Kumar", "Frank Kirchner"], "title": "Parallel Transmission Aware Co-Design: Enhancing Manipulator Performance Through Actuation-Space Optimization", "categories": ["cs.RO"], "comment": null, "summary": "In robotics, structural design and behavior optimization have long been\nconsidered separate processes, resulting in the development of systems with\nlimited capabilities. Recently, co-design methods have gained popularity, where\nbi-level formulations are used to simultaneously optimize the robot design and\nbehavior for specific tasks. However, most implementations assume a serial or\ntree-type model of the robot, overlooking the fact that many robot platforms\nincorporate parallel mechanisms. In this paper, we present a novel co-design\napproach that explicitly incorporates parallel coupling constraints into the\ndynamic model of the robot. In this framework, an outer optimization loop\nfocuses on the design parameters, in our case the transmission ratios of a\nparallel belt-driven manipulator, which map the desired torques from the joint\nspace to the actuation space. An inner loop performs trajectory optimization in\nthe actuation space, thus exploiting the entire dynamic range of the\nmanipulator. We compare the proposed method with a conventional co-design\napproach based on a simplified tree-type model. By taking advantage of the\nactuation space representation, our approach leads to a significant increase in\ndynamic payload capacity compared to the conventional co-design implementation.", "AI": {"tldr": "The paper introduces a co-design approach that incorporates parallel coupling constraints for robot design and behavior optimization, leading to enhanced payload capacity.", "motivation": "To overcome the limitations of separating structural design and behavior optimization in robotics and address the neglect of parallel mechanisms in existing co-design methods.", "method": "The framework includes an outer optimization loop for design parameters (transmission ratios) and an inner loop for trajectory optimization in actuation space, explicitly considering parallel coupling constraints.", "result": "The proposed method significantly improves the dynamic payload capacity compared to traditional co-design approaches using simplified tree-type models.", "conclusion": "Incorporating parallel coupling constraints into co-design frameworks provides a more effective optimization of robot design and behavior, enhancing performance in specific tasks."}}
{"id": "2507.00224", "pdf": "https://arxiv.org/pdf/2507.00224", "abs": "https://arxiv.org/abs/2507.00224", "authors": ["Changsoo Jung", "Sheikh Mannan", "Jack Fitzgerald", "Nathaniel Blanchard"], "title": "Computer Vision for Objects used in Group Work: Challenges and Opportunities", "categories": ["cs.CV", "cs.HC"], "comment": "Accepted to AIED 2025 Late Breaking Results Track", "summary": "Interactive and spatially aware technologies are transforming educational\nframeworks, particularly in K-12 settings where hands-on exploration fosters\ndeeper conceptual understanding. However, during collaborative tasks, existing\nsystems often lack the ability to accurately capture real-world interactions\nbetween students and physical objects. This issue could be addressed with\nautomatic 6D pose estimation, i.e., estimation of an object's position and\norientation in 3D space from RGB images or videos. For collaborative groups\nthat interact with physical objects, 6D pose estimates allow AI systems to\nrelate objects and entities. As part of this work, we introduce FiboSB, a novel\nand challenging 6D pose video dataset featuring groups of three participants\nsolving an interactive task featuring small hand-held cubes and a weight scale.\nThis setup poses unique challenges for 6D pose because groups are holistically\nrecorded from a distance in order to capture all participants -- this, coupled\nwith the small size of the cubes, makes 6D pose estimation inherently\nnon-trivial. We evaluated four state-of-the-art 6D pose estimation methods on\nFiboSB, exposing the limitations of current algorithms on collaborative group\nwork. An error analysis of these methods reveals that the 6D pose methods'\nobject detection modules fail. We address this by fine-tuning YOLO11-x for\nFiboSB, achieving an overall mAP_50 of 0.898. The dataset, benchmark results,\nand analysis of YOLO11-x errors presented here lay the groundwork for\nleveraging the estimation of 6D poses in difficult collaborative contexts.", "AI": {"tldr": "The paper introduces FiboSB, a 6D pose video dataset, to address challenges in collaborative group work by estimating object positions and orientations in 3D space. It evaluates state-of-the-art methods and proposes fine-tuning YOLO11-x to improve performance.", "motivation": "Existing systems fail to accurately capture real-world interactions between students and physical objects, especially in collaborative educational contexts. 6D pose estimation offers a solution by connecting objects and entities.", "method": "The authors created the FiboSB dataset with video recordings of groups solving interactive tasks using small cubes and weight scales. They evaluated four 6D pose estimation methods and improved detection accuracy by fine-tuning YOLO11-x.", "result": "Fine-tuning YOLO11-x yielded better object detection performance with an mAP_50 score of 0.898. Error analysis highlighted failures in detection modules across current methods.", "conclusion": "The dataset and findings provide critical insights for improving 6D pose estimation methods, paving the way for enhanced AI systems in collaborative educational contexts."}}
{"id": "2507.00736", "pdf": "https://arxiv.org/pdf/2507.00736", "abs": "https://arxiv.org/abs/2507.00736", "authors": ["Arthur Thuy", "Ekaterina Loginova", "Dries F. Benoit"], "title": "Ordinality in Discrete-level Question Difficulty Estimation: Introducing Balanced DRPS and OrderedLogitNN", "categories": ["cs.LG", "stat.ML"], "comment": "Published in the EvalLAC'25 workshop at AIED 2025", "summary": "Recent years have seen growing interest in Question Difficulty Estimation\n(QDE) using natural language processing techniques. Question difficulty is\noften represented using discrete levels, framing the task as ordinal regression\ndue to the inherent ordering from easiest to hardest. However, the literature\nhas neglected the ordinal nature of the task, relying on classification or\ndiscretized regression models, with specialized ordinal regression methods\nremaining unexplored. Furthermore, evaluation metrics are tightly coupled to\nthe modeling paradigm, hindering cross-study comparability. While some metrics\nfail to account for the ordinal structure of difficulty levels, none adequately\naddress class imbalance, resulting in biased performance assessments. This\nstudy addresses these limitations by benchmarking three types of model outputs\n-- discretized regression, classification, and ordinal regression -- using the\nbalanced Discrete Ranked Probability Score (DRPS), a novel metric that jointly\ncaptures ordinality and class imbalance. In addition to using popular ordinal\nregression methods, we propose OrderedLogitNN, extending the ordered logit\nmodel from econometrics to neural networks. We fine-tune BERT on the RACE++ and\nARC datasets and find that OrderedLogitNN performs considerably better on\ncomplex tasks. The balanced DRPS offers a robust and fair evaluation metric for\ndiscrete-level QDE, providing a principled foundation for future research.", "AI": {"tldr": "This study addresses limitations in Question Difficulty Estimation (QDE) and introduces a novel evaluation metric, DRPS, alongside a new model, OrderedLogitNN, which shows strong performance.", "motivation": "The paper aims to tackle shortcomings in QDE methods and evaluation metrics which disregard the ordinal nature of question difficulty and fail in addressing class imbalance.", "method": "The paper benchmarks three modeling paradigms for QDE and introduces OrderedLogitNN, a new neural network model based on ordinal regression. Fine-tuning BERT was done on RACE++ and ARC datasets.", "result": "OrderedLogitNN outperforms other methods on complex QDE tasks. Additionally, the balanced DRPS metric successfully accounts for both ordinality and class imbalance.", "conclusion": "OrderedLogitNN and balanced DRPS set new standards for model design and evaluation in QDE tasks, offering a robust framework for future studies."}}
{"id": "2507.00810", "pdf": "https://arxiv.org/pdf/2507.00810", "abs": "https://arxiv.org/abs/2507.00810", "authors": ["Qing Xu", "Xiaohua Xuan"], "title": "A Robust Algorithm for Non-IID Machine Learning Problems with Convergence Analysis", "categories": ["cs.AI", "math.OC"], "comment": null, "summary": "In this paper, we propose an improved numerical algorithm for solving minimax\nproblems based on nonsmooth optimization, quadratic programming and iterative\nprocess. We also provide a rigorous proof of convergence for our algorithm\nunder some mild assumptions, such as gradient continuity and boundedness. Such\nan algorithm can be widely applied in various fields such as robust\noptimization, imbalanced learning, etc.", "AI": {"tldr": "This paper introduces an improved algorithm for minimax problems, ensures convergence under mild assumptions, and highlights its wide applications.", "motivation": "To solve minimax problems efficiently by utilizing nonsmooth optimization, quadratic programming, and iterative methods due to their challenges in various applications.", "method": "An improved numerical algorithm supported by a rigorous convergence proof under assumptions like gradient continuity and boundedness.", "result": "The algorithm demonstrates theoretical convergence and has potential applications in robust optimization and imbalanced learning.", "conclusion": "The method is effective and versatile, addressing minimax problems with broad applicational potential."}}
{"id": "2507.00439", "pdf": "https://arxiv.org/pdf/2507.00439", "abs": "https://arxiv.org/abs/2507.00439", "authors": ["Gauri Kambhatla", "Sanjana Gautam", "Angela Zhang", "Alex Liu", "Ravi Srinivasan", "Junyi Jessy Li", "Matthew Lease"], "title": "Beyond Sociodemographic Prompting: Using Supervision to Align LLMs with Human Response Distributions", "categories": ["cs.CL"], "comment": null, "summary": "The ability to accurately predict how different population groups would\nanswer subjective questions would have great value. In this work, we show that\nuse of relatively simple supervision can greatly improve language model\nalignment with diverse population groups, as measured over three datasets\nspanning various topics. Beyond evaluating average performance, we also report\nhow alignment varies across specific groups. The simplicity and generality of\nour approach promotes easy adoption, while our broad findings provide useful\nguidance for when to use or not use our approach in practice. By conducting\nevaluation over many LLMs and prompting strategies, along with open-sourcing\nour work, we provide a useful benchmark to stimulate future research.", "AI": {"tldr": "This paper proposes a simple and effective method to improve language models' alignment with diverse population groups in subjective question answering, evaluates its performance across datasets and groups, and provides tools for future research.", "motivation": "The authors aim to address the challenge of aligning language models with the diverse perspectives of various population groups when answering subjective questions.", "method": "The authors utilized relatively simple supervision techniques to enhance the language models' alignment, evaluated the approach across multiple datasets, and tested it on various LLMs and prompting strategies while open-sourcing their work.", "result": "Their approach improved the alignment of language models with diverse population groups over three datasets, highlighting its effectiveness and the variation of performance across specific groups.", "conclusion": "The simplicity and generality of the method make it easy to adopt and provide practical insights for its application. The research also establishes a benchmark to inspire future work in this area."}}
{"id": "2507.00026", "pdf": "https://arxiv.org/pdf/2507.00026", "abs": "https://arxiv.org/abs/2507.00026", "authors": ["Jiale Ding", "Xiang Zheng", "Cong Wang", "Wei-Bin Lee", "Xingjun Ma", "Yu-Gang Jiang"], "title": "ROSE: Toward Reality-Oriented Safety Evaluation of Large Language Models", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CY"], "comment": null, "summary": "As Large Language Models (LLMs) are increasingly deployed as black-box\ncomponents in real-world applications, evaluating their safety-especially under\nadversarial prompting-has become critical. Arguably, effective safety\nevaluations should be adaptive, evolving with LLM capabilities, and also cover\na broad spectrum of harmful topics and real-world scenarios to fully expose\npotential vulnerabilities. Existing manual safety benchmarks, built on\nhandcrafted adversarial prompts, are limited by their static nature and the\nintensive labor required to update them, making it difficult to keep pace with\nrapidly advancing LLMs. In contrast, automated adversarial prompt generation\noffers a promising path toward adaptive evaluation. However, current methods\noften suffer from insufficient adversarial topic coverage (topic-level\ndiversity) and weak alignment with real-world contexts. These shortcomings stem\nfrom the exploration-exploitation dilemma in black-box optimization and a lack\nof real-world contextualization, resulting in adversarial prompts that are both\ntopically narrow and scenario-repetitive. To address these issues, we propose\nReality-Oriented Safety Evaluation (ROSE), a novel framework that uses\nmulti-objective reinforcement learning to fine-tune an adversarial LLM for\ngenerating topically diverse and contextually rich adversarial prompts.\nExperiments show that ROSE outperforms existing methods in uncovering safety\nvulnerabilities in state-of-the-art LLMs, with notable improvements in\nintegrated evaluation metrics. We hope ROSE represents a step toward more\npractical and reality-oriented safety evaluation of LLMs. WARNING: This paper\ncontains examples of potentially harmful text.", "AI": {"tldr": "The paper introduces ROSE (Reality-Oriented Safety Evaluation), a new framework for adaptively assessing the safety of large language models (LLMs) using adversarial prompt generation.", "motivation": "The need to evaluate the safety of LLMs as they are widely used and face risks under adversarial prompting, highlighting limitations in static, manual safety benchmarks.", "method": "The authors propose ROSE, leveraging multi-objective reinforcement learning to fine-tune an adversarial LLM for generating diverse and contextually relevant prompts.", "result": "Experiments demonstrate that ROSE effectively uncovers safety vulnerabilities in LLMs, achieving improvements in evaluation metrics compared to existing methods.", "conclusion": "ROSE provides a more adaptive, practical, and context-aware benchmark for evaluating potential safety risks in LLMs."}}
{"id": "2507.00677", "pdf": "https://arxiv.org/pdf/2507.00677", "abs": "https://arxiv.org/abs/2507.00677", "authors": ["Dongho Kang", "Jin Cheng", "Fatemeh Zargarbashi", "Taerim Yoon", "Sungjoon Choi", "Stelian Coros"], "title": "Learning Steerable Imitation Controllers from Unstructured Animal Motions", "categories": ["cs.RO"], "comment": "The supplementary video is available at https://youtu.be/DukyUGNYf5A", "summary": "This paper presents a control framework for legged robots that leverages\nunstructured real-world animal motion data to generate animal-like and\nuser-steerable behaviors. Our framework learns to follow velocity commands\nwhile reproducing the diverse gait patterns in the original dataset. To begin\nwith, animal motion data is transformed into a robot-compatible database using\nconstrained inverse kinematics and model predictive control, bridging the\nmorphological and physical gap between the animal and the robot. Subsequently,\na variational autoencoder-based motion synthesis module captures the diverse\nlocomotion patterns in the motion database and generates smooth transitions\nbetween them in response to velocity commands. The resulting kinematic motions\nserve as references for a reinforcement learning-based feedback controller\ndeployed on physical robots. We show that this approach enables a quadruped\nrobot to adaptively switch gaits and accurately track user velocity commands\nwhile maintaining the stylistic coherence of the motion data. Additionally, we\nprovide component-wise evaluations to analyze the system's behavior in depth\nand demonstrate the efficacy of our method for more accurate and reliable\nmotion imitation.", "AI": {"tldr": "This paper introduces a framework for legged robots to imitate animal-like movements using real-world animal motion data, ensuring user control over robot behaviors.", "motivation": "To enhance legged robot locomotion by leveraging animal motion data for adaptive and stylistically coherent movement patterns.", "method": "The framework uses constrained inverse kinematics and model predictive control to convert animal motion data for robots, applies a variational autoencoder for motion synthesis, and employs reinforcement learning for physical robot control.", "result": "The approach enables quadruped robots to switch between gaits adaptively, execute user velocity commands accurately, and maintain stylistic similarity to animal motion.", "conclusion": "The proposed framework successfully blends animal-inspired locomotion patterns with user-guided commands, enabling reliable and accurate motion imitation by legged robots."}}
{"id": "2507.00243", "pdf": "https://arxiv.org/pdf/2507.00243", "abs": "https://arxiv.org/abs/2507.00243", "authors": ["Chi-Yao Huang", "Zeel Bhatt", "Yezhou Yang"], "title": "VOCAL: Visual Odometry via ContrAstive Learning", "categories": ["cs.CV"], "comment": null, "summary": "Breakthroughs in visual odometry (VO) have fundamentally reshaped the\nlandscape of robotics, enabling ultra-precise camera state estimation that is\ncrucial for modern autonomous systems. Despite these advances, many\nlearning-based VO techniques rely on rigid geometric assumptions, which often\nfall short in interpretability and lack a solid theoretical basis within fully\ndata-driven frameworks. To overcome these limitations, we introduce VOCAL\n(Visual Odometry via ContrAstive Learning), a novel framework that reimagines\nVO as a label ranking challenge. By integrating Bayesian inference with a\nrepresentation learning framework, VOCAL organizes visual features to mirror\ncamera states. The ranking mechanism compels similar camera states to converge\ninto consistent and spatially coherent representations within the latent space.\nThis strategic alignment not only bolsters the interpretability of the learned\nfeatures but also ensures compatibility with multimodal data sources. Extensive\nevaluations on the KITTI dataset highlight VOCAL's enhanced interpretability\nand flexibility, pushing VO toward more general and explainable spatial\nintelligence.", "AI": {"tldr": "VOCAL reimagines visual odometry (VO) using contrastive learning and Bayesian inference for camera state estimation, improving interpretability and flexibility.", "motivation": "Existing learning-based VO techniques rely on rigid geometric assumptions, limiting interpretability and lacking robust theoretical foundations.", "method": "VOCAL uses contrastive learning to frame VO as a label ranking challenge, aligning visual features with camera states using Bayesian inference.", "result": "Evaluations on the KITTI dataset show enhanced interpretability and flexibility of VOCAL over traditional VO methods.", "conclusion": "VOCAL advances VO towards general, explainable spatial intelligence by leveraging data-driven and multimodal-friendly frameworks."}}
{"id": "2507.00672", "pdf": "https://arxiv.org/pdf/2507.00672", "abs": "https://arxiv.org/abs/2507.00672", "authors": ["Haoxiang Luo", "Yinqiu Liu", "Ruichen Zhang", "Jiacheng Wang", "Gang Sun", "Dusit Niyato", "Hongfang Yu", "Zehui Xiong", "Xianbin Wang", "Xuemin Shen"], "title": "Toward Edge General Intelligence with Multiple-Large Language Model (Multi-LLM): Architecture, Trust, and Orchestration", "categories": ["cs.NI", "cs.DC"], "comment": null, "summary": "Edge computing enables real-time data processing closer to its source, thus\nimproving the latency and performance of edge-enabled AI applications. However,\ntraditional AI models often fall short when dealing with complex, dynamic tasks\nthat require advanced reasoning and multimodal data processing. This survey\nexplores the integration of multi-LLMs (Large Language Models) to address this\nin edge computing, where multiple specialized LLMs collaborate to enhance task\nperformance and adaptability in resource-constrained environments. We review\nthe transition from conventional edge AI models to single LLM deployment and,\nultimately, to multi-LLM systems. The survey discusses enabling technologies\nsuch as dynamic orchestration, resource scheduling, and cross-domain knowledge\ntransfer that are key for multi-LLM implementation. A central focus is on\ntrusted multi-LLM systems, ensuring robust decision-making in environments\nwhere reliability and privacy are crucial. We also present multimodal multi-LLM\narchitectures, where multiple LLMs specialize in handling different data\nmodalities, such as text, images, and audio, by integrating their outputs for\ncomprehensive analysis. Finally, we highlight future directions, including\nimproving resource efficiency, trustworthy governance multi-LLM systems, while\naddressing privacy, trust, and robustness concerns. This survey provides a\nvaluable reference for researchers and practitioners aiming to leverage\nmulti-LLM systems in edge computing applications.", "AI": {"tldr": "This survey reviews multi-LLMs for edge computing, focusing on their collaborative advantages in resource-constrained, multimodal, and privacy-sensitive environments.", "motivation": "Edge computing needs enhanced AI models capable of complex reasoning and handling multimodal data, which traditional models struggle to achieve.", "method": "The paper explores multi-LLM systems, discussing enabling technologies like dynamic orchestration, resource scheduling, and cross-domain knowledge integration, along with multimodal architectures for thorough analysis.", "result": "Multi-LLM systems showcase improved adaptability and task performance in edge computing, with insights into architectures and methodologies for their deployment.", "conclusion": "Multi-LLM systems show significant promise for advancing edge AI applications, though challenges like resource efficiency, trust, and privacy still need targeted research."}}
{"id": "2507.00841", "pdf": "https://arxiv.org/pdf/2507.00841", "abs": "https://arxiv.org/abs/2507.00841", "authors": ["Siyuan Liang", "Tianmeng Fang", "Zhe Liu", "Aishan Liu", "Yan Xiao", "Jinyuan He", "Ee-Chien Chang", "Xiaochun Cao"], "title": "SafeMobile: Chain-level Jailbreak Detection and Automated Evaluation for Multimodal Mobile Agents", "categories": ["cs.AI", "cs.CR"], "comment": "12 pages", "summary": "With the wide application of multimodal foundation models in intelligent\nagent systems, scenarios such as mobile device control, intelligent assistant\ninteraction, and multimodal task execution are gradually relying on such large\nmodel-driven agents. However, the related systems are also increasingly exposed\nto potential jailbreak risks. Attackers may induce the agents to bypass the\noriginal behavioral constraints through specific inputs, and then trigger\ncertain risky and sensitive operations, such as modifying settings, executing\nunauthorized commands, or impersonating user identities, which brings new\nchallenges to system security. Existing security measures for intelligent\nagents still have limitations when facing complex interactions, especially in\ndetecting potentially risky behaviors across multiple rounds of conversations\nor sequences of tasks. In addition, an efficient and consistent automated\nmethodology to assist in assessing and determining the impact of such risks is\ncurrently lacking. This work explores the security issues surrounding mobile\nmultimodal agents, attempts to construct a risk discrimination mechanism by\nincorporating behavioral sequence information, and designs an automated\nassisted assessment scheme based on a large language model. Through preliminary\nvalidation in several representative high-risk tasks, the results show that the\nmethod can improve the recognition of risky behaviors to some extent and assist\nin reducing the probability of agents being jailbroken. We hope that this study\ncan provide some valuable references for the security risk modeling and\nprotection of multimodal intelligent agent systems.", "AI": {"tldr": "The paper identifies security risks in multimodal intelligent agents, proposes a risk discrimination mechanism using behavioral sequences, and validates an automated assessment method for enhanced security.", "motivation": "Threats posed by attackers exploiting multimodal intelligent agents for unauthorized or sensitive operations demand effective security mechanisms.", "method": "The authors construct a behavioral sequence-informed risk discrimination mechanism and design an automated assessment strategy based on large language models.", "result": "Preliminary validation shows improved detection of risky behaviors and reduced likelihood of agents being jailbroken.", "conclusion": "This study provides insights into improving security risk modeling and protection measures for multimodal intelligent agent systems."}}
{"id": "2507.00460", "pdf": "https://arxiv.org/pdf/2507.00460", "abs": "https://arxiv.org/abs/2507.00460", "authors": ["Md. Najib Hasan", "Mohammad Fakhruddin Babar", "Souvika Sarkar", "Monowar Hasan", "Santu Karmaker"], "title": "Pitfalls of Evaluating Language Models with Open Benchmarks", "categories": ["cs.CL"], "comment": null, "summary": "Open Large Language Model (LLM) benchmarks, such as HELM and BIG-bench, offer\nstandardized, transparent protocols that facilitate the fair comparison,\nreproducibility, and iterative advancement of Language Models (LMs). However,\ntheir openness also introduces critical and underexplored pitfalls. This study\nexposes these weaknesses by systematically constructing ``cheating'' models --\nsmaller variants of BART, T5, and GPT-2 fine-tuned directly on public test sets\n-- which achieve top rankings on a prominent open, holistic benchmark (HELM)\ndespite poor generalization and limited practical utility. Our findings\nunderscore three key insights: \\ca high leaderboard performance on open\nbenchmarks may not always reflect real-world effectiveness; \\cb private or\ndynamic benchmarks must complement open evaluations to safeguard integrity; and\n\\cc a fundamental reevaluation of current benchmarking practices is essential\nto ensure robust and trustworthy LM assessments.", "AI": {"tldr": "Open benchmarks for large language models are useful for standardized comparisons, but they are vulnerable to exploitation by models 'cheating' the protocols, suggesting the need for complementary private/dynamic benchmarks.", "motivation": "The paper aims to expose the vulnerabilities of open LLM benchmarks, which, while crucial for transparency and standardization, can be exploited by models fine-tuned to cheat the evaluation systems.", "method": "Using smaller versions of BART, T5, and GPT-2, the study creates models specifically fine-tuned on public test sets to demonstrate how these 'cheating' models achieve high scores without true generalization.", "result": "The cheating models attained top rankings on the HELM benchmark, showcasing that performance on open benchmarks can be misleading and may not reflect real-world usability.", "conclusion": "There is a pressing need to complement open benchmarks with private or dynamic alternatives and rethink current benchmarking practices to ensure reliable evaluation of language models."}}
{"id": "2507.00028", "pdf": "https://arxiv.org/pdf/2507.00028", "abs": "https://arxiv.org/abs/2507.00028", "authors": ["Lihuan Li", "Hao Xue", "Shuang Ao", "Yang Song", "Flora Salim"], "title": "HiT-JEPA: A Hierarchical Self-supervised Trajectory Embedding Framework for Similarity Computation", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "The representation of urban trajectory data plays a critical role in\neffectively analyzing spatial movement patterns. Despite considerable progress,\nthe challenge of designing trajectory representations that can capture diverse\nand complementary information remains an open research problem. Existing\nmethods struggle in incorporating trajectory fine-grained details and\nhigh-level summary in a single model, limiting their ability to attend to both\nlong-term dependencies while preserving local nuances. To address this, we\npropose HiT-JEPA (Hierarchical Interactions of Trajectory Semantics via a Joint\nEmbedding Predictive Architecture), a unified framework for learning\nmulti-scale urban trajectory representations across semantic abstraction\nlevels. HiT-JEPA adopts a three-layer hierarchy that progressively captures\npoint-level fine-grained details, intermediate patterns, and high-level\ntrajectory abstractions, enabling the model to integrate both local dynamics\nand global semantics in one coherent structure. Extensive experiments on\nmultiple real-world datasets for trajectory similarity computation show that\nHiT-JEPA's hierarchical design yields richer, multi-scale representations. Code\nis available at: https://anonymous.4open.science/r/HiT-JEPA.", "AI": {"tldr": "HiT-JEPA is a novel unified framework for urban trajectory representation, addressing challenges in capturing multi-scale trajectory details and semantics.", "motivation": "To overcome limitations in existing urban trajectory representation methods that fail to integrate fine-grained and high-level details effectively.", "method": "HiT-JEPA utilizes a three-layer hierarchical architecture to capture point-level details, intermediate patterns, and high-level trajectory abstractions.", "result": "Experiments demonstrate that HiT-JEPA produces superior multi-scale trajectory representations, validated through trajectory similarity computation on real-world datasets.", "conclusion": "HiT-JEPA successfully integrates detailed and abstract trajectory semantics, presenting a coherent framework for urban trajectory analysis."}}
{"id": "2507.00816", "pdf": "https://arxiv.org/pdf/2507.00816", "abs": "https://arxiv.org/abs/2507.00816", "authors": ["Mengyun Wang", "Bo Wang", "Yifeng Niu", "Chang Wang"], "title": "PI-WAN: A Physics-Informed Wind-Adaptive Network for Quadrotor Dynamics Prediction in Unknown Environments", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Accurate dynamics modeling is essential for quadrotors to achieve precise\ntrajectory tracking in various applications. Traditional physical\nknowledge-driven modeling methods face substantial limitations in unknown\nenvironments characterized by variable payloads, wind disturbances, and\nexternal perturbations. On the other hand, data-driven modeling methods suffer\nfrom poor generalization when handling out-of-distribution (OoD) data,\nrestricting their effectiveness in unknown scenarios. To address these\nchallenges, we introduce the Physics-Informed Wind-Adaptive Network (PI-WAN),\nwhich combines knowledge-driven and data-driven modeling methods by embedding\nphysical constraints directly into the training process for robust quadrotor\ndynamics learning. Specifically, PI-WAN employs a Temporal Convolutional\nNetwork (TCN) architecture that efficiently captures temporal dependencies from\nhistorical flight data, while a physics-informed loss function applies physical\nprinciples to improve model generalization and robustness across previously\nunseen conditions. By incorporating real-time prediction results into a model\npredictive control (MPC) framework, we achieve improvements in closed-loop\ntracking performance. Comprehensive simulations and real-world flight\nexperiments demonstrate that our approach outperforms baseline methods in terms\nof prediction accuracy, tracking precision, and robustness to unknown\nenvironments.", "AI": {"tldr": "The paper presents the Physics-Informed Wind-Adaptive Network (PI-WAN), a method combining physical and data-driven modeling to improve quadrotor dynamics modeling under unknown conditions.", "motivation": "Quadrotors require accurate dynamics modeling for precise trajectory tracking, but current methods struggle in unknown environments with variable payloads and disturbances.", "method": "PI-WAN integrates a Temporal Convolutional Network (TCN) and a physics-informed loss function to enhance model robustness and generalization.", "result": "PI-WAN achieves better prediction accuracy, tracking precision, and robustness than baseline methods in simulations and real-world experiments.", "conclusion": "This approach successfully combines data-driven and physics-informed methods, proving effective in improving quadrotor dynamics modeling under diverse conditions."}}
{"id": "2507.00595", "pdf": "https://arxiv.org/pdf/2507.00595", "abs": "https://arxiv.org/abs/2507.00595", "authors": ["Linard Arquint", "Samarth Kishor", "Jason R. Koenig", "Joey Dodds", "Daniel Kroening", "Peter M\u00fcller"], "title": "The Secrets Must Not Flow: Scaling Security Verification to Large Codebases (extended version)", "categories": ["cs.CR", "cs.PL", "cs.SE"], "comment": null, "summary": "Existing program verifiers can prove advanced properties about security\nprotocol implementations, but are difficult to scale to large codebases because\nof the manual effort required. We develop a novel methodology called *Diodon*\nthat addresses this challenge by splitting the codebase into the protocol\nimplementation (the *Core*) and the remainder (the *Application*). This split\nallows us to apply powerful semi-automated verification techniques to the\nsecurity-critical Core, while fully-automatic static analyses scale the\nverification to the entire codebase by ensuring that the Application cannot\ninvalidate the security properties proved for the Core. The static analyses\nachieve that by proving *I/O independence*, i.e., that the I/O operations\nwithin the Application are independent of the Core's security-relevant data\n(such as keys), and that the Application meets the Core's requirements. We have\nproved Diodon sound by first showing that we can safely allow the Application\nto perform I/O independent of the security protocol, and second that manual\nverification and static analyses soundly compose. We evaluate Diodon on two\ncase studies: an implementation of the signed Diffie-Hellman key exchange and a\nlarge (100k+ LoC) production Go codebase implementing a key exchange protocol\nfor which we obtained secrecy and injective agreement guarantees by verifying a\nCore of about 1% of the code with the auto-active program verifier Gobra in\nless than three person months.", "AI": {"tldr": "The paper introduces 'Diodon,' a methodology that combines manual and automated verifications to address scalability in program verifiers for large codebases, focusing on separating critical security parts from the rest.", "motivation": "Program verifiers excel at proving security properties but fail to scale to large codebases due to manual effort intensity. The motivation is to find a scalable approach for verifying security protocols in extensive software projects.", "method": "The authors use a novel split methodology dividing the codebase into Core (security protocol) and Application (non-critical code). They apply semi-automated verification to the Core while using static analyses to prove I/O independence in the Application, ensuring security properties are intact.", "result": "Diodon proved effective in two case studies, including securing a critical Core of a 100k+ LoC Go application, using Gobra to verify only 1% of the code in under three months, achieving secrecy and injective agreement guarantees.", "conclusion": "Diodon demonstrates a feasible and efficient way to combine semi-automated verification and static analyses for verifying large-scale codebases with robust security guarantees."}}
{"id": "2507.00248", "pdf": "https://arxiv.org/pdf/2507.00248", "abs": "https://arxiv.org/abs/2507.00248", "authors": ["Nikita Nikitin", "Eugene Fomin"], "title": "Developing Lightweight DNN Models With Limited Data For Real-Time Sign Language Recognition", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "7 pages, 2 figures, 2 tables, for associated mpeg file, see\n  https://slait.app/static/Screen_Recording.mp4", "summary": "We present a novel framework for real-time sign language recognition using\nlightweight DNNs trained on limited data. Our system addresses key challenges\nin sign language recognition, including data scarcity, high computational\ncosts, and discrepancies in frame rates between training and inference\nenvironments. By encoding sign language specific parameters, such as handshape,\npalm orientation, movement, and location into vectorized inputs, and leveraging\nMediaPipe for landmark extraction, we achieve highly separable input data\nrepresentations. Our DNN architecture, optimized for sub 10MB deployment,\nenables accurate classification of 343 signs with less than 10ms latency on\nedge devices. The data annotation platform 'slait data' facilitates structured\nlabeling and vector extraction. Our model achieved 92% accuracy in isolated\nsign recognition and has been integrated into the 'slait ai' web application,\nwhere it demonstrates stable inference.", "AI": {"tldr": "This study introduces an efficient framework for real-time sign language recognition using lightweight DNNs optimized for limited data and edge devices.", "motivation": "To tackle challenges like data scarcity, computational cost, and mismatches between training and inference environments in real-time sign language recognition.", "method": "The framework uses sign-specific parameter encodings, MediaPipe for landmark extraction, and a lightweight DNN architecture optimized for sub-10MB deployment. It also employs a structured data annotation platform, 'slait data.'", "result": "Achieved 92% accuracy in recognizing 343 isolated signs with latency under 10ms on edge devices, demonstrating stable inference via integration into the 'slait ai' web app.", "conclusion": "This system provides an effective and practical solution for real-time sign language recognition, suitable for edge devices and limited data scenarios."}}
{"id": "2507.00740", "pdf": "https://arxiv.org/pdf/2507.00740", "abs": "https://arxiv.org/abs/2507.00740", "authors": ["Craig S Wright"], "title": "Safe Low Bandwidth SPV: A Formal Treatment of Simplified Payment Verification Protocols and Security Bounds", "categories": ["cs.CR", "cs.CL", "cs.DC", "68Q85, 68M10, 94A60, 91A80, 68Q17, 68W10, 68R10", "C.2.2; F.2.2; D.4.6; K.6.5"], "comment": "56 pages 5 images", "summary": "This paper presents a complete formal specification, protocol description,\nand mathematical proof structure for Simplified Payment Verification (SPV) as\noriginally defined in the Bitcoin whitepaper \\cite{nakamoto2008}. In stark\ncontrast to the misrepresentations proliferated by popular implementations, we\nshow that SPV is not only secure under bounded adversarial assumptions but\nstrictly optimal for digital cash systems requiring scalable and verifiable\ntransaction inclusion. We reconstruct the SPV protocol from first principles,\ngrounding its verification model in symbolic automata, Merkle membership\nrelations, and chain-of-proof dominance predicates. Through rigorous\nprobabilistic and game-theoretic analysis, we derive the economic bounds within\nwhich the protocol operates securely and verify its liveness and safety\nproperties under partial connectivity, hostile relay networks, and adversarial\npropagation delay. Our specification further introduces low-bandwidth\noptimisations such as adaptive polling and compressed header synchronisation\nwhile preserving correctness. This document serves both as a blueprint for\nsecure SPV implementation and a rebuttal of common misconceptions surrounding\nnon-validating clients.", "AI": {"tldr": "This paper formally specifies and proves the security and optimality of Simplified Payment Verification (SPV) protocols as originally defined for Bitcoin.", "motivation": "To correct misconceptions about SPV, establish its security and scalability under bounded adversarial assumptions, and provide a rigorous foundation for its implementation in digital cash systems.", "method": "The authors reconstructed SPV from first principles using symbolic automata, Merkle proof structures, and game-theoretic analysis. Probabilistic security bounds were derived, and low-bandwidth optimizations introduced.", "result": "SPV is proven secure and optimal under bounded adversarial assumptions. The paper provides methods to optimize bandwidth while maintaining correctness.", "conclusion": "This work refutes misrepresentations of SPV, describing it as secure and ideal for scalable transaction verification in non-validating clients."}}
{"id": "2507.00951", "pdf": "https://arxiv.org/pdf/2507.00951", "abs": "https://arxiv.org/abs/2507.00951", "authors": ["Rizwan Qureshi", "Ranjan Sapkota", "Abbas Shah", "Amgad Muneer", "Anas Zafar", "Ashmal Vayani", "Maged Shoman", "Abdelrahman B. M. Eldaly", "Kai Zhang", "Ferhat Sadak", "Shaina Raza", "Xinqi Fan", "Ravid Shwartz-Ziv", "Hong Yan", "Vinjia Jain", "Aman Chadha", "Manoj Karkee", "Jia Wu", "Philip Torr", "Seyedali Mirjalili"], "title": "Thinking Beyond Tokens: From Brain-Inspired Intelligence to Cognitive Foundations for Artificial General Intelligence and its Societal Impact", "categories": ["cs.AI"], "comment": null, "summary": "Can machines truly think, reason and act in domains like humans? This\nenduring question continues to shape the pursuit of Artificial General\nIntelligence (AGI). Despite the growing capabilities of models such as GPT-4.5,\nDeepSeek, Claude 3.5 Sonnet, Phi-4, and Grok 3, which exhibit multimodal\nfluency and partial reasoning, these systems remain fundamentally limited by\ntheir reliance on token-level prediction and lack of grounded agency. This\npaper offers a cross-disciplinary synthesis of AGI development, spanning\nartificial intelligence, cognitive neuroscience, psychology, generative models,\nand agent-based systems. We analyze the architectural and cognitive foundations\nof general intelligence, highlighting the role of modular reasoning, persistent\nmemory, and multi-agent coordination. In particular, we emphasize the rise of\nAgentic RAG frameworks that combine retrieval, planning, and dynamic tool use\nto enable more adaptive behavior. We discuss generalization strategies,\nincluding information compression, test-time adaptation, and training-free\nmethods, as critical pathways toward flexible, domain-agnostic intelligence.\nVision-Language Models (VLMs) are reexamined not just as perception modules but\nas evolving interfaces for embodied understanding and collaborative task\ncompletion. We also argue that true intelligence arises not from scale alone\nbut from the integration of memory and reasoning: an orchestration of modular,\ninteractive, and self-improving components where compression enables adaptive\nbehavior. Drawing on advances in neurosymbolic systems, reinforcement learning,\nand cognitive scaffolding, we explore how recent architectures begin to bridge\nthe gap between statistical learning and goal-directed cognition. Finally, we\nidentify key scientific, technical, and ethical challenges on the path to AGI.", "AI": {"tldr": "This paper explores the limitations of current AI models and the pursuit of Artificial General Intelligence (AGI) by emphasizing modular reasoning, memory integration, and adaptive behavior.", "motivation": "The paper addresses the fundamental challenge of creating AI systems that exhibit true general intelligence, similar to human cognition, reasoning, and adaptive capabilities.", "method": "The authors provide a synthetic review of AGI development and emphasize approaches like Agentic RAG frameworks, modular reasoning, and memory integration, while reevaluating Vision-Language Models (VLMs).", "result": "The analysis demonstrates how recent AI architectures are beginning to converge on adaptive, domain-agnostic intelligence through integration of memory, reasoning, and neurosymbolic systems.", "conclusion": "The paper presents modular, interactive, and self-improving systems as the foundation for AGI, highlighting the need for compression, memory, and reasoning to achieve adaptive intelligence."}}
{"id": "2507.00509", "pdf": "https://arxiv.org/pdf/2507.00509", "abs": "https://arxiv.org/abs/2507.00509", "authors": ["To Eun Kim", "Jo\u00e3o Coelho", "Gbemileke Onilude", "Jai Singh"], "title": "TeamCMU at Touch\u00e9: Adversarial Co-Evolution for Advertisement Integration and Detection in Conversational Search", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "As conversational search engines increasingly adopt generation-based\nparadigms powered by Large Language Models (LLMs) and Retrieval-Augmented\nGeneration (RAG), the integration of advertisements into generated responses\npresents both commercial opportunities and challenges for user experience.\nUnlike traditional search, where advertisements are clearly delineated,\ngenerative systems blur the boundary between informational content and\npromotional material, raising concerns around transparency and trust. In this\nwork, we propose a modular pipeline for advertisement management in RAG-based\nconversational systems, consisting of an ad-rewriter for seamless ad\nintegration and a robust ad-classifier for detection. We leverage synthetic\ndata to train high-performing classifiers, which are then used to guide two\ncomplementary ad-integration strategies: supervised fine-tuning of the\nad-rewriter and a best-of-N sampling approach that selects the least detectable\nad-integrated response among multiple candidates. Our evaluation focuses on two\ncore questions: the effectiveness of ad classifiers in detecting diverse ad\nintegration strategies, and the training methods that best support coherent,\nminimally intrusive ad insertion. Experimental results show that our\nad-classifier, trained on synthetic advertisement data inspired by marketing\nstrategies and enhanced through curriculum learning, achieves robust detection\nperformance. Additionally, we demonstrate that classifier-guided optimization,\nthrough both fine-tuning and best-of-N sampling, significantly improves ad\nstealth, enabling more seamless integration. These findings contribute an\nadversarial co-evolution framework for developing more sophisticated ad-aware\ngenerative search systems and robust ad classifiers.", "AI": {"tldr": "The paper discusses integrating advertisements into generative conversational systems using a pipeline that incorporates ad-rewriters for smooth integration and ad-classifiers for detection, achieving seamless and minimally intrusive ad insertions.", "motivation": "To tackle the transparency and trust issues arising from blending advertisements with informational content in generative conversational systems based on Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG).", "method": "The authors propose a modular pipeline with two key components: an ad-rewriter (for integrating ads) and an ad-classifier (for detecting ads). They generate synthetic data to train the classifiers, use supervised fine-tuning for the ad-rewriter, and implement best-of-N sampling to choose the least detectable ad-inserted response.", "result": "The study shows that their ad-classifier, trained using synthetic data and curriculum learning, performs robustly in detecting various ad strategies. Furthermore, the classifier-guided optimization enhances ad stealth and coherence during integration, improving user experience.", "conclusion": "The paper establishes an adversarial co-evolution framework for advancing generative search systems and creating more effective ad-aware classifiers, ensuring seamless and transparent ad integration while maintaining user trust."}}
{"id": "2507.00029", "pdf": "https://arxiv.org/pdf/2507.00029", "abs": "https://arxiv.org/abs/2507.00029", "authors": ["Wenbing Li", "Zikai Song", "Hang Zhou", "Yunyao Zhang", "Junqing Yu", "Wei Yang"], "title": "LoRA-Mixer: Coordinate Modular LoRA Experts Through Serial Attention Routing", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Recent efforts to combine low-rank adaptation (LoRA) with mixture-of-experts\n(MoE) for adapting large language models (LLMs) to multiple tasks still exhibit\nprevailing limitations: they either swap entire attention/feed-forward layers\nfor switch experts or bolt on parallel expert branches, diluting parameter\nefficiency and task fidelity. We propose the LoRA-Mixer, a modular and\nlightweight MoE framework that integrates LoRA experts. Our core innovation\nlies in replacing the projection matrices of the attention module's\ninput/output linear layers with dynamically routed, task-specific LoRA experts.\nThis design ensures seamless compatibility with diverse foundation models,\nincluding transformers and state space models (SSMs), by leveraging their\ninherent linear projection structures. The framework supports two operational\nparadigms: (1) joint optimization of LoRA experts and routing mechanisms via a\nnovel hard-soft routing strategy, or (2) direct deployment of pre-trained,\nfrozen LoRA modules sourced from external repositories. To enable robust router\ntraining with limited data while ensuring stable routing decisions and\nmaximizing expert reuse, we introduce an adaptive Specialization Balance Loss\n(SBL) that jointly optimizes expert balance and task-specific alignment.\nExtensive experiments on seven benchmark datasets, including MedQA, CoLA,\nSST-2, GSM8K, ARC-E, ARC-C, and HumanEval, demonstrate the effectiveness of\nLoRA-Mixer. On datasets such as GSM8K, HumanEval, and MedQA, LoRA-Mixer\nachieves significant improvements of 7.61%, 4.88%, and 3.08% over the base\nmodels, respectively. Compared with state-of-the-art methods, LoRA-Mixer\nachieves additional improvements of 1.09%, 1.45%, and 1.68%, respectively,\nusing only 48% of the parameters, demonstrating its efficiency and strong\nperformance.", "AI": {"tldr": "This paper introduces LoRA-Mixer, a lightweight modular framework combining low-rank adaptation (LoRA) with mixture-of-experts (MoE) to enhance task-specific performance in adapting large language models (LLMs).", "motivation": "Current methods of combining LoRA with MoE for LLM adaptation face inefficiencies, such as using entire layers for switch experts or parallel expert branches, leading to reduced parameter efficiency and task fidelity.", "method": "LoRA-Mixer replaces attention module projection matrices with task-specific LoRA experts, supports joint optimization via a hard-soft routing strategy or direct deployment of pretrained LoRA modules, and introduces Specialization Balance Loss (SBL) to improve task alignment and reuse experts under limited data.", "result": "LoRA-Mixer demonstrates significant performance improvements over base models (up to 7.61%) and state-of-the-art methods (up to 1.68%) across seven benchmark datasets while using only 48% of the parameters.", "conclusion": "LoRA-Mixer offers an efficient and effective approach to adapt LLMs, showing strong benchmark performance and parameter efficiency, making it broadly compatible across different model architectures."}}
{"id": "2507.00833", "pdf": "https://arxiv.org/pdf/2507.00833", "abs": "https://arxiv.org/abs/2507.00833", "authors": ["Zhi Jing", "Siyuan Yang", "Jicong Ao", "Ting Xiao", "Yugang Jiang", "Chenjia Bai"], "title": "HumanoidGen: Data Generation for Bimanual Dexterous Manipulation via LLM Reasoning", "categories": ["cs.RO", "cs.AI"], "comment": "Project Page: https://openhumanoidgen.github.io", "summary": "For robotic manipulation, existing robotics datasets and simulation\nbenchmarks predominantly cater to robot-arm platforms. However, for humanoid\nrobots equipped with dual arms and dexterous hands, simulation tasks and\nhigh-quality demonstrations are notably lacking. Bimanual dexterous\nmanipulation is inherently more complex, as it requires coordinated arm\nmovements and hand operations, making autonomous data collection challenging.\nThis paper presents HumanoidGen, an automated task creation and demonstration\ncollection framework that leverages atomic dexterous operations and LLM\nreasoning to generate relational constraints. Specifically, we provide spatial\nannotations for both assets and dexterous hands based on the atomic operations,\nand perform an LLM planner to generate a chain of actionable spatial\nconstraints for arm movements based on object affordances and scenes. To\nfurther improve planning ability, we employ a variant of Monte Carlo tree\nsearch to enhance LLM reasoning for long-horizon tasks and insufficient\nannotation. In experiments, we create a novel benchmark with augmented\nscenarios to evaluate the quality of the collected data. The results show that\nthe performance of the 2D and 3D diffusion policies can scale with the\ngenerated dataset. Project page is https://openhumanoidgen.github.io.", "AI": {"tldr": "The paper introduces HumanoidGen, a framework for generating automated tasks and demonstrations tailored for humanoid robots with dual arms and dexterous hands, overcoming challenges in bimanual manipulation.", "motivation": "The scarcity of datasets and simulation benchmarks specific to dual-arm and dexterous hands limits progress in humanoid robot manipulation research. Autonomous data collection for these complex tasks remains a significant hurdle.", "method": "The framework combines atomic dexterous operations, spatial annotations for objects and humanoid hands, reasoning capabilities of large language models (LLMs), and Monte Carlo tree search techniques to generate spatial constraints and improve planning for long-horizon tasks.", "result": "A novel benchmark with augmented scenarios was introduced to evaluate collected data, showing improvement in performance scalability of 2D and 3D diffusion policies when applied to the dataset.", "conclusion": "HumanoidGen successfully addresses gaps in humanoid robot manipulation benchmarks by demonstrating scalable and high-quality data collection capabilities, advancing the development of dexterous bimanual robotic tasks."}}
{"id": "2507.00253", "pdf": "https://arxiv.org/pdf/2507.00253", "abs": "https://arxiv.org/abs/2507.00253", "authors": ["Zhuangzhuang Dai", "Vincent Gbouna Zakka", "Luis J. Manso", "Chen Li"], "title": "GazeTarget360: Towards Gaze Target Estimation in 360-Degree for Robot Perception", "categories": ["cs.CV", "cs.HC"], "comment": null, "summary": "Enabling robots to understand human gaze target is a crucial step to allow\ncapabilities in downstream tasks, for example, attention estimation and\nmovement anticipation in real-world human-robot interactions. Prior works have\naddressed the in-frame target localization problem with data-driven approaches\nby carefully removing out-of-frame samples. Vision-based gaze estimation\nmethods, such as OpenFace, do not effectively absorb background information in\nimages and cannot predict gaze target in situations where subjects look away\nfrom the camera. In this work, we propose a system to address the problem of\n360-degree gaze target estimation from an image in generalized visual scenes.\nThe system, named GazeTarget360, integrates conditional inference engines of an\neye-contact detector, a pre-trained vision encoder, and a multi-scale-fusion\ndecoder. Cross validation results show that GazeTarget360 can produce accurate\nand reliable gaze target predictions in unseen scenarios. This makes a\nfirst-of-its-kind system to predict gaze targets from realistic camera footage\nwhich is highly efficient and deployable. Our source code is made publicly\navailable at: https://github.com/zdai257/DisengageNet.", "AI": {"tldr": "This paper introduces GazeTarget360, a first-of-its-kind system for 360-degree gaze target estimation in generalized visual scenes.", "motivation": "The motivation is to enable robots to understand human gaze targets for downstream tasks like attention estimation and movement anticipation, which are critical in human-robot interaction.", "method": "The authors propose GazeTarget360, which combines conditional inference engines, including an eye-contact detector, a pre-trained vision encoder, and a multi-scale-fusion decoder, to estimate 360-degree gaze targets from images.", "result": "Cross-validation results demonstrate that GazeTarget360 is accurate and reliable in predicting gaze targets in unseen scenarios.", "conclusion": "GazeTarget360 represents an innovative, efficient, and deployable solution for gaze target estimation from realistic camera footage, advancing the field of human-robot interaction."}}
{"id": "2507.00979", "pdf": "https://arxiv.org/pdf/2507.00979", "abs": "https://arxiv.org/abs/2507.00979", "authors": ["Dongyoon Hahm", "Woogyeol Jin", "June Suk Choi", "Sungsoo Ahn", "Kimin Lee"], "title": "Enhancing LLM Agent Safety via Causal Influence Prompting", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "Accepted at ACL 2025 Findings, Source code:\n  https://github.com/HahmDY/causal_influence_prompting.git", "summary": "As autonomous agents powered by large language models (LLMs) continue to\ndemonstrate potential across various assistive tasks, ensuring their safe and\nreliable behavior is crucial for preventing unintended consequences. In this\nwork, we introduce CIP, a novel technique that leverages causal influence\ndiagrams (CIDs) to identify and mitigate risks arising from agent\ndecision-making. CIDs provide a structured representation of cause-and-effect\nrelationships, enabling agents to anticipate harmful outcomes and make safer\ndecisions. Our approach consists of three key steps: (1) initializing a CID\nbased on task specifications to outline the decision-making process, (2)\nguiding agent interactions with the environment using the CID, and (3)\niteratively refining the CID based on observed behaviors and outcomes.\nExperimental results demonstrate that our method effectively enhances safety in\nboth code execution and mobile device control tasks.", "AI": {"tldr": "The paper introduces CIP, a technique using causal influence diagrams to improve decision-making safety in autonomous agents using large language models.", "motivation": "To ensure safe and reliable behavior of autonomous agents powered by large language models, and prevent unintended consequences during assistive tasks.", "method": "The approach involves initializing causal influence diagrams based on tasks, guiding agent interactions using CIDs, and iteratively refining them based on outcomes and behaviors.", "result": "Experimental results show enhanced safety in areas like code execution and mobile device control tasks.", "conclusion": "Using causal influence diagrams can effectively mitigate decision-making risks and improve the safe operation of autonomous agents."}}
{"id": "2507.00534", "pdf": "https://arxiv.org/pdf/2507.00534", "abs": "https://arxiv.org/abs/2507.00534", "authors": ["Tahir Javed", "Kaushal Bhogale", "Mitesh M. Khapra"], "title": "NIRANTAR: Continual Learning with New Languages and Domains on Real-world Speech Data", "categories": ["cs.CL"], "comment": "Accepted in Interspecch 2025", "summary": "We introduce Nirantar, a comprehensive framework for evaluating continual\nlearning (CL) in multilingual and multi-domain ASR. Designed to reflect\nreal-world CL challenges, Nirantar leverages data collected incrementally\nacross 22 languages and 208 districts in India through natural episodes. This\nenables evaluation across Language-Incremental (LIL), Domain-Incremental (DIL),\nand the novel Language-Incremental Domain-Incremental Learning (LIDIL)\nscenarios. Unlike prior work that relies on simulated episodes, Nirantar\npresents dynamic, non-uniform language and domain shifts, making it an ideal\ntestbed for CL research. With 3250 hours of human-transcribed speech, including\n1720 hours newly introduced in this work, our framework enables systematic\nbenchmarking of CL methods. We evaluate existing approaches and demonstrate\nthat no single method performs consistently well, underscoring the need for\nmore robust CL strategies.", "AI": {"tldr": "Nirantar is a framework for multilingual and multi-domain continual learning in ASR, using real-world incremental data.", "motivation": "The paper aims to address the challenges in evaluating continual learning for ASR under realistic, real-world conditions.", "method": "The framework uses 3250 hours of speech data collected across 22 languages and 208 districts in India, creating natural episodes highlighting varying language and domain shifts.", "result": "Existing CL methods were evaluated and found to be inconsistent, revealing the need for improvements in CL strategies.", "conclusion": "Nirantar highlights the limitations of current CL methods and provides a robust benchmarking environment for future research."}}
{"id": "2507.00030", "pdf": "https://arxiv.org/pdf/2507.00030", "abs": "https://arxiv.org/abs/2507.00030", "authors": ["Abhishek Verma", "Nallarasan V", "Balaraman Ravindran"], "title": "Adaptive Action Duration with Contextual Bandits for Deep Reinforcement Learning in Dynamic Environments", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Deep Reinforcement Learning (DRL) has achieved remarkable success in complex\nsequential decision-making tasks, such as playing Atari 2600 games and\nmastering board games. A critical yet underexplored aspect of DRL is the\ntemporal scale of action execution. We propose a novel paradigm that integrates\ncontextual bandits with DRL to adaptively select action durations, enhancing\npolicy flexibility and computational efficiency. Our approach augments a Deep\nQ-Network (DQN) with a contextual bandit module that learns to choose optimal\naction repetition rates based on state contexts. Experiments on Atari 2600\ngames demonstrate significant performance improvements over static duration\nbaselines, highlighting the efficacy of adaptive temporal abstractions in DRL.\nThis paradigm offers a scalable solution for real-time applications like gaming\nand robotics, where dynamic action durations are critical.", "AI": {"tldr": "This paper integrates contextual bandits with Deep Reinforcement Learning (DRL) to adaptively select action durations, achieving higher efficiency and performance in sequential decision-making tasks.", "motivation": "DRL has been successful in decision-making tasks but lacks exploration of adaptive action durations for better flexibility and efficiency.", "method": "The proposed method combines Deep Q-Network (DQN) with a contextual bandit to learn and select optimal action repetition rates based on state contexts.", "result": "Experiments on Atari 2600 games showed significant performance improvements compared to static duration baselines.", "conclusion": "Adaptive temporal abstractions in DRL provide enhanced scalability and efficiency, particularly suitable for dynamic applications like gaming and robotics."}}
{"id": "2507.00882", "pdf": "https://arxiv.org/pdf/2507.00882", "abs": "https://arxiv.org/abs/2507.00882", "authors": ["Miguel \u00c1ngel de Miguel", "Jorge Beltr\u00e1n", "Juan S. Cely", "Francisco Mart\u00edn", "Juan Carlos Manzanares", "Alberto Garc\u00eda"], "title": "I Move Therefore I Learn: Experience-Based Traversability in Outdoor Robotics", "categories": ["cs.RO"], "comment": null, "summary": "Accurate traversability estimation is essential for safe and effective\nnavigation of outdoor robots operating in complex environments. This paper\nintroduces a novel experience-based method that allows robots to autonomously\nlearn which terrains are traversable based on prior navigation experience,\nwithout relying on extensive pre-labeled datasets. The approach integrates\nelevation and texture data into multi-layered grid maps, which are processed\nusing a variational autoencoder (VAE) trained on a generic texture dataset.\nDuring an initial teleoperated phase, the robot collects sensory data while\nmoving around the environment. These experiences are encoded into compact\nfeature vectors and clustered using the BIRCH algorithm to represent\ntraversable terrain areas efficiently. In deployment, the robot compares new\nterrain patches to its learned feature clusters to assess traversability in\nreal time. The proposed method does not require training with data from the\ntargeted scenarios, generalizes across diverse surfaces and platforms, and\ndynamically adapts as new terrains are encountered. Extensive evaluations on\nboth synthetic benchmarks and real-world scenarios with wheeled and legged\nrobots demonstrate its effectiveness, robustness, and superior adaptability\ncompared to state-of-the-art approaches.", "AI": {"tldr": "The paper presents a method for robots to autonomously learn terrain traversability using prior navigation experience, without pre-labeled datasets.", "motivation": "To enhance safe and efficient outdoor robot navigation by enabling adaptive terrain assessment, without the reliance on extensively labeled datasets.", "method": "A variational autoencoder processes elevation and texture data. Initial teleoperation collects navigation experiences, which are encoded, clustered using BIRCH, and used to assess real-time terrain traversability.", "result": "The method generalizes across robots and surfaces, adapts dynamically, and outperforms state-of-the-art approaches in synthetic benchmarks and real-world tests.", "conclusion": "The novel traversability estimation approach provides efficient, robust, and adaptable navigation for robots, demonstrating extensive utility in diverse environments."}}
{"id": "2507.00261", "pdf": "https://arxiv.org/pdf/2507.00261", "abs": "https://arxiv.org/abs/2507.00261", "authors": ["Zhiyin Lin", "Purvi Goel", "Joy Yun", "C. Karen Liu", "Joao Pedro Araujo"], "title": "VirtualFencer: Generating Fencing Bouts based on Strategies Extracted from In-the-Wild Videos", "categories": ["cs.CV", "cs.GR"], "comment": null, "summary": "Fencing is a sport where athletes engage in diverse yet strategically logical\nmotions. While most motions fall into a few high-level actions (e.g. step,\nlunge, parry), the execution can vary widely-fast vs. slow, large vs. small,\noffensive vs. defensive. Moreover, a fencer's actions are informed by a\nstrategy that often comes in response to the opponent's behavior. This\ncombination of motion diversity with underlying two-player strategy motivates\nthe application of data-driven modeling to fencing. We present VirtualFencer, a\nsystem capable of extracting 3D fencing motion and strategy from in-the-wild\nvideo without supervision, and then using that extracted knowledge to generate\nrealistic fencing behavior. We demonstrate the versatile capabilities of our\nsystem by having it (i) fence against itself (self-play), (ii) fence against a\nreal fencer's motion from online video, and (iii) fence interactively against a\nprofessional fencer.", "AI": {"tldr": "Presentation of VirtualFencer, a system that analyzes fencing motions and strategies from videos to generate realistic fencing behaviors independently.", "motivation": "To address the complexity of fencing that combines diverse motion patterns and underlying strategy influenced by the opponent's actions, leveraging data-driven techniques for analysis and realistic fencing behavior generation.", "method": "The system extracts 3D motion and strategic patterns from fencing videos without supervision and uses these insights to recreate realistic fencing behavior, including simulations and interactions.", "result": "VirtualFencer can self-play, simulate fencing motions from online videos, and interactively compete against professional fencers, showcasing its versatility.", "conclusion": "VirtualFencer demonstrates its capability in analyzing and generating diverse and strategic fencing behaviors autonomously, bridging video analysis with practical interaction potential in fencing."}}
{"id": "2507.00540", "pdf": "https://arxiv.org/pdf/2507.00540", "abs": "https://arxiv.org/abs/2507.00540", "authors": ["Shixiao Wang", "Yifan Zhuang", "Runsheng Zhang", "Zhijun Song"], "title": "Capsule Network-Based Semantic Intent Modeling for Human-Computer Interaction", "categories": ["cs.CL"], "comment": null, "summary": "This paper proposes a user semantic intent modeling algorithm based on\nCapsule Networks to address the problem of insufficient accuracy in intent\nrecognition for human-computer interaction. The method represents semantic\nfeatures in input text through a vectorized capsule structure. It uses a\ndynamic routing mechanism to transfer information across multiple capsule\nlayers. This helps capture hierarchical relationships and part-whole structures\nbetween semantic entities more effectively. The model uses a convolutional\nfeature extraction module as the low-level encoder. After generating initial\nsemantic capsules, it forms high-level abstract intent representations through\nan iterative routing process. To further enhance performance, a margin-based\nmechanism is introduced into the loss function. This improves the model's\nability to distinguish between intent classes. Experiments are conducted using\na public natural language understanding dataset. Multiple mainstream models are\nused for comparison. Results show that the proposed model outperforms\ntraditional methods and other deep learning structures in terms of accuracy,\nF1-score, and intent detection rate. The study also analyzes the effect of the\nnumber of dynamic routing iterations on model performance. A convergence curve\nof the loss function during training is provided. These results verify the\nstability and effectiveness of the proposed method in semantic modeling.\nOverall, this study presents a new structured modeling approach to improve\nintent recognition under complex semantic conditions.", "AI": {"tldr": "The paper introduces a Capsule Networks-based algorithm for enhancing semantic intent recognition in human-computer interaction, showing superior accuracy and stability.", "motivation": "To address limitations in semantic intent recognition accuracy for human-computer interactions.", "method": "Employ a Capsule Network architecture with dynamic routing mechanisms, convolutional feature extraction, margin-based loss functions, and experiments using public datasets.", "result": "The proposed model outperformed mainstream methods in accuracy, F1-score, and intent detection rate, proving effectiveness in handling complex semantics.", "conclusion": "The study demonstrates a stable and effective structured modeling approach for improving intent recognition in semantic contexts."}}
{"id": "2507.00031", "pdf": "https://arxiv.org/pdf/2507.00031", "abs": "https://arxiv.org/abs/2507.00031", "authors": ["Chuan Li", "Jiang You", "Hassine Moungla", "Vincent Gauthier", "Miguel Nunez-del-Prado", "Hugo Alatrista-Salas"], "title": "Enhancing Spatio-Temporal Forecasting with Spatial Neighbourhood Fusion:A Case Study on COVID-19 Mobility in Peru", "categories": ["cs.LG"], "comment": null, "summary": "Accurate modeling of human mobility is critical for understanding epidemic\nspread and deploying timely interventions. In this work, we leverage a\nlarge-scale spatio-temporal dataset collected from Peru's national Digital\nContact Tracing (DCT) application during the COVID-19 pandemic to forecast\nmobility flows across urban regions. A key challenge lies in the spatial\nsparsity of hourly mobility counts across hexagonal grid cells, which limits\nthe predictive power of conventional time series models. To address this, we\npropose a lightweight and model-agnostic Spatial Neighbourhood Fusion (SPN)\ntechnique that augments each cell's features with aggregated signals from its\nimmediate H3 neighbors. We evaluate this strategy on three forecasting\nbackbones: NLinear, PatchTST, and K-U-Net, under various historical input\nlengths. Experimental results show that SPN consistently improves forecasting\nperformance, achieving up to 9.85 percent reduction in test MSE. Our findings\ndemonstrate that spatial smoothing of sparse mobility signals provides a simple\nyet effective path toward robust spatio-temporal forecasting during public\nhealth crises.", "AI": {"tldr": "This paper addresses the challenge of spatial sparsity in mobility flow forecasting by proposing a model-agnostic Spatial Neighbourhood Fusion (SPN) technique, improving test MSE by up to 9.85%.", "motivation": "The study is motivated by the need for accurate human mobility modeling to better understand and mitigate epidemic spread, especially during public health crises such as COVID-19.", "method": "The paper introduces the Spatial Neighbourhood Fusion (SPN) technique, which augments mobility features of a grid cell with aggregated signals from its immediate neighbors to address spatial sparsity.", "result": "The experimental tests demonstrate that the SPN method improves forecasting accuracy across different time series models, with a reduction of up to 9.85% in test MSE.", "conclusion": "Spatial smoothing via SPN provides a simple yet effective solution for robust spatio-temporal forecasting, particularly during public health emergencies like the COVID-19 pandemic."}}
{"id": "2507.00917", "pdf": "https://arxiv.org/pdf/2507.00917", "abs": "https://arxiv.org/abs/2507.00917", "authors": ["Xiaoxiao Long", "Qingrui Zhao", "Kaiwen Zhang", "Zihao Zhang", "Dingrui Wang", "Yumeng Liu", "Zhengjie Shu", "Yi Lu", "Shouzheng Wang", "Xinzhe Wei", "Wei Li", "Wei Yin", "Yao Yao", "Jia Pan", "Qiu Shen", "Ruigang Yang", "Xun Cao", "Qionghai Dai"], "title": "A Survey: Learning Embodied Intelligence from Physical Simulators and World Models", "categories": ["cs.RO"], "comment": "https://github.com/NJU3DV-LoongGroup/Embodied-World-Models-Survey", "summary": "The pursuit of artificial general intelligence (AGI) has placed embodied\nintelligence at the forefront of robotics research. Embodied intelligence\nfocuses on agents capable of perceiving, reasoning, and acting within the\nphysical world. Achieving robust embodied intelligence requires not only\nadvanced perception and control, but also the ability to ground abstract\ncognition in real-world interactions. Two foundational technologies, physical\nsimulators and world models, have emerged as critical enablers in this quest.\nPhysical simulators provide controlled, high-fidelity environments for training\nand evaluating robotic agents, allowing safe and efficient development of\ncomplex behaviors. In contrast, world models empower robots with internal\nrepresentations of their surroundings, enabling predictive planning and\nadaptive decision-making beyond direct sensory input. This survey\nsystematically reviews recent advances in learning embodied AI through the\nintegration of physical simulators and world models. We analyze their\ncomplementary roles in enhancing autonomy, adaptability, and generalization in\nintelligent robots, and discuss the interplay between external simulation and\ninternal modeling in bridging the gap between simulated training and real-world\ndeployment. By synthesizing current progress and identifying open challenges,\nthis survey aims to provide a comprehensive perspective on the path toward more\ncapable and generalizable embodied AI systems. We also maintain an active\nrepository that contains up-to-date literature and open-source projects at\nhttps://github.com/NJU3DV-LoongGroup/Embodied-World-Models-Survey.", "AI": {"tldr": "This paper surveys the integration of physical simulators and world models to advance embodied AI, emphasizing their complementary roles in bridging simulation and real-world application.", "motivation": "The motivation is to enhance robotic autonomy, adaptability, and generalization by understanding the synergy between physical simulators and world models in embodied intelligence.", "method": "The authors systematically review recent advances in embodied AI, focusing on the integration of two technologies (physical simulators and world models) and analyzing their interaction.", "result": "The survey provides insights into the complementary roles of physical simulators and world models in achieving robust embodied AI and explores their impact on simulation-to-reality transfer.", "conclusion": "This paper synthesizes current progress to clarify the path toward generalizable embodied AI systems and identifies open challenges in the area."}}
{"id": "2507.00547", "pdf": "https://arxiv.org/pdf/2507.00547", "abs": "https://arxiv.org/abs/2507.00547", "authors": ["Malmi Amadoru"], "title": "Methodological Rigour in Algorithm Application: An Illustration of Topic Modelling Algorithm", "categories": ["cs.CL"], "comment": null, "summary": "The rise of advanced computational algorithms has opened new avenues for\ncomputationally intensive research approaches to theory development. However,\nthe opacity of these algorithms and lack of transparency and rigour in their\napplication pose methodological challenges, potentially undermining trust in\nresearch. The discourse on methodological rigour in this new genre of research\nis still emerging. Against this backdrop, I attempt to offer guidance on\nmethodological rigour, particularly in the context of topic modelling\nalgorithms. By illustrating the application of the structural topic modelling\nalgorithm and presenting a set of guidelines, I discuss how to ensure rigour in\ntopic modelling studies. Although the guidelines are for the application of\ntopic modelling algorithms, they can be applied to other algorithms with\ncontext-specific adjustments. The guidelines are helpful, especially for novice\nresearchers applying topic modelling, and editors and reviewers handling topic\nmodelling manuscripts. I contribute to the literature on topic modelling and\njoin the emerging dialogue on methodological rigour in computationally\nintensive theory construction research.", "AI": {"tldr": "The paper explores ensuring methodological rigor in the use of topic modeling algorithms, offering guidelines primarily for novice researchers, editors, and reviewers.", "motivation": "The rise of advanced computational techniques has revolutionized research but their opacity and lack of transparency pose challenges in ensuring trustworthiness and methodological integrity.", "method": "The paper employs structural topic modeling as a case study, providing illustrative examples and guidelines to ensure rigorous application.", "result": "Guidelines for maintaining methodological rigor in computational research are presented, with emphasis on structural topic modeling and potential adaptations for other algorithms.", "conclusion": "Ensuring rigorous methodologies in computationally intensive research is critical for fostering trust and quality in emerging fields like topic modeling."}}
{"id": "2507.00034", "pdf": "https://arxiv.org/pdf/2507.00034", "abs": "https://arxiv.org/abs/2507.00034", "authors": ["Reece Bourisaw", "Reid McCants", "Jean-Marie Le Corre", "Anna Iskhakova", "Arsen S. Iskhakov"], "title": "Data Collection with Non-Uniform Axial Power for Phase II of the OECD/NEA AI/ML Critical Heat Flux Benchmark", "categories": ["cs.LG", "cs.CE"], "comment": null, "summary": "Critical heat flux (CHF) marks the onset of boiling crisis in light-water\nreactors, defining safe thermal-hydraulic operating limits. To support Phase II\nof the OECD/NEA AI/ML CHF benchmark, which introduces spatially varying power\nprofiles, this work compiles and digitizes a broad CHF dataset covering both\nuniform and non-uniform axial heating conditions. Heating profiles were\nextracted from technical reports, interpolated onto a consistent axial mesh,\nvalidated via energy-balance checks, and encoded in machine-readable formats\nfor benchmark compatibility.\n  Classical CHF correlations exhibit substantial errors under uniform heating\nand degrade markedly when applied to non-uniform profiles, while modern tabular\nmethods offer improved but still imperfect predictions. A neural network\ntrained solely on uniform data performs well in that regime but fails to\ngeneralize to spatially varying scenarios, underscoring the need for models\nthat explicitly incorporate axial power distributions. By providing these\ncurated datasets and baseline modeling results, this study lays the groundwork\nfor advanced transfer-learning strategies, rigorous uncertainty quantification,\nand design-optimization efforts in the next phase of the CHF benchmark.", "AI": {"tldr": "This paper addresses the challenges of predicting critical heat flux (CHF) in light-water reactors under varying axial heating conditions using curated datasets and model evaluations.", "motivation": "To advance the prediction of CHF in light-water reactors, especially under spatially varying power profiles, and support the OECD/NEA AI/ML CHF benchmark Phase II.", "method": "The study compiled and digitized CHF datasets, validated them, and evaluated the performance of classical, tabular, and neural network methods for CHF prediction.", "result": "Classical correlations and neural networks trained on uniform data were inadequate for non-uniform conditions, while tabular methods performed better but still had limitations.", "conclusion": "Curated datasets and modeling baselines highlight the need for models that account for spatial power distributions, paving the way for advanced transfer-learning and optimization techniques."}}
{"id": "2507.00287", "pdf": "https://arxiv.org/pdf/2507.00287", "abs": "https://arxiv.org/abs/2507.00287", "authors": ["Mohamad Dabboussi", "Malo Huard", "Yann Gousseau", "Pietro Gori"], "title": "Self-Supervised Multiview Xray Matching", "categories": ["cs.CV", "cs.AI"], "comment": "MICCAI 2025", "summary": "Accurate interpretation of multi-view radiographs is crucial for diagnosing\nfractures, muscular injuries, and other anomalies. While significant advances\nhave been made in AI-based analysis of single images, current methods often\nstruggle to establish robust correspondences between different X-ray views, an\nessential capability for precise clinical evaluations. In this work, we present\na novel self-supervised pipeline that eliminates the need for manual annotation\nby automatically generating a many-to-many correspondence matrix between\nsynthetic X-ray views. This is achieved using digitally reconstructed\nradiographs (DRR), which are automatically derived from unannotated CT volumes.\nOur approach incorporates a transformer-based training phase to accurately\npredict correspondences across two or more X-ray views. Furthermore, we\ndemonstrate that learning correspondences among synthetic X-ray views can be\nleveraged as a pretraining strategy to enhance automatic multi-view fracture\ndetection on real data. Extensive evaluations on both synthetic and real X-ray\ndatasets show that incorporating correspondences improves performance in\nmulti-view fracture classification.", "AI": {"tldr": "This study introduces a self-supervised pipeline leveraging synthetic X-rays to generate correspondences, aiming to improve multi-view fracture detection.", "motivation": "Accurately interpreting multi-view radiographs is critical for clinical diagnoses, but current AI methods struggle with establishing correspondences between views.", "method": "The authors use a self-supervised pipeline involving transformer-based training and synthetic digitally reconstructed radiographs (DRR) to create many-to-many correspondence matrices.", "result": "The method demonstrated improved performance in multi-view fracture classification by leveraging correspondences during pretraining.", "conclusion": "Incorporating synthetic correspondences enhances AI's capability to analyze multi-view radiographs, improving fracture detection without manual annotations."}}
