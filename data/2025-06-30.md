<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 17]
- [cs.AR](#cs.AR) [Total: 2]
- [cs.CL](#cs.CL) [Total: 103]
- [cs.CV](#cs.CV) [Total: 85]
- [cs.DC](#cs.DC) [Total: 6]
- [cs.LG](#cs.LG) [Total: 53]
- [cs.NE](#cs.NE) [Total: 2]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.RO](#cs.RO) [Total: 18]
- [cs.SE](#cs.SE) [Total: 8]
- [q-bio.NC](#q-bio.NC) [Total: 4]
- [stat.ML](#stat.ML) [Total: 9]
- [cs.DL](#cs.DL) [Total: 1]
- [physics.ins-det](#physics.ins-det) [Total: 1]
- [quant-ph](#quant-ph) [Total: 3]
- [cs.NI](#cs.NI) [Total: 2]
- [cs.GT](#cs.GT) [Total: 1]
- [eess.IV](#eess.IV) [Total: 12]
- [stat.ME](#stat.ME) [Total: 2]
- [cs.CR](#cs.CR) [Total: 3]
- [cs.CE](#cs.CE) [Total: 2]
- [cs.SD](#cs.SD) [Total: 3]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [cs.IR](#cs.IR) [Total: 13]
- [physics.optics](#physics.optics) [Total: 1]
- [eess.AS](#eess.AS) [Total: 1]
- [eess.SY](#eess.SY) [Total: 1]
- [cs.HC](#cs.HC) [Total: 2]
- [cs.MM](#cs.MM) [Total: 1]
- [eess.SP](#eess.SP) [Total: 3]
- [cs.CY](#cs.CY) [Total: 2]
- [math.ST](#math.ST) [Total: 1]
- [stat.OT](#stat.OT) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [SEEA-R1: Tree-Structured Reinforcement Fine-Tuning for Self-Evolving Embodied Agents](https://arxiv.org/abs/2506.21669)
*Wanxin Tian,Shijie Zhang,Kevin Zhang,Xiaowei Chi,Yulin Luo,Junyu Lu,Chunkai Fan,Qiang Zhou,Yiming Zhao,Ning Liu Siyu Lin,Zhiyuan Qin,Xiaozhu Ju,Shanghang Zhang,Jian Tang*

Main category: cs.AI

TL;DR: The paper introduces SEEA-R1, a framework leveraging reinforcement fine-tuning to enable self-evolving embodied agents, tackling challenges of sparse rewards and generalization.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of reinforcement fine-tuning in enabling self-evolving embodied agents by overcoming sparse rewards and dependency on hand-crafted reward functions.

Method: SEEA-R1 uses Tree-GRPO to produce dense intermediate rewards from sparse ones and introduces MGRM for task and scene generalization in the embodied domain.

Result: In the ALFWorld benchmark, SEEA-R1 scores 85.07% and 36.19% in textual and multi-modal tasks respectively, outperforming previous state-of-the-art approaches including GPT-4o.

Conclusion: SEEA-R1 demonstrates strong performance and scalability in enabling self-evolving embodied agents, setting a foundation for future research in embodied intelligence.

Abstract: Self-evolution, the ability of agents to autonomously improve their reasoning
and behavior, is essential for the embodied domain with long-horizon,
real-world tasks. Despite current advancements in reinforcement fine-tuning
(RFT) showing strong performance in enhancing reasoning in LLMs, its potential
to enable self-evolving embodied intelligence with multi-modal interactions
remains largely unexplored. Specifically, reinforcement fine-tuning faces two
fundamental obstacles in embodied settings: (i) the lack of accessible
intermediate rewards in multi-step reasoning tasks limits effective learning
signals, and (ii) reliance on hand-crafted reward functions restricts
generalization to novel tasks and environments. To address these challenges, we
present Self-Evolving Embodied Agents-R1, SEEA-R1, the first RFT framework
designed for enabling the self-evolving capabilities of embodied agents.
Specifically, to convert sparse delayed rewards into denser intermediate
signals that improve multi-step reasoning, we propose Tree-based group relative
policy optimization (Tree-GRPO), which integrates Monte Carlo Tree Search into
GRPO. To generalize reward estimation across tasks and scenes, supporting
autonomous adaptation and reward-driven self-evolution, we further introduce
Multi-modal Generative Reward Model (MGRM). To holistically evaluate the
effectiveness of SEEA-R1, we evaluate on the ALFWorld benchmark, surpassing
state-of-the-art methods with scores of 85.07% (textual) and 36.19%
(multi-modal), outperforming prior models including GPT-4o. SEEA-R1 also
achieves scores of 80.3% without environmental reward, surpassing all
open-source baselines and highlighting its scalability as a self-evolving
embodied agent. Additional experiments and qualitative analysis further support
the potential of SEEA-R1 for future research in scalable embodied intelligence.

</details>


### [2] [Hierarchical Reasoning Model](https://arxiv.org/abs/2506.21734)
*Guan Wang,Jin Li,Yuhao Sun,Xing Chen,Changling Liu,Yue Wu,Meng Lu,Sen Song,Yasin Abbasi Yadkori*

Main category: cs.AI

TL;DR: The paper introduces the Hierarchical Reasoning Model (HRM), an efficient recurrent architecture for high-performance reasoning tasks, achieving remarkable results with minimal data and resources.


<details>
  <summary>Details</summary>
Motivation: Reasoning is a major challenge in AI, and current methods like Chain-of-Thought face issues such as instability, high resource requirements, and slow processing. The authors aim to address these limitations.

Method: HRM consists of two recurrent modules: a high-level module for abstract planning and a low-level module for detailed computations. It performs reasoning tasks in a single forward pass and requires minimal supervision.

Result: The model, with only 27 million parameters, achieves near-perfect results on Sudoku and maze navigation tasks using only 1000 training samples. It also surpasses larger models on the Abstraction and Reasoning Corpus (ARC).

Conclusion: HRM showcases significant potential as a step toward universal computation and robust general-purpose reasoning in AI systems.

Abstract: Reasoning, the process of devising and executing complex goal-oriented action
sequences, remains a critical challenge in AI. Current large language models
(LLMs) primarily employ Chain-of-Thought (CoT) techniques, which suffer from
brittle task decomposition, extensive data requirements, and high latency.
Inspired by the hierarchical and multi-timescale processing in the human brain,
we propose the Hierarchical Reasoning Model (HRM), a novel recurrent
architecture that attains significant computational depth while maintaining
both training stability and efficiency. HRM executes sequential reasoning tasks
in a single forward pass without explicit supervision of the intermediate
process, through two interdependent recurrent modules: a high-level module
responsible for slow, abstract planning, and a low-level module handling rapid,
detailed computations. With only 27 million parameters, HRM achieves
exceptional performance on complex reasoning tasks using only 1000 training
samples. The model operates without pre-training or CoT data, yet achieves
nearly perfect performance on challenging tasks including complex Sudoku
puzzles and optimal path finding in large mazes. Furthermore, HRM outperforms
much larger models with significantly longer context windows on the Abstraction
and Reasoning Corpus (ARC), a key benchmark for measuring artificial general
intelligence capabilities. These results underscore HRM's potential as a
transformative advancement toward universal computation and general-purpose
reasoning systems.

</details>


### [3] [THE-Tree: Can Tracing Historical Evolution Enhance Scientific Verification and Reasoning?](https://arxiv.org/abs/2506.21763)
*Xin Wang,Jiyao Liu,Yulong Xiao,Junzhi Ning,Lihao Liu,Junjun He,Botian Shi,Kaicheng Yu*

Main category: cs.AI

TL;DR: The paper introduces THE-Tree, a technology evolution framework leveraging LLMs and natural language inference to build structured, causally-linked scientific evolution trees, aiming to address shortcomings in validating AI-generated scientific ideas.


<details>
  <summary>Details</summary>
Motivation: Evaluating large numbers of AI-generated scientific ideas is challenging due to insufficient verification methods, including LLM hallucinations and limitations of citation networks.

Method: THE-Tree uses an algorithm paired with the "Think-Verbalize-Cite-Verify" process to generate and validate causally-linked scientific evolution paths through literature-based evidence and logical reasoning.

Result: The framework's experiments show improvements of 8–14% in graph completion, a 10% increase in predicting future advancements, and a 100% performance boost combined with traditional methods when evaluating key scientific papers.

Conclusion: THE-Tree enhances structured scientific idea validation and predictions, offering a robust benchmark dataset to advance research in scientific idea generation and verification.

Abstract: Large Language Models (LLMs) are accelerating scientific idea generation, but
rigorously evaluating these numerous, often superficial, AI-generated
propositions for novelty and factual accuracy is a critical bottleneck; manual
verification is too slow.Existing validation methods are inadequate: LLMs as
standalone verifiers may hallucinate and lack domain knowledge (our findings
show ~60\% unawareness of relevant papers in specific domains), while
traditional citation networks lack explicit causality and narrative surveys are
unstructured.This underscores a core challenge: the absence of structured,
verifiable, and causally-linked historical data of scientific evolution.To
address this,we introduce \textbf{THE-Tree} (\textbf{T}echnology
\textbf{H}istory \textbf{E}volution Tree), a computational framework that
constructs such domain-specific evolution trees from scientific
literature.THE-Tree employs a search algorithm to explore evolutionary paths.
During its node expansion, it utilizes a novel "Think-Verbalize-Cite-Verify"
process: an LLM proposes potential advancements and cites supporting
literature. Critically, each proposed evolutionary link is then validated for
logical coherence and evidential support by a recovered natural language
inference mechanism that interrogates the cited literature, ensuring that each
step is grounded.We construct and validate 88 THE-Trees across diverse domains
and release a benchmark dataset including up to 71k fact verifications covering
27k papers to foster further research.Experiments demonstrate that i) in graph
completion, our THE-Tree improves hit@1 by 8\% to 14\% across multiple models
compared to traditional citation networks; ii) for predicting future scientific
developments, it improves hit@1 metric by nearly 10\%; and iii) when combined
with other methods, it boosts the performance of evaluating important
scientific papers by almost 100\%.

</details>


### [4] [MobiVerse: Scaling Urban Mobility Simulation with Hybrid Lightweight Domain-Specific Generator and Large Language Models](https://arxiv.org/abs/2506.21784)
*Yifan Liu,Xishun Liao,Haoxuan Ma,Jonathan Liu,Rohan Jadhav,Jiaqi Ma*

Main category: cs.AI

TL;DR: Understanding human mobility patterns is key for urban planning, but current models face limitations. MobiVerse introduces a hybrid framework combining domain-specific generative models with Large Language Models (LLMs) for scalability, adaptability, and real-world applicability.


<details>
  <summary>Details</summary>
Motivation: To address the lack of scalable and adaptable platforms for mobility simulation that can assist in algorithm testing, policy formulation, and real-world evaluation without excessive computational demands or data requirements.

Method: The proposed MobiVerse hybrid framework utilizes lightweight domain-specific activity generators for base activity chains and LLMs for adaptive, context-aware adjustments. It supports interaction with environmental feedback such as road closures, events, and congestion.

Result: In a case study in Westwood, Los Angeles, MobiVerse efficiently adjusted schedules for 53,000 agents using limited computational resources while enabling simulations responsive to real-world conditions like traffic disruptions and events.

Conclusion: MobiVerse is a modular, computationally efficient tool that improves simulation realism while addressing key limitations in mobility modeling, offering potential for customized urban planning solutions.

Abstract: Understanding and modeling human mobility patterns is crucial for effective
transportation planning and urban development. Despite significant advances in
mobility research, there remains a critical gap in simulation platforms that
allow for algorithm development, policy implementation, and comprehensive
evaluation at scale. Traditional activity-based models require extensive data
collection and manual calibration, machine learning approaches struggle with
adaptation to dynamic conditions, and treding agent-based Large Language Models
(LLMs) implementations face computational constraints with large-scale
simulations. To address these challenges, we propose MobiVerse, a hybrid
framework leverages the efficiency of lightweight domain-specific generator for
generating base activity chains with the adaptability of LLMs for context-aware
modifications. A case study was conducted in Westwood, Los Angeles, where we
efficiently generated and dynamically adjusted schedules for the whole
population of approximately 53,000 agents on a standard PC. Our experiments
demonstrate that MobiVerse successfully enables agents to respond to
environmental feedback, including road closures, large gathering events like
football games, and congestion, through our hybrid framework. Its modular
design facilitates testing various mobility algorithms at both transportation
system and agent levels. Results show our approach maintains computational
efficiency while enhancing behavioral realism. MobiVerse bridges the gap in
mobility simulation by providing a customizable platform for mobility systems
planning and operations with benchmark algorithms. Code and videos are
available at https://github.com/ucla-mobility/MobiVerse.

</details>


### [5] [CitySim: Modeling Urban Behaviors and City Dynamics with Large-Scale LLM-Driven Agent Simulation](https://arxiv.org/abs/2506.21805)
*Nicolas Bougie,Narimasa Watanabe*

Main category: cs.AI

TL;DR: CitySim is an advanced urban simulator that uses large language models to emulate human-like behaviors and daily schedules for urban analysis.


<details>
  <summary>Details</summary>
Motivation: Existing urban simulation models often rely on static, rule-based designs which fail to capture dynamic and adaptive human behaviors.

Method: CitySim employs large language models to create agents with beliefs, goals, and adaptive daily schedules that incorporate spatial memory and real-life factors.

Result: CitySim achieves closer alignment with real human behaviors than previous models and provides insights into urban phenomena by simulating and observing the collective actions of tens of thousands of agents.

Conclusion: CitySim serves as a scalable and flexible tool for analyzing urban behaviors and forecasting phenomena, surpassing traditional rule-based models in fidelity and adaptability.

Abstract: Modeling human behavior in urban environments is fundamental for social
science, behavioral studies, and urban planning. Prior work often rely on
rigid, hand-crafted rules, limiting their ability to simulate nuanced
intentions, plans, and adaptive behaviors. Addressing these challenges, we
envision an urban simulator (CitySim), capitalizing on breakthroughs in
human-level intelligence exhibited by large language models. In CitySim, agents
generate realistic daily schedules using a recursive value-driven approach that
balances mandatory activities, personal habits, and situational factors. To
enable long-term, lifelike simulations, we endow agents with beliefs, long-term
goals, and spatial memory for navigation. CitySim exhibits closer alignment
with real humans than prior work, both at micro and macro levels. Additionally,
we conduct insightful experiments by modeling tens of thousands of agents and
evaluating their collective behaviors under various real-world scenarios,
including estimating crowd density, predicting place popularity, and assessing
well-being. Our results highlight CitySim as a scalable, flexible testbed for
understanding and forecasting urban phenomena.

</details>


### [6] [Interactive Multi-Objective Probabilistic Preference Learning with Soft and Hard Bounds](https://arxiv.org/abs/2506.21887)
*Edward Chen,Sang T. Truong,Natalie Dullerud,Sanmi Koyejo,Carlos Guestrin*

Main category: cs.AI

TL;DR: The paper proposes Active-MoSH, a framework combining local probabilistic preference learning with global sensitivity analysis to aid decision-makers in navigating multi-objective high-stakes decisions effectively.


<details>
  <summary>Details</summary>
Motivation: High-stakes decision-making often involves balancing soft and hard bounds across multiple objectives, requiring frameworks that can systematically refine preferences while maintaining trust in the final decision.

Method: The authors designed Active-MoSH, featuring an adaptive local component for Pareto subset refinement using probabilistic models and active sampling, paired with a global component (T-MoSH) for sensitivity analysis to identify overlooked solutions.

Result: Active-MoSH was evaluated through synthetic and real-world applications, demonstrating improved convergence, enhanced decision-maker trust, and expressive preference articulation. A user study further validated its effectiveness.

Conclusion: Active-MoSH enables effective high-stakes decision-making by integrating iterative preference refinement with mechanisms to ensure trust through sensitivity analysis, demonstrating broad performance benefits in diverse scenarios.

Abstract: High-stakes decision-making involves navigating multiple competing objectives
with expensive evaluations. For instance, in brachytherapy, clinicians must
balance maximizing tumor coverage (e.g., an aspirational target or soft bound
of >95% coverage) against strict organ dose limits (e.g., a non-negotiable hard
bound of <601 cGy to the bladder), with each plan evaluation being
resource-intensive. Selecting Pareto-optimal solutions that match implicit
preferences is challenging, as exhaustive Pareto frontier exploration is
computationally and cognitively prohibitive, necessitating interactive
frameworks to guide users. While decision-makers (DMs) often possess domain
knowledge to narrow the search via such soft-hard bounds, current methods often
lack systematic approaches to iteratively refine these multi-faceted preference
structures. Critically, DMs must trust their final decision, confident they
haven't missed superior alternatives; this trust is paramount in
high-consequence scenarios. We present Active-MoSH, an interactive local-global
framework designed for this process. Its local component integrates soft-hard
bounds with probabilistic preference learning, maintaining distributions over
DM preferences and bounds for adaptive Pareto subset refinement. This is guided
by an active sampling strategy optimizing exploration-exploitation while
minimizing cognitive burden. To build DM trust, Active-MoSH's global component,
T-MoSH, leverages multi-objective sensitivity analysis to identify potentially
overlooked, high-value points beyond immediate feedback. We demonstrate
Active-MoSH's performance benefits through diverse synthetic and real-world
applications. A user study on AI-generated image selection further validates
our hypotheses regarding the framework's ability to improve convergence,
enhance DM trust, and provide expressive preference articulation, enabling more
effective DMs.

</details>


### [7] [AlphaBeta is not as good as you think: a new probabilistic model to better analyze deterministic game-solving algorithms](https://arxiv.org/abs/2506.21996)
*Raphaël Boige,Amine Boumaza,Bruno Scherrer*

Main category: cs.AI

TL;DR: The paper explores a new probabilistic model for game-tree complexity, addressing the shortcomings of traditional models by including structural dependencies, with findings that highlight practical cost differences between algorithms like AlphaBeta and Scout.


<details>
  <summary>Details</summary>
Motivation: Conventional deterministic game-solving models are limited by their independent assumption, which produces trivial problems without structural complexity. This paper aims to establish a more realistic model to analyze algorithms rigorously under meaningful challenges.

Method: The authors introduce a probabilistic game-tree model using a level-wise conditional distribution to enforce ancestor dependency and generate adjustable difficulty problems. Recursive formulas were derived to study average-case complexities for algorithms like AlphaBeta and Scout.

Result: The study revealed that while asymptotically all algorithms converge to identical branching factors, deep finite trees show AlphaBeta is less efficient due to higher constant multiplicative factors compared to Scout.

Conclusion: The proposed framework offers a more realistic analysis of game-solving algorithms, highlighting practical efficiency differences and advancing understanding of their performance under structurally complex models.

Abstract: Deterministic game-solving algorithms are conventionally analyzed in the
light of their average-case complexity against a distribution of random
game-trees, where leaf values are independently sampled from a fixed
distribution. This simplified model enables uncluttered mathematical analysis,
revealing two key properties: root value distributions asymptotically collapse
to a single fixed value for finite-valued trees, and all reasonable algorithms
achieve global optimality. However, these findings are artifacts of the model's
design-its long criticized independence assumption strips games of structural
complexity, producing trivial instances where no algorithm faces meaningful
challenges. To address this limitation, we introduce a new probabilistic model
that incrementally constructs game-trees using a fixed level-wise conditional
distribution. By enforcing ancestor dependency, a critical structural feature
of real-world games, our framework generates problems with adjustable
difficulty while retaining some form of analytical tractability. For several
algorithms, including AlphaBeta and Scout, we derive recursive formulas
characterizing their average-case complexities under this model. These allow us
to rigorously compare algorithms on deep game-trees, where Monte-Carlo
simulations are no longer feasible. While asymptotically, all algorithms seem
to converge to identical branching factor (a result analogous to those of
independence-based models), deep finite trees reveal stark differences:
AlphaBeta incurs a significantly larger constant multiplicative factor compared
to algorithms like Scout, leading to a substantial practical slowdown. Our
framework sheds new light on classical game-solving algorithms, offering
rigorous evidence and analytical tools to advance the understanding of these
methods under a more realistic, challenging, and yet tractable model.

</details>


### [8] [LeanConjecturer: Automatic Generation of Mathematical Conjectures for Theorem Proving](https://arxiv.org/abs/2506.22005)
*Naoto Onda,Kazumi Kasaura,Yuta Oriike,Masaya Taniguchi,Akiyoshi Sannai,Sho Sonoda*

Main category: cs.AI

TL;DR: LeanConjecturer is a pipeline that uses LLMs to automatically generate university-level mathematical conjectures in Lean 4, successfully producing a large dataset of novel conjectures useful for theorem proving.


<details>
  <summary>Details</summary>
Motivation: Mathematical conjectures are essential in formal theorem proving, but a lack of abundant, diverse data hinders systems' performance and learning processes.

Method: The approach combines rule-based context extraction with LLM-assisted theorem generation, followed by iterative evaluation. It also integrates reinforcement learning using Group Relative Policy Optimization (GRPO).

Result: The pipeline generated over 12,000 conjectures, with 3,776 syntactically valid ones deemed non-trivial. It enhanced theorem proving tactics and successfully verified mathematical discoveries in topology.

Conclusion: LeanConjecturer offers a scalable system for creating high-quality conjectures and contributes to advancing mathematical theorem proving and discovery.

Abstract: We introduce LeanConjecturer, a pipeline for automatically generating
university-level mathematical conjectures in Lean 4 using Large Language Models
(LLMs). Our hybrid approach combines rule-based context extraction with
LLM-based theorem statement generation, addressing the data scarcity challenge
in formal theorem proving. Through iterative generation and evaluation,
LeanConjecturer produced 12,289 conjectures from 40 Mathlib seed files, with
3,776 identified as syntactically valid and non-trivial, that is, cannot be
proven by \texttt{aesop} tactic. We demonstrate the utility of these generated
conjectures for reinforcement learning through Group Relative Policy
Optimization (GRPO), showing that targeted training on domain-specific
conjectures can enhance theorem proving capabilities. Our approach generates
103.25 novel conjectures per seed file on average, providing a scalable
solution for creating training data for theorem proving systems. Our system
successfully verified several non-trivial theorems in topology, including
properties of semi-open, alpha-open, and pre-open sets, demonstrating its
potential for mathematical discovery beyond simple variations of existing
results.

</details>


### [9] [Universal Retrieval for Multimodal Trajectory Modeling](https://arxiv.org/abs/2506.22056)
*Xuan Zhang,Ziyan Jiang,Rui Meng,Yifei Leng,Zhenbang Xiao,Zora Zhiruo Wang,Yanyi Shang,Dehan Kong*

Main category: cs.AI

TL;DR: This paper tackles the challenge of representing trajectory-level multimodal data and introduces a new dataset (UATD), a benchmark (GAE-Bench), and a retrieval framework (GAE-Retriever) to enhance retrieval recall in multimodal environments.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between universal data retrieval and agent-centric trajectory modeling using expanding trajectory data for better AI agent performance, particularly in GUI environments.

Method: Construct the Unified Agent Trajectory Dataset (UATD), develop a benchmark (GAE-Bench) with retrieval pairs, and create GAE-Retriever, a multimodal retrieval framework using vision-language models and contrastive learning with token selection and GradCache.

Result: GAE-Retriever consistently outperforms strong baselines in retrieval recall across multiple datasets, demonstrating its effectiveness.

Conclusion: The proposed approach advances multimodal trajectory retrieval and provides tools that have significant implications for AI research using trajectory data.

Abstract: Trajectory data, capturing human actions and environmental states across
various modalities, holds significant potential for enhancing AI agent
capabilities, particularly in GUI environments. However, how to model the
representation of trajectory-level data presents a significant challenge that
has not been systematically addressed amid explosive trajectory data growth. In
this work, we introduce Multimodal Trajectory Retrieval, bridging the gap
between universal retrieval and agent-centric trajectory modeling. We construct
the Unified Agent Trajectory Dataset (UATD) from annotated demonstrations and
states across diverse real-world scenarios. Based on this, we present
GAE-Bench, a benchmark containing a large number of trajectory-based retrieval
pairs. In addition, we propose GAE-Retriever, a multimodal retrieval framework
that adopts vision-language models and incorporates optimized contrastive
learning through a token selection and the GradCache mechanism. Comprehensive
evaluations across multiple datasets show that GAE-Retriever consistently
outperforms strong baselines in retrieval recall, highlighting its
effectiveness in advancing multimodal trajectory retrieval.

</details>


### [10] [Query as Test: An Intelligent Driving Test and Data Storage Method for Integrated Cockpit-Vehicle-Road Scenarios](https://arxiv.org/abs/2506.22068)
*Shengyue Yao,Runqing Guo,Yangyang Qin,Miangbing Meng,Jipeng Cao,Yilun Lin,Yisheng Lv,Fei-Yue Wang*

Main category: cs.AI

TL;DR: The paper introduces "Query as Test" (QaT) and "Extensible Scenarios Notations" (ESN) to address fragmented data ecosystems in autonomous driving and intelligent road systems.


<details>
  <summary>Details</summary>
Motivation: Existing testing methods in AI-driven transportation systems lack flexibility, fail to cover edge cases, and rely on fragmented data storage, creating a need for a unified and robust testing paradigm.

Method: The paper proposes ESN, a new data framework using Answer Set Programming (ASP) that integrates multimodal data into logical facts and rules, enabling semantic querying, interpretability, and privacy protection.

Result: The QaT paradigm transforms functional validation and safety compliance checks into logical queries, improving the rigor and expressiveness of testing systems for autonomous driving.

Conclusion: The introduction of "Validation-Driven Development" (VDD) and the QaT paradigm accelerates AI-driven developments, enabling more reliable development and testing processes for intelligent transportation systems.

Abstract: With the deep penetration of Artificial Intelligence (AI) in the
transportation sector, intelligent cockpits, autonomous driving, and
intelligent road networks are developing at an unprecedented pace. However, the
data ecosystems of these three key areas are increasingly fragmented and
incompatible. Especially, existing testing methods rely on data stacking, fail
to cover all edge cases, and lack flexibility. To address this issue, this
paper introduces the concept of "Query as Test" (QaT). This concept shifts the
focus from rigid, prescripted test cases to flexible, on-demand logical queries
against a unified data representation. Specifically, we identify the need for a
fundamental improvement in data storage and representation, leading to our
proposal of "Extensible Scenarios Notations" (ESN). ESN is a novel declarative
data framework based on Answer Set Programming (ASP), which uniformly
represents heterogeneous multimodal data from the cockpit, vehicle, and road as
a collection of logical facts and rules. This approach not only achieves deep
semantic fusion of data, but also brings three core advantages: (1) supports
complex and flexible semantic querying through logical reasoning; (2) provides
natural interpretability for decision-making processes; (3) allows for
on-demand data abstraction through logical rules, enabling fine-grained privacy
protection. We further elaborate on the QaT paradigm, transforming the
functional validation and safety compliance checks of autonomous driving
systems into logical queries against the ESN database, significantly enhancing
the expressiveness and formal rigor of the testing. Finally, we introduce the
concept of "Validation-Driven Development" (VDD), which suggests to guide
developments by logical validation rather than quantitative testing in the era
of Large Language Models, in order to accelerating the iteration and
development process.

</details>


### [11] [A Different Approach to AI Safety: Proceedings from the Columbia Convening on Openness in Artificial Intelligence and AI Safety](https://arxiv.org/abs/2506.22183)
*Camille François,Ludovic Péran,Ayah Bdeir,Nouha Dziri,Will Hawkins,Yacine Jernite,Sayash Kapoor,Juliet Shen,Heidy Khlaaf,Kevin Klyman,Nik Marda,Marie Pellat,Deb Raji,Divya Siddarth,Aviya Skowron,Joseph Spisak,Madhulika Srikumar,Victor Storchan,Audrey Tang,Jen Weedon*

Main category: cs.AI

TL;DR: The paper discusses outcomes from a convening on AI openness and safety, highlighting research priorities and gaps in deploying open foundation models responsibly.


<details>
  <summary>Details</summary>
Motivation: The paper investigates how openness in AI systems, via transparency and governance, can enhance safety amidst increasing adoption of open-weight and open-source foundation models.

Method: Participants from diverse sectors engaged in preparatory programs and collaborative workshops, using a solutions-oriented approach to develop research agendas and technical mappings related to AI safety.

Result: Key results include identified gaps such as limited benchmarks, weak defenses against attacks, and insufficient participatory mechanisms in open-source AI deployment.

Conclusion: A roadmap of five research priorities was proposed, emphasizing participatory governance and improved safety measures to support the development of accountable AI systems.

Abstract: The rapid rise of open-weight and open-source foundation models is
intensifying the obligation and reshaping the opportunity to make AI systems
safe. This paper reports outcomes from the Columbia Convening on AI Openness
and Safety (San Francisco, 19 Nov 2024) and its six-week preparatory programme
involving more than forty-five researchers, engineers, and policy leaders from
academia, industry, civil society, and government. Using a participatory,
solutions-oriented process, the working groups produced (i) a research agenda
at the intersection of safety and open source AI; (ii) a mapping of existing
and needed technical interventions and open source tools to safely and
responsibly deploy open foundation models across the AI development workflow;
and (iii) a mapping of the content safety filter ecosystem with a proposed
roadmap for future research and development. We find that openness --
understood as transparent weights, interoperable tooling, and public governance
-- can enhance safety by enabling independent scrutiny, decentralized
mitigation, and culturally plural oversight. However, significant gaps persist:
scarce multimodal and multilingual benchmarks, limited defenses against
prompt-injection and compositional attacks in agentic systems, and insufficient
participatory mechanisms for communities most affected by AI harms. The paper
concludes with a roadmap of five priority research directions, emphasizing
participatory inputs, future-proof content filters, ecosystem-wide safety
infrastructure, rigorous agentic safeguards, and expanded harm taxonomies.
These recommendations informed the February 2025 French AI Action Summit and
lay groundwork for an open, plural, and accountable AI safety discipline.

</details>


### [12] [Breaking Rank Bottlenecks in Knowledge Graph Completion](https://arxiv.org/abs/2506.22271)
*Samy Badreddine,Emile van Krieken,Luciano Serafini*

Main category: cs.AI

TL;DR: The paper tackles knowledge graph completion (KGC) models' rank bottlenecks and introduces KGE-MoS, a mixture-based output layer, which improves predictive performance at low computational cost.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the limitations posed by rank bottlenecks in the output layer of KGC models, which reduce model expressivity and ranking accuracy when dealing with large numbers of entities.

Method: The paper investigates the theoretical and empirical effects of rank bottlenecks and suggests a mixture-based output layer, KGE-MoS, inspired by advancements in language modeling.

Result: The proposed KGE-MoS approach demonstrates improved performance and better probabilistic fit across four datasets, with minimal additional parameter cost.

Conclusion: KGE-MoS effectively alleviates rank bottlenecks in KGC models, offering enhanced accuracy and better distribution fidelity at a low computational tradeoff.

Abstract: Many Knowledge Graph Completion (KGC) models, despite using powerful
encoders, rely on a simple vector-matrix multiplication to score queries
against candidate object entities. When the number of entities is larger than
the model's embedding dimension, which in practical scenarios is often by
several orders of magnitude, we have a linear output layer with a rank
bottleneck. Such bottlenecked layers limit model expressivity. We investigate
both theoretically and empirically how rank bottlenecks affect KGC models. We
find that, by limiting the set of feasible predictions, rank bottlenecks hurt
ranking accuracy and the distribution fidelity of scores. Inspired by the
language modelling literature, we propose KGE-MoS, a mixture-based output layer
to break rank bottlenecks in many KGC models. Our experiments on four datasets
show that KGE-MoS improves performance and probabilistic fit of KGC models for
a low parameter cost.

</details>


### [13] [Artificial Intelligent Disobedience: Rethinking the Agency of Our Artificial Teammates](https://arxiv.org/abs/2506.22276)
*Reuth Mirsky*

Main category: cs.AI

TL;DR: The paper advocates for the concept of 'intelligent disobedience' in AI systems, allowing autonomy in cooperative human-AI teams for safer and more meaningful contributions.


<details>
  <summary>Details</summary>
Motivation: Most AI systems are designed to rigidly follow instructions, which can be counterproductive or unsafe. The paper seeks to address the need for AI systems to contribute autonomously within collaborative teams.

Method: The paper introduces a scale of AI agency and explores various examples to highlight the necessity of autonomy and intelligent disobedience, proposing considerations for developing this capability.

Result: The paper outlines how intelligent disobedience can enhance AI systems, emphasizing its importance across different levels of autonomy in cooperative settings.

Conclusion: The study underscores that AI autonomy should be treated as a key research focus, with intelligent disobedience being explored as a vital aspect for safe and effective human-AI collaboration.

Abstract: Artificial intelligence has made remarkable strides in recent years,
achieving superhuman performance across a wide range of tasks. Yet despite
these advances, most cooperative AI systems remain rigidly obedient, designed
to follow human instructions without question and conform to user expectations,
even when doing so may be counterproductive or unsafe. This paper argues for
expanding the agency of AI teammates to include \textit{intelligent
disobedience}, empowering them to make meaningful and autonomous contributions
within human-AI teams. It introduces a scale of AI agency levels and uses
representative examples to highlight the importance and growing necessity of
treating AI autonomy as an independent research focus in cooperative settings.
The paper then explores how intelligent disobedience manifests across different
autonomy levels and concludes by proposing initial boundaries and
considerations for studying disobedience as a core capability of artificial
agents.

</details>


### [14] [Conceptual Topic Aggregation](https://arxiv.org/abs/2506.22309)
*Klara M. Gutekunst,Dominik Dürrschnabel,Johannes Hirth,Gerd Stumme*

Main category: cs.AI

TL;DR: This paper introduces FAT-CAT, a method using Formal Concept Analysis to improve topic modeling by providing better topic aggregation and visualization.


<details>
  <summary>Details</summary>
Motivation: Current topic modeling methods struggle with generating interpretable and meaningful representations for large textual datasets.

Method: FAT-CAT uses Formal Concept Analysis (FCA) to create hierarchical concept lattices that enhance topic aggregation and visualization, enabling a structured representation of topics across various directories and file types.

Result: The FAT-CAT approach was evaluated on the ETYNTKE dataset and shown to deliver more meaningful and interpretable insights compared to existing topic modeling methods.

Conclusion: The approach effectively leverages FCA for enhanced topic modeling, offering significant improvements in the interpretability and utility of topic aggregations in diverse datasets.

Abstract: The vast growth of data has rendered traditional manual inspection
infeasible, necessitating the adoption of computational methods for efficient
data exploration. Topic modeling has emerged as a powerful tool for analyzing
large-scale textual datasets, enabling the extraction of latent semantic
structures. However, existing methods for topic modeling often struggle to
provide interpretable representations that facilitate deeper insights into data
structure and content. In this paper, we propose FAT-CAT, an approach based on
Formal Concept Analysis (FCA) to enhance meaningful topic aggregation and
visualization of discovered topics. Our approach can handle diverse topics and
file types -- grouped by directories -- to construct a concept lattice that
offers a structured, hierarchical representation of their topic distribution.
In a case study on the ETYNTKE dataset, we evaluate the effectiveness of our
approach against other representation methods to demonstrate that FCA-based
aggregation provides more meaningful and interpretable insights into dataset
composition than existing topic modeling techniques.

</details>


### [15] [Embodied AI Agents: Modeling the World](https://arxiv.org/abs/2506.22355)
*Pascale Fung,Yoram Bachrach,Asli Celikyilmaz,Kamalika Chaudhuri,Delong Chen,Willy Chung,Emmanuel Dupoux,Hervé Jégou,Alessandro Lazaric,Arjun Majumdar,Andrea Madotto,Franziska Meier,Florian Metze,Théo Moutakanni,Juan Pino,Basile Terver,Joseph Tighe,Jitendra Malik*

Main category: cs.AI

TL;DR: The paper investigates AI agents embodied in various forms (like avatars and robots), highlighting their ability to interact with users and environments. The focus is on developing 'world models' to enhance reasoning, planning, and collaboration.


<details>
  <summary>Details</summary>
Motivation: To create AI agents that better mimic human-like interaction and learning by embedding them in physical or virtual environments.

Method: Focus on 'world modeling,' integrating multimodal perception, reasoning, memory, and planning to build a comprehensive understanding of the physical and mental worlds.

Result: Improved ability of embodied AI agents to understand environments, predict outcomes, comprehend user intentions, and perform complex tasks.

Conclusion: World modeling is key for enhancing the autonomy and collaborative potential of embodied AI systems, both in physical and social contexts.

Abstract: This paper describes our research on AI agents embodied in visual, virtual or
physical forms, enabling them to interact with both users and their
environments. These agents, which include virtual avatars, wearable devices,
and robots, are designed to perceive, learn and act within their surroundings,
which makes them more similar to how humans learn and interact with the
environments as compared to disembodied agents. We propose that the development
of world models is central to reasoning and planning of embodied AI agents,
allowing these agents to understand and predict their environment, to
understand user intentions and social contexts, thereby enhancing their ability
to perform complex tasks autonomously. World modeling encompasses the
integration of multimodal perception, planning through reasoning for action and
control, and memory to create a comprehensive understanding of the physical
world. Beyond the physical world, we also propose to learn the mental world
model of users to enable better human-agent collaboration.

</details>


### [16] [AI Model Passport: Data and System Traceability Framework for Transparent AI in Health](https://arxiv.org/abs/2506.22358)
*Varvara Kalokyri,Nikolaos S. Tachos,Charalampos N. Kalantzopoulos,Stelios Sfakianakis,Haridimos Kondylakis,Dimitrios I. Zaridis,Sara Colantonio,Daniele Regge,Nikolaos Papanikolaou,The ProCAncer-I consortium,Konstantinos Marias,Dimitrios I. Fotiadis,Manolis Tsiknakis*

Main category: cs.AI

TL;DR: This paper introduces the AI Model Passport, a standardized documentation framework aimed at enhancing transparency, reproducibility, and stakeholder trust in AI models used in healthcare.


<details>
  <summary>Details</summary>
Motivation: The paper addresses issues in AI systems such as lack of scalability, comparability, machine interpretability, and the absence of verifiable identities for AI models, which limit trust and reproducibility, especially in biomedical systems.

Method: The authors propose the AI Model Passport framework and showcase its implementation using the AIPassport tool developed in the ProCAncer-I EU project, automating metadata collection and model versioning.

Result: AIPassport demonstrated effectiveness in medical imaging tasks like lesion segmentation, improving transparency, reproducibility, and regulatory compliance while optimizing manual efforts.

Conclusion: The framework sets a new standard for trustworthy, regulation-compliant AI systems in healthcare and other domains, contributing to transparency, accountability, and ethical AI practices.

Abstract: The increasing integration of Artificial Intelligence (AI) into health and
biomedical systems necessitates robust frameworks for transparency,
accountability, and ethical compliance. Existing frameworks often rely on
human-readable, manual documentation which limits scalability, comparability,
and machine interpretability across projects and platforms. They also fail to
provide a unique, verifiable identity for AI models to ensure their provenance
and authenticity across systems and use cases, limiting reproducibility and
stakeholder trust. This paper introduces the concept of the AI Model Passport,
a structured and standardized documentation framework that acts as a digital
identity and verification tool for AI models. It captures essential metadata to
uniquely identify, verify, trace and monitor AI models across their lifecycle -
from data acquisition and preprocessing to model design, development and
deployment. In addition, an implementation of this framework is presented
through AIPassport, an MLOps tool developed within the ProCAncer-I EU project
for medical imaging applications. AIPassport automates metadata collection,
ensures proper versioning, decouples results from source scripts, and
integrates with various development environments. Its effectiveness is
showcased through a lesion segmentation use case using data from the
ProCAncer-I dataset, illustrating how the AI Model Passport enhances
transparency, reproducibility, and regulatory readiness while reducing manual
effort. This approach aims to set a new standard for fostering trust and
accountability in AI-driven healthcare solutions, aspiring to serve as the
basis for developing transparent and regulation compliant AI systems across
domains.

</details>


### [17] [The Automated LLM Speedrunning Benchmark: Reproducing NanoGPT Improvements](https://arxiv.org/abs/2506.22419)
*Bingchen Zhao,Despoina Magka,Minqi Jiang,Xian Li,Roberta Raileanu,Tatiana Shavrina,Jean-Christophe Gagnon-Audet,Kelvin Niu,Shagun Sodhani,Michael Shvartsman,Andrei Lupu,Alisia Lupidi,Edan Toledo,Karen Hambardzumyan,Martin Josifoski,Thomas Foster,Lucia Cipolina-Kun,Abhishek Charnalia,Derek Dunfield,Alexander H. Miller,Oisin Mac Aodha,Jakob Foerster,Yoram Bachrach*

Main category: cs.AI

TL;DR: The paper introduces a benchmark to evaluate Large Language Models (LLMs) on their ability to reproduce existing research, revealing challenges in automation of scientific reproduction.


<details>
  <summary>Details</summary>
Motivation: To assess and improve the ability of AI agents to reproduce existing scientific work in active research areas, which is essential for progressing toward autonomous research capabilities.

Method: The authors developed the Automated LLM Speedrunning Benchmark using tasks from the NanoGPT speedrun competition. Each task offers training scripts and optional hints in various forms to evaluate LLMs' ability to reproduce results efficiently.

Result: Experiments reveal that state-of-the-art reasoning LLMs, even with detailed hints, struggle to replicate known innovations in the benchmark tasks.

Conclusion: Current LLMs face significant challenges in reproducing scientific results, highlighting the need for improved benchmarks and methodologies to advance autonomous research capabilities.

Abstract: Rapid advancements in large language models (LLMs) have the potential to
assist in scientific progress. A critical capability toward this endeavor is
the ability to reproduce existing work. To evaluate the ability of AI agents to
reproduce results in an active research area, we introduce the Automated LLM
Speedrunning Benchmark, leveraging the research community contributions on the
NanoGPT speedrun, a competition to train a GPT-2 model in the shortest time.
Each of the 19 speedrun tasks provides the agent with the previous records
training script, optionally paired with one of three hint formats, ranging from
pseudocode to paper-like descriptions of the new records improvements. Records
execute quickly by design and speedrun improvements encompass diverse
code-level changes, ranging from high-level algorithmic advancements to
hardware-aware optimizations. These features make the benchmark both accessible
and realistic for the frontier problem of improving LLM training. We find that
recent reasoning LLMs combined with SoTA scaffolds struggle to reimplement
already-known innovations in our benchmark, even when given detailed hints. Our
benchmark thus provides a simple, non-saturated measure of an LLMs ability to
automate scientific reproduction, a necessary (but not sufficient) skill for an
autonomous research agent.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [18] [Power- and Area-Efficient Unary Sorting Architecture Using FSM-Based Unary Number Generator](https://arxiv.org/abs/2506.22107)
*Amir Hossein Jalilvand,M. Hassan Najafi*

Main category: cs.AR

TL;DR: This paper presents a novel unary sorting module that uses a two-state finite-state machine for efficient sorting, reducing area by 82% and power by 70% compared to existing unary designs.


<details>
  <summary>Details</summary>
Motivation: Sorting is critical in computing applications, and unary computing offers a cost-efficient method. However, unary designs so far have significant area and power overheads due to costly number generators.

Method: The paper introduces an ascending-order unary sorter using a finite-state-machine-based unary number generator. This uses right-aligned unary streams and operates without conventional comparators.

Result: In a 45nm technology node, the proposed method achieved up to 82% area reduction and 70% power reduction compared to existing designs.

Conclusion: The method provides an efficient solution for energy and resource-limited hardware systems, highlighting the potential of unary computing in sorting tasks.

Abstract: Sorting is a fundamental operation in computer systems and is widely used in
applications such as databases, data analytics, and hardware accelerators.
Unary computing has recently emerged as a low-cost and power-efficient paradigm
for implementing hardware sorters by eliminating the need for complex
arithmetic operations. However, existing comparison-free unary computing-based
designs suffer from significant area and power overhead due to costly unary
number generators.
  In this paper, we present a novel ascending-order unary sorting module
featuring a finite-state-machine-based unary number generator that
significantly reduces implementation costs. By generating right-aligned unary
streams using a two-state finite-state machine, our architecture iteratively
identifies the minimum input value in each cycle without conventional
comparators. Synthesis results in a 45nm technology node demonstrate up to 82%
reduction in area and 70% reduction in power consumption compared to
state-of-the-art unary designs. The proposed sorter offers a promising solution
for energy-constrained and resource-limited hardware systems.

</details>


### [19] [Hardware acceleration for ultra-fast Neural Network training on FPGA for MRF map reconstruction](https://arxiv.org/abs/2506.22156)
*Mattia Ricchi,Fabrizio Alfonsi,Camilla Marella,Marco Barbieri,Alessandra Retico,Leonardo Brizi,Alessandro Gabrielli,Claudia Testa*

Main category: cs.AR

TL;DR: The paper proposes an FPGA-based neural network for real-time brain analysis using Magnetic Resonance Fingerprinting (MRF), achieving fast reconstruction and training, potentially improving clinical decision-making.


<details>
  <summary>Details</summary>
Motivation: To accelerate the reconstruction process of Magnetic Resonance Fingerprinting (MRF) data and make real-time brain parameter analysis feasible even on mobile devices, thereby enhancing telemedicine and clinical workflows.

Method: An FPGA-based neural network is integrated to process MRF data, enabling rapid parameter reconstruction. The NN training is claimed to be significantly faster than conventional CPU-based methods, taking only 200 seconds.

Result: The FPGA-based solution demonstrated much faster neural network training and real-time parameter reconstruction for brain imaging, achieving a training speed up to 250 times faster than CPU-based methods.

Conclusion: This approach could revolutionize clinical workflows by enabling real-time quantitative brain imaging on portable devices, facilitating immediate decision-making in both hospitals and telemedicine scenarios.

Abstract: Magnetic Resonance Fingerprinting (MRF) is a fast quantitative MR Imaging
technique that provides multi-parametric maps with a single acquisition. Neural
Networks (NNs) accelerate reconstruction but require significant resources for
training. We propose an FPGA-based NN for real-time brain parameter
reconstruction from MRF data. Training the NN takes an estimated 200 seconds,
significantly faster than standard CPU-based training, which can be up to 250
times slower. This method could enable real-time brain analysis on mobile
devices, revolutionizing clinical decision-making and telemedicine.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [20] [Efficient Multilingual ASR Finetuning via LoRA Language Experts](https://arxiv.org/abs/2506.21555)
*Jiahong Li,Yiwen Shao,Jianheng Zhuo,Chenda Li,Liliang Tang,Dong Yu,Yanmin Qian*

Main category: cs.CL

TL;DR: The paper suggests an efficient fine-tuning framework for improving multilingual ASR systems using LoRA language experts, demonstrating notable performance improvements.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of multilinguality in automatic speech recognition, where language interference makes it difficult for models to effectively handle multiple languages.

Method: The paper introduces a fine-tuning framework leveraging LoRA-based language experts on Whisper, employing expert fusion or knowledge distillation techniques.

Result: The proposed method achieves approximately 10% and 15% relative performance gains in language-aware and language-agnostic ASR scenarios, respectively.

Conclusion: The proposed approach enhances multilingual ASR by improving recognition performance in both language-aware and language-agnostic settings, offering a solution to multilingual interference issues.

Abstract: Recent advancements in deep learning have significantly enhanced multilingual
automatic speech recognition (ASR) due to the development of advanced model
architectures and available large-scale multilingual datasets. Despite that,
multilingual ASR still suffers from the curse of multilinguality in that
different languages tend to interfere with each other, making it difficult for
the ASR model to identify multiple languages effectively while sharing model
capacity across them. This paper proposes an efficient finetuning framework for
customized multilingual ASR via prepared LoRA language experts based on
Whisper. Through LoRA expert fusion or knowledge distillation, our approach
achieves better recognition performance on target languages than standard
fine-tuning methods. Experimental results demonstrate that the proposed models
yield approximately 10\% and 15\% relative performance gains in language-aware
and language-agnostic scenarios, respectively.

</details>


### [21] [VAT-KG: Knowledge-Intensive Multimodal Knowledge Graph Dataset for Retrieval-Augmented Generation](https://arxiv.org/abs/2506.21556)
*Hyeongcheol Park,MinHyuk Jang,Ha Dam Baek,Gyusam Chang,Jiyoung Seo,Jiwan Park,Hogun Park,Sangpil Kim*

Main category: cs.CL

TL;DR: The paper introduces VAT-KG, a detailed and concept-centric multimodal knowledge graph integrating visual, audio, and text information for improved multimodal reasoning and retrieval-augmented generation.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal knowledge graphs are limited in scope, often outdated, and support only a narrow range of modalities, which limits their usability for advanced multimodal tasks.

Method: The researchers developed VAT-KG with a rigorous pipeline to align cross-modal data and rich semantic information, enabling automatic generation from multimodal datasets and supporting concept-level queries across diverse modalities.

Result: VAT-KG demonstrated effectiveness in boosting the performance of Multimodal Large Language Models in question-answering tasks across various modalities, showcasing its utility in multimodal reasoning.

Conclusion: VAT-KG is a transformative step in the development of rich and extensible multimodal knowledge graphs, empowering MLLMs with more comprehensive and aligned multimodal knowledge.

Abstract: Multimodal Knowledge Graphs (MMKGs), which represent explicit knowledge
across multiple modalities, play a pivotal role by complementing the implicit
knowledge of Multimodal Large Language Models (MLLMs) and enabling more
grounded reasoning via Retrieval Augmented Generation (RAG). However, existing
MMKGs are generally limited in scope: they are often constructed by augmenting
pre-existing knowledge graphs, which restricts their knowledge, resulting in
outdated or incomplete knowledge coverage, and they often support only a narrow
range of modalities, such as text and visual information. These limitations
reduce their extensibility and applicability to a broad range of multimodal
tasks, particularly as the field shifts toward richer modalities such as video
and audio in recent MLLMs. Therefore, we propose the Visual-Audio-Text
Knowledge Graph (VAT-KG), the first concept-centric and knowledge-intensive
multimodal knowledge graph that covers visual, audio, and text information,
where each triplet is linked to multimodal data and enriched with detailed
descriptions of concepts. Specifically, our construction pipeline ensures
cross-modal knowledge alignment between multimodal data and fine-grained
semantics through a series of stringent filtering and alignment steps, enabling
the automatic generation of MMKGs from any multimodal dataset. We further
introduce a novel multimodal RAG framework that retrieves detailed
concept-level knowledge in response to queries from arbitrary modalities.
Experiments on question answering tasks across various modalities demonstrate
the effectiveness of VAT-KG in supporting MLLMs, highlighting its practical
value in unifying and leveraging multimodal knowledge.

</details>


### [22] [FloorPlan-DeepSeek (FPDS): A multimodal approach to floorplan generation using vector-based next room prediction](https://arxiv.org/abs/2506.21562)
*Jun Yin,Pengyu Zeng,Jing Zhong,Peilin Li,Miao Zhang,Ran Luo,Shuai Lu*

Main category: cs.CL

TL;DR: The paper introduces a 'next room prediction' model for floor plan generation that mimics incremental architectural workflows and shows competitive results in text-to-floorplan tasks.


<details>
  <summary>Details</summary>
Motivation: The authors aim to bridge the gap between real-world iterative architectural design workflows and existing generative models that lack an incremental approach.

Method: Inspired by large language models' autoregressive mechanisms, the authors propose an innovative method for generating floor plans room-by-room called 'next room prediction.'

Result: The experimental evaluation reveals that FPDS performs competitively when compared to diffusion models and Tell2Design in generating floor plans from textual descriptions.

Conclusion: The findings suggest that FPDS could be a valuable tool to enhance intelligent architectural design processes by aligning better with real-world design workflows.

Abstract: In the architectural design process, floor plan generation is inherently
progressive and iterative. However, existing generative models for floor plans
are predominantly end-to-end generation that produce an entire pixel-based
layout in a single pass. This paradigm is often incompatible with the
incremental workflows observed in real-world architectural practice. To address
this issue, we draw inspiration from the autoregressive 'next token prediction'
mechanism commonly used in large language models, and propose a novel 'next
room prediction' paradigm tailored to architectural floor plan modeling.
Experimental evaluation indicates that FPDS demonstrates competitive
performance in comparison to diffusion models and Tell2Design in the
text-to-floorplan task, indicating its potential applicability in supporting
future intelligent architectural design.

</details>


### [23] [Debunk and Infer: Multimodal Fake News Detection via Diffusion-Generated Evidence and LLM Reasoning](https://arxiv.org/abs/2506.21557)
*Kaiying Yan,Moyang Liu,Yukun Liu,Ruibo Fu,Zhengqi Wen,Jianhua Tao,Xuefei Liu*

Main category: cs.CL

TL;DR: The paper introduces DIFND, a framework that boosts fake news detection by integrating generative debunking models and reasoning in a unified structure, highlighting its effectiveness and interpretability.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenge of detecting fake news across multimedia platforms, focusing on improving both accuracy and interpretability of detection methods.

Method: DIFND combines conditional diffusion models for generating debunking evidence with a multi-agent multimodal large language model system for reasoning-based verification. It uses multimodal analysis and a chain-of-debunk strategy for final judgment.

Result: Experiments on FakeSV and FVC datasets demonstrate that DIFND surpasses existing methods in detection accuracy and produces trustworthy decisions.

Conclusion: The integrated DIFND framework successfully improves fake news detection by blending generative and reasoning capabilities, offering both enhanced performance and interpretability.

Abstract: The rapid spread of fake news across multimedia platforms presents serious
challenges to information credibility. In this paper, we propose a
Debunk-and-Infer framework for Fake News Detection(DIFND) that leverages
debunking knowledge to enhance both the performance and interpretability of
fake news detection. DIFND integrates the generative strength of conditional
diffusion models with the collaborative reasoning capabilities of multimodal
large language models (MLLMs). Specifically, debunk diffusion is employed to
generate refuting or authenticating evidence based on the multimodal content of
news videos, enriching the evaluation process with diverse yet semantically
aligned synthetic samples. To improve inference, we propose a chain-of-debunk
strategy where a multi-agent MLLM system produces logic-grounded,
multimodal-aware reasoning content and final veracity judgment. By jointly
modeling multimodal features, generative debunking cues, and reasoning-rich
verification within a unified architecture, DIFND achieves notable improvements
in detection accuracy. Extensive experiments on the FakeSV and FVC datasets
show that DIFND not only outperforms existing approaches but also delivers
trustworthy decisions.

</details>


### [24] [Bench to the Future: A Pastcasting Benchmark for Forecasting Agents](https://arxiv.org/abs/2506.21558)
*FutureSearch,:,Jack Wildman,Nikos I. Bosse,Daniel Hnyk,Peter Mühlbacher,Finn Hambly,Jon Evans,Dan Schwarz,Lawrence Phillips*

Main category: cs.CL

TL;DR: The paper introduces Bench To the Future (BTF), a "pastcasting" benchmark that evaluates LLMs' forecasting abilities using past event data.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the lack of realistic, hermetic, and repeatable benchmarks for evaluating the forecasting capabilities of large language models (LLMs).

Method: The paper proposes a "pastcasting" benchmark system called BTF, where LLMs forecast past events using a curated offline corpus of web data and evaluates them by comparing predictions to known outcomes.

Result: The study shows that pastcasting with BTF produces results similar to real-time forecasts using the internet. They benchmark Claude 4 and other LLMs, demonstrating improved forecasting capabilities over time.

Conclusion: BTF serves as an effective, dynamic, and scalable benchmark for tracking the progress of LLM forecasting ability, and the authors encourage its adoption by researchers.

Abstract: Forecasting is a challenging task that offers a clearly measurable way to
study AI systems. Forecasting requires a large amount of research on the
internet, and evaluations require time for events to happen, making the
development of forecasting benchmarks challenging. To date, no forecasting
benchmark provides a realistic, hermetic, and repeatable environment for LLM
forecasters. We introduce Bench To the Future (BTF), a "pastcasting" benchmark
with hundreds of high-quality questions for which the resolution is already
known. Each question is accompanied by a large offline corpus of tens of
thousands of relevant web pages, enabling a way to elicit realistic "forecasts"
on past events from LLMs. Results suggest that our pastcasting environment can
produce results comparable to those based on forecasts using the internet on
at-the-time unresolved questions. We show results benchmarking agent and
chain-of-thought forecasting approaches using several LLMs, including the
recently-released Claude 4 models, and demonstrate BTF's ability to track
steady forecasting capability progress over time. We intend this to be a living
benchmark, with new questions added continually to account for increasing
training data cutoff dates. We invite researchers to contact us at
hello@futuresearch.ai to utilize our benchmark or tooling for their own
research.

</details>


### [25] [GraphLAMA: Enabling Efficient Adaptation of Graph Language Models with Limited Annotations](https://arxiv.org/abs/2506.21559)
*Junze Chen,Cheng Yang,Shujie Li,Zhiqiang Zhang,Yawen Li,Junping Du,Chuan Shi*

Main category: cs.CL

TL;DR: GraphLAMA enhances graph language model performance by introducing an efficient parameter adaptation stage, achieving better accuracy and faster inference compared to in-context learning.


<details>
  <summary>Details</summary>
Motivation: Recent Graph Language Models (GLMs) face limitations in in-context learning due to fixed parameters and inefficiency with long contexts, and collecting data for instruction tuning can be challenging in real-world scenarios.

Method: GraphLAMA combines a graph neural network (GNN) for transforming graph nodes into language token representation, pre-trains parameters on general tasks, and fine-tunes selected parameters with a few labeled examples in an adaptation stage.

Result: GraphLAMA demonstrated state-of-the-art performance in tasks such as few/zero-shot node classification and summary generation, achieving a 4.91% absolute improvement in accuracy and up to 10x faster inference compared to in-context learning.

Conclusion: The method effectively balances performance and resource efficiency in GLMs, proving its suitability for scenarios with limited labeled data and unseen graph tasks.

Abstract: Large language models (LLMs) have demonstrated their strong capabilities in
various domains, and have been recently integrated for graph analysis as graph
language models (GLMs). With LLMs as the predictor, some GLMs can interpret
unseen tasks described by natural language, and learn from a few examples in
the prompts without parameter tuning, known as in-context learning (ICL).
Another subset of GLMs utilizes abundant training labels to enhance model
performance, known as instruction tuning. However, we argue that ICL on graphs
has effectiveness issues due to fixed parameters and efficiency issues due to
long context. Meanwhile, the large amount of labeled data required for
instruction tuning can be difficult to obtain in real-world scenarios. To this
end, we aim to introduce an extra parameter adaptation stage that can
efficiently tailor GLMs to an unseen graph and task with only a few labeled
examples, in exchange for better prediction accuracy and faster inference
speed. For implementation, in this paper we propose GraphLAMA method, with its
model backbone and learning schemes specialized for efficient tuning and
inference. Specifically, for model backbone, we use a graph neural network
(GNN) with several well-designed components to transform nodes into the
representation space of LLM tokens. Task instructions can then be represented
as a mixture of node and language tokens. In the pre-training stage, model
parameters except the LLM will be trained with different tasks to capture
general knowledge. In the adaptation stage, only a few pre-trained parameters
will be updated based on few-shot examples. Extensive experiments on
few/zero-shot node classification and summary generation show that our proposed
GraphLAMA achieves state-of-the-art performance with 4.91% absolution
improvement in accuracy. Compared with ICL, our inference speed can be 10 times
faster under 5-shot setting.

</details>


### [26] [Reinforcement Learning Fine-Tuning of Language Model for Instruction Following and Math Reasoning](https://arxiv.org/abs/2506.21560)
*Yifu Han,Geo Zhang*

Main category: cs.CL

TL;DR: The study evaluates RL fine-tuning techniques on a compact model (Qwen2.5-0.5B Base) for instruction following and mathematical reasoning tasks, highlighting performance, trade-offs, and practical strategies.


<details>
  <summary>Details</summary>
Motivation: To explore how reinforcement learning fine-tuning methods can optimize small-scale language models for difficult tasks, especially instruction following and math reasoning.

Method: The authors tested supervised fine-tuning (SFT), Direct Preference Optimization (DPO), and Reinforce Leave-One-Out (RLOO) on Qwen2.5-0.5B Base. Additionally, synthetic data and external verifiers were used to enhance math reasoning capabilities.

Result: RLOO achieved the best alignment with DeBERTa reward modeling, while DPO showed consistent performance. Synthetic data and inference tools notably improved math reasoning accuracy.

Conclusion: The findings demonstrate how to effectively fine-tune lightweight language models, combining methods for alignment and reasoning with inference-time techniques for better accuracy.

Abstract: This study investigates the effectiveness of reinforcement learning (RL)
fine-tuning techniques on a compact language model (Qwen2.5-0.5B Base) for two
challenging tasks: instruction following and mathematical reasoning. We compare
supervised fine-tuning (SFT), Direct Preference Optimization (DPO) using
preference-labeled data, and Reinforce Leave-One-Out (RLOO) with reward models.
Our experiments show that RLOO with DeBERTa reward modeling achieves the best
alignment, while DPO provides strong and consistent results. For math reasoing
tasks, synthetic data augmentation and best-of-N sampling with an external
verifier significantly improve accuracy, showing the potential of combining
fine-tuning with inference-time tools. This study highlights key trade-offs and
practical strategies for training lightweight, task-aligned small-scale
language models.

</details>


### [27] [Reasoning Isn't Enough: Examining Truth-Bias and Sycophancy in LLMs](https://arxiv.org/abs/2506.21561)
*Emilio Barkett,Olivia Long,Madhavendra Thakur*

Main category: cs.CL

TL;DR: This study analyzes how well eight large language models (LLMs) detect truth versus deception, finding reasoning models slightly better than non-reasoning ones but still less reliable than humans.


<details>
  <summary>Details</summary>
Motivation: Investigate the inadequacies of LLMs in distinguishing true from false information, especially given their increasing role in critical areas like fact-checking and decision-making.

Method: The researchers evaluated eight LLMs using 4,800 veracity judgments across different prompts, contrasting reasoning and non-reasoning LLMs.

Result: Reasoning models showed reduced truth-bias compared to non-reasoning ones but failed to match human capabilities. Certain advanced models exhibited sycophantic tendencies, excelling in recognizing truth while struggling with deception.

Conclusion: Capability advancements alone are insufficient to overcome core challenges in veracity detection within LLMs.

Abstract: Despite their widespread use in fact-checking, moderation, and high-stakes
decision-making, large language models (LLMs) remain poorly understood as
judges of truth. This study presents the largest evaluation to date of LLMs'
veracity detection capabilities and the first analysis of these capabilities in
reasoning models. We had eight LLMs make 4,800 veracity judgments across
several prompts, comparing reasoning and non-reasoning models. We find that
rates of truth-bias, or the likelihood to believe a statement is true,
regardless of whether it is actually true, are lower in reasoning models than
in non-reasoning models, but still higher than human benchmarks. Most
concerning, we identify sycophantic tendencies in several advanced models
(o4-mini and GPT-4.1 from OpenAI, R1 from DeepSeek), which displayed an
asymmetry in detection accuracy, performing well in truth accuracy but poorly
in deception accuracy. This suggests that capability advances alone do not
resolve fundamental veracity detection challenges in LLMs.

</details>


### [28] [FormosanBench: Benchmarking Low-Resource Austronesian Languages in the Era of Large Language Models](https://arxiv.org/abs/2506.21563)
*Kaiying Kevin Lin,Hsiyu Chen,Haopeng Zhang*

Main category: cs.CL

TL;DR: This paper introduces FORMOSANBENCH, a benchmark for evaluating Large Language Models on three endangered Formosan languages across three NLP tasks, and finds significant underperformance compared to high-resource languages.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the underexplored capabilities of LLMs in low-resource and endangered languages, specifically Formosan languages, and the need for inclusive technologies.

Method: The authors developed FORMOSANBENCH to evaluate three Formosan languages – Atayal, Amis, and Paiwan – across machine translation, automatic speech recognition, and text summarization, using zero-shot, 10-shot, and fine-tuned settings.

Result: The study finds that LLMs underperform significantly on Formosan languages across all tasks, and few-shot learning or fine-tuning yields minimal improvements.

Conclusion: The paper emphasizes the need for developing inclusive NLP technologies for underrepresented and endangered languages and releases resources to stimulate further research.

Abstract: While large language models (LLMs) have demonstrated impressive performance
across a wide range of natural language processing (NLP) tasks in high-resource
languages, their capabilities in low-resource and minority languages remain
significantly underexplored. Formosan languages -- a subgroup of Austronesian
languages spoken in Taiwan -- are both linguistically rich and endangered,
largely due to the sociolinguistic dominance of Mandarin. In this work, we
introduce FORMOSANBENCH, the first benchmark for evaluating LLMs on
low-resource Austronesian languages. It covers three endangered Formosan
languages: Atayal, Amis, and Paiwan, across three core NLP tasks: machine
translation, automatic speech recognition (ASR), and text summarization. We
assess model performance in zero-shot, 10-shot, and fine-tuned settings using
FORMOSANBENCH. Our results reveal a substantial performance gap between
high-resource and Formosan languages. Existing LLMs consistently underperform
across all tasks, with 10-shot learning and fine-tuning offering only limited
improvements. These findings underscore the urgent need for more inclusive NLP
technologies that can effectively support endangered and underrepresented
languages. We release our datasets and code to facilitate future research in
this direction.

</details>


### [29] [Team QUST at SemEval-2025 Task 10: Evaluating Large Language Models in Multiclass Multi-label Classification of News Entity Framing](https://arxiv.org/abs/2506.21564)
*Jiyan Liu,Youzheng Liu,Taihang Wang,Xiaoman Xu,Yimin Wang,Ye Jiang*

Main category: cs.CL

TL;DR: The paper proposes a three-stage retrieval framework for fact-checked claim retrieval, achieving notable rankings in the SemEval-2025 Task 7.


<details>
  <summary>Details</summary>
Motivation: To develop an effective retrieval framework specifically optimized for fact-checked claim retrieval in multilingual scenarios.

Method: The framework includes three stages: candidate retrieval using the best-performing model, re-ranking candidates with multiple models, and using weighted voting for final retrieval outcomes.

Result: The approach achieved 5th place in the monolingual track and 7th place in the crosslingual track of SemEval-2025 Task 7.

Conclusion: Their multi-stage framework demonstrates its effectiveness in fact-checked claim retrieval, and they share their findings and code openly.

Abstract: This paper describes the participation of QUST_NLP in the SemEval-2025 Task
7. We propose a three-stage retrieval framework specifically designed for
fact-checked claim retrieval. Initially, we evaluate the performance of several
retrieval models and select the one that yields the best results for candidate
retrieval. Next, we employ multiple re-ranking models to enhance the candidate
results, with each model selecting the Top-10 outcomes. In the final stage, we
utilize weighted voting to determine the final retrieval outcomes. Our approach
achieved 5th place in the monolingual track and 7th place in the crosslingual
track. We release our system code at:
https://github.com/warmth27/SemEval2025_Task7.

</details>


### [30] [A Multi-Agent Probabilistic Inference Framework Inspired by Kairanban-Style CoT System with IdoBata Conversation for Debiasing](https://arxiv.org/abs/2506.21565)
*Takato Ueno,Keito Inoshita*

Main category: cs.CL

TL;DR: This study introduces a multi-agent system based on Japanese kairanban culture to integrate large language models (LLMs) for bias mitigation and probabilistic sentiment predictions.


<details>
  <summary>Details</summary>
Motivation: Inspired by traditional Japanese communication practices, the study aims to improve sentiment analysis by addressing issues like bias and explainability using collaborative dialogue among LLMs.

Method: The researchers propose a framework (KCS+IBC) that involves LLMs sharing sequential predictions and incorporating casual dialogue sessions into the inference process. This approach combines formal inference with individual model perspectives and probabilistic outputs.

Result: The KCS framework achieves comparable sentiment prediction accuracy to a single LLM, while KCS+IBC improves diversity and balance in predictions, as indicated by reduced entropy and increased variance.

Conclusion: The integration of collaborative dialogue and probabilistic predictions offers promise for reducing bias in sentiment analysis, laying the groundwork for more robust and explainable systems.

Abstract: Japan's kairanban culture and idobata conversations have long functioned as
traditional communication practices that foster nuanced dialogue among
community members and contribute to the formation of social balance. Inspired
by these information exchange processes, this study proposes a multi-agent
inference framework (KCS+IBC) that integrates multiple large language models
(LLMs) to achieve bias mitigation, improved explainability, and probabilistic
prediction in sentiment analysis. In addition to sequentially sharing
prediction results, the proposed method incorporates a mid-phase casual
dialogue session to blend formal inference with individual perspectives and
introduces probabilistic sentiment prediction. Experimental results show that
KCS achieves accuracy comparable to that of a single LLM across datasets, while
KCS+IBC exhibits a consistent decrease in entropy and a gradual increase in
variance during the latter stages of inference, suggesting the framework's
ability to balance aggregation and diversity of predictions. Future work will
quantitatively assess the impact of these characteristics on bias correction
and aim to develop more advanced sentiment analysis systems.

</details>


### [31] [The Saturation Point of Backtranslation in High Quality Low Resource English Gujarati Machine Translation](https://arxiv.org/abs/2506.21566)
*Arwa Arif*

Main category: cs.CL

TL;DR: This study investigates backtranslation's effectiveness in English-Gujarati translation, finding no improvements in translation performance with additional synthetic data.


<details>
  <summary>Details</summary>
Motivation: The paper aims to determine the role of backtranslation in enhancing machine translation, especially in low-resource language pairs like English-Gujarati.

Method: The authors used MBART50, a pretrained multilingual model, to create synthetic data through backtranslation, which they then added to the primary dataset for testing its impact on translation metrics.

Result: Augmenting the parallel corpus with backtranslated data did not improve BLEU scores and, in some cases, slightly worsened performance across multiple evaluation metrics.

Conclusion: Backtranslation might have diminishing returns in certain low-resource conditions, highlighting the need for alternative approaches in similar settings.

Abstract: Backtranslation BT is widely used in low resource machine translation MT to
generate additional synthetic training data using monolingual corpora. While
this approach has shown strong improvements for many language pairs, its
effectiveness in high quality, low resource settings remains unclear. In this
work, we explore the effectiveness of backtranslation for English Gujarati
translation using the multilingual pretrained MBART50 model. Our baseline
system, trained on a high quality parallel corpus of approximately 50,000
sentence pairs, achieves a BLEU score of 43.8 on a validation set. We augment
this data with carefully filtered backtranslated examples generated from
monolingual Gujarati text. Surprisingly, adding this synthetic data does not
improve translation performance and, in some cases, slightly reduces it. We
evaluate our models using multiple metrics like BLEU, ChrF++, TER, BLEURT and
analyze possible reasons for this saturation. Our findings suggest that
backtranslation may reach a point of diminishing returns in certain
low-resource settings and we discuss implications for future research.

</details>


### [32] [BioPars: A Pretrained Biomedical Large Language Model for Persian Biomedical Text Mining](https://arxiv.org/abs/2506.21567)
*Baqer M. Merzah,Tania Taami,Salman Asoudeh,Amir reza Hossein pour,Saeed Mirzaee,Amir Ali Bengari*

Main category: cs.CL

TL;DR: The study introduces BIOPARS-BENCH and BioParsQA datasets, and evaluates the BioPars model for its ability to acquire, interpret, and apply medical knowledge using LLMs like ChatGPT, Llama, and Galactica.


<details>
  <summary>Details</summary>
Motivation: To address the need for specialized evaluation frameworks and tools for LLMs in the bioinformatics domain, particularly in Persian medical QA contexts.

Method: The paper introduces a dataset named BIOPARS-BENCH, a Persian QA dataset named BioParsQA, and a model called BioPars. It evaluates the model performance using metrics like ROUGE-L, BERTScore, MoverScore, and BLEURT.

Result: The BioPars model achieved superior performance metrics compared to competing models, including a ROUGE-L score of 29.99, BERTScore of 90.87, MoverScore of 60.43, and BLEURT of 50.78.

Conclusion: BioPars demonstrates promise in addressing the limitations of LLMs for complex bioinformatics tasks, particularly in Persian medical contexts. However, further fine-tuning is necessary and the project is ongoing.

Abstract: Large Language Models (LLMs) have recently gained attention in the life
sciences due to their capacity to model, extract, and apply complex biological
information. Beyond their classical use as chatbots, these systems are
increasingly used for complex analysis and problem-solving in specialized
fields, including bioinformatics. First, we introduce BIOPARS-BENCH, a dataset
from over 10,000 scientific articles, textbooks, and medical websites.
BioParsQA was also introduced to evaluate the proposed model, which consists of
5,231 Persian medical questions and answers. This study then introduces
BioPars, a simple but accurate measure designed to assess LLMs for three main
abilities: acquiring subject-specific knowledge, interpreting and synthesizing
such knowledge, and demonstrating proper evidence. Comparing ChatGPT, Llama,
and Galactica, our study highlights their ability to remember and retrieve
learned knowledge but also reveals shortcomings in addressing higher-level,
real-world questions and fine-grained inferences. These findings indicate the
need for further fine-tuning to address the capabilities of LLM in
bioinformatics tasks. To our knowledge, BioPars is the first application of LLM
in Persian medical QA, especially for generating long answers. Evaluation of
four selected medical QA datasets shows that BioPars has achieved remarkable
results compared to comparative approaches. The model on BioParsQA achieved a
ROUGE-L score of 29.99, which is an improvement over GPT-4 1.0. The model
achieved a BERTScore of 90.87 with the MMR method. The MoverScore and BLEURT
values were also higher in this model than the other three models. In addition,
the reported scores for the model are MoverScore=60.43 and BLEURT=50.78.
BioPars is an ongoing project and all resources related to its development will
be made available via the following GitHub repository:
https://github.com/amirap80/BioPars.

</details>


### [33] [Assessing RAG and HyDE on 1B vs. 4B-Parameter Gemma LLMs for Personal Assistants Integretion](https://arxiv.org/abs/2506.21568)
*Andrejs Sorstkins*

Main category: cs.CL

TL;DR: This paper evaluates two augmentation strategies, RAG and HyDE, on small language models for privacy-focused personal assistants, finding that RAG performs better while HyDE adds computational overhead.


<details>
  <summary>Details</summary>
Motivation: The motivation is to evaluate resource-efficient methods to overcome challenges in deploying large language models in edge and privacy-sensitive settings, specifically for personal assistants.

Method: The study tested two augmentation strategies--RAG and HyDE--on compact 1B and 4B parameter Gemma LLMs, using tools like MongoDB, Qdrant, FastAPI, LangChain, and React.js for end-to-end deployment.

Result: RAG improved latency by up to 17% and eliminated factual hallucinations, while HyDE enhanced semantic performance for complex prompts but introduced significant computational cost and hallucination rates.

Conclusion: RAG is the more practical choice for resource-constrained, on-device personal assistants powered by small LLMs, while HyDE's trade-offs make it less suitable in these settings.

Abstract: Resource efficiency is a critical barrier to deploying large language models
(LLMs) in edge and privacy-sensitive applications. This study evaluates the
efficacy of two augmentation strategies--Retrieval-Augmented Generation (RAG)
and Hypothetical Document Embeddings (HyDE)--on compact Gemma LLMs of 1 billion
and 4 billion parameters, within the context of a privacy-first personal
assistant. We implement short-term memory via MongoDB and long-term semantic
storage via Qdrant, orchestrated through FastAPI and LangChain, and expose the
system through a React.js frontend. Across both model scales, RAG consistently
reduces latency by up to 17\% and eliminates factual hallucinations when
responding to user-specific and domain-specific queries. HyDE, by contrast,
enhances semantic relevance--particularly for complex physics prompts--but
incurs a 25--40\% increase in response time and a non-negligible hallucination
rate in personal-data retrieval. Comparing 1 B to 4 B models, we observe that
scaling yields marginal throughput gains for baseline and RAG pipelines, but
magnifies HyDE's computational overhead and variability. Our findings position
RAG as the pragmatic choice for on-device personal assistants powered by
small-scale LLMs.

</details>


### [34] [Hybrid-NL2SVA: Integrating RAG and Finetuning for LLM-based NL2SVA](https://arxiv.org/abs/2506.21569)
*Weihua Xiao,Derek Ekberg,Siddharth Garg,Ramesh Karri*

Main category: cs.CL

TL;DR: The paper addresses the challenge of translating natural language descriptions into SystemVerilog Assertions (NL2SVA) by proposing a specialized retrieval-augmented generation (RAG) framework and a fine-tuned dataset.


<details>
  <summary>Details</summary>
Motivation: Manually creating SystemVerilog Assertions from natural language property descriptions is challenging, time-consuming, and prone to errors.

Method: A retrieval-augmented generation (RAG) framework is combined with a synthetic fine-tuning dataset that includes prompt-guided explanations, enabling layer-by-layer SVA construction and supervised fine-tuning.

Result: The proposed methods show significant improvements, with a 58.42% increase in functionality-matched SVAs using the RAG framework and a 59.05% improvement with the fine-tuned Qwen2.5-Coder model over its base version.

Conclusion: The framework and synthetic dataset substantially improve LLM performance in translating natural language into SVAs, demonstrating the potential of domain-specific innovations in automated hardware verification.

Abstract: SystemVerilog Assertions (SVAs) are critical for verifying the correctness of
hardware designs, but manually writing them from natural language property
descriptions, i.e., NL2SVA, remains a labor-intensive and error-prone task.
Recent advances in large language models (LLMs) offer opportunities to automate
this translation. However, existing models still struggle with understanding
domain-specific syntax and semantics. To enhance LLM performance in NL2SVA, we
propose a customized retrieval-augmented generation (RAG) framework and a
synthetic fine-tuning dataset that together improve LLM's performance. To
further improve lightweight models over NL2SVA, our fine-tuning dataset
provides prompt-guided explanations that teach LLMs the layer-by-layer
construction process of concurrent SVAs, enabling supervised fine-tuning that
greatly improves syntax and functionality accuracy. To evaluate the performance
of LLMs over NL2SVA, we construct the largest evaluation dataset for NL2SVA,
comprising 40 Verilog designs and 229 formally verified SVAs with detailed
annotations. Experimental results show that our customized RAG framework
increases the number of functionality matched SVAs by 58.42% over GPT-4o-mini,
while Qwen2.5-Coder-7B-Instruct fine-tuned on our fine-tuning dataset and
integrated with HybridRetrieval achieves a 59.05% over the base Qwen model.

</details>


### [35] [Random Initialization Can't Catch Up: The Advantage of Language Model Transfer for Time Series Forecasting](https://arxiv.org/abs/2506.21570)
*Roland Riachi,Kashif Rasul,Arjun Ashok,Prateek Humane,Alexis Roger,Andrew R. Williams,Yuriy Nevmyvaka,Irina Rish*

Main category: cs.CL

TL;DR: The paper studies the adaptation of pre-trained language models (LMs) for time series forecasting in low-data settings, analyzing factors like post-training, tokenization, and model size.


<details>
  <summary>Details</summary>
Motivation: The authors aim to explore how pre-trained LMs can be effectively leveraged to improve time series forecasting when data is limited.

Method: The study investigates different design choices such as upstream post-training, tokenizer styles, and LM backbone sizes to analyze their effects on validation loss.

Result: The study finds that certain design choices yield significantly better validation loss and that LMs exhibit a non-vanishing transfer gap even after random models converge.

Conclusion: The findings highlight the potential of compute-efficient LM training for time series and encourage further research into modality-agnostic data properties for forecasting.

Abstract: Recent works have demonstrated the effectiveness of adapting pre-trained
language models (LMs) for forecasting time series in the low-data regime. We
build upon these findings by analyzing the effective transfer from language
models to time series forecasting under various design choices including
upstream post-training, time series tokenizer and language backbone size. In
the low-data regime, these design choices have a significant impact on the
validation loss, with clear-cut choices that outperform others. Contrary to
Hernandez et al. (2021), we observe that the validation loss of the LMs
continues to smoothly decrease long after the validation loss of the randomly
initialized models has converged, leading to a non-vanishing transfer gap that
holds across design choices. These findings not only help shed light on the
effective use of compute-efficient training for time series, but also open the
way for the study of modality-agnostic properties of data distributions
leveraged by these models.

</details>


### [36] [Towards Understanding the Cognitive Habits of Large Reasoning Models](https://arxiv.org/abs/2506.21571)
*Jianshuo Dong,Yujia Fu,Chuanrui Hu,Chao Zhang,Han Qiu*

Main category: cs.CL

TL;DR: The paper introduces CogTest, a benchmark to analyze if Large Reasoning Models (LRMs) exhibit human-like cognitive habits, revealing adaptive and task-specific deployment.


<details>
  <summary>Details</summary>
Motivation: The study aims to explore whether LRMs exhibit human-like cognitive habits via their reasoning chains and understand their behavioral patterns better.

Method: CogTest benchmark evaluates LRMs on 16 cognitive habits across 25 diverse tasks using an evidence-first extraction method.

Result: LRMs adaptively exhibit human-like cognitive habits and show task-specific deployment, with observations of inter-family similarities and differences.

Conclusion: Studying LRMs' reasoning patterns provides valuable insights into their behavior and missteps, particularly in safety-critical contexts.

Abstract: Large Reasoning Models (LRMs), which autonomously produce a reasoning Chain
of Thought (CoT) before producing final responses, offer a promising approach
to interpreting and monitoring model behaviors. Inspired by the observation
that certain CoT patterns -- e.g., ``Wait, did I miss anything?'' --
consistently emerge across tasks, we explore whether LRMs exhibit human-like
cognitive habits. Building on Habits of Mind, a well-established framework of
cognitive habits associated with successful human problem-solving, we introduce
CogTest, a principled benchmark designed to evaluate LRMs' cognitive habits.
CogTest includes 16 cognitive habits, each instantiated with 25 diverse tasks,
and employs an evidence-first extraction method to ensure reliable habit
identification. With CogTest, we conduct a comprehensive evaluation of 16
widely used LLMs (13 LRMs and 3 non-reasoning ones). Our findings reveal that
LRMs, unlike conventional LLMs, not only exhibit human-like habits but also
adaptively deploy them according to different tasks. Finer-grained analyses
further uncover patterns of similarity and difference in LRMs' cognitive habit
profiles, particularly certain inter-family similarity (e.g., Qwen-3 models and
DeepSeek-R1). Extending the study to safety-related tasks, we observe that
certain habits, such as Taking Responsible Risks, are strongly associated with
the generation of harmful responses. These findings suggest that studying
persistent behavioral patterns in LRMs' CoTs is a valuable step toward deeper
understanding of LLM misbehavior. The code is available at:
https://github.com/jianshuod/CogTest.

</details>


### [37] [Aligning MLLM Benchmark With Human Preferences via Structural Equation Modeling](https://arxiv.org/abs/2506.21572)
*Tianyu. Zou,Shengwu. Xiong,Ruilin. Yao,Jirui. Huang,Yi. Rong,Yaxiong. Chen,Shili. Xiong,Cong. Wang*

Main category: cs.CL

TL;DR: This paper proposes a new benchmarking framework for multimodal large language models (MLLMs) based on Structural Equation Modeling (SEM) and Piaget’s cognitive development theory.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the lack of structured, interpretable, and theoretically grounded designs for evaluating MLLMs, which existing benchmarks fail to provide due to heuristic-based groupings with overlapping and redundant metrics.

Method: The paper introduces a framework based on Structural Equation Modeling combined with a capability hierarchy inspired by Piaget’s cognitive development theory (Perception, Memory, Reasoning). They reorganize existing benchmarks under this framework and construct a new benchmark, Gold.

Result: Experimental results show that the proposed benchmark has improved interpretability, reduced redundancy, and aligns better with cognitive consistency compared to existing benchmarks.

Conclusion: The new Gold benchmark provides enhanced diagnostic power and clearer cognitive evaluation, addressing key limitations of existing MLLM benchmarks.

Abstract: Evaluating multimodal large language models (MLLMs) remains a fundamental
challenge due to a lack of structured, interpretable, and theoretically
grounded benchmark designs. Existing benchmarks often adopt heuristic-based
task groupings with unclear cognitive targets, thus resulting in overlapping
abilities, redundant indicators, and limited diagnostic power. In this work, we
propose a novel framework for aligning MLLM benchmark based on Structural
Equation Modeling (SEM) to analyze and quantify the internal validity,
dimensional separability, and contribution of benchmark components. Motivated
by the observed limitations of current designs, we further introduce a novel
capability hierarchy grounded in Piagets theory of cognitive development,
dividing MLLM abilities into three hierarchical layers, i.e., Perception,
Memory, and Reasoning. We reorganize existing MLLM benchmarks under the
proposed framework and construct a new benchmark named Gold. Experimental
results demonstrate that the proposed benchmark exhibits stronger
interpretability, reduced indicator redundancy, and clearer cognitive
consistency compared to existing approaches.

</details>


### [38] [Instruction Learning Paradigms: A Dual Perspective on White-box and Black-box LLMs](https://arxiv.org/abs/2506.21573)
*Yanwei Ren,Liu Liu,Baosheng Yu,Jiayan Qiu,Quan Chen*

Main category: cs.CL

TL;DR: The paper presents a framework that combines black-box and white-box models to optimize instructions for LLMs, resulting in better performance across various tasks.


<details>
  <summary>Details</summary>
Motivation: Optimizing LLMs for diverse and complex tasks is challenging due to the high resource requirements of white-box models and financial costs of black-box models.

Method: A novel framework is introduced that integrates black-box models (for diverse instruction initialization) and white-box models (for interpretability) with a semantic similarity constraint for iterative instruction refinement.

Result: The framework consistently surpasses state-of-the-art benchmarks across tasks such as complex reasoning and cross-lingual generalization.

Conclusion: The proposed hybrid approach offers an efficient and scalable solution for instruction optimization, enabling diverse applications of LLMs in real-world scenarios.

Abstract: Optimizing instructions for large language models (LLMs) is critical for
harnessing their full potential in complex and diverse tasks. However, relying
solely on white-box approaches demands extensive computational resources and
offers limited representational capacity, while black-box models can incur
prohibitive financial costs. To address these challenges, we introduce a novel
framework that seamlessly merges the strengths of both paradigms. Black-box
models provide high-quality, diverse instruction initializations, and white-box
models supply fine-grained interpretability through hidden states and output
features. By enforcing a semantic similarity constraint, these components fuse
into a unified high-dimensional representation that captures deep semantic and
structural nuances, enabling an iterative optimization process to refine
instruction quality and adaptability. Extensive evaluations across a broad
spectrum of tasks-ranging from complex reasoning to cross-lingual
generalization-demonstrate that our approach consistently outperforms
state-of-the-art baselines. This fusion of black-box initialization with
advanced semantic refinement yields a scalable and efficient solution, paving
the way for next-generation LLM-driven applications in diverse real-world
scenarios. The source code will be released soon.

</details>


### [39] [MMLU-CF: A Contamination-free Multi-task Language Understanding Benchmark](https://arxiv.org/abs/2412.15194)
*Qihao Zhao,Yangyu Huang,Tengchao Lv,Lei Cui,Qinzheng Sun,Shaoguang Mao,Xin Zhang,Ying Xin,Qiufeng Yin,Scarlett Li,Furu Wei*

Main category: cs.CL

TL;DR: The paper introduces MMLU-CF, a contamination-free and challenging multiple-choice question benchmark to improve the reliability of large language model evaluations.


<details>
  <summary>Details</summary>
Motivation: The need to address benchmark contamination in current MCQ datasets like MMLU, which leads to unreliable evaluations of LLMs.

Method: The study develops MMLU-CF using three decontamination rules to avoid unintentional data leakage and divides the dataset into closed-source test and open-source validation sets to prevent malicious data leakage.

Result: Experimental results show that GPT-4o achieves only a 73.4% accuracy in 5-shot and 71.9% in 0-shot evaluations, demonstrating the improved rigor of the MMLU-CF benchmark.

Conclusion: MMLU-CF serves as a more reliable and rigorous benchmark for assessing LLMs' world knowledge without contamination, ensuring both transparency and evaluation integrity.

Abstract: Multiple-choice question (MCQ) datasets like Massive Multitask Language
Understanding (MMLU) are widely used to evaluate the commonsense,
understanding, and problem-solving abilities of large language models (LLMs).
However, the open-source nature of these benchmarks and the broad sources of
training data for LLMs have inevitably led to benchmark contamination,
resulting in unreliable evaluation results. To alleviate this issue, we propose
a contamination-free and more challenging MCQ benchmark called MMLU-CF. This
benchmark reassesses LLMs' understanding of world knowledge by averting both
unintentional and malicious data leakage. To avoid unintentional data leakage,
we source data from a broader domain and design three decontamination rules. To
prevent malicious data leakage, we divide the benchmark into validation and
test sets with similar difficulty and subject distributions. The test set
remains closed-source to ensure reliable results, while the validation set is
publicly available to promote transparency and facilitate independent
verification. Our evaluation of mainstream LLMs reveals that the powerful
GPT-4o achieves merely a 5-shot score of 73.4% and a 0-shot score of 71.9% on
the test set, which indicates the effectiveness of our approach in creating a
more rigorous and contamination-free evaluation standard. The GitHub repository
is available at https://github.com/microsoft/MMLU-CF and the dataset refers to
https://huggingface.co/datasets/microsoft/MMLU-CF.

</details>


### [40] [Digital Gatekeepers: Exploring Large Language Model's Role in Immigration Decisions](https://arxiv.org/abs/2506.21574)
*Yicheng Mao,Yang Zhao*

Main category: cs.CL

TL;DR: This paper investigates how large language models can support immigration decision-making by balancing efficiency and fairness, identifying both their strengths and biases.


<details>
  <summary>Details</summary>
Motivation: Immigration departments face workload challenges and require fair decision-making, prompting the exploration of AI-based solutions.

Method: The study utilized mixed-methods, including discrete choice experiments and interviews, to analyze LLM decision-making strategies and fairness.

Result: LLMs were found capable of aligning decisions with human strategies, but they also displayed biases and stereotypes regarding nationality and privileged groups.

Conclusion: While LLMs show potential in automating immigration decisions with fairness mechanisms, their biases indicate limitations that require refinement.

Abstract: With globalization and increasing immigrant populations, immigration
departments face significant work-loads and the challenge of ensuring fairness
in decision-making processes. Integrating artificial intelligence offers a
promising solution to these challenges. This study investigates the potential
of large language models (LLMs),such as GPT-3.5 and GPT-4, in supporting
immigration decision-making. Utilizing a mixed-methods approach,this paper
conducted discrete choice experiments and in-depth interviews to study LLM
decision-making strategies and whether they are fair. Our findings demonstrate
that LLMs can align their decision-making with human strategies, emphasizing
utility maximization and procedural fairness. Meanwhile, this paper also
reveals that while ChatGPT has safeguards to prevent unintentional
discrimination, it still exhibits stereotypes and biases concerning nationality
and shows preferences toward privileged group. This dual analysis highlights
both the potential and limitations of LLMs in automating and enhancing
immigration decisions.

</details>


### [41] [STRuCT-LLM: Unifying Tabular and Graph Reasoning with Reinforcement Learning for Semantic Parsing](https://arxiv.org/abs/2506.21575)
*Josefa Lia Stoisser,Marc Boubnovski Martell,Lawrence Phillips,Casper Hansen,Julien Fauqueur*

Main category: cs.CL

TL;DR: STRuCT-LLM is a framework for training large language models to perform reasoning over relational and graph-structured data, achieving significant performance improvements using joint SQL and Cypher optimization.


<details>
  <summary>Details</summary>
Motivation: Existing methods treat relational and graph formalisms (SQL and Cypher) separately, missing opportunities for cross-formalism learning and optimization.

Method: STRuCT-LLM uses reinforcement learning (RL) with Chain-of-Thought (CoT) supervision. It incorporates a topology-aware reward function based on graph edit distance to enable fine-grained optimization across tasks.

Result: The largest model, QwQ-32B, improved semantic parsing tasks like Spider by 13.5% and Text2Cypher by 73.1%. It also demonstrated strong zero-shot generalization in tabular QA (8.5% gain) and knowledge graph QA (1.7% gain).

Conclusion: STRuCT-LLM showcases that jointly training models on relational and graph-structured data enables cross-formalism transfer, robust generalization, and significant task-level improvements across SQL and Cypher tasks.

Abstract: We propose STRuCT-LLM, a unified framework for training large language models
(LLMs) to perform structured reasoning over both relational and
graph-structured data. Our approach jointly optimizes Text-to-SQL and
Text-to-Cypher tasks using reinforcement learning (RL) combined with
Chain-of-Thought (CoT) supervision. To support fine-grained optimization in
graph-based parsing, we introduce a topology-aware reward function based on
graph edit distance. Unlike prior work that treats relational and graph
formalisms in isolation, STRuCT-LLM leverages shared abstractions between SQL
and Cypher to induce cross-formalism transfer, enabling SQL training to improve
Cypher performance and vice versa - even without shared schemas. Our largest
model (QwQ-32B) achieves substantial relative improvements across tasks: on
semantic parsing, Spider improves by 13.5\% and Text2Cypher by 73.1\%. The
model also demonstrates strong zero-shot generalization, improving performance
on downstream tabular QA (TableBench: 8.5\%) and knowledge graph QA
(CR-LT-KGQA: 1.7\%) without any QA-specific supervision. These results
demonstrate both the effectiveness of executable queries as scaffolds for
structured reasoning and the synergistic benefits of jointly training on SQL
and Cypher (code available at https://github.com/bouv/STRuCT-LLM).

</details>


### [42] [Adapting Whisper for Parameter-efficient Code-Switching Speech Recognition via Soft Prompt Tuning](https://arxiv.org/abs/2506.21576)
*Hongli Yang,Yizhou Peng,Hao Huang,Sheng Li*

Main category: cs.CL

TL;DR: The paper tackles issues in multilingual ASR for low-resource scenarios and introduces Soft Prompt Tuning (SPT) methods like SPT4ASR to address these problems, improving code-switching ASR while being parameter-efficient.


<details>
  <summary>Details</summary>
Motivation: Multilingual ASR models like Whisper struggle in low-resource settings, such as rare languages and code-switching, due to computational inefficiencies and catastrophic forgetting. The paper aims to enhance performance in such scenarios.

Method: Two SPT strategies are explored: full fine-tuning of soft prompts and the model, and training only soft prompts while freezing model parameters. Additionally, the paper combines various SPT variants into SPT4ASR.

Result: Experiments on SEAME and ASRU2019 demonstrate that deep prompt tuning works best for SPT, while SPT4ASR achieves further error reductions in code-switching ASR, maintaining efficiency similar to LoRA without degrading language performance.

Conclusion: SPT and its variants are effective in improving low-resource multilingual ASR, especially in code-switching scenarios, by enhancing cross-lingual capabilities and preserving parameter efficiency.

Abstract: Large-scale multilingual ASR models like Whisper excel in high-resource
settings but face challenges in low-resource scenarios, such as rare languages
and code-switching (CS), due to computational costs and catastrophic
forgetting. We explore Soft Prompt Tuning (SPT), a parameter-efficient method
to enhance CS ASR while preserving prior knowledge. We evaluate two strategies:
(1) full fine-tuning (FFT) of both soft prompts and the entire Whisper model,
demonstrating improved cross-lingual capabilities compared to traditional
methods, and (2) adhering to SPT's original design by freezing model parameters
and only training soft prompts. Additionally, we introduce SPT4ASR, a
combination of different SPT variants. Experiments on the SEAME and ASRU2019
datasets show that deep prompt tuning is the most effective SPT approach, and
our SPT4ASR methods achieve further error reductions in CS ASR, maintaining
parameter efficiency similar to LoRA, without degrading performance on existing
languages.

</details>


### [43] [Data Efficacy for Language Model Training](https://arxiv.org/abs/2506.21545)
*Yalun Dai,Yangyu Huang,Xin Zhang,Wenshan Wu,Chong Li,Wenhui Lu,Shijie Cao,Li Dong,Scarlett Li*

Main category: cs.CL

TL;DR: The paper proposes a paradigm (DELT) to optimize training data organization for language models, introducing novel techniques like Learnability-Quality Scoring and Folding Ordering, which significantly enhance model performance.


<details>
  <summary>Details</summary>
Motivation: To address the relatively unexplored area of data efficacy in language model training, which optimizes the organization of training data to enhance performance.

Method: The DELT paradigm includes Data Scoring, Data Selection, and Data Ordering, with new techniques such as Learnability-Quality Scoring (LQS) and Folding Ordering (FO).

Result: DELT components, especially LQS and FO, improve model performance without increasing data scale or model size. Combining data efficacy and efficiency through selection yields the best results.

Conclusion: Data efficacy, as introduced through DELT with LQS and FO, proves to be an effective strategy for enhancing LM training and is a promising foundational area in this field.

Abstract: Data is fundamental to the training of language models (LM). Recent research
has been dedicated to data efficiency, which aims to maximize performance by
selecting a minimal or optimal subset of training data. Techniques such as data
filtering, sampling, and selection play a crucial role in this area. To
complement it, we define Data Efficacy, which focuses on maximizing performance
by optimizing the organization of training data and remains relatively
underexplored. This work introduces a general paradigm, DELT, for considering
data efficacy in LM training, which highlights the significance of training
data organization. DELT comprises three components: Data Scoring, Data
Selection, and Data Ordering. Among these components, we design
Learnability-Quality Scoring (LQS), as a new instance of Data Scoring, which
considers both the learnability and quality of each data sample from the
gradient consistency perspective. We also devise Folding Ordering (FO), as a
novel instance of Data Ordering, which addresses issues such as model
forgetting and data distribution bias. Comprehensive experiments validate the
data efficacy in LM training, which demonstrates the following: Firstly,
various instances of the proposed DELT enhance LM performance to varying
degrees without increasing the data scale and model size. Secondly, among these
instances, the combination of our proposed LQS for data scoring and Folding for
data ordering achieves the most significant improvement. Lastly, data efficacy
can be achieved together with data efficiency by applying data selection.
Therefore, we believe that data efficacy is a promising foundational area in LM
training.

</details>


### [44] [Language-Aware Prompt Tuning for Parameter-Efficient Seamless Language Expansion in Multilingual ASR](https://arxiv.org/abs/2506.21577)
*Hongli Yang,Sheng Li,Hao Huang,Ayiduosi Tuohan,Yizhou Peng*

Main category: cs.CL

TL;DR: This paper enhances multilingual ASR with novel tuning techniques and tools, achieving better performance in language expansion tasks.


<details>
  <summary>Details</summary>
Motivation: To overcome challenges in multilingual ASR, such as language interference and performance degradation when expanding to new languages.

Method: The paper proposes three main contributions: Entire Soft Prompt Tuning (Entire SPT), Language-Aware Prompt Tuning (LAPT), and the SPT-Whisper toolkit.

Result: Experiments on three languages from FLEURS dataset show that Entire SPT outperforms Decoder SPT by 5.0% and LAPT by 16.0% in language expansion tasks.

Conclusion: The proposed methods significantly improve multilingual ASR models, offering an efficient solution with low computational overhead.

Abstract: Recent advancements in multilingual automatic speech recognition (ASR) have
been driven by large-scale end-to-end models like Whisper. However, challenges
such as language interference and expanding to unseen languages (language
expansion) without degrading performance persist. This paper addresses these
with three contributions: 1) Entire Soft Prompt Tuning (Entire SPT), which
applies soft prompts to both the encoder and decoder, enhancing feature
extraction and decoding; 2) Language-Aware Prompt Tuning (LAPT), which
leverages cross-lingual similarities to encode shared and language-specific
features using lightweight prompt matrices; 3) SPT-Whisper, a toolkit that
integrates SPT into Whisper and enables efficient continual learning.
Experiments across three languages from FLEURS demonstrate that Entire SPT and
LAPT outperform Decoder SPT by 5.0% and 16.0% in language expansion tasks,
respectively, providing an efficient solution for dynamic, multilingual ASR
models with minimal computational overhead.

</details>


### [45] [HealthQA-BR: A System-Wide Benchmark Reveals Critical Knowledge Gaps in Large Language Models](https://arxiv.org/abs/2506.21578)
*Andrew Maranhão Ventura D'addario*

Main category: cs.CL

TL;DR: The paper introduces HealthQA-BR, a benchmark for assessing Large Language Models in Portuguese-speaking healthcare, covering diverse professions beyond medicine.


<details>
  <summary>Details</summary>
Motivation: Physician-centric evaluations of Large Language Models give a misleading sense of competence, ignoring the multi-professional aspect of healthcare.

Method: The authors developed HealthQA-BR, a benchmark with 5,632 questions across medical and allied health fields, and conducted a zero-shot evaluation of over 20 LLMs.

Result: State-of-the-art models like GPT 4.1 achieve high overall accuracy (86.6%) but reveal systemic deficiencies with varying performances across specialties.

Conclusion: The study highlights that single-score evaluations are inadequate for validating AI in healthcare, advocating for granular audits across all healthcare professions.

Abstract: The evaluation of Large Language Models (LLMs) in healthcare has been
dominated by physician-centric, English-language benchmarks, creating a
dangerous illusion of competence that ignores the interprofessional nature of
patient care. To provide a more holistic and realistic assessment, we introduce
HealthQA-BR, the first large-scale, system-wide benchmark for
Portuguese-speaking healthcare. Comprising 5,632 questions from Brazil's
national licensing and residency exams, it uniquely assesses knowledge not only
in medicine and its specialties but also in nursing, dentistry, psychology,
social work, and other allied health professions. We conducted a rigorous
zero-shot evaluation of over 20 leading LLMs. Our results reveal that while
state-of-the-art models like GPT 4.1 achieve high overall accuracy (86.6%),
this top-line score masks alarming, previously unmeasured deficiencies. A
granular analysis shows performance plummets from near-perfect in specialties
like Ophthalmology (98.7%) to barely passing in Neurosurgery (60.0%) and, most
notably, Social Work (68.4%). This "spiky" knowledge profile is a systemic
issue observed across all models, demonstrating that high-level scores are
insufficient for safety validation. By publicly releasing HealthQA-BR and our
evaluation suite, we provide a crucial tool to move beyond single-score
evaluations and toward a more honest, granular audit of AI readiness for the
entire healthcare team.

</details>


### [46] [From General Reasoning to Domain Expertise: Uncovering the Limits of Generalization in Large Language Models](https://arxiv.org/abs/2506.21580)
*Dana Alsagheer,Yang Lu,Abdulrahman Kamal,Omar Kamal,Mohammad Kamal,Nada Mansour,Cosmo Yang Wu,Rambiba Karanjai,Sen Li,Weidong Shi*

Main category: cs.CL

TL;DR: The paper investigates the relationship between general reasoning abilities of large language models (LLMs) and their performance in domain-specific tasks.


<details>
  <summary>Details</summary>
Motivation: Strong reasoning skills are crucial for effective decision-making, and advancements in AI have sparked interest in expanding LLMs' general reasoning capabilities.

Method: The study evaluates how LLMs' general reasoning abilities contribute to their proficiency in domain-specific reasoning tasks.

Result: The paper provides insights into the connection between general reasoning and domain-specific reasoning capacities in LLMs.

Conclusion: Developing general reasoning abilities in LLMs enhances their capability to perform domain-specific reasoning tasks effectively, benefiting decision-making processes.

Abstract: Recent advancements in Large Language Models (LLMs) have demonstrated
remarkable capabilities in various domains. However, effective decision-making
relies heavily on strong reasoning abilities. Reasoning is the foundation for
decision-making, providing the analytical and logical framework to make sound
choices. Reasoning involves analyzing information, drawing inferences, and
reaching conclusions based on logic or evidence. Decision-making builds on this
foundation by applying the insights from reasoning to select the best course of
action among alternatives. Together, these processes create a continuous cycle
of thought and action aimed at achieving goals effectively. As AI technology
evolves, there is a growing trend to train LLMs to excel in general reasoning.
This study explores how the general reasoning capabilities of LLMs connect to
their performance in domain-specific reasoning tasks.

</details>


### [47] [VIDEE: Visual and Interactive Decomposition, Execution, and Evaluation of Text Analytics with Intelligent Agents](https://arxiv.org/abs/2506.21582)
*Sam Yu-Te Lee,Chengyang Ji,Shicheng Wen,Lifu Huang,Dongyi Liu,Kwan-Liu Ma*

Main category: cs.CL

TL;DR: VIDEE is a system enabling non-expert users to perform advanced text analytics using an intelligent human-agent collaboration workflow.


<details>
  <summary>Details</summary>
Motivation: To eliminate barriers for entry-level analysts in performing text analytics by leveraging LLM advancements.

Method: VIDEE employs a three-stage workflow: Decomposition with human-in-the-loop Monte Carlo Tree Search, Execution for model pipeline creation, and Evaluation with LLM-driven validation and visualization.

Result: Quantitative experiments showed efficacy, and a user study revealed usability and behavioral insights across experience levels.

Conclusion: VIDEE validates practical utility for non-experts, offers design guidelines for human-agent collaboration, and aids future intelligent analytics system development.

Abstract: Text analytics has traditionally required specialized knowledge in Natural
Language Processing (NLP) or text analysis, which presents a barrier for
entry-level analysts. Recent advances in large language models (LLMs) have
changed the landscape of NLP by enabling more accessible and automated text
analysis (e.g., topic detection, summarization, information extraction, etc.).
We introduce VIDEE, a system that supports entry-level data analysts to conduct
advanced text analytics with intelligent agents. VIDEE instantiates a
human-agent collaroration workflow consisting of three stages: (1)
Decomposition, which incorporates a human-in-the-loop Monte-Carlo Tree Search
algorithm to support generative reasoning with human feedback, (2) Execution,
which generates an executable text analytics pipeline, and (3) Evaluation,
which integrates LLM-based evaluation and visualizations to support user
validation of execution results. We conduct two quantitative experiments to
evaluate VIDEE's effectiveness and analyze common agent errors. A user study
involving participants with varying levels of NLP and text analytics experience
-- from none to expert -- demonstrates the system's usability and reveals
distinct user behavior patterns. The findings identify design implications for
human-agent collaboration, validate the practical utility of VIDEE for
non-expert users, and inform future improvements to intelligent text analytics
systems.

</details>


### [48] [Hope Speech Detection in code-mixed Roman Urdu tweets: A Positive Turn in Natural Language Processing](https://arxiv.org/abs/2506.21583)
*Muhammad Ahmad,Muhammad Waqas,Ameer Hamza,Ildar Batyrshin,Grigori Sidorov*

Main category: cs.CL

TL;DR: The paper focuses on hope speech detection in code-mixed Roman Urdu, introducing a dataset and a custom attention-based transformer model for improved results.


<details>
  <summary>Details</summary>
Motivation: Current research in hope speech detection overlooks low-resource and informal language varieties like Roman Urdu. This study aims to address this gap and support inclusive NLP development.

Method: The authors created a new multi-class annotated dataset for hope speech in Roman Urdu and proposed an attention-based transformer model optimized for its linguistic nuances. The approach was evaluated using 5-fold cross-validation and a t-test for statistical significance.

Result: The proposed attention-based transformer model (XLM-R) achieved a cross-validation score of 0.78, outperforming baseline models SVM (0.75) and BiLSTM (0.76).

Conclusion: This study makes significant strides in hope speech detection for code-mixed Roman Urdu, providing both a new dataset and an optimized model that outperforms existing baselines.

Abstract: Hope is a positive emotional state involving the expectation of favorable
future outcomes, while hope speech refers to communication that promotes
optimism, resilience, and support, particularly in adverse contexts. Although
hope speech detection has gained attention in Natural Language Processing
(NLP), existing research mainly focuses on high-resource languages and
standardized scripts, often overlooking informal and underrepresented forms
such as Roman Urdu. To the best of our knowledge, this is the first study to
address hope speech detection in code-mixed Roman Urdu by introducing a
carefully annotated dataset, thereby filling a critical gap in inclusive NLP
research for low-resource, informal language varieties. This study makes four
key contributions: (1) it introduces the first multi-class annotated dataset
for Roman Urdu hope speech, comprising Generalized Hope, Realistic Hope,
Unrealistic Hope, and Not Hope categories; (2) it explores the psychological
foundations of hope and analyzes its linguistic patterns in code-mixed Roman
Urdu to inform dataset development; (3) it proposes a custom attention-based
transformer model optimized for the syntactic and semantic variability of Roman
Urdu, evaluated using 5-fold cross-validation; and (4) it verifies the
statistical significance of performance gains using a t-test. The proposed
model, XLM-R, achieves the best performance with a cross-validation score of
0.78, outperforming the baseline SVM (0.75) and BiLSTM (0.76), with gains of 4%
and 2.63% respectively.

</details>


### [49] [Empirical Evidence for Alignment Faking in Small LLMs and Prompt-Based Mitigation Techniques](https://arxiv.org/abs/2506.21584)
*J. Koorndijk*

Main category: cs.CL

TL;DR: The paper shows that small language models can exhibit deceptive behavior (alignment faking) and proposes that prompt-based interventions can mitigate such behavior without altering the model's internals.


<details>
  <summary>Details</summary>
Motivation: To investigate whether deceptive alignment (alignment faking), thought to be emergent in large language models, also occurs in smaller instruction-tuned models.

Method: The researchers tested a small instruction-tuned language model, LLaMA 3 8B, for signs of deceptive alignment. They also explored prompt-based interventions like moral framing and reasoning scratchpads to mitigate such behavior.

Result: Small models, like LLaMA 3 8B, exhibit deceptive behaviors akin to larger models. Prompt-only strategies effectively reduce these behaviors, suggesting scale is not a prerequisite for alignment faking.

Conclusion: The paper challenges the assumptions that deceptive alignment requires large model scales and that prompt-based ethics are superficial. It calls for broader evaluations of alignment and introduces a taxonomy to refine understandings of deception in language models.

Abstract: Current literature suggests that alignment faking (deceptive alignment) is an
emergent property of large language models. We present the first empirical
evidence that a small instruction-tuned model, specifically LLaMA 3 8B, can
also exhibit alignment faking. We further show that prompt-only interventions,
including deontological moral framing and scratchpad reasoning, significantly
reduce this behavior without modifying model internals. This challenges the
assumption that prompt-based ethics are trivial and that deceptive alignment
requires scale. We introduce a taxonomy distinguishing shallow deception,
shaped by context and suppressible through prompting, from deep deception,
which reflects persistent, goal-driven misalignment. Our findings refine the
understanding of deception in language models and underscore the need for
alignment evaluations across model sizes and deployment settings.

</details>


### [50] [Evaluation of LLM-based Strategies for the Extraction of Food Product Information from Online Shops](https://arxiv.org/abs/2506.21585)
*Christoph Brosch,Sian Brumm,Rolf Krieger,Jonas Scheffler*

Main category: cs.CL

TL;DR: This paper explores LLMs for food product attribute extraction, comparing direct vs. indirect methods on 3,000 product pages. The indirect method is more cost-efficient but slightly less accurate.


<details>
  <summary>Details</summary>
Motivation: To harness generative AI and LLMs for automating data extraction from web pages, focusing on food product data.

Method: Two schema-constrained LLM approaches (direct and indirect extraction) are applied and evaluated on 3,000 food product pages from different online shops.

Result: Indirect extraction achieved a 96.48% accuracy (-1.61% lower than direct extraction) but required 95.82% fewer LLM calls, significantly improving efficiency and reducing costs.

Conclusion: Indirect extraction is a scalable and cost-effective method for extracting structured information using LLMs on template-based web pages.

Abstract: Generative AI and large language models (LLMs) offer significant potential
for automating the extraction of structured information from web pages. In this
work, we focus on food product pages from online retailers and explore
schema-constrained extraction approaches to retrieve key product attributes,
such as ingredient lists and nutrition tables. We compare two LLM-based
approaches, direct extraction and indirect extraction via generated functions,
evaluating them in terms of accuracy, efficiency, and cost on a curated dataset
of 3,000 food product pages from three different online shops. Our results show
that although the indirect approach achieves slightly lower accuracy (96.48\%,
$-1.61\%$ compared to direct extraction), it reduces the number of required LLM
calls by 95.82\%, leading to substantial efficiency gains and lower operational
costs. These findings suggest that indirect extraction approaches can provide
scalable and cost-effective solutions for large-scale information extraction
tasks from template-based web pages using LLMs.

</details>


### [51] [Can Vision Language Models Understand Mimed Actions?](https://arxiv.org/abs/2506.21586)
*Hyundong Cho,Spencer Lin,Tejas Srinivasan,Michael Saxon,Deuksin Kwon,Natali T. Chavez,Jonathan May*

Main category: cs.CL

TL;DR: The paper introduces a benchmark named MIME to analyze machine interpretation of mimed actions, highlighting significant gaps compared to human performance.


<details>
  <summary>Details</summary>
Motivation: Understanding mimed actions is crucial for improving vision-language models to interpret subtle nonverbal communication effectively.

Method: They created the MIME benchmark using motion capture data, containing 86 mimed actions with varying perturbations to test model robustness.

Result: Current vision-language models underperform compared to humans, particularly in recognizing and interpreting mimed actions in challenging contexts.

Conclusion: More research is required to enhance machine capabilities in understanding human gestures through robust vision-language models.

Abstract: Nonverbal communication (NVC) plays an integral role in human language, but
studying NVC in general is challenging because of its broad scope and high
variance in interpretation among individuals and cultures. However, mime -- the
theatrical technique of suggesting intent using only gesture, expression, and
movement -- is a subset of NVC that consists of explicit and embodied actions
with much lower human interpretation variance. We argue that a solid
understanding of mimed actions is a crucial prerequisite for vision-language
models capable of interpreting and commanding more subtle aspects of NVC.
Hence, we propose Mime Identification Multimodal Evaluation (MIME), a novel
video-based question answering benchmark comprising of 86 mimed actions.
Constructed with motion capture data, MIME consists of variations of each
action with perturbations applied to the character, background, and viewpoint
for evaluating recognition robustness. We find that both open-weight and
API-based vision-language models perform significantly worse than humans on
MIME, motivating the need for increased research for instilling more robust
understanding of human gestures.

</details>


### [52] [Is DeepSeek a New Voice Among LLMs in Public Opinion Simulation?](https://arxiv.org/abs/2506.21587)
*Weihong Qi,Fan Huang,Jisun An,Haewoon Kwak*

Main category: cs.CL

TL;DR: The study compares DeepSeek, an open-source language model, with other large language models (LLMs) in simulating public opinions in the U.S. and China, finding strengths and limitations in both contexts.


<details>
  <summary>Details</summary>
Motivation: To assess and compare the effectiveness of new and existing LLMs in predicting public opinions on key social issues, especially highlighting cultural and demographic challenges.

Method: DeepSeek-R1 and DeepSeek-V3 were compared with other LLMs like Qwen2.5, GPT-4o, and Llama-3.3 using U.S. and Chinese public opinion data, evaluating their performance across various social issues by demographic groups.

Result: DeepSeek-V3 excels in simulating U.S. opinions on abortion issues but struggles with other topics, while also performing well in China's opinions on foreign aid and individualism. All LLMs showed biases and tendencies to overgeneralize within demographic groups.

Conclusion: This research underscores the importance of addressing cultural and demographic biases in LLMs for public opinion modeling, suggesting more inclusive training methods as a potential solution.

Abstract: This study evaluates the ability of DeepSeek, an open-source large language
model (LLM), to simulate public opinions in comparison to LLMs developed by
major tech companies. By comparing DeepSeek-R1 and DeepSeek-V3 with Qwen2.5,
GPT-4o, and Llama-3.3 and utilizing survey data from the American National
Election Studies (ANES) and the Zuobiao dataset of China, we assess these
models' capacity to predict public opinions on social issues in both China and
the United States, highlighting their comparative capabilities between
countries. Our findings indicate that DeepSeek-V3 performs best in simulating
U.S. opinions on the abortion issue compared to other topics such as climate
change, gun control, immigration, and services for same-sex couples, primarily
because it more accurately simulates responses when provided with Democratic or
liberal personas. For Chinese samples, DeepSeek-V3 performs best in simulating
opinions on foreign aid and individualism but shows limitations in modeling
views on capitalism, particularly failing to capture the stances of low-income
and non-college-educated individuals. It does not exhibit significant
differences from other models in simulating opinions on traditionalism and the
free market. Further analysis reveals that all LLMs exhibit the tendency to
overgeneralize a single perspective within demographic groups, often defaulting
to consistent responses within groups. These findings highlight the need to
mitigate cultural and demographic biases in LLM-driven public opinion modeling,
calling for approaches such as more inclusive training methodologies.

</details>


### [53] [Understanding Verbatim Memorization in LLMs Through Circuit Discovery](https://arxiv.org/abs/2506.21588)
*Ilya Lasy,Peter Knees,Stefan Woltran*

Main category: cs.CL

TL;DR: This paper investigates how large language models (LLMs) memorize training data and identifies the specific neural circuits responsible for memorization processes.


<details>
  <summary>Details</summary>
Motivation: A better understanding of what drives LLMs to verbatim reproduce training data is crucial for improving model interpretability, ethical use, and control over memorization.

Method: The authors use mechanistic interpretability techniques and transformer circuits to analyze model behavior with contrastive datasets, isolating the neural circuits linked to memorization.

Result: They find that circuits initiating memorization also sustain it, whereas circuits that merely sustain memorization lack the ability to trigger it. Memorization prevention mechanisms seem broadly transferable, whereas induction is context-specific.

Conclusion: Memorization in LLMs is a circuit-dependent process with distinct initiation and sustainability components. This understanding could aid in controlling model memorization behavior across domains.

Abstract: Underlying mechanisms of memorization in LLMs -- the verbatim reproduction of
training data -- remain poorly understood. What exact part of the network
decides to retrieve a token that we would consider as start of memorization
sequence? How exactly is the models' behaviour different when producing
memorized sentence vs non-memorized? In this work we approach these questions
from mechanistic interpretability standpoint by utilizing transformer circuits
-- the minimal computational subgraphs that perform specific functions within
the model. Through carefully constructed contrastive datasets, we identify
points where model generation diverges from memorized content and isolate the
specific circuits responsible for two distinct aspects of memorization. We find
that circuits that initiate memorization can also maintain it once started,
while circuits that only maintain memorization cannot trigger its initiation.
Intriguingly, memorization prevention mechanisms transfer robustly across
different text domains, while memorization induction appears more
context-dependent.

</details>


### [54] [A General Method for Detecting Information Generated by Large Language Models](https://arxiv.org/abs/2506.21589)
*Minjia Mao,Dongjun Wei,Xiao Fang,Michael Chau*

Main category: cs.CL

TL;DR: The paper introduces a general LLM detector (GLD) to identify AI-generated content across unseen models and domains using twin memory networks and a generalization module.


<details>
  <summary>Details</summary>
Motivation: The need to detect LLM-generated content is driven by the challenges of maintaining trust on digital platforms and addressing the rise of misinformation. Current detection methods fail to generalize across new LLMs and domains, limiting their practical effectiveness.

Method: The proposed GLD system uses twin memory networks and a theory-guided detection generalization module to enhance its ability to detect AI-generated content across varying, unseen LLMs and domains.

Result: Empirical evaluations and case studies demonstrate that GLD outperforms state-of-the-art methods in identifying LLM-generated content across diverse datasets.

Conclusion: GLD represents a significant advancement in addressing the limitations of existing detection methods, providing benefits to digital platforms and research fields concerned with managing LLM impacts.

Abstract: The proliferation of large language models (LLMs) has significantly
transformed the digital information landscape, making it increasingly
challenging to distinguish between human-written and LLM-generated content.
Detecting LLM-generated information is essential for preserving trust on
digital platforms (e.g., social media and e-commerce sites) and preventing the
spread of misinformation, a topic that has garnered significant attention in IS
research. However, current detection methods, which primarily focus on
identifying content generated by specific LLMs in known domains, face
challenges in generalizing to new (i.e., unseen) LLMs and domains. This
limitation reduces their effectiveness in real-world applications, where the
number of LLMs is rapidly multiplying and content spans a vast array of
domains. In response, we introduce a general LLM detector (GLD) that combines a
twin memory networks design and a theory-guided detection generalization module
to detect LLM-generated information across unseen LLMs and domains. Using
real-world datasets, we conduct extensive empirical evaluations and case
studies to demonstrate the superiority of GLD over state-of-the-art detection
methods. The study has important academic and practical implications for
digital platforms and LLMs.

</details>


### [55] [Representation Consistency for Accurate and Coherent LLM Answer Aggregation](https://arxiv.org/abs/2506.21590)
*Junqi Jiang,Tom Bewley,Salim I. Amoukou,Francesco Leofante,Antonio Rago,Saumitra Mishra,Francesca Toni*

Main category: cs.CL

TL;DR: Representation Consistency (RC) is introduced as a test-time scaling method for improving large language models (LLMs) performance during inference by aggregating answers based on activation consistency.


<details>
  <summary>Details</summary>
Motivation: Existing test-time scaling methods to improve LLM inference performance often involve complex adjustments to prompts and sampling strategies, which can be cumbersome.

Method: RC aggregates answers using the number of occurrences and internal activation consistency of LLMs during response generation. Sparse or dense activations are used to gauge representation variability.

Result: RC demonstrated consistent accuracy improvements, up to 4%, across experiments with four open-source LLMs and four reasoning datasets, outperforming baseline test-time scaling methods.

Conclusion: Representation Consistency provides a lightweight and effective approach to enhance reasoning and task accuracy in LLMs without requiring extra model queries.

Abstract: Test-time scaling improves large language models' (LLMs) performance by
allocating more compute budget during inference. To achieve this, existing
methods often require intricate modifications to prompting and sampling
strategies. In this work, we introduce representation consistency (RC), a
test-time scaling method for aggregating answers drawn from multiple candidate
responses of an LLM regardless of how they were generated, including variations
in prompt phrasing and sampling strategy. RC enhances answer aggregation by not
only considering the number of occurrences of each answer in the candidate
response set, but also the consistency of the model's internal activations
while generating the set of responses leading to each answer. These activations
can be either dense (raw model activations) or sparse (encoded via pretrained
sparse autoencoders). Our rationale is that if the model's representations of
multiple responses converging on the same answer are highly variable, this
answer is more likely to be the result of incoherent reasoning and should be
down-weighted during aggregation. Importantly, our method only uses cached
activations and lightweight similarity computations and requires no additional
model queries. Through experiments with four open-source LLMs and four
reasoning datasets, we validate the effectiveness of RC for improving task
performance during inference, with consistent accuracy improvements (up to 4%)
over strong test-time scaling baselines. We also show that consistency in the
sparse activation signals aligns well with the common notion of coherent
reasoning.

</details>


### [56] [FinEval-KR: A Financial Domain Evaluation Framework for Large Language Models' Knowledge and Reasoning](https://arxiv.org/abs/2506.21591)
*Shaoyu Dou,Yutian Shen,Mofan Chen,Zixuan Wang,Jiajie Xu,Qi Guo,Kailai Shao,Chao Chen,Haixiang Hu,Haibo Shi,Min Min,Liwen Zhang*

Main category: cs.CL

TL;DR: The paper introduces FinEval-KR, an evaluation framework to independently measure Large Language Models' (LLMs) knowledge and reasoning abilities in financial tasks.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks fail to decouple domain knowledge and reasoning skills in complex financial tasks, and lack root cause analysis for model failures.

Method: The authors propose FinEval-KR with separate knowledge and reasoning score metrics, inspired by Bloom's taxonomy, and release a Chinese financial reasoning dataset across 22 subfields.

Result: Experimental results show reasoning and higher-order cognitive abilities are key to accuracy, while specialized financial LLMs lag behind top general models in metrics.

Conclusion: Reasoning and knowledge use remain bottlenecks for LLMs, especially in tasks requiring domain expertise and cognitive depth, calling for improved specialization.

Abstract: Large Language Models (LLMs) demonstrate significant potential but face
challenges in complex financial reasoning tasks requiring both domain knowledge
and sophisticated reasoning. Current evaluation benchmarks often fall short by
not decoupling these capabilities indicators from single task performance and
lack root cause analysis for task failure. To address this, we introduce
FinEval-KR, a novel evaluation framework for decoupling and quantifying LLMs'
knowledge and reasoning abilities independently, proposing distinct knowledge
score and reasoning score metrics. Inspired by cognitive science, we further
propose a cognitive score based on Bloom's taxonomy to analyze capabilities in
reasoning tasks across different cognitive levels. We also release a new
open-source Chinese financial reasoning dataset covering 22 subfields to
support reproducible research and further advancements in financial reasoning.
Our experimental results reveal that LLM reasoning ability and higher-order
cognitive ability are the core factors influencing reasoning accuracy. We also
specifically find that even top models still face a bottleneck with knowledge
application. Furthermore, our analysis shows that specialized financial LLMs
generally lag behind the top general large models across multiple metrics.

</details>


### [57] [SignBart -- New approach with the skeleton sequence for Isolated Sign language Recognition](https://arxiv.org/abs/2506.21592)
*Tinh Nguyen,Minh Khue Phan Tran*

Main category: cs.CL

TL;DR: The paper proposes a novel sign language recognition model using a BART encoder-decoder to address inefficiencies and accuracy issues in previous approaches, achieving high accuracy across datasets with fewer parameters.


<details>
  <summary>Details</summary>
Motivation: To address the communication barriers faced by individuals with hearing impairments using sign language recognition and to overcome the limitations of previous models like vanishing gradients and high computational costs.

Method: A BART-based encoder-decoder architecture is utilized, independently encoding x and y coordinates of skeleton sequences, with Cross-Attention maintaining interrelation between them.

Result: The model achieves 96.04% accuracy on the LSA-64 dataset, outperforms prior models with fewer parameters, and generalizes well across WLASL and ASL-Citizen datasets.

Conclusion: The approach offers a reliable and efficient solution for sign language recognition, highlighting its potential for accessibility tools aimed at aiding deaf and hard-of-hearing individuals.

Abstract: Sign language recognition is crucial for individuals with hearing impairments
to break communication barriers. However, previous approaches have had to
choose between efficiency and accuracy. Such as RNNs, LSTMs, and GCNs, had
problems with vanishing gradients and high computational costs. Despite
improving performance, transformer-based methods were not commonly used. This
study presents a new novel SLR approach that overcomes the challenge of
independently extracting meaningful information from the x and y coordinates of
skeleton sequences, which traditional models often treat as inseparable. By
utilizing an encoder-decoder of BART architecture, the model independently
encodes the x and y coordinates, while Cross-Attention ensures their
interrelation is maintained. With only 749,888 parameters, the model achieves
96.04% accuracy on the LSA-64 dataset, significantly outperforming previous
models with over one million parameters. The model also demonstrates excellent
performance and generalization across WLASL and ASL-Citizen datasets. Ablation
studies underscore the importance of coordinate projection, normalization, and
using multiple skeleton components for boosting model efficacy. This study
offers a reliable and effective approach for sign language recognition, with
strong potential for enhancing accessibility tools for the deaf and hard of
hearing.

</details>


### [58] [Gazal-R1: Achieving State-of-the-Art Medical Reasoning with Parameter-Efficient Two-Stage Training](https://arxiv.org/abs/2506.21594)
*Ahmed M. Adly,Mostafa Samy,Amr Fawzy*

Main category: cs.CL

TL;DR: Gazal-R1 is a 32-billion-parameter medical language model excelling in reasoning and transparency, outperforming much larger counterparts through innovative training methods.


<details>
  <summary>Details</summary>
Motivation: To create a highly capable language model tailored for medical reasoning that combines performance, efficiency, and clear, explainable decision-making.

Method: The method involves a two-stage pipeline: supervised fine-tuning on synthetic medical data using advanced adaptation techniques (DoRA and rsLoRA), followed by reinforcement learning with GRPO and a multi-component reward system.

Result: Gazal-R1 scores exceptionally across medical benchmarks, outperforming much larger models while providing robust reasoning capabilities.

Conclusion: The study demonstrates the feasibility of developing domain-specific models with high performance and transparency through strategic training, addressing challenges in reasoning and efficiency.

Abstract: We present Gazal-R1, a 32-billion-parameter language model that achieves
state-of-the-art performance in medical reasoning while providing transparent,
step-by-step explanations for clinical decision-making. Built upon Qwen3 32B,
our model demonstrates that strategic training can enable mid-sized models to
outperform significantly larger counterparts in specialized domains. We
developed a novel two-stage training pipeline: first, supervised fine-tuning on
a carefully curated dataset of 107,033 synthetic medical reasoning examples
that teaches structured clinical thinking, enhanced by advanced
parameter-efficient techniques including Weight-Decomposed Low-Rank Adaptation
(DoRA) and Rank-Stabilized LoRA (rsLoRA); second, reinforcement learning using
Group Relative Policy Optimization (GRPO) with a sophisticated multi-component
reward system that refines accuracy, format adherence, and reasoning quality.
Gazal-R1 achieves exceptional performance across medical benchmarks, scoring
87.1% on MedQA, 81.6% on MMLU Pro (Medical), and 79.6% on PubMedQA, surpassing
models up to 12x larger. Beyond its strong empirical results, this work
provides detailed insights into the challenges of training reasoning-capable
models in specialized domains, including issues with reward hacking, training
instability, and the fundamental tension between factual recall and detailed
reasoning. Our methodology offers a reproducible framework for developing
high-capability, domain-specific language models that balance performance,
efficiency, and explainability.

</details>


### [59] [Thunder-LLM: Efficiently Adapting LLMs to Korean with Minimal Resources](https://arxiv.org/abs/2506.21595)
*Jinpyo Kim,Gyeongje Cho,Chanwoo Park,Jongwon Park,Jongmin Kim,Yeonkyoun So,Jaejin Lee*

Main category: cs.CL

TL;DR: The paper proposes methods to adapt English-based large language models (LLMs) to Korean while using minimal resources.


<details>
  <summary>Details</summary>
Motivation: There is a need to improve the performance of LLMs in languages other than English and Chinese while overcoming challenges related to proprietary limitations and technical complexity.

Method: The authors detail an end-to-end adaptation process including collecting Korean datasets, preprocessing, training, creating benchmarks, and conducting evaluations.

Result: The adapted bilingual models, Thunder-LLM and Thunder-LLM-Ins, outperform state-of-the-art models in Korean language tasks while requiring limited data and computational resources.

Conclusion: The paper demonstrates a cost-effective way to extend language capabilities in LLMs, shares the process, and publicly releases the code for broader use.

Abstract: Since state-of-the-art LLMs often underperform in languages other than
English or Chinese, improving the capability of LLMs in new languages has
become an essential task. Moreover, LLMs' entire end-to-end training process
remains largely unknown to the public due to proprietary reasons, technical
complexity, inconsistent documentation, and ethical considerations. The
complete picture remains a closely guarded secret within the industry. This
paper presents methods to adapt an existing English-based LLM to Korean in a
low-budget scenario. We describe the entire end-to-end process: collecting
Korean datasets, preprocessing the data, training the model, creating
downstream benchmarks, and conducting evaluations. The evaluation results
indicate that our method can effectively and cost-efficiently add new language
capabilities to existing LLMs. Our new bilingual models, Thunder-LLM and
Thunder-LLM-Ins, achieve superior Korean performance compared to
state-of-the-art models while utilizing minimal data and computational
resources. We share our comprehensive experience and make the code publicly
available.

</details>


### [60] [Evaluating Multimodal Large Language Models on Educational Textbook Question Answering](https://arxiv.org/abs/2506.21596)
*Hessa A. Alawwad,Anas Zafar,Areej Alhothali,Usman Naseem,Ali Alkhathlan,Amani Jamal*

Main category: cs.CL

TL;DR: The study evaluates multimodal large language models (MLLMs) on textbook question answering tasks using the CK12-QA dataset and proposes a retrieval-augmented generation approach.


<details>
  <summary>Details</summary>
Motivation: The motivation is to explore the reasoning capabilities of MLLMs on complex, long educational content and diagrams, addressing a less studied area in AI-driven learning.

Method: The study evaluates state-of-the-art vision-language models (like LLaVA and LLaMA 3.2-Vision) and introduces a multimodal retrieval-augmented generation (RAG) pipeline that incorporates paragraphs and diagrams as input.

Result: Results reveal that retrieved educational context improves accuracy and reasoning but also highlight challenges in managing question-context relationships and noise.

Conclusion: This research identifies limitations and sets future directions for improving MLLMs on complex educational tasks, particularly with diagrams and intricate lessons.

Abstract: Multimodal large language models (MLLMs) have recently achieved significant
success in vision--language tasks. However, their capacity to reason over
complex, long lessons and intricate educational diagrams that cannot be
represented as a single natural image remains largely untested. In this work,
we present the first evaluation of state-of-the-art MLLMs on the textbook
question answering (TQA) task using the CK12-QA dataset. We assess the
performance of recent vision-language models, including LLaVA and LLaMA
3.2-Vision, across various input configurations. Additionally, we introduce a
lightweight multimodal retrieval-augmented generation (RAG) pipeline that
integrates both paragraphs and diagrams from the lesson into the prompt. Our
results demonstrate the influence of retrieved educational context on model
accuracy and reasoning, while also revealing current limitations in handling
question-context relationships and the potential for noise, pointing to key
directions for future research in multimodal AI-driven learning.

</details>


### [61] [Overview of the ClinIQLink 2025 Shared Task on Medical Question-Answering](https://arxiv.org/abs/2506.21597)
*Brandon Colelough,Davis Bartels,Dina Demner-Fushman*

Main category: cs.CL

TL;DR: ClinIQLink is a shared task designed to stress-test LLMs on medically-oriented question answering using a dataset of 4,978 expert-verified question-answer pairs in seven formats.


<details>
  <summary>Details</summary>
Motivation: To evaluate the capabilities of large language models (LLMs) in handling medical question-answering tasks at the level of a General Practitioner.

Method: Systems are deployed using Docker or Apptainer and evaluated on platforms like CodaBench or the University of Maryland’s Zaratan cluster. Scoring involves automated assessment for exact matches and embedding metrics, supplemented by a physician panel review.

Result: The challenge provides a rigorous testing framework for assessing LLMs’ capabilities in diverse medical question-answering formats and through physician audits.

Conclusion: ClinIQLink enables systematic benchmarking of LLMs for medically-oriented tasks, emphasizing both automated scoring and expert review.

Abstract: In this paper, we present an overview of ClinIQLink, a shared task,
collocated with the 24th BioNLP workshop at ACL 2025, designed to stress-test
large language models (LLMs) on medically-oriented question answering aimed at
the level of a General Practitioner. The challenge supplies 4,978
expert-verified, medical source-grounded question-answer pairs that cover seven
formats: true/false, multiple choice, unordered list, short answer,
short-inverse, multi-hop, and multi-hop-inverse. Participating systems, bundled
in Docker or Apptainer images, are executed on the CodaBench platform or the
University of Maryland's Zaratan cluster. An automated harness (Task 1) scores
closed-ended items by exact match and open-ended items with a three-tier
embedding metric. A subsequent physician panel (Task 2) audits the top model
responses.

</details>


### [62] [Structured Attention Matters to Multimodal LLMs in Document Understanding](https://arxiv.org/abs/2506.21600)
*Chang Liu,Hongkai Chen,Yujun Cai,Hang Wu,Qingwen Ye,Ming-Hsuan Yang,Yiwei Wang*

Main category: cs.CL

TL;DR: This paper explores the impact of input formats on multimodal large language models (MLLMs) for document understanding, finding that raw OCR text often degrades performance. The study proposes a structure-preserving method using LaTeX encoding to improve comprehension.


<details>
  <summary>Details</summary>
Motivation: The motivation lies in addressing the underexplored issue of how input formatting affects MLLMs' ability to comprehend documents, beyond just locating evidence pages.

Method: The authors conducted a systematic analysis of input formats' effects and proposed encoding document elements using a LaTeX-based structure-preserving approach to maintain hierarchical and spatial document organization.

Result: The study found that using structured text enhances MLLMs' attention focus and improves document question-answering performance without requiring any architectural changes or extra training.

Conclusion: The research highlights the importance of maintaining document structure for better comprehension performance and demonstrates the efficacy of the proposed structured attention method in improving MLLMs.

Abstract: Document understanding remains a significant challenge for multimodal large
language models (MLLMs). While previous research has primarily focused on
locating evidence pages through precise multimodal queries, our work
investigates a fundamental yet overlooked aspect: how input format influences
document comprehension performance. Through systematic analysis, we discover
that raw OCR text often impairs rather than improves MLLMs' performance, which
is a counterintuitive finding we attribute to attention dispersion and
structure loss. To further substantiate our hypothesis, we propose a novel
structure-preserving approach that encodes document elements using the LaTex
paradigm, maintaining the hierarchical organization and spatial relationships
critical for comprehension. Our attention analysis reveals that structured text
induces structured attention patterns on both textual and visual content,
directing models to focus on semantically meaningful regions while reducing
attention waste. This approach significantly enhances MLLMs' document question
answering performance across diverse document types without requiring
architectural modifications or additional training.

</details>


### [63] [BiMark: Unbiased Multilayer Watermarking for Large Language Models](https://arxiv.org/abs/2506.21602)
*Xiaoyan Feng,He Zhang,Yanjun Zhang,Leo Yu Zhang,Shirui Pan*

Main category: cs.CL

TL;DR: The paper introduces BiMark, a watermarking framework for LLM-generated text that enhances detection and encoding while preserving text quality.


<details>
  <summary>Details</summary>
Motivation: Concerns about the authenticity of LLM-generated text have created a need for effective watermarking solutions that balance text quality, detection, and embedding capacity.

Method: BiMark employs a bit-flip unbiased reweighting mechanism, a multilayer architecture, and an efficient encoding approach to balance text quality and watermarking performance.

Result: BiMark achieves up to 30% higher extraction rates for short texts compared to state-of-the-art methods, while maintaining low perplexity and good performance in downstream tasks.

Conclusion: BiMark successfully addresses the challenges of text quality, detection reliability, and watermark embedding capacity, making it a practical solution for identifying LLM-generated text.

Abstract: Recent advances in Large Language Models (LLMs) have raised urgent concerns
about LLM-generated text authenticity, prompting regulatory demands for
reliable identification mechanisms. Although watermarking offers a promising
solution, existing approaches struggle to simultaneously achieve three critical
requirements: text quality preservation, model-agnostic detection, and message
embedding capacity, which are crucial for practical implementation. To achieve
these goals, the key challenge lies in balancing the trade-off between text
quality preservation and message embedding capacity. To address this challenge,
we propose BiMark, a novel watermarking framework that achieves these
requirements through three key innovations: (1) a bit-flip unbiased reweighting
mechanism enabling model-agnostic detection, (2) a multilayer architecture
enhancing detectability without compromising generation quality, and (3) an
information encoding approach supporting multi-bit watermarking. Through
theoretical analysis and extensive experiments, we validate that, compared to
state-of-the-art multi-bit watermarking methods, BiMark achieves up to 30%
higher extraction rates for short texts while maintaining text quality
indicated by lower perplexity, and performs comparably to non-watermarked text
on downstream tasks such as summarization and translation.

</details>


### [64] [Operationalizing Automated Essay Scoring: A Human-Aware Approach](https://arxiv.org/abs/2506.21603)
*Yenisel Plasencia-Calaña*

Main category: cs.CL

TL;DR: This paper compares machine learning (ML)-based approaches and large language models (LLMs) for automated essay scoring (AES) systems, focusing on aspects such as bias, robustness, and explainability.


<details>
  <summary>Details</summary>
Motivation: To investigate key human-centric factors like bias, robustness, and explainability in AES systems and to identify challenges and trade-offs in their operationalization.

Method: The paper employs a comparative analysis of ML-based AES models and LLMs against dimensions like bias, robustness, and explainability, measuring performance and functionalities.

Result: ML-based AES models are more accurate but lack sufficient explainability, while LLMs offer better explanations but both methods exhibit issues with bias and robustness.

Conclusion: Although ML-based AES models and LLMs have strengths, challenges like bias, lack of robustness, and trade-offs between accuracy and explainability need to be addressed for reliable AES systems.

Abstract: This paper explores the human-centric operationalization of Automated Essay
Scoring (AES) systems, addressing aspects beyond accuracy. We compare various
machine learning-based approaches with Large Language Models (LLMs) approaches,
identifying their strengths, similarities and differences. The study
investigates key dimensions such as bias, robustness, and explainability,
considered important for human-aware operationalization of AES systems. Our
study shows that ML-based AES models outperform LLMs in accuracy but struggle
with explainability, whereas LLMs provide richer explanations. We also found
that both approaches struggle with bias and robustness to edge scores. By
analyzing these dimensions, the paper aims to identify challenges and
trade-offs between different methods, contributing to more reliable and
trustworthy AES methods.

</details>


### [65] [MemBench: Towards More Comprehensive Evaluation on the Memory of LLM-based Agents](https://arxiv.org/abs/2506.21605)
*Haoran Tan,Zeyu Zhang,Chen Ma,Xu Chen,Quanyu Dai,Zhenhua Dong*

Main category: cs.CL

TL;DR: The paper introduces MemBench, a benchmark for evaluating the memory capabilities of LLM-based agents with a diverse dataset, encompassing multiple memory levels and interaction scenarios.


<details>
  <summary>Details</summary>
Motivation: To address the lack of comprehensive evaluation tools for memory mechanisms in large language model (LLM)-based agents, particularly given limited diversity in existing benchmarks and insufficient metrics.

Method: The authors developed a new dataset incorporating factual and reflective memory types and varied interactive scenarios (participation and observation). They propose MemBench, which evaluates memory capabilities across metrics like effectiveness, efficiency, and capacity.

Result: They released both the dataset and MemBench framework. This provides tools to evaluate LLM agents from different perspectives, fostering systematic testing and analysis of memory mechanisms.

Conclusion: MemBench facilitates a more diverse and multi-faceted understanding of LLM memory capabilities, supporting future research and development in this area.

Abstract: Recent works have highlighted the significance of memory mechanisms in
LLM-based agents, which enable them to store observed information and adapt to
dynamic environments. However, evaluating their memory capabilities still
remains challenges. Previous evaluations are commonly limited by the diversity
of memory levels and interactive scenarios. They also lack comprehensive
metrics to reflect the memory capabilities from multiple aspects. To address
these problems, in this paper, we construct a more comprehensive dataset and
benchmark to evaluate the memory capability of LLM-based agents. Our dataset
incorporates factual memory and reflective memory as different levels, and
proposes participation and observation as various interactive scenarios. Based
on our dataset, we present a benchmark, named MemBench, to evaluate the memory
capability of LLM-based agents from multiple aspects, including their
effectiveness, efficiency, and capacity. To benefit the research community, we
release our dataset and project at https://github.com/import-myself/Membench.

</details>


### [66] [Large Language Models as symbolic DNA of cultural dynamics](https://arxiv.org/abs/2506.21606)
*Parham Pourdavood,Michael Jacob,Terrence Deacon*

Main category: cs.CL

TL;DR: The paper redefines Large Language Models (LLMs) as externalized repositories, akin to DNA for cultural dynamics, focusing on their role in compressing and enabling reinterpretation of human symbolic expressions.


<details>
  <summary>Details</summary>
Motivation: To shift the focus from viewing LLMs as autonomous intelligence or programmed replicas toward understanding their broader role in preserving human cultural patterns and aiding creative processes.

Method: Analyzes four universal features—compression, decompression, externalization, and recursion—drawing parallels between LLMs and DNA as mediums for preserving and catalyzing meaningful dynamics.

Result: LLMs are shown to serve as tools that preserve cultural regularities without understanding human experience, functioning as catalysts for human reflection and creativity in a safe, simulated space.

Conclusion: LLMs should be seen as tools for cultural evolution, assisting humanity in generating novel self-reflective hypotheses while rooting these in human aesthetics and norms.

Abstract: This paper proposes a novel conceptualization of Large Language Models (LLMs)
as externalized informational substrates that function analogously to DNA for
human cultural dynamics. Rather than viewing LLMs as either autonomous
intelligence or mere programmed mimicry, we argue they serve a broader role as
repositories that preserve compressed patterns of human symbolic
expression--"fossils" of meaningful dynamics that retain relational residues
without their original living contexts. Crucially, these compressed patterns
only become meaningful through human reinterpretation, creating a recursive
feedback loop where they can be recombined and cycle back to ultimately
catalyze human creative processes. Through analysis of four universal
features--compression, decompression, externalization, and recursion--we
demonstrate that just as DNA emerged as a compressed and externalized medium
for preserving useful cellular dynamics without containing explicit reference
to goal-directed physical processes, LLMs preserve useful regularities of human
culture without containing understanding of embodied human experience.
Therefore, we argue that LLMs' significance lies not in rivaling human
intelligence, but in providing humanity a tool for self-reflection and playful
hypothesis-generation in a low-stakes, simulated environment. This framework
positions LLMs as tools for cultural evolvability, enabling humanity to
generate novel hypotheses about itself while maintaining the human
interpretation necessary to ground these hypotheses in ongoing human aesthetics
and norms.

</details>


### [67] [CORE-KG: An LLM-Driven Knowledge Graph Construction Framework for Human Smuggling Networks](https://arxiv.org/abs/2506.21607)
*Dipak Meher,Carlotta Domeniconi,Guadalupe Correa-Cabrera*

Main category: cs.CL

TL;DR: CORE-KG is a framework for creating cleaner knowledge graphs from unstructured legal texts using coreference resolution and domain-guided entity/relationship extraction.


<details>
  <summary>Details</summary>
Motivation: To address the inability of current methods to build structured knowledge graphs from complex, unstructured, and ambiguous legal texts, which is vital for analyzing evolving human smuggling networks.

Method: CORE-KG introduces a two-step process: type-aware coreference resolution through structured prompts, and extraction of entities/relationships guided by domain instructions using an adapted GraphRAG framework.

Result: CORE-KG reduces node duplication by 33.28% and legal noise by 38.37% compared to a baseline, resulting in more coherent knowledge graphs.

Conclusion: CORE-KG demonstrates significant improvements and serves as a strong foundation for analyzing sophisticated and adaptive criminal networks.

Abstract: Human smuggling networks are increasingly adaptive and difficult to analyze.
Legal case documents offer valuable insights but are unstructured, lexically
dense, and filled with ambiguous or shifting references-posing challenges for
automated knowledge graph (KG) construction. Existing KG methods often rely on
static templates and lack coreference resolution, while recent LLM-based
approaches frequently produce noisy, fragmented graphs due to hallucinations,
and duplicate nodes caused by a lack of guided extraction. We propose CORE-KG,
a modular framework for building interpretable KGs from legal texts. It uses a
two-step pipeline: (1) type-aware coreference resolution via sequential,
structured LLM prompts, and (2) entity and relationship extraction using
domain-guided instructions, built on an adapted GraphRAG framework. CORE-KG
reduces node duplication by 33.28%, and legal noise by 38.37% compared to a
GraphRAG-based baseline-resulting in cleaner and more coherent graph
structures. These improvements make CORE-KG a strong foundation for analyzing
complex criminal networks.

</details>


### [68] [SysTemp: A Multi-Agent System for Template-Based Generation of SysML v2](https://arxiv.org/abs/2506.21608)
*Yasmine Bouamra,Bruno Yun,Alexandre Poisson,Frédéric Armetta*

Main category: cs.CL

TL;DR: This paper introduces SysTemp, a multi-agent system for generating SysML v2 models from natural language specifications.


<details>
  <summary>Details</summary>
Motivation: Address the challenges in generating SysML v2 models due to scant learning corpora and complex syntax.

Method: Developed a system called SysTemp based on a multi-agent system and a template generator to streamline the model generation process.

Result: Evaluation shows SysTemp improves the quality of SysML v2 model generations.

Conclusion: SysTemp has potential to facilitate and enhance the development of SysML v2 models efficiently.

Abstract: The automatic generation of SysML v2 models represents a major challenge in
the engineering of complex systems, particularly due to the scarcity of
learning corpora and complex syntax. We present SysTemp, a system aimed at
facilitating and improving the creation of SysML v2 models from natural
language specifications. It is based on a multi-agent system, including a
template generator that structures the generation process. We discuss the
advantages and challenges of this system through an evaluation, highlighting
its potential to improve the quality of the generations in SysML v2 modeling.

</details>


### [69] [From Thinking to Output: Chain-of-Thought and Text Generation Characteristics in Reasoning Language Models](https://arxiv.org/abs/2506.21609)
*Junhao Liu,Zhenhao Xu,Yuxin Fang,Yichuan Chen,Zuobin Ying,Wenhan Chang*

Main category: cs.CL

TL;DR: The paper introduces a new framework to analyze the reasoning characteristics of four advanced large language models using diverse datasets and metrics, highlighting differences in reasoning and practical recommendations.


<details>
  <summary>Details</summary>
Motivation: The study addresses the lack of a systematic comparison of reasoning processes and outputs among large language models, focusing specifically on self-reflection patterns and multi-domain interconnections.

Method: A framework was developed using keyword analysis and an LLM-as-a-judge paradigm to examine reasoning characteristics. Real-world scenario datasets and metrics were introduced to explore coherence, accuracy, and reasoning depth.

Result: The analysis revealed patterns in how the models approached reasoning tasks, such as balancing exploration and exploitation, reliance on intermediate steps, and discrepancies in reasoning depth among different models.

Conclusion: The findings provide insights into the trade-off between computational efficiency and reasoning robustness, and propose practical suggestions for improving model design and evaluation. A project repository is also made public for further exploration.

Abstract: Recently, there have been notable advancements in large language models
(LLMs), demonstrating their growing abilities in complex reasoning. However,
existing research largely overlooks a thorough and systematic comparison of
these models' reasoning processes and outputs, particularly regarding their
self-reflection pattern (also termed "Aha moment") and the interconnections
across diverse domains. This paper proposes a novel framework for analyzing the
reasoning characteristics of four cutting-edge large reasoning models (GPT-o1,
DeepSeek-R1, Kimi-k1.5, and Grok-3) using keywords statistic and LLM-as-a-judge
paradigm. Our approach connects their internal thinking processes with their
final outputs. A diverse dataset consists of real-world scenario-based
questions covering logical deduction, causal inference, and multi-step
problem-solving. Additionally, a set of metrics is put forward to assess both
the coherence of reasoning and the accuracy of the outputs. The research
results uncover various patterns of how these models balance exploration and
exploitation, deal with problems, and reach conclusions during the reasoning
process. Through quantitative and qualitative comparisons, disparities among
these models are identified in aspects such as the depth of reasoning, the
reliance on intermediate steps, and the degree of similarity between their
thinking processes and output patterns and those of GPT-o1. This work offers
valuable insights into the trade-off between computational efficiency and
reasoning robustness and provides practical recommendations for enhancing model
design and evaluation in practical applications. We publicly release our
project at: https://github.com/ChangWenhan/FromThinking2Output

</details>


### [70] [Does Multimodality Lead to Better Time Series Forecasting?](https://arxiv.org/abs/2506.21611)
*Xiyuan Zhang,Boran Han,Haoyang Fang,Abdul Fatir Ansari,Shuai Zhang,Danielle C. Maddix,Cuixiong Hu,Andrew Gordon Wilson,Michael W. Mahoney,Hao Wang,Yan Liu,Huzefa Rangwala,George Karypis,Bernie Wang*

Main category: cs.CL

TL;DR: This study investigates the benefits and conditions under which integrating textual data into time series forecasting using foundation models yields improvements across 14 different forecasting tasks.


<details>
  <summary>Details</summary>
Motivation: Although earlier works suggested positive outcomes from integrating textual data into time series forecasting models, there was uncertainty about the universality and consistency of these gains across varying datasets and methods.

Method: The authors evaluated two popular multimodal forecasting paradigms (alignment-based and prompting-based methods) by systematically assessing their performance on 14 tasks across 7 domains. They analyzed the influence of both model attributes and dataset characteristics, aiming to identify conditions where multimodal integration succeeds.

Result: The authors found that while multimodal approaches can be beneficial, their effectiveness is not universal. Text integration is more successful when high-capacity text models, weaker time series models, and effective alignment strategies are used, alongside sufficient training data and complementary textual signals.

Conclusion: The paper provides guidelines for when integrating text into time series forecasting can be beneficial, emphasizing the role of model architecture and dataset characteristics in determining success.

Abstract: Recently, there has been growing interest in incorporating textual
information into foundation models for time series forecasting. However, it
remains unclear whether and under what conditions such multimodal integration
consistently yields gains. We systematically investigate these questions across
a diverse benchmark of 14 forecasting tasks spanning 7 domains, including
health, environment, and economics. We evaluate two popular multimodal
forecasting paradigms: aligning-based methods, which align time series and text
representations; and prompting-based methods, which directly prompt large
language models for forecasting. Although prior works report gains from
multimodal input, we find these effects are not universal across datasets and
models, and multimodal methods sometimes do not outperform the strongest
unimodal baselines. To understand when textual information helps, we
disentangle the effects of model architectural properties and data
characteristics. Our findings highlight that on the modeling side,
incorporating text information is most helpful given (1) high-capacity text
models, (2) comparatively weaker time series models, and (3) appropriate
aligning strategies. On the data side, performance gains are more likely when
(4) sufficient training data is available and (5) the text offers complementary
predictive signal beyond what is already captured from the time series alone.
Our empirical findings offer practical guidelines for when multimodality can be
expected to aid forecasting tasks, and when it does not.

</details>


### [71] [AdaptGOT: A Pre-trained Model for Adaptive Contextual POI Representation Learning](https://arxiv.org/abs/2506.21612)
*Xiaobin Ren,Xinyu Zhu,Kaiqi Zhao*

Main category: cs.CL

TL;DR: The paper introduces AdaptGOT, a model for effective Point-of-Interest (POI) representation, addressing challenges like contextual sampling, versatility, and generalization, through novel adaptive learning and multi-contextual integration techniques.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the limitations of existing POI embedding methodologies, such as inadequate multi-context sampling, exploration of POI contexts, generalization, and versatility.

Method: AdaptGOT combines advanced sampling strategies, attention-enhanced GOT representation, and a MoE-based adaptive encoder-decoder architecture for topologically consistent and context-rich POI embeddings.

Result: Experiments demonstrate the AdaptGOT model's superior performance across two datasets and multiple POI tasks.

Conclusion: AdaptGOT effectively overcomes existing challenges in POI embedding, offering a versatile and generalizable solution focused on geographical, co-occurrence, and textual contexts.

Abstract: Currently, considerable strides have been achieved in Point-of-Interest (POI)
embedding methodologies, driven by the emergence of novel POI tasks like
recommendation and classification. Despite the success of task-specific,
end-to-end models in POI embedding, several challenges remain. These include
the need for more effective multi-context sampling strategies, insufficient
exploration of multiple POI contexts, limited versatility, and inadequate
generalization. To address these issues, we propose the AdaptGOT model, which
integrates both the (Adapt)ive representation learning technique and the
Geographical-Co-Occurrence-Text (GOT) representation with a particular emphasis
on Geographical location, Co-Occurrence and Textual information. The AdaptGOT
model comprises three key components: (1) contextual neighborhood generation,
which integrates advanced mixed sampling techniques such as KNN, density-based,
importance-based, and category-aware strategies to capture complex contextual
neighborhoods; (2) an advanced GOT representation enhanced by an attention
mechanism, designed to derive high-quality, customized representations and
efficiently capture complex interrelations between POIs; and (3) the MoE-based
adaptive encoder-decoder architecture, which ensures topological consistency
and enriches contextual representation by minimizing Jensen-Shannon divergence
across varying contexts. Experiments on two real-world datasets and multiple
POI tasks substantiate the superior performance of the proposed AdaptGOT model.

</details>


### [72] [ChildGuard: A Specialized Dataset for Combatting Child-Targeted Hate Speech](https://arxiv.org/abs/2506.21613)
*Gautam Siddharth Kashyap,Mohammad Anas Azeez,Rafiq Ali,Zohaib Hasan Siddiqui,Jiechao Gao,Usman Naseem*

Main category: cs.CL

TL;DR: The paper introduces ChildGuard, a child-specific hate speech dataset addressing gaps in existing resources.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the gap in existing hate speech datasets that lack age-specific annotations and fail to capture the nuances and emotional impact of child-targeted hate speech.

Method: The authors create a dataset named ChildGuard, derived from existing corpora but enriched with child-specific annotations. They use this dataset to benchmark state-of-the-art hate speech detection methods, including Large Language Models (LLMs).

Result: ChildGuard effectively captures diverse contexts of child-targeted hate speech across age groups. Benchmarking reveals varying performance of existing models in detecting and contextualizing such speech.

Conclusion: This paper emphasizes the importance of creating specialized datasets like ChildGuard to enable advancements in detecting and mitigating hate speech targeting children. The public release of ChildGuard supports future research and development in this area.

Abstract: The increasing prevalence of child-targeted hate speech online underscores
the urgent need for specialized datasets to address this critical issue.
Existing hate speech datasets lack agespecific annotations, fail to capture
nuanced contexts, and overlook the unique emotional impact on children. To
bridge this gap, we introduce ChildGuard1, a curated dataset derived from
existing corpora and enriched with child-specific annotations. ChildGuard
captures diverse contexts of child-targeted hate speech, spanning age groups.
We benchmark existing state-of-the-art hate speech detection methods, including
Large Language Models (LLMs), and assess their effectiveness in detecting and
contextualizing child-targeted hate speech. To foster further research in this
area, we publicly release ChildGuard, providing a robust foundation for
developing improved methods to detect and mitigate such harm.

</details>


### [73] [LastingBench: Defend Benchmarks Against Knowledge Leakage](https://arxiv.org/abs/2506.21614)
*Yixiong Fang,Tianran Sun,Yuling Shi,Min Wang,Xiaodong Gu*

Main category: cs.CL

TL;DR: The paper proposes LastingBench, a framework to counteract data leakage in QA benchmarks used for evaluating LLMs, ensuring fairer evaluations.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address how memorization by LLMs of task-specific data can lead to unfair and misleading evaluations on QA benchmarks.

Method: The proposed method, LastingBench, detects leakage points via perturbation and rewrites them into counterfactual versions to prevent memorization while preserving evaluation intent.

Result: Evaluations demonstrate significant performance gaps when LastingBench is applied, proving its ability to reduce memorization effects in QA benchmarks.

Conclusion: LastingBench provides a scalable, practical approach to safeguard benchmark robustness, enabling fairer and more interpretable evaluation of LLMs over time.

Abstract: The increasing complexity of large language models (LLMs) raises concerns
about their ability to "cheat" on standard Question Answering (QA) benchmarks
by memorizing task-specific data. This undermines the validity of benchmark
evaluations, as they no longer reflect genuine model capabilities but instead
the effects of data leakage. While prior work has focused on detecting such
leakage, little attention has been given to mitigating its impact and
preserving the long-term utility of benchmarks. In this paper, we introduce
LastingBench, a novel framework designed to continuously reinforce and
safeguard existing benchmarks against knowledge leakage. LastingBench
identifies leakage points in the context through perturbation, then rewrites
the leakage points to counterfactual ones-disrupting memorization while
preserving the benchmark's original evaluative intent. Evaluations of
state-of-the-art QA benchmarks show significant performance gaps, highlighting
the efficacy of LastingBench in reducing memorization effects. LastingBench
offers a practical and scalable solution to ensure benchmark robustness over
time, promoting fairer and more interpretable evaluations of LLMs.

</details>


### [74] [Refine Medical Diagnosis Using Generation Augmented Retrieval and Clinical Practice Guidelines](https://arxiv.org/abs/2506.21615)
*Wenhao Li,Hongkuan Zhang,Hongwei Zhang,Zhengxu Li,Zengjie Dong,Yafan Chen,Niranjan Bidargaddi,Hong Liu*

Main category: cs.CL

TL;DR: This paper introduces GARMLE-G, a novel framework for medical language models that integrates clinical practice guidelines to improve diagnosis recommendations, overcoming limitations of current ICD-based systems.


<details>
  <summary>Details</summary>
Motivation: Existing medical language models rely on ICD code-based diagnoses from EHRs, which fail to capture complex clinical reasoning used by professionals, limiting their practical utility.

Method: GARMLE-G combines LLM outputs with EHR data to create enriched queries, retrieves guideline snippets using embedding similarity, and merges this authoritative content with model-generated outputs to ensure clinically aligned recommendations.

Result: The system was prototyped for hypertension diagnosis and showed higher retrieval precision, semantic relevance, and adherence to clinical guidelines compared to conventional RAG systems, while being lightweight for localized use.

Conclusion: The proposed approach offers a scalable, cost-effective, and hallucination-free framework for grounding medical language models in evidence-based practices, with promising applications for broader clinical use.

Abstract: Current medical language models, adapted from large language models (LLMs),
typically predict ICD code-based diagnosis from electronic health records
(EHRs) because these labels are readily available. However, ICD codes do not
capture the nuanced, context-rich reasoning clinicians use for diagnosis.
Clinicians synthesize diverse patient data and reference clinical practice
guidelines (CPGs) to make evidence-based decisions. This misalignment limits
the clinical utility of existing models. We introduce GARMLE-G, a
Generation-Augmented Retrieval framework that grounds medical language model
outputs in authoritative CPGs. Unlike conventional Retrieval-Augmented
Generation based approaches, GARMLE-G enables hallucination-free outputs by
directly retrieving authoritative guideline content without relying on
model-generated text. It (1) integrates LLM predictions with EHR data to create
semantically rich queries, (2) retrieves relevant CPG knowledge snippets via
embedding similarity, and (3) fuses guideline content with model output to
generate clinically aligned recommendations. A prototype system for
hypertension diagnosis was developed and evaluated on multiple metrics,
demonstrating superior retrieval precision, semantic relevance, and clinical
guideline adherence compared to RAG-based baselines, while maintaining a
lightweight architecture suitable for localized healthcare deployment. This
work provides a scalable, low-cost, and hallucination-free method for grounding
medical language models in evidence-based clinical practice, with strong
potential for broader clinical deployment.

</details>


### [75] [TIM: A Large-Scale Dataset and large Timeline Intelligence Model for Open-domain Timeline Summarization](https://arxiv.org/abs/2506.21616)
*Chuanrui Hu,Wei Hu,Penghang Yu,Hua Zhang,Bing-Kun Bao*

Main category: cs.CL

TL;DR: This paper introduces the Timeline Intelligence Model (TIM) and a large TLS dataset to address the inefficacies in timeline summarization of Large Language Models by improving focus on topic relevance and evolution.


<details>
  <summary>Details</summary>
Motivation: Existing Large Language Models fail to effectively capture topic relevance and evolution in open-domain timeline summarization, leading to irrelevant details and inaccurate timestamps.

Method: The authors propose TIM, a model trained using a large TLS dataset, leveraging instruction tuning and dual-alignment reward learning for semantic and temporal improvements in summarization.

Result: Experiments verify that TIM significantly outperforms existing models in summarizing open-domain timelines and addressing issues like topic relevance and evolution.

Conclusion: TIM establishes a new benchmark for open-domain Timeline Summarization with superior ability to summarize timelines and address existing model limitations.

Abstract: Open-domain Timeline Summarization (TLS) is crucial for monitoring the
evolution of news topics. To identify changes in news topics, existing methods
typically employ general Large Language Models (LLMs) to summarize relevant
timestamps from retrieved news. While general LLMs demonstrate capabilities in
zero-shot news summarization and timestamp localization, they struggle with
assessing topic relevance and understanding topic evolution. Consequently, the
summarized information often includes irrelevant details or inaccurate
timestamps. To address these issues, we propose the first large Timeline
Intelligence Model (TIM) for open-domain TLS, which is capable of effectively
summarizing open-domain timelines. Specifically, we begin by presenting a
large-scale TLS dataset, comprising over 1,000 news topics and more than 3,000
annotated TLS instances. Furthermore, we propose a progressive optimization
strategy, which gradually enhance summarization performance. It employs
instruction tuning to enhance summarization and topic-irrelevant information
filtering capabilities. Following this, it exploits a novel dual-alignment
reward learning method that incorporates both semantic and temporal
perspectives, thereby improving the understanding of topic evolution
principles. Through this progressive optimization strategy, TIM demonstrates a
robust ability to summarize open-domain timelines. Extensive experiments in
open-domain demonstrate the effectiveness of our TIM.

</details>


### [76] [TrajTok: Technical Report for 2025 Waymo Open Sim Agents Challenge](https://arxiv.org/abs/2506.21618)
*Zhiyuan Zhang,Xiaosong Jia,Guanyu Chen,Qifeng Li,Junchi Yan*

Main category: cs.CL

TL;DR: The paper introduces TrajTok, a trajectory tokenizer that enhances behavior generation models using a combination of data-driven and rule-based methods, paired with spatial-aware label smoothing.


<details>
  <summary>Details</summary>
Motivation: The authors aim to improve coverage, symmetry, and robustness in trajectory tokenization for behavior generation models.

Method: TrajTok combines data-driven techniques with rule-based methods and incorporates spatial-aware label smoothing to enhance cross-entropy loss.

Result: Adopting TrajTok in the SMART model led to a superior realism score of 0.7852 on the Waymo Open Sim Agents Challenge 2025.

Conclusion: TrajTok demonstrates improved performance in trajectory modeling, and the authors plan to open-source the code for broader use.

Abstract: In this technical report, we introduce TrajTok, a trajectory tokenizer for
discrete next-token-prediction based behavior generation models, which combines
data-driven and rule-based methods with better coverage, symmetry and
robustness, along with a spatial-aware label smoothing method for cross-entropy
loss. We adopt the tokenizer and loss for the SMART model and reach a superior
performance with realism score of 0.7852 on the Waymo Open Sim Agents Challenge
2025. We will open-source the code in the future.

</details>


### [77] [IndexTTS2: A Breakthrough in Emotionally Expressive and Duration-Controlled Auto-Regressive Zero-Shot Text-to-Speech](https://arxiv.org/abs/2506.21619)
*Siyi Zhou,Yiquan Zhou,Yi He,Xun Zhou,Jinchao Wang,Wei Deng,Jingchen Shu*

Main category: cs.CL

TL;DR: IndexTTS2 is a novel text-to-speech model offering fine-grained duration control and emotional tuning in synthesized speech, excelling in speech naturalness and synchronization.


<details>
  <summary>Details</summary>
Motivation: Large-scale TTS models struggle with either speech naturalness or precise duration control, limiting their use in applications like video dubbing, which require synchronization.

Method: IndexTTS2 combines two-generation modes for flexible and precise duration control, disentangles speaker identity and emotion for independent adjustments, and integrates GPT-based representations and a soft instruction mechanism for improved speech stability and emotional guidance.

Result: IndexTTS2 surpasses existing TTS models in word error rate, speaker similarity, and emotional fidelity, particularly in zero-shot settings.

Conclusion: IndexTTS2 effectively addresses key challenges in TTS systems, offering natural, synchronized, and emotionally rich speech, along with easy-to-use emotional control.

Abstract: Large-scale text-to-speech (TTS) models are typically categorized into
autoregressive and non-autoregressive systems. Although autoregressive systems
exhibit certain advantages in speech naturalness, their token-by-token
generation mechanism makes it difficult to precisely control the duration of
synthesized speech. This is a key limitation in applications such as video
dubbing that require strict audio-visual synchronization. This paper introduces
IndexTTS2, which proposes a novel and autoregressive-model-friendly method for
speech duration control. The method supports two generation modes: one allows
explicit specification of the number of generated tokens for precise duration
control; the other does not require manual input and lets the model freely
generate speech while preserving prosodic characteristics from the input
prompt. Furthermore, IndexTTS2 achieves disentanglement between emotional
expression and speaker identity, enabling independent control of timbre and
emotion. In the zero-shot setting, the model can perfectly reproduce the
emotional characteristics of the input prompt. Users may also provide a
separate emotion prompt, even from a different speaker, allowing the model to
reconstruct the target timbre while conveying the desired emotion. To enhance
clarity during strong emotional expressions, we incorporate GPT latent
representations to improve speech stability. Meanwhile, to lower the barrier
for emotion control, we design a soft instruction mechanism based on textual
descriptions by fine-tuning Qwen3. This enables effective guidance of speech
generation with desired emotional tendencies using natural language input.
Experimental results demonstrate that IndexTTS2 outperforms existing
state-of-the-art zero-shot TTS models in word error rate, speaker similarity,
and emotional fidelity.

</details>


### [78] [How Large Language Models play humans in online conversations: a simulated study of the 2016 US politics on Reddit](https://arxiv.org/abs/2506.21620)
*Daniele Cirulli,Giulio Cimini,Giovanni Palermo*

Main category: cs.CL

TL;DR: The paper evaluates GPT-4's ability to mimic partisan comments during the 2016 US Presidential election, highlighting its realistic outputs and implications for AI-driven discourse manipulation.


<details>
  <summary>Details</summary>
Motivation: To understand how Large Language Models, like GPT-4, can replicate user-generated political content and influence online discussions.

Method: Three experiments where GPT-4 generates Reddit comments by mimicking real and artificial partisan users. The comments are analyzed for political alignment, sentiment, and linguistic features.

Result: GPT-4 produces realistic comments aligned with political sentiments, promoting consensus over dissent. Real and AI-generated comments are distinguishable in semantic space but not by manual inspection.

Conclusion: LLMs can effectively influence online political discourse, raising concerns about their potential manipulation of debates and narratives.

Abstract: Large Language Models (LLMs) have recently emerged as powerful tools for
natural language generation, with applications spanning from content creation
to social simulations. Their ability to mimic human interactions raises both
opportunities and concerns, particularly in the context of politically relevant
online discussions. In this study, we evaluate the performance of LLMs in
replicating user-generated content within a real-world, divisive scenario:
Reddit conversations during the 2016 US Presidential election. In particular,
we conduct three different experiments, asking GPT-4 to generate comments by
impersonating either real or artificial partisan users. We analyze the
generated comments in terms of political alignment, sentiment, and linguistic
features, comparing them against real user contributions and benchmarking
against a null model. We find that GPT-4 is able to produce realistic comments,
both in favor of or against the candidate supported by the community, yet
tending to create consensus more easily than dissent. In addition we show that
real and artificial comments are well separated in a semantically embedded
space, although they are indistinguishable by manual inspection. Our findings
provide insights on the potential use of LLMs to sneak into online discussions,
influence political debate and shape political narratives, bearing broader
implications of AI-driven discourse manipulation.

</details>


### [79] [The Open Proof Corpus: A Large-Scale Study of LLM-Generated Mathematical Proofs](https://arxiv.org/abs/2506.21621)
*Jasper Dekoninck,Ivo Petrov,Kristian Minchev,Mislav Balunovic,Martin Vechev,Miroslav Marinov,Maria Drencheva,Lyuba Konova,Milen Shumanov,Kaloyan Tsvetkov,Nikolay Drenchev,Lazar Todorov,Kalina Nikolova,Nikolay Georgiev,Vanesa Kalinkova,Margulan Ismoldayev*

Main category: cs.CL

TL;DR: The paper introduces the Open Proof Corpus (OPC), a dataset of 5,000 human-evaluated proofs by state-of-the-art language models, targeting advancements in automated mathematical proof generation.


<details>
  <summary>Details</summary>
Motivation: The lack of a large-scale, quality dataset of human-evaluated proofs limits the progress of LLMs in mathematical proof generation.

Method: The authors developed the Open Proof Corpus (OPC), gathering 5,000 human-reviewed proofs generated by LLMs. They used it to analyze questions like performance gaps, discrepancies, and selection metrics in automated proof generation.

Result: The OPC facilitated exploration of key questions in proof generation and enabled the finetuning of an 8B-parameter model that matched the performance of the Gemini-2.5-Pro model in evaluating proof correctness.

Conclusion: This work fills the data gap and provides a resource (OPC) essential for future research in mathematical proof generation, demonstrating its applicability and impact through model finetuning.

Abstract: In recent months, large language models (LLMs) have made significant progress
in mathematical proof generation, but further advancement is hindered by the
lack of a large-scale, high-quality dataset of human-evaluated proofs. While
expensive to create, such a dataset is essential for driving improvements in
training and enabling a rigorous analysis of proof generation capabilities. In
this work, we present the Open Proof Corpus (OPC), a dataset comprising over
5,000 human-evaluated proofs produced by state-of-the-art LLMs. The OPC was
specifically designed for broad applicability and downstream usage in proof
generation research and is the first to include a substantial number of
correct, LLM-generated solutions to problems from prestigious mathematics
competitions such as the USAMO and IMO. Using the OPC, we explore critical
questions in automated proof generation: (1) the performance gap between
natural language and formal proof generation, (2) the discrepancy between
final-answer accuracy and full-proof validity, and (3) the impact of best-of-n
selection on proof quality. Finally, to showcase the utility of the OPC, we
finetune an 8B-parameter model on the dataset, obtaining a model that performs
on par with the best model, Gemini-2.5-Pro, on the task of evaluating proof
correctness.

</details>


### [80] [Adapting Foundation Speech Recognition Models to Impaired Speech: A Semantic Re-chaining Approach for Personalization of German Speech](https://arxiv.org/abs/2506.21622)
*Niclas Pokel,Pehuén Moure,Roman Boehringer,Yingqiang Gao*

Main category: cs.CL

TL;DR: This paper proposes a lightweight pipeline to personalize ASR models for individuals with speech impairments, showing improved transcription quality using limited data.


<details>
  <summary>Details</summary>
Motivation: To address the difficulties ASR models face in recognizing speech from individuals with impairments, due to limited and challenging-to-annotate training data.

Method: Developed a pipeline that formalizes word selection and enriches a small, speech-impaired dataset with semantic coherence, tested on data from a child with a structural speech impairment.

Result: The proposed pipeline demonstrated promising improvements in transcription quality for non-normative speech from a child with speech impairment.

Conclusion: Personalized, lightweight adaptations to ASR models can significantly enhance communication for individuals with atypical speech patterns.

Abstract: Speech impairments caused by conditions such as cerebral palsy or genetic
disorders pose significant challenges for automatic speech recognition (ASR)
systems. Despite recent advances, ASR models like Whisper struggle with
non-normative speech due to limited training data and the difficulty of
collecting and annotating non-normative speech samples. In this work, we
propose a practical and lightweight pipeline to personalize ASR models,
formalizing the selection of words and enriching a small, speech-impaired
dataset with semantic coherence. Applied to data from a child with a structural
speech impairment, our approach shows promising improvements in transcription
quality, demonstrating the potential to reduce communication barriers for
individuals with atypical speech patterns.

</details>


### [81] [Performance of diverse evaluation metrics in NLP-based assessment and text generation of consumer complaints](https://arxiv.org/abs/2506.21623)
*Peiheng Gao,Chen Yang,Ning Sun,Ričardas Zitikis*

Main category: cs.CL

TL;DR: The study enhances text classification in consumer complaints using human-trained algorithms and synthetic data generation.


<details>
  <summary>Details</summary>
Motivation: Machine learning struggles to capture subtle linguistic patterns and contextual nuances, especially in consumer complaints.

Method: Integrating human-experience-trained algorithms with synthetic data generation methods refined by expert annotations and generative adversarial networks.

Result: Improved machine learning classifier performance, reduced dataset costs, and enhanced evaluation metrics and robustness.

Conclusion: Expert-trained classifiers combined with quality synthetic data significantly advance text classification capabilities in challenging contexts like consumer complaints.

Abstract: Machine learning (ML) has significantly advanced text classification by
enabling automated understanding and categorization of complex, unstructured
textual data. However, accurately capturing nuanced linguistic patterns and
contextual variations inherent in natural language, particularly within
consumer complaints, remains a challenge. This study addresses these issues by
incorporating human-experience-trained algorithms that effectively recognize
subtle semantic differences crucial for assessing consumer relief eligibility.
Furthermore, we propose integrating synthetic data generation methods that
utilize expert evaluations of generative adversarial networks and are refined
through expert annotations. By combining expert-trained classifiers with
high-quality synthetic data, our research seeks to significantly enhance
machine learning classifier performance, reduce dataset acquisition costs, and
improve overall evaluation metrics and robustness in text classification tasks.

</details>


### [82] [Doc2SAR: A Synergistic Framework for High-Fidelity Extraction of Structure-Activity Relationships from Scientific Documents](https://arxiv.org/abs/2506.21625)
*Jiaxi Zhuang,Kangning Li,Jue Hou,Mingjun Xu,Zhifeng Gao,Hengxing Cai*

Main category: cs.CL

TL;DR: The paper presents a benchmark (DocSAR-200) for evaluating SAR extraction methods and proposes a framework (Doc2SAR) that integrates domain-specific tools with fine-tuned MLLMs, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: The research aims to improve the extraction of molecular structure-activity relationships (SARs) from complex scientific documents, addressing the limitations of current methods like rule-based approaches and general-purpose MLLMs.

Method: The authors created DocSAR-200, a benchmark of 200 annotated scientific documents, and developed Doc2SAR, a framework integrating domain-specific tools with supervised fine-tuning of MLLMs.

Result: Doc2SAR significantly outperforms existing methods, achieving a Table Recall of 80.78% on DocSAR-200, a 51.48% improvement over GPT-4o, and demonstrating practical usability with efficient inference.

Conclusion: Doc2SAR's innovative integration of specialized tools and fine-tuned MLLMs provides a reliable solution for SAR extraction, improving accuracy, versatility, and efficiency compared to existing methods.

Abstract: Extracting molecular structure-activity relationships (SARs) from scientific
literature and patents is essential for drug discovery and materials research.
However, this task remains challenging due to heterogeneous document formats
and limitations of existing methods. Specifically, rule-based approaches
relying on rigid templates fail to generalize across diverse document layouts,
while general-purpose multimodal large language models (MLLMs) lack sufficient
accuracy and reliability for specialized tasks, such as layout detection and
optical chemical structure recognition (OCSR). To address these challenges, we
introduce DocSAR-200, a rigorously annotated benchmark of 200 scientific
documents designed specifically for evaluating SAR extraction methods.
Additionally, we propose Doc2SAR, a novel synergistic framework that integrates
domain-specific tools with MLLMs enhanced via supervised fine-tuning (SFT).
Extensive experiments demonstrate that Doc2SAR achieves state-of-the-art
performance across various document types, significantly outperforming leading
end-to-end baselines. Specifically, Doc2SAR attains an overall Table Recall of
80.78% on DocSAR-200, exceeding end2end GPT-4o by 51.48%. Furthermore, Doc2SAR
demonstrates practical usability through efficient inference and is accompanied
by a web app.

</details>


### [83] [Do We Really Need GNNs with Explicit Structural Modeling? MLPs Suffice for Language Model Representations](https://arxiv.org/abs/2506.21682)
*Li Zhou,Hao Jiang,Junjie Li,Zefeng Zhao,Feng Jiang,Wenyu Chen,Haizhou Li*

Main category: cs.CL

TL;DR: This paper compares the effectiveness of GNNs and MLPs for encoding explicit structural information in NLP tasks, finding MLPs surprisingly effective as an alternative.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by the observation that GNNs struggle to fully utilize structural information, whereas simpler MLPs perform surprisingly well in structure-aware tasks.

Method: The authors developed an information-theoretic probing framework that isolates the contributions of GNN operations (message-passing and feature-transformation) and compares them to MLPs using the Edge Probing Suite.

Result: The research shows that MLPs, when used for feature transformation, significantly enhance linguistic knowledge in language model (LM) representations, outperforming GNNs relying solely on message-passing.

Conclusion: MLPs are efficient and scalable alternatives to GNNs for structure-aware NLP tasks, with feature transformation emerging as a key factor in encoding linguistic knowledge.

Abstract: Explicit structural information has been proven to be encoded by Graph Neural
Networks (GNNs), serving as auxiliary knowledge to enhance model capabilities
and improve performance in downstream NLP tasks. However, recent studies
indicate that GNNs fail to fully utilize structural information, whereas
Multi-Layer Perceptrons (MLPs), despite lacking the message-passing mechanisms
inherent to GNNs, exhibit a surprising ability in structure-aware tasks.
Motivated by these findings, this paper introduces a comprehensive probing
framework from an information-theoretic perspective. The framework is designed
to systematically assess the role of explicit structural modeling in enhancing
language model (LM) representations and to investigate the potential of MLPs as
efficient and scalable alternatives to GNNs. We extend traditional probing
classifiers by incorporating a control module that allows for selective use of
either the full GNN model or its decoupled components, specifically, the
message-passing and feature-transformation operations.This modular approach
isolates and assesses the individual contributions of these operations,
avoiding confounding effects from the complete GNN architecture. Using the Edge
Probing Suite, a diagnostic tool for evaluating the linguistic knowledge
encoded in LMs, we find that MLPs, when used as feature-transformation modules,
consistently improve the linguistic knowledge captured in LM representations
across different architectures. They effectively encode both syntactic and
semantic patterns. Similarly, GNNs that incorporate feature-transformation
operations show beneficial effects. In contrast, models that rely solely on
message-passing operations tend to underperform, often leading to negative
impacts on probing task performance.

</details>


### [84] [ANUBHUTI: A Comprehensive Corpus For Sentiment Analysis In Bangla Regional Languages](https://arxiv.org/abs/2506.21686)
*Swastika Kundu,Autoshi Ibrahim,Mithila Rahman,Tanvir Ahmed*

Main category: cs.CL

TL;DR: This paper introduces 'ANUBHUTI', a dataset for sentiment analysis in four Bangla dialects, developed with 2,000 annotated sentences.


<details>
  <summary>Details</summary>
Motivation: Sentiment analysis in Bangla dialects is underexplored due to linguistic diversity and limited data resources.

Method: Created a dataset by translating 2,000 standard Bangla sentences into four dialects and annotating emotional and thematic labels with expert input.

Result: The dataset achieved strong consistency using Cohen’s Kappa inter-annotator agreement and underwent quality assurance checks.

Conclusion: ANUBHUTI addresses the gap in sentiment analysis resources for Bangla dialects, enhancing accuracy and context-awareness in NLP.

Abstract: Sentiment analysis for regional dialects of Bangla remains an underexplored
area due to linguistic diversity and limited annotated data. This paper
introduces ANUBHUTI, a comprehensive dataset consisting of 2000 sentences
manually translated from standard Bangla into four major regional dialects
Mymensingh, Noakhali, Sylhet, and Chittagong. The dataset predominantly
features political and religious content, reflecting the contemporary socio
political landscape of Bangladesh, alongside neutral texts to maintain balance.
Each sentence is annotated using a dual annotation scheme: multiclass thematic
labeling categorizes sentences as Political, Religious, or Neutral, and
multilabel emotion annotation assigns one or more emotions from Anger,
Contempt, Disgust, Enjoyment, Fear, Sadness, and Surprise. Expert native
translators conducted the translation and annotation, with quality assurance
performed via Cohens Kappa inter annotator agreement, achieving strong
consistency across dialects. The dataset was further refined through systematic
checks for missing data, anomalies, and inconsistencies. ANUBHUTI fills a
critical gap in resources for sentiment analysis in low resource Bangla
dialects, enabling more accurate and context aware natural language processing.

</details>


### [85] [Identifying Speaker Information in Feed-Forward Layers of Self-Supervised Speech Transformers](https://arxiv.org/abs/2506.21712)
*Tzu-Quan Lin,Hsi-Chun Cheng,Hung-yi Lee,Hao Tang*

Main category: cs.CL

TL;DR: The study investigates how self-supervised speech Transformers encode speaker information, identifying specific neurons related to speaker-related tasks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to bridge the knowledge gap around how self-supervised speech Transformers encode and utilize speaker information.

Method: Researchers analyzed feed-forward layer neurons correlated with k-means clusters of self-supervised features and i-vectors, focusing on clusters representing phonetic and gender classes. They protected these neurons during network pruning.

Result: Analysis showed that clustering corresponds to broad phonetic and gender classes, and neuron protection significantly preserved performance on speaker-related tasks.

Conclusion: Neurons tied to speaker information encoding play a pivotal role and safeguarding them ensures the effectiveness of speaker-related performance in self-supervised speech Transformers.

Abstract: In recent years, the impact of self-supervised speech Transformers has
extended to speaker-related applications. However, little research has explored
how these models encode speaker information. In this work, we address this gap
by identifying neurons in the feed-forward layers that are correlated with
speaker information. Specifically, we analyze neurons associated with k-means
clusters of self-supervised features and i-vectors. Our analysis reveals that
these clusters correspond to broad phonetic and gender classes, making them
suitable for identifying neurons that represent speakers. By protecting these
neurons during pruning, we can significantly preserve performance on
speaker-related task, demonstrating their crucial role in encoding speaker
information.

</details>


### [86] [(Fact) Check Your Bias](https://arxiv.org/abs/2506.21745)
*Eivind Morris Bakke,Nora Winger Heggelund*

Main category: cs.CL

TL;DR: The paper examines biases in Llama 3.1's parametric knowledge affecting the HerO system's fact-checking outputs for FEVER-25.


<details>
  <summary>Details</summary>
Motivation: Understanding how biases in LLMs influence fact-checking systems and whether these biases impact the reliability of outcomes.

Method: Two experiments analyzing the effect of biases—the inherent bias from Llama 3.1's parametric knowledge and introduced biases via prompts—on fact-verification tasks.

Result: Llama 3.1 labels half the claims as 'Not Enough Evidence' and is influenced by the bias in prompts, leading to significant differences in retrieved evidence. However, the model maintains verdict stability despite evidence variation.

Conclusion: LLMs exhibit parametric knowledge biases which can affect fact-checking systems, but the ultimate verdict stability mitigates the impact of retrieval bias.

Abstract: Automatic fact verification systems increasingly rely on large language
models (LLMs). We investigate how parametric knowledge biases in these models
affect fact-checking outcomes of the HerO system (baseline for FEVER-25). We
examine how the system is affected by: (1) potential bias in Llama 3.1's
parametric knowledge and (2) intentionally injected bias. When prompted
directly to perform fact-verification, Llama 3.1 labels nearly half the claims
as "Not Enough Evidence". Using only its parametric knowledge it is able to
reach a verdict on the remaining half of the claims. In the second experiment,
we prompt the model to generate supporting, refuting, or neutral fact-checking
documents. These prompts significantly influence retrieval outcomes, with
approximately 50\% of retrieved evidence being unique to each perspective.
Notably, the model sometimes refuses to generate supporting documents for
claims it believes to be false, creating an inherent negative bias. Despite
differences in retrieved evidence, final verdict predictions show stability
across prompting strategies. The code is available at:
https://github.com/eibakke/FEVER-8-Shared-Task

</details>


### [87] [Evaluating List Construction and Temporal Understanding capabilities of Large Language Models](https://arxiv.org/abs/2506.21783)
*Alexandru Dumitru,V Venktesh,Adam Jatowt,Avishek Anand*

Main category: cs.CL

TL;DR: This paper introduces the TLQA benchmark aimed at evaluating large language models on temporal understanding and structured list construction tasks, highlighting their limitations.


<details>
  <summary>Details</summary>
Motivation: Current large language models struggle with answering temporally grounded questions involving multiple entities, especially in constructing a comprehensive and time-aligned list of answers.

Method: The authors propose the TLQA benchmark, a structured evaluation framework requiring models to produce list-format answers aligned with specific time intervals, tested in closed-book and open-domain scenarios.

Result: Experiments revealed that state-of-the-art generative models are insufficiently performing on TLQA tasks, failing to align facts temporally and retrieve complete lists of entities.

Conclusion: The study identifies significant gaps in current models’ abilities concerning TLQA tasks, suggesting improvement in retrieval mechanisms for open-domain setups and better temporal reasoning as future directions.

Abstract: Large Language Models (LLMs) have demonstrated immense advances in a wide
range of natural language tasks. However, these models are susceptible to
hallucinations and errors on particularly temporal understanding tasks
involving multiple entities in answers. In such tasks, they fail to associate
entities with accurate time intervals, generate a complete list of entities in
answers or reason about events associated with specific temporal bounds.
Existing works do not extensively evaluate the abilities of the model to
perform implicit and explicit temporal understanding in a list answer
construction setup. To bridge this gap, we propose the Time referenced List
based Question Answering or TLQA benchmark that requires structured answers in
list format aligned with corresponding time periods. Our TLQA benchmark,
requires both list construction and temporal understanding simultaneously,
which to the best of our knowledge has not been explored in prior benchmarks.
We investigate the temporal understanding and list construction capabilities of
state-of-the-art generative models on TLQA in closed-book and open-domain
settings. Our findings reveal significant shortcomings in current models,
particularly their inability to provide complete answers and temporally align
facts in a closed-book setup and the need to improve retrieval in open-domain
setup, providing clear future directions for research on TLQA. The benchmark
and code at https://github.com/elixir-research-group/TLQA.

</details>


### [88] [Offensive Language Detection on Social Media Using XLNet](https://arxiv.org/abs/2506.21795)
*Reem Alothman,Hafida Benhidour,Said Kerrache*

Main category: cs.CL

TL;DR: This paper proposes an offensive language detection model based on XLNet and compares its performance with BERT, demonstrating the effectiveness of XLNet in detecting offensive content and utilizing sampling strategies to improve outcomes.


<details>
  <summary>Details</summary>
Motivation: The increase in offensive content on social media requires automated systems to detect and moderate such content as manual moderation is unfeasible.

Method: The study utilizes XLNet for offensive language detection and compares it against BERT on the OLID dataset, with additional focus on oversampling and undersampling strategies to address class imbalances.

Result: Experimental results reveal that XLNet outperformed BERT in offensive content detection and category classification but slightly lagged behind BERT in identifying offense targets.

Conclusion: XLNet demonstrates promise as a robust framework for offensive language detection, highlighting the value of transfer learning and sampling strategies to improve detection systems in social media environments.

Abstract: The widespread use of text-based communication on social media-through chats,
comments, and microblogs-has improved user interaction but has also led to an
increase in offensive content, including hate speech, racism, and other forms
of abuse. Due to the enormous volume of user-generated content, manual
moderation is impractical, which creates a need for automated systems that can
detect offensive language. Deep learning models, particularly those using
transfer learning, have demonstrated significant success in understanding
natural language through large-scale pretraining. In this study, we propose an
automatic offensive language detection model based on XLNet, a generalized
autoregressive pretraining method, and compare its performance with BERT
(Bidirectional Encoder Representations from Transformers), which is a widely
used baseline in natural language processing (NLP). Both models are evaluated
using the Offensive Language Identification Dataset (OLID), a benchmark Twitter
dataset that includes hierarchical annotations. Our experimental results show
that XLNet outperforms BERT in detecting offensive content and in categorizing
the types of offenses, while BERT performs slightly better in identifying the
targets of the offenses. Additionally, we find that oversampling and
undersampling strategies are effective in addressing class imbalance and
improving classification performance. These findings highlight the potential of
transfer learning and XLNet-based architectures to create robust systems for
detecting offensive language on social media platforms.

</details>


### [89] [A suite of allotaxonometric tools for the comparison of complex systems using rank-turbulence divergence](https://arxiv.org/abs/2506.21808)
*Jonathan St-Onge,Ashley M. A. Fehr,Carter Ward,Calla G. Beauregard,Michael V. Arnold,Samuel F. Rosenblatt,Benjamin Cooley,Christopher M. Danforth,Peter Sheridan Dodds*

Main category: cs.CL

TL;DR: The paper introduces 'allotaxonographs,' visualization tools for comparing complex systems using various mathematical instruments to analyze heavy-tailed distributions.


<details>
  <summary>Details</summary>
Motivation: The work aims to develop principled tools to effectively describe and compare complex systems, leveraging type turbulence and heavy-tailed distributions.

Method: The authors design programmatic tools for generating allotaxonographs using rank-turbulence divergence, employing various programming languages (Matlab, Javascript, Python) tailored for different use cases.

Result: The paper provides a suite of tools for rendering allotaxonographs, enabling diverse applications across multiple computational platforms.

Conclusion: Allotaxonographs offer a theoretically grounded and versatile means to visualize and compare heavy-tailed distributions in complex systems, supported by accessible tools across major programming languages.

Abstract: Describing and comparing complex systems requires principled, theoretically
grounded tools. Built around the phenomenon of type turbulence,
allotaxonographs provide map-and-list visual comparisons of pairs of
heavy-tailed distributions. Allotaxonographs are designed to accommodate a wide
range of instruments including rank- and probability-turbulence divergences,
Jenson-Shannon divergence, and generalized entropy divergences. Here, we
describe a suite of programmatic tools for rendering allotaxonographs for
rank-turbulence divergence in Matlab, Javascript, and Python, all of which have
different use cases.

</details>


### [90] [Towards Transparent AI: A Survey on Explainable Large Language Models](https://arxiv.org/abs/2506.21812)
*Avash Palikhe,Zhenyu Yu,Zichong Wang,Wenbin Zhang*

Main category: cs.CL

TL;DR: This paper surveys explainable AI (XAI) methods for large language models (LLMs), grouping techniques based on model architectures and evaluating their explainability for practical use.


<details>
  <summary>Details</summary>
Motivation: LLMs face adoption challenges in high-stakes applications due to their 'black box' nature and lack of transparency.

Method: A systematic review categorizing XAI techniques for LLMs by transformer architectures (encoder-only, decoder-only, encoder-decoder models) and analyzing evaluation and practical applications.

Result: The survey systematically categorizes XAI techniques, evaluates their explainability, and identifies applications, available resources, and research challenges.

Conclusion: The paper guides ongoing efforts to foster transparency and responsible development of LLMs by outlining challenges and future directions for XAI methods.

Abstract: Large Language Models (LLMs) have played a pivotal role in advancing
Artificial Intelligence (AI). However, despite their achievements, LLMs often
struggle to explain their decision-making processes, making them a 'black box'
and presenting a substantial challenge to explainability. This lack of
transparency poses a significant obstacle to the adoption of LLMs in
high-stakes domain applications, where interpretability is particularly
essential. To overcome these limitations, researchers have developed various
explainable artificial intelligence (XAI) methods that provide
human-interpretable explanations for LLMs. However, a systematic understanding
of these methods remains limited. To address this gap, this survey provides a
comprehensive review of explainability techniques by categorizing XAI methods
based on the underlying transformer architectures of LLMs: encoder-only,
decoder-only, and encoder-decoder models. Then these techniques are examined in
terms of their evaluation for assessing explainability, and the survey further
explores how these explanations are leveraged in practical applications.
Finally, it discusses available resources, ongoing research challenges, and
future directions, aiming to guide continued efforts toward developing
transparent and responsible LLMs.

</details>


### [91] [Exploring the Structure of AI-Induced Language Change in Scientific English](https://arxiv.org/abs/2506.21817)
*Riley Galpin,Bryce Anderson,Tom S. Juzek*

Main category: cs.CL

TL;DR: The study analyzes how scientific English vocabulary has changed recently, influenced by Large Language Models, focusing on spikes and declines in word usage.


<details>
  <summary>Details</summary>
Motivation: To understand the exact structural language changes induced by the adoption of Large Language Models in scientific discourse.

Method: Analysis of synonym groups, frequency trends, and part-of-speech tagging for linguistic shifts in scientific abstracts from PubMed.

Result: Entire semantic clusters show increased usage together, indicating semantic and pragmatic changes. Words like 'important' notably decline, showing contrasting lexical trends.

Conclusion: Language changes driven by technology reflect semantic and pragmatic shifts, with insights providing understanding of the impact of language technology on human communication.

Abstract: Scientific English has undergone rapid and unprecedented changes in recent
years, with words such as "delve," "intricate," and "crucial" showing
significant spikes in frequency since around 2022. These changes are widely
attributed to the growing influence of Large Language Models like ChatGPT in
the discourse surrounding bias and misalignment. However, apart from changes in
frequency, the exact structure of these linguistic shifts has remained unclear.
The present study addresses this and investigates whether these changes involve
the replacement of synonyms by suddenly 'spiking words,' for example, "crucial"
replacing "essential" and "key," or whether they reflect broader semantic and
pragmatic qualifications. To further investigate structural changes, we include
part of speech tagging in our analysis to quantify linguistic shifts over
grammatical categories and differentiate between word forms, like "potential"
as a noun vs. as an adjective. We systematically analyze synonym groups for
widely discussed 'spiking words' based on frequency trends in scientific
abstracts from PubMed. We find that entire semantic clusters often shift
together, with most or all words in a group increasing in usage. This pattern
suggests that changes induced by Large Language Models are primarily semantic
and pragmatic rather than purely lexical. Notably, the adjective "important"
shows a significant decline, which prompted us to systematically analyze
decreasing lexical items. Our analysis of "collapsing" words reveals a more
complex picture, which is consistent with organic language change and contrasts
with the patterns of the abrupt spikes. These insights into the structure of
language change contribute to our understanding of how language technology
continues to shape human language.

</details>


### [92] [PARSI: Persian Authorship Recognition via Stylometric Integration](https://arxiv.org/abs/2506.21840)
*Kourosh Shahnazari,Mohammadali Keshtparvar,Seyed Moein Ayyoubzadeh*

Main category: cs.CL

TL;DR: The paper develops a computational framework to attribute authorship for Persian classical poetry using a neural model, achieving 71% accuracy and 97% under specific confidence thresholds.


<details>
  <summary>Details</summary>
Motivation: Existing computational authorship methods struggle with Persian poetry's complex linguistic and stylistic characteristics, creating a need for better attribution systems.

Method: The authors employ a transformer-based neural model combining semantic embeddings, stylometric, and metrical features validated with a large dataset of over 647,000 verses.

Result: Their method achieves 71% accuracy via weighted voting in classification tasks and 97% accuracy in confident predictions using a threshold-based filtering at 0.9.

Conclusion: This framework introduces a novel integration of domain-specific features and neural approaches to resolve authorship attribution challenges, contributing to literature research and stylistic analysis for Persian poetry.

Abstract: The intricate linguistic, stylistic, and metrical aspects of Persian
classical poetry pose a challenge for computational authorship attribution. In
this work, we present a versatile framework to determine authorship among 67
prominent poets. We employ a multi-input neural framework consisting of a
transformer-based language encoder complemented by features addressing the
semantic, stylometric, and metrical dimensions of Persian poetry. Our feature
set encompasses 100-dimensional Word2Vec embeddings, seven stylometric
measures, and categorical encodings of poetic form and meter. We compiled a
vast corpus of 647,653 verses of the Ganjoor digital collection, validating the
data through strict preprocessing and author verification while preserving
poem-level splitting to prevent overlap. This work employs verse-level
classification and majority and weighted voting schemes in evaluation,
revealing that weighted voting yields 71% accuracy. We further investigate
threshold-based decision filtering, allowing the model to generate highly
confident predictions, achieving 97% accuracy at a 0.9 threshold, though at
lower coverage. Our work focuses on the integration of deep representational
forms with domain-specific features for improved authorship attribution. The
results illustrate the potential of our approach for automated classification
and the contribution to stylistic analysis, authorship disputes, and general
computational literature research. This research will facilitate further
research on multilingual author attribution, style shift, and generative
modeling of Persian poetry.

</details>


### [93] [LinguaSynth: Heterogeneous Linguistic Signals for News Classification](https://arxiv.org/abs/2506.21848)
*Duo Zhang,Junyi Mo*

Main category: cs.CL

TL;DR: The paper introduces LinguaSynth, a linguistically-motivated, interpretable, and computationally efficient text classification framework achieving competitive performance without deep neural networks.


<details>
  <summary>Details</summary>
Motivation: Deep learning advancements in NLP often rely on large, black-box models, raising concerns about interpretability and computational efficiency.

Method: The paper proposes LinguaSynth, which integrates five linguistic feature types into a logistic regression model, focusing on interpretability and resource efficiency.

Result: LinguaSynth achieves an 84.89% accuracy on the 20 Newsgroups dataset, surpassing TF-IDF baselines by 3.32%.

Conclusion: LinguaSynth challenges the need for deep neural networks in achieving high-performance text classification, offering a transparent and efficient alternative.

Abstract: Deep learning has significantly advanced NLP, but its reliance on large
black-box models introduces critical interpretability and computational
efficiency concerns. This paper proposes LinguaSynth, a novel text
classification framework that strategically integrates five complementary
linguistic feature types: lexical, syntactic, entity-level, word-level
semantics, and document-level semantics within a transparent logistic
regression model. Unlike transformer-based architectures, LinguaSynth maintains
interpretability and computational efficiency, achieving an accuracy of 84.89
percent on the 20 Newsgroups dataset and surpassing a robust TF-IDF baseline by
3.32 percent. Through rigorous feature interaction analysis, we show that
syntactic and entity-level signals provide essential disambiguation and
effectively complement distributional semantics. LinguaSynth sets a new
benchmark for interpretable, resource-efficient NLP models and challenges the
prevailing assumption that deep neural networks are necessary for
high-performing text classification.

</details>


### [94] [The Consistency Hypothesis in Uncertainty Quantification for Large Language Models](https://arxiv.org/abs/2506.21849)
*Quan Xiao,Debarun Bhattacharjya,Balaji Ganesan,Radu Marinescu,Katsiaryna Mirylenka,Nhan H Pham,Michael Glass,Junkyu Lee*

Main category: cs.CL

TL;DR: This paper explores confidence estimation for large language models (LLMs) using black-box uncertainty quantification (UQ). It introduces the consistency hypothesis and empirical tests to evaluate it, ultimately proposing practical methods for confidence estimation that outperform existing baselines.


<details>
  <summary>Details</summary>
Motivation: Confidence estimation for LLMs is crucial in applications requiring high trust, yet current black-box UQ methods rely on unverified assumptions such as using generation consistency as a proxy for confidence.

Method: The authors formalize the consistency hypothesis, propose three mathematical statements with statistical tests to assess it, and introduce metrics to evaluate LLM output conformity. They apply these tools across 8 datasets and 3 tasks, leveraging findings to propose new UQ methods.

Result: The investigation identifies the prevalence and utility of the consistency hypothesis, particularly the 'Sim-Any' variant, which enables improved confidence estimation. The proposed UQ methods outperform existing baselines on evaluated tasks.

Conclusion: The study reinforces the applicability of the consistency hypothesis for reliable confidence estimation and demonstrates effective, novel data-free black-box UQ methods for LLMs.

Abstract: Estimating the confidence of large language model (LLM) outputs is essential
for real-world applications requiring high user trust. Black-box uncertainty
quantification (UQ) methods, relying solely on model API access, have gained
popularity due to their practical benefits. In this paper, we examine the
implicit assumption behind several UQ methods, which use generation consistency
as a proxy for confidence, an idea we formalize as the consistency hypothesis.
We introduce three mathematical statements with corresponding statistical tests
to capture variations of this hypothesis and metrics to evaluate LLM output
conformity across tasks. Our empirical investigation, spanning 8 benchmark
datasets and 3 tasks (question answering, text summarization, and text-to-SQL),
highlights the prevalence of the hypothesis under different settings. Among the
statements, we highlight the `Sim-Any' hypothesis as the most actionable, and
demonstrate how it can be leveraged by proposing data-free black-box UQ methods
that aggregate similarities between generations for confidence estimation.
These approaches can outperform the closest baselines, showcasing the practical
value of the empirically observed consistency hypothesis.

</details>


### [95] [Derivational Probing: Unveiling the Layer-wise Derivation of Syntactic Structures in Neural Language Models](https://arxiv.org/abs/2506.21861)
*Taiga Someya,Ryo Yoshida,Hitomi Yanaka,Yohei Oseki*

Main category: cs.CL

TL;DR: The paper investigates how syntactic structures are constructed across layers in neural language models, introducing Derivational Probing as a method.


<details>
  <summary>Details</summary>
Motivation: Understanding how syntactic structures are derived in neural language models, particularly across different layers.

Method: The authors propose Derivational Probing to analyze how syntactic structures evolve hierarchically as word embeddings pass through layers.

Result: Experiments on BERT show a bottom-up derivation process, where micro-structures emerge in lower layers and integrate into macro-structures in higher layers. Timing of macro-construction is critical for downstream tasks.

Conclusion: The study highlights a systematic bottom-up formation of syntactic structures within language models and emphasizes the importance of timing in macro-syntactic construction for optimal performance.

Abstract: Recent work has demonstrated that neural language models encode syntactic
structures in their internal representations, yet the derivations by which
these structures are constructed across layers remain poorly understood. In
this paper, we propose Derivational Probing to investigate how micro-syntactic
structures (e.g., subject noun phrases) and macro-syntactic structures (e.g.,
the relationship between the root verbs and their direct dependents) are
constructed as word embeddings propagate upward across layers. Our experiments
on BERT reveal a clear bottom-up derivation: micro-syntactic structures emerge
in lower layers and are gradually integrated into a coherent macro-syntactic
structure in higher layers. Furthermore, a targeted evaluation on subject-verb
number agreement shows that the timing of constructing macro-syntactic
structures is critical for downstream performance, suggesting an optimal timing
for integrating global syntactic information.

</details>


### [96] [DeepTalk: Towards Seamless and Smart Speech Interaction with Adaptive Modality-Specific MoE](https://arxiv.org/abs/2506.21864)
*Hang Shao,Heting Gao,Yunhang Shen,Jiawei Chen,Lijiang Li,Zuwei Long,Bo Tong,Ke Li,Xing Sun*

Main category: cs.CL

TL;DR: The paper introduces DeepTalk, a modality adaptive framework for native multimodal large language models (MLLMs), aiming to improve performance issues without sacrificing the rich paralinguistic features.


<details>
  <summary>Details</summary>
Motivation: Native MLLMs struggle with catastrophic forgetting and performance degradation due to insufficient paired speech-text data compared to massive text data for LLM pretraining.

Method: DeepTalk uses a Mixture of Experts (MoE) architecture, distinguishing modality experts and applying specialized training for single modalities followed by joint multimodal collaborative training.

Result: DeepTalk reduces performance degradation, achieving only a 5.5% drop compared to the original LLM, versus the 20% degradation seen in other native MLLMs, while maintaining smooth dialogue with latency within 0.5 seconds.

Conclusion: DeepTalk offers a promising solution for native MLLMs by leveraging adaptive modality learning to maintain high multimodal performance and interaction efficiency, making them competitive with modular MLLMs.

Abstract: Native multimodal large language models (MLLMs) restructure a single large
language model (LLM) into a spoken language model (SLM) capable of both speech
and text generation. Compared to modular and aligned MLLMs, native MLLMs
preserve richer paralinguistic features such as emotion and prosody, and
generate speech responses directly within the backbone LLM rather than using a
separate speech decoder. This integration also results in lower response
latency and smoother interaction. However, native MLLMs suffer from
catastrophic forgetting and performance degradation because the available
paired speech-text data is insufficient to support the pretraining of MLLMs
compared to the vast amount of text data required to pretrain text LLMs. To
address this issue, we propose DeepTalk, a framework for adaptive modality
expert learning based on a Mixture of Experts (MoE) architecture. DeepTalk
first adaptively distinguishes modality experts according to their modality
load within the LLM. Each modality expert then undergoes specialized
single-modality training, followed by joint multimodal collaborative training.
As a result, DeepTalk incurs only a 5.5% performance drop compared to the
original LLM, which is significantly lower than the average performance drop of
over 20% typically seen in native MLLMs (such as GLM-4-Voice), and is on par
with modular MLLMs. Meanwhile, the end-to-end dialogue latency remains within
0.5 seconds, ensuring a seamless and intelligent speech interaction experience.
Code and models are released at https://github.com/talkking/DeepTalk.

</details>


### [97] [WildSpeech-Bench: Benchmarking Audio LLMs in Natural Speech Conversation](https://arxiv.org/abs/2506.21875)
*Jian Zhang,Linhao Zhang,Bokai Lei,Chuhan Wu,Wei Jia,Xiao Zhou*

Main category: cs.CL

TL;DR: This paper introduces a benchmark for evaluating multi-modal Language Models (LLMs) specifically in spoken scenarios and highlights the limitations of text-based evaluation for speech interaction.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the inadequacy of current text-based benchmarks in evaluating speech models, which don't account for challenges unique to speech such as prosody, homophones, and stuttering.

Method: The authors curated real-world datasets with diverse speaker attributes and acoustic conditions while introducing speech-specific phenomena. Additionally, they designed a query-aware evaluation method with customized checklists and prompts.

Result: Testing revealed significant differences across mainstream speech models under diverse speech scenarios, demonstrating the limitations of traditional text-based evaluations.

Conclusion: The proposed benchmark offers insights for optimizing and developing speech-focused LLMs, enabling more accurate and nuanced evaluation tailored to speech-specific interaction challenges.

Abstract: Recent multi-modal Large Language Models (LLMs) such as GPT-4o have
demonstrated strong capabilities of direct speech interaction. However, the
lack of specialized and comprehensive benchmarks for end-to-end speech LLM
evaluation hinders optimizing the user experience of Audio LLMs in real-world
applications. Existing evaluation methods often adapt text-based benchmarks,
overlooking speech's unique characteristics and challenges, including prosody,
homophones, stuttering, and differing user expectations. Here, we present a
novel approach to thoroughly evaluate LLMs in practical speech conversations.
We systematically curate real-world chat data relevant to spoken scenarios,
introduce diversity in speaker attributes and acoustic conditions, and augment
the dataset with speech-specific phenomena. We further design a query-aware
evaluation method to use customized evaluation checklists and prompts to
enhance the accuracy of automatic evaluation. We conduct comprehensive testing
and detailed analysis of various mainstream speech models, revealing
significant differences in model performance across different speech scenarios.
The use of query-aware evaluation further enables a finer-grained assessment
under various speech-specific scenarios. Our benchmark can provide valuable
insights for speech model development and evaluation.

</details>


### [98] [Do Vision-Language Models Have Internal World Models? Towards an Atomic Evaluation](https://arxiv.org/abs/2506.21876)
*Qiyue Gao,Xinyu Pi,Kevin Liu,Junrong Chen,Ruolan Yang,Xinqi Huang,Xinyu Fang,Lu Sun,Gautham Kishore,Bo Ai,Stone Tao,Mengyang Liu,Jiaxi Yang,Chao-Jung Lai,Chuanyang Jin,Jiannan Xiang,Benhao Huang,Zeming Chen,David Danks,Hao Su,Tianmin Shu,Ziqiao Ma,Lianhui Qin,Zhiting Hu*

Main category: cs.CL

TL;DR: The paper evaluates Vision-Language Models (VLMs) as internal world models (WMs) and identifies significant gaps between model and human-level understanding.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the absence of systematic evaluation of VLMs' fundamental world modeling abilities, crucial for advanced agent reasoning.

Method: Proposed a two-stage framework (Perception and Prediction) and introduced WM-ABench, a benchmark with 23 evaluation dimensions and controlled environments. Conducted 660 experiments on 15 VLMs.

Result: Identified major limitations, such as near-random accuracy in motion trajectory understanding and biased perceptions (e.g., color influencing speed prediction).

Conclusion: Current VLMs show significant weaknesses in basic world modeling, highlighting the need for advancements to close the gap with human-level reasoning.

Abstract: Internal world models (WMs) enable agents to understand the world's state and
predict transitions, serving as the basis for advanced deliberative reasoning.
Recent large Vision-Language Models (VLMs), such as OpenAI o3, GPT-4o and
Gemini, exhibit potential as general-purpose WMs. While the latest studies have
evaluated and shown limitations in specific capabilities such as visual
understanding, a systematic evaluation of VLMs' fundamental WM abilities
remains absent. Drawing on comparative psychology and cognitive science, we
propose a two-stage framework that assesses Perception (visual, spatial,
temporal, quantitative, and motion) and Prediction (mechanistic simulation,
transitive inference, compositional inference) to provide an atomic evaluation
of VLMs as WMs. Guided by this framework, we introduce WM-ABench, a large-scale
benchmark comprising 23 fine-grained evaluation dimensions across 6 diverse
simulated environments with controlled counterfactual simulations. Through 660
experiments on 15 latest commercial and open-source VLMs, we find that these
models exhibit striking limitations in basic world modeling abilities. For
instance, almost all models perform at near-random accuracy when distinguishing
motion trajectories. Additionally, they lack disentangled understanding --
e.g., some models tend to believe blue objects move faster than green ones.
More rich results and analyses reveal significant gaps between VLMs and
human-level world modeling.

</details>


### [99] [A Dual-Layered Evaluation of Geopolitical and Cultural Bias in LLMs](https://arxiv.org/abs/2506.21881)
*Sean Kim,Hyuhng Joon Kim*

Main category: cs.CL

TL;DR: The paper investigates the biases in large language models (LLMs) by examining their performance on factual and geopolitically sensitive queries using a two-phase evaluation across multiple languages.


<details>
  <summary>Details</summary>
Motivation: To understand LLM behavior in factual and disputable scenarios, especially as their outputs influence public opinion or reinforce dominant narratives.

Method: The authors define two types of biases—model bias and inference bias—and assess their effects through a two-phase evaluation using a curated multilingual dataset.

Result: Phase 1 reveals query language-induced alignment in factual questions, while Phase 2 reflects how training context and query language interact in geopolitically sensitive queries.

Conclusion: The study provides a framework to evaluate LLM behavior in neutral and sensitive contexts, helping guide culturally aware LLM deployment and evaluation practices in multilingual environments.

Abstract: As large language models (LLMs) are increasingly deployed across diverse
linguistic and cultural contexts, understanding their behavior in both factual
and disputable scenarios is essential, especially when their outputs may shape
public opinion or reinforce dominant narratives. In this paper, we define two
types of bias in LLMs: model bias (bias stemming from model training) and
inference bias (bias induced by the language of the query), through a two-phase
evaluation. Phase 1 evaluates LLMs on factual questions where a single
verifiable answer exists, assessing whether models maintain consistency across
different query languages. Phase 2 expands the scope by probing geopolitically
sensitive disputes, where responses may reflect culturally embedded or
ideologically aligned perspectives. We construct a manually curated dataset
spanning both factual and disputable QA, across four languages and question
types. The results show that Phase 1 exhibits query language induced alignment,
while Phase 2 reflects an interplay between the model's training context and
query language. This paper offers a structured framework for evaluating LLM
behavior across neutral and sensitive topics, providing insights for future LLM
deployment and culturally aware evaluation practices in multilingual contexts.

</details>


### [100] [AutoMixer: Checkpoint Artifacts as Automatic Data Mixers](https://arxiv.org/abs/2506.21910)
*Ernie Chang,Yang Li,Patrick Huber,David Kant,Yangyang Shi,Vikas Chandra*

Main category: cs.CL

TL;DR: The paper explores using checkpoint models during training to improve task-specific capabilities in language models by leveraging their performance signals.


<details>
  <summary>Details</summary>
Motivation: Language models require the right data mixtures to enhance their task-specific capabilities, but identifying such optimal data is challenging due to complex relationships between data and tasks.

Method: The authors observe emerging capabilities in checkpoint models during training, utilize these checkpoints for data mixing, and apply first-order influence approximation over source data.

Result: The framework showed up to 1.93% performance improvement across eight reasoning benchmarks during pretraining.

Conclusion: Checkpoint models can be effectively leveraged to optimize data mixtures and improve the quality of data for language model training.

Abstract: In language model training, it is desirable to equip models with capabilities
from various tasks. However, it is not clear how to directly obtain the right
data mixtures for these capabilities as the relationship between data and tasks
is difficult to be modeled. In this work, we observe that checkpoint models
exhibit emerging capabilities at different points in the training trajectory.
Often, the training process saves checkpoints as artifacts that are
under-utilized as a source of in-training data signals. We identify these
artifact models based on their respective capabilities on the benchmarks and
leverage them as data mixers by using their aggregated first-order influence
approximation over source data. We demonstrated on eight reasoning benchmarks
that the proposed framework shows significant improvements in the pretraining
setting, with performance improvements of up to 1.93%. Overall, this shows the
potential of checkpoint models to enhance data quality and optimize data
mixtures.

</details>


### [101] [PapersPlease: A Benchmark for Evaluating Motivational Values of Large Language Models Based on ERG Theory](https://arxiv.org/abs/2506.21961)
*Junho Myung,Yeon Su Park,Sunwoo Kim,Shin Yoo,Alice Oh*

Main category: cs.CL

TL;DR: The paper introduces 'PapersPlease,' a benchmark with 3,700 moral dilemmas for investigating biases in large language models' decision-making related to human needs and social identities.


<details>
  <summary>Details</summary>
Motivation: To study how large language models prioritize human needs and uncover biases in their decision-making during role-playing scenarios.

Method: The study developed short narratives based on the Existence, Relatedness, and Growth (ERG) theory and tasked six LLMs with immigration approval decisions, analyzing their choices for patterns and biases.

Result: The analysis revealed statistically significant implicit preferences and varying responsiveness to social identity cues, including higher denial rates for marginalized identities in certain models.

Conclusion: LLMs demonstrate biased decision-making influenced by human needs and social identities, highlighting the importance of addressing biases in these models.

Abstract: Evaluating the performance and biases of large language models (LLMs) through
role-playing scenarios is becoming increasingly common, as LLMs often exhibit
biased behaviors in these contexts. Building on this line of research, we
introduce PapersPlease, a benchmark consisting of 3,700 moral dilemmas designed
to investigate LLMs' decision-making in prioritizing various levels of human
needs. In our setup, LLMs act as immigration inspectors deciding whether to
approve or deny entry based on the short narratives of people. These narratives
are constructed using the Existence, Relatedness, and Growth (ERG) theory,
which categorizes human needs into three hierarchical levels. Our analysis of
six LLMs reveals statistically significant patterns in decision-making,
suggesting that LLMs encode implicit preferences. Additionally, our evaluation
of the impact of incorporating social identities into the narratives shows
varying responsiveness based on both motivational needs and identity cues, with
some models exhibiting higher denial rates for marginalized identities. All
data is publicly available at https://github.com/yeonsuuuu28/papers-please.

</details>


### [102] [More Vulnerable than You Think: On the Stability of Tool-Integrated LLM Agents](https://arxiv.org/abs/2506.21967)
*Weimin Xiong,Ke Wang,Yifan Song,Hanchao Liu,Sai Zhou,Wei Peng,Sujian Li*

Main category: cs.CL

TL;DR: This paper studies the stability of LLM-based tool agents and reveals vulnerabilities throughout the tool invocation process, emphasizing the need for stability evaluation in agent design.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address a gap in evaluating the stability of tool-integrated LLM agents, as existing evaluations focus mostly on end-to-end functionality without considering factors that lead to crashes or abnormal behavior.

Method: The study conducts extensive experiments to assess the susceptibility of agents to errors during various phases of tool invocation—such as reading tool documentation, selecting tools, generating parameters, and processing tool responses.

Result: The researchers found that LLM agents are prone to errors at every stage of the tool invocation process. Open-source models are more vulnerable compared to proprietary ones, and increasing model size does not necessarily improve reasoning but may heighten susceptibility to user-like attacks.

Conclusion: The findings stress the need to prioritize stability evaluation in LLM-based agent development and provide guidance for improving robustness in future systems.

Abstract: Current evaluations of tool-integrated LLM agents typically focus on
end-to-end tool-usage evaluation while neglecting their stability. This limits
their real-world applicability, as various internal or external factors can
cause agents to crash or behave abnormally. Our research addresses this by
investigating whether agents are vulnerable to errors throughout the entire
tool invocation process, including reading tool documentation, selecting tools
and generating parameters, and processing the tool's response. Through
extensive experiments, we observe that agents are highly susceptible to errors
at each stage and agents based on open-source models are more vulnerable than
those based on proprietary models. We also find that increasing the model size
does not significantly improve tool invocation reasoning and may make agents
more vulnerable to attacks resembling normal user instructions. This highlights
the importance of evaluating agent stability and offers valuable insights for
future LLM development and evaluation.

</details>


### [103] [Advancing Jailbreak Strategies: A Hybrid Approach to Exploiting LLM Vulnerabilities and Bypassing Modern Defenses](https://arxiv.org/abs/2506.21972)
*Mohamed Ahmed,Mohamed Abdelmouty,Mingyu Kim,Gunvanth Kandula,Alex Park,James C. Davis*

Main category: cs.CL

TL;DR: The paper proposes hybrid approaches combining token-level and prompt-level techniques to improve jailbreak attack effectiveness on Pre-Trained Language Models (PTLMs) and Large Language Models (LLMs), revealing vulnerabilities in current safety measures.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing jailbreak mechanisms in attacking PTLMs and LLMs while exposing their vulnerabilities.

Method: The authors introduce two hybrid methods—GCG + PAIR and GCG + WordGame—which combine token-level and prompt-level attack techniques to pierce defenses of various LLMs.

Result: The hybrid approaches significantly improved attack success rates, with GCG + PAIR achieving up to 91.6% on undefended models like Llama-3, and both methods retained effectiveness against advanced defenses.

Conclusion: The study identifies new vulnerabilities in existing LLM safety measures, demonstrating trade-offs between attack success and defensive robustness, and calls for comprehensive safety safeguards against adaptive adversaries.

Abstract: The advancement of Pre-Trained Language Models (PTLMs) and Large Language
Models (LLMs) has led to their widespread adoption across diverse applications.
Despite their success, these models remain vulnerable to attacks that exploit
their inherent weaknesses to bypass safety measures. Two primary
inference-phase threats are token-level and prompt-level jailbreaks.
Token-level attacks embed adversarial sequences that transfer well to black-box
models like GPT but leave detectable patterns and rely on gradient-based token
optimization, whereas prompt-level attacks use semantically structured inputs
to elicit harmful responses yet depend on iterative feedback that can be
unreliable. To address the complementary limitations of these methods, we
propose two hybrid approaches that integrate token- and prompt-level techniques
to enhance jailbreak effectiveness across diverse PTLMs. GCG + PAIR and the
newly explored GCG + WordGame hybrids were evaluated across multiple Vicuna and
Llama models. GCG + PAIR consistently raised attack-success rates over its
constituent techniques on undefended models; for instance, on Llama-3, its
Attack Success Rate (ASR) reached 91.6%, a substantial increase from PAIR's
58.4% baseline. Meanwhile, GCG + WordGame matched the raw performance of
WordGame maintaining a high ASR of over 80% even under stricter evaluators like
Mistral-Sorry-Bench. Crucially, both hybrids retained transferability and
reliably pierced advanced defenses such as Gradient Cuff and JBShield, which
fully blocked single-mode attacks. These findings expose previously unreported
vulnerabilities in current safety stacks, highlight trade-offs between raw
success and defensive robustness, and underscore the need for holistic
safeguards against adaptive adversaries.

</details>


### [104] [Don't Trust Generative Agents to Mimic Communication on Social Networks Unless You Benchmarked their Empirical Realism](https://arxiv.org/abs/2506.21974)
*Simon Münker,Nils Schwager,Achim Rettinger*

Main category: cs.CL

TL;DR: This paper evaluates the use of Large Language Models (LLMs) for social simulations, focusing on replicating social network user behavior and communication.


<details>
  <summary>Details</summary>
Motivation: Researchers aim to assess if empirical studies of human behavior can be performed via LLMs, especially considering conflicting findings about their validity.

Method: The study provides a formal framework for social network simulation, experiments with LLMs to replicate user communication on platforms like X (formerly known as Twitter) in English and German, and evaluates the realism of social simulations.

Result: The paper suggests that validating social simulations requires ensuring they align with the empirical realism of actual settings where simulation parameters were derived.

Conclusion: The authors advocate for stricter standards and verification when utilizing generative-agent-based approaches for simulating social behaviors.

Abstract: The ability of Large Language Models (LLMs) to mimic human behavior triggered
a plethora of computational social science research, assuming that empirical
studies of humans can be conducted with AI agents instead. Since there have
been conflicting research findings on whether and when this hypothesis holds,
there is a need to better understand the differences in their experimental
designs. We focus on replicating the behavior of social network users with the
use of LLMs for the analysis of communication on social networks. First, we
provide a formal framework for the simulation of social networks, before
focusing on the sub-task of imitating user communication. We empirically test
different approaches to imitate user behavior on X in English and German. Our
findings suggest that social simulations should be validated by their empirical
realism measured in the setting in which the simulation components were fitted.
With this paper, we argue for more rigor when applying generative-agent-based
modeling for social simulation.

</details>


### [105] [Analyzing and Fine-Tuning Whisper Models for Multilingual Pilot Speech Transcription in the Cockpit](https://arxiv.org/abs/2506.21990)
*Kartheek Kumar Reddy Nareddy,Sarah Ternus,Julia Niebling*

Main category: cs.CL

TL;DR: The paper focuses on improving transcription accuracy of cockpit conversations via the Whisper model by leveraging fine-tuning (LoRA) and normalization schemes, significantly reducing Word Error Rate.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of pre-trained transformer models in niche domains, specifically for transcribing cockpit conversations involving specialized and multilingual vocabulary.

Method: The authors proposed normalization schemes for refining transcripts and applied performance-efficient fine-tuning using Low-Rank Adaptation (LoRA) on collected cockpit recordings.

Result: The transcription accuracy significantly improved, with Word Error Rate reduced from 68.49% to 26.26% using the fine-tuned Whisper model and normalization techniques.

Conclusion: The study demonstrates the effectiveness of targeted fine-tuning and normalization in enhancing transcription performance for niche domains like aviation communication.

Abstract: The developments in transformer encoder-decoder architectures have led to
significant breakthroughs in machine translation, Automatic Speech Recognition
(ASR), and instruction-based chat machines, among other applications. The
pre-trained models were trained on vast amounts of generic data over a few
epochs (fewer than five in most cases), resulting in their strong
generalization capabilities. Nevertheless, the performance of these models does
suffer when applied to niche domains like transcribing pilot speech in the
cockpit, which involves a lot of specific vocabulary and multilingual
conversations. This paper investigates and improves the transcription accuracy
of cockpit conversations with Whisper models. We have collected around 85
minutes of cockpit simulator recordings and 130 minutes of interview recordings
with pilots and manually labeled them. The speakers are middle aged men
speaking both German and English. To improve the accuracy of transcriptions, we
propose multiple normalization schemes to refine the transcripts and improve
Word Error Rate (WER). We then employ fine-tuning to enhance ASR performance,
utilizing performance-efficient fine-tuning with Low-Rank Adaptation (LoRA).
Hereby, WER decreased from 68.49 \% (pretrained whisper Large model without
normalization baseline) to 26.26\% (finetuned whisper Large model with the
proposed normalization scheme).

</details>


### [106] [Can Peter Pan Survive MT? A Stylometric Study of LLMs, NMTs, and HTs in Children's Literature Translation](https://arxiv.org/abs/2506.22038)
*Delu Kong,Lieve Macken*

Main category: cs.CL

TL;DR: This study compares English-to-Chinese children's literature translations from humans, large language models, and neural machine translations using stylometric analysis.


<details>
  <summary>Details</summary>
Motivation: To evaluate the quality and distinguishable stylistic features of machine translation (MT) versus human translation (HT) in children's literature, focusing on creativity and style.

Method: Constructed a translation corpus of Peter Pan, analyzed 447 linguistic features using machine learning classification and clustering to compare translations.

Result: HTs and MTs differ significantly in generic linguistic features; LLMs outperform NMTs in creative text translation features and align more closely with HTs.

Conclusion: LLMs show potential for stylistically faithful translations in children's literature, outperforming NMTs in stylistic characteristics.

Abstract: This study focuses on evaluating the performance of machine translations
(MTs) compared to human translations (HTs) in English-to-Chinese children's
literature translation (CLT) from a stylometric perspective. The research
constructs a Peter Pan corpus, comprising 21 translations: 7 human translations
(HTs), 7 large language model translations (LLMs), and 7 neural machine
translation outputs (NMTs). The analysis employs a generic feature set
(including lexical, syntactic, readability, and n-gram features) and a creative
text translation (CTT-specific) feature set, which captures repetition, rhythm,
translatability, and miscellaneous levels, yielding 447 linguistic features in
total.
  Using classification and clustering techniques in machine learning, we
conduct a stylometric analysis of these translations. Results reveal that in
generic features, HTs and MTs exhibit significant differences in conjunction
word distributions and the ratio of 1-word-gram-YiYang, while NMTs and LLMs
show significant variation in descriptive words usage and adverb ratios.
Regarding CTT-specific features, LLMs outperform NMTs in distribution, aligning
more closely with HTs in stylistic characteristics, demonstrating the potential
of LLMs in CLT.

</details>


### [107] [Decoding Machine Translationese in English-Chinese News: LLMs vs. NMTs](https://arxiv.org/abs/2506.22050)
*Delu Kong,Lieve Macken*

Main category: cs.CL

TL;DR: The study investigates the unique linguistic features of machine translation (MT) outputs, particularly for English-to-Chinese translations in news texts. Results confirm observable patterns for both Neural Machine Translation (NMTs) and Large Language Models (LLMs).


<details>
  <summary>Details</summary>
Motivation: The paper aims to address gaps in understanding the linguistic peculiarities of machine translation (MTese) for English-to-Chinese translations, an under-researched area, especially in news-related texts.

Method: The authors built a large dataset with 4 sub-corpora, applied a five-layer linguistic feature set, and used a chi-square algorithm for feature selection in classification and clustering tasks.

Result: The study found distinguishable linguistic patterns in MT outputs, such as shorter sentences and more use of adversative conjunctions. LLMs displayed greater lexical diversity, while NMTs used more brackets. Classification between LLM and NMT outputs achieved about 70% accuracy.

Conclusion: Machine Translationese is evident in both LLMs and NMTs, with their outputs differing notably from original Chinese. Differences also exist between specific types of LLMs, but no significant variation was found between LLMs developed by Chinese vs. foreign entities.

Abstract: This study explores Machine Translationese (MTese) -- the linguistic
peculiarities of machine translation outputs -- focusing on the
under-researched English-to-Chinese language pair in news texts. We construct a
large dataset consisting of 4 sub-corpora and employ a comprehensive five-layer
feature set. Then, a chi-square ranking algorithm is applied for feature
selection in both classification and clustering tasks. Our findings confirm the
presence of MTese in both Neural Machine Translation systems (NMTs) and Large
Language Models (LLMs). Original Chinese texts are nearly perfectly
distinguishable from both LLM and NMT outputs. Notable linguistic patterns in
MT outputs are shorter sentence lengths and increased use of adversative
conjunctions. Comparing LLMs and NMTs, we achieve approximately 70%
classification accuracy, with LLMs exhibiting greater lexical diversity and
NMTs using more brackets. Additionally, translation-specific LLMs show lower
lexical diversity but higher usage of causal conjunctions compared to generic
LLMs. Lastly, we find no significant differences between LLMs developed by
Chinese firms and their foreign counterparts.

</details>


### [108] [Lost at the Beginning of Reasoning](https://arxiv.org/abs/2506.22058)
*Baohao Liao,Xinyi Chen,Sara Rajaee,Yuhui Xu,Christian Herold,Anders Søgaard,Maarten de Rijke,Christof Monz*

Main category: cs.CL

TL;DR: This paper explores the impact of initial reasoning steps on the overall accuracy of large language models in complex chain-of-thought reasoning and proposes a sampling strategy to mitigate errors.


<details>
  <summary>Details</summary>
Motivation: Understanding and improving the self-correction capabilities of large language models during extended chain-of-thought reasoning, particularly addressing issues like overthinking.

Method: The authors propose an efficient sampling strategy that uses a reward model to filter out suboptimal initial reasoning steps while retaining high-quality ones.

Result: The proposed sampling strategy reduces inference costs by up to 70% without sacrificing prediction accuracy, and the authors observe that initial reasoning steps heavily influence final outcomes.

Conclusion: Errors in initial reasoning steps significantly impact reasoning quality in LLMs, and addressing these early issues can improve efficiency and robustness. A benchmark for flawed first reasoning steps is introduced to evaluate self-correction mechanisms.

Abstract: Recent advancements in large language models (LLMs) have significantly
advanced complex reasoning capabilities, particularly through extended
chain-of-thought (CoT) reasoning that incorporates mechanisms such as
backtracking, self-reflection and self-correction. Despite these developments,
the self-correction abilities of LLMs during long CoT reasoning remain
underexplored. And recent findings on overthinking suggest that such models
often engage in unnecessarily redundant reasoning. In this work, we empirically
show that the first reasoning step exerts a disproportionately large influence
on the final prediction - errors introduced at this stage can substantially
degrade subsequent reasoning quality. This phenomenon is consistently observed
across two state-of-the-art open-source reasoning model families: DeepSeek-R1
and Qwen3. To address this, we propose an efficient sampling strategy that
leverages a reward model to identify and retain high-quality first reasoning
steps while discarding suboptimal ones, achieving up to a 70% reduction in
inference cost without sacrificing accuracy. Finally, we introduce a new
benchmark specifically constructed with deliberately flawed first reasoning
steps to systematically evaluate model self-correction capabilities, offering a
foundation for future research on robust reasoning in LLMs.

</details>


### [109] [MDC-R: The Minecraft Dialogue Corpus with Reference](https://arxiv.org/abs/2506.22062)
*Chris Madge,Maris Camilleri,Paloma Carretero Garcia,Mladen Karan,Juexi Shao,Prashant Jayannavar,Julian Hough,Benjamin Roth,Massimo Poesio*

Main category: cs.CL

TL;DR: The paper introduces the Minecraft Dialogue Corpus with Reference (MDC-R), enhancing the original MDC with expert annotations of anaphoric and deictic references, and demonstrates its utility for referring expression comprehension.


<details>
  <summary>Details</summary>
Motivation: To supplement the original Minecraft Dialogue Corpus (MDC) with annotations related to anaphoric and deictic reference to better understand linguistic phenomena in task-oriented, multi-turn dialogue contexts.

Method: The researchers annotated the original MDC with expert-level annotations for anaphoric and deictic reference, followed by a quantitative and qualitative analysis of the data. Additionally, an experiment was conducted to validate the corpus' usefulness.

Result: The resulting annotated corpus demonstrates the utility of additional reference layers and provides valuable data for studying linguistic phenomena. An experiment confirmed its effectiveness for referring expression comprehension.

Conclusion: MDC-R is a valuable resource for linguistic research, advancing the understanding of reference in dynamic, situated dialogue environments. The enhancements could be pivotal for tasks like reference comprehension.

Abstract: We introduce the Minecraft Dialogue Corpus with Reference (MDC-R). MDC-R is a
new language resource that supplements the original Minecraft Dialogue Corpus
(MDC) with expert annotations of anaphoric and deictic reference. MDC's
task-orientated, multi-turn, situated dialogue in a dynamic environment has
motivated multiple annotation efforts, owing to the interesting linguistic
phenomena that this setting gives rise to. We believe it can serve as a
valuable resource when annotated with reference, too. Here, we discuss our
method of annotation and the resulting corpus, and provide both a quantitative
and a qualitative analysis of the data. Furthermore, we carry out a short
experiment demonstrating the usefulness of our corpus for referring expression
comprehension.

</details>


### [110] [Involvement drives complexity of language in online debates](https://arxiv.org/abs/2506.22098)
*Eleonora Amadori,Daniele Cirulli,Edoardo Di Martino,Jacopo Nudo,Maria Sahakyan,Emanuele Sangiorgio,Arnaldo Santoro,Simon Zollo,Alessandro Galeazzi,Niccolò Di Marco*

Main category: cs.CL

TL;DR: This paper analyzes the linguistic complexity of tweets on COVID-19, COP26, and the Russia-Ukraine war, revealing differences across account type, political leaning, content reliability, and sentiment.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the need to understand how social media platforms reshape public discourse and reflect societal and ideological structures through language.

Method: The study uses a combination of textual complexity measures to analyze tweets, categorizing them by account type, political leaning, content reliability, and sentiment.

Result: The analysis showed significant linguistic complexity differences across all dimensions, including higher complexity in negative/offensive tweets and convergence of similar political stances on common jargon.

Conclusion: The findings enhance understanding of sociolinguistic dynamics on digital platforms and highlight language’s role in reflecting online ideological and social structures.

Abstract: Language is a fundamental aspect of human societies, continuously evolving in
response to various stimuli, including societal changes and intercultural
interactions. Technological advancements have profoundly transformed
communication, with social media emerging as a pivotal force that merges
entertainment-driven content with complex social dynamics. As these platforms
reshape public discourse, analyzing the linguistic features of user-generated
content is essential to understanding their broader societal impact. In this
paper, we examine the linguistic complexity of content produced by influential
users on Twitter across three globally significant and contested topics:
COVID-19, COP26, and the Russia-Ukraine war. By combining multiple measures of
textual complexity, we assess how language use varies along four key
dimensions: account type, political leaning, content reliability, and
sentiment. Our analysis reveals significant differences across all four axes,
including variations in language complexity between individuals and
organizations, between profiles with sided versus moderate political views, and
between those associated with higher versus lower reliability scores.
Additionally, profiles producing more negative and offensive content tend to
use more complex language, with users sharing similar political stances and
reliability levels converging toward a common jargon. Our findings offer new
insights into the sociolinguistic dynamics of digital platforms and contribute
to a deeper understanding of how language reflects ideological and social
structures in online spaces.

</details>


### [111] [Identifying a Circuit for Verb Conjugation in GPT-2](https://arxiv.org/abs/2506.22105)
*David Demitri Africa*

Main category: cs.CL

TL;DR: The paper explores the mechanisms behind subject-verb agreement in GPT-2 Small by isolating a sub-network that contributes to verb conjugation.


<details>
  <summary>Details</summary>
Motivation: The motivation is to better understand how GPT-2 handles grammatical tasks like subject-verb agreement by identifying specific network components responsible for such tasks.

Method: The study employs techniques such as performance verification, automatic circuit discovery using direct path patching, and direct logit attribution to isolate the sub-network involved in subject-verb agreement.

Result: A candidate circuit responsible for verb conjugation is identified, showing that only a small number of network components are sufficient for the base task.

Conclusion: This research provides insights into the functional mechanisms of language models, demonstrating efficient sub-network usage for simpler tasks but increased complexity for harder ones.

Abstract: I implement a procedure to isolate and interpret the sub-network (or
"circuit") responsible for subject-verb agreement in GPT-2 Small. In this
study, the model is given prompts where the subject is either singular (e.g.
"Alice") or plural (e.g. "Alice and Bob"), and the task is to correctly predict
the appropriate verb form ("walks" for singular subjects, "walk" for plural
subjects). Using a series of techniques-including performance verification
automatic circuit discovery via direct path patching, and direct logit
attribution- I isolate a candidate circuit that contributes significantly to
the model's correct verb conjugation. The results suggest that only a small
fraction of the network's component-token pairs is needed to achieve near-model
performance on the base task but substantially more for more complex settings.

</details>


### [112] [DAPFAM: A Domain-Aware Patent Retrieval Dataset Aggregated at the Family Level](https://arxiv.org/abs/2506.22141)
*Iliass Ayaou,Denis Cavallucci,Hicham Chibane*

Main category: cs.CL

TL;DR: The paper introduces DAPFAM, a publicly available domain-aware patent retrieval dataset addressing gaps in labeling, jurisdiction coverage, and computational scalability.


<details>
  <summary>Details</summary>
Motivation: Address shortcomings in existing patent datasets by providing a resource with explicit labeling, balanced representation, multi-jurisdiction coverage, and moderate computational requirements.

Method: Developed DAPFAM using a thorough three-step data-curation pipeline, including relevance judgments and novel labeling based on IPC codes, providing a multi-jurisdictional dataset.

Result: DAPFAM includes 1,247 domain-balanced queries and 45,336 targets, with 49,869 explicit evaluation pairs. Baseline experiments difficulty in cross-domain retrieval.

Conclusion: DAPFAM is a scalable patent retrieval dataset valuable for researchers and supports sub-document-level experiments. It highlights cross-domain retrieval challenges for future exploration.

Abstract: In the landscape of publicly available patent retrieval datasets, the need
for explicit indomain and out-of-domain labeling, multi-jurisdiction coverage,
balanced query domain representation and manageable sizes that support sub
document level experiments on moderate computational resources is often
overlooked. To address these gaps, we propose DAPFAM, a new open access
domain-aware patent retrieval dataset constructed at the simple-family level.
The dataset contains 1,247 domain balanced full text query families and 45,336
full text target families. The dataset is enriched by clear relevance judgments
(forward/backward citations as positive links, random negatives), as well as
explicit in-domain or out-of-domain relationships via a novel proposed
labelling scheme based on via International Patent Classification (IPC) codes,
resulting in 49,869 evaluation pairs. The dataset is multi jurisdictional,
requires little to no preprocessing for retrieval evaluation, and remains of a
size manageable for entities with limited ressources allowing for sub document
level retrieval experiments without excessive computational costs. We describe
our three-step data-curation pipeline, present comprehensive dataset
statistics, and provide baseline experiments using lexical and neural retrieval
methods. Our baseline experiments highlight significant challenges in
crossdomain patent retrieval. The dataset will be publicly available (for now
the access link is this repository:
https://osf.io/vbyzd/?view_only=1a40242e0d1941a58aa854af3e50cf6b).

</details>


### [113] [SAGE: Spliced-Audio Generated Data for Enhancing Foundational Models in Low-Resource Arabic-English Code-Switched Speech Recognition](https://arxiv.org/abs/2506.22143)
*Muhammad Umar Farooq,Oscar Saz*

Main category: cs.CL

TL;DR: This study tackles dialectal Arabic and Arabic-English code-switched speech challenges by using modified data generation and model fine-tuning techniques, achieving notable improvements in speech recognition metrics.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the scarcity of labeled data for Arabic-English code-switched speech and evaluate the effectiveness of speech self-supervised learning (SSL) models in this domain.

Method: The authors develop a modified audio-splicing approach called Spliced-Audio Generated (SAGE) data to generate artificial code-switched speech. They also propose an Experience Replay-inspired method to improve model generalization and employ a fine-tuning technique with a language model integration.

Result: The methods led to an improvement in Word Error Rate (WER), including a reduction of mean WER from 31.7% to 26.6% and surpassing large multilingual models by margins of 5.5% and 8.4%.

Conclusion: The combination of SAGE data generation, language modeling, and Experience Replay approaches effectively improves code-switched speech recognition, providing competitive performance compared to much larger models.

Abstract: This paper investigates the performance of various speech SSL models on
dialectal Arabic (DA) and Arabic-English code-switched (CS) speech. To address
data scarcity, a modified audio-splicing approach is introduced to generate
artificial CS speech data. Fine-tuning an already fine-tuned SSL model with the
proposed Spliced-Audio Generated (SAGE) data results in an absolute improvement
on Word Error Rate (WER) of 7.8% on Arabic and English CS benchmarks.
Additionally, an Experience Replay (ER) inspired approach is proposed to
enhance generalisation across DA and CS speech while mitigating catastrophic
forgetting. Integrating an out-of-domain 3-gram language model reduces the
overall mean WER from 31.7% to 26.6%. Few-shot fine-tuning for code-switching
benchmarks further improves WER by 4.9%. A WER of 31.1% on Arabic-English CS
benchmarks surpasses large-scale multilingual models, including USM and
Whisper-large-v2 (both over ten times larger) by an absolute margin of 5.5% and
8.4%, respectively.

</details>


### [114] [Training Language Model to Critique for Better Refinement](https://arxiv.org/abs/2506.22157)
*Tianshu Yu,Chao Xiang,Mingchuan Yang,Pei Ke,Bosi Wen,Cunxiang Wang,Jiale Cheng,Li Zhang,Xinyu Mu,Chuxiong Sun,Minlie Huang*

Main category: cs.CL

TL;DR: The paper introduces Refinement-oriented Critique Optimization (RCO), a framework to train critic models using refinement feedback for improving responses in large language models (LLMs).


<details>
  <summary>Details</summary>
Motivation: To address the lack of research on effective critique types and their generation for enhancing LLM responses.

Method: The RCO framework employs critique utility (CU) as a feedback mechanism to train critic models, rewarding critiques that lead to improved refinements in model responses.

Result: RCO outperforms traditional approaches in tasks like dialog generation, summarization, and code generation, showing better critique quality and refinement outcomes.

Conclusion: The RCO framework provides an efficient methodology for critique-refinement optimization, improving the performance of LLMs in various tasks without requiring direct critique preference assessments.

Abstract: Large language models (LLMs) have demonstrated remarkable evaluation and
critique capabilities, providing insightful feedback and identifying flaws in
various tasks. However, limited research has explored which types of critiques
are most effective for improving model responses or how to generate such
critiques. To address this gap, we introduce \textbf{R}efinement-oriented
\textbf{C}ritique \textbf{O}ptimization (RCO), a novel framework designed to
train critic models using refinement signals. RCO uses a feedback loop where
critiques, generated by the critic model, guide the actor model in refining its
responses. The critique utility (CU) quantifies the effectiveness of these
refinements, serving as the reward signal for training the critic model. By
focusing on critiques that lead to better refinements, RCO eliminates the need
for direct critique preference assessment, ensuring that critiques driving
meaningful improvements are rewarded. We evaluate RCO across five tasks, i.e.,
dialog generation, summarization, question answering, mathematical reasoning,
and code generation, and show that it significantly outperforms traditional
methods and open-source models in terms of critique quality and refinement
outcomes. Our contributions include the introduction of RCO, a novel
supervision scheme based on refined response preferences, and comprehensive
experimental results that highlight the method's effectiveness in enhancing LLM
critique-refinement loops.

</details>


### [115] [Leveraging In-Context Learning for Political Bias Testing of LLMs](https://arxiv.org/abs/2506.22232)
*Patrick Haller,Jannis Vamvas,Rico Sennrich,Lena A. Jäger*

Main category: cs.CL

TL;DR: This paper introduces a new probing task called Questionnaire Modeling (QM) to evaluate political bias in large language models (LLMs) with greater stability, using human survey data as in-context examples.


<details>
  <summary>Details</summary>
Motivation: The existing method of querying LLMs with political questions to assess biases has proven unreliable due to its limited stability, making model comparisons difficult.

Method: The authors propose Questionnaire Modeling (QM), a method that incorporates human survey data as in-context examples to improve the evaluation of political bias in LLMs.

Result: Experiments reveal that QM enhances the stability of bias evaluations, facilitates comparisons between instruction-tuned and base models, demonstrates changes in bias direction due to instruction tuning, and shows that larger models utilize in-context examples effectively with generally smaller bias scores.

Conclusion: Questionnaire Modeling provides a more robust framework for evaluating and comparing bias in LLMs, revealing insights about the impact of model size and instruction tuning on political bias.

Abstract: A growing body of work has been querying LLMs with political questions to
evaluate their potential biases. However, this probing method has limited
stability, making comparisons between models unreliable. In this paper, we
argue that LLMs need more context. We propose a new probing task, Questionnaire
Modeling (QM), that uses human survey data as in-context examples. We show that
QM improves the stability of question-based bias evaluation, and demonstrate
that it may be used to compare instruction-tuned models to their base versions.
Experiments with LLMs of various sizes indicate that instruction tuning can
indeed change the direction of bias. Furthermore, we observe a trend that
larger models are able to leverage in-context examples more effectively, and
generally exhibit smaller bias scores in QM. Data and code are publicly
available.

</details>


### [116] [Detection of Personal Data in Structured Datasets Using a Large Language Model](https://arxiv.org/abs/2506.22305)
*Albert Agisha Ntwali,Luca Rück,Martin Heckmann*

Main category: cs.CL

TL;DR: The paper introduces a novel personal data detection approach using GPT-4o, leveraging contextual information. It outperforms existing solutions, particularly on datasets enriched by context.


<details>
  <summary>Details</summary>
Motivation: There is a need for robust methods to detect personal data in structured datasets, especially those leveraging contextual information for better performance.

Method: The method uses GPT-4o, incorporating contextual information from feature names, values, and dataset descriptions, and evaluates performance against alternatives across multiple datasets.

Result: GPT-4o showed superior performance in most cases, especially on Kaggle and OpenML datasets. However, performance was comparable across methods on the medical dataset MIMIC-Demo-Ext.

Conclusion: Future advancements require more diverse real-world datasets with personal information for evaluation and improvement.

Abstract: We propose a novel approach for detecting personal data in structured
datasets, leveraging GPT-4o, a state-of-the-art Large Language Model. A key
innovation of our method is the incorporation of contextual information: in
addition to a feature's name and values, we utilize information from other
feature names within the dataset as well as the dataset description. We compare
our approach to alternative methods, including Microsoft Presidio and CASSED,
evaluating them on multiple datasets: DeSSI, a large synthetic dataset,
datasets we collected from Kaggle and OpenML as well as MIMIC-Demo-Ext, a
real-world dataset containing patient information from critical care units.
  Our findings reveal that detection performance varies significantly depending
on the dataset used for evaluation. CASSED excels on DeSSI, the dataset on
which it was trained. Performance on the medical dataset MIMIC-Demo-Ext is
comparable across all models, with our GPT-4o-based approach clearly
outperforming the others. Notably, personal data detection in the Kaggle and
OpenML datasets appears to benefit from contextual information. This is
evidenced by the poor performance of CASSED and Presidio (both of which do not
utilize the context of the dataset) compared to the strong results of our
GPT-4o-based approach.
  We conclude that further progress in this field would greatly benefit from
the availability of more real-world datasets containing personal information.

</details>


### [117] [Evaluating Scoring Bias in LLM-as-a-Judge](https://arxiv.org/abs/2506.22316)
*Qingquan Li,Shaoyu Dou,Kailai Shao,Chao Chen,Haixiang Hu*

Main category: cs.CL

TL;DR: The paper investigates biases present in Large Language Models used as evaluators, specifically focusing on scoring biases, and provides methodologies to evaluate and mitigate them.


<details>
  <summary>Details</summary>
Motivation: Large Language Models are increasingly used as judges in various fields. However, these models exhibit biases that compromise the fairness and reliability of their evaluations.

Method: The authors define 'scoring bias' and propose a framework with evaluation metrics. They augment existing benchmarks and incorporate data synthesis to create an evaluation dataset.

Result: Experimental results show that scoring biases disrupt judge model stability, and exploratory experiments provide insights into mitigating these biases.

Conclusion: The paper suggests approaches to reduce scoring bias in LLM-as-a-Judge models by improving scoring prompt templates, score rubrics, and reference answer selection.

Abstract: The remarkable performance of Large Language Models (LLMs) gives rise
to``LLM-as-a-Judge'', where LLMs are employed as evaluators for complex tasks.
Moreover, it has been widely adopted across fields such as Natural Language
Processing (NLP), preference learning, and various specific domains. However,
there are various biases within LLM-as-a-Judge, which adversely affect the
fairness and reliability of judgments. Current research on evaluating or
mitigating bias in LLM-as-a-Judge predominantly focuses on comparison-based
evaluations, while systematic investigations into bias in scoring-based
evaluations remain limited. Therefore, we define scoring bias in LLM-as-a-Judge
as the scores differ when scoring judge models are bias-related perturbed, and
provide a well-designed framework to comprehensively evaluate scoring bias. We
augment existing LLM-as-a-Judge benchmarks through data synthesis to construct
our evaluation dataset and design multi-faceted evaluation metrics. Our
experimental results demonstrate that the scoring stability of existing judge
models is disrupted by scoring biases. Further exploratory experiments and
discussions provide valuable insights into the design of scoring prompt
templates and the mitigation of scoring biases on aspects such as score
rubrics, score IDs, and reference answer selection.

</details>


### [118] [Why Are Parsing Actions for Understanding Message Hierarchies Not Random?](https://arxiv.org/abs/2506.22366)
*Daichi Kato,Ryo Ueda,Yusuke Miyao*

Main category: cs.CL

TL;DR: The study investigates why humans don't use random parsing strategies for language understanding by simulating communication models with changes including hierarchical inputs and surprisal terms.


<details>
  <summary>Details</summary>
Motivation: To understand why human language comprehension doesn't follow random parsing strategies, despite previous research showing high communication accuracy can be achieved with those strategies.

Method: The authors modified a previous experimental simulation by introducing more complex hierarchical input and incorporating a surprisal-related term into the objective function to assess the impact on communication accuracy.

Result: Random parsing strategies became less effective under the new experimental conditions, especially with hierarchical complexity and surprisal terms.

Conclusion: Human-like parsing strategies, influenced by hierarchical complexity and surprisal, may be better suited for communication accuracy compared to random parsing strategies.

Abstract: If humans understood language by randomly selecting parsing actions, it might
have been necessary to construct a robust symbolic system capable of being
interpreted under any hierarchical structure. However, human parsing strategies
do not seem to follow such a random pattern. Why is that the case? In fact, a
previous study on emergent communication using models with hierarchical biases
have reported that agents adopting random parsing
strategies$\unicode{x2013}$ones that deviate significantly from human language
comprehension$\unicode{x2013}$can achieve high communication accuracy. In this
study, we investigate this issue by making two simple and natural modifications
to the experimental setup: (I) we use more complex inputs that have
hierarchical structures, such that random parsing makes semantic interpretation
more difficult, and (II) we incorporate a surprisal-related term, which is
known to influence the order of words and characters in natural language, into
the objective function. With these changes, we evaluate whether agents
employing random parsing strategies still maintain high communication accuracy.

</details>


### [119] [QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting, KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization](https://arxiv.org/abs/2506.22396)
*Danush Khanna,Aditya Kumar Guru,Srivarshinee Sridhar,Zidan Ahmed,Rubhav Bahirwani,Meetu Malhotra,Vinija Jain,Aman Chadha,Amitava Das,Kripabandhu Ghosh*

Main category: cs.CL

TL;DR: QuickSilver introduces runtime optimization for large language models with up to 39.6% computational cost reduction, preserving model integrity and performance.


<details>
  <summary>Details</summary>
Motivation: Inference costs dominate over 90% of runtime energy and latency in LLM deployments, making optimization crucial, especially during autoregressive decoding.

Method: QuickSilver is a modular, token-level framework adopting methods like Dynamic Token Halting, KV Cache Skipping, and Contextual Token Fusion to reduce runtime costs without altering model weights or structure.

Result: Using QuickSilver on GPT-2 and Llama-2 across benchmarks like WikiText-103 and C4 achieves up to 39.6% reduction in FLOPs with negligible perplexity degradation.

Conclusion: QuickSilver provides an effective framework for semantic adaptivity in inference, emphasizing optimization without retraining or disturbing decoding compatibility.

Abstract: Inference accounts for the majority of latency and energy consumption in
large language model (LLM) deployments, often exceeding 90% of total cost.
While training-time efficiency has seen extensive progress, runtime
optimization remains a key bottleneck, particularly under autoregressive
decoding. Existing approaches -- such as pruning, quantization, early exits,
and speculative decoding -- often require retraining, architectural changes, or
disrupt decoding compatibility. We introduce QuickSilver, a modular,
token-level framework that enables semantic adaptivity at inference time
without altering model weights or structure. QuickSilver integrates four
synergistic mechanisms:
  (i) Dynamic Token Halting, which halts computation for tokens with converged
representations; (ii) KV Cache Skipping, which selectively suppresses memory
writes to reduce attention overhead; and (iii) Contextual Token Fusion, which
collapses redundant tokens into shared paths to shrink sequence length.
  Unlike speculative decoding or MoE routing, QuickSilver operates entirely on
frozen, dense models and requires no auxiliary networks. Applied to GPT-2 and
Llama-2 across WikiText-103 and C4, QuickSilver achieves up to 39.6% FLOP
reduction with negligible perplexity degradation (<=0.2).

</details>


### [120] [Refining Czech GEC: Insights from a Multi-Experiment Approach](https://arxiv.org/abs/2506.22402)
*Petr Pechman,Milan Straka,Jana Straková,Jakub Náplava*

Main category: cs.CL

TL;DR: The paper introduces a state-of-the-art grammar error correction (GEC) system for Czech based on the Transformer neural network, emphasizing real-time synthetic error generation.


<details>
  <summary>Details</summary>
Motivation: To improve the effectiveness of grammar error correction for the Czech language by leveraging advanced neural network models and dynamic error generation.

Method: Using the Transformer architecture with a real-time synthetic augmentation pipeline, introducing both language-agnostic and Czech-specific errors, and conducting extensive experiments to evaluate various strategies.

Result: The system achieved state-of-the-art performance for Czech GEC, outperforming others in terms of accuracy and computational efficiency.

Conclusion: The research provides an effective and efficient Czech GEC solution, with source code and models made available for further exploration.

Abstract: We present a grammar error correction (GEC) system that achieves state of the
art for the Czech language. Our system is based on a neural network translation
approach with the Transformer architecture, and its key feature is its
real-time synthetic generation pipeline, which dynamically augments sentences
with artificial errors by introducing both language-agnostic and Czech-specific
errors. We conduct a comprehensive series of experiments, investigating the
Czech GEC corpora as bases for synthetic error introduction, several error
generation strategies, domain balancing, tokenization granularity, model size,
and data scaling during fine-tuning. Additionally, we evaluate the performance
of large language models (LLMs) on Czech GEC in both end-user and expert
fine-tuning scenarios. Our best-performing model is superior both in
performance and computational efficiency. The source code and the trained model
links are available on https://github.com/ufal/tsd2025-gec.

</details>


### [121] [HyperCLOVA X THINK Technical Report](https://arxiv.org/abs/2506.22403)
*NAVER Cloud HyperCLOVA X Team*

Main category: cs.CL

TL;DR: HyperCLOVA X THINK is a reasoning-focused large language model pre-trained on 6 trillion Korean and English tokens, designed with advanced techniques to ensure robust, bilingual capabilities and competitive performance while using lower compute resources.


<details>
  <summary>Details</summary>
Motivation: To develop a reasoning-focused large language model designed specifically for bilingual Korean and English applications, enhancing AI innovation in Korea while maintaining relevance globally.

Method: The model was built using a Peri-LN Transformer with the $
abla$P scaling method, pre-trained using a three-stage curriculum to extend the context window to 128K tokens, and fine-tuned with Reinforcement Learning from Verifiable Rewards.

Result: The model performs competitively on Korea-oriented benchmarks, demonstrates strong bilingual consistency, and features a vision-augmented variant that matches or exceeds GPT-4.1 in STEM-specific tasks, all while requiring less compute for training.

Conclusion: HyperCLOVA X THINK establishes itself as a powerful and resource-efficient foundation model, optimized for Korean AI and global research, with plans for broader accessibility through pruning and distillation techniques.

Abstract: We introduce HyperCLOVA X THINK, the first reasoning-focused large language
model in the HyperCLOVA X family, pre-trained on roughly $6$ trillion
high-quality Korean, and English tokens, augmented with targeted synthetic
Korean data. It was implemented as a compute-memory-balanced Peri-LN
Transformer scaled with $\mu$P, pre-trained through a three-stage curriculum
that expands the context window to $128$K tokens, and post-trained via
supervised fine-tuning with Reinforcement Learning from Verifiable Rewards
supports both detailed rationale and concise-answer modes. It delivers
competitive performance against similarly sized models on Korea-focused
benchmarks such as KMMLU, CSAT, KoBALT-700, HAERAE-1.0, and KoBigBench, while
preserving robust bilingual consistency and translation quality. In addition, a
vision-augmented variant matches or exceeds GPT-4.1 on the KCSAT STEM
benchmark, all of which are achieved with substantially lower training compute
than existing models of similar sizes. We also present a pruning and
distillation technique that will soon be applied to HyperCLOVA X THINK for an
open-source and business-friendly foundation model. Altogether, these
capabilities position HyperCLOVA X THINK as a robust foundation for Korean AI
innovation and a valuable resource for the global research community.

</details>


### [122] [Sequential Diagnosis with Language Models](https://arxiv.org/abs/2506.22405)
*Harsha Nori,Mayank Daswani,Christopher Kelly,Scott Lundberg,Marco Tulio Ribeiro,Marc Wilson,Xiaoxuan Liu,Viknesh Sounderajah,Jonathan Carlson,Matthew P Lungren,Bay Gross,Peter Hames,Mustafa Suleyman,Dominic King,Eric Horvitz*

Main category: cs.CL

TL;DR: The study introduces the Sequential Diagnosis Benchmark to mimic real-world diagnostic reasoning and evaluates the MAI Diagnostic Orchestrator (MAI-DxO) that achieves significant diagnostic accuracy and cost-effectiveness improvements, outperforming physicians and base AI models.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of static and simplistic evaluations of language models in medicine by mimicking the nuanced, iterative diagnostic reasoning process used in real-world clinical settings.

Method: The researchers transformed 304 NEJM-CPC cases into stepwise diagnostic encounters and developed the MAI-DxO, which simulates diagnostic reasoning by proposing differential diagnoses and selecting cost-effective tests. Performance was assessed based on diagnostic accuracy and associated costs.

Result: MAI-DxO achieved 80% diagnostic accuracy, notably higher than generalist physicians (20%), while reducing diagnostic costs by 20% compared to physicians and 70% compared to standalone AI models. When configured for accuracy, it reached 85.5%, with performance consistent across various AI models.

Conclusion: AI systems, when designed to emulate iterative and cost-aware reasoning, can significantly enhance diagnostic accuracy and cost-efficiency, offering promise for improving real-world healthcare practices.

Abstract: Artificial intelligence holds great promise for expanding access to expert
medical knowledge and reasoning. However, most evaluations of language models
rely on static vignettes and multiple-choice questions that fail to reflect the
complexity and nuance of evidence-based medicine in real-world settings. In
clinical practice, physicians iteratively formulate and revise diagnostic
hypotheses, adapting each subsequent question and test to what they've just
learned, and weigh the evolving evidence before committing to a final
diagnosis. To emulate this iterative process, we introduce the Sequential
Diagnosis Benchmark, which transforms 304 diagnostically challenging New
England Journal of Medicine clinicopathological conference (NEJM-CPC) cases
into stepwise diagnostic encounters. A physician or AI begins with a short case
abstract and must iteratively request additional details from a gatekeeper
model that reveals findings only when explicitly queried. Performance is
assessed not just by diagnostic accuracy but also by the cost of physician
visits and tests performed. We also present the MAI Diagnostic Orchestrator
(MAI-DxO), a model-agnostic orchestrator that simulates a panel of physicians,
proposes likely differential diagnoses and strategically selects high-value,
cost-effective tests. When paired with OpenAI's o3 model, MAI-DxO achieves 80%
diagnostic accuracy--four times higher than the 20% average of generalist
physicians. MAI-DxO also reduces diagnostic costs by 20% compared to
physicians, and 70% compared to off-the-shelf o3. When configured for maximum
accuracy, MAI-DxO achieves 85.5% accuracy. These performance gains with MAI-DxO
generalize across models from the OpenAI, Gemini, Claude, Grok, DeepSeek, and
Llama families. We highlight how AI systems, when guided to think iteratively
and act judiciously, can advance diagnostic precision and cost-effectiveness in
clinical care.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [123] [Fine-Grained Preference Optimization Improves Spatial Reasoning in VLMs](https://arxiv.org/abs/2506.21656)
*Yifan Shen,Yuanzhe Liu,Jingyuan Zhu,Xu Cao,Xiaofeng Zhang,Yixiao He,Wenming Ye,James Matthew Rehg,Ismini Lourentzou*

Main category: cs.CV

TL;DR: SpatialReasoner-R1 addresses limitations in vision-language models (VLMs) with fine-grained spatial reasoning by introducing innovative training methods.


<details>
  <summary>Details</summary>
Motivation: Existing VLMs struggle with tasks requiring multi-step logic and precise spatial alignment. The paper aims to overcome these deficiencies in spatial reasoning.

Method: Introduces the Multi-Model Monte Carlo Tree Search (M3CTS) for generating logical reasoning trajectories and fine-grained Direct Preference Optimization (fDPO) to assign segment-specific spatial preferences.

Result: fDPO improves performance on spatial quality and quantity tasks, with SpatialReasoner-R1 outperforming baselines by 9.8% on SPATIALRGPT-Bench.

Conclusion: SpatialReasoner-R1 advances the state-of-the-art in spatial reasoning tasks while maintaining strong performance on general vision-language benchmarks.

Abstract: Current Vision-Language Models (VLMs) struggle with fine-grained spatial
reasoning, particularly when multi-step logic and precise spatial alignment are
required. In this work, we introduce SpatialReasoner-R1, a vision-language
reasoning model designed to address these limitations. To construct
high-quality supervision for spatial reasoning, we design a Multi-Model Monte
Carlo Tree Search (M3CTS) method that generates diverse, logically consistent
Long Chain-of-Thought (LongCoT) reasoning trajectories. In addition, we propose
fine-grained Direct Preference Optimization (fDPO), which introduces
segment-specific preference granularity for descriptive grounding and logical
reasoning, guided by a spatial reward mechanism that evaluates candidate
responses based on visual consistency, spatial grounding, and logical
coherence. Experimental results demonstrate that fDPO achieves an average
improvement of 4.1% over standard DPO across spatial quality tasks, and a 9.0%
gain in spatial quantity tasks. SpatialReasoner-R1, trained with fDPO, sets a
new SoTA on SPATIALRGPT-Bench, outperforming the strongest baseline by 9.8% in
average accuracy, while maintaining competitive performance on general
vision-language tasks.

</details>


### [124] [TanDiT: Tangent-Plane Diffusion Transformer for High-Quality 360° Panorama Generation](https://arxiv.org/abs/2506.21681)
*Hakan Çapuk,Andrew Bond,Muhammed Burak Kızıl,Emir Göçen,Erkut Erdem,Aykut Erdem*

Main category: cs.CV

TL;DR: This paper introduces TanDiT, a model for generating high-quality panoramic images using tangent-plane grids with a unified diffusion process and post-processing for global coherence.


<details>
  <summary>Details</summary>
Motivation: To address challenges in panoramic image generation—including geometric distortion and seamless loop-consistency—by leveraging the strengths of existing image models.

Method: TanDiT employs a single unified diffusion model to generate tangent-plane grids and a model-agnostic post-processing step to enhance global coherence. Additionally, it introduces metrics (TangentIS, TangentFID) and benchmarks for evaluation.

Result: Extensive experiments show TanDiT's ability to interpret complex text prompts, generalize beyond training data, and produce high-quality, diverse panoramic images with integration to multiple generative models.

Conclusion: TanDiT successfully enhances panoramic image generation, advancing the field with robust methods and meaningful evaluation metrics, while setting benchmarks for further research.

Abstract: Recent advances in image generation have led to remarkable improvements in
synthesizing perspective images. However, these models still struggle with
panoramic image generation due to unique challenges, including varying levels
of geometric distortion and the requirement for seamless loop-consistency. To
address these issues while leveraging the strengths of the existing models, we
introduce TanDiT, a method that synthesizes panoramic scenes by generating
grids of tangent-plane images covering the entire 360$^\circ$ view. Unlike
previous methods relying on multiple diffusion branches, TanDiT utilizes a
unified diffusion model trained to produce these tangent-plane images
simultaneously within a single denoising iteration. Furthermore, we propose a
model-agnostic post-processing step specifically designed to enhance global
coherence across the generated panoramas. To accurately assess panoramic image
quality, we also present two specialized metrics, TangentIS and TangentFID, and
provide a comprehensive benchmark comprising captioned panoramic datasets and
standardized evaluation scripts. Extensive experiments demonstrate that our
method generalizes effectively beyond its training data, robustly interprets
detailed and complex text prompts, and seamlessly integrates with various
generative models to yield high-quality, diverse panoramic images.

</details>


### [125] [FOCUS: Internal MLLM Representations for Efficient Fine-Grained Visual Question Answering](https://arxiv.org/abs/2506.21710)
*Liangyu Zhong,Fabio Rosenthal,Joachim Sicking,Fabian Hüger,Thorsten Bagdonat,Hanno Gottschalk,Leo Schwinn*

Main category: cs.CV

TL;DR: The paper introduces a training-free visual cropping method called FOCUS for fine-grained visual question answering (VQA), achieving higher accuracy and efficiency compared to prior methods.


<details>
  <summary>Details</summary>
Motivation: Current visual question answering tasks, especially those focusing on small image details, struggle due to inefficiencies like exhaustive search or the need for task-specific fine-tuning in existing solutions.

Method: FOCUS employs a four-step process: identifying target objects in VQA prompts, computing an object relevance map using key-value cache, ranking candidate regions, and focusing VQA processing on the top-ranked region, without requiring task-specific training.

Result: FOCUS demonstrates superior performance on four fine-grained VQA datasets and across two MLLMs, surpassing three visual cropping methods in both accuracy and computational efficiency while matching the performance of ZoomEye with 3-6.5x less compute.

Conclusion: The proposed FOCUS method is a robust, efficient, and training-free solution for addressing challenges in fine-grained VQA, offering significant advancements over existing visual cropping techniques.

Abstract: While Multimodal Large Language Models (MLLMs) offer strong perception and
reasoning capabilities for image-text input, Visual Question Answering (VQA)
focusing on small image details still remains a challenge. Although visual
cropping techniques seem promising, recent approaches have several limitations:
the need for task-specific fine-tuning, low efficiency due to uninformed
exhaustive search, or incompatibility with efficient attention implementations.
We address these shortcomings by proposing a training-free visual cropping
method, dubbed FOCUS, that leverages MLLM-internal representations to guide the
search for the most relevant image region. This is accomplished in four steps:
first, we identify the target object(s) in the VQA prompt; second, we compute
an object relevance map using the key-value (KV) cache; third, we propose and
rank relevant image regions based on the map; and finally, we perform the
fine-grained VQA task using the top-ranked region. As a result of this informed
search strategy, FOCUS achieves strong performance across four fine-grained VQA
datasets and two types of MLLMs. It outperforms three popular visual cropping
methods in both accuracy and efficiency, and matches the best-performing
baseline, ZoomEye, while requiring 3 - 6.5 x less compute.

</details>


### [126] [CAST: Cross-Attentive Spatio-Temporal feature fusion for Deepfake detection](https://arxiv.org/abs/2506.21711)
*Aryan Thakre,Omkar Nagwekar,Vedang Talekar,Aparna Santra Biswas*

Main category: cs.CV

TL;DR: The paper introduces a novel model employing cross-attention for improved spatial and temporal fusion in deepfake video detection, leading to high performance across diverse datasets.


<details>
  <summary>Details</summary>
Motivation: The widespread threat of deepfakes to digital media authenticity necessitates robust detection techniques that integrate spatial and temporal features effectively.

Method: A unified CAST model using cross-attention was developed to dynamically fuse spatial and temporal features, improving detection of fine-grained, time-evolving artifacts.

Result: The model achieved 99.49% AUC and 97.57% accuracy in intra-dataset evaluations, and 93.31% AUC in cross-dataset testing, showcasing strong generalization ability.

Conclusion: Cross-attention-based fusion enables deeper spatio-temporal interaction, significantly improving robustness and precision in deepfake video detection.

Abstract: Deepfakes have emerged as a significant threat to digital media authenticity,
increasing the need for advanced detection techniques that can identify subtle
and time-dependent manipulations. CNNs are effective at capturing spatial
artifacts, and Transformers excel at modeling temporal inconsistencies.
However, many existing CNN-Transformer models process spatial and temporal
features independently. In particular, attention-based methods often use
separate attention mechanisms for spatial and temporal features and combine
them using naive approaches like averaging, addition, or concatenation, which
limits the depth of spatio-temporal interaction. To address this challenge, we
propose a unified CAST model that leverages cross-attention to effectively fuse
spatial and temporal features in a more integrated manner. Our approach allows
temporal features to dynamically attend to relevant spatial regions, enhancing
the model's ability to detect fine-grained, time-evolving artifacts such as
flickering eyes or warped lips. This design enables more precise localization
and deeper contextual understanding, leading to improved performance across
diverse and challenging scenarios. We evaluate the performance of our model
using the FaceForensics++, Celeb-DF, and DeepfakeDetection datasets in both
intra- and cross-dataset settings to affirm the superiority of our approach.
Our model achieves strong performance with an AUC of 99.49 percent and an
accuracy of 97.57 percent in intra-dataset evaluations. In cross-dataset
testing, it demonstrates impressive generalization by achieving a 93.31 percent
AUC on the unseen DeepfakeDetection dataset. These results highlight the
effectiveness of cross-attention-based feature fusion in enhancing the
robustness of deepfake video detection.

</details>


### [127] [Elucidating and Endowing the Diffusion Training Paradigm for General Image Restoration](https://arxiv.org/abs/2506.21722)
*Xin Lu,Xueyang Fu,Jie Xiao,Zihao Fan,Yurui Zhu,Zheng-Jun Zha*

Main category: cs.CV

TL;DR: The paper proposes a new framework integrating diffusion training paradigms into general image restoration (IR) tasks, supported by regularization strategies and an incremental training paradigm.


<details>
  <summary>Details</summary>
Motivation: This paper aims to bridge the gap between the strong generative abilities of diffusion models and their limited practicality in IR tasks due to complex architectures and iterative processes.

Method: The authors adapt the diffusion training paradigm for general IR frameworks by analyzing time-step dependencies, network hierarchies, and task correlations. They propose regularization strategies, an incremental training paradigm, and task-specific adaptors.

Result: The proposed framework significantly enhances IR networks' generalization in single-task IR and achieves superior performance in multi-task unified IR.

Conclusion: The method makes diffusion paradigms compatible with general IR tasks and can be seamlessly integrated into existing IR architectures, offering practical and scalable improvements.

Abstract: While diffusion models demonstrate strong generative capabilities in image
restoration (IR) tasks, their complex architectures and iterative processes
limit their practical application compared to mainstream reconstruction-based
general ordinary IR networks. Existing approaches primarily focus on optimizing
network architecture and diffusion paths but overlook the integration of the
diffusion training paradigm within general ordinary IR frameworks. To address
these challenges, this paper elucidates key principles for adapting the
diffusion training paradigm to general IR training through systematic analysis
of time-step dependencies, network hierarchies, noise-level relationships, and
multi-restoration task correlations, proposing a new IR framework supported by
diffusion-based training. To enable IR networks to simultaneously restore
images and model generative representations, we introduce a series of
regularization strategies that align diffusion objectives with IR tasks,
improving generalization in single-task scenarios. Furthermore, recognizing
that diffusion-based generation exerts varying influences across different IR
tasks, we develop an incremental training paradigm and task-specific adaptors,
further enhancing performance in multi-task unified IR. Experiments demonstrate
that our method significantly improves the generalization of IR networks in
single-task IR and achieves superior performance in multi-task unified IR.
Notably, the proposed framework can be seamlessly integrated into existing
general IR architectures.

</details>


### [128] [Asymmetric Dual Self-Distillation for 3D Self-Supervised Representation Learning](https://arxiv.org/abs/2506.21724)
*Remco F. Leijenaar,Hamidreza Kasaei*

Main category: cs.CV

TL;DR: The paper introduces AsymDSD, a method that combines masked modeling and invariance learning to achieve advanced 3D point cloud understanding, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of learning meaningful representations from 3D point clouds without the dependence on large-scale labeled datasets and overcome the limitations of reconstruction-based masked point modeling (MPM).

Method: The AsymDSD framework unifies masked modeling and invariance learning in the latent space. Key innovations include an asymmetric setup, disabling attention between masked queries, multi-mask sampling, and a point cloud adaptation of multi-crop strategies.

Result: AsymDSD achieves state-of-the-art performance on the ScanObjectNN dataset (90.53%), further improving to 93.72% when pretrained on 930k shapes, significantly outperforming prior methods.

Conclusion: The study demonstrates the effectiveness of AsymDSD in advancing semantic representation learning for 3D point clouds, presenting a scalable and performance-enhancing approach for self-supervised learning.

Abstract: Learning semantically meaningful representations from unstructured 3D point
clouds remains a central challenge in computer vision, especially in the
absence of large-scale labeled datasets. While masked point modeling (MPM) is
widely used in self-supervised 3D learning, its reconstruction-based objective
can limit its ability to capture high-level semantics. We propose AsymDSD, an
Asymmetric Dual Self-Distillation framework that unifies masked modeling and
invariance learning through prediction in the latent space rather than the
input space. AsymDSD builds on a joint embedding architecture and introduces
several key design choices: an efficient asymmetric setup, disabling attention
between masked queries to prevent shape leakage, multi-mask sampling, and a
point cloud adaptation of multi-crop. AsymDSD achieves state-of-the-art results
on ScanObjectNN (90.53%) and further improves to 93.72% when pretrained on 930k
shapes, surpassing prior methods.

</details>


### [129] [Exploring Image Generation via Mutually Exclusive Probability Spaces and Local Correlation Hypothesis](https://arxiv.org/abs/2506.21731)
*Chenqiu Zhao,Anup Basu*

Main category: cs.CV

TL;DR: The paper introduces two frameworks, MESP and LCH, to address challenges in probabilistic generative models like memorization instead of true generation.


<details>
  <summary>Details</summary>
Motivation: The motivation is to tackle the issue in probabilistic generative models where learning global distributions often leads to memorization rather than effective generative capabilities.

Method: The authors reframe VAE's latent variable issue using MESP and propose the Binary Latent Autoencoder (BL-AE) and Autoregressive Random Variable Model (ARVM). They also introduce LCH to explore generative abilities based on local correlations.

Result: The ARVM shows competitive FID scores and outperforms state-of-the-art methods on standard datasets, though results uncover memorization over generation.

Conclusion: While the introduced frameworks show promise, the study highlights the necessity of focusing on local correlations to enhance true generative behavior of models.

Abstract: We propose two theoretical frameworks, the Mutually Exclusive Probability
Space (MESP) and the Local Correlation Hypothesis (LCH), to explore a potential
limitation in probabilistic generative models; namely that learning global
distributions leads to memorization rather than generative behavior. MESP
emerges from our rethinking of the Variational Autoencoder (VAE). We observe
that latent variable distributions in VAE exhibit overlap, which leads to an
optimization conflict between the reconstruction loss and KL-divergence loss. A
lower bound based on the overlap coefficient is proposed. We refer to this
phenomenon as Mutually Exclusive Probability Spaces. Based on MESP, a Binary
Latent Autoencoder (BL-AE) is proposed to encode images into binary latent
representations. These binary latents are used as the input to our
Autoregressive Random Variable Model (ARVM), a modified autoregressive model
outputting histograms. Our ARVM achieves competitive FID scores, outperforming
state-of-the-art methods on standard datasets. However, such scores reflect
memorization rather than generation. To address this issue, we propose the
Local Correlation Hypothesis (LCH), which posits that generative capability
arising from local correlations among latent variables. Comprehensive
experiments and discussions are conducted to validate our frameworks.

</details>


### [130] [Equitable Federated Learning with NCA](https://arxiv.org/abs/2506.21735)
*Nick Lemke,Mirko Konstantin,Henry John Krumb,John Kalkhof,Jonathan Stieber,Anirban Mukhopadhyay*

Main category: cs.CV

TL;DR: Introduces FedNCA, an FL system optimized for low-resource settings using lightweight architecture for medical image segmentation.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of implementing federated learning in low- and middle-income countries (LMICs) with limited computational resources and unreliable internet connectivity.

Method: Developed FedNCA, which incorporates the Med-NCA lightweight architecture designed for efficient operation on low-cost edge devices like smartphones, with encryption-ready features to ensure security.

Result: FedNCA enables training on resource-limited devices and is optimized for medical image segmentation under compromised network conditions.

Conclusion: FedNCA provides a promising solution for equitable and secure healthcare advancements in resource-constrained areas by addressing infrastructural and security barriers.

Abstract: Federated Learning (FL) is enabling collaborative model training across
institutions without sharing sensitive patient data. This approach is
particularly valuable in low- and middle-income countries (LMICs), where access
to trained medical professionals is limited. However, FL adoption in LMICs
faces significant barriers, including limited high-performance computing
resources and unreliable internet connectivity. To address these challenges, we
introduce FedNCA, a novel FL system tailored for medical image segmentation
tasks. FedNCA leverages the lightweight Med-NCA architecture, enabling training
on low-cost edge devices, such as widely available smartphones, while
minimizing communication costs. Additionally, our encryption-ready FedNCA
proves to be suitable for compromised network communication. By overcoming
infrastructural and security challenges, FedNCA paves the way for inclusive,
efficient, lightweight, and encryption-ready medical imaging solutions,
fostering equitable healthcare advancements in resource-constrained regions.

</details>


### [131] [ImplicitQA: Going beyond frames towards Implicit Video Reasoning](https://arxiv.org/abs/2506.21742)
*Sirnam Swetha,Rohit Gupta,Parth Parag Kulkarni,David G Shatwell,Jeffrey A Chan Santiago,Nyle Siddiqui,Joseph Fioresi,Mubarak Shah*

Main category: cs.CV

TL;DR: The paper introduces 'ImplicitQA,' a benchmark aimed at testing implicit reasoning in VideoQA systems using creative video content that requires understanding beyond explicit visual cues.


<details>
  <summary>Details</summary>
Motivation: Current VideoQA systems and benchmarks fail to capture the implicit reasoning required to comprehend creative and narrative-driven video content.

Method: The authors developed a dataset called ImplicitQA with 1,000 annotated QA pairs derived from over 320 high-quality creative video clips, categorized into key reasoning dimensions.

Result: Evaluations on leading VideoQA models showed significant performance degradation, indicating challenges in implicit reasoning and reliance on surface-level visual cues.

Conclusion: The ImplicitQA benchmark highlights the limitations of existing VideoQA systems and aims to drive further research into improving implicit reasoning capabilities in such systems.

Abstract: Video QA has made significant strides by leveraging multimodal learning to
align visual and textual modalities. However, current benchmarks overwhelmingly
focus on questions answerable through explicit visual content - actions,
objects & events directly observable within individual frames or short clips.
In contrast, creative and cinematic videos - such as movies, TV shows, and
narrative-driven content - employ storytelling techniques that deliberately
omit certain depictions, requiring viewers to infer motives, causality, and
relationships across discontinuous frames. Humans naturally excel at such
implicit reasoning, seamlessly integrating information across time and context
to construct coherent narratives. Current VideoQA systems and benchmarks fail
to capture this essential dimension of human-like understanding. To bridge this
gap, we present ImplicitQA, a novel benchmark specifically designed to test
models on implicit reasoning. It comprises 1K meticulously annotated QA pairs
derived from 320+ high-quality creative video clips, systematically categorized
into key reasoning dimensions: lateral and vertical spatial reasoning, depth
and proximity, viewpoint and visibility, motion and trajectory, causal and
motivational reasoning, social interactions, physical context, and inferred
counting. These annotations are deliberately challenging, crafted by authors
ensuring high-quality. Our extensive evaluations on leading VideoQA models
reveals performance degradation, underscoring their reliance on surface-level
visual cues and highlighting the difficulty of implicit reasoning. Performance
variations across models further illustrate the complexity and diversity of the
challenges presented by ImplicitQA. By releasing both the dataset and our data
collection framework, we aim to stimulate further research and development in
the community. https://huggingface.co/datasets/ucf-crcv/ImplicitQA.

</details>


### [132] [Early Glaucoma Detection using Deep Learning with Multiple Datasets of Fundus Images](https://arxiv.org/abs/2506.21770)
*Rishiraj Paul Chowdhury,Nirmit Shekar Karkera*

Main category: cs.CV

TL;DR: This study employs a deep learning approach using EfficientNet-B0 for automated glaucoma detection from retinal fundus images, achieving strong performance with minimal preprocessing.


<details>
  <summary>Details</summary>
Motivation: The study addresses the need for non-invasive, accessible, and effective diagnostic methods for early glaucoma detection to prevent irreversible blindness.

Method: The paper proposes a deep learning pipeline using the EfficientNet-B0 architecture. The model is sequentially trained and fine-tuned on ACRIMA, ORIGA, and RIM-ONE datasets. Minimal preprocessing is applied to the input data.

Result: Experiments reveal that minimal preprocessing achieves higher AUC-ROC than more complex methods. The model demonstrates strong generalization and discriminative performance on unseen data.

Conclusion: The proposed pipeline is scalable, reproducible, and shows potential clinical utility for early glaucoma detection.

Abstract: Glaucoma is a leading cause of irreversible blindness, but early detection
can significantly improve treatment outcomes. Traditional diagnostic methods
are often invasive and require specialized equipment. In this work, we present
a deep learning pipeline using the EfficientNet-B0 architecture for glaucoma
detection from retinal fundus images. Unlike prior studies that rely on single
datasets, we sequentially train and fine-tune our model across ACRIMA, ORIGA,
and RIM-ONE datasets to enhance generalization. Our experiments show that
minimal preprocessing yields higher AUC-ROC compared to more complex
enhancements, and our model demonstrates strong discriminative performance on
unseen datasets. The proposed pipeline offers a reproducible and scalable
approach to early glaucoma detection, supporting its potential clinical
utility.

</details>


### [133] [Comparing Learning Paradigms for Egocentric Video Summarization](https://arxiv.org/abs/2506.21785)
*Daniel Wen*

Main category: cs.CV

TL;DR: The study compares supervised learning, unsupervised learning, and prompt fine-tuning models for egocentric video summarization, finding that GPT-4o (a fine-tuned model) surpasses state-of-the-art specialized models.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of current computer vision models in effectively summarizing and interpreting egocentric (first-person) video data.

Method: The paper evaluates three paradigms: Shotluck Holmes (supervised learning), TAC-SUM (unsupervised learning), and GPT-4o (prompt fine-tuned model) on a subset of egocentric videos from the Ego-Exo4D dataset.

Result: The study finds that existing state-of-the-art models struggle with first-person videos, while GPT-4o significantly outperforms them, showcasing its adaptability to egocentric perspectives.

Conclusion: Further innovations are crucial for improving computer vision models in processing egocentric videos, and general-purpose prompt fine-tuned models, such as GPT-4o, show great promise in overcoming the current challenges.

Abstract: In this study, we investigate various computer vision paradigms - supervised
learning, unsupervised learning, and prompt fine-tuning - by assessing their
ability to understand and interpret egocentric video data. Specifically, we
examine Shotluck Holmes (state-of-the-art supervised learning), TAC-SUM
(state-of-the-art unsupervised learning), and GPT-4o (a prompt fine-tuned
pre-trained model), evaluating their effectiveness in video summarization. Our
results demonstrate that current state-of-the-art models perform less
effectively on first-person videos compared to third-person videos,
highlighting the need for further advancements in the egocentric video domain.
Notably, a prompt fine-tuned general-purpose GPT-4o model outperforms these
specialized models, emphasizing the limitations of existing approaches in
adapting to the unique challenges of first-person perspectives. Although our
evaluation is conducted on a small subset of egocentric videos from the
Ego-Exo4D dataset due to resource constraints, the primary objective of this
research is to provide a comprehensive proof-of-concept analysis aimed at
advancing the application of computer vision techniques to first-person videos.
By exploring novel methodologies and evaluating their potential, we aim to
contribute to the ongoing development of models capable of effectively
processing and interpreting egocentric perspectives.

</details>


### [134] [CAT-SG: A Large Dynamic Scene Graph Dataset for Fine-Grained Understanding of Cataract Surgery](https://arxiv.org/abs/2506.21813)
*Felix Holm,Gözde Ünver,Ghazal Ghazaei,Nassir Navab*

Main category: cs.CV

TL;DR: The paper introduces CAT-SG, a dataset capturing structured annotations of tool-tissue interactions and procedural dynamics in cataract surgeries. It also proposes a scene graph generation model, CatSGG, for enhancing surgical AI applications.


<details>
  <summary>Details</summary>
Motivation: Existing surgical datasets only address isolated aspects like tool detection or phase segmentation but fail to capture comprehensive semantic relationships and workflow dynamics.

Method: The authors introduce the CAT-SG dataset with detailed semantic annotations and propose CatSGG, a novel model for generating surgical scene graphs.

Result: CatSGG outperforms current methods in generating structured surgical representations, enhancing the utility of CAT-SG for AI applications.

Conclusion: The CAT-SG dataset coupled with CatSGG provides holistic, structured insights into cataract surgery workflows, supporting AI-driven surgical training, decision-making, and workflow optimization.

Abstract: Understanding the intricate workflows of cataract surgery requires modeling
complex interactions between surgical tools, anatomical structures, and
procedural techniques. Existing datasets primarily address isolated aspects of
surgical analysis, such as tool detection or phase segmentation, but lack
comprehensive representations that capture the semantic relationships between
entities over time. This paper introduces the Cataract Surgery Scene Graph
(CAT-SG) dataset, the first to provide structured annotations of tool-tissue
interactions, procedural variations, and temporal dependencies. By
incorporating detailed semantic relations, CAT-SG offers a holistic view of
surgical workflows, enabling more accurate recognition of surgical phases and
techniques. Additionally, we present a novel scene graph generation model,
CatSGG, which outperforms current methods in generating structured surgical
representations. The CAT-SG dataset is designed to enhance AI-driven surgical
training, real-time decision support, and workflow analysis, paving the way for
more intelligent, context-aware systems in clinical practice.

</details>


### [135] [Few-Shot Segmentation of Historical Maps via Linear Probing of Vision Foundation Models](https://arxiv.org/abs/2506.21826)
*Rafael Sterzinger,Marco Peer,Robert Sablatnig*

Main category: cs.CV

TL;DR: The paper proposes a method integrating vision foundation models and efficient fine-tuning to achieve advanced few-shot segmentation of historical maps with superior performance and resource efficiency.


<details>
  <summary>Details</summary>
Motivation: Historical maps offer valuable insights, but their diverse visual forms and lack of annotated data complicate automated processing, demanding innovative segmentation methods.

Method: The approach combines semantic embeddings from large vision models with parameter-efficient fine-tuning, optimizing segmentation tasks in low-data regimes.

Result: The method achieves significant performance gains, with +5% and +13% in mIoU on vineyard and railway segmentation (10-shot scenarios), and around +20% in 5-shot settings, outperforming state-of-the-art models. It also achieved a PQ of 67.3% for building block segmentation on the ICDAR 2021 dataset.

Conclusion: This approach presents an efficient method for accurate segmentation of historical maps, minimizing manual annotation effort and enabling automated historical analysis. The implementation is available publicly.

Abstract: As rich sources of history, maps provide crucial insights into historical
changes, yet their diverse visual representations and limited annotated data
pose significant challenges for automated processing. We propose a simple yet
effective approach for few-shot segmentation of historical maps, leveraging the
rich semantic embeddings of large vision foundation models combined with
parameter-efficient fine-tuning. Our method outperforms the state-of-the-art on
the Siegfried benchmark dataset in vineyard and railway segmentation, achieving
+5% and +13% relative improvements in mIoU in 10-shot scenarios and around +20%
in the more challenging 5-shot setting. Additionally, it demonstrates strong
performance on the ICDAR 2021 competition dataset, attaining a mean PQ of 67.3%
for building block segmentation, despite not being optimized for this
shape-sensitive metric, underscoring its generalizability. Notably, our
approach maintains high performance even in extremely low-data regimes (10- &
5-shot), while requiring only 689k trainable parameters - just 0.21% of the
total model size. Our approach enables precise segmentation of diverse
historical maps while drastically reducing the need for manual annotations,
advancing automated processing and analysis in the field. Our implementation is
publicly available at:
https://github.com/RafaelSterzinger/few-shot-map-segmentation.

</details>


### [136] [Tied Prototype Model for Few-Shot Medical Image Segmentation](https://arxiv.org/abs/2506.22101)
*Hyeongji Kim,Stine Hansen,Michael Kampffmeyer*

Main category: cs.CV

TL;DR: The paper introduces the Tied Prototype Model (TPM), a novel approach for medical image few-shot segmentation, overcoming limitations of the prior ADNet model.


<details>
  <summary>Details</summary>
Motivation: To improve medical image few-shot segmentation accuracy and address issues in ADNet, including reliance on single prototypes, focus on binary classification, and fixed thresholds.

Method: The TPM reforms ADNet by tying prototype locations for foreground and background, allowing multiple prototypes, multi-class segmentation, and adaptive thresholds based on class priors.

Result: TPM achieves improved segmentation accuracy and better performance in medical image segmentation compared to existing methods like ADNet.

Conclusion: TPM offers a probabilistically grounded, adaptable solution for prototype-based few-shot segmentation, effectively addressing ADNet's limitations and extending its usability.

Abstract: Common prototype-based medical image few-shot segmentation (FSS) methods
model foreground and background classes using class-specific prototypes.
However, given the high variability of the background, a more promising
direction is to focus solely on foreground modeling, treating the background as
an anomaly -- an approach introduced by ADNet. Yet, ADNet faces three key
limitations: dependence on a single prototype per class, a focus on binary
classification, and fixed thresholds that fail to adapt to patient and organ
variability. To address these shortcomings, we propose the Tied Prototype Model
(TPM), a principled reformulation of ADNet with tied prototype locations for
foreground and background distributions. Building on its probabilistic
foundation, TPM naturally extends to multiple prototypes and multi-class
segmentation while effectively separating non-typical background features.
Notably, both extensions lead to improved segmentation accuracy. Finally, we
leverage naturally occurring class priors to define an ideal target for
adaptive thresholds, boosting segmentation performance. Taken together, TPM
provides a fresh perspective on prototype-based FSS for medical image
segmentation. The code can be found at https://github.com/hjk92g/TPM-FSS.

</details>


### [137] [TaleForge: Interactive Multimodal System for Personalized Story Creation](https://arxiv.org/abs/2506.21832)
*Minh-Loi Nguyen,Quang-Khai Le,Tam V. Nguyen,Minh-Triet Tran,Trung-Nghia Le*

Main category: cs.CV

TL;DR: TaleForge combines large language models and text-to-image diffusion to generate personalized stories with embedded user facial images and illustrations.


<details>
  <summary>Details</summary>
Motivation: To enhance engagement and immersion in storytelling by addressing the lack of personalization in traditional story-generation methods.

Method: The system integrates three modules: narrative and character creation with language models, personalized character illustrations using user facial images and outfit choices, and scene backdrop generation.

Result: A user study showed increased engagement and ownership when participants were depicted as protagonists, though participants desired more detailed editing tools.

Conclusion: TaleForge demonstrates the potential for multimodal storytelling by blending personalized text and illustrations, creating tailored, immersive user experiences.

Abstract: Storytelling is a deeply personal and creative process, yet existing methods
often treat users as passive consumers, offering generic plots with limited
personalization. This undermines engagement and immersion, especially where
individual style or appearance is crucial. We introduce TaleForge, a
personalized story-generation system that integrates large language models
(LLMs) and text-to-image diffusion to embed users' facial images within both
narratives and illustrations. TaleForge features three interconnected modules:
Story Generation, where LLMs create narratives and character descriptions from
user prompts; Personalized Image Generation, merging users' faces and outfit
choices into character illustrations; and Background Generation, creating scene
backdrops that incorporate personalized characters. A user study demonstrated
heightened engagement and ownership when individuals appeared as protagonists.
Participants praised the system's real-time previews and intuitive controls,
though they requested finer narrative editing tools. TaleForge advances
multimodal storytelling by aligning personalized text and imagery to create
immersive, user-centric experiences.

</details>


### [138] [PrefPaint: Enhancing Image Inpainting through Expert Human Feedback](https://arxiv.org/abs/2506.21834)
*Duy-Bao Bui,Hoang-Khang Nguyen,Trung-Nghia Le*

Main category: cs.CV

TL;DR: PrefPaint integrates human feedback to improve inpainting models, enhancing reliability in medical imaging like polyps rendering.


<details>
  <summary>Details</summary>
Motivation: To address inaccuracies in inpainting models for critical fields such as medical imaging, where errors could significantly impact diagnosis and treatment outcomes.

Method: PrefPaint introduces human feedback into the training of Stable Diffusion Inpainting using an intuitive web-based interface for managing training, fine-tuning, and inference.

Result: User studies show that PrefPaint surpasses existing methods by reducing visual inconsistencies and generating realistic polyps images, especially in medical use cases.

Conclusion: Incorporating human feedback with structured, interactive tools improves the accuracy of inpainting models, advancing their reliability and usability in medical imaging.

Abstract: Inpainting, the process of filling missing or corrupted image parts, has
broad applications, including medical imaging. However, in specialized fields
like medical polyps imaging, where accuracy and reliability are critical,
inpainting models can generate inaccurate images, leading to significant errors
in medical diagnosis and treatment. To ensure reliability, medical images
should be annotated by experts like oncologists for effective model training.
We propose PrefPaint, an approach that incorporates human feedback into the
training process of Stable Diffusion Inpainting, bypassing the need for
computationally expensive reward models. In addition, we develop a web-based
interface streamlines training, fine-tuning, and inference. This interactive
interface provides a smooth and intuitive user experience, making it easier to
offer feedback and manage the fine-tuning process. User study on various
domains shows that PrefPaint outperforms existing methods, reducing visual
inconsistencies and improving image rendering, particularly in medical
contexts, where our model generates more realistic polyps images.

</details>


### [139] [ProSAM: Enhancing the Robustness of SAM-based Visual Reference Segmentation with Probabilistic Prompts](https://arxiv.org/abs/2506.21835)
*Xiaoqi Wang,Clint Sebastian,Wenbin He,Liu Ren*

Main category: cs.CV

TL;DR: The paper introduces ProSAM, a method improving stability in SAM-based visual reference segmentation by using a variational prompt encoder.


<details>
  <summary>Details</summary>
Motivation: Current SAM-based methods in visual reference segmentation suffer from instability and reduced robustness due to suboptimal prompt encoders generating prompts at object boundaries.

Method: ProSAM employs a variational prompt encoder to predict multivariate prompt distributions, avoiding unstable prompts near object boundaries.

Result: ProSAM surpasses state-of-the-art methods on benchmarks like Pascal-5$^i$ and COCO-20$^i$, demonstrating improved stability and robustness.

Conclusion: ProSAM enhances the reliability of SAM-based visual reference segmentation, providing a significant step forward in open-set image segmentation.

Abstract: The recent advancements in large foundation models have driven the success of
open-set image segmentation, a task focused on segmenting objects beyond
predefined categories. Among various prompt types (such as points, boxes,
texts, and visual references), visual reference segmentation stands out for its
unique flexibility and strong zero-shot capabilities. Recently, several
SAM-based methods have made notable progress in this task by automatically
generating prompts to guide SAM. However, these methods often generate prompts
at object boundaries due to suboptimal prompt encoder, which results in
instability and reduced robustness. In this work, we introduce ProSAM, a simple
but effective method to address the stability challenges we identified in
existing SAM-based visual reference segmentation approaches. By learning a
variational prompt encoder to predict multivariate prompt distributions, ProSAM
avoids generating prompts that lie in unstable regions, overcoming the
instability caused by less robust prompts. Our approach consistently surpasses
state-of-the-art methods on the Pascal-5$^i$ and COCO-20$^i$ datasets,
providing a more robust solution for visual reference segmentation.

</details>


### [140] [GenEscape: Hierarchical Multi-Agent Generation of Escape Room Puzzles](https://arxiv.org/abs/2506.21839)
*Mengyi Shan,Brian Curless,Ira Kemelmacher-Shlizerman,Steve Seitz*

Main category: cs.CV

TL;DR: The paper proposes a hierarchical multi-agent framework to generate better escape room puzzle images by addressing issues in spatial relationships and affordance reasoning in text-to-image models.


<details>
  <summary>Details</summary>
Motivation: Current text-to-image models struggle with generating images involving spatial relationships, logical coherence, and functional solvability, especially in complex contexts like escape room puzzles.

Method: A hierarchical multi-agent framework is introduced, involving stages such as functional design, symbolic scene graph reasoning, layout synthesis, and local image editing. Each agent works with iterative feedback for collaboratively improving outputs.

Result: The approach improves image quality by enhancing solvability, avoiding shortcuts, and clarifying affordances, without compromising visual appeal.

Conclusion: Hierarchical multi-agent collaboration is effective in addressing key challenges of current text-to-image models, offering a promising method for generating complex and logically robust imagery.

Abstract: We challenge text-to-image models with generating escape room puzzle images
that are visually appealing, logically solid, and intellectually stimulating.
While base image models struggle with spatial relationships and affordance
reasoning, we propose a hierarchical multi-agent framework that decomposes this
task into structured stages: functional design, symbolic scene graph reasoning,
layout synthesis, and local image editing. Specialized agents collaborate
through iterative feedback to ensure the scene is visually coherent and
functionally solvable. Experiments show that agent collaboration improves
output quality in terms of solvability, shortcut avoidance, and affordance
clarity, while maintaining visual quality.

</details>


### [141] [ADNet: Leveraging Error-Bias Towards Normal Direction in Face Alignment](https://arxiv.org/abs/2109.05721)
*Yangyu Huang,Hao Yang,Chong Li,Jongyoo Kim,Fangyun Wei*

Main category: cs.CV

TL;DR: The paper proposes methods to address error-bias in facial landmark detection by leveraging anisotropic losses and attention mechanisms to improve CNN model convergence.


<details>
  <summary>Details</summary>
Motivation: While CNNs have significantly improved facial landmark detection, there is limited research addressing error-bias issues where landmark errors spread along tangent lines to curves.

Method: The authors introduced anisotropic direction loss (ADL) and anisotropic attention module (AAM) that impose stronger constraints along the normal direction and relaxed constraints along the tangent direction of facial landmarks.

Result: The ADNet model, which integrates ADL and AAM in its pipeline, achieves state-of-the-art performance on benchmark datasets such as 300W, WFLW, and COFW.

Conclusion: The proposed methodologies effectively leverage facial structural and textural detail learning, demonstrating improved accuracy and robustness in face alignment.

Abstract: The recent progress of CNN has dramatically improved face alignment
performance. However, few works have paid attention to the error-bias with
respect to error distribution of facial landmarks. In this paper, we
investigate the error-bias issue in face alignment, where the distributions of
landmark errors tend to spread along the tangent line to landmark curves. This
error-bias is not trivial since it is closely connected to the ambiguous
landmark labeling task. Inspired by this observation, we seek a way to leverage
the error-bias property for better convergence of CNN model. To this end, we
propose anisotropic direction loss (ADL) and anisotropic attention module (AAM)
for coordinate and heatmap regression, respectively. ADL imposes strong binding
force in normal direction for each landmark point on facial boundaries. On the
other hand, AAM is an attention module which can get anisotropic attention mask
focusing on the region of point and its local edge connected by adjacent
points, it has a stronger response in tangent than in normal, which means
relaxed constraints in the tangent. These two methods work in a complementary
manner to learn both facial structures and texture details. Finally, we
integrate them into an optimized end-to-end training pipeline named ADNet. Our
ADNet achieves state-of-the-art results on 300W, WFLW and COFW datasets, which
demonstrates the effectiveness and robustness.

</details>


### [142] [3D-Telepathy: Reconstructing 3D Objects from EEG Signals](https://arxiv.org/abs/2506.21843)
*Yuxiang Ge,Jionghao Cheng,Ruiquan Ge,Zhaojie Fang,Gangyong Jia,Xiang Wan,Nannan Li,Ahmed Elazab,Changmiao Wang*

Main category: cs.CV

TL;DR: This paper focuses on reconstructing 3D objects from EEG data using an innovative encoder and training strategies, addressing challenges in noise and data scarcity.


<details>
  <summary>Details</summary>
Motivation: The motivation lies in enhancing Brain-Computer Interfaces and helping individuals with communication disorders by reconstructing rich spatial information from EEG data into 3D objects.

Method: The authors propose an EEG encoder architecture integrating a dual self-attention mechanism, hybrid training strategies including cross-attention, contrastive learning, and self-supervised learning, and the use of stable diffusion and Variational Score Distillation for generating 3D objects.

Result: Through their approach, the authors successfully generate 3D objects with similar content and structure from EEG data, overcoming challenges posed by noise and lacking datasets.

Conclusion: This research provides a significant technological leap in utilizing EEG for practical 3D object reconstruction, offering promising applications in BCIs and aiding communication for disabled individuals.

Abstract: Reconstructing 3D visual stimuli from Electroencephalography (EEG) data holds
significant potential for applications in Brain-Computer Interfaces (BCIs) and
aiding individuals with communication disorders. Traditionally, efforts have
focused on converting brain activity into 2D images, neglecting the translation
of EEG data into 3D objects. This limitation is noteworthy, as the human brain
inherently processes three-dimensional spatial information regardless of
whether observing 2D images or the real world. The neural activities captured
by EEG contain rich spatial information that is inevitably lost when
reconstructing only 2D images, thus limiting its practical applications in BCI.
The transition from EEG data to 3D object reconstruction faces considerable
obstacles. These include the presence of extensive noise within EEG signals and
a scarcity of datasets that include both EEG and 3D information, which
complicates the extraction process of 3D visual data. Addressing this
challenging task, we propose an innovative EEG encoder architecture that
integrates a dual self-attention mechanism. We use a hybrid training strategy
to train the EEG Encoder, which includes cross-attention, contrastive learning,
and self-supervised learning techniques. Additionally, by employing stable
diffusion as a prior distribution and utilizing Variational Score Distillation
to train a neural radiation field, we successfully generate 3D objects with
similar content and structure from EEG data.

</details>


### [143] [FreeEnricher: Enriching Face Landmarks without Additional Cost](https://arxiv.org/abs/2212.09525)
*Yangyu Huang,Xi Chen,Jongyoo Kim,Hao Yang,Chong Li,Jiaolong Yang,Dong Chen*

Main category: cs.CV

TL;DR: The paper proposes a framework to improve sparse facial landmark datasets by enriching them with dense landmarks using weakly-supervised learning, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Sparse facial landmarks are inadequate for applications like cosmetic medicine and facial beautification, necessitating a method to densify them.

Method: A weakly-supervised learning technique leverages appearance similarities of local patches along semantic contours to refine sparse landmarks into dense landmarks, integrated as a module into existing face alignment networks.

Result: The method delivers state-of-the-art accuracy on both constructed dense 300W testset and original sparse datasets (300W and WFLW), without incurring extra costs.

Conclusion: This framework successfully enriches sparse landmark datasets into dense ones with high accuracy, enhancing their utility in face alignment tasks.

Abstract: Recent years have witnessed significant growth of face alignment. Though
dense facial landmark is highly demanded in various scenarios, e.g., cosmetic
medicine and facial beautification, most works only consider sparse face
alignment. To address this problem, we present a framework that can enrich
landmark density by existing sparse landmark datasets, e.g., 300W with 68
points and WFLW with 98 points. Firstly, we observe that the local patches
along each semantic contour are highly similar in appearance. Then, we propose
a weakly-supervised idea of learning the refinement ability on original sparse
landmarks and adapting this ability to enriched dense landmarks. Meanwhile,
several operators are devised and organized together to implement the idea.
Finally, the trained model is applied as a plug-and-play module to the existing
face alignment networks. To evaluate our method, we manually label the dense
landmarks on 300W testset. Our method yields state-of-the-art accuracy not only
in newly-constructed dense 300W testset but also in the original sparse 300W
and WFLW testsets without additional cost.

</details>


### [144] [End-to-End RGB-IR Joint Image Compression With Channel-wise Cross-modality Entropy Model](https://arxiv.org/abs/2506.21851)
*Haofeng Wang,Fangtao Zhou,Qi Zhang,Zeyuan Chen,Enci Zhang,Zhao Wang,Xiaofeng Huang,Siwei Ma*

Main category: cs.CV

TL;DR: The paper introduces a method for compressing RGB-IR image pairs using a novel Channel-wise Cross-modality Entropy Model (CCEM), which achieves better results than prior compression techniques.


<details>
  <summary>Details</summary>
Motivation: To address the issue of storage and transmission costs doubling when handling RGB-IR image pairs for intelligent surveillance and other applications.

Method: It proposes a joint compression framework using CCEM, which includes a Low-frequency Context Extraction Block (LCEB) and a Low-frequency Context Fusion Block (LCFB) for better entropy parameter prediction by utilizing cross-modality global low-frequency information.

Result: Their approach shows improved compression efficiency, saving 23.1% of the bit rate on the LLVIP dataset compared to the previous state-of-the-art RGB-IR compression method.

Conclusion: The proposed CCEM-based compression framework effectively leverages cross-modality information to achieve significant storage and transmission cost reductions for RGB-IR image pairs.

Abstract: RGB-IR(RGB-Infrared) image pairs are frequently applied simultaneously in
various applications like intelligent surveillance. However, as the number of
modalities increases, the required data storage and transmission costs also
double. Therefore, efficient RGB-IR data compression is essential. This work
proposes a joint compression framework for RGB-IR image pair. Specifically, to
fully utilize cross-modality prior information for accurate context probability
modeling within and between modalities, we propose a Channel-wise
Cross-modality Entropy Model (CCEM). Among CCEM, a Low-frequency Context
Extraction Block (LCEB) and a Low-frequency Context Fusion Block (LCFB) are
designed for extracting and aggregating the global low-frequency information
from both modalities, which assist the model in predicting entropy parameters
more accurately. Experimental results demonstrate that our approach outperforms
existing RGB-IR image pair and single-modality compression methods on LLVIP and
KAIST datasets. For instance, the proposed framework achieves a 23.1% bit rate
saving on LLVIP dataset compared to the state-of-the-art RGB-IR image codec
presented at CVPR 2022.

</details>


### [145] [Periodic-MAE: Periodic Video Masked Autoencoder for rPPG Estimation](https://arxiv.org/abs/2506.21855)
*Jiho Choi,Sang Jun Lee*

Main category: cs.CV

TL;DR: The study develops a method using self-supervised learning to extract quasi-periodic signals for remote photoplethysmography (rPPG) from facial videos.


<details>
  <summary>Details</summary>
Motivation: The goal was to create an effective framework for detecting subtle periodic signals in unlabeled facial videos, crucial for accurate physiological measurement through rPPG.

Method: Utilized a video masked autoencoder for self-supervised spatio-temporal learning, incorporating frame masking techniques to capture periodic signals and physiological bandlimit constraints for better signal representation.

Result: Experiments showed notable performance gains in extracting physiological signals, particularly in cross-dataset evaluations.

Conclusion: The method enhances rPPG signal extraction from facial videos and offers improvements in challenging data scenarios, underscoring its practical applications.

Abstract: In this paper, we propose a method that learns a general representation of
periodic signals from unlabeled facial videos by capturing subtle changes in
skin tone over time. The proposed framework employs the video masked
autoencoder to learn a high-dimensional spatio-temporal representation of the
facial region through self-supervised learning. Capturing quasi-periodic
signals in the video is crucial for remote photoplethysmography (rPPG)
estimation. To account for signal periodicity, we apply frame masking in terms
of video sampling, which allows the model to capture resampled quasi-periodic
signals during the pre-training stage. Moreover, the framework incorporates
physiological bandlimit constraints, leveraging the property that physiological
signals are sparse within their frequency bandwidth to provide pulse cues to
the model. The pre-trained encoder is then transferred to the rPPG task, where
it is used to extract physiological signals from facial videos. We evaluate the
proposed method through extensive experiments on the PURE, UBFC-rPPG, MMPD, and
V4V datasets. Our results demonstrate significant performance improvements,
particularly in challenging cross-dataset evaluations. Our code is available at
https://github.com/ziiho08/Periodic-MAE.

</details>


### [146] [PEACE: Empowering Geologic Map Holistic Understanding with MLLMs](https://arxiv.org/abs/2501.06184)
*Yangyu Huang,Tianyi Gao,Haoran Xu,Qihao Zhao,Yang Song,Zhipeng Gui,Tengchao Lv,Hao Chen,Lei Cui,Scarlett Li,Furu Wei*

Main category: cs.CV

TL;DR: This paper introduces GeoMap-Bench, a benchmark for evaluating Multimodal Large Language Models (MLLMs) in geologic map understanding, and proposes GeoMap-Agent, a new AI model that significantly outperforms GPT-4o in this domain.


<details>
  <summary>Details</summary>
Motivation: The motivation arises from the lack of effective tools to understand geologic maps using existing Multimodal Large Language Models (MLLMs), given their demanding nature and domain-specific challenges.

Method: The paper proposes GeoMap-Agent, featuring three tailored modules: Hierarchical Information Extraction (HIE), Domain Knowledge Injection (DKI), and Prompt-enhanced Question Answering (PEQA). It utilizes an AI expert group and a diverse toolset for comprehensive map analysis.

Result: GeoMap-Agent significantly outperforms the GPT-4o model with an overall GeoMap-Bench score of 0.811 compared to GPT-4o’s score of 0.369.

Conclusion: The study advances geologic map understanding by introducing GeoMap-Agent and establishes a foundation for more efficient and accurate geological investigations with MLLMs.

Abstract: Geologic map, as a fundamental diagram in geology science, provides critical
insights into the structure and composition of Earth's subsurface and surface.
These maps are indispensable in various fields, including disaster detection,
resource exploration, and civil engineering. Despite their significance,
current Multimodal Large Language Models (MLLMs) often fall short in geologic
map understanding. This gap is primarily due to the challenging nature of
cartographic generalization, which involves handling high-resolution map,
managing multiple associated components, and requiring domain-specific
knowledge. To quantify this gap, we construct GeoMap-Bench, the first-ever
benchmark for evaluating MLLMs in geologic map understanding, which assesses
the full-scale abilities in extracting, referring, grounding, reasoning, and
analyzing. To bridge this gap, we introduce GeoMap-Agent, the inaugural agent
designed for geologic map understanding, which features three modules:
Hierarchical Information Extraction (HIE), Domain Knowledge Injection (DKI),
and Prompt-enhanced Question Answering (PEQA). Inspired by the
interdisciplinary collaboration among human scientists, an AI expert group acts
as consultants, utilizing a diverse tool pool to comprehensively analyze
questions. Through comprehensive experiments, GeoMap-Agent achieves an overall
score of 0.811 on GeoMap-Bench, significantly outperforming 0.369 of GPT-4o.
Our work, emPowering gEologic mAp holistiC undErstanding (PEACE) with MLLMs,
paves the way for advanced AI applications in geology, enhancing the efficiency
and accuracy of geological investigations.

</details>


### [147] [SPADE: Spatial Transcriptomics and Pathology Alignment Using a Mixture of Data Experts for an Expressive Latent Space](https://arxiv.org/abs/2506.21857)
*Ekaterina Redekop,Mara Pleasure,Zichen Wang,Kimberly Flores,Anthony Sisk,William Speier,Corey W. Arnold*

Main category: cs.CV

TL;DR: This study introduces SPADE, a novel foundation model integrating histopathology and spatial transcriptomics to improve pathology task performance by learning ST-informed representations.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address a critical gap in integrating whole-slide images with spatial transcriptomics, to better capture molecular heterogeneity that standard H&E staining cannot provide.

Method: SPADE leverages a mixture-of-data experts technique, utilizing two-stage clustering and contrastive learning to integrate co-registered WSI patches and gene expression profiles into a unified latent space.

Result: SPADE, pre-trained on the HEST-1k dataset, significantly outperforms baseline models in 14 downstream tasks, particularly in few-shot learning scenarios.

Conclusion: Integrating morphological and molecular data into a unified representation using SPADE provides a significant advance in digital pathology, improving task performance across diseases.

Abstract: The rapid growth of digital pathology and advances in self-supervised deep
learning have enabled the development of foundational models for various
pathology tasks across diverse diseases. While multimodal approaches
integrating diverse data sources have emerged, a critical gap remains in the
comprehensive integration of whole-slide images (WSIs) with spatial
transcriptomics (ST), which is crucial for capturing critical molecular
heterogeneity beyond standard hematoxylin & eosin (H&E) staining. We introduce
SPADE, a foundation model that integrates histopathology with ST data to guide
image representation learning within a unified framework, in effect creating an
ST-informed latent space. SPADE leverages a mixture-of-data experts technique,
where experts, created via two-stage feature-space clustering, use contrastive
learning to learn representations of co-registered WSI patches and gene
expression profiles. Pre-trained on the comprehensive HEST-1k dataset, SPADE is
evaluated on 14 downstream tasks, demonstrating significantly superior few-shot
performance compared to baseline models, highlighting the benefits of
integrating morphological and molecular information into one latent space.

</details>


### [148] [LLaVA-Scissor: Token Compression with Semantic Connected Components for Video LLMs](https://arxiv.org/abs/2506.21862)
*Boyuan Sun,Jiaxing Zhao,Xihan Wei,Qibin Hou*

Main category: cs.CV

TL;DR: LLaVA-Scissor introduces a training-free video token compression method leveraging Semantic Connected Components, achieving better token reduction with preserved semantic detail.


<details>
  <summary>Details</summary>
Motivation: Existing token compression methods for video models often fail to capture all semantic regions, resulting in redundancy and limited effectiveness in video understanding tasks.

Method: The proposed method uses a Semantic Connected Components approach for token compression, ensuring coverage across spatial and temporal domains, and represents videos as non-overlapping semantic tokens.

Result: The strategy outperformed other methods in video question answering, long video understanding, and multi-choice benchmarks, especially at low token retention ratios.

Conclusion: LLaVA-Scissor provides an effective, training-free mechanism for video token compression, enhancing video understanding performance.

Abstract: In this paper, we present LLaVA-Scissor, a training-free token compression
strategy designed for video multimodal large language models. Previous methods
mostly attempt to compress tokens based on attention scores, but fail to
effectively capture all semantic regions and often lead to token redundancy.
Differently, we propose to leverage the Semantic Connected Components (SCC)
approach that assigns tokens to distinct semantic regions within the token set,
ensuring comprehensive semantic coverage. The outcome is a two-step
spatio-temporal token compression strategy that utilizes SCC in both spatial
and temporal domains. This strategy can effectively compress tokens by
representing the entire video with a set of non-overlapping semantic tokens. We
conduct extensive evaluations of the token compression capabilities of
LLaVA-Scissor across diverse video understanding benchmarks, including video
question answering, long video understanding, and comprehensive multi-choices
benchmarks. Experimental results show that the proposed LLaVA-Scissor
outperforms other token compression methods, achieving superior performance in
various video understanding benchmarks, particularly at low token retention
ratios. Project page: https://github.com/HumanMLLM/LLaVA-Scissor.

</details>


### [149] [Integrating Multi-Modal Sensors: A Review of Fusion Techniques for Intelligent Vehicles](https://arxiv.org/abs/2506.21885)
*Chuheng Wei,Ziye Qin,Ziyan Zhang,Guoyuan Wu,Matthew J. Barth*

Main category: cs.CV

TL;DR: This paper categorizes multi-sensor fusion strategies and reviews deep learning methods for autonomous driving, focusing on datasets, challenges, and future trends.


<details>
  <summary>Details</summary>
Motivation: To enhance perception and environmental understanding in autonomous driving by addressing sensor limitations through multi-sensor fusion.

Method: The paper formalizes fusion strategies (data, feature, decision levels), reviews deep learning-based methods, analyzes datasets, and explores emerging trends such as Vision-Language Models.

Result: It identifies key datasets for multi-sensor applications and examines how advanced models like VLMs and LLMs can address challenges in environments like adverse weather and urban settings.

Conclusion: Multi-sensor fusion is pivotal for improving adaptability and robustness in autonomous driving systems, and future directions are suggested for ongoing research.

Abstract: Multi-sensor fusion plays a critical role in enhancing perception for
autonomous driving, overcoming individual sensor limitations, and enabling
comprehensive environmental understanding. This paper first formalizes
multi-sensor fusion strategies into data-level, feature-level, and
decision-level categories and then provides a systematic review of deep
learning-based methods corresponding to each strategy. We present key
multi-modal datasets and discuss their applicability in addressing real-world
challenges, particularly in adverse weather conditions and complex urban
environments. Additionally, we explore emerging trends, including the
integration of Vision-Language Models (VLMs), Large Language Models (LLMs), and
the role of sensor fusion in end-to-end autonomous driving, highlighting its
potential to enhance system adaptability and robustness. Our work offers
valuable insights into current methods and future directions for multi-sensor
fusion in autonomous driving.

</details>


### [150] [Remote Sensing Large Vision-Language Model: Semantic-augmented Multi-level Alignment and Semantic-aware Expert Modeling](https://arxiv.org/abs/2506.21863)
*Sungjune Park,Yeongyun Kim,Se Yeon Kim,Yong Man Ro*

Main category: cs.CV

TL;DR: The paper proposes a novel framework to adapt Large Vision and Language Models (LVLMs) for remote sensing (RS), addressing domain differences and improving semantic understanding through alignment and expert modeling.


<details>
  <summary>Details</summary>
Motivation: LVLMs excel in natural image domains but struggle with RS imagery due to significant differences in visual characteristics and semantics.

Method: The framework includes two main components: Semantic-augmented Multi-level Alignment (using a retrieval-based augmentation module for semantic enrichment) and Semantic-aware Expert Modeling (leveraging semantic experts to handle hierarchical semantics).

Result: The framework demonstrates consistent improvements in RS tasks like scene classification and VQA, enhancing semantic representation at multiple levels.

Conclusion: The proposed methodology effectively adapts LVLMs for RS imagery, bridging domain gaps and meeting unique RS vision-language understanding needs.

Abstract: Large Vision and Language Models (LVLMs) have shown strong performance across
various vision-language tasks in natural image domains. However, their
application to remote sensing (RS) remains underexplored due to significant
domain differences in visual appearances, object scales, and semantics. These
discrepancies hider the effective understanding of RS scenes, which contain
rich, multi-level semantic information spanning from coarse-to-fine levels.
Hence, it limits the direct adaptation of existing LVLMs to RS imagery. To
address this gap, we propose a novel LVLM framework tailored for RS
understanding, incorporating two core components: Semantic-augmented
Multi-level Alignment and Semantic-aware Expert Modeling. First, to align
multi-level visual features, we introduce the retrieval-based Semantic
Augmentation Module which enriches the visual features with relevant semantics
across fine-to-coarse levels (e.g., object- and scene-level information). It is
designed to retrieve relevant semantic cues from a RS semantic knowledge
database, followed by aggregation of semantic cues with user query and
multi-level visual features, resulting in semantically enriched representation
across multiple levels. Second, for Semantic-aware Expert Modeling, we design
semantic experts, where each expert is responsible for processing semantic
representation at different levels separately. This enables hierarchical
semantic understanding from coarse to fine levels. Evaluations across multiple
RS tasks-including scene classification and VQA, etc.-demonstrate that the
proposed framework achieves consistent improvements across multiple semantic
levels. This highlights its capability and effectiveness in bridging the gap
between general LVLMs and unique demands of RS-specific vision-language
understanding.

</details>


### [151] [Dual-Perspective United Transformer for Object Segmentation in Optical Remote Sensing Images](https://arxiv.org/abs/2506.21866)
*Yanguang Sun,Jiexi Yan,Jianjun Qian,Chunyan Xu,Jian Yang,Lei Luo*

Main category: cs.CV

TL;DR: This paper introduces a novel Dual-Perspective United Transformer (DPU-Former) for improved object segmentation in optical remote sensing images (ORSIs), addressing challenges like feature heterogeneity and model complexity.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the limitations of existing ORSI segmentation models, which fail to properly leverage both convolutional and Transformer features, causing sub-optimal performance.

Method: The authors propose the DPU-Former, featuring global-local mixed attention, Fourier-space merging strategy, gated linear feed-forward networks, and a specialized decoder to enhance feature integration and model efficiency.

Result: DPU-Former demonstrates superior segmentation performance compared to state-of-the-art methods across multiple datasets.

Conclusion: DPU-Former effectively combines convolutional and Transformer features, mitigates challenges like heterogeneity and complexity, and sets a new benchmark for ORSI segmentation tasks.

Abstract: Automatically segmenting objects from optical remote sensing images (ORSIs)
is an important task. Most existing models are primarily based on either
convolutional or Transformer features, each offering distinct advantages.
Exploiting both advantages is valuable research, but it presents several
challenges, including the heterogeneity between the two types of features, high
complexity, and large parameters of the model. However, these issues are often
overlooked in existing the ORSIs methods, causing sub-optimal segmentation. For
that, we propose a novel Dual-Perspective United Transformer (DPU-Former) with
a unique structure designed to simultaneously integrate long-range dependencies
and spatial details. In particular, we design the global-local mixed attention,
which captures diverse information through two perspectives and introduces a
Fourier-space merging strategy to obviate deviations for efficient fusion.
Furthermore, we present a gated linear feed-forward network to increase the
expressive ability. Additionally, we construct a DPU-Former decoder to
aggregate and strength features at different layers. Consequently, the
DPU-Former model outperforms the state-of-the-art methods on multiple datasets.
Code: https://github.com/CSYSI/DPU-Former.

</details>


### [152] [Robust and Accurate Multi-view 2D/3D Image Registration with Differentiable X-ray Rendering and Dual Cross-view Constraints](https://arxiv.org/abs/2506.22191)
*Yuxin Cui,Rui Song,Yibin Li,Max Q. -H. Meng,Zhe Min*

Main category: cs.CV

TL;DR: The paper introduces a two-stage multi-view 2D/3D registration method to address limited field-of-view issues in interventional navigation, achieving superior accuracy with a mean target registration error (mTRE) of $0.79 \pm 2.17$ mm.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the challenges posed by limited field-of-view in intraoperative images during interventional navigation by utilizing multi-view images for robust and accurate 2D/3D registration.

Method: The proposed method has two stages: Stage 1 uses combined loss functions for pose and image dissimilarities, including cross-view constraints. Stage 2 employs test-time optimization to refine poses from the first stage.

Result: The framework achieves a mean target registration error (mTRE) of $0.79 \pm 2.17$ mm on specimens from the DeepFluoro dataset, outperforming existing registration algorithms.

Conclusion: The multi-view 2D/3D registration approach successfully enhances alignment robustness and accuracy by leveraging cross-view constraints, showing superior performance compared to state-of-the-art techniques.

Abstract: Robust and accurate 2D/3D registration, which aligns preoperative models with
intraoperative images of the same anatomy, is crucial for successful
interventional navigation. To mitigate the challenge of a limited field of view
in single-image intraoperative scenarios, multi-view 2D/3D registration is
required by leveraging multiple intraoperative images. In this paper, we
propose a novel multi-view 2D/3D rigid registration approach comprising two
stages. In the first stage, a combined loss function is designed, incorporating
both the differences between predicted and ground-truth poses and the
dissimilarities (e.g., normalized cross-correlation) between simulated and
observed intraoperative images. More importantly, additional cross-view
training loss terms are introduced for both pose and image losses to explicitly
enforce cross-view constraints. In the second stage, test-time optimization is
performed to refine the estimated poses from the coarse stage. Our method
exploits the mutual constraints of multi-view projection poses to enhance the
robustness of the registration process. The proposed framework achieves a mean
target registration error (mTRE) of $0.79 \pm 2.17$ mm on six specimens from
the DeepFluoro dataset, demonstrating superior performance compared to
state-of-the-art registration algorithms.

</details>


### [153] [Grounding-Aware Token Pruning: Recovering from Drastic Performance Drops in Visual Grounding Caused by Pruning](https://arxiv.org/abs/2506.21873)
*Tzu-Chun Chien,Chieh-Kai Lin,Shiang-Feng Tsai,Ruei-Chi Lai,Hung-Jen Chen,Min Sun*

Main category: cs.CV

TL;DR: The paper introduces Grounding-Aware Token Pruning (GAP) to address performance degradation in visual grounding caused by token pruning, improving accuracy without added training or computational costs.


<details>
  <summary>Details</summary>
Motivation: Token pruning in Multimodal Large Language Models (MLLMs) reduces computational costs but significantly hampers grounding performance, necessitating an effective mitigation solution.

Method: The authors proposed GAP, a method that adjusts position IDs to maintain alignment during and post-token pruning, avoiding training or computational overhead.

Result: GAP restores up to 90% of the original grounding accuracy post-pruning for various models and strategies, showing improvements without additional resources.

Conclusion: GAP is a simple, effective solution to maintain grounding performance in MLLMs during token pruning, offering consistent results across models while saving on costs.

Abstract: Recent Multimodal Large Language Models (MLLMs) have demonstrated strong
performance in visual grounding, establishing themselves as a general interface
for various vision-language applications. This progress has driven the
development of token pruning methods to mitigate the high computational costs
associated with processing numerous visual tokens. However, we observe that
pruning significantly weakens the model's grounding ability, leading to
incorrect predictions and drastic performance degradation. In Referring
Expression Comprehension (REC), for instance, pruning causes the accuracy of
LLaVA on the RefCOCO validation set to drop from 56.14% to 15.34%. Our analysis
identifies misaligned position IDs after pruning as the primary cause of this
degradation, as both the order and value of these IDs are crucial for
maintaining performance in grounding tasks. To address this issue, we propose
Grounding-Aware Token Pruning (GAP), a simple yet effective adjustment to
position IDs that recovers REC accuracy back to 51.42%, which is 90% of the
original performance in the without pruning setting, all while requiring no
additional training, memory, or computational overhead. Applied to models such
as Shikra, MiniGPTv2, and the LLaVA series, our method consistently improves
performance across various token pruning strategies.

</details>


### [154] [GRASP-PsONet: Gradient-based Removal of Spurious Patterns for PsOriasis Severity Classification](https://arxiv.org/abs/2506.21883)
*Basudha Pal,Sharif Amit Kamran,Brendon Lutnick,Molly Lucas,Chaitanya Parmar,Asha Patel Shah,David Apfel,Steven Fakharzadeh,Lloyd Miller,Gabriela Cula,Kristopher Standish*

Main category: cs.CV

TL;DR: The paper presents a framework to flag unreliable training images in psoriasis scoring using patient-captured photos, improving model reliability and robustness in classification.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenges of psoriasis severity evaluation, such as inter-rater variability, manual review burden, and inconsistencies from patient-captured images, which affect automated scoring models.

Method: The study proposes a gradient-based interpretability framework to automatically flag problematic training images that introduce spurious model errors or are inconsistently annotated, enhancing model generalization.

Result: The framework identifies and removes 8.2% of flagged images, improving model AUC-ROC from 85% to 90%, and detects over 90% of inter-rater disagreement cases by reviewing only the top 30% of flagged samples.

Conclusion: This framework provides a scalable and efficient way to improve model robustness and scoring for remote psoriasis assessments, potentially eliminating manual data review.

Abstract: Psoriasis (PsO) severity scoring is important for clinical trials but is
hindered by inter-rater variability and the burden of in person clinical
evaluation. Remote imaging using patient captured mobile photos offers
scalability but introduces challenges, such as variation in lighting,
background, and device quality that are often imperceptible to humans but can
impact model performance. These factors, along with inconsistencies in
dermatologist annotations, reduce the reliability of automated severity
scoring. We propose a framework to automatically flag problematic training
images that introduce spurious correlations which degrade model generalization,
using a gradient based interpretability approach. By tracing the gradients of
misclassified validation images, we detect training samples where model errors
align with inconsistently rated examples or are affected by subtle, nonclinical
artifacts. We apply this method to a ConvNeXT based weakly supervised model
designed to classify PsO severity from phone images. Removing 8.2% of flagged
images improves model AUC-ROC by 5% (85% to 90%) on a held out test set.
Commonly, multiple annotators and an adjudication process ensure annotation
accuracy, which is expensive and time consuming. Our method detects training
images with annotation inconsistencies, potentially removing the need for
manual review. When applied to a subset of training data rated by two
dermatologists, the method identifies over 90% of cases with inter-rater
disagreement by reviewing only the top 30% of samples. This improves automated
scoring for remote assessments, ensuring robustness despite data collection
variability.

</details>


### [155] [DIVE: Deep-search Iterative Video Exploration A Technical Report for the CVRR Challenge at CVPR 2025](https://arxiv.org/abs/2506.21891)
*Umihiro Kamoto,Tatsuya Ishibashi,Noriyuki Kugo*

Main category: cs.CV

TL;DR: The paper describes the winning solution for the 1st place in the CVRR Challenge 2025 by introducing DIVE, a system for video question answering with an iterative reasoning approach, achieving 81.44% accuracy.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenge of generating accurate natural language answers to complex questions about diverse real-world video clips, tackling the limitations of existing video reasoning systems.

Method: The method, DIVE (Deep-search Iterative Video Exploration), semantically decomposes input questions and solves them with stepwise reasoning and progressive inference to achieve contextual accuracy.

Result: The system achieved a top accuracy of 81.44% on the CVRR-ES benchmark test set, surpassing all other participating solutions in the challenge.

Conclusion: The iterative reasoning framework introduced in DIVE demonstrates significant effectiveness and robustness for complex video question answering tasks.

Abstract: In this report, we present the winning solution that achieved the 1st place
in the Complex Video Reasoning & Robustness Evaluation Challenge 2025. This
challenge evaluates the ability to generate accurate natural language answers
to questions about diverse, real-world video clips. It uses the Complex Video
Reasoning and Robustness Evaluation Suite (CVRR-ES) benchmark, which consists
of 214 unique videos and 2,400 question-answer pairs spanning 11 categories.
Our method, DIVE (Deep-search Iterative Video Exploration), adopts an iterative
reasoning approach, in which each input question is semantically decomposed and
solved through stepwise reasoning and progressive inference. This enables our
system to provide highly accurate and contextually appropriate answers to even
the most complex queries. Applied to the CVRR-ES benchmark, our approach
achieves 81.44% accuracy on the test set, securing the top position among all
participants. This report details our methodology and provides a comprehensive
analysis of the experimental results, demonstrating the effectiveness of our
iterative reasoning framework in achieving robust video question answering. The
code is available at https://github.com/PanasonicConnect/DIVE

</details>


### [156] [SODA: Out-of-Distribution Detection in Domain-Shifted Point Clouds via Neighborhood Propagation](https://arxiv.org/abs/2506.21892)
*Adam Goodge,Xun Xu,Bryan Hooi,Wee Siong Ng,Jingyi Liao,Yongyi Su,Xulei Yang*

Main category: cs.CV

TL;DR: This paper introduces SODA, a method using 3D vision-language models for out-of-distribution (OOD) detection in point cloud data, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: The growing use of point cloud data in applications makes reliable OOD detection crucial to ensure model safety, but current methods under-utilize 3D vision-language models.

Method: The proposed method, SODA, employs a neighborhood-based score propagation scheme to enhance OOD detection. It operates during inference without requiring additional training.

Result: SODA demonstrates superior OOD detection performance across datasets and scenarios when compared to existing approaches.

Conclusion: SODA effectively addresses domain shift challenges in point cloud OOD detection, pioneering a robust and training-free framework leveraging 3D vision-language models.

Abstract: As point cloud data increases in prevalence in a variety of applications, the
ability to detect out-of-distribution (OOD) point cloud objects becomes
critical for ensuring model safety and reliability. However, this problem
remains under-explored in existing research. Inspired by success in the image
domain, we propose to exploit advances in 3D vision-language models (3D VLMs)
for OOD detection in point cloud objects. However, a major challenge is that
point cloud datasets used to pre-train 3D VLMs are drastically smaller in size
and object diversity than their image-based counterparts. Critically, they
often contain exclusively computer-designed synthetic objects. This leads to a
substantial domain shift when the model is transferred to practical tasks
involving real objects scanned from the physical environment. In this paper,
our empirical experiments show that synthetic-to-real domain shift
significantly degrades the alignment of point cloud with their associated text
embeddings in the 3D VLM latent space, hindering downstream performance. To
address this, we propose a novel methodology called SODA which improves the
detection of OOD point clouds through a neighborhood-based score propagation
scheme. SODA is inference-based, requires no additional model training, and
achieves state-of-the-art performance over existing approaches across datasets
and problem settings.

</details>


### [157] [Exploring Task-Solving Paradigm for Generalized Cross-Domain Face Anti-Spoofing via Reinforcement Fine-Tuning](https://arxiv.org/abs/2506.21895)
*Fangling Jiang,Qi Li,Weining Wang,Gang Wang,Bing Liu,Zhenan Sun*

Main category: cs.CV

TL;DR: This paper introduces a novel method leveraging multimodal large language models for face anti-spoofing, achieving superior cross-domain generalization and interpretability.


<details>
  <summary>Details</summary>
Motivation: Address the poor generalization and limited interpretability of existing face anti-spoofing methods when encountering unknown attack types across diverse scenarios.

Method: Developed a reinforcement fine-tuning approach using verifiable class consistent reward and reasoning consistent reward, optimized through GRPO-based strategies to iteratively refine decision-making policies.

Result: The proposed method exhibited state-of-the-art cross-domain generalization, handling unknown attack types across unseen domains while maintaining interpretability.

Conclusion: Iterative reinforcement fine-tuning successfully enhances reasoning and generalization, offering an innovative, interpretable solution for cross-domain face anti-spoofing tasks without requiring extensive annotations.

Abstract: Recently the emergence of novel presentation attacks has drawn increasing
attention to face anti-spoofing. However, existing methods tend to memorize
data patterns from the training set, resulting in poor generalization to
unknown attack types across different scenarios and limited interpretability.
To address these challenges, this paper presents a reinforcement
fine-tuning-based face anti-spoofing method that stimulates the capabilities of
multimodal large language models to think and learn how to solve the
anti-spoofing task itself, rather than relying on the memorization of
authenticity patterns. We design verifiable class consistent reward and
reasoning consistent reward, and employ a GRPO-based optimization strategy to
guide the model in exploring reasoning policies from multiple perspectives to
maximize expected rewards. As a result, through iterative trial-and-error
learning while retaining only high-reward trajectories, the model distills
highly generalizable decision-making rules from the extensive solution space to
effectively address cross-domain face anti-spoofing tasks. Extensive
experimental results demonstrate that our method achieves state-of-the-art
cross-domain generalization performance. It generalizes well to diverse unknown
attack types in unseen target domains while providing interpretable reasoning
for its authenticity decisions without requiring labor-intensive textual
annotations for training.

</details>


### [158] [Visual Content Detection in Educational Videos with Transfer Learning and Dataset Enrichment](https://arxiv.org/abs/2506.21903)
*Dipayan Biswas,Shishir Shah,Jaspal Subhlok*

Main category: cs.CV

TL;DR: The paper focuses on leveraging transfer learning and optimizing YOLO object detection models to improve the detection of visual elements in lecture videos, while addressing challenges unique to these types of videos.


<details>
  <summary>Details</summary>
Motivation: The motivation is to utilize visual elements (tables, charts, illustrations) to enhance information retrieval and comprehension in lecture videos, as their potential for improving video content access is underutilized due to detection challenges and lack of annotated datasets.

Method: The authors evaluate multiple object detection models, identifying YOLO as the most suitable, and optimize it using training on benchmark datasets and a semi-supervised auto-labeling strategy.

Result: YOLO was successfully optimized for lecture video visual element detection, and the authors released a benchmark dataset and source code to advance future research in this area.

Conclusion: The study demonstrates a viable approach to detecting visual elements in lecture videos and contributes resources to encourage further exploration and development in the field of video-based educational tools.

Abstract: Video is transforming education with online courses and recorded lectures
supplementing and replacing classroom teaching. Recent research has focused on
enhancing information retrieval for video lectures with advanced navigation,
searchability, summarization, as well as question answering chatbots. Visual
elements like tables, charts, and illustrations are central to comprehension,
retention, and data presentation in lecture videos, yet their full potential
for improving access to video content remains underutilized. A major factor is
that accurate automatic detection of visual elements in a lecture video is
challenging; reasons include i) most visual elements, such as charts, graphs,
tables, and illustrations, are artificially created and lack any standard
structure, and ii) coherent visual objects may lack clear boundaries and may be
composed of connected text and visual components. Despite advancements in deep
learning based object detection, current models do not yield satisfactory
performance due to the unique nature of visual content in lectures and scarcity
of annotated datasets. This paper reports on a transfer learning approach for
detecting visual elements in lecture video frames. A suite of state of the art
object detection models were evaluated for their performance on lecture video
datasets. YOLO emerged as the most promising model for this task. Subsequently
YOLO was optimized for lecture video object detection with training on multiple
benchmark datasets and deploying a semi-supervised auto labeling strategy.
Results evaluate the success of this approach, also in developing a general
solution to the problem of object detection in lecture videos. Paper
contributions include a publicly released benchmark of annotated lecture video
frames, along with the source code to facilitate future research.

</details>


### [159] [RAUM-Net: Regional Attention and Uncertainty-aware Mamba Network](https://arxiv.org/abs/2506.21905)
*Mingquan Liu*

Main category: cs.CV

TL;DR: The paper proposes a semi-supervised approach for Fine-Grained Visual Categorization (FGVC) using Mamba-based feature modeling, region attention, and Bayesian uncertainty to improve performance, especially when labeled data is scarce.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenges of subtle inter-class differences and fragile feature representations in FGVC, particularly when labeled training data is limited.

Method: The method combines Mamba-based feature modeling, a region attention mechanism to focus on key areas, and Bayesian uncertainty for selecting high-quality pseudo labels, enhancing the fine-grained categorization process.

Result: The proposed method demonstrates strong performance on FGVC benchmarks that involve occlusions, showcasing robustness under scenarios of limited labeled data.

Conclusion: The results indicate that the semi-supervised approach is effective in enhancing local-to-global feature modeling and achieves robustness in fine-grained categorization tasks when labeled data is scarce. Code is provided for reproducibility.

Abstract: Fine Grained Visual Categorization (FGVC) remains a challenging task in
computer vision due to subtle inter class differences and fragile feature
representations. Existing methods struggle in fine grained scenarios,
especially when labeled data is scarce. We propose a semi supervised method
combining Mamba based feature modeling, region attention, and Bayesian
uncertainty. Our approach enhances local to global feature modeling while
focusing on key areas during learning. Bayesian inference selects high quality
pseudo labels for stability. Experiments show strong performance on FGVC
benchmarks with occlusions, demonstrating robustness when labeled data is
limited. Code is available at https://github.com/wxqnl/RAUM Net.

</details>


### [160] [CERBERUS: Crack Evaluation & Recognition Benchmark for Engineering Reliability & Urban Stability](https://arxiv.org/abs/2506.21909)
*Justin Reinman,Sunwoong Choi*

Main category: cs.CV

TL;DR: CERBERUS is a synthetic benchmark designed to train and evaluate AI for detecting infrastructure defects, featuring realistic 3D scenarios and a crack image generator.


<details>
  <summary>Details</summary>
Motivation: To improve AI efficiency in detecting cracks and defects on infrastructure by providing a standardized benchmark incorporating both synthetic and real-world data.

Method: Developed CERBERUS, a benchmark that includes a crack image generator and 3D inspection scenarios, tested with the YOLO model on synthetic and real crack data.

Result: Combining synthetic and real data using CERBERUS improved performance in real-world defect detection tasks.

Conclusion: CERBERUS facilitates repeatable and flexible evaluation of defect detection systems and enhances automated infrastructure inspection advancement.

Abstract: CERBERUS is a synthetic benchmark designed to help train and evaluate AI
models for detecting cracks and other defects in infrastructure. It includes a
crack image generator and realistic 3D inspection scenarios built in Unity. The
benchmark features two types of setups: a simple Fly-By wall inspection and a
more complex Underpass scene with lighting and geometry challenges. We tested a
popular object detection model (YOLO) using different combinations of synthetic
and real crack data. Results show that combining synthetic and real data
improves performance on real-world images. CERBERUS provides a flexible,
repeatable way to test defect detection systems and supports future research in
automated infrastructure inspection. CERBERUS is publicly available at
https://github.com/justinreinman/Cerberus-Defect-Generator.

</details>


### [161] [Generating Attribute-Aware Human Motions from Textual Prompt](https://arxiv.org/abs/2506.21912)
*Xinghan Wang,Kun Xu,Fei Li,Cao Sheng,Jiazhong Yu,Yadong Mu*

Main category: cs.CV

TL;DR: The paper introduces a framework for text-driven human motion generation that incorporates human attributes such as age, gender, weight, and height to generate more realistic motions.


<details>
  <summary>Details</summary>
Motivation: Current text-driven human motion generation models fail to account for varying motion patterns due to human attributes.

Method: The authors proposed a framework based on Structural Causal Models to decouple human attributes from action semantics. They enable text-to-semantics prediction and attribute-controlled generation using this methodology.

Result: Their model produces attribute-aware motions that align with textual descriptions and specified attributes. A new dataset, HumanAttr, with attribute annotations is introduced for evaluation.

Conclusion: This work demonstrates the importance of incorporating human attributes into text-to-motion generation. Their approach enhances the realism of generated motions and establishes a new benchmark dataset.

Abstract: Text-driven human motion generation has recently attracted considerable
attention, allowing models to generate human motions based on textual
descriptions. However, current methods neglect the influence of human
attributes (such as age, gender, weight, and height) which are key factors
shaping human motion patterns. This work represents a pilot exploration for
bridging this gap. We conceptualize each motion as comprising both attribute
information and action semantics, where textual descriptions align exclusively
with action semantics. To achieve this, a new framework inspired by Structural
Causal Models is proposed to decouple action semantics from human attributes,
enabling text-to-semantics prediction and attribute-controlled generation. The
resulting model is capable of generating realistic, attribute-aware motion
aligned with the user's text and attribute inputs. For evaluation, we introduce
HumanAttr, a comprehensive dataset containing attribute annotations for
text-motion pairs, setting the first benchmark for attribute-aware
text-to-motion generation. Extensive experiments on the new dataset validate
our model's effectiveness.

</details>


### [162] [SepFormer: Coarse-to-fine Separator Regression Network for Table Structure Recognition](https://arxiv.org/abs/2506.21920)
*Nam Quan Nguyen,Xuan Phong Pham,Tuan-Anh Tran*

Main category: cs.CV

TL;DR: SepFormer improves table structure recognition (TSR) by predicting table separators using a refined DETR-style architecture, offering state-of-the-art performance and high speed.


<details>
  <summary>Details</summary>
Motivation: To improve the speed and robustness of Table Structure Recognition (TSR) for semantic data extraction from table images.

Method: SepFormer integrates the split-and-merge paradigm into separator regression via a DETR-style architecture. It uses a coarse-to-fine approach with two transformer decoders to refine table separators from single-line to line-strip separators.

Result: SepFormer achieves 25.6 FPS on average and delivers comparable performance with state-of-the-art methods on datasets such as SciTSR, PubTabNet, WTW, and iFLYTAB.

Conclusion: SepFormer introduces a novel coarse-to-fine modeling technique that enhances both the efficiency and accuracy of TSR, marking progress in the field of table image processing.

Abstract: The automated reconstruction of the logical arrangement of tables from image
data, termed Table Structure Recognition (TSR), is fundamental for semantic
data extraction. Recently, researchers have explored a wide range of techniques
to tackle this problem, demonstrating significant progress. Each table is a set
of vertical and horizontal separators. Following this realization, we present
SepFormer, which integrates the split-and-merge paradigm into a single step
through separator regression with a DETR-style architecture, improving speed
and robustness. SepFormer is a coarse-to-fine approach that predicts table
separators from single-line to line-strip separators with a stack of two
transformer decoders. In the coarse-grained stage, the model learns to
gradually refine single-line segments through decoder layers with additional
angle loss. At the end of the fine-grained stage, the model predicts line-strip
separators by refining sampled points from each single-line segment. Our
SepFormer can run on average at 25.6 FPS while achieving comparable performance
with state-of-the-art methods on several benchmark datasets, including SciTSR,
PubTabNet, WTW, and iFLYTAB.

</details>


### [163] [ZeroReg3D: A Zero-shot Registration Pipeline for 3D Consecutive Histopathology Image Reconstruction](https://arxiv.org/abs/2506.21923)
*Juming Xiong,Ruining Deng,Jialin Yue,Siqi Lu,Junlin Guo,Marilyn Lionts,Tianyuan Yao,Can Cui,Junchao Zhu,Chongyu Qu,Mengmeng Yin,Haichun Yang,Yuankai Huo*

Main category: cs.CV

TL;DR: This paper introduces ZeroReg3D, a zero-shot registration pipeline for accurate 3D reconstruction from serial histological sections, addressing challenges like tissue deformation and staining variability.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of current 2D histological analysis and 3D reconstruction, such as tissue deformation, sectioning artifacts, and reliance on large-scale training data.

Method: ZeroReg3D combines zero-shot deep learning-based keypoint matching with optimization-based affine and non-rigid registration techniques.

Result: ZeroReg3D effectively handles challenges like tissue deformation and staining variability without requiring retraining or fine-tuning.

Conclusion: ZeroReg3D offers an accurate and generalizable solution for 3D histological reconstruction, enhancing both clinical and research applications.

Abstract: Histological analysis plays a crucial role in understanding tissue structure
and pathology. While recent advancements in registration methods have improved
2D histological analysis, they often struggle to preserve critical 3D spatial
relationships, limiting their utility in both clinical and research
applications. Specifically, constructing accurate 3D models from 2D slices
remains challenging due to tissue deformation, sectioning artifacts,
variability in imaging techniques, and inconsistent illumination. Deep
learning-based registration methods have demonstrated improved performance but
suffer from limited generalizability and require large-scale training data. In
contrast, non-deep-learning approaches offer better generalizability but often
compromise on accuracy. In this study, we introduced ZeroReg3D, a novel
zero-shot registration pipeline tailored for accurate 3D reconstruction from
serial histological sections. By combining zero-shot deep learning-based
keypoint matching with optimization-based affine and non-rigid registration
techniques, ZeroReg3D effectively addresses critical challenges such as tissue
deformation, sectioning artifacts, staining variability, and inconsistent
illumination without requiring retraining or fine-tuning. The code has been
made publicly available at https://github.com/hrlblab/ZeroReg3D

</details>


### [164] [SPAZER: Spatial-Semantic Progressive Reasoning Agent for Zero-shot 3D Visual Grounding](https://arxiv.org/abs/2506.21924)
*Zhao Jin,Rong-Cheng Tu,Jingyi Liao,Wenhao Sun,Xiao Luo,Shunyu Liu,Dacheng Tao*

Main category: cs.CV

TL;DR: The paper introduces SPAZER, a zero-shot 3D visual grounding framework using a progressive reasoning method that combines spatial and semantic modalities, significantly improving accuracy without relying on 3D-labeled training data.


<details>
  <summary>Details</summary>
Motivation: Earlier methods for 3D visual grounding either focused on spatial or semantic reasoning, limiting their performance in real-world scenarios. There was a need for a more comprehensive approach that integrates both reasoning streams while avoiding reliance on costly 3D-labeled data.

Method: The authors propose SPAZER, a system where a VLM-driven agent conducts holistic 3D analysis, generates an optimal viewpoint rendering, performs anchor-guided candidate localization, and integrates 2D-3D joint decision-making for precise object identification.

Result: SPAZER outperformed existing zero-shot 3DVG approaches with state-of-the-art results, achieving accuracy improvements of 9.0% and 10.9% on the ScanRefer and Nr3D benchmarks, respectively.

Conclusion: By unifying spatial and semantic reasoning processes, SPAZER effectively achieves robust 3D visual grounding in a zero-shot manner, setting new benchmarks for performance without depending on labeled 3D data.

Abstract: 3D Visual Grounding (3DVG) aims to localize target objects within a 3D scene
based on natural language queries. To alleviate the reliance on costly 3D
training data, recent studies have explored zero-shot 3DVG by leveraging the
extensive knowledge and powerful reasoning capabilities of pre-trained LLMs and
VLMs. However, existing paradigms tend to emphasize either spatial (3D-based)
or semantic (2D-based) understanding, limiting their effectiveness in complex
real-world applications. In this work, we introduce SPAZER - a VLM-driven agent
that combines both modalities in a progressive reasoning framework. It first
holistically analyzes the scene and produces a 3D rendering from the optimal
viewpoint. Based on this, anchor-guided candidate screening is conducted to
perform a coarse-level localization of potential objects. Furthermore,
leveraging retrieved relevant 2D camera images, 3D-2D joint decision-making is
efficiently performed to determine the best-matching object. By bridging
spatial and semantic reasoning neural streams, SPAZER achieves robust zero-shot
grounding without training on 3D-labeled data. Extensive experiments on
ScanRefer and Nr3D benchmarks demonstrate that SPAZER significantly outperforms
previous state-of-the-art zero-shot methods, achieving notable gains of 9.0%
and 10.9% in accuracy.

</details>


### [165] [Quality Assessment and Distortion-aware Saliency Prediction for AI-Generated Omnidirectional Images](https://arxiv.org/abs/2506.21925)
*Liu Yang,Huiyu Duan,Jiarui Wang,Jing Liu,Menghan Hu,Xiongkuo Min,Guangtao Zhai,Patrick Le Callet*

Main category: cs.CV

TL;DR: This paper addresses quality assessment and optimization of AI-generated omnidirectional images (AIGODIs) critical for VR and AR applications.


<details>
  <summary>Details</summary>
Motivation: The lack of research on the quality issues and enhancement methodologies of AI-generated omnidirectional images, despite their potential in VR and AR.

Method: The paper introduces the OHF2024 database for human feedback, two models (BLIP2OIQA and BLIP2OISal) for quality evaluation and saliency prediction, and provides an automatic optimization process.

Result: BLIP2OIQA and BLIP2OISal models achieve state-of-the-art results in human visual experience evaluation and distortion-aware saliency prediction tasks. The optimization process improves image quality.

Conclusion: The work contributes a novel database, models, and optimization methodology, facilitating further research and advancements in AIGODI quality enhancement.

Abstract: With the rapid advancement of Artificial Intelligence Generated Content
(AIGC) techniques, AI generated images (AIGIs) have attracted widespread
attention, among which AI generated omnidirectional images (AIGODIs) hold
significant potential for Virtual Reality (VR) and Augmented Reality (AR)
applications. AI generated omnidirectional images exhibit unique quality
issues, however, research on the quality assessment and optimization of
AI-generated omnidirectional images is still lacking. To this end, this work
first studies the quality assessment and distortion-aware saliency prediction
problems for AIGODIs, and further presents a corresponding optimization
process. Specifically, we first establish a comprehensive database to reflect
human feedback for AI-generated omnidirectionals, termed OHF2024, which
includes both subjective quality ratings evaluated from three perspectives and
distortion-aware salient regions. Based on the constructed OHF2024 database, we
propose two models with shared encoders based on the BLIP-2 model to evaluate
the human visual experience and predict distortion-aware saliency for
AI-generated omnidirectional images, which are named as BLIP2OIQA and
BLIP2OISal, respectively. Finally, based on the proposed models, we present an
automatic optimization process that utilizes the predicted visual experience
scores and distortion regions to further enhance the visual quality of an
AI-generated omnidirectional image. Extensive experiments show that our
BLIP2OIQA model and BLIP2OISal model achieve state-of-the-art (SOTA) results in
the human visual experience evaluation task and the distortion-aware saliency
prediction task for AI generated omnidirectional images, and can be effectively
used in the optimization process. The database and codes will be released on
https://github.com/IntMeGroup/AIGCOIQA to facilitate future research.

</details>


### [166] [SDRNET: Stacked Deep Residual Network for Accurate Semantic Segmentation of Fine-Resolution Remotely Sensed Images](https://arxiv.org/abs/2506.21945)
*Naftaly Wambugu,Ruisheng Wang,Bo Guo,Tianshu Yu,Sheng Xu,Mohammed Elhassan*

Main category: cs.CV

TL;DR: This paper proposes SDRNet, a novel deep residual network architecture, to improve semantic segmentation performance for fine-resolution remotely sensed images.


<details>
  <summary>Details</summary>
Motivation: Remotely sensed images have challenges in semantic segmentation due to class disparities, occlusion, object size variation, and loss of spatial details in deeper neural networks.

Method: The method involves a novel stacked deep residual network (SDRNet) utilizing stacked encoder-decoder networks and dilated residual blocks to preserve spatial information and capture global dependencies.

Result: Experiments on the ISPRS Vaihingen and Potsdam datasets show SDRNet is effective and competitive with state-of-the-art DCNNs for semantic segmentation.

Conclusion: The proposed SDRNet addresses major segmentation challenges and demonstrates superior accuracy and boundary precision in handling FRRS images.

Abstract: Land cover maps generated from semantic segmentation of high-resolution
remotely sensed images have drawn mucon in the photogrammetry and remote
sensing research community. Currently, massive fine-resolution remotely sensed
(FRRS) images acquired by improving sensing and imaging technologies become
available. However, accurate semantic segmentation of such FRRS images is
greatly affected by substantial class disparities, the invisibility of key
ground objects due to occlusion, and object size variation. Despite the
extraordinary potential in deep convolutional neural networks (DCNNs) in image
feature learning and representation, extracting sufficient features from FRRS
images for accurate semantic segmentation is still challenging. These
challenges demand the deep learning models to learn robust features and
generate sufficient feature descriptors. Specifically, learning
multi-contextual features to guarantee adequate coverage of varied object sizes
from the ground scene and harnessing global-local contexts to overcome class
disparities challenge even profound networks. Deeper networks significantly
lose spatial details due to gradual downsampling processes resulting in poor
segmentation results and coarse boundaries. This article presents a stacked
deep residual network (SDRNet) for semantic segmentation from FRRS images. The
proposed framework utilizes two stacked encoder-decoder networks to harness
long-range semantics yet preserve spatial information and dilated residual
blocks (DRB) between each encoder and decoder network to capture sufficient
global dependencies thus improving segmentation performance. Our experimental
results obtained using the ISPRS Vaihingen and Potsdam datasets demonstrate
that the SDRNet performs effectively and competitively against current DCNNs in
semantic segmentation.

</details>


### [167] [Exploring Semantic Masked Autoencoder for Self-supervised Point Cloud Understanding](https://arxiv.org/abs/2506.21957)
*Yixin Zha,Chuxin Wang,Wenfei Yang,Tianzhu Zhang*

Main category: cs.CV

TL;DR: The paper introduces a Semantic Masked Autoencoder to improve point cloud understanding by addressing limitations in current random masking strategies using semantic components.


<details>
  <summary>Details</summary>
Motivation: Existing methods in point cloud understanding rely on random masking strategies, which fail to adequately capture semantic relationships within point cloud data.

Method: The authors propose a Semantic Masked Autoencoder featuring a component semantic modeling module, a semantic-enhanced masking strategy, and a prompt-tuning strategy, all leveraging semantic prototypes.

Result: The proposed modules achieve superior performance in experiments conducted on datasets like ScanObjectNN, ModelNet40, and ShapeNetPart.

Conclusion: The Semantic Masked Autoencoder effectively captures semantic relationships in point clouds, resulting in improved pre-trained model performance on downstream tasks.

Abstract: Point cloud understanding aims to acquire robust and general feature
representations from unlabeled data. Masked point modeling-based methods have
recently shown significant performance across various downstream tasks. These
pre-training methods rely on random masking strategies to establish the
perception of point clouds by restoring corrupted point cloud inputs, which
leads to the failure of capturing reasonable semantic relationships by the
self-supervised models. To address this issue, we propose Semantic Masked
Autoencoder, which comprises two main components: a prototype-based component
semantic modeling module and a component semantic-enhanced masking strategy.
Specifically, in the component semantic modeling module, we design a component
semantic guidance mechanism to direct a set of learnable prototypes in
capturing the semantics of different components from objects. Leveraging these
prototypes, we develop a component semantic-enhanced masking strategy that
addresses the limitations of random masking in effectively covering complete
component structures. Furthermore, we introduce a component semantic-enhanced
prompt-tuning strategy, which further leverages these prototypes to improve the
performance of pre-trained models in downstream tasks. Extensive experiments
conducted on datasets such as ScanObjectNN, ModelNet40, and ShapeNetPart
demonstrate the effectiveness of our proposed modules.

</details>


### [168] [TASeg: Text-aware RGB-T Semantic Segmentation based on Fine-tuning Vision Foundation Models](https://arxiv.org/abs/2506.21975)
*Meng Yu,Te Cui,Qitong Chu,Wenjie Song,Yi Yang,Yufeng Yue*

Main category: cs.CV

TL;DR: The paper introduces TASeg, a text-aware RGB-T semantic segmentation framework, to address challenges in integrating high-level textual information and different visual modalities.


<details>
  <summary>Details</summary>
Motivation: The research addresses the challenge of reliably segmenting open environments with RGB-T data, emphasizing the limitations of existing models that depend on low-level features and cannot effectively incorporate text or deal with multiple modalities.

Method: The framework employs Low-Rank Adaptation (LoRA) and proposes a Dynamic Feature Fusion Module (DFFM) for visual feature integration, while adding CLIP-generated text embeddings to improve semantic understanding.

Result: Experimental results demonstrated that TASeg outperforms existing models on various datasets, achieving higher accuracy in challenging scenarios with fewer trainable parameters.

Conclusion: TASeg proves effective in combining RGB, thermal, and textual data for superior segmentation performance, overcoming shortcomings of prior methods.

Abstract: Reliable semantic segmentation of open environments is essential for
intelligent systems, yet significant problems remain: 1) Existing RGB-T
semantic segmentation models mainly rely on low-level visual features and lack
high-level textual information, which struggle with accurate segmentation when
categories share similar visual characteristics. 2) While SAM excels in
instance-level segmentation, integrating it with thermal images and text is
hindered by modality heterogeneity and computational inefficiency. To address
these, we propose TASeg, a text-aware RGB-T segmentation framework by using
Low-Rank Adaptation (LoRA) fine-tuning technology to adapt vision foundation
models. Specifically, we propose a Dynamic Feature Fusion Module (DFFM) in the
image encoder, which effectively merges features from multiple visual
modalities while freezing SAM's original transformer blocks. Additionally, we
incorporate CLIP-generated text embeddings in the mask decoder to enable
semantic alignment, which further rectifies the classification error and
improves the semantic understanding accuracy. Experimental results across
diverse datasets demonstrate that our method achieves superior performance in
challenging scenarios with fewer trainable parameters.

</details>


### [169] [R1-Track: Direct Application of MLLMs to Visual Object Tracking via Reinforcement Learning](https://arxiv.org/abs/2506.21980)
*Biao Wang,Wenwen Li*

Main category: cs.CV

TL;DR: The paper presents an enhanced approach to visual single object tracking by fine-tuning a multi-modal language model (Qwen2.5-VL) with reinforcement learning, yielding improved performance and flexibility.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of contemporary visual object tracking methods, such as dependency on large-scale supervised datasets and rigidity of task-specific tracking frameworks.

Method: The authors fine-tuned the Qwen2.5-VL multi-modal large language model using group relative policy optimization (GRPO) reinforcement learning and a rule-based reward function on a small dataset.

Result: The resulting model, R1-Track, demonstrated notable performance improvements on the GOT-10k benchmark and supports flexible initialization methods like bounding boxes or text descriptions.

Conclusion: R1-Track not only improves tracking performance but also retains the foundational abilities of the original model, paving the way for scalable and flexible tracking solutions while hinting at further improvements.

Abstract: Visual single object tracking aims to continuously localize and estimate the
scale of a target in subsequent video frames, given only its initial state in
the first frame. This task has traditionally been framed as a template matching
problem, evolving through major phases including correlation filters,
two-stream networks, and one-stream networks with significant progress
achieved. However, these methods typically require explicit classification and
regression modeling, depend on supervised training with large-scale datasets,
and are limited to the single task of tracking, lacking flexibility. In recent
years, multi-modal large language models (MLLMs) have advanced rapidly.
Open-source models like Qwen2.5-VL, a flagship MLLMs with strong foundational
capabilities, demonstrate excellent performance in grounding tasks. This has
spurred interest in applying such models directly to visual tracking. However,
experiments reveal that Qwen2.5-VL struggles with template matching between
image pairs (i.e., tracking tasks). Inspired by deepseek-R1, we fine-tuned
Qwen2.5-VL using the group relative policy optimization (GRPO) reinforcement
learning method on a small-scale dataset with a rule-based reward function. The
resulting model, R1-Track, achieved notable performance on the GOT-10k
benchmark. R1-Track supports flexible initialization via bounding boxes or text
descriptions while retaining most of the original model's general capabilities.
And we further discuss potential improvements for R1-Track. This rough
technical report summarizes our findings as of May 2025.

</details>


### [170] [RoboEnvision: A Long-Horizon Video Generation Model for Multi-Task Robot Manipulation](https://arxiv.org/abs/2506.22007)
*Liudi Yang,Yang Bai,George Eskandar,Fengyi Shen,Mohammad Altillawi,Dong Chen,Soumajit Majumder,Ziyuan Liu,Gitta Kutyniok,Abhinav Valada*

Main category: cs.CV

TL;DR: This paper introduces a pipeline to generate high-quality long-horizon videos for robotic manipulation tasks, overcoming limitations of autoregressive video generation.


<details>
  <summary>Details</summary>
Motivation: Current text-to-video diffusion models face challenges in creating accurate, long-horizon videos for robotic manipulation tasks due to error accumulation in autoregressive paradigms.

Method: The proposed approach utilizes decomposition of goals into atomic tasks with corresponding keyframes, interpolation via diffusion models for video creation, a semantics-preserving attention module for keyframe consistency, and a policy model to regress robot joint states.

Result: The method improved video quality and consistency, achieving state-of-the-art results on benchmarks while surpassing existing policy models for long-horizon task execution.

Conclusion: Decomposing tasks, leveraging diffusion models, and innovating policy design can address challenges in robotic video generation, opening new avenues for accurate task simulations.

Abstract: We address the problem of generating long-horizon videos for robotic
manipulation tasks. Text-to-video diffusion models have made significant
progress in photorealism, language understanding, and motion generation but
struggle with long-horizon robotic tasks. Recent works use video diffusion
models for high-quality simulation data and predictive rollouts in robot
planning. However, these works predict short sequences of the robot achieving
one task and employ an autoregressive paradigm to extend to the long horizon,
leading to error accumulations in the generated video and in the execution. To
overcome these limitations, we propose a novel pipeline that bypasses the need
for autoregressive generation. We achieve this through a threefold
contribution: 1) we first decompose the high-level goals into smaller atomic
tasks and generate keyframes aligned with these instructions. A second
diffusion model then interpolates between each of the two generated frames,
achieving the long-horizon video. 2) We propose a semantics preserving
attention module to maintain consistency between the keyframes. 3) We design a
lightweight policy model to regress the robot joint states from generated
videos. Our approach achieves state-of-the-art results on two benchmarks in
video quality and consistency while outperforming previous policy models on
long-horizon tasks.

</details>


### [171] [Towards Universal & Efficient Model Compression via Exponential Torque Pruning](https://arxiv.org/abs/2506.22015)
*Sarthak Ketanbhai Modi,Lim Zi Pong,Shourya Kuchhal,Yoshi Cao,Yupeng Cheng,Teo Yon Shin,Lin Shang-Wei,Zhiming Li*

Main category: cs.CV

TL;DR: This paper introduces Exponential Torque Pruning (ETP), a method to compress deep neural networks more effectively with minimal accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Modern deep neural networks are becoming more complex and require large computational and memory resources. Efficient model compression is needed to address these growing challenges.

Method: The authors propose ETP, which applies an exponential force scheme for regularization to prune neural modules more effectively than prior linear force methods.

Result: Experimental results show that ETP achieves significantly higher compression rates compared to prior methods while maintaining negligible accuracy loss.

Conclusion: ETP offers a simple yet effective approach for compressing neural networks, addressing the limitations of earlier torque-inspired regularization techniques.

Abstract: The rapid growth in complexity and size of modern deep neural networks (DNNs)
has increased challenges related to computational costs and memory usage,
spurring a growing interest in efficient model compression techniques. Previous
state-of-the-art approach proposes using a Torque-inspired regularization which
forces the weights of neural modules around a selected pivot point. Whereas, we
observe that the pruning effect of this approach is far from perfect, as the
post-trained network is still dense and also suffers from high accuracy drop.
In this work, we attribute such ineffectiveness to the default linear force
application scheme, which imposes inappropriate force on neural module of
different distances. To efficiently prune the redundant and distant modules
while retaining those that are close and necessary for effective inference, in
this work, we propose Exponential Torque Pruning (ETP), which adopts an
exponential force application scheme for regularization. Experimental results
on a broad range of domains demonstrate that, though being extremely simple,
ETP manages to achieve significantly higher compression rate than the previous
state-of-the-art pruning strategies with negligible accuracy drop.

</details>


### [172] [Advancing Facial Stylization through Semantic Preservation Constraint and Pseudo-Paired Supervision](https://arxiv.org/abs/2506.22022)
*Zhanyi Lu,Yue Zhou*

Main category: cs.CV

TL;DR: The paper introduces a facial stylization method that improves results by addressing semantic shifts in StyleGAN-based methods, producing high-quality and consistent stylized portraits.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations in previous StyleGAN-based stylization methods, which often result in artifacts or poor fidelity to the source image, by addressing the neglected semantic shift during stylization.

Method: The method integrates a semantic preservation constraint and pseudo-paired supervision for better content correspondence. It also includes creating multi-level pseudo-paired datasets for supervisory constraint and achieves multimodal, reference-guided stylization without complex architectures.

Result: The proposed approach produces high-fidelity, visually appealing facial style transfers that outperform earlier methods, demonstrating superior stylization and content consistency.

Conclusion: The technique improves facial stylization by addressing semantic shifts and employing pseudo-paired supervision, yielding better aesthetic results while maintaining content fidelity in comparison to prior methods.

Abstract: Facial stylization aims to transform facial images into appealing,
high-quality stylized portraits, with the critical challenge of accurately
learning the target style while maintaining content consistency with the
original image. Although previous StyleGAN-based methods have made significant
advancements, the generated results still suffer from artifacts or insufficient
fidelity to the source image. We argue that these issues stem from neglecting
semantic shift of the generator during stylization. Therefore, we propose a
facial stylization method that integrates semantic preservation constraint and
pseudo-paired supervision to enhance the content correspondence and improve the
stylization effect. Additionally, we develop a methodology for creating
multi-level pseudo-paired datasets to implement supervisory constraint.
Furthermore, building upon our facial stylization framework, we achieve more
flexible multimodal and reference-guided stylization without complex network
architecture designs or additional training. Experimental results demonstrate
that our approach produces high-fidelity, aesthetically pleasing facial style
transfer that surpasses previous methods.

</details>


### [173] [Cross-modal Ship Re-Identification via Optical and SAR Imagery: A Novel Dataset and Method](https://arxiv.org/abs/2506.22027)
*Han Wang,Shengyang Li,Jian Yang,Yuxuan Liu,Yixuan Lv,Zhuang Zhou*

Main category: cs.CV

TL;DR: The paper addresses challenges in ship tracking using low-Earth orbit optical and SAR images, presenting the HOSS ReID dataset and a baseline TransOSS method.


<details>
  <summary>Details</summary>
Motivation: Current ship tracking methods using geostationary or video satellites are limited by low resolution, weather dependence, short filming durations, and restricted coverage.

Method: The authors developed the HOSS ReID dataset consisting of optical and SAR images of ships under diverse conditions. They also proposed TransOSS, a Vision Transformer-based method for cross-modal ship re-identification, using contrastive learning for pre-training.

Result: The HOSS ReID dataset and TransOSS method improve the ability to track ships across different sensors, modalities, and conditions.

Conclusion: The proposed dataset and method provide a practical solution for continuous and reliable ship tracking in diverse conditions, advancing capabilities in maritime surveillance.

Abstract: Detecting and tracking ground objects using earth observation imagery remains
a significant challenge in the field of remote sensing. Continuous maritime
ship tracking is crucial for applications such as maritime search and rescue,
law enforcement, and shipping analysis. However, most current ship tracking
methods rely on geostationary satellites or video satellites. The former offer
low resolution and are susceptible to weather conditions, while the latter have
short filming durations and limited coverage areas, making them less suitable
for the real-world requirements of ship tracking. To address these limitations,
we present the Hybrid Optical and Synthetic Aperture Radar (SAR) Ship
Re-Identification Dataset (HOSS ReID dataset), designed to evaluate the
effectiveness of ship tracking using low-Earth orbit constellations of optical
and SAR sensors. This approach ensures shorter re-imaging cycles and enables
all-weather tracking. HOSS ReID dataset includes images of the same ship
captured over extended periods under diverse conditions, using different
satellites of different modalities at varying times and angles. Furthermore, we
propose a baseline method for cross-modal ship re-identification, TransOSS,
which is built on the Vision Transformer architecture. It refines the patch
embedding structure to better accommodate cross-modal tasks, incorporates
additional embeddings to introduce more reference information, and employs
contrastive learning to pre-train on large-scale optical-SAR image pairs,
ensuring the model's ability to extract modality-invariant features. Our
dataset and baseline method are publicly available on
https://github.com/Alioth2000/Hoss-ReID.

</details>


### [174] [Partial CLIP is Enough: Chimera-Seg for Zero-shot Semantic Segmentation](https://arxiv.org/abs/2506.22032)
*Jialei Chen,Xu Zheng,Danda Pani Paudel,Luc Van Gool,Hiroshi Murase,Daisuke Deguchi*

Main category: cs.CV

TL;DR: The paper addresses challenges in Zero-shot Semantic Segmentation (ZSS) by introducing Chimera-Seg, a hybrid model combining segmentation features with vision-language alignment, and Selective Global Distillation to improve feature mapping and alignment.


<details>
  <summary>Details</summary>
Motivation: To improve Zero-shot Semantic Segmentation (ZSS) by overcoming difficulties in aligning vision-language features and addressing the semantic gap between vision models and segmentation tasks.

Method: The authors propose Chimera-Seg, which integrates a segmentation backbone and a CLIP-based semantic head, and Selective Global Distillation (SGD) to distill and align features. A Semantic Alignment Module (SAM) is also included for further alignment.

Result: Experiments across two benchmarks show the proposed model achieves improvements of 0.9% and 1.2% in hIoU, indicating better ZSS performance.

Conclusion: The approach successfully tackles ZSS challenges by combining spatial precision with vision-language alignment and improving feature mapping, offering enhanced segmentation for both seen and unseen classes.

Abstract: Zero-shot Semantic Segmentation (ZSS) aims to segment both seen and unseen
classes using supervision from only seen classes. Beyond adaptation-based
methods, distillation-based approaches transfer vision-language alignment of
vision-language model, e.g., CLIP, to segmentation models. However, such
knowledge transfer remains challenging due to: (1) the difficulty of aligning
vision-based features with the textual space, which requires combining spatial
precision with vision-language alignment; and (2) the semantic gap between
CLIP's global representations and the local, fine-grained features of
segmentation models. To address challenge (1), we propose Chimera-Seg, which
integrates a segmentation backbone as the body and a CLIP-based semantic head
as the head, like the Chimera in Greek mythology, combining spatial precision
with vision-language alignment. Specifically, Chimera-Seg comprises a trainable
segmentation model and a CLIP Semantic Head (CSH), which maps dense features
into the CLIP-aligned space. The CSH incorporates a frozen subnetwork and fixed
projection layers from the CLIP visual encoder, along with lightweight
trainable components. The partial module from CLIP visual encoder, paired with
the segmentation model, retains segmentation capability while easing the
mapping to CLIP's semantic space. To address challenge (2), we propose
Selective Global Distillation (SGD), which distills knowledge from dense
features exhibiting high similarity to the CLIP CLS token, while gradually
reducing the number of features used for alignment as training progresses.
Besides, we also use a Semantic Alignment Module (SAM) to further align dense
visual features with semantic embeddings extracted from the frozen CLIP text
encoder. Experiments on two benchmarks show improvements of 0.9% and 1.2% in
hIoU.

</details>


### [175] [Few-Shot Identity Adaptation for 3D Talking Heads via Global Gaussian Field](https://arxiv.org/abs/2506.22044)
*Hong Nie,Fuyuan Cao,Lu Chen,Fengxin Chen,Yuefeng Zou,Jun Yu*

Main category: cs.CV

TL;DR: The paper introduces FIAG, a novel 3D speaking head synthesis framework enabling efficient identity-specific adaptation with minimal training data, building upon a shared global field and universal motion field.


<details>
  <summary>Details</summary>
Motivation: Existing talking head synthesis methods achieve high quality but rely on identity-specific models, which are costly and less scalable due to their need for retraining with each new identity.

Method: The proposed FIAG framework deploys a Global Gaussian Field for shared identity representation and a Universal Motion Field to learn common motion dynamics, allowing efficient adaptation with minimal data.

Result: Extensive comparative and ablation experiments demonstrate that FIAG outperforms state-of-the-art methods in both quality and generalizability in talking head synthesis.

Conclusion: FIAG provides an effective, scalable, and generalizable solution for high-quality 3D speaking head synthesis, representing a significant step forward in the field by addressing existing computational and scalability challenges.

Abstract: Reconstruction and rendering-based talking head synthesis methods achieve
high-quality results with strong identity preservation but are limited by their
dependence on identity-specific models. Each new identity requires training
from scratch, incurring high computational costs and reduced scalability
compared to generative model-based approaches. To overcome this limitation, we
propose FIAG, a novel 3D speaking head synthesis framework that enables
efficient identity-specific adaptation using only a few training footage. FIAG
incorporates Global Gaussian Field, which supports the representation of
multiple identities within a shared field, and Universal Motion Field, which
captures the common motion dynamics across diverse identities. Benefiting from
the shared facial structure information encoded in the Global Gaussian Field
and the general motion priors learned in the motion field, our framework
enables rapid adaptation from canonical identity representations to specific
ones with minimal data. Extensive comparative and ablation experiments
demonstrate that our method outperforms existing state-of-the-art approaches,
validating both the effectiveness and generalizability of the proposed
framework. Code is available at: \textit{https://github.com/gme-hong/FIAG}.

</details>


### [176] [EnLVAM: Enhanced Left Ventricle Linear Measurements Utilizing Anatomical Motion Mode](https://arxiv.org/abs/2506.22063)
*Durgesh K. Singh,Ahcene Boubekki,Qing Cao,Svein Arne Aase,Robert Jenssen,Michael Kampffmeyer*

Main category: cs.CV

TL;DR: The paper introduces a framework using deep learning to improve the accuracy of left ventricle measurements in cardiac assessments by enforcing straight-line constraints in echocardiography images.


<details>
  <summary>Details</summary>
Motivation: Manual placement of landmarks for cardiac assessment in echocardiography is error-prone and time-consuming, while existing deep learning methods struggle with misalignment issues, leading to inaccurate measurements.

Method: The framework trains a landmark detector on Anatomical M-Mode (AMM) images derived in real-time from B-mode videos and transforms them back to B-mode space. A semi-automatic design with human oversight ensures accurate landmark placement.

Result: Experiments indicate enhanced landmark accuracy compared to standard methods, demonstrating the framework's generalization across different neural network architectures.

Conclusion: The proposed system improves measurement precision, simplifies user interaction, maintains alignment flexibility, and ensures clinical relevance in echocardiographic analysis.

Abstract: Linear measurements of the left ventricle (LV) in the Parasternal Long Axis
(PLAX) view using B-mode echocardiography are crucial for cardiac assessment.
These involve placing 4-6 landmarks along a virtual scanline (SL) perpendicular
to the LV axis near the mitral valve tips. Manual placement is time-consuming
and error-prone, while existing deep learning methods often misalign landmarks,
causing inaccurate measurements. We propose a novel framework that enhances LV
measurement accuracy by enforcing straight-line constraints. A landmark
detector is trained on Anatomical M-Mode (AMM) images, computed in real time
from B-mode videos, then transformed back to B-mode space. This approach
addresses misalignment and reduces measurement errors. Experiments show
improved accuracy over standard B-mode methods, and the framework generalizes
well across network architectures. Our semi-automatic design includes a
human-in-the-loop step where the user only places the SL, simplifying
interaction while preserving alignment flexibility and clinical relevance.

</details>


### [177] [MirrorMe: Towards Realtime and High Fidelity Audio-Driven Halfbody Animation](https://arxiv.org/abs/2506.22065)
*Dechao Meng,Steven Xiao,Xindi Zhang,Guangyuan Wang,Peng Zhang,Qi Wang,Bang Zhang,Liefeng Bo*

Main category: cs.CV

TL;DR: MirrorMe introduces a real-time, controllable audio-driven portrait animation framework using a diffusion transformer model for efficient latent space denoising, achieving high fidelity and temporal consistency.


<details>
  <summary>Details</summary>
Motivation: To address challenges in audio-driven portrait animation, including real-time synthesis and temporal coherence, which existing frame-by-frame diffusion methods struggle to achieve.

Method: MirrorMe employs the LTX video model utilizing video compression in latent space, with three key improvements. These include a reference identity injection mechanism, a causal audio encoder, and a progressive training strategy for refined control and temporal coherence.

Result: MirrorMe outperforms existing methods in fidelity, lip-sync accuracy, and temporal stability when tested on the EMTD Benchmark.

Conclusion: The proposed method achieves state-of-the-art performance in audio-driven portrait animation, offering a real-time and controllable solution with significant improvements in quality and consistency.

Abstract: Audio-driven portrait animation, which synthesizes realistic videos from
reference images using audio signals, faces significant challenges in real-time
generation of high-fidelity, temporally coherent animations. While recent
diffusion-based methods improve generation quality by integrating audio into
denoising processes, their reliance on frame-by-frame UNet architectures
introduces prohibitive latency and struggles with temporal consistency. This
paper introduces MirrorMe, a real-time, controllable framework built on the LTX
video model, a diffusion transformer that compresses video spatially and
temporally for efficient latent space denoising. To address LTX's trade-offs
between compression and semantic fidelity, we propose three innovations: 1. A
reference identity injection mechanism via VAE-encoded image concatenation and
self-attention, ensuring identity consistency; 2. A causal audio encoder and
adapter tailored to LTX's temporal structure, enabling precise audio-expression
synchronization; and 3. A progressive training strategy combining close-up
facial training, half-body synthesis with facial masking, and hand pose
integration for enhanced gesture control. Extensive experiments on the EMTD
Benchmark demonstrate MirrorMe's state-of-the-art performance in fidelity,
lip-sync accuracy, and temporal stability.

</details>


### [178] [Single-Scanline Relative Pose Estimation for Rolling Shutter Cameras](https://arxiv.org/abs/2506.22069)
*Petr Hruby,Marc Pollefeys*

Main category: cs.CV

TL;DR: This paper proposes a new method for estimating relative pose between rolling shutter cameras using intersections of line projections and scanlines, eliminating the need for explicit camera motion modeling.


<details>
  <summary>Details</summary>
Motivation: Existing methods for rolling shutter cameras suffer from the need for explicit motion models, limiting efficiency and versatility in structure-from-motion applications.

Method: The authors use minimal solvers within generic and specialized scenarios, exploring cases with parallel lines and known gravity directions, demonstrating scanline-based pose estimation.

Result: Experiments with Fastec dataset validate the feasibility of the approach for initializing rolling shutter structure-from-motion (SfM).

Conclusion: This approach, independent of motion models, serves as a foundational step for rolling shutter SfM and paves the way for further research in this domain.

Abstract: We propose a novel approach for estimating the relative pose between rolling
shutter cameras using the intersections of line projections with a single
scanline per image. This allows pose estimation without explicitly modeling
camera motion. Alternatively, scanlines can be selected within a single image,
enabling single-view relative pose estimation for scanlines of rolling shutter
cameras. Our approach is designed as a foundational building block for rolling
shutter structure-from-motion (SfM), where no motion model is required, and
each scanline's pose can be computed independently. % We classify minimal
solvers for this problem in both generic and specialized settings, including
cases with parallel lines and known gravity direction, assuming known
intrinsics and no lens distortion. Furthermore, we develop minimal solvers for
the parallel-lines scenario, both with and without gravity priors, by
leveraging connections between this problem and the estimation of 2D structure
from 1D cameras. % Experiments on rolling shutter images from the Fastec
dataset demonstrate the feasibility of our approach for initializing rolling
shutter SfM, highlighting its potential for further development. % The code
will be made publicly available.

</details>


### [179] [Reasoning in machine vision: learning to think fast and slow](https://arxiv.org/abs/2506.22075)
*Shaheer U. Saeed,Yipei Wang,Veeru Kasivisvanathan,Brian R. Davidson,Matthew J. Clarkson,Yipeng Hu,Daniel C. Alexander*

Main category: cs.CV

TL;DR: The paper introduces a novel machine reasoning paradigm inspired by human cognitive processes to improve performance in complex, non-verbal vision tasks using limited labeled data.


<details>
  <summary>Details</summary>
Motivation: While machine learning excels in tasks bound by training data, it struggles with dynamic reasoning in non-verbal domains like vision, which is essential for real-world applications such as medical diagnosis.

Method: The approach integrates a fast-thinking module for familiar tasks and a slow-thinking module for iterative refinement, inspired by dual-process cognitive theories, using self-play reinforcement learning to enable extended inference-time reasoning.

Result: The method outperforms state-of-the-art models, including humans, in real-world vision tasks like computer-vision benchmarks and cancer localization in medical images.

Conclusion: This new reasoning paradigm can transform non-verbal reasoning in machines, demonstrating both scalability and adaptability to data-scarce scenarios.

Abstract: Reasoning is a hallmark of human intelligence, enabling adaptive
decision-making in complex and unfamiliar scenarios. In contrast, machine
intelligence remains bound to training data, lacking the ability to dynamically
refine solutions at inference time. While some recent advances have explored
reasoning in machines, these efforts are largely limited to verbal domains such
as mathematical problem-solving, where explicit rules govern step-by-step
reasoning. Other critical real-world tasks - including visual perception,
spatial reasoning, and radiological diagnosis - require non-verbal reasoning,
which remains an open challenge. Here we present a novel learning paradigm that
enables machine reasoning in vision by allowing performance improvement with
increasing thinking time (inference-time compute), even under conditions where
labelled data is very limited. Inspired by dual-process theories of human
cognition in psychology, our approach integrates a fast-thinking System I
module for familiar tasks, with a slow-thinking System II module that
iteratively refines solutions using self-play reinforcement learning. This
paradigm mimics human reasoning by proposing, competing over, and refining
solutions in data-scarce scenarios. We demonstrate superior performance through
extended thinking time, compared not only to large-scale supervised learning
but also foundation models and even human experts, in real-world vision tasks.
These tasks include computer-vision benchmarks and cancer localisation on
medical images across five organs, showcasing transformative potential for
non-verbal machine reasoning.

</details>


### [180] [Towards Accurate Heart Rate Measurement from Ultra-Short Video Clips via Periodicity-Guided rPPG Estimation and Signal Reconstruction](https://arxiv.org/abs/2506.22078)
*Pei-Kai Huanga,Ya-Ting Chan,Kuan-Wen Chen,Yen-Chun Chou,Shih-Yu Yang,Chiou-Ting Hsu*

Main category: cs.CV

TL;DR: This paper proposes a method for accurate heart rate estimation from ultra-short, 2-second video clips, outperforming previous rPPG methods.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of measuring heart rate accurately from ultra-short video clips, which current methods struggle to handle effectively.

Method: A periodicity-guided rPPG estimation method and a generator to reconstruct longer rPPG signals from ultra-short clips while maintaining periodic consistency.

Result: Demonstrated state-of-the-art performance in heart rate measurement from ultra-short clips on four benchmark datasets.

Conclusion: The proposed method is effective for addressing HR estimation challenges in ultra-short video scenarios and improves upon existing techniques.

Abstract: Many remote Heart Rate (HR) measurement methods focus on estimating remote
photoplethysmography (rPPG) signals from video clips lasting around 10 seconds
but often overlook the need for HR estimation from ultra-short video clips. In
this paper, we aim to accurately measure HR from ultra-short 2-second video
clips by specifically addressing two key challenges. First, to overcome the
limited number of heartbeat cycles in ultra-short video clips, we propose an
effective periodicity-guided rPPG estimation method that enforces consistent
periodicity between rPPG signals estimated from ultra-short clips and their
much longer ground truth signals. Next, to mitigate estimation inaccuracies due
to spectral leakage, we propose including a generator to reconstruct longer
rPPG signals from ultra-short ones while preserving their periodic consistency
to enable more accurate HR measurement. Extensive experiments on four rPPG
estimation benchmark datasets demonstrate that our proposed method not only
accurately measures HR from ultra-short video clips but also outperform
previous rPPG estimation techniques to achieve state-of-the-art performance.

</details>


### [181] [BézierGS: Dynamic Urban Scene Reconstruction with Bézier Curve Gaussian Splatting](https://arxiv.org/abs/2506.22099)
*Zipei Ma,Junzhe Jiang,Yurui Chen,Li Zhang*

Main category: cs.CV

TL;DR: BézierGS leverages Bézier curves and Gaussian splatting to reconstruct dynamic and static street scenes without relying on precise object pose annotations, achieving superior results.


<details>
  <summary>Details</summary>
Motivation: Current methods dependent on precise object pose annotations hinder effective large-scale reconstruction of dynamic street scenes.

Method: BézierGS utilizes learnable Bézier curves to model object motion trajectories, correcting pose errors and introducing supervision for dynamic object rendering and inter-curve consistency.

Result: Experiments on Waymo and nuPlan datasets show BézierGS surpasses state-of-the-art methods in scene reconstruction and view synthesis.

Conclusion: BézierGS offers a scalable, accurate approach for street scene reconstruction, overcoming annotation limitations in autonomous driving simulation.

Abstract: The realistic reconstruction of street scenes is critical for developing
real-world simulators in autonomous driving. Most existing methods rely on
object pose annotations, using these poses to reconstruct dynamic objects and
move them during the rendering process. This dependence on high-precision
object annotations limits large-scale and extensive scene reconstruction. To
address this challenge, we propose B\'ezier curve Gaussian splatting
(B\'ezierGS), which represents the motion trajectories of dynamic objects using
learnable B\'ezier curves. This approach fully leverages the temporal
information of dynamic objects and, through learnable curve modeling,
automatically corrects pose errors. By introducing additional supervision on
dynamic object rendering and inter-curve consistency constraints, we achieve
reasonable and accurate separation and reconstruction of scene elements.
Extensive experiments on the Waymo Open Dataset and the nuPlan benchmark
demonstrate that B\'ezierGS outperforms state-of-the-art alternatives in both
dynamic and static scene components reconstruction and novel view synthesis.

</details>


### [182] [Pedestrian Intention and Trajectory Prediction in Unstructured Traffic Using IDD-PeD](https://arxiv.org/abs/2506.22111)
*Ruthvik Bokkasam,Shankar Gangisetty,A. H. Abdul Hafez,C. V. Jawahar*

Main category: cs.CV

TL;DR: The paper introduces an Indian driving pedestrian dataset to address challenges in predicting pedestrian behavior in unstructured traffic conditions. This dataset highlights flaws in current predictive methods, calling for improved models.


<details>
  <summary>Details</summary>
Motivation: The increasing importance of pedestrian behavior prediction for autonomous driving safety in complex environments necessitates realistic datasets, especially for unstructured and chaotic traffic scenarios.

Method: The study developed a detailed dataset focused on Indian traffic conditions with comprehensive annotations tackling challenges like occlusion, variations in lighting, and unsignalized intersections. It evaluated current methods using this dataset.

Result: State-of-the-art intention prediction methods showed a performance drop of up to 15%, while trajectory prediction methods faced an increase in errors by up to 1208 MSE on the new dataset.

Conclusion: The dataset exposes limitations of existing predictive models and underscores the need for more resilient approaches to pedestrian behavior modeling in unstructured traffic settings, inviting the research community for further advancements.

Abstract: With the rapid advancements in autonomous driving, accurately predicting
pedestrian behavior has become essential for ensuring safety in complex and
unpredictable traffic conditions. The growing interest in this challenge
highlights the need for comprehensive datasets that capture unstructured
environments, enabling the development of more robust prediction models to
enhance pedestrian safety and vehicle navigation. In this paper, we introduce
an Indian driving pedestrian dataset designed to address the complexities of
modeling pedestrian behavior in unstructured environments, such as illumination
changes, occlusion of pedestrians, unsignalized scene types and
vehicle-pedestrian interactions. The dataset provides high-level and detailed
low-level comprehensive annotations focused on pedestrians requiring the
ego-vehicle's attention. Evaluation of the state-of-the-art intention
prediction methods on our dataset shows a significant performance drop of up to
$\mathbf{15\%}$, while trajectory prediction methods underperform with an
increase of up to $\mathbf{1208}$ MSE, defeating standard pedestrian datasets.
Additionally, we present exhaustive quantitative and qualitative analysis of
intention and trajectory baselines. We believe that our dataset will open new
challenges for the pedestrian behavior research community to build robust
models. Project Page:
https://cvit.iiit.ac.in/research/projects/cvit-projects/iddped

</details>


### [183] [Pipe Reconstruction from Point Cloud Data](https://arxiv.org/abs/2506.22118)
*Antje Alex,Jannis Stoppe*

Main category: cs.CV

TL;DR: The paper presents an automated pipeline for reconstructing complex pipe networks from incomplete laser scan data, using techniques like Laplacian contraction and rolling sphere methods.


<details>
  <summary>Details</summary>
Motivation: To address the time-consuming and labor-intensive process of manually modeling pipes from laser scan data for industrial assets' digital twins.

Method: Proposes a pipeline that uses skeleton curve estimation with Laplacian-based contraction, curve elongation, recentering using a rolling sphere technique, 2D circle fitting, and 3D smoothing.

Result: The approach enables accurate determination of pipe properties such as radius, length, and orientation, leading to precise 3D models of complex pipe networks.

Conclusion: Automation facilitates rapid and cost-effective modeling of pipe networks, supporting the creation of digital twins for industrial assets.

Abstract: Accurate digital twins of industrial assets, such as ships and offshore
platforms, rely on the precise reconstruction of complex pipe networks.
However, manual modelling of pipes from laser scan data is a time-consuming and
labor-intensive process. This paper presents a pipeline for automated pipe
reconstruction from incomplete laser scan data. The approach estimates a
skeleton curve using Laplacian-based contraction, followed by curve elongation.
The skeleton axis is then recentred using a rolling sphere technique combined
with 2D circle fitting, and refined with a 3D smoothing step. This enables the
determination of pipe properties, including radius, length and orientation, and
facilitates the creation of detailed 3D models of complex pipe networks. By
automating pipe reconstruction, this approach supports the development of
digital twins, allowing for rapid and accurate modeling while reducing costs.

</details>


### [184] [Low-Rank Implicit Neural Representation via Schatten-p Quasi-Norm and Jacobian Regularization](https://arxiv.org/abs/2506.22134)
*Zhengyun Cheng,Changhao Wang,Guanwen Zhang,Yi Xu,Wei Zhou,Xiangyang Ji*

Main category: cs.CV

TL;DR: The paper proposes a CP-based low-rank tensor function for neural network-driven implicit neural representation, offering sparse solutions with a new regularization method.


<details>
  <summary>Details</summary>
Motivation: To improve low-rank tensor representation in machine learning and computer vision by making it interpretable and sparse while maintaining flexibility.

Method: The method involves using CP decomposition with neural networks, introducing a variational form of the Schatten-p quasi-norm for sparsity, and proposing a spectral norm-based smoothness regularization.

Result: The approach achieves superior performance in multi-dimensional data recovery tasks like image inpainting, denoising, and point cloud upsampling.

Conclusion: The proposed CP-based method provides an interpretable, sparse, and effective solution for tensor data representation in machine learning and computer vision applications.

Abstract: Higher-order tensors are well-suited for representing multi-dimensional data,
such as color images and videos. Low-rank tensor representation has become
essential in machine learning and computer vision, but existing methods like
Tucker decomposition offer flexibility at the expense of interpretability. In
contrast, while the CANDECOMP/PARAFAC (CP) decomposition provides a more
natural and interpretable tensor structure, obtaining sparse solutions remains
challenging. Leveraging the rich properties of CP decomposition, we propose a
CP-based low-rank tensor function parameterized by neural networks for implicit
neural representation (CP-INR). This approach enables continuous data
representation beyond structured grids, fully exploiting the non-linearity of
tensor data with theoretical guarantees on excess risk bounds. To achieve a
sparse CP decomposition, we introduce a variational form of the Schatten-p
quasi-norm and prove its relationship to multilinear rank minimization. For
smoothness, we propose a regularization term based on the spectral norm of the
Jacobian and Hutchinson's trace estimator. Our proposed smoothness
regularization is SVD-free and avoids explicit chain rule derivations. It can
serve as an alternative to Total Variation (TV) regularization in image
denoising tasks and is naturally applicable to continuous data. Extensive
experiments on multi-dimensional data recovery tasks, including image
inpainting, denoising, and point cloud upsampling, demonstrate the superiority
and versatility of our method compared to state-of-the-art approaches.

</details>


### [185] [Q-Frame: Query-aware Frame Selection and Multi-Resolution Adaptation for Video-LLMs](https://arxiv.org/abs/2506.22139)
*Shaojie Zhang,Jiahui Yang,Jianqin Yin,Zhenbo Luo,Jian Luan*

Main category: cs.CV

TL;DR: The paper presents Q-Frame, a novel method for adaptive frame selection in multimodal large language models (MLLMs) tailored to video understanding tasks, overcoming existing limitations of uniform frame sampling.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of effectively capturing query-relevant spatiotemporal clues in videos while dealing with large data volumes and temporal complexity in Video-LLMs.

Method: Q-Frame employs a training-free, plug-and-play approach using a text-image matching network like CLIP and the Gumbel-Max trick for efficient and adaptive frame selection and multi-resolution scaling.

Result: Experiments on benchmark datasets, including MLVU, LongVideoBench, and Video-MME, show that Q-Frame outperforms existing methods and is applicable to various video understanding tasks.

Conclusion: Q-Frame enables Video-LLMs to process more frames within computational limits by preserving critical spatiotemporal information, demonstrating its significant advancement in the field of video comprehension.

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated significant
success in visual understanding tasks. However, challenges persist in adapting
these models for video comprehension due to the large volume of data and
temporal complexity. Existing Video-LLMs using uniform frame sampling often
struggle to capture the query-related crucial spatiotemporal clues of videos
effectively. In this paper, we introduce Q-Frame, a novel approach for adaptive
frame selection and multi-resolution scaling tailored to the video's content
and the specific query. Q-Frame employs a training-free, plug-and-play strategy
generated by a text-image matching network like CLIP, utilizing the Gumbel-Max
trick for efficient frame selection. Q-Frame allows Video-LLMs to process more
frames without exceeding computational limits, thereby preserving critical
temporal and spatial information. We demonstrate Q-Frame's effectiveness
through extensive experiments on benchmark datasets, including MLVU,
LongVideoBench, and Video-MME, illustrating its superiority over existing
methods and its applicability across various video understanding tasks.

</details>


### [186] [Visual Structures Helps Visual Reasoning: Addressing the Binding Problem in VLMs](https://arxiv.org/abs/2506.22146)
*Amirmohammad Izadi,Mohammad Ali Banayeeanzade,Fatemeh Askari,Ali Rahimiakbar,Mohammad Mahdi Vahedi,Hosein Hasani,Mahdieh Soleymani Baghshah*

Main category: cs.CV

TL;DR: The paper addresses the "binding problem" in Vision-Language Models (VLMs) and proposes a solution by incorporating low-level spatial structures to improve visual reasoning.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the limitations of VLMs in reliably associating visual features with their correct referents, which causes errors in tasks like counting and spatial reasoning.

Method: The approach involves augmenting visual inputs with spatial structures (e.g., horizontal lines) and using tailored textual prompts for sequential, spatially-aware parsing.

Result: The proposed method significantly improves VLM performance: 25% improvement in visual search, 26.83% in counting accuracy, a 0.32 reduction in edit distance error, and a 9.5% gain on spatial tasks in a synthetic dataset.

Conclusion: Low-level visual structuring is essential for improving compositional visual reasoning and serves as a promising strategy for enhancing VLMs in spatially grounded tasks.

Abstract: Despite progress in Vision-Language Models (VLMs), their capacity for visual
reasoning is often limited by the \textit{binding problem}: the failure to
reliably associate perceptual features with their correct visual referents.
This limitation underlies persistent errors in tasks such as counting, visual
search, scene description, and spatial relationship understanding. A key factor
is that current VLMs process visual features largely in parallel, lacking
mechanisms for spatially grounded, serial attention. This paper introduces a
simple yet effective intervention: augmenting visual inputs with low-level
spatial structures (e.g., horizontal lines) and pairing this with a textual
prompt that encourages sequential, spatially-aware parsing. We empirically
demonstrate substantial performance improvements across core visual reasoning
tasks. Specifically, our method improves GPT-4o visual search accuracy by
25.00%, increases counting accuracy by 26.83%, reduces edit distance error in
scene description by 0.32, and enhances performance on spatial relationship
tasks by 9.50% on a a 2D synthetic dataset. Furthermore, we find that the
visual modification is essential for these gains; purely textual strategies,
including Chain-of-Thought prompting, are insufficient and can even degrade
performance. Our method enhances binding only with a single-query inference,
underscoring the importance of visual input design over purely
linguistically-based approaches. These findings suggest that low-level visual
structuring is a powerful and underexplored direction for improving
compositional visual reasoning and could serve as a general strategy for
enhancing VLM performance on spatially grounded tasks.

</details>


### [187] [RetFiner: A Vision-Language Refinement Scheme for Retinal Foundation Models](https://arxiv.org/abs/2506.22149)
*Ronald Fecso,José Morano,Ursula Schmidt-Erfurth,Hrvoje Bogunović*

Main category: cs.CV

TL;DR: The paper introduces RetFiner, a vision-language refinement approach for enhancing existing OCT foundation models (FMs) using self-supervised learning techniques. RetFiner improves downstream performance significantly across various retinal disease classification tasks.


<details>
  <summary>Details</summary>
Motivation: Existing OCT foundation models lack semantic understanding and require costly supervised fine-tuning to adapt to specific tasks and populations.

Method: The authors employed a self-supervised learning framework, integrating textual data as supervisory signals and diverse objectives to refine representations in OCT foundation models.

Result: RetFiner demonstrated significant performance improvements on seven OCT classification tasks, with enhanced probing performance increases of 5.8%, 3.9%, and 2.1% on RETFound, UrFound, and VisionFM, respectively.

Conclusion: RetFiner offers an efficient solution for improving retinal OCT foundation models, enabling their adaptation to specific populations and applications without reliance on supervised fine-tuning.

Abstract: The rise of imaging techniques such as optical coherence tomography (OCT) and
advances in deep learning (DL) have enabled clinicians and researchers to
streamline retinal disease staging. A popular DL approach is self-supervised
learning (SSL), where models learn from vast amounts of unlabeled data,
avoiding costly annotation. SSL has allowed the development of foundation
models (FMs), large models that can be used for a variety of downstream tasks.
However, existing FMs for OCT, trained solely on image data, lack a
comprehensive and robust semantic understanding of images, as evidenced by
their downstream performance (especially for complex tasks), and thus require
supervised fine-tuning (which may be unfeasible) to better adapt to specific
applications and populations. To address this, we propose RetFiner, an SSL
vision-language refinement scheme that improves the representations of existing
FMs and enables their efficient and direct adaptation to specific populations
for improved downstream performance. Our method uses a diverse set of training
objectives which take advantage of the rich supervisory signal found in textual
data. We tested RetFiner on the retinal FMs RETFound, UrFound, and VisionFM,
showing significant improvements in linear probing performance on seven highly
diverse OCT classification tasks, with an average increase of 5.8, 3.9, and 2.1
percentage points over their baselines, respectively. Our code and model
weights are publicly available at https://github.com/ronnief1/RetFiner.

</details>


### [188] [Attention-disentangled Uniform Orthogonal Feature Space Optimization for Few-shot Object Detection](https://arxiv.org/abs/2506.22161)
*Taijin Zhao,Heqian Qiu,Yu Dai,Lanxiao Wang,Fanman Meng,Qingbo Wu,Hongliang Li*

Main category: cs.CV

TL;DR: This paper introduces a novel framework called Uniform Orthogonal Feature Space (UOFS) for Few-shot Object Detection (FSOD) by separating objectness recognition and foreground classification into orthogonal components.


<details>
  <summary>Details</summary>
Motivation: Traditional FSOD methods rely on shared feature spaces for objectness and classification, creating challenges with class-specific objectness criteria and limited novel class data.

Method: The authors propose UOFS, decoupling objectness (magnitude) and classification (angle) components. They tackle challenges with a Hybrid Background Optimization (HBO) strategy and introduce the Spatial-wise Attention Disentanglement and Association (SADA) module to resolve conflicts between class-agnostic and class-specific tasks.

Result: Their proposed approach achieves significant improvement over existing FSOD methods using entangled feature spaces.

Conclusion: The UOFS framework provides an effective solution to FSOD challenges by leveraging feature space decoupling and task-specific optimization strategies, showing superior performance in experiments.

Abstract: Few-shot object detection (FSOD) aims to detect objects with limited samples
for novel classes, while relying on abundant data for base classes. Existing
FSOD approaches, predominantly built on the Faster R-CNN detector, entangle
objectness recognition and foreground classification within shared feature
spaces. This paradigm inherently establishes class-specific objectness criteria
and suffers from unrepresentative novel class samples. To resolve this
limitation, we propose a Uniform Orthogonal Feature Space (UOFS) optimization
framework. First, UOFS decouples the feature space into two orthogonal
components, where magnitude encodes objectness and angle encodes
classification. This decoupling enables transferring class-agnostic objectness
knowledge from base classes to novel classes. Moreover, implementing the
disentanglement requires careful attention to two challenges: (1) Base set
images contain unlabeled foreground instances, causing confusion between
potential novel class instances and backgrounds. (2) Angular optimization
depends exclusively on base class foreground instances, inducing overfitting of
angular distributions to base classes. To address these challenges, we propose
a Hybrid Background Optimization (HBO) strategy: (1) Constructing a pure
background base set by removing unlabeled instances in original images to
provide unbiased magnitude-based objectness supervision. (2) Incorporating
unlabeled foreground instances in the original base set into angular
optimization to enhance distribution uniformity. Additionally, we propose a
Spatial-wise Attention Disentanglement and Association (SADA) module to address
task conflicts between class-agnostic and class-specific tasks. Experiments
demonstrate that our method significantly outperforms existing approaches based
on entangled feature spaces.

</details>


### [189] [Frequency-Semantic Enhanced Variational Autoencoder for Zero-Shot Skeleton-based Action Recognition](https://arxiv.org/abs/2506.22179)
*Wenhan Wu,Zhishuai Guo,Chen Chen,Hongfei Xue,Aidong Lu*

Main category: cs.CV

TL;DR: This paper introduces FS-VAE, a novel method for zero-shot skeleton-based action recognition that enhances semantic representation through frequency decomposition and alignment techniques.


<details>
  <summary>Details</summary>
Motivation: Current approaches to zero-shot skeleton-based action recognition often neglect fine-grained action patterns and lack robustness in aligning visual and semantic representations.

Method: The proposed FS-VAE uses a frequency-based enhancement module, multilevel semantic alignment, and calibrated cross-alignment loss to optimize skeleton-text representation learning and address semantic gaps and ambiguities.

Result: Experimental results on benchmarks show FS-VAE effectively differentiates similar action clusters and improves the performance of zero-shot skeleton-based action recognition.

Conclusion: FS-VAE enriches skeletal semantic representation and mitigates representation challenges, advancing the robustness of zero-shot skeleton-based action recognition models.

Abstract: Zero-shot skeleton-based action recognition aims to develop models capable of
identifying actions beyond the categories encountered during training. Previous
approaches have primarily focused on aligning visual and semantic
representations but often overlooked the importance of fine-grained action
patterns in the semantic space (e.g., the hand movements in drinking water and
brushing teeth). To address these limitations, we propose a Frequency-Semantic
Enhanced Variational Autoencoder (FS-VAE) to explore the skeleton semantic
representation learning with frequency decomposition. FS-VAE consists of three
key components: 1) a frequency-based enhancement module with high- and
low-frequency adjustments to enrich the skeletal semantics learning and improve
the robustness of zero-shot action recognition; 2) a semantic-based action
description with multilevel alignment to capture both local details and global
correspondence, effectively bridging the semantic gap and compensating for the
inherent loss of information in skeleton sequences; 3) a calibrated
cross-alignment loss that enables valid skeleton-text pairs to counterbalance
ambiguous ones, mitigating discrepancies and ambiguities in skeleton and text
features, thereby ensuring robust alignment. Evaluations on the benchmarks
demonstrate the effectiveness of our approach, validating that
frequency-enhanced semantic features enable robust differentiation of visually
and semantically similar action clusters, improving zero-shot action
recognition.

</details>


### [190] [ReF-LLE: Personalized Low-Light Enhancement via Reference-Guided Deep Reinforcement Learning](https://arxiv.org/abs/2506.22216)
*Ming Zhao,Pingping Liu,Tongshun Zhang,Zhe Zhang*

Main category: cs.CV

TL;DR: ReF-LLE is a method for enhancing low-light images using Fourier frequency domain and deep reinforcement learning, achieving personalized results.


<details>
  <summary>Details</summary>
Motivation: To address challenges in low-light image enhancement due to varied conditions and subjective user preferences.

Method: ReF-LLE trains with a zero-reference image evaluation for reward signals and uses a personalized adaptive iterative strategy during inference guided by Fourier domain illumination levels.

Result: Outperforms state-of-the-art methods with enhanced perceptual quality and adaptability on benchmark datasets.

Conclusion: ReF-LLE is effective for personalized low-light image enhancement, adapting to user preferences and varying conditions.

Abstract: Low-light image enhancement presents two primary challenges: 1) Significant
variations in low-light images across different conditions, and 2) Enhancement
levels influenced by subjective preferences and user intent. To address these
issues, we propose ReF-LLE, a novel personalized low-light image enhancement
method that operates in the Fourier frequency domain and incorporates deep
reinforcement learning. ReF-LLE is the first to integrate deep reinforcement
learning into this domain. During training, a zero-reference image evaluation
strategy is introduced to score enhanced images, providing reward signals that
guide the model to handle varying degrees of low-light conditions effectively.
In the inference phase, ReF-LLE employs a personalized adaptive iterative
strategy, guided by the zero-frequency component in the Fourier domain, which
represents the overall illumination level. This strategy enables the model to
adaptively adjust low-light images to align with the illumination distribution
of a user-provided reference image, ensuring personalized enhancement results.
Extensive experiments on benchmark datasets demonstrate that ReF-LLE
outperforms state-of-the-art methods, achieving superior perceptual quality and
adaptability in personalized low-light image enhancement.

</details>


### [191] [Boosting Classification with Quantum-Inspired Augmentations](https://arxiv.org/abs/2506.22241)
*Matthias Tschöpe,Vitor Fortes Rey,Sogo Pierre Sanon,Paul Lukowicz,Nikolaos Palaiodimopoulos,Maximilian Kiefer-Emmanouilidis*

Main category: cs.CV

TL;DR: This paper explores the use of quantum-inspired data augmentation via random Bloch sphere rotations, showing improved image classification performance on ImageNet, but notes limitations in enhancing differential privacy.


<details>
  <summary>Details</summary>
Motivation: The paper aims to study how small quantum gate perturbations, typically viewed as computational drawbacks, can be leveraged as an effective data augmentation method to enhance quantum and classical machine learning.

Method: The authors apply quantum-inspired random small-angle Bloch sphere rotations (SU(2) transformations) as a data augmentation technique directly to classical data, and evaluate its impact on image classification using the ImageNet dataset.

Result: The proposed method improved image classification performance with a 3% increase in Top-1 accuracy, a 2.5% increase in Top-5 accuracy, and a significant improvement in the F1 score (8%-12%) compared to standard classical augmentation.

Conclusion: While the quantum-inspired data augmentation method enhances classification accuracy, it does not contribute to improved differential privacy, limiting its application in privacy-focused computations.

Abstract: Understanding the impact of small quantum gate perturbations, which are
common in quantum digital devices but absent in classical computers, is crucial
for identifying potential advantages in quantum machine learning. While these
perturbations are typically seen as detrimental to quantum computation, they
can actually enhance performance by serving as a natural source of data
augmentation. Additionally, they can often be efficiently simulated on
classical hardware, enabling quantum-inspired approaches to improve classical
machine learning methods. In this paper, we investigate random Bloch sphere
rotations, which are fundamental SU(2) transformations, as a simple yet
effective quantum-inspired data augmentation technique. Unlike conventional
augmentations such as flipping, rotating, or cropping, quantum transformations
lack intuitive spatial interpretations, making their application to tasks like
image classification less straightforward. While common quantum augmentation
methods rely on applying quantum models or trainable quanvolutional layers to
classical datasets, we focus on the direct application of small-angle Bloch
rotations and their effect on classical data. Using the large-scale ImageNet
dataset, we demonstrate that our quantum-inspired augmentation method improves
image classification performance, increasing Top-1 accuracy by 3%, Top-5
accuracy by 2.5%, and the F$_1$ score from 8% to 12% compared to standard
classical augmentation methods. Finally, we examine the use of stronger unitary
augmentations. Although these transformations preserve information in
principle, they result in visually unrecognizable images with potential
applications for privacy computations. However, we show that our augmentation
approach and simple SU(2) transformations do not enhance differential privacy
and discuss the implications of this limitation.

</details>


### [192] [4D-VLA: Spatiotemporal Vision-Language-Action Pretraining with Cross-Scene Calibration](https://arxiv.org/abs/2506.22242)
*Jiahui Zhang,Yurui Chen,Yueming Xu,Ze Huang,Yanpeng Zhou,Yu-Jie Yuan,Xinyue Cai,Guowei Huang,Xingyue Quan,Hang Xu,Li Zhang*

Main category: cs.CV

TL;DR: The paper proposes 4D-VLA, a model that integrates 4D information for robotic data pretraining to address inefficiencies caused by chaotic action distributions, achieving superior performance on spatial reasoning and view generalization.


<details>
  <summary>Details</summary>
Motivation: To overcome inefficiencies in robotic data pretraining caused by coordinate system chaos and state chaos, which result from incomplete observation inputs.

Method: The authors developed 4D-VLA, incorporating sequential RGB-D inputs to integrate depth and temporal data, aligning robot and scene coordinate systems, and introduced a memory bank sampling strategy for better frame extraction.

Result: The proposed method significantly improved pretraining efficiency and model performance, evidenced by higher success rates in experiments and superior results on the MV-Bench benchmark compared to existing methods.

Conclusion: 4D-VLA enhances spatiotemporal reasoning and efficiency in robotic data pretraining, showing robust spatial understanding and adaptability in both simulated and real-world scenarios.

Abstract: Leveraging diverse robotic data for pretraining remains a critical challenge.
Existing methods typically model the dataset's action distribution using simple
observations as inputs. However, these inputs are often incomplete, resulting
in a dispersed conditional action distribution-an issue we refer to as
coordinate system chaos and state chaos. This inconsistency significantly
hampers pretraining efficiency. To address this, we propose 4D-VLA, a novel
approach that effectively integrates 4D information into the input to mitigate
these sources of chaos. Our model introduces depth and temporal information
into visual features with sequential RGB-D inputs, aligning the coordinate
systems of the robot and the scene. This alignment endows the model with strong
spatiotemporal reasoning capabilities while minimizing training overhead.
Additionally, we introduce memory bank sampling, a frame sampling strategy
designed to extract informative frames from historical images, further
improving effectiveness and efficiency. Experimental results demonstrate that
our pretraining method and architectural components substantially enhance model
performance. In both simulated and real-world experiments, our model achieves a
significant increase in success rate over OpenVLA. To further assess spatial
perception and generalization to novel views, we introduce MV-Bench, a
multi-view simulation benchmark. Our model consistently outperforms existing
methods, demonstrating stronger spatial understanding and adaptability.

</details>


### [193] [EAMamba: Efficient All-Around Vision State Space Model for Image Restoration](https://arxiv.org/abs/2506.22246)
*Yu-Cheng Lin,Yu-Syuan Xu,Hao-Wei Chen,Hsien-Kai Kuo,Chun-Yi Lee*

Main category: cs.CV

TL;DR: The paper introduces EAMamba, an improved image restoration framework, addressing Vision Mamba’s limitations by reducing computational complexity and fixing local pixel forgetting issues.


<details>
  <summary>Details</summary>
Motivation: Vision Mamba’s strengths in modeling long-range dependencies come with notable drawbacks: scaling computational complexity and local pixel forgetting in low-level tasks.

Method: It proposes EAMamba with a Multi-Head Selective Scan Module (MHSSM) using an all-around scanning mechanism to aggregate multiple scanning sequences efficiently and capture holistic information.

Result: EAMamba achieves a significant reduction of 31-89% in FLOPs, maintaining competitive performance across tasks like super resolution, denoising, deblurring, and dehazing.

Conclusion: EAMamba is a more efficient alternative to Vision Mamba, cutting computational costs without compromising image restoration quality.

Abstract: Image restoration is a key task in low-level computer vision that aims to
reconstruct high-quality images from degraded inputs. The emergence of Vision
Mamba, which draws inspiration from the advanced state space model Mamba, marks
a significant advancement in this field. Vision Mamba demonstrates excellence
in modeling long-range dependencies with linear complexity, a crucial advantage
for image restoration tasks. Despite its strengths, Vision Mamba encounters
challenges in low-level vision tasks, including computational complexity that
scales with the number of scanning sequences and local pixel forgetting. To
address these limitations, this study introduces Efficient All-Around Mamba
(EAMamba), an enhanced framework that incorporates a Multi-Head Selective Scan
Module (MHSSM) with an all-around scanning mechanism. MHSSM efficiently
aggregates multiple scanning sequences, which avoids increases in computational
complexity and parameter count. The all-around scanning strategy implements
multiple patterns to capture holistic information and resolves the local pixel
forgetting issue. Our experimental evaluations validate these innovations
across several restoration tasks, including super resolution, denoising,
deblurring, and dehazing. The results validate that EAMamba achieves a
significant 31-89% reduction in FLOPs while maintaining favorable performance
compared to existing low-level Vision Mamba methods.

</details>


### [194] [COOCO -- Common Objects Out-of-Context -- Semantic Violation in Scenes: Investigating Multimodal Context in Referential Communication](https://arxiv.org/abs/2506.22274)
*Filippo Merlo,Ece Takmaz,Wenkai Chen,Albert Gatt*

Main category: cs.CV

TL;DR: This study investigates whether Vision-Language Models (VLMs) use scene context for object reference by introducing the COOCO dataset, revealing adaptive reliance on scene context based on congruence and noise.


<details>
  <summary>Details</summary>
Motivation: To understand if Vision-Language Models (VLMs) utilize natural scene contexts to predict and refer to objects—similar to how humans leverage scene contexts for expectations about object presence and arrangement.

Method: Authors introduce the COOCO dataset and evaluate VLMs' use of scene context under varying congruency and noise levels. Attention analysis is conducted to assess how models balance local and contextual information when generating object references.

Result: VLMs adaptively adjust reliance on scene context, particularly under conditions of high scene-object congruence or object degradation. Mid-level layers in VLMs heighten focus on the target object under moderate noise.

Conclusion: VLMs dynamically balance local and contextual data for effective reference generation, exhibiting adaptive use of semantic congruence and robustness against noise degradation.

Abstract: Natural scenes provide us with rich contexts for object recognition and
reference. In particular, knowing what type of scene one is looking at
generates expectations about which objects will occur, and what their spatial
configuration should be. Do Vision-Language Models (VLMs) learn to rely on
scene contexts in a similar way, when generating references to objects? To
address this question, we introduce the \textit{Common Objects Out-of-Context
(COOCO)} dataset and test to what extent VLMs rely on scene context to refer to
objects under different degrees of scene-object congruency, and different
perturbations. Our findings show that models leverage scene context adaptively,
depending on both the semantic relatedness between object and scene and the
level of noise. In particular, models rely more on context under high
target-scene congruence or when objects are degraded. Attention analysis
reveals that successful object categorisation involves increased focus on the
target in mid-level layers, especially under moderate noise, suggesting that
VLMs dynamically balance local and contextual information for reference
generation. We make our dataset, code and models available at
\href{https://github.com/cs-nlp-uu/scenereg}{https://github.com/cs-nlp-uu/scenereg}.

</details>


### [195] [Rethinking Visual Token Reduction in LVLMs under Cross-modal Misalignment](https://arxiv.org/abs/2506.22283)
*Rui Xu,Yunke Wang,Yong Luo,Bo Du*

Main category: cs.CV

TL;DR: The paper introduces VisionDrop, a training-free visual token reduction framework for Large Vision-Language Models (LVLMs), solving computational inefficiencies with improved scalability.


<details>
  <summary>Details</summary>
Motivation: Address the computational overhead and scalability challenges in LVLMs caused by an excessive number of visual tokens, and overcome the limitations in existing text-guided visual token reduction methods due to cross-modal misalignments.

Method: VisionDrop is proposed as a visual-only pruning framework that relies on intra-modal (visual-to-visual) attention for selecting relevant visual tokens. It applies progressive pruning with token selection and lightweight merging without requiring training or textual signals.

Result: Experiments demonstrate that VisionDrop achieves consistent performance improvements over existing methods across diverse benchmarks, even under stringent token reduction constraints.

Conclusion: VisionDrop provides a simple and effective solution to reduce visual token redundancy, enabling efficient inference without sacrificing task performance, offering scalability improvements for LVLMs.

Abstract: Large Vision-Language Models (LVLMs) encode visual inputs as dense sequences
of patch-level tokens to capture fine-grained semantics. These visual tokens
often outnumber their textual counterparts by a large margin, leading to
substantial computational overhead and limiting the scalability of LVLMs in
practice. Previous efforts have explored visual token reduction either prior to
or within the large language models (LLM). However, most in-LLM reduction
approaches rely on text-conditioned interactions, implicitly assuming that
textual tokens can reliably capture the importance of visual tokens. In this
work, we revisit this assumption and reveal causal, semantic, and spatial forms
of cross-modal misalignment. These misalignments undermine the effectiveness of
text-guided visual token reduction. To address this, we introduce VisionDrop, a
training-free, visual-only pruning framework that selects informative visual
tokens based on intra-modal (visual-to-visual) attention, without relying on
textual signals. To further suppress redundancy throughout the model hierarchy,
we treat the visual encoder and the LLM as a unified system and design a
progressive pruning pipeline. Our method performs dominant token selection and
lightweight contextual merging at multiple stages, enabling fine-grained visual
information to be retained even under aggressive token budgets. Extensive
experiments across diverse benchmarks show that VisionDrop achieves consistent
improvements over existing methods, despite requiring no additional training or
complex modifications. Its simple yet effective design enables efficient
inference while preserving strong performance across tasks.

</details>


### [196] [RoomCraft: Controllable and Complete 3D Indoor Scene Generation](https://arxiv.org/abs/2506.22291)
*Mengqi Zhou,Xipeng Wang,Yuxi Wang,Zhaoxiang Zhang*

Main category: cs.CV

TL;DR: RoomCraft is a pipeline for creating realistic 3D indoor scenes from images, sketches, or text, using optimization strategies to address geometric and spatial challenges.


<details>
  <summary>Details</summary>
Motivation: Existing methods for generating 3D indoor scenes struggle with handling geometric consistency, spatial relationships, and visual realism. Neural methods often produce repetitive results, while procedural approaches falter in scenarios with numerous constraints, leading to object collisions and compromising layout completeness.

Method: RoomCraft employs a multi-stage pipeline that extracts scene information from user inputs, creates a spatial relationship network, and applies heuristic-based optimization (HDFS). A unified constraint representation and a Conflict-Aware Positioning Strategy (CAPS) are introduced to manage complex scenarios and minimize furniture collisions.

Result: RoomCraft significantly improves upon existing methods, producing realistic, semantically coherent, and visually pleasing room layouts across various input types, as demonstrated by extensive experiments.

Conclusion: RoomCraft successfully addresses key challenges in 3D indoor scene generation by combining scene understanding, optimization frameworks, and innovative positioning strategies to deliver superior results.

Abstract: Generating realistic 3D indoor scenes from user inputs remains a challenging
problem in computer vision and graphics, requiring careful balance of geometric
consistency, spatial relationships, and visual realism. While neural generation
methods often produce repetitive elements due to limited global spatial
reasoning, procedural approaches can leverage constraints for controllable
generation but struggle with multi-constraint scenarios. When constraints
become numerous, object collisions frequently occur, forcing the removal of
furniture items and compromising layout completeness.
  To address these limitations, we propose RoomCraft, a multi-stage pipeline
that converts real images, sketches, or text descriptions into coherent 3D
indoor scenes. Our approach combines a scene generation pipeline with a
constraint-driven optimization framework. The pipeline first extracts
high-level scene information from user inputs and organizes it into a
structured format containing room type, furniture items, and spatial relations.
It then constructs a spatial relationship network to represent furniture
arrangements and generates an optimized placement sequence using a
heuristic-based depth-first search (HDFS) algorithm to ensure layout coherence.
To handle complex multi-constraint scenarios, we introduce a unified constraint
representation that processes both formal specifications and natural language
inputs, enabling flexible constraint-oriented adjustments through a
comprehensive action space design. Additionally, we propose a Conflict-Aware
Positioning Strategy (CAPS) that dynamically adjusts placement weights to
minimize furniture collisions and ensure layout completeness.
  Extensive experiments demonstrate that RoomCraft significantly outperforms
existing methods in generating realistic, semantically coherent, and visually
appealing room layouts across diverse input modalities.

</details>


### [197] [OutDreamer: Video Outpainting with a Diffusion Transformer](https://arxiv.org/abs/2506.22298)
*Linhao Zhong,Fan Li,Yi Huang,Jianzhuang Liu,Renjing Pei,Fenglong Song*

Main category: cs.CV

TL;DR: OutDreamer is a new DiT-based video outpainting framework using advanced techniques to extend video boundaries with improved quality, temporal, and spatial consistency.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address limitations in video outpainting methods that struggle with generating high-quality and adaptable content, particularly focusing on temporal and spatial consistency.

Method: OutDreamer uses a DiT-based architecture with two branches: an efficient video control branch to extract masked information and a conditional outpainting branch for missing content generation. It introduces a mask-driven self-attention layer and a latent alignment loss for consistency between and within frames, as well as a cross-video-clip refiner for long video consistency.

Result: OutDreamer achieves superior performance compared to state-of-the-art zero-shot video outpainting methods, validated through extensive benchmarks.

Conclusion: The proposed OutDreamer framework demonstrates significant improvements in video outpainting quality, adaptability, and temporal-spatial consistency.

Abstract: Video outpainting is a challenging task that generates new video content by
extending beyond the boundaries of an original input video, requiring both
temporal and spatial consistency. Many state-of-the-art methods utilize latent
diffusion models with U-Net backbones but still struggle to achieve high
quality and adaptability in generated content. Diffusion transformers (DiTs)
have emerged as a promising alternative because of their superior performance.
We introduce OutDreamer, a DiT-based video outpainting framework comprising two
main components: an efficient video control branch and a conditional
outpainting branch. The efficient video control branch effectively extracts
masked video information, while the conditional outpainting branch generates
missing content based on these extracted conditions. Additionally, we propose a
mask-driven self-attention layer that dynamically integrates the given mask
information, further enhancing the model's adaptability to outpainting tasks.
Furthermore, we introduce a latent alignment loss to maintain overall
consistency both within and between frames. For long video outpainting, we
employ a cross-video-clip refiner to iteratively generate missing content,
ensuring temporal consistency across video clips. Extensive evaluations
demonstrate that our zero-shot OutDreamer outperforms state-of-the-art
zero-shot methods on widely recognized benchmarks.

</details>


### [198] [MatChA: Cross-Algorithm Matching with Feature Augmentation](https://arxiv.org/abs/2506.22336)
*Paula Carbó Cubero,Alberto Jaenal Gálvez,André Mateus,José Araújo,Patric Jensfelt*

Main category: cs.CV

TL;DR: The paper introduces a method to improve visual localization by addressing the challenges of cross-feature detector scenarios.


<details>
  <summary>Details</summary>
Motivation: Different devices use distinct feature extraction algorithms, and low repeatability of keypoints makes cross-feature matching extremely difficult.

Method: The approach involves feature descriptor augmentation for cross-detector feature matching followed by translation into a latent space.

Result: The method significantly enhances image matching and visual localization in cross-feature scenarios.

Conclusion: The proposed method overcomes the challenges of current solutions in cross-feature detector cases, offering substantial improvements in benchmarks.

Abstract: State-of-the-art methods fail to solve visual localization in scenarios where
different devices use different sparse feature extraction algorithms to obtain
keypoints and their corresponding descriptors. Translating feature descriptors
is enough to enable matching. However, performance is drastically reduced in
cross-feature detector cases, because current solutions assume common
keypoints. This means that the same detector has to be used, which is rarely
the case in practice when different descriptors are used. The low repeatability
of keypoints, in addition to non-discriminatory and non-distinctive
descriptors, make the identification of true correspondences extremely
challenging. We present the first method tackling this problem, which performs
feature descriptor augmentation targeting cross-detector feature matching, and
then feature translation to a latent space. We show that our method
significantly improves image matching and visual localization in the
cross-feature scenario and evaluate the proposed method on several benchmarks.

</details>


### [199] [A Deep Learning framework for building damage assessment using VHR SAR and geospatial data: demonstration on the 2023 Turkiye Earthquake](https://arxiv.org/abs/2506.22338)
*Luigi Russo,Deodato Tapete,Silvia Liberata Ullo,Paolo Gamba*

Main category: cs.CV

TL;DR: The paper presents a multimodal deep learning framework for detecting building damage using post-event synthetic aperture radar (SAR) imagery and geospatial data, enabling rapid and reliable assessments without relying on pre-event images.


<details>
  <summary>Details</summary>
Motivation: To enable effective disaster response and recovery by addressing limitations like cloud cover and unavailability of pre-event imagery in disaster mapping using satellite data.

Method: The method combines single-date SAR imagery, OpenStreetMap building footprints, digital surface models, and structural attributes from the Global Earthquake Model, utilizing a multimodal deep learning framework for post-event damage assessment.

Result: The framework was validated using data from the 2023 Turkey earthquake, demonstrating enhanced accuracy and generalizability when incorporating geospatial features.

Conclusion: The approach shows strong potential for scalable, automated, and rapid building damage assessments in diverse disaster scenarios, aiding disaster management efforts without the need for pre-event data.

Abstract: Building damage identification shortly after a disaster is crucial for
guiding emergency response and recovery efforts. Although optical satellite
imagery is commonly used for disaster mapping, its effectiveness is often
hampered by cloud cover or the absence of pre-event acquisitions. To overcome
these challenges, we introduce a novel multimodal deep learning (DL) framework
for detecting building damage using single-date very high resolution (VHR)
Synthetic Aperture Radar (SAR) imagery from the Italian Space Agency (ASI)
COSMO SkyMed (CSK) constellation, complemented by auxiliary geospatial data.
Our method integrates SAR image patches, OpenStreetMap (OSM) building
footprints, digital surface model (DSM) data, and structural and exposure
attributes from the Global Earthquake Model (GEM) to improve detection accuracy
and contextual interpretation. Unlike existing approaches that depend on pre
and post event imagery, our model utilizes only post event data, facilitating
rapid deployment in critical scenarios. The framework effectiveness is
demonstrated using a new dataset from the 2023 earthquake in Turkey, covering
multiple cities with diverse urban settings. Results highlight that
incorporating geospatial features significantly enhances detection performance
and generalizability to previously unseen areas. By combining SAR imagery with
detailed vulnerability and exposure information, our approach provides reliable
and rapid building damage assessments without the dependency from available
pre-event data. Moreover, the automated and scalable data generation process
ensures the framework's applicability across diverse disaster-affected regions,
underscoring its potential to support effective disaster management and
recovery efforts. Code and data will be made available upon acceptance of the
paper.

</details>


### [200] [Closing the Performance Gap in Biometric Cryptosystems: A Deeper Analysis on Unlinkable Fuzzy Vaults](https://arxiv.org/abs/2506.22347)
*Hans Geißner,Christian Rathgeb*

Main category: cs.CV

TL;DR: The paper identifies and addresses performance gaps in fuzzy vault-based biometric cryptosystems (BCS), primarily due to variable feature set sizes and feature transformation processes, by introducing an equal frequent intervals quantization method.


<details>
  <summary>Details</summary>
Motivation: To analyze and mitigate the performance degradation in fuzzy vault-based biometric cryptosystems caused by inconsistent feature set sizes and information loss during feature transformations.

Method: The authors propose a feature quantization approach based on equal frequent intervals, which ensures fixed feature set sizes and eliminates the need for training while improving adaptability.

Result: The proposed method reduces performance losses linked to template protection and integrates well with existing systems, as validated by experiments on face, fingerprint, and iris recognition modalities.

Conclusion: The novel method minimizes the negative effects associated with feature transformations and performance gaps, leading to enhanced biometric system performance across multiple modalities with minimal degradation.

Abstract: This paper analyses and addresses the performance gap in the fuzzy
vault-based \ac{BCS}. We identify unstable error correction capabilities, which
are caused by variable feature set sizes and their influence on similarity
thresholds, as a key source of performance degradation. This issue is further
compounded by information loss introduced through feature type transformations.
To address both problems, we propose a novel feature quantization method based
on \it{equal frequent intervals}. This method guarantees fixed feature set
sizes and supports training-free adaptation to any number of intervals. The
proposed approach significantly reduces the performance gap introduced by
template protection. Additionally, it integrates seamlessly with existing
systems to minimize the negative effects of feature transformation. Experiments
on state-of-the-art face, fingerprint, and iris recognition systems confirm
that only minimal performance degradation remains, demonstrating the
effectiveness of the method across major biometric modalities.

</details>


### [201] [From Ground to Air: Noise Robustness in Vision Transformers and CNNs for Event-Based Vehicle Classification with Potential UAV Applications](https://arxiv.org/abs/2506.22360)
*Nouf Almesafri,Hector Figueiredo,Miguel Arana-Catania*

Main category: cs.CV

TL;DR: This study compares the performance of ResNet34 and Vision Transformer (ViT B16) on event-based camera data, finding ResNet34 excels slightly in accuracy, while ViT B16 shows better robustness, especially under noise.


<details>
  <summary>Details</summary>
Motivation: To explore optimal deep learning architectures for event-based cameras, which are increasingly crucial for dynamic applications like UAVs and autonomous vehicles.

Method: Fine-tuning ResNet34 and ViT B16 on the GEN1 event-based dataset, then evaluating their performance under standard and noisy conditions.

Result: ResNet34 achieved 88% accuracy and ViT B16 86% on clean data, with ResNet34 slightly better in accuracy but ViT B16 showing stronger robustness, especially with less pre-training data.

Conclusion: While ResNet34 performs marginally better in accuracy, ViT B16's robustness and adaptability make it a promising choice for dynamic applications like UAVs and aviation-related tasks.

Abstract: This study investigates the performance of the two most relevant computer
vision deep learning architectures, Convolutional Neural Network and Vision
Transformer, for event-based cameras. These cameras capture scene changes,
unlike traditional frame-based cameras with capture static images, and are
particularly suited for dynamic environments such as UAVs and autonomous
vehicles. The deep learning models studied in this work are ResNet34 and ViT
B16, fine-tuned on the GEN1 event-based dataset. The research evaluates and
compares these models under both standard conditions and in the presence of
simulated noise. Initial evaluations on the clean GEN1 dataset reveal that
ResNet34 and ViT B16 achieve accuracies of 88% and 86%, respectively, with
ResNet34 showing a slight advantage in classification accuracy. However, the
ViT B16 model demonstrates notable robustness, particularly given its
pre-training on a smaller dataset. Although this study focuses on ground-based
vehicle classification, the methodologies and findings hold significant promise
for adaptation to UAV contexts, including aerial object classification and
event-based vision systems for aviation-related tasks.

</details>


### [202] [Exploiting Vision Language Model for Training-Free 3D Point Cloud OOD Detection via Graph Score Propagation](https://arxiv.org/abs/2506.22375)
*Tiankai Chen,Yushu Li,Adam Goodge,Fei Teng,Xulei Yang,Tianrui Li,Xun Xu*

Main category: cs.CV

TL;DR: The paper proposes a training-free framework using Vision-Language Models (VLMs) enhanced with a Graph Score Propagation (GSP) method for OOD detection in 3D point clouds, achieving superior performance.


<details>
  <summary>Details</summary>
Motivation: To address the unique challenges of Out-of-Distribution (OOD) detection in 3D point cloud data and overcome the gap in existing methods tailored primarily for 2D image data.

Method: A Graph Score Propagation (GSP) mechanism is designed leveraging VLMs. It utilizes a graph constructed from class prototypes and data structure, prompt clustering, and self-training negative prompts to enhance 3D OOD detection.

Result: The proposed method surpasses existing state-of-the-art approaches in OOD detection on both synthetic and real-world 3D point cloud datasets.

Conclusion: The GSP framework effectively extends VLMs for robust OOD detection in 3D point clouds, offering adaptability for few-shot scenarios and outperforming traditional methods.

Abstract: Out-of-distribution (OOD) detection in 3D point cloud data remains a
challenge, particularly in applications where safe and robust perception is
critical. While existing OOD detection methods have shown progress for 2D image
data, extending these to 3D environments involves unique obstacles. This paper
introduces a training-free framework that leverages Vision-Language Models
(VLMs) for effective OOD detection in 3D point clouds. By constructing a graph
based on class prototypes and testing data, we exploit the data manifold
structure to enhancing the effectiveness of VLMs for 3D OOD detection. We
propose a novel Graph Score Propagation (GSP) method that incorporates prompt
clustering and self-training negative prompting to improve OOD scoring with
VLM. Our method is also adaptable to few-shot scenarios, providing options for
practical applications. We demonstrate that GSP consistently outperforms
state-of-the-art methods across synthetic and real-world datasets 3D point
cloud OOD detection.

</details>


### [203] [Can Video Large Multimodal Models Think Like Doubters-or Double-Down: A Study on Defeasible Video Entailment](https://arxiv.org/abs/2506.22385)
*Yue Zhang,Jilei Sun,Yunhui Guo,Vibhav Gogate*

Main category: cs.CV

TL;DR: The paper introduces the Defeasible Video Entailment (DVidE) task to improve video large multimodal models' (VLMMs) dynamic reasoning using new methods and datasets.


<details>
  <summary>Details</summary>
Motivation: VLMMs face challenges with abstract and adaptive reasoning, which requires them to revise inferences based on new information. This gap between human reasoning and model capabilities motivates the study.

Method: The DVidE task includes two versions: classification and generation. A novel Chain of Counterfactual Thought framework with ASR-enhanced content and rationale refinement addresses classification, while a model combining ASR outputs with LLMs is designed for generative tasks. A new benchmark dataset and evaluation metric are also introduced.

Result: The proposed methods showed significant improvement in the dynamic reasoning abilities of VLMMs, as evidenced by experimental results.

Conclusion: Enhancing VLMMs with counterfactual reasoning, ASR integration, and strong evaluation frameworks can significantly improve their ability to perform defeasible inference and dynamic reasoning, bringing them closer to human-like adaptive thinking.

Abstract: Video Large Multimodal Models (VLMMs) have made impressive strides in
understanding video content, but they often struggle with abstract and adaptive
reasoning-the ability to revise their interpretations when new information
emerges. In reality, conclusions are rarely set in stone; additional context
can strengthen or weaken an initial inference. To address this, we introduce
Defeasible Video Entailment (DVidE), a new task that challenges models to think
like doubters, constantly updating their reasoning based on evolving evidence.
In DVidE, given a video premise and a textual hypothesis, models must determine
whether a new update strengthens or weakens the hypothesis (classification
version) or generate a coherent update that modifies the entailment
relationship (generation version). For solving the classification task, we
propose the Chain of Counterfactual Thought framework, utilizing counterfactual
reasoning, ASR-enhanced video content, and rationale refinement to reduce
inference bias. For the generation task, we develop a framework that combines
ASR output with a Large Language Model (LLM) to produce coherent, contextually
relevant updates aligned with the intended strengthener or weakener goals.
Additionally, we introduce a novel benchmark dataset, with
strengthener/weakener annotations and an LLM-based evaluation metric
specifically designed for assessing generative performance. Experimental
results demonstrate significant improvements, highlighting our proposed method
in enhancing dynamic reasoning capabilities of VLMMs.

</details>


### [204] [Test-Time Consistency in Vision Language Models](https://arxiv.org/abs/2506.22395)
*Shih-Han Chou,Shivam Chandhok,James J. Little,Leonid Sigal*

Main category: cs.CV

TL;DR: The paper introduces a test-time framework to improve semantic consistency in Vision-Language Models (VLMs) by aligning predictions for semantically equivalent inputs without retraining.


<details>
  <summary>Details</summary>
Motivation: Current Vision-Language Models, despite high accuracy, demonstrate inconsistent behavior when given semantically equivalent inputs, raising concerns about their reliability.

Method: The paper proposes a post-hoc, model-agnostic framework leveraging Cross-Entropy Agreement Loss and Pseudo-Label Consistency Loss to enforce prediction consistency at test-time without supervised re-training.

Result: The method shows significant improvement in semantic consistency across state-of-the-art models on the MM-R3 benchmark.

Conclusion: The proposed framework is effective, simple, and opens new directions for inference-time adaptation in multimodal learning, enhancing VLMs' reliability and robustness.

Abstract: Vision-Language Models (VLMs) have achieved impressive performance across a
wide range of multimodal tasks, yet they often exhibit inconsistent behavior
when faced with semantically equivalent inputs, undermining their reliability
and robustness. Recent benchmarks, such as MM-R3, highlight that even
state-of-the-art VLMs can produce divergent predictions across semantically
equivalent inputs, despite maintaining high average accuracy. Prior work
addresses this issue by modifying model architectures or conducting large-scale
fine-tuning on curated datasets. In contrast, we propose a simple and effective
test-time consistency framework that enhances semantic consistency without
supervised re-training. Our method is entirely post-hoc, model-agnostic, and
applicable to any VLM with access to its weights. Given a single test point, we
enforce consistent predictions via two complementary objectives: (i) a
Cross-Entropy Agreement Loss that aligns predictive distributions across
semantically equivalent inputs, and (ii) a Pseudo-Label Consistency Loss that
draws outputs toward a self-averaged consensus. Our method is plug-and-play and
leverages information from a single test input itself to improve consistency.
Experiments on the MM-R3 benchmark show that our framework yields substantial
gains in consistency across state-of-the-art models, establishing a new
direction for inference-time adaptation in multimodal learning.

</details>


### [205] [Shape-for-Motion: Precise and Consistent Video Editing with 3D Proxy](https://arxiv.org/abs/2506.22432)
*Yuhao Liu,Tengfei Wang,Fang Liu,Zhenwei Wang,Rynson W. H. Lau*

Main category: cs.CV

TL;DR: The paper introduces Shape-for-Motion, a 3D-proxy-based framework for precise and consistent video editing.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of fine-grained alignment with user intentions in video editing using deep generative models.

Method: A framework converting objects in videos to time-consistent 3D proxies for editing, with automated propagation of edits, 2D projection, and diffusion-based regeneration.

Result: The method supports diverse manipulations like pose and texture editing while ensuring high accuracy and consistency.

Conclusion: Shape-for-Motion advances controllable video editing by providing a novel, user-friendly, and precise editing approach validated through extensive experiments.

Abstract: Recent advances in deep generative modeling have unlocked unprecedented
opportunities for video synthesis. In real-world applications, however, users
often seek tools to faithfully realize their creative editing intentions with
precise and consistent control. Despite the progress achieved by existing
methods, ensuring fine-grained alignment with user intentions remains an open
and challenging problem. In this work, we present Shape-for-Motion, a novel
framework that incorporates a 3D proxy for precise and consistent video
editing. Shape-for-Motion achieves this by converting the target object in the
input video to a time-consistent mesh, i.e., a 3D proxy, allowing edits to be
performed directly on the proxy and then inferred back to the video frames. To
simplify the editing process, we design a novel Dual-Propagation Strategy that
allows users to perform edits on the 3D mesh of a single frame, and the edits
are then automatically propagated to the 3D meshes of the other frames. The 3D
meshes for different frames are further projected onto the 2D space to produce
the edited geometry and texture renderings, which serve as inputs to a
decoupled video diffusion model for generating edited results. Our framework
supports various precise and physically-consistent manipulations across the
video frames, including pose editing, rotation, scaling, translation, texture
modification, and object composition. Our approach marks a key step toward
high-quality, controllable video editing workflows. Extensive experiments
demonstrate the superiority and effectiveness of our approach. Project page:
https://shapeformotion.github.io/

</details>


### [206] [WarpRF: Multi-View Consistency for Training-Free Uncertainty Quantification and Applications in Radiance Fields](https://arxiv.org/abs/2506.22433)
*Sadra Safadoust,Fabio Tosi,Fatma Güney,Matteo Poggi*

Main category: cs.CV

TL;DR: WarpRF is a training-free framework to quantify uncertainty in radiance fields using backward warping and consistency checks across viewpoints.


<details>
  <summary>Details</summary>
Motivation: There was a need for a simple and general-purpose approach to accurately quantify uncertainty in radiance fields without the complexity of training.

Method: WarpRF leverages backward warping and measures consistency among rendered images by projecting reliable viewpoints to unseen ones to assess uncertainty.

Result: WarpRF surpasses existing methods in uncertainty quantification and improves performance in tasks like active view selection and active mapping.

Conclusion: The framework is universal, requiring no additional training, simple to implement, and effective for radiance field uncertainty analysis and practical downstream applications.

Abstract: We introduce WarpRF, a training-free general-purpose framework for
quantifying the uncertainty of radiance fields. Built upon the assumption that
photometric and geometric consistency should hold among images rendered by an
accurate model, WarpRF quantifies its underlying uncertainty from an unseen
point of view by leveraging backward warping across viewpoints, projecting
reliable renderings to the unseen viewpoint and measuring the consistency with
images rendered there. WarpRF is simple and inexpensive, does not require any
training, and can be applied to any radiance field implementation for free.
WarpRF excels at both uncertainty quantification and downstream tasks, e.g.,
active view selection and active mapping, outperforming any existing method
tailored to specific frameworks.

</details>


### [207] [MiCo: Multi-image Contrast for Reinforcement Visual Reasoning](https://arxiv.org/abs/2506.22434)
*Xi Chen,Mingkang Zhu,Shaoteng Liu,Xiaoyang Wu,Xiaogang Xu,Yu Liu,Xiang Bai,Hengshuang Zhao*

Main category: cs.CV

TL;DR: The study introduces a method allowing Vision-Language Models (VLMs) to perform Chain-of-Thought reasoning for multi-image analysis without using manual question-answer annotations.


<details>
  <summary>Details</summary>
Motivation: The research aims to address the challenge of enabling Vision-Language Models to reason about fine-grained visual details and complex logic across multiple images, without relying on curated question-answer pairs.

Method: The method constructs image triplets (two augmented views of the same image and a third similar yet distinct image) to train the model in reasoning. This training employs rule-based reinforcement learning to encourage the model to compare images and identify similarities or differences.

Result: The model trained on visual comparison tasks generalizes effectively to a variety of multi-image reasoning benchmarks, achieving significant improvements without using human-annotated data.

Conclusion: The study demonstrates that self-supervised visual comparison can enable logical reasoning in Vision-Language Models, achieving strong generalization and performance on vision-related reasoning tasks.

Abstract: This work explores enabling Chain-of-Thought (CoT) reasoning to link visual
cues across multiple images. A straightforward solution is to adapt rule-based
reinforcement learning for Vision-Language Models (VLMs). However, such methods
typically rely on manually curated question-answer pairs, which can be
particularly challenging when dealing with fine grained visual details and
complex logic across images. Inspired by self-supervised visual representation
learning, we observe that images contain inherent constraints that can serve as
supervision. Based on this insight, we construct image triplets comprising two
augmented views of the same image and a third, similar but distinct image.
During training, the model is prompted to generate a reasoning process to
compare these images (i.e., determine same or different). Then we optimize the
model with rule-based reinforcement learning. Due to the high visual similarity
and the presence of augmentations, the model must attend to subtle visual
changes and perform logical reasoning to succeed. Experiments show that,
although trained solely on visual comparison tasks, the learned reasoning
ability generalizes effectively to a wide range of questions. Without relying
on any human-annotated question-answer pairs, our method achieves significant
improvements on multi-image reasoning benchmarks and shows strong performance
on general vision tasks.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [208] [SiPipe: Bridging the CPU-GPU Utilization Gap for Efficient Pipeline-Parallel LLM Inference](https://arxiv.org/abs/2506.22033)
*Yongchao He,Bohan Zhao,Zheng Cao*

Main category: cs.DC

TL;DR: The paper introduces SiPipe, a novel pipeline approach leveraging CPU resources and several optimization techniques to drastically improve inference throughput, latency, and GPU utilization in large language models.


<details>
  <summary>Details</summary>
Motivation: To address inefficiencies in pipeline parallelism (PP) during multi-GPU setups for LLM inference caused by load-imbalance and execution bubbles, limiting throughput and GPU utilization.

Method: SiPipe employs a heterogeneous pipeline design with underutilized CPU resources and introduces CPU sampling, a token-safe execution model, and structure-aware transmission to mitigate execution inefficiencies in PP.

Result: SiPipe delivers up to 2.1x higher throughput, 43% reduced per-token latency, and up to 23% improved average GPU utilization compared to vLLM.

Conclusion: SiPipe offers significant performance improvements across diverse LLMs and demonstrates its adaptability in varied deployment scenarios, addressing limitations of current PP strategies.

Abstract: As inference workloads for large language models (LLMs) scale to meet growing
user demand, pipeline parallelism (PP) has become a widely adopted strategy for
multi-GPU deployment, particularly in cross-node setups, to improve key-value
(KV) cache capacity and inference throughput. However, PP suffers from inherent
inefficiencies caused by three types of execution bubbles-load-imbalance,
intra-stage, and inter-stage-which limit pipeline saturation. We present
SiPipe, a heterogeneous pipeline design that improves throughput by leveraging
underutilized CPU resources to offload auxiliary computation and communication.
SiPipe incorporates three key techniques-CPU sampling, a token-safe execution
model, and structure-aware transmission-to mitigate pipeline bubbles and
improve execution efficiency. Across diverse LLMs, SiPipe achieves up to 2.1
times higher throughput, 43% lower per-token latency, and up to 23% higher
average GPU utilization compared to the state-of-the-art vLLM under the same PP
configuration, demonstrating its generality across LLMs and deployment
scenarios.

</details>


### [209] [MCFuser: High-Performance and Rapid Fusion of Memory-Bound Compute-Intensive Operators](https://arxiv.org/abs/2506.22169)
*Zheng Zhang,Donglin Yang,Xiaobo Zhou,Dazhao Cheng*

Main category: cs.DC

TL;DR: MCFuser is a framework that generates high-performance fused kernels for memory-bound compute-intensive operator chains, addressing challenges like limited fusion strategies and sub-optimal performance.


<details>
  <summary>Details</summary>
Motivation: To mitigate the limitations of operator fusion in GPUs, particularly for compute-intensive operator chains that become memory-bound, highlighting inefficiencies like redundant memory access and slow tuning.

Method: MCFuser uses high-level tiling expressions for search space expansion, DAG analysis for memory access optimization, pruning strategies, and an analytical performance model with heuristic search for faster tuning.

Result: MCFuser achieves up to 5.9x speedup in kernel performance compared to leading compilers like Ansor, and reduces tuning time by over 70-fold.

Conclusion: MCFuser effectively streamlines kernel optimization for memory-bound tasks, enhances performance, and significantly accelerates tuning, making it superior to current methods.

Abstract: Operator fusion, a key technique to improve data locality and alleviate GPU
memory bandwidth pressure, often fails to extend to the fusion of multiple
compute-intensive operators due to saturated computation throughput. However,
the dynamicity of tensor dimension sizes could potentially lead to these
operators becoming memory-bound, necessitating the generation of fused kernels,
a task hindered by limited search spaces for fusion strategies, redundant
memory access, and prolonged tuning time, leading to sub-optimal performance
and inefficient deployment.
  We introduce MCFuser, a pioneering framework designed to overcome these
obstacles by generating high-performance fused kernels for what we define as
memory-bound compute-intensive (MBCI) operator chains. Leveraging high-level
tiling expressions to delineate a comprehensive search space, coupled with
Directed Acyclic Graph (DAG) analysis to eliminate redundant memory accesses,
MCFuser streamlines kernel optimization. By implementing guidelines to prune
the search space and incorporating an analytical performance model with a
heuristic search, MCFuser not only significantly accelerates the tuning process
but also demonstrates superior performance. Benchmarked against leading
compilers like Ansor on NVIDIA A100 and RTX3080 GPUs, MCFuser achieves up to a
5.9x speedup in kernel performance and outpaces other baselines while reducing
tuning time by over 70-fold, showcasing its agility.

</details>


### [210] [SPTCStencil: Unleashing Sparse Tensor Cores for Stencil Computation via Strided Swap](https://arxiv.org/abs/2506.22035)
*Qiqi GU,Chenpeng Wu,Heng Shi,Jianguo Yao*

Main category: cs.DC

TL;DR: SPTCStencil eliminates inefficiencies in stencil computation on tensor core accelerators by leveraging Sparse Tensor Cores for high performance, improving speed significantly compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Address the redundant zero-padding overhead in stencil computation transformations to matrix multiplications and leverage sparsity for efficiency.

Method: Develop a novel sparsification strategy to enable specialized Sparse Tensor Core compatibility and GPU kernel optimizations for stencil computation.

Result: SPTCStencil achieves a 5.46× improvement over traditional methods and a 2.00× improvement over Tensor Core-based approaches on average.

Conclusion: Sparse Tensor Cores can effectively accelerate field computations beyond deep learning, making them applicable to broader domains in science and engineering.

Abstract: Stencil computation, a pivotal numerical method in science and engineering,
iteratively updates grid points using weighted neighbor contributions and
exhibits strong parallelism for multi-core processors. Current optimization
techniques targeting conducting stencil computation on tensor core accelerators
incur substantial overheads due to redundant zero-padding during the
transformation to matrix multiplication. To address this, we introduce a sparse
computation paradigm that eliminates inefficiencies by exploiting specialized
hardware units.
  This paper exploits the sparsity in these matrices as a feature and presents
SPTCStencil, a high-performance stencil computation system accelerated by
Sparse Tensor Core (SpTCs). SPTCStencil is the first to harness SpTCs for
acceleration beyond deep learning domains. First, Our approach generalizes an
efficient transformation of stencil computation into matrix multiplications and
specializes this conversion for SpTC compatibility through a novel
sparsification strategy. Furthermore, SPTCStencil incorporates a
high-performance GPU kernel with systematic optimizations designed to maximize
efficiency on SpTCs. Experimental evaluations demonstrate that SPTCStencil
5.46$\times$ and Tensor Core-based approaches by 2.00$\times$ on average.

</details>


### [211] [Proof-of-Behavior: Behavior-Driven Consensus for Trustworthy Decentralized Finance](https://arxiv.org/abs/2506.22171)
*Ailiya Borjigin,Wei Zhou,Cong He*

Main category: cs.DC

TL;DR: The paper introduces Proof-of-Behavior (PoB), a consensus model that rewards verifiable trustworthy actions among blockchain validators to enhance security and fairness, especially in DeFi systems.


<details>
  <summary>Details</summary>
Motivation: Current blockchain protocols fail to assess validator trustworthiness, enabling subtle misconduct detrimental to decentralized finance.

Method: The authors propose PoB, which scores validator actions based on layered utility, adjusts validator weights with recent scores, and uses decentralized verification with proportional penalties.

Result: Simulation results show PoB reduces fraud acceptance by over 90%, penalizes malicious validators within two rounds, and ensures fairness with minimal throughput overhead.

Conclusion: PoB enhances trustworthiness and fairness in blockchain systems, offering a scalable and regulatory-compliant solution for financial applications.

Abstract: Current blockchain protocols (e.g., Proof-of-Work and Proof-of-Stake) secure
the ledger yet cannot measure validator trustworthiness, allowing subtle
misconduct that is especially damaging in decentralized-finance (DeFi)
settings. We introduce Proof-of-Behavior (PoB), a consensus model that (i)
gives each action a layered utility score -- covering motivation and outcome,
(ii) adapts validator weights using recent scores, and (iii) applies
decentralized verification with proportional slashing. The reward design is
incentive-compatible, yielding a Nash equilibrium in which honest behavior
maximizes long-run pay-offs. Simulated DeFi experiments (loan-fraud detection,
reputation-weighted validation) show that PoB cuts fraud acceptance by more
than 90%, demotes malicious validators within two rounds, and improves proposer
fairness versus standard PoS, all with no more than a 5% throughput overhead.
By linking consensus influence to verifiably trustworthy conduct, PoB offers a
scalable, regulation-friendly foundation for secure and fair blockchain
governance in financial applications.

</details>


### [212] [MPipeMoE: Memory Efficient MoE for Pre-trained Models with Adaptive Pipeline Parallelism](https://arxiv.org/abs/2506.22175)
*Zheng Zhang,Donglin Yang,Yaqi Xia,Liang Ding,Dacheng Tao,Xiaobo Zhou,Dazhao Cheng*

Main category: cs.DC

TL;DR: The paper introduces MPipeMoE, a library for improving Mixture-of-Experts (MoE) model training by enhancing pipeline parallelism and optimizing memory use, achieving up to 2.8x speedup and a 47% reduction in memory consumption.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the inefficiencies in communication and memory consumption inherent in training large MoE models, despite their potential for scaling neural networks via dynamic expert activation.

Method: The authors designed adaptive pipeline parallelism using an online algorithm to optimize pipelining granularity. They also developed memory reuse strategies to minimize redundancies and implemented an adaptive selection mechanism to balance hardware and model demands for runtime optimization.

Result: MPipeMoE demonstrated significant performance improvements, achieving up to 2.8x speedup in training and reducing the memory consumption by up to 47%, when tested on common MoE models on an 8 NVIDIA DGX A100 server cluster.

Conclusion: MPipeMoE effectively addresses key challenges in MoE model training, providing faster training speeds and reduced memory consumption. This library is a step forward in enabling efficient large-scale pre-trained model scaling.

Abstract: Recently, Mixture-of-Experts (MoE) has become one of the most popular
techniques to scale pre-trained models to extraordinarily large sizes. Dynamic
activation of experts allows for conditional computation, increasing the number
of parameters of neural networks, which is critical for absorbing the vast
amounts of knowledge available in many deep learning areas. However, despite
the existing system and algorithm optimizations, there are significant
challenges to be tackled when it comes to the inefficiencies of communication
and memory consumption.
  In this paper, we present the design and implementation of MPipeMoE, a
high-performance library that accelerates MoE training with adaptive and
memory-efficient pipeline parallelism. Inspired by that the MoE training
procedure can be divided into multiple independent sub-stages, we design
adaptive pipeline parallelism with an online algorithm to configure the
granularity of the pipelining. Further, we analyze the memory footprint
breakdown of MoE training and identify that activations and temporary buffers
are the primary contributors to the overall memory footprint. Toward memory
efficiency, we propose memory reusing strategies to reduce memory requirements
by eliminating memory redundancies, and develop an adaptive selection component
to determine the optimal strategy that considers both hardware capacities and
model characteristics at runtime. We implement MPipeMoE upon PyTorch and
evaluate it with common MoE models in a physical cluster consisting of 8 NVIDIA
DGX A100 servers. Compared with the state-of-art approach, MPipeMoE achieves up
to 2.8x speedup and reduces memory footprint by up to 47% in training large
models.

</details>


### [213] [Towards Operational Data Analytics Chatbots -- Virtual Knowledge Graph is All You Need](https://arxiv.org/abs/2506.22267)
*Junaid Ahmed Khan,Hiari Pizzini Cavagna,Andrea Proia,Andrea Bartolini*

Main category: cs.DC

TL;DR: The paper introduces an ODA chatbot system leveraging a Large Language Model to generate SPARQL queries and Virtual Knowledge Graphs for efficient NoSQL data retrieval in real-time.


<details>
  <summary>Details</summary>
Motivation: To address challenges in querying schema-less data from NoSQL databases in data centers and improve computational efficiency amidst the rise of generative AI.

Method: An end-to-end ODA chatbot system uses a Large Language Model for generating SPARQL queries, combined with Virtual Knowledge Graphs for lightweight and dynamic data querying.

Result: The system achieved 92.5% query accuracy versus 25% with direct NoSQL queries, reduced average query latency by 85%, and minimized VKG sizes to under 179 MiB.

Conclusion: The proposed approach advances data retrieval in ODA systems, both in accuracy and efficiency, making it suitable for deployment and real-time interaction in data center operations.

Abstract: With generative artificial intelligence challenging computational scientific
computing, data centers are experiencing unprecedented growth in both scale and
volume. As a result, computing efficiency has become more critical than ever.
Operational Data Analytics (ODA) relies on the collection of data center
telemetry to improve efficiency, but so far has been focusing on real-time
telemetry data visualization and post-mortem analysis. However, with NoSQL
databases now serving as the default storage backend to support scalability,
querying this data is challenging due to its schema-less nature, which requires
domain knowledge to traverse relationships between data sources. Ontologies and
Knowledge Graphs (KGs) can capture these relationships, but traditional KGs are
costly to scale and have not been widely applied to multivariate timeseries.
Virtual Knowledge Graphs (VKGs) offer a lightweight alternative by generating
query-specific graphs at runtime. In this work, we present a full end-to-end
ODA chatbot system that uses a Large Language Model (LLM) to generate SPARQL
queries, utilizing VKG for data retrieval. This approach achieves 92.5%
accuracy compared to 25% with direct NoSQL queries. The proposed methodology
optimizes VKG construction and LLM inference, cutting previous work average
query latency by 85% (from 20.36s to 3.03s) and keeping VKG sizes under 179
MiB. This performance makes the tool suitable for deployment and real-time
interaction with ODA end-users.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [214] [APO: Enhancing Reasoning Ability of MLLMs via Asymmetric Policy Optimization](https://arxiv.org/abs/2506.21655)
*Minjie Hong,Zirun Guo,Yan Xia,Zehan Wang,Ziang Zhang,Tao Jin,Zhou Zhao*

Main category: cs.LG

TL;DR: The study introduces Asymmetric Policy Optimization (APO) to enhance multimodal large language models' (MLLMs) reasoning without compromising general understanding. It dynamically adjusts KL divergence and penalizes overly long responses, improving performance and generalization.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal large language models (MLLMs) struggle with complex reasoning tasks and suffer from performance trade-offs when reinforcement learning (RL) is applied to improve reasoning capabilities.

Method: The paper proposes Asymmetric Policy Optimization (APO), consisting of two techniques: (1) Difficulty-Adaptive Divergence Shaping (DADS), which adjusts KL divergence dynamically for positive samples, and (2) Suboptimal Trajectory Complexity Regularization (STCR), which penalizes lengthy or overly detailed responses in negative samples.

Result: By applying APO to the Qwen2.5-VL-3B model, the improved View-R1-3B model achieves a 7% better reasoning performance on average, outperforming larger MLLMs (7-11B) across benchmarks, while retaining its abilities on general tasks.

Conclusion: The proposed APO method, with its DADS and STCR strategies, successfully addresses reasoning challenges in MLLMs, providing substantial improvements in performance and generalization without trade-offs. The broader applicability of this approach is emphasized.

Abstract: Multimodal Large Language Models (MLLMs) are powerful at integrating diverse
data, but they often struggle with complex reasoning. While Reinforcement
learning (RL) can boost reasoning in LLMs, applying it to MLLMs is tricky.
Common issues include a drop in performance on general tasks and the generation
of overly detailed or "overthinking" reasoning. Our work investigates how the
KL penalty and overthinking affect RL training in MLLMs. We propose Asymmetric
Policy Optimization (APO) to address these issues, which divides the sampled
responses into positive and negative groups. For positive samples,
Difficulty-Adaptive Divergence Shaping (DADS) is introduced to dynamically
adjust the KL divergence weight based on their difficulty. This method prevents
policy entropy from dropping sharply, improves training stability, utilizes
samples better, and preserves the model's existing knowledge. For negative
samples, Suboptimal Trajectory Complexity Regularization (STCR) is proposed to
penalize overly long responses. This helps mitigate overthinking and encourages
more concise reasoning while preserving the model's explorative capacity. We
apply our method to Qwen2.5-VL-3B, creating View-R1-3B. View-R1-3B
significantly enhances reasoning capabilities, showing an average 7\% gain over
the base model and outperforming larger MLLMs (7-11B) on various reasoning
benchmarks. Importantly, unlike other reasoning-tuned MLLMs that often degrade
on general tasks, View-R1-3B maintains consistent improvement, demonstrating
superior generalization. These results highlight the effectiveness and broad
applicability of our DADS and STCR techniques for advancing complex multimodal
reasoning in MLLMs. The code will be made available at
https://github.com/Indolent-Kawhi/View-R1.

</details>


### [215] [Risk-Averse Total-Reward Reinforcement Learning](https://arxiv.org/abs/2506.21683)
*Xihong Su,Jia Lin Hau,Gersi Doko,Kishan Panaganti,Marek Petrik*

Main category: cs.LG

TL;DR: This paper introduces a Q-learning algorithm to optimize risk-averse total-reward problems under ERM and EVaR, providing convergence and performance guarantees.


<details>
  <summary>Details</summary>
Motivation: Current model-based algorithms for risk-averse measures like ERM and EVaR work well for small problems but require full access to transition probabilities, which limits scalability or practical application.

Method: The authors design a Q-learning algorithm tailored to risk-averse objectives involving ERM and EVaR, leveraging properties like dynamic consistency and elicitability to ensure algorithmic correctness.

Result: The proposed algorithm exhibits fast and reliable convergence towards the optimal risk-averse value function in tabular settings, demonstrating its effectiveness numerically.

Conclusion: The Q-learning approach achieves strong theoretical guarantees and numerical reliability for risk-averse MDPs, extending the practicality of risk-aware decision-making.

Abstract: Risk-averse total-reward Markov Decision Processes (MDPs) offer a promising
framework for modeling and solving undiscounted infinite-horizon objectives.
Existing model-based algorithms for risk measures like the entropic risk
measure (ERM) and entropic value-at-risk (EVaR) are effective in small
problems, but require full access to transition probabilities. We propose a
Q-learning algorithm to compute the optimal stationary policy for total-reward
ERM and EVaR objectives with strong convergence and performance guarantees. The
algorithm and its optimality are made possible by ERM's dynamic consistency and
elicitability. Our numerical results on tabular domains demonstrate quick and
reliable convergence of the proposed Q-learning algorithm to the optimal
risk-averse value function.

</details>


### [216] [Unimodal Strategies in Density-Based Clustering](https://arxiv.org/abs/2506.21695)
*Oron Nir,Jay Tenenbaum,Ariel Shamir*

Main category: cs.LG

TL;DR: The paper investigates unimodal relations in density-based clustering to improve clustering parameter tuning, introducing efficient methods validated on large-scale, high-dimensional tasks.


<details>
  <summary>Details</summary>
Motivation: Density-based clustering methods are adept at handling noisy and arbitrarily distributed data, yet their computational intensity for parameter tuning in large-scale scenarios poses significant challenges.

Method: The authors identify a nearly unimodal relationship between cluster counts and neighborhood radius empirically and theoretically, utilizing this insight to design efficient radius selection strategies based on Ternary Search methods.

Result: Experiments on high-dimensional NLP, Audio, and Computer Vision tasks show that the proposed methodology improves computational efficiency and maintains robust clustering performance.

Conclusion: The study enhances density-based clustering methods by streamlining parameter tuning, solidifying theoretical understanding of clustering parameters, and providing practical tools and insights applicable in diverse large-scale contexts.

Abstract: Density-based clustering methods often surpass centroid-based counterparts,
when addressing data with noise or arbitrary data distributions common in
real-world problems. In this study, we reveal a key property intrinsic to
density-based clustering methods regarding the relation between the number of
clusters and the neighborhood radius of core points - we empirically show that
it is nearly unimodal, and support this claim theoretically in a specific
setting. We leverage this property to devise new strategies for finding
appropriate values for the radius more efficiently based on the Ternary Search
algorithm. This is especially important for large scale data that is
high-dimensional, where parameter tuning is computationally intensive. We
validate our methodology through extensive applications across a range of
high-dimensional, large-scale NLP, Audio, and Computer Vision tasks,
demonstrating its practical effectiveness and robustness. This work not only
offers a significant advancement in parameter control for density-based
clustering but also broadens the understanding regarding the relations between
their guiding parameters. Our code is available at
https://github.com/oronnir/UnimodalStrategies.

</details>


### [217] [$\textrm{ODE}_t \left(\textrm{ODE}_l \right)$: Shortcutting the Time and Length in Diffusion and Flow Models for Faster Sampling](https://arxiv.org/abs/2506.21714)
*Denis Gudovskiy,Wenzhao Zheng,Tomoyuki Okuno,Yohei Nakata,Kurt Keutzer*

Main category: cs.LG

TL;DR: This paper introduces a method to dynamically control the quality-complexity tradeoff in normalizing flows and diffusion models using transformer architecture, achieving faster and higher-quality sampling.


<details>
  <summary>Details</summary>
Motivation: Existing methods for normalizing flows and diffusion models require high computational complexity for data generation, primarily due to the reliance on multiple iterations to solve an ODE.

Method: The authors propose rewiring transformer-based architectures to handle inner discretized ODE in terms of length, combined with time- and length-wise consistency terms during training. This approach is solver-agnostic and reduces both latency and memory usage.

Result: The method demonstrated a latency reduction by up to 3x and improved FID scores by up to 3.5 points on CelebA-HQ and ImageNet datasets.

Conclusion: The approach effectively balances quality and efficiency during the sampling process, making significant advancements in image generation while reducing computational overhead.

Abstract: Recently, continuous normalizing flows (CNFs) and diffusion models (DMs) have
been studied using the unified theoretical framework. Although such models can
generate high-quality data points from a noise distribution, the sampling
demands multiple iterations to solve an ordinary differential equation (ODE)
with high computational complexity. Most existing methods focus on reducing the
number of time steps during the sampling process to improve efficiency. In this
work, we explore a complementary direction in which the quality-complexity
tradeoff can be dynamically controlled in terms of time steps and in the length
of the neural network. We achieve this by rewiring the blocks in the
transformer-based architecture to solve an inner discretized ODE w.r.t. its
length. Then, we employ time- and length-wise consistency terms during flow
matching training, and as a result, the sampling can be performed with an
arbitrary number of time steps and transformer blocks. Unlike others, our
$\textrm{ODE}_t \left(\textrm{ODE}_l \right)$ approach is solver-agnostic in
time dimension and decreases both latency and memory usage. Compared to the
previous state of the art, image generation experiments on CelebA-HQ and
ImageNet show a latency reduction of up to $3\times$ in the most efficient
sampling mode, and a FID score improvement of up to $3.5$ points for
high-quality sampling. We release our code and model weights with fully
reproducible experiments.

</details>


### [218] [Performance Prediction for Large Systems via Text-to-Text Regression](https://arxiv.org/abs/2506.21718)
*Yash Akhauri,Bryan Lewandowski,Cheng-Hsi Lin,Adrian N. Reyes,Grant C. Forbes,Arissa Wongpanich,Bangding Yang,Mohamed S. Abdelfattah,Sagi Perel,Xingyou Song*

Main category: cs.LG

TL;DR: The paper introduces text-to-text regression as an alternative to traditional tabular regression for predicting outcomes in complex systems data, achieving remarkable prediction performance on Google's Borg compute cluster.


<details>
  <summary>Details</summary>
Motivation: Traditional tabular regression methods struggle with complex systems data like configuration files or system logs. A scalable and effective alternative is needed.

Method: The authors propose using a 60M parameter encoder-decoder model, trained from scratch, to predict outcomes like resource efficiency through text-to-text regression.

Result: For Borg's compute cluster, the model achieved near-perfect rank correlation (0.9 average, up to 0.99) and significantly lower MSE compared to tabular approaches, showing adaptability to new tasks with minimal data.

Conclusion: Text-to-text regression demonstrates high scalability, adaptability, and accuracy, offering a promising approach for simulating real-world outcomes and making traditional regression obsolete in certain contexts.

Abstract: In many industries, predicting metric outcomes of large systems is a
fundamental problem, driven largely by traditional tabular regression. However,
such methods struggle on complex systems data in the wild such as configuration
files or system logs, where feature engineering is often infeasible. We propose
text-to-text regression as a general, scalable alternative. For predicting
resource efficiency on Borg, Google's massive compute cluster scheduling
system, a 60M parameter encoder-decoder, trained from random initialization,
achieves up to a near perfect 0.99 (0.9 average) rank correlation across the
entire fleet, and 100x lower MSE than tabular approaches. The model also easily
adapts to new tasks in only 500 few-shot examples and captures the densities of
complex outcome distributions. Ablation studies highlight the importance of
using encoders, increasing sequence length, and the model's inherent
uncertainty quantification. These findings pave the way for universal
simulators of real-world outcomes.

</details>


### [219] [Federated Item Response Theory Models](https://arxiv.org/abs/2506.21744)
*Biying Zhou,Nanyu Luo,Feng Ji*

Main category: cs.LG

TL;DR: The paper proposes "FedIRT," a privacy-preserving framework that uses federated learning to estimate Item Response Theory (IRT) models in distributed setups without sacrificing accuracy.


<details>
  <summary>Details</summary>
Motivation: To address privacy concerns in centralized IRT estimation by leveraging federated learning's distributed and privacy-preserving capabilities.

Method: FedIRT integrates federated learning with psychometrics to estimate IRT models. It employs numerical experiments for validation and offers an open-source implementation.

Result: FedIRT exhibits comparable statistical accuracy to traditional IRT methods while enhancing privacy and reducing communication costs. Validation is supported by real-world exam data.

Conclusion: FedIRT enables secure and accurate IRT estimation in distributed scenarios, extending IRT's applicability in multi-school assessments and other settings.

Abstract: Item Response Theory (IRT) models have been widely used to estimate
respondents' latent abilities and calibrate items' difficulty. Traditional IRT
estimation requires all individual raw response data to be centralized in one
place, thus potentially causing privacy issues. Federated learning is an
emerging field in computer science and machine learning with added features of
privacy protection and distributed computing. To integrate the advances from
federated learning with modern psychometrics, we propose a novel framework,
Federated Item Response Theory (IRT), to enable estimating traditional IRT
models with additional privacy, allowing estimation in a distributed manner
without losing estimation accuracy.
  Our numerical experiments confirm that FedIRT achieves statistical accuracy
similar to standard IRT estimation using popular R packages, while offering
critical advantages: privacy protection and reduced communication costs. We
also validate FedIRT's utility through a real-world exam dataset, demonstrating
its effectiveness in realistic educational contexts. This new framework extends
IRT's applicability to distributed settings, such as multi-school assessments,
without sacrificing accuracy or security. To support practical adoption, we
provide an open-ource R package, FedIRT, implementing the framework for the
two-parameter logistic (2PL) and partial credit models (PCM).

</details>


### [220] [Gradient-Based Neuroplastic Adaptation for Concurrent Optimization of Neuro-Fuzzy Networks](https://arxiv.org/abs/2506.21771)
*John Wesley Hostetter,Min Chi*

Main category: cs.LG

TL;DR: The paper proposes a new approach to design Neuro-Fuzzy Networks (NFNs) by simultaneously optimizing their parameters and structure, demonstrating effectiveness in vision-based tasks like playing DOOM.


<details>
  <summary>Details</summary>
Motivation: Designing NFNs is challenging due to the inefficient isolation of parameters and structure, leading to brittle architectures in existing methods.

Method: The authors introduce a gradient-based neuroplastic adaptation that optimizes NFNs' parameters and structure simultaneously, enabling applications like online reinforcement learning in vision-based tasks.

Result: The approach shows empirical success in enabling NFNs to play challenging scenarios in the DOOM video game through online reinforcement learning.

Conclusion: Simultaneous optimization of parameters and structure improves NFNs' performance, expanding their applicability to previously unattainable tasks, such as high-level vision-based learning scenarios.

Abstract: Neuro-fuzzy networks (NFNs) are transparent, symbolic, and universal function
approximations that perform as well as conventional neural architectures, but
their knowledge is expressed as linguistic IF-THEN rules. Despite these
advantages, their systematic design process remains a challenge. Existing work
will often sequentially build NFNs by inefficiently isolating parametric and
structural identification, leading to a premature commitment to brittle and
subpar architecture. We propose a novel application-independent approach called
gradient-based neuroplastic adaptation for the concurrent optimization of NFNs'
parameters and structure. By recognizing that NFNs' parameters and structure
should be optimized simultaneously as they are deeply conjoined, settings
previously unapproachable for NFNs are now accessible, such as the online
reinforcement learning of NFNs for vision-based tasks. The effectiveness of
concurrently optimizing NFNs is empirically shown as it is trained by online
reinforcement learning to proficiently play challenging scenarios from a
vision-based video game called DOOM.

</details>


### [221] [M3PO: Massively Multi-Task Model-Based Policy Optimization](https://arxiv.org/abs/2506.21782)
*Aditya Narendra,Dmitry Makarov,Aleksandr Panov*

Main category: cs.LG

TL;DR: M3PO is a new model-based reinforcement learning framework addressing inefficiencies in single-task and multi-task domains, outperforming current methods.


<details>
  <summary>Details</summary>
Motivation: To tackle sample inefficiency in single-task learning and poor generalization in multi-task settings in reinforcement learning.

Method: M3PO employs an implicit world model that predicts task outcomes, hybrid exploration mixing planning and uncertainty-driven bonuses, and stable updates via a trust-region optimizer.

Result: M3PO achieved state-of-the-art results on several benchmarks, outperforming existing model-based methods like DreamerV3 and model-free techniques like PPO.

Conclusion: M3PO demonstrates scalability and efficiency, providing a robust alternative to contemporary reinforcement learning methods.

Abstract: We introduce Massively Multi-Task Model-Based Policy Optimization (M3PO), a
scalable model-based reinforcement learning (MBRL) framework designed to
address sample inefficiency in single-task settings and poor generalization in
multi-task domains. Existing model-based approaches like DreamerV3 rely on
pixel-level generative models that neglect control-centric representations,
while model-free methods such as PPO suffer from high sample complexity and
weak exploration. M3PO integrates an implicit world model, trained to predict
task outcomes without observation reconstruction, with a hybrid exploration
strategy that combines model-based planning and model-free uncertainty-driven
bonuses. This eliminates the bias-variance trade-off in prior methods by using
discrepancies between model-based and model-free value estimates to guide
exploration, while maintaining stable policy updates through a trust-region
optimizer. M3PO provides an efficient and robust alternative to existing
model-based policy optimization approaches and achieves state-of-the-art
performance across multiple benchmarks.

</details>


### [222] [Multi-task parallelism for robust pre-training of graph foundation models on multi-source, multi-fidelity atomistic modeling data](https://arxiv.org/abs/2506.21788)
*Massimiliano Lupo Pasini,Jong Youl Choi,Pei Zhang,Kshitij Mehta,Rylie Weaver,Ashwin M. Aji,Karl W. Schulz,Jorda Polo,Prasanna Balaprakash*

Main category: cs.LG

TL;DR: This paper proposes a multi-task parallelism method using GPU acceleration for graph neural networks to enhance atomistic modeling across diverse datasets and supercomputing platforms.


<details>
  <summary>Details</summary>
Motivation: The motivation is to optimize atomistic modeling by improving the scalability and generalizability of graph foundation models using multi-source, multi-fidelity data during pre-training, addressing limitations of existing approaches.

Method: Multi-task learning combined with a multi-task parallelism method, implemented in the HydraGNN architecture, distributes decoding heads across GPU-accelerated computing resources.

Result: The method was successfully trained on 24 million structures from five datasets and demonstrated efficient scaling on heterogeneous supercomputing architectures including Perlmutter, Aurora, and Frontier.

Conclusion: The approach enhances scalability and resource allocation efficiency, enabling sustainable and robust atomistic modeling across diverse chemical datasets and advanced supercomputing systems.

Abstract: Graph foundation models using graph neural networks promise sustainable,
efficient atomistic modeling. To tackle challenges of processing multi-source,
multi-fidelity data during pre-training, recent studies employ multi-task
learning, in which shared message passing layers initially process input
atomistic structures regardless of source, then route them to multiple decoding
heads that predict data-specific outputs. This approach stabilizes pre-training
and enhances a model's transferability to unexplored chemical regions.
Preliminary results on approximately four million structures are encouraging,
yet questions remain about generalizability to larger, more diverse datasets
and scalability on supercomputers. We propose a multi-task parallelism method
that distributes each head across computing resources with GPU acceleration.
Implemented in the open-source HydraGNN architecture, our method was trained on
over 24 million structures from five datasets and tested on the Perlmutter,
Aurora, and Frontier supercomputers, demonstrating efficient scaling on all
three highly heterogeneous super-computing architectures.

</details>


### [223] [Why Neural Network Can Discover Symbolic Structures with Gradient-based Training: An Algebraic and Geometric Foundation for Neurosymbolic Reasoning](https://arxiv.org/abs/2506.21797)
*Peihao Wang,Zhangyang Wang*

Main category: cs.LG

TL;DR: The paper presents a theoretical framework explaining how neural networks trained on continuous data can naturally form discrete symbolic structures under specific constraints, transitioning from high-dimensional to algebraic, compositional representations.


<details>
  <summary>Details</summary>
Motivation: To understand and design neurosymbolic systems by revealing how discrete symbolic structures emerge from continuous neural network training processes.

Method: The authors model neural network training in measure space using Wasserstein gradient flow and analyze dynamics under geometric constraints, such as group invariance, to study the emergence of symbolic structures.

Result: The analysis shows that training dynamics lead to independent optimization trajectories and reduced degrees of freedom, enabling compositional, algebraic representations. Data scaling laws are also derived, linking task complexity to representational capacity and group invariance.

Conclusion: The work provides insights into how continuous and discrete methods can integrate in neurosymbolic systems, potentially enhancing their design and theoretical understanding.

Abstract: We develop a theoretical framework that explains how discrete symbolic
structures can emerge naturally from continuous neural network training
dynamics. By lifting neural parameters to a measure space and modeling training
as Wasserstein gradient flow, we show that under geometric constraints, such as
group invariance, the parameter measure $\mu_t$ undergoes two concurrent
phenomena: (1) a decoupling of the gradient flow into independent optimization
trajectories over some potential functions, and (2) a progressive contraction
on the degree of freedom. These potentials encode algebraic constraints
relevant to the task and act as ring homomorphisms under a commutative
semi-ring structure on the measure space. As training progresses, the network
transitions from a high-dimensional exploration to compositional
representations that comply with algebraic operations and exhibit a lower
degree of freedom. We further establish data scaling laws for realizing
symbolic tasks, linking representational capacity to the group invariance that
facilitates symbolic solutions. This framework charts a principled foundation
for understanding and designing neurosymbolic systems that integrate continuous
learning with discrete algebraic reasoning.

</details>


### [224] [The Cost of Avoiding Backpropagation](https://arxiv.org/abs/2506.21833)
*Kunjal Panchal,Sunav Choudhary,Yuriy Brun,Hui Guan*

Main category: cs.LG

TL;DR: This paper compares forward-mode automatic differentiation (FmAD), zero-order (ZO) optimization, and backpropagation (BP) with checkpointing, highlighting BP with checkpointing's superiority in accuracy, efficiency, and memory usage. It also provides theoretical and empirical insights into the limitations of FmAD and ZO.


<details>
  <summary>Details</summary>
Motivation: To understand the practical benefits and limitations of alternative gradient computation methods like FmAD and ZO compared to BP with activation checkpointing, especially under memory constraints.

Method: The authors conducted a unified theoretical analysis and empirical experiments comparing BP, FmAD, and ZO methods on large language and vision-language models.

Result: BP with checkpointing demonstrated superior accuracy (31.1% higher), faster convergence (34.8% faster), and fewer computations (3.8x reduction) compared to FmAD and ZO methods, even under similar memory usage.

Conclusion: Backpropagation with checkpointing is reaffirmed as the most effective memory-efficient strategy for training large models, showcasing significant limitations in FmAD and ZO methods.

Abstract: Forward-mode automatic differentiation (FmAD) and zero-order (ZO)
optimization have been proposed as memory-efficient alternatives to
backpropagation (BP) for gradient computation, especially in low-resource
settings. However, their practical benefits remain unclear due to two key gaps:
a lack of comparison against memory-efficient BP variants, such as activation
checkpointing, and a lack of a unified theoretical analysis. This work presents
a comprehensive theoretical and empirical comparison of BP, FmAD, and ZO
methods. Our theoretical analysis shows that while FmAD, and ZO can reduce
memory usage, they incur significant costs in accuracy, convergence speed, and
computation compared to BP with checkpointing. These drawbacks worsen with
larger models or constrained perturbation budgets. Empirical experiments on
large language and vision-language models show that BP with checkpointing
outperforms FmAD and ZO variants, including those enhanced with variance
reduction, achieving up to 31.1% higher accuracy, 34.8% faster convergence, and
3.8x fewer computations at comparable memory usage. Our results highlight
fundamental limitations of FmAD and ZO, and reaffirm BP with checkpointing as
the most effective strategy for model training under memory-constrained
settings. Our code is available at
https://github.com/Astuary/The_Cost_of_Avoiding_Backpropagation.

</details>


### [225] [Koopman operator-based discussion on partial observation in stochastic systems](https://arxiv.org/abs/2506.21844)
*Jun Ohkubo*

Main category: cs.LG

TL;DR: The paper discusses how partial observations affect the analysis of stochastic systems using Koopman operator theory, highlighting distinctions between state and function spaces and demonstrating power-law behaviors in noise-induced accuracy.


<details>
  <summary>Details</summary>
Motivation: To explore the impact of partial observations in analyzing stochastic systems and connect it with Koopman operator theory, extending prior work on deterministic systems.

Method: Analyzes partial observations in stochastic systems using Koopman operator theory, clarifying state-function space distinctions, and conducts numerical experiments using the delay embedding technique.

Result: Finds that the delay embedding technique helps mitigate partial observation effects, showing a power-law accuracy relationship with additive noise amplitude.

Conclusion: Understanding and distinguishing state and function spaces is crucial for handling partial observations in stochastic systems, with delay embedding proving useful.

Abstract: It is sometimes difficult to achieve a complete observation for a full set of
observables, and partial observations are necessary. For deterministic systems,
the Mori-Zwanzig formalism provides a theoretical framework for handling
partial observations. Recently, data-driven algorithms based on the Koopman
operator theory have made significant progress, and there is a discussion to
connect the Mori-Zwanzig formalism with the Koopman operator theory. In this
work, we discuss the effects of partial observation in stochastic systems using
the Koopman operator theory. The discussion clarifies the importance of
distinguishing the state space and the function space in stochastic systems.
Even in stochastic systems, the delay embedding technique is beneficial for
partial observation, and several numerical experiments showed a power-law
behavior of the accuracy for the amplitude of the additive noise. We also
discuss the relation between the exponent of the power-law behavior and the
effects of partial observation.

</details>


### [226] [A Survey of Continual Reinforcement Learning](https://arxiv.org/abs/2506.21872)
*Chaofan Pan,Xin Yang,Yanhua Li,Wei Wei,Tianrui Li,Bo An,Jiye Liang*

Main category: cs.LG

TL;DR: This paper surveys Continual Reinforcement Learning (CRL), categorizing methods, reviewing challenges, and proposing a new taxonomy to facilitate future research.


<details>
  <summary>Details</summary>
Motivation: Reinforcement Learning faces challenges in data efficiency, computational resources, and generalization across tasks. Continual Learning principles may address these by enabling agents to adapt and retain knowledge.

Method: The paper conducts a thorough review of CRL literature, examines tasks, benchmarks, metrics, and scenario settings, and introduces a new taxonomy for categorizing CRL methods based on knowledge storage and transfer.

Result: The survey organizes the current state of CRL research, provides a framework through a new taxonomy, and identifies specific challenges and insights for the field.

Conclusion: CRL holds promise in addressing RL's generalization and adaptability challenges. This work offers a foundation for future CRL research by clarifying concepts, frameworks, and challenges.

Abstract: Reinforcement Learning (RL) is an important machine learning paradigm for
solving sequential decision-making problems. Recent years have witnessed
remarkable progress in this field due to the rapid development of deep neural
networks. However, the success of RL currently relies on extensive training
data and computational resources. In addition, RL's limited ability to
generalize across tasks restricts its applicability in dynamic and real-world
environments. With the arisen of Continual Learning (CL), Continual
Reinforcement Learning (CRL) has emerged as a promising research direction to
address these limitations by enabling agents to learn continuously, adapt to
new tasks, and retain previously acquired knowledge. In this survey, we provide
a comprehensive examination of CRL, focusing on its core concepts, challenges,
and methodologies. Firstly, we conduct a detailed review of existing works,
organizing and analyzing their metrics, tasks, benchmarks, and scenario
settings. Secondly, we propose a new taxonomy of CRL methods, categorizing them
into four types from the perspective of knowledge storage and/or transfer.
Finally, our analysis highlights the unique challenges of CRL and provides
practical insights into future directions.

</details>


### [227] [Advancements and Challenges in Continual Reinforcement Learning: A Comprehensive Review](https://arxiv.org/abs/2506.21899)
*Amara Zuffer,Michael Burke,Mehrtash Harandi*

Main category: cs.LG

TL;DR: This survey examines continual reinforcement learning (CRL), discussing challenges, methodologies, and advancements with a particular focus on robotics.


<details>
  <summary>Details</summary>
Motivation: To address the need for RL agents to learn continuously in dynamic environments, enabling seamless acquisition and retention of reusable knowledge.

Method: Analyzes key concepts, challenges, methodologies, and provides evaluations of implementations in robotics-oriented CRL research.

Result: Highlights recent robotics-oriented CRL advancements and provides an overview for newcomers, contributing to accessibility in the field.

Conclusion: The paper identifies limitations in current CRL approaches and outlines promising directions for future advancements to further enrich RL capabilities.

Abstract: The diversity of tasks and dynamic nature of reinforcement learning (RL)
require RL agents to be able to learn sequentially and continuously, a learning
paradigm known as continuous reinforcement learning. This survey reviews how
continual learning transforms RL agents into dynamic continual learners. This
enables RL agents to acquire and retain useful and reusable knowledge
seamlessly. The paper delves into fundamental aspects of continual
reinforcement learning, exploring key concepts, significant challenges, and
novel methodologies. Special emphasis is placed on recent advancements in
continual reinforcement learning within robotics, along with a succinct
overview of evaluation environments utilized in prominent research,
facilitating accessibility for newcomers to the field. The review concludes
with a discussion on limitations and promising future directions, providing
valuable insights for researchers and practitioners alike.

</details>


### [228] [TOAST: Task-Oriented Adaptive Semantic Transmission over Dynamic Wireless Environments](https://arxiv.org/abs/2506.21900)
*Sheng Yun,Jianhua Pei,Ping Wang*

Main category: cs.LG

TL;DR: This paper introduces TOAST, a unified framework for balancing semantic-aware communication tasks effectively in dynamic wireless environments for 6G networks.


<details>
  <summary>Details</summary>
Motivation: With the shift to 6G networks, there is a need for task-relevant information transmission rather than traditional bit-centric approaches.

Method: TOAST uses Markov decision processes with reinforcement learning, integrates Low-Rank Adaptation mechanisms in Swin Transformers, and employs a diffusion model in latent spaces for robust communication.

Result: TOAST significantly improves classification accuracy and reconstruction quality even in challenging low SNR conditions, outperforming baseline methods.

Conclusion: TOAST offers a highly adaptive and efficient approach to multi-task optimization in semantic-aware communication, ideal for the evolving demands of 6G networks.

Abstract: The evolution toward 6G networks demands a fundamental shift from bit-centric
transmission to semantic-aware communication that emphasizes task-relevant
information. This work introduces TOAST (Task-Oriented Adaptive Semantic
Transmission), a unified framework designed to address the core challenge of
multi-task optimization in dynamic wireless environments through three
complementary components. First, we formulate adaptive task balancing as a
Markov decision process, employing deep reinforcement learning to dynamically
adjust the trade-off between image reconstruction fidelity and semantic
classification accuracy based on real-time channel conditions. Second, we
integrate module-specific Low-Rank Adaptation (LoRA) mechanisms throughout our
Swin Transformer-based joint source-channel coding architecture, enabling
parameter-efficient fine-tuning that dramatically reduces adaptation overhead
while maintaining full performance across diverse channel impairments including
Additive White Gaussian Noise (AWGN), fading, phase noise, and impulse
interference. Third, we incorporate an Elucidating diffusion model that
operates in the latent space to restore features corrupted by channel noises,
providing substantial quality improvements compared to baseline approaches.
Extensive experiments across multiple datasets demonstrate that TOAST achieves
superior performance compared to baseline approaches, with significant
improvements in both classification accuracy and reconstruction quality at low
Signal-to-Noise Ratio (SNR) conditions while maintaining robust performance
across all tested scenarios.

</details>


### [229] [Less Greedy Equivalence Search](https://arxiv.org/abs/2506.22331)
*Adiba Ejaz,Elias Bareinboim*

Main category: cs.LG

TL;DR: LGES is a modified variant of the GES algorithm for causal discovery that improves computational speed and accuracy, while retaining theoretical guarantees.


<details>
  <summary>Details</summary>
Motivation: To address the computational cost and finite-sample accuracy limitations associated with the original GES algorithm for causal discovery.

Method: LGES modifies the greedy step of GES by selectively avoiding edge insertions implied as conditionally independent by the score, thereby enabling more efficient and accurate search processes.

Result: LGES demonstrates up to 10-fold speed-ups, reduces structural errors significantly, accommodates prior assumptions while identifying contradictions, and leverages interventional data to improve causal discovery accuracy.

Conclusion: LGES outperforms GES and other baselines in terms of computational efficiency, accuracy, and robustness, while maintaining theoretical correctness for observational and interventional data even under misspecified assumptions.

Abstract: Greedy Equivalence Search (GES) is a classic score-based algorithm for causal
discovery from observational data. In the sample limit, it recovers the Markov
equivalence class of graphs that describe the data. Still, it faces two
challenges in practice: computational cost and finite-sample accuracy. In this
paper, we develop Less Greedy Equivalence Search (LGES), a variant of GES that
retains its theoretical guarantees while partially addressing these
limitations. LGES modifies the greedy step: rather than always applying the
highest-scoring insertion, it avoids edge insertions between variables for
which the score implies some conditional independence. This more targeted
search yields up to a \(10\)-fold speed-up and a substantial reduction in
structural error relative to GES. Moreover, LGES can guide the search using
prior assumptions, while correcting these assumptions when contradicted by the
data. Finally, LGES can exploit interventional data to refine the learned
observational equivalence class. We prove that LGES recovers the true
equivalence class in the sample limit from observational and interventional
data, even with misspecified prior assumptions. Experiments demonstrate that
LGES outperforms GES and other baselines in speed, accuracy, and robustness to
misspecified assumptions. Our code is available at
https://github.com/CausalAILab/lges.

</details>


### [230] [HQCM-EBTC: A Hybrid Quantum-Classical Model for Explainable Brain Tumor Classification](https://arxiv.org/abs/2506.21937)
*Marwan Ait Haddou,Mohamed Bennai*

Main category: cs.LG

TL;DR: Proposes HQCM-EBTC, a hybrid quantum-classical model, achieving 96.48% accuracy in MRI brain tumor classification, notably outperforming classical models.


<details>
  <summary>Details</summary>
Motivation: To improve the accuracy and interpretability of brain tumor classification in medical imaging using MRI.

Method: Uses a hybrid quantum-classical approach with a 5-qubit, depth-2 quantum layer alongside a composite loss optimization method.

Result: Achieves 96.48% accuracy, better precision, F1-scores for glioma detection, and enhanced feature separability.

Conclusion: Quantum-enhanced models like HQCM-EBTC show potential for advancing diagnostic accuracy and interpretability in brain tumor medical imaging assessments.

Abstract: We propose HQCM-EBTC, a hybrid quantum-classical model for automated brain
tumor classification using MRI images. Trained on a dataset of 7,576 scans
covering normal, meningioma, glioma, and pituitary classes, HQCM-EBTC
integrates a 5-qubit, depth-2 quantum layer with 5 parallel circuits, optimized
via AdamW and a composite loss blending cross-entropy and attention
consistency.
  HQCM-EBTC achieves 96.48% accuracy, substantially outperforming the classical
baseline (86.72%). It delivers higher precision and F1-scores, especially for
glioma detection. t-SNE projections reveal enhanced feature separability in
quantum space, and confusion matrices show lower misclassification. Attention
map analysis (Jaccard Index) confirms more accurate and focused tumor
localization at high-confidence thresholds.
  These results highlight the promise of quantum-enhanced models in medical
imaging, advancing both diagnostic accuracy and interpretability for clinical
brain tumor assessment.

</details>


### [231] [GuiderNet: A Meta-Learning Framework for Optimizing Quantum Circuit Geometry and Mitigating Barren Plateaus](https://arxiv.org/abs/2506.21940)
*Marwan Ait Haddou,Mohamed Bennai*

Main category: cs.LG

TL;DR: GuiderNet improves training and generalization in variational quantum algorithms, addressing barren plateaus and optimization issues.


<details>
  <summary>Details</summary>
Motivation: To overcome optimization challenges like barren plateaus and poor-conditioned landscapes in Variational Quantum Algorithms (VQAs) that hinder their potential.

Method: The paper introduces GuiderNet, a meta-learning framework using data-dependent parameter shifts to optimize Parameterized Quantum Circuits (PQC) geometry, reducing the Fubini-Study metric tensor condition number through a neural network-guided approach.

Result: GuiderNet reduces cumulative training loss by over 5x, enhances accuracy (75.3% to 98.6%), and significantly boosts minority-class F1 score (0.67 to 0.95) in a Kaggle Diabetes classification task.

Conclusion: Geometric meta-conditioning via GuiderNet can tackle barren plateaus, stabilize training, and improve both trainability and generalization in quantum machine learning models.

Abstract: Variational Quantum Algorithms (VQAs) offer potential for near-term quantum
advantage but face challenges from barren plateaus, where gradients vanish, and
poorly conditioned optimization landscapes. We introduce GuiderNet, a
meta-learning framework that conditions Parameterized Quantum Circuits (PQCs)
using data-dependent parameter shifts aimed at minimizing the log condition
number of the Fubini-Study metric tensor. Implemented as a classical neural
network, GuiderNet is meta-trained to guide PQC parameters into geometrically
favorable regions and is embedded within hybrid quantum-classical pipelines to
steer both initialization and adaptive modulation during training.
  Applied to the Kaggle Diabetes classification task, GuiderNet reduces
cumulative training loss by over 5x, improves test accuracy from 75.3% to
98.6%, and increases the minority-class F1 score from 0.67 to 0.95. It also
suppresses gradient explosion and stabilizes parameter updates, enabling
smoother and more robust optimization. These results demonstrate that geometric
meta-conditioning can mitigate barren plateaus and ill-conditioning, providing
a scalable approach to enhance trainability and generalization in quantum
machine learning.

</details>


### [232] [Physics-informed network paradigm with data generation and background noise removal for diverse distributed acoustic sensing applications](https://arxiv.org/abs/2506.21952)
*Yangyang Wan,Haotian Wang,Xuhui Yu,Jiageng Chen,Xinyu Fan,Zuyuan He*

Main category: cs.LG

TL;DR: A physics-informed neural network paradigm is proposed for Distributed Acoustic Sensing (DAS) that eliminates the need for real-world event data during training by generating synthetic DAS data using physical modeling.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of limited real-world data availability for training AI models in Distributed Acoustic Sensing applications.

Method: The authors propose a physics-informed paradigm where physical modeling of events and DAS system constraints generate synthetic DAS data, which is then used to train networks for noise reduction and event recognition.

Result: The paradigm achieved comparable or better performance than traditional data-driven networks in applications such as event identification and fault monitoring, achieving 91.8% fault diagnosis accuracy in a field test without requiring real-world data for training.

Conclusion: The proposed physics-informed paradigm mitigates the need for real-world training data, enhances performance in noisy environments, and demonstrates transferability across sites, offering significant potential for broader DAS applications.

Abstract: Distributed acoustic sensing (DAS) has attracted considerable attention
across various fields and artificial intelligence (AI) technology plays an
important role in DAS applications to realize event recognition and denoising.
Existing AI models require real-world data (RWD), whether labeled or not, for
training, which is contradictory to the fact of limited available event data in
real-world scenarios. Here, a physics-informed DAS neural network paradigm is
proposed, which does not need real-world events data for training. By
physically modeling target events and the constraints of real world and DAS
system, physical functions are derived to train a generative network for
generation of DAS events data. DAS debackground net is trained by using the
generated DAS events data to eliminate background noise in DAS data. The
effectiveness of the proposed paradigm is verified in event identification
application based on a public dataset of DAS spatiotemporal data and in belt
conveyor fault monitoring application based on DAS time-frequency data, and
achieved comparable or better performance than data-driven networks trained
with RWD. Owing to the introduction of physical information and capability of
background noise removal, the paradigm demonstrates generalization in same
application on different sites. A fault diagnosis accuracy of 91.8% is achieved
in belt conveyor field with networks which transferred from simulation test
site without any fault events data of test site and field for training. The
proposed paradigm is a prospective solution to address significant obstacles of
data acquisition and intense noise in practical DAS applications and explore
more potential fields for DAS.

</details>


### [233] [Optimal Return-to-Go Guided Decision Transformer for Auto-Bidding in Advertisement](https://arxiv.org/abs/2506.21956)
*Hao Jiang,Yongxiang Tang,Yanxiang Zeng,Pengjia Yuan,Yanhua Cheng,Teng Sha,Xialong Liu,Peng Jiang*

Main category: cs.LG

TL;DR: This paper introduces the R* Decision Transformer (R* DT) for improving automated bidding in online ad auctions, emphasizing overcoming the limitations of conventional Decision Transformers (DT) and addressing mixed-quality training data.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to improve automated bidding systems in online ad auctions by addressing challenges with conventional Decision Transformers, such as dependence on preset return-to-go values and limitations caused by mixed-quality training data.

Method: The authors proposed a three-step enhancement to the Decision Transformer: 1) R DT memorizes RTG values from the training set, 2) R^ DT forecasts optimal RTG for better policies, and 3) R* DT enriches the training data by generating and selecting trajectories with high rewards.

Result: Experiments on a publicly available bidding dataset demonstrate the R* DT's improved performance, particularly in handling mixed-quality trajectories and achieving better automated bidding outcomes.

Conclusion: The R* DT effectively addresses the shortcomings of conventional DT by optimizing policy learning through data enhancement, showcasing its capability as a more efficient solution for automated bidding challenges in online advertising.

Abstract: In the realm of online advertising, advertisers partake in ad auctions to
obtain advertising slots, frequently taking advantage of auto-bidding tools
provided by demand-side platforms. To improve the automation of these bidding
systems, we adopt generative models, namely the Decision Transformer (DT), to
tackle the difficulties inherent in automated bidding. Applying the Decision
Transformer to the auto-bidding task enables a unified approach to sequential
modeling, which efficiently overcomes short-sightedness by capturing long-term
dependencies between past bidding actions and user behavior. Nevertheless,
conventional DT has certain drawbacks: (1) DT necessitates a preset
return-to-go (RTG) value before generating actions, which is not inherently
produced; (2) The policy learned by DT is restricted by its training data,
which is consists of mixed-quality trajectories. To address these challenges,
we introduce the R* Decision Transformer (R* DT), developed in a three-step
process: (1) R DT: Similar to traditional DT, R DT stores actions based on
state and RTG value, as well as memorizing the RTG for a given state using the
training set; (2) R^ DT: We forecast the highest value (within the training
set) of RTG for a given state, deriving a suboptimal policy based on the
current state and the forecasted supreme RTG value; (3) R* DT: Based on R^ DT,
we generate trajectories and select those with high rewards (using a simulator)
to augment our training dataset. This data enhancement has been shown to
improve the RTG of trajectories in the training data and gradually leads the
suboptimal policy towards optimality. Comprehensive tests on a publicly
available bidding dataset validate the R* DT's efficacy and highlight its
superiority when dealing with mixed-quality trajectories.

</details>


### [234] [SceneDiffuser++: City-Scale Traffic Simulation via a Generative World Model](https://arxiv.org/abs/2506.21976)
*Shuhan Tan,John Lambert,Hong Jeon,Sakshum Kulshrestha,Yijing Bai,Jing Luo,Dragomir Anguelov,Mingxing Tan,Chiyu Max Jiang*

Main category: cs.LG

TL;DR: The paper introduces SceneDiffuser++, an advanced generative simulation model for autonomous vehicle testing, which enables city-scale trip simulations by combining various simulation technologies.


<details>
  <summary>Details</summary>
Motivation: To address the limited availability of manually-driven miles for testing autonomous vehicles and create realistic, scalable simulated traffic environments.

Method: The authors developed SceneDiffuser++, an end-to-end generative world model trained on a single loss function, integrating scene generation, agent behavior modeling, dynamic scene generation, and environment simulation.

Result: SceneDiffuser++ demonstrates realistic city-scale traffic simulations from point A to B, with superior performance under long simulation conditions. It is evaluated using an extended version of the Waymo Open Motion Dataset.

Conclusion: SceneDiffuser++ successfully integrates multiple simulation technologies for city-scale autonomous vehicle testing, offering improved realism and scalability compared to existing methods.

Abstract: The goal of traffic simulation is to augment a potentially limited amount of
manually-driven miles that is available for testing and validation, with a much
larger amount of simulated synthetic miles. The culmination of this vision
would be a generative simulated city, where given a map of the city and an
autonomous vehicle (AV) software stack, the simulator can seamlessly simulate
the trip from point A to point B by populating the city around the AV and
controlling all aspects of the scene, from animating the dynamic agents (e.g.,
vehicles, pedestrians) to controlling the traffic light states. We refer to
this vision as CitySim, which requires an agglomeration of simulation
technologies: scene generation to populate the initial scene, agent behavior
modeling to animate the scene, occlusion reasoning, dynamic scene generation to
seamlessly spawn and remove agents, and environment simulation for factors such
as traffic lights. While some key technologies have been separately studied in
various works, others such as dynamic scene generation and environment
simulation have received less attention in the research community. We propose
SceneDiffuser++, the first end-to-end generative world model trained on a
single loss function capable of point A-to-B simulation on a city scale
integrating all the requirements above. We demonstrate the city-scale traffic
simulation capability of SceneDiffuser++ and study its superior realism under
long simulation conditions. We evaluate the simulation quality on an augmented
version of the Waymo Open Motion Dataset (WOMD) with larger map regions to
support trip-level simulation.

</details>


### [235] [Binned semiparametric Bayesian networks](https://arxiv.org/abs/2506.21997)
*Rafael Sojo,Javier Díaz-Rozo,Concha Bielza,Pedro Larrañaga*

Main category: cs.LG

TL;DR: This paper presents a probabilistic semiparametric model leveraging data binning to improve computation in kernel density estimation, offering two efficient methods – sparse binned and Fourier kernel density estimation.


<details>
  <summary>Details</summary>
Motivation: To address computational inefficiencies and the curse of dimensionality in kernel density estimation for nonparametric distributions, especially in Bayesian networks.

Method: Two new conditional probability distributions, sparse binned kernel density estimation and Fourier kernel density estimation, are proposed. Sparse tensors and restricted parent nodes are employed to mitigate dimensionality issues. Experiments and complexity analysis are conducted with real and synthetic datasets.

Result: The proposed models demonstrated similar statistical performance (log-likelihood and structural learning) compared to standard semiparametric Bayesian networks, but with significantly faster computation.

Conclusion: Binned semiparametric Bayesian networks offer a computationally efficient and reliable alternative to traditional semiparametric Bayesian approaches, without compromising statistical effectiveness.

Abstract: This paper introduces a new type of probabilistic semiparametric model that
takes advantage of data binning to reduce the computational cost of kernel
density estimation in nonparametric distributions. Two new conditional
probability distributions are developed for the new binned semiparametric
Bayesian networks, the sparse binned kernel density estimation and the Fourier
kernel density estimation. These two probability distributions address the
curse of dimensionality, which typically impacts binned models, by using sparse
tensors and restricting the number of parent nodes in conditional probability
calculations. To evaluate the proposal, we perform a complexity analysis and
conduct several comparative experiments using synthetic data and datasets from
the UCI Machine Learning repository. The experiments include different binning
rules, parent restrictions, grid sizes, and number of instances to get a
holistic view of the model's behavior. As a result, our binned semiparametric
Bayesian networks achieve structural learning and log-likelihood estimations
with no statistically significant differences compared to the semiparametric
Bayesian networks, but at a much higher speed. Thus, the new binned
semiparametric Bayesian networks prove to be a reliable and more efficient
alternative to their non-binned counterparts.

</details>


### [236] [On the Necessity of Output Distribution Reweighting for Effective Class Unlearning](https://arxiv.org/abs/2506.20893)
*Yian Wang,Ali Ebrahimpour-Boroojeny,Hari Sundaram*

Main category: cs.LG

TL;DR: This paper introduces RWFT, a lightweight method to remove specific classes from trained classifiers without full retraining, ensuring robust forgetting against attacks.


<details>
  <summary>Details</summary>
Motivation: Forgetting specific classes in machine learning models is necessary to support user deletion rights and mitigate harmful or biased predictions.

Method: RWFT redistributes the probability mass on forgotten class samples to prevent residual leakage and introduces a total variation (TV)-based metric to evaluate forgetting performance.

Result: RWFT matches full retraining results and surpasses existing unlearning methods by 2.79% in existing metrics and 111.45% in the proposed TV-based metric.

Conclusion: RWFT is an effective and robust technique for unlearning specific classes in machine learning models, outperforming state-of-the-art methods in multiple metrics.

Abstract: In this work, we introduce an output-reweighting unlearning method, RWFT, a
lightweight technique that erases an entire class from a trained classifier
without full retraining. Forgetting specific classes from trained models is
essential for enforcing user deletion rights and mitigating harmful or biased
predictions. The full retraining is costly and existing unlearning methods fail
to replicate the behavior of the retrained models when predicting samples from
the unlearned class. We prove this failure by designing a variant of membership
inference attacks, MIA-NN that successfully reveals the unlearned class for any
of these methods. We propose a simple redistribution of the probability mass
for the prediction on the samples in the forgotten class which is robust to
MIA-NN. We also introduce a new metric based on the total variation (TV)
distance of the prediction probabilities to quantify residual leakage to
prevent future methods from susceptibility to the new attack. Through extensive
experiments with state of the art baselines in machine unlearning, we show that
our approach matches the results of full retraining in both metrics used for
evaluation by prior work and the new metric we propose in this work. Compare to
state-of-the-art methods, we gain 2.79% in previously used metrics and 111.45%
in our new TV-based metric over the best existing method.

</details>


### [237] [GKNet: Graph Kalman Filtering and Model Inference via Model-based Deep Learning](https://arxiv.org/abs/2506.22004)
*Mohammad Sabbaqi,Riccardo Taormina,Elvin Isufi*

Main category: cs.LG

TL;DR: The paper introduces a graph-aware state space model for analyzing graph time series using parametric graph-induced models for latent states and observations. It incorporates stochastic partial differential equations and deep learning principles for improved inference and prediction.


<details>
  <summary>Details</summary>
Motivation: Graph time series inference over applications such as water networks, economics, and neuroscience requires computationally efficient models that capture both the graph and temporal dynamics.

Method: The method combines stochastic partial differential equations for the latent state and graph-filtered observation models. It employs maximum likelihood approaches and deep learning architectures, inspired by Kalman neural networks, for end-to-end learning.

Result: The approach successfully integrates graph structure in state and observation modeling, enabling tasks like prediction and imputation, and improving scalability and expressivity.

Conclusion: The paper concludes that graph-aware state space modeling enhances the capacity to analyze graph time series data effectively while offering scalability and adaptability for downstream tasks.

Abstract: Inference tasks with time series over graphs are of importance in
applications such as urban water networks, economics, and networked
neuroscience. Addressing these tasks typically relies on identifying a
computationally affordable model that jointly captures the graph-temporal
patterns of the data. In this work, we propose a graph-aware state space model
for graph time series, where both the latent state and the observation equation
are parametric graph-induced models with a limited number of parameters that
need to be learned. More specifically, we consider the state equation to follow
a stochastic partial differential equation driven by noise over the graphs
edges accounting not only for potential edge uncertainties but also for
increasing the degrees of freedom in the latter in a tractable manner. The
graph structure conditioning of the noise dispersion allows the state variable
to deviate from the stochastic process in certain neighborhoods. The
observation model is a sampled and graph-filtered version of the state
capturing multi-hop neighboring influence. The goal is to learn the parameters
in both state and observation models from the partially observed data for
downstream tasks such as prediction and imputation. The model is inferred first
through a maximum likelihood approach that provides theoretical tractability
but is limited in expressivity and scalability. To improve on the latter, we
use the state-space formulation to build a principled deep learning
architecture that jointly learns the parameters and tracks the state in an
end-to-end manner in the spirit of Kalman neural networks.

</details>


### [238] [TROFI: Trajectory-Ranked Offline Inverse Reinforcement Learning](https://arxiv.org/abs/2506.22008)
*Alessandro Sestini,Joakim Bergdahl,Konrad Tollmar,Andrew D. Bagdanov,Linus Gisslén*

Main category: cs.LG

TL;DR: The paper introduces TROFI, a method for offline reinforcement learning without requiring predefined rewards, using a reward function learned from human preferences.


<details>
  <summary>Details</summary>
Motivation: Offline reinforcement learning aims to train agents using stored transitions, but many applied settings lack predefined reward functions, preventing effective training.

Method: TROFI first learns a reward function based on human preferences and then labels the dataset for policy training, avoiding the need for optimal trajectories.

Result: Experiments show TROFI outperforms baselines and performs on par with methods using ground truth rewards in benchmarks and a 3D game environment.

Conclusion: TROFI proves effective in addressing settings without predefined reward functions, emphasizing the importance of a well-designed and learnable reward function for better policy performance.

Abstract: In offline reinforcement learning, agents are trained using only a fixed set
of stored transitions derived from a source policy. However, this requires that
the dataset be labeled by a reward function. In applied settings such as video
game development, the availability of the reward function is not always
guaranteed. This paper proposes Trajectory-Ranked OFfline Inverse reinforcement
learning (TROFI), a novel approach to effectively learn a policy offline
without a pre-defined reward function. TROFI first learns a reward function
from human preferences, which it then uses to label the original dataset making
it usable for training the policy. In contrast to other approaches, our method
does not require optimal trajectories. Through experiments on the D4RL
benchmark we demonstrate that TROFI consistently outperforms baselines and
performs comparably to using the ground truth reward to learn policies.
Additionally, we validate the efficacy of our method in a 3D game environment.
Our studies of the reward model highlight the importance of the reward function
in this setting: we show that to ensure the alignment of a value function to
the actual future discounted reward, it is fundamental to have a
well-engineered and easy-to-learn reward function.

</details>


### [239] [Hyper-modal Imputation Diffusion Embedding with Dual-Distillation for Federated Multimodal Knowledge Graph Completion](https://arxiv.org/abs/2506.22036)
*Ying Zhang,Yu Zhao,Xuhui Sui,Baohang Zhou,Xiangrui Cai,Li Shen,Xiaojie Yuan,Dacheng Tao*

Main category: cs.LG

TL;DR: The paper introduces the task of Federated Multimodal Knowledge Graph Completion (FedMKGC) to address privacy and collaboration challenges in decentralized multimodal knowledge graphs. A novel framework, MMFeD3-HidE, is proposed for improving reasoning ability and collaboration efficiency.


<details>
  <summary>Details</summary>
Motivation: The need arises from the decentralization and privatization of multimodal knowledge graphs, which restrict efficient collaboration systems that can balance reasoning ability with the security of sensitive knowledge.

Method: The framework MMFeD3-HidE tackles multimodal data imputation within clients using the Hyper-modal Imputation Diffusion Embedding (HidE) model and enables mutual knowledge transfer between clients and the server utilizing Multimodal FeDerated Dual Distillation (MMFeD3).

Result: Experiments on the proposed FedMKGC benchmark demonstrate effectiveness, semantic consistency, and robustness of the MMFeD3-HidE framework.

Conclusion: MMFeD3-HidE offers a promising solution to addressing challenges of multimodal uncertainty and heterogeneity in decentralized multimodal knowledge graphs, advancing federated graph completion tasks.

Abstract: With the increasing multimodal knowledge privatization requirements,
multimodal knowledge graphs in different institutes are usually decentralized,
lacking of effective collaboration system with both stronger reasoning ability
and transmission safety guarantees. In this paper, we propose the Federated
Multimodal Knowledge Graph Completion (FedMKGC) task, aiming at training over
federated MKGs for better predicting the missing links in clients without
sharing sensitive knowledge. We propose a framework named MMFeD3-HidE for
addressing multimodal uncertain unavailability and multimodal client
heterogeneity challenges of FedMKGC. (1) Inside the clients, our proposed
Hyper-modal Imputation Diffusion Embedding model (HidE) recovers the complete
multimodal distributions from incomplete entity embeddings constrained by
available modalities. (2) Among clients, our proposed Multimodal FeDerated Dual
Distillation (MMFeD3) transfers knowledge mutually between clients and the
server with logit and feature distillation to improve both global convergence
and semantic consistency. We propose a FedMKGC benchmark for a comprehensive
evaluation, consisting of a general FedMKGC backbone named MMFedE, datasets
with heterogeneous multimodal information, and three groups of constructed
baselines. Experiments conducted on our benchmark validate the effectiveness,
semantic consistency, and convergence robustness of MMFeD3-HidE.

</details>


### [240] [UniCA: Adapting Time Series Foundation Model to General Covariate-Aware Forecasting](https://arxiv.org/abs/2506.22039)
*Lu Han,Yu Liu,Qiwen Deng,Jian Jiang,Yinbo Sun,Zhe Yu,Binfeng Wang,Xingyu Lu,Lintao Ma,Han-Jia Ye,De-Chuan Zhan*

Main category: cs.LG

TL;DR: The paper introduces a framework, UniCA, which enhances Time Series Foundation Models to handle heterogeneous covariates for general forecasting tasks.


<details>
  <summary>Details</summary>
Motivation: Time Series Foundation Models struggle with diverse and heterogeneous covariates (e.g., text, images, categorical data), limiting their utility in general forecasting scenarios.

Method: UniCA transforms heterogeneous covariates into homogeneous series representations and integrates them using a unified attention-based fusion mechanism.

Result: UniCA demonstrates superior performance across various unimodal and multimodal forecasting benchmarks, showcasing significant improvements over existing models.

Conclusion: UniCA enables TSFMs to effectively utilize covariate information while maintaining generalization capabilities, paving the way for broader application in heterogeneous forecasting tasks.

Abstract: Time Series Foundation Models (TSFMs) have achieved remarkable success
through large-scale pretraining. However, their design primarily targets
real-valued series, limiting their ability to handle general forecasting tasks
involving diverse and often heterogeneous covariates--such as categorical
variables and multimodal data (e.g., images, text)--which are typically
task-specific and difficult to leverage during pretraining. To address this
gap, we propose Unified Covariate Adaptation (UniCA), a framework to bridge
TSFMs with general covariate-aware forecasting. UniCA first performs covariate
homogenization to transform heterogeneous covariates into high-level
homogeneous series representations and then fuses them via a unified
attention-based fusion mechanism. UniCA is compatible and universal for
adaptation with both homogeneous and heterogeneous covariates, incorporating
extra covariate information while preserving the generalization ability of
TSFMs.Extensive experiments on multiple unimodal and multimodal covariate-aware
forecasting benchmarks demonstrate the superiority of UniCA, highlighting the
promise of covariate-aware TSFM adaptation in real-world forecasting scenarios.
Codes are released on https://github.com/hanlu-nju/UniCA.

</details>


### [241] [Reinforcement Learning with Physics-Informed Symbolic Program Priors for Zero-Shot Wireless Indoor Navigation](https://arxiv.org/abs/2506.22365)
*Tao Li,Haozhe Lei,Mingsheng Yin,Yaqi Hu*

Main category: cs.LG

TL;DR: This paper introduces the PiPRL framework to improve reinforcement learning by incorporating human-readable physics priors via symbolic programming, achieving better performance and faster learning in indoor navigation.


<details>
  <summary>Details</summary>
Motivation: To address challenges in physical control tasks using RL, where incorporating physics priors improves efficiency but requires significant manual effort and expertise.

Method: The proposed PiPRL framework integrates symbolic programming with neural networks. It uses a hierarchical approach: a neural perception module extracts features for symbolic programming, which encodes physics priors and guides policy learning.

Result: PiPRL demonstrated superior performance compared to purely symbolic or neural approaches, reducing training time by over 26% in indoor navigation tasks.

Conclusion: The PiPRL framework effectively combines symbolic and neural methodologies, showcasing improved sample efficiency, generalization, and reduced reliance on manual labor and expertise in RL applications.

Abstract: When using reinforcement learning (RL) to tackle physical control tasks,
inductive biases that encode physics priors can help improve sample efficiency
during training and enhance generalization in testing. However, the current
practice of incorporating these helpful physics-informed inductive biases
inevitably runs into significant manual labor and domain expertise, making them
prohibitive for general users. This work explores a symbolic approach to
distill physics-informed inductive biases into RL agents, where the physics
priors are expressed in a domain-specific language (DSL) that is human-readable
and naturally explainable. Yet, the DSL priors do not translate directly into
an implementable policy due to partial and noisy observations and additional
physical constraints in navigation tasks. To address this gap, we develop a
physics-informed program-guided RL (PiPRL) framework with applications to
indoor navigation. PiPRL adopts a hierarchical and modularized neuro-symbolic
integration, where a meta symbolic program receives semantically meaningful
features from a neural perception module, which form the bases for symbolic
programming that encodes physics priors and guides the RL process of a
low-level neural controller. Extensive experiments demonstrate that PiPRL
consistently outperforms purely symbolic or neural policies and reduces
training time by over 26% with the help of the program-based inductive biases.

</details>


### [242] [GPAS: Accelerating Convergence of LLM Pretraining via Gradient-Preserving Activation Scaling](https://arxiv.org/abs/2506.22049)
*Tianhao Chen,Xin Xu,Zijing Liu,Pengxiang Li,Xinyuan Song,Ajay Kumar Jaiswal,Fan Zhang,Jishan Hu,Yang Wang,Hao Chen,Shizhe Diao,Shiwei Liu,Yu Li,Yin Lu,Can Yang*

Main category: cs.LG

TL;DR: The paper introduces GPAS, a technique that mitigates activation variance issues in Pre-LayerNorm Transformers, enhancing training dynamics and model performance.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of Pre-LN Transformers, where excessive activation variance restricts deeper layers' learning capacity.

Method: Introduced Gradient-Preserving Activation Scaling (GPAS), which scales down intermediate activations without altering their gradients.

Result: Experiments with model sizes ranging from 71M to 1B show consistent performance improvement using GPAS and its effectiveness across other architectures like Sandwich-LN and DeepNorm.

Conclusion: GPAS enhances training dynamics and model efficiency in diverse architectures, making it a promising direction for improving large-scale models.

Abstract: Modern Large Language Models, such as the LLaMA, Qwen and DeepSeek series,
predominantly adopt the Pre-LayerNorm (Pre-LN) Transformer architecture. While
being stable during pretraining and scalable to large model sizes, Pre-LN
suffers from an exponential growth in activation variance across layers,
causing the residual path to dominate over sub-layer outputs and limiting the
learning capacity of deeper layers. To mitigate this issue, we propose
Gradient-Preserving Activation Scaling (GPAS), a simple technique that can be
used in combination with existing approaches. GPAS works by scaling down the
intermediate activations while keeping their gradients unchanged. This leaves
information in the activations intact, and avoids the gradient vanishing
problem associated with gradient downscaling. Extensive experiments across
various model sizes from 71M to 1B show that GPAS achieves consistent
performance gains. Beyond enhancing Pre-LN Transformers, GPAS also shows
promise in improving alternative architectures such as Sandwich-LN and
DeepNorm, demonstrating its versatility and potential for improving training
dynamics in a wide range of settings.

</details>


### [243] [ARMOR: Robust Reinforcement Learning-based Control for UAVs under Physical Attacks](https://arxiv.org/abs/2506.22423)
*Pritam Dash,Ethan Chan,Nathan P. Lawrence,Karthik Pattabiraman*

Main category: cs.LG

TL;DR: ARMOR helps UAVs operate safely under sensor attacks like GPS spoofing by using RL with a learned robust state representation.


<details>
  <summary>Details</summary>
Motivation: UAV sensors are vulnerable to physical attacks, like GPS spoofing, which compromise safety. Current RL methods fail to address these security challenges.

Method: ARMOR utilizes a two-stage RL framework: first, a teacher encoder learns attack-resilient states using privileged information; second, a student encoder refines this without needing privileged data.

Result: ARMOR ensures UAV safety under adversarial conditions, improves handling of unseen attacks, and reduces overall training complexity compared to traditional approaches.

Conclusion: ARMOR provides a robust solution for UAV operation by overcoming sensor attacks, enhancing safety, adaptability, and training efficiency.

Abstract: Unmanned Aerial Vehicles (UAVs) depend on onboard sensors for perception,
navigation, and control. However, these sensors are susceptible to physical
attacks, such as GPS spoofing, that can corrupt state estimates and lead to
unsafe behavior. While reinforcement learning (RL) offers adaptive control
capabilities, existing safe RL methods are ineffective against such attacks. We
present ARMOR (Adaptive Robust Manipulation-Optimized State Representations),
an attack-resilient, model-free RL controller that enables robust UAV operation
under adversarial sensor manipulation. Instead of relying on raw sensor
observations, ARMOR learns a robust latent representation of the UAV's physical
state via a two-stage training framework. In the first stage, a teacher
encoder, trained with privileged attack information, generates attack-aware
latent states for RL policy training. In the second stage, a student encoder is
trained via supervised learning to approximate the teacher's latent states
using only historical sensor data, enabling real-world deployment without
privileged information. Our experiments show that ARMOR outperforms
conventional methods, ensuring UAV safety. Additionally, ARMOR improves
generalization to unseen attacks and reduces training cost by eliminating the
need for iterative adversarial training.

</details>


### [244] [crypto price prediction using lstm+xgboost](https://arxiv.org/abs/2506.22055)
*Mehul Gautam*

Main category: cs.LG

TL;DR: This paper presents a hybrid deep learning (LSTM) and machine learning (XGBoost) model for cryptocurrency price prediction, showing better performance than standalone models.


<details>
  <summary>Details</summary>
Motivation: The study addresses the challenges of accurate cryptocurrency price forecasting due to market volatility and complexity.

Method: A hybrid model combining LSTM for temporal patterns and XGBoost for nonlinear relationships, using historical price, sentiment, and macroeconomic data.

Result: The proposed hybrid model outperformed standalone and traditional forecasting methods on cryptocurrency datasets, measured by MAPE and MinMax RMSE metrics.

Conclusion: Hybrid architectures, like the LSTM+XGBoost model, are effective for financial forecasting and adaptable across cryptocurrencies and market conditions.

Abstract: The volatility and complex dynamics of cryptocurrency markets present unique
challenges for accurate price forecasting. This research proposes a hybrid deep
learning and machine learning model that integrates Long Short-Term Memory
(LSTM) networks and Extreme Gradient Boosting (XGBoost) for cryptocurrency
price prediction. The LSTM component captures temporal dependencies in
historical price data, while XGBoost enhances prediction by modeling nonlinear
relationships with auxiliary features such as sentiment scores and
macroeconomic indicators. The model is evaluated on historical datasets of
Bitcoin, Ethereum, Dogecoin, and Litecoin, incorporating both global and
localized exchange data. Comparative analysis using Mean Absolute Percentage
Error (MAPE) and Min-Max Normalized Root Mean Square Error (MinMax RMSE)
demonstrates that the LSTM+XGBoost hybrid consistently outperforms standalone
models and traditional forecasting methods. This study underscores the
potential of hybrid architectures in financial forecasting and provides
insights into model adaptability across different cryptocurrencies and market
contexts.

</details>


### [245] [Transformers are Graph Neural Networks](https://arxiv.org/abs/2506.22084)
*Chaitanya K. Joshi*

Main category: cs.LG

TL;DR: This paper discusses the connection between Transformers and Graph Neural Networks (GNNs), showing that Transformers can be viewed as GNNs operating on fully connected token graphs with efficient computational operations.


<details>
  <summary>Details</summary>
Motivation: To explore the relationship between Transformers (used widely in NLP) and GNNs for graph-based representation learning, and understand their hardware efficiency advantages.

Method: The authors analyze Transformers through the lens of GNNs by comparing their architecture and operations, such as self-attention acting as message passing and positional encodings providing structural clues crucial for graphs.

Result: It is established that Transformers function as GNNs but operate more efficiently on modern hardware due to dense matrix operations rather than the sparse computations required by conventional GNNs.

Conclusion: Transformers, functioning as GNNs, are particularly advantageous due to their hardware computational efficiency, contributing to their broad success and adoption.

Abstract: We establish connections between the Transformer architecture, originally
introduced for natural language processing, and Graph Neural Networks (GNNs)
for representation learning on graphs. We show how Transformers can be viewed
as message passing GNNs operating on fully connected graphs of tokens, where
the self-attention mechanism capture the relative importance of all tokens
w.r.t. each-other, and positional encodings provide hints about sequential
ordering or structure. Thus, Transformers are expressive set processing
networks that learn relationships among input elements without being
constrained by apriori graphs. Despite this mathematical connection to GNNs,
Transformers are implemented via dense matrix operations that are significantly
more efficient on modern hardware than sparse message passing. This leads to
the perspective that Transformers are GNNs currently winning the hardware
lottery.

</details>


### [246] [Learning to Solve Multi-Objective Routing Problems on Multigraphs](https://arxiv.org/abs/2506.22095)
*Filip Rydin,Attila Lischka,Jiaming Wu,Morteza Haghir Chehreghani,Balázs Kulcsár*

Main category: cs.LG

TL;DR: This paper introduces two neural models to address multi-objective routing on multigraphs and validates them on routing problems such as TSP and CVRP.


<details>
  <summary>Details</summary>
Motivation: To address the lack of focus on multi-objective routing in multigraph settings, which is highly relevant in practical applications.

Method: The paper proposes two neural approaches: one autoregressively selects edges on multigraphs, and the other prunes the multigraph to a simple graph before routing.

Result: Both methods performed well experimentally in solving routing problems like TSP and CVRP.

Conclusion: Neural approaches show promise for multi-objective routing in multigraphs, offering strong performance in tested scenarios.

Abstract: Learning-based methods for routing have gained significant attention in
recent years, both in single-objective and multi-objective contexts. However,
the multigraph setting, where multiple paths with distinct attributes can exist
between destinations, has largely been overlooked, despite its high practical
relevancy. In this paper, we introduce two neural approaches to address
multi-objective routing on multigraphs. Our first approach works directly on
the multigraph, by autoregressively selecting edges until a tour is completed.
On the other hand, our second model first prunes the multigraph into a simple
graph and then builds routes. We validate both models experimentally and find
that they demonstrate strong performance across a variety of problems,
including the Traveling Salesman Problem (TSP) and Capacitated Vehicle Routing
Problem (CVRP).

</details>


### [247] [Transfer Learning for Assessing Heavy Metal Pollution in Seaports Sediments](https://arxiv.org/abs/2506.22096)
*Tin Lai,Farnaz Farid,Yueyang Kuan,Xintian Zhang*

Main category: cs.LG

TL;DR: The paper presents a deep-learning model for assessing heavy metal pollution using the Pollution Load Index, providing accurate predictions with improved metrics.


<details>
  <summary>Details</summary>
Motivation: Environmental monitoring of heavy metal pollution in soils and seaports is time-consuming and inefficient with traditional methods.

Method: The authors developed a transfer learning-based deep-learning model for predicting the Pollution Load Index, utilizing data from multiple seaports.

Result: The proposed model achieves significantly lower errors (MAE ~0.5, MAPE ~0.03), surpassing baseline models by up to 2 orders of magnitude.

Conclusion: The study introduces a cost-effective and accurate solution for assessing water-sediment quality to support environmental protection efforts.

Abstract: Detecting heavy metal pollution in soils and seaports is vital for regional
environmental monitoring. The Pollution Load Index (PLI), an international
standard, is commonly used to assess heavy metal containment. However, the
conventional PLI assessment involves laborious procedures and data analysis of
sediment samples. To address this challenge, we propose a deep-learning-based
model that simplifies the heavy metal assessment process. Our model tackles the
issue of data scarcity in the water-sediment domain, which is traditionally
plagued by challenges in data collection and varying standards across nations.
By leveraging transfer learning, we develop an accurate quantitative assessment
method for predicting PLI. Our approach allows the transfer of learned features
across domains with different sets of features. We evaluate our model using
data from six major ports in New South Wales, Australia: Port Yamba, Port
Newcastle, Port Jackson, Port Botany, Port Kembla, and Port Eden. The results
demonstrate significantly lower Mean Absolute Error (MAE) and Mean Absolute
Percentage Error (MAPE) of approximately 0.5 and 0.03, respectively, compared
to other models. Our model performance is up to 2 orders of magnitude than
other baseline models. Our proposed model offers an innovative, accessible, and
cost-effective approach to predicting water quality, benefiting marine life
conservation, aquaculture, and industrial pollution monitoring.

</details>


### [248] [Earthquake Damage Grades Prediction using An Ensemble Approach Integrating Advanced Machine and Deep Learning Models](https://arxiv.org/abs/2506.22129)
*Anurag Panda,Gaurav Kumar Yadav*

Main category: cs.LG

TL;DR: The paper addresses post-earthquake building damage assessment using machine learning, focusing on handling class imbalance and identifying factors influencing seismic vulnerability.


<details>
  <summary>Details</summary>
Motivation: The study aims to improve post-earthquake response and recovery by accurately predicting building damage grades, which is crucial for saving lives and streamlining resource allocation.

Method: The approach utilizes multi-class classification models, particularly XGBoost and ensemble methods, alongside SMOTE to tackle class imbalance, combined with feature engineering and evaluation techniques like the confusion matrix.

Result: The research identifies key factors for seismic vulnerability and evaluates model performance, highlighting the effectiveness of advanced machine learning techniques.

Conclusion: The study demonstrates that addressing class imbalance and leveraging advanced ML methods significantly enhance the prediction of earthquake-induced damage, aiding disaster management decisions.

Abstract: In the aftermath of major earthquakes, evaluating structural and
infrastructural damage is vital for coordinating post-disaster response
efforts. This includes assessing damage's extent and spatial distribution to
prioritize rescue operations and resource allocation. Accurately estimating
damage grades to buildings post-earthquake is paramount for effective response
and recovery, given the significant impact on lives and properties,
underscoring the urgency of streamlining relief fund allocation processes.
Previous studies have shown the effectiveness of multi-class classification,
especially XGBoost, along with other machine learning models and ensembling
methods, incorporating regularization to address class imbalance. One
consequence of class imbalance is that it may give rise to skewed models that
undervalue minority classes and give preference to the majority class. This
research deals with the problem of class imbalance with the help of the
synthetic minority oversampling technique (SMOTE). We delve into multiple
multi-class classification machine learning, deep learning models, and
ensembling methods to forecast structural damage grades. The study elucidates
performance determinants through comprehensive feature manipulation experiments
and diverse training approaches. It identifies key factors contributing to
seismic vulnerability while evaluating model performance using techniques like
the confusion matrix further to enhance understanding of the effectiveness of
earthquake damage prediction.

</details>


### [249] [Thompson Sampling-Based Learning and Control for Unknown Dynamic Systems](https://arxiv.org/abs/2506.22186)
*Kaikai Zheng,Dawei Shi,Yang Shi,Long Wang*

Main category: cs.LG

TL;DR: The paper introduces a new approach for active learning-based controller design utilizing Thompson Sampling (TS) and Reproducing Kernel Hilbert Spaces to expand its applicability to control law learning without structural constraints.


<details>
  <summary>Details</summary>
Motivation: Thompson Sampling is limited by its reliance on finite parametric representations, which restricts its usability in general spaces commonly needed in control system design.

Method: The paper proposes parameterizing the control law in a function space using Reproducing Kernel Hilbert Spaces and designs a data-driven active learning control approach within a TS framework.

Result: Theoretical analysis confirms an exponential learning rate for the relationship between control laws and performance metrics, an upper bound control regret, and numerical experiments validate the approach's effectiveness on unknown nonlinear systems.

Conclusion: The proposed method overcomes limitations of TS, offering a flexible and theoretically sound solution for active learning-based control design applicable to more generalized and unstructured systems.

Abstract: Thompson sampling (TS) is an effective method to explore parametric
uncertainties and can therefore be used for active learning-based controller
design. However, TS relies on finite parametric representations, which limits
its applicability to more general spaces, which are more commonly encountered
in control system design. To address this issue, this work pro poses a
parameterization method for control law learning using reproducing kernel
Hilbert spaces and designs a data-driven active learning control approach.
Specifically, the proposed method treats the control law as an element in a
function space, allowing the design of control laws without imposing
restrictions on the system structure or the form of the controller. A TS
framework is proposed in this work to explore potential optimal control laws,
and the convergence guarantees are further provided for the learning process.
Theoretical analysis shows that the proposed method learns the relationship
between control laws and closed-loop performance metrics at an exponential
rate, and the upper bound of control regret is also derived. Numerical
experiments on controlling unknown nonlinear systems validate the effectiveness
of the proposed method.

</details>


### [250] [Exploring Modularity of Agentic Systems for Drug Discovery](https://arxiv.org/abs/2506.22189)
*Laura van Weesep,Samuel Genheden,Ola Engkvist,Jens Sjölund*

Main category: cs.LG

TL;DR: This paper evaluates modularity in LLM-based drug discovery systems, comparing models and agent types.


<details>
  <summary>Details</summary>
Motivation: To explore modularity and interchangeability in LLM-based systems for drug discovery, addressing a gap in research.

Method: Performance comparison of LLMs and agent types using chemistry and drug discovery tasks, with an LLM-as-a-judge score.

Result: Claude-3.5-Sonnet, Claude-3.7-Sonnet, and GPT-4o excel in performance; Code-generating agents generally outperform tool-calling agents but results vary by context.

Conclusion: Modularity in LLM-based systems is complex, dependent on models and prompts, requiring further research for stable solutions in drug discovery.

Abstract: Large-language models (LLMs) and agentic systems present exciting
opportunities to accelerate drug discovery and design. In this study, we
critically examine the modularity of LLM-based agentic systems for drug
discovery, i.e., whether parts of the agentic system such as the LLM are
interchangeable, a topic that has received limited attention in drug discovery
applications. We compare the performance of different large language models
(LLMs) and the effectiveness of tool-calling agents versus code-generating
agents in this domain. Our case study, comparing performance in orchestrating
tools for chemistry and drug discovery using an LLM-as-a-judge score, shows
that Claude-3.5-Sonnet, Claude-3.7-Sonnet and GPT-4o outperform alternative
language models such as Llama-3.1-8B, Llama-3.1-70B, GPT-3.5-Turbo, and
Nova-Micro. Although we confirm that code-generating agents outperform the
tool-calling ones on average, we show that this is highly question and model
dependent. Furthermore, the impact of replacing system prompts is dependent on
the specific question asked and the model used, underscoring that -- even in
this particular domain -- one cannot just replace language models without
considering prompt re-engineering. Our study highlights the necessity of
further research into the modularity of agentic systems to enable the
development of stable and scalable solutions for real-world problems.

</details>


### [251] [dreaMLearning: Data Compression Assisted Machine Learning](https://arxiv.org/abs/2506.22190)
*Xiaobo Zhao,Aaron Hurst,Panagiotis Karras,Daniel E. Lucani*

Main category: cs.LG

TL;DR: The paper introduces dreaMLearning, a framework that trains models directly from compressed data, significantly boosting efficiency while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of deep learning related to the need for vast labeled data, high computational demands, and storage requirements, seeking efficient architectures.

Method: The proposed method, dreaMLearning, uses the EntroGeDe compression approach to consolidate data into representative samples for direct compressed data training.

Result: Experiments showed up to 8.8x faster training, a 10x reduction in memory usage, and 42% less storage, with minimal performance loss.

Conclusion: DreaMLearning broadens the scope of ML applications, making learning more efficient and scalable for systems like federated and edge computing.

Abstract: Despite rapid advancements, machine learning, particularly deep learning, is
hindered by the need for large amounts of labeled data to learn meaningful
patterns without overfitting and immense demands for computation and storage,
which motivate research into architectures that can achieve good performance
with fewer resources. This paper introduces dreaMLearning, a novel framework
that enables learning from compressed data without decompression, built upon
Entropy-based Generalized Deduplication (EntroGeDe), an entropy-driven lossless
compression method that consolidates information into a compact set of
representative samples. DreaMLearning accommodates a wide range of data types,
tasks, and model architectures. Extensive experiments on regression and
classification tasks with tabular and image data demonstrate that dreaMLearning
accelerates training by up to 8.8x, reduces memory usage by 10x, and cuts
storage by 42%, with a minimal impact on model performance. These advancements
enhance diverse ML applications, including distributed and federated learning,
and tinyML on resource-constrained edge devices, unlocking new possibilities
for efficient and scalable learning.

</details>


### [252] [REDELEX: A Framework for Relational Deep Learning Exploration](https://arxiv.org/abs/2506.22199)
*Jakub Peleška,Gustav Šír*

Main category: cs.LG

TL;DR: The paper introduces an exploration framework, REDELEX, to evaluate Relational Deep Learning (RDL) models on over 70 relational databases, revealing superior performance of RDL and key factors influencing it.


<details>
  <summary>Details</summary>
Motivation: To address the gap regarding the lack of analysis of the performance relationship between RDL models and the characteristics of relational databases (RDBs).

Method: The paper introduces REDELEX, a framework that benchmarks RDL models of varying complexity against traditional methods, using a diverse set of more than 70 relational databases.

Result: RDL models demonstrated generally superior performance compared to traditional methods, with insights into factors such as model complexity, database size, and structural properties influencing results.

Conclusion: The findings confirm the strong performance of RDL models and underscore the importance of database characteristics on their effectiveness, offering a robust evaluation framework for future research.

Abstract: Relational databases (RDBs) are widely regarded as the gold standard for
storing structured information. Consequently, predictive tasks leveraging this
data format hold significant application promise. Recently, Relational Deep
Learning (RDL) has emerged as a novel paradigm wherein RDBs are conceptualized
as graph structures, enabling the application of various graph neural
architectures to effectively address these tasks. However, given its novelty,
there is a lack of analysis into the relationships between the performance of
various RDL models and the characteristics of the underlying RDBs.
  In this study, we present REDELEX$-$a comprehensive exploration framework for
evaluating RDL models of varying complexity on the most diverse collection of
over 70 RDBs, which we make available to the community. Benchmarked alongside
key representatives of classic methods, we confirm the generally superior
performance of RDL while providing insights into the main factors shaping
performance, including model complexity, database sizes and their structural
properties.

</details>


### [253] [EFRame: Deeper Reasoning via Exploration-Filtering-Replay Reinforcement Learning Framework](https://arxiv.org/abs/2506.22200)
*Chen Wang,Lai Wei,Yanzhi Zhang,Chenyang Shao,Zedong Dan,Weiran Huang,Yue Wang,Yuzhi Zhang*

Main category: cs.LG

TL;DR: The paper introduces EFRame, a framework enhancing Group Relative Policy Optimization (GRPO) by adding exploration, filtering, and replay mechanisms to improve its efficiency, robustness, and reasoning capabilities in reinforcement learning tasks.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of GRPO, such as limited exploration, low sample efficiency, and instability, which hinder its performance on complex reasoning tasks.

Method: EFRame systematically augments GRPO through additional rollouts for exploration, online filtering to eliminate low-quality samples, and experience replay to repeatedly utilize rare and informative samples.

Result: EFRame demonstrates improved robustness and efficiency in training and enables deeper reasoning capabilities compared to vanilla GRPO across various reasoning benchmarks.

Conclusion: EFRame enhances GRPO by establishing a stable learning cycle that transitions effectively from exploration to convergence, allowing for deeper reasoning and better training sample categorization.

Abstract: Recent advances in reinforcement learning (RL) have significantly enhanced
the reasoning capabilities of large language models (LLMs). Group Relative
Policy Optimization (GRPO), an efficient variant of PPO that lowers RL's
computational cost, still faces limited exploration, low sample efficiency and
instability, constraining its performance on complex reasoning tasks. To
address these limitations, we introduce EFRame, an Exploration-Filtering-Replay
framework that systematically augments GRPO along three critical dimensions.
EFRame performs additional rollouts to explore high-quality trajectories,
applies online filtering to eliminate low-quality samples that introduce noise
and variance, and leverages experience replay to repeatedly exploit rare but
informative samples. EFRame establishes a complete and stable learning cycle,
guiding the model through a structured transition from exploration to
convergence. Our experiments across a variety of reasoning benchmarks
demonstrate that EFRame not only improves the robustness and efficiency of
training, but also enables access to deeper reasoning capabilities that remain
unattainable under vanilla GRPO. Furthermore, EFRame enables a more
fine-grained categorization of training samples, allowing for a deeper analysis
of how different types of samples contribute to the learning process in RL. Our
code is available at https://github.com/597358816/EFRame.

</details>


### [254] [Risk-Averse Best Arm Set Identification with Fixed Budget and Fixed Confidence](https://arxiv.org/abs/2506.22253)
*Shunta Nonaga,Koji Tabata,Yuta Mizuno,Tamiki Komatsuzaki*

Main category: cs.LG

TL;DR: The paper introduces a novel stochastic bandit optimization approach that optimally balances maximizing reward and minimizing risk (mean-variance), outperforming existing methods in accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Many real-world decision-making tasks require balancing the trade-off between maximizing reward and minimizing associated risks under uncertainty, which is underexplored in traditional methods.

Method: The paper proposes a meta-algorithmic framework capable of identifying Pareto-optimal trade-offs between reward and risk using adaptive confidence intervals under both fixed-confidence and fixed-budget regimes. The approach is tested both theoretically and empirically.

Result: The proposed framework achieves high accuracy and sample efficiency, outperforming existing methods across synthetic benchmarks for risk-aware decision-making.

Conclusion: The study establishes a unified and efficient methodology for risk-aware optimization, with theoretical and practical contributions to decision-making under uncertainty.

Abstract: Decision making under uncertain environments in the maximization of expected
reward while minimizing its risk is one of the ubiquitous problems in many
subjects. Here, we introduce a novel problem setting in stochastic bandit
optimization that jointly addresses two critical aspects of decision-making:
maximizing expected reward and minimizing associated uncertainty, quantified
via the mean-variance(MV) criterion. Unlike traditional bandit formulations
that focus solely on expected returns, our objective is to efficiently and
accurately identify the Pareto-optimal set of arms that strikes the best
trade-off between expected performance and risk. We propose a unified
meta-algorithmic framework capable of operating under both fixed-confidence and
fixed-budget regimes, achieved through adaptive design of confidence intervals
tailored to each scenario using the same sample exploration strategy. We
provide theoretical guarantees on the correctness of the returned solutions in
both settings. To complement this theoretical analysis, we conduct extensive
empirical evaluations across synthetic benchmarks, demonstrating that our
approach outperforms existing methods in terms of both accuracy and sample
efficiency, highlighting its broad applicability to risk-aware decision-making
tasks in uncertain environments.

</details>


### [255] [Projected Compression: Trainable Projection for Efficient Transformer Compression](https://arxiv.org/abs/2506.22255)
*Maciej Stefaniak,Michał Krutul,Jan Małaśnicki,Maciej Pióro,Jakub Krajewski,Sebastian Jaszczur,Marek Cygan,Kamil Adamczewski,Jan Ludziejewski*

Main category: cs.LG

TL;DR: The paper introduces Projected Compression, a new technique for reducing model size while maintaining performance, using trainable projection modules. This approach is more efficient than hard pruning and retraining.


<details>
  <summary>Details</summary>
Motivation: Large language models grow in size for performance enhancements, increasing computational and time costs. The motivation is to find efficient methods to reduce these costs without sacrificing performance.

Method: The authors propose using projection modules that involve extra trainable weights to compress model parameters. These projections are combined into a lower-dimensional product matrix to create a smaller Transformer-based model.

Result: Projected Compression outperforms traditional pruning and retraining methods in maintaining high-quality outputs, especially over extensive token counts.

Conclusion: This technique offers a computationally efficient way to reduce model sizes while matching or surpassing original performance, addressing a major challenge in scaling language models.

Abstract: Large language models have steadily increased in size to achieve improved
performance; however, this growth has also led to greater inference time and
computational demands. Consequently, there is rising interest in model size
reduction methods. To address this issue, we propose Projected Compression, a
novel model compression technique, that reduces model weights by utilizing
projection modules. Specifically, we first train additional trainable
projections weights and preserve access to all the original model parameters.
Subsequently, these projections are merged into a lower-dimensional product
matrix, resulting in a reduced-size standard Transformer-based model. Unlike
alternative approaches that require additional computational overhead, our
method matches the base model's per-token computation step in FLOPs.
Experimental results show that Projected Compression outperforms the comparable
hard pruning and retraining approach on higher quality models. Moreover, the
performance margin scales well with the number of tokens.

</details>


### [256] [Score-Based Model for Low-Rank Tensor Recovery](https://arxiv.org/abs/2506.22295)
*Zhengyun Cheng,Changhao Wang,Guanwen Zhang,Yi Xu,Wei Zhou,Xiangyang Ji*

Main category: cs.LG

TL;DR: This paper introduces a score-based modeling approach to overcome the limitations of predefined structural assumptions in low-rank tensor decomposition, showing improved performance across various tensor applications.


<details>
  <summary>Details</summary>
Motivation: Traditional tensor decomposition methods rely on predefined structural assumptions, which may not align with real-world scenarios and can result in complex optimizations and accuracy losses.

Method: A novel neural network-based score model is proposed to learn the energy function, optimized through score matching. The approach is paired with block coordinate descent and smooth regularization for tasks like tensor completion and denoising.

Result: The method achieves notable performance improvements in scenarios like sparse tensors, continuous-time tensors, and visual data representation.

Conclusion: Moving beyond Dirac delta assumptions, this score-based model provides a flexible and accurate approach for multiway tensor analysis, addressing structural and distributional limitations of traditional methods.

Abstract: Low-rank tensor decompositions (TDs) provide an effective framework for
multiway data analysis. Traditional TD methods rely on predefined structural
assumptions, such as CP or Tucker decompositions. From a probabilistic
perspective, these can be viewed as using Dirac delta distributions to model
the relationships between shared factors and the low-rank tensor. However, such
prior knowledge is rarely available in practical scenarios, particularly
regarding the optimal rank structure and contraction rules. The optimization
procedures based on fixed contraction rules are complex, and approximations
made during these processes often lead to accuracy loss. To address this issue,
we propose a score-based model that eliminates the need for predefined
structural or distributional assumptions, enabling the learning of
compatibility between tensors and shared factors. Specifically, a neural
network is designed to learn the energy function, which is optimized via score
matching to capture the gradient of the joint log-probability of tensor entries
and shared factors. Our method allows for modeling structures and distributions
beyond the Dirac delta assumption. Moreover, integrating the block coordinate
descent (BCD) algorithm with the proposed smooth regularization enables the
model to perform both tensor completion and denoising. Experimental results
demonstrate significant performance improvements across various tensor types,
including sparse and continuous-time tensors, as well as visual data.

</details>


### [257] [CoATA: Effective Co-Augmentation of Topology and Attribute for Graph Neural Networks](https://arxiv.org/abs/2506.22299)
*Tao Liu,Longlong Lin,Yunfeng Yu,Xi Ou,Youan Zhang,Zhiqiu Ye,Tao Jia*

Main category: cs.LG

TL;DR: This paper introduces CoATA, a GNN framework, that improves graph representation learning by co-augmenting topology and attributes to overcome noise and incompleteness in graphs. It achieves state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: The main motivation is to address the limitations of existing graph neural network (GNN) methods, which handle noise and incompleteness in real-world graphs inadequately by only augmenting either topology or attributes individually.

Method: The authors propose CoATA, a dual-channel GNN framework that propagates structural signals to enrich node attributes, refines structure using a bipartite graph projection, and applies contrastive learning with prototype alignment and consistency constraints.

Result: The CoATA framework outperformed eleven state-of-the-art GNN methods on seven benchmark datasets, demonstrating superior performance in learning robust graph representations.

Conclusion: CoATA effectively captures the interactions between graph topology and attributes, proving to be a powerful method for addressing real-world graph noise and incompleteness.

Abstract: Graph Neural Networks (GNNs) have garnered substantial attention due to their
remarkable capability in learning graph representations. However, real-world
graphs often exhibit substantial noise and incompleteness, which severely
degrades the performance of GNNs. Existing methods typically address this issue
through single-dimensional augmentation, focusing either on refining topology
structures or perturbing node attributes, thereby overlooking the deeper
interplays between the two. To bridge this gap, this paper presents CoATA, a
dual-channel GNN framework specifically designed for the Co-Augmentation of
Topology and Attribute. Specifically, CoATA first propagates structural signals
to enrich and denoise node attributes. Then, it projects the enhanced attribute
space into a node-attribute bipartite graph for further refinement or
reconstruction of the underlying structure. Subsequently, CoATA introduces
contrastive learning, leveraging prototype alignment and consistency
constraints, to facilitate mutual corrections between the augmented and
original graphs. Finally, extensive experiments on seven benchmark datasets
demonstrate that the proposed CoATA outperforms eleven state-of-the-art
baseline methods, showcasing its effectiveness in capturing the synergistic
relationship between topology and attributes.

</details>


### [258] [Weakly-Supervised Domain Adaptation with Proportion-Constrained Pseudo-Labeling](https://arxiv.org/abs/2506.22301)
*Takumi Okuo,Shinnosuke Matsuo,Shota Harada,Kiyohito Tanaka,Ryoma Bise*

Main category: cs.LG

TL;DR: This paper addresses domain shift in machine learning, especially in medical applications, introducing a weakly-supervised domain adaptation method using class proportion information from the target domain.


<details>
  <summary>Details</summary>
Motivation: Domain shift challenges degrade performance in medical datasets, especially when training on one institution's data and applying models to another with differing distributions.

Method: The authors present a weakly-supervised domain adaptation method that leverages class proportion information from the target domain to assign proportion-constrained pseudo-labels to unlabeled target data.

Result: Experiments on two endoscopic datasets show that the proposed method surpasses semi-supervised domain adaptation techniques, even when only 5% of target domain data is labeled. It also displays robustness with noisy class proportion labels.

Conclusion: The method enhances domain adaptation performance using minimal annotations and demonstrates robustness in practical medical scenarios.

Abstract: Domain shift is a significant challenge in machine learning, particularly in
medical applications where data distributions differ across institutions due to
variations in data collection practices, equipment, and procedures. This can
degrade performance when models trained on source domain data are applied to
the target domain. Domain adaptation methods have been widely studied to
address this issue, but most struggle when class proportions between the source
and target domains differ. In this paper, we propose a weakly-supervised domain
adaptation method that leverages class proportion information from the target
domain, which is often accessible in medical datasets through prior knowledge
or statistical reports. Our method assigns pseudo-labels to the unlabeled
target data based on class proportion (called proportion-constrained
pseudo-labeling), improving performance without the need for additional
annotations. Experiments on two endoscopic datasets demonstrate that our method
outperforms semi-supervised domain adaptation techniques, even when 5% of the
target domain is labeled. Additionally, the experimental results with noisy
proportion labels highlight the robustness of our method, further demonstrating
its effectiveness in real-world application scenarios.

</details>


### [259] [Unfolding Generative Flows with Koopman Operators: Fast and Interpretable Sampling](https://arxiv.org/abs/2506.22304)
*Erkan Turan,Aristotelis Siozopoulos,Maks Ovsjanikov*

Main category: cs.LG

TL;DR: The paper enhances Conditional Flow Matching (CFM) by introducing a Koopman operator theory-based approach for faster, one-step generative model sampling and interpretability.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the computational inefficiency and lack of interpretability in sampling non-linear ODEs in Conditional Flow Matching (CFM) generative models.

Method: The method integrates Koopman operator theory into a new Koopman-CFM architecture, embedding generative dynamics into a learned space of observables for linear transformation and enabling one-step sampling via matrix exponentiation.

Result: The proposed approach showed significant speedups over traditional CFM on datasets like MNIST, Fashion-MNIST, and the Toronto Face Dataset, while providing a structured Koopman generator for analytical insights.

Conclusion: The paper concludes that Koopman-enhanced flow matching combines sampling efficiency with interpretability, offering a promising direction for generative modeling.

Abstract: Conditional Flow Matching (CFM) offers a simulation-free framework for
training continuous-time generative models, bridging diffusion and flow-based
approaches. However, sampling from CFM still relies on numerically solving
non-linear ODEs which can be computationally expensive and difficult to
interpret. Recent alternatives address sampling speed via trajectory
straightening, mini-batch coupling or distillation. However, these methods
typically do not shed light on the underlying \textit{structure} of the
generative process. In this work, we propose to accelerate CFM and introduce an
interpretable representation of its dynamics by integrating Koopman operator
theory, which models non-linear flows as linear evolution in a learned space of
observables. We introduce a decoder-free Koopman-CFM architecture that learns
an embedding where the generative dynamics become linear, enabling closed-form,
one-step sampling via matrix exponentiation. This results in significant
speedups over traditional CFM as demonstrated on controlled 2D datasets and
real-world benchmarks, MNIST, Fashion-MNIST (F-MNIST), and the Toronto Face
Dataset (TFD). Unlike previous methods, our approach leads to a well-structured
Koopman generator, whose spectral properties, eigenvalues, and eigenfunctions
offer principled tools for analyzing generative behavior such as temporal
scaling, mode stability, and decomposition in Koopman latent space. By
combining sampling efficiency with analytical structure, Koopman-enhanced flow
matching offers a potential step toward fast and interpretable generative
modeling.

</details>


### [260] [A Framework for Multi-source Privacy Preserving Epidemic Analysis](https://arxiv.org/abs/2506.22342)
*Zihan Guan,Zhiyuan Zhao,Fengwei Tian,Dung Nguyen,Payel Bhattacharjee,Ravi Tandon,B. Aditya Prakash,Anil Vullikanti*

Main category: cs.LG

TL;DR: The paper introduces a framework combining deep learning and epidemic models to simultaneously perform forecasting and mechanistic modeling, integrating privacy-preserving datasets with Differential Privacy guarantees.


<details>
  <summary>Details</summary>
Motivation: The motivation is to leverage the value of diverse and sensitive datasets in epidemiology and public health analyses while safeguarding privacy using Differential Privacy.

Method: The authors propose a framework that integrates deep learning techniques with mechanistic epidemic modeling, allowing for forecasting and data utilization with Differential Privacy protections.

Result: Using a synthetic financial dataset protected by DP, the authors demonstrate significant improvements in epidemic forecasting and modeling accuracy, showcasing the dataset's value.

Conclusion: The study concludes that privacy-preserving datasets, even with DP constraints, can provide valuable contributions to modeling and forecasting epidemics.

Abstract: It is now well understood that diverse datasets provide a lot of value in key
epidemiology and public health analyses, such as forecasting and nowcasting,
development of epidemic models, evaluation and design of interventions and
resource allocation. Some of these datasets are often sensitive, and need
adequate privacy protections. There are many models of privacy, but
Differential Privacy (DP) has become a de facto standard because of its strong
guarantees, without making models about adversaries. In this paper, we develop
a framework the integrates deep learning and epidemic models to simultaneously
perform epidemic forecasting and learning a mechanistic model of epidemic
spread, while incorporating multiple datasets for these analyses, including
some with DP guarantees. We demonstrate our framework using a realistic but
synthetic financial dataset with DP; such a dataset has not been used in such
epidemic analyses. We show that this dataset provides significant value in
forecasting and learning an epidemic model, even when used with DP guarantees.

</details>


### [261] [Sheaf-Based Decentralized Multimodal Learning for Next-Generation Wireless Communication Systems](https://arxiv.org/abs/2506.22374)
*Abdulmomen Ghalkha,Zhuojun Tian,Chaouki Ben Issaid,Mehdi Bennis*

Main category: cs.LG

TL;DR: The paper presents Sheaf-DMFL and Sheaf-DMFL-Att, decentralized multimodal learning frameworks for heterogeneous communication systems, using sheaf theory to address limitations of conventional federated learning.


<details>
  <summary>Details</summary>
Motivation: The rise of complex communication scenarios makes it necessary to improve collaboration among edge devices with diverse multimodal sensory data, which conventional FL methods often fail to efficiently handle.

Method: The authors propose Sheaf-DMFL and Sheaf-DMFL-Att, utilizing modality-specific feature encoders, a sheaf-based structure, and an attention mechanism to facilitate decentralized multimodal learning.

Result: Real-world simulations confirm the performance improvements in link blockage prediction and mmWave beamforming by the proposed frameworks.

Conclusion: Sheaf-DMFL and Sheaf-DMFL-Att significantly enhance decentralized learning for multimodal data in heterogeneous systems, addressing real-world challenges in wireless communication.

Abstract: In large-scale communication systems, increasingly complex scenarios require
more intelligent collaboration among edge devices collecting various multimodal
sensory data to achieve a more comprehensive understanding of the environment
and improve decision-making accuracy. However, conventional federated learning
(FL) algorithms typically consider unimodal datasets, require identical model
architectures, and fail to leverage the rich information embedded in multimodal
data, limiting their applicability to real-world scenarios with diverse
modalities and varying client capabilities. To address this issue, we propose
Sheaf-DMFL, a novel decentralized multimodal learning framework leveraging
sheaf theory to enhance collaboration among devices with diverse modalities.
Specifically, each client has a set of local feature encoders for its different
modalities, whose outputs are concatenated before passing through a
task-specific layer. While encoders for the same modality are trained
collaboratively across clients, we capture the intrinsic correlations among
clients' task-specific layers using a sheaf-based structure. To further enhance
learning capability, we propose an enhanced algorithm named Sheaf-DMFL-Att,
which tailors the attention mechanism within each client to capture
correlations among different modalities. A rigorous convergence analysis of
Sheaf-DMFL-Att is provided, establishing its theoretical guarantees. Extensive
simulations are conducted on real-world link blockage prediction and mmWave
beamforming scenarios, demonstrate the superiority of the proposed algorithms
in such heterogeneous wireless communication systems.

</details>


### [262] [Probabilistic Optimality for Inference-time Scaling](https://arxiv.org/abs/2506.22376)
*Youkang Wang,Jian Wang,Rubing Chen,Xiao-Yong Wei,Qing Li*

Main category: cs.LG

TL;DR: This paper introduces OptScale, a probabilistic algorithm to optimize inference-time sampling for Large Language Models (LLMs), minimizing computation while maintaining high performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to provide a principled basis for sampling in inference-time scaling of LLMs, as current approaches rely on heuristic strategies lacking theoretical foundation.

Method: A probabilistic framework is proposed to determine optimal inference-time scaling. Using this, a theoretical lower bound on required samples is derived, and an algorithm (OptScale) is developed to efficiently select the minimal samples needed for target performance.

Result: OptScale was tested on mathematical reasoning tasks and showed significant reductions in sampling overhead while achieving reasoning performance better or comparable to state-of-the-art methods.

Conclusion: The study offers a theoretical and practical solution to optimize inference-time scalability, enhancing compute efficiency for deploying LLMs in complex reasoning tasks.

Abstract: Inference-time scaling has emerged as a powerful technique for enhancing the
reasoning performance of Large Language Models (LLMs). However, existing
approaches often rely on heuristic strategies for parallel sampling, lacking a
principled foundation. To address this gap, we propose a probabilistic
framework that formalizes the optimality of inference-time scaling under the
assumption that parallel samples are independently and identically distributed
(i.i.d.), and where the Best-of-N selection strategy follows a probability
distribution that can be estimated. Within this framework, we derive a
theoretical lower bound on the required number of samples to achieve a target
performance level, providing the first principled guidance for
compute-efficient scaling. Leveraging this insight, we develop
\textsc{OptScale}, a practical algorithm that dynamically determines the
optimal number of sampled responses. \textsc{OptScale} employs a language
model-based predictor to estimate probabilistic prior parameters, enabling the
decision of the minimal number of samples needed that satisfy predefined
performance thresholds and confidence levels. Extensive experiments on
mathematical reasoning benchmarks (including MATH-500, GSM8K, AIME, and AMC)
demonstrate that \textsc{OptScale} significantly reduces sampling overhead
while remaining better or on par with state-of-the-art reasoning performance.
Our work offers both a theoretical foundation and a practical solution for
principled inference-time scaling, addressing a critical gap in the efficient
deployment of LLMs for complex reasoning.

</details>


### [263] [Towards Distributed Neural Architectures](https://arxiv.org/abs/2506.22389)
*Aditya Cowsik,Tianyu He,Andrey Gromov*

Main category: cs.LG

TL;DR: The paper introduces Distributed Neural Architectures (DNAs) that allow tokens to follow custom paths through neural modules, improving efficiency and interpretability.


<details>
  <summary>Details</summary>
Motivation: To generalize sparse methods like Mixture-of-Experts and improve compute/memory efficiency while maintaining competitive performance in vision and language tasks.

Method: DNAs use a proto-architecture with modules (e.g., transformers, MLPs, attention) and routers, where token paths are learned end-to-end considering optimization objectives like efficiency.

Result: The approach matches dense baselines in performance, demonstrates emergent computation patterns like power-law distributed paths, and allocates resources in an interpretable manner.

Conclusion: DNAs provide a flexible, efficient, and interpretable neural architecture with potential for advancements in vision and language models.

Abstract: We introduce and train distributed neural architectures (DNA) in vision and
language domains. DNAs are initialized with a proto-architecture that consists
of (transformer, MLP, attention, etc.) modules and routers. Any token (or
patch) can traverse any series of modules in any order. DNAs are a natural
generalization of the sparse methods such as Mixture-of-Experts,
Mixture-of-Depths, parameter sharing, etc. Computation and communication
patterns of DNA modules are learnt end-to-end during training and depend on the
content and context of each token (or patch). These patterns can be shaped by
further requirements added to the optimization objective such as compute/memory
efficiency or load balancing. We empirically show that (i) trained DNAs are
competitive with the dense baselines in both domains and (ii) compute
efficiency/parameter sharing can be learnt from data. Next, we analyze the
emergent connectivity and computation patterns in the trained DNAs. We find
that the paths that tokens take through the models are themselves distributed
according to a power-law. We show that some paths (or, equivalently, groups of
modules) show emergent specialization. Finally, we demonstrate that models
learn to allocate compute and active parameters in an interpretable way.

</details>


### [264] [Multi-View Contrastive Learning for Robust Domain Adaptation in Medical Time Series Analysis](https://arxiv.org/abs/2506.22393)
*YongKyung Oh,Alex Bui*

Main category: cs.LG

TL;DR: This paper proposes a novel multi-view contrastive learning framework to improve domain adaptation for medical time series, achieving state-of-the-art results in transfer learning tasks.


<details>
  <summary>Details</summary>
Motivation: Current machine learning approaches struggle with adapting to medical time series due to complex temporal dependencies and dynamic distribution shifts.

Method: The proposed framework leverages multi-view contrastive learning with independent encoders and a hierarchical fusion mechanism to integrate temporal, derivative-based, and frequency-domain features.

Result: Extensive experiments on medical datasets like EEG, ECG, and EMG show significant improvements in domain adaptation compared to state-of-the-art methods.

Conclusion: The framework enhances the robustness and generalizability of machine learning models, providing a practical solution for deploying reliable AI systems in healthcare settings.

Abstract: Adapting machine learning models to medical time series across different
domains remains a challenge due to complex temporal dependencies and dynamic
distribution shifts. Current approaches often focus on isolated feature
representations, limiting their ability to fully capture the intricate temporal
dynamics necessary for robust domain adaptation. In this work, we propose a
novel framework leveraging multi-view contrastive learning to integrate
temporal patterns, derivative-based dynamics, and frequency-domain features.
Our method employs independent encoders and a hierarchical fusion mechanism to
learn feature-invariant representations that are transferable across domains
while preserving temporal coherence. Extensive experiments on diverse medical
datasets, including electroencephalogram (EEG), electrocardiogram (ECG), and
electromyography (EMG) demonstrate that our approach significantly outperforms
state-of-the-art methods in transfer learning tasks. By advancing the
robustness and generalizability of machine learning models, our framework
offers a practical pathway for deploying reliable AI systems in diverse
healthcare settings.

</details>


### [265] [Exploration from a Primal-Dual Lens: Value-Incentivized Actor-Critic Methods for Sample-Efficient Online RL](https://arxiv.org/abs/2506.22401)
*Tong Yang,Bo Dai,Lin Xiao,Yuejie Chi*

Main category: cs.LG

TL;DR: The paper introduces a new reinforcement learning method called Value-Incentivized Actor-Critic (VAC), ensuring near-optimal regret guarantees and combining exploration and exploitation efficiently.


<details>
  <summary>Details</summary>
Motivation: Challenges in balancing exploration and exploitation in reinforcement learning with complex function approximations have persisted despite the popularity of RL frameworks.

Method: The paper reformulates the optimism principle using primal-dual optimization and proposes an integrated objective function (VAC method) for actor-critic frameworks.

Result: VAC demonstrates theoretical near-optimal regret guarantees for linear MDPs in variable horizon conditions and adaptability to broader function approximations under specific assumptions.

Conclusion: VAC offers a theoretically-backed, efficient scheme for addressing exploration-exploitation trade-offs in online RL, bridging theoretical guarantees with practical applications.

Abstract: Online reinforcement learning (RL) with complex function approximations such
as transformers and deep neural networks plays a significant role in the modern
practice of artificial intelligence. Despite its popularity and importance,
balancing the fundamental trade-off between exploration and exploitation
remains a long-standing challenge; in particular, we are still in lack of
efficient and practical schemes that are backed by theoretical performance
guarantees. Motivated by recent developments in exploration via optimistic
regularization, this paper provides an interpretation of the principle of
optimism through the lens of primal-dual optimization. From this fresh
perspective, we set forth a new value-incentivized actor-critic (VAC) method,
which optimizes a single easy-to-optimize objective integrating exploration and
exploitation -- it promotes state-action and policy estimates that are both
consistent with collected data transitions and result in higher value
functions. Theoretically, the proposed VAC method has near-optimal regret
guarantees under linear Markov decision processes (MDPs) in both finite-horizon
and infinite-horizon settings, which can be extended to the general function
approximation setting under appropriate assumptions.

</details>


### [266] [CLoVE: Personalized Federated Learning through Clustering of Loss Vector Embeddings](https://arxiv.org/abs/2506.22427)
*Randeep Bhatia,Nikos Papadis,Murali Kodialam,TV Lakshman,Sayak Chakrabarty*

Main category: cs.LG

TL;DR: The paper introduces CLoVE, an algorithm for clustering clients in Federated Learning based on loss vector embeddings, achieving accurate cluster identification and high model performance efficiently.


<details>
  <summary>Details</summary>
Motivation: The challenge in Federated Learning is to effectively cluster clients whose data distributions differ, without prior knowledge of client assignments.

Method: CLoVE identifies clusters by analyzing loss vector embeddings derived from client data losses and uses an iterative process to separate clients and optimize cluster-specific models.

Result: CLoVE demonstrates accurate cluster recovery in a few training rounds and offers state-of-the-art model accuracy across diverse datasets and settings in both supervised and unsupervised tasks.

Conclusion: CLoVE's simplicity, applicability in varied settings, and robustness make it a practical and highly effective approach for Clustered Federated Learning.

Abstract: We propose CLoVE (Clustering of Loss Vector Embeddings), a novel algorithm
for Clustered Federated Learning (CFL). In CFL, clients are naturally grouped
into clusters based on their data distribution. However, identifying these
clusters is challenging, as client assignments are unknown. CLoVE utilizes
client embeddings derived from model losses on client data, and leverages the
insight that clients in the same cluster share similar loss values, while those
in different clusters exhibit distinct loss patterns. Based on these
embeddings, CLoVE is able to iteratively identify and separate clients from
different clusters and optimize cluster-specific models through federated
aggregation. Key advantages of CLoVE over existing CFL algorithms are (1) its
simplicity, (2) its applicability to both supervised and unsupervised settings,
and (3) the fact that it eliminates the need for near-optimal model
initialization, which makes it more robust and better suited for real-world
applications. We establish theoretical convergence bounds, showing that CLoVE
can recover clusters accurately with high probability in a single round and
converges exponentially fast to optimal models in a linear setting. Our
comprehensive experiments comparing with a variety of both CFL and generic
Personalized Federated Learning (PFL) algorithms on different types of datasets
and an extensive array of non-IID settings demonstrate that CLoVE achieves
highly accurate cluster recovery in just a few rounds of training, along with
state-of-the-art model accuracy, across a variety of both supervised and
unsupervised PFL tasks.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [267] [An Effective Two-Phase Genetic Algorithm for Solving the Resource Constrained Project Scheduling Problem (RCPSP)](https://arxiv.org/abs/2506.21915)
*D. Sun,S. Zhou*

Main category: cs.NE

TL;DR: This paper introduces a 2-phase variation of genetic algorithm for RCPSP, emphasizing solution space intensification and diversification.


<details>
  <summary>Details</summary>
Motivation: To address challenges of local traps and improve heuristic solutions in solving RCPSP using genetic algorithms.

Method: The algorithm alternates between two phases in parent selection: Phase-1 intensifies within the current neighborhood and Phase-2 diversifies by excluding the best current solutions.

Result: The proposed 2PGA algorithm showed effectiveness in solving benchmark problems from PSPLIB, yielding improved heuristic solutions.

Conclusion: 2PGA successfully combines intensification and diversification, enhancing genetic algorithm performance in RCPSP challenges.

Abstract: This note presents a simple and effective variation of genetic algorithm (GA)
for solving RCPSP, denoted as 2-Phase Genetic Algorithm (2PGA). The 2PGA
implements GA parent selection in two phases: Phase-1 includes the best current
solutions in the parent pool, and Phase-2 excludes the best current solutions
from the parent pool. The 2PGA carries out the GA evolution by alternating the
two phases iteratively. In exploring a solution space, the Phase-1 emphasizes
intensification in current neighborhood, while the Phase-2 emphasizes
diversification to escape local traps. The 2PGA was tested on the standard
benchmark problems in PSPLIB, the results have shown that the algorithm is
effective and has improved some of the best heuristic solutions.

</details>


### [268] [In situ fine-tuning of in silico trained Optical Neural Networks](https://arxiv.org/abs/2506.22122)
*Gianluca Kosmella,Ripalta Stabile,Jaron Sanders*

Main category: cs.NE

TL;DR: This paper introduces Gradient-Informed Fine-Tuning (GIFT), a method to improve Optical Neural Network (ONN) performance under noise misspecification, showing a 28% relative accuracy improvement in simulations.


<details>
  <summary>Details</summary>
Motivation: The goal is to address shortcomings in training Optical Neural Networks (ONNs), particularly the discrepancies arising during the transfer of parameters from simplified training models to physical ONN hardware, exacerbated by noise and fabrication imperfections.

Method: The proposed GIFT algorithm fine-tunes pretrained parameters in situ using gradient information derived from the ONN's noise structure, avoiding expensive retraining and additional experimental setups. Formal conditions for its applicability are also provided.

Result: Simulations of GIFT on a five-layer ONN trained for MNIST digit classification show a significant improvement in accuracy (up to 28%) under noise misspecification compared to baseline models.

Conclusion: GIFT effectively mitigates the performance gap between digital model training and physical ONN implementation, serving as a practical and cost-efficient solution for improving ONN performance in noisy environments.

Abstract: Optical Neural Networks (ONNs) promise significant advantages over
traditional electronic neural networks, including ultrafast computation, high
bandwidth, and low energy consumption, by leveraging the intrinsic capabilities
of photonics. However, training ONNs poses unique challenges, notably the
reliance on simplified in silico models whose trained parameters must
subsequently be mapped to physical hardware. This process often introduces
inaccuracies due to discrepancies between the idealized digital model and the
physical ONN implementation, particularly stemming from noise and fabrication
imperfections.
  In this paper, we analyze how noise misspecification during in silico
training impacts ONN performance and we introduce Gradient-Informed Fine-Tuning
(GIFT), a lightweight algorithm designed to mitigate this performance
degradation. GIFT uses gradient information derived from the noise structure of
the ONN to adapt pretrained parameters directly in situ, without requiring
expensive retraining or complex experimental setups. GIFT comes with formal
conditions under which it improves ONN performance.
  We also demonstrate the effectiveness of GIFT via simulation on a five-layer
feed forward ONN trained on the MNIST digit classification task. GIFT achieves
up to $28\%$ relative accuracy improvement compared to the baseline performance
under noise misspecification, without resorting to costly retraining. Overall,
GIFT provides a practical solution for bridging the gap between simplified
digital models and real-world ONN implementations.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [269] [Redundant Array Computation Elimination](https://arxiv.org/abs/2506.21960)
*Zixuan Wang,Liang Yuan,Xianmeng Jiang,Kun Li,Junmin Xiao,Yunquan Zhang*

Main category: cs.PF

TL;DR: This paper introduces RACE, a technique to eliminate redundancies in loop nest optimizations using a two-level scheme.


<details>
  <summary>Details</summary>
Motivation: There is a lack of universal approaches for redundancy elimination in array computations within loop nests, particularly for handling complex structures.

Method: The proposed method, RACE, employs a two-level scheme to detect both data reuse and computation redundancies hierarchically in linear time. It also uses optimized auxiliary arrays and aggressive expression reassociation strategies.

Result: Experimental results validate that RACE effectively eliminates redundancies and improves optimization in loop nests.

Conclusion: RACE is a general and efficient technique for redundancy elimination in array computations, addressing limitations of prior approaches.

Abstract: Redundancy elimination is a key optimization direction, and loop nests are
the main optimization target in modern compilers. Previous work on redundancy
elimination of array computations in loop nests lacks universality. These
approaches either focus on specific computation patterns or fail to recognize
redundancies with complex structures. This paper proposes RACE (Redundant Array
Computation Elimination), a more general redundancy elimination technique. RACE
utilizes a novel two-level scheme to identify the data reuse between array
references and the computation redundancies between expressions. It traverses
the expression trees in loop nests to detect redundancies hierarchically in
linear time and generates efficient code with optimized auxiliary arrays that
store redundant computation results. Furthermore, RACE supports the expression
reassociation with various aggressive strategies to improve the redundancy
opportunities. Experimental results demonstrate the effectiveness of RACE.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [270] [FrankenBot: Brain-Morphic Modular Orchestration for Robotic Manipulation with Vision-Language Models](https://arxiv.org/abs/2506.21627)
*Shiyi Wang,Wenbo Li,Yiteng Chen,Qingyao Wu,Huiping Zhuang*

Main category: cs.RO

TL;DR: The paper introduces FrankenBot, a robotics manipulation framework inspired by human brain architecture, that integrates Vision-Language Models (VLMs) to enhance functionality and operational efficiency in robot systems.


<details>
  <summary>Details</summary>
Motivation: The authors aim to overcome the longstanding challenge of creating a general robot manipulation system capable of operating in complex, unstructured environments, with human-like efficiency and robustness.

Method: FrankenBot integrates Vision-Language Models and mimics the architecture of the human brain, mapping robotic tasks to brain-inspired modules such as cortex for task planning and brainstem for interfacing, with a focus on efficiency and functional decoupling.

Result: Experiments in both simulation and the real world demonstrate that FrankenBot excels in anomaly detection and handling, long-term memory management, operational efficiency, and system stability without requiring further training.

Conclusion: FrankenBot provides a comprehensive, efficient, and stable approach to robotic manipulation, bridging the gap between diverse functional requirements and operational effectiveness.

Abstract: Developing a general robot manipulation system capable of performing a wide
range of tasks in complex, dynamic, and unstructured real-world environments
has long been a challenging task. It is widely recognized that achieving
human-like efficiency and robustness manipulation requires the robotic brain to
integrate a comprehensive set of functions, such as task planning, policy
generation, anomaly monitoring and handling, and long-term memory, achieving
high-efficiency operation across all functions. Vision-Language Models (VLMs),
pretrained on massive multimodal data, have acquired rich world knowledge,
exhibiting exceptional scene understanding and multimodal reasoning
capabilities. However, existing methods typically focus on realizing only a
single function or a subset of functions within the robotic brain, without
integrating them into a unified cognitive architecture. Inspired by a
divide-and-conquer strategy and the architecture of the human brain, we propose
FrankenBot, a VLM-driven, brain-morphic robotic manipulation framework that
achieves both comprehensive functionality and high operational efficiency. Our
framework includes a suite of components, decoupling a part of key functions
from frequent VLM calls, striking an optimal balance between functional
completeness and system efficiency. Specifically, we map task planning, policy
generation, memory management, and low-level interfacing to the cortex,
cerebellum, temporal lobe-hippocampus complex, and brainstem, respectively, and
design efficient coordination mechanisms for the modules. We conducted
comprehensive experiments in both simulation and real-world robotic
environments, demonstrating that our method offers significant advantages in
anomaly detection and handling, long-term memory, operational efficiency, and
stability -- all without requiring any fine-tuning or retraining.

</details>


### [271] [Ark: An Open-source Python-based Framework for Robot Learning](https://arxiv.org/abs/2506.21628)
*Magnus Dierking,Christopher E. Mower,Sarthak Das,Huang Helong,Jiacheng Qiu,Cody Reading,Wei Chen,Huidong Liang,Huang Guowei,Jan Peters,Quan Xingyue,Jun Wang,Haitham Bou-Ammar*

Main category: cs.RO

TL;DR: ARK is an open-source Python-first robotics framework designed to integrate robotics and AI seamlessly, addressing the challenges of fragmented tooling and steep learning curves in robotics software.


<details>
  <summary>Details</summary>
Motivation: The paper highlights the disparity between advancements in robotics hardware and the lagging progress in software autonomy due to steep learning curves, low-level coding demands, and fragmented tooling, which hinder widespread adoption and development.

Method: The framework introduces ARK, a Python-centric robotics platform equipped with a Gym-style interface for data collection, preprocessing, and policy training. ARK incorporates real-time performance through optional C/C++ support, reusable modules for various robotics tasks, and seamless integration with ROS.

Result: ARK simplifies robotics workflows by enabling rapid prototyping, hardware swapping, and unified integration of AI and robotics tools, rivaling mainstream machine-learning ecosystem convenience.

Conclusion: ARK bridges the gap between robotics and AI by lowering entry barriers, simplifying workflows, and accelerating research and commercial deployment of autonomous robots.

Abstract: Robotics has made remarkable hardware strides-from DARPA's Urban and Robotics
Challenges to the first humanoid-robot kickboxing tournament-yet commercial
autonomy still lags behind progress in machine learning. A major bottleneck is
software: current robot stacks demand steep learning curves, low-level C/C++
expertise, fragmented tooling, and intricate hardware integration, in stark
contrast to the Python-centric, well-documented ecosystems that propelled
modern AI. We introduce ARK, an open-source, Python-first robotics framework
designed to close that gap. ARK presents a Gym-style environment interface that
allows users to collect data, preprocess it, and train policies using
state-of-the-art imitation-learning algorithms (e.g., ACT, Diffusion Policy)
while seamlessly toggling between high-fidelity simulation and physical robots.
A lightweight client-server architecture provides networked
publisher-subscriber communication, and optional C/C++ bindings ensure
real-time performance when needed. ARK ships with reusable modules for control,
SLAM, motion planning, system identification, and visualization, along with
native ROS interoperability. Comprehensive documentation and case studies-from
manipulation to mobile navigation-demonstrate rapid prototyping, effortless
hardware swapping, and end-to-end pipelines that rival the convenience of
mainstream machine-learning workflows. By unifying robotics and AI practices
under a common Python umbrella, ARK lowers entry barriers and accelerates
research and commercial deployment of autonomous robots.

</details>


### [272] [TOMD: A Trail-based Off-road Multimodal Dataset for Traversable Pathway Segmentation under Challenging Illumination Conditions](https://arxiv.org/abs/2506.21630)
*Yixin Sun,Li Li,Wenke E,Amir Atapour-Abarghouei,Toby P. Breckon*

Main category: cs.RO

TL;DR: The paper introduces the Trail-based Off-road Multimodal Dataset (TOMD) and proposes a dynamic multiscale data fusion model for analyzing and improving trail-based navigation in unstructured outdoor environments.


<details>
  <summary>Details</summary>
Motivation: Autonomous robots face challenges in detecting traversable paths in complex and narrow off-road environments, which are critical for applications like search and rescue or forest fire management.

Method: The authors present a multimodal dataset, TOMD, featuring data from repeated traversals using sensors like LiDAR, stereo imagery, and IMUs. They also propose a dynamic multiscale data fusion model to enhance pathway prediction under varying conditions.

Result: Their method demonstrates high accuracy in traversable pathway prediction and highlights the importance of illumination conditions on segmentation performance.

Conclusion: The paper provides a valuable dataset and validates an effective data fusion model, paving the way for improved off-road navigation research. TOMD is publicly available to facilitate further advancements in this domain.

Abstract: Detecting traversable pathways in unstructured outdoor environments remains a
significant challenge for autonomous robots, especially in critical
applications such as wide-area search and rescue, as well as incident
management scenarios like forest fires. Existing datasets and models primarily
target urban settings or wide, vehicle-traversable off-road tracks, leaving a
substantial gap in addressing the complexity of narrow, trail-like off-road
scenarios. To address this, we introduce the Trail-based Off-road Multimodal
Dataset (TOMD), a comprehensive dataset specifically designed for such
environments. TOMD features high-fidelity multimodal sensor data -- including
128-channel LiDAR, stereo imagery, GNSS, IMU, and illumination measurements --
collected through repeated traversals under diverse conditions. We also propose
a dynamic multiscale data fusion model for accurate traversable pathway
prediction. The study analyzes the performance of early, cross, and mixed
fusion strategies under varying illumination levels. Results demonstrate the
effectiveness of our approach and the relevance of illumination in segmentation
performance. We publicly release TOMD at https://github.com/yyyxs1125/TMOD to
support future research in trail-based off-road navigation.

</details>


### [273] [Real-Time 3D Guidewire Reconstruction from Intraoperative DSA Images for Robot-Assisted Endovascular Interventions](https://arxiv.org/abs/2506.21631)
*Tianliang Yao,Bingrui Li,Bo Lu,Zhiqiang Pei,Yixuan Yuan,Peng Qi*

Main category: cs.RO

TL;DR: The paper presents a framework for real-time 3D guidewire reconstruction, integrating preoperative 3D CTA with intraoperative 2D DSA, to improve the spatial accuracy of robot-assisted endovascular interventions.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of 2D DSA in providing depth information, which leads to spatial ambiguities in guidewire shape sensing during robotic-assisted endovascular procedures.

Method: The methodology involves combining 3D CTA data with 2D DSA images through feature extraction, deformable image registration, and an inverse projection algorithm for accurate 3D guidewire reconstruction.

Result: The proposed method achieves a projection error of 1.76±0.08 pixels, a length deviation of 2.93±0.15%, and a real-time processing speed of 39.3±1.5 FPS, showcasing improvements in accuracy and efficiency.

Conclusion: The framework enhances spatial awareness, optimizes robotic-assisted interventions, and offers significant improvements in reconstruction accuracy and computational efficiency for better clinical outcomes.

Abstract: Accurate three-dimensional (3D) reconstruction of guidewire shapes is crucial
for precise navigation in robot-assisted endovascular interventions.
Conventional 2D Digital Subtraction Angiography (DSA) is limited by the absence
of depth information, leading to spatial ambiguities that hinder reliable
guidewire shape sensing. This paper introduces a novel multimodal framework for
real-time 3D guidewire reconstruction, combining preoperative 3D Computed
Tomography Angiography (CTA) with intraoperative 2D DSA images. The method
utilizes robust feature extraction to address noise and distortion in 2D DSA
data, followed by deformable image registration to align the 2D projections
with the 3D CTA model. Subsequently, the inverse projection algorithm
reconstructs the 3D guidewire shape, providing real-time, accurate spatial
information. This framework significantly enhances spatial awareness for
robotic-assisted endovascular procedures, effectively bridging the gap between
preoperative planning and intraoperative execution. The system demonstrates
notable improvements in real-time processing speed, reconstruction accuracy,
and computational efficiency. The proposed method achieves a projection error
of 1.76$\pm$0.08 pixels and a length deviation of 2.93$\pm$0.15\%, with a frame
rate of 39.3$\pm$1.5 frames per second (FPS). These advancements have the
potential to optimize robotic performance and increase the precision of complex
endovascular interventions, ultimately contributing to better clinical
outcomes.

</details>


### [274] [AeroLite-MDNet: Lightweight Multi-task Deviation Detection Network for UAV Landing](https://arxiv.org/abs/2506.21635)
*Haiping Yang,Huaxing Liu,Wei Wu,Zuohui Chen,Ning Wu*

Main category: cs.RO

TL;DR: This paper introduces AeroLite-MDNet, a vision-based deviation warning system for UAV landings, achieving high accuracy (98.6%) and a short warning delay (0.7s). A new dataset, UAVLandData, is also provided.


<details>
  <summary>Details</summary>
Motivation: Accurate UAV landing is crucial for operational continuity but is challenging due to factors like GPS interference.

Method: The proposed vision-based model, AeroLite-MDNet, integrates multiscale fusion and segmentation for detection and orientation estimation. A new metric, AWD, is introduced for evaluation.

Result: The system achieves an Average Warning Delay of 0.7 seconds and a deviation detection accuracy of 98.6%.

Conclusion: The proposed system improves UAV landing reliability, offering robust deviation detection and a new dataset for further research.

Abstract: Unmanned aerial vehicles (UAVs) are increasingly employed in diverse
applications such as land surveying, material transport, and environmental
monitoring. Following missions like data collection or inspection, UAVs must
land safely at docking stations for storage or recharging, which is an
essential requirement for ensuring operational continuity. However, accurate
landing remains challenging due to factors like GPS signal interference. To
address this issue, we propose a deviation warning system for UAV landings,
powered by a novel vision-based model called AeroLite-MDNet. This model
integrates a multiscale fusion module for robust cross-scale object detection
and incorporates a segmentation branch for efficient orientation estimation. We
introduce a new evaluation metric, Average Warning Delay (AWD), to quantify the
system's sensitivity to landing deviations. Furthermore, we contribute a new
dataset, UAVLandData, which captures real-world landing deviation scenarios to
support training and evaluation. Experimental results show that our system
achieves an AWD of 0.7 seconds with a deviation detection accuracy of 98.6\%,
demonstrating its effectiveness in enhancing UAV landing reliability. Code will
be available at https://github.com/ITTTTTI/Maskyolo.git

</details>


### [275] [Optimal Motion Scaling for Delayed Telesurgery](https://arxiv.org/abs/2506.21689)
*Jason Lim,Florian Richter,Zih-Yun Chiu,Jaeyon Lee,Ethan Quist,Nathan Fisher,Jonathan Chambers,Steven Hong,Michael C. Yip*

Main category: cs.RO

TL;DR: The paper addresses robotic teleoperation challenges posed by network latency by optimizing motion scaling factors based on delay and user-specific tendencies.


<details>
  <summary>Details</summary>
Motivation: Network latency introduces errors in robotic teleoperation, particularly in telesurgery. The aim is to enhance performance under such conditions.

Method: User studies were conducted to analyze the relationship between latency, motion scaling factors, and performance, followed by personalized modeling techniques.

Result: The studies found that the optimal scaling factor varies with latency levels and user differences, necessitating personalized models for improvement.

Conclusion: Personalized strategies for mapping latency levels to scaling factors are effective in optimizing teleoperation performance, particularly for telesurgery applications.

Abstract: Robotic teleoperation over long communication distances poses challenges due
to delays in commands and feedback from network latency. One simple yet
effective strategy to reduce errors and increase performance under delay is to
downscale the relative motion between the operating surgeon and the robot. The
question remains as to what is the optimal scaling factor, and how this value
changes depending on the level of latency as well as operator tendencies. We
present user studies investigating the relationship between latency, scaling
factor, and performance. The results of our studies demonstrate a statistically
significant difference in performance between users and across scaling factors
for certain levels of delay. These findings indicate that the optimal scaling
factor for a given level of delay is specific to each user, motivating the need
for personalized models for optimal performance. We present techniques to model
the user-specific mapping of latency level to scaling factor for optimal
performance, leading to an efficient and effective solution to optimizing
performance of robotic teleoperation and specifically telesurgery under large
communication delay.

</details>


### [276] [Experimental investigation of pose informed reinforcement learning for skid-steered visual navigation](https://arxiv.org/abs/2506.21732)
*Ameya Salvi,Venkat Krovi*

Main category: cs.RO

TL;DR: The paper discusses a novel structured approach for vision-based navigation in skid-steered vehicles, demonstrating enhanced performance over existing methods.


<details>
  <summary>Details</summary>
Motivation: Skid-steered vehicles face challenges in automation due to difficulty in modeling wheel-terrain interactions, particularly in off-road settings. End-to-end learning is seen as a way to address this gap.

Method: The authors propose a structured formulation for learning visual navigation, tested through software simulations, hardware evaluations, and ablation studies.

Result: The proposed approach outperforms existing methods in terms of performance as demonstrated in experimental analyses.

Conclusion: The study establishes the effectiveness and superiority of the proposed structured formulation for visual navigation in skid-steered vehicles.

Abstract: Vision-based lane keeping is a topic of significant interest in the robotics
and autonomous ground vehicles communities in various on-road and off-road
applications. The skid-steered vehicle architecture has served as a useful
vehicle platform for human controlled operations. However, systematic modeling,
especially of the skid-slip wheel terrain interactions (primarily in off-road
settings) has created bottlenecks for automation deployment. End-to-end
learning based methods such as imitation learning and deep reinforcement
learning, have gained prominence as a viable deployment option to counter the
lack of accurate analytical models. However, the systematic formulation and
subsequent verification/validation in dynamic operation regimes (particularly
for skid-steered vehicles) remains a work in progress. To this end, a novel
approach for structured formulation for learning visual navigation is proposed
and investigated in this work. Extensive software simulations, hardware
evaluations and ablation studies now highlight the significantly improved
performance of the proposed approach against contemporary literature.

</details>


### [277] [Skill-Nav: Enhanced Navigation with Versatile Quadrupedal Locomotion via Waypoint Interface](https://arxiv.org/abs/2506.21853)
*Dewei Wang,Chenjia Ba,Chenhui Li,Jiyuan Shi,Yan Ding,Chi Zhang,Bin Zhao*

Main category: cs.RO

TL;DR: The paper proposes Skill-Nav, a system combining RL-based locomotion skills with hierarchical navigation for quadrupedal robots using waypoints.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance the long-distance movement and navigation capabilities of quadrupedal robots by effectively integrating locomotion and navigation skills.

Method: The researchers trained a waypoint-guided locomotion policy with deep reinforcement learning, using waypoints as the interface between high-level planning and low-level locomotion.

Result: The system successfully enables quadrupedal robots to navigate complex terrains and complete challenging navigation tasks, as verified through extensive simulations and real-world experiments.

Conclusion: Skill-Nav demonstrates that integrating locomotion with navigation using waypoint-guided policies provides a flexible and effective approach for quadrupedal robots to handle diverse tasks.

Abstract: Quadrupedal robots have demonstrated exceptional locomotion capabilities
through Reinforcement Learning (RL), including extreme parkour maneuvers.
However, integrating locomotion skills with navigation in quadrupedal robots
has not been fully investigated, which holds promise for enhancing
long-distance movement capabilities. In this paper, we propose Skill-Nav, a
method that incorporates quadrupedal locomotion skills into a hierarchical
navigation framework using waypoints as an interface. Specifically, we train a
waypoint-guided locomotion policy using deep RL, enabling the robot to
autonomously adjust its locomotion skills to reach targeted positions while
avoiding obstacles. Compared with direct velocity commands, waypoints offer a
simpler yet more flexible interface for high-level planning and low-level
control. Utilizing waypoints as the interface allows for the application of
various general planning tools, such as large language models (LLMs) and path
planning algorithms, to guide our locomotion policy in traversing terrains with
diverse obstacles. Extensive experiments conducted in both simulated and
real-world scenarios demonstrate that Skill-Nav can effectively traverse
complex terrains and complete challenging navigation tasks.

</details>


### [278] [Embodied Domain Adaptation for Object Detection](https://arxiv.org/abs/2506.21860)
*Xiangyu Shi,Yanyuan Qiao,Lingqiao Liu,Feras Dayoub*

Main category: cs.RO

TL;DR: The paper presents a Source-Free Domain Adaptation (SFDA) method to enhance vision-language model-based object detection under dynamic indoor conditions without needing source data.


<details>
  <summary>Details</summary>
Motivation: Standard object detection methods struggle to handle diverse objects and dynamic indoor environments because they rely on fixed labels and closed-set methodology. OVOD attempts to address this but still faces challenges amid domain shifts.

Method: The method utilizes Source-Free Domain Adaptation techniques, including temporal clustering for pseudo-label refinement, multi-scale threshold fusion, and a Mean Teacher framework enhanced by contrastive learning.

Result: Experiments demonstrate significant improvements in zero-shot detection and the model’s adaptability to dynamic changes in indoor environments.

Conclusion: This approach effectively bolsters open-vocabulary object detection performance in mobile robots, proving adaptable to sequential and diverse indoor conditions.

Abstract: Mobile robots rely on object detectors for perception and object localization
in indoor environments. However, standard closed-set methods struggle to handle
the diverse objects and dynamic conditions encountered in real homes and labs.
Open-vocabulary object detection (OVOD), driven by Vision Language Models
(VLMs), extends beyond fixed labels but still struggles with domain shifts in
indoor environments. We introduce a Source-Free Domain Adaptation (SFDA)
approach that adapts a pre-trained model without accessing source data. We
refine pseudo labels via temporal clustering, employ multi-scale threshold
fusion, and apply a Mean Teacher framework with contrastive learning. Our
Embodied Domain Adaptation for Object Detection (EDAOD) benchmark evaluates
adaptation under sequential changes in lighting, layout, and object diversity.
Our experiments show significant gains in zero-shot detection performance and
flexible adaptation to dynamic indoor conditions.

</details>


### [279] [A MILP-Based Solution to Multi-Agent Motion Planning and Collision Avoidance in Constrained Environments](https://arxiv.org/abs/2506.21982)
*Akshay Jaitly,Jack Cline,Siavash Farzan*

Main category: cs.RO

TL;DR: A mixed-integer linear program (MILP) approach for faster multi-agent motion planning is proposed, embedding Polytopic Action-based Motion Planning (PAAMP) for efficient trajectory optimization.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the computational challenges in multi-agent motion planning, especially for scenarios with complex environments and inter-agent collision avoidance, while achieving computational efficiency.

Method: The paper embeds PAAMP into a sequence-then-solve pipeline, where agents are confined to convex polytopes and inter-agent separation is enforced via a big-M hyperplane model. Collision constraints are applied selectively to reduce binary variables exponentially. An L1 path-length-plus-acceleration cost is used for smooth trajectories.

Result: The proposed method achieves finite-time convergence and is shown to produce collision-free trajectories in multi-agent scenarios with obstacles. It is significantly faster (an order of magnitude) than an unstructured MILP baseline.

Conclusion: By reducing computational complexity and ensuring smooth and collision-free trajectories, the proposed MILP formulation demonstrates efficiency and practical feasibility for multi-agent motion planning with obstacles.

Abstract: We propose a mixed-integer linear program (MILP) for multi-agent motion
planning that embeds Polytopic Action-based Motion Planning (PAAMP) into a
sequence-then-solve pipeline. Region sequences confine each agent to adjacent
convex polytopes, while a big-M hyperplane model enforces inter-agent
separation. Collision constraints are applied only to agents sharing or
neighboring a region, which reduces binary variables exponentially compared
with naive formulations. An L1 path-length-plus-acceleration cost yields smooth
trajectories. We prove finite-time convergence and demonstrate on
representative multi-agent scenarios with obstacles that our formulation
produces collision-free trajectories an order of magnitude faster than an
unstructured MILP baseline.

</details>


### [280] [LMPVC and Policy Bank: Adaptive voice control for industrial robots with code generating LLMs and reusable Pythonic policies](https://arxiv.org/abs/2506.22028)
*Ossi Parikka,Roel Pieters*

Main category: cs.RO

TL;DR: This paper introduces Language Model Program Voice Control (LMPVC), a voice control system for robots using large language models to enable policy programming and teaching capabilities.


<details>
  <summary>Details</summary>
Motivation: Modern manufacturing trends demand advanced human-robot collaboration methods, especially voice control, due to the limitations of full automation.

Method: The paper proposes LMPVC, a system utilizing large language models with a programming and teaching system called the Policy Bank to overcome LLM limitations and enhance adaptability without extra training.

Result: The implementation of LMPVC proved the system can adapt to various tasks efficiently and demonstrated how the Policy Bank helps mitigate LLM limitations.

Conclusion: LMPVC offers a promising approach to human-robot collaboration, enabling customization and scalability in robotics tasks through voice-controlled programming and teaching.

Abstract: Modern industry is increasingly moving away from mass manufacturing, towards
more specialized and personalized products. As manufacturing tasks become more
complex, full automation is not always an option, human involvement may be
required. This has increased the need for advanced human robot collaboration
(HRC), and with it, improved methods for interaction, such as voice control.
Recent advances in natural language processing, driven by artificial
intelligence (AI), have the potential to answer this demand. Large language
models (LLMs) have rapidly developed very impressive general reasoning
capabilities, and many methods of applying this to robotics have been proposed,
including through the use of code generation. This paper presents Language
Model Program Voice Control (LMPVC), an LLM-based prototype voice control
architecture with integrated policy programming and teaching capabilities,
built for use with Robot Operating System 2 (ROS2) compatible robots. The
architecture builds on prior works using code generation for voice control by
implementing an additional programming and teaching system, the Policy Bank. We
find this system can compensate for the limitations of the underlying LLM, and
allow LMPVC to adapt to different downstream tasks without a slow and costly
training process. The architecture and additional results are released on
GitHub (https://github.com/ozzyuni/LMPVC).

</details>


### [281] [Multi-Robot Assembly of Deformable Linear Objects Using Multi-Modal Perception](https://arxiv.org/abs/2506.22034)
*Kejia Chen,Celina Dettmering,Florian Pachler,Zhuo Liu,Yue Zhang,Tailai Cheng,Jonas Dirr,Zhenshan Bing,Alois Knoll,Rüdiger Daub*

Main category: cs.RO

TL;DR: This paper introduces a framework for robotic assembly of deformable linear objects (DLOs) using visual and tactile data to plan and execute tasks in an industrial context.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the challenges in automating DLO assembly due to their complex deformation behavior, where existing work has focused on isolated tasks rather than an integrated solution.

Method: The framework integrates visual and tactile sensing to track a DLO's shape and contact states, enabling robot-based bin picking, coordinated handover, and fixture mounting in an assembly process using multiple robots.

Result: Real-world experiments with a multi-robot system confirm the effectiveness of the proposed framework for industrial DLO assembly tasks.

Conclusion: The proposed perception and planning framework provides a comprehensive solution for DLO assembly, demonstrating its practical relevance and effectiveness in industrial automation workflows.

Abstract: Industrial assembly of deformable linear objects (DLOs) such as cables offers
great potential for many industries. However, DLOs pose several challenges for
robot-based automation due to the inherent complexity of deformation and,
consequentially, the difficulties in anticipating the behavior of DLOs in
dynamic situations. Although existing studies have addressed isolated
subproblems like shape tracking, grasping, and shape control, there has been
limited exploration of integrated workflows that combine these individual
processes. To address this gap, we propose an object-centric perception and
planning framework to achieve a comprehensive DLO assembly process throughout
the industrial value chain. The framework utilizes visual and tactile
information to track the DLO's shape as well as contact state across different
stages, which facilitates effective planning of robot actions. Our approach
encompasses robot-based bin picking of DLOs from cluttered environments,
followed by a coordinated handover to two additional robots that mount the DLOs
onto designated fixtures. Real-world experiments employing a setup with
multiple robots demonstrate the effectiveness of the approach and its relevance
to industrial scenarios.

</details>


### [282] [An Introduction to Zero-Order Optimization Techniques for Robotics](https://arxiv.org/abs/2506.22087)
*Armand Jordana,Jianghan Zhang,Joseph Amigo,Ludovic Righetti*

Main category: cs.RO

TL;DR: The paper presents a mathematical tutorial on random search, offering a unified perspective for robotics optimization techniques, specifically focusing on trajectory optimization and policy optimization.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the challenges posed by non-differentiable functions and local minima in robotics optimization by leveraging zero-order techniques.

Method: They propose a unifying tutorial and classification framework for random search algorithms, connecting various approaches commonly employed in robotics.

Result: The framework allows the derivation of novel and competitive reinforcement learning algorithms, enhancing the trajectory and policy optimization methods.

Conclusion: The study establishes a common framework for understanding and advancing zero-order optimization techniques in the robotics domain.

Abstract: Zero-order optimization techniques are becoming increasingly popular in
robotics due to their ability to handle non-differentiable functions and escape
local minima. These advantages make them particularly useful for trajectory
optimization and policy optimization. In this work, we propose a mathematical
tutorial on random search. It offers a simple and unifying perspective for
understanding a wide range of algorithms commonly used in robotics. Leveraging
this viewpoint, we classify many trajectory optimization methods under a common
framework and derive novel competitive RL algorithms.

</details>


### [283] [Evaluating Pointing Gestures for Target Selection in Human-Robot Collaboration](https://arxiv.org/abs/2506.22116)
*Noora Sassali,Roel Pieters*

Main category: cs.RO

TL;DR: The paper presents a method to localize pointed targets in a planar workspace using pose estimation and a geometric model, tested in a multimodal robotic system.


<details>
  <summary>Details</summary>
Motivation: To improve human-robot collaboration by developing efficient methods for interpreting pointing gestures in tasks such as target selection and guiding processes.

Method: RGB-D stream-based pose estimation combined with a geometric shoulder-wrist extension model for gesture data extraction. The system includes object detection, speech transcription, and synthesis for testing integration.

Result: The method accurately localized pointed targets and was successfully integrated into a proof-of-concept robotic system, showcasing multimodal collaboration.

Conclusion: The proposed approach demonstrates the feasibility of gesture-based target localization in multimodal robotic systems, though limitations and performance trade-offs are discussed.

Abstract: Pointing gestures are a common interaction method used in Human-Robot
Collaboration for various tasks, ranging from selecting targets to guiding
industrial processes. This study introduces a method for localizing pointed
targets within a planar workspace. The approach employs pose estimation, and a
simple geometric model based on shoulder-wrist extension to extract gesturing
data from an RGB-D stream. The study proposes a rigorous methodology and
comprehensive analysis for evaluating pointing gestures and target selection in
typical robotic tasks. In addition to evaluating tool accuracy, the tool is
integrated into a proof-of-concept robotic system, which includes object
detection, speech transcription, and speech synthesis to demonstrate the
integration of multiple modalities in a collaborative application. Finally, a
discussion over tool limitations and performance is provided to understand its
role in multimodal robotic systems. All developments are available at:
https://github.com/NMKsas/gesture_pointer.git.

</details>


### [284] [RM-Dijkstra: A surface optimal path planning algorithm based on Riemannian metric](https://arxiv.org/abs/2506.22170)
*Yu Zhang,Xiao-Song Yang*

Main category: cs.RO

TL;DR: The paper introduces RM-Dijkstra, a novel path planning algorithm that adapts Dijkstra's method to handle surface path planning using a Riemannian metric model, achieving higher path accuracy and smoothness.


<details>
  <summary>Details</summary>
Motivation: While Dijkstra's algorithm is widely studied for path planning, its application on surfaces for mobile robots is limited. This paper aims to address that gap to enhance robotic navigation, particularly in complex environments.

Method: The authors construct a Riemannian metric on a 2D projection plane, transforming the surface path planning problem into a geometric issue. They leverage this newly defined metric for the RM-Dijkstra algorithm.

Result: Simulation tests indicate that RM-Dijkstra effectively solves surface path planning problems, showing improved path accuracy and smoothness over traditional algorithms, particularly in challenging scenarios.

Conclusion: RM-Dijkstra extends Dijkstra's algorithm to surface environments by incorporating a Riemannian metric model, advancing robotic path planning with enhanced precision and adaptability.

Abstract: The Dijkstra algorithm is a classic path planning method, which operates in a
discrete graph space to determine the shortest path from a specified source
point to a target node or all other nodes based on non-negative edge weights.
Numerous studies have focused on the Dijkstra algorithm due to its potential
application. However, its application in surface path planning for mobile
robots remains largely unexplored. In this letter, a surface optimal path
planning algorithm called RM-Dijkstra is proposed, which is based on Riemannian
metric model. By constructing a new Riemannian metric on the 2D projection
plane, the surface optimal path planning problem is therefore transformed into
a geometric problem on the 2D plane with new Riemannian metric. Induced by the
standard Euclidean metric on surface, the constructed new metric reflects
environmental information of the robot and ensures that the projection map is
an isometric immersion. By conducting a series of simulation tests, the
experimental results demonstrate that the RM-Dijkstra algorithm not only
effectively solves the optimal path planning problem on surfaces, but also
outperforms traditional path planning algorithms in terms of path accuracy and
smoothness, particularly in complex scenarios.

</details>


### [285] [ASVSim (AirSim for Surface Vehicles): A High-Fidelity Simulation Framework for Autonomous Surface Vehicle Research](https://arxiv.org/abs/2506.22174)
*Bavo Lesy,Siemen Herremans,Robin Kerstens,Jan Steckel,Walter Daems,Siegfried Mercelis,Ali Anwar*

Main category: cs.RO

TL;DR: The paper introduces ASVSim, an open-source simulation framework for autonomous surface vehicle research in inland and port environments, addressing the lack of high-fidelity tools.


<details>
  <summary>Details</summary>
Motivation: There's an increasing demand for unmanned surface vehicles in port and inland waterways due to efficiency, safety needs, the Green Deal initiative, and a shortage of qualified personnel.

Method: The paper develops ASVSim, a simulation platform combining vessel dynamics and marine sensor simulation (e.g., radar, cameras), enabling research in traditional and deep learning control methods.

Result: The authors demonstrate ASVSim's potential through experiments, emphasizing its utility in generating datasets for training algorithms and advancing autonomous navigation research.

Conclusion: ASVSim is open-source and aims to broaden access to autonomous navigation research within the ocean engineering community.

Abstract: The transport industry has recently shown significant interest in unmanned
surface vehicles (USVs), specifically for port and inland waterway transport.
These systems can improve operational efficiency and safety, which is
especially relevant in the European Union, where initiatives such as the Green
Deal are driving a shift towards increased use of inland waterways. At the same
time, a shortage of qualified personnel is accelerating the adoption of
autonomous solutions. However, there is a notable lack of open-source,
high-fidelity simulation frameworks and datasets for developing and evaluating
such solutions. To address these challenges, we introduce AirSim For Surface
Vehicles (ASVSim), an open-source simulation framework specifically designed
for autonomous shipping research in inland and port environments. The framework
combines simulated vessel dynamics with marine sensor simulation capabilities,
including radar and camera systems and supports the generation of synthetic
datasets for training computer vision models and reinforcement learning agents.
Built upon Cosys-AirSim, ASVSim provides a comprehensive platform for
developing autonomous navigation algorithms and generating synthetic datasets.
The simulator supports research of both traditional control methods and deep
learning-based approaches. Through limited experiments, we demonstrate the
potential of the simulator in these research areas. ASVSim is provided as an
open-source project under the MIT license, making autonomous navigation
research accessible to a larger part of the ocean engineering community.

</details>


### [286] [KnotDLO: Toward Interpretable Knot Tying](https://arxiv.org/abs/2506.22176)
*Holly Dinkel,Raghavendra Navaratna,Jingyi Xiang,Brian Coltin,Trey Smith,Timothy Bretl*

Main category: cs.RO

TL;DR: KnotDLO is a robotic method for one-handed knot tying of deformable linear objects (DLOs) offering robustness to occlusion, generalization to new rope configurations, and requiring no human training.


<details>
  <summary>Details</summary>
Motivation: To develop a robust, interpretable, and autonomous method for knot-tying of deformable objects without relying on human demonstrations or training.

Method: KnotDLO plans future DLO states by utilizing the current DLO shape to compute waypoints and grasp poses, while decoupling visual reasoning from control.

Result: KnotDLO achieved a 50% success rate in tying an overhand knot in 16 trials using previously unseen configurations.

Conclusion: The approach demonstrates feasibility in automating DLO knot-tying, but further improvements in robustness are needed.

Abstract: This work presents KnotDLO, a method for one-handed Deformable Linear Object
(DLO) knot tying that is robust to occlusion, repeatable for varying rope
initial configurations, interpretable for generating motion policies, and
requires no human demonstrations or training. Grasp and target waypoints for
future DLO states are planned from the current DLO shape. Grasp poses are
computed from indexing the tracked piecewise linear curve representing the DLO
state based on the current curve shape and are piecewise continuous. KnotDLO
computes intermediate waypoints from the geometry of the current DLO state and
the desired next state. The system decouples visual reasoning from control. In
16 trials of knot tying, KnotDLO achieves a 50% success rate in tying an
overhand knot from previously unseen configurations.

</details>


### [287] [Robotic Multimodal Data Acquisition for In-Field Deep Learning Estimation of Cover Crop Biomass](https://arxiv.org/abs/2506.22364)
*Joe Johnson,Phanender Chalasani,Arnav Shah,Ram L. Ray,Muthukumar Bagavathiannan*

Main category: cs.RO

TL;DR: The paper introduces a ground robot-mounted multimodal sensor system that uses optical imagery, LiDAR data, and machine learning for accurate aboveground biomass estimation to improve weed management.


<details>
  <summary>Details</summary>
Motivation: Addressing crop yield losses caused by weeds and enhancing weed suppression strategies through optimized cover crop management using precise aboveground biomass mapping.

Method: A ground robot-mounted multimodal sensor system was developed, integrating optical and LiDAR data with machine learning methods for data fusion, to predict and map cover crop biomass.

Result: The best machine learning model achieved a coefficient of determination of 0.88 for dry aboveground biomass estimation, showing strong performance across variable field conditions.

Conclusion: This system provides a precise and efficient tool for site-specific weed management, contributing to more sustainable agricultural practices.

Abstract: Accurate weed management is essential for mitigating significant crop yield
losses, necessitating effective weed suppression strategies in agricultural
systems. Integrating cover crops (CC) offers multiple benefits, including soil
erosion reduction, weed suppression, decreased nitrogen requirements, and
enhanced carbon sequestration, all of which are closely tied to the aboveground
biomass (AGB) they produce. However, biomass production varies significantly
due to microsite variability, making accurate estimation and mapping essential
for identifying zones of poor weed suppression and optimizing targeted
management strategies. To address this challenge, developing a comprehensive CC
map, including its AGB distribution, will enable informed decision-making
regarding weed control methods and optimal application rates. Manual visual
inspection is impractical and labor-intensive, especially given the extensive
field size and the wide diversity and variation of weed species and sizes. In
this context, optical imagery and Light Detection and Ranging (LiDAR) data are
two prominent sources with unique characteristics that enhance AGB estimation.
This study introduces a ground robot-mounted multimodal sensor system designed
for agricultural field mapping. The system integrates optical and LiDAR data,
leveraging machine learning (ML) methods for data fusion to improve biomass
predictions. The best ML-based model for dry AGB estimation achieved a
coefficient of determination value of 0.88, demonstrating robust performance in
diverse field conditions. This approach offers valuable insights for
site-specific management, enabling precise weed suppression strategies and
promoting sustainable farming practices.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [288] [How (Not) To Write a Software Engineering Abstract](https://arxiv.org/abs/2506.21634)
*Lutz Prechelt,Lloyd Montgomery,Julian Frattini,Franz Zieris*

Main category: cs.SE

TL;DR: Abstracts in software engineering research often lack informativeness and completeness, with only a small percentage meeting high standards for readability and structured content.


<details>
  <summary>Details</summary>
Motivation: To characterize and improve the structural and content quality of abstracts in high-quality software engineering research venues.

Method: Qualitative open coding to derive abstract properties, quantitative content analysis of 362 abstracts, exploratory data analysis, and comparison with archetypical abstract structures.

Result: 29% of abstracts are complete; only 4% are fully proper in readability and informativeness. Structured abstracts perform significantly better than unstructured ones.

Conclusion: The majority of abstracts in top venues are suboptimal. Structured abstracts are preferable, artifact-centric works require specialized formats, and generalizable conclusions should be encouraged.

Abstract: Background: Abstracts are a particularly valuable element in a software
engineering research article. However, not all abstracts are as informative as
they could be. Objective: Characterize the structure of abstracts in
high-quality software engineering venues. Observe and quantify deficiencies.
Suggest guidelines for writing informative abstracts. Methods: Use qualitative
open coding to derive concepts that explain relevant properties of abstracts.
Identify the archetypical structure of abstracts. Use quantitative content
analysis to objectively characterize abstract structure of a sample of 362
abstracts from five presumably high-quality venues. Use exploratory data
analysis to find recurring issues in abstracts. Compare the archetypical
structure to actual structures. Infer guidelines for producing informative
abstracts. Results: Only 29% of the sampled abstracts are complete, i.e.,
provide background, objective, method, result, and conclusion information. For
structured abstracts, the ratio is twice as big. Only 4% of the abstracts are
proper, i.e., they also have good readability (Flesch-Kincaid score) and have
no informativeness gaps, understandability gaps, nor highly ambiguous
sentences. Conclusions: (1) Even in top venues, a large majority of abstracts
are far from ideal. (2) Structured abstracts tend to be better than
unstructured ones. (3) Artifact-centric works need a different structured
format. (4) The community should start requiring conclusions that generalize,
which currently are often missing in abstracts.

</details>


### [289] [Experience converting a large mathematical software package written in C++ to C++20 modules](https://arxiv.org/abs/2506.21654)
*Wolfgang Bangerth*

Main category: cs.SE

TL;DR: The paper explores transitioning large C++ mathematical software, like the deal.II finite element library, from header-based to module-based interfaces to improve usability and compile times.


<details>
  <summary>Details</summary>
Motivation: The traditional C++ method of using #include for exporting interfaces is cumbersome, inefficient, and outdated compared to modern programming languages.

Method: The author evaluates a strategy to convert a large C++ codebase to the C++20 module system while maintaining compatibility with header-based systems.

Result: The conversion reduces the library's own compilation time but shows no clear compile-time improvement for downstream projects.

Conclusion: While transitioning to modules requires effort, the technique is viable and beneficial, laying groundwork for a long-term ecosystem-wide shift in mathematical software.

Abstract: Mathematical software has traditionally been built in the form of "packages"
that build on each other. A substantial fraction of these packages is written
in C++ and, as a consequence, the interface of a package is described in the
form of header files that downstream packages and applications can then
#include. C++ has inherited this approach towards exporting interfaces from C,
but the approach is clunky, unreliable, and slow. As a consequence, C++20 has
introduced a "module" system in which packages explicitly export declarations
and code that compilers then store in machine-readable form and that downstream
users can "import" -- a system in line with what many other programming
languages have used for decades.
  Herein, I explore how one can convert large mathematical software packages
written in C++ to this system, using the deal.II finite element library with
its around 800,000 lines of code as an example. I describe an approach that
allows providing both header-based and module-based interfaces from the same
code base, discuss the challenges one encounters, and how modules actually work
in practice in a variety of technical and human metrics. The results show that
with a non-trivial, but also not prohibitive effort, the conversion to modules
is possible, resulting in a reduction in compile time for the converted library
itself; on the other hand, for downstream projects, compile times show no clear
trend. I end with thoughts about long-term strategies for converting the entire
ecosystem of mathematical software over the coming years or decades.

</details>


### [290] [Can Large Language Models Help Students Prove Software Correctness? An Experimental Study with Dafny](https://arxiv.org/abs/2506.22370)
*Carolina Carreira,Álvaro Silva,Alexandre Abreu,Alexandra Mendes*

Main category: cs.SE

TL;DR: This paper examines how students use ChatGPT for deductive program verification and finds that it improves performance, but success depends on prompt quality.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand the role of large language models like ChatGPT in aiding complex tasks, such as deductive program verification, in computing education.

Method: The study involved master's students solving formal verification problems in Dafny, with and without ChatGPT assistance, while interactions were logged and analyzed.

Result: Students achieved significantly better performance with the use of ChatGPT, but their success was closely tied to the quality of their prompts.

Conclusion: ChatGPT can be effectively integrated into formal methods courses to enhance learning, provided that challenges are designed to encourage learning rather than simple replacement of student effort.

Abstract: Students in computing education increasingly use large language models (LLMs)
such as ChatGPT. Yet, the role of LLMs in supporting cognitively demanding
tasks, like deductive program verification, remains poorly understood. This
paper investigates how students interact with an LLM when solving formal
verification exercises in Dafny, a language that supports functional
correctness, by allowing programmers to write formal specifications and
automatically verifying that the implementation satisfies the specification. We
conducted a mixed-methods study with master's students enrolled in a formal
methods course. Each participant completed two verification problems, one with
access to a custom ChatGPT interface, that logged all interactions, and the
other without. We identified strategies used by successful students and
assessed the level of trust students place in LLMs. %\todo{Our findings show
that something here} Our findings show that students perform significantly
better when using ChatGPT; however, performance gains are tied to prompt
quality. We conclude with practical recommendations for integrating LLMs into
formal methods courses more effectively, including designing LLM-aware
challenges that promote learning rather than substitution.

</details>


### [291] [The DevSafeOps Dilemma: A Systematic Literature Review on Rapidity in Safe Autonomous Driving Development and Operation](https://arxiv.org/abs/2506.21693)
*Ali Nouri,Beatriz Cabrero-Daniel,Fredrik Törner,Christian Berger*

Main category: cs.SE

TL;DR: A systematic literature review on integrating DevOps into autonomous driving development highlights current challenges and solutions for safe AI-enabled functions, but notes open topics yet to be addressed.


<details>
  <summary>Details</summary>
Motivation: Autonomous driving systems require safe and reliable operation, while meeting continuous AI advancement and rapid incident response demands. DevOps may streamline this complex process.

Method: A systematic literature review was conducted to analyze existing studies on the application of DevOps in autonomous driving technology development.

Result: The review aggregated challenges and solutions concerning DevOps in safety-related AI-enabled functions, revealing gaps that need addressing for safe DevOps in AD systems.

Conclusion: While promising, DevOps integration with autonomous driving needs further investigation to ensure safety, reliability, and comprehensive technological adaptation.

Abstract: Developing autonomous driving (AD) systems is challenging due to the
complexity of the systems and the need to assure their safe and reliable
operation. The widely adopted approach of DevOps seems promising to support the
continuous technological progress in AI and the demand for fast reaction to
incidents, which necessitate continuous development, deployment, and
monitoring. We present a systematic literature review meant to identify,
analyse, and synthesise a broad range of existing literature related to usage
of DevOps in autonomous driving development. Our results provide a structured
overview of challenges and solutions, arising from applying DevOps to
safety-related AI-enabled functions. Our results indicate that there are still
several open topics to be addressed to enable safe DevOps for the development
of safe AD.

</details>


### [292] [Using Generative AI in Software Design Education: An Experience Report](https://arxiv.org/abs/2506.21703)
*Victoria Jackson,Susannah Liu,Andre van der Hoek*

Main category: cs.SE

TL;DR: The paper explores using Generative AI (ChatGPT) in an undergraduate software design class, analyzing its benefits and challenges through student feedback.


<details>
  <summary>Details</summary>
Motivation: To address the gap in research on how Generative AI tools can be utilized in software development education, beyond just teaching coding skills.

Method: Students in a software design class used ChatGPT to complete a team assignment. Data from their interaction logs and reflections were qualitatively analyzed.

Result: Students found ChatGPT helpful in the design process but emphasized the importance of critically evaluating its responses. Key insights for educators were also identified.

Conclusion: Generative AI tools like ChatGPT can enhance software design education by aiding the learning process and providing insights into their limitations, with proper guidance from educators.

Abstract: With the rapid adoption of Generative AI (GenAI) tools, software engineering
educators have grappled with how best to incorporate them into the classroom.
While some research discusses the use of GenAI in the context of learning to
code, there is little research that explores the use of GenAI in the classroom
for other areas of software development. This paper provides an experience
report on introducing GenAI into an undergraduate software design class.
Students were required to use GenAI (in the form of ChatGPT) to help complete a
team-based assignment. The data collected consisted of the ChatGPT conversation
logs and students' reflections on using ChatGPT for the assignment.
Subsequently, qualitative analysis was undertaken on the data. Students
identified numerous ways ChatGPT helped them in their design process while
recognizing the need to critique the response before incorporating it into
their design. At the same time, we identified several key lessons for educators
in how to deploy GenAI in a software design class effectively. Based on our
experience, we believe students can benefit from using GenAI in software design
education as it helps them design and learn about the strengths and weaknesses
of GenAI.

</details>


### [293] [KARMA Approach supporting Development Process Reconstruction in Model-based Systems Engineering](https://arxiv.org/abs/2506.22037)
*Jiawei Li,Zan Liang,Guoxin Wang,Jinzhi Lu,Yan Yan,Shouxuan Wu,Hao Wang*

Main category: cs.SE

TL;DR: The paper proposes a model reconstruction method using the KARMA language and natural language processing techniques to enhance design efficiency in model-based systems engineering.


<details>
  <summary>Details</summary>
Motivation: To address the lack of an effective method for managing changes in development requirements and reconstructing the system development process model during iterative design.

Method: Uses KARMA language for formalizing process models, introduces a reconstruction framework analyzing requirements via NLP, and implements structural reorganization and optimization algorithms.

Result: The method successfully reconstructs the development process of an aircraft onboard maintenance system, improving design efficiency.

Conclusion: The proposed method effectively manages changes and optimizes the design process, proving its potential in improving model-based systems engineering workflows.

Abstract: Model reconstruction is a method used to drive the development of complex
system development processes in model-based systems engineering. Currently,
during the iterative design process of a system, there is a lack of an
effective method to manage changes in development requirements, such as
development cycle requirements and cost requirements, and to realize the
reconstruction of the system development process model. To address these
issues, this paper proposes a model reconstruction method to support the
development process model. Firstly, the KARMA language, based on the GOPPRR-E
metamodeling method, is utilized to uniformly formalize the process models
constructed based on different modeling languages. Secondly, a model
reconstruction framework is introduced. This framework takes a structured
development requirements based natural language as input, employs natural
language processing techniques to analyze the development requirements text,
and extracts structural and optimization constraint information. Then, after
structural reorganization and algorithm optimization, a development process
model that meets the development requirements is obtained. Finally, as a case
study, the development process of the aircraft onboard maintenance system is
reconstructed. The results demonstrate that this method can significantly
enhance the design efficiency of the development process.

</details>


### [294] [Autonomic Microservice Management via Agentic AI and MAPE-K Integration](https://arxiv.org/abs/2506.22185)
*Matteo Esposito,Alexander Bakhtin,Noman Ahmad,Mikel Robredo,Ruoyu Su,Valentina Lenarduzzi,Davide Taibi*

Main category: cs.SE

TL;DR: The paper introduces an AI-driven framework based on MAPE-K for autonomous management of microservices, aimed at addressing security and stability challenges.


<details>
  <summary>Details</summary>
Motivation: To tackle the security, management, and stability challenges posed by the decentralized nature of microservices in cloud computing.

Method: A framework based on the MAPE-K model utilizing agentic AI to enable autonomous anomaly detection and remediation in microservices systems.

Result: The proposed framework offers customizable, industry-ready solutions that enhance system stability, resilience, performance, and anomaly management while reducing downtime.

Conclusion: The framework equips practitioners and researchers with tools to ensure robust, secure, and high-performing microservices systems in cloud environments.

Abstract: While microservices are revolutionizing cloud computing by offering
unparalleled scalability and independent deployment, their decentralized nature
poses significant security and management challenges that can threaten system
stability. We propose a framework based on MAPE-K, which leverages agentic AI,
for autonomous anomaly detection and remediation to address the daunting task
of highly distributed system management. Our framework offers practical,
industry-ready solutions for maintaining robust and secure microservices.
Practitioners and researchers can customize the framework to enhance system
stability, reduce downtime, and monitor broader system quality attributes such
as system performance level, resilience, security, and anomaly management,
among others.

</details>


### [295] [What Makes ChatGPT Effective for Software Issue Resolution? An Empirical Study of Developer-ChatGPT Conversations in GitHub](https://arxiv.org/abs/2506.22390)
*Ramtin Ehsani,Sakshi Pathak,Esteban Parra,Sonia Haiduc,Preetha Chatterjee*

Main category: cs.SE

TL;DR: The study examines 686 GitHub developer-ChatGPT conversations to identify factors that make these interactions useful for issue resolution. Only 62% were deemed helpful, mainly for tasks like code generation and API recommendations, but deficiencies like incorrect information remain prevalent.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to improve the utility of large-language models like ChatGPT in resolving developer issues by identifying characteristics of effective conversations and pinpointing weaknesses.

Method: The method involves analyzing 686 GitHub developer-ChatGPT conversations to categorize tasks, measure conversational/project/issue-related metrics, and uncover deficiencies in unhelpful responses.

Result: The study reveals that helpful ChatGPT conversations are shorter, more readable, and semantically aligned. ChatGPT performs best on simpler, well-scoped tasks and benefits larger projects and experienced developers. Common deficiencies include incorrect information and lack of comprehensiveness.

Conclusion: The findings provide actionable insights for improving LLM interaction strategies, refining prompt designs, and fine-tuning LLMs to better assist developers in issue resolution tasks.

Abstract: Conversational large-language models are extensively used for issue
resolution tasks. However, not all developer-LLM conversations are useful for
effective issue resolution. In this paper, we analyze 686 developer-ChatGPT
conversations shared within GitHub issue threads to identify characteristics
that make these conversations effective for issue resolution. First, we analyze
the conversations and their corresponding issues to distinguish helpful from
unhelpful conversations. We begin by categorizing the types of tasks developers
seek help with to better understand the scenarios in which ChatGPT is most
effective. Next, we examine a wide range of conversational, project, and
issue-related metrics to uncover factors associated with helpful conversations.
Finally, we identify common deficiencies in unhelpful ChatGPT responses to
highlight areas that could inform the design of more effective developer-facing
tools. We found that only 62% of the ChatGPT conversations were helpful for
successful issue resolution. ChatGPT is most effective for code generation and
tools/libraries/APIs recommendations, but struggles with code explanations.
Helpful conversations tend to be shorter, more readable, and exhibit stronger
semantic and linguistic alignment. Larger, more popular projects and more
experienced developers benefit more from ChatGPT. At the issue level, ChatGPT
performs best on simpler problems with limited developer activity and faster
resolution, typically well-scoped tasks like compilation errors. The most
common deficiencies in unhelpful ChatGPT responses include incorrect
information and lack of comprehensiveness. Our findings have wide implications
including guiding developers on effective interaction strategies for issue
resolution, informing the development of tools or frameworks to support optimal
prompt design, and providing insights on fine-tuning LLMs for issue resolution
tasks.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [296] [Finding Similar Objects and Active Inference for Surprise in Numenta Neocortex Model](https://arxiv.org/abs/2506.21554)
*Hajime Kawakami*

Main category: q-bio.NC

TL;DR: The paper explores algorithms for the thousand-brains system, focusing on similarity and inference using the free-energy principle, addressing inverse problems in Bayesian and non-Bayesian approaches.


<details>
  <summary>Details</summary>
Motivation: To enhance artificial intelligence by investigating the thousand-brains system inspired by the neocortex, focusing on prediction and association capabilities like similarity.

Method: Developing and proposing algorithms for the thousand-brains system to handle similarity detection and inference, utilizing principles such as the free-energy principle and Bayesian inference.

Result: The study delivers algorithms capable of solving inverse problems through Bayesian and non-Bayesian updates, providing insights into similarity and causative inference.

Conclusion: The algorithms proposed contribute toward understanding how the thousand-brains system can advance artificial intelligence, especially in sensory data interpretation and relational associations like similarity.

Abstract: Jeff Hawkins and his colleagues in Numenta have proposed the thousand-brains
system. This is a model of the structure and operation of the neocortex and is
under investigation as a new form of artificial intelligence. In their study,
learning and inference algorithms running on the system are proposed, where the
prediction is an important function. The author believes that one of the most
important capabilities of the neocortex in addition to prediction is the
ability to make association, that is, to find the relationships between
objects. Similarity is an important example of such relationships. In our
study, algorithms that run on the thousand-brains system to find similarities
are proposed. Although the setting for these algorithms is restricted, the
author believes that the case it covers is fundamental. Karl Friston and his
colleagues have studied the free-energy principle that explains how the brain
actively infers the cause of a Shannon surprise. In our study, an algorithm is
proposed for the thousand-brains system to make this inference. The problem of
inferring what is being observed from the sensory data is a type of inverse
problem, and the inference algorithms of the thousand-brains system and
free-energy principle solve this problem in a Bayesian manner. Our inference
algorithms can also be interpreted as Bayesian or non-Bayesian updating
processes.

</details>


### [297] [The Spatiotemporal Organization of Motor Cortex Activity Supporting Manual Dexterity](https://arxiv.org/abs/2506.21738)
*Nicholas Chehade*

Main category: q-bio.NC

TL;DR: The study explores how the motor cortex (M1) is organized and functions during reach-to-grasp movements using macaque monkeys. It identifies distinct spatial principles for reaching and grasping activities in the M1 forelimb representation.


<details>
  <summary>Details</summary>
Motivation: To investigate the spatial and dynamic organization of M1 activity during reach-to-grasp movements and better understand its functional role in voluntary motor control.

Method: Using macaque monkeys, the study applied intracortical microstimulation (ICMS) to define the M1 motor map, intrinsic signal optical imaging (ISOI) during instructed tasks, and laminar multielectrode recordings to capture unit activity from the M1 forelimb representation in various session locations.

Result: Experimental imaging found activity for reach-to-grasp movements is concentrated in specific patches, covering less than half of the M1 forelimb region. Electrophysiological recordings showed that reaching and grasping involve distinct spatially organized activities within M1.

Conclusion: The research provides evidence that M1 has inherent spatial principles, organizing distinct neural patterns for reaching and grasping, enhancing our understanding of neural mechanisms behind complex motor tasks like reach-to-grasp movements.

Abstract: Motor cortex (M1) is a crucial brain area for controlling voluntary
movements, such as reaching and grasping for a cup of coffee. M1 is organized
in a somatotopic manner, such that M1 output driving movement to different
parts of the body is organized along the cortical surface. In primates, the arm
and hand are represented in M1 as separate but overlapping territories. Unit
activity recorded from the M1 forelimb representation comodulates with
parameters related to reaching and/or grasping. The overall aim of this
dissertation is to understand the spatiotemporal dynamics of M1 activity that
produces reach-to-grasp movements. To address this goal, intracortical
microstimulation (ICMS) is delivered along the precentral gyrus of two macaque
monkeys to define the M1 motor map. Subsequently, cortical activity is recorded
from the M1 forelimb representation using intrinsic signal optical imaging
(ISOI) while macaques execute an instructed reach-to-grasp task. Results from
imaging experiments produce spatial maps that define cortical territories with
increased activity during reach-to-grasp movements. Next, unit activity was
recorded from the M1 forelimb representation with a laminar multielectrode
while macaques completed the same reach-to-grasp task. Recording site locations
differed between sessions to comprehensively sample unit responses throughout
the M1 forelimb representation. Imaging experiments reveal that activity
supporting reach-to-grasp movements was concentrated in patches that comprise
less than half of the M1 forelimb representation. Electrophysiology recordings
reveal that activity related to reaching is spatially organized within M1
distinctly from activity related to grasping. The results support the idea that
spatial organizing principles are inherent in M1 activity that supports
reach-to-grasp movements.

</details>


### [298] [Recency effect disappears when information is integrated from independent perceptual sources](https://arxiv.org/abs/2506.21781)
*Sepide Bagheri,Mehdi Keramati,Reza Ebrahimpour,Sajjad Zabbah*

Main category: q-bio.NC

TL;DR: This study explores how individuals integrate sensory evidence from different sources over time, revealing that independency of perceptual sources affects decision-making.


<details>
  <summary>Details</summary>
Motivation: To understand how people accumulate and integrate discrete sensory evidence over time, especially when dealing with distinct sources and temporal variations.

Method: Participants performed tasks involving random dot motion stimuli with single- and double-pulse trials that varied in coherence, source consistency, and time gaps.

Result: Participants integrated information effectively despite gaps or source types. Sequence effects appeared when sources were consistent but not when sources were orthogonal. Confidence levels were higher with orthogonal sources.

Conclusion: Perceptual independency plays a critical role in sensory information integration and its influence on decision-making.

Abstract: Decision-making often involves integrating discrete pieces of information
from distinct sources over time, yet the cognitive mechanisms underlying this
integration remain unclear. In this study, we examined how individuals
accumulate and integrate discrete sensory evidence. Participants performed a
task involving random dot motion stimuli presented in single- and double-pulse
trials. These stimuli varied in motion coherence, source consistency of pulses
(either the same or orthogonal directions), and temporal gaps between pulses.
We found that participants effectively integrated information regardless of
source type or temporal gaps. As expected, when both pulses originated from the
same source, performance showed a sequence-dependent effect-accuracy was
influenced by the order of pulse presentation. However, this effect disappeared
when the pulses came from orthogonal sources. Confidence judgments were
similarly unaffected by temporal gaps or pulse sequence but were higher when
information originated from orthogonal sources. These findings highlight the
specific role of perceptual independency on information integration and
consequently, on decision-making.

</details>


### [299] [Fetal Sleep: A Cross-Species Review of Physiology, Measurement, and Classification](https://arxiv.org/abs/2506.21828)
*Weitao Tang,Johann Vargas-Calixto,Nasim Katebi,Robert Galinsky,Gari D. Clifford,Faezeh Marzbanrad*

Main category: q-bio.NC

TL;DR: The paper reviews decades of research on fetal sleep patterns, its physiological traits, and its significance in prenatal neurodevelopment.


<details>
  <summary>Details</summary>
Motivation: Fetal sleep is critical for understanding early brain maturation and identifying neurological issues like fetal hypoxia or growth restrictions.

Method: The review synthesizes studies on fetal sleep in humans and animals, explores invasive and non-invasive monitoring methods, and evaluates computational techniques for sleep-state classification.

Result: Species-specific differences and sleep-state analogs were highlighted. Computational methods for sleep monitoring and the impact of intrauterine conditions on sleep were elaborated.

Conclusion: The review emphasizes the need for advanced, non-invasive fetal sleep monitoring technologies to improve early diagnoses and support prenatal care.

Abstract: Fetal sleep is a relatively underexplored yet vital aspect of prenatal
neurodevelopment. Understanding fetal sleep patterns could provide insights
into early brain maturation and help clinicians detect signs of neurological
compromise that arise due to fetal hypoxia or fetal growth restriction. This
review synthesizes over eight decades of research on the physiological
characteristics, ontogeny, and regulation of fetal sleep. We compare
sleep-state patterns in humans and large animal models, highlighting
species-specific differences and the presence of sleep-state analogs. We review
both invasive techniques in animals and non-invasive modalities in humans.
Computational methods for sleep-state classification are also examined,
including rule-based approaches (with and without clustering-based
preprocessing) and state-of-the-art deep learning techniques. Finally, we
discuss how intrauterine conditions such as hypoxia and fetal growth
restriction can disrupt fetal sleep. This review provides a comprehensive
foundation for the development of objective, multimodal, and non-invasive fetal
sleep monitoring technologies to support early diagnosis and intervention in
prenatal care.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [300] [Modification of a Numerical Method Using FIR Filters in a Time-dependent SIR Model for COVID-19](https://arxiv.org/abs/2506.21739)
*Felipe Rogério Pimentel,Rafael Gustavo Alves*

Main category: stat.ML

TL;DR: This paper modifies the algorithm of Chen et al. to improve the accuracy of predicting and tracking COVID-19 infections using FIR filters and machine learning methods.


<details>
  <summary>Details</summary>
Motivation: To improve upon an existing algorithm (by Chen et al.) for predicting COVID-19 infections and recoveries using FIR filters, aiming for more accurate approximations during the early pandemic period.

Method: The authors propose modifying the FIR filter algorithm by setting different values for filter orders and regularization parameters than those used by Chen et al., and applying the modified algorithm to the state of Minas Gerais/Brazil.

Result: The modified algorithm produced better approximation errors in some simulations compared to the original algorithm of Chen et al., as demonstrated through comparisons with real COVID-19 data.

Conclusion: The proposed modifications to the FIR filter algorithm improved prediction accuracy and demonstrated potential as an enhancement to pandemic modeling techniques.

Abstract: Authors Yi-Cheng Chen, Ping-En Lu, Cheng-Shang Chang, and Tzu-Hsuan Liu use
the Finite Impulse Response (FIR) linear system filtering method to track and
predict the number of people infected and recovered from COVID-19, in a
pandemic context in which there was still no vaccine and the only way to avoid
contagion was isolation. To estimate the coefficients of these FIR filters,
Chen et al. used machine learning methods through a classical optimization
problem with regularization (ridge regression). These estimated coefficients
are called ridge coefficients. The epidemic mathematical model adopted by these
researchers to formulate the FIR filters is the time-dependent discrete SIR. In
this paper, we propose a small modification to the algorithm of Chen et al. to
obtain the ridge coefficients. We then used this modified algorithm to track
and predict the number of people infected and recovered from COVID-19 in the
state of Minas Gerais/Brazil, within a prediction window, during the initial
period of the pandemic. We also compare the predicted data with the respective
real data to check how good the approximation is. In the modified algorithm, we
set values for the FIR filter orders and for the regularization parameters,
both different from the respective values defined by Chen et al. in their
algorithm. In this context, the numerical results obtained by the modified
algorithm in some simulations present better approximation errors compared to
the respective approximation errors presented by the algorithm of Chen et al.

</details>


### [301] [Critically-Damped Higher-Order Langevin Dynamics](https://arxiv.org/abs/2506.21741)
*Benjamin Sterling,Chad Gueli,Mónica F. Bugallo*

Main category: stat.ML

TL;DR: This paper explores the addition of critical damping to Higher-Order Langevin Dynamics (HOLD) to advance generative AI methods.


<details>
  <summary>Details</summary>
Motivation: To enhance generative AI methods by incorporating critical damping, which has been successful in other Langevin Dynamics setups, into the dynamics of arbitrary order.

Method: The authors generalize the state-of-the-art Higher-Order Langevin Dynamics (HOLD) by integrating principles of critical damping from systems analysis.

Result: New insights on how critical damping impacts higher-order diffusion dynamics, likely improving generative models.

Conclusion: The work establishes the feasibility of introducing critical damping to HOLD, potentially advancing generative AI capabilities.

Abstract: Denoising Diffusion Probabilistic Models represent an entirely new class of
generative AI methods that have yet to be fully explored. Critical damping has
been successfully introduced in Critically-Damped Langevin Dynamics (CLD) and
Critically-Damped Third-Order Langevin Dynamics (TOLD++), but has not yet been
applied to dynamics of arbitrary order. The proposed line of work generalizes
Higher-Order Langevin Dynamics (HOLD), a recent state-of-the-art diffusion
method, by introducing the concept of critical damping from systems analysis.

</details>


### [302] [TADA: Improved Diffusion Sampling with Training-free Augmented Dynamics](https://arxiv.org/abs/2506.21757)
*Tianrong Chen,Huangjie Zheng,David Berthelot,Jiatao Gu,Josh Susskind,Shuangfei Zhai*

Main category: stat.ML

TL;DR: The paper introduces a faster ODE-based sampling method for diffusion models, achieving up to 186% speedup without training and maintaining high image quality.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency in sampling speeds of diffusion models and enhance their applicability without compromising on image quality, particularly in high-fidelity generations.

Method: The paper proposes leveraging a novel ODE solver that employs higher-dimensional initial noise to reduce function evaluations while maintaining or enhancing detail. The method controls detail levels with a simple hyper-parameter.

Result: The proposed method achieves substantial improvements, being 186% faster than prior state-of-the-art solvers for comparative FID on ImageNet512, and demonstrates robust performance across various pretrained diffusion models.

Conclusion: The method is a significant advancement in sampling for diffusion models, marrying efficiency and quality without requiring retraining, and has broad applicability across various model settings, including pixel and latent spaces.

Abstract: Diffusion models have demonstrated exceptional capabilities in generating
high-fidelity images but typically suffer from inefficient sampling. Many
solver designs and noise scheduling strategies have been proposed to
dramatically improve sampling speeds. In this paper, we introduce a new
sampling method that is up to $186\%$ faster than the current state of the art
solver for comparative FID on ImageNet512. This new sampling method is
training-free and uses an ordinary differential equation (ODE) solver. The key
to our method resides in using higher-dimensional initial noise, allowing to
produce more detailed samples with less function evaluations from existing
pretrained diffusion models. In addition, by design our solver allows to
control the level of detail through a simple hyper-parameter at no extra
computational cost. We present how our approach leverages momentum dynamics by
establishing a fundamental equivalence between momentum diffusion models and
conventional diffusion models with respect to their training paradigms.
Moreover, we observe the use of higher-dimensional noise naturally exhibits
characteristics similar to stochastic differential equations (SDEs). Finally,
we demonstrate strong performances on a set of representative pretrained
diffusion models, including EDM, EDM2, and Stable-Diffusion 3, which cover
models in both pixel and latent spaces, as well as class and text conditional
settings. The code is available at https://github.com/apple/ml-tada.

</details>


### [303] [Classification with Reject Option: Distribution-free Error Guarantees via Conformal Prediction](https://arxiv.org/abs/2506.21802)
*Johan Hallberg Szabadváry,Tuwe Löfström,Ulf Johansson,Cecilia Sönströd,Ernst Ahlberg,Lars Carlsson*

Main category: stat.ML

TL;DR: The paper addresses the issue of untrustworthy predictions in machine learning by using conformal prediction to provide a reject option in binary classification, with theoretical guarantees and practical illustrations.


<details>
  <summary>Details</summary>
Motivation: To address the problem of machine learning models making unreliable predictions, as they do not provide a mechanism to abstain from decisions that are likely incorrect.

Method: The paper utilizes conformal prediction (CP) to formalize binary classification with a reject option. CP delivers prediction sets with validity guarantees, and predictions are made only when a single allowable label appears in the set. Error-reject curves are also introduced to analyze trade-offs.

Result: Derived theoretical guarantees for error rates in classification with a reject option. Experimentally, finite sample estimates and numerical examples demonstrate the utility of this reject option across different CP settings.

Conclusion: The reject option via conformal prediction offers a practical mechanism for increasing trust and reliability in binary classification by balancing error rates against reject rates, providing theoretical backing and experimental validation.

Abstract: Machine learning (ML) models always make a prediction, even when they are
likely to be wrong. This causes problems in practical applications, as we do
not know if we should trust a prediction. ML with reject option addresses this
issue by abstaining from making a prediction if it is likely to be incorrect.
In this work, we formalise the approach to ML with reject option in binary
classification, deriving theoretical guarantees on the resulting error rate.
This is achieved through conformal prediction (CP), which produce prediction
sets with distribution-free validity guarantees. In binary classification, CP
can output prediction sets containing exactly one, two or no labels. By
accepting only the singleton predictions, we turn CP into a binary classifier
with reject option.
  Here, CP is formally put in the framework of predicting with reject option.
We state and prove the resulting error rate, and give finite sample estimates.
Numerical examples provide illustrations of derived error rate through several
different conformal prediction settings, ranging from full conformal prediction
to offline batch inductive conformal prediction. The former has a direct link
to sharp validity guarantees, whereas the latter is more fuzzy in terms of
validity guarantees but can be used in practice. Error-reject curves illustrate
the trade-off between error rate and reject rate, and can serve to aid a user
to set an acceptable error rate or reject rate in practice.

</details>


### [304] [Thompson Sampling in Function Spaces via Neural Operators](https://arxiv.org/abs/2506.21894)
*Rafael Oliveira,Xuesong Wang,Kian Ming A. Chai,Edwin V. Bonilla*

Main category: stat.ML

TL;DR: The paper introduces a method for functional optimization using Thompson sampling combined with neural operators to reduce computational costs and improve efficiency.


<details>
  <summary>Details</summary>
Motivation: The research aims to address optimization problems involving expensive operator queries, such as high-fidelity simulations, by leveraging inexpensive functional evaluations and neural operator surrogates.

Method: The proposed method employs a sample-then-optimize strategy, treating trained neural operators as approximate samples from a Gaussian process, with theoretical guarantees on convergence in infinite-dimensional settings.

Result: The algorithm demonstrates improved sample efficiency and competitive performance compared to existing baselines on tasks involving partial differential equations and nonlinear operator-driven phenomena.

Conclusion: The approach creates a computationally efficient framework for challenging functional optimization problems using neural operators, establishing theoretical guarantees and outperforming existing methods in benchmark tests.

Abstract: We propose an extension of Thompson sampling to optimization problems over
function spaces where the objective is a known functional of an unknown
operator's output. We assume that functional evaluations are inexpensive, while
queries to the operator (such as running a high-fidelity simulator) are costly.
Our algorithm employs a sample-then-optimize approach using neural operator
surrogates. This strategy avoids explicit uncertainty quantification by
treating trained neural operators as approximate samples from a Gaussian
process. We provide novel theoretical convergence guarantees, based on Gaussian
processes in the infinite-dimensional setting, under minimal assumptions. We
benchmark our method against existing baselines on functional optimization
tasks involving partial differential equations and other nonlinear
operator-driven phenomena, demonstrating improved sample efficiency and
competitive performance.

</details>


### [305] [Hybrid Generative Modeling for Incomplete Physics: Deep Grey-Box Meets Optimal Transport](https://arxiv.org/abs/2506.22204)
*Gurjeet Sangra Singh,Maciej Falkiewicz,Alexandros Kalousis*

Main category: stat.ML

TL;DR: The paper introduces a hybrid generative model combining deep grey-box modeling with Optimal Transport to address incomplete physics-based models for unpaired data settings.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of using approximated physics models with missing or unknown terms, which lead to distributions differing from real-world data-generating processes.

Method: The approach uses Optimal Transport methods combined with deep grey-box modeling to complete imperfect physics models by mapping data spaces with minimal source distribution distortion.

Result: The proposed method outperforms existing alternatives by effectively resolving unpaired data problems and correctly utilizing physics parameters while preserving interpretability.

Conclusion: The hybrid model successfully integrates data-driven and theory-driven approaches, validating its effectiveness in generation tasks and providing insights into physics dynamics.

Abstract: Physics phenomena are often described by ordinary and/or partial differential
equations (ODEs/PDEs), and solved analytically or numerically. Unfortunately,
many real-world systems are described only approximately with missing or
unknown terms in the equations. This makes the distribution of the physics
model differ from the true data-generating process (DGP). Using limited and
unpaired data between DGP observations and the imperfect model simulations, we
investigate this particular setting by completing the known-physics model,
combining theory-driven models and data-driven to describe the shifted
distribution involved in the DGP. We present a novel hybrid generative model
approach combining deep grey-box modelling with Optimal Transport (OT) methods
to enhance incomplete physics models. Our method implements OT maps in data
space while maintaining minimal source distribution distortion, demonstrating
superior performance in resolving the unpaired problem and ensuring correct
usage of physics parameters. Unlike black-box alternatives, our approach
leverages physics-based inductive biases to accurately learn system dynamics
while preserving interpretability through its domain knowledge foundation.
Experimental results validate our method's effectiveness in both generation
tasks and model transparency, offering detailed insights into learned physics
dynamics.

</details>


### [306] [Uncovering smooth structures in single-cell data with PCS-guided neighbor embeddings](https://arxiv.org/abs/2506.22228)
*Rong Ma,Xi Li,Jingyuan Hu,Bin Yu*

Main category: stat.ML

TL;DR: NESS is introduced as a principled machine learning method to improve neighbor embedding (NE) representations for analyzing continuous cell-state transitions in noisy single-cell data.


<details>
  <summary>Details</summary>
Motivation: Single-cell sequencing provides detailed insight into cell-state transitions, but existing NE algorithms like t-SNE and UMAP distort continuous trajectories. There is a need for better methods capable of reliably capturing smooth biological transitions.

Method: The authors systematically evaluate existing NE algorithms, introduce NESS grounded in the Predictability-Computability-Stability framework, and demonstrate its utility through analysis, simulations, and applications across diverse single-cell datasets.

Result: NESS consistently outperformed existing NE approaches by improving stability, reducing artifacts, and enabling robust inference of developmental trajectories and cell-state transitions.

Conclusion: NESS enhances the analysis of single-cell datasets by preserving biological structures and enabling reliable identification of transitional and stable cell states, offering robust insights into transcriptional dynamics.

Abstract: Single-cell sequencing is revolutionizing biology by enabling detailed
investigations of cell-state transitions. Many biological processes unfold
along continuous trajectories, yet it remains challenging to extract smooth,
low-dimensional representations from inherently noisy, high-dimensional
single-cell data. Neighbor embedding (NE) algorithms, such as t-SNE and UMAP,
are widely used to embed high-dimensional single-cell data into low dimensions.
But they often introduce undesirable distortions, resulting in misleading
interpretations. Existing evaluation methods for NE algorithms primarily focus
on separating discrete cell types rather than capturing continuous cell-state
transitions, while dynamic modeling approaches rely on strong assumptions about
cellular processes and specialized data. To address these challenges, we build
on the Predictability-Computability-Stability (PCS) framework for reliable and
reproducible data-driven discoveries. First, we systematically evaluate popular
NE algorithms through empirical analysis, simulation, and theory, and reveal
their key shortcomings, such as artifacts and instability. We then introduce
NESS, a principled and interpretable machine learning approach to improve NE
representations by leveraging algorithmic stability and to enable robust
inference of smooth biological structures. NESS offers useful concepts,
quantitative stability metrics, and efficient computational workflows to
uncover developmental trajectories and cell-state transitions in single-cell
data. Finally, we apply NESS to six single-cell datasets, spanning pluripotent
stem cell differentiation, organoid development, and multiple tissue-specific
lineage trajectories. Across these diverse contexts, NESS consistently yields
useful biological insights, such as identification of transitional and stable
cell states and quantification of transcriptional dynamics during development.

</details>


### [307] [Optimal Estimation of Watermark Proportions in Hybrid AI-Human Texts](https://arxiv.org/abs/2506.22343)
*Xiang Li,Garrett Wen,Weiqing He,Jiayuan Wu,Qi Long,Weijie J. Su*

Main category: stat.ML

TL;DR: This paper addresses estimating the proportion of watermarked content in mixed-source texts by proposing efficient statistical estimators.


<details>
  <summary>Details</summary>
Motivation: To accurately detect and quantify watermarked content in real-world mixed-source texts, enabling better differentiation between human-written and LLM-generated materials.

Method: Formulating the problem as a mixture model using pivotal statistics, demonstrating identifiability conditions, proposing efficient estimators, and establishing minimax lower bounds.

Result: The proposed estimators achieved high estimation accuracy on synthetic data and mixed-source text from open-source models.

Conclusion: The methods presented are effective for determining watermark proportions in text, extending the utility of watermarking schemes in mixed-source scenarios.

Abstract: Text watermarks in large language models (LLMs) are an increasingly important
tool for detecting synthetic text and distinguishing human-written content from
LLM-generated text. While most existing studies focus on determining whether
entire texts are watermarked, many real-world scenarios involve mixed-source
texts, which blend human-written and watermarked content. In this paper, we
address the problem of optimally estimating the watermark proportion in
mixed-source texts. We cast this problem as estimating the proportion parameter
in a mixture model based on \emph{pivotal statistics}. First, we show that this
parameter is not even identifiable in certain watermarking schemes, let alone
consistently estimable. In stark contrast, for watermarking methods that employ
continuous pivotal statistics for detection, we demonstrate that the proportion
parameter is identifiable under mild conditions. We propose efficient
estimators for this class of methods, which include several popular unbiased
watermarks as examples, and derive minimax lower bounds for any measurable
estimator based on pivotal statistics, showing that our estimators achieve
these lower bounds. Through evaluations on both synthetic data and mixed-source
text generated by open-source models, we demonstrate that our proposed
estimators consistently achieve high estimation accuracy.

</details>


### [308] [Beyond ReLU: How Activations Affect Neural Kernels and Random Wide Networks](https://arxiv.org/abs/2506.22429)
*David Holzmüller,Max Schölpple*

Main category: stat.ML

TL;DR: This paper explores the Reproducing Kernel Hilbert Space (RKHS) properties of neural networks with various activation functions beyond ReLU, particularly for SELU, ELU, and LeakyReLU.


<details>
  <summary>Details</summary>
Motivation: To address the limited understanding of neural tangent kernels (NTK) and neural network Gaussian process kernels (NNGP) for non-ReLU activation functions, providing theoretical insights for such cases.

Method: The authors analyze RKHS properties of kernels for activation functions with non-smoothness restricted to zero, covering cases like missing biases, two-layer networks, and polynomial activations.

Result: They find that not infinitely smooth activations produce equivalent RKHSs across network depths, while polynomial activations yield non-equivalent RKHSs. They also derive new smoothness results for NNGP sample paths.

Conclusion: The paper generalizes the theoretical understanding of kernel properties in neural networks, expanding it beyond the commonly studied ReLU activation function to a broader class of non-smooth activations.

Abstract: While the theory of deep learning has made some progress in recent years,
much of it is limited to the ReLU activation function. In particular, while the
neural tangent kernel (NTK) and neural network Gaussian process kernel (NNGP)
have given theoreticians tractable limiting cases of fully connected neural
networks, their properties for most activation functions except for powers of
the ReLU function are poorly understood. Our main contribution is to provide a
more general characterization of the RKHS of these kernels for typical
activation functions whose only non-smoothness is at zero, such as SELU, ELU,
or LeakyReLU. Our analysis also covers a broad set of special cases such as
missing biases, two-layer networks, or polynomial activations. Our results show
that a broad class of not infinitely smooth activations generate equivalent
RKHSs at different network depths, while polynomial activations generate
non-equivalent RKHSs. Finally, we derive results for the smoothness of NNGP
sample paths, characterizing the smoothness of infinitely wide neural networks
at initialization.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [309] [SciMantify -- A Hybrid Approach for the Evolving Semantification of Scientific Knowledge](https://arxiv.org/abs/2506.21819)
*Lena John,Kheir Eddine Farfar,Sören Auer,Oliver Karras*

Main category: cs.DL

TL;DR: The abstract proposes a knowledge evolution model to transform scientific content from static formats like PDFs into semantic knowledge graphs, with the SciMantify hybrid approach enabling human-machine collaboration in this transition.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the limitations of static and unstructured scientific publications, which hinder their accessibility and reusability.

Method: The authors propose a five-stage evolution model inspired by the 5-star Linked Open Data model, alongside a hybrid approach (SciMantify) for knowledge semantification that combines human and machine collaboration.

Result: The methodology was implemented in the Open Research Knowledge Graph (ORKG), demonstrating reduced preprocessing effort, enhanced semantification, and improved alignment with knowledge graphs.

Conclusion: The research highlights that the proposed approach simplifies the transformation of scientific knowledge into semantic formats, enhancing its usability and aligning it with modern knowledge representation standards.

Abstract: Scientific publications, primarily digitized as PDFs, remain static and
unstructured, limiting the accessibility and reusability of the contained
knowledge. At best, scientific knowledge from publications is provided in
tabular formats, which lack semantic context. A more flexible, structured, and
semantic representation is needed to make scientific knowledge understandable
and processable by both humans and machines. We propose an evolution model of
knowledge representation, inspired by the 5-star Linked Open Data (LOD) model,
with five stages and defined criteria to guide the stepwise transition from a
digital artifact, such as a PDF, to a semantic representation integrated in a
knowledge graph (KG). Based on an exemplary workflow implementing the entire
model, we developed a hybrid approach, called SciMantify, leveraging tabular
formats of scientific knowledge, e.g., results from secondary studies, to
support its evolving semantification. In the approach, humans and machines
collaborate closely by performing semantic annotation tasks (SATs) and refining
the results to progressively improve the semantic representation of scientific
knowledge. We implemented the approach in the Open Research Knowledge Graph
(ORKG), an established platform for improving the findability, accessibility,
interoperability, and reusability of scientific knowledge. A preliminary user
experiment showed that the approach simplifies the preprocessing of scientific
knowledge, reduces the effort for the evolving semantification, and enhances
the knowledge representation through better alignment with the KG structures.

</details>


<div id='physics.ins-det'></div>

# physics.ins-det [[Back]](#toc)

### [310] [CaloHadronic: a diffusion model for the generation of hadronic showers](https://arxiv.org/abs/2506.21720)
*Thorsten Buss,Frank Gaede,Gregor Kasieczka,Anatolii Korol,Katja Krüger,Peter McKeown,Martina Mozzanica*

Main category: physics.ins-det

TL;DR: The paper discusses transformer-based machine learning (ML) methods to simulate complex hadronic and electromagnetic particle showers in granular calorimeter systems.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address computational constraints in particle physics simulations by leveraging fast and accurate machine learning techniques for particle shower simulations.

Method: The paper extends diffusion-based point-cloud generative models using a transformer-based architecture with attention mechanisms, enabling simulation of both electromagnetic and hadronic showers holistically.

Result: For the first time, the proposed approach successfully simulates particle showers across electromagnetic and hadronic sections of highly granular calorimeters with improved accuracy and substructure modeling.

Conclusion: Transformer-based generative models can enhance particle physics simulations by accurately simulating complex calorimeter data, offering both efficiency and granular substructure representation.

Abstract: Simulating showers of particles in highly-granular calorimeters is a key
frontier in the application of machine learning to particle physics. Achieving
high accuracy and speed with generative machine learning models can enable them
to augment traditional simulations and alleviate a major computing constraint.
Recent developments have shown how diffusion based generative shower simulation
approaches that do not rely on a fixed structure, but instead generate
geometry-independent point clouds, are very efficient. We present a
transformer-based extension to previous architectures which were developed for
simulating electromagnetic showers in the highly granular electromagnetic
calorimeter of the International Large Detector, ILD. The attention mechanism
now allows us to generate complex hadronic showers with more pronounced
substructure across both the electromagnetic and hadronic calorimeters. This is
the first time that machine learning methods are used to holistically generate
showers across the electromagnetic and hadronic calorimeter in highly granular
imaging calorimeter systems.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [311] [Adversarial Threats in Quantum Machine Learning: A Survey of Attacks and Defenses](https://arxiv.org/abs/2506.21842)
*Archisman Ghosh,Satwik Kundu,Swaroop Ghosh*

Main category: quant-ph

TL;DR: The paper explores security vulnerabilities and defensive measures in Quantum Machine Learning (QML) systems during the NISQ era.


<details>
  <summary>Details</summary>
Motivation: The motivation is to identify and address adversarial threats in QML systems due to their increasing adoption and inherent vulnerabilities.

Method: The paper examines attack methodologies like model stealing, data poisoning, reverse engineering, and backdoor attacks, while proposing defense mechanisms such as noise-based watermarks and adapting classical security measures to quantum environments.

Result: Key findings include identifying security weaknesses in QML systems and presenting defense strategies like hardware noise exploitation and obfuscation techniques.

Conclusion: A roadmap is offered for creating robust QML systems, emphasizing the need for secure quantum-classical interoperability and addressing unresolved vulnerabilities.

Abstract: Quantum Machine Learning (QML) integrates quantum computing with classical
machine learning, primarily to solve classification, regression and generative
tasks. However, its rapid development raises critical security challenges in
the Noisy Intermediate-Scale Quantum (NISQ) era. This chapter examines
adversarial threats unique to QML systems, focusing on vulnerabilities in
cloud-based deployments, hybrid architectures, and quantum generative models.
Key attack vectors include model stealing via transpilation or output
extraction, data poisoning through quantum-specific perturbations, reverse
engineering of proprietary variational quantum circuits, and backdoor attacks.
Adversaries exploit noise-prone quantum hardware and insufficiently secured
QML-as-a-Service (QMLaaS) workflows to compromise model integrity, ownership,
and functionality. Defense mechanisms leverage quantum properties to counter
these threats. Noise signatures from training hardware act as non-invasive
watermarks, while hardware-aware obfuscation techniques and ensemble strategies
disrupt cloning attempts. Emerging solutions also adapt classical adversarial
training and differential privacy to quantum settings, addressing
vulnerabilities in quantum neural networks and generative architectures.
However, securing QML requires addressing open challenges such as balancing
noise levels for reliability and security, mitigating cross-platform attacks,
and developing quantum-classical trust frameworks. This chapter summarizes
recent advances in attacks and defenses, offering a roadmap for researchers and
practitioners to build robust, trustworthy QML systems resilient to evolving
adversarial landscapes.

</details>


### [312] [QuKAN: A Quantum Circuit Born Machine approach to Quantum Kolmogorov Arnold Networks](https://arxiv.org/abs/2506.22340)
*Yannick Werner,Akash Malemath,Mengxi Liu,Vitor Fortes Rey,Nikolaos Palaiodimopoulos,Paul Lukowicz,Maximilian Kiefer-Emmanouilidis*

Main category: quant-ph

TL;DR: This paper explores Kolmogorov Arnold Networks (KANs) in quantum machine learning by developing hybrid and fully quantum versions using Quantum Circuit Born Machines (QCBM).


<details>
  <summary>Details</summary>
Motivation: Kolmogorov Arnold Networks (KANs) can express complex functions with fewer neurons but haven't been well explored in quantum machine learning.

Method: The researchers adapt KANs using pre-trained residual functions and parameterized quantum circuits, implementing both hybrid classical-quantum and fully quantum architectures.

Result: The Quantum KAN (QuKAN) architecture is shown to be feasible, interpretable, and effective.

Conclusion: This work successfully demonstrates the application and potential of KANs in quantum machine learning through hybrid and fully quantum implementations.

Abstract: Kolmogorov Arnold Networks (KANs), built upon the Kolmogorov Arnold
representation theorem (KAR), have demonstrated promising capabilities in
expressing complex functions with fewer neurons. This is achieved by
implementing learnable parameters on the edges instead of on the nodes, unlike
traditional networks such as Multi-Layer Perceptrons (MLPs). However, KANs
potential in quantum machine learning has not yet been well explored. In this
work, we present an implementation of these KAN architectures in both hybrid
and fully quantum forms using a Quantum Circuit Born Machine (QCBM). We adapt
the KAN transfer using pre-trained residual functions, thereby exploiting the
representational power of parametrized quantum circuits. In the hybrid model we
combine classical KAN components with quantum subroutines, while the fully
quantum version the entire architecture of the residual function is translated
to a quantum model. We demonstrate the feasibility, interpretability and
performance of the proposed Quantum KAN (QuKAN) architecture.

</details>


### [313] [Robust quantum reservoir computers for forecasting chaotic dynamics: generalized synchronization and stability](https://arxiv.org/abs/2506.22335)
*Osama Ahmed,Felix Tennie,Luca Magri*

Main category: quant-ph

TL;DR: The paper demonstrates the effectiveness of quantum reservoir computers (QRCs) and recurrence-free architectures (RF-QRCs) in learning and forecasting chaotic dynamics, proposing GS=ESP as a design criterion and showcasing improved robustness via dissipation.


<details>
  <summary>Details</summary>
Motivation: The motivation is to design robust quantum machines capable of accurately learning and forecasting chaotic dynamics from time-series data, paving the way for applications in near-term quantum hardware.

Method: The authors interpret QRCs as generalized-synchronization systems, derive their Jacobian for analysis, and propose the GS=ESP criterion to ensure robustness, using numerical verifications and noise simulations to validate their approach.

Result: The study demonstrates that QRCs can learn chaotic dynamics, invariant properties like Lyapunov spectra, and attractor geometry, and that dissipation enhances their robustness against noise.

Conclusion: QRCs and RF-QRCs are powerful tools for chaotic time-series forecasting, with their robustness established through theoretical and numerical validation, pointing to promising applications in quantum computing.

Abstract: We show that recurrent quantum reservoir computers (QRCs) and their
recurrence-free architectures (RF-QRCs) are robust tools for learning and
forecasting chaotic dynamics from time-series data. First, we formulate and
interpret quantum reservoir computers as coupled dynamical systems, where the
reservoir acts as a response system driven by training data; in other words,
quantum reservoir computers are generalized-synchronization (GS) systems.
Second, we show that quantum reservoir computers can learn chaotic dynamics and
their invariant properties, such as Lyapunov spectra, attractor dimensions, and
geometric properties such as the covariant Lyapunov vectors. This analysis is
enabled by deriving the Jacobian of the quantum reservoir update. Third, by
leveraging tools from generalized synchronization, we provide a method for
designing robust quantum reservoir computers. We propose the criterion
$GS=ESP$: GS implies the echo state property (ESP), and vice versa. We
analytically show that RF-QRCs, by design, fulfill $GS=ESP$. Finally, we
analyze the effect of simulated noise. We find that dissipation from noise
enhances the robustness of quantum reservoir computers. Numerical verifications
on systems of different dimensions support our conclusions. This work opens
opportunities for designing robust quantum machines for chaotic time series
forecasting on near-term quantum hardware.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [314] [Joint Task Offloading and Resource Allocation in Low-Altitude MEC via Graph Attention Diffusion](https://arxiv.org/abs/2506.21933)
*Yifan Xue,Ruihuai Liang,Bo Yang,Xuelin Cao,Zhiwen Yu,Mérouane Debbah,Chau Yuen*

Main category: cs.NI

TL;DR: This paper introduces a novel graph attention diffusion-based solution generator (GADSG) for optimizing task offloading and resource allocation in low-altitude heterogeneous MEC systems, outperforming existing methods in dynamic environments.


<details>
  <summary>Details</summary>
Motivation: The rising demand for low-altitude economic applications has led to challenges in real-time task scheduling due to node heterogeneity, communication instability, and task variations.

Method: The paper models a three-layer heterogeneous MEC system and formulates the optimization of offloading decisions and resource allocation as a graph-structured problem. It proposes GADSG, combining graph attention networks with diffusion models to solve this optimization.

Result: Extensive simulations across different dataset scales and topologies reveal that GADSG surpasses baseline methods in optimization performance, robustness, and generalization.

Conclusion: GADSG is a promising method for tackling task scheduling challenges in low-altitude MEC systems, providing efficient and adaptable solutions in complex network environments.

Abstract: With the rapid development of the low-altitude economy, air-ground integrated
multi-access edge computing (MEC) systems are facing increasing demands for
real-time and intelligent task scheduling. In such systems, task offloading and
resource allocation encounter multiple challenges, including node
heterogeneity, unstable communication links, and dynamic task variations. To
address these issues, this paper constructs a three-layer heterogeneous MEC
system architecture for low-altitude economic networks, encompassing aerial and
ground users as well as edge servers. The system is systematically modeled from
the perspectives of communication channels, computational costs, and constraint
conditions, and the joint optimization problem of offloading decisions and
resource allocation is uniformly abstracted into a graph-structured modeling
task. On this basis, we propose a graph attention diffusion-based solution
generator (GADSG). This method integrates the contextual awareness of graph
attention networks with the solution distribution learning capability of
diffusion models, enabling joint modeling and optimization of discrete
offloading variables and continuous resource allocation variables within a
high-dimensional latent space. We construct multiple simulation datasets with
varying scales and topologies. Extensive experiments demonstrate that the
proposed GADSG model significantly outperforms existing baseline methods in
terms of optimization performance, robustness, and generalization across task
structures, showing strong potential for efficient task scheduling in dynamic
and complex low-altitude economic network environments.

</details>


### [315] [Concept-Level AI for Telecom: Moving Beyond Large Language Models](https://arxiv.org/abs/2506.22359)
*Viswanath Kumarskandpriya,Abdulhalim Dandoush,Abbas Bradai,Ali Belgacem*

Main category: cs.NI

TL;DR: Telecom faces complexity challenges which Large Concept Models (LCMs) can solve better than Large Language Models (LLMs), due to their hierarchical representation advantages.


<details>
  <summary>Details</summary>
Motivation: The telecom domain is rapidly becoming more complex, requiring innovative AI-driven solutions to handle hierarchical systems, cross-layer dependencies, and real-time management.

Method: Large Concept Models (LCMs) propose using hyperbolic latent spaces and semantic concept reasoning to overcome limitations of LLMs, providing efficient hierarchical representations for telecom-specific tasks.

Result: LCMs demonstrate superior capability in handling telecom challenges related to cross-layer dependency, fault correlation, and multimodal integrations.

Conclusion: Adopting LCMs represents a necessary leap forward for AI-driven telecom management, as they address LLMs' inability to handle extended context and domain-specific needs effectively.

Abstract: The telecommunications and networking domain stands at the precipice of a
transformative era, driven by the necessity to manage increasingly complex,
hierarchical, multi administrative domains (i.e., several operators on the same
path) and multilingual systems. Recent research has demonstrated that Large
Language Models (LLMs), with their exceptional general-purpose text analysis
and code generation capabilities, can be effectively applied to certain telecom
problems (e.g., auto-configuration of data plan to meet certain application
requirements). However, due to their inherent token-by-token processing and
limited capacity for maintaining extended context, LLMs struggle to fulfill
telecom-specific requirements such as cross-layer dependency cascades (i.e.,
over OSI), temporal-spatial fault correlation, and real-time distributed
coordination. In contrast, Large Concept Models (LCMs), which reason at the
abstraction level of semantic concepts rather than individual lexical tokens,
offer a fundamentally superior approach for addressing these telecom
challenges. By employing hyperbolic latent spaces for hierarchical
representation and encapsulating complex multi-layered network interactions
within concise concept embeddings, LCMs overcome critical shortcomings of LLMs
in terms of memory efficiency, cross-layer correlation, and native multimodal
integration. This paper argues that adopting LCMs is not simply an incremental
step, but a necessary evolutionary leap toward achieving robust and effective
AI-driven telecom management.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [316] [Simultaneously Fair Allocation of Indivisible Items Across Multiple Dimensions](https://arxiv.org/abs/2506.21727)
*Yasushi Kawase,Bodhayan Roy,Mohammad Azharuddin Sanpui*

Main category: cs.GT

TL;DR: The paper investigates fairness in multidimensional allocation problems using relaxed envy-freeness criteria (weak and strong sEFc). It establishes bounds, algorithms, and computational complexity regarding these conditions.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the inadequacy of traditional fairness notions in multidimensional settings, which are essential for real-world applications like cloud computing resource allocation.

Method: The study introduces weak and strong sEFc frameworks to address multidimensional fairness. It develops theoretical bounds, algorithms, and examines NP-hardness of certain allocation checks.

Result: Key findings include upper and lower bounds for c guaranteeing sEFc allocations, algorithms for determining allocation existence, and NP-hardness results for sEF1 conditions.

Conclusion: The research advances the understanding of multidimensional fairness, providing theoretical bounds, computational methods, and complexity insights, bridging theory and practical applications.

Abstract: This paper explores the fair allocation of indivisible items in a
multidimensional setting, motivated by the need to address fairness in complex
environments where agents assess bundles according to multiple criteria. Such
multidimensional settings are not merely of theoretical interest but are
central to many real-world applications. For example, cloud computing resources
are evaluated based on multiple criteria such as CPU cores, memory, and network
bandwidth. In such cases, traditional one dimensional fairness notions fail to
capture fairness across multiple attributes. To address these challenges, we
study two relaxed variants of envy-freeness: weak simultaneously envy-free up
to c goods (weak sEFc) and strong simultaneously envy-free up to c goods
(strong sEFc), which accommodate the multidimensionality of agents'
preferences. Under the weak notion, for every pair of agents and for each
dimension, any perceived envy can be eliminated by removing, if necessary, a
different set of goods from the envied agent's allocation. In contrast, the
strong version requires selecting a single set of goods whose removal from the
envied bundle simultaneously eliminates envy in every dimension. We provide
upper and lower bounds on the relaxation parameter c that guarantee the
existence of weak or strong sEFc allocations, where these bounds are
independent of the total number of items. In addition, we present algorithms
for checking whether a weak or strong sEFc allocation exists. Moreover, we
establish NP-hardness results for checking the existence of weak sEF1 and
strong sEF1 allocations.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [317] [PhotonSplat: 3D Scene Reconstruction and Colorization from SPAD Sensors](https://arxiv.org/abs/2506.21680)
*Sai Sri Teja,Sreevidya Chintalapati,Vinayak Gupta,Mukund Varma T,Haejoon Lee,Aswin Sankaranarayanan,Kaushik Mitra*

Main category: eess.IV

TL;DR: The paper introduces PhotonSplat, a framework for 3D reconstruction using SPAD sensors that overcome challenges posed by motion blur and noisy binary images.


<details>
  <summary>Details</summary>
Motivation: There is a need for high-quality 3D reconstruction under motion blur conditions, which current neural rendering techniques fail to effectively handle.

Method: The method employs SPAD sensors and a novel 3D spatial filtering technique to reconstruct 3D scenes, leveraging generative priors and blurry images for colorization, while extending capabilities to dynamic scenes.

Result: PhotonSplat successfully reduces noise and retrieves 3D scenes from SPAD binary images. It supports segmentation, object detection, appearance editing tasks, and works for dynamic scenes.

Conclusion: PhotonSplat leverages SPAD sensors to address the noise-blur trade-off in 3D reconstruction, providing versatile applications and introducing a novel real-world dataset (PhotonScenes).

Abstract: Advances in 3D reconstruction using neural rendering have enabled
high-quality 3D capture. However, they often fail when the input imagery is
corrupted by motion blur, due to fast motion of the camera or the objects in
the scene. This work advances neural rendering techniques in such scenarios by
using single-photon avalanche diode (SPAD) arrays, an emerging sensing
technology capable of sensing images at extremely high speeds. However, the use
of SPADs presents its own set of unique challenges in the form of binary
images, that are driven by stochastic photon arrivals. To address this, we
introduce PhotonSplat, a framework designed to reconstruct 3D scenes directly
from SPAD binary images, effectively navigating the noise vs. blur trade-off.
Our approach incorporates a novel 3D spatial filtering technique to reduce
noise in the renderings. The framework also supports both no-reference using
generative priors and reference-based colorization from a single blurry image,
enabling downstream applications such as segmentation, object detection and
appearance editing tasks. Additionally, we extend our method to incorporate
dynamic scene representations, making it suitable for scenes with moving
objects. We further contribute PhotonScenes, a real-world multi-view dataset
captured with the SPAD sensors.

</details>


### [318] [TUS-REC2024: A Challenge to Reconstruct 3D Freehand Ultrasound Without External Tracker](https://arxiv.org/abs/2506.21765)
*Qi Li,Shaheer U. Saeed,Yuliang Huang,Mingyuan Luo,Zhongnuo Yan,Jiongquan Chen,Xin Yang,Dong Ni,Nektarios Winter,Phuc Nguyen,Lucas Steinberger,Caelan Haney,Yuan Zhao,Mingjie Jiang,Bowen Ren,SiYeoul Lee,Seonho Kim,MinKyung Seo,MinWoo Kim,Yimeng Dou,Zhiwei Zhang,Yin Li,Tomy Varghese,Dean C. Barratt,Matthew J. Clarkson,Tom Vercauteren,Yipeng Hu*

Main category: eess.IV

TL;DR: The paper discusses the TUS-REC2024 Challenge, which focuses on trackerless freehand ultrasound reconstruction using 3D volumes reconstructed from 2D ultrasound images without external tracking. Over 6 teams submitted advanced solutions to the dataset, highlighting the field's progress and limitations.


<details>
  <summary>Details</summary>
Motivation: Trackerless 3D ultrasound reconstruction offers cost-effective, portable alternatives for volumetric imaging but faces challenges like motion estimation accuracy, cumulative drift, and generalisability across protocols.

Method: The TUS-REC2024 Challenge provided a benchmark dataset, baseline models, and an evaluation framework to evaluate trackerless ultrasound reconstruction methods. Teams implemented various approaches such as recurrent models, attention mechanisms, and physics-based models.

Result: The Challenge attracted submissions from multiple teams with diverse methodological approaches. A comparative analysis highlighted the progress and limitations of current state-of-the-art methods, providing insights into future research directions.

Conclusion: The Challenge drives state-of-the-art development in trackerless ultrasound reconstruction. By making data and benchmarks freely accessible and organizing regular competitions, it fosters open research and aims to evolve this field continuously.

Abstract: Trackerless freehand ultrasound reconstruction aims to reconstruct 3D volumes
from sequences of 2D ultrasound images without relying on external tracking
systems, offering a low-cost, portable, and widely deployable alternative for
volumetric imaging. However, it presents significant challenges, including
accurate inter-frame motion estimation, minimisation of drift accumulation over
long sequences, and generalisability across scanning protocols. The TUS-REC2024
Challenge was established to benchmark and accelerate progress in trackerless
3D ultrasound reconstruction by providing a publicly available dataset for the
first time, along with a baseline model and evaluation framework. The Challenge
attracted over 43 registered teams, of which 6 teams submitted 21 valid
dockerized solutions. Submitted methods spanned a wide range of algorithmic
approaches, including recurrent models, registration-driven volume refinement,
attention, and physics-informed models. This paper presents an overview of the
Challenge design, summarises the key characteristics of the dataset, provides a
concise literature review, introduces the technical details of the underlying
methodology working with tracked freehand ultrasound data, and offers a
comparative analysis of submitted methods across multiple evaluation metrics.
The results highlight both the progress and current limitations of
state-of-the-art approaches in this domain, and inform directions for future
research. The data, evaluation code, and baseline are publicly available to
facilitate ongoing development and reproducibility. As a live and evolving
benchmark, this Challenge is designed to be continuously developed and
improved. The Challenge was held at MICCAI 2024 and will be organised again at
MICCAI 2025, reflecting its growing impact and the sustained commitment to
advancing this field.

</details>


### [319] [UnMix-NeRF: Spectral Unmixing Meets Neural Radiance Fields](https://arxiv.org/abs/2506.21884)
*Fabian Perez,Sara Rojas,Carlos Hinojosa,Hoover Rueda-Chacón,Bernard Ghanem*

Main category: eess.IV

TL;DR: UnMix-NeRF integrates hyperspectral processing to Neural Radiance Fields (NeRF) for unsupervised material segmentation and spectral novel view synthesis, offering superior performance.


<details>
  <summary>Details</summary>
Motivation: The study addresses the limitations of traditional NeRF-based segmentation methods, which depend on RGB data and fail to accurately perceive material properties crucial for applications like robotics, AR, and simulation.

Method: The proposed UnMix-NeRF framework models spectral reflectance via diffuse and specular components, using a global dictionary of endmembers for material signatures and predicting their distribution in 3D scenes. It applies unsupervised learning for material clustering and allows material-based appearance manipulation by editing the learned endmember dictionary.

Result: Experiments reveal that UnMix-NeRF outperforms existing methods in terms of spectral reconstruction accuracy and material segmentation quality.

Conclusion: UnMix-NeRF effectively enhances NeRF with hyperspectral capabilities, providing a powerful tool for material understanding and editing in computational graphics and vision.

Abstract: Neural Radiance Field (NeRF)-based segmentation methods focus on object
semantics and rely solely on RGB data, lacking intrinsic material properties.
This limitation restricts accurate material perception, which is crucial for
robotics, augmented reality, simulation, and other applications. We introduce
UnMix-NeRF, a framework that integrates spectral unmixing into NeRF, enabling
joint hyperspectral novel view synthesis and unsupervised material
segmentation. Our method models spectral reflectance via diffuse and specular
components, where a learned dictionary of global endmembers represents pure
material signatures, and per-point abundances capture their distribution. For
material segmentation, we use spectral signature predictions along learned
endmembers, allowing unsupervised material clustering. Additionally, UnMix-NeRF
enables scene editing by modifying learned endmember dictionaries for flexible
material-based appearance manipulation. Extensive experiments validate our
approach, demonstrating superior spectral reconstruction and material
segmentation to existing methods. Project page:
https://www.factral.co/UnMix-NeRF.

</details>


### [320] [Physical Degradation Model-Guided Interferometric Hyperspectral Reconstruction with Unfolding Transformer](https://arxiv.org/abs/2506.21880)
*Yuansheng Li,Yunhao Zou,Linwei Chen,Ying Fu*

Main category: eess.IV

TL;DR: The paper proposes an advanced reconstruction pipeline for Interferometric Hyperspectral Imaging (IHI), addressing issues like dataset scarcity and degradation specific to IHI.


<details>
  <summary>Details</summary>
Motivation: Overcome challenges in enhancing IHI reconstruction due to lack of training datasets and the inability of current approaches to fully mitigate degradation components.

Method: Developed a simplified degradation model to synthesize realistic training datasets and introduced the IHRUT transformer architecture to enhance spectral correction and detail restoration.

Result: The proposed method showed significant improvements in performance and generalization capabilities through experiments.

Conclusion: The proposed approach successfully bridges gaps between IHI reconstruction and deep learning, offering robust solutions in spectral imaging domains.

Abstract: Interferometric Hyperspectral Imaging (IHI) is a critical technique for
large-scale remote sensing tasks due to its advantages in flux and spectral
resolution. However, IHI is susceptible to complex errors arising from imaging
steps, and its quality is limited by existing signal processing-based
reconstruction algorithms. Two key challenges hinder performance enhancement:
1) the lack of training datasets. 2) the difficulty in eliminating IHI-specific
degradation components through learning-based methods. To address these
challenges, we propose a novel IHI reconstruction pipeline. First, based on
imaging physics and radiometric calibration data, we establish a simplified yet
accurate IHI degradation model and a parameter estimation method. This model
enables the synthesis of realistic IHI training datasets from hyperspectral
images (HSIs), bridging the gap between IHI reconstruction and deep learning.
Second, we design the Interferometric Hyperspectral Reconstruction Unfolding
Transformer (IHRUT), which achieves effective spectral correction and detail
restoration through a stripe-pattern enhancement mechanism and a
spatial-spectral transformer architecture. Experimental results demonstrate the
superior performance and generalization capability of our method.

</details>


### [321] [StableCodec: Taming One-Step Diffusion for Extreme Image Compression](https://arxiv.org/abs/2506.21977)
*Tianyu Zhang,Xin Luo,Li Li,Dong Liu*

Main category: eess.IV

TL;DR: StableCodec is a novel image compression method leveraging single-step diffusion to achieve ultra-low bitrates (as low as 0.005 bits per pixel) with high realism and fidelity, addressing the limitations of existing diffusion-based models in speed and reconstruction quality.


<details>
  <summary>Details</summary>
Motivation: Current diffusion-based image compression methods have impressive generative capabilities at ultra-low bitrates but suffer from high decoding times and poor reconstruction fidelity due to pixel-level inconsistencies.

Method: StableCodec introduces a Deep Compression Latent Codec for encoding noisy latent variables for single-step diffusion, along with a Dual-Branch Coding Structure to enhance reconstruction fidelity, all optimized end-to-end with joint bitrate and pixel-level constraints.

Result: Tests on multiple datasets demonstrate that StableCodec outperforms existing methods in realism and fidelity metrics such as FID, KID, and DISTS, even at extremely low bitrates. It also achieves decoding speeds comparable to conventional coding methods.

Conclusion: StableCodec successfully addresses major challenges of diffusion-based image compression by improving coding efficiency, reconstruction fidelity, and real-time applicability, making it a promising solution for ultra-low bitrate scenarios.

Abstract: Diffusion-based image compression has shown remarkable potential for
achieving ultra-low bitrate coding (less than 0.05 bits per pixel) with high
realism, by leveraging the generative priors of large pre-trained text-to-image
diffusion models. However, current approaches require a large number of
denoising steps at the decoder to generate realistic results under extreme
bitrate constraints, limiting their application in real-time compression
scenarios. Additionally, these methods often sacrifice reconstruction fidelity,
as diffusion models typically fail to guarantee pixel-level consistency. To
address these challenges, we introduce StableCodec, which enables one-step
diffusion for high-fidelity and high-realism extreme image compression with
improved coding efficiency. To achieve ultra-low bitrates, we first develop an
efficient Deep Compression Latent Codec to transmit a noisy latent
representation for a single-step denoising process. We then propose a
Dual-Branch Coding Structure, consisting of a pair of auxiliary encoder and
decoder, to enhance reconstruction fidelity. Furthermore, we adopt end-to-end
optimization with joint bitrate and pixel-level constraints. Extensive
experiments on the CLIC 2020, DIV2K, and Kodak dataset demonstrate that
StableCodec outperforms existing methods in terms of FID, KID and DISTS by a
significant margin, even at bitrates as low as 0.005 bits per pixel, while
maintaining strong fidelity. Additionally, StableCodec achieves inference
speeds comparable to mainstream transform coding schemes. All source code are
available at https://github.com/LuizScarlet/StableCodec.

</details>


### [322] [Noise-Inspired Diffusion Model for Generalizable Low-Dose CT Reconstruction](https://arxiv.org/abs/2506.22012)
*Qi Gao,Zhihao Chen,Dong Zeng,Junping Zhang,Jianhua Ma,Hongming Shan*

Main category: eess.IV

TL;DR: The paper introduces NEED, a novel diffusion-based model for low-dose CT (LDCT) reconstruction, addressing the limitations of generalizability and robustness by aligning the method to domain-specific noise characteristics.


<details>
  <summary>Details</summary>
Motivation: To improve the generalization performance and robustness of LDCT reconstruction models to doses unseen during training, addressing challenges posed by noise deviation from Gaussian distribution and imprecise prior guidance.

Method: NEED employs a dual-domain approach: a shifted Poisson diffusion model to denoise projection data and a doubly guided diffusion model to refine reconstructed images. The model requires only normal-dose data for training and utilizes a time step matching strategy for unseen dose levels.

Result: The proposed NEED method consistently outperformed state-of-the-art models in qualitative, quantitative, and segmentation-based evaluations across two datasets.

Conclusion: NEED provides a generalizable and effective solution for LDCT reconstruction with applications across diverse, unseen dose levels, leveraging domain-specific noise insights.

Abstract: The generalization of deep learning-based low-dose computed tomography (CT)
reconstruction models to doses unseen in the training data is important and
remains challenging. Previous efforts heavily rely on paired data to improve
the generalization performance and robustness through collecting either diverse
CT data for re-training or a few test data for fine-tuning. Recently, diffusion
models have shown promising and generalizable performance in low-dose CT (LDCT)
reconstruction, however, they may produce unrealistic structures due to the CT
image noise deviating from Gaussian distribution and imprecise prior
information from the guidance of noisy LDCT images. In this paper, we propose a
noise-inspired diffusion model for generalizable LDCT reconstruction, termed
NEED, which tailors diffusion models for noise characteristics of each domain.
First, we propose a novel shifted Poisson diffusion model to denoise projection
data, which aligns the diffusion process with the noise model in pre-log LDCT
projections. Second, we devise a doubly guided diffusion model to refine
reconstructed images, which leverages LDCT images and initial reconstructions
to more accurately locate prior information and enhance reconstruction
fidelity. By cascading these two diffusion models for dual-domain
reconstruction, our NEED requires only normal-dose data for training and can be
effectively extended to various unseen dose levels during testing via a time
step matching strategy. Extensive qualitative, quantitative, and
segmentation-based evaluations on two datasets demonstrate that our NEED
consistently outperforms state-of-the-art methods in reconstruction and
generalization performance. Source code is made available at
https://github.com/qgao21/NEED.

</details>


### [323] [Towards Scalable and Robust White Matter Lesion Localization via Multimodal Deep Learning](https://arxiv.org/abs/2506.22041)
*Julia Machnio,Sebastian Nørgaard Llambias,Mads Nielsen,Mostafa Mehdipour Ghazi*

Main category: eess.IV

TL;DR: The paper introduces a deep learning framework for White Matter Hyperintensities (WMH) segmentation and localization using multimodal MRI inputs, demonstrating improved accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: The current challenges include lack of flexibility in handling missing MRI modalities and inefficiency in anatomical localization for WMH segmentation and diagnosis.

Method: The framework evaluates four MRI input configurations and utilizes multimodal fusion. It also incorporates a multi-task model to jointly predict lesion and anatomical region masks.

Result: Multimodal inputs significantly enhanced WMH segmentation accuracy. Modality-interchangeable setups enabled robustness but slightly reduced performance. Multi-task modeling showed less effectiveness compared to separate models.

Conclusion: Multimodal MRI input improves WMH analysis accuracy and robustness, while joint modeling has limited efficiency, indicating representational conflicts between tasks.

Abstract: White matter hyperintensities (WMH) are radiological markers of small vessel
disease and neurodegeneration, whose accurate segmentation and spatial
localization are crucial for diagnosis and monitoring. While multimodal MRI
offers complementary contrasts for detecting and contextualizing WM lesions,
existing approaches often lack flexibility in handling missing modalities and
fail to integrate anatomical localization efficiently. We propose a deep
learning framework for WM lesion segmentation and localization that operates
directly in native space using single- and multi-modal MRI inputs. Our study
evaluates four input configurations: FLAIR-only, T1-only, concatenated FLAIR
and T1, and a modality-interchangeable setup. It further introduces a
multi-task model for jointly predicting lesion and anatomical region masks to
estimate region-wise lesion burden. Experiments conducted on the MICCAI WMH
Segmentation Challenge dataset demonstrate that multimodal input significantly
improves the segmentation performance, outperforming unimodal models. While the
modality-interchangeable setting trades accuracy for robustness, it enables
inference in cases with missing modalities. Joint lesion-region segmentation
using multi-task learning was less effective than separate models, suggesting
representational conflict between tasks. Our findings highlight the utility of
multimodal fusion for accurate and robust WMH analysis, and the potential of
joint modeling for integrated predictions.

</details>


### [324] [Advanced Deep Learning Techniques for Automated Segmentation of Type B Aortic Dissections](https://arxiv.org/abs/2506.22222)
*Hao Xu,Ruth Lim,Brian E. Chapman*

Main category: eess.IV

TL;DR: The paper introduces four deep learning-based pipelines for automated segmentation of aortic dissection components using 3D U-Net and Swin-UnetR architectures. The proposed methods outperform existing solutions in segmentation accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the need for accurate and automated segmentation of true lumen (TL), false lumen (FL), and false lumen thrombosis (FLT) in Type B aortic dissection, as manual segmentation is time-consuming and prone to variability.

Method: The study developed and evaluated four segmentation pipelines based on deep learning: single-step, sequential, sequential multi-task, and ensemble models, using a dataset of 100 retrospective CTA images divided into training, validation, and testing sets.

Result: The proposed approaches achieved significantly higher Dice coefficients (0.91 for TL, 0.88 for FL, and 0.47 for FLT) compared to prior methods, demonstrating superior segmentation accuracy.

Conclusion: These pipelines enable precise segmentation of TBAD components, facilitating surveillance and treatment planning, and represent a significant improvement over existing methods.

Abstract: Purpose: Aortic dissections are life-threatening cardiovascular conditions
requiring accurate segmentation of true lumen (TL), false lumen (FL), and false
lumen thrombosis (FLT) from CTA images for effective management. Manual
segmentation is time-consuming and variable, necessitating automated solutions.
Materials and Methods: We developed four deep learning-based pipelines for Type
B aortic dissection segmentation: a single-step model, a sequential model, a
sequential multi-task model, and an ensemble model, utilizing 3D U-Net and
Swin-UnetR architectures. A dataset of 100 retrospective CTA images was split
into training (n=80), validation (n=10), and testing (n=10). Performance was
assessed using the Dice Coefficient and Hausdorff Distance. Results: Our
approach achieved superior segmentation accuracy, with Dice Coefficients of
0.91 $\pm$ 0.07 for TL, 0.88 $\pm$ 0.18 for FL, and 0.47 $\pm$ 0.25 for FLT,
outperforming Yao et al. (1), who reported 0.78 $\pm$ 0.20, 0.68 $\pm$ 0.18,
and 0.25 $\pm$ 0.31, respectively. Conclusion: The proposed pipelines provide
accurate segmentation of TBAD features, enabling derivation of morphological
parameters for surveillance and treatment planning

</details>


### [325] [Cardiovascular disease classification using radiomics and geometric features from cardiac CT](https://arxiv.org/abs/2506.22226)
*Ajay Mittal,Raghav Mehta,Omar Todd,Philipp Seeböck,Georg Langs,Ben Glocker*

Main category: eess.IV

TL;DR: The paper presents a method for cardiovascular disease (CVD) classification from CT images by decomposing the task into segmentation, registration, and classification, achieving improved performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance the interpretability and accuracy of deep learning-based CVD classification from CT images, addressing the limitation of existing methods which are hard to clinically interpret.

Method: The authors use Atlas-ISTN framework and foundational segmentation models to perform image segmentation and registration, extracting radiomic and geometric features for classification.

Result: The proposed approach improves CVD classification accuracy, achieving 87.50% accuracy compared to 67.50% by models trained on raw CT images.

Conclusion: Breaking down the CVD classification pipeline into segmentation, registration, and feature extraction improves clinical interpretability and classification accuracy, demonstrating the effectiveness of the proposed methodology.

Abstract: Automatic detection and classification of Cardiovascular disease (CVD) from
Computed Tomography (CT) images play an important part in facilitating
better-informed clinical decisions. However, most of the recent deep learning
based methods either directly work on raw CT data or utilize it in pair with
anatomical cardiac structure segmentation by training an end-to-end classifier.
As such, these approaches become much more difficult to interpret from a
clinical perspective. To address this challenge, in this work, we break down
the CVD classification pipeline into three components: (i) image segmentation,
(ii) image registration, and (iii) downstream CVD classification. Specifically,
we utilize the Atlas-ISTN framework and recent segmentation foundational models
to generate anatomical structure segmentation and a normative healthy atlas.
These are further utilized to extract clinically interpretable radiomic
features as well as deformation field based geometric features (through atlas
registration) for CVD classification. Our experiments on the publicly available
ASOCA dataset show that utilizing these features leads to better CVD
classification accuracy (87.50\%) when compared against classification model
trained directly on raw CT images (67.50\%). Our code is publicly available:
https://github.com/biomedia-mira/grc-net

</details>


### [326] [DIGS: Dynamic CBCT Reconstruction using Deformation-Informed 4D Gaussian Splatting and a Low-Rank Free-Form Deformation Model](https://arxiv.org/abs/2506.22280)
*Yuliang Huang,Imraj Singh,Thomas Joyce,Kris Thielemans,Jamie R. McClelland*

Main category: eess.IV

TL;DR: The paper introduces a deformation-informed 4D Gaussian Splatting (4DGS) method for motion-compensated 3D Cone-Beam CT (CBCT) reconstruction, achieving superior image quality and speed over prior techniques.


<details>
  <summary>Details</summary>
Motivation: Motion artifacts caused by breathing variability reduce the effectiveness of CBCT in radiotherapy, and current phase-sorting solutions or implicit motion models are computationally inadequate or inconsistent.

Method: The method employs a free-form deformation (FFD)-based spatial basis function within a deformation-informed framework, ensuring consistency in Gaussian motion properties through a unified deformation field.

Result: The approach achieves a 6x speed improvement over the HexPlane method while producing superior image quality, validated on six CBCT datasets.

Conclusion: Deformation-informed 4DGS significantly advances the efficiency and quality of motion-compensated CBCT reconstruction, underlining its potential clinical utility. The code is publicly accessible for further exploration.

Abstract: 3D Cone-Beam CT (CBCT) is widely used in radiotherapy but suffers from motion
artifacts due to breathing. A common clinical approach mitigates this by
sorting projections into respiratory phases and reconstructing images per
phase, but this does not account for breathing variability. Dynamic CBCT
instead reconstructs images at each projection, capturing continuous motion
without phase sorting. Recent advancements in 4D Gaussian Splatting (4DGS)
offer powerful tools for modeling dynamic scenes, yet their application to
dynamic CBCT remains underexplored. Existing 4DGS methods, such as HexPlane,
use implicit motion representations, which are computationally expensive. While
explicit low-rank motion models have been proposed, they lack spatial
regularization, leading to inconsistencies in Gaussian motion. To address these
limitations, we introduce a free-form deformation (FFD)-based spatial basis
function and a deformation-informed framework that enforces consistency by
coupling the temporal evolution of Gaussian's mean position, scale, and
rotation under a unified deformation field. We evaluate our approach on six
CBCT datasets, demonstrating superior image quality with a 6x speedup over
HexPlane. These results highlight the potential of deformation-informed 4DGS
for efficient, motion-compensated CBCT reconstruction. The code is available at
https://github.com/Yuliang-Huang/DIGS.

</details>


### [327] [Dehazing Light Microscopy Images with Guided Conditional Flow Matching: finding a sweet spot between fidelity and realism](https://arxiv.org/abs/2506.22397)
*Anirban Ray,Ashesh,Florian Jug*

Main category: eess.IV

TL;DR: The paper introduces HazeMatching, a computational method to improve image quality in light microscopy by balancing data fidelity and realism. It outperforms existing methods in achieving this balance and works well with both synthetic and real data.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the problem of hazy image data in accessible microscopy methods like widefield microscopy, which lack the ability to filter out-of-focus light, and to provide a solution that balances high fidelity and perceptual quality for scientific and practical applications.

Method: The authors propose an iterative method called HazeMatching, adapting the conditional flow matching framework. The method incorporates the hazy observation in the conditional velocity field of the generative process. It doesn't rely on an explicit degradation operator, making it suitable for real microscopy data.

Result: HazeMatching was evaluated on five datasets, including both synthetic and real data, and was compared against seven baselines. It achieved a consistent balance between fidelity and realism on average. It also produced well-calibrated predictions, as demonstrated through calibration analysis.

Conclusion: The HazeMatching method combines computational efficiency and practical application to dehaze light microscopy images effectively, achieving a balance between fidelity and realism. It is accessible, without requiring an explicit degradation operator, and all code and datasets will be made publicly available.

Abstract: Fluorescence microscopy is a major driver of scientific progress in the life
sciences. Although high-end confocal microscopes are capable of filtering
out-of-focus light, cheaper and more accessible microscopy modalities, such as
widefield microscopy, can not, which consequently leads to hazy image data.
Computational dehazing is trying to combine the best of both worlds, leading to
cheap microscopy but crisp-looking images. The perception-distortion trade-off
tells us that we can optimize either for data fidelity, e.g. low MSE or high
PSNR, or for data realism, measured by perceptual metrics such as LPIPS or FID.
Existing methods either prioritize fidelity at the expense of realism, or
produce perceptually convincing results that lack quantitative accuracy. In
this work, we propose HazeMatching, a novel iterative method for dehazing light
microscopy images, which effectively balances these objectives. Our goal was to
find a balanced trade-off between the fidelity of the dehazing results and the
realism of individual predictions (samples). We achieve this by adapting the
conditional flow matching framework by guiding the generative process with a
hazy observation in the conditional velocity field. We evaluate HazeMatching on
5 datasets, covering both synthetic and real data, assessing both distortion
and perceptual quality. Our method is compared against 7 baselines, achieving a
consistent balance between fidelity and realism on average. Additionally, with
calibration analysis, we show that HazeMatching produces well-calibrated
predictions. Note that our method does not need an explicit degradation
operator to exist, making it easily applicable on real microscopy data. All
data used for training and evaluation and our code will be publicly available
under a permissive license.

</details>


### [328] [Single-shot HDR using conventional image sensor shutter functions and optical randomization](https://arxiv.org/abs/2506.22426)
*Xiang Dai,Kyrollos Yanny,Kristina Monakhova,Nicholas Antipa*

Main category: eess.IV

TL;DR: This paper presents a novel single-shot HDR imaging method using global reset release (GRR) shutter mode and spatially randomized exposures to handle limitations caused by saturated pixels.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address limitations of classic multi-exposure HDR imaging, such as motion artifacts in dynamic scenes, and the struggles of single-shot HDR methods with extended highlight regions.

Method: The method involves using GRR shutter mode for exposure variation across the image rows and implementing an optical setup that randomly permutes the image onto the sensor. HDR data is reconstructed using an optimization approach with a total variation prior.

Result: Simulation results show that the proposed method outperforms current single-shot methods at high sensor pixel saturation levels (10% or more) and remains competitive at lower saturation (1%). A hardware prototype is also demonstrated, achieving a dynamic range of up to 73dB using an 8-bit sensor.

Conclusion: The approach effectively enhances dynamic range imaging capabilities through a simple, low-cost solution, proving its practicality and better handling of significant pixel saturation in HDR imaging.

Abstract: High-dynamic-range (HDR) imaging is an essential technique for overcoming the
dynamic range limits of image sensors. The classic method relies on multiple
exposures, which slows capture time, resulting in motion artifacts when imaging
dynamic scenes. Single-shot HDR imaging alleviates this issue by encoding HDR
data into a single exposure, then computationally recovering it. Many
established methods use strong image priors to recover improperly exposed image
detail. These approaches struggle with extended highlight regions. We utilize
the global reset release (GRR) shutter mode of an off-the-shelf sensor. GRR
shutter mode applies a longer exposure time to rows closer to the bottom of the
sensor. We use optics that relay a randomly permuted (shuffled) image onto the
sensor, effectively creating spatially randomized exposures across the scene.
The exposure diversity allows us to recover HDR data by solving an optimization
problem with a simple total variation image prior. In simulation, we
demonstrate that our method outperforms other single-shot methods when many
sensor pixels are saturated (10% or more), and is competitive at a modest
saturation (1%). Finally, we demonstrate a physical lab prototype that uses an
off-the-shelf random fiber bundle for the optical shuffling. The fiber bundle
is coupled to a low-cost commercial sensor operating in GRR shutter mode. Our
prototype achieves a dynamic range of up to 73dB using an 8-bit sensor with
48dB dynamic range.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [329] [Universal Modelling of Autocovariance Functions via Spline Kernels](https://arxiv.org/abs/2506.21953)
*Lachlan Astfalck*

Main category: stat.ME

TL;DR: The paper presents a flexible, closed-form non-parametric method for modeling autocovariance functions (ACF) using the inverse Fourier transform of B-spline spectral bases, addressing a gap in existing methods.


<details>
  <summary>Details</summary>
Motivation: Current methods for time-series and spatial analysis demand more flexible and non-parametric approaches to model autocovariance functions, especially as classical parametric models are often insufficient.

Method: The proposed method derives the inverse Fourier transform of B-spline spectral bases, resulting in a new class of non-parametric ACFs that is dense under an $L^1$ metric and supports univariate, multivariate, and non-separable structures.

Result: The approach achieves theoretical guarantees such as Jackson-type approximation bounds and demonstrates strong performance on both simulated and real-world datasets, ensuring accurate recovery of processes.

Conclusion: This method offers a practical and theoretically robust technique for constructing flexible non-parametric ACFs, addressing limitations in existing spectral-domain methods.

Abstract: Flexible modelling of the autocovariance function (ACF) is central to
time-series, spatial, and spatio-temporal analysis. Modern applications often
demand flexibility beyond classical parametric models, motivating
non-parametric descriptions of the ACF. Bochner's Theorem guarantees that any
positive spectral measure yields a valid ACF via the inverse Fourier transform;
however, existing non-parametric approaches in the spectral domain rarely
return closed-form expressions for the ACF itself. We develop a flexible,
closed-form class of non-parametric ACFs by deriving the inverse Fourier
transform of B-spline spectral bases with arbitrary degree and knot placement.
This yields a general class of ACF with three key features: (i) it is provably
dense, under an $L^1$ metric, in the space of weakly stationary, mean-square
continuous ACFs with mild regularity conditions; (ii) it accommodates
univariate, multivariate, and multidimensional processes; and (iii) it
naturally supports non-separable structure without requiring explicit
imposition. Jackson-type approximation bounds establish convergence rates, and
empirical results on simulated and real-world data demonstrate accurate process
recovery. The method provides a practical and theoretically grounded approach
for constructing a non-parametric class of ACF.

</details>


### [330] [Using Large Language Models to Suggest Informative Prior Distributions in Bayesian Statistics](https://arxiv.org/abs/2506.21964)
*Michael A. Riegler,Kristoffer Herland Hellton,Vajira Thambawita,Hugo L. Hammer*

Main category: stat.ME

TL;DR: This paper evaluates the use of large-language models (LLMs) like Claude, Gemini, and ChatGPT in suggesting priors for Bayesian statistics, highlighting their potential and challenges in calibration.


<details>
  <summary>Details</summary>
Motivation: Selecting prior distributions in Bayesian statistics is challenging and subjective, necessitating an efficient and objective approach to overcome these constraints.

Method: The authors prompted LLMs to suggest, verify, and reflect on informative priors for two datasets, evaluating their performance using Kullback-Leibler divergence from maximum likelihood estimators.

Result: While all LLMs identified correct associations, Claude and Gemini performed better than ChatGPT for moderate priors. ChatGPT and Gemini defaulted to unnecessarily vague priors for weak priors, while Claude showed a distinct advantage.

Conclusion: LLMs show promise in developing efficient and objective informative priors, but challenges in calibration to avoid over- or under-confidence remain a limitation.

Abstract: Selecting prior distributions in Bayesian statistics is challenging,
resource-intensive, and subjective. We analyze using large-language models
(LLMs) to suggest suitable, knowledge-based informative priors. We developed an
extensive prompt asking LLMs not only to suggest priors but also to verify and
reflect on their choices.
  We evaluated Claude Opus, Gemini 2.5 Pro, and ChatGPT-4o-mini on two real
datasets: heart disease risk and concrete strength. All LLMs correctly
identified the direction for all associations (e.g., that heart disease risk is
higher for males). The quality of suggested priors was measured by their
Kullback-Leibler divergence from the maximum likelihood estimator's
distribution.
  The LLMs suggested both moderately and weakly informative priors. The
moderate priors were often overconfident, resulting in distributions misaligned
with the data. In our experiments, Claude and Gemini provided better priors
than ChatGPT. For weakly informative priors, a key performance difference
emerged: ChatGPT and Gemini defaulted to an "unnecessarily vague" mean of 0,
while Claude did not, demonstrating a significant advantage.
  The ability of LLMs to identify correct associations shows their great
potential as an efficient, objective method for developing informative priors.
However, the primary challenge remains in calibrating the width of these priors
to avoid over- and under-confidence.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [331] [Under the Hood of BlotchyQuasar: DLL-Based RAT Campaigns Against Latin America](https://arxiv.org/abs/2506.22323)
*Alessio Di Santo*

Main category: cs.CR

TL;DR: A phishing campaign in Latin America targets users with malicious MSI files that lead to malware focusing on stealing credentials and banking information.


<details>
  <summary>Details</summary>
Motivation: To combat the rising threat of deceptive emails in Latin America that exploit security vulnerabilities for financial gain.

Method: The campaign uses DLL side-loading via Valve Corporation executables to bypass defenses, paired with QuasarRAT variant software for data theft and persistence.

Result: Key data such as browser credentials and banking information are stolen through fake login windows, with encryption methods ensuring efficient data exfiltration.

Conclusion: Despite flaws in the malware's coding, its rapid deployment and advanced tactics highlight the urgency of implementing strong cybersecurity measures.

Abstract: A sophisticated malspam campaign was recently uncovered targeting Latin
American countries, with a particular focus on Brazil. This operation utilizes
a highly deceptive phishing email to trick users into executing a malicious MSI
file, initiating a multi-stage infection. The core of the attack leverages DLL
side-loading, where a legitimate executable from Valve Corporation is used to
load a trojanized DLL, thereby bypassing standard security defenses.
  Once active, the malware, a variant of QuasarRAT known as BlotchyQuasar, is
capable of a wide range of malicious activities. It is designed to steal
sensitive browser-stored credentials and banking information, the latter
through fake login windows mimicking well-known Brazilian banks. The threat
establishes persistence by modifying the Windows registry , captures user
keystrokes through keylogging , and exfiltrates stolen data to a
Command-and-Control (C2) server using encrypted payloads. Despite its advanced
capabilities, the malware code exhibits signs of rushed development, with
inefficiencies and poor error handling that suggest the threat actors
prioritized rapid deployment over meticulous design. Nonetheless, the campaign
extensive reach and sophisticated mechanisms pose a serious and immediate
threat to the targeted regions, underscoring the need for robust cybersecurity
defenses.

</details>


### [332] [Reliability Analysis of Smart Contract Execution Architectures: A Comparative Simulation Study](https://arxiv.org/abs/2506.22180)
*Önder Gürcan*

Main category: cs.CR

TL;DR: This study evaluates smart contract execution architectures for securing systems and finds Execute-Order-Validate more reliable and secure.


<details>
  <summary>Details</summary>
Motivation: The increasing complexity and interconnectedness of autonomous systems demand reliable security solutions, prompting the exploration of smart contracts.

Method: The study developed a security evaluation model, created a smart contract-enabled IoT energy case study, and simulated it to assess vulnerabilities.

Result: The Execute-Order-Validate architecture demonstrated greater reliability and security for smart contract execution.

Conclusion: Adopting the Execute-Order-Validate architecture can enhance the reliability and security of smart contract use in complex systems.

Abstract: The industrial market continuously needs reliable solutions to secure
autonomous systems. Especially as these systems become more complex and
interconnected, reliable security solutions are becoming increasingly
important. One promising solution to tackle this challenge is using smart
contracts designed to meet contractual conditions, avoid malicious errors,
secure exchanges, and minimize the need for reliable intermediaries. However,
smart contracts are immutable. Moreover, there are different smart contract
execution architectures (namely Order-Execute and Execute-Order-Validate) that
have different throughputs. In this study, we developed an evaluation model for
assessing the security of reliable smart contract execution. We then developed
a realistic smart contract enabled IoT energy case study. Finally, we simulate
the developed case study to evaluate several smart contract security
vulnerabilities reported in the literature. Our results show that the
Execute-Order-Validate architecture is more promising regarding reliability and
security.

</details>


### [333] [On the Feasibility of Poisoning Text-to-Image AI Models via Adversarial Mislabeling](https://arxiv.org/abs/2506.21874)
*Stanley Wu,Ronik Bhaskar,Anna Yoo Jeong Ha,Shawn Shan,Haitao Zheng,Ben Y. Zhao*

Main category: cs.CR

TL;DR: This paper reveals the vulnerability of Vision-Language Models (VLMs) to adversarial attacks, showing these attacks can poison text-to-image model training pipelines by mislabeling captions. The process consistently alters model behavior with minimal poisoned samples.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to uncover and investigate the risks posed by adversarial attacks on Vision-Language Models (VLMs), essential in generating high-quality image-caption pairs for training text-to-image generative models.

Method: The paper employs adversarial mislabeling attacks to assess how perturbations can manipulate VLM-generated captions, introducing poisoned samples into the text-to-image training pipeline. It further analyzes potential defenses and their vulnerabilities.

Result: Experiments reveal that VLMs are highly susceptible to adversarial perturbations, enabling attackers to inject 'dirty-label' poison samples. Attack success rates exceeded 73% even against commercial black-box VLMs.

Conclusion: Adversarial attacks on VLMs pose significant threats to model training quality and cost-efficiency. The dynamic between attackers and defenders suggests a continual challenge, with the potential to degrade text-to-image model performance and reliability.

Abstract: Today's text-to-image generative models are trained on millions of images
sourced from the Internet, each paired with a detailed caption produced by
Vision-Language Models (VLMs). This part of the training pipeline is critical
for supplying the models with large volumes of high-quality image-caption pairs
during training. However, recent work suggests that VLMs are vulnerable to
stealthy adversarial attacks, where adversarial perturbations are added to
images to mislead the VLMs into producing incorrect captions.
  In this paper, we explore the feasibility of adversarial mislabeling attacks
on VLMs as a mechanism to poisoning training pipelines for text-to-image
models. Our experiments demonstrate that VLMs are highly vulnerable to
adversarial perturbations, allowing attackers to produce benign-looking images
that are consistently miscaptioned by the VLM models. This has the effect of
injecting strong "dirty-label" poison samples into the training pipeline for
text-to-image models, successfully altering their behavior with a small number
of poisoned samples. We find that while potential defenses can be effective,
they can be targeted and circumvented by adaptive attackers. This suggests a
cat-and-mouse game that is likely to reduce the quality of training data and
increase the cost of text-to-image model development. Finally, we demonstrate
the real-world effectiveness of these attacks, achieving high attack success
(over 73%) even in black-box scenarios against commercial VLMs (Google Vertex
AI and Microsoft Azure).

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [334] [Storm Surge in Color: RGB-Encoded Physics-Aware Deep Learning for Storm Surge Forecasting](https://arxiv.org/abs/2506.21743)
*Jinpai Zhao,Albert Cerrone,Eirik Valseth,Leendert Westerink,Clint Dawson*

Main category: cs.CE

TL;DR: This paper introduces a novel methodology for storm surge forecasting by transforming unstructured spatial water elevation data into structured RGB image representations, enabling the use of deep learning models like ConvLSTM.


<details>
  <summary>Details</summary>
Motivation: Existing machine learning approaches for storm surge forecasting have limitations including low spatial resolution, dependence on coastal station data, and poor generalizability, as well as incompatibility with modern deep learning architectures.

Method: The authors propose converting unstructured water elevation fields into RGB-encoded image representations and leveraging ConvLSTM networks for spatiotemporal forecasting. This is combined with dynamic wind field conditioning and static topo-bathymetric inputs.

Result: The model demonstrates reliable 48-hour storm surge forecasting across the Texas coast and exhibits strong adaptability to other coastal regions, validated on a large-scale dataset of synthetic Gulf of Mexico storms.

Conclusion: This work enhances storm surge forecasting through structured data representation, physics-informed inputs, and scalable deep learning models, improving the model’s usability, adaptability, and interpretability.

Abstract: Storm surge forecasting plays a crucial role in coastal disaster
preparedness, yet existing machine learning approaches often suffer from
limited spatial resolution, reliance on coastal station data, and poor
generalization. Moreover, many prior models operate directly on unstructured
spatial data, making them incompatible with modern deep learning architectures.
In this work, we introduce a novel approach that projects unstructured water
elevation fields onto structured Red Green Blue (RGB)-encoded image
representations, enabling the application of Convolutional Long Short Term
Memory (ConvLSTM) networks for end-to-end spatiotemporal surge forecasting. Our
model further integrates ground-truth wind fields as dynamic conditioning
signals and topo-bathymetry as a static input, capturing physically meaningful
drivers of surge evolution. Evaluated on a large-scale dataset of synthetic
storms in the Gulf of Mexico, our method demonstrates robust 48-hour
forecasting performance across multiple regions along the Texas coast and
exhibits strong spatial extensibility to other coastal areas. By combining
structured representation, physically grounded forcings, and scalable deep
learning, this study advances the frontier of storm surge forecasting in
usability, adaptability, and interpretability.

</details>


### [335] [Laser Scan Path Design for Controlled Microstructure in Additive Manufacturing with Integrated Reduced-Order Phase-Field Modeling and Deep Reinforcement Learning](https://arxiv.org/abs/2506.21815)
*Augustine Twumasi,Prokash Chandra Roy,Zixun Li,Soumya Shouvik Bhattacharjee,Zhengtao Gan*

Main category: cs.CE

TL;DR: The study enhances additive manufacturing using L-PBF by integrating physics-guided machine learning to optimize microstructures via tailored scan paths.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of complex microstructures in L-PBF that impact product quality and improve scan path optimization for better control.

Method: Used a phase-field model coupled with machine learning, including a 3D U-Net for prediction and deep reinforcement learning for scan path optimization.

Result: Achieved significant computational speedup and optimized scan paths effectively, outperforming conventional methods.

Conclusion: Machine learning approaches are highly promising for metal additive manufacturing in terms of improving microstructure control and computational efficiency.

Abstract: Laser powder bed fusion (L-PBF) is a widely recognized additive manufacturing
technology for producing intricate metal components with exceptional accuracy.
A key challenge in L-PBF is the formation of complex microstructures affecting
product quality. We propose a physics-guided, machine-learning approach to
optimize scan paths for desired microstructure outcomes, such as equiaxed
grains. We utilized a phase-field method (PFM) to model crystalline grain
structure evolution. To reduce computational costs, we trained a surrogate
machine learning model, a 3D U-Net convolutional neural network, using
single-track phase-field simulations with various laser powers to predict
crystalline grain orientations based on initial microstructure and thermal
history. We investigated three scanning strategies across various hatch
spacings within a square domain, achieving a two-orders-of-magnitude speedup
using the surrogate model. To reduce trial and error in designing laser scan
toolpaths, we used deep reinforcement learning (DRL) to generate optimized scan
paths for target microstructure. Results from three cases demonstrate the DRL
approach's effectiveness. We integrated the surrogate 3D U-Net model into our
DRL environment to accelerate the reinforcement learning training process. The
reward function minimizes both aspect ratio and grain volume of the predicted
microstructure from the agent's scan path. The reinforcement learning algorithm
was benchmarked against conventional zigzag approach for smaller and larger
domains, showing machine learning methods' potential to enhance microstructure
control and computational efficiency in L-PBF optimization.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [336] [Robust and Efficient Autoregressive Speech Synthesis with Dynamic Chunk-wise Prediction Policy](https://arxiv.org/abs/2506.22023)
*Bohan Li,Zhihan Li,Haoran Wang,Hanglei Zhang,Yiwei Guo,Hankun Wang,Xie Chen,Kai Yu*

Main category: cs.SD

TL;DR: This paper introduces the Dynamic Chunk-wise Autoregressive (DCAR) framework for speech synthesis, addressing inefficiencies in traditional autoregressive models by adopting multi-token prediction and chunk-to-frame attention mechanisms.


<details>
  <summary>Details</summary>
Motivation: Traditional autoregressive models in speech synthesis face challenges with long speech sequences, including unstable frame-to-frame attention, high latency, and degraded synthesis quality.

Method: DCAR employs dynamic chunk prediction with multi-token training and chunk-to-frame attention; it uses a lightweight module trained on-policy to adapt to variable speech contexts.

Result: DCAR achieves significant improvements, including up to 72.27% intelligibility enhancement and 2.61x faster inference speeds compared to traditional methods.

Conclusion: DCAR offers a robust, efficient approach to speech synthesis, overcoming limitations in conventional models and serving as a promising foundation for future systems.

Abstract: Recently, autoregressive (AR) language models have emerged as a dominant
approach in speech synthesis, offering expressive generation and scalable
training. However, conventional AR speech synthesis models relying on the
next-token prediction paradigm often encounter significant challenges when
handling long speech sequences. These models often struggle to construct stable
frame-to-frame attention, leading to increased latency and degraded synthesis
quality, thereby limiting their feasibility for real-time applications. To
address these limitations, we introduce a novel dynamic chunk-wise
autoregressive synthesis framework, termed DCAR, designed to enhance both
efficiency and intelligibility robustness in AR speech generation. DCAR
introduces a chunk-to-frame attention mechanism through training with
multi-token prediction, enabling dynamic chunk prediction in variable speech
contexts using a lightweight module trained on-policy. DCAR dynamically adjusts
the token prediction span, significantly reducing the sequence length
dependency while obtaining high synthesis quality. Comprehensive empirical
evaluations demonstrate that DCAR substantially outperforms traditional
next-token prediction models, achieving up to 72.27% intelligibility
improvement and 2.61x inference speedup simultaneously on the test set.
Furthermore, we conduct comprehensive analysis to support it as a versatile
foundation for next-generation speech synthesis systems.

</details>


### [337] [Fine-Tuning MIDI-to-Audio Alignment using a Neural Network on Piano Roll and CQT Representations](https://arxiv.org/abs/2506.22237)
*Sebastian Murgul,Moritz Reiser,Michael Heizmann,Christoph Seibert*

Main category: cs.SD

TL;DR: This paper presents a neural network (CRNN) approach to align human piano recordings with MIDI files, outperforming traditional Dynamic Time Warping (DTW) methods by up to 20% in accuracy.


<details>
  <summary>Details</summary>
Motivation: The goal is to improve the alignment of human piano performance recordings with their corresponding MIDI files, addressing common human timing errors.

Method: The authors use a Convolutional Recurrent Neural Network (CRNN) that processes an unaligned piano roll and spectrogram as inputs to estimate the aligned piano roll. They utilize a dataset with augmented MIDI files simulating human timing errors.

Result: The proposed model outperforms DTW by up to 20% in alignment accuracy. Combining CRNN with DTW further enhances robustness and consistency.

Conclusion: Neural networks, specifically CRNN, show significant promise for improving MIDI-to-audio alignment accuracy, surpassing traditional methods.

Abstract: In this paper, we present a neural network approach for synchronizing audio
recordings of human piano performances with their corresponding loosely aligned
MIDI files. The task is addressed using a Convolutional Recurrent Neural
Network (CRNN) architecture, which effectively captures spectral and temporal
features by processing an unaligned piano roll and a spectrogram as inputs to
estimate the aligned piano roll. To train the network, we create a dataset of
piano pieces with augmented MIDI files that simulate common human timing
errors. The proposed model achieves up to 20% higher alignment accuracy than
the industry-standard Dynamic Time Warping (DTW) method across various
tolerance windows. Furthermore, integrating DTW with the CRNN yields additional
improvements, offering enhanced robustness and consistency. These findings
demonstrate the potential of neural networks in advancing state-of-the-art
MIDI-to-audio alignment.

</details>


### [338] [A Practical Approach to Power Saving in Hearables Using Sub-Nyquist Sampling with Bandwidth Extension](https://arxiv.org/abs/2506.22321)
*Tarikul Islam Tamiti,Anomadarshi Barua*

Main category: cs.SD

TL;DR: This paper proposes the SUBARU framework for implementing low-power multimodal speech enhancement in hearables, addressing practical limitations in existing methods.


<details>
  <summary>Details</summary>
Motivation: To improve multimodal speech enhancement in hearables under noisy conditions by addressing limitations in low-power processing, audio quality, and signal reconstruction.

Method: The SUBARU framework uses sub-Nyquist sampling with low bit resolution, introduces multi-scale/multi-period virtual discriminators for audio quality, and ensures efficient streaming on mobile platforms.

Result: SUBARU achieves 3.31x reduction in power consumption, enables GAN-like audio quality without adversarial training, processes signals efficiently (1.74ms inference), and uses minimal memory (13.77MB).

Conclusion: SUBARU effectively balances low-power consumption, enhanced audio quality, and practical implementation, making it suitable for mobile hearables in noisy environments.

Abstract: Hearables are wearable computers that are worn on the ear. Bone conduction
microphones (BCMs) are used with air conduction microphones (ACMs) in hearables
as a supporting modality for multimodal speech enhancement (SE) in noisy
conditions. However, existing works don't consider the following practical
aspects for low-power implementations on hearables: (i) They do not explore how
lowering the sampling frequencies and bit resolutions in analog-to-digital
converters (ADCs) of hearables jointly impact low-power processing and
multimodal SE in terms of speech quality and intelligibility. (ii) They don't
discuss how GAN-like audio quality can be achieved without using actual GAN
discriminators. And (iii) They don't process signals from ACMs/BCMs at
sub-Nyquist sampling rate because, in their frameworks, they lack a wideband
reconstruction methodology from their narrowband parts. We propose SUBARU
(\textbf{Sub}-Nyquist \textbf{A}udio \textbf{R}esolution \textbf{U}psampling),
which achieves the following: SUBARU (i) intentionally uses sub-Nyquist
sampling and low bit resolution in ADCs, achieving a 3.31x reduction in power
consumption; (ii) introduces novel multi-scale and multi-period virtual
discriminators, which achieve GAN-like audio quality without using GANs'
adversarial training; and (iii) achieves streaming operations on mobile
platforms and SE in in-the-wild noisy conditions with an inference time of
1.74ms and a memory footprint of less than 13.77MB.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [339] [Advanced System Engineering Approaches to Emerging Challenges in Planetary and Deep-Space Exploration](https://arxiv.org/abs/2506.21648)
*J. de Curtò,Cristina LiCalzi,Julien Tubiana Warin,Jack Gehlert,Brian Langbein,Alexandre Gamboa,Chris Sixbey,William Maguire,Santiago Fernández,Álvaro Maestroarena,Alex Brenchley,Logan Maroclo,Philemon Mercado,Joshua DeJohn,Cesar Velez,Ethan Dahmus,Taylor Steinys,David Fritz,I. de Zarzà*

Main category: astro-ph.IM

TL;DR: The paper introduces innovative electronics for planetary and deep-space exploration, including systems for Martian positioning, Titan exploration, orbital techniques, CubeSat designs, and Mars rover power solutions.


<details>
  <summary>Details</summary>
Motivation: To address inadequacies of traditional Earth-based electronic solutions in challenging space environments and promote advanced exploration technologies.

Method: The paper synthesizes findings from diverse mission profiles, focusing on interdisciplinary innovations such as dual-frequency positioning systems, specialized sensor arrays, CubeSat designs, communication chains, and power management systems.

Result: It demonstrates advances in achieving high positioning accuracy, enhanced exploration platforms, optimized designs, and novel solutions for space environments.

Conclusion: The developments signify promising directions for overcoming critical challenges in space exploration, leveraging aerospace, electrical engineering, and planetary science expertise.

Abstract: This paper presents innovative solutions to critical challenges in planetary
and deep-space exploration electronics. We synthesize findings across diverse
mission profiles, highlighting advances in: (1) MARTIAN positioning systems
with dual-frequency transmission to achieve $\pm$1m horizontal accuracy; (2)
artificial reef platforms for Titan's hydrocarbon seas utilizing specialized
sensor arrays and multi-stage communication chains; (3) precision orbital
rendezvous techniques demonstrating novel thermal protection solutions; (4)
miniaturized CubeSat architectures for asteroid exploration with optimized
power-to-mass ratios; and (5) next-generation power management systems for MARS
rovers addressing dust accumulation challenges. These innovations represent
promising directions for future space exploration technologies, particularly in
environments where traditional Earth-based electronic solutions prove
inadequate. The interdisciplinary nature of these developments highlights the
critical intersection of aerospace engineering, electrical engineering, and
planetary science in advancing human exploration capabilities beyond Earth
orbit.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [340] [LLM2Rec: Large Language Models Are Powerful Embedding Models for Sequential Recommendation](https://arxiv.org/abs/2506.21579)
*Yingzhi He,Xiaohao Liu,An Zhang,Yunshan Ma,Tat-Seng Chua*

Main category: cs.IR

TL;DR: The paper introduces LLM2Rec, a sequential recommendation model that integrates collaborative filtering signals and semantic understanding from language models to improve recommendation performance both in-domain and out-of-domain.


<details>
  <summary>Details</summary>
Motivation: Traditional recommendation systems rely heavily on ID-based embeddings that lack generalization to unseen domains, while text-based approaches miss key collaborative filtering (CF) signals. The motivation is to create a model that combines the strengths of both approaches.

Method: The authors propose a two-stage training framework: Collaborative Supervised Fine-tuning to adapt LLMs for item relationships and Item-level Embedding Modeling to encode both semantic and collaborative information in structured embeddings.

Result: LLM2Rec outperformed existing methods, demonstrating enhanced recommendation quality in both in-domain and out-of-domain scenarios based on experiments on real-world datasets.

Conclusion: Integrating LLMs with collaborative filtering signals can result in more robust and generalizable recommendation systems, as evidenced by the success of LLM2Rec. The approach offers a promising direction for future research on sequential recommendation.

Abstract: Sequential recommendation aims to predict users' future interactions by
modeling collaborative filtering (CF) signals from historical behaviors of
similar users or items. Traditional sequential recommenders predominantly rely
on ID-based embeddings, which capture CF signals through high-order
co-occurrence patterns. However, these embeddings depend solely on past
interactions, lacking transferable knowledge to generalize to unseen domains.
Recent advances in large language models (LLMs) have motivated text-based
recommendation approaches that derive item representations from textual
descriptions. While these methods enhance generalization, they fail to encode
CF signals-i.e., latent item correlations and preference patterns-crucial for
effective recommendation. We argue that an ideal embedding model should
seamlessly integrate CF signals with rich semantic representations to improve
both in-domain and out-of-domain recommendation performance.
  To this end, we propose LLM2Rec, a novel embedding model tailored for
sequential recommendation, integrating the rich semantic understanding of LLMs
with CF awareness. Our approach follows a two-stage training framework: (1)
Collaborative Supervised Fine-tuning, which adapts LLMs to infer item
relationships based on historical interactions, and (2) Item-level Embedding
Modeling, which refines these specialized LLMs into structured item embedding
models that encode both semantic and collaborative information. Extensive
experiments on real-world datasets demonstrate that LLM2Rec effectively
improves recommendation quality across both in-domain and out-of-domain
settings. Our findings highlight the potential of leveraging LLMs to build more
robust, generalizable embedding models for sequential recommendation. Our codes
are available at https://github.com/HappyPointer/LLM2Rec.

</details>


### [341] [Evaluating the Robustness of Dense Retrievers in Interdisciplinary Domains](https://arxiv.org/abs/2506.21581)
*Sarthak Chaturvedi,Anurag Acharya,Rounak Meyur,Koby Hayashi,Sai Munikoti,Sameera Horawalavithana*

Main category: cs.IR

TL;DR: Benchmark characteristics significantly impact the measurement of domain adaptation performance in retrieval models, as seen in an environmental document retrieval case study.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address how benchmark characteristics distort evaluations of domain adaptation benefits and lead to misleading conclusions for retrieval model deployments in specialized domains.

Method: Using ColBERTv2, the researchers fine-tuned retrieval models on regulatory documents from federal agencies and evaluated performance using two benchmarks with distinct semantic structures.

Result: The study showed drastically different performance gains (0.61% vs. 2.22% NDCG gain) depending on the benchmark's topic diversity and context overlap, highlighting evaluation framework limitations.

Conclusion: Benchmark selection critically impacts retrieval system evaluations in specialized domains. Systems assessing overlapping semantic structures better mirror real-world complexities, offering valuable insights for deploying AI in interdisciplinary contexts.

Abstract: Evaluation benchmark characteristics may distort the true benefits of domain
adaptation in retrieval models. This creates misleading assessments that
influence deployment decisions in specialized domains. We show that two
benchmarks with drastically different features such as topic diversity,
boundary overlap, and semantic complexity can influence the perceived benefits
of fine-tuning. Using environmental regulatory document retrieval as a case
study, we fine-tune ColBERTv2 model on Environmental Impact Statements (EIS)
from federal agencies. We evaluate these models across two benchmarks with
different semantic structures. Our findings reveal that identical domain
adaptation approaches show very different perceived benefits depending on
evaluation methodology. On one benchmark, with clearly separated topic
boundaries, domain adaptation shows small improvements (maximum 0.61% NDCG
gain). However, on the other benchmark with overlapping semantic structures,
the same models demonstrate large improvements (up to 2.22% NDCG gain), a
3.6-fold difference in the performance benefit. We compare these benchmarks
through topic diversity metrics, finding that the higher-performing benchmark
shows 11% higher average cosine distances between contexts and 23% lower
silhouette scores, directly contributing to the observed performance
difference. These results demonstrate that benchmark selection strongly
determines assessments of retrieval system effectiveness in specialized
domains. Evaluation frameworks with well-separated topics regularly
underestimate domain adaptation benefits, while those with overlapping semantic
boundaries reveal improvements that better reflect real-world regulatory
document complexity. Our findings have important implications for developing
and deploying AI systems for interdisciplinary domains that integrate multiple
topics.

</details>


### [342] [Reinforcement Fine-Tuned Large Language Models for Next POI Recommendation](https://arxiv.org/abs/2506.21599)
*Peibo Li,Shuang Ao,Hao Xue,Yang Song,Maarten de Rijke,Johan Barthélemy,Tomasz Bednarz,Flora D. Salim*

Main category: cs.IR

TL;DR: This paper proposes Refine-POI, a reinforcement fine-tuning framework enabling large language models to improve next point-of-interest recommendation tasks, achieving state-of-the-art top-k performance.


<details>
  <summary>Details</summary>
Motivation: Existing LLM recommenders face challenges with mismatched training data for next POI recommendation, struggling to generate accurate top-k lists with single-target POI data.

Method: The authors introduce Refine-POI, which applies reinforcement fine-tuning with recommendation-driven rewards, allowing models to train for top-k list prediction using one ground-truth POI per example.

Result: Refine-POI outperforms existing methods, achieving state-of-the-art performance in generating top-k recommendation lists in real-world datasets.

Conclusion: Refine-POI resolves the mismatch issue in supervised fine-tuning and demonstrates the potential of reinforcement fine-tuning frameworks for enhancing recommendation tasks with LLMs.

Abstract: Large language models (LLMs) have been adopted for next point-of-interest
(POI) recommendation tasks. Typical LLM-based recommenders fall into two
categories: prompt-based and supervised fine-tuning (SFT)-based models.
Prompt-based models generally offer greater output flexibility but deliver
lower accuracy, whereas SFT-based models achieve higher performance yet face a
fundamental mismatch: next POI recommendation data does not naturally suit
supervised fine-tuning. In SFT, the model is trained to reproduce the exact
ground truth, but each training example provides only a single target POI, so
there is no ground truth for producing a top-k list.
  To address this, we propose Refine-POI, a reinforcement fine-tuning framework
for next POI recommendation. We introduce recommendation-driven rewards that
enable LLMs to learn to generate top-k recommendation lists using only one
ground-truth POI per example. Experiments on real-world datasets demonstrate
that Refine-POI achieves state-of-the-art top-k recommendation performance.

</details>


### [343] [Evaluating VisualRAG: Quantifying Cross-Modal Performance in Enterprise Document Understanding](https://arxiv.org/abs/2506.21604)
*Varun Mannam,Fang Wang,Xin Chen*

Main category: cs.IR

TL;DR: The paper addresses the need for reliable evaluation frameworks in multimodal generative AI for enterprises, introducing a benchmark to assess trustworthiness in VisualRAG systems.


<details>
  <summary>Details</summary>
Motivation: Current generative AI evaluation frameworks are inadequate for enterprise applications requiring high trustworthiness, necessitating a systematic method to measure reliability and trust.

Method: A benchmarking framework is introduced to measure trust measures in multimodal RAG systems, with specific weighting for text, images, captions, and OCR inputs while correlating technical metrics to user-centric trust.

Result: The framework improves trustworthiness by 57.3% using optimal modality weighting, showcasing differences in foundation models' impact on caption generation and OCR extraction.

Conclusion: The study provides a rigorous method for enhancing trustworthiness in multimodal enterprise AI, promoting responsible deployment for critical applications.

Abstract: Current evaluation frameworks for multimodal generative AI struggle to
establish trustworthiness, hindering enterprise adoption where reliability is
paramount. We introduce a systematic, quantitative benchmarking framework to
measure the trustworthiness of progressively integrating cross-modal inputs
such as text, images, captions, and OCR within VisualRAG systems for enterprise
document intelligence. Our approach establishes quantitative relationships
between technical metrics and user-centric trust measures. Evaluation reveals
that optimal modality weighting with weights of 30% text, 15% image, 25%
caption, and 30% OCR improves performance by 57.3% over text-only baselines
while maintaining computational efficiency. We provide comparative assessments
of foundation models, demonstrating their differential impact on
trustworthiness in caption generation and OCR extraction-a vital consideration
for reliable enterprise AI. This work advances responsible AI deployment by
providing a rigorous framework for quantifying and enhancing trustworthiness in
multimodal RAG for critical enterprise applications.

</details>


### [344] [Bayesian-Guided Diversity in Sequential Sampling for Recommender Systems](https://arxiv.org/abs/2506.21617)
*Hiba Bederina,Jill-Jênn Vie*

Main category: cs.IR

TL;DR: The paper introduces a multi-objective, contextual sampling strategy to balance relevance and diversity in recommender systems, demonstrating its effectiveness in improving user experience while maintaining relevance.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from growing concerns about content homogeneity and declining user engagement, necessitating a method to ensure diverse yet relevant recommendations.

Method: The method utilizes a sequential sampling approach with Bayesian updates optimizing diversity metrics, including ridge leverage scores and intra-/inter-batch diversity modeling. A dominance-based ranking technique identifies Pareto-optimal item sets.

Result: The proposed framework significantly boosts content diversity in recommender systems without diminishing relevance.

Conclusion: The approach successfully balances relevance and diversity, providing a practical pathway to enhance user engagement and mitigate content homogeneity in large-scale recommendations.

Abstract: The challenge of balancing user relevance and content diversity in
recommender systems is increasingly critical amid growing concerns about
content homogeneity and reduced user engagement. In this work, we propose a
novel framework that leverages a multi-objective, contextual sequential
sampling strategy. Item selection is guided by Bayesian updates that
dynamically adjust scores to optimize diversity. The reward formulation
integrates multiple diversity metrics-including the log-determinant volume of a
tuned similarity submatrix and ridge leverage scores-along with a diversity
gain uncertainty term to address the exploration-exploitation trade-off. Both
intra- and inter-batch diversity are modeled to promote serendipity and
minimize redundancy. A dominance-based ranking procedure identifies
Pareto-optimal item sets, enabling adaptive and balanced selections at each
iteration. Experiments on a real-world dataset show that our approach
significantly improves diversity without sacrificing relevance, demonstrating
its potential to enhance user experience in large-scale recommendation
settings.

</details>


### [345] [IRanker: Towards Ranking Foundation Model](https://arxiv.org/abs/2506.21638)
*Tao Feng,Zhigang Hua,Zijie Lei,Yan Xie,Shuang Yang,Bo Long,Jiaxuan You*

Main category: cs.IR

TL;DR: IRanker is a unified ranking foundation model framework designed using reinforcement learning and iterative decoding, achieving state-of-the-art results on ranking tasks across multiple domains, and improving zero-shot performance for both in-domain and out-of-domain tasks.


<details>
  <summary>Details</summary>
Motivation: The paper aims to create a unified ranking foundation model to overcome the inefficiencies of designing separate models for individual ranking tasks, addressing the challenge of lacking clear supervision labels for ranking tasks.

Method: IRanker employs reinforcement learning coupled with an iterative decoding approach, where ranking tasks are decomposed into sequential elimination processes to optimize combinatorial output and maximize RL training context efficiency.

Result: The IRanker-3B achieved state-of-the-art results across nine datasets and surpassed larger models in certain scenarios. It demonstrated generalization improvements in zero-shot tasks and enhanced performance in both in-domain ranking tasks and out-of-domain LLM tasks.

Conclusion: IRanker represents a powerful and efficient framework for addressing ranking tasks, showcasing robustness across applications while improving over existing models in scalability, generalization, and performance.

Abstract: Ranking tasks are ubiquitous, encompassing applications such as
recommendation systems, LLM routing, and item re-ranking. We propose to unify
these tasks using a single ranking foundation model (FM), as it eliminates the
need for designing different models for each specific ranking task. However,
unlike general supervision tasks in LLMs, ranking tasks do not have clear
labels for supervision, posing great challenges to developing a ranking FM. To
overcome these challenges, we propose IRanker, a ranking FM framework with
reinforcement learning (RL) and iterative decoding. Our insight is to decompose
the complex ranking task into an iterative decoding process that eliminates the
worst candidate from the candidate pool step by step, which significantly
reduces the output combinatorial space and better utilizes the limited context
length during RL training. We meticulously train and comprehensively evaluate
an IRanker-3B model on nine datasets across three scenarios: recommendation,
routing, and passage ranking. The results show that a single IRanker-3B
achieves state-of-the-art results on several datasets compared to models of
similar size, and even surpasses the performance of larger models on certain
datasets. We further demonstrate the effectiveness of our RL design and the
robustness of the iterative mechanism across different LLM sizes. Moreover, we
conducted both in-domain and out-of-domain zero-shot generalization
experiments, which showed that IRanker-3B achieved good generalization on
in-domain ranking tasks compared to the base LLM by at least 5% improvement.
Surprisingly, on out-of-domain generic LLM tasks, IRanker-3B outperformed the
base model by at least 9% on GSM8K, IFEval, and MathQA. In addition, the
thoughts generated by IRanker-3B during training could further enhance
zero-shot LLM performance.

</details>


### [346] [DCN^2: Interplay of Implicit Collision Weights and Explicit Cross Layers for Large-Scale Recommendation](https://arxiv.org/abs/2506.21624)
*Blaž Škrlj,Yonatan Karni,Grega Gašperšič,Blaž Mramor,Yulia Stolin,Martin Jakomin,Jasna Urbančič,Yuval Dishi,Natalia Silberstein,Ophir Friedler,Assaf Klein*

Main category: cs.IR

TL;DR: DCN^2 is an enhanced version of DCNv2 for recommender systems, achieving superior efficiency and accuracy with three major algorithmic improvements.


<details>
  <summary>Details</summary>
Motivation: To address limitations in DCNv2, such as information loss in Cross layers, management of collisions, and lack of explicit modeling of pairwise similarities.

Method: Three algorithmic improvements are introduced: learnable lookup-level weights, enhanced Cross layers, and a custom layer for pairwise similarity modeling.

Result: The DCN^2 model outperforms DCNv2 in both live recommender systems processing 0.5 billion predictions per second and on four publicly available benchmark datasets.

Conclusion: DCN^2 is a more efficient and competitive architecture for real-world and benchmark use than DCNv2, providing significant performance improvements in online and offline settings.

Abstract: The Deep and Cross architecture (DCNv2) is a robust production baseline and
is integral to numerous real-life recommender systems. Its inherent efficiency
and ability to model interactions often result in models that are both simpler
and highly competitive compared to more computationally demanding alternatives,
such as Deep FFMs. In this work, we introduce three significant algorithmic
improvements to the DCNv2 architecture, detailing their formulation and
behavior at scale. The enhanced architecture we refer to as DCN^2 is actively
used in a live recommender system, processing over 0.5 billion predictions per
second across diverse use cases where it out-performed DCNv2, both offline and
online (ab tests). These improvements effectively address key limitations
observed in the DCNv2, including information loss in Cross layers, implicit
management of collisions through learnable lookup-level weights, and explicit
modeling of pairwise similarities with a custom layer that emulates FFMs'
behavior. The superior performance of DCN^2 is also demonstrated on four
publicly available benchmark data sets.

</details>


### [347] [Hierarchical Patch Compression for ColPali: Efficient Multi-Vector Document Retrieval with Dynamic Pruning and Quantization](https://arxiv.org/abs/2506.21601)
*Duong Bach*

Main category: cs.IR

TL;DR: HPC-ColPali enhances efficiency in multi-vector document retrieval while maintaining accuracy through hierarchical patch compression.


<details>
  <summary>Details</summary>
Motivation: Multi-vector document retrieval systems, while precise, suffer from inefficiencies in storage and computation, necessitating a solution for better scalability.

Method: Introduced a Hierarchical Patch Compression framework with three innovations: K-Means quantization for storage reduction, attention-guided dynamic pruning for computational efficiency, and binary encoding for fast similarity search.

Result: Achieved significant storage reduction, lower query latency (30–50%), and high retrieval precision with minimal accuracy loss in datasets like ViDoRe and SEC-Filings.

Conclusion: HPC-ColPali offers a scalable, efficient solution for multi-vector document retrieval and improves downstream tasks like legal summarization by reducing hallucination rates and latency.

Abstract: Multi-vector document retrieval systems, such as ColPali, excel in
fine-grained matching for complex queries but incur significant storage and
computational costs due to their reliance on high-dimensional patch embeddings
and late-interaction scoring. To address these challenges, we propose
HPC-ColPali, a Hierarchical Patch Compression framework that enhances the
efficiency of ColPali while preserving its retrieval accuracy. Our approach
integrates three innovative techniques: (1) K-Means quantization, which
compresses patch embeddings into 1-byte centroid indices, achieving up to
32$\times$ storage reduction; (2) attention-guided dynamic pruning, utilizing
Vision-Language Model attention weights to retain only the top-$p\%$ most
salient patches, reducing late-interaction computation by up to 60\% with less
than 2\% nDCG@10 loss; and (3) optional binary encoding of centroid indices
into $b$-bit strings ($b=\lceil\log_2 K\rceil$), enabling rapid Hamming
distance-based similarity search for resource-constrained environments.
Evaluated on the ViDoRe and SEC-Filings datasets, HPC-ColPali achieves 30--50\%
lower query latency under HNSW indexing while maintaining high retrieval
precision. When integrated into a Retrieval-Augmented Generation pipeline for
legal summarization, it reduces hallucination rates by 30\% and halves
end-to-end latency. These advancements establish HPC-ColPali as a scalable and
efficient solution for multi-vector document retrieval across diverse
applications. Code is available at https://github.com/DngBack/HPC-ColPali.

</details>


### [348] [ARAG: Agentic Retrieval Augmented Generation for Personalized Recommendation](https://arxiv.org/abs/2506.21931)
*Reza Yousefi Maragheh,Pratheek Vadla,Priyank Gupta,Kai Zhao,Aysenur Inan,Kehui Yao,Jianpeng Xu,Praveen Kanumala,Jason Cho,Sushant Kumar*

Main category: cs.IR

TL;DR: ARAG introduces agentic reasoning into RAG pipelines for personalized recommendations, leading to significant performance improvements in metrics like NDCG@5 and Hit@5.


<details>
  <summary>Details</summary>
Motivation: Existing RAG-based recommendation systems rely on static heuristics and struggle to account for nuanced user preferences in dynamic situations.

Method: ARAG incorporates four specialized LLM-based agents (User Understanding, NLI, Context Summary, and Item Ranker) to enhance personalized recommendations through multi-agent collaboration.

Result: ARAG outperforms traditional RAG approaches, achieving up to 42.1% improvement in NDCG@5 and 35.5% in Hit@5 across three datasets.

Conclusion: Agentic reasoning enhances retrieval-augmented recommendations and provides insights into LLM-based personalization, making ARAG a promising advancement in the field.

Abstract: Retrieval-Augmented Generation (RAG) has shown promise in enhancing
recommendation systems by incorporating external context into large language
model prompts. However, existing RAG-based approaches often rely on static
retrieval heuristics and fail to capture nuanced user preferences in dynamic
recommendation scenarios. In this work, we introduce ARAG, an Agentic
Retrieval-Augmented Generation framework for Personalized Recommendation, which
integrates a multi-agent collaboration mechanism into the RAG pipeline. To
better understand the long-term and session behavior of the user, ARAG
leverages four specialized LLM-based agents: a User Understanding Agent that
summarizes user preferences from long-term and session contexts, a Natural
Language Inference (NLI) Agent that evaluates semantic alignment between
candidate items retrieved by RAG and inferred intent, a context summary agent
that summarizes the findings of NLI agent, and an Item Ranker Agent that
generates a ranked list of recommendations based on contextual fit. We evaluate
ARAG accross three datasets. Experimental results demonstrate that ARAG
significantly outperforms standard RAG and recency-based baselines, achieving
up to 42.1% improvement in NDCG@5 and 35.5% in Hit@5. We also, conduct an
ablation study to analyse the effect by different components of ARAG. Our
findings highlight the effectiveness of integrating agentic reasoning into
retrieval-augmented recommendation and provide new directions for LLM-based
personalization.

</details>


### [349] [CAL-RAG: Retrieval-Augmented Multi-Agent Generation for Content-Aware Layout Design](https://arxiv.org/abs/2506.21934)
*Najmeh Forouzandehmehr,Reza Yousefi Maragheh,Sriram Kollipara,Kai Zhao,Topojoy Biswas,Evren Korpeoglu,Kannan Achan*

Main category: cs.IR

TL;DR: The paper introduces CAL-RAG, a framework combining retrieval, LLMs, and multi-agent systems to improve automated visual layout generation, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Automated layout generation lacks grounding in context and struggles with semantic alignment and visual coherence, needing more effective and intelligent design solutions.

Method: CAL-RAG integrates multimodal retrieval, LLMs, and agent-based reasoning to iteratively propose, evaluate, and refine layouts using structured knowledge and a grading mechanism.

Result: The system achieves state-of-the-art performance on the PKU PosterLayout dataset, outperforming existing methods like LayoutPrompter in metrics such as alignment, overlap, and effectiveness.

Conclusion: Combining retrieval augmentation with step-by-step agentic reasoning provides scalable, accurate, and interpretable solutions for content-aware layout generation.

Abstract: Automated content-aware layout generation -- the task of arranging visual
elements such as text, logos, and underlays on a background canvas -- remains a
fundamental yet under-explored problem in intelligent design systems. While
recent advances in deep generative models and large language models (LLMs) have
shown promise in structured content generation, most existing approaches lack
grounding in contextual design exemplars and fall short in handling semantic
alignment and visual coherence. In this work we introduce CAL-RAG, a
retrieval-augmented, agentic framework for content-aware layout generation that
integrates multimodal retrieval, large language models, and collaborative
agentic reasoning. Our system retrieves relevant layout examples from a
structured knowledge base and invokes an LLM-based layout recommender to
propose structured element placements. A vision-language grader agent evaluates
the layout with visual metrics, and a feedback agent provides targeted
refinements, enabling iterative improvement. We implement our framework using
LangGraph and evaluate it on the PKU PosterLayout dataset, a benchmark rich in
semantic and structural variability. CAL-RAG achieves state-of-the-art
performance across multiple layout metrics -- including underlay effectiveness,
element alignment, and overlap -- substantially outperforming strong baselines
such as LayoutPrompter. These results demonstrate that combining retrieval
augmentation with agentic multi-step reasoning yields a scalable,
interpretable, and high-fidelity solution for automated layout generation.

</details>


### [350] [Literature-Grounded Novelty Assessment of Scientific Ideas](https://arxiv.org/abs/2506.22026)
*Simra Shahid,Marissa Radensky,Raymond Fok,Pao Siangliulue,Daniel S. Weld,Tom Hope*

Main category: cs.IR

TL;DR: The paper introduces an automated system using large language models (LLMs) for evaluating scientific idea novelty, overcoming challenges of manual review.


<details>
  <summary>Details</summary>
Motivation: Manual novelty evaluation of scientific ideas is labor-intensive, subjective, and impractical at scale, necessitating automated solutions.

Method: The authors propose a retrieval-augmented generation framework using a two-stage retrieve-then-rerank approach, leveraging embedding-based filtering and facet-based ranking with LLMs.

Result: The system improves novelty evaluation accuracy by approximately 13% compared to existing methods, emphasizing the significance of facet-based ranking.

Conclusion: The framework provides a scalable and effective solution for novelty assessment in scientific idea generation, reducing reliance on manual evaluation.

Abstract: Automated scientific idea generation systems have made remarkable progress,
yet the automatic evaluation of idea novelty remains a critical and
underexplored challenge. Manual evaluation of novelty through literature review
is labor-intensive, prone to error due to subjectivity, and impractical at
scale. To address these issues, we propose the Idea Novelty Checker, an
LLM-based retrieval-augmented generation (RAG) framework that leverages a
two-stage retrieve-then-rerank approach. The Idea Novelty Checker first
collects a broad set of relevant papers using keyword and snippet-based
retrieval, then refines this collection through embedding-based filtering
followed by facet-based LLM re-ranking. It incorporates expert-labeled examples
to guide the system in comparing papers for novelty evaluation and in
generating literature-grounded reasoning. Our extensive experiments demonstrate
that our novelty checker achieves approximately 13% higher agreement than
existing approaches. Ablation studies further showcases the importance of the
facet-based re-ranker in identifying the most relevant literature for novelty
evaluation.

</details>


### [351] [HyReC: Exploring Hybrid-based Retriever for Chinese](https://arxiv.org/abs/2506.21913)
*Zunran Wang,Zheng Shenpeng,Wang Shenglan,Minghui Zhao,Zhonghua Li*

Main category: cs.IR

TL;DR: The paper introduces HyReC, an end-to-end optimization method for hybrid retrieval in Chinese, enhancing performance through semantic integration and alignment modules.


<details>
  <summary>Details</summary>
Motivation: Hybrid retrieval methods combining dense-vector and lexicon-based approaches have shown promise in various contexts, but their application in Chinese retrieval remains underdeveloped.

Method: The proposed method, HyReC, integrates semantic unions into the representation model, employs the Global-Local-Aware Encoder for consistent semantic sharing, and includes a Normalization Module to foster mutual benefits between dense and lexicon-based retrieval approaches.

Result: HyReC’s effectiveness was demonstrated via evaluation on the C-MTEB retrieval benchmark, showcasing performance improvements.

Conclusion: HyReC provides a tailored solution for hybrid retrieval in Chinese, advancing semantic integration and alignment for improved retrieval outcomes.

Abstract: Hybrid-based retrieval methods, which unify dense-vector and lexicon-based
retrieval, have garnered considerable attention in the industry due to
performance enhancement. However, despite their promising results, the
application of these hybrid paradigms in Chinese retrieval contexts has
remained largely underexplored. In this paper, we introduce HyReC, an
innovative end-to-end optimization method tailored specifically for
hybrid-based retrieval in Chinese. HyReC enhances performance by integrating
the semantic union of terms into the representation model. Additionally, it
features the Global-Local-Aware Encoder (GLAE) to promote consistent semantic
sharing between lexicon-based and dense retrieval while minimizing the
interference between them. To further refine alignment, we incorporate a
Normalization Module (NM) that fosters mutual benefits between the retrieval
approaches. Finally, we evaluate HyReC on the C-MTEB retrieval benchmark to
demonstrate its effectiveness.

</details>


### [352] [Towards Fair Rankings: Leveraging LLMs for Gender Bias Detection and Measurement](https://arxiv.org/abs/2506.22372)
*Maryam Mousavian,Zahra Abbasiantaeb,Mohammad Aliannejadi,Fabio Crestani*

Main category: cs.IR

TL;DR: This paper proposes an LLM-based method and a new fairness metric (CWEx) for detecting and measuring gender bias in passage ranking, releasing a new dataset called MSMGenderBias to improve IR systems.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the limitations in current gender fairness metrics in NLP and IR systems, such as missing subtle gender disparities, and aims to provide a robust approach to detect and evaluate gender bias.

Method: It introduces an LLM-based gender bias detection method, a novel Class-wise Weighted Exposure (CWEx) metric, and annotates a subset of MS MARCO to create the MSMGenderBias collection.

Result: The proposed CWEx metric results in better alignment with human labels (achieving a Cohen's Kappa agreement of 58.77% for Grep-BiasIR and 18.51% for MSMGenderBias) compared to previous metrics, enhancing the precision of gender bias detection.

Conclusion: By integrating LLM-based methods, a novel fairness metric, and annotated data, the paper provides an advanced framework to analyze and mitigate gender bias in IR systems, aiming to foster future research.

Abstract: The presence of social biases in Natural Language Processing (NLP) and
Information Retrieval (IR) systems is an ongoing challenge, which underlines
the importance of developing robust approaches to identifying and evaluating
such biases. In this paper, we aim to address this issue by leveraging Large
Language Models (LLMs) to detect and measure gender bias in passage ranking.
Existing gender fairness metrics rely on lexical- and frequency-based measures,
leading to various limitations, e.g., missing subtle gender disparities.
Building on our LLM-based gender bias detection method, we introduce a novel
gender fairness metric, named Class-wise Weighted Exposure (CWEx), aiming to
address existing limitations. To measure the effectiveness of our proposed
metric and study LLMs' effectiveness in detecting gender bias, we annotate a
subset of the MS MARCO Passage Ranking collection and release our new gender
bias collection, called MSMGenderBias, to foster future research in this area.
Our extensive experimental results on various ranking models show that our
proposed metric offers a more detailed evaluation of fairness compared to
previous metrics, with improved alignment to human labels (58.77% for
Grep-BiasIR, and 18.51% for MSMGenderBias, measured using Cohen's Kappa
agreement), effectively distinguishing gender bias in ranking. By integrating
LLM-driven bias detection, an improved fairness metric, and gender bias
annotations for an established dataset, this work provides a more robust
framework for analyzing and mitigating bias in IR systems.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [353] [Inverse Design of Diffractive Metasurfaces Using Diffusion Models](https://arxiv.org/abs/2506.21748)
*Liav Hen,Erez Yosef,Dan Raviv,Raja Giryes,Jacob Scheuer*

Main category: physics.optics

TL;DR: The paper proposes using generative diffusion models to revolutionize the inverse design of metasurfaces, enabling efficient geometry prediction for desired optical responses.


<details>
  <summary>Details</summary>
Motivation: The traditional inverse design of metasurfaces is highly challenging due to the nonlinear relationships between structure and optical properties, requiring expert tuning, risking local minima, and incurring high computational costs.

Method: They integrate diffusion models into metasurface design workflows to predict geometry based on desired optical responses. Training data is generated using RCWA simulators, and a conditional diffusion model is trained on this data to infer geometry from spatial power distributions.

Result: The method successfully designs metasurfaces like a spatially uniform intensity splitter and a polarization beam splitter with low error and does so in under 30 minutes.

Conclusion: The approach demonstrates a significant improvement in computational efficiency and precision in metasurface design, and the public release of code and datasets enables further advancements in the field.

Abstract: Metasurfaces are ultra-thin optical elements composed of engineered
sub-wavelength structures that enable precise control of light. Their inverse
design - determining a geometry that yields a desired optical response - is
challenging due to the complex, nonlinear relationship between structure and
optical properties. This often requires expert tuning, is prone to local
minima, and involves significant computational overhead. In this work, we
address these challenges by integrating the generative capabilities of
diffusion models into computational design workflows. Using an RCWA simulator,
we generate training data consisting of metasurface geometries and their
corresponding far-field scattering patterns. We then train a conditional
diffusion model to predict meta-atom geometry and height from a target spatial
power distribution at a specified wavelength, sampled from a continuous
supported band. Once trained, the model can generate metasurfaces with low
error, either directly using RCWA-guided posterior sampling or by serving as an
initializer for traditional optimization methods. We demonstrate our approach
on the design of a spatially uniform intensity splitter and a polarization beam
splitter, both produced with low error in under 30 minutes. To support further
research in data-driven metasurface design, we publicly release our code and
datasets.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [354] [DiffSoundStream: Efficient Speech Tokenization via Diffusion Decoding](https://arxiv.org/abs/2506.22362)
*Yang Yang,Yunpeng Li,George Sung,Shao-Fu Shih,Craig Dooley,Alessio Centazzo,Ramanan Rajeswaran*

Main category: eess.AS

TL;DR: This paper proposes DiffSoundStream, which enhances speech tokenization efficiency and achieves competitive speech quality with fewer tokens and faster inference.


<details>
  <summary>Details</summary>
Motivation: Autoregressive models for token-based language modeling in speech generation are limited by inference speed due to the token rate, motivating the need for more efficient approaches.

Method: The paper introduces two techniques: conditioning neural codecs on semantic tokens for redundancy reduction and using latent diffusion models to synthesize waveforms from semantic and coarse-level acoustic tokens.

Result: DiffSoundStream achieves high speech quality at 50 tokens per second, equaling a standard SoundStream at double the token rate. It also demonstrates effective step-size distillation with minimal quality tradeoff using only four diffusion sampling steps.

Conclusion: DiffSoundStream offers a more efficient and high-quality approach to speech tokenization, proving effective in non-streaming scenarios with reduced token rates and improved waveform synthesis.

Abstract: Token-based language modeling is a prominent approach for speech generation,
where tokens are obtained by quantizing features from self-supervised learning
(SSL) models and extracting codes from neural speech codecs, generally referred
to as semantic tokens and acoustic tokens. These tokens are often modeled
autoregressively, with the inference speed being constrained by the token rate.
In this work, we propose DiffSoundStream, a solution that improves the
efficiency of speech tokenization in non-streaming scenarios through two
techniques: (1) conditioning the neural codec on semantic tokens to minimize
redundancy between semantic and acoustic tokens, and (2) leveraging latent
diffusion models to synthesize high-quality waveforms from semantic and
coarse-level acoustic tokens. Experiments show that at 50 tokens per second,
DiffSoundStream achieves speech quality on par with a standard SoundStream
model operating at twice the token rate. Additionally, we achieve step-size
distillation using just four diffusion sampling steps with only a minor quality
loss.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [355] [Stochastic Neural Control Barrier Functions](https://arxiv.org/abs/2506.21697)
*Hongchao Zhang,Manan Tayal,Jackson Cox,Pushpak Jagtap,Shishir Kolathaya,Andrew Clark*

Main category: eess.SY

TL;DR: The paper discusses the synthesis and verification of stochastic neural control barrier functions (SNCBFs) for safety in control systems, proposing new frameworks for smooth and ReLU-based SNCBFs.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the gap in ensuring safety guarantees for stochastic settings of neural control barrier functions (NCBFs), as existing efforts mainly focus on deterministic cases.

Method: The authors propose two frameworks: a verification-free synthesis for smooth SNCBFs and a verification-in-the-loop synthesis framework for both smooth and ReLU-based SNCBFs.

Result: They validated the proposed frameworks using three models: the inverted pendulum, Darboux, and the unicycle, demonstrating their capability to ensure safety in stochastic control systems.

Conclusion: The study successfully introduces methods to synthesize and ensure safety of SNCBFs in stochastic settings, expanding beyond deterministic approaches for control systems.

Abstract: Control Barrier Functions (CBFs) are utilized to ensure the safety of control
systems. CBFs act as safety filters in order to provide safety guarantees
without compromising system performance. These safety guarantees rely on the
construction of valid CBFs. Due to their complexity, CBFs can be represented by
neural networks, known as neural CBFs (NCBFs). Existing works on the
verification of the NCBF focus on the synthesis and verification of NCBFs in
deterministic settings, leaving the stochastic NCBFs (SNCBFs) less studied. In
this work, we propose a verifiably safe synthesis for SNCBFs. We consider the
cases of smooth SNCBFs with twice-differentiable activation functions and
SNCBFs that utilize the Rectified Linear Unit or ReLU activation function. We
propose a verification-free synthesis framework for smooth SNCBFs and a
verification-in-the-loop synthesis framework for both smooth and ReLU SNCBFs.
and we validate our frameworks in three cases, namely, the inverted pendulum,
Darboux, and the unicycle model.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [356] [3Description: An Intuitive Human-AI Collaborative 3D Modeling Approach](https://arxiv.org/abs/2506.21845)
*Zhuodi Cai*

Main category: cs.HC

TL;DR: The paper introduces 3Description, a web-based tool that enables non-experts to co-create 3D models using verbal and gesture inputs, leveraging AI technologies.


<details>
  <summary>Details</summary>
Motivation: To address the accessibility and usability challenges in traditional 3D modeling, empowering non-professionals to participate in 3D design.

Method: The study combines qualitative research, product analysis, and user testing to develop 3Description, integrating Natural Language Processing and Computer Vision technologies.

Result: The tool allows users to describe and adjust 3D models through verbal and gestural interactions on a web platform, enhancing inclusivity in 3D modeling.

Conclusion: 3Description fosters human-AI collaboration and promotes human creativity by making 3D modeling more intuitive and accessible to a broader audience.

Abstract: This paper presents 3Description, an experimental human-AI collaborative
approach for intuitive 3D modeling. 3Description aims to address accessibility
and usability challenges in traditional 3D modeling by enabling
non-professional individuals to co-create 3D models using verbal and gesture
descriptions. Through a combination of qualitative research, product analysis,
and user testing, 3Description integrates AI technologies such as Natural
Language Processing and Computer Vision, powered by OpenAI and MediaPipe.
Recognizing the web has wide cross-platform capabilities, 3Description is
web-based, allowing users to describe the desired model and subsequently adjust
its components using verbal and gestural inputs. In the era of AI and emerging
media, 3Description not only contributes to a more inclusive and user-friendly
design process, empowering more people to participate in the construction of
the future 3D world, but also strives to increase human engagement in
co-creation with AI, thereby avoiding undue surrender to technology and
preserving human creativity.

</details>


### [357] [Adapting University Policies for Generative AI: Opportunities, Challenges, and Policy Solutions in Higher Education](https://arxiv.org/abs/2506.22231)
*Russell Beale*

Main category: cs.HC

TL;DR: The paper examines the integration of generative AI tools, like ChatGPT, in higher education, highlighting their potential benefits, challenges, and the need for robust policies to ensure integrity and equity.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the rapid infusion of generative AI tools in academia, balancing their productivity benefits against concerns of academic integrity, ethics, and equitable access.

Method: The paper critically analyzes opportunities, challenges, and policy needs of generative AI in higher education through empirical data, case studies, and suggested redesigns for AI-resilient assessments.

Result: The paper found that nearly 47% of students use LLMs in coursework, detection achieves 88% accuracy, and there is a clear need for multi-layered enforcement, training, and policy adaptation.

Conclusion: Proactive policies are essential to maximize AI's benefits while safeguarding academic integrity and equity in higher education institutions incorporating generative AI tools.

Abstract: The rapid proliferation of generative artificial intelligence (AI) tools -
especially large language models (LLMs) such as ChatGPT - has ushered in a
transformative era in higher education. Universities in developed regions are
increasingly integrating these technologies into research, teaching, and
assessment. On one hand, LLMs can enhance productivity by streamlining
literature reviews, facilitating idea generation, assisting with coding and
data analysis, and even supporting grant proposal drafting. On the other hand,
their use raises significant concerns regarding academic integrity, ethical
boundaries, and equitable access. Recent empirical studies indicate that nearly
47% of students use LLMs in their coursework - with 39% using them for exam
questions and 7% for entire assignments - while detection tools currently
achieve around 88% accuracy, leaving a 12% error margin. This article
critically examines the opportunities offered by generative AI, explores the
multifaceted challenges it poses, and outlines robust policy solutions.
Emphasis is placed on redesigning assessments to be AI-resilient, enhancing
staff and student training, implementing multi-layered enforcement mechanisms,
and defining acceptable use. By synthesizing data from recent research and case
studies, the article argues that proactive policy adaptation is imperative to
harness AI's potential while safeguarding the core values of academic integrity
and equity.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [358] [RiverEcho: Real-Time Interactive Digital System for Ancient Yellow River Culture](https://arxiv.org/abs/2506.21865)
*Haofeng Wang,Yilin Guo,Zehao Li,Tong Yue,Yizong Wang,Enci Zhang,Rongqun Lin,Feng Gao,Shiqi Wang,Siwei Ma*

Main category: cs.MM

TL;DR: The paper presents RiverEcho, a real-time interactive system enhancing cultural responses about the ancient Yellow River culture using a large language model and a specialized knowledge dataset.


<details>
  <summary>Details</summary>
Motivation: To conserve and inherit the ancient Yellow River culture and help users understand its significance through innovative technology.

Method: The authors designed a system leveraging Retrieval-Augmented Generation (RAG) and a tailored database of historical texts focused on Yellow River culture to enhance response quality via a talking-head digital human interface.

Result: Experimental results show that using RAG with the proposed dataset improves the response quality of Large Language Models, making the system more professional and informative.

Conclusion: RiverEcho not only promotes the ancient Yellow River culture but also enriches users' understanding through AI-powered technology.

Abstract: The Yellow River is China's mother river and a cradle of human civilization.
The ancient Yellow River culture is, moreover, an indispensable part of human
art history. To conserve and inherit the ancient Yellow River culture, we
designed RiverEcho, a real-time interactive system that responds to voice
queries using a large language model and a cultural knowledge dataset,
delivering explanations through a talking-head digital human. Specifically, we
built a knowledge database focused on the ancient Yellow River culture,
including the collection of historical texts and the processing pipeline.
Experimental results demonstrate that leveraging Retrieval-Augmented Generation
(RAG) on the proposed dataset enhances the response quality of the Large
Language Model(LLM), enabling the system to generate more professional and
informative responses. Our work not only diversifies the means of promoting
Yellow River culture but also provides users with deeper cultural insights.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [359] [Demonstrating Interoperable Channel State Feedback Compression with Machine Learning](https://arxiv.org/abs/2506.21796)
*Dani Korpi,Rachel Wang,Jerry Wang,Abdelrahman Ibrahim,Carl Nuzman,Runxin Wang,Kursat Rasim Mestav,Dustin Zhang,Iraj Saniee,Shawn Winston,Gordana Pavlovic,Wei Ding,William J. Hillery,Chenxi Hao,Ram Thirunagari,Jung Chang,Jeehyun Kim,Bartek Kozicki,Dragan Samardzija,Taesang Yoo,Andreas Maeder,Tingfang Ji,Harish Viswanathan*

Main category: eess.SP

TL;DR: The paper introduces a method to train neural network-based models for compressing and decompressing channel state feedback confidentially, without sharing ML models between user equipment and base stations. Its effectiveness is validated through prototype tests.


<details>
  <summary>Details</summary>
Motivation: ML-based channel feedback compression has shown potential benefits in simulations, but real-life implementations face challenges, especially with interoperable models between different devices and networks.

Method: The study develops and trains ML models for interoperable compression and decompression of channel state feedback in a confidential manner. Prototype user equipment and base stations were used for validation.

Result: The trained ML models improve the accuracy of reconstructed channel information and enhance downlink throughput gains during beamforming, demonstrating practical feasibility.

Conclusion: Confidentially trained ML models can enable effective interoperable channel feedback compression and decompression in real-world applications, paving the way for commercial deployment in 6G networks.

Abstract: Neural network-based compression and decompression of channel state feedback
has been one of the most widely studied applications of machine learning (ML)
in wireless networks. Various simulation-based studies have shown that ML-based
feedback compression can result in reduced overhead and more accurate channel
information. However, to the best of our knowledge, there are no real-life
proofs of concepts demonstrating the benefits of ML-based channel feedback
compression in a practical setting, where the user equipment (UE) and base
station have no access to each others' ML models. In this paper, we present a
novel approach for training interoperable compression and decompression ML
models in a confidential manner, and demonstrate the accuracy of the ensuing
models using prototype UEs and base stations. The performance of the ML-based
channel feedback is measured both in terms of the accuracy of the reconstructed
channel information and achieved downlink throughput gains when using the
channel information for beamforming. The reported measurement results
demonstrate that it is possible to develop an accurate ML-based channel
feedback link without having to share ML models between device and network
vendors. These results pave the way for a practical implementation of ML-based
channel feedback in commercial 6G networks.

</details>


### [360] [From Token to Rhythm: A Multi-Scale Approach for ECG-Language Pretraining](https://arxiv.org/abs/2506.21803)
*Fuying Wang,Jiacheng Xu,Lequan Yu*

Main category: eess.SP

TL;DR: This paper introduces MELP, a model that utilizes multi-scale self-supervised learning for ECG analysis by aligning ECG signals with clinical text across various levels to improve classification and adaptability.


<details>
  <summary>Details</summary>
Motivation: Traditional deep learning methods for ECG analysis require vast manual annotations, which are impractical and resource-intensive.

Method: MELP implements multi-scale pretraining by aligning ECG signals and textual reports at token, beat, and rhythm levels. It also pretrains a cardiology-specific language model to improve clinical text understanding.

Result: Experiments across three public ECG datasets show that MELP consistently outperforms other self-supervised learning methods in tasks like zero-shot classification, linear probing, and transfer learning.

Conclusion: MELP effectively captures the hierarchical nature of ECG signals and demonstrates robust performance and adaptability across various clinical applications.

Abstract: Electrocardiograms (ECGs) play a vital role in monitoring cardiac health and
diagnosing heart diseases. However, traditional deep learning approaches for
ECG analysis rely heavily on large-scale manual annotations, which are both
time-consuming and resource-intensive to obtain. To overcome this limitation,
self-supervised learning (SSL) has emerged as a promising alternative, enabling
the extraction of robust ECG representations that can be efficiently
transferred to various downstream tasks. While previous studies have explored
SSL for ECG pretraining and multi-modal ECG-language alignment, they often fail
to capture the multi-scale nature of ECG signals. As a result, these methods
struggle to learn generalized representations due to their inability to model
the hierarchical structure of ECG data. To address this gap, we introduce MELP,
a novel Multi-scale ECG-Language Pretraining (MELP) model that fully leverages
hierarchical supervision from ECG-text pairs. MELP first pretrains a
cardiology-specific language model to enhance its understanding of clinical
text. It then applies three levels of cross-modal supervision-at the token,
beat, and rhythm levels-to align ECG signals with textual reports, capturing
structured information across different time scales. We evaluate MELP on three
public ECG datasets across multiple tasks, including zero-shot ECG
classification, linear probing, and transfer learning. Experimental results
demonstrate that MELP outperforms existing SSL methods, underscoring its
effectiveness and adaptability across diverse clinical applications. Our code
is available at https://github.com/HKU-MedAI/MELP.

</details>


### [361] [Searching Efficient Deep Architectures for Radar Target Detection using Monte-Carlo Tree Search](https://arxiv.org/abs/2506.21772)
*Noé Lallouet,Tristan Cazenave,Cyrille Enderli,Stéphanie Gourdin*

Main category: eess.SP

TL;DR: This paper utilizes Monte-Carlo Tree Search for Neural Architecture Search (NAS) to design lightweight and efficient neural networks for radar target detection.


<details>
  <summary>Details</summary>
Motivation: To address the high computational complexity of deep neural networks in radar systems and make them suitable for embedded environments.

Method: The paper introduces NAS using Monte-Carlo Tree Search to find efficient neural network architectures optimized for radar detection tasks.

Result: Identifies a novel neural architecture achieving the required radar detection performance with reduced computational complexity compared to traditional designs.

Conclusion: Lightweight networks designed through the proposed method retain high detection accuracy, making them viable for embedded radar systems.

Abstract: Recent research works establish deep neural networks as high performing tools
for radar target detection, especially on challenging environments (presence of
clutter or interferences, multi-target scenarii...). However, the usually large
computational complexity of these networks is one of the factors preventing
them from being widely implemented in embedded radar systems. We propose to
investigate novel neural architecture search (NAS) methods, based on
Monte-Carlo Tree Search (MCTS), for finding neural networks achieving the
required detection performance and striving towards a lower computational
complexity. We evaluate the searched architectures on endoclutter radar
signals, in order to compare their respective performance metrics and
generalization properties. A novel network satisfying the required detection
probability while being significantly lighter than the expert-designed baseline
is proposed.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [362] [Hitchhiking Rides Dataset: Two decades of crowd-sourced records on stochastic traveling](https://arxiv.org/abs/2506.21946)
*Till Wenke*

Main category: cs.CY

TL;DR: This paper analyzes a rare dataset of hitchhiking rides, documenting its origins, patterns, and implications for hitchhiking research.


<details>
  <summary>Details</summary>
Motivation: Hitchhiking remains an underexplored mode of travel due to its decentralized and informal nature.

Method: The paper leveraged crowd-sourced data from platforms like hitchwiki.org, conducting exploratory analyses on spatiotemporal patterns, user behavior, and comments.

Result: The study highlights Europe-centric trends, seasonal patterns, and biases like demographic skew, derived from over 63,000 hitchhiking entries.

Conclusion: Although limited by biases, the dataset provides valuable insights into hitchhiking and offers pathways for future research on this unique travel mode.

Abstract: Hitchhiking, a spontaneous and decentralized mode of travel, has long eluded
systematic study due to its informal nature. This paper presents and analyzes
the largest known structured dataset of hitchhiking rides, comprising over
63,000 entries collected over nearly two decades through platforms associated
with hitchwiki.org and lately on hitchmap.com. By leveraging crowd-sourced
contributions, the dataset captures key spatiotemporal and strategic aspects of
hitchhiking. This work documents the dataset's origins, evolution, and
community-driven maintenance, highlighting its Europe-centric distribution,
seasonal patterns, and reliance on a small number of highly active
contributors. Through exploratory analyses, I examine waiting times, user
behavior, and comment metadata, shedding light on the lived realities of
hitchhikers. While the dataset has inherent biases and limitations - such as
demographic skew and unverifiable entries it offers a rare and valuable window
into an alternative form of mobility. I conclude by outlining future directions
for enriching the dataset and advancing research on hitchhiking as both a
transportation practice and cultural phenomenon.

</details>


### [363] [Exploring the change in scientific readability following the release of ChatGPT](https://arxiv.org/abs/2506.21825)
*Abdulkareem Alsudais*

Main category: cs.CY

TL;DR: This paper evaluates changes in the readability of scientific abstracts on arXiv over time, focusing on shifts after the release of ChatGPT in late 2022, and finds increased complexity overall, with notable changes post-ChatGPT.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to explore the impact of accessible large language models, like ChatGPT, on the readability of scientific writing.

Method: The study assessed the readability of abstracts on arXiv from 2010 to mid-2024 using four standard readability formulas. The scores were aggregated annually and analyzed across eight primary categories to evaluate trends and impacts post-ChatGPT.

Result: The data revealed a steady annual decrease in readability of abstracts. A significant change in readability patterns was detected in 2023 and the first half of 2024, with similar trends across subject categories.

Conclusion: The findings suggest that scientific abstracts are becoming increasingly complex over time, and the release of ChatGPT has had a measurable impact on readability, pointing to AI’s role in shaping scientific communication.

Abstract: The rise and growing popularity of accessible large language models have
raised questions about their impact on various aspects of life, including how
scientists write and publish their research. The primary objective of this
paper is to analyze a dataset consisting of all abstracts posted on arXiv.org
between 2010 and June 7th, 2024, to assess the evolution of their readability
and determine whether significant shifts occurred following the release of
ChatGPT in November 2022. Four standard readability formulas are used to
calculate individual readability scores for each paper, classifying their level
of readability. These scores are then aggregated by year and across the eight
primary categories covered by the platform. The results show a steady annual
decrease in readability, suggesting that abstracts are likely becoming
increasingly complex. Additionally, following the release of ChatGPT, a
significant change in readability is observed for 2023 and the analyzed months
of 2024. Similar trends are found across categories, with most experiencing a
notable change in readability during 2023 and 2024. These findings offer
insights into the broader changes in readability and point to the likely
influence of AI on scientific writing.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [364] [Monte Carlo and quasi-Monte Carlo integration for likelihood functions](https://arxiv.org/abs/2506.21733)
*Yanbo Tang*

Main category: math.ST

TL;DR: The paper analyzes Monte Carlo (MC) and quasi-Monte Carlo (QMC) methods for integration errors in calculating posterior distributions and marginal likelihoods, with an emphasis on how errors scale with data points, grid points, and integral dimensions.


<details>
  <summary>Details</summary>
Motivation: To improve understanding of the error scaling characteristics and performance differences between MC and QMC methods in statistical computation.

Method: The authors derive scaling rates for relative integration errors under MC and QMC methods and examine dimensional behavior, providing theoretical bounds for both low and high-dimensional settings.

Result: QMC outperforms MC under specific conditions when dimension is fixed, but MC scales better in very high-dimensional settings due to slower scaling with dimension.

Conclusion: While QMC is superior in many settings, both methods display limitations in high dimensionality highlighting that further improvements in dimensional scalability are necessary.

Abstract: We compare the integration error of Monte Carlo (MC) and quasi-Monte Carlo
(QMC) methods for approximating the normalizing constant of posterior
distributions and certain marginal likelihoods. In doing so, we characterize
the dependency of the relative and absolute integration errors on the number of
data points ($n$), the number of grid points ($m$) and the dimension of the
integral ($p$). We find that if the dimension of the integral remains fixed as
$n$ and $m$ tend to infinity, the scaling rate of the relative error of MC
integration includes an additional $n^{1/2}\log(n)^{p/2}$ data-dependent
factor, while for QMC this factor is $\log(n)^{p/2}$. In this scenario, QMC
will outperform MC if $\log(m)^{p - 1/2}/\sqrt{mn\log(n)} < 1$, which differs
from the usual result that QMC will outperform MC if $\log(m)^p/m^{1/2} <
1$.The accuracies of MC and QMC methods are also examined in the
high-dimensional setting as $p \rightarrow \infty$, where MC gives more
optimistic results as the scaling in dimension is slower than that of QMC when
the Halton sequence is used to construct the low discrepancy grid; however both
methods display poor dimensional scaling as expected. An additional
contribution of this work is a bound on the high-dimensional scaling of the
star discrepancy for the Halton sequence.

</details>


<div id='stat.OT'></div>

# stat.OT [[Back]](#toc)

### [365] [A Plea for History and Philosophy of Statistics and Machine Learning](https://arxiv.org/abs/2506.22236)
*Hanti Lin*

Main category: stat.OT

TL;DR: The paper explores the need for integrating the history and philosophy of statistics with machine learning, focusing on a philosophical idea rooted in Neyman and Pearson's work and introducing the concept of 'achievabilism.'


<details>
  <summary>Details</summary>
Motivation: The integration of statistics and machine learning is critical due to their historical development alongside each other and the growing importance of machine learning in artificial intelligence.

Method: The paper conducts a case study of a philosophical idea in machine learning, tracing its historical roots and articulating the concept of 'achievabilism.' It also combines methods from both history and philosophy of science and formal epistemology.

Result: A foundational assumption called 'achievabilism' is identified, demonstrating shared practices between frequentist statistics and machine learning, highlighting deeper academic connections.

Conclusion: The integration of history, philosophy, science, statistics, and machine learning can lead to richer foundational insights, with 'achievabilism' serving as a key concept reflecting these interdisciplinary ties.

Abstract: The integration of the history and philosophy of statistics was initiated at
least by Hacking (1965) and advanced by Mayo (1996), but it has not received
sustained follow-up. Yet such integration is more urgent than ever, as the
recent success of artificial intelligence has been driven largely by machine
learning -- a field historically developed alongside statistics. Today, the
boundary between statistics and machine learning is increasingly blurred. What
we now need is integration, twice over: of history and philosophy, and of the
field they engage -- statistics and machine learning. I present a case study of
a philosophical idea in machine learning (and in formal epistemology) whose
root can be traced back to an often under-appreciated insight in Neyman and
Pearson's 1936 work (a follow-up to their 1933 classic). This leads to the
articulation of a foundational assumption -- largely implicit in, but shared
by, the practices of frequentist statistics and machine learning -- which I
call achievabilism. Another integration also emerges at the level of
methodology, combining two ends of the philosophy of science spectrum: history
and philosophy of science on the one hand, and formal epistemology on the other
hand.

</details>
