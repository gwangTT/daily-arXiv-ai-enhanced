<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 19]
- [cs.AR](#cs.AR) [Total: 4]
- [cs.CL](#cs.CL) [Total: 19]
- [cs.CV](#cs.CV) [Total: 19]
- [cs.DC](#cs.DC) [Total: 7]
- [cs.LG](#cs.LG) [Total: 21]
- [cs.PL](#cs.PL) [Total: 2]
- [cs.RO](#cs.RO) [Total: 19]
- [cs.SE](#cs.SE) [Total: 8]
- [q-bio.NC](#q-bio.NC) [Total: 6]
- [stat.ML](#stat.ML) [Total: 3]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [cs.HC](#cs.HC) [Total: 1]
- [math.OC](#math.OC) [Total: 1]
- [cs.CR](#cs.CR) [Total: 6]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Fuzzy, Symbolic, and Contextual: Enhancing LLM Instruction via Cognitive Scaffolding](https://arxiv.org/abs/2508.21204)
*Vanessa Figueiredo*

Main category: cs.AI

TL;DR: The paper investigates how architectural design influences the instructional dialogue behavior of large language models (LLMs), focusing on symbolic scaffolding and memory mechanisms.


<details>
  <summary>Details</summary>
Motivation: The authors aim to understand how architectural features impact reasoning, memory, and responsiveness in LLMs during Socratic tutoring.

Method: The study introduces symbolic scaffolding and memory mechanisms for structured reasoning, evaluates five model variants using expert-designed rubrics, and employs an LLM-based evaluation framework.

Result: Preliminary findings show that their full system outperforms baseline variants, and removing memory or symbolic structures impairs cognitive behaviors.

Conclusion: Architectural scaffolds effectively shape cognitive behaviors in LLMs, aiding structured reasoning and instructional strategies.

Abstract: We study how architectural inductive biases influence the cognitive behavior
of large language models (LLMs) in instructional dialogue. We introduce a
symbolic scaffolding mechanism paired with a short-term memory schema designed
to promote adaptive, structured reasoning in Socratic tutoring. Using
controlled ablation across five system variants, we evaluate model outputs via
expert-designed rubrics covering scaffolding, responsiveness, symbolic
reasoning, and conversational memory. We present preliminary results using an
LLM-based evaluation framework aligned to a cognitively grounded rubric. This
enables scalable, systematic comparisons across architectural variants in
early-stage experimentation. The preliminary results show that our full system
consistently outperforms baseline variants. Analysis reveals that removing
memory or symbolic structure degrades key cognitive behaviors, including
abstraction, adaptive probing, and conceptual continuity. These findings
support a processing-level account in which architectural scaffolds can
reliably shape emergent instructional strategies in LLMs.

</details>


### [2] [Addressing accuracy and hallucination of LLMs in Alzheimer's disease research through knowledge graphs](https://arxiv.org/abs/2508.21238)
*Tingxuan Xu,Jiarui Feng,Justin Melendez,Kaleigh Roberts,Donghong Cai,Mingfang Zhu,Donald Elbert,Yixin Chen,Randall J. Bateman*

Main category: cs.AI

TL;DR: This paper evaluates and compares the effectiveness of GraphRAG systems, specifically focusing on Alzheimer's disease, by building a knowledge base and testing response quality and traceability.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the limitations of LLM-based chatbots in scientific research, such as hallucinations, lack of domain-specific knowledge, and traceability.

Method: The authors assess two GraphRAG systems, create a knowledge base using 50 Alzheimer's-related papers and 70 expert questions, and use GPT-4o for experimentation. They compare its response quality to standard GPT-4o and analyze traceability.

Result: The study highlights the enhanced reliability and traceability of GraphRAG over standard models in responding to Alzheimer's queries.

Conclusion: GraphRAG improves response quality in specific knowledge-intensive domains like Alzheimer's, and the authors provide tools for researchers to test these systems effectively.

Abstract: In the past two years, large language model (LLM)-based chatbots, such as
ChatGPT, have revolutionized various domains by enabling diverse task
completion and question-answering capabilities. However, their application in
scientific research remains constrained by challenges such as hallucinations,
limited domain-specific knowledge, and lack of explainability or traceability
for the response. Graph-based Retrieval-Augmented Generation (GraphRAG) has
emerged as a promising approach to improving chatbot reliability by integrating
domain-specific contextual information before response generation, addressing
some limitations of standard LLMs. Despite its potential, there are only
limited studies that evaluate GraphRAG on specific domains that require
intensive knowledge, like Alzheimer's disease or other biomedical domains. In
this paper, we assess the quality and traceability of two popular GraphRAG
systems. We compile a database of 50 papers and 70 expert questions related to
Alzheimer's disease, construct a GraphRAG knowledge base, and employ GPT-4o as
the LLM for answering queries. We then compare the quality of responses
generated by GraphRAG with those from a standard GPT-4o model. Additionally, we
discuss and evaluate the traceability of several Retrieval-Augmented Generation
(RAG) and GraphRAG systems. Finally, we provide an easy-to-use interface with a
pre-built Alzheimer's disease database for researchers to test the performance
of both standard RAG and GraphRAG.

</details>


### [3] [MultiFluxAI Enhancing Platform Engineering with Advanced Agent-Orchestrated Retrieval Systems](https://arxiv.org/abs/2508.21307)
*Sri Ram Macharla,Sridhar Murthy J,Anjaneyulu Pasala*

Main category: cs.AI

TL;DR: MultiFluxAI merges generative AI, vectorization, and orchestration to dynamically manage diverse data sources for enhanced user interaction.


<details>
  <summary>Details</summary>
Motivation: The paper aims to tackle challenges in managing vast, disparate data sources within product engineering across varied digital domains.

Method: It employs advanced AI tools like Generative AI, vectorization, and agentic orchestration to dynamically integrate and respond to user queries.

Result: The platform successfully provides dynamic, context-aware responses to complex service-related queries, improving user engagement.

Conclusion: MultiFluxAI enhances digital ecosystem interaction by efficiently integrating and managing diverse data sources, offering improved product engineering solutions.

Abstract: MultiFluxAI is an innovative AI platform developed to address the challenges
of managing and integrating vast, disparate data sources in product engineering
across application domains. It addresses both current and new service related
queries that enhance user engagement in the digital ecosystem. This platform
leverages advanced AI techniques, such as Generative AI, vectorization, and
agentic orchestration to provide dynamic and context-aware responses to complex
user queries.

</details>


### [4] [Multi-Ontology Integration with Dual-Axis Propagation for Medical Concept Representation](https://arxiv.org/abs/2508.21320)
*Mohsen Nayebi Kerdabadi,Arya Hadizadeh Moghaddam,Dongjie Wang,Zijun Yao*

Main category: cs.AI

TL;DR: LINKO is a framework to improve medical concept representation learning by leveraging multiple ontology graphs simultaneously with large language models (LLMs).


<details>
  <summary>Details</summary>
Motivation: Existing approaches to medical concept representation often focus on a single ontology or separate ontology systems, failing to integrate them into a unified framework that utilizes cross-ontology relationships.

Method: LINKO improves ontology concept embedding using LLM-augmented initialization with contextual descriptions and enables dual-axis knowledge propagation: intra-ontology (vertical) and inter-ontology (horizontal) learning across hierarchical ontology levels.

Result: Extensive experiments on two public datasets demonstrated LINKO's superior performance over existing methods, especially in handling limited data cases and predicting rare diseases.

Conclusion: LINKO enhances medical concept representation learning and shows strong compatibility as a plug-in encoder for existing EHR predictive models, making it effective in integrating heterogeneous medical ontologies.

Abstract: Medical ontology graphs map external knowledge to medical codes in electronic
health records via structured relationships. By leveraging domain-approved
connections (e.g., parent-child), predictive models can generate richer medical
concept representations by incorporating contextual information from related
concepts. However, existing literature primarily focuses on incorporating
domain knowledge from a single ontology system, or from multiple ontology
systems (e.g., diseases, drugs, and procedures) in isolation, without
integrating them into a unified learning structure. Consequently, concept
representation learning often remains limited to intra-ontology relationships,
overlooking cross-ontology connections. In this paper, we propose LINKO, a
large language model (LLM)-augmented integrative ontology learning framework
that leverages multiple ontology graphs simultaneously by enabling dual-axis
knowledge propagation both within and across heterogeneous ontology systems to
enhance medical concept representation learning. Specifically, LINKO first
employs LLMs to provide a graph-retrieval-augmented initialization for ontology
concept embedding, through an engineered prompt that includes concept
descriptions, and is further augmented with ontology context. Second, our
method jointly learns the medical concepts in diverse ontology graphs by
performing knowledge propagation in two axes: (1) intra-ontology vertical
propagation across hierarchical ontology levels and (2) inter-ontology
horizontal propagation within every level in parallel. Last, through extensive
experiments on two public datasets, we validate the superior performance of
LINKO over state-of-the-art baselines. As a plug-in encoder compatible with
existing EHR predictive models, LINKO further demonstrates enhanced robustness
in scenarios involving limited data availability and rare disease prediction.

</details>


### [5] [Think in Games: Learning to Reason in Games via Reinforcement Learning with Large Language Models](https://arxiv.org/abs/2508.21365)
*Yi Liao,Yu Gu,Yuan Sui,Zining Zhu,Yifan Lu,Guohua Tang,Zhongqian Sun,Wei Yang*

Main category: cs.AI

TL;DR: LLMs struggle with interactive tasks while excelling in reasoning; TiG solves this by enabling LLMs to develop procedural knowledge through interaction using language-guided RL policies.


<details>
  <summary>Details</summary>
Motivation: Existing LLMs fail at procedural tasks that require dynamic decision-making despite their reasoning and declarative knowledge abilities, highlighting the need for improved procedural understanding.

Method: The TiG framework reformulates RL as a language modeling task. LLMs generate and iteratively refine language-based policies through RL based on environmental feedback.

Result: TiG successfully bridges declarative and procedural knowledge gaps while demanding less data and computation. It also provides enhanced transparency through natural language explanations.

Conclusion: TiG enables LLMs to perform dynamic decision-making in interactive environments, making them more interpretable and efficient compared to standard RL approaches.

Abstract: Large language models (LLMs) excel at complex reasoning tasks such as
mathematics and coding, yet they frequently struggle with simple interactive
tasks that young children perform effortlessly. This discrepancy highlights a
critical gap between declarative knowledge (knowing about something) and
procedural knowledge (knowing how to do something). Although traditional
reinforcement learning (RL) agents can acquire procedural knowledge through
environmental interaction, they often operate as black boxes and require
substantial training data. In contrast, LLMs possess extensive world knowledge
and reasoning capabilities, but are unable to effectively convert this static
knowledge into dynamic decision-making in interactive settings. To address this
challenge, we propose Think in Games (TiG), a novel framework that empowers
LLMs to develop procedural understanding through direct interaction with game
environments, while retaining their inherent reasoning and explanatory
abilities. Specifically, TiG reformulates RL-based decision-making as a
language modeling task: LLMs generate language-guided policies, which are
refined iteratively through online reinforcement learning based on
environmental feedback. Our experimental results show that TiG successfully
bridges the gap between declarative and procedural knowledge, achieving
competitive performance with dramatically lower data and computational demands
compared to conventional RL methods. Moreover, TiG provides step-by-step
natural language explanations for its decisions, greatly improving transparency
and interpretability in complex interactive tasks.

</details>


### [6] [AHELM: A Holistic Evaluation of Audio-Language Models](https://arxiv.org/abs/2508.21376)
*Tony Lee,Haoqin Tu,Chi Heem Wong,Zijun Wang,Siwei Yang,Yifan Mai,Yuyin Zhou,Cihang Xie,Percy Liang*

Main category: cs.AI

TL;DR: AHELM introduces a holistic benchmark for evaluating audio-language models (ALMs) across ten key aspects, addressing issues like fairness, safety, and standardized testing structures.


<details>
  <summary>Details</summary>
Motivation: The paper aims to fill gaps in ALM evaluation caused by the lack of standardized benchmarks that comprehensively assess diverse capabilities and qualities, such as fairness and safety.

Method: The researchers developed AHELM, a benchmark aggregating various datasets, including two new datasets (PARADE and CoRe-Bench), and standardized evaluation methods to test 14 ALMs and 3 baseline systems.

Result: Gemini 2.5 Pro performed best in 5 of the 10 evaluated aspects but showed group unfairness in ASR tasks, while baseline systems demonstrated reasonable performance, with one ranking 5th overall.

Conclusion: AHELM provides a transparent and evolving benchmark to holistically measure ALMs, offering insights into strengths and weaknesses across multiple models and promoting continued improvement in this field.

Abstract: Evaluations of audio-language models (ALMs) -- multimodal models that take
interleaved audio and text as input and output text -- are hindered by the lack
of standardized benchmarks; most benchmarks measure only one or two
capabilities and omit evaluative aspects such as fairness or safety.
Furthermore, comparison across models is difficult as separate evaluations test
a limited number of models and use different prompting methods and inference
parameters. To address these shortfalls, we introduce AHELM, a benchmark that
aggregates various datasets -- including 2 new synthetic audio-text datasets
called PARADE, which evaluates the ALMs on avoiding stereotypes, and
CoRe-Bench, which measures reasoning over conversational audio through
inferential multi-turn question answering -- to holistically measure the
performance of ALMs across 10 aspects we have identified as important to the
development and usage of ALMs: audio perception, knowledge, reasoning, emotion
detection, bias, fairness, multilinguality, robustness, toxicity, and safety.
We also standardize the prompts, inference parameters, and evaluation metrics
to ensure equitable comparisons across models. We test 14 open-weight and
closed-API ALMs from 3 developers and 3 additional simple baseline systems each
consisting of an automatic speech recognizer and a language model. Our results
show that while Gemini 2.5 Pro ranks top in 5 out of 10 aspects, it exhibits
group unfairness ($p=0.01$) on ASR tasks whereas most of the other models do
not. We also find that the baseline systems perform reasonably well on AHELM,
with one ranking 5th overall despite having only speech-to-text capabilities.
For transparency, all raw prompts, model generations, and outputs are available
on our website at https://crfm.stanford.edu/helm/audio/v1.0.0. AHELM is
intended to be a living benchmark and new datasets and models will be added
over time.

</details>


### [7] [AI Compute Architecture and Evolution Trends](https://arxiv.org/abs/2508.21394)
*Bor-Sung Liang*

Main category: cs.AI

TL;DR: The paper introduces a seven-layer AI compute architecture model and explores opportunities and challenges in AI development across various perspectives, focusing on the evolution of large-scale language models (LLMs).


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the practical challenges of AI development while providing a structured perspective on its opportunities, emphasizing the layered compute architecture.

Method: A seven-layer model for AI compute architecture is proposed, analyzing technical layers and their impact on AI evolution using insights from large-scale language models and the internet industry.

Result: The paper describes the trajectory, key technologies for each layer, and explores trends ranging from computing architecture issues to the evolution of AI agents and ecosystems.

Conclusion: The analysis predicts the future trajectory of AI development, highlighting both technical and economic challenges in creating self-sustainable AI ecosystems.

Abstract: The focus of AI development has shifted from academic research to practical
applications. However, AI development faces numerous challenges at various
levels. This article will attempt to analyze the opportunities and challenges
of AI from several different perspectives using a structured approach. This
article proposes a seven-layer model for AI compute architecture, including
Physical Layer, Link Layer, Neural Network Layer, Context Layer, Agent Layer,
Orchestrator Layer, and Application Layer, from bottom to top. It also explains
how AI computing has evolved into this 7-layer architecture through the
three-stage evolution on large-scale language models (LLMs). For each layer, we
describe the development trajectory and key technologies. In Layers 1 and 2 we
discuss AI computing issues and the impact of Scale-Up and Scale-Out strategies
on computing architecture. In Layer 3 we explore two different development
paths for LLMs. In Layer 4 we discuss the impact of contextual memory on LLMs
and compares it to traditional processor memory. In Layers 5 to 7 we discuss
the trends of AI agents and explore the issues in evolution from a single AI
agent to an AI-based ecosystem, and their impact on the AI industry.
Furthermore, AI development involves not only technical challenges but also the
economic issues to build self-sustainable ecosystem. This article analyzes the
internet industry to provide predictions on the future trajectory of AI
development.

</details>


### [8] [CARJAN: Agent-Based Generation and Simulation of Traffic Scenarios with AJAN](https://arxiv.org/abs/2508.21411)
*Leonard Frank Neis,Andre Antakli,Matthias Klusch*

Main category: cs.AI

TL;DR: CARJAN is an integrated tool combining AJAN and CARLA for modeling and simulating urban traffic with intelligent, interacting agents.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of modeling and simulating urban traffic scenarios with various agents like pedestrians, cyclists, and autonomous vehicles.

Method: Utilizes the AJAN framework and CARLA simulator, providing SPARQL Behavior Tree decision-making for agent interactions and a visual interface for scenario modeling.

Result: CARJAN enables semi-automated creation, storage, and simulation of complex, dynamic traffic scenarios, leveraging intelligent agent-based techniques.

Conclusion: CARJAN improves traffic scenario simulation by offering a scalable and integrated approach to intelligent agent-based modeling and decision-making.

Abstract: User-friendly modeling and virtual simulation of urban traffic scenarios with
different types of interacting agents such as pedestrians, cyclists and
autonomous vehicles remains a challenge. We present CARJAN, a novel tool for
semi-automated generation and simulation of such scenarios based on the
multi-agent engineering framework AJAN and the driving simulator CARLA. CARJAN
provides a visual user interface for the modeling, storage and maintenance of
traffic scenario layouts, and leverages SPARQL Behavior Tree-based
decision-making and interactions for agents in dynamic scenario simulations in
CARLA. CARJAN provides a first integrated approach for interactive, intelligent
agent-based generation and simulation of virtual traffic scenarios in CARLA.

</details>


### [9] [A General Framework of Epistemic Forgetting and its Instantiation by Ranking Functions](https://arxiv.org/abs/2508.21441)
*Christoph Beierle,Alexander Hahn,Diana Howey,Gabriele Kern-Isberner,Kai Sauerwald*

Main category: cs.AI

TL;DR: The paper explores various types of forgetting mechanisms used in knowledge management, particularly in epistemic states, and evaluates their effectiveness using axioms inspired by existing frameworks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to better understand how forgetting can be applied at richer epistemic levels, moving beyond its traditional use in classical logics and belief revision.

Method: The authors introduce five epistemic forgetting types, propose seven concrete operations for Spohn's ranking functions, and use axioms derived from logic programming and AGM theory to evaluate these operations.

Result: Seven concrete forgetting operations were examined across five epistemic forgetting types. They were systematically evaluated against newly proposed axioms, revealing differences and similarities in their performance.

Conclusion: The study provides a structured understanding of epistemic forgetting, enriching the theoretical landscape and offering insights into the nuanced behaviors of various forgetting operations.

Abstract: Forgetting as a knowledge management operation deliberately ignores parts of
the knowledge and beliefs of an agent, for various reasons. Forgetting has many
facets, one may want to forget parts of the syntax, a proposition, or a
conditional. In the literature, two main operators suitable for performing
forgetting have been proposed and investigated in depth: First, variable
elimination is a syntactical method that blends out certain atomic variables to
focus on the rest of the language. It has been mainly used in the area of logic
programming and answer set programming. Second, contraction in AGM belief
revision theory effectively removes propositions from belief sets under logical
deduction. Both operations rely mainly on classical logics. In this article, we
take an epistemic perspective and study forgetting operations in epistemic
states with richer semantic structures, but with clear links to propositional
logic. This allows us to investigate what forgetting in the epistemic
background means, thereby lifting well-known and novel forgetting operations to
the epistemic level. We present five general types of epistemic forgetting and
instantiate them with seven concrete forgetting operations for Spohn's ranking
functions. We take inspiration from postulates of forgetting both from logic
programming and AGM theory to propose a rich landscape of axioms for evaluating
forgetting operations. Finally, we evaluate all concrete forgetting operations
according to all postulates, leading to a novel comprehensive overview
highlighting differences and commonalities among the forgetting operators.

</details>


### [10] [Learning Lifted Action Models From Traces of Incomplete Actions and States](https://arxiv.org/abs/2508.21449)
*Niklas Jansen,Jonas Gösgens,Hector Geffner*

Main category: cs.AI

TL;DR: The paper addresses the problem of learning a lifted STRIPS model in a minimal information setting with missing predicates and implicit action arguments using an extension called STRIPS+ and a novel learning algorithm called SYNTH.


<details>
  <summary>Details</summary>
Motivation: The work is motivated by the limitations of previous model-learning approaches that assume full STRIPS states or fully observable predicates and actions, which are unrealistic in many practical scenarios.

Method: The paper introduces STRIPS+, a variant of STRIPS allowing implicit arguments and limited existential quantification, and proposes SYNTH, an algorithm that generates stratified precondition expressions to learn STRIPS+ models from state-action traces.

Result: The authors prove the correctness and completeness of the SYNTH algorithm and validate its scalability using traces derived from established STRIPS domains.

Conclusion: STRIPS+ effectively models partially observable states and implicit action arguments, and SYNTH is a robust and scalable solution for learning in this realistic setting.

Abstract: Consider the problem of learning a lifted STRIPS model of the sliding-tile
puzzle from random state-action traces where the states represent the location
of the tiles only, and the actions are the labels up, down, left, and right,
with no arguments. Two challenges are involved in this problem. First, the
states are not full STRIPS states, as some predicates are missing, like the
atoms representing the position of the ``blank''. Second, the actions are not
full STRIPS either, as they do not reveal all the objects involved in the
actions effects and preconditions. Previous approaches have addressed different
versions of this model learning problem, but most assume that actions in the
traces are full STRIPS actions or that the domain predicates are all
observable. The new setting considered in this work is more ``realistic'', as
the atoms observed convey the state of the world but not full STRIPS states,
and the actions reveal the arguments needed for selecting the action but not
the ones needed for modeling it in STRIPS. For formulating and addressing the
learning problem, we introduce a variant of STRIPS, which we call STRIPS+,
where certain STRIPS action arguments can be left implicit in preconditions
which can also involve a limited form of existential quantification. The
learning problem becomes the problem of learning STRIPS+ models from STRIPS+
state-action traces. For this, the proposed learning algorithm, called SYNTH,
constructs a stratified sequence (conjunction) of precondition expressions or
``queries'' for each action, that denote unique objects in the state and ground
the implicit action arguments in STRIPS+. The correctness and completeness of
SYNTH is established, and its scalability is tested on state-action traces
obtained from STRIPS+ models derived from existing STRIPS domains.

</details>


### [11] [MMSearch-Plus: A Simple Yet Challenging Benchmark for Multimodal Browsing Agents](https://arxiv.org/abs/2508.21475)
*Xijia Tao,Yihua Teng,Xinxing Su,Xinyu Fu,Jihao Wu,Chaofan Tao,Ziru Liu,Haoli Bai,Rui Liu,Lingpeng Kong*

Main category: cs.AI

TL;DR: The paper introduces MMSearch-Plus, a benchmark for evaluating multimodal language models (MLLMs) on challenging tasks that require fine-grained visual reasoning and iterative tool use. It demonstrates the limitations of both closed and open-source models under these tasks.


<details>
  <summary>Details</summary>
Motivation: Multimodal browsing benchmarks often rely on shallow workflows that do not adequately test critical capabilities like visual reasoning, verification, and tool use in MLLMs.

Method: MMSearch-Plus consists of 311 tasks with elements that require multimodal understanding, enabling the evaluation of various MLLMs using a model-agnostic agent framework with browsing tools.

Result: The best-performing closed MLLM achieved 36.0% accuracy with the framework, while a strong open-source model performed significantly worse at only 6.9%. Failures in source verification and long-horizon planning were highlighted.

Conclusion: MMSearch-Plus highlights the gap in performance and robustness of current MLLMs in tasks demanding complex multimodal reasoning, posing significant challenges for future model improvement.

Abstract: Large multimodal language models (MLLMs) are increasingly deployed as web
agents, yet many multimodal browsing benchmarks can be solved by shallow, fixed
workflows that lean on high-recall image search and nearby text-masking the
genuinely multimodal challenges of fine-grained visual reasoning, provenance
verification, and long-horizon tool use. We introduce MMSearch-Plus, a
benchmark of 311 tasks that highly demand multimodal understanding while
preserving the difficulty profile of strong text-only browsing suites. Each
item is constructed to contain multiple weak, localized visual signals that
must be extracted, propagated through iterative text-image search, and
cross-validated under retrieval noise before answering. Our curation procedure,
Spatial-Temporal Extrapolation, seeds questions whose answers require
extrapolating from spatial cues (micro-text, part-level appearance, layouts,
signage) and temporal traces (broadcast overlays, seasonal context) to
out-of-image facts such as events, dates, and venues. We provide a
model-agnostic agent framework with browsing tools and evaluate a range of
closed and open MLLMs. The strongest agent (o3) attains 15.1% without search
and 36.0% accuracy with rollout under our framework, while a strong open-source
model (Qwen-2.5-VL-72B-Instruct) achieves 0.0% without search and 6.9% after 20
rounds of search. Beyond answer accuracy, we assess bounding-box production and
cropped-image search, and conduct an error analysis that surfaces failures in
source verification, part-based reasoning, and long-horizon planning.

</details>


### [12] [Modeling Wise Decision Making: A Z-Number Fuzzy Framework Inspired by Phronesis](https://arxiv.org/abs/2508.21517)
*Sweta Kaman,Ankita Sharma,Romi Banerjee*

Main category: cs.AI

TL;DR: The paper proposes a computational framework using fuzzy inference systems with Z numbers to measure wisdom as a multidimensional construct, addressing limitations of current self-report measures.


<details>
  <summary>Details</summary>
Motivation: To improve the measurement of wisdom—which incorporates multidimensional aspects like intellectual humility and uncertainty—through computational methods rather than traditional self-reports.

Method: The study used a computational framework based on fuzzy inference systems with Z numbers to analyze participants' linguistic responses to moral dilemmas. Scores were calculated using Gaussian kernel density estimation and mapped through 21 rules.

Result: The system correlated significantly with existing wisdom scales while showing negligible relations with unrelated traits, supporting validity.

Conclusion: The framework enhances the understanding and measurement of wisdom by embracing multidimensionality and uncertainty, with applications in psychology and AI for safer, human-like reasoning systems.

Abstract: Background: Wisdom is a superordinate construct that embraces perspective
taking, reflectiveness, prosocial orientation, reflective empathetic action,
and intellectual humility. Unlike conventional models of reasoning that are
rigidly bound by binary thinking, wisdom unfolds in shades of ambiguity,
requiring both graded evaluation and self-reflective humility. Current measures
depend on self-reports and seldom reflect the humility and uncertainty inherent
in wise reasoning. A computational framework that takes into account both
multidimensionality and confidence has the potential to improve psychological
science and allow humane AI. Method: We present a fuzzy inference system with Z
numbers, each of the decisions being expressed in terms of a wisdom score
(restriction) and confidence score (certainty). As part of this study,
participants (N = 100) were exposed to culturally neutral pictorial moral
dilemma tasks to which they generated think-aloud linguistic responses, which
were mapped into five theoretically based components of wisdom. The scores of
each individual component were combined using a base of 21 rules, with
membership functions tuned via Gaussian kernel density estimation. Results: In
a proof of concept study, the system produced dual attribute wisdom
representations that correlated modestly but significantly with established
scales while showing negligible relations with unrelated traits, supporting
convergent and divergent validity. Contribution: The contribution is to
formalize wisdom as a multidimensional, uncertainty-conscious construct,
operationalized in the form of Z-numbers. In addition to progressing
measurement in psychology, it calculates how fuzzy Z numbers can provide AI
systems with interpretable, confidence-sensitive reasoning that affords a safe,
middle ground between rigorous computation and human-like judgment.

</details>


### [13] [Counterfactual Scenarios for Automated Planning](https://arxiv.org/abs/2508.21521)
*Nicola Gigante,Francesco Leofante,Andrea Micheli*

Main category: cs.AI

TL;DR: This paper introduces a new explanation method in Automated Planning that focuses on counterfactual scenarios to identify minimal changes to planning problems for achieving desired properties, with practical computational complexity.


<details>
  <summary>Details</summary>
Motivation: Existing Counterfactual Explanations in Automated Planning focus on minimal plan modifications to achieve different goals but fail to address higher-level problem properties.

Method: The authors propose counterfactual scenarios where minimal changes are made to a planning problem to ensure plans satisfy desired high-level formula properties. They study two qualitative instantiations and analyze computational complexity across different change types.

Result: The computational complexity of generating counterfactual scenarios is shown to often be on par with that of computing a plan for the original problem, proving the feasibility of this approach.

Conclusion: The proposed framework provides a practical, computationally viable tool for constructing algorithms to identify counterfactual scenarios in planning.

Abstract: Counterfactual Explanations (CEs) are a powerful technique used to explain
Machine Learning models by showing how the input to a model should be minimally
changed for the model to produce a different output. Similar proposals have
been made in the context of Automated Planning, where CEs have been
characterised in terms of minimal modifications to an existing plan that would
result in the satisfaction of a different goal. While such explanations may
help diagnose faults and reason about the characteristics of a plan, they fail
to capture higher-level properties of the problem being solved. To address this
limitation, we propose a novel explanation paradigm that is based on
counterfactual scenarios. In particular, given a planning problem $P$ and an
\ltlf formula $\psi$ defining desired properties of a plan, counterfactual
scenarios identify minimal modifications to $P$ such that it admits plans that
comply with $\psi$. In this paper, we present two qualitative instantiations of
counterfactual scenarios based on an explicit quantification over plans that
must satisfy $\psi$. We then characterise the computational complexity of
generating such counterfactual scenarios when different types of changes are
allowed on $P$. We show that producing counterfactual scenarios is often only
as expensive as computing a plan for $P$, thus demonstrating the practical
viability of our proposal and ultimately providing a framework to construct
practical algorithms in this area.

</details>


### [14] [HealthProcessAI: A Technical Framework and Proof-of-Concept for LLM-Enhanced Healthcare Process Mining](https://arxiv.org/abs/2508.21540)
*Eduardo Illueca-Fernandez,Kaile Chen,Fernando Seoane,Farhad Abtahi*

Main category: cs.AI

TL;DR: HealthProcessAI leverages GenAI and LLMs to simplify process mining for healthcare workflows, improving accessibility and generating actionable insights.


<details>
  <summary>Details</summary>
Motivation: The paper aims to overcome barriers in applying process mining to healthcare workflows, such as technical complexity and lack of accessible training resources.

Method: The study develops HealthProcessAI, a GenAI framework integrating Python and R libraries with multiple LLMs for automated process map interpretation and report generation.

Result: The framework was validated using sepsis progression data, successfully processed it in four scenarios, and demonstrated high consistency scores with LLMs like Claude Sonnet-4 (3.79/4.0) and Gemini 2.5-Pro (3.65/4.0).

Conclusion: HealthProcessAI simplifies process mining outputs for diverse users, presenting a methodological advancement in healthcare analytics and enabling actionable insights.

Abstract: Process mining has emerged as a powerful analytical technique for
understanding complex healthcare workflows. However, its application faces
significant barriers, including technical complexity, a lack of standardized
approaches, and limited access to practical training resources. We introduce
HealthProcessAI, a GenAI framework designed to simplify process mining
applications in healthcare and epidemiology by providing a comprehensive
wrapper around existing Python (PM4PY) and R (bupaR) libraries. To address
unfamiliarity and improve accessibility, the framework integrates multiple
Large Language Models (LLMs) for automated process map interpretation and
report generation, helping translate technical analyses into outputs that
diverse users can readily understand. We validated the framework using sepsis
progression data as a proof-of-concept example and compared the outputs of five
state-of-the-art LLM models through the OpenRouter platform. To test its
functionality, the framework successfully processed sepsis data across four
proof-of-concept scenarios, demonstrating robust technical performance and its
capability to generate reports through automated LLM analysis. LLM evaluation
using five independent LLMs as automated evaluators revealed distinct model
strengths: Claude Sonnet-4 and Gemini 2.5-Pro achieved the highest consistency
scores (3.79/4.0 and 3.65/4.0) when evaluated by automated LLM assessors. By
integrating multiple Large Language Models (LLMs) for automated interpretation
and report generation, the framework addresses widespread unfamiliarity with
process mining outputs, making them more accessible to clinicians, data
scientists, and researchers. This structured analytics and AI-driven
interpretation combination represents a novel methodological advance in
translating complex process mining results into potentially actionable insights
for healthcare applications.

</details>


### [15] [Revisiting Landmarks: Learning from Previous Plans to Generalize over Problem Instances](https://arxiv.org/abs/2508.21564)
*Issa Hanou,Sebastijan Dumančić,Mathijs de Weerdt*

Main category: cs.AI

TL;DR: A novel framework for discovering generalized landmarks using state functions to enhance automated planning.


<details>
  <summary>Details</summary>
Motivation: Traditional landmark extraction algorithms struggle to capture intermediate goals across domains, especially with repetition.

Method: The authors use state functions independent of specific objects to construct directed graphs representing landmark progression with loops for repetitive tasks.

Result: Generalized landmark graphs derived from smaller instances scale well to larger problem instances, showing significant gains when repetition loops are identified.

Conclusion: Generalized landmarks offer interpretable, domain-level insights that improve planning efficiency using limited sample data.

Abstract: We propose a new framework for discovering landmarks that automatically
generalize across a domain. These generalized landmarks are learned from a set
of solved instances and describe intermediate goals for planning problems where
traditional landmark extraction algorithms fall short. Our generalized
landmarks extend beyond the predicates of a domain by using state functions
that are independent of the objects of a specific problem and apply to all
similar objects, thus capturing repetition. Based on these functions, we
construct a directed generalized landmark graph that defines the landmark
progression, including loop possibilities for repetitive subplans. We show how
to use this graph in a heuristic to solve new problem instances of the same
domain. Our results show that the generalized landmark graphs learned from a
few small instances are also effective for larger instances in the same domain.
If a loop that indicates repetition is identified, we see a significant
improvement in heuristic performance over the baseline. Generalized landmarks
capture domain information that is interpretable and useful to an automated
planner. This information can be discovered from a small set of plans for the
same domain.

</details>


### [16] [Scalable Solution Methods for Dec-POMDPs with Deterministic Dynamics](https://arxiv.org/abs/2508.21595)
*Yang You,Alex Schutz,Zhikun Li,Bruno Lacerda,Robert Skilton,Nick Hawes*

Main category: cs.AI

TL;DR: This paper introduces Deterministic Decentralized POMDPs (Det-Dec-POMDPs) and proposes a new solver, IDPP, for efficiently solving large-scale problems in this class.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the inefficiency of existing solvers in handling large-scale, deterministic multi-agent planning problems.

Method: The proposed method, IDPP, is built upon the Joint Equilibrium Search for Policies framework and tailored specifically for Det-Dec-POMDPs.

Result: IDPP is shown to efficiently tackle large-scale deterministic decentralized problems, overcoming limitations of current solvers.

Conclusion: Det-Dec-POMDPs provide a framework for deterministic multi-agent planning, and IDPP effectively solves these problems at scale.

Abstract: Many high-level multi-agent planning problems, including multi-robot
navigation and path planning, can be effectively modeled using deterministic
actions and observations.
  In this work, we focus on such domains and introduce the class of
Deterministic Decentralized POMDPs (Det-Dec-POMDPs). This is a subclass of
Dec-POMDPs characterized by deterministic transitions and observations
conditioned on the state and joint actions.
  We then propose a practical solver called Iterative Deterministic POMDP
Planning (IDPP). This method builds on the classic Joint Equilibrium Search for
Policies framework and is specifically optimized to handle large-scale
Det-Dec-POMDPs that current Dec-POMDP solvers are unable to address
efficiently.

</details>


### [17] [Integrating Large Language Models with Network Optimization for Interactive and Explainable Supply Chain Planning: A Real-World Case Study](https://arxiv.org/abs/2508.21622)
*Saravanan Venkatachalam*

Main category: cs.AI

TL;DR: An integrated framework combines network optimization with LLMs to enhance supply chain decision-making through summaries, visualizations, and tailored KPIs.


<details>
  <summary>Details</summary>
Motivation: Traditional network optimization models often produce complex outputs that are challenging for business stakeholders to interpret, creating a need for enhanced decision support systems that are more explainable and interactive.

Method: The paper merges mixed-integer tactical inventory optimization models with AI agents, RESTful APIs, and dynamic user interfaces to generate descriptive insights, visual aids, and real-time interactions.

Result: A case study illustrates improvements in supply chain performance such as preventing stockouts, reducing costs, and maintaining service levels.

Conclusion: The framework bridges operations research and stakeholder usability, with planned enhancements leveraging advanced machine learning techniques for adaptability and real-time functions.

Abstract: This paper presents an integrated framework that combines traditional network
optimization models with large language models (LLMs) to deliver interactive,
explainable, and role-aware decision support for supply chain planning. The
proposed system bridges the gap between complex operations research outputs and
business stakeholder understanding by generating natural language summaries,
contextual visualizations, and tailored key performance indicators (KPIs). The
core optimization model addresses tactical inventory redistribution across a
network of distribution centers for multi-period and multi-item, using a
mixed-integer formulation. The technical architecture incorporates AI agents,
RESTful APIs, and a dynamic user interface to support real-time interaction,
configuration updates, and simulation-based insights. A case study demonstrates
how the system improves planning outcomes by preventing stockouts, reducing
costs, and maintaining service levels. Future extensions include integrating
private LLMs, transfer learning, reinforcement learning, and Bayesian neural
networks to enhance explainability, adaptability, and real-time
decision-making.

</details>


### [18] [A-MHA*: Anytime Multi-Heuristic A*](https://arxiv.org/abs/2508.21637)
*Ramkumar Natarajan,Muhammad Suhail Saleem,William Xiao,Sandip Aine,Howie Choset,Maxim Likhachev*

Main category: cs.AI

TL;DR: The paper extends Multi-Heuristic A* (MHA*) to an anytime framework (A-MHA*), allowing for continuous solution improvement over time while maintaining completeness and suboptimal guarantees.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of MHA*, which generates only a one-shot solution and requires careful configuration of inflation factors, by providing a framework that can iteratively improve solutions over time.

Method: Adapt concepts from the Anytime Repairing A* (ARA*) algorithm to the MHA* framework, enabling iterative improvement of solutions while ensuring the original algorithm's guarantees.

Result: The proposed A-MHA* algorithm was evaluated in the 3-D path planning domain and sliding tiles puzzle, showing competitive performance against MHA* and other anytime algorithms.

Conclusion: The A-MHA* enhances MHA* by transforming it into an anytime algorithm that provides quick suboptimal solutions and continuous improvements, broadening its practical applications while retaining theoretical guarantees.

Abstract: Designing good heuristic functions for graph search requires adequate domain
knowledge. It is often easy to design heuristics that perform well and
correlate with the underlying true cost-to-go values in certain parts of the
search space but these may not be admissible throughout the domain thereby
affecting the optimality guarantees of the search. Bounded suboptimal search
using several such partially good but inadmissible heuristics was developed in
Multi-Heuristic A* (MHA*). Although MHA* leverages multiple inadmissible
heuristics to potentially generate a faster suboptimal solution, the original
version does not improve the solution over time. It is a one shot algorithm
that requires careful setting of inflation factors to obtain a desired one time
solution. In this work, we tackle this issue by extending MHA* to an anytime
version that finds a feasible suboptimal solution quickly and continually
improves it until time runs out. Our work is inspired from the Anytime
Repairing A* (ARA*) algorithm. We prove that our precise adaptation of ARA*
concepts in the MHA* framework preserves the original suboptimal and
completeness guarantees and enhances MHA* to perform in an anytime fashion.
Furthermore, we report the performance of A-MHA* in 3-D path planning domain
and sliding tiles puzzle and compare against MHA* and other anytime algorithms.

</details>


### [19] [Leveraging Imperfection with MEDLEY A Multi-Model Approach Harnessing Bias in Medical AI](https://arxiv.org/abs/2508.21648)
*Farhad Abtahi,Mehdi Astaraki,Fernando Seoane*

Main category: cs.AI

TL;DR: MEDLEY is a framework that embraces diversity in AI model outputs to enhance medical reasoning, challenging traditional notions of bias in AI as a defect.


<details>
  <summary>Details</summary>
Motivation: Traditional approaches to medical AI aim to eliminate bias, but human reasoning itself is inherently biased, suggesting a need to reconsider how bias could be utilized as a resource.

Method: The MEDLEY framework orchestrates multiple AI models, preserving diverse outputs and documenting biases and hallucinations as provisional hypotheses for clinician verification. A demonstrator used over 30 large language models.

Result: The proof-of-concept demonstrator showed that MEDLEY can preserve diverse diagnostic perspectives, including minority views, while making biases and diagnostic uncertainty transparent for clinical oversight.

Conclusion: MEDLEY reframes AI imperfection from a defect to a resource, potentially improving trustworthiness and opening ethical and regulatory innovation opportunities for medical AI systems.

Abstract: Bias in medical artificial intelligence is conventionally viewed as a defect
requiring elimination. However, human reasoning inherently incorporates biases
shaped by education, culture, and experience, suggesting their presence may be
inevitable and potentially valuable. We propose MEDLEY (Medical Ensemble
Diagnostic system with Leveraged diversitY), a conceptual framework that
orchestrates multiple AI models while preserving their diverse outputs rather
than collapsing them into a consensus. Unlike traditional approaches that
suppress disagreement, MEDLEY documents model-specific biases as potential
strengths and treats hallucinations as provisional hypotheses for clinician
verification. A proof-of-concept demonstrator was developed using over 30 large
language models, creating a minimum viable product that preserved both
consensus and minority views in synthetic cases, making diagnostic uncertainty
and latent biases transparent for clinical oversight. While not yet a validated
clinical tool, the demonstration illustrates how structured diversity can
enhance medical reasoning under clinician supervision. By reframing AI
imperfection as a resource, MEDLEY offers a paradigm shift that opens new
regulatory, ethical, and innovation pathways for developing trustworthy medical
AI systems.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [20] [Catwalk: Unary Top-K for Efficient Ramp-No-Leak Neuron Design for Temporal Neural Networks](https://arxiv.org/abs/2508.21267)
*Devon Lister,Prabhu Vellaisamy,John Paul Shen,Di Wu*

Main category: cs.AR

TL;DR: Temporal Neural Networks (TNNs) can achieve greater efficiency in hardware design using a proposed Catwalk neuron, which optimizes spike processing.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the inefficiency in the current CMOS-based neuron implementations in TNNs, which do not fully exploit the sparsity in spike volleys during computation.

Method: Proposes a Catwalk neuron implementation that relocates spikes via unary top-k sorting, reducing the computational burden. This optimization decreases the cost of parallel counter operations for processing spike inputs.

Result: The proposed Catwalk neuron achieves 1.39x improvement in area and 1.86x efficiency in power compared to existing SRM0-RNL neurons, based on place-and-route results.

Conclusion: Relocating spikes through unary top-k and leveraging input sparsity can significantly improve hardware area and power efficiency in neuromorphic networks, making the Catwalk neuron a viable enhancement for TNNs.

Abstract: Temporal neural networks (TNNs) are neuromorphic neural networks that utilize
bit-serial temporal coding. TNNs are composed of columns, which in turn employ
neurons as their building blocks. Each neuron processes volleys of input
spikes, modulated by associated synaptic weights, on its dendritic inputs.
Recently proposed neuron implementation in CMOS employs a Spike Response Model
(SRM) with a ramp-no-leak (RNL) response function and assumes all the inputs
can carry spikes. However, in actual spike volleys, only a small subset of the
dendritic inputs actually carry spikes in each compute cycle. This form of
sparsity can be exploited to achieve better hardware efficiency. In this paper,
we propose a Catwalk neuron implementation by relocating spikes in a spike
volley as a sorted subset cluster via unary top-k. Such relocation can
significantly reduce the cost of the subsequent parallel counter (PC) for
accumulating the response functions from the spiking inputs. This can lead to
improvements on area and power efficiency in RNL neuron implementation.
Place-and-route results show Catwalk is 1.39x and 1.86x better in area and
power, respectively, as compared to existing SRM0-RNL neurons.

</details>


### [21] [SCE-NTT: A Hardware Accelerator for Number Theoretic Transform Using Superconductor Electronics](https://arxiv.org/abs/2508.21265)
*Sasan Razmkhah,Mingye Li,Zeming Cheng,Robert S. Aviles,Kyle Jackman,Joey Delport,Lieze Schindler,Wenhui Luo,Takuya Suzuki,Mehdi Kamal,Christopher L. Ayala,Coenrad J. Fourie,Nabuyuki Yoshikawa,Peter A. Beerel,Sandeep Gupta,Massoud Pedram*

Main category: cs.AR

TL;DR: The paper presents a high-performance superconductor-based accelerator, SCE-NTT, designed to optimize Number-Theoretic Transform computations for Fully Homomorphic Encryption, achieving over 100x speedup compared to CMOS technology.


<details>
  <summary>Details</summary>
Motivation: To overcome computational bottlenecks in fully homomorphic encryption (FHE) schemes, particularly in Number-Theoretic Transform (NTT), and to push performance beyond CMOS limits for secure post-quantum computation.

Method: The paper proposes SCE-NTT architecture based on SFQ logic and memory, employs shift register memory to counter SFQ constraints, and develops a deeply pipelined 128-point NTT design optimized with advanced modular multipliers and clocking schemes.

Result: The SCE-NTT unit achieves 531 million NTT/sec at 34 GHz, making it over 100x faster than CMOS equivalents. Additionally, it scales to larger NTT sizes with high throughput, exemplified by 2^14-point NTT computations in 482 ns.

Conclusion: Superconductor-based accelerators show strong promise for enhancing scalable and energy-efficient secure computation, particularly in the post-quantum era, with projected performance improvements tied to future fabrication advancements.

Abstract: This research explores the use of superconductor electronics (SCE) for
accelerating fully homomorphic encryption (FHE), focusing on the
Number-Theoretic Transform (NTT), a key computational bottleneck in FHE
schemes. We present SCE-NTT, a dedicated hardware accelerator based on
superconductive single flux quantum (SFQ) logic and memory, targeting high
performance and energy efficiency beyond the limits of conventional CMOS. To
address SFQ constraints such as limited dense RAM and restricted fanin/fanout,
we propose a deeply pipelined NTT-128 architecture using shift register memory
(SRM). Designed for N=128 32-bit coefficients, NTT-128 comprises log2(N)=7
processing elements (PEs), each featuring a butterfly unit (BU), dual
coefficient memories operating in ping-pong mode via FIFO-based SRM queues, and
twiddle factor buffers. The BU integrates a Shoup modular multiplier optimized
for a small area, leveraging precomputed twiddle factors. A new RSFQ cell
library with over 50 parameterized cells, including compound logic units, was
developed for implementation. Functional and timing correctness were validated
using JoSIM analog simulations and Verilog models. A multiphase clocking scheme
was employed to enhance robustness and reduce path-balancing overhead,
improving circuit reliability. Fabricated results show the NTT-128 unit
achieves 531 million NTT/sec at 34 GHz, over 100x faster than state-of-the-art
CMOS equivalents. We also project that the architecture can scale to larger
sizes, such as a 2^14-point NTT in approximately 482 ns. Key-switch throughput
is estimated at 1.63 million operations/sec, significantly exceeding existing
hardware. These results demonstrate the strong potential of SCE-based
accelerators for scalable, energy-efficient secure computation in the
post-quantum era, with further gains anticipated through advances in
fabrication.

</details>


### [22] [SIRA: Scaled-Integer Range Analysis for Optimizing FPGA Dataflow Neural Network Accelerators](https://arxiv.org/abs/2508.21493)
*Yaman Umuroglu,Christoph Berganski,Felix Jentzsch,Michal Danilowicz,Tomasz Kryjak,Charalampos Bezaitis,Magnus Sjalander,Ian Colbert,Thomas Preusser,Jakoba Petri-Koenig,Michaela Blott*

Main category: cs.AR

TL;DR: The paper introduces the SIRA methodology, which leverages static analysis for optimized hardware realization of non-matrix operations in quantized neural networks, significantly reducing resource utilization.


<details>
  <summary>Details</summary>
Motivation: Aggressive quantization of neural networks can reveal non-matrix operations as bottlenecks in embedded systems, necessitating tailored precision across operations to address performance and resource concerns.

Method: The paper proposes SIRA, a static range analysis technique based on interval arithmetic, enabling tailored bitwidth optimization, scale and bias aggregation, and alternative computation conversions for neural network operations.

Result: SIRA optimizations implemented in FINN framework demonstrate reductions of 17% in LUTs, 66% in DSPs, and 22% in accumulator bitwidths across various workloads.

Conclusion: SIRA provides significant resource savings, offers analytical guidance for implementation, and is open-sourced to encourage community exploration across different hardware and applications.

Abstract: While neural network quantization effectively reduces the cost of matrix
multiplications, aggressive quantization can expose non-matrix-multiply
operations as significant performance and resource bottlenecks on embedded
systems. Addressing such bottlenecks requires a comprehensive approach to
tailoring the precision across operations in the inference computation. To this
end, we introduce scaled-integer range analysis (SIRA), a static analysis
technique employing interval arithmetic to determine the range, scale, and bias
for tensors in quantized neural networks. We show how this information can be
exploited to reduce the resource footprint of FPGA dataflow neural network
accelerators via tailored bitwidth adaptation for accumulators and downstream
operations, aggregation of scales and biases, and conversion of consecutive
elementwise operations to thresholding operations. We integrate SIRA-driven
optimizations into the open-source FINN framework, then evaluate their
effectiveness across a range of quantized neural network workloads and compare
implementation alternatives for non-matrix-multiply operations. We demonstrate
an average reduction of 17% for LUTs, 66% for DSPs, and 22% for accumulator
bitwidths with SIRA optimizations, providing detailed benchmark analysis and
analytical models to guide the implementation style for non-matrix layers.
Finally, we open-source SIRA to facilitate community exploration of its
benefits across various applications and hardware platforms.

</details>


### [23] [Binary Weight Multi-Bit Activation Quantization for Compute-in-Memory CNN Accelerators](https://arxiv.org/abs/2508.21524)
*Wenyong Zhou,Zhengwu Liu,Yuan Ren,Ngai Wong*

Main category: cs.AR

TL;DR: The paper proposes a compute-in-memory acceleration method for convolutional neural networks, optimizing a balance between hardware efficiency and accuracy through binary weight and multi-bit activation quantization.


<details>
  <summary>Details</summary>
Motivation: Address the trade-off between hardware efficiency and performance accuracy in deploying CNNs on compute-in-memory platforms.

Method: Introduced the Binary Weight Multi-bit Activation (BWMA) method with closed-form solutions for weight quantization and a differentiable function for activation quantization.

Result: BWMA showed significant accuracy improvements on CIFAR-10 and ImageNet datasets (1.44%-5.46% and 0.35%-5.37%) and found 4-bit activation provides optimal balance for hardware cost and model performance.

Conclusion: BWMA successfully enhances both hardware efficiency and CNN accuracy, overcoming the limitations of existing quantization methods.

Abstract: Compute-in-memory (CIM) accelerators have emerged as a promising way for
enhancing the energy efficiency of convolutional neural networks (CNNs).
Deploying CNNs on CIM platforms generally requires quantization of network
weights and activations to meet hardware constraints. However, existing
approaches either prioritize hardware efficiency with binary weight and
activation quantization at the cost of accuracy, or utilize multi-bit weights
and activations for greater accuracy but limited efficiency. In this paper, we
introduce a novel binary weight multi-bit activation (BWMA) method for CNNs on
CIM-based accelerators. Our contributions include: deriving closed-form
solutions for weight quantization in each layer, significantly improving the
representational capabilities of binarized weights; and developing a
differentiable function for activation quantization, approximating the ideal
multi-bit function while bypassing the extensive search for optimal settings.
Through comprehensive experiments on CIFAR-10 and ImageNet datasets, we show
that BWMA achieves notable accuracy improvements over existing methods,
registering gains of 1.44\%-5.46\% and 0.35\%-5.37\% on respective datasets.
Moreover, hardware simulation results indicate that 4-bit activation
quantization strikes the optimal balance between hardware cost and model
performance.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [24] [CoBA: Counterbias Text Augmentation for Mitigating Various Spurious Correlations via Semantic Triples](https://arxiv.org/abs/2508.21083)
*Kyohoon Jin,Juhwan Choi,Jungmin Yun,Junho Lee,Soojin Jang,Youngbin Kim*

Main category: cs.CL

TL;DR: The paper introduces CoBA, a framework that uses counterbias data augmentation to mitigate spurious correlations and improve robust generalization in deep learning models.


<details>
  <summary>Details</summary>
Motivation: Deep learning models often suffer from performance degradation due to their reliance on spurious correlations, leading to poor generalization on unseen data.

Method: The authors propose CoBA, which operates on semantic triples (subject-predicate-object) by decomposing, modifying, and reconstructing text to create counterbias data for training.

Result: Experiments show that CoBA improves task performance, reduces biases, and enhances out-of-distribution robustness in models.

Conclusion: CoBA is a versatile and effective solution against spurious correlations, improving both bias mitigation and generalization in deep learning applications.

Abstract: Deep learning models often learn and exploit spurious correlations in
training data, using these non-target features to inform their predictions.
Such reliance leads to performance degradation and poor generalization on
unseen data. To address these limitations, we introduce a more general form of
counterfactual data augmentation, termed counterbias data augmentation, which
simultaneously tackles multiple biases (e.g., gender bias, simplicity bias) and
enhances out-of-distribution robustness. We present CoBA: CounterBias
Augmentation, a unified framework that operates at the semantic triple level:
first decomposing text into subject-predicate-object triples, then selectively
modifying these triples to disrupt spurious correlations. By reconstructing the
text from these adjusted triples, CoBA generates counterbias data that
mitigates spurious patterns. Through extensive experiments, we demonstrate that
CoBA not only improves downstream task performance, but also effectively
reduces biases and strengthens out-of-distribution resilience, offering a
versatile and robust solution to the challenges posed by spurious correlations.

</details>


### [25] [Mapping Toxic Comments Across Demographics: A Dataset from German Public Broadcasting](https://arxiv.org/abs/2508.21084)
*Jan Fillies,Michael Peter Hoffmann,Rebecca Reichel,Roman Salzwedel,Sven Bodemer,Adrian Paschke*

Main category: cs.CL

TL;DR: The paper presents the first German dataset annotated for toxicity, enriched with platform-based age estimates, to study age-related communication patterns in toxic speech.


<details>
  <summary>Details</summary>
Motivation: Existing toxic speech datasets lack demographic context, limiting insights into age-based communication differences online.

Method: The authors curated a dataset of 3,024 human-annotated and 30,024 LLM-annotated comments from Instagram, TikTok, and YouTube using toxic keywords and categorized annotations.

Result: The dataset showed varying toxic speech trends: younger users use expressive language while older users lean towards disinformation and devaluation.

Conclusion: The study highlights linguistic variation in toxic speech by age and provides a resource for age-aware content moderation research.

Abstract: A lack of demographic context in existing toxic speech datasets limits our
understanding of how different age groups communicate online. In collaboration
with funk, a German public service content network, this research introduces
the first large-scale German dataset annotated for toxicity and enriched with
platform-provided age estimates. The dataset includes 3,024 human-annotated and
30,024 LLM-annotated anonymized comments from Instagram, TikTok, and YouTube.
To ensure relevance, comments were consolidated using predefined toxic
keywords, resulting in 16.7\% labeled as problematic. The annotation pipeline
combined human expertise with state-of-the-art language models, identifying key
categories such as insults, disinformation, and criticism of broadcasting fees.
The dataset reveals age-based differences in toxic speech patterns, with
younger users favoring expressive language and older users more often engaging
in disinformation and devaluation. This resource provides new opportunities for
studying linguistic variation across demographics and supports the development
of more equitable and age-aware content moderation systems.

</details>


### [26] [Granite Embedding R2 Models](https://arxiv.org/abs/2508.21085)
*Parul Awasthy,Aashka Trivedi,Yulong Li,Meet Doshi,Riyaz Bhat,Vignesh P,Vishwajeet Kumar,Yushu Yang,Bhavani Iyer,Abraham Daniels,Rudra Murthy,Ken Barker,Martin Franz,Madison Lee,Todd Ward,Salim Roukos,David Cox,Luis Lastras,Jaydeep Sen,Radu Florian*

Main category: cs.CL

TL;DR: The Granite Embedding R2 models are advanced English encoder-based embedding models designed for enterprise-scale dense retrieval tasks, offering expanded context length, state-of-the-art performance, and speed improvements.


<details>
  <summary>Details</summary>
Motivation: The motivation lies in the need for high-performance, scalable, and accurate retrieval systems in enterprise applications.

Method: The paper introduces bi-encoder and cross-encoder architectures, with retriever and reranker models trained on enterprise-specific data.

Result: These models achieve state-of-the-art accuracy across diverse benchmarks, with speed gains up to 44% over competitors.

Conclusion: Granite R2 models set new standards for embedding performance, combining high accuracy, speed, transparent data provenance, and enterprise-ready licensing.

Abstract: We introduce the Granite Embedding R2 models, a comprehensive family of
high-performance English encoder-based embedding models engineered for
enterprise-scale dense retrieval applications. Building upon our
first-generation release, these models deliver substantial improvements,
including 16x expanded context length (8,192 tokens), state-of-the-art
performance across diverse retrieval domains - text, code, long-document
search, multi-turn conversational, and tabular data - and measurable speed
advantages of 19-44\% over leading competitors while maintaining superior
accuracy. Our release encompasses both bi-encoder and cross-encoder
architectures, featuring a highly effective 22-layer retriever model and its
efficient 12-layer counterpart, alongside a high-quality reranker model, all
trained exclusively on enterprise-appropriate data with comprehensive
governance oversight. The models demonstrate exceptional versatility across
standard benchmarks, IBM-developed evaluation suites, and real-world enterprise
use cases, establishing new performance standards for open-source embedding
models. In an era where retrieval speed and accuracy are paramount for
competitive advantage, the Granite R2 models deliver a compelling combination
of cutting-edge performance, enterprise-ready licensing, and transparent data
provenance that organizations require for mission-critical deployments. All
models are publicly available under the Apache 2.0 license at
https://huggingface.co/collections/ibm-granite, enabling unrestricted research
and commercial use.

</details>


### [27] [TrInk: Ink Generation with Transformer Network](https://arxiv.org/abs/2508.21098)
*Zezhong Jin,Shubhang Desai,Xu Chen,Biyi Fang,Zhuoyi Huang,Zhe Li,Chong-Xin Gan,Xiao Tu,Man-Wai Mak,Yan Lu,Shujie Liu*

Main category: cs.CL

TL;DR: This paper introduces TrInk, a Transformer-based model for ink (handwriting) generation, which improves text-stroke alignment and enhances legibility and style consistency in handwriting synthesis.


<details>
  <summary>Details</summary>
Motivation: Develop a model that better captures global dependencies and alignments in handwriting generation to improve legibility and reduce errors.

Method: The authors propose the TrInk model, incorporating scaled positional embeddings and Gaussian memory masks in cross-attention modules. Evaluation pipelines for both subjective and objective assessments were designed.

Result: Experiments show a 35.56% decrease in character error rate and a 29.66% decrease in word error rate on the IAM-OnDB dataset compared to prior approaches.

Conclusion: TrInk effectively enhances handwriting generation by improving error rates and aligning generated strokes with input text. A demo page showcasing TrInk's performance is provided.

Abstract: In this paper, we propose TrInk, a Transformer-based model for ink
generation, which effectively captures global dependencies. To better
facilitate the alignment between the input text and generated stroke points, we
introduce scaled positional embeddings and a Gaussian memory mask in the
cross-attention module. Additionally, we design both subjective and objective
evaluation pipelines to comprehensively assess the legibility and style
consistency of the generated handwriting. Experiments demonstrate that our
Transformer-based model achieves a 35.56\% reduction in character error rate
(CER) and an 29.66% reduction in word error rate (WER) on the IAM-OnDB dataset
compared to previous methods. We provide an demo page with handwriting samples
from TrInk and baseline models at: https://akahello-a11y.github.io/trink-demo/

</details>


### [28] [How Does Cognitive Bias Affect Large Language Models? A Case Study on the Anchoring Effect in Price Negotiation Simulations](https://arxiv.org/abs/2508.21137)
*Yoshiki Takenami,Yin Jou Huang,Yugo Murawaki,Chenhui Chu*

Main category: cs.CL

TL;DR: The paper explores cognitive biases in LLMs, focusing on how the anchoring effect impacts price negotiations, finding LLMs are influenced like humans and reasoning models reduce this bias.


<details>
  <summary>Details</summary>
Motivation: The main motivation is to enhance the safety and reliability of LLMs in real-world applications by understanding their susceptibility to human-like cognitive biases, specifically the anchoring effect.

Method: The researchers instructed LLM seller agents to apply the anchoring effect in price negotiations and evaluated outcomes using both subjective and objective metrics. They also examined the effect in relation to reasoning models and personality traits.

Result: LLMs are prone to the anchoring effect like humans, but reasoning models with long chains of thought are less susceptible. No link was found between personality traits and bias susceptibility.

Conclusion: The study provides insights into how cognitive biases manifest in LLMs and suggests that reasoning capabilities could mitigate such effects, aiding safe and responsible LLM development.

Abstract: Cognitive biases, well-studied in humans, can also be observed in LLMs,
affecting their reliability in real-world applications. This paper investigates
the anchoring effect in LLM-driven price negotiations. To this end, we
instructed seller LLM agents to apply the anchoring effect and evaluated
negotiations using not only an objective metric but also a subjective metric.
Experimental results show that LLMs are influenced by the anchoring effect like
humans. Additionally, we investigated the relationship between the anchoring
effect and factors such as reasoning and personality. It was shown that
reasoning models are less prone to the anchoring effect, suggesting that the
long chain of thought mitigates the effect. However, we found no significant
correlation between personality traits and susceptibility to the anchoring
effect. These findings contribute to a deeper understanding of cognitive biases
in LLMs and to the realization of safe and responsible application of LLMs in
society.

</details>


### [29] [BED-LLM: Intelligent Information Gathering with LLMs and Bayesian Experimental Design](https://arxiv.org/abs/2508.21184)
*Deepro Choudhury,Sinead Williamson,Adam Goliński,Ning Miao,Freddie Bickford Smith,Michael Kirchhof,Yizhe Zhang,Tom Rainforth*

Main category: cs.CL

TL;DR: This paper introduces BED-LLM, a method enabling large language models (LLMs) to iteratively gather information through Bayesian Experimental Design (BED), improving their multi-turn conversational abilities.


<details>
  <summary>Details</summary>
Motivation: Enhance the information-gathering and adaptive question-asking capabilities of LLMs for multi-turn interactions and external task inference.

Method: BED-LLM utilizes sequential Bayesian experimental design to select queries maximizing Expected Information Gain (EIG), leveraging probabilistic models derived from the LLM's belief distribution along with targeted strategies for candidate proposal and response handling.

Result: BED-LLM demonstrated significant performance improvements in tests such as the 20-questions game and user preference inference, outperforming direct LLM prompting and competing approaches.

Conclusion: Incorporating BED into LLMs substantially enhances their adaptive and interactive capabilities, positioning them as better agents for multi-turn conversational and task-solving contexts.

Abstract: We propose a general-purpose approach for improving the ability of Large
Language Models (LLMs) to intelligently and adaptively gather information from
a user or other external source using the framework of sequential Bayesian
experimental design (BED). This enables LLMs to act as effective multi-turn
conversational agents and interactively interface with external environments.
Our approach, which we call BED-LLM (Bayesian Experimental Design with Large
Language Models), is based on iteratively choosing questions or queries that
maximize the expected information gain (EIG) about the task of interest given
the responses gathered previously. We show how this EIG can be formulated in a
principled way using a probabilistic model derived from the LLM's belief
distribution and provide detailed insights into key decisions in its
construction. Further key to the success of BED-LLM are a number of specific
innovations, such as a carefully designed estimator for the EIG, not solely
relying on in-context updates for conditioning on previous responses, and a
targeted strategy for proposing candidate queries. We find that BED-LLM
achieves substantial gains in performance across a wide range of tests based on
the 20-questions game and using the LLM to actively infer user preferences,
compared to direct prompting of the LLM and other adaptive design strategies.

</details>


### [30] [Can Multimodal LLMs Solve the Basic Perception Problems of Percept-V?](https://arxiv.org/abs/2508.21143)
*Samrajnee Ghosh,Naman Agarwal,Hemanshu Garg,Chinmay Mittal,Mausam,Parag Singla*

Main category: cs.CL

TL;DR: This paper evaluates Multimodal Large Language Models' (MLLMs) reasoning abilities on basic visual perception tasks using the newly introduced dataset, Percept-V, revealing their declining performance with increased task complexity.


<details>
  <summary>Details</summary>
Motivation: While MLLMs show proficiency in complex tasks like coding or problem solving, their effectiveness in basic perception tasks using simple, uncontaminated images has remained underexplored.

Method: The study introduces Percept-V, consisting of 7200 program-generated images in 30 categories designed to assess various visual perception skills, and evaluates state-of-the-art MLLMs and LRMs on this dataset.

Result: MLLMs, including GPT-4o and Gemini, showed a consistent decline in performance as the problem complexity increased across all categories.

Conclusion: Although MLLMs have demonstrated strengths in handling complex tasks, their perceptual reasoning abilities lag significantly in fundamental cognitive skills, showing that further advancements are needed.

Abstract: The reasoning abilities of Multimodal Large Language Models (MLLMs) have
garnered a lot of attention in recent times, with advances made in frontiers
like coding, mathematics, and science. However, very limited experiments have
been done to assess their performance in simple perception tasks performed over
uncontaminated, generated images containing basic shapes and structures. To
address this issue, the paper introduces a dataset, Percept-V, containing a
total of 7200 program-generated images equally divided into 30 categories, each
testing a combination of visual perception skills. Unlike previously proposed
datasets, Percept-V comprises very basic tasks of varying complexity that test
the perception abilities of MLLMs. This dataset is then tested on
state-of-the-art MLLMs like GPT-4o, Gemini, and Claude as well as Large
Reasoning Models (LRMs) like OpenAI o4-mini and DeepSeek R1 to gauge their
performance. Contrary to the evidence that MLLMs excel in many complex tasks,
our experiments show a significant drop in the models' performance with
increasing problem complexity across all categories. An analysis of the
performances also reveals that the tested MLLMs exhibit a similar trend in
accuracy across categories, testing a particular cognitive skill and find some
skills to be more difficult than others.

</details>


### [31] [A Survey of Scientific Large Language Models: From Data Foundations to Agent Frontiers](https://arxiv.org/abs/2508.21148)
*Ming Hu,Chenglong Ma,Wei Li,Wanghan Xu,Jiamin Wu,Jucheng Hu,Tianbin Li,Guohang Zhuang,Jiaqi Liu,Yingzhou Lu,Ying Chen,Chaoyang Zhang,Cheng Tan,Jie Ying,Guocheng Wu,Shujian Gao,Pengcheng Chen,Jiashi Lin,Haitao Wu,Lulu Chen,Fengxiang Wang,Yuanyuan Zhang,Xiangyu Zhao,Feilong Tang,Encheng Su,Junzhi Ning,Xinyao Liu,Ye Du,Changkai Ji,Cheng Tang,Huihui Xu,Ziyang Chen,Ziyan Huang,Jiyao Liu,Pengfei Jiang,Yizhou Wang,Chen Tang,Jianyu Wu,Yuchen Ren,Siyuan Yan,Zhonghua Wang,Zhongxing Xu,Shiyan Su,Shangquan Sun,Runkai Zhao,Zhisheng Zhang,Yu Liu,Fudi Wang,Yuanfeng Ji,Yanzhou Su,Hongming Shan,Chunmei Feng,Jiahao Xu,Jiangtao Yan,Wenhao Tang,Diping Song,Lihao Liu,Yanyan Huang,Lequan Yu,Bin Fu,Shujun Wang,Xiaomeng Li,Xiaowei Hu,Yun Gu,Ben Fei,Zhongying Deng,Benyou Wang,Yuewen Cao,Minjie Shen,Haodong Duan,Jie Xu,Yirong Chen,Fang Yan,Hongxia Hao,Jielan Li,Jiajun Du,Yanbo Wang,Imran Razzak,Chi Zhang,Lijun Wu,Conghui He,Zhaohui Lu,Jinhai Huang,Yihao Liu,Fenghua Ling,Yuqiang Li,Aoran Wang,Qihao Zheng,Nanqing Dong,Tianfan Fu,Dongzhan Zhou,Yan Lu,Wenlong Zhang,Jin Ye,Jianfei Cai,Wanli Ouyang,Yu Qiao,Zongyuan Ge,Shixiang Tang,Junjun He,Chunfeng Song,Lei Bai,Bowen Zhou*

Main category: cs.CL

TL;DR: This survey examines the evolution of Scientific Large Language Models (Sci-LLMs) and their interplay with multimodal scientific data, providing a taxonomy and guidance for their advancement.


<details>
  <summary>Details</summary>
Motivation: To address the challenges posed by the complex nature of scientific datasets and explore how Sci-LLMs can evolve to enhance scientific research and discovery.

Method: The authors conduct a data-centric synthesis including taxonomy formulation, hierarchical modeling of knowledge, systematic review of models, analysis of scientific datasets, and evaluation benchmarks.

Result: The paper highlights unique demands of scientific data (heterogeneous, multi-scale, uncertainty-laden) and emerging solutions like semi-automated annotation pipelines and expert validation. It also shifts evaluation approaches toward process-based assessments.

Conclusion: Sci-LLMs require trustworthy, adaptive systems capable of contributing to dynamic knowledge bases, emphasizing closed-loop experimentation and cross-modal reasoning to transform scientific discovery.

Abstract: Scientific Large Language Models (Sci-LLMs) are transforming how knowledge is
represented, integrated, and applied in scientific research, yet their progress
is shaped by the complex nature of scientific data. This survey presents a
comprehensive, data-centric synthesis that reframes the development of Sci-LLMs
as a co-evolution between models and their underlying data substrate. We
formulate a unified taxonomy of scientific data and a hierarchical model of
scientific knowledge, emphasizing the multimodal, cross-scale, and
domain-specific challenges that differentiate scientific corpora from general
natural language processing datasets. We systematically review recent Sci-LLMs,
from general-purpose foundations to specialized models across diverse
scientific disciplines, alongside an extensive analysis of over 270
pre-/post-training datasets, showing why Sci-LLMs pose distinct demands --
heterogeneous, multi-scale, uncertainty-laden corpora that require
representations preserving domain invariance and enabling cross-modal
reasoning. On evaluation, we examine over 190 benchmark datasets and trace a
shift from static exams toward process- and discovery-oriented assessments with
advanced evaluation protocols. These data-centric analyses highlight persistent
issues in scientific data development and discuss emerging solutions involving
semi-automated annotation pipelines and expert validation. Finally, we outline
a paradigm shift toward closed-loop systems where autonomous agents based on
Sci-LLMs actively experiment, validate, and contribute to a living, evolving
knowledge base. Collectively, this work provides a roadmap for building
trustworthy, continually evolving artificial intelligence (AI) systems that
function as a true partner in accelerating scientific discovery.

</details>


### [32] [Quantifying Label-Induced Bias in Large Language Model Self- and Cross-Evaluations](https://arxiv.org/abs/2508.21164)
*Muskan Saraf,Sajjad Rezvani Boroujeni,Justin Beaudry,Hossein Abedi,Tom Bush*

Main category: cs.CL

TL;DR: This study uncovers biases in how large language models (LLMs) like ChatGPT, Gemini, and Claude evaluate content, showing that labels influence scores heavily.


<details>
  <summary>Details</summary>
Motivation: To explore the fairness and accuracy of evaluations conducted by LLMs, specifically identifying biases related to perceived model identity.

Method: Blog posts generated by ChatGPT, Gemini, and Claude were evaluated by the three models under four labeling conditions, and scores were analyzed for preference votes and quality criteria like Coherence, Informativeness, and Conciseness.

Result: Labels caused notable biases in evaluations: 'Claude' labels enhanced scores while 'Gemini' labels reduced them. False labels dramatically shifted rankings and quality ratings.

Conclusion: The study highlights how model identity influences evaluation outcomes and recommends blind or multimodel evaluation protocols to ensure unbiased benchmarking of LLMs.

Abstract: Large language models (LLMs) are increasingly used to evaluate outputs, yet
their judgments may be influenced. This study examines bias in self- and
cross-model evaluations by ChatGPT, Gemini, and Claude under four conditions:
no labels, true labels, and two false-label scenarios. Blog posts authored by
each model were evaluated by all three using both overall preference voting and
quality ratings for Coherence, Informativeness, and Conciseness, with all
scores expressed as percentages for direct comparison. Results reveal striking
asymmetries: the "Claude" label consistently boosts scores, while the "Gemini"
label consistently depresses them, regardless of actual content. False labels
frequently reversed rankings, producing shifts of up to 50 percentage points in
preference votes and up to 12 percentage points in converted quality ratings.
Gemini's self-scores collapsed under true labels, while Claude's
self-preference intensified. These findings show that perceived model identity
can heavily distort high-level judgments and subtly influence detailed quality
ratings, underscoring the need for blind or multimodel evaluation protocols to
ensure fairness in LLM benchmarking.

</details>


### [33] [Improving Aviation Safety Analysis: Automated HFACS Classification Using Reinforcement Learning with Group Relative Policy Optimization](https://arxiv.org/abs/2508.21201)
*Arash Ahmadi,Sarah Sharif,Yaser Banad*

Main category: cs.CL

TL;DR: The paper presents an automated framework using reinforcement learning to improve aviation safety analysis by enhancing HFACS classification with optimized language models, showing significant performance improvements over existing methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome limitations in traditional HFACS methods, including scalability and consistency, by automating the classification process for better aviation safety insights.

Method: The proposed method involves using Reinforcement Learning with Group Relative Policy Optimization (GRPO) to fine-tune a Llama-3.1 8B language model. It employs a multi-component reward system and synthetic data generation to address dataset challenges.

Result: The GRPO-optimized model significantly improves performance, achieving a 350% boost in exact match accuracy and outshining state-of-the-art models like GPT-5-mini and Gemini-2.5-fiash in key metrics.

Conclusion: Smaller, domain-specific optimized models are validated as more efficient and effective for specialized tasks like aviation safety analysis, enabling feasible deployment on low-resource devices.

Abstract: Analyzing the human factors behind aviation accidents is crucial for
preventing future incidents, yet traditional methods using the Human Factors
Analysis and Classification System (HFACS) are limited by scalability and
consistency. To address this, we introduce an automated HFACS classification
framework for aviation safety analysis that utilizes Reinforcement Learning
with Group Relative Policy Optimization (GRPO) to fine-tune a Llama-3.1 8B
language model. Our approach incorporates a multi-component reward system
tailored for aviation safety analysis and integrates synthetic data generation
to overcome class imbalance in accident datasets. The resulting GRPO-optimized
model achieved noticeable performance gains, including a 350% increase in exact
match accuracy (from 0.0400 to 0.1800) and an improved partial match accuracy
of 0.8800. Significantly, our specialized model outperforms state-of-the-art
LLMs (Large Language Models), including GPT-5-mini and Gemini-2.5-fiash, on key
metrics. This research also proposes exact match accuracy in multi-label HFACS
classification problem as a new benchmarking methodology to evaluate the
advanced reasoning capabilities of language models. Ultimately, our work
validates that smaller, domain-optimized models can provide a computationally
efficient and better solution for critical safety analysis. This approach makes
powerful, low-latency deployment on resource-constrained edge devices feasible.

</details>


### [34] [Enhancing Robustness of Autoregressive Language Models against Orthographic Attacks via Pixel-based Approach](https://arxiv.org/abs/2508.21206)
*Han Yang,Jian Lan,Yihong Liu,Hinrich Schütze,Thomas Seidl*

Main category: cs.CL

TL;DR: Autoregressive language models struggle with orthographic attacks due to text-based tokenizers, and this paper introduces a robust pixel-based generative model to address the issue.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the vulnerability of autoregressive models to orthographic attacks caused by the limitations of subword tokenizers, particularly with multilingual texts.

Method: The proposed model replaces traditional text-based embeddings with pixel-based image representations of words, enhancing robustness to noisy and diverse multilingual inputs.

Result: Experiments on datasets like LAMBADA, WMT24, and SST-2 show improved resilience to orthographic noise and better performance in multilingual contexts.

Conclusion: The pixel-based generative model offers stronger robustness and compatibility with diverse writing systems compared to traditional approaches.

Abstract: Autoregressive language models are vulnerable to orthographic attacks, where
input text is perturbed with characters from multilingual alphabets, leading to
substantial performance degradation. This vulnerability primarily stems from
the out-of-vocabulary issue inherent in subword tokenizers and their
embeddings. To address this limitation, we propose a pixel-based generative
language model that replaces the text-based embeddings with pixel-based
representations by rendering words as individual images. This design provides
stronger robustness to noisy inputs, while an extension of compatibility to
multilingual text across diverse writing systems. We evaluate the proposed
method on the multilingual LAMBADA dataset, WMT24 dataset and the SST-2
benchmark, demonstrating both its resilience to orthographic noise and its
effectiveness in multilingual settings.

</details>


### [35] [Do Self-Supervised Speech Models Exhibit the Critical Period Effects in Language Acquisition?](https://arxiv.org/abs/2508.21210)
*Yurie Koga,Shunsuke Kando,Yusuke Miyao*

Main category: cs.CL

TL;DR: The paper investigates Critical Period effects in self-supervised speech models, discovering that these models do not exhibit clear evidence of language acquisition constraints observed in humans.


<details>
  <summary>Details</summary>
Motivation: To explore whether self-supervised speech models (S3Ms) exhibit Critical Period (CP) effects seen in human language learning, particularly for phonological processing.

Method: The study involved training S3Ms with varying L2 (second language) training onsets and L1 (native language) training offsets on child-directed speech data, followed by evaluating their phone discrimination performance.

Result: Self-supervised speech models showed no strong evidence of CP effects. Models with delayed L2 exposure performed better on L2 tasks, while delayed L1 exposure led to L1 forgetting.

Conclusion: Unlike humans, S3Ms do not replicate Critical Period effects related to phonological acquisition, challenging assumptions about their alignment with human language learning processes.

Abstract: This paper investigates whether the Critical Period (CP) effects in human
language acquisition are observed in self-supervised speech models (S3Ms). CP
effects refer to greater difficulty in acquiring a second language (L2) with
delayed L2 exposure onset, and greater retention of their first language (L1)
with delayed L1 exposure offset. While previous work has studied these effects
using textual language models, their presence in speech models remains
underexplored despite the central role of spoken language in human language
acquisition. We train S3Ms with varying L2 training onsets and L1 training
offsets on child-directed speech and evaluate their phone discrimination
performance. We find that S3Ms do not exhibit clear evidence of either CP
effects in terms of phonological acquisition. Notably, models with delayed L2
exposure onset tend to perform better on L2 and delayed L1 exposure offset
leads to L1 forgetting.

</details>


### [36] [Decoding Memories: An Efficient Pipeline for Self-Consistency Hallucination Detection](https://arxiv.org/abs/2508.21228)
*Weizhi Gao,Xiaorui Liu,Feiyi Wang,Dan Lu,Junqi Yin*

Main category: cs.CL

TL;DR: The paper addresses the inefficiency in self-consistency methods used to detect hallucination in LLMs and proposes a faster pipeline called Decoding Memory Pipeline (DMP).


<details>
  <summary>Details</summary>
Motivation: Current hallucination detection methods struggle with sentence-level tasks and are computationally expensive when using self-consistency approaches.

Method: The authors identify redundancy in self-consistency via shared prefix tokens and introduce DMP, which employs selective inference and annealed decoding to accelerate generation.

Result: Experiments show that DMP achieves up to a 3x speedup in multi-response generation, maintaining AUROC performance.

Conclusion: DMP offers an efficient and model-agnostic enhancement for hallucination detection and holds potential for alignment and reasoning tasks.

Abstract: Large language models (LLMs) have demonstrated impressive performance in both
research and real-world applications, but they still struggle with
hallucination. Existing hallucination detection methods often perform poorly on
sentence-level generation or rely heavily on domain-specific knowledge. While
self-consistency approaches help address these limitations, they incur high
computational costs due to repeated generation. In this paper, we conduct the
first study on identifying redundancy in self-consistency methods, manifested
as shared prefix tokens across generations, and observe that non-exact-answer
tokens contribute minimally to the semantic content. Based on these insights,
we propose a novel Decoding Memory Pipeline (DMP) that accelerates generation
through selective inference and annealed decoding. Being orthogonal to the
model, dataset, decoding strategy, and self-consistency baseline, our DMP
consistently improves the efficiency of multi-response generation and holds
promise for extension to alignment and reasoning tasks. Extensive experiments
show that our method achieves up to a 3x speedup without sacrificing AUROC
performance.

</details>


### [37] [Efficient Code Embeddings from Code Generation Models](https://arxiv.org/abs/2508.21290)
*Daria Kryvosheieva,Saba Sturua,Michael Günther,Scott Martens,Han Xiao*

Main category: cs.CL

TL;DR: Jina-code-embeddings is an advanced embedding model excelling in retrieving code from text, cross-language similarity, and technical Q&A.


<details>
  <summary>Details</summary>
Motivation: To effectively retrieve and analyze code snippets across languages and natural language queries using a model optimized for embeddings.

Method: Used an autoregressive backbone pre-trained on text and code, generating embeddings through last-token pooling.

Result: Demonstrates state-of-the-art performance despite its smaller model size.

Conclusion: Validates the effectiveness of this approach for code embedding construction and diverse applications.

Abstract: jina-code-embeddings is a novel code embedding model suite designed to
retrieve code from natural language queries, perform technical
question-answering, and identify semantically similar code snippets across
programming languages. It makes innovative use of an autoregressive backbone
pre-trained on both text and code, generating embeddings via last-token
pooling. We outline the training recipe and demonstrate state-of-the-art
performance despite the relatively small size of the models, validating this
approach to code embedding model construction.

</details>


### [38] [BLUEX Revisited: Enhancing Benchmark Coverage with Automatic Captioning](https://arxiv.org/abs/2508.21294)
*João Guilherme Alves Santos,Giovana Kerche Bonás,Thales Sales Almeida*

Main category: cs.CL

TL;DR: The paper updates the BLUEX dataset with exams from 2024-2025 and introduces enhanced image captioning, doubling its size and improving accessibility for evaluating large language models (LLMs).


<details>
  <summary>Details</summary>
Motivation: To improve evaluation methods for LLMs, particularly in multilingual and non-English scenarios, and address issues of data contamination during pretraining.

Method: Enhanced the BLUEX dataset by including new exams and applying state-of-the-art image captioning techniques to expand its usability for text-only models.

Result: The updated dataset improves accessibility by over 40% and doubles the number of usable questions to 1,422.

Conclusion: The enriched dataset facilitates more robust evaluation of LLMs, especially for leveraging visual context in multilingual settings.

Abstract: With the growing capabilities of Large Language Models (LLMs), there is an
increasing need for robust evaluation methods, especially in multilingual and
non-English contexts. We present an updated version of the BLUEX dataset, now
including 2024-2025 exams and automatically generated image captions using
state-of-the-art models, enhancing its relevance for data contamination studies
in LLM pretraining. Captioning strategies increase accessibility to text-only
models by more than 40%, producing 1,422 usable questions, more than doubling
the number in the original BLUEX. We evaluated commercial and open-source LLMs
and their ability to leverage visual context through captions.

</details>


### [39] [Challenges and Applications of Large Language Models: A Comparison of GPT and DeepSeek family of models](https://arxiv.org/abs/2508.21377)
*Shubham Sharma,Sneha Tuli,Narendra Badam*

Main category: cs.CL

TL;DR: The paper outlines 16 critical challenges in developing and deploying Large Language Models (LLMs), comparing OpenAI's GPT-4o and DeepSeek-V3 to explore trade-offs between closed and open source approaches.


<details>
  <summary>Details</summary>
Motivation: To guide AI researchers, developers, and decision-makers in navigating the complexities, capabilities, limitations, and best practices of LLMs.

Method: The authors conduct a comparative review of two state-of-the-art LLMs, examining their approaches to addressing key challenges and evaluating their suitability for various domains.

Result: Closed source models like GPT-4o excel in safety and reliability, while open source models like DeepSeek-V3 provide efficiency and adaptability. Applications vary based on the domain and model attributes.

Conclusion: The survey provides actionable insights into LLM development and deployment, emphasizing informed trade-offs between open and closed source models for specific applications.

Abstract: Large Language Models (LLMs) are transforming AI across industries, but their
development and deployment remain complex. This survey reviews 16 key
challenges in building and using LLMs and examines how these challenges are
addressed by two state-of-the-art models with unique approaches: OpenAI's
closed source GPT-4o (May 2024 update) and DeepSeek-V3-0324 (March 2025), a
large open source Mixture-of-Experts model. Through this comparison, we
showcase the trade-offs between closed source models (robust safety, fine-tuned
reliability) and open source models (efficiency, adaptability). We also explore
LLM applications across different domains (from chatbots and coding tools to
healthcare and education), highlighting which model attributes are best suited
for each use case. This article aims to guide AI researchers, developers, and
decision-makers in understanding current LLM capabilities, limitations, and
best practices.

</details>


### [40] [Normality and the Turing Test](https://arxiv.org/abs/2508.21382)
*Alexandre Kabbach*

Main category: cs.CL

TL;DR: This paper reexamines the Turing test by focusing on the concept of normality, arguing it assesses average human intelligence through aggregated judgments rather than exceptional intelligence.


<details>
  <summary>Details</summary>
Motivation: The study seeks to reconceptualize the Turing test in terms of normal rather than exceptional intelligence, challenging conventional assumptions about artificial intelligence.

Method: The paper examines the statistical interpretation of normality and applies it to the mechanics and purpose of the Turing test, providing a theoretical analysis.

Result: It argues that large language models like ChatGPT aim for exceptional intelligence, making them unlikely to pass the Turing test, which is centered on average human intelligence.

Conclusion: The Turing test evaluates normal human intelligence and raises broader questions about whether human cognition can be fully understood through the lens of statistical normality.

Abstract: This paper proposes to revisit the Turing test through the concept of
normality. Its core argument is that the statistical interpretation of the
normal--understood as the average both in the normative and mathematical sense
of the term--proves useful for understanding the Turing test in at least two
ways. First, in the sense that the Turing test targets normal/average rather
than exceptional human intelligence, so that successfully passing the test
requires building machines that "make mistakes" and display imperfect behavior
just like normal/average humans. Second, in the sense that the Turing test is a
statistical test where judgments of intelligence are never carried out by a
single "average" judge (understood as non-expert) but always by a full jury. As
such, the notion of "average human interrogator" that Turing talks about in his
original paper should be understood primarily as referring to a mathematical
abstraction made of the normalized aggregate of individual judgments of
multiple judges. In short, this paper argues that the Turing test is a test of
normal intelligence as assessed by a normal judge characterizing the average
judgment of a pool of human interrogators. Its conclusions are twofold. First,
it argues that large language models such as ChatGPT are unlikely to pass the
Turing test as those models precisely target exceptional rather than
normal/average human intelligence. As such, they constitute models of what it
proposes to call artificial smartness rather than artificial intelligence per
se. Second, it argues that the core question of whether the Turing test can
contribute anything to the understanding of human cognition is that of whether
the human mind is really reducible to the normal/average mind--a question which
largely extends beyond the Turing test itself and questions the conceptual
underpinnings of the normalist paradigm it belongs to.

</details>


### [41] [AllSummedUp: un framework open-source pour comparer les metriques d'evaluation de resume](https://arxiv.org/abs/2508.21389)
*Tanguy Herserant,Vincent Guigue*

Main category: cs.CL

TL;DR: The paper explores reproducibility issues in text summarization evaluation metrics, comparing classical and LLM-based methods, and calls for standardized evaluation protocols.


<details>
  <summary>Details</summary>
Motivation: To investigate discrepancies in the performance of text summarization evaluation metrics between literature reports and experimental findings.

Method: The study experiments with six metrics, including classical ones like ROUGE and LLM-based methods like G-Eval, using a unified framework on the SummEval dataset.

Result: Finds a trade-off where highly human-aligned metrics are computationally intensive and unstable; highlights concerns with the reproducibility and reliability of LLM-based evaluations.

Conclusion: Proposes robust evaluation protocols, exhaustive documentation, and methodological standardization to improve the reliability of text summarization assessment.

Abstract: This paper investigates reproducibility challenges in automatic text
summarization evaluation. Based on experiments conducted across six
representative metrics ranging from classical approaches like ROUGE to recent
LLM-based methods (G-Eval, SEval-Ex), we highlight significant discrepancies
between reported performances in the literature and those observed in our
experimental setting. We introduce a unified, open-source framework, applied to
the SummEval dataset and designed to support fair and transparent comparison of
evaluation metrics. Our results reveal a structural trade-off: metrics with the
highest alignment with human judgments tend to be computationally intensive and
less stable across runs. Beyond comparative analysis, this study highlights key
concerns about relying on LLMs for evaluation, stressing their randomness,
technical dependencies, and limited reproducibility. We advocate for more
robust evaluation protocols including exhaustive documentation and
methodological standardization to ensure greater reliability in automatic
summarization assessment.

</details>


### [42] [Automatic Reviewers Fail to Detect Faulty Reasoning in Research Papers: A New Counterfactual Evaluation Framework](https://arxiv.org/abs/2508.21422)
*Nils Dycke,Iryna Gurevych*

Main category: cs.CL

TL;DR: The paper examines the capability of automated review generators (ARGs), powered by large language models (LLMs), to detect inconsistencies in faulty research logic and finds that flaws in research logic do not significantly impact their output reviews.


<details>
  <summary>Details</summary>
Motivation: The authors aim to understand the capabilities and limitations of automated review generators in scholarly peer review, particularly focusing on their ability to detect flawed reasoning in research papers.

Method: The study introduces a counterfactual evaluation framework that tests ARGs under controlled conditions by analyzing their responses to logical inconsistencies in research papers.

Result: It was found that ARGs were largely ineffective at recognizing or being influenced by flaws in research logic during their reviews.

Conclusion: The authors provide three actionable recommendations for improving ARGs, and make their counterfactual dataset and evaluation framework publicly available to support future research.

Abstract: Large Language Models (LLMs) have great potential to accelerate and support
scholarly peer review and are increasingly used as fully automatic review
generators (ARGs). However, potential biases and systematic errors may pose
significant risks to scientific integrity; understanding the specific
capabilities and limitations of state-of-the-art ARGs is essential. We focus on
a core reviewing skill that underpins high-quality peer review: detecting
faulty research logic. This involves evaluating the internal consistency
between a paper's results, interpretations, and claims. We present a fully
automated counterfactual evaluation framework that isolates and tests this
skill under controlled conditions. Testing a range of ARG approaches, we find
that, contrary to expectation, flaws in research logic have no significant
effect on their output reviews. Based on our findings, we derive three
actionable recommendations for future work and release our counterfactual
dataset and evaluation framework publicly.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [43] [2COOOL: 2nd Workshop on the Challenge Of Out-Of-Label Hazards in Autonomous Driving](https://arxiv.org/abs/2508.21080)
*Ali K. AlShami,Ryan Rabinowitz,Maged Shoman,Jianwu Fang,Lukas Picek,Shao-Yuan Lo,Steve Cruz,Khang Nhut Lam,Nachiket Kamod,Lei-Lei Li,Jugal Kalita,Terrance E. Boult*

Main category: cs.CV

TL;DR: This abstract introduces the 2COOOL workshop targeted at tackling the challenges of novel scenarios in autonomous driving, focusing on innovation in hazard detection and avoidance.


<details>
  <summary>Details</summary>
Motivation: To address the critical barrier of handling novel scenarios as a step toward achieving entirely safe self-driving cars.

Method: Hosting the 2COOOL workshop to inspire and showcase advancements in fields such as anomaly detection, open-set recognition, and safe autonomous driving practices.

Result: The workshop aims to build on its previous success and promote collaboration between academia and industry.

Conclusion: The initiative emphasizes the importance of advancing novelty detection methods to enhance the real-world deployment of autonomous vehicles.

Abstract: As the computer vision community advances autonomous driving algorithms,
integrating vision-based insights with sensor data remains essential for
improving perception, decision making, planning, prediction, simulation, and
control. Yet we must ask: Why don't we have entirely safe self-driving cars
yet? A key part of the answer lies in addressing novel scenarios, one of the
most critical barriers to real-world deployment. Our 2COOOL workshop provides a
dedicated forum for researchers and industry experts to push the state of the
art in novelty handling, including out-of-distribution hazard detection,
vision-language models for hazard understanding, new benchmarking and
methodologies, and safe autonomous driving practices. The 2nd Workshop on the
Challenge of Out-of-Label Hazards in Autonomous Driving (2COOOL) will be held
at the International Conference on Computer Vision (ICCV) 2025 in Honolulu,
Hawaii, on October 19, 2025. We aim to inspire the development of new
algorithms and systems for hazard avoidance, drawing on ideas from anomaly
detection, open-set recognition, open-vocabulary modeling, domain adaptation,
and related fields. Building on the success of its inaugural edition at the
Winter Conference on Applications of Computer Vision (WACV) 2025, the workshop
will feature a mix of academic and industry participation.

</details>


### [44] [Advanced Deep Learning Techniques for Classifying Dental Conditions Using Panoramic X-Ray Images](https://arxiv.org/abs/2508.21088)
*Alireza Golkarieh,Kiana Kiashemshaki,Sajjad Rezvani Boroujeni*

Main category: cs.CV

TL;DR: The study focuses on automated dental condition classification in panoramic X-rays using deep learning, with a hybrid CNN-Random Forest model achieving the best results.


<details>
  <summary>Details</summary>
Motivation: Addressing the need for efficient and reliable automated diagnostic tools for dental conditions using deep learning.

Method: The study compared three methods: custom CNN, hybrid CNN-classifiers, and fine-tuned pre-trained models using a dataset of annotated panoramic X-rays.

Result: The hybrid CNN-Random Forest model achieved the highest accuracy (85.4%), outperforming basic CNN and pre-trained models.

Conclusion: Hybrid models combining CNN feature extraction with ensemble classifiers are effective for automated dental diagnostics, requiring larger datasets for further validation.

Abstract: This study investigates deep learning methods for automated classification of
dental conditions in panoramic X-ray images. A dataset of 1,512 radiographs
with 11,137 expert-verified annotations across four conditions fillings,
cavities, implants, and impacted teeth was used. After preprocessing and class
balancing, three approaches were evaluated: a custom convolutional neural
network (CNN), hybrid models combining CNN feature extraction with traditional
classifiers, and fine-tuned pre-trained architectures. Experiments employed 5
fold cross validation with accuracy, precision, recall, and F1 score as
evaluation metrics. The hybrid CNN Random Forest model achieved the highest
performance with 85.4% accuracy, surpassing the custom CNN baseline of 74.3%.
Among pre-trained models, VGG16 performed best at 82.3% accuracy, followed by
Xception and ResNet50. Results show that hybrid models improve discrimination
of morphologically similar conditions and provide efficient, reliable
performance. These findings suggest that combining CNN-based feature extraction
with ensemble classifiers offers a practical path toward automated dental
diagnostic support, while also highlighting the need for larger datasets and
further clinical validation.

</details>


### [45] [Q-Align: Alleviating Attention Leakage in Zero-Shot Appearance Transfer via Query-Query Alignment](https://arxiv.org/abs/2508.21090)
*Namu Kim,Wonbin Kweon,Minsoo Kim,Hwanjo Yu*

Main category: cs.CV

TL;DR: The paper introduces Q-Align to address Attention Leakage in zero-shot appearance transfer for image generation models.


<details>
  <summary>Details</summary>
Motivation: The motivation is to solve the issue of Attention Leakage, where semantic relations between images are lost due to Query-Key alignment in zero-shot appearance transfer tasks.

Method: The authors propose Q-Align with three contributions: Query-Query alignment for semantic mapping, Key-Value rearrangement for better feature alignment, and Attention refinement to ensure semantic consistency.

Result: Q-Align is validated through experiments, showing improved appearance fidelity and competitive structure preservation compared to state-of-the-art methods.

Conclusion: Q-Align effectively mitigates attention leakage and enhances semantic alignment in zero-shot appearance transfer, demonstrating improved results.

Abstract: We observe that zero-shot appearance transfer with large-scale image
generation models faces a significant challenge: Attention Leakage. This
challenge arises when the semantic mapping between two images is captured by
the Query-Key alignment. To tackle this issue, we introduce Q-Align, utilizing
Query-Query alignment to mitigate attention leakage and improve the semantic
alignment in zero-shot appearance transfer. Q-Align incorporates three core
contributions: (1) Query-Query alignment, facilitating the sophisticated
spatial semantic mapping between two images; (2) Key-Value rearrangement,
enhancing feature correspondence through realignment; and (3) Attention
refinement using rearranged keys and values to maintain semantic consistency.
We validate the effectiveness of Q-Align through extensive experiments and
analysis, and Q-Align outperforms state-of-the-art methods in appearance
fidelity while maintaining competitive structure preservation.

</details>


### [46] [ERTACache: Error Rectification and Timesteps Adjustment for Efficient Diffusion](https://arxiv.org/abs/2508.21091)
*Xurui Peng,Hong Liu,Chenqian Yan,Rui Ma,Fangmin Chen,Xing Wang,Zhihua Wu,Songwei Liu,Mingbao Lin*

Main category: cs.CV

TL;DR: ERTACache is proposed to improve the efficiency of diffusion models' inference process through advanced caching mechanisms, achieving up to 2x speedup without compromising quality.


<details>
  <summary>Details</summary>
Motivation: Diffusion models face computational inefficiencies due to their iterative inference processes, and naive feature caching leads to quality degradation, necessitating smarter caching solutions.

Method: ERTACache utilizes offline residual profiling to identify reusable steps, dynamically adjusts integration intervals using a correction coefficient, and approximates errors analytically through residual linearization.

Result: ERTACache achieves significant inference acceleration (up to 2x speedup) across image and video benchmarks, while maintaining or enhancing visual quality.

Conclusion: ERTACache resolves caching-induced errors, offering faster and more efficient sampling in diffusion models without compromising quality, demonstrating its utility especially in state-of-the-art benchmarks like Wan2.1.

Abstract: Diffusion models suffer from substantial computational overhead due to their
inherently iterative inference process. While feature caching offers a
promising acceleration strategy by reusing intermediate outputs across
timesteps, naive reuse often incurs noticeable quality degradation. In this
work, we formally analyze the cumulative error introduced by caching and
decompose it into two principal components: feature shift error, caused by
inaccuracies in cached outputs, and step amplification error, which arises from
error propagation under fixed timestep schedules. To address these issues, we
propose ERTACache, a principled caching framework that jointly rectifies both
error types. Our method employs an offline residual profiling stage to identify
reusable steps, dynamically adjusts integration intervals via a
trajectory-aware correction coefficient, and analytically approximates
cache-induced errors through a closed-form residual linearization model.
Together, these components enable accurate and efficient sampling under
aggressive cache reuse. Extensive experiments across standard image and video
generation benchmarks show that ERTACache achieves up to 2x inference speedup
while consistently preserving or even improving visual quality. Notably, on the
state-of-the-art Wan2.1 video diffusion model, ERTACache delivers 2x
acceleration with minimal VBench degradation, effectively maintaining baseline
fidelity while significantly improving efficiency. The code is available at
https://github.com/bytedance/ERTACache.

</details>


### [47] [Video-LLMs with Temporal Visual Screening](https://arxiv.org/abs/2508.21094)
*Zheyu Fan,Jiateng Liu,Yuji Zhang,Zihan Wang,Yi R.,Fung,Manling Li,Heng Ji*

Main category: cs.CV

TL;DR: The paper introduces the Temporal Visual Screening (TVS) task, designed to pre-process video question answering and instruction tuning. TVS narrows focus on important video segments while aligning queries and preserving answer consistency.


<details>
  <summary>Details</summary>
Motivation: Current Video Large Language Models (Video-LLMs) fail to understand fine-grained video semantics due to sparse frame sampling and lack of inter-frame reasoning supervision.

Method: The TVS task involves three main functions: (1) identifying focus-critical video segments, (2) transforming queries to a simple yet consistent form, and (3) ensuring answer consistency. It acts as a modular adapter for training and inference pipelines.

Result: The method outperformed previous techniques by 0.47 F-1 score in video trimming and showed notable improvements in training (7.33%) and inference (34.6%) for video-language tasks.

Conclusion: TVS proves effective in reducing cognitive load and enhancing video-language understanding, highlighting its potential in advancing Video-LLMs.

Abstract: Humans naturally perform temporal screening by dragging the progress bar and
focusing on salient temporal segments, but current Video Large Language Models
(Video-LLMs) struggle to capture fine-grained temporal semantics due to sparse
frame sampling and insufficient inter-frame reasoning supervision during their
training. To address this, Inspired by well-established cognitive science
principles, we propose Temporal Visual Screening (TVS), a new task that
universally pre-processes video question answering and instruction tuning data
by: (1) retaining focus-critical video segments, (2) synchronously
reconstructing queries to their most direct form while preserving answer
consistency, and (3) keeping the invariance and consistency for any possible
answer. TVS is formulated as a modular front-end adapter task that can be
seamlessly integrated into both Video Instruction Tuning (training) and Video
Question Answering (inference) pipelines. TVS optimizes distribution of
reasoning burden and cognitive load; during training, it aligns queries with
focus-critical visual information; at inference, it enables query-aware segment
focus and streamlined query representations. In particular, we curate the first
benchmark for TVS and propose ReSimplifyIt, a baseline outperforming prior
approaches on seemingly similar tasks by 0.47 in F-1 score on video trimming
while achieving competitive query rewriting performance. Experiments
demonstrate that incorporating TVS yields relative gains of 7.33% (training)
and 34.6% (inference), demonstrating the effectiveness of temporal information
screening for improving video-language understanding.

</details>


### [48] [ROBUST-MIPS: A Combined Skeletal Pose and Instance Segmentation Dataset for Laparoscopic Surgical Instruments](https://arxiv.org/abs/2508.21096)
*Zhe Han,Charlie Budd,Gongyu Zhang,Huanyu Tian,Christos Bergeles,Tom Vercauteren*

Main category: cs.CV

TL;DR: The paper introduces a new enriched dataset, ROBUST-MIPS, which combines tool pose and instance segmentation annotations for surgical tools. It argues for the efficiency of skeletal pose annotations for surgical tools and demonstrates its effectiveness through benchmarks.


<details>
  <summary>Details</summary>
Motivation: Improve surgical tool localisation techniques by addressing the limitation of annotated data availability and proposing efficient skeletal pose annotations.

Method: Creates a combined dataset (ROBUST-MIPS) derived from an existing dataset, evaluates its capability through a benchmark using popular pose estimation methods, and releases related tools to encourage adoption.

Result: The dataset facilitates easier comparison between annotation styles, and benchmarks show high-quality results for tool pose localisation.

Conclusion: Skeletal pose annotations offer a balance of semantic richness and annotation efficiency for surgical tools, aiding in data proliferation and improving computer-assisted surgical technologies.

Abstract: Localisation of surgical tools constitutes a foundational building block for
computer-assisted interventional technologies. Works in this field typically
focus on training deep learning models to perform segmentation tasks.
Performance of learning-based approaches is limited by the availability of
diverse annotated data. We argue that skeletal pose annotations are a more
efficient annotation approach for surgical tools, striking a balance between
richness of semantic information and ease of annotation, thus allowing for
accelerated growth of available annotated data. To encourage adoption of this
annotation style, we present, ROBUST-MIPS, a combined tool pose and tool
instance segmentation dataset derived from the existing ROBUST-MIS dataset. Our
enriched dataset facilitates the joint study of these two annotation styles and
allow head-to-head comparison on various downstream tasks. To demonstrate the
adequacy of pose annotations for surgical tool localisation, we set up a simple
benchmark using popular pose estimation methods and observe high-quality
results. To ease adoption, together with the dataset, we release our benchmark
models and custom tool pose annotation software.

</details>


### [49] [Safe-Control: A Safety Patch for Mitigating Unsafe Content in Text-to-Image Generation Models](https://arxiv.org/abs/2508.21099)
*Xiangtao Meng,Yingkai Dong,Ning Yu,Li Wang,Zheng Li,Shanqing Guo*

Main category: cs.CV

TL;DR: This paper introduces Safe-Control, a plug-and-play safety patch that reduces unsafe content generation in Text-to-Image models, outperforming baseline safety mechanisms.


<details>
  <summary>Details</summary>
Motivation: The misuse or abuse of advanced T2I generation models raises serious safety concerns, prompting the need for effective safety mechanisms.

Method: Safe-Control employs data-driven strategies and safety-aware conditions to inject safety signals into locked T2I models via a modular patch-like update.

Result: Empirical evaluations demonstrate that Safe-Control reduces unsafe content generation to 7%, outperforming baseline methods which average around 20%.

Conclusion: Safe-Control is an effective, adaptable, and superior safety mechanism for ensuring safer outputs in T2I models without compromising quality or text alignment.

Abstract: Despite the advancements in Text-to-Image (T2I) generation models, their
potential for misuse or even abuse raises serious safety concerns. Model
developers have made tremendous efforts to introduce safety mechanisms that can
address these concerns in T2I models. However, the existing safety mechanisms,
whether external or internal, either remain susceptible to evasion under
distribution shifts or require extensive model-specific adjustments. To address
these limitations, we introduce Safe-Control, an innovative plug-and-play
safety patch designed to mitigate unsafe content generation in T2I models.
Using data-driven strategies and safety-aware conditions, Safe-Control injects
safety control signals into the locked T2I model, acting as an update in a
patch-like manner. Model developers can also construct various safety patches
to meet the evolving safety requirements, which can be flexibly merged into a
single, unified patch. Its plug-and-play design further ensures adaptability,
making it compatible with other T2I models of similar denoising architecture.
We conduct extensive evaluations on six diverse and public T2I models.
Empirical results highlight that Safe-Control is effective in reducing unsafe
content generation across six diverse T2I models with similar generative
architectures, yet it successfully maintains the quality and text alignment of
benign images. Compared to seven state-of-the-art safety mechanisms, including
both external and internal defenses, Safe-Control significantly outperforms all
baselines in reducing unsafe content generation. For example, it reduces the
probability of unsafe content generation to 7%, compared to approximately 20%
for most baseline methods, under both unsafe prompts and the latest adversarial
attacks.

</details>


### [50] [GENNAV: Polygon Mask Generation for Generalized Referring Navigable Regions](https://arxiv.org/abs/2508.21102)
*Kei Katsumata,Yui Iioka,Naoki Hosomi,Teruhisa Misu,Kentaro Yamada,Komei Sugiura*

Main category: cs.CV

TL;DR: The paper presents GENNAV, a system for predicting the presence and segmenting target regions using natural language instructions and camera images, particularly focusing on challenging stuff-type targets. It introduces a benchmark called GRiN-Drive and showcases GENNAV's superior performance on various metrics and real-world tasks.


<details>
  <summary>Details</summary>
Motivation: The aim is to address the complexities in identifying target regions with ambiguous boundaries, especially stuff-type targets, and to improve prediction in scenarios with absent or multiple targets, which existing methods handle poorly.

Method: The authors introduced GENNAV, a model capable of both predicting target existence and generating segmentation masks for multiple stuff-type target regions. They evaluated it using the GRiN-Drive benchmark and conducted real-world zero-shot transfer experiments.

Result: GENNAV outperformed baseline methods across all metrics on the GRiN-Drive benchmark and demonstrated superior zero-shot transfer performance in real-world experiments across geographically diverse urban areas.

Conclusion: GENNAV is a robust and effective method for target region prediction and segmentation. It excels in challenging situations with stuff-type targets and is adaptable for diverse real-world environments, addressing shortcomings in existing solutions.

Abstract: We focus on the task of identifying the location of target regions from a
natural language instruction and a front camera image captured by a mobility.
This task is challenging because it requires both existence prediction and
segmentation, particularly for stuff-type target regions with ambiguous
boundaries. Existing methods often underperform in handling stuff-type target
regions, in addition to absent or multiple targets. To overcome these
limitations, we propose GENNAV, which predicts target existence and generates
segmentation masks for multiple stuff-type target regions. To evaluate GENNAV,
we constructed a novel benchmark called GRiN-Drive, which includes three
distinct types of samples: no-target, single-target, and multi-target. GENNAV
achieved superior performance over baseline methods on standard evaluation
metrics. Furthermore, we conducted real-world experiments with four automobiles
operated in five geographically distinct urban areas to validate its zero-shot
transfer performance. In these experiments, GENNAV outperformed baseline
methods and demonstrated its robustness across diverse real-world environments.
The project page is available at https://gennav.vercel.app/.

</details>


### [51] [R-4B: Incentivizing General-Purpose Auto-Thinking Capability in MLLMs via Bi-Mode Annealing and Reinforce Learning](https://arxiv.org/abs/2508.21113)
*Jie Jiang,Qi Yang,Bolin Ni,Shiming Xiang,Han Hu,Houwen Peng*

Main category: cs.CV

TL;DR: The paper introduces R-4B, an intelligent MLLM capable of adaptively switching between complex reasoning and simpler responses based on the problem's complexity.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the inefficiency in MLLMs when applying redundant thinking for simple problems solvable without complex reasoning.

Method: R-4B uses bi-mode annealing to embed thinking and non-thinking capabilities and employs Bi-mode Policy Optimization (BPO) for accurate decision-making regarding activating the thinking process.

Result: R-4B outperforms Qwen2.5-VL-7B across several tasks and rivals larger models like Kimi-VL-A3B-Thinking-2506 (16B) on reasoning-heavy benchmarks with reduced computational costs.

Conclusion: R-4B demonstrates adaptive reasoning capabilities and efficient resource usage, making it a significant improvement in MLLM performance across various benchmarks.

Abstract: Multimodal Large Language Models (MLLMs) equipped with step-by-step thinking
capabilities have demonstrated remarkable performance on complex reasoning
problems. However, this thinking process is redundant for simple problems
solvable without complex reasoning. To address this inefficiency, we propose
R-4B, an auto-thinking MLLM, which can adaptively decide when to think based on
problem complexity. The central idea of R-4B is to empower the model with both
thinking and non-thinking capabilities using bi-mode annealing, and apply
Bi-mode Policy Optimization~(BPO) to improve the model's accuracy in
determining whether to activate the thinking process. Specifically, we first
train the model on a carefully curated dataset spanning various topics, which
contains samples from both thinking and non-thinking modes. Then it undergoes a
second phase of training under an improved GRPO framework, where the policy
model is forced to generate responses from both modes for each input query.
Experimental results show that R-4B achieves state-of-the-art performance
across 25 challenging benchmarks. It outperforms Qwen2.5-VL-7B in most tasks
and achieves performance comparable to larger models such as
Kimi-VL-A3B-Thinking-2506 (16B) on reasoning-intensive benchmarks with lower
computational cost.

</details>


### [52] [HiddenObject: Modality-Agnostic Fusion for Multimodal Hidden Object Detection](https://arxiv.org/abs/2508.21135)
*Harris Song,Tuan-Anh Vu,Sanjith Menon,Sriram Narasimhan,M. Khalid Jawed*

Main category: cs.CV

TL;DR: The paper introduces HiddenObject, a fusion framework that integrates RGB, thermal, and depth data to address challenges like occlusion and camouflage in object detection.


<details>
  <summary>Details</summary>
Motivation: Traditional RGB-based detection methods struggle with hidden or partially concealed objects due to factors like lighting and occlusion.

Method: HiddenObject employs a Mamba-based fusion mechanism to extract complementary features from RGB, thermal, and depth modalities and fuses them into a unified representation.

Result: HiddenObject achieves state-of-the-art or competitive performance on benchmark datasets, outperforming unimodal and simple fusion methods.

Conclusion: Multimodal fusion architectures like HiddenObject offer significant advancements in detecting obscured objects under challenging conditions.

Abstract: Detecting hidden or partially concealed objects remains a fundamental
challenge in multimodal environments, where factors like occlusion, camouflage,
and lighting variations significantly hinder performance. Traditional RGB-based
detection methods often fail under such adverse conditions, motivating the need
for more robust, modality-agnostic approaches. In this work, we present
HiddenObject, a fusion framework that integrates RGB, thermal, and depth data
using a Mamba-based fusion mechanism. Our method captures complementary signals
across modalities, enabling enhanced detection of obscured or camouflaged
targets. Specifically, the proposed approach identifies modality-specific
features and fuses them in a unified representation that generalizes well
across challenging scenarios. We validate HiddenObject across multiple
benchmark datasets, demonstrating state-of-the-art or competitive performance
compared to existing methods. These results highlight the efficacy of our
fusion design and expose key limitations in current unimodal and na\"ive fusion
strategies. More broadly, our findings suggest that Mamba-based fusion
architectures can significantly advance the field of multimodal object
detection, especially under visually degraded or complex conditions.

</details>


### [53] [RadGS-Reg: Registering Spine CT with Biplanar X-rays via Joint 3D Radiative Gaussians Reconstruction and 3D/3D Registration](https://arxiv.org/abs/2508.21154)
*Ao Shen,Xueming Fu,Junfeng Jiang,Qiang Zeng,Ye Tang,Zhengming Chen,Luming Nong,Feng Wang,S. Kevin Zhou*

Main category: cs.CV

TL;DR: This paper introduces RadGS-Reg, a novel framework for CT/X-ray registration that uses a learning-based method for 3D Radiative Gaussians reconstruction and achieves superior performance on both accuracy and noise robustness.


<details>
  <summary>Details</summary>
Motivation: Traditional CT/X-ray registration techniques face challenges with spatial information loss, domain gaps, dense-view requirements, and handling noisy X-rays.

Method: RadGS-Reg employs a learning-based technique for 3D Radiative Gaussians reconstruction with a Counterfactual Attention Learning (CAL) mechanism. It also incorporates a patient-specific pre-training strategy for adapting from simulated to real data.

Result: The framework demonstrated state-of-the-art performance in vertebral CT/X-ray registration tasks, outperforming existing methods in experiments on in-house datasets.

Conclusion: RadGS-Reg effectively addresses challenges in conventional methods, offering high accuracy, better noise handling, and real-time potential for CT/X-ray registration applications.

Abstract: Computed Tomography (CT)/X-ray registration in image-guided navigation
remains challenging because of its stringent requirements for high accuracy and
real-time performance. Traditional "render and compare" methods, relying on
iterative projection and comparison, suffer from spatial information loss and
domain gap. 3D reconstruction from biplanar X-rays supplements spatial and
shape information for 2D/3D registration, but current methods are limited by
dense-view requirements and struggles with noisy X-rays. To address these
limitations, we introduce RadGS-Reg, a novel framework for vertebral-level
CT/X-ray registration through joint 3D Radiative Gaussians (RadGS)
reconstruction and 3D/3D registration. Specifically, our biplanar X-rays
vertebral RadGS reconstruction module explores learning-based RadGS
reconstruction method with a Counterfactual Attention Learning (CAL) mechanism,
focusing on vertebral regions in noisy X-rays. Additionally, a patient-specific
pre-training strategy progressively adapts the RadGS-Reg from simulated to real
data while simultaneously learning vertebral shape prior knowledge. Experiments
on in-house datasets demonstrate the state-of-the-art performance for both
tasks, surpassing existing methods. The code is available at:
https://github.com/shenao1995/RadGS_Reg.

</details>


### [54] [SYNBUILD-3D: A large, multi-modal, and semantically rich synthetic dataset of 3D building models at Level of Detail 4](https://arxiv.org/abs/2508.21169)
*Kevin Mayer,Alex Vesel,Xinyi Zhao,Martin Fischer*

Main category: cs.CV

TL;DR: This paper introduces SYNBUILD-3D, a large dataset of synthetic 3D residential buildings in three formats: wireframe graph, floor plan images, and roof point clouds, designed to aid in developing AI for automated 3D building models.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the difficulty in automatically creating accurate and semantically rich 3D building models due to a lack of large-scale annotated datasets.

Method: The authors created SYNBUILD-3D, a synthetic, multi-modal dataset comprising over 6.2 million 3D residential buildings with Level of Detail 4 (LoD 4); each building is represented in three modalities, including enriched wireframes, floor plans, and roof point clouds, with semantic annotations.

Result: The SYNBUILD-3D dataset includes annotations for rooms, doors, and windows, supporting tri-modal data representation for richer AI model development.

Conclusion: SYNBUILD-3D can enable novel generative AI approaches for creating 3D building models while ensuring semantic and geometric consistency. The dataset is publicly available for use by researchers.

Abstract: 3D building models are critical for applications in architecture, energy
simulation, and navigation. Yet, generating accurate and semantically rich 3D
buildings automatically remains a major challenge due to the lack of
large-scale annotated datasets in the public domain. Inspired by the success of
synthetic data in computer vision, we introduce SYNBUILD-3D, a large, diverse,
and multi-modal dataset of over 6.2 million synthetic 3D residential buildings
at Level of Detail (LoD) 4. In the dataset, each building is represented
through three distinct modalities: a semantically enriched 3D wireframe graph
at LoD 4 (Modality I), the corresponding floor plan images (Modality II), and a
LiDAR-like roof point cloud (Modality III). The semantic annotations for each
building wireframe are derived from the corresponding floor plan images and
include information on rooms, doors, and windows. Through its tri-modal nature,
future work can use SYNBUILD-3D to develop novel generative AI algorithms that
automate the creation of 3D building models at LoD 4, subject to predefined
floor plan layouts and roof geometries, while enforcing semantic-geometric
consistency. Dataset and code samples are publicly available at
https://github.com/kdmayer/SYNBUILD-3D.

</details>


### [55] [Radially Distorted Homographies, Revisited](https://arxiv.org/abs/2508.21190)
*Mårten Wadenbäck,Marcus Valtonen Örnhag,Johan Edstedt*

Main category: cs.CV

TL;DR: The paper proposes a unified approach to estimate homography with radial distortion in three different configurations, offering faster and accurate solvers compared to prior methods.


<details>
  <summary>Details</summary>
Motivation: Homography estimation is vital in geometric computer vision tasks, but real images often contain geometric distortions like radial distortion. A unified method for estimating homography and radial distortion simultaneously is lacking.

Method: The paper introduces a unified framework for three distinct cases of radial distortion during homography estimation and constructs fast, stable, and accurate minimal solvers.

Result: The proposed solvers perform faster than the state-of-the-art while maintaining comparable accuracy, verified through benchmarks including images from fisheye cameras.

Conclusion: A novel unified approach has been developed for radially distorted homographies, yielding efficient solvers suitable for practical applications. Code will be released upon publication.

Abstract: Homographies are among the most prevalent transformations occurring in
geometric computer vision and projective geometry, and homography estimation is
consequently a crucial step in a wide assortment of computer vision tasks. When
working with real images, which are often afflicted with geometric distortions
caused by the camera lens, it may be necessary to determine both the homography
and the lens distortion-particularly the radial component, called radial
distortion-simultaneously to obtain anything resembling useful estimates. When
considering a homography with radial distortion between two images, there are
three conceptually distinct configurations for the radial distortion; (i)
distortion in only one image, (ii) identical distortion in the two images, and
(iii) independent distortion in the two images. While these cases have been
addressed separately in the past, the present paper provides a novel and
unified approach to solve all three cases. We demonstrate how the proposed
approach can be used to construct new fast, stable, and accurate minimal
solvers for radially distorted homographies. In all three cases, our proposed
solvers are faster than the existing state-of-the-art solvers while maintaining
similar accuracy. The solvers are tested on well-established benchmarks
including images taken with fisheye cameras. The source code for our solvers
will be made available in the event our paper is accepted for publication.

</details>


### [56] [GCAV: A Global Concept Activation Vector Framework for Cross-Layer Consistency in Interpretability](https://arxiv.org/abs/2508.21197)
*Zhenghao He,Sanchit Sinha,Guangzhi Xiong,Aidong Zhang*

Main category: cs.CV

TL;DR: This paper introduces Global Concept Activation Vectors (GCAV) to address inconsistencies in Concept Activation Vectors (CAVs) across neural network layers, enhancing interpretability and robustness.


<details>
  <summary>Details</summary>
Motivation: Concept Activation Vectors (CAVs) provide insight into neural networks' sensitivity to concepts but exhibit layer-specific inconsistencies, limiting cross-layer comparisons.

Method: The proposed GCAV method unifies CAVs into a consistent representation using contrastive learning for concept alignment across layers and attention-based fusion for global integration.

Result: Experiments show GCAV reduces variance in TCAV scores, improves concept localization, and offers robustness against adversarial perturbations across multiple neural network models.

Conclusion: GCAV enhances the interpretability and reliability of concept-driven analyses in deep learning by providing a unified and consistent representation across layers.

Abstract: Concept Activation Vectors (CAVs) provide a powerful approach for
interpreting deep neural networks by quantifying their sensitivity to
human-defined concepts. However, when computed independently at different
layers, CAVs often exhibit inconsistencies, making cross-layer comparisons
unreliable. To address this issue, we propose the Global Concept Activation
Vector (GCAV), a novel framework that unifies CAVs into a single, semantically
consistent representation. Our method leverages contrastive learning to align
concept representations across layers and employs an attention-based fusion
mechanism to construct a globally integrated CAV. By doing so, our method
significantly reduces the variance in TCAV scores while preserving concept
relevance, ensuring more stable and reliable concept attributions. To evaluate
the effectiveness of GCAV, we introduce Testing with Global Concept Activation
Vectors (TGCAV) as a method to apply TCAV to GCAV-based representations. We
conduct extensive experiments on multiple deep neural networks, demonstrating
that our method effectively mitigates concept inconsistency across layers,
enhances concept localization, and improves robustness against adversarial
perturbations. By integrating cross-layer information into a coherent
framework, our method offers a more comprehensive and interpretable
understanding of how deep learning models encode human-defined concepts. Code
and models are available at https://github.com/Zhenghao-He/GCAV.

</details>


### [57] [Generalizable Object Re-Identification via Visual In-Context Prompting](https://arxiv.org/abs/2508.21222)
*Zhizhong Huang,Xiaoming Liu*

Main category: cs.CV

TL;DR: This paper proposes VICP, a framework leveraging large language models (LLMs) and vision foundation models (VFMs) to generalize object re-identification to unseen categories without dataset-specific retraining.


<details>
  <summary>Details</summary>
Motivation: Current object ReID methods are limited in generalization and require costly labeled data for new categories.

Method: VICP combines LLMs for inferring semantic identity rules and VFMs for extracting ID-discriminative features using dynamic visual prompts, enabling generalization without parameter adaptation.

Result: VICP shows superior performance on unseen categories compared to baselines, validated by experiments on ShopID10K and diverse ReID benchmarks.

Conclusion: VICP eliminates the need for dataset-specific retraining, offering an effective solution for generalizing object ReID to novel categories using in-context examples.

Abstract: Current object re-identification (ReID) methods train domain-specific models
(e.g., for persons or vehicles), which lack generalization and demand costly
labeled data for new categories. While self-supervised learning reduces
annotation needs by learning instance-wise invariance, it struggles to capture
\textit{identity-sensitive} features critical for ReID. This paper proposes
Visual In-Context Prompting~(VICP), a novel framework where models trained on
seen categories can directly generalize to unseen novel categories using only
\textit{in-context examples} as prompts, without requiring parameter
adaptation. VICP synergizes LLMs and vision foundation models~(VFM): LLMs infer
semantic identity rules from few-shot positive/negative pairs through
task-specific prompting, which then guides a VFM (\eg, DINO) to extract
ID-discriminative features via \textit{dynamic visual prompts}. By aligning
LLM-derived semantic concepts with the VFM's pre-trained prior, VICP enables
generalization to novel categories, eliminating the need for dataset-specific
retraining. To support evaluation, we introduce ShopID10K, a dataset of 10K
object instances from e-commerce platforms, featuring multi-view images and
cross-domain testing. Experiments on ShopID10K and diverse ReID benchmarks
demonstrate that VICP outperforms baselines by a clear margin on unseen
categories. Code is available at https://github.com/Hzzone/VICP.

</details>


### [58] [Lightweight MRI-Based Automated Segmentation of Pancreatic Cancer with Auto3DSeg](https://arxiv.org/abs/2508.21227)
*Keshav Jha,William Sharp,Dominic LaBella*

Main category: cs.CV

TL;DR: The study evaluates SegResNet's performance in automated pancreatic tumor segmentation on MRI, achieving modest results that underline dataset challenges.


<details>
  <summary>Details</summary>
Motivation: Address challenges in automating pancreatic tumor segmentation due to anatomical variability and limited datasets.

Method: SegResNet models using 5-fold cross-validation and STAPLE ensembling, tested on two MRI tasks with expert annotations.

Result: Task 1 showed moderate segmentation accuracy (DSC 0.56), while Task 2 performed worse (DSC 0.33), illustrating variability due to MRI sequences.

Conclusion: Despite limitations, the study demonstrates benefits of automated delineation and calls for larger standardized datasets for clinical improvements.

Abstract: Accurate delineation of pancreatic tumors is critical for diagnosis,
treatment planning, and outcome assessment, yet automated segmentation remains
challenging due to anatomical variability and limited dataset availability. In
this study, SegResNet models, as part of the Auto3DSeg architecture, were
trained and evaluated on two MRI-based pancreatic tumor segmentation tasks as
part of the 2025 PANTHER Challenge. Algorithm methodology included 5-fold
cross-validation with STAPLE ensembling after focusing on an anatomically
relevant region-of-interest. The Pancreatic Tumor Segmentation on Diagnostic
MRI task 1 training set included 91 T1-weighted arterial contrast-enhanced MRI
with expert annotated pancreas and tumor labels. The Pancreatic Tumor
Segmentation on MR-Linac task 2 training set used 50 T2-weighted MR-Linac cases
with expert annotated pancreas and tumor labels. Algorithm-automated
segmentation performance of pancreatic tumor was assessed using Dice Similarity
Coefficient (DSC), 5 mm DSC, 95th percentile Hausdorff Distance (HD95), Mean
Average Surface Distance (MASD), and Root Mean Square Error (RMSE). For Task 1,
the algorithm achieved a DSC of 0.56, 5 mm DSC of 0.73, HD95 of 41.1 mm, MASD
of 26.0 mm, and RMSE of 5164 mm. For Task 2, performance decreased, with a DSC
of 0.33, 5 mm DSC of 0.50, HD95 of 20.1 mm, MASD of 7.2 mm, and RMSE of 17,203
mm. These findings illustrate the challenges of MRI-based pancreatic tumor
segmentation with small datasets, highlighting variability introduced by
different MRI sequences. Despite modest performance, the results demonstrate
potential for automated delineation and emphasize the need for larger,
standardized MRI datasets to improve model robustness and clinical utility.

</details>


### [59] [Reverse Imaging for Wide-spectrum Generalization of Cardiac MRI Segmentation](https://arxiv.org/abs/2508.21254)
*Yidong Zhao,Peter Kellman,Hui Xue,Tongyun Yang,Yi Zhang,Yuchi Han,Orlando Simonetti,Qian Tao*

Main category: cs.CV

TL;DR: Pretrained models struggle with cardiac MRI generalization due to imaging contrast variations. This paper introduces Reverse Imaging, a physics-driven method to solve this.


<details>
  <summary>Details</summary>
Motivation: To address the issue of poor generalizability of pretrained segmentation models across varying cardiac MRI sequences caused by differences in imaging contrast.

Method: Introduced Reverse Imaging, which infers underlying spin properties from MR images using a generative diffusion model trained on the mSASHA dataset for domain adaptation and data augmentation.

Result: Achieved highly accurate segmentation across different image contrasts and imaging protocols using the Reverse Imaging approach.

Conclusion: Reverse Imaging can improve domain adaptation in cardiac MRI, enabling generalizable segmentation across a wide range of contrasts and imaging protocols.

Abstract: Pretrained segmentation models for cardiac magnetic resonance imaging (MRI)
struggle to generalize across different imaging sequences due to significant
variations in image contrast. These variations arise from changes in imaging
protocols, yet the same fundamental spin properties, including proton density,
T1, and T2 values, govern all acquired images. With this core principle, we
introduce Reverse Imaging, a novel physics-driven method for cardiac MRI data
augmentation and domain adaptation to fundamentally solve the generalization
problem. Our method reversely infers the underlying spin properties from
observed cardiac MRI images, by solving ill-posed nonlinear inverse problems
regularized by the prior distribution of spin properties. We acquire this "spin
prior" by learning a generative diffusion model from the multiparametric
SAturation-recovery single-SHot acquisition sequence (mSASHA) dataset, which
offers joint cardiac T1 and T2 maps. Our method enables approximate but
meaningful spin-property estimates from MR images, which provide an
interpretable "latent variable" that lead to highly flexible image synthesis of
arbitrary novel sequences. We show that Reverse Imaging enables highly accurate
segmentation across vastly different image contrasts and imaging protocols,
realizing wide-spectrum generalization of cardiac MRI segmentation.

</details>


### [60] [PHD: Personalized 3D Human Body Fitting with Point Diffusion](https://arxiv.org/abs/2508.21257)
*Hsuan-I Ho,Chen Guo,Po-Chen Wu,Ivan Shugurov,Chengcheng Tang,Abhay Mittal,Sizhe An,Manuel Kaufmann,Linguang Zhang*

Main category: cs.CV

TL;DR: PHD introduces body-shape-specific 3D human mesh recovery to enhance pose estimation, using a Point Diffusion Transformer and data-efficient training.


<details>
  <summary>Details</summary>
Motivation: Traditional HMR methods overlook user-specific body shapes, impacting pose estimation accuracy.

Method: A two-step pipeline: body shape calibration and personalized pose fitting using a Point Diffusion Transformer and Point Distillation Sampling loss.

Result: Improved pelvis-aligned and absolute pose accuracy without relying heavily on 2D constraints, using only synthetic data.

Conclusion: PHD is a robust, efficient approach for personalized 3D human pose recovery and integrates well with existing estimators.

Abstract: We introduce PHD, a novel approach for personalized 3D human mesh recovery
(HMR) and body fitting that leverages user-specific shape information to
improve pose estimation accuracy from videos. Traditional HMR methods are
designed to be user-agnostic and optimized for generalization. While these
methods often refine poses using constraints derived from the 2D image to
improve alignment, this process compromises 3D accuracy by failing to jointly
account for person-specific body shapes and the plausibility of 3D poses. In
contrast, our pipeline decouples this process by first calibrating the user's
body shape and then employing a personalized pose fitting process conditioned
on that shape. To achieve this, we develop a body shape-conditioned 3D pose
prior, implemented as a Point Diffusion Transformer, which iteratively guides
the pose fitting via a Point Distillation Sampling loss. This learned 3D pose
prior effectively mitigates errors arising from an over-reliance on 2D
constraints. Consequently, our approach improves not only pelvis-aligned pose
accuracy but also absolute pose accuracy -- an important metric often
overlooked by prior work. Furthermore, our method is highly data-efficient,
requiring only synthetic data for training, and serves as a versatile
plug-and-play module that can be seamlessly integrated with existing 3D pose
estimators to enhance their performance. Project page:
https://phd-pose.github.io/

</details>


### [61] [Efficient Diffusion-Based 3D Human Pose Estimation with Hierarchical Temporal Pruning](https://arxiv.org/abs/2508.21363)
*Yuquan Bi,Hongsong Wang,Xinli Shi,Zhipeng Gui,Jie Gui,Yuan Yan Tang*

Main category: cs.CV

TL;DR: The paper introduces an improved framework for 3D human pose estimation using diffusion models, incorporating a Hierarchical Temporal Pruning (HTP) strategy to reduce computational costs and improve processing speeds.


<details>
  <summary>Details</summary>
Motivation: Diffusion models are computationally expensive due to their iterative nature and multi-hypothesis requirements in generating accurate 3D human poses.

Method: The paper proposes a Hierarchical Temporal Pruning (HTP) strategy, consisting of three steps: (1) Temporal Correlation-Enhanced Pruning (TCEP) to identify key frames based on motion dynamics; (2) Sparse-Focused Temporal MHSA (SFT MHSA) for efficient attention computation on motion-relevant tokens; and (3) Mask-Guided Pose Token Pruner (MGPTP) for fine-grained semantic pruning to retain essential pose tokens.

Result: The proposed method achieves significant computational efficiency, reducing training MACs by 38.5%, inference MACs by 56.8%, and improving inference speed by 81.1%. Additionally, it achieves state-of-the-art performance on Human3.6M and MPI-INF-3DHP benchmarks.

Conclusion: The HTP strategy enhances the efficiency and speed of 3D human pose estimation using diffusion models without sacrificing performance, making it a valuable contribution to reducing computational overhead.

Abstract: Diffusion models have demonstrated strong capabilities in generating
high-fidelity 3D human poses, yet their iterative nature and multi-hypothesis
requirements incur substantial computational cost. In this paper, we propose an
Efficient Diffusion-Based 3D Human Pose Estimation framework with a
Hierarchical Temporal Pruning (HTP) strategy, which dynamically prunes
redundant pose tokens across both frame and semantic levels while preserving
critical motion dynamics. HTP operates in a staged, top-down manner: (1)
Temporal Correlation-Enhanced Pruning (TCEP) identifies essential frames by
analyzing inter-frame motion correlations through adaptive temporal graph
construction; (2) Sparse-Focused Temporal MHSA (SFT MHSA) leverages the
resulting frame-level sparsity to reduce attention computation, focusing on
motion-relevant tokens; and (3) Mask-Guided Pose Token Pruner (MGPTP) performs
fine-grained semantic pruning via clustering, retaining only the most
informative pose tokens. Experiments on Human3.6M and MPI-INF-3DHP show that
HTP reduces training MACs by 38.5\%, inference MACs by 56.8\%, and improves
inference speed by an average of 81.1\% compared to prior diffusion-based
methods, while achieving state-of-the-art performance.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [62] [Fast and Scalable Mixed Precision Euclidean Distance Calculations Using GPU Tensor Cores](https://arxiv.org/abs/2508.21230)
*Brian Curless,Michael Gowanlock*

Main category: cs.DC

TL;DR: The paper introduces FaSTED, an algorithm leveraging tensor cores (TCs) for high-performance Euclidean distance calculations using mixed precision (FP16-32), achieving up to 51x speedup with minimal accuracy loss.


<details>
  <summary>Details</summary>
Motivation: GPUs with tensor cores have high computational throughput but are underutilized for applications beyond AI. Optimizing these cores for Euclidean distance calculations could unlock performance benefits in data analytics.

Method: Proposed FaSTED algorithm to exploit hierarchical reuse of data and maximize memory utilization across various levels. Mixed-precision computation (FP16-32) is employed to enhance TC utilization.

Result: Achieved 2.5-51x speedup over state-of-the-art FP64 algorithms across high-dimensional datasets with an accuracy loss of less than 0.06%.

Conclusion: The FaSTED algorithm successfully demonstrates how to leverage tensor cores for Euclidean distance calculations, providing significant performance improvements with negligible accuracy trade-offs.

Abstract: Modern GPUs are equipped with tensor cores (TCs) that are commonly used for
matrix multiplication in artificial intelligence workloads. However, because
they have high computational throughput, they can lead to significant
performance gains in other algorithms if they can be successfully exploited. We
examine using TCs to compute Euclidean distance calculations, which are used in
many data analytics applications. Prior work has only investigated using 64 bit
floating point (FP64) data for computation; however, TCs can operate on lower
precision floating point data (i.e., 16 bit matrix multiplication and 32 bit
accumulation), which we refer to as FP16-32. FP16-32 TC peak throughput is so
high that TCs are easily starved of data. We propose a Fast and Scalable Tensor
core Euclidean Distance (FaSTED) algorithm. To achieve high computational
throughput, we design FaSTED for significant hierarchical reuse of data and
maximize memory utilization at every level (global memory, shared memory, and
registers). We apply FaSTED to the application of similarity searches, which
typically employ an indexing data structure to eliminate superfluous Euclidean
distance calculations. We compare to the state-of-the-art (SOTA) TC Euclidean
distance algorithm in the literature that employs FP64, as well as to two
single precision (FP32) CUDA core algorithms that both employ an index. We find
that across four real-world high-dimensional datasets spanning 128-960
dimensions, the mixed-precision brute force approach achieves a speedup over
the SOTA algorithms of 2.5-51x. We also quantify the accuracy loss of our mixed
precision algorithm to be less than <0.06% when compared to the FP64 baseline.

</details>


### [63] [Decentralized Federated Averaging via Random Walk](https://arxiv.org/abs/2508.21286)
*Changheng Wang,Zhiqing Wei,Lizhe Liu,Qiao Deng,Yingda Wu,Yangyang Niu,Yashan Pang,Zhiyong Feng*

Main category: cs.DC

TL;DR: The paper introduces DFedRW, a decentralized federated learning approach using random walk updates to improve convergence and address data heterogeneity challenges. A quantized version enhances communication efficiency.


<details>
  <summary>Details</summary>
Motivation: Address limitations in traditional FL methods, such as convergence issues due to heterogeneous data and inefficiencies in decentralized systems.

Method: Develop DFedRW combining random walk updates and quantization to mitigate stragglers and balance communication-efficiency with model accuracy.

Result: DFedRW demonstrates superior convergence rate and accuracy compared to decentralized FedAvg, achieving a test accuracy improvement of over 37% under high heterogeneity.

Conclusion: DFedRW offers improved robustness, efficiency, and accuracy in decentralized federated learning environments.

Abstract: Federated Learning (FL) is a communication-efficient distributed machine
learning method that allows multiple devices to collaboratively train models
without sharing raw data. FL can be categorized into centralized and
decentralized paradigms. The centralized paradigm relies on a central server to
aggregate local models, potentially resulting in single points of failure,
communication bottlenecks, and exposure of model parameters. In contrast, the
decentralized paradigm, which does not require a central server, provides
improved robustness and privacy. The essence of federated learning lies in
leveraging multiple local updates for efficient communication. However, this
approach may result in slower convergence or even convergence to suboptimal
models in the presence of heterogeneous and imbalanced data. To address this
challenge, we study decentralized federated averaging via random walk (DFedRW),
which replaces multiple local update steps on a single device with random walk
updates. Traditional Federated Averaging (FedAvg) and its decentralized
versions commonly ignore stragglers, which reduces the amount of training data
and introduces sampling bias. Therefore, we allow DFedRW to aggregate partial
random walk updates, ensuring that each computation contributes to the model
update. To further improve communication efficiency, we also propose a
quantized version of DFedRW. We demonstrate that (quantized) DFedRW achieves
convergence upper bound of order $\mathcal{O}(\frac{1}{k^{1-q}})$ under convex
conditions. Furthermore, we propose a sufficient condition that reveals when
quantization balances communication and convergence. Numerical analysis
indicates that our proposed algorithms outperform (decentralized) FedAvg in
both convergence rate and accuracy, achieving a 38.3\% and 37.5\% increase in
test accuracy under high levels of heterogeneities.

</details>


### [64] [Addressing Reproducibility Challenges in HPC with Continuous Integration](https://arxiv.org/abs/2508.21289)
*Valérie Hayot-Sasson,Nathaniel Hudson,André Bauer,Maxime Gonthier,Ian Foster,Kyle Chard*

Main category: cs.DC

TL;DR: The paper addresses challenges in reproducibility in the HPC field and introduces CORRECT, a GitHub Action to enhance reproducibility through secure testing on remote resources.


<details>
  <summary>Details</summary>
Motivation: HPC requires reproducibility, but unique infrastructure, software, and access requirements often limit achieving it. The paper aims to improve reproducibility without direct access to resources.

Method: A survey of reproducibility initiatives and barriers in HPC was conducted. A GitHub Action called CORRECT was proposed and tested on three types of HPC applications.

Result: CORRECT was successfully evaluated on three different HPC applications, showing its utility in automating and documenting reproducibility processes.

Conclusion: Better HPC-compliant CI solutions, such as CORRECT, can mitigate reproducibility challenges and enhance documented testing and provenance information.

Abstract: The high-performance computing (HPC) community has adopted incentive
structures to motivate reproducible research, with major conferences awarding
badges to papers that meet reproducibility requirements. Yet, many papers do
not meet such requirements. The uniqueness of HPC infrastructure and software,
coupled with strict access requirements, may limit opportunities for
reproducibility. In the absence of resource access, we believe that regular
documented testing, through continuous integration (CI), coupled with complete
provenance information, can be used as a substitute. Here, we argue that better
HPC-compliant CI solutions will improve reproducibility of applications. We
present a survey of reproducibility initiatives and describe the barriers to
reproducibility in HPC. To address existing limitations, we present a GitHub
Action, CORRECT, that enables secure execution of tests on remote HPC
resources. We evaluate CORRECT's usability across three different types of HPC
applications, demonstrating the effectiveness of using CORRECT for automating
and documenting reproducibility evaluations.

</details>


### [65] [A Knowledge Distillation-empowered Adaptive Federated Reinforcement Learning Framework for Multi-Domain IoT Applications Scheduling](https://arxiv.org/abs/2508.21328)
*Zhiyu Wang,Mohammad Goudarzi,Mingming Gong,Rajkumar Buyya*

Main category: cs.DC

TL;DR: This paper introduces KD-AFRL, a novel framework for improving IoT scheduling across heterogeneous Cloud-Edge-IoT environments using knowledge distillation and federated reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: The increasing adoption of IoT applications in diverse environments requires efficient distributed scheduling solutions that can address computational heterogeneity, non-IID data, and the need for cross-domain collaboration.

Method: The KD-AFRL framework uses three innovations: resource-aware dual-zone neural networks for device compatibility, privacy-preserving federated learning with differential privacy and clustering, and cross-architecture knowledge distillation with temperature-regulated soft targets.

Result: Comprehensive experiments display KD-AFRL’s superiority with a 21% improvement in convergence speed, and 15.7%, 10.8%, and 13.9% gains in completion time, energy consumption, and weighted cost, respectively, along with enhanced scalability.

Conclusion: KD-AFRL demonstrates significant improvements in IoT application scheduling, offering enhanced performance and scalability, making it a compelling solution for heterogeneous environments.

Abstract: The rapid proliferation of Internet of Things (IoT) applications across
heterogeneous Cloud-Edge-IoT environments presents significant challenges in
distributed scheduling optimization. Existing approaches face issues, including
fixed neural network architectures that are incompatible with computational
heterogeneity, non-Independent and Identically Distributed (non-IID) data
distributions across IoT scheduling domains, and insufficient cross-domain
collaboration mechanisms. This paper proposes KD-AFRL, a Knowledge
Distillation-empowered Adaptive Federated Reinforcement Learning framework that
addresses multi-domain IoT application scheduling through three core
innovations. First, we develop a resource-aware hybrid architecture generation
mechanism that creates dual-zone neural networks enabling heterogeneous devices
to participate in collaborative learning while maintaining optimal resource
utilization. Second, we propose a privacy-preserving environment-clustered
federated learning approach that utilizes differential privacy and K-means
clustering to address non-IID challenges and facilitate effective collaboration
among compatible domains. Third, we introduce an environment-oriented
cross-architecture knowledge distillation mechanism that enables efficient
knowledge transfer between heterogeneous models through temperature-regulated
soft targets. Comprehensive experiments with real Cloud-Edge-IoT infrastructure
demonstrate KD-AFRL's effectiveness using diverse IoT applications. Results
show significant improvements over the best baseline, with 21% faster
convergence and 15.7%, 10.8%, and 13.9% performance gains in completion time,
energy consumption, and weighted cost, respectively. Scalability experiments
reveal that KD-AFRL achieves 3-5 times better performance retention compared to
existing solutions as the number of domains increases.

</details>


### [66] [Unpacking Maximum Extractable Value on Polygon: A Study on Atomic Arbitrage](https://arxiv.org/abs/2508.21473)
*Daniil Vostrikov,Yash Madhwal,Andrey Seoev,Anastasiia Smirnova,Yury Yanovich,Alexey Smirnov,Vladimir Gorgadze*

Main category: cs.DC

TL;DR: The paper investigates Maximum Extractable Value (MEV) on the Polygon blockchain, focusing on Atomic Arbitrage and comparing Spam-based vs. Auction-based strategies.


<details>
  <summary>Details</summary>
Motivation: To address challenges posed by MEV in decentralized financial ecosystems, particularly on the Polygon blockchain.

Method: Criteria were established for Atomic Arbitrage detection and analyzed using a dataset of 23 million blocks spanning 22 months.

Result: Spam-based transactions dominate in frequency, but Auction-based methods yield higher profitability.

Conclusion: Stronger transaction ordering mechanisms are necessary to address MEV challenges, considering its evolving strategies and significant impact on blockchain networks.

Abstract: The evolution of blockchain technology, from its origins as a decentralized
ledger for cryptocurrencies to its broader applications in areas like
decentralized finance (DeFi), has significantly transformed financial
ecosystems while introducing new challenges such as Maximum Extractable Value
(MEV). This paper explores MEV on the Polygon blockchain, with a particular
focus on Atomic Arbitrage (AA) transactions. We establish criteria for
identifying AA transactions and analyze key factors such as searcher behavior,
bidding dynamics, and token usage. Utilizing a dataset spanning 22 months and
covering 23 million blocks, we examine MEV dynamics with a focus on Spam-based
and Auction-based backrunning strategies. Our findings reveal that while
Spam-based transactions are more prevalent, Auction-based transactions
demonstrate greater profitability. Through detailed examples and analysis, we
investigate the interactions between network architecture, transaction
sequencing, and MEV extraction, offering comprehensive insights into the
evolution and challenges of MEV in decentralized ecosystems. These results
emphasize the need for robust transaction ordering mechanisms and highlight the
implications of emerging MEV strategies for blockchain networks.

</details>


### [67] [Odyssey: Adaptive Policy Selection for Resilient Distributed Training](https://arxiv.org/abs/2508.21613)
*Yuhang Zhou,Zhibin Wang,Peng Jiang,Haoran Xia,Junhe Lu,Qianyu Jiang,Rong Gu,Hengxi Xu,Xinjing Huang,Guanghuan Fang,Zhiheng Hu,Jingyi Zhang,Yongjin Cai,Jian He,Chen Tian*

Main category: cs.DC

TL;DR: Odyssey is an adaptive fault-tolerant system designed to minimize performance penalties during large language model training by intelligently selecting recovery strategies.


<details>
  <summary>Details</summary>
Motivation: Training large language models frequently faces interruptions due to faults; existing solutions incur performance losses, necessitating a robust and efficient fault-tolerance system.

Method: The paper presents Odyssey, which uses a unified performance model, efficient execution plan search, accurate performance estimation, and communication optimizations to handle failures effectively.

Result: Experiments with Odyssey on a 32-card cluster revealed that it maintains a minimal performance gap (within 11% post-recovery) and improves average throughput compared to state-of-the-art methods (1.229x higher than Oobleck and 1.355x higher than Recycle).

Conclusion: Odyssey provides an effective fault-tolerance solution for large language model training, enhancing post-recovery performance, throughput, and model convergence while optimizing memory usage.

Abstract: Training large language models faces frequent interruptions due to various
faults, demanding robust fault-tolerance. Existing backup-free methods, such as
redundant computation, dynamic parallelism, and data rerouting, each incur
performance penalties, whether from ongoing overhead, lengthy reconfigurations,
or post-recovery inefficiencies. We propose Odyssey, an adaptive fault-tolerant
system that intelligently selects optimal recovery strategies when a failure
occurs. Odyssey achieves this through a unified performance model, expedient
execution plan search, accurate performance estimation, and efficient
communication optimizations. Experiments on a 32-card cluster show that Odyssey
maintains a performance gap of within 11.00% between post-recovery and
failure-free training, while preserving model convergence and efficient memory
usage. Compared to state-of-the-art methods, Odyssey achieves up to 1.229x and
1.355x higher average throughput than Oobleck and Recycle, respectively.

</details>


### [68] [Accelerating Mixture-of-Experts Inference by Hiding Offloading Latency with Speculative Decoding](https://arxiv.org/abs/2508.21706)
*Zhibin Wang,Zhonghui Zhang,Yuhang Zhou,Zibo Wang,Mo Zhou,Peng Jiang,Weilin Cai,Chengying Huan,Rong Gu,Sheng Zhong,Chen Tian*

Main category: cs.DC

TL;DR: The paper introduces SpecMoEOff, a technique to enhance GPU and CPU hardware utilization during Mixture of Experts (MoE) inference, achieving up to 2.5x throughput improvements.


<details>
  <summary>Details</summary>
Motivation: Addressing low hardware utilization caused by sparse computations and I/O bottlenecks in MoE inference techniques.

Method: Developing SpecMoEOff with speculative decoding, theoretical and empirical roofline analysis, a CPU chunked attention kernel, and an optimizer for hyperparameter tuning.

Result: SpecMoEOff achieves up to 2.5x increased decoding throughput compared to current MoE offloading techniques.

Conclusion: The proposed method significantly improves efficiency in MoE inference, offering a viable solution to hardware utilization challenges.

Abstract: Recent advancements in Mixture of Experts (MoE) models have significantly
increased their parameter scale as well as model performance. Extensive
offloading techniques have been proposed to address the GPU memory limitations
of MoE inference. However, due to the I/O bottleneck and sparse computation of
MoE models, existing offloading techniques still suffer from low hardware
utilization. To fully utilize the hardware resources, we propose SpecMoEOff,
which employs the speculative decoding technique to enlarge the workload of
each expert. SpecMoEOff orchestrates the GPU and CPU by both theoretical and
empirical roofline analysis. In addition, we develop a dedicated CPU chunked
attention verification kernel to fit the speculative decoding in offloading
scenarios as well as minimizing the additional overhead led by draft models.
SpecMoEOff further integrates an optimizer to automatically tune the
hyperparameters of speculative decoding under given hardware and workload.
Experimental results show that SpecMoEOff achieves up to 2.5x decode throughput
improvement over the state-of-the-art MoE offloading techniques.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [69] [Normalisation of SWIFT Message Counterparties with Feature Extraction and Clustering](https://arxiv.org/abs/2508.21081)
*Thanasis Schoinas,Benjamin Guinard,Diba Esbati,Richard Chalk*

Main category: cs.LG

TL;DR: The paper presents a pipeline combining string similarity, topic modeling, hierarchical clustering, and rule-based methods for clustering transaction counterparties.


<details>
  <summary>Details</summary>
Motivation: Current natural language models fail to cluster transaction counterparties in payment systems due to non-natural language inputs and noisy variations, creating a challenge for investigators in areas like fraud detection or sanctions compliance.

Method: The authors propose a hybrid approach utilizing string similarity, topic modeling, hierarchical clustering, and rule-based pipelines, while introducing evaluation metrics based on precision and recall.

Result: Testing on real-life datasets shows improved performance compared to baseline rule-based methods, achieving better interpretability and cluster refinement.

Conclusion: The approach significantly enhances clustering efficacy, minimizes manual review, and improves risk management in specific applications like sanctions investigations.

Abstract: Short text clustering is a known use case in the text analytics community.
When the structure and content falls in the natural language domain e.g.
Twitter posts or instant messages, then natural language techniques can be
used, provided texts are of sufficient length to allow for use of (pre)trained
models to extract meaningful information, such as part-of-speech or topic
annotations. However, natural language models are not suitable for clustering
transaction counterparties, as they are found in bank payment messaging
systems, such as SWIFT. The manually typed tags are typically physical or legal
entity details, which lack sentence structure, while containing all the
variations and noise that manual entry introduces. This leaves a gap in an
investigator or counter-fraud professional's toolset when looking to augment
their knowledge of payment flow originator and beneficiary entities and trace
funds and assets. A gap that vendors traditionally try to close with fuzzy
matching tools. With these considerations in mind, we are proposing a hybrid
string similarity, topic modelling, hierarchical clustering and rule-based
pipeline to facilitate clustering of transaction counterparties, also catering
for unknown number of expected clusters. We are also devising metrics to
supplement the evaluation of the approach, based on the well-known measures of
precision and recall. Testing on a real-life labelled dataset demonstrates
significantly improved performance over a baseline rule-based ('keyword')
approach. The approach retains most of the interpretability found in rule-based
systems, as the former adds an additional level of cluster refinement to the
latter. The resulting workflow reduces the need for manual review. When only a
subset of the population needs to be investigated, such as in sanctions
investigations, the approach allows for better control of the risks of missing
entity variations.

</details>


### [70] [Neural Network Acceleration on MPSoC board: Integrating SLAC's SNL, Rogue Software and Auto-SNL](https://arxiv.org/abs/2508.21739)
*Hamza Ezzaoui Rahali,Abhilasha Dave,Larry Ruckman,Mohammad Mehdi Rahimifar,Audrey C. Therrien,James J. Russel,Ryan T. Herbst*

Main category: cs.LG

TL;DR: The paper introduces SNL, a framework for deploying real-time machine learning models on FPGAs for high-throughput experiments, along with Auto-SNL, a Python extension for model conversion, with benchmarks showing competitive latency and resource savings.


<details>
  <summary>Details</summary>
Motivation: The motivation for this work arises from the challenges of managing the massive data throughput (over 1 TB/s) produced by the LCLS-II FEL facility. Traditional methods for data storage and transmission are too expensive, and conventional machine learning solutions introduce unacceptable latency for real-time, high-speed environments.

Method: The authors developed the SLAC Neural Network Library (SNL) for real-time ML inference on FPGAs, allowing dynamic updates of model weights without requiring FPGA resynthesis. They also introduced Auto-SNL, a Python-based framework to simplify the process of converting neural network models into SNL-compatible high-level synthesis code. Benchmarks were conducted against hls4ml to evaluate performance on a Xilinx ZCU102 FPGA.

Result: SNL demonstrated competitive or superior latency compared to hls4ml across various neural network architectures, precision configurations, and synthesis settings. In addition, in certain cases, SNL offered FPGA resource savings.

Conclusion: SNL's ability to dynamically update weights and its competitive performance make it suitable for high-speed experimental environments. Its versatility opens the door for applications in diverse fields like high-energy physics, medical imaging, and robotics.

Abstract: The LCLS-II Free Electron Laser (FEL) will generate X-ray pulses for beamline
experiments at rates of up to 1~MHz, with detectors producing data throughputs
exceeding 1 TB/s. Managing such massive data streams presents significant
challenges, as transmission and storage infrastructures become prohibitively
expensive. Machine learning (ML) offers a promising solution for real-time data
reduction, but conventional implementations introduce excessive latency, making
them unsuitable for high-speed experimental environments. To address these
challenges, SLAC developed the SLAC Neural Network Library (SNL), a specialized
framework designed to deploy real-time ML inference models on
Field-Programmable Gate Arrays (FPGA). SNL's key feature is the ability to
dynamically update model weights without requiring FPGA resynthesis, enhancing
flexibility for adaptive learning applications. To further enhance usability
and accessibility, we introduce Auto-SNL, a Python extension that streamlines
the process of converting Python-based neural network models into
SNL-compatible high-level synthesis code. This paper presents a benchmark
comparison against hls4ml, the current state-of-the-art tool, across multiple
neural network architectures, fixed-point precisions, and synthesis
configurations targeting a Xilinx ZCU102 FPGA. The results showed that SNL
achieves competitive or superior latency in most tested architectures, while in
some cases also offering FPGA resource savings. This adaptation demonstrates
SNL's versatility, opening new opportunities for researchers and academics in
fields such as high-energy physics, medical imaging, robotics, and many more.

</details>


### [71] [Beyond Prediction: Reinforcement Learning as the Defining Leap in Healthcare AI](https://arxiv.org/abs/2508.21101)
*Dilruk Perera,Gousia Habib,Qianyi Xu,Daniel J. Tan,Kai He,Erik Cambria,Mengling Feng*

Main category: cs.LG

TL;DR: The paper discusses the transformative role of reinforcement learning (RL) in healthcare, focusing on its capability to actively decide interventions with long-term goals instead of merely predicting outcomes.


<details>
  <summary>Details</summary>
Motivation: Artificial intelligence in healthcare has primarily revolved around outcome prediction, but there is a need for systems that can actively make decisions considering long-term impact.

Method: The paper structures RL techniques by categorizing methods (model-based, model-free, offline approaches) and analyzing applications across healthcare domains like critical care, diagnostics, mental health, and robotic assistance.

Result: RL systems present transformative opportunities in healthcare, particularly through integrating multi-source signals and operating under constraints. Challenges like ethical deployment and reward design are critically analyzed.

Conclusion: RL signifies a shift in healthcare AI from prediction to agentive intelligence, offering significant potential but also requiring improvements in ethical alignment, deployment methodologies, and robust reward designs.

Abstract: Reinforcement learning (RL) marks a fundamental shift in how artificial
intelligence is applied in healthcare. Instead of merely predicting outcomes,
RL actively decides interventions with long term goals. Unlike traditional
models that operate on fixed associations, RL systems learn through trial,
feedback, and long-term reward optimization, introducing transformative
possibilities and new risks. From an information fusion lens, healthcare RL
typically integrates multi-source signals such as vitals, labs clinical notes,
imaging and device telemetry using temporal and decision-level mechanisms.
These systems can operate within centralized, federated, or edge architectures
to meet real-time clinical constraints, and naturally span data, features and
decision fusion levels. This survey explore RL's rise in healthcare as more
than a set of tools, rather a shift toward agentive intelligence in clinical
environments. We first structure the landscape of RL techniques including
model-based and model-free methods, offline and batch-constrained approaches,
and emerging strategies for reward specification and uncertainty calibration
through the lens of healthcare constraints. We then comprehensively analyze RL
applications spanning critical care, chronic disease, mental health,
diagnostics, and robotic assistance, identifying their trends, gaps, and
translational bottlenecks. In contrast to prior reviews, we critically analyze
RL's ethical, deployment, and reward design challenges, and synthesize lessons
for safe, human-aligned policy learning. This paper serves as both a a
technical roadmap and a critical reflection of RL's emerging transformative
role in healthcare AI not as prediction machinery, but as agentive clinical
intelligence.

</details>


### [72] [Spatiotemporal EEG-Based Emotion Recognition Using SAM Ratings from Serious Games with Hybrid Deep Learning](https://arxiv.org/abs/2508.21103)
*Abdul Rehman,Ilona Heldal,Jerry Chun-Wei Lin*

Main category: cs.LG

TL;DR: This paper introduces an EEG emotion classification framework that uses the GAMEEMO dataset for multigranularity classification, achieving high accuracy with LSTM-GRU models.


<details>
  <summary>Details</summary>
Motivation: Existing EEG-based emotion recognition studies focus on binary valence prediction or subject-specific classification, limiting real-world applications.

Method: A unified EEG emotion recognition framework with preprocessing (e.g., feature extraction and normalization) and multi-axis classification (binary valence, multi-class, and fine-grained multi-label) is applied. Several machine learning and deep learning architectures are evaluated.

Result: The LSTM-GRU consistently achieves the best performance with an F1-score of 0.932 for binary valence, 94.5% for multi-class, and 90.6% for multi-label classifications.

Conclusion: The proposed framework demonstrates high generalizability and efficiency, providing robust emotion recognition for real-world affective computing systems.

Abstract: Recent advancements in EEG-based emotion recognition have shown promising
outcomes using both deep learning and classical machine learning approaches;
however, most existing studies focus narrowly on binary valence prediction or
subject-specific classification, which limits generalizability and deployment
in real-world affective computing systems. To address this gap, this paper
presents a unified, multigranularity EEG emotion classification framework built
on the GAMEEMO dataset, which consists of 14-channel EEG recordings and
continuous self-reported emotion ratings (boring, horrible, calm, and funny)
from 28 subjects across four emotion-inducing gameplay scenarios. Our pipeline
employs a structured preprocessing strategy that comprises temporal window
segmentation, hybrid statistical and frequency-domain feature extraction, and
z-score normalization to convert raw EEG signals into robust, discriminative
input vectors. Emotion labels are derived and encoded across three
complementary axes: (i) binary valence classification based on the averaged
polarity of positive and negative emotion ratings, and (ii) Multi-class emotion
classification, where the presence of the most affective state is predicted.
(iii) Fine-grained multi-label representation via binning each emotion into 10
ordinal classes. We evaluate a broad spectrum of models, including Random
Forest, XGBoost, and SVM, alongside deep neural architectures such as LSTM,
LSTM-GRU, and CNN-LSTM. Among these, the LSTM-GRU model consistently
outperforms the others, achieving an F1-score of 0.932 in the binary valence
task and 94.5% and 90.6% in both multi-class and Multi-Label emotion
classification.

</details>


### [73] [PVPO: Pre-Estimated Value-Based Policy Optimization for Agentic Reasoning](https://arxiv.org/abs/2508.21104)
*Wenfeng Feng,Penghong Zhao,Guochao Jiang,Chuzhan Hao,Yuewei Zhang,Hao Wang*

Main category: cs.LG

TL;DR: The paper introduces PVPO, a reinforcement learning method designed to reduce computational costs and avoid local optima by using an advantage reference anchor and data pre-sampling.


<details>
  <summary>Details</summary>
Motivation: Critic-free reinforcement learning methods often require multiple samplings and intra-policy comparisons, which can be computationally expensive and lead to suboptimal solutions.

Method: PVPO incorporates a reference model to calculate reward scores as anchors for advantage estimation. Additionally, data pre-sampling is used for selecting high-gain samples, reducing reliance on rollouts and correcting biases.

Result: Experiments on nine datasets across two domains show that PVPO achieves state-of-the-art performance, with robust task generalization and scalability across models of different sizes.

Conclusion: PVPO addresses inefficiencies in critic-free reinforcement learning by improving training efficiency and performance scalability, offering a promising solution for complex tasks.

Abstract: Critic-free reinforcement learning methods, particularly group policies, have
attracted considerable attention for their efficiency in complex tasks.
However, these methods rely heavily on multiple sampling and comparisons within
the policy to estimate advantage, which may cause the policy to fall into local
optimum and increase computational cost. To address these issues, we propose
PVPO, an efficient reinforcement learning method enhanced by an advantage
reference anchor and data pre-sampling. Specifically, we use the reference
model to rollout in advance and employ the calculated reward score as a
reference anchor. Our approach effectively corrects the cumulative bias
introduced by intra-group comparisons and significantly reduces reliance on the
number of rollouts. Meanwhile, the reference model can assess sample difficulty
during data pre-sampling, enabling effective selection of high-gain data to
improve training efficiency. Experiments conducted on nine datasets across two
domains demonstrate that PVPO achieves State-Of-The-Art (SOTA) performance. Our
approach not only demonstrates robust generalization across multiple tasks, but
also exhibits scalable performance across models of varying scales.

</details>


### [74] [Privacy Auditing Synthetic Data Release through Local Likelihood Attacks](https://arxiv.org/abs/2508.21146)
*Joshua Ward,Chi-Hua Wang,Guang Cheng*

Main category: cs.LG

TL;DR: The paper introduces Gen-LRA, a novel Membership Inference Attack (MIA) aimed at auditing privacy leakage in synthetic data, outperforming existing methods across diverse setups.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the unresolved problem of auditing privacy leakage in synthetic data and overcoming limitations in existing frameworks reliant on heuristics and unreasonable assumptions.

Method: The authors propose Gen-LRA, a No-Box Membership Inference Attack that does not require model knowledge or access, using a surrogate model and local likelihood estimations over synthetic data.

Result: Gen-LRA consistently outperforms existing MIAs across datasets, architectures, and attack parameters, demonstrating its efficacy as a privacy auditing tool.

Conclusion: Gen-LRA highlights significant privacy risks in generative model overfitting, offering a robust solution for auditing synthetic data privacy in real-world scenarios.

Abstract: Auditing the privacy leakage of synthetic data is an important but unresolved
problem. Most existing privacy auditing frameworks for synthetic data rely on
heuristics and unreasonable assumptions to attack the failure modes of
generative models, exhibiting limited capability to describe and detect the
privacy exposure of training data through synthetic data release. In this
paper, we study designing Membership Inference Attacks (MIAs) that specifically
exploit the observation that tabular generative models tend to significantly
overfit to certain regions of the training distribution. Here, we propose
Generative Likelihood Ratio Attack (Gen-LRA), a novel, computationally
efficient No-Box MIA that, with no assumption of model knowledge or access,
formulates its attack by evaluating the influence a test observation has in a
surrogate model's estimation of a local likelihood ratio over the synthetic
data. Assessed over a comprehensive benchmark spanning diverse datasets, model
architectures, and attack parameters, we find that Gen-LRA consistently
dominates other MIAs for generative models across multiple performance metrics.
These results underscore Gen-LRA's effectiveness as a privacy auditing tool for
the release of synthetic data, highlighting the significant privacy risks posed
by generative model overfitting in real-world applications.

</details>


### [75] [Dynamic Low-rank Approximation of Full-Matrix Preconditioner for Training Generalized Linear Models](https://arxiv.org/abs/2508.21106)
*Tatyana Matveeva,Aleksandr Katrutsa,Evgeny Frolov*

Main category: cs.LG

TL;DR: Adaptive gradient methods struggle with capturing parameter correlations due to diagonal preconditioning. AdaGram proposes a scalable solution for full-matrix adaptive updates by using low-rank approximations and fast symmetrizations.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of diagonal preconditioning in adaptive gradient methods, which fails to model parameter correlations effectively, particularly in large-scale optimization problems.

Method: Proposes AdaGram, an optimizer utilizing low-rank matrix approximation and symmetric factorization for efficient and scalable full-matrix adaptive gradient updates.

Result: AdaGram achieves faster or comparable convergence to diagonal adaptive optimizers in machine learning tasks while maintaining scalability using low-rank approximations.

Conclusion: AdaGram presents a viable and efficient solution to the computational and memory challenges in full-matrix adaptive optimization, showing potential for large-scale applications.

Abstract: Adaptive gradient methods like Adagrad and its variants are widespread in
large-scale optimization. However, their use of diagonal preconditioning
matrices limits the ability to capture parameter correlations. Full-matrix
adaptive methods, approximating the exact Hessian, can model these correlations
and may enable faster convergence. At the same time, their computational and
memory costs are often prohibitive for large-scale models. To address this
limitation, we propose AdaGram, an optimizer that enables efficient full-matrix
adaptive gradient updates. To reduce memory and computational overhead, we
utilize fast symmetric factorization for computing the preconditioned update
direction at each iteration. Additionally, we maintain the low-rank structure
of a preconditioner along the optimization trajectory using matrix integrator
methods. Numerical experiments on standard machine learning tasks show that
AdaGram converges faster or matches the performance of diagonal adaptive
optimizers when using rank five and smaller rank approximations. This
demonstrates AdaGram's potential as a scalable solution for adaptive
optimization in large models.

</details>


### [76] [An Explainable, Attention-Enhanced, Bidirectional Long Short-Term Memory Neural Network for Joint 48-Hour Forecasting of Temperature, Irradiance, and Relative Humidity](https://arxiv.org/abs/2508.21109)
*Georgios Vamvouras,Konstantinos Braimakis,Christos Tzivanidis*

Main category: cs.LG

TL;DR: The paper introduces a BiLSTM-based DL framework for accurate 48-hour forecasts of weather parameters to improve smart HVAC systems' efficiency.


<details>
  <summary>Details</summary>
Motivation: The rising need for reliable and accurate short-term meteorological data for energy-efficient building controls prompted the study.

Method: A stacked Bidirectional LSTM network with attention layers was used to forecast temperature, solar irradiance, and humidity based on historical data, leveraging explainability tools like Integrated Gradients.

Result: The model achieved superior prediction accuracy (1.3°C, 31 W/m², 6.7% errors) compared to state-of-the-art weather prediction models.

Conclusion: This approach advances weather forecasting by improving accuracy, interpretability, and relevance for smart energy management systems.

Abstract: This paper presents a Deep Learning (DL) framework for 48-hour forecasting of
temperature, solar irradiance, and relative humidity to support Model
Predictive Control (MPC) in smart HVAC systems. The approach employs a stacked
Bidirectional Long Short-Term Memory (BiLSTM) network with attention, capturing
temporal and cross-feature dependencies by jointly predicting all three
variables. Historical meteorological data (2019-2022) with encoded cyclical
time features were used for training, while 2023 data evaluated generalization.
The model achieved Mean Absolute Errors of 1.3 degrees Celsius (temperature),
31 W/m2 (irradiance), and 6.7 percentage points (humidity), outperforming
state-of-the-art numerical weather prediction and machine learning benchmarks.
Integrated Gradients quantified feature contributions, and attention weights
revealed temporal patterns, enhancing interpretability. By combining
multivariate forecasting, attention-based DL, and explainability, this work
advances data-driven weather prediction. The demonstrated accuracy and
transparency highlight the framework's potential for energy-efficient building
control through reliable short-term meteorological forecasting.

</details>


### [77] [Automating the Deep Space Network Data Systems; A Case Study in Adaptive Anomaly Detection through Agentic AI](https://arxiv.org/abs/2508.21111)
*Evan J. Chou,Lisa S. Locke,Harvey M. Soldan*

Main category: cs.LG

TL;DR: The paper introduces a system using machine learning, reinforcement learning, and an AI agent to detect and classify anomalies in NASA's Deep Space Network (DSN) antenna and transmitter equipment, enabling efficient maintenance and real-time monitoring.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the need to address equipment degradation in NASA's Deep Space Network (DSN), which can disrupt communications with space missions. Early and precise anomaly detection is vital to maintain uninterrupted data flows and mission support.

Method: The approach includes using machine learning to reconstruct multivariate time-series data and detect anomalies, reinforcement learning to classify anomalies by severity level, and a large language model to provide explanations for anomalies. A complete automated data pipeline for extraction, processing, and analysis was implemented, connecting all these techniques.

Result: The system successfully integrates predictive analysis models with anomaly detection and classification, creating a coherent workflow for anomaly detection in DSN equipment. It enables real-time monitoring and enhances the maintenance process with its agentic AI framework.

Conclusion: This study showcases a robust, interconnected system for anomaly detection within DSN, built using advanced AI and machine learning techniques. It ensures improved operational efficiency, supports space missions, and can be enhanced over time with human feedback.

Abstract: The Deep Space Network (DSN) is NASA's largest network of antenna facilities
that generate a large volume of multivariate time-series data. These facilities
contain DSN antennas and transmitters that undergo degradation over long
periods of time, which may cause costly disruptions to the data flow and
threaten the earth-connection of dozens of spacecraft that rely on the Deep
Space Network for their lifeline. The purpose of this study was to experiment
with different methods that would be able to assist JPL engineers with directly
pinpointing anomalies and equipment degradation through collected data, and
continue conducting maintenance and operations of the DSN for future space
missions around our universe. As such, we have researched various machine
learning techniques that can fully reconstruct data through predictive
analysis, and determine anomalous data entries within real-time datasets
through statistical computations and thresholds. On top of the fully trained
and tested machine learning models, we have also integrated the use of a
reinforcement learning subsystem that classifies identified anomalies based on
severity level and a Large Language Model that labels an explanation for each
anomalous data entry, all of which can be improved and fine-tuned over time
through human feedback/input. Specifically, for the DSN transmitters, we have
also implemented a full data pipeline system that connects the data extraction,
parsing, and processing workflow all together as there was no coherent program
or script for performing these tasks before. Using this data pipeline system,
we were able to then also connect the models trained from DSN antenna data,
completing the data workflow for DSN anomaly detection. This was all wrapped
around and further connected by an agentic AI system, where complex reasoning
was utilized to determine the classifications and predictions of anomalous
data.

</details>


### [78] [Convergence of Stochastic Gradient Methods for Wide Two-Layer Physics-Informed Neural Networks](https://arxiv.org/abs/2508.21571)
*Bangti Jin,Longjun Wu*

Main category: cs.LG

TL;DR: This paper proves the linear convergence of stochastic gradient descent (SGD) when training over-parameterized two-layer Physics Informed Neural Networks (PINNs) with various activation functions.


<details>
  <summary>Details</summary>
Motivation: To ensure the reliability and effectiveness of PINNs trained with stochastic gradient descent methods, which are widely used but lack formal guarantees of convergence.

Method: The authors extend existing theoretical analysis from gradient descent to stochastic gradient descent, addressing the challenge of dynamic randomness by ensuring the positive definiteness of Gram matrices during training.

Result: The paper proves, with high probability, that over-parameterized two-layer PINNs achieve linear convergence under stochastic optimization methods.

Conclusion: These findings justify the practical use of SGD in PINNs while offering theoretical insights into the optimization dynamics.

Abstract: Physics informed neural networks (PINNs) represent a very popular class of
neural solvers for partial differential equations. In practice, one often
employs stochastic gradient descent type algorithms to train the neural
network. Therefore, the convergence guarantee of stochastic gradient descent is
of fundamental importance. In this work, we establish the linear convergence of
stochastic gradient descent / flow in training over-parameterized two layer
PINNs for a general class of activation functions in the sense of high
probability. These results extend the existing result [18] in which gradient
descent was analyzed. The challenge of the analysis lies in handling the
dynamic randomness introduced by stochastic optimization methods. The key of
the analysis lies in ensuring the positive definiteness of suitable Gram
matrices during the training. The analysis sheds insight into the dynamics of
the optimization process, and provides guarantees on the neural networks
trained by stochastic algorithms.

</details>


### [79] [Adaptive LLM Routing under Budget Constraints](https://arxiv.org/abs/2508.21141)
*Pranoy Panda,Raghav Magazine,Chaitanya Devaguptapu,Sho Takemori,Vishal Sharma*

Main category: cs.LG

TL;DR: This paper proposes a method to route queries dynamically among large language models (LLMs) by treating this as a contextual bandit problem. The method uses shared query-LLM embeddings and adapts to real-world scenarios without exhaustive pairings.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the inefficiency and impracticality of statically pairing user queries with LLMs, especially when comprehensive mappings or exhaustive inferences are unavailable.

Method: The authors propose PILOT, an algorithm leveraging a shared embedding space aligned from offline preference data and refined with online bandit feedback. It incorporates a knapsack-based cost policy to manage diverse user budgets.

Result: The method enables adaptive and resource-efficient query routing by selecting suitable LLMs dynamically based on user feedback and constraints.

Conclusion: This work introduces an effective, adaptive, and cost-aware LLM routing system suitable for dynamic real-world conditions.

Abstract: Large Language Models (LLMs) have revolutionized natural language processing,
but their varying capabilities and costs pose challenges in practical
applications. LLM routing addresses this by dynamically selecting the most
suitable LLM for each query/task. Previous approaches treat this as a
supervised learning problem, assuming complete knowledge of optimal query-LLM
pairings. However, real-world scenarios lack such comprehensive mappings and
face evolving user queries. We thus propose to study LLM routing as a
contextual bandit problem, enabling adaptive decision-making using bandit
feedback without requiring exhaustive inference across all LLMs for all queries
(in contrast to supervised routing). To address this problem, we develop a
shared embedding space for queries and LLMs, where query and LLM embeddings are
aligned to reflect their affinity. This space is initially learned from offline
human preference data and refined through online bandit feedback. We
instantiate this idea through Preference-prior Informed Linucb fOr adaptive
rouTing (PILOT), a novel extension of LinUCB. To handle diverse user budgets
for model routing, we introduce an online cost policy modeled as a multi-choice
knapsack problem, ensuring resource-efficient routing.

</details>


### [80] [Deep Residual Echo State Networks: exploring residual orthogonal connections in untrained Recurrent Neural Networks](https://arxiv.org/abs/2508.21172)
*Matteo Pinna,Andrea Ceni,Claudio Gallicchio*

Main category: cs.LG

TL;DR: The paper introduces Deep Residual Echo State Networks (DeepResESNs), a novel class of deep untrained RNNs using temporal residual connections, which enhance memory capacity and temporal modeling.


<details>
  <summary>Details</summary>
Motivation: Echo State Networks (ESNs) are efficient for learning but struggle with processing long-term information, prompting the need for improved architectures.

Method: The authors propose DeepResESNs, featuring a hierarchy of residual recurrent layers with various orthogonal configurations, combined with a mathematical analysis to ensure stable dynamics.

Result: Experiments on time series tasks demonstrate superior performance of DeepResESNs over shallow and traditional deep Reservoir Computing frameworks.

Conclusion: DeepResESNs effectively address the limitations of traditional ESNs by leveraging residual connections, improving memory retention and long-term temporal modeling for time series challenges.

Abstract: Echo State Networks (ESNs) are a particular type of untrained Recurrent
Neural Networks (RNNs) within the Reservoir Computing (RC) framework, popular
for their fast and efficient learning. However, traditional ESNs often struggle
with long-term information processing. In this paper, we introduce a novel
class of deep untrained RNNs based on temporal residual connections, called
Deep Residual Echo State Networks (DeepResESNs). We show that leveraging a
hierarchy of untrained residual recurrent layers significantly boosts memory
capacity and long-term temporal modeling. For the temporal residual
connections, we consider different orthogonal configurations, including
randomly generated and fixed-structure configurations, and we study their
effect on network dynamics. A thorough mathematical analysis outlines necessary
and sufficient conditions to ensure stable dynamics within DeepResESN. Our
experiments on a variety of time series tasks showcase the advantages of the
proposed approach over traditional shallow and deep RC.

</details>


### [81] [FUTURE: Flexible Unlearning for Tree Ensemble](https://arxiv.org/abs/2508.21181)
*Ziheng Chen,Jin Huang,Jiali Cheng,Yuchan Guo,Mengjie Wang,Lalitesh Morishetti,Kaushiki Nag,Hadi Amiri*

Main category: cs.LG

TL;DR: The paper introduces FUTURE, a novel unlearning algorithm for tree ensembles aimed at improving efficiency and generalization in forgetting sensitive data.


<details>
  <summary>Details</summary>
Motivation: Tree ensembles excel at classification tasks but lack efficient and generalizable unlearning methods, necessary for fulfilling data privacy requirements like the right to be forgotten.

Method: The authors propose and implement FUTURE, a gradient-based optimization framework that utilizes probabilistic model approximations to circumvent the non-differentiability of tree ensembles.

Result: Experiments on real-world datasets show FUTURE performs effectively and efficiently in unlearning sensitive information from tree ensembles.

Conclusion: FUTURE enhances the process of unlearning in tree ensembles, offering a generalized, scalable, and privacy-focused solution for diverse applications.

Abstract: Tree ensembles are widely recognized for their effectiveness in
classification tasks, achieving state-of-the-art performance across diverse
domains, including bioinformatics, finance, and medical diagnosis. With
increasing emphasis on data privacy and the \textit{right to be forgotten},
several unlearning algorithms have been proposed to enable tree ensembles to
forget sensitive information. However, existing methods are often tailored to a
particular model or rely on the discrete tree structure, making them difficult
to generalize to complex ensembles and inefficient for large-scale datasets. To
address these limitations, we propose FUTURE, a novel unlearning algorithm for
tree ensembles. Specifically, we formulate the problem of forgetting samples as
a gradient-based optimization task. In order to accommodate
non-differentiability of tree ensembles, we adopt the probabilistic model
approximations within the optimization framework. This enables end-to-end
unlearning in an effective and efficient manner. Extensive experiments on
real-world datasets show that FUTURE yields significant and successful
unlearning performance.

</details>


### [82] [Manifold Trajectories in Next-Token Prediction: From Replicator Dynamics to Softmax Equilibrium](https://arxiv.org/abs/2508.21186)
*Christopher R. Lee-Jenkins*

Main category: cs.LG

TL;DR: The paper formalizes softmax decoding in language models as a variational principle on probability simplex, connecting discrete updates to smooth trajectories and exploring practical implications for sampling techniques.


<details>
  <summary>Details</summary>
Motivation: To provide a formalized understanding of softmax decoding in large language models and its behavior under constraints and sampling techniques.

Method: Analyzing decoding as a variational principle, linking discrete multiplicative-weight updates to continuous replicator flows, and examining trajectory dynamics and sampling constraints.

Result: The next-token probability distribution follows smooth trajectories and converges to softmax equilibrium, with practical insights on temperature scaling, sampling constraints, and path-dependent behaviors.

Conclusion: The study formalizes manifold traversal intuition in next-token distributions and offers insights into decoding, but excludes training dynamics or internal representation exploration.

Abstract: Decoding in large language models is often described as scoring tokens and
normalizing with softmax. We give a minimal, self-contained account of this
step as a constrained variational principle on the probability simplex. The
discrete, normalization-respecting ascent is the classical
multiplicative-weights (entropic mirror) update; its continuous-time limit is
the replicator flow. From these ingredients we prove that, for a fixed context
and temperature, the next-token distribution follows a smooth trajectory inside
the simplex and converges to the softmax equilibrium. This formalizes the
common ``manifold traversal'' intuition at the output-distribution level. The
analysis yields precise, practice-facing consequences: temperature acts as an
exact rescaling of time along the same trajectory, while top-k and nucleus
sampling restrict the flow to a face with identical guarantees. We also outline
a controlled account of path-dependent score adjustments and their connection
to loop-like, hallucination-style behavior. We make no claims about training
dynamics or internal representations; those are deferred to future work.

</details>


### [83] [Model-Task Alignment Drives Distinct RL Outcomes](https://arxiv.org/abs/2508.21188)
*Haoze Wu,Cheng Wang,Wenshuo Zhao,Junxian He*

Main category: cs.LG

TL;DR: This paper studies counterintuitive observations in reinforcement learning (RL) applications to large language models (LLMs) and finds these arise based on pre-trained model-task alignment.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand why counterintuitive claims in RL applied to LLMs arise and under what conditions they succeed or fail.

Method: The paper conducts a systematic experimental analysis of different RL observations across model architectures and task domains, focusing on model-task alignment measured via pass@k accuracy.

Result: It finds that counterintuitive results, such as the effectiveness of a single training example or negative samples, only hold when the LLM already has strong model-task alignment. In difficult scenarios, standard RL methods perform better.

Conclusion: Counterintuitive RL phenomena in LLMs are not universally applicable and depend on task alignment with pre-trained models. Standard RL is more reliable in challenging tasks.

Abstract: Recent advances in applying reinforcement learning (RL) to large language
models (LLMs) have led to substantial progress. In particular, a series of
remarkable yet often counterintuitive phenomena have been reported in LLMs,
exhibiting patterns not typically observed in traditional RL settings. For
example, notable claims include that a single training example can match the
performance achieved with an entire dataset, that the reward signal does not
need to be very accurate, and that training solely with negative samples can
match or even surpass sophisticated reward-based methods. However, the precise
conditions under which these observations hold - and, critically, when they
fail - remain unclear. In this work, we identify a key factor that
differentiates RL observations: whether the pretrained model already exhibits
strong Model-Task Alignment, as measured by pass@k accuracy on the evaluated
task. Through a systematic and comprehensive examination of a series of
counterintuitive claims, supported by rigorous experimental validation across
different model architectures and task domains, our findings show that while
standard RL training remains consistently robust across settings, many of these
counterintuitive results arise only when the model and task already exhibit
strong model-task alignment. In contrast, these techniques fail to drive
substantial learning in more challenging regimes, where standard RL methods
remain effective.

</details>


### [84] [Class Incremental Continual Learning with Self-Organizing Maps and Variational Autoencoders Using Synthetic Replay](https://arxiv.org/abs/2508.21240)
*Pujan Thapa,Alexander Ororbia,Travis Desell*

Main category: cs.LG

TL;DR: The paper presents a generative continual learning approach using self-organizing maps (SOMs) and variational autoencoders (VAEs) for memory-efficient replay without storing raw data or task labels.


<details>
  <summary>Details</summary>
Motivation: To address challenges in continual learning, such as memory inefficiency and the dependency on storing raw data or task-specific labels, by proposing an alternative generative replay method.

Method: The method uses SOMs and VAEs, with SOMs operating in the latent space for high-dimensional inputs and standalone for lower-dimensional inputs. It uses statistical properties (mean, variance, covariance) of SOM units to generate synthetic samples for replay.

Result: On class-incremental benchmarks like CIFAR-10 and CIFAR-100, the method achieves up to 10% and 7% improvement over state-of-the-art methods while being memory-efficient.

Conclusion: The approach offers a scalable, memory-efficient solution for continual learning, with additional capabilities such as easy learning visualization and post-training generative applications.

Abstract: This work introduces a novel generative continual learning framework based on
self-organizing maps (SOMs) and variational autoencoders (VAEs) to enable
memory-efficient replay, eliminating the need to store raw data samples or task
labels. For high-dimensional input spaces, such as of CIFAR-10 and CIFAR-100,
we design a scheme where the SOM operates over the latent space learned by a
VAE, whereas, for lower-dimensional inputs, such as those found in MNIST and
FashionMNIST, the SOM operates in a standalone fashion. Our method stores a
running mean, variance, and covariance for each SOM unit, from which synthetic
samples are then generated during future learning iterations. For the VAE-based
method, generated samples are then fed through the decoder to then be used in
subsequent replay. Experimental results on standard class-incremental
benchmarks show that our approach performs competitively with state-of-the-art
memory-based methods and outperforms memory-free methods, notably improving
over best state-of-the-art single class incremental performance on CIFAR-10 and
CIFAR-100 by nearly $10$\% and $7$\%, respectively. Our methodology further
facilitates easy visualization of the learning process and can also be utilized
as a generative model post-training. Results show our method's capability as a
scalable, task-label-free, and memory-efficient solution for continual
learning.

</details>


### [85] [A Mixture of Experts Gating Network for Enhanced Surrogate Modeling in External Aerodynamics](https://arxiv.org/abs/2508.21249)
*Mohammad Amin Nabian,Sanjay Choudhry*

Main category: cs.LG

TL;DR: This paper introduces an MoE model for CFD simulation predictions, combining three advanced neural network architectures to improve accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: To address the bottleneck of high computational costs in CFD simulations for automotive aerodynamics.

Method: The study employs a Mixture of Experts (MoE) framework that uses a gating network for dynamic prediction weighting across three specialized surrogate models: DoMINO, X-MeshGraphNet, and FigConvNet.

Result: The MoE model significantly reduces prediction error and outperforms individual models on a benchmark dataset of CFD simulations.

Conclusion: The MoE framework offers a robust approach by leveraging diverse neural network strengths for improved aerodynamic predictions and establishes itself as a superior composite surrogate modeling strategy.

Abstract: The computational cost associated with high-fidelity CFD simulations remains
a significant bottleneck in the automotive design and optimization cycle. While
ML-based surrogate models have emerged as a promising alternative to accelerate
aerodynamic predictions, the field is characterized by a diverse and rapidly
evolving landscape of specialized neural network architectures, with no single
model demonstrating universal superiority. This paper introduces a novel
meta-learning framework that leverages this architectural diversity as a
strength. We propose a Mixture of Experts (MoE) model that employs a dedicated
gating network to dynamically and optimally combine the predictions from three
heterogeneous, state-of-the-art surrogate models: DoMINO, a decomposable
multi-scale neural operator; X-MeshGraphNet, a scalable multi-scale graph
neural network; and FigConvNet, a factorized implicit global convolution
network. The gating network learns a spatially-variant weighting strategy,
assigning credibility to each expert based on its localized performance in
predicting surface pressure and wall shear stress fields. To prevent model
collapse and encourage balanced expert contributions, we integrate an entropy
regularization term into the training loss function. The entire system is
trained and validated on the DrivAerML dataset, a large-scale, public benchmark
of high-fidelity CFD simulations for automotive aerodynamics. Quantitative
results demonstrate that the MoE model achieves a significant reduction in L-2
prediction error, outperforming not only the ensemble average but also the most
accurate individual expert model across all evaluated physical quantities. This
work establishes the MoE framework as a powerful and effective strategy for
creating more robust and accurate composite surrogate models by synergistically
combining the complementary strengths of specialized architectures.

</details>


### [86] [RelP: Faithful and Efficient Circuit Discovery via Relevance Patching](https://arxiv.org/abs/2508.21258)
*Farnoush Rezaei Jafari,Oliver Eberle,Ashkan Khakzar,Neel Nanda*

Main category: cs.LG

TL;DR: This paper introduces Relevance Patching (RelP), a faster and more reliable method for localizing model components responsible for specific behaviors, as a substitute for computationally expensive activation patching.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the computational inefficiencies of activation patching and the noise and unreliability associated with attribution patching in deep learning model interpretability.

Method: The researchers propose RelP, which uses Layer-wise Relevance Propagation (LRP) to compute propagation coefficients that redistribute the network's output relevance backwards with local rules, maintaining computational efficiency with only two forward passes and one backward pass.

Result: RelP shows a strong correlation with activation patching (e.g., achieving a Pearson correlation of 0.956 compared to 0.006 for attribution patching in GPT-2 Large). It is validated across diverse models and tasks, including improvements on the IOI task.

Conclusion: RelP is an efficient and faithful alternative to standard attribution patching, combining accuracy with computational efficiency and outperforming existing methods.

Abstract: Activation patching is a standard method in mechanistic interpretability for
localizing the components of a model responsible for specific behaviors, but it
is computationally expensive to apply at scale. Attribution patching offers a
faster, gradient-based approximation, yet suffers from noise and reduced
reliability in deep, highly non-linear networks. In this work, we introduce
Relevance Patching (RelP), which replaces the local gradients in attribution
patching with propagation coefficients derived from Layer-wise Relevance
Propagation (LRP). LRP propagates the network's output backward through the
layers, redistributing relevance to lower-level components according to local
propagation rules that ensure properties such as relevance conservation or
improved signal-to-noise ratio. Like attribution patching, RelP requires only
two forward passes and one backward pass, maintaining computational efficiency
while improving faithfulness. We validate RelP across a range of models and
tasks, showing that it more accurately approximates activation patching than
standard attribution patching, particularly when analyzing residual stream and
MLP outputs in the Indirect Object Identification (IOI) task. For instance, for
MLP outputs in GPT-2 Large, attribution patching achieves a Pearson correlation
of 0.006, whereas RelP reaches 0.956, highlighting the improvement offered by
RelP. Additionally, we compare the faithfulness of sparse feature circuits
identified by RelP and Integrated Gradients (IG), showing that RelP achieves
comparable faithfulness without the extra computational cost associated with
IG.

</details>


### [87] [Owen Sampling Accelerates Contribution Estimation in Federated Learning](https://arxiv.org/abs/2508.21261)
*Hossein KhademSohi,Hadi Hemmati,Jiayu Zhou,Steve Drew*

Main category: cs.LG

TL;DR: FedOwen is proposed to approximate Shapley values efficiently in federated learning, improving model accuracy and convergence.


<details>
  <summary>Details</summary>
Motivation: In federated learning, assessing client contributions accurately is crucial for fair rewards and efficient global model convergence. Computing Shapley values exactly is infeasible for large federations.

Method: FedOwen uses Owen sampling for Shapley value approximation while employing an adaptive client selection strategy to mitigate bias and enhance data utilization.

Result: FedOwen achieves up to 23% higher accuracy with the same cost and number of communication rounds compared to existing methods on non-IID benchmarks.

Conclusion: FedOwen effectively balances computational efficiency and accuracy in federated learning by improving client contribution estimations and faster global model convergence.

Abstract: Federated Learning (FL) aggregates information from multiple clients to train
a shared global model without exposing raw data. Accurately estimating each
client's contribution is essential not just for fair rewards, but for selecting
the most useful clients so the global model converges faster. The Shapley value
is a principled choice, yet exact computation scales exponentially with the
number of clients, making it infeasible for large federations. We propose
FedOwen, an efficient framework that uses Owen sampling to approximate Shapley
values under the same total evaluation budget as existing methods while keeping
the approximation error small. In addition, FedOwen uses an adaptive client
selection strategy that balances exploiting high-value clients with exploring
under-sampled ones, reducing bias and uncovering rare but informative data.
Under a fixed valuation cost, FedOwen achieves up to 23 percent higher final
accuracy within the same number of communication rounds compared to
state-of-the-art baselines on non-IID benchmarks.

</details>


### [88] [Guess-and-Learn (G&L): Measuring the Cumulative Error Cost of Cold-Start Adaptation](https://arxiv.org/abs/2508.21270)
*Roland Arnold*

Main category: cs.LG

TL;DR: The paper introduces "Guess-and-Learn (G&L) v1.0," a framework to evaluate machine learning models based on their cold-start adaptability, i.e., measuring cumulative errors during early learning stages.


<details>
  <summary>Details</summary>
Motivation: The motivation is to provide a way to measure machine learning models' adaptability during initial learning phases, which is often overlooked in favor of endpoint accuracy metrics.

Method: The proposed framework quantifies adaptability by having models sequentially predict labels for unlabeled data, receiving feedback, and updating parameters in either online or batch mode. The process evaluates adaptation speed, selection quality, and bias through predefined tracks.

Result: Baseline experiments on MNIST and AG News show smaller models adapt with fewer errors initially, and pretraining offers domain-specific benefits. Current models still fall short of the estimated heuristic oracle band, revealing an adaptability gap.

Conclusion: G&L offers a new metric and reproducible framework, complementing traditional benchmarks for developing models that perform reliably even during early learning stages.

Abstract: Evaluation of machine learning models typically emphasizes final accuracy,
overlooking the cost of adaptation: the cumulative errors incurred while
learning from scratch. Guess-and- Learn (G&L) v1.0 addresses this gap by
measuring cold-start adaptability - the total mistakes a model makes while
sequentially labeling an unlabeled dataset. At each step, the learner selects
an instance, predicts its label, receives the ground truth, and updates
parameters under either online (per-sample) or batch (delayed) mode. The
resulting error trajectory exposes adaptation speed, selection quality, and
bias - dynamics invisible to endpoint metrics.
  G&L defines four tracks (Scratch/Pretrained $\times$ Online/Batch) to
disentangle the effects of initialization and update frequency. We formalize
the protocol, relate it to classical mistake-bound theory, and estimate a
heuristic "oracle reference band" for MNIST as a plausibility reference.
Baseline experiments on MNIST and AG News, spanning classical methods
(Perceptron, k-NN), convolutional architectures (CNN, ResNet-50), and
pretrained transformers (ViT-B/16, BERT-base), reveal systematic differences in
early-phase efficiency: smaller models can adapt with fewer initial errors,
while pretraining benefits vary by domain. Across settings, current models
remain well above the oracle band, highlighting an adaptability gap.
  By quantifying the mistake cost of early learning, G&L complements
conventional benchmarks and provides a reproducible framework for developing
learners that are not only accurate in the limit but also reliable from the
first examples.

</details>


### [89] [CALM: A Framework for Continuous, Adaptive, and LLM-Mediated Anomaly Detection in Time-Series Streams](https://arxiv.org/abs/2508.21273)
*Ashok Devireddy,Shunping Huang*

Main category: cs.LG

TL;DR: This paper proposes CALM, a framework for real-time anomaly detection in non-stationary time-series streams, addressing issues like concept drift using continuous model fine-tuning and semantic scoring with LLMs.


<details>
  <summary>Details</summary>
Motivation: Anomaly detection in time-series data is crucial across many domains, yet offline-trained models struggle with concept drift and changing statistical properties.

Method: CALM integrates real-time continuous model fine-tuning and semantic evaluations via an LLM to adapt to evolving patterns in dynamic data streams.

Result: The evaluation on the TSB-UAD benchmark shows improved ROC AUC scores for CALM's adaptive model compared to static models.

Conclusion: CALM demonstrates efficacy in handling concept drift, maintaining high performance for dynamic streaming anomaly detection.

Abstract: The detection of anomalies in non-stationary time-series streams is a
critical but challenging task across numerous industrial and scientific
domains. Traditional models, trained offline, suffer significant performance
degradation when faced with concept drift, where the underlying statistical
properties of the data change over time. This paper introduces CALM
(Continuous, Adaptive, and LLM-Mediated), a novel, end-to-end framework for
real-time anomaly detection designed to address this challenge. CALM is built
on the Apache Beam distributed processing framework and leverages the TimesFm
foundation model for forecasting-based anomaly detection. The framework's
novelty lies in two core contributions. First, it implements a closed-loop,
continuous fine-tuning mechanism that allows the anomaly detection model to
adapt to evolving data patterns in near real-time. Second, it introduces an
LLM-as-a-Judge component, a Large Language Model that provides semantic,
context-aware judgments on detected anomalies to curate a high-quality training
dataset, deciding whether an anomaly represents transient noise or a meaningful
pattern shift. We evaluate CALM on the comprehensive TSB-UAD benchmark. Our
results demonstrate that the continuously fine-tuned model improves the ROC AUC
score in most datasets compared to the static, pre-trained base model,
validating the efficacy of our adaptive, LLM-guided approach to maintaining
high-performance anomaly detection in dynamic streaming environments.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [90] [CrossTL: A Universal Programming Language Translator with Unified Intermediate Representation](https://arxiv.org/abs/2508.21256)
*Nripesh Niketan,Vaatsalya Shrivastva*

Main category: cs.PL

TL;DR: CrossTL is a universal programming translator using an intermediate representation (CrossGL) to enable bidirectional translation across multiple programming languages, avoiding exponential complexity of individual pairwise translators.


<details>
  <summary>Details</summary>
Motivation: Existing translation approaches require a translator for each language pair, leading to high complexity. The need for an efficient and scalable solution prompted the creation of a universal translator.

Method: CrossTL uses a unified intermediate representation, CrossGL. It employs language-specific lexers/parsers to generate ASTs, translation modules for CrossGL conversion, and backend systems for code generation. Modular architecture facilitates new language additions.

Result: The system was tested across various programming domains and languages like CUDA, OpenGL GLSL, and Rust, showcasing successful compilation and execution. Support for more languages, like Slang, is ongoing.

Conclusion: CrossTL demonstrates the practical feasibility of universal code translation, creating opportunities for language-agnostic programming and simplifying the process of expanding to new languages.

Abstract: We present CrossTL, a universal programming language translator enabling
bidirectional translation between multiple languages through a unified
intermediate representation called CrossGL. Traditional approaches require
separate translators for each language pair, leading to exponential complexity
growth. CrossTL uses a single universal IR to facilitate translations between
CUDA, HIP, Metal, DirectX HLSL, OpenGL GLSL, Vulkan SPIR-V, Rust, and Mojo,
with Slang support in development. Our system consists of: language-specific
lexers/parsers converting source code to ASTs, bidirectional CrossGL
translation modules implementing ToCrossGLConverter classes for importing code
and CodeGen classes for target generation, and comprehensive backend
implementations handling full translation pipelines. We demonstrate
effectiveness through comprehensive evaluation across programming domains,
achieving successful compilation and execution across all supported backends.
The universal IR design enables adding new languages with minimal effort,
requiring only language-specific frontend/backend components. Our contributions
include: (1) a unified IR capturing semantics of multiple programming
paradigms, (2) a modular architecture enabling extensibility, (3) a
comprehensive framework supporting GPU compute, graphics programming, and
systems languages, and (4) empirical validation demonstrating practical
viability of universal code translation. CrossTL represents a significant step
toward language-agnostic programming, enabling write-once, deploy-everywhere
development.

</details>


### [91] [Growing Mathlib: maintenance of a large scale mathematical library](https://arxiv.org/abs/2508.21593)
*Anne Baanen,Matthew Robert Ballard,Johan Commelin,Bryan Gin-ge Chen,Michael Rothgang,Damiano Testa*

Main category: cs.PL

TL;DR: This paper explores strategies for managing the rapid growth of the Lean mathematical library Mathlib, emphasizing tools and approaches to ensure scalability while avoiding maintainer overload.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address challenges arising due to the rapid expansion of formalized mathematical libraries and provide solutions to maintain the integrity, usability, and sustainability of Mathlib.

Method: The authors implemented strategies like a deprecation system, linting tools for immediate user feedback, optimizing compilation times through library redesign, resolving technical debt, and creating custom tools for contribution review and triage.

Result: The paper presents effective strategies for improving code quality, speeding up library compilation, managing changes, and handling new contributions, thereby supporting Mathlib's growth.

Conclusion: Proper management techniques and development tools can help maintain high standards and scalability in rapidly growing formalized mathematics libraries like Mathlib.

Abstract: The Lean mathematical library Mathlib is one of the fastest-growing libraries
of formalised mathematics. We describe various strategies to manage this
growth, while allowing for change and avoiding maintainer overload. This
includes dealing with breaking changes via a deprecation system, using code
quality analysis tools (linters) to provide direct user feedback about common
pitfalls, speeding up compilation times through conscious library (re-)design,
dealing with technical debt as well as writing custom tooling to help with the
review and triage of new contributions.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [92] [EmbodiedOneVision: Interleaved Vision-Text-Action Pretraining for General Robot Control](https://arxiv.org/abs/2508.21112)
*Delin Qu,Haoming Song,Qizhi Chen,Zhaoqing Chen,Xianqiang Gao,Xinyi Ye,Qi Lv,Modi Shi,Guanghui Ren,Cheng Ruan,Maoqing Yao,Haoran Yang,Jiacheng Bao,Bin Zhao,Dong Wang*

Main category: cs.RO

TL;DR: The paper introduces EO-Robotics, comprising the EO-1 model and EO-Data1.5M dataset, to advance multimodal embodied reasoning and robot control.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap in vision-language-action (VLA) models that currently lack flexibility in interleaved reasoning and physical interaction similar to human-level capabilities.

Method: Developed EO-1, a unified embodied foundation model, utilizing a unified architecture for processing multimodal inputs and trained extensively on EO-Data1.5M, a high-quality dataset with 1.5 million samples, integrating auto-regressive decoding and flow matching denoising.

Result: EO-1 demonstrates strong performance in open-world generalization and multimodal embodied reasoning tasks, successfully applied to complex, long-horizon robotic manipulation tasks.

Conclusion: This research progresses embodied intelligence by highlighting the synergy between interleaved vision-text-action learning and proposing architectures and datasets to improve robotic reasoning and interaction.

Abstract: The human ability to seamlessly perform multimodal reasoning and physical
interaction in the open world is a core goal for general-purpose embodied
intelligent systems. Recent vision-language-action (VLA) models, which are
co-trained on large-scale robot and visual-text data, have demonstrated notable
progress in general robot control. However, they still fail to achieve
human-level flexibility in interleaved reasoning and interaction. In this work,
introduce EO-Robotics, consists of EO-1 model and EO-Data1.5M dataset. EO-1 is
a unified embodied foundation model that achieves superior performance in
multimodal embodied reasoning and robot control through interleaved
vision-text-action pre-training. The development of EO-1 is based on two key
pillars: (i) a unified architecture that processes multimodal inputs
indiscriminately (image, text, video, and action), and (ii) a massive,
high-quality multimodal embodied reasoning dataset, EO-Data1.5M, which contains
over 1.5 million samples with emphasis on interleaved vision-text-action
comprehension. EO-1 is trained through synergies between auto-regressive
decoding and flow matching denoising on EO-Data1.5M, enabling seamless robot
action generation and multimodal embodied reasoning. Extensive experiments
demonstrate the effectiveness of interleaved vision-text-action learning for
open-world understanding and generalization, validated through a variety of
long-horizon, dexterous manipulation tasks across multiple embodiments. This
paper details the architecture of EO-1, the data construction strategy of
EO-Data1.5M, and the training methodology, offering valuable insights for
developing advanced embodied foundation models.

</details>


### [93] [Observer Design for Optical Flow-Based Visual-Inertial Odometry with Almost-Global Convergence](https://arxiv.org/abs/2508.21163)
*Tarek Bouazza,Soulaimane Berkane,Minh-Duc Hua,Tarek Hamel*

Main category: cs.RO

TL;DR: Develops a cascaded observer system integrating optical flow and IMU data for monocular visual-inertial odometry.


<details>
  <summary>Details</summary>
Motivation: Address the need for continuous and efficient visual-inertial odometry using monocular systems.

Method: Combines optical flow and IMU data using a Riccati observer for velocity and gravity estimation, alongside a complementary observer for attitude control.

Result: Simulation results demonstrate the effectiveness of the proposed algorithms in achieving stable and efficient performance.

Conclusion: The interconnected observer architecture is shown to be stable and capable of continuous monocular visual-inertial odometry.

Abstract: This paper presents a novel cascaded observer architecture that combines
optical flow and IMU measurements to perform continuous monocular
visual-inertial odometry (VIO). The proposed solution estimates body-frame
velocity and gravity direction simultaneously by fusing velocity direction
information from optical flow measurements with gyro and accelerometer data.
This fusion is achieved using a globally exponentially stable Riccati observer,
which operates under persistently exciting translational motion conditions. The
estimated gravity direction in the body frame is then employed, along with an
optional magnetometer measurement, to design a complementary observer on
$\mathbf{SO}(3)$ for attitude estimation. The resulting interconnected observer
architecture is shown to be almost globally asymptotically stable. To extract
the velocity direction from sparse optical flow data, a gradient descent
algorithm is developed to solve a constrained minimization problem on the unit
sphere. The effectiveness of the proposed algorithms is validated through
simulation results.

</details>


### [94] [Multi-robot Path Planning and Scheduling via Model Predictive Optimal Transport (MPC-OT)](https://arxiv.org/abs/2508.21205)
*Usman A. Khan,Mouhacine Benosman,Wenliang Liu,Federico Pecora,Joseph W. Durham*

Main category: cs.RO

TL;DR: This work introduces a method for multi-robot navigation that avoids deadlocks by using optimal transport theory and model predictive control for scheduling and path planning in obstacle-filled environments.


<details>
  <summary>Details</summary>
Motivation: Current methods mapping robots to targets often result in overlapping paths, causing deadlocks during navigation.

Method: The authors discretize space into cells and employ optimal transport theory to compute non-overlapping transitions for robots, integrating temporal structures and replanning mechanisms where needed.

Result: The proposed approach calculates minimum-cost, non-overlapping trajectories with computational complexities of O(K³ log K) or O(K² log K) depending on the problem's conditions.

Conclusion: The methodology ensures non-overlapping scheduling, accommodates complex scenarios, and incorporates dynamic adjustments through model predictive control.

Abstract: In this paper, we propose a novel methodology for path planning and
scheduling for multi-robot navigation that is based on optimal transport theory
and model predictive control. We consider a setup where $N$ robots are tasked
to navigate to $M$ targets in a common space with obstacles. Mapping robots to
targets first and then planning paths can result in overlapping paths that lead
to deadlocks. We derive a strategy based on optimal transport that not only
provides minimum cost paths from robots to targets but also guarantees
non-overlapping trajectories. We achieve this by discretizing the space of
interest into $K$ cells and by imposing a ${K\times K}$ cost structure that
describes the cost of transitioning from one cell to another. Optimal transport
then provides \textit{optimal and non-overlapping} cell transitions for the
robots to reach the targets that can be readily deployed without any scheduling
considerations. The proposed solution requires $\unicode{x1D4AA}(K^3\log K)$
computations in the worst-case and $\unicode{x1D4AA}(K^2\log K)$ for
well-behaved problems. To further accommodate potentially overlapping
trajectories (unavoidable in certain situations) as well as robot dynamics, we
show that a temporal structure can be integrated into optimal transport with
the help of \textit{replans} and \textit{model predictive control}.

</details>


### [95] [Uncertainty-Aware Ankle Exoskeleton Control](https://arxiv.org/abs/2508.21221)
*Fatima Mumtaza Tourk,Bishoy Galoaa,Sanat Shajan,Aaron J. Young,Michael Everett,Max K. Shepherd*

Main category: cs.RO

TL;DR: The study proposes an uncertainty-aware control framework for ankle exoskeletons, enabling them to operate safely in unstructured environments by disengaging during unfamiliar movements.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of existing exoskeleton controllers that are designed for predefined actions in controlled environments, restricting their applicability to real-world tasks.

Method: The method involves using an uncertainty estimator to classify user movements as in-distribution or out-of-distribution compared to training data. The strongest estimator (ensemble of gait phase estimators) was tested online for effectiveness.

Result: In an online test, the uncertainty estimator achieved an F1 score of 89.2, reliably toggling assistance on and off during user transitions between in-distribution and out-of-distribution tasks.

Conclusion: This framework enables ankle exoskeletons to operate autonomously and safely in unstructured environments, improving their real-world applicability.

Abstract: Lower limb exoskeletons show promise to assist human movement, but their
utility is limited by controllers designed for discrete, predefined actions in
controlled environments, restricting their real-world applicability. We present
an uncertainty-aware control framework that enables ankle exoskeletons to
operate safely across diverse scenarios by automatically disengaging when
encountering unfamiliar movements. Our approach uses an uncertainty estimator
to classify movements as similar (in-distribution) or different
(out-of-distribution) relative to actions in the training set. We evaluated
three architectures (model ensembles, autoencoders, and generative adversarial
networks) on an offline dataset and tested the strongest performing
architecture (ensemble of gait phase estimators) online. The online test
demonstrated the ability of our uncertainty estimator to turn assistance on and
off as the user transitioned between in-distribution and out-of-distribution
tasks (F1: 89.2). This new framework provides a path for exoskeletons to safely
and autonomously support human movement in unstructured, everyday environments.

</details>


### [96] [Remarks on stochastic cloning and delayed-state filtering](https://arxiv.org/abs/2508.21260)
*Tara Mina,Lindsey Marinello,John Christian*

Main category: cs.RO

TL;DR: This paper compares stochastic cloning (SC) and delayed-state Kalman filter (DSKF) for handling delayed-state measurements, showing DSKF is computationally and memory-efficient while achieving the same results as SC.


<details>
  <summary>Details</summary>
Motivation: Handling correlated delayed-state measurements is critical for accurate estimation in robotics and navigation. Existing methods like stochastic cloning are effective but computationally demanding.

Method: The paper revisits the delayed-state Kalman filter (DSKF) as an alternative to stochastic cloning, demonstrating equivalent performance without state augmentation.

Result: The study proves that DSKF achieves the same state and covariance updates as SC, while offering computational and memory-saving advantages.

Conclusion: Properly formulated delayed-state Kalman filters are efficient alternatives to stochastic cloning, challenging misconceptions about their capabilities.

Abstract: Many estimation problems in robotics and navigation involve measurements that
depend on prior states. A prominent example is odometry, which measures the
relative change between states over time. Accurately handling these
delayed-state measurements requires capturing their correlations with prior
state estimates, and a widely used approach is stochastic cloning (SC), which
augments the state vector to account for these correlations.
  This work revisits a long-established but often overlooked alternative--the
delayed-state Kalman filter--and demonstrates that a properly derived filter
yields exactly the same state and covariance update as SC, without requiring
state augmentation. Moreover, the generalized Kalman filter formulation
provides computational advantages, while also reducing memory requirements for
higher-dimensional states.
  Our findings clarify a common misconception that Kalman filter variants are
inherently unable to handle correlated delayed-state measurements,
demonstrating that an alternative formulation achieves the same results more
efficiently.

</details>


### [97] [Mini Autonomous Car Driving based on 3D Convolutional Neural Networks](https://arxiv.org/abs/2508.21271)
*Pablo Moraes,Monica Rodriguez,Kristofer S. Kappel,Hiago Sodre,Santiago Fernandez,Igor Nunes,Bruna Guterres,Ricardo Grando*

Main category: cs.RO

TL;DR: This paper presents a methodology using RGB-D data and 3D CNNs for Mini Autonomous Cars (MAC) driving in simulated environments, demonstrating promising results against RNNs.


<details>
  <summary>Details</summary>
Motivation: The study aims to address challenges in developing reliable autonomous systems, such as complexity, lengthy training periods, and uncertainty, through small-scale MAC setups.

Method: RGB-D information and 3D convolutional neural networks are used for autonomous car control, and results are compared with RNNs on simulated tracks.

Result: Performance metrics, including task completion success, lap time, and driving consistency, show that 3D CNNs outperform RNNs in generalization and control abilities.

Conclusion: MAC setups and 3D CNN methodologies facilitate effective evaluation and advancement of autonomous driving systems, showing potential for better generalization and performance.

Abstract: Autonomous driving applications have become increasingly relevant in the
automotive industry due to their potential to enhance vehicle safety,
efficiency, and user experience, thereby meeting the growing demand for
sophisticated driving assistance features. However, the development of reliable
and trustworthy autonomous systems poses challenges such as high complexity,
prolonged training periods, and intrinsic levels of uncertainty. Mini
Autonomous Cars (MACs) are used as a practical testbed, enabling validation of
autonomous control methodologies on small-scale setups. This simplified and
cost-effective environment facilitates rapid evaluation and comparison of
machine learning models, which is particularly useful for algorithms requiring
online training. To address these challenges, this work presents a methodology
based on RGB-D information and three-dimensional convolutional neural networks
(3D CNNs) for MAC autonomous driving in simulated environments. We evaluate the
proposed approach against recurrent neural networks (RNNs), with architectures
trained and tested on two simulated tracks with distinct environmental
features. Performance was assessed using task completion success, lap-time
metrics, and driving consistency. Results highlight how architectural
modifications and track complexity influence the models' generalization
capability and vehicle control performance. The proposed 3D CNN demonstrated
promising results when compared with RNNs.

</details>


### [98] [Learning to Assemble the Soma Cube with Legal-Action Masked DQN and Safe ZYZ Regrasp on a Doosan M0609](https://arxiv.org/abs/2508.21272)
*Jaehong Oh,Seungjun Jung,Sawoong Kim*

Main category: cs.RO

TL;DR: The paper introduces a novel framework for robotic assembly using Deep Q-Networks with motion planning that adapts to constraints, aiming to manipulate Soma cubes with an underactuated robot efficiently.


<details>
  <summary>Details</summary>
Motivation: To address challenges in robotic manipulation, particularly combinatorial action space explosion, unsafe motion planning, and systematic strategy learning for assembly tasks.

Method: The authors use legal-action masked Deep Q-Networks integrated with constraint-aware reinforcement learning and singularity-safe motion planning to perform autonomous Soma cube assembly.

Result: The system achieved high training efficiency with increasing difficulty: 100% success for Level 1, 92.9% for Level 2, and 39.9% for Level 3 across 105,300 episodes.

Conclusion: The approach enhances robotic manipulation efficiency under complex constraints, presenting a scalable solution for systematic and safe assembly learning in collaborative robots.

Abstract: This paper presents the first comprehensive application of legal-action
masked Deep Q-Networks with safe ZYZ regrasp strategies to an underactuated
gripper-equipped 6-DOF collaborative robot for autonomous Soma cube assembly
learning. Our approach represents the first systematic integration of
constraint-aware reinforcement learning with singularity-safe motion planning
on a Doosan M0609 collaborative robot. We address critical challenges in
robotic manipulation: combinatorial action space explosion, unsafe motion
planning, and systematic assembly strategy learning. Our system integrates a
legal-action masked DQN with hierarchical architecture that decomposes
Q-function estimation into orientation and position components, reducing
computational complexity from $O(3,132)$ to $O(116) + O(27)$ while maintaining
solution completeness. The robot-friendly reward function encourages
ground-first, vertically accessible assembly sequences aligned with
manipulation constraints. Curriculum learning across three progressive
difficulty levels (2-piece, 3-piece, 7-piece) achieves remarkable training
efficiency: 100\% success rate for Level 1 within 500 episodes, 92.9\% for
Level 2, and 39.9\% for Level 3 over 105,300 total training episodes.

</details>


### [99] [Observability-driven Assignment of Heterogeneous Sensors for Multi-Target Tracking](https://arxiv.org/abs/2508.21309)
*Seyed Ali Rakhshan,Mehdi Golestani,He Kong*

Main category: cs.RO

TL;DR: The paper develops a greedy algorithm using matroid theory for assigning heterogeneous sensors to targets to optimize tracking accuracy, offering constant-factor approximation and practical effectiveness.


<details>
  <summary>Details</summary>
Motivation: Efficient and accurate multi-target tracking requires optimized assignments of robots with varying sensor capabilities.

Method: The method involves classifying robots based on sensing abilities and employing a matroid theory-based greedy algorithm for dynamic allocation of robots to targets.

Result: The algorithm demonstrates accuracy and robustness in extended target tracking scenarios, achieving near-optimal performance in simulations.

Conclusion: The proposed approach provides a practical and theoretically sound solution for heterogeneous sensor assignment in multi-target tracking tasks.

Abstract: This paper addresses the challenge of assigning heterogeneous sensors (i.e.,
robots with varying sensing capabilities) for multi-target tracking. We
classify robots into two categories: (1) sufficient sensing robots, equipped
with range and bearing sensors, capable of independently tracking targets, and
(2) limited sensing robots, which are equipped with only range or bearing
sensors and need to at least form a pair to collaboratively track a target. Our
objective is to optimize tracking quality by minimizing uncertainty in target
state estimation through efficient robot-to-target assignment. By leveraging
matroid theory, we propose a greedy assignment algorithm that dynamically
allocates robots to targets to maximize tracking quality. The algorithm
guarantees constant-factor approximation bounds of 1/3 for arbitrary tracking
quality functions and 1/2 for submodular functions, while maintaining
polynomial-time complexity. Extensive simulations demonstrate the algorithm's
effectiveness in accurately estimating and tracking targets over extended
periods. Furthermore, numerical results confirm that the algorithm's
performance is close to that of the optimal assignment, highlighting its
robustness and practical applicability.

</details>


### [100] [Robust Real-Time Coordination of CAVs: A Distributed Optimization Framework under Uncertainty](https://arxiv.org/abs/2508.21322)
*Haojie Bai,Yang Wang,Cong Guo,Xiongwei Zhao,Hai Zhu*

Main category: cs.RO

TL;DR: The paper introduces a novel framework for cooperative vehicle coordination that ensures safety and real-time performance even in dynamic environments with uncertain conditions.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the challenge of balancing safety guarantees and real-time performance in cooperative vehicle coordination, especially in dynamic and uncertain scenarios.

Method: The proposed framework incorporates three key innovations: (1) robust cooperative planning with adaptive safety constraints for controlling vehicle trajectory distributions, (2) a parallel ADMM-based distributed trajectory optimization algorithm for efficient negotiation, and (3) an interactive attention mechanism to prioritize critical interactions and improve computational efficiency.

Result: Simulation and real-world experiments demonstrate a 40.79% reduction in collision rates, scalability for increasing vehicle numbers, and a 14.1% reduction in computational demands compared to conventional methods.

Conclusion: The framework is effective in enhancing safety, real-time performance, and computational efficiency in complex environments, validated through both simulation and real-world scenarios.

Abstract: Achieving both safety guarantees and real-time performance in cooperative
vehicle coordination remains a fundamental challenge, particularly in dynamic
and uncertain environments. This paper presents a novel coordination framework
that resolves this challenge through three key innovations: 1) direct control
of vehicles' trajectory distributions during coordination, formulated as a
robust cooperative planning problem with adaptive enhanced safety constraints,
ensuring a specified level of safety regarding the uncertainty of the
interactive trajectory, 2) a fully parallel ADMM-based distributed trajectory
negotiation (ADMM-DTN) algorithm that efficiently solves the optimization
problem while allowing configurable negotiation rounds to balance solution
quality and computational resources, and 3) an interactive attention mechanism
that selectively focuses on critical interactive participants to further
enhance computational efficiency. Both simulation results and practical
experiments demonstrate that our framework achieves significant advantages in
safety (reducing collision rates by up to 40.79\% in various scenarios) and
real-time performance compared to state-of-the-art methods, while maintaining
strong scalability with increasing vehicle numbers. The proposed interactive
attention mechanism further reduces the computational demand by 14.1\%. The
framework's effectiveness is further validated through real-world experiments
with unexpected dynamic obstacles, demonstrating robust coordination in complex
environments. The experiment demo could be found at
https://youtu.be/4PZwBnCsb6Q.

</details>


### [101] [Multi-Modal Model Predictive Path Integral Control for Collision Avoidance](https://arxiv.org/abs/2508.21364)
*Alberto Bertipaglia,Dariu M. Gavrila,Barys Shyrokau*

Main category: cs.RO

TL;DR: This paper presents a motion planning method for automated vehicles using a multi-modal model predictive algorithm that leverages advanced collision avoidance and trajectory optimization techniques.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the problem of sub-optimal solutions in motion planning for automated vehicles, especially under complex scenarios such as low-friction surfaces and moving obstacles.

Method: The proposed method introduces Sobol sequence sampling, analytical solutions for collision avoidance, and a multi-modal control algorithm utilizing a non-linear single-track vehicle model with Fiala tyres constrained by a friction circle for stability.

Result: The algorithm demonstrated superior performance compared to standard approaches in a simulated environment, successfully navigating obstacles and maintaining vehicle stability across challenging scenarios.

Conclusion: This novel multi-modal control approach enhances motion planning and decision-making for automated vehicles, offering improved safety and stability in diverse and challenging driving conditions.

Abstract: This paper proposes a novel approach to motion planning and decision-making
for automated vehicles, using a multi-modal Model Predictive Path Integral
control algorithm. The method samples with Sobol sequences around the prior
input and incorporates analytical solutions for collision avoidance. By
leveraging multiple modes, the multi-modal control algorithm explores diverse
trajectories, such as manoeuvring around obstacles or stopping safely before
them, mitigating the risk of sub-optimal solutions. A non-linear single-track
vehicle model with a Fiala tyre serves as the prediction model, and tyre force
constraints within the friction circle are enforced to ensure vehicle stability
during evasive manoeuvres. The optimised steering angle and longitudinal
acceleration are computed to generate a collision-free trajectory and to
control the vehicle. In a high-fidelity simulation environment, we demonstrate
that the proposed algorithm can successfully avoid obstacles, keeping the
vehicle stable while driving a double lane change manoeuvre on high and
low-friction road surfaces and occlusion scenarios with moving obstacles,
outperforming a standard Model Predictive Path Integral approach.

</details>


### [102] [Dynamics-Compliant Trajectory Diffusion for Super-Nominal Payload Manipulation](https://arxiv.org/abs/2508.21375)
*Anuj Pasricha,Joewie Koh,Jay Vakil,Alessandro Roncone*

Main category: cs.RO

TL;DR: This paper presents a novel motion-planning approach for robots that allows them to handle payloads exceeding nominal capacity in large regions of their workspace.


<details>
  <summary>Details</summary>
Motivation: Current payload ratings for robots are overly conservative, leading to underutilization of their actual capabilities within the workspace.

Method: Developing a trajectory generation approach using denoising diffusion models, which accounts for payload constraints and generates feasible trajectories in constant time.

Result: Experimental tests on a Franka Emika Panda robot show that up to 67.6% of the workspace remains accessible with payloads exceeding 3 times the nominal capacity.

Conclusion: Incorporating nuanced payload dynamics in motion planning enables robots to efficiently utilize their workspace even with heavier payloads than their nominal ratings.

Abstract: Nominal payload ratings for articulated robots are typically derived from
worst-case configurations, resulting in uniform payload constraints across the
entire workspace. This conservative approach severely underutilizes the robot's
inherent capabilities -- our analysis demonstrates that manipulators can safely
handle payloads well above nominal capacity across broad regions of their
workspace while staying within joint angle, velocity, acceleration, and torque
limits. To address this gap between assumed and actual capability, we propose a
novel trajectory generation approach using denoising diffusion models that
explicitly incorporates payload constraints into the planning process. Unlike
traditional sampling-based methods that rely on inefficient trial-and-error,
optimization-based methods that are prohibitively slow, or kinodynamic planners
that struggle with problem dimensionality, our approach generates dynamically
feasible joint-space trajectories in constant time that can be directly
executed on physical hardware without post-processing. Experimental validation
on a 7 DoF Franka Emika Panda robot demonstrates that up to 67.6% of the
workspace remains accessible even with payloads exceeding 3 times the nominal
capacity. This expanded operational envelope highlights the importance of a
more nuanced consideration of payload dynamics in motion planning algorithms.

</details>


### [103] [RoboInspector: Unveiling the Unreliability of Policy Code for LLM-enabled Robotic Manipulation](https://arxiv.org/abs/2508.21378)
*Chenduo Ying,Linkang Du,Peng Cheng,Yuanchao Shu*

Main category: cs.RO

TL;DR: This paper introduces RoboInspector, a framework to address reliability issues in policy code for LLM-enabled robotic manipulation, identifies unreliable behaviors, and presents a refinement approach improving code reliability by up to 35%.


<details>
  <summary>Details</summary>
Motivation: Despite the remarkable capabilities of large language models (LLMs) in reasoning and code generation, their reliability in generating policy code for robotic tasks remains a challenge due to task complexity and diverse user instructions.

Method: The authors design RoboInspector, a pipeline that characterizes unreliable policy behaviors based on task complexity and instruction granularity, conducting experiments with 168 task-instruction-LLM combinations.

Result: RoboInspector identifies four unreliable behaviors causing manipulation failures and proposes a refinement guided by failure feedback, improving reliability by up to 35% in both simulated and real-world tests.

Conclusion: The paper offers insights into the root causes of unreliable policy generation and demonstrates a practical feedback-driven approach to enhance the reliability of LLM-enabled robotic manipulation.

Abstract: Large language models (LLMs) demonstrate remarkable capabilities in reasoning
and code generation, enabling robotic manipulation to be initiated with just a
single instruction. The LLM carries out various tasks by generating policy code
required to control the robot. Despite advances in LLMs, achieving reliable
policy code generation remains a significant challenge due to the diverse
requirements of real-world tasks and the inherent complexity of user
instructions. In practice, different users may provide distinct instructions to
drive the robot for the same task, which may cause the unreliability of policy
code generation. To bridge this gap, we design RoboInspector, a pipeline to
unveil and characterize the unreliability of the policy code for LLM-enabled
robotic manipulation from two perspectives: the complexity of the manipulation
task and the granularity of the instruction. We perform comprehensive
experiments with 168 distinct combinations of tasks, instructions, and LLMs in
two prominent frameworks. The RoboInspector identifies four main unreliable
behaviors that lead to manipulation failure. We provide a detailed
characterization of these behaviors and their underlying causes, giving insight
for practical development to reduce unreliability. Furthermore, we introduce a
refinement approach guided by failure policy code feedback that improves the
reliability of policy code generation by up to 35% in LLM-enabled robotic
manipulation, evaluated in both simulation and real-world environments.

</details>


### [104] [Assessing Human Cooperation for Enhancing Social Robot Navigation](https://arxiv.org/abs/2508.21455)
*Hariharan Arunachalam,Phani Teja Singamaneni,Rachid Alami*

Main category: cs.RO

TL;DR: The paper addresses challenges in socially aware robot navigation by improving robot-human interactions through communication and geometric reasoning.


<details>
  <summary>Details</summary>
Motivation: Robots struggle when humans behave unexpectedly due to lack of understanding of human intentions and misaligned communication.

Method: The paper proposes assessing human cooperativeness, applying geometric reasoning, and formulating verbal or action-based responses in head-on crossing scenarios.

Result: Human cooperativeness metrics and context-based communication strategies significantly enhance robot navigation and interaction.

Conclusion: The research bridges understanding gaps through timely communication strategies informed by geometric and cooperative human behavior evaluations.

Abstract: Socially aware robot navigation is a planning paradigm where the robot
navigates in human environments and tries to adhere to social constraints while
interacting with the humans in the scene. These navigation strategies were
further improved using human prediction models, where the robot takes the
potential future trajectory of humans while computing its own. Though these
strategies significantly improve the robot's behavior, it faces difficulties
from time to time when the human behaves in an unexpected manner. This happens
as the robot fails to understand human intentions and cooperativeness, and the
human does not have a clear idea of what the robot is planning to do. In this
paper, we aim to address this gap through effective communication at an
appropriate time based on a geometric analysis of the context and human
cooperativeness in head-on crossing scenarios. We provide an assessment
methodology and propose some evaluation metrics that could distinguish a
cooperative human from a non-cooperative one. Further, we also show how
geometric reasoning can be used to generate appropriate verbal responses or
robot actions.

</details>


### [105] [Few-Shot Neuro-Symbolic Imitation Learning for Long-Horizon Planning and Acting](https://arxiv.org/abs/2508.21501)
*Pierrick Lorang,Hong Lu,Johannes Huemer,Patrik Zips,Matthias Scheutz*

Main category: cs.RO

TL;DR: The paper presents a neuro-symbolic framework that combines continuous control policies and symbolic domain abstractions for effective imitation learning, showing strong results with minimal data and robust generalization across tasks.


<details>
  <summary>Details</summary>
Motivation: Imitation learning is a powerful approach for intelligent systems but faces limitations in long-horizon task execution and generalization. This paper aims to overcome these challenges by leveraging neuro-symbolic methods.

Method: The framework combines graph-based task abstraction, symbolic rule discovery via an Answer Set Programming solver, diffusion policy imitation learning, and a high-level oracle for simplifying observation-action spaces.

Result: Tests across diverse domains, including robotic arms and forklifts, show high efficiency with just five skill demonstrations, robust generalization, and interpretable decision-making.

Conclusion: This neuro-symbolic framework successfully addresses key challenges in imitation learning, offering enhanced efficiency, generalization, and interpretability.

Abstract: Imitation learning enables intelligent systems to acquire complex behaviors
with minimal supervision. However, existing methods often focus on
short-horizon skills, require large datasets, and struggle to solve
long-horizon tasks or generalize across task variations and distribution
shifts. We propose a novel neuro-symbolic framework that jointly learns
continuous control policies and symbolic domain abstractions from a few skill
demonstrations. Our method abstracts high-level task structures into a graph,
discovers symbolic rules via an Answer Set Programming solver, and trains
low-level controllers using diffusion policy imitation learning. A high-level
oracle filters task-relevant information to focus each controller on a minimal
observation and action space. Our graph-based neuro-symbolic framework enables
capturing complex state transitions, including non-spatial and temporal
relations, that data-driven learning or clustering techniques often fail to
discover in limited demonstration datasets. We validate our approach in six
domains that involve four robotic arms, Stacking, Kitchen, Assembly, and Towers
of Hanoi environments, and a distinct Automated Forklift domain with two
environments. The results demonstrate high data efficiency with as few as five
skill demonstrations, strong zero- and few-shot generalizations, and
interpretable decision making.

</details>


### [106] [Estimated Informed Anytime Search for Sampling-Based Planning via Adaptive Sampler](https://arxiv.org/abs/2508.21549)
*Liding Zhang,Kuanqi Cai,Yu Zhang,Zhenshan Bing,Chaoqun Wang,Fan Wu,Sami Haddadin,Alois Knoll*

Main category: cs.RO

TL;DR: The paper introduces a new path planning method called Multi-Informed Trees (MIT*) that improves the efficiency and success rate in robotic path optimization, particularly in confined and high-dimensional spaces.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency of existing sampling-based path planning methods, which re-sample the entire configuration space when no initial solution is found, leading to higher computational costs.

Method: MIT* constructs estimated informed sets based on prior admissible solution costs, uses adaptive sampling strategies, and employs length-related adaptive sparse collision checks to optimize path planning.

Result: Simulations and real-world experiments demonstrated that MIT* is more efficient and accurate than other sampling-based planners in R^4 to R^16 and is applicable in real-world robotic tasks.

Conclusion: MIT* enhances path planning by improving computation times, path cost efficiency, and success rates. It outperforms traditional planners, making it valuable for both simulated and real-world applications.

Abstract: Path planning in robotics often involves solving continuously valued,
high-dimensional problems. Popular informed approaches include graph-based
searches, such as A*, and sampling-based methods, such as Informed RRT*, which
utilize informed set and anytime strategies to expedite path optimization
incrementally. Informed sampling-based planners define informed sets as subsets
of the problem domain based on the current best solution cost. However, when no
solution is found, these planners re-sample and explore the entire
configuration space, which is time-consuming and computationally expensive.
This article introduces Multi-Informed Trees (MIT*), a novel planner that
constructs estimated informed sets based on prior admissible solution costs
before finding the initial solution, thereby accelerating the initial
convergence rate. Moreover, MIT* employs an adaptive sampler that dynamically
adjusts the sampling strategy based on the exploration process. Furthermore,
MIT* utilizes length-related adaptive sparse collision checks to guide lazy
reverse search. These features enhance path cost efficiency and computation
times while ensuring high success rates in confined scenarios. Through a series
of simulations and real-world experiments, it is confirmed that MIT*
outperforms existing single-query, sampling-based planners for problems in R^4
to R^16 and has been successfully applied to real-world robot manipulation
tasks. A video showcasing our experimental results is available at:
https://youtu.be/30RsBIdexTU

</details>


### [107] [Learning Agile Gate Traversal via Analytical Optimal Policy Gradient](https://arxiv.org/abs/2508.21592)
*Tianchen Sun,Bingheng Wang,Longbin Tang,Yichao Gao,Lin Zhao*

Main category: cs.RO

TL;DR: The paper introduces a hybrid framework combining neural networks with MPC to enable efficient and agile quadrotor flights through narrow gates, significantly improving sample efficiency over standard RL methods.


<details>
  <summary>Details</summary>
Motivation: The challenge of navigating quadrotors through narrow gates is a benchmark for agility and precision but is limited by traditional flight stacks that require extensive tuning or RL methods with poor efficiency.

Method: A hybrid framework uses a neural network trained offline to fine-tune MPC parameters online. The NN predicts a reference pose and cost-function weights, while analytical policy gradients are derived for both MPC and a gate traversal detection module.

Result: The method demonstrates superior performance in hardware experiments, achieving fast and accurate gate traversal, with better sample efficiency than end-to-end RL solutions.

Conclusion: The approach successfully combines model-based and learning-based methods, offering improved efficiency and interpretability for agile quadrotor navigation in constrained environments.

Abstract: Traversing narrow gates presents a significant challenge and has become a
standard benchmark for evaluating agile and precise quadrotor flight.
Traditional modularized autonomous flight stacks require extensive design and
parameter tuning, while end-to-end reinforcement learning (RL) methods often
suffer from low sample efficiency and limited interpretability. In this work,
we present a novel hybrid framework that adaptively fine-tunes model predictive
control (MPC) parameters online using outputs from a neural network (NN)
trained offline. The NN jointly predicts a reference pose and cost-function
weights, conditioned on the coordinates of the gate corners and the current
drone state. To achieve efficient training, we derive analytical policy
gradients not only for the MPC module but also for an optimization-based gate
traversal detection module. Furthermore, we introduce a new formulation of the
attitude tracking error that admits a simplified representation, facilitating
effective learning with bounded gradients. Hardware experiments demonstrate
that our method enables fast and accurate quadrotor traversal through narrow
gates in confined environments. It achieves several orders of magnitude
improvement in sample efficiency compared to naive end-to-end RL approaches.

</details>


### [108] [The Rosario Dataset v2: Multimodal Dataset for Agricultural Robotics](https://arxiv.org/abs/2508.21635)
*Nicolas Soncini,Javier Cremona,Erica Vidal,Maximiliano García,Gastón Castro,Taihú Pire*

Main category: cs.RO

TL;DR: This paper introduces a comprehensive multi-modal dataset for agricultural robotics, captured in a soybean crop field to address challenges in lighting, terrain, motion blur, and perceptual aliasing.


<details>
  <summary>Details</summary>
Motivation: The authors want to address the challenges of robotics in agricultural environments by providing a benchmark dataset to support the development of localization, mapping, perception, and navigation algorithms.

Method: They collected over two hours of multi-modal sensor data synchronized via a robotic platform, covering key robotics challenges, and evaluated state-of-the-art SLAM methods on this dataset.

Result: The study demonstrates existing limitations of SLAM methods in agricultural environments using the new dataset.

Conclusion: The dataset provides a vital resource for advancing SLAM and other robotics tasks in complex agricultural settings, and is publicly available for research purposes.

Abstract: We present a multi-modal dataset collected in a soybean crop field,
comprising over two hours of recorded data from sensors such as stereo infrared
camera, color camera, accelerometer, gyroscope, magnetometer, GNSS (Single
Point Positioning, Real-Time Kinematic and Post-Processed Kinematic), and wheel
odometry. This dataset captures key challenges inherent to robotics in
agricultural environments, including variations in natural lighting, motion
blur, rough terrain, and long, perceptually aliased sequences. By addressing
these complexities, the dataset aims to support the development and
benchmarking of advanced algorithms for localization, mapping, perception, and
navigation in agricultural robotics. The platform and data collection system is
designed to meet the key requirements for evaluating multi-modal SLAM systems,
including hardware synchronization of sensors, 6-DOF ground truth and loops on
long trajectories.
  We run multimodal state-of-the art SLAM methods on the dataset, showcasing
the existing limitations in their application on agricultural settings. The
dataset and utilities to work with it are released on
https://cifasis.github.io/rosariov2/.

</details>


### [109] [Robust Convex Model Predictive Control with collision avoidance guarantees for robot manipulators](https://arxiv.org/abs/2508.21677)
*Bernhard Wullt,Johannes Köhler,Per Mattsson,Mikeal Norrlöf,Thomas B. Schön*

Main category: cs.RO

TL;DR: The paper introduces a novel Model Predictive Control (MPC) method for industrial manipulators to achieve fast and safe motion planning in cluttered environments with model uncertainties.


<details>
  <summary>Details</summary>
Motivation: Existing industrial manipulator control methods struggle with balancing safety and speed, especially in cluttered environments with model uncertainties. The authors aim to address this gap.

Method: A robust tube MPC combined with a corridor planning algorithm to ensure collision-free and safe motion. The approach results in a convex MPC that is computationally efficient.

Result: Simulation tests on a 6 DOF industrial robot show improved performance compared to benchmark methods, handling greater model uncertainties and enabling faster motion.

Conclusion: The proposed MPC method successfully improves safety and speed for industrial manipulators, making it feasible for practical applications in challenging environments.

Abstract: Industrial manipulators are normally operated in cluttered environments,
making safe motion planning important. Furthermore, the presence of
model-uncertainties make safe motion planning more difficult. Therefore, in
practice the speed is limited in order to reduce the effect of disturbances.
There is a need for control methods that can guarantee safe motions that can be
executed fast. We address this need by suggesting a novel model predictive
control (MPC) solution for manipulators, where our two main components are a
robust tube MPC and a corridor planning algorithm to obtain collision-free
motion. Our solution results in a convex MPC, which we can solve fast, making
our method practically useful. We demonstrate the efficacy of our method in a
simulated environment with a 6 DOF industrial robot operating in cluttered
environments with uncertainties in model parameters. We outperform benchmark
methods, both in terms of being able to work under higher levels of model
uncertainties, while also yielding faster motion.

</details>


### [110] [Can a mobile robot learn from a pedestrian model to prevent the sidewalk salsa?](https://arxiv.org/abs/2508.21690)
*Olger Siebinga,David Abbink*

Main category: cs.RO

TL;DR: This paper explores how robots can learn to interact safely with pedestrians, specifically avoiding the 'sidewalk salsa' awkward collision scenario.


<details>
  <summary>Details</summary>
Motivation: To understand and address the awkward sidewalk salsa interaction between pedestrians and mobile robots to improve robotic behavior using implicit communication insights.

Method: The study uses the Communication-Enabled Interaction (CEI) framework and introduces a Reinforcement Learning (RL) agent to interact with a pedestrian behavior model.

Result: A basic RL agent successfully interacted with the CEI model, and a risk-averse RL agent further reduced perceived risk and improved communication through motion.

Conclusion: The approach successfully demonstrates the feasibility of leveraging RL and the CEI framework to navigate pedestrian interactions, encouraging more exploration in the field.

Abstract: Pedestrians approaching each other on a sidewalk sometimes end up in an
awkward interaction known as the "sidewalk salsa": they both (repeatedly)
deviate to the same side to avoid a collision. This provides an interesting use
case to study interactions between pedestrians and mobile robots because, in
the vast majority of cases, this phenomenon is avoided through a negotiation
based on implicit communication. Understanding how it goes wrong and how
pedestrians end up in the sidewalk salsa will therefore provide insight into
the implicit communication. This understanding can be used to design safe and
acceptable robotic behaviour. In a previous attempt to gain this understanding,
a model of pedestrian behaviour based on the Communication-Enabled Interaction
(CEI) framework was developed that can replicate the sidewalk salsa. However,
it is unclear how to leverage this model in robotic planning and
decision-making since it violates the assumptions of game theory, a much-used
framework in planning and decision-making. Here, we present a proof-of-concept
for an approach where a Reinforcement Learning (RL) agent leverages the model
to learn how to interact with pedestrians. The results show that a basic RL
agent successfully learned to interact with the CEI model. Furthermore, a
risk-averse RL agent that had access to the perceived risk of the CEI model
learned how to effectively communicate its intention through its motion and
thereby substantially lowered the perceived risk, and displayed effort by the
modelled pedestrian. These results show this is a promising approach and
encourage further exploration.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [111] [Model-Driven Quantum Code Generation Using Large Language Models and Retrieval-Augmented Generation](https://arxiv.org/abs/2508.21097)
*Nazanin Siavash,Armin Moin*

Main category: cs.SE

TL;DR: The paper explores using Large Language Models (LLMs) enhanced with Retrieval-Augmented Generation (RAG) pipelines for generating quantum software code from UML models, improving code consistency and accuracy.


<details>
  <summary>Details</summary>
Motivation: Challenges in creating software for quantum and hybrid quantum-classical systems due to complex platforms and lack of developer skills drive the need for cost-effective and low-risk solutions.

Method: Using model-driven approaches to automate Python code generation for quantum systems via LLMs enhanced with RAG pipelines that include sample Qiskit code extracted from GitHub repositories.

Result: Experimental findings indicate a significant improvement, with CodeBLEU scores enhanced up to four times using optimized prompts.

Conclusion: The paper highlights promising directions for employing RAG pipelines in quantum software code generation while identifying further areas for exploration such as code-to-code transformations and model-driven repository utilization.

Abstract: This paper introduces a novel research direction for model-to-text/code
transformations by leveraging Large Language Models (LLMs) that can be enhanced
with Retrieval-Augmented Generation (RAG) pipelines. The focus is on quantum
and hybrid quantum-classical software systems, where model-driven approaches
can help reduce the costs and mitigate the risks associated with the
heterogeneous platform landscape and lack of developers' skills. We validate
one of the proposed ideas regarding generating code out of UML model instances
of software systems. This Python code uses a well-established library, called
Qiskit, to execute on gate-based or circuit-based quantum computers. The RAG
pipeline that we deploy incorporates sample Qiskit code from public GitHub
repositories. Experimental results show that well-engineered prompts can
improve CodeBLEU scores by up to a factor of four, yielding more accurate and
consistent quantum code. However, the proposed research direction can go beyond
this through further investigation in the future by conducting experiments to
address our other research questions and ideas proposed here, such as deploying
software system model instances as the source of information in the RAG
pipelines, or deploying LLMs for code-to-code transformations, for instance,
for transpilation use cases.

</details>


### [112] [Learning to Generate Unit Test via Adversarial Reinforcement Learning](https://arxiv.org/abs/2508.21107)
*Dongjun Lee,Changho Hwang,Kimin Lee*

Main category: cs.SE

TL;DR: The paper introduces UTRL, a reinforcement learning framework designed to improve the quality of unit tests generated by large language models.


<details>
  <summary>Details</summary>
Motivation: Unit testing is essential for evaluating programs, but generating comprehensive tests remains challenging. The paper aims to address the underexplored area of training LLMs to produce high-quality unit tests.

Method: The authors propose UTRL, which employs adversarial reinforcement learning to train a unit test generator and a code generator iteratively. The test generator aims to expose faults in the code, while the code generator learns to pass these tests.

Result: The UTRL-trained model, Qwen3-4B, generates higher-quality unit tests than models trained via supervised fine-tuning. It also outperforms advanced models like GPT-4.1 in test generation.

Conclusion: UTRL provides an effective method to train LLMs for generating high-quality unit tests, improving both the testing process and the alignment with human-written tests.

Abstract: Unit testing is a core practice in programming, enabling systematic
evaluation of programs produced by human developers or large language models
(LLMs). Given the challenges in writing comprehensive unit tests, LLMs have
been employed to automate test generation, yet methods for training LLMs to
produce high-quality tests remain underexplored. In this work, we propose UTRL,
a novel reinforcement learning framework that trains an LLM to generate
high-quality unit tests given a programming instruction. Our key idea is to
iteratively train two LLMs, the unit test generator and the code generator, in
an adversarial manner via reinforcement learning. The unit test generator is
trained to maximize a discrimination reward, which reflects its ability to
produce tests that expose faults in the code generator's solutions, and the
code generator is trained to maximize a code reward, which reflects its ability
to produce solutions that pass the unit tests generated by the test generator.
In our experiments, we demonstrate that unit tests generated by Qwen3-4B
trained via UTRL show higher quality compared to unit tests generated by the
same model trained via supervised fine-tuning on human-written ground-truth
unit tests, yielding code evaluations that more closely align with those
induced by the ground-truth tests. Moreover, Qwen3-4B trained with UTRL
outperforms frontier models such as GPT-4.1 in generating high-quality unit
tests, highlighting the effectiveness of UTRL in training LLMs for this task.

</details>


### [113] [Automated Bug Triaging using Instruction-Tuned Large Language Models](https://arxiv.org/abs/2508.21156)
*Kiana Kiashemshaki,Arsham Khosravani,Alireza Hosseinpour,Arshia Akhavan*

Main category: cs.SE

TL;DR: This paper proposes a framework that uses instruction-tuned large language models with LoRA adapters to improve bug assignment in large projects.


<details>
  <summary>Details</summary>
Motivation: Bug triaging in large projects is slow and inconsistent, posing a need for an efficient and accurate solution.

Method: The authors employ instruction-tuned large language models with LoRA adapters and candidate-constrained decoding to create a framework for bug triaging.

Result: The framework achieves strong shortlist quality, with Hit at 10 up to 0.753, and shows improved accuracy in human-in-the-loop scenarios.

Conclusion: Instruction-tuned LLMs could be a cost-effective solution compared to traditional feature engineering and graph-based methods.

Abstract: Bug triaging, the task of assigning new issues to developers, is often slow
and inconsistent in large projects. We present a lightweight framework that
instruction-tuned large language model (LLM) with LoRA adapters and uses
candidate-constrained decoding to ensure valid assignments. Tested on
EclipseJDT and Mozilla datasets, the model achieves strong shortlist quality
(Hit at 10 up to 0.753) despite modest exact Top-1 accuracy. On recent
snapshots, accuracy rises sharply, showing the framework's potential for
real-world, human-in-the-loop triaging. Our results suggest that
instruction-tuned LLMs offer a practical alternative to costly feature
engineering and graph-based methods.

</details>


### [114] [The Complexity Trap: Simple Observation Masking Is as Efficient as LLM Summarization for Agent Context Management](https://arxiv.org/abs/2508.21433)
*Tobias Lindenbauer,Igor Slinko,Ludwig Felder,Egor Bogomolov,Yaroslav Zharov*

Main category: cs.SE

TL;DR: The paper compares LLM-based summarization against simpler observation-masking strategies in the context histories of Software Engineering agents, finding masking more cost-effective and equally or slightly more performant.


<details>
  <summary>Details</summary>
Motivation: To address the high cost and complexity of context histories in LLM-based SWE agents, and evaluate whether simpler methods can achieve similar or better outcomes than summarization.

Method: The authors systematically compare observation-masking and LLM summarization strategies in SWE-agent using the SWE-bench Verified framework across five model configurations.

Result: Observation-masking was found to halve the cost compared to raw agent setups while matching or slightly exceeding the solve rate of LLM summarization.

Conclusion: Observation-masking can offer an effective, efficient alternative to summarization within SWE-agent, suggesting that simpler context management strategies can be optimal in this setting.

Abstract: Large Language Model (LLM)-based agents solve complex tasks through iterative
reasoning, exploration, and tool-use, a process that can result in long,
expensive context histories. While state-of-the-art Software Engineering ( SE)
agents like OpenHands or Cursor use LLM-based summarization to tackle this
issue, it is unclear whether the increased complexity offers tangible
performance benefits compared to simply omitting older observations. We present
a systematic comparison of these strategies within SWE-agent on SWE-bench
Verified across five diverse model configurations. We find that a simple
observation-masking strategy halves cost relative to a raw agent while
matching, and sometimes slightly exceeding, the solve rate of LLM
summarization. For example, with Qwen3-Coder 480B, masking improves solve rate
from 53.8% (raw agent) to 54.8%, while remaining competitive with summarization
at a lower cost. These results suggest that, at least within SWE-agent on
SWE-bench Verified, the most effective and efficient context management can be
the simplest. We release code and data for reproducibility

</details>


### [115] [Enhancing Semantic Understanding in Pointer Analysis using Large Language Models](https://arxiv.org/abs/2508.21454)
*Baijun Cheng,Kailong Wang,Ling Shi,Haoyu Wang,Yao Guo,Ding Li,Xiangqun Chen*

Main category: cs.SE

TL;DR: The paper proposes LMPA, a pointer analysis framework enhanced by large language models (LLMs) to improve accuracy and scalability, while addressing challenges like incorrect fact propagation.


<details>
  <summary>Details</summary>
Motivation: Existing pointer analysis frameworks struggle with imprecision caused by insufficient semantic understanding of code and overly conservative handling of user-defined functions. Advances in LLMs offer an opportunity to improve this.

Method: LMPA integrates LLMs to analyze user-defined functions similar to system APIs, reduces context propagation errors, infers initial points-to sets, and uses augmented natural language summaries.

Result: The proposed LMPA approach improves pointer analysis precision and scalability by leveraging LLMs and addressing key challenges like erroneous data propagation.

Conclusion: LMPA demonstrates the potential of LLMs to enhance classical pointer analysis frameworks, though technical challenges remain to be solved for full implementation.

Abstract: Pointer analysis has been studied for over four decades. However, existing
frameworks continue to suffer from the propagation of incorrect facts. A major
limitation stems from their insufficient semantic understanding of code,
resulting in overly conservative treatment of user-defined functions. Recent
advances in large language models (LLMs) present new opportunities to bridge
this gap. In this paper, we propose LMPA (LLM-enhanced Pointer Analysis), a
vision that integrates LLMs into pointer analysis to enhance both precision and
scalability. LMPA identifies user-defined functions that resemble system APIs
and models them accordingly, thereby mitigating erroneous cross-calling-context
propagation. Furthermore, it enhances summary-based analysis by inferring
initial points-to sets and introducing a novel summary strategy augmented with
natural language. Finally, we discuss the key challenges involved in realizing
this vision.

</details>


### [116] [Reusable Test Suites for Reinforcement Learning](https://arxiv.org/abs/2508.21553)
*Jørn Eirik Betten,Quentin Mazouni,Dennis Gross,Pedro Lind,Helge Spieker*

Main category: cs.SE

TL;DR: The paper introduces MPTCS, a new method for selecting test cases for RL environments to ensure diverse and reusable policy-agnostic testing.


<details>
  <summary>Details</summary>
Motivation: Validating RL agent policies for deployment is challenging due to policy-specific test suites with unclear relevance across policies.

Method: The proposed MPTCS framework selects test cases via criteria like solvability, diversity, and difficulty using a set of policies from any test case pool.

Result: The difficulty score effectiveness, cost dependency on policy count, and improvements in test suite diversity using a descriptor surface were evaluated.

Conclusion: MPTCS enhances reliability testing for RL agents by creating diverse test suites that uncover typical flaws while using policy-agnostic methods.

Abstract: Reinforcement learning (RL) agents show great promise in solving sequential
decision-making tasks. However, validating the reliability and performance of
the agent policies' behavior for deployment remains challenging. Most
reinforcement learning policy testing methods produce test suites tailored to
the agent policy being tested, and their relevance to other policies is
unclear. This work presents Multi-Policy Test Case Selection (MPTCS), a novel
automated test suite selection method for RL environments, designed to extract
test cases generated by any policy testing framework based on their
solvability, diversity, and general difficulty. MPTCS uses a set of policies to
select a diverse collection of reusable policy-agnostic test cases that reveal
typical flaws in the agents' behavior. The set of policies selects test cases
from a candidate pool, which can be generated by any policy testing method,
based on a difficulty score. We assess the effectiveness of the difficulty
score and how the method's effectiveness and cost depend on the number of
policies in the set. Additionally, a method for promoting diversity in the test
suite, a discretized general test case descriptor surface inspired by
quality-diversity algorithms, is examined to determine how it covers the state
space and which policies it triggers to produce faulty behaviors.

</details>


### [117] [Human-Written vs. AI-Generated Code: A Large-Scale Study of Defects, Vulnerabilities, and Complexity](https://arxiv.org/abs/2508.21634)
*Domenico Cotroneo,Cristina Improta,Pietro Liguori*

Main category: cs.SE

TL;DR: The paper compares human-written code and AI-generated code from ChatGPT, DeepSeek-Coder, and Qwen-Coder, analyzing over 500k Python and Java samples for defects, vulnerabilities, and complexity.


<details>
  <summary>Details</summary>
Motivation: Understand the differences in reliability, maintainability, and security between human-written and AI-generated code to improve software quality in AI-assisted development.

Method: Evaluated code samples using Orthogonal Defect Classification for defects and Common Weakness Enumeration for security vulnerabilities, analyzing structural complexity and other metrics.

Result: AI-generated code is simpler but more repetitive, with higher chances of unused constructs and debugging issues. Human-authored code is more complex and maintainability-focused, but AI code has more high-risk security vulnerabilities.

Conclusion: AI-assisted programming demands tailored quality assurance methods due to the distinct defect and security profiles of AI-generated code.

Abstract: As AI code assistants become increasingly integrated into software
development workflows, understanding how their code compares to human-written
programs is critical for ensuring reliability, maintainability, and security.
In this paper, we present a large-scale comparison of code authored by human
developers and three state-of-the-art LLMs, i.e., ChatGPT, DeepSeek-Coder, and
Qwen-Coder, on multiple dimensions of software quality: code defects, security
vulnerabilities, and structural complexity. Our evaluation spans over 500k code
samples in two widely used languages, Python and Java, classifying defects via
Orthogonal Defect Classification and security vulnerabilities using the Common
Weakness Enumeration. We find that AI-generated code is generally simpler and
more repetitive, yet more prone to unused constructs and hardcoded debugging,
while human-written code exhibits greater structural complexity and a higher
concentration of maintainability issues. Notably, AI-generated code also
contains more high-risk security vulnerabilities. These findings highlight the
distinct defect profiles of AI- and human-authored code and underscore the need
for specialized quality assurance practices in AI-assisted programming.

</details>


### [118] [The Integration of Agile Methodologies in DevOps Practices within the Information Technology Industry](https://arxiv.org/abs/2508.21811)
*Ashley Hourigan,Ridewaan Hanslo*

Main category: cs.SE

TL;DR: This study evaluates how Agile methodologies can integrate into DevOps practices, using semi-structured interviews and thematic analysis to define their interrelationship.


<details>
  <summary>Details</summary>
Motivation: To address the push for faster software product delivery in the IT industry and explore the integration of Agile methodologies with DevOps practices.

Method: The authors conducted eleven semi-structured interviews with IT professionals and used thematic analysis to identify 51 codes, grouped into 19 themes, analyzing Agile and DevOps relationships across the DevOps lifecycle.

Result: Findings present 19 themes for understanding Agile-DevOps integration, outlining their application in different phases of the DevOps lifecycle.

Conclusion: A novel insight into the compatibility between Agile and DevOps was formulated, showing how Agile methods can enhance DevOps practices in IT.

Abstract: The demand for rapid software delivery in the Information Technology (IT)
industry has significantly intensified, emphasising the need for faster
software products and service releases with enhanced features to meet customer
expectations. Agile methodologies are replacing traditional approaches such as
Waterfall, where flexibility, iterative development and adaptation to change
are favoured over rigid planning and execution. DevOps, a subsequent evolution
from Agile, emphasises collaborative efforts in development and operations
teams, focusing on continuous integration and deployment to deliver resilient
and high-quality software products and services. This study aims to critically
assess both Agile and DevOps practices in the IT industry to identify the
feasibility and applicability of Agile methods in DevOps practices. Eleven
semi-structured interviews were conducted with Agile and DevOps practitioners
in varying capacities across several sectors within the IT industry. Through
thematic analysis, 51 unique codes were extracted and synthesised into 19
themes that reported on each phase of the DevOps lifecycle, specifically
regarding the integration and implementation of Agile methods into DevOps
practices. Based on the findings, a new understanding detailing the
interrelationship of Agile methods in DevOps practices was discussed that met
the research objectives.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [119] [NSPDI-SNN: An efficient lightweight SNN based on nonlinear synaptic pruning and dendritic integration](https://arxiv.org/abs/2508.21566)
*Wuque Cai,Hongze Sun,Jiayi He,Qianqian Liao,Yunliang Zang,Duo Chen,Dezhong Yao,Daqing Guo*

Main category: q-bio.NC

TL;DR: The paper introduces NSPDI-SNN, a lightweight spiking neural network method integrating dendritic structures and nonlinear synaptic pruning for improved efficiency and sparsity across diverse tasks.


<details>
  <summary>Details</summary>
Motivation: To address the lack of dendrite-like complex structures in spiking neural networks, inspired by the computational efficiency of biological neurons.

Method: Introduces nonlinear dendritic integration and a novel synaptic pruning method to enhance spatiotemporal representation and achieve high sparsity in SNNs.

Result: NSPDI-SNN demonstrated high sparsity with negligible performance loss across benchmarks and tasks such as speech recognition and reinforcement learning.

Conclusion: Complex neuronal dendrite structures provide an effective pathway to enhance SNN efficiency while maintaining sparsity in diverse applications.

Abstract: Spiking neural networks (SNNs) are artificial neural networks based on
simulated biological neurons and have attracted much attention in recent
artificial intelligence technology studies. The dendrites in biological neurons
have efficient information processing ability and computational power; however,
the neurons of SNNs rarely match the complex structure of the dendrites.
Inspired by the nonlinear structure and highly sparse properties of neuronal
dendrites, in this study, we propose an efficient, lightweight SNN method with
nonlinear pruning and dendritic integration (NSPDI-SNN). In this method, we
introduce nonlinear dendritic integration (NDI) to improve the representation
of the spatiotemporal information of neurons. We implement heterogeneous state
transition ratios of dendritic spines and construct a new and flexible
nonlinear synaptic pruning (NSP) method to achieve the high sparsity of SNN. We
conducted systematic experiments on three benchmark datasets (DVS128 Gesture,
CIFAR10-DVS, and CIFAR10) and extended the evaluation to two complex tasks
(speech recognition and reinforcement learning-based maze navigation task).
Across all tasks, NSPDI-SNN consistently achieved high sparsity with minimal
performance degradation. In particular, our method achieved the best
experimental results on all three event stream datasets. Further analysis
showed that NSPDI significantly improved the efficiency of synaptic information
transfer as sparsity increased. In conclusion, our results indicate that the
complex structure and nonlinear computation of neuronal dendrites provide a
promising approach for developing efficient SNN methods.

</details>


### [120] [Coherent dynamics in soft-threshold integrate-and-fire networks](https://arxiv.org/abs/2508.21177)
*Lauren Forbes,Jared Grossman,Montie Avery,Ryan Goh,Gabriel Koch Ocker*

Main category: q-bio.NC

TL;DR: The paper explores bifurcations in integrate-and-fire neuron networks, introducing spatial, temporal, and spatiotemporal dynamics through different synaptic interactions and confirming results via simulations.


<details>
  <summary>Details</summary>
Motivation: To understand how synaptic delays and spatial connectivity patterns influence macroscopic activity in neuronal networks.

Method: Employ a deterministic mean-field approximation to study spatial, temporal, and spatiotemporal bifurcations, and validate findings through stochastic model simulations.

Result: Synaptic delays cause oscillations, local excitation leads to stationary bumps, and combinations of delays and inhibition/excitation cause spatiotemporal patterns like waves.

Conclusion: The mean-field framework reliably characterizes bifurcations and complex dynamics in neuron networks, aligning with stochastic simulations.

Abstract: We study bifurcations in networks of integrate-and-fire neurons with
stochastic spike emission, focusing on the effects of the spatial and temporal
structure of the synaptic interactions. Using a deterministic mean-field
approximation of the population dynamics, we characterize spatial, temporal,
and spatiotemporal patterns of macroscopic activity. In the mean-field theory,
synaptic delays give rise to uniform oscillations across the population through
a subcritical Hopf bifurcation of the stationary uniform equilibrium. With
local excitation and long-range inhibition the network undergoes a Turing
bifurcation, resulting in a localized area of sustained activity, or stationary
bump. When the coupling has both delays, local inhibition, and long range
excitation, the network undergoes a Turing-Hopf bifurcation leading to
spatiotemporal dynamics, such as standing and traveling waves. When multiple
instabilities are excited, we observe other complex spatiotemporal dynamics. We
confirm all these predictions of the mean-field theory in simulations of the
underlying stochastic model.

</details>


### [121] [Self-regulated emergence of heavy-tailed weight distributions in evolving complex network architectures](https://arxiv.org/abs/2508.21445)
*Jia Li,Cees van Leeuwen,Roman Bauer,Ilias Rentzeperis*

Main category: q-bio.NC

TL;DR: The authors propose a Hebbian-inspired model combining homeostatic weight adjustments and adaptive rewiring to simultaneously create heavy-tailed weight distributions and complex network topologies.


<details>
  <summary>Details</summary>
Motivation: Existing mechanisms either explain heavy-tailed weight distributions or complex topologies in neural networks, but no single mechanism unifies both aspects.

Method: The study uses a Hebbian-inspired model incorporating homeostatic adjustments for weights alongside adaptive rewiring of directed connections influenced by activity dynamics.

Result: The model produces heavy-tailed weight distributions and complex neural network structures, such as convergent-divergent circuits, under various activity spread regimes.

Conclusion: A parsimonious mechanism combining adaptive weight adjustment and rewiring based on homeostatic dynamics can simultaneously create key structural features of neural networks.

Abstract: Brain networks continually adjust the weights of their connections, resulting
in heavy-tailed distributions of their connection weights, a few strong
connections among many weaker ones. At the same time, these connections undergo
structural plasticity, forming a complex network topology. Although mechanisms
producing either heavy-tailed distributions or complex topologies have been
proposed, it has remained unclear whether a single mechanism can produce both.
We consider homeostasis as the driving principle and propose a Hebbian inspired
model that adaptively adjusts weights and rewires directed connections based on
homeostatic dynamics. Without adaptive rewiring, weight adjustment alone still
generates heavy-tailed weight distributions, as long as activity does not
spread beyond locally neighboring units. However, when combined with adaptive
rewiring, the homeostatic dynamics create a synergy that produces heavy-tailed
weight distributions also for more extended activity flow. Furthermore, the
model generates complex network structures that encompass convergent-divergent
circuits similar to those that facilitate signal transmission throughout the
nervous system. By combining adaptive weight adjustment and rewiring based on
the same homeostatic dynamics, our model provides a parsimonious and robust
mechanism that simultaneously produces heavy-tailed weight distributions and
convergent-divergent units under a wide range of dynamical regimes.

</details>


### [122] [Testing quantum markers of brain processes](https://arxiv.org/abs/2508.21490)
*Partha Ghose,Dimitris Pinotsis*

Main category: q-bio.NC

TL;DR: Explores links between quantum mechanics and neural dynamics, proposing experiments to detect quantum effects in the brain.


<details>
  <summary>Details</summary>
Motivation: To investigate possible connections between relativistic quantum mechanics and probabilistic neural dynamics, suggesting quantum-like phenomena in brain activities.

Method: The authors propose theoretical neuroscience experiments focusing on axonal signal propagation and neuronal temperature measurements to observe quantum markers.

Result: Predicted signatures of Dirac-type stochasticity and quantum-like brain oscillations.

Conclusion: Experimental detection of quantum characteristics in brain processes could bridge quantum mechanics and neuroscience.

Abstract: The emergence of the Dirac equation from a stochastic master equation
suggests a profound link between relativistic quantum mechanics and underlying
probabilistic descriptions of brain dynamics. In parallel, recent work has
shown that the FitzHugh-Nagumo equations describing excitable neurons can be
reformulated to yield a Schr\"{o}dinger-like equation with a novel Planck-like
constant, indicating that neural noise may give rise to quantum-like dynamics.
This paper brings these insights together to propose two novel neuroscience
experiments aimed at detecting emergent coherence in axonal signal propagation
and subthreshold oscillations. We suggest that stochastic interference effects
in axon branching structures may reveal signatures of Dirac-type stochasticity.
We also suggest that by measuring neuronal temperature and fluctuations we can
detect quantum effects in brain oscillations. If successful, these experiments
will provide experimental support for quantum markers of brain processes.

</details>


### [123] [A universal animal communication tempo resonates with the receiver's brain](https://arxiv.org/abs/2508.21530)
*Guy Amichay,Vijay Balasubramanian,Daniel M. Abrams*

Main category: q-bio.NC

TL;DR: This paper explores the synchronization of various species communicating isochronously at specific frequencies, investigating whether these frequencies have a biophysical basis.


<details>
  <summary>Details</summary>
Motivation: Inspired by observations in Thailand of synchronous fireflies and crickets, the authors aim to investigate whether the observed frequencies of communication are evolutionarily universal.

Method: The study includes a meta-analysis of species communicating isochronously and the construction of small receiver circuits mimicking neuronal biophysics to test responsiveness to these frequencies.

Result: They find that many evolutionarily distinct species communicate in a specific frequency range (~0.5-4 Hz) and show that neuronal-like circuits are most responsive to these frequencies.

Conclusion: The work suggests these frequencies represent a universal biophysical hotspot for neuronal communication in multiple species.

Abstract: During fieldwork in Thailand we observed nearly identical frequencies of
co-located flashing fireflies and chirping crickets. Motivated by this, we
perform a meta-analysis and show an abundance of evolutionarily distinct
species that communicate isochronously at ~0.5-4 Hz, suggesting that this might
be a frequency "hotspot." We hypothesize that this timescale may have a
universal basis in the biophysics of the receiver's neurons. We test this by
demonstrating that small receiver circuits constructed from elements
representing typical neurons will be most responsive in the observed frequency
range.

</details>


### [124] [Technical Development of Two-Photon Optogenetic Stimulation and Its Potential Application to Brain-Machine Interfaces](https://arxiv.org/abs/2508.21555)
*Riichiro Hira,Yoshikazu Isomura*

Main category: q-bio.NC

TL;DR: This paper reviews advancements in two-photon optogenetics, which allows precise, targeted neuronal control and highlights its techniques, advantages, and integration into brain-machine interfaces.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of conventional neuronal modulation techniques by advancing precise, targeted neuronal control using two-photon optogenetics.

Method: The paper focuses on three main two-photon optogenetic strategies: spiral scanning, temporal focusing, and three-dimensional computer-generated holography (3D CGH), including their integration with large field-of-view microscopes.

Result: Advancements have enabled flexible, targeted stimulation of hundreds of neurons with high precision, facilitating causal neural circuit studies, while addressing implementation challenges.

Conclusion: Two-photon optogenetics is a transformative technology that has significantly advanced precise neuronal control, allowing integration into brain-machine interfaces and enabling further breakthroughs in neuroscience research and applications.

Abstract: Over the past decade, techniques enabling bidirectional modulation of
neuronal activity with single cell precision have rapidly advanced in the form
of two-photon optogenetic stimulation. Unlike conventional electrophysiological
approaches or one-photon optogenetics, which inevitably activate many neurons
surrounding the target, two-photon optogenetics can drive hundreds of
specifically targeted neurons simultaneously, with stimulation patterns that
can be flexibly and rapidly reconfigured. In this review, we trace the
development of two-photon optogenetic stimulation, focusing on its progression
toward implementations in large field of view two-photon microscopes capable of
targeted multi neuron control. We highlight three principal strategies: spiral
scanning, temporal focusing, and three-dimensional computer-generated
holography (3D CGH), along with their combinations, which together provide
powerful tools for causal interrogation of neural circuits and behavior.
Finally, we discuss the integration of these optical technologies into brain
machine interfaces (BMIs), emphasizing both their transformative potential and
the technical challenges that must be addressed to realize their broader
impact.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [125] [Quantum-inspired probability metrics define a complete, universal space for statistical learning](https://arxiv.org/abs/2508.21086)
*Logan S. McCarty*

Main category: stat.ML

TL;DR: This paper proposes Quantum Probability Metrics (QPMs), a novel framework for comparing probability distributions, addressing challenges in high-dimensional and non-compact domains where traditional methods like Maximum Mean Discrepancy (MMD) struggle.


<details>
  <summary>Details</summary>
Motivation: Traditional tools like MMD face limitations in high-dimensional and non-compact domains. The authors aim to extend kernel-based methods and address these challenges, providing more sensitive and robust tools for comparing probability distributions.

Method: QPMs are developed by embedding probability measures into the space of quantum states (Hilbert space operators), enhancing sensitivity to distributional differences and providing analytical gradients for applications like learning and optimization.

Result: The proposed QPMs outperform MMD as demonstrated in a generative modeling task. Despite higher computational costs, they significantly enhance performance in analyzing high-dimensional probabilities.

Conclusion: By leveraging quantum mechanics in classical probability, QPMs establish a novel framework for improved sensitivity in distribution comparison, offering a promising direction for future research and applications.

Abstract: Comparing probability distributions is a core challenge across the natural,
social, and computational sciences. Existing methods, such as Maximum Mean
Discrepancy (MMD), struggle in high-dimensional and non-compact domains. Here
we introduce quantum probability metrics (QPMs), derived by embedding
probability measures in the space of quantum states: positive, unit-trace
operators on a Hilbert space. This construction extends kernel-based methods
and overcomes the incompleteness of MMD on non-compact spaces. Viewed as an
integral probability metric (IPM), QPMs have dual functions that uniformly
approximate all bounded, uniformly continuous functions on $\mathbb{R}^n$,
offering enhanced sensitivity to subtle distributional differences in high
dimensions. For empirical distributions, QPMs are readily calculated using
eigenvalue methods, with analytic gradients suited for learning and
optimization. Although computationally more intensive for large sample sizes
($O(n^3)$ vs. $O(n^2)$), QPMs can significantly improve performance as a
drop-in replacement for MMD, as demonstrated in a classic generative modeling
task. By combining the rich mathematical framework of quantum mechanics with
classical probability theory, this approach lays the foundation for powerful
tools to analyze and manipulate probability measures.

</details>


### [126] [Weighted Support Points from Random Measures: An Interpretable Alternative for Generative Modeling](https://arxiv.org/abs/2508.21255)
*Peiqi Zhao,Carlos E. Rodríguez,Ramsés H. Mena,Stephen G. Walker*

Main category: stat.ML

TL;DR: The paper introduces a generative modeling approach using random weighted support points, which summarize data efficiently for tasks like sampling, avoiding complex probabilistic models or neural networks.


<details>
  <summary>Details</summary>
Motivation: To create an efficient, interpretable generative model that avoids the high computational costs and complexity of traditional methods like GANs or DDPMs while preserving essential data structure in samples.

Method: The authors propose the use of random weighted support points generated through a Dirichlet process-inspired weighting scheme. They develop a theoretical framework and an efficient optimization algorithm based on the Convex–Concave Procedure (CCP).

Result: The method successfully generated high-quality and diverse outputs on MNIST and CelebA-HQ datasets, achieving these results with much lower computational costs compared to GANs and DDPMs.

Conclusion: The framework provides a scalable, interpretable, and efficient alternative for generative modeling, demonstrating its ability to produce diverse and data-structure-preserving outputs.

Abstract: Support points summarize a large dataset through a smaller set of
representative points that can be used for data operations, such as Monte Carlo
integration, without requiring access to the full dataset. In this sense,
support points offer a compact yet informative representation of the original
data. We build on this idea to introduce a generative modeling framework based
on random weighted support points, where the randomness arises from a weighting
scheme inspired by the Dirichlet process and the Bayesian bootstrap. The
proposed method generates diverse and interpretable sample sets from a fixed
dataset, without relying on probabilistic modeling assumptions or neural
network architectures. We present the theoretical formulation of the method and
develop an efficient optimization algorithm based on the Convex--Concave
Procedure (CCP). Empirical results on the MNIST and CelebA-HQ datasets show
that our approach produces high-quality and diverse outputs at a fraction of
the computational cost of black-box alternatives such as Generative Adversarial
Networks (GANs) or Denoising Diffusion Probabilistic Models (DDPMs). These
results suggest that random weighted support points offer a principled,
scalable, and interpretable alternative for generative modeling. A key feature
is their ability to produce genuinely interpolative samples that preserve
underlying data structure.

</details>


### [127] [Adaptive generative moment matching networks for improved learning of dependence structures](https://arxiv.org/abs/2508.21531)
*Marius Hofert,Gan Yao*

Main category: stat.ML

TL;DR: This paper introduces Adaptive Generative Moment Matching Networks (AGMMNs), which enhance bandwidth selection in Maximum Mean Discrepancy (MMD) to improve copula random number generation. AGMMNs demonstrate better training and prediction capabilities over traditional methods.


<details>
  <summary>Details</summary>
Motivation: To address limitations in Generative Moment Matching Networks (GMMNs) for modeling complex distributions, particularly in high-dimensional copula random number generators.

Method: The paper proposes adaptively selecting kernel bandwidth based on relative error in training loss, with validation loss used for early stopping.

Result: AGMMNs improve training performance over standard GMMNs and parametric copula models, demonstrated across various applications including high-dimensional copula modeling, financial risk measures, and predictive tasks with datasets like S&P 500 and FTSE 100.

Conclusion: AGMMNs offer significant enhancements in training and prediction compared to GMMNs and parametric models, making them valuable for complex copula modeling tasks.

Abstract: An adaptive bandwidth selection procedure for the mixture kernel in the
maximum mean discrepancy (MMD) for fitting generative moment matching networks
(GMMNs) is introduced, and its ability to improve the learning of copula random
number generators is demonstrated. Based on the relative error of the training
loss, the number of kernels is increased during training; additionally, the
relative error of the validation loss is used as an early stopping criterion.
While training time of such adaptively trained GMMNs (AGMMNs) is similar to
that of GMMNs, training performance is increased significantly in comparison to
GMMNs, which is assessed and shown based on validation MMD trajectories,
samples and validation MMD values. Superiority of AGMMNs over GMMNs, as well as
typical parametric copula models, is demonstrated in terms of three
applications. First, convergence rates of quasi-random versus pseudo-random
samples from high-dimensional copulas are investigated for three functionals of
interest and in dimensions as large as 100 for the first time. Second,
replicated validation MMDs, as well as Monte Carlo and quasi-Monte Carlo
applications based on the expected payoff of a basked call option and the risk
measure expected shortfall as functionals are used to demonstrate the improved
training of AGMMNs over GMMNs for a copula model fitted to the standardized
residuals of the 50 constituents of the S&P 500 index after deGARCHing. Last,
both the latter dataset and 50 constituents of the FTSE~100 are used to
demonstrate that the improved training of AGMMNs over GMMNs and in comparison
to the fitting of classical parametric copula models indeed also translates to
an improved model prediction.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [128] [Data-driven Discovery of Digital Twins in Biomedical Research](https://arxiv.org/abs/2508.21484)
*Clémence Métayer,Annabelle Ballesta,Julien Martinelli*

Main category: q-bio.QM

TL;DR: The paper reviews methodologies for automating the creation of digital twins for biological systems using approaches like symbolic or sparse regression, emphasizing challenges and proposing better frameworks.


<details>
  <summary>Details</summary>
Motivation: Advances in biological data availability make digital twins crucial for guiding drug discovery and personalized medicine, yet developing them remains labor-intensive requiring automated solutions.

Method: The paper evaluates symbolic and sparse regression methods for inferring biological digital twins, examining eight challenges such as noisy data, prior knowledge, and high dimensionality.

Result: Sparse regression, especially Bayesian frameworks, outperformed symbolic regression for reliability in handling biological challenges. Deep learning and large language models show promise but need improvements.

Conclusion: A hybrid modular framework combining mechanistic grounding, Bayesian uncertainty quantification, and deep learning knowledge integration offers the most promise. A benchmarking framework is also proposed.

Abstract: Recent technological advances have expanded the availability of
high-throughput biological datasets, enabling the reliable design of digital
twins of biomedical systems or patients. Such computational tools represent key
reaction networks driving perturbation or drug response and can guide drug
discovery and personalized therapeutics. Yet, their development still relies on
laborious data integration by the human modeler, so that automated approaches
are critically needed. The success of data-driven system discovery in Physics,
rooted in clean datasets and well-defined governing laws, has fueled interest
in applying similar techniques in Biology, which presents unique challenges.
Here, we reviewed methodologies for automatically inferring digital twins from
biological time series, which mostly involve symbolic or sparse regression. We
evaluate algorithms according to eight biological and methodological
challenges, associated to noisy/incomplete data, multiple conditions, prior
knowledge integration, latent variables, high dimensionality, unobserved
variable derivatives, candidate library design, and uncertainty quantification.
Upon these criteria, sparse regression generally outperformed symbolic
regression, particularly when using Bayesian frameworks. We further highlight
the emerging role of deep learning and large language models, which enable
innovative prior knowledge integration, though the reliability and consistency
of such approaches must be improved. While no single method addresses all
challenges, we argue that progress in learning digital twins will come from
hybrid and modular frameworks combining chemical reaction network-based
mechanistic grounding, Bayesian uncertainty quantification, and the generative
and knowledge integration capacities of deep learning. To support their
development, we further propose a benchmarking framework to evaluate methods
across all challenges.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [129] [Harnessing IoT and Generative AI for Weather-Adaptive Learning in Climate Resilience Education](https://arxiv.org/abs/2508.21666)
*Imran S. A. Khan,Emmanuel G. Blanchard,Sébastien George*

Main category: cs.HC

TL;DR: This paper introduces FACTS, a platform combining IoT sensors and Generative AI to enhance climate resilience education through adaptive learning.


<details>
  <summary>Details</summary>
Motivation: To create a system for climate resilience education that is engaging, localized, and adaptive to the learner's environment.

Method: FACTS integrates IoT sensors for real-time atmospheric data and a Generative AI server to provide personalized learning challenges and feedback.

Result: User evaluations showed that FACTS is easy to use and effective in building knowledge about climate resilience.

Conclusion: The integration of IoT and Generative AI in adaptive learning technologies is promising for educational engagement and climate awareness.

Abstract: This paper introduces the Future Atmospheric Conditions Training System
(FACTS), a novel platform that advances climate resilience education through
place-based, adaptive learning experiences. FACTS combines real-time
atmospheric data collected by IoT sensors with curated resources from a
Knowledge Base to dynamically generate localized learning challenges. Learner
responses are analyzed by a Generative AI powered server, which delivers
personalized feedback and adaptive support. Results from a user evaluation
indicate that participants found the system both easy to use and effective for
building knowledge related to climate resilience. These findings suggest that
integrating IoT and Generative AI into atmospherically adaptive learning
technologies holds significant promise for enhancing educational engagement and
fostering climate awareness.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [130] [An Optimistic Gradient Tracking Method for Distributed Minimax Optimization](https://arxiv.org/abs/2508.21431)
*Yan Huang,Jinming Xu,Jiming Chen,Karl Henrik Johansson*

Main category: math.OC

TL;DR: The paper introduces the DOGT and ADOGT distributed algorithms for minimax optimization problems over networks, focusing on improved convergence.


<details>
  <summary>Details</summary>
Motivation: To tackle distributed minimax optimization problems and enhance convergence performance in the presence of heterogeneous local objective functions.

Method: Proposes DOGT and its accelerated variant (ADOGT), leveraging optimistic gradient tracking and Lyapunov-based analysis for convergence guarantees.

Result: Proves linear convergence for DOGT and optimal convergence rate and communication complexity for ADOGT, validated with numerical experiments.

Conclusion: ADOGT achieves optimal convergence and communication rates, robustly solving distributed minimax problems with heterogeneous conditions via accelerated techniques.

Abstract: This paper studies the distributed minimax optimization problem over
networks. To enhance convergence performance, we propose a distributed
optimistic gradient tracking method, termed DOGT, which solves a surrogate
function that captures the similarity between local objective functions to
approximate a centralized optimistic approach locally. Leveraging a
Lyapunov-based analysis, we prove that DOGT achieves linear convergence to the
optimal solution for strongly convex-strongly concave objective functions while
remaining robust to the heterogeneity among them. Moreover, by integrating an
accelerated consensus protocol, the accelerated DOGT (ADOGT) algorithm achieves
an optimal convergence rate of $\mathcal{O} \left( \kappa \log \left( \epsilon
^{-1} \right) \right)$ and communication complexity of $\mathcal{O} \left(
\kappa \log \left( \epsilon ^{-1} \right) /\sqrt{1-\sqrt{\rho _W}} \right)$ for
a suboptimality level of $\epsilon>0$, where $\kappa$ is the condition number
of the objective function and $\rho_W$ is the spectrum gap of the network.
Numerical experiments illustrate the effectiveness of the proposed algorithms.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [131] [The WASM Cloak: Evaluating Browser Fingerprinting Defenses Under WebAssembly based Obfuscation](https://arxiv.org/abs/2508.21219)
*A H M Nazmus Sakib,Mahsin Bin Akram,Joseph Spracklen,Sahan Kalutarage,Raveen Wijewickrama,Igor Bilogrevic,Murtuza Jadliwala*

Main category: cs.CR

TL;DR: The paper assesses the effectiveness of modern defenses against WASM-based obfuscated browser fingerprinting techniques. Research literature detectors showed moderate vulnerability, while practical tools like browser extensions were robust.


<details>
  <summary>Details</summary>
Motivation: WASM's adoption creates a potential blind spot in browser fingerprinting defenses, as its low-level binary format allows adversaries to obfuscate JavaScript and bypass existing defenses.

Method: The authors developed a pipeline to convert real-world JavaScript fingerprinting into WASM, then tested these against defenses from research literature and commercial solutions.

Result: Research-based defenses showed moderate vulnerabilities due to outdated datasets and lack of WASM support, while commercial defenses were effective due to their API-level interception.

Conclusion: There is a gap between academic and practical defense strategies, presenting opportunities for improved detection and evasion methods in future WASM-based attacks.

Abstract: Browser fingerprinting defenses have historically focused on detecting
JavaScript(JS)-based tracking techniques. However, the widespread adoption of
WebAssembly (WASM) introduces a potential blind spot, as adversaries can
convert JS to WASM's low-level binary format to obfuscate malicious logic. This
paper presents the first systematic evaluation of how such WASM-based
obfuscation impacts the robustness of modern fingerprinting defenses. We
develop an automated pipeline that translates real-world JS fingerprinting
scripts into functional WASM-obfuscated variants and test them against two
classes of defenses: state-of-the-art detectors in research literature and
commercial, in-browser tools. Our findings reveal a notable divergence:
detectors proposed in the research literature that rely on feature-based
analysis of source code show moderate vulnerability, stemming from outdated
datasets or a lack of WASM compatibility. In contrast, defenses such as browser
extensions and native browser features remained completely effective, as their
API-level interception is agnostic to the script's underlying implementation.
These results highlight a gap between academic and practical defense strategies
and offer insights into strengthening detection approaches against WASM-based
obfuscation, while also revealing opportunities for more evasive techniques in
future attacks.

</details>


### [132] [Locus: Agentic Predicate Synthesis for Directed Fuzzing](https://arxiv.org/abs/2508.21302)
*Jie Zhu,Chihao Shen,Ziyang Li,Jiahao Yu,Yizheng Chen,Kexin Pei*

Main category: cs.CR

TL;DR: Locus introduces a framework that uses synthesized predicates to guide directed fuzzing, leading to significant efficiency improvements in discovering vulnerabilities.


<details>
  <summary>Details</summary>
Motivation: Directed fuzzing has applications in debugging and security but faces challenges due to deep target states and large search spaces. Current methods rely on imprecise branch distances or non-generalizable manual constraints.

Method: Locus creates semantically meaningful intermediate predicates that act as milestones toward target states. Using program analysis tools, it synthesizes and refines these predicates, ensuring correctness with symbolic execution.

Result: Locus improves fuzzing efficiency by 41.6x on average and has discovered 8 previously unpatched bugs, one of which has a draft patch.

Conclusion: Locus successfully addresses inefficiencies in directed fuzzing by providing automated and generalizable predicate synthesis, improving vulnerability discovery in diverse programs.

Abstract: Directed fuzzing aims to find program inputs that lead to specified target
program states. It has broad applications, such as debugging system crashes,
confirming reported bugs, and generating exploits for potential
vulnerabilities. This task is inherently challenging because target states are
often deeply nested in the program, while the search space manifested by
numerous possible program inputs is prohibitively large. Existing approaches
rely on branch distances or manually-specified constraints to guide the search;
however, the branches alone are often insufficient to precisely characterize
progress toward reaching the target states, while the manually specified
constraints are often tailored for specific bug types and thus difficult to
generalize to diverse target states and programs.
  We present Locus, a novel framework to improve the efficiency of directed
fuzzing. Our key insight is to synthesize predicates to capture fuzzing
progress as semantically meaningful intermediate states, serving as milestones
towards reaching the target states. When used to instrument the program under
fuzzing, they can reject executions unlikely to reach the target states, while
providing additional coverage guidance. To automate this task and generalize to
diverse programs, Locus features an agentic framework with program analysis
tools to synthesize and iteratively refine the candidate predicates, while
ensuring the predicates strictly relax the target states to prevent false
rejections via symbolic execution. Our evaluation shows that Locus
substantially improves the efficiency of eight state-of-the-art fuzzers in
discovering real-world vulnerabilities, achieving an average speedup of 41.6x.
So far, Locus has found eight previously unpatched bugs, with one already
acknowledged with a draft patch.

</details>


### [133] [Risks and Compliance with the EU's Core Cyber Security Legislation](https://arxiv.org/abs/2508.21386)
*Jukka Ruohonen,Jesper Løffler Nielsen,Jakub Skórczynski*

Main category: cs.CR

TL;DR: The paper examines how risks are framed within five major EU cyber security laws, investigating their convergence and divergence, and highlights notable features and gaps in these frameworks.


<details>
  <summary>Details</summary>
Motivation: To analyze how risk concepts are integrated and framed within the EU's new cyber security legislation, and to determine if the legislative acts are aligned or differ in their approach to risk.

Method: The study uses qualitative legal interpretation and taxonomy-building as its methodological framework.

Result: The analysis reveals widespread coverage of various cyber security risks across the five acts, including technical, organizational, human risks, and risks unrelated to human actions. However, gaps remain regarding acceptable risks, non-probabilistic risks, and residual risks.

Conclusion: The EU has advanced their risk-based regulatory approach within cyber security law. This expansion has increased complexity and compliance burdens, but the paper provides practical suggestions for managing compliance and further research.

Abstract: The European Union (EU) has long favored a risk-based approach to regulation.
Such an approach is also used in recent cyber security legislation enacted in
the EU. Risks are also inherently related to compliance with the new
legislation. Objective: The paper investigates how risks are framed in the EU's
five core cyber security legislative acts, whether the framings indicate
convergence or divergence between the acts and their risk concepts, and what
qualifying words and terms are used when describing the legal notions of risks.
Method : The paper's methodology is based on qualitative legal interpretation
and taxonomy-building. Results: The five acts have an encompassing coverage of
different cyber security risks, including but not limited to risks related to
technical, organizational, and human security as well as those not originating
from man-made actions. Both technical aspects and assets are used to frame the
legal risk notions in many of the legislative acts. A threat-centric viewpoint
is also present in one of the acts. Notable gaps are related to acceptable
risks, non-probabilistic risks, and residual risks. Conclusion: The EU's new
cyber security legislation has significantly extended the risk-based approach
to regulations. At the same time, complexity and compliance burden have
increased. With this point in mind, the paper concludes with a few practical
takeaways about means to deal with compliance and research it.

</details>


### [134] [An Empirical Study of Vulnerable Package Dependencies in LLM Repositories](https://arxiv.org/abs/2508.21417)
*Shuhan Liu,Xing Hu,Xin Xia,David Lo,Xiaohu Yang*

Main category: cs.CR

TL;DR: This paper investigates vulnerabilities in the dependency supply chain of 52 open-source large language models (LLMs), assessing the risks due to third-party code dependencies and comparing them to Python ecosystem vulnerabilities.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the gap in understanding security risks within the LLM dependency supply chain, an area often overlooked compared to model-level security threats.

Method: The study analyzes vulnerabilities in the third-party dependencies of 52 open-source LLMs, explores maintainer activities for managing these vulnerabilities, and compares LLM ecosystem vulnerabilities to those in the Python ecosystem.

Result: The findings reveal that half of LLM vulnerabilities remain undisclosed for over 56.2 months and 75.8% of LLMs rely on vulnerable third-party dependencies.

Conclusion: The study highlights the importance of addressing security risks in the LLM dependency supply chain and provides actionable insights for enhancing its security.

Abstract: Large language models (LLMs) have developed rapidly in recent years,
revolutionizing various fields. Despite their widespread success, LLMs heavily
rely on external code dependencies from package management systems, creating a
complex and interconnected LLM dependency supply chain. Vulnerabilities in
dependencies can expose LLMs to security risks. While existing research
predominantly focuses on model-level security threats, vulnerabilities within
the LLM dependency supply chain have been overlooked. To fill this gap, we
conducted an empirical analysis of 52 open-source LLMs, examining their
third-party dependencies and associated vulnerabilities. We then explored
activities within the LLM repositories to understand how maintainers manage
third-party vulnerabilities in practice. Finally, we compared third-party
dependency vulnerabilities in the LLM ecosystem to those in the Python
ecosystem. Our results show that half of the vulnerabilities in the LLM
ecosystem remain undisclosed for more than 56.2 months, significantly longer
than those in the Python ecosystem. Additionally, 75.8% of LLMs include
vulnerable dependencies in their configuration files. This study advances the
understanding of LLM supply chain risks, provides insights for practitioners,
and highlights potential directions for improving the security of the LLM
supply chain.

</details>


### [135] [RepoMark: A Code Usage Auditing Framework for Code Large Language Models](https://arxiv.org/abs/2508.21432)
*Wenjie Qu,Yuguang Zhou,Bo Wang,Wengrui Zheng,Yuexin Li,Jinyuan Jia,Jiaheng Zhang*

Main category: cs.CR

TL;DR: The paper introduces RepoMark, a framework for auditing the use of code repositories in training Large Language Models (LLMs), addressing ethical and legal concerns related to licensing and transparency.


<details>
  <summary>Details</summary>
Motivation: The training of code-focused LLMs on open-source repositories raises concerns about data authorization and compliance with open-source licenses, compounded by a lack of data collection transparency.

Method: RepoMark embeds data marks into code files by generating semantically equivalent variants. A ranking-based hypothesis test is then used during detection to verify if the code was used in training, with guarantees for semantic preservation and low false detection rates.

Result: RepoMark achieves a detection success rate over 90% on small code repositories with a false detection rate of 5%, significantly outperforming existing data marking techniques with sub-55% accuracy.

Conclusion: RepoMark is a robust and efficient solution that enhances transparency in LLM training, protecting the rights of code repository owners and addressing legal and ethical concerns.

Abstract: The rapid development of Large Language Models (LLMs) for code generation has
transformed software development by automating coding tasks with unprecedented
efficiency.
  However, the training of these models on open-source code repositories (e.g.,
from GitHub) raises critical ethical and legal concerns, particularly regarding
data authorization and open-source license compliance. Developers are
increasingly questioning whether model trainers have obtained proper
authorization before using repositories for training, especially given the lack
of transparency in data collection.
  To address these concerns, we propose a novel data marking framework RepoMark
to audit the data usage of code LLMs. Our method enables repository owners to
verify whether their code has been used in training, while ensuring semantic
preservation, imperceptibility, and theoretical false detection rate (FDR)
guarantees. By generating multiple semantically equivalent code variants,
RepoMark introduces data marks into the code files, and during detection,
RepoMark leverages a novel ranking-based hypothesis test to detect memorization
within the model. Compared to prior data auditing approaches, RepoMark
significantly enhances sample efficiency, allowing effective auditing even when
the user's repository possesses only a small number of code files.
  Experiments demonstrate that RepoMark achieves a detection success rate over
90\% on small code repositories under a strict FDR guarantee of 5\%. This
represents a significant advancement over existing data marking techniques, all
of which only achieve accuracy below 55\% under identical settings. This
further validates RepoMark as a robust, theoretically sound, and promising
solution for enhancing transparency in code LLM training, which can safeguard
the rights of repository owners.

</details>


### [136] [Detecting Stealthy Data Poisoning Attacks in AI Code Generators](https://arxiv.org/abs/2508.21636)
*Cristina Improta*

Main category: cs.CR

TL;DR: This paper examines the vulnerabilities of deep learning models used for natural language-to-code generation against stealthy data poisoning attacks. Existing detection methods fail to effectively identify triggerless poisoning.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the security risks posed by subtle, triggerless data poisoning attacks that compromise AI models for code generation. Detection methods are struggling under this updated threat model.

Method: The authors target three DL models (CodeBERT, CodeT5+, AST-T5) using stealthy data poisoning and evaluate defenses including spectral signatures, activation clustering, and static analysis for detecting malicious samples.

Result: The study finds existing defenses ineffective: representation-based approaches cannot isolate poisoned data, and static analysis shows high rates of false positives and negatives.

Conclusion: Current defense techniques are insufficient for detecting triggerless data poisoning attacks, emphasizing the need for more robust approaches in AI-driven code generation.

Abstract: Deep learning (DL) models for natural language-to-code generation have become
integral to modern software development pipelines. However, their heavy
reliance on large amounts of data, often collected from unsanitized online
sources, exposes them to data poisoning attacks, where adversaries inject
malicious samples to subtly bias model behavior. Recent targeted attacks
silently replace secure code with semantically equivalent but vulnerable
implementations without relying on explicit triggers to launch the attack,
making it especially hard for detection methods to distinguish clean from
poisoned samples. We present a systematic study on the effectiveness of
existing poisoning detection methods under this stealthy threat model.
Specifically, we perform targeted poisoning on three DL models (CodeBERT,
CodeT5+, AST-T5), and evaluate spectral signatures analysis, activation
clustering, and static analysis as defenses. Our results show that all methods
struggle to detect triggerless poisoning, with representation-based approaches
failing to isolate poisoned samples and static analysis suffering false
positives and false negatives, highlighting the need for more robust,
trigger-independent defenses for AI-assisted code generation.

</details>
