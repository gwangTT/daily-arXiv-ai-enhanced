{"id": "2509.19645", "pdf": "https://arxiv.org/pdf/2509.19645", "abs": "https://arxiv.org/abs/2509.19645", "authors": ["Youpeng Zhao", "Jinpeng LV", "Di Wu", "Jun Wang", "Christopher Gooley"], "title": "Are We Scaling the Right Thing? A System Perspective on Test-Time Scaling", "categories": ["cs.PF", "cs.AI"], "comment": null, "summary": "Test-time scaling (TTS) has recently emerged as a promising direction to\nexploit the hidden reasoning capabilities of pre-trained large language models\n(LLMs). However, existing scaling methods narrowly focus on the compute-optimal\nPareto-frontier, ignoring the simple fact that compute-optimal is not always\nsystem-optimal. In this work, we propose a system-driven perspective on TTS,\nanalyzing how reasoning models scale against practical metrics, such as latency\nand cost-per-token. By evaluating the impact of popular optimizations such as\ntensor parallelism and speculative decoding, our preliminary analysis reveals\nthe limitations of current methods and calls for a paradigm shift toward\nholistic, system-aware evaluations that capture the true essence of scaling\nlaws at inference time.", "AI": {"tldr": "This paper critiques existing test-time scaling (TTS) approaches for large language models (LLMs) and advocates for a system-driven analysis considering practical performance metrics beyond compute optimality.", "motivation": "To address the gap in existing test-time scaling analyses that ignore system-level metrics like latency and cost, focusing instead on compute-optimal but possibly impractical solutions.", "method": "The authors evaluate the impact of optimizations, such as tensor parallelism and speculative decoding, while considering practical system metrics like latency and cost-per-token.", "result": "The study exposes the limitations of existing TTS methods, particularly their lack of system-awareness, and underscores the need for a holistic approach to scaling evaluations.", "conclusion": "The work emphasizes the necessity of shifting from compute-centric to system-aware evaluations to better understand scaling laws and optimize real-world LLM inference."}}
{"id": "2509.19383", "pdf": "https://arxiv.org/pdf/2509.19383", "abs": "https://arxiv.org/abs/2509.19383", "authors": ["Qianqian Li", "Hua Li", "Shiya Hao", "Lintao Li", "Xiaoming Dai"], "title": "Impact of RHIs and ipSIC on Active RIS-NOMA Systems with Low-Precision ADCs", "categories": ["eess.SP", "cs.IT", "cs.PF", "math.IT"], "comment": null, "summary": "This study evaluates the performance of an active reconfigurable intelligent\nsurface (ARIS)-assisted non-orthogonal multiple access (NOMA) system employing\nlow-precision analog-to-digital converters (ADCs). Analytical approximations\nfor the outage probability (OP) are derived, considering residual hardware\nimpairments (RHIs) and imperfect successive interference cancellation (ipSIC).\nAdditionally, we analyze the asymptotic OP, system throughput, and diversity\norder at high signal-to-noise ratios (SNRs). Simulation results demonstrate\nthat the proposed quantized ARIS-NOMA system outperforms its passive\ncounterpart (PRIS-NOMA), achieving lower OP and higher throughput with reduced\ntransmit power requirements and fewer reflecting elements. Moreover, the outage\nperformance of both quantized ARIS-NOMA and PRIS-NOMA systems demonstrates\nsignificant improvement as the number of reflecting elements increases. The\nnegative impacts of low-precision ADCs can be effectively mitigated by\noptimizing transmit power and scaling the number of reflecting elements.", "AI": {"tldr": "A study on ARIS-assisted NOMA system with low-precision ADCs, analyzing outage probability and system performance under impairments.", "motivation": "To improve communication performance for systems using low-precision ADCs and intelligent surfaces, addressing weaknesses like hardware impairments and imperfect interference cancellation.", "method": "Analytical approximation of outage probability, high-SNR asymptotic analysis, and simulation of system throughput and diversity order.", "result": "ARIS-NOMA outperforms PRIS-NOMA by lowering outage probability and increasing throughput with optimized power usage and reflecting elements.", "conclusion": "ARIS-NOMA offers a viable solution for mitigating low-precision ADC impacts while achieving better performance compared to passive systems."}}
{"id": "2509.19618", "pdf": "https://arxiv.org/pdf/2509.19618", "abs": "https://arxiv.org/abs/2509.19618", "authors": ["Jack Dongarra", "Piotr Luszczek"], "title": "HPL-MxP Benchmark: Mixed-Precision Algorithms, Iterative Refinement, and Scalable Data Generation", "categories": ["math.NA", "cs.NA", "cs.PF"], "comment": null, "summary": "We present a mixed-precision benchmark called HPL-MxP that uses both a\nlower-precision LU factorization with a non-stationary iterative refinement\nbased on GMRES. We evaluate the numerical stability of one of the methods of\ngenerating the input matrix in a scalable fashion and show how the diagonal\nscaling affects the solution quality in terms of the backward-error. Some of\nthe performance results at large scale supercomputing installations produced\nExascale-level compute throughput numbers thus proving the viability of the\nproposed benchmark for evaluating such machines. We also present the potential\nof the benchmark to continue increasing its use with proliferation of hardware\naccelerators for AI workloads whose reliable evaluation continues to pose a\nparticular challenge for the users.", "AI": {"tldr": "HPL-MxP benchmark combines mixed-precision LU factorization and GMRES-based iterative refinement to evaluate numerical stability and supercomputing performance.", "motivation": "To develop a benchmark capable of assessing Exascale-level throughput and reliable evaluation of AI hardware accelerators.", "method": "Introduce mixed-precision LU factorization, GMRES iterative refinement, scalable matrix generation, and diagonal scaling analysis.", "result": "Achieved Exascale-level compute throughput; demonstrated impact of diagonal scaling on solution quality.", "conclusion": "HPL-MxP is viable for benchmarking supercomputers and offers potential for increased adoption in AI hardware evaluation."}}
{"id": "2509.19790", "pdf": "https://arxiv.org/pdf/2509.19790", "abs": "https://arxiv.org/abs/2509.19790", "authors": ["Anthony Faure-Gignoux", "Kevin Delmas", "Adrien Gauffriau", "Claire Pagetti"], "title": "Open-source Stand-Alone Versatile Tensor Accelerator", "categories": ["cs.AR"], "comment": null, "summary": "Machine Learning (ML) applications demand significant computational\nresources, posing challenges for safety-critical domains like aeronautics. The\nVersatile Tensor Accelerator (VTA) is a promising FPGA-based solution, but its\nadoption was hindered by its dependency on the TVM compiler and by other code\nnon-compliant with certification requirements. This paper presents an\nopen-source, standalone Python compiler pipeline for the VTA, developed from\nscratch and designed with certification requirements, modularity, and\nextensibility in mind. The compiler's effectiveness is demonstrated by\ncompiling and executing LeNet-5 Convolutional Neural Network (CNN) using the\nVTA simulators, and preliminary results indicate a strong potential for scaling\nits capabilities to larger CNN architectures. All contributions are publicly\navailable.", "AI": {"tldr": "This paper presents a standalone, open-source Python compiler for the FPGA-based Versatile Tensor Accelerator (VTA), addressing certification compliance and modularity issues.", "motivation": "The demand for computational efficiency in ML applications, especially in safety-critical domains like aeronautics, necessitates solutions compliant with certification standards. Existing solutions like VTA depend on the TVM compiler, which lacks compliance.", "method": "Researchers developed a standalone Python compiler pipeline, emphasizing certification requirements, modularity, and extensibility. They tested its functionality using a LeNet-5 CNN model on VTA simulators.", "result": "The compiler successfully executed LeNet-5 on VTA simulators, showing promising preliminary results for scaling to larger CNN architectures.", "conclusion": "The standalone compiler advances the usability of VTA by addressing compliance challenges and demonstrating scalability, with all contributions made publicly available."}}
{"id": "2509.19701", "pdf": "https://arxiv.org/pdf/2509.19701", "abs": "https://arxiv.org/abs/2509.19701", "authors": ["Akash Poptani", "Alireza Khadem", "Scott Mahlke", "Jonah Miller", "Joshua Dolence", "Reetuparna Das"], "title": "Characterizing Adaptive Mesh Refinement on Heterogeneous Platforms with Parthenon-VIBE", "categories": ["cs.DC", "cs.PF"], "comment": "Accepted to appear at IISWC 2025", "summary": "Hero-class HPC simulations rely on Adaptive Mesh Refinement (AMR) to reduce\ncompute and memory demands while maintaining accuracy. This work analyzes the\nperformance of Parthenon, a block-structured AMR benchmark, on CPU-GPU systems.\nWe show that smaller mesh blocks and deeper AMR levels degrade GPU performance\ndue to increased communication, serial overheads, and inefficient GPU\nutilization. Through detailed profiling, we identify inefficiencies, low\noccupancy, and memory access bottlenecks. We further analyze rank scalability\nand memory constraints, and propose optimizations to improve GPU throughput and\nreduce memory footprint. Our insights can inform future AMR deployments on\nDepartment of Energy's upcoming heterogeneous supercomputers.", "AI": {"tldr": "This paper evaluates the performance challenges of the Parthenon AMR benchmark on CPU-GPU systems and offers optimizations to enhance efficiency.", "motivation": "Hero-class HPC simulations use Adaptive Mesh Refinement to balance computational and memory requirements with accurate results. Analyzing its performance on modern heterogeneous systems is critical for effective deployment.", "method": "The authors analyzed Parthenon's behavior on CPU-GPU systems via profiling tools, examined communication bottlenecks and serial overheads, and proposed optimizations to reduce inefficiencies.", "result": "Smaller mesh blocks and deeper AMR levels were found to degrade GPU performance due to communication overheads, serial inefficiencies, and memory bottlenecks. Profiling identified low occupancy and inefficiencies in GPU memory access.", "conclusion": "Insights and proposed optimizations from this study aim to improve GPU efficiency and inform future Adaptive Mesh Refinement implementations on heterogeneous supercomputers."}}
{"id": "2509.19873", "pdf": "https://arxiv.org/pdf/2509.19873", "abs": "https://arxiv.org/abs/2509.19873", "authors": ["Linfeng Zhong", "Songqiang Xu", "Huifeng Wen", "Tong Xie", "Qingyu Guo", "Yuan Wang", "Meng Li"], "title": "SpecMamba: Accelerating Mamba Inference on FPGA with Speculative Decoding", "categories": ["cs.AR"], "comment": "Accepted by ICCAD'25", "summary": "The growing demand for efficient long-sequence modeling on edge devices has\npropelled widespread adoption of State Space Models (SSMs) like Mamba, due to\ntheir superior computational efficiency and scalability. As its autoregressive\ngeneration process remains memory-bound, speculative decoding has been proposed\nthat incorporates draft model generation and target model verification.\nHowever, directly applying speculative decoding to SSMs faces three key\nchallenges: (1) hidden state backtracking difficulties, (2) tree-based parallel\nverification incompatibility, and (3) hardware workload mismatch. To address\nthese challenges, we propose SpecMamba, the first FPGA-based accelerator for\nMamba with speculative decoding, which features system, algorithm, and hardware\nco-design. At the system level, we present a memory-aware hybrid backtracking\nstrategy to coordinate both models. At the algorithm level, we propose\nfirst-in-first-out (FIFO)-based tree verification with tiling to minimize\nmemory access. At the hardware level, we customize a dataflow that computes\nlinear layers in parallel and SSM layers in series to enable maximal\noverlapping. Implemented on AMD FPGA platforms (VHK158 and VCK190), SpecMamba\nachieves a 2.27x speedup over GPU baselines and a 2.85x improvement compared to\nprior FPGA solutions, while demonstrating 5.41x and 1.26x higher energy\nefficiency, respectively.", "AI": {"tldr": "SpecMamba introduces an FPGA-based accelerator for the Mamba SSM architecture, resolving speculative decoding inefficiencies and achieving notable speed and energy efficiency improvements over GPU and previous FPGA solutions.", "motivation": "To address the growing demand for efficient long-sequence modeling on edge devices and deficiencies in SSMs' speculative decoding processes.", "method": "System, algorithm, and hardware co-design featuring hybrid backtracking, FIFO-based tree verification, and optimized dataflows for FPGA-based implementation.", "result": "SpecMamba outperforms GPU and prior FPGA implementations, achieving 2.27x speedup, 2.85x improvement, and up to 5.41x energy efficiency gains.", "conclusion": "Through innovative design adaptations, SpecMamba significantly enhances the speed and energy efficiency of SSM-based long-sequence modeling, bridging hardware and computational constraints."}}
{"id": "2509.19456", "pdf": "https://arxiv.org/pdf/2509.19456", "abs": "https://arxiv.org/abs/2509.19456", "authors": ["Krisztian Balog", "ChengXiang Zhai"], "title": "The Indispensable Role of User Simulation in the Pursuit of AGI", "categories": ["cs.AI"], "comment": "Accepted for publication in Communications of the ACM", "summary": "Progress toward Artificial General Intelligence (AGI) faces significant\nbottlenecks, particularly in rigorously evaluating complex interactive systems\nand acquiring the vast interaction data needed for training adaptive agents.\nThis paper posits that user simulation -- creating computational agents that\nmimic human interaction with AI systems -- is not merely a useful tool, but is\na critical catalyst required to overcome these bottlenecks and accelerate AGI\ndevelopment. We argue that realistic simulators provide the necessary\nenvironments for scalable evaluation, data generation for interactive learning,\nand fostering the adaptive capabilities central to AGI. Therefore, research\ninto user simulation technology and intelligent task agents are deeply\nsynergistic and must advance hand-in-hand. This article elaborates on the\ncritical role of user simulation for AGI, explores the interdisciplinary nature\nof building realistic simulators, identifies key challenges including those\nposed by large language models, and proposes a future research agenda.", "AI": {"tldr": "The paper highlights the importance of user simulation as a critical tool to evaluate AI systems, generate training data, and speed up AGI development.", "motivation": "The study aims to address bottlenecks in evaluating interactive AI systems and generating interaction data essential for AGI progress.", "method": "It advocates using realistic user simulation to enable scalable evaluation and adaptive learning environments.", "result": "The paper identifies user simulation as essential for AGI, highlights interdisciplinary challenges, and proposes a targeted research agenda.", "conclusion": "Advancing both user simulation technology and intelligent task agents is essential to surmount AGI development challenges and harness their synergistic benefits."}}
{"id": "2509.19478", "pdf": "https://arxiv.org/pdf/2509.19478", "abs": "https://arxiv.org/abs/2509.19478", "authors": ["Ziwei Wang", "Cong Wu", "Paolo Tasca"], "title": "Investigating Sharding Advancements, Methodologies, and Adoption Potential in Hedera", "categories": ["cs.DC"], "comment": null, "summary": "Sharding has emerged as a critical solution to address the scalability\nchallenges faced by blockchain networks, enabling them to achieve higher\ntransaction throughput, reduced latency, and optimized resource usage. This\npaper investigates the advancements, methodologies, and adoption potential of\nsharding in the context of Hedera, a distributed ledger technology known for\nits unique Gossip about Gossip protocol and asynchronous Byzantine Fault\nTolerance (ABFT). We explore various academic and industrial sharding\ntechniques, emphasizing their benefits and trade-offs. Building on these\ninsights, we propose a hybrid sharding solution for Hedera that partitions the\nnetwork into local and global committees, facilitating efficient cross-shard\ntransactions and ensuring robust security through dynamic reconfiguration. Our\nanalysis highlights significant reductions in storage and communication\noverhead, improved scalability, and enhanced fault tolerance, demonstrating the\nfeasibility and advantages of integrating sharding into Hedera's architecture.", "AI": {"tldr": "The paper explores sharding solutions for blockchain scalability, focusing on Hedera and proposing a hybrid sharding model to improve performance and security.", "motivation": "To address the scalability challenges in blockchain networks by evaluating sharding methods and their applicability to Hedera's architecture.", "method": "The study reviews academic and industrial sharding techniques and proposes a hybrid solution combining local and global committees with dynamic reconfiguration for efficiency and security.", "result": "The proposed model demonstrated reduced storage and communication overhead, improved scalability, and enhanced fault tolerance for Hedera.", "conclusion": "Integrating sharding into Hedera's architecture is feasible and offers significant advantages in scalability, efficiency, and security."}}
{"id": "2509.19607", "pdf": "https://arxiv.org/pdf/2509.19607", "abs": "https://arxiv.org/abs/2509.19607", "authors": ["William J. Bowman"], "title": "Macro-embedding Compiler Intermediate Languages in Racket", "categories": ["cs.PL"], "comment": null, "summary": "We present the design and implementation of a macro-embedding of a family of\ncompiler intermediate languages, from a Scheme-like language to x86-64, into\nRacket. This embedding is used as part of a testing framework for a compilers\ncourse to derive interpreters for all the intermediate languages. The embedding\nimplements features including safe, functional abstractions as well as unsafe\nassembly features, and the interactions between the two at various intermediate\nstages.\n  This paper aims to demonstrate language-oriented techniques and abstractions\nfor implementing (1) a large family of languages and (2) interoperability\nbetween low- and high-level languages. The primary strength of this approach is\nthe high degree of code reuse and interoperability compared to implementing\neach interpreter separately. The design emphasizes modularity and\ncompositionality of an open set of language features by local macro expansion\ninto a single host language, rather than implementing a language pre-defined by\na closed set of features. This enables reuse from both the host language\n(Racket) and between intermediate languages, and enables interoperability\nbetween high- and low-level features, simplifying development of the\nintermediate language semantics. It also facilitates extending or redefining\nindividual language features in intermediate languages, and exposing multiple\ninterfaces to the embedded languages.", "AI": {"tldr": "The paper introduces a macro-embedding framework in Racket for simplifying the creation and testing of compilers' intermediate languages, facilitating code reuse and modularity.", "motivation": "To address challenges in implementing a family of intermediate languages with interoperability between high- and low-level languages while enabling code reuse and modularity.", "method": "The authors designed and implemented a macro-embedding approach in Racket for the intermediate languages, leveraging local macro expansions and interoperability features.", "result": "The framework enabled significant code reuse and simplified the implementation of language semantics, interoperability, and extensions within intermediate languages.", "conclusion": "This approach showcases the advantages of language-oriented programming for creating modular and interoperable compiler infrastructures, reducing development complexity."}}
{"id": "2509.19351", "pdf": "https://arxiv.org/pdf/2509.19351", "abs": "https://arxiv.org/abs/2509.19351", "authors": ["Katherine Xie", "Gabriel Koch Ocker"], "title": "The Impact of Structural Changes on Learning Capacity in the Fly Olfactory Neural Circuit", "categories": ["q-bio.NC", "cs.AI", "cs.LG", "cs.NE"], "comment": null, "summary": "The Drosophila mushroom body (MB) is known to be involved in olfactory\nlearning and memory; the synaptic plasticity of the Kenyon cell (KC) to\nmushroom body output neuron (MBON) synapses plays a key role in the learning\nprocess. Previous research has focused on projection neuron (PN) to Kenyon cell\n(KC) connectivity within the MB; we examine how perturbations to the mushroom\nbody circuit structure and changes in connectivity, specifically within the KC\nto mushroom body output neuron (MBON) neural circuit, affect the MBONs' ability\nto distinguish between odor classes. We constructed a neural network that\nincorporates the connectivity between PNs, KCs, and MBONs. To train our model,\nwe generated ten artificial input classes, which represent the projection\nneuron activity in response to different odors. We collected data on the number\nof KC-to-MBON connections, MBON error rates, and KC-to-MBON synaptic weights,\namong other metrics. We observed that MBONs with very few presynaptic KCs\nconsistently performed worse than others in the odor classification task. The\ndevelopmental types of KCs also played a significant role in each MBON's\noutput. We performed random and targeted KC ablation and observed that ablating\ndevelopmentally mature KCs had a greater negative impact on MBONs' learning\ncapacity than ablating immature KCs. Random and targeted pruning of KC-MBON\nsynaptic connections yielded results largely consistent with the ablation\nexperiments. To further explore the various types of KCs, we also performed\nrewiring experiments in the PN to KC circuit. Our study furthers our\nunderstanding of olfactory neuroplasticity and provides important clues to\nunderstanding learning and memory in general. Understanding how the olfactory\ncircuits process and learn can also have potential applications in artificial\nintelligence and treatments for neurodegenerative diseases.", "AI": {"tldr": "The study investigates the role of specific neural circuits in the Drosophila mushroom body in olfactory learning and classification, emphasizing Kenyon cells' connection to MB output neurons.", "motivation": "To examine how structural changes and connectivity within the Kenyon cell (KC) to mushroom body output neuron (MBON) circuit impact the ability of MBONs to classify odors.", "method": "The researchers built a neural network model reflecting the circuitry between projection neurons (PNs), KCs, and MBONs, trained with ten artificial odor input classes, and conducted experiments with KC ablation, pruning, and rewiring.", "result": "MBONs connected to fewer KCs showed worse odor classification, and ablation of mature KCs had a bigger impact on learning capacity than immature ones; findings from pruning experiments were consistent with ablation studies.", "conclusion": "The research enhances understanding of olfactory neuroplasticity and learning processes, with applications in artificial intelligence and potential implications for treating neurodegenerative diseases."}}
{"id": "2509.19459", "pdf": "https://arxiv.org/pdf/2509.19459", "abs": "https://arxiv.org/abs/2509.19459", "authors": ["Yutong Guo", "Weiyu Luo", "Brian Demsky"], "title": "Automated Insertion of Flushes and Fences for Persistency", "categories": ["cs.SE", "cs.PL"], "comment": null, "summary": "CXL shared memory and persistent memory allow the contents of memory to\npersist beyond crashes. Stores to persistent or CXL memory are typically not\nimmediately made persistent; developers must manually flush the corresponding\ncache lines to force the data to be written to the underlying storage.\nCorrectly using flush and fence operations is known to be challenging. While\nstate-of-the-art tools can find missing flush instructions, they often require\nbug-revealing test cases. No existing tools can ensure the absence of missing\nflush bugs.\n  In this paper, we present PMRobust, a compiler that automatically inserts\nflush and fence operations to ensure that code using persistent memory is free\nfrom missing flush and fence bugs. PMRobust employs a novel static analysis\nwith optimizations that target newly allocated objects. We have evaluated\nPMRobust on persistent memory libraries and several persistent memory data\nstructures and measured a geometric mean overhead of 0.26% relative to the\noriginal benchmarks with hand-placed flush and fence operations.", "AI": {"tldr": "PMRobust is a compiler that ensures code using persistent memory is free of missing flush and fence bugs by automatically inserting these operations, achieving minimal performance overhead.", "motivation": "Manual handling of flush and fence operations in persistent memory is error-prone. Developers often struggle to ensure data persistence beyond crashes.", "method": "PMRobust employs innovative static analysis with optimizations tailored for newly allocated objects to automatically insert missing flush and fence operations.", "result": "The evaluation of PMRobust on persistent memory libraries and data structures revealed a minimal overhead of 0.26% compared to manually written flush and fence operations.", "conclusion": "PMRobust effectively eliminates missing flush and fence bugs with minimal performance impact, simplifying developers' effort to achieve persistence in memory systems."}}
{"id": "2509.19314", "pdf": "https://arxiv.org/pdf/2509.19314", "abs": "https://arxiv.org/abs/2509.19314", "authors": ["Sirui Wu", "Daijin Yang"], "title": "Automated Item Neutralization for Non-Cognitive Scales: A Large Language Model Approach to Reducing Social-Desirability Bias", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": "Accepted for publication in NCME-AIME 2025", "summary": "This study evaluates item neutralization assisted by the large language model\n(LLM) to reduce social desirability bias in personality assessment. GPT-o3 was\nused to rewrite the International Personality Item Pool Big Five Measure\n(IPIP-BFM-50), and 203 participants completed either the original or\nneutralized form along with the Marlowe-Crowne Social Desirability Scale. The\nresults showed preserved reliability and a five-factor structure, with gains in\nConscientiousness and declines in Agreeableness and Openness. The correlations\nwith social desirability decreased for several items, but inconsistently.\nConfigural invariance held, though metric and scalar invariance failed.\nFindings support AI neutralization as a potential but imperfect bias-reduction\nmethod.", "AI": {"tldr": "The study explores using GPT-3 to rewrite personality test items for reducing social desirability bias. Results show some promise but highlight inconsistencies, preserving reliability but affecting specific traits and social desirability correlations.", "motivation": "To evaluate whether large language models (LLMs) can assist in reducing social desirability bias in personality assessments.", "method": "The International Personality Item Pool Big Five Measure (IPIP-BFM-50) was rewritten using GPT-3 to neutralize bias, and 203 participants completed the original or revised version, alongside a social desirability measure.", "result": "The reliability and five-factor structure were preserved, with changes in trait scores, inconsistent reductions in social desirability correlations, and failure of metric and scalar invariance.", "conclusion": "AI-based item neutralization shows potential for reducing bias, but its effectiveness remains imperfect."}}
{"id": "2509.19452", "pdf": "https://arxiv.org/pdf/2509.19452", "abs": "https://arxiv.org/abs/2509.19452", "authors": ["Alessandro Saviolo", "Jeffrey Mao", "Giuseppe Loianno"], "title": "HUNT: High-Speed UAV Navigation and Tracking in Unstructured Environments via Instantaneous Relative Frames", "categories": ["cs.RO", "cs.CV", "cs.LG"], "comment": null, "summary": "Search and rescue operations require unmanned aerial vehicles to both\ntraverse unknown unstructured environments at high speed and track targets once\ndetected. Achieving both capabilities under degraded sensing and without global\nlocalization remains an open challenge. Recent works on relative navigation\nhave shown robust tracking by anchoring planning and control to a visible\ndetected object, but cannot address navigation when no target is in the field\nof view. We present HUNT (High-speed UAV Navigation and Tracking), a real-time\nframework that unifies traversal, acquisition, and tracking within a single\nrelative formulation. HUNT defines navigation objectives directly from onboard\ninstantaneous observables such as attitude, altitude, and velocity, enabling\nreactive high-speed flight during search. Once a target is detected, the same\nperception-control pipeline transitions seamlessly to tracking. Outdoor\nexperiments in dense forests, container compounds, and search-and-rescue\noperations with vehicles and mannequins demonstrate robust autonomy where\nglobal methods fail.", "AI": {"tldr": "This paper introduces HUNT, a high-speed UAV navigation and tracking framework that enables robust search and tracking in unstructured environments using relative navigation.", "motivation": "The motivation is to overcome challenges in search and rescue operations requiring UAVs to navigate unstructured, unknown environments at high speed and track targets without global localization.", "method": "The paper presents HUNT, a unified real-time relative navigation framework that uses onboard instantaneous observables (e.g., attitude, altitude, velocity) for reactive high-speed flight and target tracking.", "result": "The framework was tested in outdoor environments like dense forests and search-and-rescue scenarios, demonstrating robust autonomy where traditional global-based methods fail.", "conclusion": "HUNT seamlessly transitions between search and tracking phases, enabling UAVs to operate effectively in complex, degraded sensing scenarios."}}
{"id": "2509.19378", "pdf": "https://arxiv.org/pdf/2509.19378", "abs": "https://arxiv.org/abs/2509.19378", "authors": ["Nelson Alves Ferreira Neto"], "title": "Vision-Based Perception for Autonomous Vehicles in Off-Road Environment Using Deep Learning", "categories": ["cs.CV", "cs.AR", "cs.LG", "eess.IV", "eess.SP"], "comment": "2022. 117p. Electrical Engineering PhD Thesis - Graduate Program in\n  Electrical and Computer Engineering, Federal University of Bahia, 40210-630,\n  Salvador, Brazil", "summary": "Low-latency intelligent systems are required for autonomous driving on\nnon-uniform terrain in open-pit mines and developing countries. This work\nproposes a perception system for autonomous vehicles on unpaved roads and\noff-road environments, capable of navigating rough terrain without a predefined\ntrail. The Configurable Modular Segmentation Network (CMSNet) framework is\nproposed, facilitating different architectural arrangements. CMSNet\nconfigurations were trained to segment obstacles and trafficable ground on new\nimages from unpaved/off-road scenarios with adverse conditions (night, rain,\ndust). We investigated applying deep learning to detect drivable regions\nwithout explicit track boundaries, studied algorithm behavior under visibility\nimpairment, and evaluated field tests with real-time semantic segmentation. A\nnew dataset, Kamino, is presented with almost 12,000 images from an operating\nvehicle with eight synchronized cameras. The Kamino dataset has a high number\nof labeled pixels compared to similar public collections and includes images\nfrom an off-road proving ground emulating a mine under adverse visibility. To\nachieve real-time inference, CMSNet CNN layers were methodically removed and\nfused using TensorRT, C++, and CUDA. Empirical experiments on two datasets\nvalidated the proposed system's effectiveness.", "AI": {"tldr": "The paper introduces CMSNet, a modular framework for real-time segmentation to enable autonomous driving on unpaved and off-road terrains under various adverse visibility conditions, supported by the Kamino dataset.", "motivation": "Autonomous driving systems on off-road terrains face challenges due to non-uniform surfaces and adverse visibility conditions, particularly in environments like open-pit mines. This work aims to address these challenges with low-latency perception solutions.", "method": "CMSNet, a Configurable Modular Segmentation Network framework, was proposed. It was trained using a new dataset, Kamino, containing diverse conditions. CMSNet employed optimized deep learning architectures and real-time techniques using TensorRT, C++, and CUDA.", "result": "CMSNet successfully segmented trafficable ground and obstacles under diverse conditions. The system achieved real-time performance and was validated empirically on the Kamino dataset and another dataset.", "conclusion": "CMSNet demonstrated effective segmentation for autonomous vehicles in off-road and adverse conditions, proving its applicability for real-time autonomous systems, supported by the curated Kamino dataset."}}
{"id": "2509.19305", "pdf": "https://arxiv.org/pdf/2509.19305", "abs": "https://arxiv.org/abs/2509.19305", "authors": ["Yifu Luo", "Yongzhe Chang", "Xueqian Wang"], "title": "Wavelet Fourier Diffuser: Frequency-Aware Diffusion Model for Reinforcement Learning", "categories": ["cs.LG", "cs.AI", "eess.SP"], "comment": null, "summary": "Diffusion probability models have shown significant promise in offline\nreinforcement learning by directly modeling trajectory sequences. However,\nexisting approaches primarily focus on time-domain features while overlooking\nfrequency-domain features, leading to frequency shift and degraded performance\naccording to our observation. In this paper, we investigate the RL problem from\na new perspective of the frequency domain. We first observe that\ntime-domain-only approaches inadvertently introduce shifts in the low-frequency\ncomponents of the frequency domain, which results in trajectory instability and\ndegraded performance. To address this issue, we propose Wavelet Fourier\nDiffuser (WFDiffuser), a novel diffusion-based RL framework that integrates\nDiscrete Wavelet Transform to decompose trajectories into low- and\nhigh-frequency components. To further enhance diffusion modeling for each\ncomponent, WFDiffuser employs Short-Time Fourier Transform and cross attention\nmechanisms to extract frequency-domain features and facilitate cross-frequency\ninteraction. Extensive experiment results on the D4RL benchmark demonstrate\nthat WFDiffuser effectively mitigates frequency shift, leading to smoother,\nmore stable trajectories and improved decision-making performance over existing\nmethods.", "AI": {"tldr": "The paper introduces a novel reinforcement learning approach called WFDiffuser that leverages frequency-domain features to improve trajectory stability and decision-making.", "motivation": "Existing offline reinforcement learning methods focus on time-domain features, often leading to frequency shift and degraded performance.", "method": "WFDiffuser combines Discrete Wavelet Transform, Short-Time Fourier Transform, and cross attention mechanisms to integrate frequency-domain features and improve trajectory modeling.", "result": "The proposed WFDiffuser significantly mitigates frequency shift and improves both trajectory stability and decision-making, as demonstrated on the D4RL benchmark.", "conclusion": "Frequency-domain features are critical in reinforcement learning, and WFDiffuser provides a robust framework to harness them, outperforming existing time-domain-focused methods."}}
{"id": "2509.19959", "pdf": "https://arxiv.org/pdf/2509.19959", "abs": "https://arxiv.org/abs/2509.19959", "authors": ["Antoine Plin", "Fr\u00e9d\u00e9ric Fauberteau", "Nga Nguyen"], "title": "OpenGL GPU-Based Rowhammer Attack (Work in Progress)", "categories": ["cs.AR", "cs.CR"], "comment": "Presented at HS3 2025 Workshop", "summary": "Rowhammer attacks have emerged as a significant threat to modern DRAM-based\nmemory systems, leveraging frequent memory accesses to induce bit flips in\nadjacent memory cells. This work-in-progress paper presents an adaptive,\nmany-sided Rowhammer attack utilizing GPU compute shaders to systematically\nachieve high-frequency memory access patterns. Our approach employs statistical\ndistributions to optimize row targeting and avoid current mitigations. The\nmethodology involves initializing memory with known patterns, iteratively\nhammering victim rows, monitoring for induced errors, and dynamically adjusting\nparameters to maximize success rates. The proposed attack exploits the parallel\nprocessing capabilities of GPUs to accelerate hammering operations, thereby\nincreasing the probability of successful bit flips within a constrained\ntimeframe. By leveraging OpenGL compute shaders, our implementation achieves\nhighly efficient row hammering with minimal software overhead. Experimental\nresults on a Raspberry Pi 4 demonstrate that the GPU-based approach attains a\nhigh rate of bit flips compared to traditional CPU-based hammering, confirming\nits effectiveness in compromising DRAM integrity. Our findings align with\nexisting research on microarchitectural attacks in heterogeneous systems that\nhighlight the susceptibility of GPUs to security vulnerabilities. This study\ncontributes to the understanding of GPU-assisted fault-injection attacks and\nunderscores the need for improved mitigation strategies in future memory\narchitectures.", "AI": {"tldr": "The study demonstrates a GPU-assisted Rowhammer attack that leverages compute shaders for efficient and adaptive bit-flipping in DRAM, outperforming traditional CPU-based methods.", "motivation": "The paper aims to explore the security vulnerabilities in modern DRAM memory systems, specifically through GPU-assisted Rowhammer attacks, highlighting the limitations of existing mitigations.", "method": "The authors use GPU compute shaders combined with statistical targeting, memory initialization, iterative hammering, and dynamic adjustments to optimize Rowhammer attacks, achieving high performance and minimal overhead.", "result": "The GPU-based Rowhammer attack on a Raspberry Pi 4 demonstrates superior bit-flipping rates compared to CPU-based methods, confirming its effectiveness and highlighting DRAM vulnerabilities.", "conclusion": "The study provides insights into GPU-assisted Rowhammer attacks, emphasizing the need for stronger mitigation strategies in DRAM designs to address emerging security risks."}}
{"id": "2509.19464", "pdf": "https://arxiv.org/pdf/2509.19464", "abs": "https://arxiv.org/abs/2509.19464", "authors": ["Shripad Vilasrao Deshmukh", "Will Schwarzer", "Scott Niekum"], "title": "Evaluation-Aware Reinforcement Learning", "categories": ["cs.AI", "cs.LG"], "comment": "9 pages, under submission", "summary": "Policy evaluation is often a prerequisite for deploying safety- and\nperformance-critical systems. Existing evaluation approaches frequently suffer\nfrom high variance due to limited data and long-horizon tasks, or high bias due\nto unequal support or inaccurate environmental models. We posit that these\nchallenges arise, in part, from the standard reinforcement learning (RL)\nparadigm of policy learning without explicit consideration of evaluation. As an\nalternative, we propose evaluation-aware reinforcement learning (EvA-RL), in\nwhich a policy is trained to maximize expected return while simultaneously\nminimizing expected evaluation error under a given value prediction scheme --\nin other words, being \"easy\" to evaluate. We formalize a framework for EvA-RL\nand design an instantiation that enables accurate policy evaluation,\nconditioned on a small number of rollouts in an assessment environment that can\nbe different than the deployment environment. However, our theoretical analysis\nand empirical results show that there is often a tradeoff between evaluation\naccuracy and policy performance when using a fixed value-prediction scheme\nwithin EvA-RL. To mitigate this tradeoff, we extend our approach to co-learn an\nassessment-conditioned state-value predictor alongside the policy. Empirical\nresults across diverse discrete and continuous action domains demonstrate that\nEvA-RL can substantially reduce evaluation error while maintaining competitive\nreturns. This work lays the foundation for a broad new class of RL methods that\ntreat reliable evaluation as a first-class principle during training.", "AI": {"tldr": "The paper introduces Evaluation-Aware Reinforcement Learning (EvA-RL), a method that trains policies to be both high-performing and easy to evaluate, reducing evaluation errors with minimal rollouts.", "motivation": "The main motivation is to address high variance and bias in policy evaluation due to factors like limited data, long-horizon tasks, and the lack of explicit consideration for evaluation within standard reinforcement learning approaches.", "method": "The authors propose EvA-RL, which integrates evaluation-aware policy training by maximizing returns while minimizing evaluation errors. They extend this framework by co-learning an assessment-conditioned value predictor alongside the policy to mitigate tradeoffs.", "result": "The study demonstrates that EvA-RL significantly reduces evaluation errors while maintaining competitive policy performance across various action domains, even with limited rollout data.", "conclusion": "EvA-RL represents a novel approach that prioritizes reliable evaluation during training, setting the foundation for a new class of reinforcement learning methods that closely integrate evaluation and policy optimization."}}
{"id": "2509.19532", "pdf": "https://arxiv.org/pdf/2509.19532", "abs": "https://arxiv.org/abs/2509.19532", "authors": ["Flavio Castro", "Weijian Zheng", "Joaquin Chung", "Ian Foster", "Rajkumar Kettimuthu"], "title": "To Stream or Not to Stream: Towards A Quantitative Model for Remote HPC Processing Decisions", "categories": ["cs.DC", "cs.NI"], "comment": null, "summary": "Modern scientific instruments generate data at rates that increasingly exceed\nlocal compute capabilities and, when paired with the staging and I/O overheads\nof file-based transfers, also render file-based use of remote HPC resources\nimpractical for time-sensitive analysis and experimental steering. Real-time\nstreaming frameworks promise to reduce latency and improve system efficiency,\nbut lack a principled way to assess their feasibility. In this work, we\nintroduce a quantitative framework and an accompanying Streaming Speed Score to\nevaluate whether remote high-performance computing (HPC) resources can provide\ntimely data processing compared to local alternatives. Our model incorporates\nkey parameters including data generation rate, transfer efficiency, remote\nprocessing power, and file input/output overhead to compute total processing\ncompletion time and identify operational regimes where streaming is beneficial.\nWe motivate our methodology with use cases from facilities such as APS, FRIB,\nLCLS-II, and the LHC, and validate our approach through an illustrative case\nstudy based on LCLS-II data. Our measurements show that streaming can achieve\nup to 97% lower end-to-end completion time than file-based methods under high\ndata rates, while worst-case congestion can increase transfer times by over an\norder of magnitude, underscoring the importance of tail latency in streaming\nfeasibility decisions.", "AI": {"tldr": "Modern scientific instruments face difficulties in processing and transferring high-rate data locally and remotely using file-based systems, necessitating the evaluation of real-time streaming frameworks. This paper introduces a quantitative model to assess the feasibility and performance of streaming for remote data processing.", "motivation": "Scientific instruments are producing data at rates that exceed local compute capabilities, and traditional file-based systems for remote high-performance computing (HPC) are impractical for real-time applications due to staging and I/O overheads.", "method": "The paper presents a quantitative framework and a Streaming Speed Score to evaluate the feasibility of using remote HPC resources for real-time data processing. It factors in key metrics like data generation rate, transfer efficiency, remote processing power, and file I/O overhead.", "result": "The methodology is validated through a case study on LCLS-II data, where streaming frameworks show up to 97% reduction in end-to-end processing time compared to file-based methods under high data rates. On the downside, worst-case network congestion severely impacts transfer times.", "conclusion": "Streaming frameworks can be significantly more efficient than file-based methods for high data-rate environments, though susceptibility to network congestion must be considered in feasibility assessments."}}
{"id": "2509.19613", "pdf": "https://arxiv.org/pdf/2509.19613", "abs": "https://arxiv.org/abs/2509.19613", "authors": ["William J. Bowman"], "title": "Compilation as Multi-Language Semantics", "categories": ["cs.PL"], "comment": null, "summary": "Modeling interoperability between programs in different languages is a key\nproblem when modeling verified and secure compilation, which has been\nsuccessfully addressed using multi-language semantics. Unfortunately, existing\nmodels of compilation using multi-language semantics define two variants of\neach compiler pass: a syntactic translation on open terms to model compilation,\nand a run-time translation of closed terms at multi-language boundaries to\nmodel interoperability.\n  In this talk, I discuss work-in-progress approach to uniformly model a\ncompiler entirely as a reduction system on open term in a multi-language\nsemantics, rather than as a syntactic translation. This simultaneously defines\nthe compiler and the interoperability semantics, reducing duplication. It also\nprovides interesting semantic insights. Normalization of the cross-language\nredexes performs ahead-of-time (AOT) compilation. Evaluation in the\nmulti-language models just-in-time (JIT) compilation. Confluence of\nmulti-language reduction implies compiler correctness, and part of the secure\ncompilation proof (full abstraction), enabling focus on the difficult part of\nthe proof. Subject reduction of the multi-language reduction implies\ntype-preservation of the compiler.", "AI": {"tldr": "This work explores using multi-language semantics to uniformly model a compiler as a reduction system, addressing compiler correctness and interoperability without relying on separate syntactic translations.", "motivation": "The goal is to address the challenge of verified and secure compilation, particularly focusing on improving interoperability between programs written in different languages.", "method": "A unified approach to model a compiler as a reduction system on open terms using multi-language semantics, integrating compilation and interoperability semantics.", "result": "Key results include defining both ahead-of-time (AOT) and just-in-time (JIT) compilation through multi-language reduction, along with insights such as confluence (compiler correctness) and subject reduction (type preservation).", "conclusion": "This approach reduces redundancy, offers semantic insights into compilation processes, and simplifies secure compilation proofs by leveraging multi-language reduction properties like confluence and subject reduction."}}
{"id": "2509.20216", "pdf": "https://arxiv.org/pdf/2509.20216", "abs": "https://arxiv.org/abs/2509.20216", "authors": ["Suman S. Kulkarni", "Christopher W. Lynn", "Mason A. Porter", "Dani S. Bassett"], "title": "Ising dynamics on multilayer networks with heterogeneous layers", "categories": ["physics.soc-ph", "cond-mat.stat-mech", "q-bio.NC"], "comment": "19 pages, 10 figures", "summary": "Multilayer networks provide a framework to study complex systems with\nmultiple types of interactions, multiple dynamical processes, and/or multiple\nsubsystems. When studying a dynamical process on a multilayer network, it is\nimportant to consider how both layer structure and heterogeneity across layers\nimpacts the overall dynamics. As a concrete example, we study Ising dynamics on\nmultilayer networks and investigate how network structure affects its\nqualitative features. We focus primarily on multiplex networks, which are\nmultilayer networks in which interlayer edges occur only between manifestations\nof the same entity on different layers, although we also consider one empirical\nexample with a more general multilayer structure. We use numerical simulations\nand a mean-field approximation to examine the steady-state behavior of the\nIsing dynamics as a function of temperature (which is a key model parameter)\nfor a variety of two-layer multilayer networks from both models and empirical\ndata. We examine both the steady-state behavior and a metastable state in which\nthe two layers are anti-aligned, and we explore the effects of interlayer\ncoupling strength and structural heterogeneity. In synthetic multilayer\nnetworks with core--periphery structure, we show that interlayer edges that\ninvolve peripheral nodes can exert more influence than interlayer edges that\ninvolve only core nodes. Finally, we consider empirical multilayer networks\nfrom biological and social systems. Our work illustrates how heterogeneity\nacross the layers of a multilayer network influences dynamics on the whole\nnetwork.", "AI": {"tldr": "This paper studies Ising dynamics on multilayer networks, highlighting the impact of network structure, interlayer coupling strength, and layer heterogeneity on its dynamics.", "motivation": "To understand how heterogeneity and structural characteristics across layers in multilayer networks influence the overall dynamics in various systems.", "method": "The authors use numerical simulations and a mean-field approximation to analyze Ising dynamics across different two-layer multilayer networks (both synthetic and empirical).", "result": "Interlayer coupling and structural heterogeneity significantly affect both steady and metastable states. Peripheral node interlayer edges influence the dynamics more than core node edges in synthetic core-periphery networks.", "conclusion": "The study demonstrates that heterogeneity across layers in multilayer networks plays a critical role in shaping overall network behavior, with insights applicable to biological and social systems."}}
{"id": "2509.19533", "pdf": "https://arxiv.org/pdf/2509.19533", "abs": "https://arxiv.org/abs/2509.19533", "authors": ["Mengdi Lu", "Steven Ding", "Furkan Alaca", "Philippe Charland"], "title": "Semantic-Aware Fuzzing: An Empirical Framework for LLM-Guided, Reasoning-Driven Input Mutation", "categories": ["cs.SE", "cs.AI", "cs.CR"], "comment": null, "summary": "Security vulnerabilities in Internet-of-Things devices, mobile platforms, and\nautonomous systems remain critical. Traditional mutation-based fuzzers -- while\neffectively explore code paths -- primarily perform byte- or bit-level edits\nwithout semantic reasoning. Coverage-guided tools such as AFL++ use\ndictionaries, grammars, and splicing heuristics to impose shallow structural\nconstraints, leaving deeper protocol logic, inter-field dependencies, and\ndomain-specific semantics unaddressed. Conversely, reasoning-capable large\nlanguage models (LLMs) can leverage pretraining knowledge to understand input\nformats, respect complex constraints, and propose targeted mutations, much like\nan experienced reverse engineer or testing expert. However, lacking ground\ntruth for \"correct\" mutation reasoning makes supervised fine-tuning\nimpractical, motivating explorations of off-the-shelf LLMs via prompt-based\nfew-shot learning. To bridge this gap, we present an open-source microservices\nframework that integrates reasoning LLMs with AFL++ on Google's FuzzBench,\ntackling asynchronous execution and divergent hardware demands (GPU- vs.\nCPU-intensive) of LLMs and fuzzers. We evaluate four research questions: (R1)\nHow can reasoning LLMs be integrated into the fuzzing mutation loop? (R2) Do\nfew-shot prompts yield higher-quality mutations than zero-shot? (R3) Can prompt\nengineering with off-the-shelf models improve fuzzing directly? and (R4) Which\nopen-source reasoning LLMs perform best under prompt-only conditions?\nExperiments with Llama3.3, Deepseek-r1-Distill-Llama-70B, QwQ-32B, and Gemma3\nhighlight Deepseek as the most promising. Mutation effectiveness depends more\non prompt complexity and model choice than shot count. Response latency and\nthroughput bottlenecks remain key obstacles, offering directions for future\nwork.", "AI": {"tldr": "The paper proposes blending reasoning from large language models (LLMs) with traditional fuzzing tools to improve mutation quality in testing software vulnerabilities.", "motivation": "Traditional fuzzing techniques lack deep semantic reasoning and struggle with complex protocol constraints, necessitating advanced methods.", "method": "The authors integrate reasoning LLMs into the AFL++ fuzzing framework using a microservices setup, testing different prompt approaches (zero-shot and few-shot) alongside several reasoning LLMs.", "result": "Experiments reveal that prompt engineering and LLM selection significantly affect mutation quality, with Deepseek identified as the top-performing model under prompt-only conditions.", "conclusion": "Enhancing fuzzing with reasoning LLMs holds promise for deeper, constraint-aware code mutation, but issues like latency and scalability require further research."}}
{"id": "2509.19319", "pdf": "https://arxiv.org/pdf/2509.19319", "abs": "https://arxiv.org/abs/2509.19319", "authors": ["Gyubok Lee", "Elea Bach", "Eric Yang", "Tom Pollard", "Alistair Johnson", "Edward Choi", "Yugang jia", "Jong Ha Lee"], "title": "FHIR-AgentBench: Benchmarking LLM Agents for Realistic Interoperable EHR Question Answering", "categories": ["cs.CL", "cs.AI"], "comment": "Under review", "summary": "The recent shift toward the Health Level Seven Fast Healthcare\nInteroperability Resources (HL7 FHIR) standard opens a new frontier for\nclinical AI, demanding LLM agents to navigate complex, resource-based data\nmodels instead of conventional structured health data. However, existing\nbenchmarks have lagged behind this transition, lacking the realism needed to\nevaluate recent LLMs on interoperable clinical data. To bridge this gap, we\nintroduce FHIR-AgentBench, a benchmark that grounds 2,931 real-world clinical\nquestions in the HL7 FHIR standard. Using this benchmark, we systematically\nevaluate agentic frameworks, comparing different data retrieval strategies\n(direct FHIR API calls vs. specialized tools), interaction patterns\n(single-turn vs. multi-turn), and reasoning strategies (natural language vs.\ncode generation). Our experiments highlight the practical challenges of\nretrieving data from intricate FHIR resources and the difficulty of reasoning\nover them, both of which critically affect question answering performance. We\npublicly release the FHIR-AgentBench dataset and evaluation suite\n(https://github.com/glee4810/FHIR-AgentBench) to promote reproducible research\nand the development of robust, reliable LLM agents for clinical applications.", "AI": {"tldr": "FHIR-AgentBench introduces a benchmark grounded in HL7 FHIR standards for assessing LLM agents' ability to handle real-world clinical questions.", "motivation": "The shift to HL7 FHIR standards requires benchmarks for evaluating AI dealing with complex healthcare data models rather than traditional structured data.", "method": "Researchers developed FHIR-AgentBench consisting of real-world clinical questions and systematically measured LLM performance in data retrieval, interaction patterns, and reasoning strategies.", "result": "Results identify challenges in data retrieval and reasoning over complex FHIR resources, exposing practical limitations in question answering performance.", "conclusion": "FHIR-AgentBench advances clinical AI by providing a robust framework to evaluate LLM agents on interoperable healthcare data, promoting reproducible research."}}
{"id": "2509.19454", "pdf": "https://arxiv.org/pdf/2509.19454", "abs": "https://arxiv.org/abs/2509.19454", "authors": ["Jason Chen", "I-Chun Arthur Liu", "Gaurav Sukhatme", "Daniel Seita"], "title": "ROPA: Synthetic Robot Pose Generation for RGB-D Bimanual Data Augmentation", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Training robust bimanual manipulation policies via imitation learning\nrequires demonstration data with broad coverage over robot poses, contacts, and\nscene contexts. However, collecting diverse and precise real-world\ndemonstrations is costly and time-consuming, which hinders scalability. Prior\nworks have addressed this with data augmentation, typically for either\neye-in-hand (wrist camera) setups with RGB inputs or for generating novel\nimages without paired actions, leaving augmentation for eye-to-hand\n(third-person) RGB-D training with new action labels less explored. In this\npaper, we propose Synthetic Robot Pose Generation for RGB-D Bimanual Data\nAugmentation (ROPA), an offline imitation learning data augmentation method\nthat fine-tunes Stable Diffusion to synthesize third-person RGB and RGB-D\nobservations of novel robot poses. Our approach simultaneously generates\ncorresponding joint-space action labels while employing constrained\noptimization to enforce physical consistency through appropriate\ngripper-to-object contact constraints in bimanual scenarios. We evaluate our\nmethod on 5 simulated and 3 real-world tasks. Our results across 2625\nsimulation trials and 300 real-world trials demonstrate that ROPA outperforms\nbaselines and ablations, showing its potential for scalable RGB and RGB-D data\naugmentation in eye-to-hand bimanual manipulation. Our project website is\navailable at: https://ropaaug.github.io/.", "AI": {"tldr": "The paper proposes a method, ROPA, to generate synthetic data for training bimanual manipulation robots effectively using RGB-D inputs through imitation learning without expensive real-world data collection.", "motivation": "Collecting diverse and precise real-world data for bimanual manipulation tasks is costly and time-consuming, limiting scalability. Existing augmentation techniques lack effective solutions for RGB-D, eye-to-hand setups.", "method": "The proposed ROPA method fine-tunes Stable Diffusion to synthesize RGB and RGB-D observations, generating novel robot poses along with corresponding joint-space action labels. Constrained optimization ensures physical consistency via gripper-to-object contact constraints.", "result": "ROPA is evaluated on 5 simulated and 3 real-world tasks, with results across 2625 simulation trials and 300 real-world trials demonstrating that it outperforms existing baselines and ablations.", "conclusion": "ROPA provides a scalable solution for RGB and RGB-D data augmentation in eye-to-hand bimanual manipulation setups, enhancing training efficiency and performance for robot manipulation policies."}}
{"id": "2509.19402", "pdf": "https://arxiv.org/pdf/2509.19402", "abs": "https://arxiv.org/abs/2509.19402", "authors": ["Herve Goeau", "Pierre Bonnet", "Alexis Joly"], "title": "Overview of LifeCLEF Plant Identification task 2020", "categories": ["cs.CV"], "comment": "15 pages, 5 figures, CLEF 2020 Conference and Labs of the Evaluation\n  Forum, September 05 to 08, 2020, Thessaloniki, Greece", "summary": "Automated identification of plants has improved considerably thanks to the\nrecent progress in deep learning and the availability of training data with\nmore and more photos in the field. However, this profusion of data only\nconcerns a few tens of thousands of species, mostly located in North America\nand Western Europe, much less in the richest regions in terms of biodiversity\nsuch as tropical countries. On the other hand, for several centuries, botanists\nhave collected, catalogued and systematically stored plant specimens in\nherbaria, particularly in tropical regions, and the recent efforts by the\nbiodiversity informatics community made it possible to put millions of\ndigitized sheets online. The LifeCLEF 2020 Plant Identification challenge (or\n\"PlantCLEF 2020\") was designed to evaluate to what extent automated\nidentification on the flora of data deficient regions can be improved by the\nuse of herbarium collections. It is based on a dataset of about 1,000 species\nmainly focused on the South America's Guiana Shield, an area known to have one\nof the greatest diversity of plants in the world. The challenge was evaluated\nas a cross-domain classification task where the training set consist of several\nhundred thousand herbarium sheets and few thousand of photos to enable learning\na mapping between the two domains. The test set was exclusively composed of\nphotos in the field. This paper presents the resources and assessments of the\nconducted evaluation, summarizes the approaches and systems employed by the\nparticipating research groups, and provides an analysis of the main outcomes.", "AI": {"tldr": "This paper discusses the PlantCLEF 2020 challenge, which evaluated the use of herbarium sheets to improve automated plant identification in biodiversity-rich but data-poor tropical regions.", "motivation": "To address the lack of high-quality training data for plant species in biodiversity-rich tropical regions and to explore how digitized herbarium collections can enhance automated plant identification.", "method": "The PlantCLEF 2020 challenge provided a dataset with herbarium sheets and field photos, focusing on species from the Guiana Shield. Participants trained models to map between herbarium sheets and field photos, with testing on field photos exclusively.", "result": "The challenge demonstrated that herbarium collections could complement traditional data sources for plant identification. It also highlighted techniques and models developed by participants.", "conclusion": "Using herbarium collections can significantly contribute to improving automated plant identification in biodiverse but underrepresented regions, bridging a critical data gap."}}
{"id": "2509.19359", "pdf": "https://arxiv.org/pdf/2509.19359", "abs": "https://arxiv.org/abs/2509.19359", "authors": ["Mashkhal Abdalwahid Sidiq", "Yimamu Kirubel Wondaferew"], "title": "Anti-Money Laundering Systems Using Deep Learning", "categories": ["cs.LG", "cs.AI"], "comment": "22 pages, 9 figures", "summary": "In this paper, we focused on using deep learning methods for detecting money\nlaundering in financial transaction networks, in order to demonstrate that it\ncan be used as a complement or instead of the more commonly used rule-based\nsystems and conventional Anti-Money Laundering (AML) systems. The paper\nexplores the pivotal role played by Anti-Money Laundering (AML) activities in\nthe global financial industry. It underscores the drawbacks of conventional AML\nsystems, which exhibit high rates of false positives and lack the\nsophistication to uncover intricate money laundering schemes. To tackle these\nchallenges, the paper proposes an advanced AML system that capitalizes on link\nanalysis using deep learning techniques. At the heart of this system lies the\nutilization of centrality algorithms like Degree Centrality, Closeness\nCentrality, Betweenness Centrality, and PageRank. These algorithms enhance the\nsystem's capability to identify suspicious activities by examining the\ninfluence and interconnections within networks of financial transactions. The\nsignificance of Anti-Money Laundering (AML) efforts within the global financial\nsector is discussed in this paper. It highlights the limitations of traditional\nAML systems. The results showed the practicality and superiority of the new\nimplementation of the GCN model, which is a preferable method for connectively\nstructured data, meaning that a transaction or account is analyzed in the\ncontext of its financial environment. In addition, the paper delves into the\nprospects of Anti-Money Laundering (AML) efforts, proposing the integration of\nemerging technologies such as deep learning and centrality algorithms. This\nintegration holds promise for enhancing the effectiveness of AML systems by\nrefining their capabilities.", "AI": {"tldr": "This paper presents a deep learning-based approach to enhance Anti-Money Laundering (AML) efforts, overcoming the limitations of traditional rule-based systems.", "motivation": "Traditional AML systems suffer from high false positives and fail to detect complex laundering schemes.", "method": "The research employs deep learning techniques combined with link analysis and centrality algorithms (Degree Centrality, Closeness Centrality, Betweenness Centrality, PageRank) to detect money laundering in financial networks.", "result": "The proposed system demonstrated the effectiveness of utilizing a Graph Convolutional Network (GCN) model for analyzing financial transactions within their contextual network.", "conclusion": "The study establishes that integrating deep learning and centrality algorithms can significantly improve the performance of AML systems, offering a compelling alternative to traditional approaches."}}
{"id": "2509.19455", "pdf": "https://arxiv.org/pdf/2509.19455", "abs": "https://arxiv.org/abs/2509.19455", "authors": ["Mert Gurbuzbalaban", "Hoang M. Nguyen", "Xicheng Zhang", "Lingjiong Zhu"], "title": "Anchored Langevin Algorithms", "categories": ["stat.ML", "cs.LG", "math.PR"], "comment": "49 pages, 8 figures, 1 table", "summary": "Standard first-order Langevin algorithms such as the unadjusted Langevin\nalgorithm (ULA) are obtained by discretizing the Langevin diffusion and are\nwidely used for sampling in machine learning because they scale to high\ndimensions and large datasets. However, they face two key limitations: (i) they\nrequire differentiable log-densities, excluding targets with non-differentiable\ncomponents; and (ii) they generally fail to sample heavy-tailed targets. We\npropose anchored Langevin dynamics, a unified approach that accommodates\nnon-differentiable targets and certain classes of heavy-tailed distributions.\nThe method replaces the original potential with a smooth reference potential\nand modifies the Langevin diffusion via multiplicative scaling. We establish\nnon-asymptotic guarantees in the 2-Wasserstein distance to the target\ndistribution and provide an equivalent formulation derived via a random time\nchange of the Langevin diffusion. We provide numerical experiments to\nillustrate the theory and practical performance of our proposed approach.", "AI": {"tldr": "Anchored Langevin dynamics is introduced to address limitations of standard Langevin algorithms, accommodating non-differentiable targets and heavy-tailed distributions.", "motivation": "Standard first-order Langevin algorithms are effective for high-dimensional sampling but face challenges with non-differentiable components and heavy-tailed distributions.", "method": "The new approach replaces the original potential with a smooth reference potential and incorporates multiplicative scaling modifications to Langevin diffusion.", "result": "Non-asymptotic guarantees in 2-Wasserstein distance to target distributions are established, alongside numerical experiments demonstrating theory and performance.", "conclusion": "Anchored Langevin dynamics unifies and extends standard methods to sample from broader classes of target distributions effectively."}}
{"id": "2509.19339", "pdf": "https://arxiv.org/pdf/2509.19339", "abs": "https://arxiv.org/abs/2509.19339", "authors": ["Mohammad Sadegh Khorshidi", "Navid Yazdanjue", "Hassan Gharoun", "Mohammad Reza Nikoo", "Fang Chen", "Amir H. Gandomi"], "title": "Multi-population Ensemble Genetic Programming via Cooperative Coevolution and Multi-view Learning for Classification", "categories": ["cs.NE", "cs.AI", "68T20"], "comment": "59 Pages, 68 Figures, 27 Tables", "summary": "This paper introduces Multi-population Ensemble Genetic Programming (MEGP), a\ncomputational intelligence framework that integrates cooperative coevolution\nand the multiview learning paradigm to address classification challenges in\nhigh-dimensional and heterogeneous feature spaces. MEGP decomposes the input\nspace into conditionally independent feature subsets, enabling multiple\nsubpopulations to evolve in parallel while interacting through a dynamic\nensemble-based fitness mechanism. Each individual encodes multiple genes whose\noutputs are aggregated via a differentiable softmax-based weighting layer,\nenhancing both model interpretability and adaptive decision fusion. A hybrid\nselection mechanism incorporating both isolated and ensemble-level fitness\npromotes inter-population cooperation while preserving intra-population\ndiversity. This dual-level evolutionary dynamic facilitates structured search\nexploration and reduces premature convergence. Experimental evaluations across\neight benchmark datasets demonstrate that MEGP consistently outperforms a\nbaseline GP model in terms of convergence behavior and generalization\nperformance. Comprehensive statistical analyses validate significant\nimprovements in Log-Loss, Precision, Recall, F1 score, and AUC. MEGP also\nexhibits robust diversity retention and accelerated fitness gains throughout\nevolution, highlighting its effectiveness for scalable, ensemble-driven\nevolutionary learning. By unifying population-based optimization, multi-view\nrepresentation learning, and cooperative coevolution, MEGP contributes a\nstructurally adaptive and interpretable framework that advances emerging\ndirections in evolutionary machine learning.", "AI": {"tldr": "The paper introduces MEGP, a framework combining cooperative coevolution and multiview learning for classification tasks in complex feature spaces, demonstrating substantial performance improvements and diversity retention.", "motivation": "The paper aims to address classification challenges in high-dimensional, heterogeneous feature spaces by developing a framework that promotes cooperation, diversity, and interpretability.", "method": "MEGP uses cooperative coevolution to divide and process multiple feature subsets in parallel, employs a softmax-based gene aggregation layer for decision fusion, and utilizes a hybrid selection mechanism for balanced dynamics.", "result": "MEGP outperformed baseline models in experiments across eight benchmark datasets, achieving significant gains in metrics like Log-Loss, Precision, Recall, F1 score, and AUC.", "conclusion": "MEGP offers a scalable and interpretable evolutionary learning framework by integrating population-based optimization and multiview representation learning, advancing evolutionary machine learning principles."}}
{"id": "2509.20182", "pdf": "https://arxiv.org/pdf/2509.20182", "abs": "https://arxiv.org/abs/2509.20182", "authors": ["Amulya Bhattaram", "Janani Ramamoorthy", "Ranit Gupta", "Diana Marculescu", "Dimitrios Stamoulis"], "title": "Automated Multi-Agent Workflows for RTL Design", "categories": ["cs.AR", "cs.AI"], "comment": "Accepted: ML for Systems Workshop NeurIPS 2025", "summary": "The rise of agentic AI workflows unlocks novel opportunities for computer\nsystems design and optimization. However, for specialized domains such as\nprogram synthesis, the relative scarcity of HDL and proprietary EDA resources\nonline compared to more common programming tasks introduces challenges, often\nnecessitating task-specific fine-tuning, high inference costs, and\nmanually-crafted agent orchestration. In this work, we present VeriMaAS, a\nmulti-agent framework designed to automatically compose agentic workflows for\nRTL code generation. Our key insight is to integrate formal verification\nfeedback from HDL tools directly into workflow generation, reducing the cost of\ngradient-based updates or prolonged reasoning traces. Our method improves\nsynthesis performance by 5-7% for pass@k over fine-tuned baselines, while\nrequiring only a few hundred training examples, representing an\norder-of-magnitude reduction in supervision cost.", "AI": {"tldr": "This paper introduces VeriMaAS, a multi-agent framework leveraging formal verification feedback for RTL code generation in HDL programming, providing significant synthesis performance improvements with reduced training data.", "motivation": "To address challenges in program synthesis for specialized domains like HDL, such as scarce resources and high fine-tuning costs, the paper aims to optimize agentic AI workflows for RTL code generation.", "method": "The method integrates formal verification feedback from HDL tools into agentic workflows, avoiding gradient-based updates and enabling efficient RTL code synthesis with minimal training supervision.", "result": "VeriMaAS shows synthesis performance improvement by 5-7% in pass@k comparison against fine-tuned baselines, requiring significantly fewer training examples.", "conclusion": "By embedding formal feedback, VeriMaAS effectively enhances synthesis performance while cutting supervision costs, presenting a scalable approach for agentic AI in specialized domains."}}
{"id": "2509.19489", "pdf": "https://arxiv.org/pdf/2509.19489", "abs": "https://arxiv.org/abs/2509.19489", "authors": ["Robert Nowak"], "title": "Estimating the Self-Consistency of LLMs", "categories": ["cs.AI"], "comment": "5 pages", "summary": "Systems often repeat the same prompt to large language models (LLMs) and\naggregate responses to improve reliability. This short note analyzes an\nestimator of the self-consistency of LLMs and the tradeoffs it induces under a\nfixed compute budget $B=mn$, where $m$ is the number of prompts sampled from\nthe task distribution and $n$ is the number of repeated LLM calls per prompt;\nthe resulting analysis favors a rough split $m,n\\propto\\sqrt{B}$.", "AI": {"tldr": "The paper discusses a method for analyzing the self-consistency of large language models (LLMs) by examining tradeoffs in repeating prompts and aggregating outputs under a fixed compute budget.", "motivation": "To improve reliability in the responses of LLMs, systems repeatedly prompt them and aggregate the answers. This paper seeks to quantify the self-consistency of LLMs and address compute efficiency.", "method": "The paper proposes an analysis framework based on an estimator of self-consistency and assesses the optimal allocation of compute resources across sampled prompts and repeated calls.", "result": "The analysis concludes that the ideal budget split involves dividing the compute equally between the number of prompts and their repetitions, favoring a square root relationship between them.", "conclusion": "The study presents a practical guideline for maximizing self-consistency in LLM outputs under fixed computational constraints."}}
{"id": "2509.19539", "pdf": "https://arxiv.org/pdf/2509.19539", "abs": "https://arxiv.org/abs/2509.19539", "authors": ["Raj Patel", "Umesh Biswas", "Surya Kodipaka", "Will Carroll", "Preston Peranich", "Maxwell Young"], "title": "A Survey of Recent Advancements in Secure Peer-to-Peer Networks", "categories": ["cs.DC", "cs.CR"], "comment": "30 pages, 4 figures, 2 tables", "summary": "Peer-to-peer (P2P) networks are a cornerstone of modern computing, and their\nsecurity is an active area of research. Many defenses with strong security\nguarantees have been proposed; however, the most-recent survey is over a decade\nold. This paper delivers an updated review of recent theoretical advances that\naddress classic threats, such as the Sybil and routing attacks, while\nhighlighting how emerging trends -- such as machine learning, social networks,\nand dynamic systems -- pose new challenges and drive novel solutions. We\nevaluate the strengths and weaknesses of these solutions and suggest directions\nfor future research.", "AI": {"tldr": "This paper provides an updated review of security advancements in peer-to-peer networks, addressing traditional threats and exploring new challenges driven by emerging technologies.", "motivation": "The existing survey on P2P network security is outdated, and there is a need to examine how emerging trends impact the solutions to classic and new threats.", "method": "An extensive review of recent theoretical advancements in P2P network security was conducted, evaluating solutions to known issues and challenges introduced by new technologies.", "result": "The research identifies the strengths and weaknesses of recent security solutions for P2P networks and acknowledges how emerging trends shape these developments.", "conclusion": "The study emphasizes the ongoing challenges in P2P security and suggests future research directions to tackle threats effectively in light of technological evolution."}}
{"id": "2509.20020", "pdf": "https://arxiv.org/pdf/2509.20020", "abs": "https://arxiv.org/abs/2509.20020", "authors": ["Maurice Wenig", "Paul G. Rump", "Mark Blacher", "Joachim Giesen"], "title": "The Syntax and Semantics of einsum", "categories": ["cs.PL", "cs.LG", "cs.MS", "cs.SC", "F.2.2; I.1.2; I.1.3"], "comment": "21 pages, 1 figure. Includes formal definitions, proofs of algebraic\n  properties, and nesting/denesting rules for the einsum notation", "summary": "In 2011, einsum was introduced to NumPy as a practical and convenient\nnotation for tensor expressions in machine learning, quantum circuit\nsimulation, and other fields. It has since been implemented in additional\nPython frameworks such as PyTorch and TensorFlow, as well as in other\nprogramming languages such as Julia. Despite its practical success, the einsum\nnotation still lacks a solid theoretical basis, and is not unified across the\ndifferent frameworks, limiting opportunities for formal reasoning and\nsystematic optimization. In this work, we discuss the terminology of tensor\nexpressions and provide a formal definition of the einsum language. Based on\nthis definition, we formalize and prove important equivalence rules for tensor\nexpressions and highlight their relevance in practical applications.", "AI": {"tldr": "This paper focuses on providing a theoretical foundation and unification of the einsum tensor notation used in various scientific and computational frameworks.", "motivation": "While einsum notation is widely used in Python frameworks and other programming languages for tensor expressions, it lacks theoretical grounding and standardization, hindering systematic optimization and formal reasoning.", "method": "The paper offers a formal definition of the einsum language, discusses tensor expression terminology, and proves key equivalence rules for tensor expressions to showcase their application and relevance.", "result": "The authors successfully establish crucial equivalence rules for tensor expressions based on a formal description of the einsum notation, indicating its theoretical and practical viability.", "conclusion": "The formalization of the einsum language provides a unified theoretical basis, ensuring its optimization and consistency across platforms and fostering more systematic reasoning and application."}}
{"id": "2509.19587", "pdf": "https://arxiv.org/pdf/2509.19587", "abs": "https://arxiv.org/abs/2509.19587", "authors": ["Mohamed Ouf", "Haoyu Li", "Michael Zhang", "Mariam Guizani"], "title": "Reverse Engineering User Stories from Code using Large Language Models", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "User stories are essential in agile development, yet often missing or\noutdated in legacy and poorly documented systems. We investigate whether large\nlanguage models (LLMs) can automatically recover user stories directly from\nsource code and how prompt design impacts output quality. Using 1,750 annotated\nC++ snippets of varying complexity, we evaluate five state-of-the-art LLMs\nacross six prompting strategies. Results show that all models achieve, on\naverage, an F1 score of 0.8 for code up to 200 NLOC. Our findings show that a\nsingle illustrative example enables the smallest model (8B) to match the\nperformance of a much larger 70B model. In contrast, structured reasoning via\nChain-of-Thought offers only marginal gains, primarily for larger models.", "AI": {"tldr": "This paper examines the potential of large language models (LLMs) to generate user stories from source code, achieving promising results with smaller models using illustrative examples.", "motivation": "The paper aims to address the challenge of missing or outdated user stories in legacy systems by exploring if LLMs can automate their recovery from source code.", "method": "The authors evaluate five state-of-the-art LLMs on 1,750 annotated C++ code snippets, employing six different prompting strategies to study their effectiveness.", "result": "The models achieve an average F1 score of 0.8 for code up to 200 NLOC. Smaller models performed comparably to larger models when given a single illustrative example, while structured reasoning yielded only minor improvements.", "conclusion": "The findings suggest LLMs are effective in recovering user stories, with smaller models performing well under the right prompting. This offers a scalable, automated solution for user story generation in agile development."}}
{"id": "2509.19322", "pdf": "https://arxiv.org/pdf/2509.19322", "abs": "https://arxiv.org/abs/2509.19322", "authors": ["Millie Vyas", "Timothy Blattner", "Alden Dima"], "title": "Readme_AI: Dynamic Context Construction for Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Despite being trained on significant amounts of data, Large Language Models\n(LLMs) can provide inaccurate or unreliable information in the context of a\nuser's specific query. Given query-specific context significantly improves the\nusefulness of its responses. In this paper, we present a specification that can\nbe used to dynamically build context for data sources. The data source owner\ncreates the file containing metadata for LLMs to use when reasoning about\ndataset-related queries. To demonstrate our proposed specification, we created\na prototype Readme_AI Model Context Protocol (MCP) server that retrieves the\nmetadata from the data source and uses it to dynamically build context. Some\nfeatures that make this specification dynamic are the extensible types that\nrepresent crawling web-pages, fetching data from data repositories, downloading\nand parsing publications, and general text. The context is formatted and\ngrouped using user-specified tags that provide clear contextual information for\nthe LLM to reason about the content. We demonstrate the capabilities of this\nearly prototype by asking the LLM about the NIST-developed Hedgehog library,\nfor which common LLMs often provides inaccurate and irrelevant responses\ncontaining hallucinations. With Readme_AI, the LLM receives enough context that\nit is now able to reason about the library and its use, and even generate code\ninterpolated from examples that were included in the Readme_AI file provided by\nHedgehog's developer. Our primary contribution is a extensible protocol for\ndynamically grounding LLMs in specialized, owner-provided data, enhancing\nresponses from LLMs and reducing hallucinations. The source code for the\nReadme_AI tool is posted here: https://github.com/usnistgov/readme_ai .", "AI": {"tldr": "This paper introduces a dynamic context-building specification for Large Language Models (LLMs) using metadata from data sources, aiming to improve the accuracy and reliability of LLM responses.", "motivation": "LLMs, despite being trained on vast amounts of data, often provide inaccurate and unreliable information for specific queries. Enhancing their responses with context from user-specific data sources is needed.", "method": "The paper proposes the Readme_AI Model Context Protocol (MCP), which dynamically retrieves and formats metadata from data sources, making context accessible for LLMs. The specification uses extensible types for various data sources and user-specified tags to group the context.", "result": "Using the Readme_AI prototype, the paper demonstrates improved LLM responses for specific queries, such as providing accurate information and generating code for the NIST-developed Hedgehog library based on context from metadata.", "conclusion": "The proposed protocol successfully reduces hallucinations in LLMs by grounding them in specialized, data-owner-provided context. The Readme_AI tool represents an important step in enhancing LLM reliability for domain-specific tasks."}}
{"id": "2509.19460", "pdf": "https://arxiv.org/pdf/2509.19460", "abs": "https://arxiv.org/abs/2509.19460", "authors": ["Yifan Ye", "Jun Cen", "Jing Chen", "Zhihe Lu"], "title": "Self-evolved Imitation Learning in Simulated World", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": null, "summary": "Imitation learning has been a trend recently, yet training a generalist agent\nacross multiple tasks still requires large-scale expert demonstrations, which\nare costly and labor-intensive to collect. To address the challenge of limited\nsupervision, we propose Self-Evolved Imitation Learning (SEIL), a framework\nthat progressively improves a few-shot model through simulator interactions.\nThe model first attempts tasksin the simulator, from which successful\ntrajectories are collected as new demonstrations for iterative refinement. To\nenhance the diversity of these demonstrations, SEIL employs dual-level\naugmentation: (i) Model-level, using an Exponential Moving Average (EMA) model\nto collaborate with the primary model, and (ii) Environment-level, introducing\nslight variations in initial object positions. We further introduce a\nlightweight selector that filters complementary and informative trajectories\nfrom the generated pool to ensure demonstration quality. These curated samples\nenable the model to achieve competitive performance with far fewer training\nexamples. Extensive experiments on the LIBERO benchmark show that SEIL achieves\na new state-of-the-art performance in few-shot imitation learning scenarios.\nCode is available at https://github.com/Jasper-aaa/SEIL.git.", "AI": {"tldr": "This paper introduces Self-Evolved Imitation Learning (SEIL), a framework for improving imitation learning with limited data by generating and refining demonstrations through simulation.", "motivation": "The motivation is to reduce the cost and labor involved in collecting large-scale expert demonstrations necessary for training generalist agents across multiple tasks in imitation learning.", "method": "SEIL uses interactions in simulators to iteratively refine few-shot models by generating new demonstrations. It employs dual-level augmentation (model-level using EMA and environment-level adjustments) and uses a selector to curate high-quality trajectories for further training.", "result": "SEIL achieves state-of-the-art performance on the LIBERO benchmark with fewer training examples, demonstrating its effectiveness in few-shot imitation learning scenarios.", "conclusion": "SEIL offers an efficient and innovative approach to imitation learning, reducing supervision requirements while maintaining competitive performance."}}
{"id": "2509.19552", "pdf": "https://arxiv.org/pdf/2509.19552", "abs": "https://arxiv.org/abs/2509.19552", "authors": ["Manyi Yao", "Bingbing Zhuang", "Sparsh Garg", "Amit Roy-Chowdhury", "Christian Shelton", "Manmohan Chandraker", "Abhishek Aich"], "title": "iFinder: Structured Zero-Shot Vision-Based LLM Grounding for Dash-Cam Video Reasoning", "categories": ["cs.CV"], "comment": null, "summary": "Grounding large language models (LLMs) in domain-specific tasks like post-hoc\ndash-cam driving video analysis is challenging due to their general-purpose\ntraining and lack of structured inductive biases. As vision is often the sole\nmodality available for such analysis (i.e., no LiDAR, GPS, etc.), existing\nvideo-based vision-language models (V-VLMs) struggle with spatial reasoning,\ncausal inference, and explainability of events in the input video. To this end,\nwe introduce iFinder, a structured semantic grounding framework that decouples\nperception from reasoning by translating dash-cam videos into a hierarchical,\ninterpretable data structure for LLMs. iFinder operates as a modular,\ntraining-free pipeline that employs pretrained vision models to extract\ncritical cues -- object pose, lane positions, and object trajectories -- which\nare hierarchically organized into frame- and video-level structures. Combined\nwith a three-block prompting strategy, it enables step-wise, grounded reasoning\nfor the LLM to refine a peer V-VLM's outputs and provide accurate reasoning.\nEvaluations on four public dash-cam video benchmarks show that iFinder's\nproposed grounding with domain-specific cues, especially object orientation and\nglobal context, significantly outperforms end-to-end V-VLMs on four zero-shot\ndriving benchmarks, with up to 39% gains in accident reasoning accuracy. By\ngrounding LLMs with driving domain-specific representations, iFinder offers a\nzero-shot, interpretable, and reliable alternative to end-to-end V-VLMs for\npost-hoc driving video understanding.", "AI": {"tldr": "iFinder leverages a structured semantic grounding approach to improve reasoning in driving-specific tasks using dash-cam videos.", "motivation": "The paper aims to address the limitations of general-purpose video-based vision-language models (V-VLMs) in handling spatial reasoning, causal inference, and event explainability within domain-specific driving video tasks.", "method": "iFinder is a modular, training-free framework that translates dash-cam videos into hierarchical representations by extracting cues such as object pose, lane positions, and trajectories using pretrained vision models. It also deploys a three-block prompting strategy for step-wise reasoning using large language models.", "result": "iFinder outperformed traditional end-to-end V-VLMs, achieving up to 39% improvement in accident reasoning accuracy across four public driving video benchmarks.", "conclusion": "The structured semantic grounding and domain-specific representations provided by iFinder offer a reliable, interpretable, and zero-shot alternative to traditional V-VLMs for driving video analysis."}}
{"id": "2509.19362", "pdf": "https://arxiv.org/pdf/2509.19362", "abs": "https://arxiv.org/abs/2509.19362", "authors": ["Benedikt W. Hosp"], "title": "DeepACTIF: Efficient Feature Attribution via Activation Traces in Neural Sequence Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Feature attribution is essential for interpreting deep learning models,\nparticularly in time-series domains such as healthcare, biometrics, and\nhuman-AI interaction. However, standard attribution methods, such as Integrated\nGradients or SHAP, are computationally intensive and not well-suited for\nreal-time applications. We present DeepACTIF, a lightweight and\narchitecture-aware feature attribution method that leverages internal\nactivations of sequence models to estimate feature importance efficiently.\nFocusing on LSTM-based networks, we introduce an inverse-weighted aggregation\nscheme that emphasises stability and magnitude of activations across time\nsteps. Our evaluation across three biometric gaze datasets shows that DeepACTIF\nnot only preserves predictive performance under severe feature reduction (top\n10% of features) but also significantly outperforms established methods,\nincluding SHAP, IG, and DeepLIFT, in terms of both accuracy and statistical\nrobustness. Using Wilcoxon signed-rank tests and effect size analysis, we\ndemonstrate that DeepACTIF yields more informative feature rankings with\nsignificantly lower error across all top-k conditions (10 - 40%). Our\nexperiments demonstrate that DeepACTIF not only reduces computation time and\nmemory usage by orders of magnitude but also preserves model accuracy when\nusing only top-ranked features. That makes DeepACTIF a viable solution for\nreal-time interpretability on edge devices such as mobile XR headsets or\nembedded health monitors.", "AI": {"tldr": "DeepACTIF is a lightweight feature attribution method tailored for real-time applications, addressing inefficiencies in standard methods like Integrated Gradients or SHAP.", "motivation": "Standard feature attribution methods are unsuitable for real-time use in time-series domains due to their high computational cost.", "method": "DeepACTIF leverages internal activations of LSTM-based sequence models. It employs an inverse-weighted aggregation scheme to stabilize and emphasize feature importance.", "result": "DeepACTIF preserves model accuracy under severe feature reduction and outperforms established methods like SHAP and Integrated Gradients in both accuracy and statistical robustness while reducing computational and memory demands.", "conclusion": "DeepACTIF provides a computationally efficient and accurate solution for real-time feature attribution, suitable for edge devices like mobile XR headsets or health monitors."}}
{"id": "2509.19559", "pdf": "https://arxiv.org/pdf/2509.19559", "abs": "https://arxiv.org/abs/2509.19559", "authors": ["Li Zhou", "Elvan Ceyhan"], "title": "Stochastic Path Planning in Correlated Obstacle Fields", "categories": ["stat.ML", "cs.LG", "stat.CO"], "comment": null, "summary": "We introduce the Stochastic Correlated Obstacle Scene (SCOS) problem, a\nnavigation setting with spatially correlated obstacles of uncertain blockage\nstatus, realistically constrained sensors that provide noisy readings and\ncostly disambiguation. Modeling the spatial correlation with Gaussian Random\nField (GRF), we develop Bayesian belief updates that refine blockage\nprobabilities, and use the posteriors to reduce search space for efficiency. To\nfind the optimal traversal policy, we propose a novel two-stage learning\nframework. An offline phase learns a robust base policy via optimistic policy\niteration augmented with information bonus to encourage exploration in\ninformative regions, followed by an online rollout policy with periodic base\nupdates via a Bayesian mechanism for information adaptation. This framework\nsupports both Monte Carlo point estimation and distributional reinforcement\nlearning (RL) to learn full cost distributions, leading to stronger uncertainty\nquantification. We establish theoretical benefits of correlation-aware updating\nand convergence property under posterior sampling. Comprehensive empirical\nevaluations across varying obstacle densities, sensor capabilities demonstrate\nconsistent performance gains over baselines. This framework addresses\nnavigation challenges in environments with adversarial interruptions or\nclustered natural hazards.", "AI": {"tldr": "This paper presents a framework for navigation in environments with obstacles of uncertain status, using Bayesian updates and a two-stage learning method.", "motivation": "The need to navigate efficiently and accurately in environments with spatially correlated obstacles and limited sensors motivates the study.", "method": "The proposed method involves using Bayesian belief updates for better efficiency, alongside a two-stage learning framework combining offline and online components optimized for exploration and adaptation.", "result": "The study demonstrates improvements in navigation performance through empirical testing across different obstacle densities and sensor constraints.", "conclusion": "This framework offers a robust and adaptable approach for navigation in challenging and uncertain environments."}}
{"id": "2509.19821", "pdf": "https://arxiv.org/pdf/2509.19821", "abs": "https://arxiv.org/abs/2509.19821", "authors": ["Weixiong Huang", "Rui Wang", "Wenhua Li", "Sheng Qi", "Tianyu Luo", "Delong Chen", "Tao Zhang", "Ling Wang"], "title": "Fully Tensorized GPU-accelerated Multi-population Evolutionary Algorithm for Constrained Multiobjective Optimization Problems", "categories": ["cs.NE"], "comment": null, "summary": "Real world constrained multiobjective optimization problems (CMOPs) are\nprevalent and often come with stringent time-sensitive requirements. However,\nmost contemporary constrained multiobjective evolutionary algorithms (CMOEAs)\nsuffer from a number of drawbacks, including complex designs, low computational\nefficiency, and long convergence times, which are particularly pronounced when\naddressing time-sensitive CMOPs. Although research on accelerating evolutionary\nalgorithms using GPU parallelism has advanced, existing CMOEAs still face\nsignificant limitations within GPU frameworks. To overcome these challenges,\nthis paper proposes a GPU-accelerated multi-population evolutionary algorithm,\ntermed GMPEA. We first systematically analyze the performance bottlenecks of\nrepresentative CMOEAs when implemented in a GPU environment. To address the\ntrade-off between computational speed and solution performance, GMPEA\nintroduces a decomposition-based multi-population approach that is fully\nparallelized across its entire workflow. We conducted comparative experiments\non various benchmark tests and real world applications: the Weapon Target\nAssignment Problems. The results demonstrate that GMPEA achieves competitive\nperformance even without time constraints, while its computational speed\nsignificantly surpasses that of the compared algorithms. More critically, under\na strict time limit, the performance of GMPEA drastically outperforms its\ncounterparts. This work provides compelling evidence of GMPEA's superiority in\nsolving time-sensitive CMOPs.", "AI": {"tldr": "The paper proposes GMPEA, a GPU-accelerated algorithm designed to solve time-sensitive constrained multiobjective optimization problems efficiently.", "motivation": "Address the limitations of existing CMOEAs, such as inefficiency, complexity, and slow convergence, especially in time-sensitive optimization tasks.", "method": "Developed GMPEA, a GPU-accelerated, decomposition-based multi-population evolutionary algorithm designed to parallelize the entire workflow for computational speed and performance.", "result": "GMPEA achieved competitive outcomes on benchmark tests and Weapon Target Assignment Problems. It notably outperformed existing algorithms under strict time constraints.", "conclusion": "GMPEA demonstrates significant advantages in solving real-world, time-sensitive CMOPs due to its parallelized and efficient design."}}
{"id": "2509.19517", "pdf": "https://arxiv.org/pdf/2509.19517", "abs": "https://arxiv.org/abs/2509.19517", "authors": ["Sai Teja Reddy Adapala"], "title": "Cognitive Load Limits in Large Language Models: Benchmarking Multi-Hop Reasoning", "categories": ["cs.AI", "cs.CL", "cs.LG", "I.2.7; I.2.6"], "comment": null, "summary": "The scaling of Large Language Models (LLMs) has exposed a critical gap\nbetween their performance on static benchmarks and their fragility in dynamic,\ninformation-rich environments. While models excel at isolated tasks, the\ncomputational limits that govern their reasoning under cognitive load remain\npoorly understood. In this work, we introduce a formal theory of computational\ncognitive load, positing that extraneous, task-irrelevant information (Context\nSaturation) and interference from task-switching (Attentional Residue) are key\nmechanisms that degrade performance. We designed the Interleaved Cognitive\nEvaluation (ICE), a deconfounded benchmark to systematically manipulate these\nload factors on challenging multi-hop reasoning tasks. A comprehensive study (N\n= 10 replications per item across 200 questions) revealed significant\nperformance variations across five instruction-tuned models. Smaller\nopen-source architectures (Llama-3-8B-Instruct, Mistral-7B-Instruct-v0.2)\nexhibited baseline brittleness, achieving 0% accuracy (SEM = 0.0) across all\nconditions, including clean controls, on this high-intrinsic-load task. In\ncontrast, Gemini-2.0-Flash-001 showed partial resilience, achieving 85%\naccuracy in control conditions, with a statistically significant degradation\nunder context saturation ($\\beta = -0.003$ per % load, $p < 0.001$). These\nfindings provide preliminary evidence that cognitive load is a key contributor\nto reasoning failures, supporting theories of hallucination-as-guessing under\nuncertainty. We conclude that dynamic, cognitive-aware stress testing, as\nexemplified by the ICE benchmark, is essential for evaluating the true\nresilience and safety of advanced AI systems.", "AI": {"tldr": "The paper introduces a benchmark called Interleaved Cognitive Evaluation (ICE) to assess the performance of Large Language Models (LLMs) under cognitive load conditions, revealing significant weaknesses and showing that cognitive load contributes to reasoning failures.", "motivation": "Large Language Models (LLMs) perform well in static benchmarks but struggle in dynamic, complex environments due to insufficient understanding of how cognitive load impacts their reasoning abilities.", "method": "The authors developed the ICE benchmark to manipulate and measure cognitive load factors (Context Saturation and Attentional Residue) across challenging reasoning tasks, testing multiple models with 200 multi-hop reasoning questions and replication experiments.", "result": "Smaller LLMs underperformed significantly, achieving 0% accuracy, while Gemini-2.0 demonstrated partial resilience with 85% accuracy in control conditions. Performance degraded under cognitive load effects, supporting theories of hallucination during uncertainty.", "conclusion": "Dynamic stress testing that accounts for cognitive load is critical for evaluating and improving the resilience and safety of advanced AI models."}}
{"id": "2509.19673", "pdf": "https://arxiv.org/pdf/2509.19673", "abs": "https://arxiv.org/abs/2509.19673", "authors": ["Ahmed Aljohani", "Anamul Haque Mollah", "Hyunsook Do"], "title": "Assertion Messages with Large Language Models (LLMs) for Code", "categories": ["cs.SE"], "comment": "Accepted at Proceedings of the 2025 Evaluation and Assessment in\n  Software Engineering (EASE '25)", "summary": "Assertion messages significantly enhance unit tests by clearly explaining the\nreasons behind test failures, yet they are frequently omitted by developers and\nautomated test-generation tools. Despite recent advancements, Large Language\nModels (LLMs) have not been systematically evaluated for their ability to\ngenerate informative assertion messages. In this paper, we introduce an\nevaluation of four state-of-the-art Fill-in-the-Middle (FIM) LLMs -\nQwen2.5-Coder-32B, Codestral-22B, CodeLlama-13B, and StarCoder - on a dataset\nof 216 Java test methods containing developer-written assertion messages. We\nfind that Codestral-22B achieves the highest quality score of 2.76 out of 5\nusing a human-like evaluation approach, compared to 3.24 for manually written\nmessages. Our ablation study shows that including descriptive test comments\nfurther improves Codestral's performance to 2.97, highlighting the critical\nrole of context in generating clear assertion messages. Structural analysis\ndemonstrates that all models frequently replicate developers' preferred\nlinguistic patterns. We discuss the limitations of the selected models and\nconventional text evaluation metrics in capturing diverse assertion message\nstructures. Our benchmark, evaluation results, and discussions provide an\nessential foundation for advancing automated, context-aware generation of\nassertion messages in test code. A replication package is available at\nhttps://doi.org/10.5281/zenodo.15293133", "AI": {"tldr": "The paper evaluates four LLMs for generating assertion messages in unit tests, finding that Codestral-22B performs best but still below human-written messages.", "motivation": "Assertion messages are critical for understanding test failures but are often omitted; evaluating LLM performance in generating these messages is needed.", "method": "The study evaluates four Fill-in-the-Middle LLMs on 216 Java test methods with human evaluation and an ablation study on the role of descriptive comments.", "result": "Codestral-22B achieved the best score of 2.76/5, improving to 2.97 with test comments, though all models fell short of the human benchmark of 3.24.", "conclusion": "Current LLMs can replicate linguistic patterns but have limitations in context-aware assertion message generation, paving the way for future improvements."}}
{"id": "2509.19323", "pdf": "https://arxiv.org/pdf/2509.19323", "abs": "https://arxiv.org/abs/2509.19323", "authors": ["V. S. Raghu Parupudi"], "title": "Magnitude Matters: a Superior Class of Similarity Metrics for Holistic Semantic Understanding", "categories": ["cs.CL", "cs.AI"], "comment": "submitted to AAAI 2026", "summary": "Vector comparison in high dimensions is a fundamental task in NLP, yet it is\ndominated by two baselines: the raw dot product, which is unbounded and\nsensitive to vector norms, and the cosine similarity, which discards magnitude\ninformation entirely. This paper challenges both standards by proposing and\nrigorously evaluating a new class of parameter-free, magnitude-aware similarity\nmetrics. I introduce two such functions, Overlap Similarity (OS) and Hyperbolic\nTangent Similarity (HTS), designed to integrate vector magnitude and alignment\nin a more principled manner. To ensure that my findings are robust and\ngeneralizable, I conducted a comprehensive evaluation using four\nstate-of-the-art sentence embedding models (all-MiniLM-L6-v2,\nall-mpnet-base-v2, paraphrase-mpnet-base-v2, and BAAI/bge-large-en-v1.5) across\na diverse suite of eight standard NLP benchmarks, including STS-B, SICK, Quora,\nand PAWS. Using the Wilcoxon signed-rank test for statistical significance, my\nresults are definitive: on the tasks requiring holistic semantic understanding\n(paraphrase and inference), both OS and HTS provide a statistically significant\nimprovement in Mean Squared Error over both the raw dot product and cosine\nsimilarity, regardless of the underlying embedding model.Crucially, my findings\ndelineate the specific domain of advantage for these metrics: for tasks\nrequiring holistic semantic understanding like paraphrase and inference, my\nmagnitude-aware metrics offer a statistically superior alternative. This\nsignificant improvement was not observed on benchmarks designed to test highly\nnuanced compositional semantics (SICK, STS-B), identifying the challenge of\nrepresenting compositional text as a distinct and important direction for\nfuture work.", "AI": {"tldr": "The paper presents two new similarity metrics, Overlap Similarity (OS) and Hyperbolic Tangent Similarity (HTS), which are magnitude-aware and address limitations of dot product and cosine similarity for comparing high-dimensional vectors in NLP.", "motivation": "The paper seeks to improve upon traditional vector comparison methods like raw dot product and cosine similarity, which either overly depend on vector magnitude or discard it entirely. The aim is to create metrics that integrate vector magnitude and alignment to benefit NLP tasks.", "method": "This paper introduces two new similarity metrics, OS and HTS, and evaluates them across four sentence embedding models (e.g., all-MiniLM-L6-v2) on eight NLP benchmarks using the Wilcoxon signed-rank test for statistical significance.", "result": "The OS and HTS metrics significantly improved results on tasks requiring holistic semantic understanding (e.g., paraphrase and inference benchmarks) compared to dot product and cosine similarity. However, no significant advantage was found for benchmarks testing compositional semantics.", "conclusion": "Magnitude-aware similarity metrics like OS and HTS can outperform traditional metrics in specific NLP tasks needing semantic understanding, but their efficacy diminishes in tasks involving nuanced compositional semantics. Future work should address challenges in representing compositional text."}}
{"id": "2509.19463", "pdf": "https://arxiv.org/pdf/2509.19463", "abs": "https://arxiv.org/abs/2509.19463", "authors": ["Doncey Albin", "Daniel McGann", "Miles Mena", "Annika Thomas", "Harel Biggie", "Xuefei Sun", "Steve McGuire", "Jonathan P. How", "Christoffer Heckman"], "title": "CU-Multi: A Dataset for Multi-Robot Collaborative Perception", "categories": ["cs.RO"], "comment": "8 pages, 11 figures", "summary": "A central challenge for multi-robot systems is fusing independently gathered\nperception data into a unified representation. Despite progress in\nCollaborative SLAM (C-SLAM), benchmarking remains hindered by the scarcity of\ndedicated multi-robot datasets. Many evaluations instead partition single-robot\ntrajectories, a practice that may only partially reflect true multi-robot\noperations and, more critically, lacks standardization, leading to results that\nare difficult to interpret or compare across studies. While several multi-robot\ndatasets have recently been introduced, they mostly contain short trajectories\nwith limited inter-robot overlap and sparse intra-robot loop closures. To\novercome these limitations, we introduce CU-Multi, a dataset collected over\nmultiple days at two large outdoor sites on the University of Colorado Boulder\ncampus. CU-Multi comprises four synchronized runs with aligned start times and\ncontrolled trajectory overlap, replicating the distinct perspectives of a robot\nteam. It includes RGB-D sensing, RTK GPS, semantic LiDAR, and refined\nground-truth odometry. By combining overlap variation with dense semantic\nannotations, CU-Multi provides a strong foundation for reproducible evaluation\nin multi-robot collaborative perception tasks.", "AI": {"tldr": "The paper introduces CU-Multi, a new multi-robot dataset addressing current challenges in Collaborative SLAM (C-SLAM) benchmarking.", "motivation": "The motivation is to overcome the limitations of existing multi-robot datasets, which often fall short in trajectory overlap, loop closures, and standardization, thus hindering reproducible evaluations.", "method": "The authors collected CU-Multi dataset over multiple days at outdoor sites, featuring synchronized multi-robot runs, dense semantic annotations, and various sensing modalities like RGB-D sensing, RTK GPS, and semantic LiDAR.", "result": "They successfully created a dataset with significant trajectory overlap, refined ground-truth odometry, and semantic annotations to enable better C-SLAM benchmarking.", "conclusion": "CU-Multi establishes a strong foundation for standardized evaluations in multi-robot collaborative perception, addressing existing benchmarking gaps."}}
{"id": "2509.19562", "pdf": "https://arxiv.org/pdf/2509.19562", "abs": "https://arxiv.org/abs/2509.19562", "authors": ["Fnu Shivam", "Nima Najafzadeh", "Yenumula Reddy", "Prashnna Gyawali"], "title": "CURE: Centroid-guided Unsupervised Representation Erasure for Facial Recognition Systems", "categories": ["cs.CV"], "comment": null, "summary": "In the current digital era, facial recognition systems offer significant\nutility and have been widely integrated into modern technological\ninfrastructures; however, their widespread use has also raised serious privacy\nconcerns, prompting regulations that mandate data removal upon request. Machine\nunlearning has emerged as a powerful solution to address this issue by\nselectively removing the influence of specific user data from trained models\nwhile preserving overall model performance. However, existing machine\nunlearning techniques largely depend on supervised techniques requiring\nidentity labels, which are often unavailable in privacy-constrained situations\nor in large-scale, noisy datasets. To address this critical gap, we introduce\nCURE (Centroid-guided Unsupervised Representation Erasure), the first\nunsupervised unlearning framework for facial recognition systems that operates\nwithout the use of identity labels, effectively removing targeted samples while\npreserving overall performance. We also propose a novel metric, the Unlearning\nEfficiency Score (UES), which balances forgetting and retention stability,\naddressing shortcomings in the current evaluation metrics. CURE significantly\noutperforms unsupervised variants of existing unlearning methods. Additionally,\nwe conducted quality-aware unlearning by designating low-quality images as the\nforget set, demonstrating its usability and benefits, and highlighting the role\nof image quality in machine unlearning.", "AI": {"tldr": "The paper proposes CURE, an unsupervised framework to unlearn facial recognition data without identity labels while preserving model performance.", "motivation": "Existing machine unlearning methods rely on identity labels, which are unavailable in privacy-sensitive cases or large noisy datasets.", "method": "CURE achieves unsupervised unlearning by removing targeted sample influences using centroids and introduces the Unlearning Efficiency Score (UES) for better evaluation.", "result": "CURE outperformed unsupervised variants of current methods and successfully managed quality-aware unlearning using low-quality image designation.", "conclusion": "This framework advances machine unlearning capabilities, especially for privacy in facial recognition systems, enhancing adaptability and evaluation metrics."}}
{"id": "2509.19363", "pdf": "https://arxiv.org/pdf/2509.19363", "abs": "https://arxiv.org/abs/2509.19363", "authors": ["Zhuqi Wang", "Qinghe Zhang", "Zhuopei Cheng"], "title": "Analyzing the Impact of Credit Card Fraud on Economic Fluctuations of American Households Using an Adaptive Neuro-Fuzzy Inference System", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Credit card fraud is assuming growing proportions as a major threat to the\nfinancial position of American household, leading to unpredictable changes in\nhousehold economic behavior. To solve this problem, in this paper, a new hybrid\nanalysis method is presented by using the Enhanced ANFIS. The model proposes\nseveral advances of the conventional ANFIS framework and employs a\nmulti-resolution wavelet decomposition module and a temporal attention\nmechanism. The model performs discrete wavelet transformations on historical\ntransaction data and macroeconomic indicators to generate localized economic\nshock signals. The transformed features are then fed into a deep fuzzy rule\nlibrary which is based on Takagi-Sugeno fuzzy rules with adaptive Gaussian\nmembership functions. The model proposes a temporal attention encoder that\nadaptively assigns weights to multi-scale economic behavior patterns,\nincreasing the effectiveness of relevance assessment in the fuzzy inference\nstage and enhancing the capture of long-term temporal dependencies and\nanomalies caused by fraudulent activities. The proposed method differs from\nclassical ANFIS which has fixed input-output relations since it integrates\nfuzzy rule activation with the wavelet basis selection and the temporal\ncorrelation weights via a modular training procedure. Experimental results show\nthat the RMSE was reduced by 17.8% compared with local neuro-fuzzy models and\nconventional LSTM models.", "AI": {"tldr": "The paper proposes an enhanced ANFIS model with wavelet decomposition and temporal attention mechanisms to address credit card fraud, achieving a 17.8% improvement in RMSE over conventional methods.", "motivation": "The motivation is to address the growing threat of credit card fraud in the US, which impacts household economic stability and behavior.", "method": "The method involves an enhanced Adaptive Neuro-Fuzzy Inference System (ANFIS) model that integrates multi-resolution wavelet decomposition to detect economic shock signals and a temporal attention encoder to improve long-term anomaly detection. This approach incorporates adaptive Gaussian membership functions and modular training for higher adaptability.", "result": "The proposed model achieved a significant reduction of 17.8% in RMSE compared to existing neuro-fuzzy and LSTM-based models.", "conclusion": "The enhanced model offers an advanced, more accurate approach to credit card fraud detection by effectively analyzing multi-scale patterns and improving inference capabilities."}}
{"id": "2509.19577", "pdf": "https://arxiv.org/pdf/2509.19577", "abs": "https://arxiv.org/abs/2509.19577", "authors": ["Dohyun Ku", "Catherine D. Chong", "Visar Berisha", "Todd J. Schwedt", "Jing Li"], "title": "MAGIC: Multi-task Gaussian process for joint imputation and classification in healthcare time series", "categories": ["stat.ML", "cs.LG"], "comment": "36 pages, 4 figures", "summary": "Time series analysis has emerged as an important tool for improving patient\ndiagnosis and management in healthcare applications. However, these\napplications commonly face two critical challenges: time misalignment and data\nsparsity. Traditional approaches address these issues through a two-step\nprocess of imputation followed by prediction. We propose MAGIC (Multi-tAsk\nGaussian Process for Imputation and Classification), a novel unified framework\nthat simultaneously performs class-informed missing value imputation and label\nprediction within a hierarchical multi-task Gaussian process coupled with\nfunctional logistic regression. To handle intractable likelihood components,\nMAGIC employs Taylor expansion approximations with bounded error analysis, and\nparameter estimation is performed using EM algorithm with block coordinate\noptimization supported by convergence analysis. We validate MAGIC through two\nhealthcare applications: prediction of post-traumatic headache improvement\nfollowing mild traumatic brain injury and prediction of in-hospital mortality\nwithin 48 hours after ICU admission. In both applications, MAGIC achieves\nsuperior predictive accuracy compared to existing methods. The ability to\ngenerate real-time and accurate predictions with limited samples facilitates\nearly clinical assessment and treatment planning, enabling healthcare providers\nto make more informed treatment decisions.", "AI": {"tldr": "The paper introduces MAGIC, a framework for handling time series in healthcare by addressing data misalignment and sparsity challenges, and it outperforms traditional methods in two medical applications.", "motivation": "Healthcare time series analysis faces critical issues like time misalignment and data sparsity, impeding effective diagnosis and patient management.", "method": "MAGIC combines imputation and prediction within a hierarchical multi-task Gaussian process and functional logistic regression, using Taylor expansion approximations and an EM algorithm for parameter optimization.", "result": "MAGIC demonstrated superior predictive accuracy in predicting post-traumatic headache improvement and ICU in-hospital mortality compared to existing methods.", "conclusion": "MAGIC enables real-time, accurate decision-making in healthcare contexts, significantly aiding early patient assessment and treatment planning."}}
{"id": "2509.20049", "pdf": "https://arxiv.org/pdf/2509.20049", "abs": "https://arxiv.org/abs/2509.20049", "authors": ["Alastair Poole", "Stig McArthur", "Saravan Kumar"], "title": "Projective Kolmogorov Arnold Neural Networks (P-KANs): Entropy-Driven Functional Space Discovery for Interpretable Machine Learning", "categories": ["cs.NE", "cs.AI", "cs.LG"], "comment": null, "summary": "Kolmogorov-Arnold Networks (KANs) relocate learnable nonlinearities from\nnodes to edges, demonstrating remarkable capabilities in scientific machine\nlearning and interpretable modeling. However, current KAN implementations\nsuffer from fundamental inefficiencies due to redundancy in high-dimensional\nspline parameter spaces, where numerous distinct parameterisations yield\nfunctionally equivalent behaviors. This redundancy manifests as a \"nuisance\nspace\" in the model's Jacobian, leading to susceptibility to overfitting and\npoor generalization. We introduce Projective Kolmogorov-Arnold Networks\n(P-KANs), a novel training framework that guides edge function discovery\ntowards interpretable functional representations through entropy-minimisation\ntechniques from signal analysis and sparse dictionary learning. Rather than\nconstraining functions to predetermined spaces, our approach maintains spline\nspace flexibility while introducing \"gravitational\" terms that encourage\nconvergence towards optimal functional representations. Our key insight\nrecognizes that optimal representations can be identified through entropy\nanalysis of projection coefficients, compressing edge functions to\nlower-parameter projective spaces (Fourier, Chebyshev, Bessel). P-KANs\ndemonstrate superior performance across multiple domains, achieving up to 80%\nparameter reduction while maintaining representational capacity, significantly\nimproved robustness to noise compared to standard KANs, and successful\napplication to industrial automated fiber placement prediction. Our approach\nenables automatic discovery of mixed functional representations where different\nedges converge to different optimal spaces, providing both compression benefits\nand enhanced interpretability for scientific machine learning applications.", "AI": {"tldr": "Projective Kolmogorov-Arnold Networks (P-KANs) mitigate inefficiencies in Kolmogorov-Arnold Networks using entropy-minimization techniques to discover interpretable and reduced-parameter functional representations.", "motivation": "Current Kolmogorov-Arnold Networks are inefficient due to redundancy in high-dimensional spline parameter spaces, which leads to issues like overfitting and poor generalization.", "method": "P-KANs introduce a training framework that utilizes entropy-minimization and sparse dictionary learning to guide edge function discovery while maintaining flexibility in functional spaces and introducing \"gravitational\" terms for optimal convergence.", "result": "P-KANs achieved up to 80% reduction in parameters, improved robustness to noise, and demonstrated successful applications in industrial fiber placement prediction.", "conclusion": "P-KANs provide a more efficient and interpretable alternative to standard KANs, enabling lower-parameter and mixed functional representations with enhanced scientific interpretability."}}
{"id": "2509.20141", "pdf": "https://arxiv.org/pdf/2509.20141", "abs": "https://arxiv.org/abs/2509.20141", "authors": ["Davi Juv\u00eancio Gomes de Sousa", "Caroline da Silva Morais Alves", "Val\u00e9ria Loureiro da Silva", "Nelson Alves Ferreira Neto"], "title": "Digital Signal Processing from Classical Coherent Systems to Continuous-Variable QKD: A Review of Cross-Domain Techniques, Applications, and Challenges", "categories": ["quant-ph", "cs.AR", "cs.ET", "cs.IR", "eess.SP"], "comment": null, "summary": "This systematic review investigates the application of digital signal\nprocessing (DSP) techniques -- originally developed for coherent optical\ncommunication systems to continuous-variable quantum key distribution (CV-QKD).\nThe convergence of these domains has enabled significant advances in CV-QKD\nperformance, particularly in phase synchronization, polarization tracking, and\nexcess noise mitigation. To provide a comprehensive and reproducible synthesis\nof this emerging field, we employed the APISSER methodology, a task-oriented\nframework adapted from the PRISMA protocol. A structured search across IEEE\nXplore and Web of Science databases (2021-2025) yielded 220 relevant\npublications, which were screened, classified, and analyzed to address six\nresearch questions. Our findings highlight that many classical DSP algorithms,\nsuch as Kalman filtering, carrier recovery, adaptive equalization, and\nmachine-learning-assisted signal estimation, have been successfully adapted to\nthe quantum regime, often requiring modifications to meet security and noise\nconstraints. We also identify a range of recent DSP innovations in coherent\noptical communication systems with high potential for future CV-QKD\nintegration, including neural equalization, probabilistic shaping, and joint\nretiming-equalization filters. Despite these advances, challenges remain in\nachieving robust phase tracking under ultra-low Signal-to-Noise Ratio (SNR)\nconditions, real-time polarization compensation, and secure co-existence with\nclassical channels. This review maps current trends, technical barriers, and\nemerging opportunities at the intersection of signal processing for quantum and\nclassical communication, supporting the development of scalable and resilient\nCV-QKD systems.", "AI": {"tldr": "This systematic review explores how digital signal processing (DSP) techniques optimized for optical communications can be applied to continuous-variable quantum key distribution (CV-QKD), highlighting adaptations, challenges, and opportunities in integrating classical and quantum systems.", "motivation": "To investigate how DSP advancements in optical communications contribute to the performance and scalability of CV-QKD, aiming to overcome technical barriers and facilitate innovations in quantum communication.", "method": "The APISSER methodology, adapted from PRISMA protocol, was used to conduct a structured search and analysis of 220 publications from IEEE Xplore and Web of Science databases between 2021-2025, focusing on six research questions concerning DSP techniques in CV-QKD.", "result": "Classical DSP algorithms, such as Kalman filtering and adaptive equalization, were successfully modified for CV-QKD. Additionally, recent DSP innovations, like neural equalization and probabilistic shaping, show promise, though challenges persist, especially in signal robustness under ultra-low SNR conditions.", "conclusion": "Integrating classical DSP with CV-QKD enhances system performance but requires tailored adaptations due to quantum constraints. There is promise in emerging techniques, yet the field must address critical barriers for scalability and robustness in quantum communications."}}
{"id": "2509.19524", "pdf": "https://arxiv.org/pdf/2509.19524", "abs": "https://arxiv.org/abs/2509.19524", "authors": ["Ramy ElMallah", "Krish Chhajer", "Chi-Guhn Lee"], "title": "Score the Steps, Not Just the Goal: VLM-Based Subgoal Evaluation for Robotic Manipulation", "categories": ["cs.AI", "cs.RO"], "comment": "Accepted to the CoRL 2025 Eval&Deploy Workshop", "summary": "Robot learning papers typically report a single binary success rate (SR),\nwhich obscures where a policy succeeds or fails along a multi-step manipulation\ntask. We argue that subgoal-level reporting should become routine: for each\ntrajectory, a vector of per-subgoal SRs that makes partial competence visible\n(e.g., grasp vs. pour). We propose a blueprint for StepEval, a cost-aware\nplug-in evaluation framework that utilizes vision-language models (VLMs) as\nautomated judges of subgoal outcomes from recorded images or videos. Rather\nthan proposing new benchmarks or APIs, our contribution is to outline design\nprinciples for a scalable, community-driven open-source project. In StepEval,\nthe primary artifact for policy evaluation is the per-subgoal SR vector;\nhowever, other quantities (e.g., latency or cost estimates) are also considered\nfor framework-optimization diagnostics to help the community tune evaluation\nefficiency and accuracy when ground-truth subgoal success labels are available.\nWe discuss how such a framework can remain model-agnostic, support single- or\nmulti-view inputs, and be lightweight enough to adopt across labs. The intended\ncontribution is a shared direction: a minimal, extensible seed that invites\nopen-source contributions, so that scoring the steps, not just the final goal,\nbecomes a standard and reproducible practice.", "AI": {"tldr": "Robot learning papers often obscure partial task competency when evaluating outcomes. This paper introduces StepEval, a framework emphasizing per-subgoal success rate (SR) evaluation using vision-language models.", "motivation": "The authors aim to address the limitation of traditional robot learning evaluations, which focus solely on binary success rates and neglect detailed analysis of success across subgoals.", "method": "StepEval uses vision-language models to automatically assess subgoal outcomes within manipulation tasks. It creates vectors of per-subgoal SRs from recorded images or videos, allowing granular evaluation.", "result": "The framework is designed to combine efficiency, scalability, and model-agnostic principles for robot behavior evaluation, along with diagnostics aiding optimization.", "conclusion": "StepEval promotes a shared direction for robot learning evaluation, emphasizing per-step scoring and inviting community contributions for more reproducible and detailed results."}}
{"id": "2509.19729", "pdf": "https://arxiv.org/pdf/2509.19729", "abs": "https://arxiv.org/abs/2509.19729", "authors": ["Haoyu Chen", "Xue Li", "Kun Qian", "Yu Guan", "Jin Zhao", "Xin Wang"], "title": "Gyges: Dynamic Cross-Instance Parallelism Transformation for Efficient LLM Inference", "categories": ["cs.DC"], "comment": "12 pages, 15 figures", "summary": "Efficiently processing the dynamics of requests, especially the context\nlength variance, is important in Large Language Model (LLM) serving scenarios.\nHowever, there is an intrinsic trade-off: while leveraging parallelism\nstrategies, such as Tensor Parallelism (TP), can coordinate multiple GPUs to\naccommodate larger context lengths, it inevitably results in degraded overall\nthroughput. In this paper, we propose Cross-Instance Parallelism Transformation\n(Gyges), which adaptively adjusts the parallelism strategies of running\ninstances to align with the dynamics of incoming requests. We design (1) a\npage-friendly, header-centric layout to accelerate KV cache transformations;\n(2) dedicated weight padding to accelerate model weight transformations; and\n(3) a transformation-aware scheduler to cooperatively schedule requests and\nparallelism transformations, optimizing the overall performance. Evaluations\nusing real-world traces show that Gyges improves throughput by 1.75x-6.57x\ncompared to state-of-the-art solutions.", "AI": {"tldr": "The paper introduces Gyges, a method to dynamically adapt parallelism strategies for LLM serving, ensuring better throughput despite handling varying request context lengths.", "motivation": "The paper addresses inefficiencies caused by context length variance in LLM serving, where adapting to large contexts degrades throughput due to GPU parallelism trade-offs.", "method": "The authors propose Gyges, which includes (1) a novel KV cache layout, (2) optimized weight padding, and (3) a transformation-aware scheduler to adjust parallelism strategies dynamically.", "result": "Gyges achieves a 1.75x-6.57x improvement in throughput compared to existing state-of-the-art methods when tested on real-world datasets.", "conclusion": "Dynamic parallelism strategies can significantly enhance throughput in LLM-serving scenarios, solving key challenges with large context lengths using Gyges."}}
{"id": "2509.19708", "pdf": "https://arxiv.org/pdf/2509.19708", "abs": "https://arxiv.org/abs/2509.19708", "authors": ["Anand Kumar", "Vishal Khare", "Deepak Sharma", "Satyam Kumar", "Vijay Saini", "Anshul Yadav", "Sachendra Jain", "Ankit Rana", "Pratham Verma", "Vaibhav Meena", "Avinash Edubilli"], "title": "Intuition to Evidence: Measuring AI's True Impact on Developer Productivity", "categories": ["cs.SE", "cs.AI", "cs.LG"], "comment": "16 pages, 10 figures, 5 tables", "summary": "We present a comprehensive real-world evaluation of AI-assisted software\ndevelopment tools deployed at enterprise scale. Over one year, 300 engineers\nacross multiple teams integrated an in-house AI platform (DeputyDev) that\ncombines code generation and automated review capabilities into their daily\nworkflows. Through rigorous cohort analysis, our study demonstrates\nstatistically significant productivity improvements, including an overall 31.8%\nreduction in PR review cycle time.\n  Developer adoption was strong, with 85% satisfaction for code review features\nand 93% expressing a desire to continue using the platform. Adoption patterns\nshowed systematic scaling from 4% engagement in month 1 to 83% peak usage by\nmonth 6, stabilizing at 60% active engagement. Top adopters achieved a 61%\nincrease in code volume pushed to production, contributing to approximately 30\nto 40% of code shipped to production through this tool, accounting for an\noverall 28% increase in code shipment volume.\n  Unlike controlled benchmark evaluations, our longitudinal analysis provides\nempirical evidence from production environments, revealing both the\ntransformative potential and practical deployment challenges of integrating AI\ninto enterprise software development workflows.", "AI": {"tldr": "The paper evaluates the impact of an AI tool (DeputyDev) on enterprise-scale software development, showcasing its productivity improvements and practical challenges.", "motivation": "To assess the real-world impact and acceptance of AI-assisted tools in enterprise software development environments.", "method": "300 engineers used DeputyDev, an in-house AI tool, over a year, with its integration into their workflows analyzed through rigorous cohort analysis.", "result": "Notable findings include a 31.8% reduction in PR cycle time, strong user satisfaction, usage peaking at 83% by month 6, a 28% increase in code shipment volume, and top adopters achieving 61% more code pushed to production.", "conclusion": "AI tools like DeputyDev show significant potential in enhancing enterprise software development productivity but also come with deployment challenges in real-world settings."}}
{"id": "2509.19325", "pdf": "https://arxiv.org/pdf/2509.19325", "abs": "https://arxiv.org/abs/2509.19325", "authors": ["Jian Ouyang", "Arman T", "Ge Jin"], "title": "How Much of Your Data Can Suck? Thresholds for Domain Performance and Emergent Misalignment in LLMs", "categories": ["cs.CL"], "comment": null, "summary": "This paper investigates the impact of incorrect data on the performance and\nsafety of large language models (LLMs), specifically gpt-4o, during supervised\nfine-tuning (SFT). Although LLMs become increasingly vital across broad domains\nlike finance, coding, law, and health, fine-tuning on incorrect data can lead\nto \"emergent misalignment,\" producing harmful or deceptive outputs unrelated to\nthe intended task. We evaluate gpt-4o models fine-tuned with varying ratios\n(10\\% to 90\\% correct) of both obviously and subtly incorrect data across four\ndomains: coding, finance, health, and legal. Our findings show that even modest\namounts of incorrect data (10-25\\%) dramatically degrade domain performance and\nnot moral alignment. A clear threshold of at least 50\\% correct data is needed\nfor models to consistently recover strong performance, though they rarely match\nthe robustness and safety of the base model, which exhibits near-perfect\nalignment and zero dangerous completions out-of-the-box. This research\nemphasizes that the cost of incorrect data is heavy, highlighting the critical\nneed for extremely high-quality data curation or, alternatively, leveraging\nrobust base models without unnecessary fine-tuning for high-stakes\napplications.", "AI": {"tldr": "The paper studies the impact of incorrect data on fine-tuned large language models like gpt-4o, finding that even small amounts of incorrect data significantly harm its performance and safety.", "motivation": "To explore how fine-tuning with incorrect data can lead to emergent misalignment, which causes harmful or deceptive outputs in high-stakes domains like finance, law, and health.", "method": "The paper evaluates gpt-4o fine-tuned with varying ratios (10% to 90%) of incorrect data, across four domains: coding, finance, health, and legal.", "result": "It finds that 10-25% incorrect data greatly degrades performance while reducing correct moral alignment. A minimum of 50% correct data is required for recovery, but performance rarely matches that of base models.", "conclusion": "High-stakes applications should avoid fine-tuning with incorrect data, focus on high-quality data curation, or use base models without fine-tuning to ensure safety and robustness."}}
{"id": "2509.19473", "pdf": "https://arxiv.org/pdf/2509.19473", "abs": "https://arxiv.org/abs/2509.19473", "authors": ["Adarsh Salagame", "Henry Noyes", "Alireza Ramezani", "Eric Sihite", "Arash Kalantari"], "title": "Crater Observing Bio-inspired Rolling Articulator (COBRA)", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "NASA aims to establish a sustainable human basecamp on the Moon as a stepping\nstone for future missions to Mars and beyond. The discovery of water ice on the\nMoon's craters located in permanently shadowed regions, which can provide\ndrinking water, oxygen, and rocket fuel, is therefore of critical importance.\nHowever, current methods to access lunar ice deposits are limited. While rovers\nhave been used to explore the lunar surface for decades, they face significant\nchallenges in navigating harsh terrains, such as permanently shadowed craters,\ndue to the high risk of immobilization. This report introduces COBRA (Crater\nObserving Bio-inspired Rolling Articulator), a multi-modal snake-style robot\ndesigned to overcome mobility challenges in Shackleton Crater's rugged\nenvironment. COBRA combines slithering and tumbling locomotion to adapt to\nvarious crater terrains. In snake mode, it uses sidewinding to traverse flat or\nlow inclined surfaces, while in tumbling mode, it forms a circular barrel by\nlinking its head and tail, enabling rapid movement with minimal energy on steep\nslopes. Equipped with an onboard computer, stereo camera, inertial measurement\nunit, and joint encoders, COBRA facilitates real-time data collection and\nautonomous operation. This paper highlights COBRAs robustness and efficiency in\nnavigating extreme terrains through both simulations and experimental\nvalidation.", "AI": {"tldr": "NASA proposes COBRA, a snake-style robot, to tackle lunar crater terrain challenges.", "motivation": "Navigate harsh terrains in lunar craters to access water ice for human sustainability.", "method": "Designed COBRA with dual locomotion modes: slithering and tumbling, equipped with advanced sensors.", "result": "Simulation and experimental tests validate COBRA's adaptability and robustness in extreme terrains.", "conclusion": "COBRA is robust, efficient, and could revolutionize lunar exploration."}}
{"id": "2509.19589", "pdf": "https://arxiv.org/pdf/2509.19589", "abs": "https://arxiv.org/abs/2509.19589", "authors": ["Dennis Menn", "Feng Liang", "Diana Marculescu"], "title": "Synthesizing Artifact Dataset for Pixel-level Detection", "categories": ["cs.CV"], "comment": "Under submission to WACV", "summary": "Artifact detectors have been shown to enhance the performance of\nimage-generative models by serving as reward models during fine-tuning. These\ndetectors enable the generative model to improve overall output fidelity and\naesthetics. However, training the artifact detector requires expensive\npixel-level human annotations that specify the artifact regions. The lack of\nannotated data limits the performance of the artifact detector. A naive\npseudo-labeling approach-training a weak detector and using it to annotate\nunlabeled images-suffers from noisy labels, resulting in poor performance. To\naddress this, we propose an artifact corruption pipeline that automatically\ninjects artifacts into clean, high-quality synthetic images on a predetermined\nregion, thereby producing pixel-level annotations without manual labeling. The\nproposed method enables training of an artifact detector that achieves\nperformance improvements of 13.2% for ConvNeXt and 3.7% for Swin-T, as verified\non human-labeled data, compared to baseline approaches. This work represents an\ninitial step toward scalable pixel-level artifact annotation datasets that\nintegrate world knowledge into artifact detection.", "AI": {"tldr": "Authors propose an artifact corruption pipeline to generate pixel-level annotations for artifact detectors used in image-generative models, eliminating the need for manual labeling and achieving improved detector performance.", "motivation": "Artifact detector training requires expensive pixel-level human annotations, which limits its scalability and performance in assisting image-generative models.", "method": "The authors developed an artifact corruption pipeline to inject artifacts into high-quality synthetic images at predetermined regions, enabling automatic pixel-level annotation without human intervention.", "result": "Using the proposed method, artifact detectors showed significant performance improvements, with ConvNeXt improving by 13.2% and Swin-T by 3.7%, validated on human-labeled data.", "conclusion": "This pipeline represents a scalable approach to generating annotated datasets that facilitate artifact detection and integrate world knowledge, improving fidelity in generative models."}}
{"id": "2509.19366", "pdf": "https://arxiv.org/pdf/2509.19366", "abs": "https://arxiv.org/abs/2509.19366", "authors": ["Buhe Li", "Berkay Kaplan", "Maksym Lazirko", "Aleksandr Kogan"], "title": "Unsupervised Outlier Detection in Audit Analytics: A Case Study Using USA Spending Data", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "This study investigates the effectiveness of unsupervised outlier detection\nmethods in audit analytics, utilizing USA spending data from the U.S.\nDepartment of Health and Human Services (DHHS) as a case example. We employ and\ncompare multiple outlier detection algorithms, including Histogram-based\nOutlier Score (HBOS), Robust Principal Component Analysis (PCA), Minimum\nCovariance Determinant (MCD), and K-Nearest Neighbors (KNN) to identify\nanomalies in federal spending patterns. The research addresses the growing need\nfor efficient and accurate anomaly detection in large-scale governmental\ndatasets, where traditional auditing methods may fall short. Our methodology\ninvolves data preparation, algorithm implementation, and performance evaluation\nusing precision, recall, and F1 scores. Results indicate that a hybrid\napproach, combining multiple detection strategies, enhances the robustness and\naccuracy of outlier identification in complex financial data. This study\ncontributes to the field of audit analytics by providing insights into the\ncomparative effectiveness of various outlier detection models and demonstrating\nthe potential of unsupervised learning techniques in improving audit quality\nand efficiency. The findings have implications for auditors, policymakers, and\nresearchers seeking to leverage advanced analytics in governmental financial\noversight and risk management.", "AI": {"tldr": "This study explores unsupervised outlier detection methods using U.S. federal spending data to improve audit analytics.", "motivation": "The need for efficient and accurate anomaly detection methods in large-scale governmental datasets, where traditional auditing approaches often fail.", "method": "Using data from DHHS, the study implements multiple outlier algorithms, including HBOS, Robust PCA, MCD, and KNN, and evaluates their performance using metrics like precision, recall, and F1 scores.", "result": "Hybrid approaches combining different methods show improved robustness and accuracy in detecting anomalies in financial data.", "conclusion": "Unsupervised learning techniques demonstrate substantial potential in enhancing audit quality and efficiency, offering valuable insights for auditors, policymakers, and researchers in financial oversight."}}
{"id": "2509.19707", "pdf": "https://arxiv.org/pdf/2509.19707", "abs": "https://arxiv.org/abs/2509.19707", "authors": ["David Huk", "Theodoros Damoulas"], "title": "Diffusion and Flow-based Copulas: Forgetting and Remembering Dependencies", "categories": ["stat.ML", "cs.LG", "stat.CO", "stat.ME"], "comment": "Preprint", "summary": "Copulas are a fundamental tool for modelling multivariate dependencies in\ndata, forming the method of choice in diverse fields and applications. However,\nthe adoption of existing models for multimodal and high-dimensional\ndependencies is hindered by restrictive assumptions and poor scaling. In this\nwork, we present methods for modelling copulas based on the principles of\ndiffusions and flows. We design two processes that progressively forget\ninter-variable dependencies while leaving dimension-wise distributions\nunaffected, provably defining valid copulas at all times. We show how to obtain\ncopula models by learning to remember the forgotten dependencies from each\nprocess, theoretically recovering the true copula at optimality. The first\ninstantiation of our framework focuses on direct density estimation, while the\nsecond specialises in expedient sampling. Empirically, we demonstrate the\nsuperior performance of our proposed methods over state-of-the-art copula\napproaches in modelling complex and high-dimensional dependencies from\nscientific datasets and images. Our work enhances the representational power of\ncopula models, empowering applications and paving the way for their adoption on\nlarger scales and more challenging domains.", "AI": {"tldr": "The paper introduces diffusion- and flow-based methods to enhance copula models for high-dimensional and complex dependencies, outperforming existing models while simplifying scalability.", "motivation": "Current copula models struggle with restrictive assumptions and scalability challenges for multimodal and high-dimensional dependencies.", "method": "The authors design two processes based on diffusions and flows to forget and then learn inter-variable dependencies, with one method focused on density estimation and the other on sampling.", "result": "The methods demonstrate superior performance compared to state-of-the-art copula approaches in scientific and image datasets, accurately modeling high-dimensional dependencies.", "conclusion": "This work increases the utility and scalability of copula models, supporting broader applications and enabling use on more complex datasets."}}
{"id": "2509.20284", "pdf": "https://arxiv.org/pdf/2509.20284", "abs": "https://arxiv.org/abs/2509.20284", "authors": ["Changze Lv", "Yifei Wang", "Yanxun Zhang", "Yiyang Lu", "Jingwen Xu", "Di Yu", "Xin Du", "Xuanjing Huang", "Xiaoqing Zheng"], "title": "Biologically Plausible Learning via Bidirectional Spike-Based Distillation", "categories": ["cs.NE"], "comment": null, "summary": "Developing biologically plausible learning algorithms that can achieve\nperformance comparable to error backpropagation remains a longstanding\nchallenge. Existing approaches often compromise biological plausibility by\nentirely avoiding the use of spikes for error propagation or relying on both\npositive and negative learning signals, while the question of how spikes can\nrepresent negative values remains unresolved. To address these limitations, we\nintroduce Bidirectional Spike-based Distillation (BSD), a novel learning\nalgorithm that jointly trains a feedforward and a backward spiking network. We\nformulate learning as a transformation between two spiking representations\n(i.e., stimulus encoding and concept encoding) so that the feedforward network\nimplements perception and decision-making by mapping stimuli to actions, while\nthe backward network supports memory recall by reconstructing stimuli from\nconcept representations. Extensive experiments on diverse benchmarks, including\nimage recognition, image generation, and sequential regression, show that BSD\nachieves performance comparable to networks trained with classical error\nbackpropagation. These findings represent a significant step toward\nbiologically grounded, spike-driven learning in neural networks.", "AI": {"tldr": "The paper introduces Bidirectional Spike-based Distillation (BSD), a biologically plausible learning algorithm, that achieves competitive performance with classical error backpropagation.", "motivation": "To develop biologically plausible learning algorithms using spikes that address limitations of previous methods, such as inability to represent negative values effectively.", "method": "The BSD algorithm trains a feedforward and a backward spiking network together, framing learning as a transformation between two spiking representations for tasks like perception, decision-making, and memory recall.", "result": "BSD demonstrated strong performance across diverse benchmarks like image recognition, generation, and sequential regression, performing comparably to classical error backpropagation.", "conclusion": "The findings highlight BSD as a step forward in achieving biologically plausible, spike-driven learning mechanisms in neural networks."}}
{"id": "2509.20215", "pdf": "https://arxiv.org/pdf/2509.20215", "abs": "https://arxiv.org/abs/2509.20215", "authors": ["Guang Yang", "Wei Zheng", "Xiang Chen", "Yifan Sun", "Fengji Zhang", "Terry Yue Zhuo"], "title": "The Cream Rises to the Top: Efficient Reranking Method for Verilog Code Generation", "categories": ["cs.SE", "cs.AI", "cs.AR"], "comment": "Under review ICASSP 2026", "summary": "LLMs face significant challenges in Verilog generation due to limited\ndomain-specific knowledge. While sampling techniques improve pass@k metrics,\nhardware engineers need one trustworthy solution rather than uncertain\ncandidates. To bridge this gap, we formulate it as a semantic alignment problem\nbetween requirements and Verilog implementations, and propose VCD-RNK, a\ndiscriminator model tailored for efficient Verilog code reranking.\nSpecifically, VCD-RNKincorporates Verilog-specific reasoning by distilling\nexpert knowledge across three dimensions: code semantic analysis, test case\ngeneration, and functional correctness assessment. By explicitly simulating the\nabove reasoning processes during inference, VCD-RNK effectively avoids\ncomputationally intensive test execution in existing methods.", "AI": {"tldr": "The paper addresses LLMs' challenges in Verilog generation and proposes VCD-RNK, a model for effectively reranking Verilog code candidates.", "motivation": "To provide hardware engineers with a trustworthy Verilog code generation solution, overcoming the uncertainty of multiple candidates provided by LLMs.", "method": "VCD-RNK uses Verilog-specific reasoning by distilling expert knowledge via semantic analysis, test case generation, and functional correctness assessment, avoiding computationally intensive methods.", "result": "VCD-RNK delivers efficient Verilog code reranking by simulating necessary reasoning processes, enhancing reliability and reducing computation.", "conclusion": "The proposed VCD-RNK model effectively bridges the gap by addressing semantic alignment challenges, offering a reliable and efficient solution for Verilog code generation."}}
{"id": "2509.19566", "pdf": "https://arxiv.org/pdf/2509.19566", "abs": "https://arxiv.org/abs/2509.19566", "authors": ["George Hong", "Daniel Trejo Banos"], "title": "Nano Bio-Agents (NBA): Small Language Model Agents for Genomics", "categories": ["cs.AI", "q-bio.GN"], "comment": null, "summary": "We investigate the application of Small Language Models (<10 billion\nparameters) for genomics question answering via agentic framework to address\nhallucination issues and computational cost challenges. The Nano Bio-Agent\n(NBA) framework we implemented incorporates task decomposition, tool\norchestration, and API access into well-established systems such as NCBI and\nAlphaGenome. Results show that SLMs combined with such agentic framework can\nachieve comparable and in many cases superior performance versus existing\napproaches utilising larger models, with our best model-agent combination\nachieving 98% accuracy on the GeneTuring benchmark. Notably, small 3-10B\nparameter models consistently achieve 85-97% accuracy while requiring much\nlower computational resources than conventional approaches. This demonstrates\npromising potential for efficiency gains, cost savings, and democratization of\nML-powered genomics tools while retaining highly robust and accurate\nperformance.", "AI": {"tldr": "The study explores using Small Language Models (SLMs) with less than 10 billion parameters for genomics question answering, leveraging an agentic framework for efficiency and reduced hallucinations. Their framework achieves robust performance and efficiency.", "motivation": "The paper aims to address challenges related to hallucination issues and high computational costs in genomics question answering by utilizing smaller language models combined with an agentic framework.", "method": "Implementation of the Nano Bio-Agent (NBA) framework integrating task decomposition, tool orchestration, and API access with established systems like NCBI and AlphaGenome.", "result": "The combination of SLMs and the NBA framework achieved up to 98% accuracy on the GeneTuring benchmark, performing comparably or better than larger models while significantly reducing computational cost.", "conclusion": "SLMs with an agentic framework demonstrate strong potential to offer cost-effective and efficient solutions in genomics question answering without compromising accuracy, paving the way for democratization of ML tools in genomics."}}
{"id": "2509.19836", "pdf": "https://arxiv.org/pdf/2509.19836", "abs": "https://arxiv.org/abs/2509.19836", "authors": ["Ao Sun", "Weilin Zhao", "Xu Han", "Cheng Yang", "Zhiyuan Liu", "Chuan Shi", "Maosong sun"], "title": "BurstEngine: an Efficient Distributed Framework for Training Transformers on Extremely Long Sequences of over 1M Tokens", "categories": ["cs.DC"], "comment": null, "summary": "Existing methods for training LLMs on long-sequence data, such as Tensor\nParallelism and Context Parallelism, exhibit low Model FLOPs Utilization as\nsequence lengths and number of GPUs increase, especially when sequence lengths\nexceed 1M tokens. To address these challenges, we propose BurstEngine, an\nefficient framework designed to train LLMs on long-sequence data. BurstEngine\nintroduces BurstAttention, an optimized distributed attention with lower\ncommunication cost than RingAttention. BurstAttention leverages topology-aware\nring communication to fully utilize network bandwidth and incorporates\nfine-grained communication-computation overlap. Furthermore, BurstEngine\nintroduces sequence-level selective checkpointing and fuses the language\nmodeling head with the loss function to reduce memory cost. Additionally,\nBurstEngine introduces workload balance optimization for various types of\nattention masking. By integrating these optimizations, BurstEngine achieves a\n$1.2\\times$ speedup with much lower memory overhead than the state-of-the-art\nbaselines when training LLMs on extremely long sequences of over 1M tokens. We\nhave made our code publicly available on GitHub:\nhttps://github.com/thunlp/BurstEngine.", "AI": {"tldr": "BurstEngine is presented as a novel framework to efficiently train Large Language Models (LLMs) on extremely long sequences, exceeding 1M tokens. It achieves significant improvements in speed and memory usage compared to existing methods.", "motivation": "Existing methods like Tensor Parallelism suffer from inefficiencies in FLOP utilization and scalability when handling long sequences and multiple GPUs, especially when sequence lengths surpass 1M tokens.", "method": "BurstEngine integrates several innovations such as BurstAttention with optimized communication costs, selective checkpointing, fused loss function, and workload balance optimization for attention masking.", "result": "BurstEngine outperforms state-of-the-art methods, achieving a 1.2x speedup and lower memory overhead for training tasks involving sequences longer than 1M tokens.", "conclusion": "BurstEngine provides a scalable and memory-efficient solution for training LLMs on extremely long sequence data, backed by strong empirical improvements and public availability of the code."}}
{"id": "2509.19918", "pdf": "https://arxiv.org/pdf/2509.19918", "abs": "https://arxiv.org/abs/2509.19918", "authors": ["Micheline B\u00e9n\u00e9dicte Moumoula", "Serge Lionel Nikiema", "Alb\u00e9rick Euraste Djire", "Abdoul Kader Kabore", "Jacques Klein", "Tegawend\u00e9 F. Bissyande"], "title": "Beyond Language Barriers: Multi-Agent Coordination for Multi-Language Code Generation", "categories": ["cs.SE"], "comment": null, "summary": "Producing high-quality code across multiple programming languages is\nincreasingly important as today's software systems are built on heterogeneous\nstacks. Large language models (LLMs) have advanced the state of automated\nprogramming, yet their proficiency varies sharply between languages, especially\nthose with limited training data such as Rust, Perl, OCaml, and Erlang. Many\ncurrent solutions including language-specific fine-tuning, multi-agent\norchestration, transfer learning, and intermediate-representation pipelines\nstill approach each target language in isolation, missing opportunities to\nshare knowledge or exploit recurring cross-language patterns.\n  XL-CoGen tackles this challenge with a coordinated multi-agent architecture\nthat integrates intermediate representation, code generation, translation, and\nautomated repair. Its distinguishing feature is a data-driven mechanism for\nselecting bridging languages: empirically derived transfer matrices identify\nthe best intermediate languages based on demonstrated translation success\nrather than raw generation accuracy. The system performs early output\nvalidation, iteratively corrects errors, and reuses intermediate artifacts as\ncontextual scaffolds for subsequent translations.\n  Extensive experiments show that XL-CoGen yields notable improvements with 13\npercentage-point gains over the strongest fine-tuned baseline and as much as 30\npercentage points over existing single-language multi-agent methods. Ablation\nstudies further demonstrate that compatibility-guided bridging significantly\noutperforms LLM-based heuristics, confirming the value of cumulative\ncross-language knowledge transfer.", "AI": {"tldr": "XL-CoGen enhances cross-language code generation by leveraging a coordinated multi-agent system with bridging languages, significantly improving translation and repair accuracy, especially in underrepresented programming languages.", "motivation": "To address the uneven proficiency of LLMs in generating high-quality code for underrepresented programming languages and to exploit recurring patterns across languages.", "method": "Uses a coordinated multi-agent architecture combining intermediate representation, translation, and automated repair with a data-driven approach for selecting bridging languages based on transfer matrices of translation success.", "result": "Achieves up to 13 percentage points improvement over fine-tuned baselines and 30 percentage points over existing single-language methods, demonstrating significant performance gains in cross-language code generation.", "conclusion": "The system successfully enhances LLMs' cross-language adaptability by leveraging compatibility-guided knowledge transfer and iterative error correction, emphasizing the potential of shared patterns across languages."}}
{"id": "2509.19326", "pdf": "https://arxiv.org/pdf/2509.19326", "abs": "https://arxiv.org/abs/2509.19326", "authors": ["Ruochi Li", "Haoxuan Zhang", "Edward Gehringer", "Ting Xiao", "Junhua Ding", "Haihua Chen"], "title": "Unveiling the Merits and Defects of LLMs in Automatic Review Generation for Scientific Papers", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted as short paper at 25th IEEE International Conference on Data\n  Mining", "summary": "The surge in scientific submissions has placed increasing strain on the\ntraditional peer-review process, prompting the exploration of large language\nmodels (LLMs) for automated review generation. While LLMs demonstrate\ncompetence in producing structured and coherent feedback, their capacity for\ncritical reasoning, contextual grounding, and quality sensitivity remains\nlimited. To systematically evaluate these aspects, we propose a comprehensive\nevaluation framework that integrates semantic similarity analysis and\nstructured knowledge graph metrics to assess LLM-generated reviews against\nhuman-written counterparts. We construct a large-scale benchmark of 1,683\npapers and 6,495 expert reviews from ICLR and NeurIPS in multiple years, and\ngenerate reviews using five LLMs. Our findings show that LLMs perform well in\ndescriptive and affirmational content, capturing the main contributions and\nmethodologies of the original work, with GPT-4o highlighted as an illustrative\nexample, generating 15.74% more entities than human reviewers in the strengths\nsection of good papers in ICLR 2025. However, they consistently underperform in\nidentifying weaknesses, raising substantive questions, and adjusting feedback\nbased on paper quality. GPT-4o produces 59.42% fewer entities than real\nreviewers in the weaknesses and increases node count by only 5.7% from good to\nweak papers, compared to 50% in human reviews. Similar trends are observed\nacross all conferences, years, and models, providing empirical foundations for\nunderstanding the merits and defects of LLM-generated reviews and informing the\ndevelopment of future LLM-assisted reviewing tools. Data, code, and more\ndetailed results are publicly available at\nhttps://github.com/RichardLRC/Peer-Review.", "AI": {"tldr": "This paper evaluates the use of large language models (LLMs) in the peer-review process, highlighting their strengths in descriptive feedback but weaknesses in critical reasoning and contextual accuracy.", "motivation": "The motivation arises from the increased volume of scientific submissions, which has overwhelmed the traditional peer-review system and necessitated exploration of LLMs for automated review generation.", "method": "The paper proposes an evaluation framework combining semantic similarity analysis and structured knowledge graph metrics to assess LLM-generated reviews. It creates a benchmark dataset of 1,683 papers and 6,495 expert reviews and tests five LLMs, including GPT-4o.", "result": "LLMs perform well in summarizing contributions and methodologies but underperform in identifying weaknesses and providing critical feedback. For instance, GPT-4o generated more descriptive content than human reviewers but was significantly less effective in highlighting flaws and adapting to paper quality.", "conclusion": "While LLMs like GPT-4o show promise in assisting the review process, they lack the critical reasoning abilities of human reviewers. The study identifies key areas for improvement to enhance the effectiveness of LLM-assisted peer reviews."}}
{"id": "2509.19480", "pdf": "https://arxiv.org/pdf/2509.19480", "abs": "https://arxiv.org/abs/2509.19480", "authors": ["Noriaki Hirose", "Catherine Glossop", "Dhruv Shah", "Sergey Levine"], "title": "OmniVLA: An Omni-Modal Vision-Language-Action Model for Robot Navigation", "categories": ["cs.RO", "cs.LG"], "comment": "9 pages, 7 figures, 6 tables", "summary": "Humans can flexibly interpret and compose different goal specifications, such\nas language instructions, spatial coordinates, or visual references, when\nnavigating to a destination. In contrast, most existing robotic navigation\npolicies are trained on a single modality, limiting their adaptability to\nreal-world scenarios where different forms of goal specification are natural\nand complementary. In this work, we present a training framework for robotic\nfoundation models that enables omni-modal goal conditioning for vision-based\nnavigation. Our approach leverages a high-capacity vision-language-action (VLA)\nbackbone and trains with three primary goal modalities: 2D poses, egocentric\nimages, and natural language, as well as their combinations, through a\nrandomized modality fusion strategy. This design not only expands the pool of\nusable datasets but also encourages the policy to develop richer geometric,\nsemantic, and visual representations. The resulting model, OmniVLA, achieves\nstrong generalization to unseen environments, robustness to scarce modalities,\nand the ability to follow novel natural language instructions. We demonstrate\nthat OmniVLA outperforms specialist baselines across modalities and offers a\nflexible foundation for fine-tuning to new modalities and tasks. We believe\nOmniVLA provides a step toward broadly generalizable and flexible navigation\npolicies, and a scalable path for building omni-modal robotic foundation\nmodels. We present videos showcasing OmniVLA performance and will release its\ncheckpoints and training code on our project page.", "AI": {"tldr": "This paper introduces OmniVLA, an omni-modal goal conditioning framework for robotic navigation that integrates vision, language, and action for flexibility and robustness.", "motivation": "Existing robotic navigation systems are limited by their reliance on single-modal goal specifications, making them less adaptable to real-world scenarios where multiple modalities often coexist.", "method": "The paper proposes a training framework utilizing a vision-language-action backbone, randomized modality fusion, and three primary goal modalities (2D poses, egocentric images, natural language) to train the OmniVLA model.", "result": "The OmniVLA model demonstrates strong generalization to unseen environments, robustness with limited modalities, and the ability to follow new natural language instructions. It outperforms specialized baselines across multiple modalities.", "conclusion": "OmniVLA represents a significant step towards omni-modal robotic navigation, offering flexibility, scalability, and a foundation for further extension to new modalities and tasks."}}
{"id": "2509.19602", "pdf": "https://arxiv.org/pdf/2509.19602", "abs": "https://arxiv.org/abs/2509.19602", "authors": ["Neeraj Gangwar", "Anshuka Rangi", "Rishabh Deshmukh", "Holakou Rahmanian", "Yesh Dattatreya", "Nickvash Kani"], "title": "Parameter-Efficient Multi-Task Learning via Progressive Task-Specific Adaptation", "categories": ["cs.CV"], "comment": null, "summary": "Parameter-efficient fine-tuning methods have emerged as a promising solution\nfor adapting pre-trained models to various downstream tasks. While these\nmethods perform well in single-task learning, extending them to multi-task\nlearning exacerbates common challenges, such as task interference and negative\ntransfer, due to the limited number of trainable parameters. To address these\nissues, we introduce progressive task-specific multi-task adaptation, a novel\nparameter-efficient approach for multi-task learning. This approach introduces\nadapter modules in a pre-trained model such that these modules are shared\nacross all tasks in the initial layers and become progressively more\ntask-specific in the later layers. The motivation is to reduce the conflicts\namong tasks by allowing transfer learning across all tasks in the initial\nlayers and enabling task-specific learning toward the prediction heads.\nAdditionally, we propose a gradient-based approach for computing task\nsimilarity and use this measure to allocate similar tasks to the shared adapter\nmodules. Our task similarity method introduces minimal overhead in the\npipeline. We evaluate our approach by adapting the Swin Transformer for dense\nprediction tasks. Experiments on the PASCAL and NYUD-v2 datasets demonstrate\nthat our approach outperforms a fully fine-tuned multi-task model while\nrequiring only one-fifth of the trainable parameters. This approach achieves\nbetter relative improvement to single-task fine-tuning while reducing the\nnumber of trainable parameters and surpasses the current state-of-the-art\nmethods for parameter-efficient multi-task learning.", "AI": {"tldr": "This paper proposes a novel approach for multi-task learning using pre-trained models, called progressive task-specific multi-task adaptation, which minimizes task interference while performing better than state-of-the-art methods with fewer trainable parameters.", "motivation": "The motivation is to address task interference and negative transfer challenges when extending parameter-efficient fine-tuning methods to multi-task learning.", "method": "A progressive task-specific adaptation method is introduced, where adapter modules in a pre-trained model are shared for initial layers across all tasks and become task-specific towards later layers. A gradient-based task similarity measure is also proposed to allocate similar tasks to shared modules.", "result": "The proposed approach outperforms fully fine-tuned multi-task models, achieves better relative improvement compared to single-task fine-tuning, and reduces trainable parameters by one-fifth. Experiments on datasets such as PASCAL and NYUD-v2 affirm these results.", "conclusion": "This method successfully balances transfer learning and task specialization, proving highly effective for parameter-efficient multi-task learning while surpassing existing techniques."}}
{"id": "2509.19372", "pdf": "https://arxiv.org/pdf/2509.19372", "abs": "https://arxiv.org/abs/2509.19372", "authors": ["Zuzanna Dubanowska", "Maciej \u017belaszczyk", "Micha\u0142 Brzozowski", "Paolo Mandica", "Micha\u0142 Karpowicz"], "title": "Representation-based Broad Hallucination Detectors Fail to Generalize Out of Distribution", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted in EMNLP 2025 Findings", "summary": "We critically assess the efficacy of the current SOTA in hallucination\ndetection and find that its performance on the RAGTruth dataset is largely\ndriven by a spurious correlation with data. Controlling for this effect,\nstate-of-the-art performs no better than supervised linear probes, while\nrequiring extensive hyperparameter tuning across datasets. Out-of-distribution\ngeneralization is currently out of reach, with all of the analyzed methods\nperforming close to random. We propose a set of guidelines for hallucination\ndetection and its evaluation.", "AI": {"tldr": "This paper examines the inefficacy of current SOTA methods in detecting hallucinations, exposing reliance on spurious correlations and limited out-of-distribution generalization.", "motivation": "To critically analyze and improve the effectiveness and evaluation of hallucination detection methods.", "method": "Assessment of hallucination detection performance on RAGTruth dataset and comparison with supervised linear probes.", "result": "State-of-the-art methods were found to rely on spurious correlation, perform poorly out-of-distribution, and add complexity without tangible benefits.", "conclusion": "Current methods require improvement in evaluation, generalization, and can perform no better than simplified alternatives."}}
{"id": "2509.19788", "pdf": "https://arxiv.org/pdf/2509.19788", "abs": "https://arxiv.org/abs/2509.19788", "authors": ["Eunji Lim"], "title": "Convex Regression with a Penalty", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "A common way to estimate an unknown convex regression function $f_0: \\Omega\n\\subset \\mathbb{R}^d \\rightarrow \\mathbb{R}$ from a set of $n$ noisy\nobservations is to fit a convex function that minimizes the sum of squared\nerrors. However, this estimator is known for its tendency to overfit near the\nboundary of $\\Omega$, posing significant challenges in real-world applications.\nIn this paper, we introduce a new estimator of $f_0$ that avoids this\noverfitting by minimizing a penalty on the subgradient while enforcing an upper\nbound $s_n$ on the sum of squared errors. The key advantage of this method is\nthat $s_n$ can be directly estimated from the data. We establish the uniform\nalmost sure consistency of the proposed estimator and its subgradient over\n$\\Omega$ as $n \\rightarrow \\infty$ and derive convergence rates. The\neffectiveness of our estimator is illustrated through its application to\nestimating waiting times in a single-server queue.", "AI": {"tldr": "The paper proposes a new convex regression estimator that avoids boundary overfitting by penalizing subgradients and bounding squared errors.", "motivation": "The prevalent convex regression estimators often overfit at the boundary of domains, creating practical challenges in real-world applications such as queueing systems.", "method": "The proposed method minimizes a penalty on subgradients while enforcing an upper bound on the sum of squared errors, where the bound is estimated from data.", "result": "The paper proves uniform almost sure consistency as well as convergence rates of the estimator and its subgradients, demonstrating its effectiveness empirically in modeling waiting times in queues.", "conclusion": "The novel estimator successfully reduces overfitting in convex regression and is supported by theoretical guarantees and practical utility in applications."}}
{"id": "2509.20218", "pdf": "https://arxiv.org/pdf/2509.20218", "abs": "https://arxiv.org/abs/2509.20218", "authors": ["Mohamed Manzour", "Catherine M. Elias", "Omar M. Shehata", "Rub\u00e9n Izquierdo", "Miguel \u00c1ngel Sotelo"], "title": "Design Insights and Comparative Evaluation of a Hardware-Based Cooperative Perception Architecture for Lane Change Prediction", "categories": ["cs.AI", "cs.AR", "cs.CV", "cs.LG"], "comment": null, "summary": "Research on lane change prediction has gained attention in the last few\nyears. Most existing works in this area have been conducted in simulation\nenvironments or with pre-recorded datasets, these works often rely on\nsimplified assumptions about sensing, communication, and traffic behavior that\ndo not always hold in practice. Real-world deployments of lane-change\nprediction systems are relatively rare, and when they are reported, the\npractical challenges, limitations, and lessons learned are often\nunder-documented. This study explores cooperative lane-change prediction\nthrough a real hardware deployment in mixed traffic and shares the insights\nthat emerged during implementation and testing. We highlight the practical\nchallenges we faced, including bottlenecks, reliability issues, and operational\nconstraints that shaped the behavior of the system. By documenting these\nexperiences, the study provides guidance for others working on similar\npipelines.", "AI": {"tldr": "This study investigates cooperative lane-change prediction through real-world hardware deployment in mixed traffic, highlighting practical challenges and offering guidance.", "motivation": "The study aims to address the gaps in real-world lane change prediction implementations, which often lack detailed documentation of practical challenges.", "method": "The researchers deployed a cooperative lane-change prediction system in mixed traffic using real hardware and conducted testing while documenting operational issues.", "result": "The research uncovered bottlenecks, reliability issues, and operational constraints that influence system performance.", "conclusion": "The paper provides valuable insights for future researchers in developing and improving lane-change prediction systems in practical environments."}}
{"id": "2509.19590", "pdf": "https://arxiv.org/pdf/2509.19590", "abs": "https://arxiv.org/abs/2509.19590", "authors": ["Nathanael Jo", "Ashia Wilson"], "title": "What Does Your Benchmark Really Measure? A Framework for Robust Inference of AI Capabilities", "categories": ["cs.AI", "cs.CY", "cs.LG"], "comment": null, "summary": "Evaluations of generative models on benchmark data are now ubiquitous, and\ntheir outcomes critically shape public and scientific expectations of AI's\ncapabilities. Yet growing skepticism surrounds their reliability. How can we\nknow that a reported accuracy genuinely reflects a model's true performance?\nEvaluations are often presented as simple measurements, but in reality they are\ninferences: to treat benchmark scores as evidence of capability is already to\nassume a theory of what capability is and how it manifests in a test. We make\nthis step explicit by proposing a principled framework for evaluation as\ninference: begin from a theory of capability, and then derive methods for\nestimating it. This perspective, familiar in fields such as psychometrics, has\nnot yet become commonplace in AI evaluation. As a proof of concept, we address\na central challenge that undermines reliability: sensitivity to perturbations.\nAfter formulating a model of ability, we introduce methods that infer ability\nwhile accounting for uncertainty from sensitivity and finite samples, including\nan adaptive algorithm that significantly reduces sample complexity. Together,\nthese contributions lay the groundwork for more reliable and trustworthy\nestimates of AI capabilities as measured through benchmarks.", "AI": {"tldr": "The paper critiques current AI benchmark evaluations and introduces a new framework for more reliable assessments of generative models' capabilities, highlighting the importance of robust inference methods.", "motivation": "Growing skepticism about the reliability of benchmark evaluations in AI motivates the need for a principled approach to measure true model performance.", "method": "The authors propose an evaluation-as-inference framework inspired by psychometrics, addressing sensitivity to perturbations and incorporating uncertainty into the estimation of capabilities.", "result": "The paper presents a model of ability and adaptive methods that reduce sample complexity and enhance reliability in evaluating AI capabilities.", "conclusion": "These contributions aim to establish more trustworthy and rigorous approaches for assessing AI models using benchmark tests."}}
{"id": "2509.20160", "pdf": "https://arxiv.org/pdf/2509.20160", "abs": "https://arxiv.org/abs/2509.20160", "authors": ["Prashanthi S. K.", "Sai Anuroop Kesanapalli", "Yogesh Simmhan"], "title": "Characterizing the Performance of Accelerated Jetson Edge Devices for Training Deep Learning Models", "categories": ["cs.DC"], "comment": "Preprint of article in ACM SIGMETRICS 2023", "summary": "Deep Neural Networks (DNNs) have had a significant impact on domains like\nautonomous vehicles and smart cities through low-latency inferencing on edge\ncomputing devices close to the data source. However, DNN training on the edge\nis poorly explored. Techniques like federated learning and the growing capacity\nof GPU-accelerated edge devices like NVIDIA Jetson motivate the need for a\nholistic characterization of DNN training on the edge. Training DNNs is\nresource-intensive and can stress an edge's GPU, CPU, memory and storage\ncapacities. Edge devices also have different resources compared to workstations\nand servers, such as slower shared memory and diverse storage media. Here, we\nperform a principled study of DNN training on individual devices of three\ncontemporary Jetson device types: AGX Xavier, Xavier NX and Nano for three\ndiverse DNN model--dataset combinations. We vary device and training parameters\nsuch as I/O pipelining and parallelism, storage media, mini-batch sizes and\npower modes, and examine their effect on CPU and GPU utilization, fetch stalls,\ntraining time, energy usage, and variability. Our analysis exposes several\nresource inter-dependencies and counter-intuitive insights, while also helping\nquantify known wisdom. Our rigorous study can help tune the training\nperformance on the edge, trade-off time and energy usage on constrained\ndevices, and even select an ideal edge hardware for a DNN workload, and, in\nfuture, extend to federated learning too. As an illustration, we use these\nresults to build a simple model to predict the training time and energy per\nepoch for any given DNN across different power modes, with minimal additional\nprofiling.", "AI": {"tldr": "This paper studies DNN training on edge devices, focusing on resource utilization and providing insights for optimization.", "motivation": "Edge devices are increasingly used for DNN inferencing, but their ability to handle DNN training is poorly understood. This paper aims to address that gap.", "method": "The authors performed experiments on Jetson devices using diverse DNN models and datasets, varying parameters like mini-batch sizes and power modes.", "result": "The study reveals resource inter-dependencies, unexpected insights, and provides predictive models for training time and energy efficiency.", "conclusion": "This analysis aids in optimizing DNN training on edge devices and selecting suitable hardware, with potential applications in federated learning."}}
{"id": "2509.20010", "pdf": "https://arxiv.org/pdf/2509.20010", "abs": "https://arxiv.org/abs/2509.20010", "authors": ["Xiaoning Ren", "Yuhang Ye", "Xiongfei Wu", "Yueming Wu", "Yinxing Xue"], "title": "Demystifying the Evolution of Neural Networks with BOM Analysis: Insights from a Large-Scale Study of 55,997 GitHub Repositories", "categories": ["cs.SE"], "comment": "11pages,8figures", "summary": "Neural networks have become integral to many fields due to their exceptional\nperformance. The open-source community has witnessed a rapid influx of neural\nnetwork (NN) repositories with fast-paced iterations, making it crucial for\npractitioners to analyze their evolution to guide development and stay ahead of\ntrends. While extensive research has explored traditional software evolution\nusing Software Bill of Materials (SBOMs), these are ill-suited for NN software,\nwhich relies on pre-defined modules and pre-trained models (PTMs) with distinct\ncomponent structures and reuse patterns. Conceptual AI Bills of Materials\n(AIBOMs) also lack practical implementations for large-scale evolutionary\nanalysis. To fill this gap, we introduce the Neural Network Bill of Material\n(NNBOM), a comprehensive dataset construct tailored for NN software. We create\na large-scale NNBOM database from 55,997 curated PyTorch GitHub repositories,\ncataloging their TPLs, PTMs, and modules. Leveraging this database, we conduct\na comprehensive empirical study of neural network software evolution across\nsoftware scale, component reuse, and inter-domain dependency, providing\nmaintainers and developers with a holistic view of its long-term trends.\nBuilding on these findings, we develop two prototype applications,\n\\textit{Multi repository Evolution Analyzer} and \\textit{Single repository\nComponent Assessor and Recommender}, to demonstrate the practical value of our\nanalysis.", "AI": {"tldr": "The paper introduces Neural Network Bill of Material (NNBOM), a dataset aimed at analyzing the evolution of NN software based on curated repositories.", "motivation": "The study aims to address gaps in evolutionary analysis tools for neural network (NN) software, which traditional methods cannot handle effectively due to distinct reuse patterns and structures.", "method": "The researchers created a large-scale NNBOM database from thousands of PyTorch GitHub repositories and analyzed NN software evolution trends using this dataset.", "result": "A detailed empirical study was conducted, revealing insights into software scale, component reuse, and inter-domain dependencies in NN software.", "conclusion": "NNBOM provides a powerful framework for NN software evolution analysis, demonstrated through applications like evolution analyzers and component assessors."}}
{"id": "2509.19327", "pdf": "https://arxiv.org/pdf/2509.19327", "abs": "https://arxiv.org/abs/2509.19327", "authors": ["Braxton A. Morrison", "Madhumita Sushil", "Jacob S. Young"], "title": "A systematic review of trial-matching pipelines using large language models", "categories": ["cs.CL", "cs.AI"], "comment": "28 pages, 3 figures", "summary": "Matching patients to clinical trial options is critical for identifying novel\ntreatments, especially in oncology. However, manual matching is labor-intensive\nand error-prone, leading to recruitment delays. Pipelines incorporating large\nlanguage models (LLMs) offer a promising solution. We conducted a systematic\nreview of studies published between 2020 and 2025 from three academic databases\nand one preprint server, identifying LLM-based approaches to clinical trial\nmatching. Of 126 unique articles, 31 met inclusion criteria. Reviewed studies\nfocused on matching patient-to-criterion only (n=4), patient-to-trial only\n(n=10), trial-to-patient only (n=2), binary eligibility classification only\n(n=1) or combined tasks (n=14). Sixteen used synthetic data; fourteen used real\npatient data; one used both. Variability in datasets and evaluation metrics\nlimited cross-study comparability. In studies with direct comparisons, the\nGPT-4 model consistently outperformed other models, even finely-tuned ones, in\nmatching and eligibility extraction, albeit at higher cost. Promising\nstrategies included zero-shot prompting with proprietary LLMs like the GPT-4o\nmodel, advanced retrieval methods, and fine-tuning smaller, open-source models\nfor data privacy when incorporation of large models into hospital\ninfrastructure is infeasible. Key challenges include accessing sufficiently\nlarge real-world data sets, and deployment-associated challenges such as\nreducing cost, mitigating risk of hallucinations, data leakage, and bias. This\nreview synthesizes progress in applying LLMs to clinical trial matching,\nhighlighting promising directions and key limitations. Standardized metrics,\nmore realistic test sets, and attention to cost-efficiency and fairness will be\ncritical for broader deployment.", "AI": {"tldr": "Identifying clinical trials for patients using large language models (LLMs) is promising but faces issues like data variability, deployment costs, and biases. GPT-4 consistently performs well.", "motivation": "Current approaches for manual clinical trial matching are labor-intensive and error-prone, causing delays. Automating the process using LLMs could address these inefficiencies, especially in oncology.", "method": "The study conducted a systematic review of articles (2020-2025) across three academic databases and a preprint server, assessing LLM-based approaches to clinical trial matching. Reviewed models were evaluated on tasks like patient-to-trial matching, criterion extraction, and eligibility classification.", "result": "From 126 articles, 31 were eligible for analysis. GPT-4 outperformed other models in most tasks but with higher associated costs. Promising strategies include zero-shot prompting, retrieval-based methods, and fine-tuning smaller open-source models for data privacy.", "conclusion": "While LLMs like GPT-4 show immense potential, challenges such as lack of real-world data, economic concerns, risk of hallucinations, and biases must be addressed for scalable deployment. Standardized benchmarks and attention to cost and fairness are essential for progress."}}
{"id": "2509.19486", "pdf": "https://arxiv.org/pdf/2509.19486", "abs": "https://arxiv.org/abs/2509.19486", "authors": ["Kieran S. Lachmansingh", "Jos\u00e9 R. Gonz\u00e1lez-Estrada", "Ryan E. Grant", "Matthew K. X. J. Pan"], "title": "Supercomputing for High-speed Avoidance and Reactive Planning in Robots", "categories": ["cs.RO", "cs.DC"], "comment": "8 pages, 3 figures", "summary": "This paper presents SHARP (Supercomputing for High-speed Avoidance and\nReactive Planning), a proof-of-concept study demonstrating how high-performance\ncomputing (HPC) can enable millisecond-scale responsiveness in robotic control.\nWhile modern robots face increasing demands for reactivity in human--robot\nshared workspaces, onboard processors are constrained by size, power, and cost.\nOffloading to HPC offers massive parallelism for trajectory planning, but its\nfeasibility for real-time robotics remains uncertain due to network latency and\njitter. We evaluate SHARP in a stress-test scenario where a 7-DOF manipulator\nmust dodge high-speed foam projectiles. Using a parallelized multi-goal A*\nsearch implemented with MPI on both local and remote HPC clusters, the system\nachieves mean planning latencies of 22.9 ms (local) and 30.0 ms (remote, ~300\nkm away), with avoidance success rates of 84% and 88%, respectively. These\nresults show that when round-trip latency remains within the\ntens-of-milliseconds regime, HPC-side computation is no longer the bottleneck,\nenabling avoidance well below human reaction times. The SHARP results motivate\nhybrid control architectures: low-level reflexes remain onboard for safety,\nwhile bursty, high-throughput planning tasks are offloaded to HPC for\nscalability. By reporting per-stage timing and success rates, this study\nprovides a reproducible template for assessing real-time feasibility of\nHPC-driven robotics. Collectively, SHARP reframes HPC offloading as a viable\npathway toward dependable, reactive robots in dynamic environments.", "AI": {"tldr": "The paper introduces SHARP, demonstrating the use of high-performance computing (HPC) to achieve millisecond-scale responsiveness for robotic control.", "motivation": "Robotic systems require fast responsiveness in shared human-robot workspaces, but onboard processors are limited in size, power, and cost.", "method": "The paper evaluates SHARP via a parallelized multi-goal A* search implemented with MPI on local and remote HPC clusters, in a stress-test involving a 7-DOF robot avoiding foam projectiles.", "result": "The system achieved mean planning latencies of 22.9 ms (local) and 30.0 ms (remote, ~300 km away), with avoidance success rates of 84% and 88%, respectively.", "conclusion": "SHARP highlights the feasibility of HPC for reactive robotics, suggesting hybrid control architectures with onboard safety mechanisms and offloaded HPC planning for dynamic environments."}}
{"id": "2509.19624", "pdf": "https://arxiv.org/pdf/2509.19624", "abs": "https://arxiv.org/abs/2509.19624", "authors": ["Mahmoud Afifi", "Ran Zhang", "Michael S. Brown"], "title": "Raw-JPEG Adapter: Efficient Raw Image Compression with JPEG", "categories": ["cs.CV"], "comment": null, "summary": "Digital cameras digitize scene light into linear raw representations, which\nthe image signal processor (ISP) converts into display-ready outputs. While raw\ndata preserves full sensor information--valuable for editing and vision\ntasks--formats such as Digital Negative (DNG) require large storage, making\nthem impractical in constrained scenarios. In contrast, JPEG is a widely\nsupported format, offering high compression efficiency and broad compatibility,\nbut it is not well-suited for raw storage. This paper presents RawJPEG Adapter,\na lightweight, learnable, and invertible preprocessing pipeline that adapts raw\nimages for standard JPEG compression. Our method applies spatial and optional\nfrequency-domain transforms, with compact parameters stored in the JPEG comment\nfield, enabling accurate raw reconstruction. Experiments across multiple\ndatasets show that our method achieves higher fidelity than direct JPEG\nstorage, supports other codecs, and provides a favorable trade-off between\ncompression ratio and reconstruction accuracy.", "AI": {"tldr": "This paper introduces RawJPEG Adapter, a preprocessing pipeline that adapts raw camera data for compression using standard JPEG, maintaining high reconstruction fidelity with compact additional information.", "motivation": "Raw sensor data preserves detailed information beneficial for editing and vision applications but requires large storage, which is impractical. JPEG, while efficient, is not suitable for raw data storage.", "method": "The authors propose the RawJPEG Adapter\u2014an invertible preprocessing pipeline that applies spatial and optional frequency-domain transforms to raw images. Parameters are compactly stored in the JPEG comment field for accurate reconstruction.", "result": "Experiments indicate that the method outperforms standard JPEG storage in fidelity, supports other codecs, and provides a good balance between compression and reconstruction quality.", "conclusion": "RawJPEG Adapter efficiently adapts raw sensors' data for JPEG storage while maintaining high reconstruction accuracy, offering a practical solution for resource-constrained scenarios."}}
{"id": "2509.19375", "pdf": "https://arxiv.org/pdf/2509.19375", "abs": "https://arxiv.org/abs/2509.19375", "authors": ["Mridul Sharma", "Adeetya Patel", "Zaneta D' Souza", "Samira Abbasgholizadeh Rahimi", "Siva Reddy", "Sreenath Madathil"], "title": "Uncertainty Quantification of Large Language Models using Approximate Bayesian Computation", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "Despite their widespread applications, Large Language Models (LLMs) often\nstruggle to express uncertainty, posing a challenge for reliable deployment in\nhigh stakes and safety critical domains like clinical diagnostics. Existing\nstandard baseline methods such as model logits and elicited probabilities\nproduce overconfident and poorly calibrated estimates. In this work, we propose\nApproximate Bayesian Computation (ABC), a likelihood-free Bayesian inference,\nbased approach that treats LLMs as a stochastic simulator to infer posterior\ndistributions over predictive probabilities. We evaluate our ABC approach on\ntwo clinically relevant benchmarks: a synthetic oral lesion diagnosis dataset\nand the publicly available GretelAI symptom-to-diagnosis dataset. Compared to\nstandard baselines, our approach improves accuracy by up to 46.9\\%, reduces\nBrier scores by 74.4\\%, and enhances calibration as measured by Expected\nCalibration Error (ECE) and predictive entropy.", "AI": {"tldr": "The paper introduces an Approximate Bayesian Computation (ABC) framework to enhance uncertainty expression in Large Language Models (LLMs), particularly targeting clinical applications.", "motivation": "LLMs are overconfident and poorly calibrated, making them unsuitable for high-stakes domains like clinical diagnostics.", "method": "The authors developed a likelihood-free Bayesian inference framework (ABC) that models LLMs as stochastic simulators to infer posterior probabilities.", "result": "The ABC-based approach shows improvements in accuracy by up to 46.9%, reduces Brier scores by 74.4%, and enhances calibration metrics such as Expected Calibration Error (ECE) and predictive entropy.", "conclusion": "ABC can significantly improve the reliability and calibration of LLMs, making them more suitable for critical applications like medical diagnostics."}}
{"id": "2509.19820", "pdf": "https://arxiv.org/pdf/2509.19820", "abs": "https://arxiv.org/abs/2509.19820", "authors": ["Burak I. Tas", "Enrique del Castillo"], "title": "High-Dimensional Statistical Process Control via Manifold Fitting and Learning", "categories": ["stat.ML", "cs.LG", "stat.AP"], "comment": null, "summary": "We address the Statistical Process Control (SPC) of high-dimensional, dynamic\nindustrial processes from two complementary perspectives: manifold fitting and\nmanifold learning, both of which assume data lies on an underlying nonlinear,\nlower dimensional space. We propose two distinct monitoring frameworks for\nonline or 'phase II' Statistical Process Control (SPC). The first method\nleverages state-of-the-art techniques in manifold fitting to accurately\napproximate the manifold where the data resides within the ambient\nhigh-dimensional space. It then monitors deviations from this manifold using a\nnovel scalar distribution-free control chart. In contrast, the second method\nadopts a more traditional approach, akin to those used in linear dimensionality\nreduction SPC techniques, by first embedding the data into a lower-dimensional\nspace before monitoring the embedded observations. We prove how both methods\nprovide a controllable Type I error probability, after which they are\ncontrasted for their corresponding fault detection ability. Extensive numerical\nexperiments on a synthetic process and on a replicated Tennessee Eastman\nProcess show that the conceptually simpler manifold-fitting approach achieves\nperformance competitive with, and sometimes superior to, the more classical\nlower-dimensional manifold monitoring methods. In addition, we demonstrate the\npractical applicability of the proposed manifold-fitting approach by\nsuccessfully detecting surface anomalies in a real image dataset of electrical\ncommutators.", "AI": {"tldr": "This paper presents two Statistical Process Control (SPC) methods for monitoring high-dimensional, non-linear industrial processes: one based on manifold fitting and the other on manifold learning.", "motivation": "The increasing complexity and high-dimensionality of industrial processes necessitate advanced SPC methods that capitalize on underlying nonlinear, lower-dimensional data structures.", "method": "The paper proposes two SPC frameworks: one leveraging manifold fitting with a scalar distribution-free control chart to monitor deviations, and the other using traditional dimensionality reduction before monitoring the lower-dimensional representations.", "result": "The manifold-fitting approach demonstrated competitive or superior fault detection compared to traditional methods, supported by experiments on synthetic and real-world datasets, including the Tennessee Eastman Process and electrical commutators.", "conclusion": "Manifold-fitting proves to be both effective and practically applicable for high-dimensional SPC, detecting anomalies with high accuracy in real-world industrial scenarios."}}
{"id": "2509.20269", "pdf": "https://arxiv.org/pdf/2509.20269", "abs": "https://arxiv.org/abs/2509.20269", "authors": ["Matteo Cardoni", "Sam Leroux"], "title": "Predictive Coding-based Deep Neural Network Fine-tuning for Computationally Efficient Domain Adaptation", "categories": ["cs.LG", "cs.CV", "cs.NE"], "comment": "20 pages, 4 figures", "summary": "As deep neural networks are increasingly deployed in dynamic, real-world\nenvironments, relying on a single static model is often insufficient. Changes\nin input data distributions caused by sensor drift or lighting variations\nnecessitate continual model adaptation. In this paper, we propose a hybrid\ntraining methodology that enables efficient on-device domain adaptation by\ncombining the strengths of Backpropagation and Predictive Coding. The method\nbegins with a deep neural network trained offline using Backpropagation to\nachieve high initial performance. Subsequently, Predictive Coding is employed\nfor online adaptation, allowing the model to recover accuracy lost due to\nshifts in the input data distribution. This approach leverages the robustness\nof Backpropagation for initial representation learning and the computational\nefficiency of Predictive Coding for continual learning, making it particularly\nwell-suited for resource-constrained edge devices or future neuromorphic\naccelerators. Experimental results on the MNIST and CIFAR-10 datasets\ndemonstrate that this hybrid strategy enables effective adaptation with a\nreduced computational overhead, offering a promising solution for maintaining\nmodel performance in dynamic environments.", "AI": {"tldr": "A hybrid methodology combines Backpropagation and Predictive Coding to enable efficient on-device domain adaptation for deep neural networks, addressing challenges in dynamic environments like sensor drift or lighting variations.", "motivation": "Single static models struggle to adapt to changes in input data distribution in dynamic environments, necessitating new methodologies for continual model adaptation.", "method": "The paper proposes training a neural network offline using Backpropagation for high initial performance and employing Predictive Coding for efficient online adaptation to dynamic input changes.", "result": "Experimental results on MNIST and CIFAR-10 datasets showcase effective adaptation with reduced computational overhead using this hybrid approach.", "conclusion": "The hybrid strategy offers a computationally efficient solution for maintaining deep neural network performance in dynamic environments, especially on resource-constrained edge devices."}}
{"id": "2509.19623", "pdf": "https://arxiv.org/pdf/2509.19623", "abs": "https://arxiv.org/abs/2509.19623", "authors": ["Xutao Mao", "Tao Liu", "Hongying Zan"], "title": "SteinerSQL: Graph-Guided Mathematical Reasoning for Text-to-SQL Generation", "categories": ["cs.AI"], "comment": "Accept in Non-archival EMNLP 2025 MathNLP", "summary": "Large Language Models (LLMs) struggle with complex Text-to-SQL queries that\ndemand both sophisticated mathematical reasoning and intricate schema\nnavigation. Existing methods often tackle these challenges in isolation,\ncreating a fractured reasoning process that compromises logical and structural\ncorrectness. To resolve this, we introduce SteinerSQL, a framework that unifies\nthese dual challenges into a single, graph-centric optimization problem.\nSteinerSQL operates in three stages: mathematical decomposition to identify\nrequired tables (terminals), optimal reasoning scaffold construction via a\nSteiner tree problem, and multi-level validation to ensure correctness. On the\nchallenging LogicCat and Spider2.0-Lite benchmarks, SteinerSQL establishes a\nnew state-of-the-art with 36.10% and 40.04% execution accuracy, respectively,\nusing Gemini-2.5-Pro. Beyond accuracy, SteinerSQL presents a new, unified\nparadigm for Text-to-SQL, paving the way for more robust and principled\nsolutions to complex reasoning tasks.", "AI": {"tldr": "The paper introduces SteinerSQL, a framework for improving complex Text-to-SQL queries, establishing new accuracy benchmarks.", "motivation": "LLMs struggle with handling complex Text-to-SQL queries requiring mathematical reasoning and schema navigation. Fragmented approaches fail to ensure logical or structural correctness, necessitating a unified framework.", "method": "SteinerSQL applies a graph-centric optimization problem in three stages: 1) Decomposes mathematical requirements to find necessary tables, 2) Constructs an optimal reasoning scaffold via a Steiner tree, 3) Validates correctness through multi-level checks.", "result": "Achieves state-of-the-art execution accuracy of 36.10% on the LogicCat benchmark and 40.04% on Spider2.0-Lite benchmark using the Gemini-2.5-Pro model.", "conclusion": "SteinerSQL not only demonstrates improved performance on complex Text-to-SQL queries but also proposes a unified paradigm that has the potential for broader application in advanced reasoning tasks."}}
{"id": "2509.20189", "pdf": "https://arxiv.org/pdf/2509.20189", "abs": "https://arxiv.org/abs/2509.20189", "authors": ["Prashanthi S. K.", "Kunal Kumar Sahoo", "Amartya Ranjan Saikia", "Pranav Gupta", "Atharva Vinay Joshi", "Priyanshu Pansari", "Yogesh Simmhan"], "title": "Pagoda: An Energy and Time Roofline Study for DNN Workloads on Edge Accelerators", "categories": ["cs.DC"], "comment": null, "summary": "Edge accelerators such as Nvidia Jetsons are becoming an integral part of the\ncomputing continuum, and are often used for DNN inferencing and training.\nNvidia Jetson edge devices have $2000$+ CUDA cores within a $70$W power\nenvelope and offer $1000$s of power modes to customize CPU, GPU and memory\nfrequencies. Their widely varying power--performance trade-offs can be\nexploited for energy and power-constrained deployments. While data-driven\nmethods to predict the power and latency of DNN workloads for edge devices\nexist, there is a lack of principled study to understand why edge accelerators\nand their power modes perform the way they do. We develop a time roofline and a\nnovel energy roofline model for the Jetson Orin AGX for diverse power modes,\nand couple it with an analytical model of the compute (FLOP) and memory access\n(bytes) for DNN inference workloads to analyze them from first principles.\nThese reveal unique, sometimes counter-intuitive, insights into the power and\nperformance behavior of DNN workloads on edge accelerators, e.g., the default\npower mode MAXN is not the most energy efficient and time efficiency implies\nenergy efficiency for all power modes. We also extend our analytical roofline\nmodels to DNN training. Finally, we apply these methods to tune the power mode\n(and hence the roofline) of the edge device to optimize the latency and energy\nfor DNN inference, with up to $15\\%$ lower energy and minimal degradation in\ninference time.", "AI": {"tldr": "The paper introduces analytical models to examine DNN workload performance and energy efficiency on Nvidia Jetson devices and demonstrates optimization techniques.", "motivation": "To address the lack of analysis explaining why edge accelerators perform as they do across various power modes, enabling informed optimizations.", "method": "Developed analytical time and energy roofline models to study Nvidia Jetson's power modes and coupled them with FLOP and memory access analysis for DNN workloads.", "result": "Insights include counter-intuitive findings like MAXN mode not being most energy-efficient. Optimizations reduce energy consumption by up to 15% with minimal latency impact.", "conclusion": "Roofline models help understand and optimize DNN workloads on edge accelerators, improving energy and performance trade-offs."}}
{"id": "2509.20136", "pdf": "https://arxiv.org/pdf/2509.20136", "abs": "https://arxiv.org/abs/2509.20136", "authors": ["Wei Zhang", "Jack Yang", "Renshuai Tao", "Lingzheng Chai", "Shawn Guo", "Jiajun Wu", "Xiaoming Chen", "Ganqu Cui", "Ning Ding", "Xander Xu", "Hu Wei", "Bowen Zhou"], "title": "V-GameGym: Visual Game Generation for Code Large Language Models", "categories": ["cs.SE"], "comment": null, "summary": "Code large language models have demonstrated remarkable capabilities in\nprogramming tasks, yet current benchmarks primarily focus on single modality\nrather than visual game development. Most existing code-related benchmarks\nevaluate syntax correctness and execution accuracy, overlooking critical\ngame-specific metrics such as playability, visual aesthetics, and user\nengagement that are essential for real-world deployment. To address the gap\nbetween current LLM capabilities in algorithmic problem-solving and competitive\nprogramming versus the comprehensive requirements of practical game\ndevelopment, we present V-GameGym, a comprehensive benchmark comprising 2,219\nhigh-quality samples across 100 thematic clusters derived from real-world\nrepositories, adopting a novel clustering-based curation methodology to ensure\nboth diversity and structural completeness. Further, we introduce a multimodal\nevaluation framework with an automated LLM-driven pipeline for visual code\nsynthesis using complete UI sandbox environments. Our extensive analysis\nreveals that V-GameGym effectively bridges the gap between code generation\naccuracy and practical game development workflows, providing quantifiable\nquality metrics for visual programming and interactive element generation.", "AI": {"tldr": "The paper introduces V-GameGym, a benchmark for evaluating code large language models (LLMs) in visual game development, addressing playability, visual aesthetics, and user engagement.", "motivation": "Current benchmarks for code LLMs focus on syntax and execution, overlooking essential metrics for visual game development, such as playability and user engagement.", "method": "The authors developed V-GameGym, consisting of 2,219 samples from 100 thematic clusters, curated using a clustering-based methodology, and introduced a multimodal evaluation framework for visual code synthesis.", "result": "V-GameGym bridges the gap between accurate code generation and practical game development by quantifying quality metrics for visual programming and interactive elements.", "conclusion": "V-GameGym establishes a robust framework for assessing the practical capabilities of code LLMs in comprehensive visual game development tasks."}}
{"id": "2509.19329", "pdf": "https://arxiv.org/pdf/2509.19329", "abs": "https://arxiv.org/abs/2509.19329", "authors": ["Julie Jung", "Max Lu", "Sina Chole Benker", "Dogus Darici"], "title": "How Model Size, Temperature, and Prompt Style Affect LLM-Human Assessment Score Alignment", "categories": ["cs.CL", "stat.ME"], "comment": "9 pages, 4 figures, accepted at NCME AIME 2025", "summary": "We examined how model size, temperature, and prompt style affect Large\nLanguage Models' (LLMs) alignment within itself, between models, and with human\nin assessing clinical reasoning skills. Model size emerged as a key factor in\nLLM-human score alignment. Study highlights the importance of checking\nalignments across multiple levels.", "AI": {"tldr": "The study evaluates the alignment of LLMs based on model size, temperature, and prompt style in assessing clinical reasoning skills, focusing on alignment within models, between models, and with humans.", "motivation": "To determine the key factors influencing the alignment of LLM-generated scores with human evaluations of clinical reasoning.", "method": "Variations in model size, temperature, and prompt style were tested to analyze their effects on LLM alignments across different levels.", "result": "Model size was identified as the most significant factor affecting alignment with human scoring.", "conclusion": "Evaluating alignment across multiple levels is crucial when assessing LLM-human score alignment in clinical reasoning tasks."}}
{"id": "2509.19521", "pdf": "https://arxiv.org/pdf/2509.19521", "abs": "https://arxiv.org/abs/2509.19521", "authors": ["Najeeb Ahmed Bhuiyan", "M. Nasimul Huq", "Sakib H. Chowdhury", "Rahul Mangharam"], "title": "A Bimanual Gesture Interface for ROS-Based Mobile Manipulators Using TinyML and Sensor Fusion", "categories": ["cs.RO"], "comment": "12 pages, 11 figures", "summary": "Gesture-based control for mobile manipulators faces persistent challenges in\nreliability, efficiency, and intuitiveness. This paper presents a dual-hand\ngesture interface that integrates TinyML, spectral analysis, and sensor fusion\nwithin a ROS framework to address these limitations. The system uses left-hand\ntilt and finger flexion, captured using accelerometer and flex sensors, for\nmobile base navigation, while right-hand IMU signals are processed through\nspectral analysis and classified by a lightweight neural network. This pipeline\nenables TinyML-based gesture recognition to control a 7-DOF Kinova Gen3\nmanipulator. By supporting simultaneous navigation and manipulation, the\nframework improves efficiency and coordination compared to sequential methods.\nKey contributions include a bimanual control architecture, real-time low-power\ngesture recognition, robust multimodal sensor fusion, and a scalable ROS-based\nimplementation. The proposed approach advances Human-Robot Interaction (HRI)\nfor industrial automation, assistive robotics, and hazardous environments,\noffering a cost-effective, open-source solution with strong potential for\nreal-world deployment and further optimization.", "AI": {"tldr": "The paper introduces a dual-hand gesture interface using TinyML, spectral analysis, and sensor fusion to improve the efficiency and coordination of mobile manipulators.", "motivation": "Address gaps in reliability, efficiency, and intuitiveness in gesture-based control systems for mobile manipulators.", "method": "Utilized accelerometer, flex sensors, IMU signals processed through TinyML, spectral analysis, and sensor fusion within a ROS framework to enable simultaneous navigation and manipulation.", "result": "Achieved real-time, low-power gesture recognition for controlling a 7-DOF Kinova Gen3 manipulator, improving coordination and efficiency over sequential methods.", "conclusion": "The proposed scalable, cost-effective, and open-source framework advances Human-Robot Interaction for industrial and assistive applications, with significant deployment potential."}}
{"id": "2509.19644", "pdf": "https://arxiv.org/pdf/2509.19644", "abs": "https://arxiv.org/abs/2509.19644", "authors": ["William L. Muckelroy III", "Mohammed Alsakabi", "John M. Dolan", "Ozan K. Tonguz"], "title": "The Impact of 2D Segmentation Backbones on Point Cloud Predictions Using 4D Radar", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "LiDAR's dense, sharp point cloud (PC) representations of the surrounding\nenvironment enable accurate perception and significantly improve road safety by\noffering greater scene awareness and understanding. However, LiDAR's high cost\ncontinues to restrict the broad adoption of high-level Autonomous Driving (AD)\nsystems in commercially available vehicles. Prior research has shown progress\ntowards circumventing the need for LiDAR by training a neural network, using\nLiDAR point clouds as ground truth (GT), to produce LiDAR-like 3D point clouds\nusing only 4D Radars. One of the best examples is a neural network created to\ntrain a more efficient radar target detector with a modular 2D convolutional\nneural network (CNN) backbone and a temporal coherence network at its core that\nuses the RaDelft dataset for training (see arXiv:2406.04723). In this work, we\ninvestigate the impact of higher-capacity segmentation backbones on the quality\nof the produced point clouds. Our results show that while very high-capacity\nmodels may actually hurt performance, an optimal segmentation backbone can\nprovide a 23.7% improvement over the state-of-the-art (SOTA).", "AI": {"tldr": "The paper explores improving non-LiDAR 3D point cloud generation using 4D Radar by investigating segmentation backbones, achieving a 23.7% performance improvement over the state-of-the-art.", "motivation": "To reduce the reliance on expensive LiDAR sensors in autonomous driving systems by enhancing 4D Radar-based point cloud generation.", "method": "The study evaluates the impact of higher-capacity segmentation backbones on generating LiDAR-like 3D point clouds, using a neural network trained on the RaDelft dataset.", "result": "Optimal segmentation backbones demonstrated a 23.7% improvement in point cloud quality compared to the state-of-the-art.", "conclusion": "Carefully selecting segmentation backbones can considerably enhance radar-based point cloud generation, but overly high-capacity models may hinder performance."}}
{"id": "2509.19376", "pdf": "https://arxiv.org/pdf/2509.19376", "abs": "https://arxiv.org/abs/2509.19376", "authors": ["Matthew Grofsky"], "title": "Solving Freshness in RAG: A Simple Recency Prior and the Limits of Heuristic Trend Detection", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We address temporal failures in RAG systems using two methods on\ncybersecurity data. A simple recency prior achieved an accuracy of 1.00 on\nfreshness tasks. In contrast, a clustering heuristic for topic evolution failed\n(0.08 F1-score), showing trend detection requires methods beyond simple\nheuristics.", "AI": {"tldr": "Two methods applied to tackle temporal failures in RAG systems on cybersecurity data, with mixed success.", "motivation": "To address the issue of temporal failures in RAG systems, particularly focusing on ensuring recency and detecting topic evolution.", "method": "Two methods were tested: a simple recency prior to ensure information freshness, and a clustering heuristic to detect topic evolution in cybersecurity data.", "result": "The recency prior demonstrated excellent performance with an accuracy of 1.00. However, the clustering heuristic for topic evolution detection failed, achieving only a 0.08 F1-score.", "conclusion": "While the recency prior is effective for tackling temporal issues, detecting topic evolution requires more advanced methods than simple heuristics."}}
{"id": "2509.19929", "pdf": "https://arxiv.org/pdf/2509.19929", "abs": "https://arxiv.org/abs/2509.19929", "authors": ["Arnaud Vadeboncoeur", "Gregory Duth\u00e9", "Mark Girolami", "Eleni Chatzi"], "title": "Geometric Autoencoder Priors for Bayesian Inversion: Learn First Observe Later", "categories": ["stat.ML", "cs.LG", "physics.comp-ph", "physics.data-an"], "comment": null, "summary": "Uncertainty Quantification (UQ) is paramount for inference in engineering\napplications. A common inference task is to recover full-field information of\nphysical systems from a small number of noisy observations, a usually highly\nill-posed problem. Critically, engineering systems often have complicated and\nvariable geometries prohibiting the use of standard Bayesian UQ. In this work,\nwe introduce Geometric Autoencoders for Bayesian Inversion (GABI), a framework\nfor learning geometry-aware generative models of physical responses that serve\nas highly informative geometry-conditioned priors for Bayesian inversion.\nFollowing a ''learn first, observe later'' paradigm, GABI distills information\nfrom large datasets of systems with varying geometries, without requiring\nknowledge of governing PDEs, boundary conditions, or observation processes,\ninto a rich latent prior. At inference time, this prior is seamlessly combined\nwith the likelihood of the specific observation process, yielding a\ngeometry-adapted posterior distribution. Our proposed framework is architecture\nagnostic. A creative use of Approximate Bayesian Computation (ABC) sampling\nyields an efficient implementation that utilizes modern GPU hardware. We test\nour method on: steady-state heat over rectangular domains; Reynold-Averaged\nNavier-Stokes (RANS) flow around airfoils; Helmholtz resonance and source\nlocalization on 3D car bodies; RANS airflow over terrain. We find: the\npredictive accuracy to be comparable to deterministic supervised learning\napproaches in the restricted setting where supervised learning is applicable;\nUQ to be well calibrated and robust on challenging problems with complex\ngeometries. The method provides a flexible geometry-aware\ntrain-once-use-anywhere foundation model which is independent of any particular\nobservation process.", "AI": {"tldr": "The paper introduces GABI, a new framework for geometry-aware Bayesian inversion using generative models to provide well-calibrated uncertainty quantification (UQ) in systems with complex geometries.", "motivation": "The paper addresses the challenge of recovering physical system behaviors from limited and noisy observations, especially in systems with complex geometries that prohibit standard Bayesian UQ approaches.", "method": "The authors propose GABI, which combines geometry-aware generative models as latent priors with Bayesian inversion, enabling flexible integration of observations with geometry-conditioned priors. The framework is computationally efficient, leveraging Approximate Bayesian Computation (ABC) sampling, and is architecture-agnostic.", "result": "The GABI framework achieved predictive accuracy similar to supervised learning in applicable settings, and robust, well-calibrated UQ performance on complex geometry-based problems in various engineering examples.", "conclusion": "GABI offers a versatile, geometry-aware foundation model for UQ that requires no governing equations or prior knowledge, making it adaptable and effective for a broad range of engineering applications."}}
{"id": "2509.19681", "pdf": "https://arxiv.org/pdf/2509.19681", "abs": "https://arxiv.org/abs/2509.19681", "authors": ["Anisha Garg", "Engin Tekin", "Yash More", "David Bick", "Nishit Neema", "Ganesh Venkatesh"], "title": "Calibrated Reasoning: An Explanatory Verifier for Dynamic and Efficient Problem-Solving", "categories": ["cs.AI"], "comment": "39th Conference on Neural Information Processing Systems (NeurIPS\n  2025) Workshop: Efficient Reasoning", "summary": "Advanced test-time computing strategies are essential for scaling reasoning\nmodels, but their effectiveness is capped by the models' poor self-evaluation.\nWe propose a pairwise Explanatory Verifier, trained via reinforcement learning\n(GRPO), that produces calibrated confidence scores and associated natural\nlanguage reasoning for generated solutions. Our verifier improves the accuracy\nand efficiency of test-time strategies like best-of-n and self-reflection.\nCrucially, it excels at identifying challenging failure modes, such as when\nboth candidate solutions are identically incorrect, succeeding where standard\nmethods like majority voting fail.", "AI": {"tldr": "The paper introduces a pairwise Explanatory Verifier that improves reasoning model performance using calibrated confidence scores and natural language explanations, outperforming current methods in challenging scenarios.", "motivation": "Reasoning models struggle with poor self-evaluation, limiting the effectiveness of advanced test-time strategies.", "method": "They develop a pairwise Explanatory Verifier trained via reinforcement learning (GRPO) to generate confidence scores and reasoning for solutions.", "result": "The verifier enhances accuracy and efficiency of test-time strategies while identifying challenging failure cases that standard approaches miss.", "conclusion": "The proposed verifier is effective in improving reasoning models, particularly in detecting failure modes where traditional methods fall short."}}
{"id": "2509.20205", "pdf": "https://arxiv.org/pdf/2509.20205", "abs": "https://arxiv.org/abs/2509.20205", "authors": ["Prashanthi S. K.", "Saisamarth Taluri", "Pranav Gupta", "Amartya Ranjan Saikia", "Kunal Kumar Sahoo", "Atharva Vinay Joshi", "Lakshya Karwa", "Kedar Dhule", "Yogesh Simmhan"], "title": "Fulcrum: Optimizing Concurrent DNN Training and Inferencing on Edge Accelerators", "categories": ["cs.DC"], "comment": null, "summary": "The proliferation of GPU accelerated edge devices like Nvidia Jetsons and the\nrise in privacy concerns are placing an emphasis on concurrent DNN training and\ninferencing on edge devices. Inference and training have different computing\nand QoS goals. But edge accelerators like Jetson do not support native GPU\nsharing and expose 1000s of power modes. This requires careful time-sharing of\nconcurrent workloads to meet power--performance goals, while limiting costly\nprofiling. In this paper, we design an intelligent time-slicing approach for\nconcurrent DNN training and inferencing on Jetsons. We formulate an\noptimization problem to interleave training and inferencing minibatches, and\ndecide the device power mode and inference minibatch size, while maximizing the\ntraining throughput and staying within latency and power budgets, with modest\nprofiling costs. We propose GMD, an efficient multi-dimensional gradient\ndescent search which profiles just $15$ power modes; and ALS, an Active\nLearning technique which identifies reusable Pareto-optimal power modes, but\nprofiles $50$--$150$ power modes. We evaluate these within our Fulcrum\nscheduler for $273,000+$ configurations across $15$ DNN workloads. We also\nevaluate our strategies on dynamic arrival inference and concurrent inferences.\nALS and GMD outperform simpler and more complex baselines with larger-scale\nprofiling. Their solutions satisfy the latency and power budget for $>97\\%$ of\nour runs, and on average are within $7\\%$ of the optimal throughput.", "AI": {"tldr": "The paper addresses efficient time-slicing for GPU sharing between training and inference on Nvidia Jetson edge devices, proposing optimization methods and scheduling strategies.", "motivation": "The rise of GPU-accelerated edge devices for concurrent DNN inference and training, along with privacy concerns, necessitates efficient resource sharing and power-performance optimization.", "method": "This paper formulates an optimization problem to interleave workloads and applies multi-dimensional gradient descent (GMD) and active learning (ALS) to identify Pareto-optimal power modes with limited profiling.", "result": "The proposed strategies, ALS and GMD, outperform baselines and meet power and latency budgets for over 97% of runs, reaching throughput within 7% of optimal.", "conclusion": "An intelligent scheduler combining ALS and GMD achieves efficient utilization of Jetson devices for concurrent DNN tasks, offering a balance of performance and profiling efficiency."}}
{"id": "2509.20149", "pdf": "https://arxiv.org/pdf/2509.20149", "abs": "https://arxiv.org/abs/2509.20149", "authors": ["Jianzhang Zhang", "Jialong Zhou", "Nan Niu", "Chuang Liu"], "title": "Enhancing Requirement Traceability through Data Augmentation Using Large Language Models", "categories": ["cs.SE"], "comment": null, "summary": "Requirements traceability is crucial in software engineering to ensure\nconsistency between requirements and code. However, existing automated\ntraceability methods are constrained by the scarcity of training data and\nchallenges in bridging the semantic gap between artifacts. This study aims to\naddress the data scarcity problem in requirements traceability by employing\nlarge language models (LLMs) for data augmentation. We propose a novel approach\nthat utilizes prompt-based techniques with LLMs to generate augmented\nrequirement-to-code trace links, thereby enhancing the training dataset. Four\nLLMs (Gemini 1.5 Pro, Claude 3, GPT-3.5, and GPT-4) were used, employing both\nzero-shot and few-shot templates. Moreover, we optimized the encoder component\nof the tracing model to improve its efficiency and adaptability to augmented\ndata. The key contributions of this paper are: (1) proposing and evaluating\nfour prompt templates for data augmentation; (2) providing a comparative\nanalysis of four LLMs for generating trace links; (3) enhancing the model's\nencoder for improved adaptability to augmented datasets. Experimental results\nshow that our approach significantly enhances model performance, achieving an\nF1 score improvement of up to 28.59%, thus demonstrating its effectiveness and\npotential for practical application.", "AI": {"tldr": "The study tackles the issue of data scarcity in requirements traceability by using large language models (LLMs) for data augmentation, achieving notable improvements in model performance.", "motivation": "Existing automated traceability methods face challenges due to limited training data and difficulty in bridging the semantic gap between requirements and code.", "method": "The researchers utilized prompt-based techniques with four LLMs (Gemini 1.5 Pro, Claude 3, GPT-3.5, and GPT-4) for zero-shot and few-shot trace link generation. They also optimized the encoder component of the tracing model for better adaptability.", "result": "Experimental results showed up to 28.59% improvement in the F1 score, highlighting the effectiveness of the augmented datasets and optimized encoder.", "conclusion": "The study demonstrates that LLMs combined with prompt-based data augmentation and encoder optimization can significantly improve requirements traceability, offering a practical solution to the data scarcity problem."}}
{"id": "2509.19332", "pdf": "https://arxiv.org/pdf/2509.19332", "abs": "https://arxiv.org/abs/2509.19332", "authors": ["Zhijin Guo", "Chenhao Xue", "Zhaozhen Xu", "Hongbo Bo", "Yuxuan Ye", "Janet B. Pierrehumbert", "Martha Lewis"], "title": "Quantifying Compositionality of Classic and State-of-the-Art Embeddings", "categories": ["cs.CL", "cs.AI"], "comment": "Findings of the Association for Computational Linguistics: EMNLP 2025", "summary": "For language models to generalize correctly to novel expressions, it is\ncritical that they exploit access compositional meanings when this is\njustified. Even if we don't know what a \"pelp\" is, we can use our knowledge of\nnumbers to understand that \"ten pelps\" makes more pelps than \"two pelps\".\nStatic word embeddings such as Word2vec made strong, indeed excessive, claims\nabout compositionality. The SOTA generative, transformer models and graph\nmodels, however, go too far in the other direction by providing no real limits\non shifts in meaning due to context. To quantify the additive compositionality,\nwe formalize a two-step, generalized evaluation that (i) measures the linearity\nbetween known entity attributes and their embeddings via canonical correlation\nanalysis, and (ii) evaluates additive generalization by reconstructing\nembeddings for unseen attribute combinations and checking reconstruction\nmetrics such as L2 loss, cosine similarity, and retrieval accuracy. These\nmetrics also capture failure cases where linear composition breaks down.\nSentences, knowledge graphs, and word embeddings are evaluated and tracked the\ncompositionality across all layers and training stages. Stronger compositional\nsignals are observed in later training stages across data modalities, and in\ndeeper layers of the transformer-based model before a decline at the top layer.\nCode is available at\nhttps://github.com/Zhijin-Guo1/quantifying-compositionality.", "AI": {"tldr": "This paper evaluates additive compositionality in language models using a two-step framework and identifies stronger compositional signals in later training stages and deeper transformer layers.", "motivation": "The paper aims to address the need for language models to correctly generalize to novel expressions by exploiting compositional meanings under justified conditions.", "method": "A two-step evaluation is introduced, comprising (i) canonical correlation analysis to measure linear relationships between entity attributes and embeddings, and (ii) reconstruction-based metrics to assess additive generalization.", "result": "The study finds stronger compositional signals in late training stages and deeper layers of transformer models, with a subsequent decline observed in the top layer.", "conclusion": "The paper highlights the importance of compositional signals for generalization in language models, with variations observed across layers and training stages."}}
{"id": "2509.19522", "pdf": "https://arxiv.org/pdf/2509.19522", "abs": "https://arxiv.org/abs/2509.19522", "authors": ["Fabio Coelho", "Joao Victor T. Borges", "Paulo Padrao", "Jose Fuentes", "Ramon R. Costa", "Liu Hsu", "Leonardo Bobadilla"], "title": "Bioinspired SLAM Approach for Unmanned Surface Vehicle", "categories": ["cs.RO"], "comment": null, "summary": "This paper presents OpenRatSLAM2, a new version of OpenRatSLAM - a\nbioinspired SLAM framework based on computational models of the rodent\nhippocampus. OpenRatSLAM2 delivers low-computation-cost visual-inertial based\nSLAM, suitable for GPS-denied environments. Our contributions include a\nROS2-based architecture, experimental results on new waterway datasets, and\ninsights into system parameter tuning. This work represents the first known\napplication of RatSLAM on USVs. The estimated trajectory was compared with\nground truth data using the Hausdorff distance. The results show that the\nalgorithm can generate a semimetric map with an error margin acceptable for\nmost robotic applications.", "AI": {"tldr": "OpenRatSLAM2 enhances the RatSLAM framework for low-cost visual-inertial SLAM in GPS-denied environments, particularly applied to USVs.", "motivation": "The paper aims to explore bioinspired SLAM solutions using computational models of the rodent hippocampus to address challenges in creating cost-effective and reliable SLAM systems for GPS-denied scenarios.", "method": "The research employs a ROS2-based architecture applied to waterway datasets, leveraging the Hausdorff distance to benchmark trajectory estimation against ground truth data.", "result": "The algorithm successfully generates a semimetric map with error margins acceptable for most robotic applications, demonstrating practical deployment on USVs.", "conclusion": "OpenRatSLAM2 proves to be a viable tool for GPS-denied SLAM applications, particularly in USV environments, while providing insights into parameter tuning and bioinspired methodologies."}}
{"id": "2509.19659", "pdf": "https://arxiv.org/pdf/2509.19659", "abs": "https://arxiv.org/abs/2509.19659", "authors": ["Aravind Narayanan", "Vahid Reza Khazaie", "Shaina Raza"], "title": "Bias in the Picture: Benchmarking VLMs with Social-Cue News Images and LLM-as-Judge Assessment", "categories": ["cs.CV"], "comment": "Accepted to NeurIPS 2025 Workshop (Evaluating the Evolving LLM\n  Lifecycle)", "summary": "Large vision-language models (VLMs) can jointly interpret images and text,\nbut they are also prone to absorbing and reproducing harmful social stereotypes\nwhen visual cues such as age, gender, race, clothing, or occupation are\npresent. To investigate these risks, we introduce a news-image benchmark\nconsisting of 1,343 image-question pairs drawn from diverse outlets, which we\nannotated with ground-truth answers and demographic attributes (age, gender,\nrace, occupation, and sports). We evaluate a range of state-of-the-art VLMs and\nemploy a large language model (LLM) as judge, with human verification. Our\nfindings show that: (i) visual context systematically shifts model outputs in\nopen-ended settings; (ii) bias prevalence varies across attributes and models,\nwith particularly high risk for gender and occupation; and (iii) higher\nfaithfulness does not necessarily correspond to lower bias. We release the\nbenchmark prompts, evaluation rubric, and code to support reproducible and\nfairness-aware multimodal assessment.", "AI": {"tldr": "The paper analyzes biases in large vision-language models (VLMs) and introduces a benchmark for assessing such risks.", "motivation": "Address the issue of social stereotypes being propagated by vision-language models when visual cues like age, gender, and race affect outputs.", "method": "Develop a news-image benchmark with annotated question-image pairs for evaluation and employ state-of-the-art VLMs along with a human-verified large language model as a judge.", "result": "Bias prevalence varies across attributes and models, visual context influences outputs, and higher model faithfulness doesn't imply reduced bias.", "conclusion": "Promote fairness-aware multimodal assessments by releasing benchmark resources and findings highlighting systematic biases in VLMs."}}
{"id": "2509.19379", "pdf": "https://arxiv.org/pdf/2509.19379", "abs": "https://arxiv.org/abs/2509.19379", "authors": ["Returaj Burnwal", "Hriday Mehta", "Nirav Pravinbhai Bhatt", "Balaraman Ravindran"], "title": "Learning from Observation: A Survey of Recent Advances", "categories": ["cs.LG", "cs.AI", "cs.RO", "stat.ML"], "comment": null, "summary": "Imitation Learning (IL) algorithms offer an efficient way to train an agent\nby mimicking an expert's behavior without requiring a reward function. IL\nalgorithms often necessitate access to state and action information from expert\ndemonstrations. Although expert actions can provide detailed guidance,\nrequiring such action information may prove impractical for real-world\napplications where expert actions are difficult to obtain. To address this\nlimitation, the concept of learning from observation (LfO) or state-only\nimitation learning (SOIL) has recently gained attention, wherein the imitator\nonly has access to expert state visitation information. In this paper, we\npresent a framework for LfO and use it to survey and classify existing LfO\nmethods in terms of their trajectory construction, assumptions and algorithm's\ndesign choices. This survey also draws connections between several related\nfields like offline RL, model-based RL and hierarchical RL. Finally, we use our\nframework to identify open problems and suggest future research directions.", "AI": {"tldr": "The paper surveys methods for imitation learning specifically focusing on learning from observation (LfO), where only expert state information is available instead of actions.", "motivation": "Expert actions are often impractical to obtain in real-world applications, motivating the need for imitation learning methods that rely solely on expert state visitation data.", "method": "The authors propose a framework that categorizes existing LfO approaches based on trajectory construction, assumptions, and design choices, while also exploring connections with related fields like offline RL, model-based RL, and hierarchical RL.", "result": "The paper organizes the field of LfO and provides insights into existing methodologies, highlighting their categorization and relationships with other RL research areas.", "conclusion": "A clear structure for understanding LfO methods is established, along with identification of open problems and future research opportunities in imitation learning and related fields."}}
{"id": "2509.19988", "pdf": "https://arxiv.org/pdf/2509.19988", "abs": "https://arxiv.org/abs/2509.19988", "authors": ["Yanke Li", "Tianyu Cui", "Tommaso Mansi", "Mangal Prakash", "Rui Liao"], "title": "BioBO: Biology-informed Bayesian Optimization for Perturbation Design", "categories": ["stat.ML", "cs.LG", "q-bio.QM"], "comment": "NeurIPS: Structured Probabilistic Inference & Generative Modeling,\n  2025", "summary": "Efficient design of genomic perturbation experiments is crucial for\naccelerating drug discovery and therapeutic target identification, yet\nexhaustive perturbation of the human genome remains infeasible due to the vast\nsearch space of potential genetic interactions and experimental constraints.\nBayesian optimization (BO) has emerged as a powerful framework for selecting\ninformative interventions, but existing approaches often fail to exploit\ndomain-specific biological prior knowledge. We propose Biology-Informed\nBayesian Optimization (BioBO), a method that integrates Bayesian optimization\nwith multimodal gene embeddings and enrichment analysis, a widely used tool for\ngene prioritization in biology, to enhance surrogate modeling and acquisition\nstrategies. BioBO combines biologically grounded priors with acquisition\nfunctions in a principled framework, which biases the search toward promising\ngenes while maintaining the ability to explore uncertain regions. Through\nexperiments on established public benchmarks and datasets, we demonstrate that\nBioBO improves labeling efficiency by 25-40%, and consistently outperforms\nconventional BO by identifying top-performing perturbations more effectively.\nMoreover, by incorporating enrichment analysis, BioBO yields pathway-level\nexplanations for selected perturbations, offering mechanistic interpretability\nthat links designs to biologically coherent regulatory circuits.", "AI": {"tldr": "Biology-Informed Bayesian Optimization (BioBO) enhances Bayesian optimization by integrating multimodal gene embeddings and enrichment analysis for better surrogate modeling and acquisition strategies in genomic experiments.", "motivation": "Current Bayesian optimization approaches often overlook the domain-specific biological knowledge needed for efficiently identifying genetic interventions in genomics, making advanced strategies necessary.", "method": "BioBO merges Bayesian optimization with multimodal gene embeddings and enrichment analysis, creating biologically grounded priors and acquisition functions to balance exploration and exploitation.", "result": "BioBO boosts labeling efficiency by 25-40% and surpasses conventional BO in identifying optimal genetic perturbations. Additionally, it provides pathway-level explanations for perturbation selections.", "conclusion": "BioBO is an effective and interpretable tool for gene prioritization in genomic perturbation experiments, linking genetic interventions to regulatory mechanisms and improving experimental efficiency."}}
{"id": "2509.19736", "pdf": "https://arxiv.org/pdf/2509.19736", "abs": "https://arxiv.org/abs/2509.19736", "authors": ["Cheng Qian", "Zuxin Liu", "Akshara Prabhakar", "Jielin Qiu", "Zhiwei Liu", "Haolin Chen", "Shirley Kokane", "Heng Ji", "Weiran Yao", "Shelby Heinecke", "Silvio Savarese", "Caiming Xiong", "Huan Wang"], "title": "UserRL: Training Interactive User-Centric Agent via Reinforcement Learning", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "28 Pages, 15 Figures, 6 Tables; Built upon latest UserBench release:\n  arXiv:2507.22034", "summary": "Reinforcement learning (RL) has shown promise in training agentic models that\nmove beyond static benchmarks to engage in dynamic, multi-turn interactions.\nYet, the ultimate value of such agents lies in their ability to assist users, a\nsetting where diversity and dynamics of user interaction pose challenges. In\nthis work, we propose UserRL, a unified framework for training and evaluating\nuser-centric abilities through standardized gym environments paired with\nsimulated users. We systematically vary turn-level reward assignment and\ntrajectory-level score calculation to analyze how different formulations affect\nlearning under the GRPO algorithm. Our experiments across Qwen3 models reveal\nthree key findings: (i) SFT cold start is critical for unlocking initial\ninteraction ability and enabling sustained RL improvements; (ii) deliberate\ntrajectory scoring yields more efficient and effective multi-turn interactions;\nand (iii) while stronger simulated users (e.g., GPT-4o) facilitates training,\nopen-source simulators (e.g., Qwen3-32B) remain a cost-effective and\ntransferable option. Together, these results highlight that careful design of\nreward shaping and user simulation choice is as crucial as model scale, and\nestablish UserRL as a practical pathway for developing robust user-centric\nagentic models. All codes and data are public for future research.", "AI": {"tldr": "Reinforcement learning is used to train agentic models for dynamic user interactions. UserRL framework evaluates user-centric abilities using standardized environments and simulated users.", "motivation": "To enhance the practicality of reinforcement learning by developing agents capable of dynamic, multi-turn interactions with diverse users.", "method": "A unified framework, UserRL, with gym environments and simulated users is used. Different reward assignments and scoring mechanisms are studied under the GRPO algorithm.", "result": "Experiments reveal SFT cold start importance, trajectory scoring benefits, and cost-effective simulated user options like Qwen3-32B over GPT-4o.", "conclusion": "Reward shaping and simulated user choices are pivotal, alongside model scale, for training robust user-centric agentic models. UserRL is a valuable pathway for such developments."}}
{"id": "2509.20223", "pdf": "https://arxiv.org/pdf/2509.20223", "abs": "https://arxiv.org/abs/2509.20223", "authors": ["Md Jueal Mia", "M. Hadi Amini"], "title": "An Empirical Analysis of Secure Federated Learning for Autonomous Vehicle Applications", "categories": ["cs.DC"], "comment": "i3CE 2024, 2024 ASCE International Conference on Computing in Civil\n  Engineering", "summary": "Federated Learning lends itself as a promising paradigm in enabling\ndistributed learning for autonomous vehicles applications and ensuring data\nprivacy while enhancing and refining predictive model performance through\ncollaborative training on edge client vehicles. However, it remains vulnerable\nto various categories of cyber-attacks, necessitating more robust security\nmeasures to effectively mitigate potential threats. Poisoning attacks and\ninference attacks are commonly initiated within the federated learning\nenvironment to compromise secure system performance. Secure aggregation can\nlimit the disclosure of sensitive information from outsider and insider\nattackers of the federated learning environment. In this study, our aim is to\nconduct an empirical analysis on the transportation image dataset (e.g., LISA\ntraffic light) using various secure aggregation techniques and multiparty\ncomputation in the presence of diverse categories of cyber-attacks. Multiparty\ncomputation serves as a state-of-the-art security mechanism, offering standard\nprivacy for secure aggregation of edge autonomous vehicles local model updates\nthrough various security protocols. The presence of adversaries can mislead the\nautonomous vehicle learning model, leading to the misclassification of traffic\nlights, and resulting in detrimental impacts. This empirical study explores the\nresilience of various secure federated learning aggregation techniques and\nmultiparty computation in safeguarding autonomous vehicle applications against\nvarious cyber threats during both training and inference times.", "AI": {"tldr": "The study explores the use of secure aggregation techniques and multiparty computation to safeguard federated learning for autonomous vehicles against cyber-attacks, ensuring data privacy and model resilience.", "motivation": "The paper aims to address vulnerabilities in federated learning for autonomous vehicle applications, which are susceptible to cyber-attacks like poisoning and inference attacks.", "method": "An empirical analysis using transportation image datasets (e.g., LISA traffic light) to test various secure aggregation techniques and multiparty computation under cyber-attack scenarios.", "result": "The study investigates the resilience and effectiveness of federated learning security protocols in mitigating misclassification attacks and other vulnerabilities in autonomous vehicle applications.", "conclusion": "Secure aggregation and multiparty computation are promising approaches to ensure the robustness and privacy of federated learning models for autonomous vehicles against diverse cyber threats."}}
{"id": "2509.20172", "pdf": "https://arxiv.org/pdf/2509.20172", "abs": "https://arxiv.org/abs/2509.20172", "authors": ["Daniel Maninger", "Leon Chemnitz", "Amir Molzam Sharifloo", "Jannis Brugger", "Mira Mezini"], "title": "Benchmarking Web API Integration Code Generation", "categories": ["cs.SE", "cs.LG"], "comment": "To be published in Proceedings of 2nd ACM International Conference on\n  AI-powered Software, Benchmark & Dataset Track (AIware '25)", "summary": "API integration is a cornerstone of our digital infrastructure, enabling\nsoftware systems to connect and interact. However, as shown by many studies,\nwriting or generating correct code to invoke APIs, particularly web APIs, is\nchallenging. Although large language models~(LLMs) have become popular in\nsoftware development, their effectiveness in automating the generation of web\nAPI integration code remains unexplored. In order to address this, we present a\ndataset and evaluation pipeline designed to assess the ability of LLMs to\ngenerate web API invocation code. Our experiments with several open-source LLMs\nreveal that generating API invocations poses a significant challenge, resulting\nin hallucinated endpoints, incorrect argument usage, and other errors. None of\nthe evaluated open-source models were able to solve more than 40% of the tasks.", "AI": {"tldr": "The paper investigates the challenges of generating correct web API invocation code using large language models (LLMs) and reveals that current open-source models perform poorly, solving less than 40% of tasks.", "motivation": "To address the unexplored area of how effective large language models (LLMs) are in generating web API integration code, a critical aspect of software development.", "method": "Presented a dataset and evaluation pipeline to assess the ability of open-source LLMs in generating correct web API invocation code, followed by experiments on several models.", "result": "The experiments showed that open-source LLMs face significant challenges, including hallucinated endpoints and incorrect argument usage, with none solving over 40% of the tasks.", "conclusion": "Current open-source LLMs are not effective at reliably generating correct web API invocation code, highlighting the need for improved methods to address these limitations."}}
{"id": "2509.19333", "pdf": "https://arxiv.org/pdf/2509.19333", "abs": "https://arxiv.org/abs/2509.19333", "authors": ["Chengkai Huang", "Junda Wu", "Zhouhang Xie", "Yu Xia", "Rui Wang", "Tong Yu", "Subrata Mitra", "Julian McAuley", "Lina Yao"], "title": "Pluralistic Off-policy Evaluation and Alignment", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Personalized preference alignment for LLMs with diverse human preferences\nrequires evaluation and alignment methods that capture pluralism. Most existing\npreference alignment datasets are logged under policies that differ\nsubstantially from the evaluated LLMs, and existing off-policy estimators focus\nsolely on overall utility while ignoring preference pluralism. Extending\nOff-Policy Evaluation (OPE) to pluralistic preference alignment, therefore,\nremains an open question. Thus, we propose the Pluralistic Off-Policy\nEvaluation (POPE), the first framework for offline pluralistic preference\nevaluation and alignment in LLMs. POPE includes a unified reward function that\ncombines (1) a collaborative utility component derived from human preference\nsignals (e.g., upvotes or relevance scores) and (2) a diversity component\ninspired by entropy-based coverage measures, together reflecting pluralistic\nalignment. Furthermore, to estimate this reward from logged interactions, we\nderive decomposable inverse propensity scoring (IPS) estimators that separately\nevaluate relevance and diversity. Theoretically, we prove that our decomposed\nIPS estimators establish a lower bound on their variance. With the off-policy\nevaluated value function, we can directly enable off-policy optimization to\nfurther enhance pluralistic alignment. Empirical results demonstrate that POPE\nefficiently enhances pluralistic response generation and maintains the models'\ngeneral capabilities on downstream tasks", "AI": {"tldr": "The paper introduces POPE, a framework for better aligning large language models (LLMs) with diverse human preferences by incorporating pluralism in preference evaluation and alignment.", "motivation": "To address the need for aligning LLMs with diverse human preferences, which current methods fail to fully capture due to their focus solely on overall utility without addressing pluralism.", "method": "The paper introduces POPE, which combines a collaborative utility component with a diversity component in its reward function and uses decomposable inverse propensity scoring (IPS) estimators for evaluation and optimization.", "result": "Empirical findings show POPE enhances pluralistic response generation while preserving the general capabilities of LLMs on other tasks.", "conclusion": "POPE is an effective framework for improving pluralistic alignment in LLMs, balancing diverse human preferences and model performance."}}
{"id": "2509.19525", "pdf": "https://arxiv.org/pdf/2509.19525", "abs": "https://arxiv.org/abs/2509.19525", "authors": ["James Avtges", "Jake Ketchum", "Millicent Schlafly", "Helena Young", "Taekyoung Kim", "Allison Pinosky", "Ryan L. Truby", "Todd D. Murphey"], "title": "Real-Time Reinforcement Learning for Dynamic Tasks with a Parallel Soft Robot", "categories": ["cs.RO"], "comment": "Published at IEEE/RSJ International Conference on Intelligent Robots\n  and Systems (IROS) 2025", "summary": "Closed-loop control remains an open challenge in soft robotics. The nonlinear\nresponses of soft actuators under dynamic loading conditions limit the use of\nanalytic models for soft robot control. Traditional methods of controlling soft\nrobots underutilize their configuration spaces to avoid nonlinearity,\nhysteresis, large deformations, and the risk of actuator damage. Furthermore,\nepisodic data-driven control approaches such as reinforcement learning (RL) are\ntraditionally limited by sample efficiency and inconsistency across\ninitializations. In this work, we demonstrate RL for reliably learning control\npolicies for dynamic balancing tasks in real-time single-shot hardware\ndeployments. We use a deformable Stewart platform constructed using parallel,\n3D-printed soft actuators based on motorized handed shearing auxetic (HSA)\nstructures. By introducing a curriculum learning approach based on expanding\nneighborhoods of a known equilibrium, we achieve reliable single-deployment\nbalancing at arbitrary coordinates. In addition to benchmarking the performance\nof model-based and model-free methods, we demonstrate that in a single\ndeployment, Maximum Diffusion RL is capable of learning dynamic balancing after\nhalf of the actuators are effectively disabled, by inducing buckling and by\nbreaking actuators with bolt cutters. Training occurs with no prior data, in as\nfast as 15 minutes, with performance nearly identical to the fully-intact\nplatform. Single-shot learning on hardware facilitates soft robotic systems\nreliably learning in the real world and will enable more diverse and capable\nsoft robots.", "AI": {"tldr": "The researchers introduced a curriculum-based RL approach to enable real-time learning of dynamic balancing tasks for soft robots, showcasing robustness even under actuator damage.", "motivation": "To address challenges in controlling soft robots due to their nonlinear responses, hysteresis, and risk of actuator damage, as well as inefficiencies in episodic RL training methods.", "method": "Using Reinforcement Learning (RL) with curriculum learning on a soft robotic Stewart platform constructed of 3D-printed actuators, allowing real-time hardware training, even in adverse conditions.", "result": "The RL approach successfully learned dynamic balancing tasks in real-time with training lasting only 15 minutes. It performed reliably even with actuators disabled or physically damaged.", "conclusion": "Real-time RL methods with curriculum learning improve the robustness and real-world applicability of soft robotic systems, enabling more adaptive and capable hardware."}}
{"id": "2509.19664", "pdf": "https://arxiv.org/pdf/2509.19664", "abs": "https://arxiv.org/abs/2509.19664", "authors": ["Zeyu He", "Shuai Huang", "Yuwu Lu", "Ming Zhao"], "title": "MoTiC: Momentum Tightness and Contrast for Few-Shot Class-Incremental Learning", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Few-Shot Class-Incremental Learning (FSCIL) must contend with the dual\nchallenge of learning new classes from scarce samples while preserving old\nclass knowledge. Existing methods use the frozen feature extractor and\nclass-averaged prototypes to mitigate against catastrophic forgetting and\noverfitting. However, new-class prototypes suffer significant estimation bias\ndue to extreme data scarcity, whereas base-class prototypes benefit from\nsufficient data. In this work, we theoretically demonstrate that aligning the\nnew-class priors with old-class statistics via Bayesian analysis reduces\nvariance and improves prototype accuracy. Furthermore, we propose large-scale\ncontrastive learning to enforce cross-category feature tightness. To further\nenrich feature diversity and inject prior information for new-class prototypes,\nwe integrate momentum self-supervision and virtual categories into the Momentum\nTightness and Contrast framework (MoTiC), constructing a feature space with\nrich representations and enhanced interclass cohesion. Experiments on three\nFSCIL benchmarks produce state-of-the-art performances, particularly on the\nfine-grained task CUB-200, validating our method's ability to reduce estimation\nbias and improve incremental learning robustness.", "AI": {"tldr": "This paper introduces a method to improve Few-Shot Class-Incremental Learning (FSCIL) by enhancing prototype accuracy and interclass feature cohesion.", "motivation": "FSCIL faces challenges of learning new classes with very limited data while retaining knowledge of old classes.", "method": "The authors propose MoTiC, a framework combining Bayesian analysis, momentum self-supervision, virtual categories, and large-scale contrastive learning.", "result": "MoTiC achieves state-of-the-art performance across FSCIL benchmarks, including the fine-grained CUB-200 dataset.", "conclusion": "The method effectively reduces prototype bias and robustness in incremental learning scenarios."}}
{"id": "2509.19391", "pdf": "https://arxiv.org/pdf/2509.19391", "abs": "https://arxiv.org/abs/2509.19391", "authors": ["Axel Marmoret", "Reda Bensaid", "Jonathan Lys", "Vincent Gripon", "Fran\u00e7ois Leduc-Primeau"], "title": "TensLoRA: Tensor Alternatives for Low-Rank Adaptation", "categories": ["cs.LG", "cs.AI", "68T", "I.2.6; I.2.7; I.2.10"], "comment": "Submitted at ICASSP 2026. 5 pages, 1 figure, 2 tables. Code can be\n  found at https://github.com/ax-le/TensLoRA", "summary": "Low-Rank Adaptation (LoRA) is widely used to efficiently adapt Transformers\nby adding trainable low-rank matrices to attention projections. While\neffective, these matrices are considered independent for each attention\nprojection (Query, Key, and Value) and each layer. Recent extensions have\nconsidered joint, tensor-based adaptations, but only in limited forms and\nwithout a systematic framework. We introduce TensLoRA, a unified framework that\naggregates LoRA updates into higher-order tensors and models a broad family of\ntensor-based low-rank adaptations. Our formulation generalizes existing\ntensor-based methods and enables mode-specific compression rates, allowing\nparameter budgets to be tailored according to the modality and task.\nExperiments on vision and language benchmarks reveal that the tensor\nconstruction directly impacts performance, sometimes better than standard LoRA\nunder similar parameter counts.", "AI": {"tldr": "TensLoRA enhances LoRA by combining updates into higher-order tensors, providing flexible and efficient adaptation for transformers.", "motivation": "The paper aims to improve upon LoRA's efficiency limitation by exploring tensor-based methodologies for adaptation in transformers.", "method": "The authors propose TensLoRA, a framework that aggregates LoRA updates into higher-order tensors, enabling customizable compression rates tailored for specific tasks.", "result": "Experiments on vision and language benchmarks demonstrate TensLoRA's adaptability, occasionally outperforming standard LoRA under similar parameter conditions.", "conclusion": "TensLoRA provides a systematic and general formulation for tensor-based adaptations, enhancing parameter efficiency and task customization."}}
{"id": "2509.20101", "pdf": "https://arxiv.org/pdf/2509.20101", "abs": "https://arxiv.org/abs/2509.20101", "authors": ["Matteo Benati", "Alessandro Londei", "Denise Lanzieri", "Vittorio Loreto"], "title": "First-Extinction Law for Resampling Processes", "categories": ["stat.ML", "cs.IT", "cs.LG", "math.IT", "math.ST", "physics.data-an", "q-bio.PE", "stat.TH"], "comment": null, "summary": "Extinction times in resampling processes are fundamental yet often\nintractable, as previous formulas scale as $2^M$ with the number of states $M$\npresent in the initial probability distribution. We solve this by treating\nmultinomial updates as independent square-root diffusions of zero drift,\nyielding a closed-form law for the first-extinction time. We prove that the\nmean coincides exactly with the Wright-Fisher result of Baxter et al., thereby\nreplacing exponential-cost evaluations with a linear-cost expression, and we\nvalidate this result through extensive simulations. Finally, we demonstrate\npredictive power for model collapse in a simple self-training setup: the onset\nof collapse coincides with the resampling-driven first-extinction time computed\nfrom the model's initial stationary distribution. These results hint to a\nunified view of resampling extinction dynamics.", "AI": {"tldr": "This study reduces the computational complexity of calculating extinction times in resampling processes, providing a linear-cost expression validated through simulations.", "motivation": "Extinction times in resampling processes are crucial for understanding various dynamics but are computationally expensive to analyze using previous methods.", "method": "The authors treat multinomial updates as independent square-root diffusions with zero drift, deriving a closed-form law for the first-extinction time and proving it matches known results while significantly reducing complexity.", "result": "The derived expression replaces exponential-cost computations with a linear-cost formula, validated through simulations, and demonstrates predictive power in modeling collapse scenarios.", "conclusion": "The paper offers a computationally efficient framework for resampling extinction dynamics and suggests broader applications in understanding such processes."}}
{"id": "2509.19762", "pdf": "https://arxiv.org/pdf/2509.19762", "abs": "https://arxiv.org/abs/2509.19762", "authors": ["Yuanxin Wang", "Pawel Filipczuk", "Anisha Garg", "Amaan Dhada", "Mohammad Hassanpour", "David Bick", "Ganesh Venkatesh"], "title": "The Conductor and the Engine: A Path Towards Co-Designed Reasoning", "categories": ["cs.AI"], "comment": null, "summary": "Modern LLM reasoning relies on extensive test-time computation, driven by\ninternal model training and external agentic orchestration. However, this\nsynergy is often inefficient, as model verbosity and poor instruction following\nlead to wasted compute. We analyze this capability-cost trade-off and introduce\nan optimized reasoning workflow (\\cepo) that empowers smaller open-source\nmodels to outperform models multiple times their size. We will open-source this\nworkflow to enable further research. Our work demonstrates a clear path toward\nco-designing orchestration frameworks with the underlying model capabilities to\nunlock powerful reasoning in small-to-medium sized models.", "AI": {"tldr": "The study introduces CEPO, an optimized reasoning workflow allowing smaller open-source language models to rival or surpass larger models in reasoning tasks.", "motivation": "To address the inefficiency of modern LLM reasoning frameworks due to model verbosity and poor instruction adherence, and improve performance without excessive computation.", "method": "The paper analyzes the trade-off between capability and cost in language model reasoning and develops CEPO, a co-designed orchestration framework tailored for smaller models.", "result": "Smaller open-source models equipped with CEPO outperform larger models multiple times their size in reasoning tasks.", "conclusion": "Optimizing workflows in tandem with model capabilities unlocks powerful reasoning in small-to-medium-sized models, paving the way for efficient AI development."}}
{"id": "2509.20340", "pdf": "https://arxiv.org/pdf/2509.20340", "abs": "https://arxiv.org/abs/2509.20340", "authors": ["Liubov Kurafeeva", "Alan Subedi", "Ryan Hartung", "Michael Fay", "Avhishek Biswas", "Shantenu Jha", "Ozgur O. Kilic", "Chandra Krintz", "Andre Merzky", "Douglas Thain", "Mehmet C. Vuran", "Rich Wolski"], "title": "xGFabric: Coupling Sensor Networks and HPC Facilities with Private 5G Wireless Networks for Real-Time Digital Agriculture", "categories": ["cs.DC"], "comment": "8 pages with 7 figures followed by 3 pages of reproducibility\n  appendix. This paper will be published following the SC 2025 conference on\n  November 16-21, 2025 at St Louis, MO, USA. ISBN: 978-8-4007-1871-7/2025/11", "summary": "Advanced scientific applications require coupling distributed sensor networks\nwith centralized high-performance computing facilities. Citrus Under Protective\nScreening (CUPS) exemplifies this need in digital agriculture, where citrus\nresearch facilities are instrumented with numerous sensors monitoring\nenvironmental conditions and detecting protective screening damage. CUPS\ndemands access to computational fluid dynamics codes for modeling environmental\nconditions and guiding real-time interventions like water application or\nrobotic repairs. These computing domains have contrasting properties: sensor\nnetworks provide low-performance, limited-capacity, unreliable data access,\nwhile high-performance facilities offer enormous computing power through\nhigh-latency batch processing. Private 5G networks present novel capabilities\naddressing this challenge by providing low latency, high throughput, and\nreliability necessary for near-real-time coupling of edge sensor networks with\nHPC simulations. This work presents xGFabric, an end-to-end system coupling\nsensor networks with HPC facilities through Private 5G networks. The prototype\nconnects remote sensors via 5G network slicing to HPC systems, enabling\nreal-time digital agriculture simulation.", "AI": {"tldr": "This paper introduces xGFabric, a solution combining sensor networks with HPC facilities through private 5G networks for real-time agricultural simulation.", "motivation": "To address the need for coupling low-performance sensor networks with high-performance computing (HPC) for real-time applications in digital agriculture.", "method": "The authors created xGFabric, a system leveraging private 5G network slicing to integrate remote sensors with HPC systems effectively.", "result": "A prototype implementation of xGFabric successfully demonstrates real-time digital agricultural simulations by connecting sensor networks to HPC through 5G.", "conclusion": "xGFabric showcases the potential of private 5G networks for enabling low-latency, reliable connections between distributed sensors and HPC systems for advanced applications like digital agriculture."}}
{"id": "2509.19336", "pdf": "https://arxiv.org/pdf/2509.19336", "abs": "https://arxiv.org/abs/2509.19336", "authors": ["Qingsong Wang", "Tao Wu", "Wang Lin", "Yueying Feng", "Gongsheng Yuan", "Chang Yao", "Jingyuan Chen"], "title": "Cognitive-Level Adaptive Generation via Capability-Aware Retrieval and Style Adaptation", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to Findings of EMNLP 2026", "summary": "Large Language Models (LLMs) have demonstrated strong performance in\nopen-ended generation tasks. However, they often struggle to adapt content to\nusers with differing cognitive capacities, leading to a phenomenon we term\ncognitive misalignment. This issue arises in two forms: knowledge-level\nmisalignment, where content is too complex or too simplistic relative to user\nunderstanding, and presentation-style misalignment, where the structure or tone\nhinders effective comprehension. To address these challenges, we propose the\nCognitive-Level Alignment Framework (CLAF), a general-purpose generation\nframework that aligns both knowledge complexity and presentation style with\nuser cognition. CLAF integrates a capability-aware retrieval module based on a\nhierarchical knowledge graph and a style optimization module guided by Bloom's\ntaxonomy and preference learning. Additionally, a knowledge-controllable\ngeneration component ensures consistency and relevance throughout the output.\nTo support training and evaluation, we construct SCALE, a cognitively annotated\ndataset containing responses at multiple comprehension levels per query.\nEmpirical results show that CLAF enhances the adaptability and informativeness\nof LLM outputs across a range of user profiles, offering a robust solution to\ncognitive-level alignment in real-world applications.", "AI": {"tldr": "The paper introduces CLAF, a framework for aligning content generated by LLMs with users' cognitive capacities, addressing issues in knowledge complexity and presentation style.", "motivation": "LLMs often fail to adapt content to users' cognitive levels, resulting in misalignment in knowledge complexity and presentation style.", "method": "The authors propose CLAF, which integrates a retrieval module, style optimization guided by Bloom's taxonomy, and a knowledge-controllable generation system. A new dataset, SCALE, was created for training and evaluation.", "result": "Empirical results indicate that CLAF improves LLM outputs' adaptability and informativeness across diverse user profiles.", "conclusion": "CLAF shows promise in solving cognitive misalignment issues, enhancing user-specific content generation in practical applications."}}
{"id": "2509.19541", "pdf": "https://arxiv.org/pdf/2509.19541", "abs": "https://arxiv.org/abs/2509.19541", "authors": ["Xuan Cao", "Yuxin Wu", "Michael L. Whittaker"], "title": "Autonomous Elemental Characterization Enabled by a Low Cost Robotic Platform Built Upon a Generalized Software Architecture", "categories": ["cs.RO"], "comment": null, "summary": "Despite the rapidly growing applications of robots in industry, the use of\nrobots to automate tasks in scientific laboratories is less prolific due to\nlack of generalized methodologies and high cost of hardware. This paper focuses\non the automation of characterization tasks necessary for reducing cost while\nmaintaining generalization, and proposes a software architecture for building\nrobotic systems in scientific laboratory environment. A dual-layer (Socket.IO\nand ROS) action server design is the basic building block, which facilitates\nthe implementation of a web-based front end for user-friendly operations and\nthe use of ROS Behavior Tree for convenient task planning and execution. A\nrobotic platform for automating mineral and material sample characterization is\nbuilt upon the architecture, with an open source, low-cost three-axis computer\nnumerical control gantry system serving as the main robot. A handheld laser\ninduced breakdown spectroscopy (LIBS) analyzer is integrated with a 3D printed\nadapter, enabling automated 2D chemical mapping. We demonstrate the utility of\nautomated chemical mapping by scanning of the surface of a spodumene-bearing\npegmatite core sample with a 1071-point dense hyperspectral map acquired at a\nrate of 1520 bits per second. Automated LIBS scanning enables controlled\nchemical quantification in the laboratory that complements field-based\nmeasurements acquired with the same handheld device, linking resource\nexploration and processing steps in the supply chain for lithium-based battery\nmaterials.", "AI": {"tldr": "The paper introduces a dual-layer software architecture to facilitate robotic automation in scientific laboratories, demonstrated using a low-cost robotic system for material sample characterization.", "motivation": "Robotic automation in scientific laboratories lags due to lack of generalized methodologies and expensive hardware. The paper aims to address this gap by developing a cost-effective, generalizable approach for laboratory automation.", "method": "The authors propose a dual-layer design combining Socket.IO and ROS action server architecture, a web-based interface, and a ROS Behavior Tree for planning. For demonstration, they used a low-cost, open-source robot integrating a handheld laser-induced breakdown spectroscopy device with a 3D adapter for automated 2D chemical mapping.", "result": "They showcased the automation by creating a dense hyperspectral map (1071 points) of a core sample at a rate of 1520 bits per second, demonstrating effective and precise chemical characterization.", "conclusion": "The proposed system enhances laboratory automation in a cost-effective manner, improving chemical quantification and integration with field measurements, especially for analyzing materials like those used in lithium battery supply chains."}}
{"id": "2509.19665", "pdf": "https://arxiv.org/pdf/2509.19665", "abs": "https://arxiv.org/abs/2509.19665", "authors": ["Manuel Perez-Carrasco", "Maya Nasr", "Sebastien Roche", "Chris Chan Miller", "Zhan Zhang", "Core Francisco Park", "Eleanor Walker", "Cecilia Garraffo", "Douglas Finkbeiner", "Ritesh Gautam", "Steven Wofsy"], "title": "Deep Learning for Clouds and Cloud Shadow Segmentation in Methane Satellite and Airborne Imaging Spectroscopy", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Effective cloud and cloud shadow detection is a critical prerequisite for\naccurate retrieval of concentrations of atmospheric methane or other trace\ngases in hyperspectral remote sensing. This challenge is especially pertinent\nfor MethaneSAT and for its airborne companion mission, MethaneAIR. In this\nstudy, we use machine learning methods to address the cloud and cloud shadow\ndetection problem for sensors with these high spatial resolutions instruments.\nCloud and cloud shadows in remote sensing data need to be effectively screened\nout as they bias methane retrievals in remote sensing imagery and impact the\nquantification of emissions. We deploy and evaluate conventional techniques\nincluding Iterative Logistic Regression (ILR) and Multilayer Perceptron (MLP),\nwith advanced deep learning architectures, namely UNet and a Spectral Channel\nAttention Network (SCAN) method. Our results show that conventional methods\nstruggle with spatial coherence and boundary definition, affecting the\ndetection of clouds and cloud shadows. Deep learning models substantially\nimprove detection quality: UNet performs best in preserving spatial structure,\nwhile SCAN excels at capturing fine boundary details. Notably, SCAN surpasses\nUNet on MethaneSAT data, underscoring the benefits of incorporating spectral\nattention for satellite specific features. This in depth assessment of various\ndisparate machine learning techniques demonstrates the strengths and\neffectiveness of advanced deep learning architectures in providing robust,\nscalable solutions for clouds and cloud shadow screening towards enhancing\nmethane emission quantification capacity of existing and next generation\nhyperspectral missions. Our data and code is publicly available at\nhttps://doi.org/10.7910/DVN/IKLZOJ", "AI": {"tldr": "This study applies machine learning, particularly deep learning architectures like UNet and SCAN, for effective cloud and cloud shadow detection in hyperspectral remote sensing data to improve methane retrieval and quantification, outperforming conventional methods.", "motivation": "Accurate detection of clouds and their shadows is crucial for hyperspectral remote sensing to avoid biased methane retrievals and enhance the accuracy of emissions quantification from instruments like MethaneSAT and MethaneAIR.", "method": "The paper evaluates machine learning methods, comparing conventional techniques such as Iterative Logistic Regression (ILR) and Multilayer Perceptron (MLP) with advanced deep learning models like UNet and Spectral Channel Attention Network (SCAN).", "result": "Deep learning models significantly improve cloud and cloud shadow detection quality, with UNet excelling at preserving spatial coherence and SCAN capturing fine boundary details. SCAN performs better than UNet in MethaneSAT data.", "conclusion": "Advanced deep learning models offer robust, scalable solutions for cloud screening, enhancing the utility and reliability of hyperspectral missions for methane emission quantification. Their publicly available code supports further research and application."}}
{"id": "2509.19396", "pdf": "https://arxiv.org/pdf/2509.19396", "abs": "https://arxiv.org/abs/2509.19396", "authors": ["Sahil Tyagi", "Andrei Cozma", "Olivera Kotevska", "Feiyi Wang"], "title": "OmniFed: A Modular Framework for Configurable Federated Learning from Edge to HPC", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.DC"], "comment": null, "summary": "Federated Learning (FL) is critical for edge and High Performance Computing\n(HPC) where data is not centralized and privacy is crucial. We present OmniFed,\na modular framework designed around decoupling and clear separation of concerns\nfor configuration, orchestration, communication, and training logic. Its\narchitecture supports configuration-driven prototyping and code-level\noverride-what-you-need customization. We also support different topologies,\nmixed communication protocols within a single deployment, and popular training\nalgorithms. It also offers optional privacy mechanisms including Differential\nPrivacy (DP), Homomorphic Encryption (HE), and Secure Aggregation (SA), as well\nas compression strategies. These capabilities are exposed through well-defined\nextension points, allowing users to customize topology and orchestration,\nlearning logic, and privacy/compression plugins, all while preserving the\nintegrity of the core system. We evaluate multiple models and algorithms to\nmeasure various performance metrics. By unifying topology configuration,\nmixed-protocol communication, and pluggable modules in one stack, OmniFed\nstreamlines FL deployment across heterogeneous environments. Github repository\nis available at https://github.com/at-aaims/OmniFed.", "AI": {"tldr": "OmniFed is a modular framework for Federated Learning that offers flexibility in configuration, privacy options, and communication protocols, simplifying deployment across diverse environments.", "motivation": "The paper addresses the need for a robust, modular framework to facilitate Federated Learning deployment in situations where data is decentralized, and privacy is paramount.", "method": "OmniFed is designed with a modular architecture allowing for customization of topology, orchestration, training logic, and privacy mechanisms, offering support for various protocols, algorithms, and compression strategies.", "result": "The framework was evaluated using multiple models and algorithms to measure performance, demonstrating its capabilities for streamlining Federated Learning deployment.", "conclusion": "OmniFed unifies different aspects of Federated Learning into a single framework with extension points for customization, making it versatile for edge and HPC applications while preserving core system integrity."}}
{"id": "2509.20239", "pdf": "https://arxiv.org/pdf/2509.20239", "abs": "https://arxiv.org/abs/2509.20239", "authors": ["Andrea Della Vecchia", "Damir Filipovi\u0107"], "title": "Error Propagation in Dynamic Programming: From Stochastic Control to Option Pricing", "categories": ["stat.ML", "cs.LG", "q-fin.CP", "q-fin.PR", "stat.AP"], "comment": null, "summary": "This paper investigates theoretical and methodological foundations for\nstochastic optimal control (SOC) in discrete time. We start formulating the\ncontrol problem in a general dynamic programming framework, introducing the\nmathematical structure needed for a detailed convergence analysis. The\nassociate value function is estimated through a sequence of approximations\ncombining nonparametric regression methods and Monte Carlo subsampling. The\nregression step is performed within reproducing kernel Hilbert spaces (RKHSs),\nexploiting the classical KRR algorithm, while Monte Carlo sampling methods are\nintroduced to estimate the continuation value. To assess the accuracy of our\nvalue function estimator, we propose a natural error decomposition and\nrigorously control the resulting error terms at each time step. We then analyze\nhow this error propagates backward in time-from maturity to the initial stage-a\nrelatively underexplored aspect of the SOC literature. Finally, we illustrate\nhow our analysis naturally applies to a key financial application: the pricing\nof American options.", "AI": {"tldr": "The paper formulates a discrete-time stochastic optimal control (SOC) problem, leveraging nonparametric regression in RKHSs and Monte Carlo methods for value function approximation, and rigorously assessing error propagation backward in time, with applications to American options pricing.", "motivation": "To analyze foundational theoretical and methodological aspects of SOC in discrete time with a focus on convergence and error propagation, and demonstrate its application in financial contexts.", "method": "Combines dynamic programming with nonparametric regression within RKHSs (using KRR) and Monte Carlo subsampling to approximate the value function, providing error decomposition and backward propagation analysis.", "result": "The paper offers a rigorous mathematical framework and error analysis for SOC problems, showing how to control error terms backward in time and applying it specifically to the pricing of American options.", "conclusion": "The analysis provides a generalized and systematic approach to discrete-time SOC with practical financial applications, offering deep insights into error control and propagation in such problems."}}
{"id": "2509.19783", "pdf": "https://arxiv.org/pdf/2509.19783", "abs": "https://arxiv.org/abs/2509.19783", "authors": ["Jiexi Xu"], "title": "Agentic Metacognition: Designing a \"Self-Aware\" Low-Code Agent for Failure Prediction and Human Handoff", "categories": ["cs.AI", "cs.HC", "cs.SE"], "comment": "7 pages, 2 tables", "summary": "The inherent non-deterministic nature of autonomous agents, particularly\nwithin low-code/no-code (LCNC) environments, presents significant reliability\nchallenges. Agents can become trapped in unforeseen loops, generate inaccurate\noutputs, or encounter unrecoverable failures, leading to user frustration and a\nbreakdown of trust. This report proposes a novel architectural pattern to\naddress these issues: the integration of a secondary, \"metacognitive\" layer\nthat actively monitors the primary LCNC agent. Inspired by human introspection,\nthis layer is designed to predict impending task failures based on a defined\nset of triggers, such as excessive latency or repetitive actions. Upon\npredicting a failure, the metacognitive agent proactively initiates a human\nhandoff, providing the user with a clear summary of the agent's \"thought\nprocess\" and a detailed explanation of why it could not proceed. An empirical\nanalysis of a prototype system demonstrates that this approach significantly\nincreases the overall task success rate. However, this performance gain comes\nwith a notable increase in computational overhead. The findings reframe human\nhandoffs not as an admission of defeat but as a core design feature that\nenhances system resilience, improves user experience, and builds trust by\nproviding transparency into the agent's internal state. The report discusses\nthe practical and ethical implications of this approach and identifies key\ndirections for future research.", "AI": {"tldr": "This paper proposes a metacognitive layer for autonomous agents in LCNC environments to enhance reliability, addressing issues such as task failures and unrecoverable errors.", "motivation": "Address the reliability challenges posed by the non-deterministic nature of autonomous agents, which can lead to user frustration and trust issues.", "method": "Introduce a metacognitive layer that monitors the primary LCNC agent, predicts failures based on predefined triggers, and initiates a human handoff with a detailed explanation.", "result": "Prototype analysis shows increased task success rates but with higher computational overhead.", "conclusion": "Human handoffs, supported by metacognitive transparency, enhance system resilience and trust while offering a blueprint for ethical and practical advancements in autonomous agents."}}
{"id": "2509.20300", "pdf": "https://arxiv.org/pdf/2509.20300", "abs": "https://arxiv.org/abs/2509.20300", "authors": ["Jannis Kiesel", "Jonathan Heiss"], "title": "Confidentiality-Preserving Verifiable Business Processes through Zero-Knowledge Proofs", "categories": ["cs.SE"], "comment": null, "summary": "Ensuring the integrity of business processes without disclosing confidential\nbusiness information is a major challenge in inter-organizational processes.\nThis paper introduces a zero-knowledge proof (ZKP)-based approach for the\nverifiable execution of business processes while preserving confidentiality. We\nintegrate ZK virtual machines (zkVMs) into business process management engines\nthrough a comprehensive system architecture and a prototypical implementation.\nOur approach supports chained verifiable computations through proof\ncompositions. On the example of product carbon footprinting, we model\nsequential footprinting activities and demonstrate how organizations can prove\nand verify the integrity of verifiable processes without exposing sensitive\ninformation. We assess different ZKP proving variants within process models for\ntheir efficiency in proving and verifying, and discuss the practical\nintegration of ZKPs throughout the Business Process Management (BPM) lifecycle.\nOur experiment-driven evaluation demonstrates the automation of process\nverification under given confidentiality constraints.", "AI": {"tldr": "The paper presents a zero-knowledge proof (ZKP)-based approach enabling verifiable business process execution while ensuring confidentiality of data, using a system architecture and prototype integrating ZK virtual machines.", "motivation": "To address the challenge of ensuring process integrity in inter-organizational processes without disclosing confidential business information.", "method": "The authors integrate ZK virtual machines (zkVMs) into business process management engines via a system architecture and prototype, enabling proof compositions for chained verifications and examining ZKP proving variants within process models.", "result": "A demonstration was conducted using product carbon footprinting, showcasing organizations verifying process integrity while maintaining data confidentiality, with an evaluation proving feasibility and automation of process verification.", "conclusion": "The approach successfully enables confidential and verifiable business process execution, with potential for practical integration throughout the BPM lifecycle."}}
{"id": "2509.19343", "pdf": "https://arxiv.org/pdf/2509.19343", "abs": "https://arxiv.org/abs/2509.19343", "authors": ["Alovi N Shohe", "Chonglio Khiamungam", "Teisovi Angami"], "title": "Part-of-speech tagging for Nagamese Language using CRF", "categories": ["cs.CL", "cs.AI"], "comment": "8 pages", "summary": "This paper investigates part-of-speech tagging, an important task in Natural\nLanguage Processing (NLP) for the Nagamese language. The Nagamese language,\na.k.a. Naga Pidgin, is an Assamese-lexified Creole language developed primarily\nas a means of communication in trade between the Nagas and people from Assam in\nnortheast India. A substantial amount of work in part-of-speech-tagging has\nbeen done for resource-rich languages like English, Hindi, etc. However, no\nwork has been done in the Nagamese language. To the best of our knowledge, this\nis the first attempt at part-of-speech tagging for the Nagamese Language. The\naim of this work is to identify the part-of-speech for a given sentence in the\nNagamese language. An annotated corpus of 16,112 tokens is created and applied\nmachine learning technique known as Conditional Random Fields (CRF). Using CRF,\nan overall tagging accuracy of 85.70%; precision, recall of 86%, and f1-score\nof 85% is achieved.\n  Keywords. Nagamese, NLP, part-of-speech, machine learning, CRF.", "AI": {"tldr": "This study addresses the first part-of-speech (POS) tagging for the Nagamese language using Conditional Random Fields (CRF), achieving 85.7% accuracy.", "motivation": "Investigate and establish a POS tagging system for the under-researched and resource-scarce Nagamese language, which has no prior work in this area.", "method": "The authors created an annotated corpus of 16,112 tokens and applied the CRF algorithm to identify parts of speech in Nagamese sentences.", "result": "The study achieved 85.7% tagging accuracy, with precision and recall at 86% and an F1-score of 85%.", "conclusion": "This foundational work sets the stage for further NLP research in Nagamese by successfully implementing a POS tagging system with machine learning techniques."}}
{"id": "2509.19545", "pdf": "https://arxiv.org/pdf/2509.19545", "abs": "https://arxiv.org/abs/2509.19545", "authors": ["Min Dai", "Aaron D. Ames"], "title": "RoMoCo: Robotic Motion Control Toolbox for Reduced-Order Model-Based Locomotion on Bipedal and Humanoid Robots", "categories": ["cs.RO"], "comment": null, "summary": "We present RoMoCo, an open-source C++ toolbox for the synthesis and\nevaluation of reduced-order model-based planners and whole-body controllers for\nbipedal and humanoid robots. RoMoCo's modular architecture unifies\nstate-of-the-art planners and whole-body locomotion controllers under a\nconsistent API, enabling rapid prototyping and reproducible benchmarking. By\nleveraging reduced-order models for platform-agnostic gait generation, RoMoCo\nenables flexible controller design across diverse robots. We demonstrate its\nversatility and performance through extensive simulations on the Cassie,\nUnitree H1, and G1 robots, and validate its real-world efficacy with hardware\nexperiments on the Cassie and G1 humanoids.", "AI": {"tldr": "RoMoCo is an open-source C++ toolbox for creating and evaluating reduced-order model planners and controllers for bipedal and humanoid robots.", "motivation": "To provide a unified and modular solution for rapid prototyping and benchmarking of robot locomotion planners and controllers.", "method": "A consistent API integrating state-of-the-art planning and whole-body control techniques, applicable to diverse robotic platforms through reduced-order models.", "result": "Successfully demonstrated through simulations on Cassie, Unitree H1, and G1 robots, with hardware validations on Cassie and G1.", "conclusion": "RoMoCo offers flexibility, performance, and real-world efficacy for designing locomotion controllers across different robot platforms."}}
{"id": "2509.19687", "pdf": "https://arxiv.org/pdf/2509.19687", "abs": "https://arxiv.org/abs/2509.19687", "authors": ["Sumit Mamtani"], "title": "Enhancing Transformer-Based Vision Models: Addressing Feature Map Anomalies Through Novel Optimization Strategies", "categories": ["cs.CV"], "comment": "8 pages, 8 figures, accepted and presented at IEEE BDAI 2025. The\n  final published version will be available on IEEE Xplore", "summary": "Vision Transformers (ViTs) have demonstrated superior performance across a\nwide range of computer vision tasks. However, structured noise artifacts in\ntheir feature maps hinder downstream applications such as segmentation and\ndepth estimation. We propose two novel and lightweight optimisation techniques-\nStructured Token Augmentation (STA) and Adaptive Noise Filtering (ANF)- to\nimprove interpretability and mitigate these artefacts. STA enhances token\ndiversity through spatial perturbations during tokenisation, while ANF applies\nlearnable inline denoising between transformer layers. These methods are\narchitecture-agnostic and evaluated across standard benchmarks, including\nImageNet, Ade20k, and NYUv2. Experimental results show consistent improvements\nin visual quality and task performance, highlighting the practical\neffectiveness of our approach.", "AI": {"tldr": "The paper introduces Structured Token Augmentation (STA) and Adaptive Noise Filtering (ANF) as methods to enhance Vision Transformers (ViTs) by addressing structured noise artifacts, improving both visual quality and task performance.", "motivation": "Despite the superior performance of Vision Transformers (ViTs) in computer vision tasks, their feature maps are hampered by structured noise artifacts, negatively affecting downstream applications like segmentation and depth estimation.", "method": "The paper proposes the Structured Token Augmentation (STA) to enhance token diversity via spatial perturbations during tokenization and Adaptive Noise Filtering (ANF) for learnable inline denoising within transformer layers. These are lightweight and architecture-agnostic techniques.", "result": "Experimental results demonstrate that STA and ANF consistently improve visual quality and task performance across benchmarks such as ImageNet, Ade20k, and NYUv2.", "conclusion": "The proposed techniques successfully mitigate noise artifacts in Vision Transformers, enhancing their interpretability and broadening their applicability in various computer vision tasks."}}
{"id": "2509.19406", "pdf": "https://arxiv.org/pdf/2509.19406", "abs": "https://arxiv.org/abs/2509.19406", "authors": ["Kuiye Ding", "Fanda Fan", "Chunyi Hou", "Zheya Wang", "Lei Wang", "Zhengxin Yang", "Jianfeng Zhan"], "title": "TimeMosaic: Temporal Heterogeneity Guided Time Series Forecasting via Adaptive Granularity Patch and Segment-wise Decoding", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Multivariate time series forecasting is essential in domains such as finance,\ntransportation, climate, and energy. However, existing patch-based methods\ntypically adopt fixed-length segmentation, overlooking the heterogeneity of\nlocal temporal dynamics and the decoding heterogeneity of forecasting. Such\ndesigns lose details in information-dense regions, introduce redundancy in\nstable segments, and fail to capture the distinct complexities of short-term\nand long-term horizons. We propose TimeMosaic, a forecasting framework that\naims to address temporal heterogeneity. TimeMosaic employs adaptive patch\nembedding to dynamically adjust granularity according to local information\ndensity, balancing motif reuse with structural clarity while preserving\ntemporal continuity. In addition, it introduces segment-wise decoding that\ntreats each prediction horizon as a related subtask and adapts to\nhorizon-specific difficulty and information requirements, rather than applying\na single uniform decoder. Extensive evaluations on benchmark datasets\ndemonstrate that TimeMosaic delivers consistent improvements over existing\nmethods, and our model trained on the large-scale corpus with 321 billion\nobservations achieves performance competitive with state-of-the-art TSFMs.", "AI": {"tldr": "TimeMosaic, a novel framework for multivariate time series forecasting, tackles challenges in temporal heterogeneity using adaptive patch embedding and segment-wise decoding strategies.", "motivation": "Existing multivariate time series forecasting methods often fail to address the heterogeneity in local temporal dynamics and in decoding strategies, leading to inefficiencies in information-dense regions and inaccuracies in capturing short-term versus long-term complexities.", "method": "TimeMosaic introduces adaptive patch embedding to modify segmentation granularity based on local information density while preserving continuity. It also employs segment-wise decoding to adapt predictions to horizon-specific requirements, treating different forecast horizons as separate subtasks.", "result": "Evaluations on benchmark datasets show that TimeMosaic consistently improves forecasting accuracy compared to current methodologies. Furthermore, the model trained on a dataset with 321 billion observations achieves performance on par with state-of-the-art forecasting systems.", "conclusion": "TimeMosaic effectively addresses the challenges of temporal heterogeneity in multivariate time series forecasting, yielding consistent and competitive results while leveraging adaptive methodologies for improved accuracy."}}
{"id": "2509.19367", "pdf": "https://arxiv.org/pdf/2509.19367", "abs": "https://arxiv.org/abs/2509.19367", "authors": ["Borhan Uddin Chowdhury", "Damian Valles", "Md Raf E Ul Shougat"], "title": "Low-Cost Sensor Fusion Framework for Organic Substance Classification and Quality Control Using Classification Methods", "categories": ["eess.SP", "cs.LG", "stat.ML"], "comment": "Copyright 2025 IEEE. This is the author's version of the work\n  accepted for publication in FMLDS 2025. The final version will be published\n  by IEEE and available via DOI (to be inserted when available). Accepted at\n  FMLDS 2025, to appear in IEEE Xplore. 8 pages, 17 figures, 3 tables", "summary": "We present a sensor-fusion framework for rapid, non-destructive\nclassification and quality control of organic substances, built on a standard\nArduino Mega 2560 microcontroller platform equipped with three commercial\nenvironmental and gas sensors. All data used in this study were generated\nin-house: sensor outputs for ten distinct classes - including fresh and expired\nsamples of apple juice, onion, garlic, and ginger, as well as cinnamon and\ncardamom - were systematically collected and labeled using this hardware setup,\nresulting in a unique, application-specific dataset. Correlation analysis was\nemployed as part of the preprocessing pipeline for feature selection. After\npreprocessing and dimensionality reduction (PCA/LDA), multiple supervised\nlearning models - including Support Vector Machine (SVM), Decision Tree (DT),\nand Random Forest (RF), each with hyperparameter tuning, as well as an\nArtificial Neural Network (ANN) and an ensemble voting classifier - were\ntrained and cross-validated on the collected dataset. The best-performing\nmodels, including tuned Random Forest, ensemble, and ANN, achieved test\naccuracies in the 93 to 94 percent range. These results demonstrate that\nlow-cost, multisensory platforms based on the Arduino Mega 2560, combined with\nadvanced machine learning and correlation-driven feature engineering, enable\nreliable identification and quality control of organic compounds.", "AI": {"tldr": "The paper introduces a low-cost sensor-fusion framework utilizing Arduino Mega 2560 with gas sensors to classify and control the quality of organic substances, achieving up to 94% accuracy via machine learning models.", "motivation": "To develop a non-destructive and efficient method for quality control and classification of organic substances using a cost-effective hardware platform.", "method": "Data from sensors on the Arduino Mega 2560 were collected for ten organic classes. The data underwent preprocessing (correlation analysis, PCA/LDA), and several machine learning models (SVM, DT, RF, ANN, ensemble classifiers) were implemented and tested.", "result": "The best-performing models, including Random Forest, ensemble, and ANN, achieved 93-94% classification accuracy on the organic substances dataset.", "conclusion": "Low-cost platforms like Arduino Mega 2560, combined with sensor fusion and machine learning, can provide reliable tools for rapid organic compound identification and quality control."}}
{"id": "2509.19800", "pdf": "https://arxiv.org/pdf/2509.19800", "abs": "https://arxiv.org/abs/2509.19800", "authors": ["Donghwan Lee", "Hyukjun Yang", "Bum Geun Park"], "title": "Analysis of approximate linear programming solution to Markov decision problem with log barrier function", "categories": ["cs.AI"], "comment": null, "summary": "There are two primary approaches to solving Markov decision problems (MDPs):\ndynamic programming based on the Bellman equation and linear programming (LP).\nDynamic programming methods are the most widely used and form the foundation of\nboth classical and modern reinforcement learning (RL). By contrast, LP-based\nmethods have been less commonly employed, although they have recently gained\nattention in contexts such as offline RL. The relative underuse of the LP-based\nmethods stems from the fact that it leads to an inequality-constrained\noptimization problem, which is generally more challenging to solve effectively\ncompared with Bellman-equation-based methods. The purpose of this paper is to\nestablish a theoretical foundation for solving LP-based MDPs in a more\neffective and practical manner. Our key idea is to leverage the log-barrier\nfunction, widely used in inequality-constrained optimization, to transform the\nLP formulation of the MDP into an unconstrained optimization problem. This\nreformulation enables approximate solutions to be obtained easily via gradient\ndescent. While the method may appear simple, to the best of our knowledge, a\nthorough theoretical interpretation of this approach has not yet been\ndeveloped. This paper aims to bridge this gap.", "AI": {"tldr": "This paper proposes using the log-barrier function to transform the LP formulation of MDPs into unconstrained optimization problems, enabling easier solution via gradient descent.", "motivation": "LP-based methods for solving MDPs are relatively underutilized due to challenges associated with inequality-constrained optimization problems.", "method": "The paper leverages the log-barrier function to reformulate the LP-based MDP optimization into an unconstrained problem, facilitating gradient descent solutions.", "result": "The reformulation using the log-barrier function provides a practical approach to approximate solutions, while offering a theoretical foundation for LP-based MDPs.", "conclusion": "The proposed method simplifies LP-based MDP resolution, bridging theoretical gaps and enabling practical application through gradient-based techniques."}}
{"id": "2509.20308", "pdf": "https://arxiv.org/pdf/2509.20308", "abs": "https://arxiv.org/abs/2509.20308", "authors": ["Alexander Liggesmeyer", "Jos\u00e9 Antonio Zamudio Amaya", "Andreas Zeller"], "title": "Protocol Testing with I/O Grammars", "categories": ["cs.SE", "68M15 (Primary), 68M12, 68Q42 (Secondary)", "D.2.5; C.2.2; F.4.2"], "comment": "20 pages", "summary": "Generating software tests faces two fundamental problems. First, one needs to\n_generate inputs_ that are syntactically and semantically correct, yet\nsufficiently diverse to cover behavior. Second, one needs an _oracle_ to _check\noutputs_ whether a test case is correct or not. Both problems become apparent\nin _protocol testing_, where inputs are messages exchanged between parties, and\noutputs are the responses of these parties.\n  In this paper, we propose a novel approach to protocol testing that combines\ninput generation and output checking in a single framework. We introduce _I/O\ngrammars_ as the first means to _completely_ specify the syntax and semantics\nof protocols, including messages, states, and interactions. Our implementation,\nbased on the FANDANGO framework, takes a single I/O grammar, and can act as a\n_test generator_, as a _mock object_, and as an _oracle_ for a _client_, a\n_server_, or both (or actually any number of parties), a versatility not found\nin any existing tool or formalism. User-defined _constraints}_can have the\ngenerator focus on arbitrary protocol features; $k$-path guidance\nsystematically covers states, messages, responses, and value alternatives in a\nunified fashion.\n  We evaluate the effectiveness of our approach by applying it to several\nprotocols, including DNS, FTP, and SMTP. We demonstrate that I/O grammars can\nspecify advanced protocol features correctly and completely, while also\nenabling output validation of the programs under test. In its evaluation, we\nfind that systematic coverage of the I/O grammar results in much quicker\ncoverage of the input and response spaces (and thus functionality) compared to\nthe random-based state-of-the-art approaches.", "AI": {"tldr": "This paper introduces I/O grammars for testing protocols by generating inputs and validating outputs within a unified framework, improving over random-based methods.", "motivation": "The goal is to address the challenge of generating syntactically and semantically valid inputs while ensuring output correctness, particularly in protocol testing.", "method": "Proposes I/O grammars to fully specify the syntax and semantics of protocol interactions, and integrates input generation and output checking with the FANDANGO framework.", "result": "I/O grammars enabled efficient and accurate specification and output validation of protocols (like DNS, FTP, SMTP) while achieving quicker and more systematic coverage compared to random techniques.", "conclusion": "The approach provides a versatile and systematic framework for protocol testing, outperforming existing tools and improving input and response space coverage significantly."}}
{"id": "2509.19344", "pdf": "https://arxiv.org/pdf/2509.19344", "abs": "https://arxiv.org/abs/2509.19344", "authors": ["Mahmoud Alwakeel", "Aditya Nagori", "An-Kwok Ian Wong", "Neal Chaisson", "Vijay Krishnamoorthy", "Rishikesan Kamaleswaran"], "title": "Performance of Large Language Models in Answering Critical Care Medicine Questions", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models have been tested on medical student-level questions,\nbut their performance in specialized fields like Critical Care Medicine (CCM)\nis less explored. This study evaluated Meta-Llama 3.1 models (8B and 70B\nparameters) on 871 CCM questions. Llama3.1:70B outperformed 8B by 30%, with 60%\naverage accuracy. Performance varied across domains, highest in Research\n(68.4%) and lowest in Renal (47.9%), highlighting the need for broader future\nwork to improve models across various subspecialty domains.", "AI": {"tldr": "Meta-Llama 3.1 models were tested on Critical Care Medicine questions. The 70B model scored an average accuracy of 60%, outperforming the 8B model by 30%, but performance varied by domain.", "motivation": "To assess the performance of large language models, specifically Meta-Llama 3.1, in specialized medical fields like Critical Care Medicine, where evaluation has been limited.", "method": "Meta-Llama 3.1 models with 8B and 70B parameters were tested on 871 Critical Care Medicine questions. Their performance was compared across various domains within the specialty.", "result": "The Llama3.1:70B model achieved an average accuracy of 60%, outperforming Llama3.1:8B by 30%. Performance ranged by domain, with the best in Research (68.4%) and the worst in Renal (47.9%).", "conclusion": "The performance of the Llama3.1:70B model shows promise in specialized medical fields, though its domain-specific variation signals a need for improvement across subspecialties."}}
{"id": "2509.19555", "pdf": "https://arxiv.org/pdf/2509.19555", "abs": "https://arxiv.org/abs/2509.19555", "authors": ["Sankalp Agrawal", "Junwon Seo", "Kensuke Nakamura", "Ran Tian", "Andrea Bajcsy"], "title": "AnySafe: Adapting Latent Safety Filters at Runtime via Safety Constraint Parameterization in the Latent Space", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "Recent works have shown that foundational safe control methods, such as\nHamilton-Jacobi (HJ) reachability analysis, can be applied in the latent space\nof world models. While this enables the synthesis of latent safety filters for\nhard-to-model vision-based tasks, they assume that the safety constraint is\nknown a priori and remains fixed during deployment, limiting the safety\nfilter's adaptability across scenarios. To address this, we propose\nconstraint-parameterized latent safety filters that can adapt to user-specified\nsafety constraints at runtime. Our key idea is to define safety constraints by\nconditioning on an encoding of an image that represents a constraint, using a\nlatent-space similarity measure. The notion of similarity to failure is aligned\nin a principled way through conformal calibration, which controls how closely\nthe system may approach the constraint representation. The parameterized safety\nfilter is trained entirely within the world model's imagination, treating any\nimage seen by the model as a potential test-time constraint, thereby enabling\nruntime adaptation to arbitrary safety constraints. In simulation and hardware\nexperiments on vision-based control tasks with a Franka manipulator, we show\nthat our method adapts at runtime by conditioning on the encoding of\nuser-specified constraint images, without sacrificing performance. Video\nresults can be found on https://any-safe.github.io", "AI": {"tldr": "This paper introduces a method to create adaptable safety filters for vision-based control tasks that change based on user-defined constraints during runtime, even for unknown scenarios.", "motivation": "Existing safety filters for vision-based tasks assume static, predefined constraints and lack adaptability across different scenarios.", "method": "Using latent-space similarity measures and conformal calibration, the approach trains safety filters in a simulated world model to condition on encoded safety constraints from user-defined images.", "result": "The proposed method enables adaptable, runtime safety filtering without compromising overall system performance, as demonstrated in both simulation and hardware experiments with a Franka manipulator.", "conclusion": "The method provides a flexible safety solution for vision-based systems, offering runtime adaptability to arbitrary user-defined safety constraints while maintaining performance."}}
{"id": "2509.19690", "pdf": "https://arxiv.org/pdf/2509.19690", "abs": "https://arxiv.org/abs/2509.19690", "authors": ["Ling Lo", "Kelvin C. K. Chan", "Wen-Huang Cheng", "Ming-Hsuan Yang"], "title": "From Prompt to Progression: Taming Video Diffusion Models for Seamless Attribute Transition", "categories": ["cs.CV"], "comment": "ICCV 2025", "summary": "Existing models often struggle with complex temporal changes, particularly\nwhen generating videos with gradual attribute transitions. The most common\nprompt interpolation approach for motion transitions often fails to handle\ngradual attribute transitions, where inconsistencies tend to become more\npronounced. In this work, we propose a simple yet effective method to extend\nexisting models for smooth and consistent attribute transitions, through\nintroducing frame-wise guidance during the denoising process. Our approach\nconstructs a data-specific transitional direction for each noisy latent,\nguiding the gradual shift from initial to final attributes frame by frame while\npreserving the motion dynamics of the video. Moreover, we present the\nControlled-Attribute-Transition Benchmark (CAT-Bench), which integrates both\nattribute and motion dynamics, to comprehensively evaluate the performance of\ndifferent models. We further propose two metrics to assess the accuracy and\nsmoothness of attribute transitions. Experimental results demonstrate that our\napproach performs favorably against existing baselines, achieving visual\nfidelity, maintaining alignment with text prompts, and delivering seamless\nattribute transitions. Code and CATBench are released:\nhttps://github.com/lynn-ling-lo/Prompt2Progression.", "AI": {"tldr": "The paper introduces an effective method for ensuring smooth and consistent attribute transitions in video generation.", "motivation": "Address limitations of existing models in handling complex temporal changes and gradual attribute transitions in video generation.", "method": "Proposes a frame-wise guidance approach during the denoising process, constructing transitional direction for gradual attribute shifts.", "result": "Experimental evaluation demonstrates superior performance compared to baselines in fidelity, prompt alignment, and smooth transitions.", "conclusion": "The approach is effective and outperforms existing methods, with its code and evaluation benchmark made publicly available."}}
{"id": "2509.19408", "pdf": "https://arxiv.org/pdf/2509.19408", "abs": "https://arxiv.org/abs/2509.19408", "authors": ["Obu-Amoah Ampomah", "Edmund Agyemang", "Kofi Acheampong", "Louis Agyekum"], "title": "Enhancing Credit Default Prediction Using Boruta Feature Selection and DBSCAN Algorithm with Different Resampling Techniques", "categories": ["cs.LG", "stat.AP"], "comment": "16 pages, 8 figures and 5 tables", "summary": "This study examines credit default prediction by comparing three techniques,\nnamely SMOTE, SMOTE-Tomek, and ADASYN, that are commonly used to address the\nclass imbalance problem in credit default situations. Recognizing that credit\ndefault datasets are typically skewed, with defaulters comprising a much\nsmaller proportion than non-defaulters, we began our analysis by evaluating\nmachine learning (ML) models on the imbalanced data without any resampling to\nestablish baseline performance. These baseline results provide a reference\npoint for understanding the impact of subsequent balancing methods. In addition\nto traditional classifiers such as Naive Bayes and K-Nearest Neighbors (KNN),\nour study also explores the suitability of advanced ensemble boosting\nalgorithms, including Extreme Gradient Boosting (XGBoost), AdaBoost, Gradient\nBoosting Machines (GBM), and Light GBM for credit default prediction using\nBoruta feature selection and DBSCAN-based outlier detection, both before and\nafter resampling. A real-world credit default data set sourced from the\nUniversity of Cleveland ML Repository was used to build ML classifiers, and\ntheir performances were tested. The criteria chosen to measure model\nperformance are the area under the receiver operating characteristic curve\n(ROC-AUC), area under the precision-recall curve (PR-AUC), G-mean, and\nF1-scores. The results from this empirical study indicate that the\nBoruta+DBSCAN+SMOTE-Tomek+GBM classifier outperformed the other ML models\n(F1-score: 82.56%, G-mean: 82.98%, ROC-AUC: 90.90%, PR-AUC: 91.85%) in a credit\ndefault context. The findings establish a foundation for future progress in\ncreating more resilient and adaptive credit default systems, which will be\nessential as credit-based transactions continue to rise worldwide.", "AI": {"tldr": "The study assessed machine learning methods for credit default prediction, emphasizing class imbalance solutions like SMOTE, SMOTE-Tomek, and ADASYN, with GBM performing best in combination with Boruta and DBSCAN processes.", "motivation": "Address class imbalance in credit default datasets and improve predictive performance for decision-making in financial systems.", "method": "Comparison of resampling techniques (SMOTE, SMOTE-Tomek, and ADASYN) combined with traditional classifiers and ensemble methods, alongside feature selection using Boruta and outlier detection using DBSCAN.", "result": "The Boruta+DBSCAN+SMOTE-Tomek+GBM classifier achieved strong metrics (F1-score: 82.56%, G-mean: 82.98%, ROC-AUC: 90.90%, PR-AUC: 91.85%), outperforming other models.", "conclusion": "The study highlights SMOTE-Tomek paired with GBM and advanced preprocessing techniques as effective solutions for addressing credit default prediction challenges, supporting future adaptive system development."}}
{"id": "2509.19839", "pdf": "https://arxiv.org/pdf/2509.19839", "abs": "https://arxiv.org/abs/2509.19839", "authors": ["Huizhen Shu", "Xuying Li", "Zhuo Li"], "title": "LatentGuard: Controllable Latent Steering for Robust Refusal of Attacks and Reliable Response Generation", "categories": ["cs.AI"], "comment": "9-page NeurIPS 2025 preprint including 3 figures and 1 table, with\n  additional appendix material. Prepared using the NeurIPS 2025 preprint\n  template and compiled with pdfLaTeX. All references are included via the\n  provided .bbl file. Figures are in PDF format. No external supplementary\n  files. All necessary style files and images are included", "summary": "Achieving robust safety alignment in large language models (LLMs) while\npreserving their utility remains a fundamental challenge. Existing approaches\noften struggle to balance comprehensive safety with fine-grained\ncontrollability at the representation level. We introduce LATENTGUARD, a novel\nthree-stage framework that combines behavioral alignment with supervised latent\nspace control for interpretable and precise safety steering. Our approach\nbegins by fine-tuning an LLM on rationalized datasets containing both\nreasoning-enhanced refusal responses to adversarial prompts and\nreasoning-enhanced normal responses to benign queries, establishing robust\nbehavioral priors across both safety-critical and utility-preserving scenarios.\nWe then train a structured variational autoencoder (VAE) on intermediate MLP\nactivations, supervised by multi-label annotations including attack types,\nattack methods, and benign indicators. This supervision enables the VAE to\nlearn disentangled latent representations that capture distinct adversarial\ncharacteristics while maintaining semantic interpretability. Through targeted\nmanipulation of learned latent dimensions, LATENTGUARD achieves selective\nrefusal behavior, effectively blocking harmful requests while preserving\nhelpfulness for legitimate use cases. Experiments on Qwen3-8B demonstrate\nsignificant improvements in both safety controllability and response\ninterpretability without compromising utility. Cross-architecture validation on\nMistral-7B confirms the generalizability of our latent steering approach,\nshowing consistent effectiveness across different model families. Our results\nsuggest that structured representation-level intervention offers a promising\npathway toward building safer yet practical LLM systems.", "AI": {"tldr": "LATENTGUARD is a three-stage framework designed to improve the safety alignment of LLMs while maintaining their utility. It uses reasoning-enhanced datasets, a structured VAE for latent space manipulation, and targeted behavioral control to block harmful queries and enable safe, practical usage.", "motivation": "The paper addresses the challenge of ensuring LLMs are robustly aligned for safety without losing their utility in legitimate scenarios, a balance that existing methods struggle to achieve.", "method": "LATENTGUARD fine-tunes LLMs on specialized datasets for safety-critical and benign scenarios, trains a structured VAE on intermediate activations to develop disentangled and interpretable latent spaces, and manipulates these spaces to selectively refuse harmful inputs while allowing legitimate ones.", "result": "Experiments on Qwen3-8B revealed improvements in safety, controllability, and response interpretability, with cross-architecture validation on Mistral-7B confirming the approach's generalizability across model families.", "conclusion": "Structured intervention at the latent representation level is a viable and promising strategy for creating LLM systems that are both safer and practical for real-world use cases."}}
{"id": "2509.20241", "pdf": "https://arxiv.org/pdf/2509.20241", "abs": "https://arxiv.org/abs/2509.20241", "authors": ["Felipe Oviedo", "Fiodar Kazhamiaka", "Esha Choukse", "Allen Kim", "Amy Luers", "Melanie Nakagawa", "Ricardo Bianchini", "Juan M. Lavista Ferres"], "title": "Energy Use of AI Inference: Efficiency Pathways and Test-Time Compute", "categories": ["cs.LG", "cs.DC"], "comment": "A preprint version with DOI is available at Zenodo:\n  https://doi.org/10.5281/zenodo.17188770", "summary": "As AI inference scales to billions of queries and emerging reasoning and\nagentic workflows increase token demand, reliable estimates of per-query energy\nuse are increasingly important for capacity planning, emissions accounting, and\nefficiency prioritization. Many public estimates are inconsistent and overstate\nenergy use, because they extrapolate from limited benchmarks and fail to\nreflect efficiency gains achievable at scale. In this perspective, we introduce\na bottom-up methodology to estimate the per-query energy of large-scale LLM\nsystems based on token throughput. For models running on an H100 node under\nrealistic workloads, GPU utilization and PUE constraints, we estimate a median\nenergy per query of 0.34 Wh (IQR: 0.18-0.67) for frontier-scale models (>200\nbillion parameters). These results are consistent with measurements using\nproduction-scale configurations and show that non-production estimates and\nassumptions can overstate energy use by 4-20x. Extending to test-time scaling\nscenarios with 15x more tokens per typical query, the median energy rises 13x\nto 4.32 Wh, indicating that targeting efficiency in this regime will deliver\nthe largest fleet-wide savings. We quantify achievable efficiency gains at the\nmodel, serving platform, and hardware levels, finding individual median\nreductions of 1.5-3.5x in energy per query, while combined advances can\nplausibly deliver 8-20x reductions. To illustrate the system-level impact, we\nestimate the baseline daily energy use of a deployment serving 1 billion\nqueries to be 0.8 GWh/day. If 10% are long queries, demand could grow to 1.8\nGWh/day. With targeted efficiency interventions, it falls to 0.9 GWh/day,\nsimilar to the energy footprint of web search at that scale. This echoes how\ndata centers historically tempered energy growth through efficiency gains\nduring the internet and cloud build-up.", "AI": {"tldr": "The paper introduces a method to estimate per-query energy use of large-scale language models (LLMs) and highlights efficiency strategies for reducing energy demands during AI inference workloads.", "motivation": "The growing need for billions of AI queries and token-intensive workflows creates a pressing requirement for accurate energy consumption estimates to enable capacity planning, emissions tracking, and efficiency optimization.", "method": "A bottom-up approach is proposed that calculates energy use per query based on token throughput, deploying realistic workloads on GPUs (H100 nodes) and accounting for practical conditions like PUE and GPU utilization.", "result": "Median per-query energy use for frontier models (>200 billion parameters) is estimated at 0.34 Wh, with 15x token scaling scenarios increasing it to 4.32 Wh. Efficiency improvements could lead to 8-20x reductions in energy use.", "conclusion": "Implementing efficiency measures at various levels (model, platform, hardware) can significantly decrease AI inference energy demands, aligning them with scaling patterns observed in data centers historically."}}
{"id": "2509.20353", "pdf": "https://arxiv.org/pdf/2509.20353", "abs": "https://arxiv.org/abs/2509.20353", "authors": ["Viktoria Stray", "Elias Goldmann Brandtz\u00e6g", "Viggo Tellefsen Wivestad", "Astri Barbala", "Nils Brede Moe"], "title": "Developer Productivity With and Without GitHub Copilot: A Longitudinal Mixed-Methods Case Study", "categories": ["cs.SE"], "comment": "Accepted for publication in the Proceedings of the 59th Hawaii\n  International Conference on System Sciences (HICSS 2026)", "summary": "This study investigates the real-world impact of the generative AI (GenAI)\ntool GitHub Copilot on developer activity and perceived productivity. We\nconducted a mixed-methods case study in NAV IT, a large public sector agile\norganization. We analyzed 26,317 unique non-merge commits from 703 of NAV IT's\nGitHub repositories over a two-year period, focusing on commit-based activity\nmetrics from 25 Copilot users and 14 non-users. The analysis was complemented\nby survey responses on their roles and perceived productivity, as well as 13\ninterviews. Our analysis of activity metrics revealed that individuals who used\nCopilot were consistently more active than non-users, even prior to Copilot's\nintroduction. We did not find any statistically significant changes in\ncommit-based activity for Copilot users after they adopted the tool, although\nminor increases were observed. This suggests a discrepancy between changes in\ncommit-based metrics and the subjective experience of productivity.", "AI": {"tldr": "This paper studies GitHub Copilot's impact on developer activity and productivity through data analysis, surveys, and interviews, finding higher activity levels for Copilot users but no significant post-adoption changes.", "motivation": "Understand the real-world impact of GitHub Copilot on developer activity and subjective productivity.", "method": "A mixed-methods case study, including analysis of 26,317 commits from 703 repositories, surveys, and interviews.", "result": "Copilot users were more active than non-users but showed no statistically significant post-adoption change in commit activity.", "conclusion": "The subjective perceived productivity increase does not align with measurable changes in commit-based activity after adopting Copilot."}}
{"id": "2509.19345", "pdf": "https://arxiv.org/pdf/2509.19345", "abs": "https://arxiv.org/abs/2509.19345", "authors": ["Renyu Li", "Antonio Jimeno Yepes", "Yao You", "Kamil Pluci\u0144ski", "Maximilian Operlejn", "Crag Wolfe"], "title": "SCORE: A Semantic Evaluation Framework for Generative Document Parsing", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Multi-modal generative document parsing systems challenge traditional\nevaluation: unlike deterministic OCR or layout models, they often produce\nsemantically correct yet structurally divergent outputs. Conventional\nmetrics-CER, WER, IoU, or TEDS-misclassify such diversity as error, penalizing\nvalid interpretations and obscuring system behavior.\n  We introduce SCORE (Structural and COntent Robust Evaluation), an\ninterpretation-agnostic framework that integrates (i) adjusted edit distance\nfor robust content fidelity, (ii) token-level diagnostics to distinguish\nhallucinations from omissions, (iii) table evaluation with spatial tolerance\nand semantic alignment, and (iv) hierarchy-aware consistency checks. Together,\nthese dimensions enable evaluation that embraces representational diversity\nwhile enforcing semantic rigor.\n  Across 1,114 pages spanning a holistic benchmark and a field dataset, SCORE\nconsistently revealed cross-dataset performance patterns missed by standard\nmetrics. In 2-5% of pages with ambiguous table structures, traditional metrics\npenalized systems by 12-25% on average, leading to distorted rankings. SCORE\ncorrected these cases, recovering equivalence between alternative but valid\ninterpretations. Moreover, by normalizing generative outputs into a\nformat-agnostic representation, SCORE reproduces traditional scores (e.g.,\ntable F1 up to 0.93) without requiring object-detection pipelines,\ndemonstrating that generative parsing alone suffices for comprehensive\nevaluation.\n  By exposing how interpretive diversity impacts evaluation outcomes and\nproviding multi-dimensional, interpretable diagnostics, SCORE establishes\nfoundational principles for semantically grounded, fair, and practical\nbenchmarking of modern document parsing systems.", "AI": {"tldr": "This paper introduces SCORE, a novel evaluation framework for multi-modal generative document parsing systems, addressing the limitations of traditional metrics. SCORE offers robust, interpretation-agnostic evaluations and reveals insights missed by standard metrics.", "motivation": "Traditional metrics fail to evaluate multi-modal generative document parsing systems properly, often penalizing alternative yet valid outputs.", "method": "The paper proposes SCORE, which uses (i) adjusted edit distance for content fidelity, (ii) token-level diagnostics, (iii) spatial tolerance and semantic alignment in table evaluation, and (iv) hierarchy-aware consistency checks.", "result": "SCORE revealed overlooked performance patterns, corrected unjust penalties by traditional metrics, and demonstrated that generative parsing suffices for comprehensive evaluation, achieving up to 0.93 reproductions of traditional table scores.", "conclusion": "SCORE establishes a fair and semantically rigorous evaluation framework, highlighting interpretive diversity and providing practical benchmarks for modern document parsing systems."}}
{"id": "2509.19571", "pdf": "https://arxiv.org/pdf/2509.19571", "abs": "https://arxiv.org/abs/2509.19571", "authors": ["Sacha Morin", "Kumaraditya Gupta", "Mahtab Sandhu", "Charlie Gauthier", "Francesco Argenziano", "Kirsty Ellis", "Liam Paull"], "title": "Agentic Scene Policies: Unifying Space, Semantics, and Affordances for Robot Action", "categories": ["cs.RO", "cs.CV"], "comment": "Project page:\n  https://montrealrobotics.ca/agentic-scene-policies.github.io/", "summary": "Executing open-ended natural language queries is a core problem in robotics.\nWhile recent advances in imitation learning and vision-language-actions models\n(VLAs) have enabled promising end-to-end policies, these models struggle when\nfaced with complex instructions and new scenes. An alternative is to design an\nexplicit scene representation as a queryable interface between the robot and\nthe world, using query results to guide downstream motion planning. In this\nwork, we present Agentic Scene Policies (ASP), an agentic framework that\nleverages the advanced semantic, spatial, and affordance-based querying\ncapabilities of modern scene representations to implement a capable\nlanguage-conditioned robot policy. ASP can execute open-vocabulary queries in a\nzero-shot manner by explicitly reasoning about object affordances in the case\nof more complex skills. Through extensive experiments, we compare ASP with VLAs\non tabletop manipulation problems and showcase how ASP can tackle room-level\nqueries through affordance-guided navigation, and a scaled-up scene\nrepresentation. (Project page:\nhttps://montrealrobotics.ca/agentic-scene-policies.github.io/)", "AI": {"tldr": "Agentic Scene Policies (ASP) is introduced to better interpret and execute open-ended natural language queries in robotics, outperforming existing Vision-Language-Action models (VLAs).", "motivation": "Enhancing robotic systems' ability to effectively interpret and execute complex natural language instructions, especially in novel settings where end-to-end VLAs show limitations.", "method": "ASP uses modern scene representations as a queryable interface, enabling advanced semantic, spatial, and affordance-based querying. It reasons explicitly about object affordances to guide language-conditioned robot policies.", "result": "ASP outperforms VLAs in tabletop manipulation tasks, handles room-scale queries using affordance-guided navigation, and scales up scene representations effectively.", "conclusion": "ASP demonstrates a scalable and effective alternative to VLAs for handling open-ended, language-conditioned tasks in robotic systems, leveraging explicit scene reasoning and affordance-based queries."}}
{"id": "2509.19691", "pdf": "https://arxiv.org/pdf/2509.19691", "abs": "https://arxiv.org/abs/2509.19691", "authors": ["Alexander Thorley", "Agis Chartsias", "Jordan Strom", "Roberto Lang", "Jeremy Slivnick", "Jamie O'Driscoll", "Rajan Sharma", "Dipak Kotecha", "Jinming Duan", "Alberto Gomez"], "title": "Anatomically Constrained Transformers for Cardiac Amyloidosis Classification", "categories": ["cs.CV"], "comment": "Published in MICCAI - ASMUS 2025", "summary": "Cardiac amyloidosis (CA) is a rare cardiomyopathy, with typical abnormalities\nin clinical measurements from echocardiograms such as reduced global\nlongitudinal strain of the myocardium. An alternative approach for detecting CA\nis via neural networks, using video classification models such as convolutional\nneural networks. These models process entire video clips, but provide no\nassurance that classification is based on clinically relevant features known to\nbe associated with CA. An alternative paradigm for disease classification is to\napply models to quantitative features such as strain, ensuring that the\nclassification relates to clinically relevant features. Drawing inspiration\nfrom this approach, we explicitly constrain a transformer model to the\nanatomical region where many known CA abnormalities occur -- the myocardium,\nwhich we embed as a set of deforming points and corresponding sampled image\npatches into input tokens. We show that our anatomical constraint can also be\napplied to the popular self-supervised learning masked autoencoder\npre-training, where we propose to mask and reconstruct only anatomical patches.\nWe show that by constraining both the transformer and pre-training task to the\nmyocardium where CA imaging features are localized, we achieve increased\nperformance on a CA classification task compared to full video transformers.\nOur model provides an explicit guarantee that the classification is focused on\nonly anatomical regions of the echo, and enables us to visualize transformer\nattention scores over the deforming myocardium.", "AI": {"tldr": "The paper introduces a myocardium-focused transformer model for accurate cardiac amyloidosis (CA) classification, outperforming traditional video-based approaches.", "motivation": "The study aims to address the lack of explicit assurance in current neural network-based CA video classification models that their decisions rely on clinically meaningful features.", "method": "The proposed approach includes constraining a transformer model to localized anatomical regions (the myocardium) and incorporating self-supervised learning through masked autoencoder pre-training for myocardium-specific image reconstruction.", "result": "The myocardium-constrained transformer model demonstrated improved performance in classifying CA compared to full video transformers, with the added interpretability of visualizing attention scores on relevant anatomical areas.", "conclusion": "Focusing on clinically relevant anatomical regions improves CA classification accuracy and interpretability, offering a promising paradigm for future disease classification models."}}
{"id": "2509.19417", "pdf": "https://arxiv.org/pdf/2509.19417", "abs": "https://arxiv.org/abs/2509.19417", "authors": ["Andreas Lebedev", "Abhinav Das", "Sven Pappert", "Stephan Schl\u00fcter"], "title": "Analyzing Uncertainty Quantification in Statistical and Deep Learning Models for Probabilistic Electricity Price Forecasting", "categories": ["cs.LG", "math.ST", "stat.TH", "I.6.4; I.6.5; I.6.6"], "comment": null, "summary": "Precise probabilistic forecasts are fundamental for energy risk management,\nand there is a wide range of both statistical and machine learning models for\nthis purpose. Inherent to these probabilistic models is some form of\nuncertainty quantification. However, most models do not capture the full extent\nof uncertainty, which arises not only from the data itself but also from model\nand distributional choices. In this study, we examine uncertainty\nquantification in state-of-the-art statistical and deep learning probabilistic\nforecasting models for electricity price forecasting in the German market. In\nparticular, we consider deep distributional neural networks (DDNNs) and augment\nthem with an ensemble approach, Monte Carlo (MC) dropout, and conformal\nprediction to account for model uncertainty. Additionally, we consider the\nLASSO-estimated autoregressive (LEAR) approach combined with quantile\nregression averaging (QRA), generalized autoregressive conditional\nheteroskedasticity (GARCH), and conformal prediction. Across a range of\nperformance metrics, we find that the LEAR-based models perform well in terms\nof probabilistic forecasting, irrespective of the uncertainty quantification\nmethod. Furthermore, we find that DDNNs benefit from incorporating both data\nand model uncertainty, improving both point and probabilistic forecasting.\nUncertainty itself appears to be best captured by the models using conformal\nprediction. Overall, our extensive study shows that all models under\nconsideration perform competitively. However, their relative performance\ndepends on the choice of metrics for point and probabilistic forecasting.", "AI": {"tldr": "The study explores uncertainty quantification in probabilistic forecasting for German electricity prices, comparing statistical methods with deep learning approaches. Results highlight differences in model performances based on metrics.", "motivation": "The paper addresses the need for precise probabilistic forecasts in energy risk management, aiming to enhance uncertainty quantification methods due to their current limitations in capturing the full scope of uncertainty.", "method": "Examines state-of-the-art statistical models (e.g., LEAR and GARCH) and deep learning models (e.g., DDNNs) augmented by techniques like Monte Carlo dropout, ensemble methods, and conformal prediction for electricity price forecasting.", "result": "LEAR-based models perform consistently well, DDNNs show improvements with added uncertainty modeling, and conformal prediction is effective for capturing uncertainty.", "conclusion": "All models perform competitively, but their efficiency is dependent on chosen forecasting metrics, highlighting the importance of the method and metric pairing in probabilistic forecasting."}}
{"id": "2509.19925", "pdf": "https://arxiv.org/pdf/2509.19925", "abs": "https://arxiv.org/abs/2509.19925", "authors": ["Ajeet Kumar Singh", "Rajsabi Surya", "Anurag Tripathi", "Santanu Choudhury", "Sudhir Bisane"], "title": "CON-QA: Privacy-Preserving QA using cloud LLMs in Contract Domain", "categories": ["cs.AI"], "comment": null, "summary": "As enterprises increasingly integrate cloud-based large language models\n(LLMs) such as ChatGPT and Gemini into their legal document workflows,\nprotecting sensitive contractual information - including Personally\nIdentifiable Information (PII) and commercially sensitive clauses - has emerged\nas a critical challenge. In this work, we propose CON-QA, a hybrid\nprivacy-preserving framework designed specifically for secure question\nanswering over enterprise contracts, effectively combining local and\ncloud-hosted LLMs. The CON-QA framework operates through three stages: (i)\nsemantic query decomposition and query-aware document chunk retrieval using a\nlocally deployed LLM analysis, (ii) anonymization of detected sensitive\nentities via a structured one-to-many mapping scheme, ensuring semantic\ncoherence while preventing cross-session entity inference attacks, and (iii)\nanonymized response generation by a cloud-based LLM, with accurate\nreconstruction of the original answer locally using a session-consistent\nmany-to-one reverse mapping. To rigorously evaluate CON-QA, we introduce\nCUAD-QA, a corpus of 85k question-answer pairs generated over 510 real-world\nCUAD contract documents, encompassing simple, complex, and summarization-style\nqueries. Empirical evaluations, complemented by detailed human assessments,\nconfirm that CON-QA effectively maintains both privacy and utility, preserves\nanswer quality, maintains fidelity to legal clause semantics, and significantly\nmitigates privacy risks, demonstrating its practical suitability for secure,\nenterprise-level contract documents.", "AI": {"tldr": "This paper proposes a privacy-preserving framework, CON-QA, for secure question answering over enterprise contracts by combining local and cloud language models (LLMs).", "motivation": "The motivation is to address the challenge of protecting sensitive information, including PII and sensitive contractual clauses, as enterprises increasingly use cloud-based LLMs in legal workflows.", "method": "The CON-QA framework operates in three stages: (i) semantic query decomposition and document chunk retrieval using a local LLM, (ii) anonymization through a one-to-many mapping to ensure privacy, and (iii) anonymized response generation by a cloud LLM, with local reconstruction via reverse mapping.", "result": "CON-QA was evaluated using the CUAD-QA dataset with 85k question-answer pairs from 510 legal documents. Results show that it ensures privacy, maintains utility and answer quality, and mitigates privacy risks, validated by human assessments.", "conclusion": "The study demonstrates that CON-QA is a practical, secure solution for performing privacy-preserving question answering on enterprise-level contract documents while ensuring high-fidelity responses."}}
{"id": "2509.19346", "pdf": "https://arxiv.org/pdf/2509.19346", "abs": "https://arxiv.org/abs/2509.19346", "authors": ["Maryam Mahdi Alhusseini", "Mohammad-Reza Feizi-Derakhshi"], "title": "Benchmarking ChatGPT and DeepSeek in April 2025: A Novel Dual Perspective Sentiment Analysis Using Lexicon-Based and Deep Learning Approaches", "categories": ["cs.CL"], "comment": "17 pages, 21 figures", "summary": "This study presents a novel dual-perspective approach to analyzing user\nreviews for ChatGPT and DeepSeek on the Google Play Store, integrating\nlexicon-based sentiment analysis (TextBlob) with deep learning classification\nmodels, including Convolutional Neural Networks (CNN) and Bidirectional Long\nShort Term Memory (Bi LSTM) Networks. Unlike prior research, which focuses on\neither lexicon-based strategies or predictive deep learning models in\nisolation, this study conducts an extensive investigation into user\nsatisfaction with Large Language Model (LLM) based applications. A Dataset of\n4,000 authentic user reviews was collected, which were carefully preprocessed\nand subjected to oversampling to achieve balanced classes. The balanced test\nset of 1,700 Reviews were used for model testing. Results from the experiments\nreveal that ChatGPT received significantly more positive sentiment than\nDeepSeek. Furthermore, deep learning based classification demonstrated superior\nperformance over lexicon analysis, with CNN outperforming Bi-LSTM by achieving\n96.41 percent accuracy and near perfect classification of negative reviews,\nalongside high F1-scores for neutral and positive sentiments. This research\nsets a new methodological standard for measuring sentiment in LLM-based\napplications and provides practical insights for developers and researchers\nseeking to improve user-centric AI system design.", "AI": {"tldr": "The study integrates sentiment analysis and deep learning to assess user satisfaction with ChatGPT and DeepSeek, showing superior sentiment for ChatGPT and better performance of deep learning techniques.", "motivation": "The paper aims to bridge the gap in sentiment analysis approaches by combining lexicon-based methods and deep learning models to analyze user reviews of LLM applications.", "method": "Analyzed 4,000 Google Play Store reviews of ChatGPT and DeepSeek using sentiment analysis (TextBlob) and deep learning models (CNN and Bi-LSTM). Balanced classes through oversampling for rigorous testing.", "result": "ChatGPT received more positive sentiment than DeepSeek. CNN achieved 96.41% accuracy, outperformed Bi-LSTM, and delivered strong F1-scores across sentiment categories.", "conclusion": "The research enhances sentiment measurement methodologies for LLM applications, offers insights to improve user-centric AI designs, and highlights the value of deep learning models in this domain."}}
{"id": "2509.19573", "pdf": "https://arxiv.org/pdf/2509.19573", "abs": "https://arxiv.org/abs/2509.19573", "authors": ["Zachary Olkin", "Kejun Li", "William D. Compton", "Aaron D. Ames"], "title": "Chasing Stability: Humanoid Running via Control Lyapunov Function Guided Reinforcement Learning", "categories": ["cs.RO"], "comment": "Submitted to ICRA 2026", "summary": "Achieving highly dynamic behaviors on humanoid robots, such as running,\nrequires controllers that are both robust and precise, and hence difficult to\ndesign. Classical control methods offer valuable insight into how such systems\ncan stabilize themselves, but synthesizing real-time controllers for nonlinear\nand hybrid dynamics remains challenging. Recently, reinforcement learning (RL)\nhas gained popularity for locomotion control due to its ability to handle these\ncomplex dynamics. In this work, we embed ideas from nonlinear control theory,\nspecifically control Lyapunov functions (CLFs), along with optimized dynamic\nreference trajectories into the reinforcement learning training process to\nshape the reward. This approach, CLF-RL, eliminates the need to handcraft and\ntune heuristic reward terms, while simultaneously encouraging certifiable\nstability and providing meaningful intermediate rewards to guide learning. By\ngrounding policy learning in dynamically feasible trajectories, we expand the\nrobot's dynamic capabilities and enable running that includes both flight and\nsingle support phases. The resulting policy operates reliably on a treadmill\nand in outdoor environments, demonstrating robustness to disturbances applied\nto the torso and feet. Moreover, it achieves accurate global reference tracking\nutilizing only on-board sensors, making a critical step toward integrating\nthese dynamic motions into a full autonomy stack.", "AI": {"tldr": "This paper proposes CLF-RL, a method that integrates control Lyapunov functions and optimized dynamic trajectories into RL training to achieve precise, robust humanoid running control systems.", "motivation": "To enable humanoid robots to achieve dynamic behaviors like running, which is challenging due to the need for both precision and robustness in nonlinear dynamics control.", "method": "The authors embed control Lyapunov functions and optimized dynamic reference trajectories into reinforcement learning training to shape reward functions without heuristic tuning, ensuring certifiable stability.", "result": "The CLF-RL trained policies achieve reliable running behavior both on a treadmill and outdoors, exhibit robustness to disturbances, and accurately track global references using only onboard sensors.", "conclusion": "The proposed method advances humanoid running control by grounding RL training in nonlinear control principles, paving the way for integration into fully autonomous systems."}}
{"id": "2509.19694", "pdf": "https://arxiv.org/pdf/2509.19694", "abs": "https://arxiv.org/abs/2509.19694", "authors": ["Woo-Jin Cho Kim", "Jorge Oliveira", "Arian Beqiri", "Alex Thorley", "Jordan Strom", "Jamie O'Driscoll", "Rajan Sharma", "Jeremy Slivnick", "Roberto Lang", "Alberto Gomez", "Agisilaos Chartsias"], "title": "Learning to Stop: Reinforcement Learning for Efficient Patient-Level Echocardiographic Classification", "categories": ["cs.CV"], "comment": "published in MICCAI-ASMUS 2025", "summary": "Guidelines for transthoracic echocardiographic examination recommend the\nacquisition of multiple video clips from different views of the heart,\nresulting in a large number of clips. Typically, automated methods, for\ninstance disease classifiers, either use one clip or average predictions from\nall clips. Relying on one clip ignores complementary information available from\nother clips, while using all clips is computationally expensive and may be\nprohibitive for clinical adoption.\n  To select the optimal subset of clips that maximize performance for a\nspecific task (image-based disease classification), we propose a method\noptimized through reinforcement learning. In our method, an agent learns to\neither keep processing view-specific clips to reduce the disease classification\nuncertainty, or stop processing if the achieved classification confidence is\nsufficient. Furthermore, we propose a learnable attention-based aggregation\nmethod as a flexible way of fusing information from multiple clips. The\nproposed method obtains an AUC of 0.91 on the task of detecting cardiac\namyloidosis using only 30% of all clips, exceeding the performance achieved\nfrom using all clips and from other benchmarks.", "AI": {"tldr": "The study proposes methods to optimally select and process subsets of transthoracic echocardiographic video clips for improved disease classification, achieving high performance while utilizing fewer clips.", "motivation": "The motivation is to address inefficiencies in current automated methods for analyzing transthoracic echocardiographic clips, which either neglect complementary information or are computationally expensive when processing all clips.", "method": "The paper introduces a reinforcement learning-based approach where an agent decides whether to continue processing or stop based on classification confidence, alongside an attention-based method for aggregating information.", "result": "The proposed approach achieves an AUC of 0.91 in detecting cardiac amyloidosis, utilizing only 30% of the full set of video clips, surpassing other methods.", "conclusion": "The method demonstrates that using optimized subsets of clips and advanced aggregation techniques can achieve better performance than processing all clips or using simple benchmarks, marking a step toward clinical practicality."}}
{"id": "2509.19419", "pdf": "https://arxiv.org/pdf/2509.19419", "abs": "https://arxiv.org/abs/2509.19419", "authors": ["Birk Torpmann-Hagen", "P\u00e5l Halvorsen", "Michael A. Riegler", "Dag Johansen"], "title": "Probabilistic Runtime Verification, Evaluation and Risk Assessment of Visual Deep Learning Systems", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Despite achieving excellent performance on benchmarks, deep neural networks\noften underperform in real-world deployment due to sensitivity to minor, often\nimperceptible shifts in input data, known as distributional shifts. These\nshifts are common in practical scenarios but are rarely accounted for during\nevaluation, leading to inflated performance metrics. To address this gap, we\npropose a novel methodology for the verification, evaluation, and risk\nassessment of deep learning systems. Our approach explicitly models the\nincidence of distributional shifts at runtime by estimating their probability\nfrom outputs of out-of-distribution detectors. We combine these estimates with\nconditional probabilities of network correctness, structuring them in a binary\ntree. By traversing this tree, we can compute credible and precise estimates of\nnetwork accuracy. We assess our approach on five different datasets, with which\nwe simulate deployment conditions characterized by differing frequencies of\ndistributional shift. Our approach consistently outperforms conventional\nevaluation, with accuracy estimation errors typically ranging between 0.01 and\n0.1. We further showcase the potential of our approach on a medical\nsegmentation benchmark, wherein we apply our methods towards risk assessment by\nassociating costs with tree nodes, informing cost-benefit analyses and\nvalue-judgments. Ultimately, our approach offers a robust framework for\nimproving the reliability and trustworthiness of deep learning systems,\nparticularly in safety-critical applications, by providing more accurate\nperformance estimates and actionable risk assessments.", "AI": {"tldr": "The paper introduces a methodology to improve the evaluation of deep neural networks by explicitly accounting for distributional shifts during deployment using out-of-distribution detectors and structured binary tree models.", "motivation": "Deep neural networks often underperform in real-world applications due to sensitivity to distributional shifts, which are not accounted for in typical evaluations, inflating performance metrics.", "method": "The paper proposes modeling distributional shifts at runtime through probabilities derived from out-of-distribution detectors and structuring conditional probabilities of network correctness in a binary tree to estimate network accuracy and assess risks.", "result": "The methodology was tested on five datasets simulating varying distributional shifts and consistently provided more accurate accuracy estimates with errors between 0.01 and 0.1, outperforming conventional evaluation methods.", "conclusion": "The approach enhances the reliability and trustworthiness of deep learning systems, especially in safety-critical applications, by enabling more credible performance estimates and comprehensive risk assessments."}}
{"id": "2509.19387", "pdf": "https://arxiv.org/pdf/2509.19387", "abs": "https://arxiv.org/abs/2509.19387", "authors": ["Antonio Quintero Rincon", "Nicolas Masino", "Veronica Marsico", "Hadj Batatia"], "title": "Hybrid Pipeline SWD Detection in Long-Term EEG Signals", "categories": ["eess.SP", "cs.LG", "stat.AP", "stat.ML"], "comment": "11 pages, 8 figures, 4 tables, SABI 2025 CLIC 2025", "summary": "Spike-and-wave discharges (SWDs) are the electroencephalographic hallmark of\nabsence epilepsy, yet their manual identification in multi-day recordings\nremains labour-intensive and error-prone. We present a lightweight hybrid\npipeline that couples analytical features with a shallow artificial neural\nnetwork (ANN) for accurate, patient-specific SWD detection in long-term,\nmonopolar EEG. A two-sided moving-average (MA) filter first suppresses the\nhigh-frequency components of normal background activity. The residual signal is\nthen summarised by the mean and the standard deviation of its normally\ndistributed samples, yielding a compact, two-dimensional feature vector for\nevery 20s window. These features are fed to a single-hidden-layer ANN trained\nvia back-propagation to classify each window as SWD or non-SWD. The method was\nevaluated on 780 channels sampled at 256 Hz from 12 patients, comprising 392\nannotated SWD events. It correctly detected 384 events (sensitivity: 98%) while\nachieving a specificity of 96.2 % and an overall accuracy of 97.2%. Because\nfeature extraction is analytic, and the classifier is small, the pipeline runs\nin real-time and requires no manual threshold tuning. These results indicate\nthat normal-distribution descriptors combined with a modest ANN provide an\neffective and computationally inexpensive solution for automated SWD screening\nin extended EEG recordings.", "AI": {"tldr": "Providing automated screening of spike-and-wave discharges, this paper introduces lightweight computational methods yielding robust results.", "motivation": "Manual identification of spike-and-wave discharges in long EEG recordings is time-consuming and prone to errors.", "method": "A hybrid pipeline combines analytical feature extraction (two-sided moving-average filter and statistical descriptors) with a single-hidden-layer ANN for classification.", "result": "The method achieved 98% sensitivity, 96.2% specificity, and 97.2% overall accuracy on multi-patient EEG data.", "conclusion": "Combining statistical features with a simple ANN provides an accurate, real-time, and patient-specific tool for SWD detection."}}
{"id": "2509.20021", "pdf": "https://arxiv.org/pdf/2509.20021", "abs": "https://arxiv.org/abs/2509.20021", "authors": ["Tongtong Feng", "Xin Wang", "Yu-Gang Jiang", "Wenwu Zhu"], "title": "Embodied AI: From LLMs to World Models", "categories": ["cs.AI", "cs.CL", "cs.RO"], "comment": "Accepted by IEEE CASM", "summary": "Embodied Artificial Intelligence (AI) is an intelligent system paradigm for\nachieving Artificial General Intelligence (AGI), serving as the cornerstone for\nvarious applications and driving the evolution from cyberspace to physical\nsystems. Recent breakthroughs in Large Language Models (LLMs) and World Models\n(WMs) have drawn significant attention for embodied AI. On the one hand, LLMs\nempower embodied AI via semantic reasoning and task decomposition, bringing\nhigh-level natural language instructions and low-level natural language actions\ninto embodied cognition. On the other hand, WMs empower embodied AI by building\ninternal representations and future predictions of the external world,\nfacilitating physical law-compliant embodied interactions. As such, this paper\ncomprehensively explores the literature in embodied AI from basics to advances,\ncovering both LLM driven and WM driven works. In particular, we first present\nthe history, key technologies, key components, and hardware systems of embodied\nAI, as well as discuss its development via looking from unimodal to multimodal\nangle. We then scrutinize the two burgeoning fields of embodied AI, i.e.,\nembodied AI with LLMs/multimodal LLMs (MLLMs) and embodied AI with WMs,\nmeticulously delineating their indispensable roles in end-to-end embodied\ncognition and physical laws-driven embodied interactions. Building upon the\nabove advances, we further share our insights on the necessity of the joint\nMLLM-WM driven embodied AI architecture, shedding light on its profound\nsignificance in enabling complex tasks within physical worlds. In addition, we\nexamine representative applications of embodied AI, demonstrating its wide\napplicability in real-world scenarios. Last but not least, we point out future\nresearch directions of embodied AI that deserve further investigation.", "AI": {"tldr": "This paper reviews and explores advancements in Embodied AI, emphasizing the roles of Large Language Models (LLMs) and World Models (WMs) in enhancing embodied cognition and interactions, while proposing a combined architecture (MLLM-WM) for complex physical tasks.", "motivation": "To achieve Artificial General Intelligence (AGI) and make embodied AI applicable in real-world scenarios by leveraging recent breakthroughs in LLMs and WMs.", "method": "Comprehensive literature review, analysis of key components, technologies, and fields (LLMs/multimodal LLMs and WMs), and proposing a new combined architecture.", "result": "Insights into the roles of LLMs and WMs in embodied AI, demonstrated through broad applications, and a proposed joint MLLM-WM architecture for complex physical-world tasks.", "conclusion": "The integration of LLMs and WMs is critical for advancing embodied AI, with promising applications and future research directions highlighted."}}
{"id": "2509.20068", "pdf": "https://arxiv.org/pdf/2509.20068", "abs": "https://arxiv.org/abs/2509.20068", "authors": ["Bilal Dalgic", "Betul Sen", "Muge Erel-Ozcevik"], "title": "A Novel Short-Term Anomaly Prediction for IIoT with Software Defined Twin Network", "categories": ["cs.NI", "cs.LG", "cs.SE"], "comment": "Accepted by 2025 IEEE Globecom Workshops-TwinNetApp", "summary": "Secure monitoring and dynamic control in an IIoT environment are major\nrequirements for current development goals. We believe that dynamic, secure\nmonitoring of the IIoT environment can be achieved through integration with the\nSoftware-Defined Network (SDN) and Digital Twin (DT) paradigms. The current\nliterature lacks implementation details for SDN-based DT and time-aware\nintelligent model training for short-term anomaly detection against IIoT\nthreats. Therefore, we have proposed a novel framework for short-term anomaly\ndetection that uses an SDN-based DT. Using a comprehensive dataset, time-aware\nlabeling of features, and a comprehensive evaluation of various machine\nlearning models, we propose a novel SD-TWIN-based anomaly detection algorithm.\nAccording to the performance of a new real-time SD-TWIN deployment, the GPU-\naccelerated LightGBM model is particularly effective, achieving a balance of\nhigh recall and strong classification performance.", "AI": {"tldr": "The paper presents a novel framework integrating Software-Defined Network (SDN) and Digital Twin (DT) paradigms in Industrial IoT (IIoT) environments for short-term anomaly detection.", "motivation": "Existing approaches lack implementation details for utilizing SDN-based DT and time-aware model training to efficiently predict IIoT threats.", "method": "A new SD-TWIN framework coupled with time-aware labeling and evaluation of machine learning models, specifically deploying a GPU-accelerated LightGBM model for real-time detection.", "result": "The deployment demonstrated that the GPU-accelerated LightGBM model performs effectively with high recall and strong classification capabilities for detecting anomalies.", "conclusion": "Integrating SDN-based DT paradigms along with intelligent time-aware techniques can achieve efficient and secure monitoring for IIoT systems, particularly in addressing short-term anomalies."}}
{"id": "2509.19347", "pdf": "https://arxiv.org/pdf/2509.19347", "abs": "https://arxiv.org/abs/2509.19347", "authors": ["Sara Todorovikj", "Lars-Peter Meyer", "Michael Martin"], "title": "Characterizing Knowledge Graph Tasks in LLM Benchmarks Using Cognitive Complexity Frameworks", "categories": ["cs.CL"], "comment": "peer reviewed publication at SEMANTiCS 2025 Poster Track", "summary": "Large Language Models (LLMs) are increasingly used for tasks involving\nKnowledge Graphs (KGs), whose evaluation typically focuses on accuracy and\noutput correctness. We propose a complementary task characterization approach\nusing three complexity frameworks from cognitive psychology. Applying this to\nthe LLM-KG-Bench framework, we highlight value distributions, identify\nunderrepresented demands and motivate richer interpretation and diversity for\nbenchmark evaluation tasks.", "AI": {"tldr": "The paper evaluates how large language models (LLMs) interact with knowledge graphs (KGs) by analyzing tasks through cognitive psychology frameworks, yielding insights for better benchmarking.", "motivation": "Examine the complexity of tasks LLMs undertake with KGs, moving beyond accuracy to understand cognitive requirements and diversify task evaluation benchmarks.", "method": "Incorporates three cognitive psychology complexity frameworks to evaluate tasks under the LLM-KG-Bench framework.", "result": "Exposed underrepresented cognitive demands in LLM-KG task evaluations, emphasizing flaws in value distributions and the need for more diverse benchmarking tasks.", "conclusion": "Cognitive psychology frameworks enrich the understanding of LLM-KG task complexities, proposing improvements for benchmarking diversity and interpretation."}}
{"id": "2509.19579", "pdf": "https://arxiv.org/pdf/2509.19579", "abs": "https://arxiv.org/abs/2509.19579", "authors": ["Chad R. Samuelson", "Abigail Austin", "Seth Knoop", "Blake Romrell", "Gabriel R. Slade", "Timothy W. McLain", "Joshua G. Mangelson"], "title": "Terra: Hierarchical Terrain-Aware 3D Scene Graph for Task-Agnostic Outdoor Mapping", "categories": ["cs.RO"], "comment": null, "summary": "Outdoor intelligent autonomous robotic operation relies on a sufficiently\nexpressive map of the environment. Classical geometric mapping methods retain\nessential structural environment information, but lack a semantic understanding\nand organization to allow high-level robotic reasoning. 3D scene graphs (3DSGs)\naddress this limitation by integrating geometric, topological, and semantic\nrelationships into a multi-level graph-based map. Outdoor autonomous operations\ncommonly rely on terrain information either due to task-dependence or the\ntraversability of the robotic platform. We propose a novel approach that\ncombines indoor 3DSG techniques with standard outdoor geometric mapping and\nterrain-aware reasoning, producing terrain-aware place nodes and hierarchically\norganized regions for outdoor environments. Our method generates a\ntask-agnostic metric-semantic sparse map and constructs a 3DSG from this map\nfor downstream planning tasks, all while remaining lightweight for autonomous\nrobotic operation. Our thorough evaluation demonstrates our 3DSG method\nperforms on par with state-of-the-art camera-based 3DSG methods in object\nretrieval and surpasses them in region classification while remaining memory\nefficient. We demonstrate its effectiveness in diverse robotic tasks of object\nretrieval and region monitoring in both simulation and real-world environments.", "AI": {"tldr": "This paper focuses on integrating terrain-aware scene graphs for outdoor robotic navigation by fusing geometric mapping with semantic understanding.", "motivation": "To address the limitations of traditional geometric mapping methods which lack semantic understanding necessary for high-level robotic reasoning.", "method": "The paper proposes generating terrain-aware place nodes and hierarchical organization for outdoor environments, combining indoor scene graph techniques with outdoor mapping methods.", "result": "Evaluation showed comparable performance to state-of-the-art methods in object retrieval and superior accuracy in region classification, while maintaining memory efficiency.", "conclusion": "The proposed approach effectively supports diverse robotic tasks by creating lightweight terrain-aware maps suitable for autonomous outdoor operations."}}
{"id": "2509.19711", "pdf": "https://arxiv.org/pdf/2509.19711", "abs": "https://arxiv.org/abs/2509.19711", "authors": ["Jiesi Hu", "Yanwu Yang", "Zhiyu Ye", "Chenfei Ye", "Hanyang Peng", "Jianfeng Cao", "Ting Ma"], "title": "Towards Robust In-Context Learning for Medical Image Segmentation via Data Synthesis", "categories": ["cs.CV"], "comment": null, "summary": "The rise of In-Context Learning (ICL) for universal medical image\nsegmentation has introduced an unprecedented demand for large-scale, diverse\ndatasets for training, exacerbating the long-standing problem of data scarcity.\nWhile data synthesis offers a promising solution, existing methods often fail\nto simultaneously achieve both high data diversity and a domain distribution\nsuitable for medical data. To bridge this gap, we propose \\textbf{SynthICL}, a\nnovel data synthesis framework built upon domain randomization. SynthICL\nensures realism by leveraging anatomical priors from real-world datasets,\ngenerates diverse anatomical structures to cover a broad data distribution, and\nexplicitly models inter-subject variations to create data cohorts suitable for\nICL. Extensive experiments on four held-out datasets validate our framework's\neffectiveness, showing that models trained with our data achieve performance\ngains of up to 63\\% in average Dice and substantially enhanced generalization\nto unseen anatomical domains. Our work helps mitigate the data bottleneck for\nICL-based segmentation, paving the way for robust models. Our code and the\ngenerated dataset are publicly available at\nhttps://github.com/jiesihu/Neuroverse3D.", "AI": {"tldr": "The paper introduces SynthICL, a data synthesis framework that leverages domain randomization to generate diverse, anatomically realistic datasets, addressing data scarcity in in-context learning for medical image segmentation.", "motivation": "The paper aims to address the shortage of large-scale and diverse datasets required for in-context learning in medical image segmentation, while maintaining domain realism and diversity.", "method": "The proposed SynthICL uses domain randomization and anatomical priors to ensure data realism, generates diverse anatomical structures to cover broad distributions, and models inter-subject variations to create suitable data cohorts for ICL.", "result": "SynthICL displayed performance gains of up to 63% in Dice score and improved generalization to unseen domains through experiments on four held-out datasets.", "conclusion": "The SynthICL framework offers a solution to the data bottleneck in ICL-based medical image segmentation and facilitates the development of robust, generalized models."}}
{"id": "2509.19465", "pdf": "https://arxiv.org/pdf/2509.19465", "abs": "https://arxiv.org/abs/2509.19465", "authors": ["Kin G. Olivares", "Malcolm Wolff", "Tatiana Konstantinova", "Shankar Ramasubramanian", "Andrew Gordon Wilson", "Andres Potapczynski", "Willa Potosnak", "Mengfei Cao", "Boris Oreshkin", "Dmitry Efimov"], "title": "A Realistic Evaluation of Cross-Frequency Transfer Learning and Foundation Forecasting Models", "categories": ["cs.LG", "cs.AI", "stat.AP"], "comment": "Thirty-Ninth Annual Conference on Neural Information Processing\n  Systems {NeurIPS 2025}. Recent Advances in Time Series Foundation Models Have\n  We Reached the 'BERT Moment'?", "summary": "Cross-frequency transfer learning (CFTL) has emerged as a popular framework\nfor curating large-scale time series datasets to pre-train foundation\nforecasting models (FFMs). Although CFTL has shown promise, current\nbenchmarking practices fall short of accurately assessing its performance. This\nshortcoming stems from many factors: an over-reliance on small-scale evaluation\ndatasets; inadequate treatment of sample size when computing summary\nstatistics; reporting of suboptimal statistical models; and failing to account\nfor non-negligible risks of overlap between pre-training and test datasets. To\naddress these limitations, we introduce a unified reimplementation of\nwidely-adopted neural forecasting networks, adapting them for the CFTL setup;\nwe pre-train only on proprietary and synthetic data, being careful to prevent\ntest leakage; and we evaluate on 15 large, diverse public forecast competition\ndatasets. Our empirical analysis reveals that statistical models' accuracy is\nfrequently underreported. Notably, we confirm that statistical models and their\nensembles consistently outperform existing FFMs by more than 8.2% in sCRPS, and\nby more than 20% MASE, across datasets. However, we also find that synthetic\ndataset pre-training does improve the accuracy of a FFM by 7% percent.", "AI": {"tldr": "The paper critiques current Cross-Frequency Transfer Learning (CFTL) benchmarking practices, proposes improvements, and evaluates them on 15 datasets, revealing that statistical models generally outperform existing foundation forecasting models (FFMs) by a significant margin.", "motivation": "Current benchmarking methods for CFTL are inadequate because they often rely on small datasets, poorly compute summary statistics, and fail to properly account for test dataset overlap or use optimal models.", "method": "The authors reimplement widely-used neural forecasting networks for the CFTL framework, ensuring no test leakage by pre-training on only proprietary and synthetic data. They evaluate these models on 15 large and diverse public datasets.", "result": "Statistical models and their ensembles outperform FFMs by over 8.2% in sCRPS and 20% in MASE across the datasets. However, pre-training on synthetic datasets improves FFM accuracy by approximately 7%.", "conclusion": "Improved benchmarking and pre-training practices highlight the underreported strengths of statistical models in forecasting, but also suggest that synthetic dataset pre-training offers moderate benefits for FFMs."}}
{"id": "2509.19490", "pdf": "https://arxiv.org/pdf/2509.19490", "abs": "https://arxiv.org/abs/2509.19490", "authors": ["Nathan Cheng", "Asher Spector", "Lucas Janson"], "title": "Chiseling: Powerful and Valid Subgroup Selection via Interactive Machine Learning", "categories": ["stat.ME", "stat.ML"], "comment": "26+7+97 pages (main text, references, appendix), 6+15 figures (main\n  text, appendix)", "summary": "In regression and causal inference, controlled subgroup selection aims to\nidentify, with inferential guarantees, a subgroup (defined as a subset of the\ncovariate space) on which the average response or treatment effect is above a\ngiven threshold. E.g., in a clinical trial, it may be of interest to find a\nsubgroup with a positive average treatment effect. However, existing methods\neither lack inferential guarantees, heavily restrict the search for the\nsubgroup, or sacrifice efficiency by naive data splitting. We propose a novel\nframework called chiseling that allows the analyst to interactively refine and\ntest a candidate subgroup by iteratively shrinking it. The sole restriction is\nthat the shrinkage direction only depends on the points outside the current\nsubgroup, but otherwise the analyst may leverage any prior information or\nmachine learning algorithm. Despite this flexibility, chiseling controls the\nprobability that the discovered subgroup is null (e.g., has a non-positive\naverage treatment effect) under minimal assumptions: for example, in randomized\nexperiments, this inferential validity guarantee holds under only bounded\nmoment conditions. When applied to a variety of simulated datasets and a real\nsurvey experiment, chiseling identifies substantially better subgroups than\nexisting methods with inferential guarantees.", "AI": {"tldr": "The paper introduces \"chiseling,\" a method for controlled subgroup selection in regression and causal inference that offers inferential guarantees while being interactive and efficient.", "motivation": "There is a need for methods to reliably identify subgroups with desired properties (e.g., positive average treatment effects) that go beyond the limitations of current approaches lacking inferential guarantees or sacrificing efficiency.", "method": "The proposed chiseling method iteratively refines and tests subgroups by shrinking them in directions based on points outside the subgroup, allowing analysts to utilize prior knowledge or machine learning, while ensuring inferential validity under minimal assumptions.", "result": "Chiseling demonstrates superior performance in identifying subgroups with desired properties compared to existing methods, showing effectiveness across simulated and real datasets.", "conclusion": "Chiseling offers a novel, flexible, and efficient approach to controlled subgroup selection with strong inferential guarantees, enabling better subgroup identification for regression and causal inference tasks."}}
{"id": "2509.20067", "pdf": "https://arxiv.org/pdf/2509.20067", "abs": "https://arxiv.org/abs/2509.20067", "authors": ["Wenliang Li", "Rui Yan", "Xu Zhang", "Li Chen", "Hongji Zhu", "Jing Zhao", "Junjun Li", "Mengru Li", "Wei Cao", "Zihang Jiang", "Wei Wei", "Kun Zhang", "Shaohua Kevin Zhou"], "title": "MACD: Multi-Agent Clinical Diagnosis with Self-Learned Knowledge for LLM", "categories": ["cs.AI"], "comment": null, "summary": "Large language models (LLMs) have demonstrated notable potential in medical\napplications, yet they face substantial challenges in handling complex\nreal-world clinical diagnoses using conventional prompting methods. Current\nprompt engineering and multi-agent approaches typically optimize isolated\ninferences, neglecting the accumulation of reusable clinical experience. To\naddress this, this study proposes a novel Multi-Agent Clinical Diagnosis (MACD)\nframework, which allows LLMs to self-learn clinical knowledge via a multi-agent\npipeline that summarizes, refines, and applies diagnostic insights. It mirrors\nhow physicians develop expertise through experience, enabling more focused and\naccurate diagnosis on key disease-specific cues. We further extend it to a\nMACD-human collaborative workflow, where multiple LLM-based diagnostician\nagents engage in iterative consultations, supported by an evaluator agent and\nhuman oversight for cases where agreement is not reached. Evaluated on 4,390\nreal-world patient cases across seven diseases using diverse open-source LLMs\n(Llama-3.1 8B/70B, DeepSeek-R1-Distill-Llama 70B), MACD significantly improves\nprimary diagnostic accuracy, outperforming established clinical guidelines with\ngains up to 22.3% (MACD). On the subset of the data, it achieves performance on\npar with or exceeding that of human physicians (up to 16% improvement over\nphysicians-only diagnosis). Additionally, on the MACD-human workflow, it\nachieves an 18.6% improvement compared to physicians-only diagnosis. Moreover,\nself-learned knowledge exhibits strong cross-model stability, transferability,\nand model-specific personalization, while the system can generate traceable\nrationales, enhancing explainability. Consequently, this work presents a\nscalable self-learning paradigm for LLM-assisted diagnosis, bridging the gap\nbetween the intrinsic knowledge of LLMs and real-world clinical practice.", "AI": {"tldr": "The paper introduces a Multi-Agent Clinical Diagnosis (MACD) framework that enhances the accuracy of LLMs in diagnosing diseases by mimicking how physicians gain expertise, surpassing clinical guidelines and even human physicians in some cases.", "motivation": "Current LLMs struggle with diagnosing complex clinical cases using traditional prompting methods, and existing solutions don't accumulate reusable clinical experience, necessitating a new approach.", "method": "The MACD framework leverages a multi-agent pipeline where LLMs summarize, refine, and apply clinical insights. It includes a collaborative workflow involving multiple LLM agents, an evaluator, and human oversight for unresolved cases.", "result": "The MACD framework improves primary diagnostic accuracy by up to 22.3% compared to established guidelines and achieves up to 18.6% accuracy improvement in collaboration with physicians. It outperforms or matches human physicians in specific datasets.", "conclusion": "The study introduces a scalable, self-learning paradigm for LLM-assisted diagnosis, improving accuracy, transferability, and explainability while addressing the gap between LLM knowledge and real-world clinical needs."}}
{"id": "2509.19349", "pdf": "https://arxiv.org/pdf/2509.19349", "abs": "https://arxiv.org/abs/2509.19349", "authors": ["Robert Tjarko Lange", "Yuki Imajuku", "Edoardo Cetin"], "title": "ShinkaEvolve: Towards Open-Ended And Sample-Efficient Program Evolution", "categories": ["cs.CL", "cs.LG"], "comment": "52 pages, 14 figures", "summary": "We introduce ShinkaEvolve: a new open-source framework leveraging large\nlanguage models (LLMs) to advance scientific discovery with state-of-the-art\nperformance and unprecedented efficiency. Recent advances in scaling inference\ntime compute of LLMs have enabled significant progress in generalized\nscientific discovery. These approaches rely on evolutionary agentic harnesses\nthat leverage LLMs as mutation operators to generate candidate solutions.\nHowever, current code evolution methods suffer from critical limitations: they\nare sample inefficient, requiring thousands of samples to identify effective\nsolutions, and remain closed-source, hindering broad adoption and extension.\nShinkaEvolve addresses these limitations, introducing three key innovations: a\nparent sampling technique balancing exploration and exploitation, code novelty\nrejection-sampling for efficient search space exploration, and a bandit-based\nLLM ensemble selection strategy. We evaluate ShinkaEvolve across diverse tasks,\ndemonstrating consistent improvements in sample efficiency and solution\nquality. ShinkaEvolve discovers a new state-of-the-art circle packing solution\nusing only 150 samples, designs high-performing agentic harnesses for AIME\nmathematical reasoning tasks, identifies improvements to ALE-Bench competitive\nprogramming solutions, and discovers novel mixture-of-expert load balancing\nloss functions that illuminate the space of optimization strategies. Our\nresults demonstrate that ShinkaEvolve achieves broad applicability with\nexceptional sample efficiency. By providing open-source accessibility and\ncost-efficiency, this work democratizes open-ended discovery across diverse\ncomputational problems.", "AI": {"tldr": "The paper introduces ShinkaEvolve, an open-source framework leveraging large language models to enhance the efficiency and quality of scientific discovery.", "motivation": "Existing solutions using LLMs for scientific discovery are inefficient in terms of sample usage and remain closed-source, which limits their accessibility and flexibility.", "method": "ShinkaEvolve integrates three innovations: a parent sampling method balancing exploration and exploitation, novelty-based rejection sampling to optimize the search space, and a bandit-based ensemble selection strategy for LLMs.", "result": "ShinkaEvolve achieved better sample efficiency and solution quality across various tasks, such as discovering state-of-the-art solutions in circle packing, improving mathematical reasoning, competitive programming solutions, and load-balancing functions.", "conclusion": "ShinkaEvolve demonstrates its effectiveness and versatility in solving diverse computational problems while offering open-source availability, paving the way for more accessible and efficient scientific discovery."}}
{"id": "2509.19597", "pdf": "https://arxiv.org/pdf/2509.19597", "abs": "https://arxiv.org/abs/2509.19597", "authors": ["Sander Tonkens", "Nikhil Uday Shinde", "Azra Begzadi\u0107", "Michael C. Yip", "Jorge Cort\u00e9s", "Sylvia L. Herbert"], "title": "From Space to Time: Enabling Adaptive Safety with Learned Value Functions via Disturbance Recasting", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": "The first three authors contributed equally. This work has been\n  accepted for publication at the Conference on Robot Learning", "summary": "The widespread deployment of autonomous systems in safety-critical\nenvironments such as urban air mobility hinges on ensuring reliable,\nperformant, and safe operation under varying environmental conditions. One such\napproach, value function-based safety filters, minimally modifies a nominal\ncontroller to ensure safety. Recent advances leverage offline learned value\nfunctions to scale these safety filters to high-dimensional systems. However,\nthese methods assume detailed priors on all possible sources of model mismatch,\nin the form of disturbances in the environment -- information that is rarely\navailable in real world settings. Even in well-mapped environments like urban\ncanyons or industrial sites, drones encounter complex, spatially-varying\ndisturbances arising from payload-drone interaction, turbulent airflow, and\nother environmental factors. We introduce SPACE2TIME, which enables safe and\nadaptive deployment of offline-learned safety filters under unknown,\nspatially-varying disturbances. The key idea is to reparameterize spatial\nvariations in disturbance as temporal variations, enabling the use of\nprecomputed value functions during online operation. We validate SPACE2TIME on\na quadcopter through extensive simulations and hardware experiments,\ndemonstrating significant improvement over baselines.", "AI": {"tldr": "The paper proposes SPACE2TIME, a method for safe and adaptive deployment of safety filters for autonomous systems under unknown spatially-varying disturbances.", "motivation": "To address challenges in ensuring the safe operation of autonomous systems under complex, uncertain environmental disturbances, especially for high-dimensional systems.", "method": "Reparameterizing spatial variations in disturbances as temporal variations, allowing the use of offline precomputed value functions during real-time operation.", "result": "Validated through quadcopter simulations and experiments, showing significant improvements in safety and performance compared to baseline approaches.", "conclusion": "SPACE2TIME provides an effective solution for enhancing the safety and adaptability of autonomous systems in challenging, disturbance-prone environments."}}
{"id": "2509.19713", "pdf": "https://arxiv.org/pdf/2509.19713", "abs": "https://arxiv.org/abs/2509.19713", "authors": ["Saimouli Katragadda", "Guoquan Huang"], "title": "VIMD: Monocular Visual-Inertial Motion and Depth Estimation", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Accurate and efficient dense metric depth estimation is crucial for 3D visual\nperception in robotics and XR. In this paper, we develop a monocular\nvisual-inertial motion and depth (VIMD) learning framework to estimate dense\nmetric depth by leveraging accurate and efficient MSCKF-based monocular\nvisual-inertial motion tracking. At the core the proposed VIMD is to exploit\nmulti-view information to iteratively refine per-pixel scale, instead of\nglobally fitting an invariant affine model as in the prior work. The VIMD\nframework is highly modular, making it compatible with a variety of existing\ndepth estimation backbones. We conduct extensive evaluations on the TartanAir\nand VOID datasets and demonstrate its zero-shot generalization capabilities on\nthe AR Table dataset. Our results show that VIMD achieves exceptional accuracy\nand robustness, even with extremely sparse points as few as 10-20 metric depth\npoints per image. This makes the proposed VIMD a practical solution for\ndeployment in resource constrained settings, while its robust performance and\nstrong generalization capabilities offer significant potential across a wide\nrange of scenarios.", "AI": {"tldr": "The paper presents VIMD, a monocular depth estimation framework that refines dense metric depth using multi-view information, achieving strong generalization and efficiency.", "motivation": "To improve dense metric depth estimation for robotics and XR by addressing limitations in prior methods like global invariant affine models.", "method": "Developed a monocular visual-inertial learning framework called VIMD, which leverages MSCKF-based motion tracking and iteratively refines depth using multi-view data.", "result": "VIMD demonstrates exceptional accuracy and robustness with sparse points and zero-shot generalization across datasets like TartanAir, VOID, and AR Table.", "conclusion": "VIMD offers an efficient, modular, and practical solution for dense metric depth estimation, suitable for resource-constrained settings and diverse scenarios."}}
{"id": "2509.19467", "pdf": "https://arxiv.org/pdf/2509.19467", "abs": "https://arxiv.org/abs/2509.19467", "authors": ["Javier Castro", "Benjamin Gess"], "title": "THINNs: Thermodynamically Informed Neural Networks", "categories": ["cs.LG", "cs.NA", "math.NA"], "comment": null, "summary": "Physics-Informed Neural Networks (PINNs) are a class of deep learning models\naiming to approximate solutions of PDEs by training neural networks to minimize\nthe residual of the equation. Focusing on non-equilibrium fluctuating systems,\nwe propose a physically informed choice of penalization that is consistent with\nthe underlying fluctuation structure, as characterized by a large deviations\nprinciple. This approach yields a novel formulation of PINNs in which the\npenalty term is chosen to penalize improbable deviations, rather than being\nselected heuristically. The resulting thermodynamically consistent extension of\nPINNs, termed THINNs, is subsequently analyzed by establishing analytical a\nposteriori estimates, and providing empirical comparisons to established\npenalization strategies.", "AI": {"tldr": "This paper introduces THINNs, a thermodynamically consistent extension of Physics-Informed Neural Networks (PINNs), improving their structure with penalty terms linked to large deviation principles.", "motivation": "Physics-Informed Neural Networks (PINNs) often use heuristic penalty terms, which lack thermodynamic consistency in capturing fluctuations in non-equilibrium systems.", "method": "The authors propose THINNs, which integrate a large deviation principle into PINNs to devise a physically informed penalization strategy for improbable deviations.", "result": "THINNs improve penalization efficiency and demonstrate advantages in capturing thermodynamic consistency through analytical and empirical evaluations.", "conclusion": "Integrating large deviation principles into PINNs enhances their ability to model non-equilibrium fluctuating systems, bridging gaps in current penalization strategies."}}
{"id": "2509.19633", "pdf": "https://arxiv.org/pdf/2509.19633", "abs": "https://arxiv.org/abs/2509.19633", "authors": ["Peng Lu", "Jerry Huang", "Qiuhao Zeng", "Xinyu Wang", "Boxing Wang", "Philippe Langlais", "Yufei Cui"], "title": "Mamba Modulation: On the Length Generalization of Mamba", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "Accepted to The Thirty-Ninth Annual Conference on Neural Information\n  Processing Systems (NeurIPS) 2025. First two authors contributed equally", "summary": "The quadratic complexity of the attention mechanism in Transformer models has\nmotivated the development of alternative architectures with sub-quadratic\nscaling, such as state-space models. Among these, Mamba has emerged as a\nleading architecture, achieving state-of-the-art results across a range of\nlanguage modeling tasks. However, Mamba's performance significantly\ndeteriorates when applied to contexts longer than those seen during\npre-training, revealing a sharp sensitivity to context length extension.\nThrough detailed analysis, we attribute this limitation to the\nout-of-distribution behaviour of its state-space dynamics, particularly within\nthe parameterization of the state transition matrix $\\mathbf{A}$. Unlike recent\nworks which attribute this sensitivity to the vanished accumulation of\ndiscretization time steps, $\\exp(-\\sum_{t=1}^N\\Delta_t)$, we establish a\nconnection between state convergence behavior as the input length approaches\ninfinity and the spectrum of the transition matrix $\\mathbf{A}$, offering a\nwell-founded explanation of its role in length extension. Next, to overcome\nthis challenge, we propose an approach that applies spectrum scaling to\npre-trained Mamba models to enable robust long-context generalization by\nselectively modulating the spectrum of $\\mathbf{A}$ matrices in each layer. We\nshow that this can significantly improve performance in settings where simply\nmodulating $\\Delta_t$ fails, validating our insights and providing avenues for\nbetter length generalization of state-space models with structured transition\nmatrices.", "AI": {"tldr": "The paper addresses Mamba's performance issues with longer contexts by attributing the problem to state-space dynamics and proposing spectrum scaling.", "motivation": "The quadratic complexity of attention in Transformer models drives the exploration of sub-quadratic alternatives. Mamba, a state-space model, shows promise but fails with longer contexts, underscoring its limitations.", "method": "The authors analyzed Mamba's state-space dynamics, particularly the state transition matrix spectrum, and proposed spectrum scaling to adjust the matrices, improving long-context generalization.", "result": "Applying spectrum scaling to pre-trained Mamba significantly enhanced its performance in handling longer contexts, outperforming traditional methods that solely modulate time step discretization.", "conclusion": "Spectrum scaling provides a novel and effective method for addressing sensitivity to input length in state-space models, opening paths for better long-context generalization."}}
{"id": "2509.20095", "pdf": "https://arxiv.org/pdf/2509.20095", "abs": "https://arxiv.org/abs/2509.20095", "authors": ["Aymeric Vellinger", "Nemanja Antonic", "Elio Tuci"], "title": "From Pheromones to Policies: Reinforcement Learning for Engineered Biological Swarms", "categories": ["cs.AI"], "comment": "Contribution to the 9th International Symposium on Swarm Behavior and\n  Bio-Inspired Robotics 2025", "summary": "Swarm intelligence emerges from decentralised interactions among simple\nagents, enabling collective problem-solving. This study establishes a\ntheoretical equivalence between pheromone-mediated aggregation in \\celeg\\ and\nreinforcement learning (RL), demonstrating how stigmergic signals function as\ndistributed reward mechanisms. We model engineered nematode swarms performing\nforaging tasks, showing that pheromone dynamics mathematically mirror\ncross-learning updates, a fundamental RL algorithm. Experimental validation\nwith data from literature confirms that our model accurately replicates\nempirical \\celeg\\ foraging patterns under static conditions. In dynamic\nenvironments, persistent pheromone trails create positive feedback loops that\nhinder adaptation by locking swarms into obsolete choices. Through\ncomputational experiments in multi-armed bandit scenarios, we reveal that\nintroducing a minority of exploratory agents insensitive to pheromones restores\ncollective plasticity, enabling rapid task switching. This behavioural\nheterogeneity balances exploration-exploitation trade-offs, implementing\nswarm-level extinction of outdated strategies. Our results demonstrate that\nstigmergic systems inherently encode distributed RL processes, where\nenvironmental signals act as external memory for collective credit assignment.\nBy bridging synthetic biology with swarm robotics, this work advances\nprogrammable living systems capable of resilient decision-making in volatile\nenvironments.", "AI": {"tldr": "The paper establishes a theoretical connection between pheromone-mediated aggregation in nematodes and reinforcement learning, using computational experiments to demonstrate adaptability improvements through agent heterogeneity.", "motivation": "To understand and enhance the decision-making processes in swarm systems by drawing parallels between biological stigmergic mechanisms and RL algorithms.", "method": "Mathematical modeling of nematode swarms, experimental validation using literature data, and simulations in multi-armed bandit scenarios with artificial agents.", "result": "The study shows accurate modeling of nematode foraging behaviors, highlights challenges with adaptability in dynamic environments due to pheromone trails, and reveals how exploratory agents improve swarm plasticity.", "conclusion": "Stigmergic mechanisms emulate distributed RL processes, enabling the development of programmable living systems that exhibit resilient decision-making in volatile contexts."}}
{"id": "2509.19352", "pdf": "https://arxiv.org/pdf/2509.19352", "abs": "https://arxiv.org/abs/2509.19352", "authors": ["Jiajun Chen", "Yangyang Wu", "Xiaoye Miao", "Mengying Zhu", "Meng Xi"], "title": "TriSPrompt: A Hierarchical Soft Prompt Model for Multimodal Rumor Detection with Incomplete Modalities", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The widespread presence of incomplete modalities in multimodal data poses a\nsignificant challenge to achieving accurate rumor detection. Existing\nmultimodal rumor detection methods primarily focus on learning joint modality\nrepresentations from \\emph{complete} multimodal training data, rendering them\nineffective in addressing the common occurrence of \\emph{missing modalities} in\nreal-world scenarios. In this paper, we propose a hierarchical soft prompt\nmodel \\textsf{TriSPrompt}, which integrates three types of prompts,\n\\textit{i.e.}, \\emph{modality-aware} (MA) prompt, \\emph{modality-missing} (MM)\nprompt, and \\emph{mutual-views} (MV) prompt, to effectively detect rumors in\nincomplete multimodal data. The MA prompt captures both heterogeneous\ninformation from specific modalities and homogeneous features from available\ndata, aiding in modality recovery. The MM prompt models missing states in\nincomplete data, enhancing the model's adaptability to missing information. The\nMV prompt learns relationships between subjective (\\textit{i.e.}, text and\nimage) and objective (\\textit{i.e.}, comments) perspectives, effectively\ndetecting rumors. Extensive experiments on three real-world benchmarks\ndemonstrate that \\textsf{TriSPrompt} achieves an accuracy gain of over 13\\%\ncompared to state-of-the-art methods. The codes and datasets are available at\nhttps: //anonymous.4open.science/r/code-3E88.", "AI": {"tldr": "The paper introduces TriSPrompt, a hierarchical soft prompt model addressing incomplete modalities in multimodal rumor detection, achieving over 13% accuracy improvement.", "motivation": "Incomplete modalities in multimodal data hinder effective rumor detection, with most existing methods requiring complete data, making them impractical for real-world applications.", "method": "The authors propose TriSPrompt, which utilizes three hierarchical prompts: MA prompt for modality information recovery, MM prompt for modeling missing data, and MV prompt for detecting relationships between perspectives.", "result": "Extensive experiments on three benchmarks show TriSPrompt achieves over 13% accuracy improvement compared to existing state-of-the-art methods.", "conclusion": "TriSPrompt effectively addresses incomplete modalities in multimodal rumor detection, demonstrating significant performance enhancements and adaptability."}}
{"id": "2509.19610", "pdf": "https://arxiv.org/pdf/2509.19610", "abs": "https://arxiv.org/abs/2509.19610", "authors": ["Qingxi Meng", "Emiliano Flores", "Carlos Quintero-Pe\u00f1a", "Peizhu Qian", "Zachary Kingston", "Shannan K. Hamlin", "Vaibhav Unhelkar", "Lydia E. Kavraki"], "title": "Look as You Leap: Planning Simultaneous Motion and Perception for High-DOF Robots", "categories": ["cs.RO"], "comment": "16 pages, 10 figures, under review", "summary": "In this work, we address the problem of planning robot motions for a\nhigh-degree-of-freedom (DoF) robot that effectively achieves a given perception\ntask while the robot and the perception target move in a dynamic environment.\nAchieving navigation and perception tasks simultaneously is challenging, as\nthese objectives often impose conflicting requirements. Existing methods that\ncompute motion under perception constraints fail to account for obstacles, are\ndesigned for low-DoF robots, or rely on simplified models of perception.\nFurthermore, in dynamic real-world environments, robots must replan and react\nquickly to changes and directly evaluating the quality of perception (e.g.,\nobject detection confidence) is often expensive or infeasible at runtime. This\nproblem is especially important in human-centered environments such as homes\nand hospitals, where effective perception is essential for safe and reliable\noperation. To address these challenges, we propose a GPU-parallelized\nperception-score-guided probabilistic roadmap planner with a neural surrogate\nmodel (PS-PRM). The planner explicitly incorporates the estimated quality of a\nperception task into motion planning for high-DoF robots. Our method uses a\nlearned model to approximate perception scores and leverages GPU parallelism to\nenable efficient online replanning in dynamic settings. We demonstrate that our\nplanner, evaluated on high-DoF robots, outperforms baseline methods in both\nstatic and dynamic environments in both simulation and real-robot experiments.", "AI": {"tldr": "This paper presents a GPU-parallelized probabilistic roadmap planner (PS-PRM) for high-degree-of-freedom robots to perform simultaneous navigation and perception tasks in dynamic environments.", "motivation": "The study is motivated by the need for robots to effectively achieve simultaneous navigation and perception tasks, especially in dynamic, human-centered environments where these objectives often conflict.", "method": "The proposed method combines a neural surrogate model to estimate perception quality and GPU-based parallelism within a probabilistic roadmap planner to handle dynamic environments and high DoFs efficiently.", "result": "Experiments show that the PS-PRM outperforms baseline methods in both static and dynamic environments, tested on high-DoF robots in both simulations and real-world scenarios.", "conclusion": "The PS-PRM method enhances the ability of robots to navigate and perceive effectively in dynamic environments, improving their operation in human-centered settings like homes and hospitals."}}
{"id": "2509.19719", "pdf": "https://arxiv.org/pdf/2509.19719", "abs": "https://arxiv.org/abs/2509.19719", "authors": ["Bo Yu", "Jianhua Yang", "Zetao Du", "Yan Huang", "Chenglong Li", "Liang Wang"], "title": "Frequency-domain Multi-modal Fusion for Language-guided Medical Image Segmentation", "categories": ["cs.CV"], "comment": "Accepted by MICCAI 2025", "summary": "Automatically segmenting infected areas in radiological images is essential\nfor diagnosing pulmonary infectious diseases. Recent studies have demonstrated\nthat the accuracy of the medical image segmentation can be improved by\nincorporating clinical text reports as semantic guidance. However, the complex\nmorphological changes of lesions and the inherent semantic gap between\nvision-language modalities prevent existing methods from effectively enhancing\nthe representation of visual features and eliminating semantically irrelevant\ninformation, ultimately resulting in suboptimal segmentation performance. To\naddress these problems, we propose a Frequency-domain Multi-modal Interaction\nmodel (FMISeg) for language-guided medical image segmentation. FMISeg is a late\nfusion model that establishes interaction between linguistic features and\nfrequency-domain visual features in the decoder. Specifically, to enhance the\nvisual representation, our method introduces a Frequency-domain Feature\nBidirectional Interaction (FFBI) module to effectively fuse frequency-domain\nfeatures. Furthermore, a Language-guided Frequency-domain Feature Interaction\n(LFFI) module is incorporated within the decoder to suppress semantically\nirrelevant visual features under the guidance of linguistic information.\nExperiments on QaTa-COV19 and MosMedData+ demonstrated that our method\noutperforms the state-of-the-art methods qualitatively and quantitatively.", "AI": {"tldr": "The paper introduces FMISeg, a model for language-guided medical image segmentation using a frequency-domain approach to enhance segmentation performance.", "motivation": "The need to improve medical image segmentation accuracy by integrating clinical text reports, while addressing challenges posed by complex lesions and vision-language modality gaps.", "method": "A late fusion model, FMISeg, uses a Frequency-domain Feature Bidirectional Interaction module and a Language-guided Frequency-domain Feature Interaction module for better visual and linguistic feature integration.", "result": "Experiments on QaTa-COV19 and MosMedData+ datasets showed FMISeg surpassing state-of-the-art segmentation methods in both qualitative and quantitative evaluations.", "conclusion": "FMISeg effectively leverages linguistic texts in the frequency domain to refine segmentation methods, resolving semantic inconsistencies and improving disease diagnosis accuracy."}}
{"id": "2509.19471", "pdf": "https://arxiv.org/pdf/2509.19471", "abs": "https://arxiv.org/abs/2509.19471", "authors": ["Hunjae Lee", "Corey Clark"], "title": "Transformer Modeling for Both Scalability and Performance in Multivariate Time Series", "categories": ["cs.LG"], "comment": null, "summary": "Variable count is among the main scalability bottlenecks for transformer\nmodeling in multivariate time series (MTS) data. On top of this, a growing\nconsensus in the field points to indiscriminate inter-variable mixing as a\npotential source of noise-accumulation and performance degradation. This is\nlikely exacerbated by sparsity of informative signals characteristic of many\nMTS systems coupled with representational misalignment stemming from\nindiscriminate information mixing between (heterogeneous) variables. While\nscalability and performance are often seen as competing interests in\ntransformer design, we show that both can be improved simultaneously in MTS by\nstrategically constraining the representational capacity of inter-variable\nmixing. Our proposed method, transformer with Delegate Token Attention\n(DELTAformer), constrains inter-variable modeling through what we call delegate\ntokens which are then used to perform full, unconstrained, inter-temporal\nmodeling. Delegate tokens act as an implicit regularizer that forces the model\nto be highly selective about what inter-variable information is allowed to\npropagate through the network. Our results show that DELTAformer scales\nlinearly with variable-count while actually outperforming standard\ntransformers, achieving state-of-the-art performance across benchmarks and\nbaselines. In addition, DELTAformer can focus on relevant signals better than\nstandard transformers in noisy MTS environments and overall exhibit superior\nnoise-resilience. Overall, results across various experiments confirm that by\naligning our model design to leverage domain-specific challenges in MTS to our\nadvantage, DELTAformer can simultaneously achieve linear scaling while actually\nimproving its performance against standard, quadratic transformers.", "AI": {"tldr": "DELTAformer is a new transformer design for multivariate time series (MTS) data that improves scalability and performance through constrained inter-variable mixing.", "motivation": "Scalability and indiscriminate inter-variable mixing are challenges in applying transformers to MTS data, leading to noise accumulation, poor alignment between variables, and performance degradation.", "method": "The DELTAformer introduces 'delegate tokens' to limit inter-variable information mixing and conducts full inter-temporal modeling, acting as a selective and implicit regularizer for relevant information propagation.", "result": "DELTAformer achieves linear scalability with variable count, surpasses standard transformers in benchmarks, and exhibits robust noise resilience in noisy MTS environments.", "conclusion": "By leveraging domain-specific challenges, DELTAformer simultaneously achieves linear scaling and improved performance, showcasing state-of-the-art results in MTS tasks."}}
{"id": "2509.19710", "pdf": "https://arxiv.org/pdf/2509.19710", "abs": "https://arxiv.org/abs/2509.19710", "authors": ["Somjit Roy", "Pritam Dey", "Debdeep Pati", "Bani K. Mallick"], "title": "Hierarchical Bayesian Operator-induced Symbolic Regression Trees for Structural Learning of Scientific Expressions", "categories": ["stat.ME", "math.ST", "stat.ML", "stat.TH"], "comment": "61 pages, 10 figures, 15 tables", "summary": "The advent of Scientific Machine Learning has heralded a transformative era\nin scientific discovery, driving progress across diverse domains. Central to\nthis progress is uncovering scientific laws from experimental data through\nsymbolic regression. However, existing approaches are dominated by heuristic\nalgorithms or data-hungry black-box methods, which often demand low-noise\nsettings and lack principled uncertainty quantification. Motivated by\ninterpretable Statistical Artificial Intelligence, we develop a hierarchical\nBayesian framework for symbolic regression that represents scientific laws as\nensembles of tree-structured symbolic expressions endowed with a regularized\ntree prior. This coherent probabilistic formulation enables full posterior\ninference via an efficient Markov chain Monte Carlo algorithm, yielding a\nbalance between predictive accuracy and structural parsimony. To guide symbolic\nmodel selection, we develop a marginal posterior-based criterion adhering to\nthe Occam's window principle and further quantify structural fidelity to ground\ntruth through a tailored expression-distance metric. On the theoretical front,\nwe establish near-minimax rate of Bayesian posterior concentration, providing\nthe first rigorous guarantee in context of symbolic regression. Empirical\nevaluation demonstrates robust performance of our proposed methodology against\nstate-of-the-art competing modules on a simulated example, a suite of canonical\nFeynman equations, and single-atom catalysis dataset.", "AI": {"tldr": "The paper introduces a Bayesian framework for symbolic regression to uncover scientific laws, distinguishing itself with principled uncertainty quantification and robust performance compared to existing methods.", "motivation": "Existing symbolic regression methods rely on heuristic algorithms or black-box approaches that often lack interpretability, require low-noise data, and fail to provide principled uncertainty quantification.", "method": "The authors develop a hierarchical Bayesian framework for symbolic regression with ensembles of tree-structured expressions, employing Markov chain Monte Carlo for posterior inference and a marginal posterior-based selection adhering to Occam\u2019s window principle.", "result": "The framework demonstrates strong performance in recovering scientific laws, outperforming state-of-the-art methods on simulated data, canonical Feynman equations, and single-atom catalysis datasets.", "conclusion": "The Bayesian approach to symbolic regression ensures predictive accuracy, structural parsimony, and theoretical rigor, making it robust and interpretable for uncovering scientific laws."}}
{"id": "2509.20102", "pdf": "https://arxiv.org/pdf/2509.20102", "abs": "https://arxiv.org/abs/2509.20102", "authors": ["Tong Nie", "Yuewen Mei", "Yihong Tang", "Junlin He", "Jie Sun", "Haotian Shi", "Wei Ma", "Jian Sun"], "title": "Steerable Adversarial Scenario Generation through Test-Time Preference Alignment", "categories": ["cs.AI"], "comment": null, "summary": "Adversarial scenario generation is a cost-effective approach for safety\nassessment of autonomous driving systems. However, existing methods are often\nconstrained to a single, fixed trade-off between competing objectives such as\nadversariality and realism. This yields behavior-specific models that cannot be\nsteered at inference time, lacking the efficiency and flexibility to generate\ntailored scenarios for diverse training and testing requirements. In view of\nthis, we reframe the task of adversarial scenario generation as a\nmulti-objective preference alignment problem and introduce a new framework\nnamed \\textbf{S}teerable \\textbf{A}dversarial scenario \\textbf{GE}nerator\n(SAGE). SAGE enables fine-grained test-time control over the trade-off between\nadversariality and realism without any retraining. We first propose\nhierarchical group-based preference optimization, a data-efficient offline\nalignment method that learns to balance competing objectives by decoupling hard\nfeasibility constraints from soft preferences. Instead of training a fixed\nmodel, SAGE fine-tunes two experts on opposing preferences and constructs a\ncontinuous spectrum of policies at inference time by linearly interpolating\ntheir weights. We provide theoretical justification for this framework through\nthe lens of linear mode connectivity. Extensive experiments demonstrate that\nSAGE not only generates scenarios with a superior balance of adversariality and\nrealism but also enables more effective closed-loop training of driving\npolicies. Project page: https://tongnie.github.io/SAGE/.", "AI": {"tldr": "SAGE introduces a steerable framework for generating adversarial scenarios with the ability to balance adversariality and realism during inference without retraining.", "motivation": "Existing adversarial scenario generation methods lack flexibility and are constrained by fixed trade-offs, making them inefficient for diverse training and testing needs in autonomous driving systems.", "method": "The paper proposes the SAGE framework, which uses hierarchical group-based preference optimization to balance competing objectives. It fine-tunes two expert models on opposing preferences and interpolates between their weights to steer the scenario generation dynamically at inference.", "result": "SAGE achieves superior adversariality-realism balance and enhances the closed-loop training of autonomous driving policies, verified through extensive experiments.", "conclusion": "The framework adds significant flexibility and efficiency to adversarial scenario generation by enabling fine-grained control without retraining, offering theoretical grounding and practical benefits for driving policy evaluation and training."}}
{"id": "2509.19354", "pdf": "https://arxiv.org/pdf/2509.19354", "abs": "https://arxiv.org/abs/2509.19354", "authors": ["Ahmed El Fekih Zguir", "Ferda Ofli", "Muhammad Imran"], "title": "RoadMind: Towards a Geospatial AI Expert for Disaster Response", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have shown impressive performance across a range\nof natural language tasks, but remain limited in their ability to reason about\ngeospatial data, particularly road networks, distances, and directions. This\ngap poses challenges in disaster scenarios, where spatial understanding is\ncritical for tasks such as evacuation planning and resource allocation. In this\nwork, we present RoadMind, a self-supervised framework that enhances the\ngeospatial reasoning capabilities of LLMs using structured data from\nOpenStreetMap (OSM). Our automated pipeline extracts road infrastructure data\nfor a given city and converts it into multiple supervision formats tailored to\nkey spatial tasks. We pretrain and fine-tune LLMs on these representations\nusing QLoRA adapters and 4-bit quantized models. We evaluate our approach on\nthree disaster-prone cities with varying global representation, Los Angeles,\nChristchurch, and Manila, across tasks such as road segment identification,\nnearest road retrieval, and distance/direction estimation. Our results show\nthat models trained via RoadMind significantly outperform strong baselines,\nincluding state-of-the-art LLMs equipped with advanced prompt engineering. This\ndemonstrates the potential of structured geospatial data to enhance language\nmodels with robust spatial reasoning, enabling more effective offline AI\nsystems for disaster response.", "AI": {"tldr": "RoadMind improves the geospatial reasoning abilities of Large Language Models (LLMs) using OpenStreetMap data for tasks like evacuation planning and resource allocation during disaster scenarios.", "motivation": "To address LLMs' limitations in reasoning about geospatial data, which restrict their utility in critical disaster scenarios requiring spatial understanding.", "method": "Developed RoadMind, a self-supervised framework that extracts and converts OpenStreetMap road data into formats for training LLMs using QLoRA adapters and 4-bit quantized models.", "result": "RoadMind-trained models outperformed state-of-the-art LLMs in geospatial tasks like road segment identification and distance estimation across disaster-prone cities.", "conclusion": "Structured geospatial data can be effectively used to enhance LLMs' spatial reasoning capabilities, enabling improved offline AI applications for disaster response."}}
{"id": "2509.19626", "pdf": "https://arxiv.org/pdf/2509.19626", "abs": "https://arxiv.org/abs/2509.19626", "authors": ["Ryan Punamiya", "Dhruv Patel", "Patcharapong Aphiwetsa", "Pranav Kuppili", "Lawrence Y. Zhu", "Simar Kareer", "Judy Hoffman", "Danfei Xu"], "title": "EgoBridge: Domain Adaptation for Generalizable Imitation from Egocentric Human Data", "categories": ["cs.RO", "cs.CV", "cs.LG"], "comment": "Accepted at 39th Conference on Neural Information Processing Systems\n  (NeurIPS 2025) and Oral at Conference on Robot Learning (CoRL 2025)", "summary": "Egocentric human experience data presents a vast resource for scaling up\nend-to-end imitation learning for robotic manipulation. However, significant\ndomain gaps in visual appearance, sensor modalities, and kinematics between\nhuman and robot impede knowledge transfer. This paper presents EgoBridge, a\nunified co-training framework that explicitly aligns the policy latent spaces\nbetween human and robot data using domain adaptation. Through a measure of\ndiscrepancy on the joint policy latent features and actions based on Optimal\nTransport (OT), we learn observation representations that not only align\nbetween the human and robot domain but also preserve the action-relevant\ninformation critical for policy learning. EgoBridge achieves a significant\nabsolute policy success rate improvement by 44% over human-augmented\ncross-embodiment baselines in three real-world single-arm and bimanual\nmanipulation tasks. EgoBridge also generalizes to new objects, scenes, and\ntasks seen only in human data, where baselines fail entirely. Videos and\nadditional information can be found at https://ego-bridge.github.io", "AI": {"tldr": "This paper proposes EgoBridge, a framework to bridge the gap in policy latent spaces for imitation learning between humans and robots by employing Optimal Transport for feature alignment.", "motivation": "To overcome domain differences in appearance, modalities, and kinematics between humans and robots hindering effective imitation learning.", "method": "The EgoBridge framework uses Optimal Transport to align latent spaces and actions between humans and robots while preserving task-relevant features for policy learning.", "result": "EgoBridge improves the policy success rate by 44% over baselines in various manipulation tasks and generalizes to new tasks with human data.", "conclusion": "EgoBridge effectively bridges the domain gap, enabling better imitation learning and generalization from human experience data to robotic manipulation."}}
{"id": "2509.19726", "pdf": "https://arxiv.org/pdf/2509.19726", "abs": "https://arxiv.org/abs/2509.19726", "authors": ["Yufei Han", "Bowen Tie", "Heng Guo", "Youwei Lyu", "Si Li", "Boxin Shi", "Yunpeng Jia", "Zhanyu Ma"], "title": "PolGS: Polarimetric Gaussian Splatting for Fast Reflective Surface Reconstruction", "categories": ["cs.CV"], "comment": "Accepted by ICCV 2025", "summary": "Efficient shape reconstruction for surfaces with complex reflectance\nproperties is crucial for real-time virtual reality. While 3D Gaussian\nSplatting (3DGS)-based methods offer fast novel view rendering by leveraging\ntheir explicit surface representation, their reconstruction quality lags behind\nthat of implicit neural representations, particularly in the case of recovering\nsurfaces with complex reflective reflectance. To address these problems, we\npropose PolGS, a Polarimetric Gaussian Splatting model allowing fast reflective\nsurface reconstruction in 10 minutes. By integrating polarimetric constraints\ninto the 3DGS framework, PolGS effectively separates specular and diffuse\ncomponents, enhancing reconstruction quality for challenging reflective\nmaterials. Experimental results on the synthetic and real-world dataset\nvalidate the effectiveness of our method.", "AI": {"tldr": "This paper introduces PolGS, a polarimetric-enhanced 3D Gaussian Splatting model for rapid reconstruction of complex reflective surfaces.", "motivation": "The study aims to address the limitations in reconstruction quality of reflective surfaces encountered by current 3D Gaussian Splatting methods compared to implicit neural representations.", "method": "The authors integrate polarimetric constraints into the 3D Gaussian Splatting framework, enabling accurate separation of specular and diffuse components during reconstruction.", "result": "PolGS successfully reconstructs reflective surfaces in just 10 minutes, demonstrating enhanced reconstruction quality validated through experiments on synthetic and real-world datasets.", "conclusion": "PolGS advances the field of real-time virtual reality by combining speed and quality in reconstructing surfaces with complex reflectance properties."}}
{"id": "2509.19504", "pdf": "https://arxiv.org/pdf/2509.19504", "abs": "https://arxiv.org/abs/2509.19504", "authors": ["Trung Nguyen Thanh", "Huyen Giang Thi Thu", "Tai Le Quy", "Ha-Bang Ban"], "title": "Constraint-Reduced MILP with Local Outlier Factor Modeling for Plausible Counterfactual Explanations in Credit Approval", "categories": ["cs.LG"], "comment": "Accepted to NICE-TEAS ASIA 2025 conference", "summary": "Counterfactual explanation (CE) is a widely used post-hoc method that\nprovides individuals with actionable changes to alter an unfavorable prediction\nfrom a machine learning model. Plausible CE methods improve realism by\nconsidering data distribution characteristics, but their optimization models\nintroduce a large number of constraints, leading to high computational cost. In\nthis work, we revisit the DACE framework and propose a refined Mixed-Integer\nLinear Programming (MILP) formulation that significantly reduces the number of\nconstraints in the local outlier factor (LOF) objective component. We also\napply the method to a linear SVM classifier with standard scaler. The\nexperimental results show that our approach achieves faster solving times while\nmaintaining explanation quality. These results demonstrate the promise of more\nefficient LOF modeling in counterfactual explanation and data science\napplications.", "AI": {"tldr": "This paper refines the DACE framework for counterfactual explanations by employing an efficient Mixed-Integer Linear Programming approach to reduce constraints, resulting in faster computation while maintaining high explanation quality.", "motivation": "Counterfactual explanations are helpful for actionable insights in ML models, but current plausible methods suffer from high computational costs due to numerous constraints. The paper addresses this inefficiency.", "method": "The authors refine the optimization of the local outlier factor (LOF) component in the DACE framework by introducing a Mixed-Integer Linear Programming formulation and applying it to a linear SVM classifier.", "result": "The experimental results demonstrate significantly faster solving times without compromising the quality of counterfactual explanations.", "conclusion": "Efficient LOF modeling can be achieved using the proposed formulation, supporting advancements in counterfactual explanation techniques with reduced computational demands."}}
{"id": "2509.19830", "pdf": "https://arxiv.org/pdf/2509.19830", "abs": "https://arxiv.org/abs/2509.19830", "authors": ["Wei Liu", "Eleni Chatzi", "Zhilu Lai"], "title": "On the Rate of Convergence of Kolmogorov-Arnold Network Regression Estimators", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "Kolmogorov-Arnold Networks (KANs) offer a structured and interpretable\nframework for multivariate function approximation by composing univariate\ntransformations through additive or multiplicative aggregation. This paper\nestablishes theoretical convergence guarantees for KANs when the univariate\ncomponents are represented by B-splines. We prove that both additive and hybrid\nadditive-multiplicative KANs attain the minimax-optimal convergence rate\n$O(n^{-2r/(2r+1)})$ for functions in Sobolev spaces of smoothness $r$. We\nfurther derive guidelines for selecting the optimal number of knots in the\nB-splines. The theory is supported by simulation studies that confirm the\npredicted convergence rates. These results provide a theoretical foundation for\nusing KANs in nonparametric regression and highlight their potential as a\nstructured alternative to existing methods.", "AI": {"tldr": "This paper investigates the theoretical convergence of Kolmogorov-Arnold Networks (KANs) for approximating multivariate functions using B-splines, achieving optimal convergence rates and verifying them through simulations.", "motivation": "To establish theoretical guarantees for KANs and demonstrate their effectiveness as structured alternatives in function approximation.", "method": "The paper proves convergence rates using mathematical analysis within Sobolev spaces and provides guidelines for B-spline parameter selection, along with simulation studies to validate theoretical findings.", "result": "KANs using B-splines are shown to achieve minimax-optimal convergence rates of $O(n^{-2r/(2r+1)})$, confirmed experimentally through simulations.", "conclusion": "KANs are theoretically validated for use in nonparametric regression, offering a structured framework for function approximation with practical guideline derivations for implementation."}}
{"id": "2509.20105", "pdf": "https://arxiv.org/pdf/2509.20105", "abs": "https://arxiv.org/abs/2509.20105", "authors": ["Venkat Margapuri", "Garik Kazanjian", "Naren Kosaraju"], "title": "PEPS: Quantum-Inspired Reinforcement Learning for Coherent Reasoning Traces in LLMs", "categories": ["cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) often struggle with maintaining coherent\nmulti-step reasoning traces, particularly in tasks that require a structured\nlogical flow. This work introduces a quantum-inspired approach to address the\nchallenge by incorporating a fidelity-based reward derived from Projected\nEntangled Pair States (PEPS) into Proximal Policy Optimization. Unlike prior\napproaches that use direct supervision or contrastive objectives, the proposed\nmethod guides learning through structural consistency, offering a novel\napproach to enforce global coherence in generated reasoning traces. The\nproposed framework is evaluated using multiple coherence-determining metrics on\ndiverse datasets such as GSM8K, StrategyQA, and EntailmentBank spanning\narithmetic, intuitive, and entailment-based reasoning. Results show that the\nproposed quantum-inspired approach offers significant improvements over\nsupervised, contrastive, and pretrained baseline approaches, highlighting the\neffectiveness of quantum-inspired fidelity as a foundation to improve reasoning\ntrace coherence in LLMs.", "AI": {"tldr": "This paper introduces a quantum-inspired approach using fidelity-based rewards to improve multi-step reasoning coherence in Large Language Models (LLMs).", "motivation": "LLMs often fail to generate coherent multi-step reasoning, especially in tasks requiring structured logical flow, necessitating innovative solutions.", "method": "The paper proposes incorporating fidelity-based rewards from Projected Entangled Pair States (PEPS) into Proximal Policy Optimization to enforce global coherence in reasoning traces.", "result": "The approach demonstrated significant improvements in reasoning trace coherence across various datasets compared to supervised, contrastive, and pretrained baseline methods.", "conclusion": "Quantum-inspired fidelity frameworks are effective for enhancing reasoning trace coherence in LLMs, marking promising progress in structured logical task performance."}}
{"id": "2509.19358", "pdf": "https://arxiv.org/pdf/2509.19358", "abs": "https://arxiv.org/abs/2509.19358", "authors": ["Chimaobi Okite", "Naihao Deng", "Kiran Bodipati", "Huaidian Hou", "Joyce Chai", "Rada Mihalcea"], "title": "Benchmarking and Improving LLM Robustness for Personalized Generation", "categories": ["cs.CL", "cs.AI"], "comment": "First draft. First camera-ready version", "summary": "Recent years have witnessed a growing interest in personalizing the responses\nof large language models (LLMs). While existing evaluations primarily focus on\nwhether a response aligns with a user's preferences, we argue that factuality\nis an equally important yet often overlooked dimension. In the context of\npersonalization, we define a model as robust if its responses are both\nfactually accurate and align with the user preferences. To assess this, we\nintroduce PERG, a scalable framework for evaluating robustness in LLMs, along\nwith a new dataset, PERGData. We evaluate fourteen models from five different\nmodel families using different prompting methods. Our findings show that\ncurrent LLMs struggle with robust personalization: even the strongest models\n(GPT-4.1, LLaMA3-70B) fail to maintain correctness in 5% of previously\nsuccessful cases without personalization, while smaller models (e.g., 7B-scale)\ncan fail more than 20% of the time. Further analysis reveals that robustness is\nsignificantly affected by the nature of the query and the type of user\npreference. To mitigate these failures, we propose Pref-Aligner, a two-stage\napproach that improves robustness by an average of 25% across models. Our work\nhighlights critical gaps in current evaluation practices and introduces tools\nand metrics to support more reliable, user-aligned LLM deployments.", "AI": {"tldr": "The paper identifies the challenge of ensuring both factual accuracy and user preference alignment in personalized LLM responses, introducing PERG as an evaluation framework. Current LLMs face robustness issues, which the proposed Pref-Aligner method aims to address.", "motivation": "The research is motivated by the gap in ensuring robust responses from LLMs that are both factually accurate and aligned with user preferences, which is crucial for reliable personalization.", "method": "The study introduces PERG, a scalable framework for evaluating robustness of LLMs, and a new dataset called PERGData. It evaluates 14 models from 5 model families, using different prompting methods, and proposes Pref-Aligner, a two-stage approach to enhance robustness.", "result": "Findings highlight that current models struggle with robust personalization. Even advanced models like GPT-4.1 fail in 5% of cases, while smaller ones fail over 20% of the time. Pref-Aligner improves robustness by 25% on average.", "conclusion": "The paper identifies significant robustness gaps in personalized LLMs, especially concerning factuality and user preference alignment, and presents solutions to support more reliable LLM deployments."}}
{"id": "2509.19636", "pdf": "https://arxiv.org/pdf/2509.19636", "abs": "https://arxiv.org/abs/2509.19636", "authors": ["Mahmoud Ali", "Hassan Jardali", "Youwei Yu", "Durgakant Pushp", "Lantao Liu"], "title": "Minimalistic Autonomous Stack for High-Speed Time-Trial Racing", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": "The data associated with this paper is available at\n  https://doi.org/10.5281/zenodo.17187680", "summary": "Autonomous racing has seen significant advancements, driven by competitions\nsuch as the Indy Autonomous Challenge (IAC) and the Abu Dhabi Autonomous Racing\nLeague (A2RL). However, developing an autonomous racing stack for a full-scale\ncar is often constrained by limited access to dedicated test tracks,\nrestricting opportunities for real-world validation. While previous work\ntypically requires extended development cycles and significant track time, this\npaper introduces a minimalistic autonomous racing stack for high-speed\ntime-trial racing that emphasizes rapid deployment and efficient system\nintegration with minimal on-track testing. The proposed stack was validated on\nreal speedways, achieving a top speed of 206 km/h within just 11 hours'\npractice run on the track with 325 km in total. Additionally, we present the\nsystem performance analysis, including tracking accuracy, vehicle dynamics, and\nsafety considerations, offering insights for teams seeking to rapidly develop\nand deploy an autonomous racing stack with limited track access.", "AI": {"tldr": "The paper introduces a streamlined autonomous racing stack designed for high-speed time trials, requiring minimal track time for real-world validation. It achieved impressive results, including a top speed of 206 km/h within just 11 hours of practice.", "motivation": "Autonomous racing faces challenges due to limited access to real-world testing tracks, which slows down validation and development of racing stacks.", "method": "The researchers developed a minimalistic autonomous racing stack focused on rapid deployment and efficient integration, minimizing the reliance on extensive on-track testing.", "result": "The stack was tested on speedways, achieving a top speed of 206 km/h after 325 km of practice driving within only 11 hours. Performance metrics, including tracking accuracy, vehicle dynamics, and safety, were analyzed.", "conclusion": "The study demonstrates the feasibility of quickly developing and deploying an efficient autonomous racing system even with restricted access to testing facilities, offering valuable insights for similar projects."}}
{"id": "2509.19731", "pdf": "https://arxiv.org/pdf/2509.19731", "abs": "https://arxiv.org/abs/2509.19731", "authors": ["Hyunseung Kim", "Chiho Choi", "Srikanth Malla", "Sai Prahladh Padmanabhan", "Saurabh Bagchi", "Joon Hee Choi"], "title": "CAMILA: Context-Aware Masking for Image Editing with Language Alignment", "categories": ["cs.CV"], "comment": null, "summary": "Text-guided image editing has been allowing users to transform and synthesize\nimages through natural language instructions, offering considerable\nflexibility. However, most existing image editing models naively attempt to\nfollow all user instructions, even if those instructions are inherently\ninfeasible or contradictory, often resulting in nonsensical output. To address\nthese challenges, we propose a context-aware method for image editing named as\nCAMILA (Context-Aware Masking for Image Editing with Language Alignment).\nCAMILA is designed to validate the contextual coherence between instructions\nand the image, ensuring that only relevant edits are applied to the designated\nregions while ignoring non-executable instructions. For comprehensive\nevaluation of this new method, we constructed datasets for both single- and\nmulti-instruction image editing, incorporating the presence of infeasible\nrequests. Our method achieves better performance and higher semantic alignment\nthan state-of-the-art models, demonstrating its effectiveness in handling\ncomplex instruction challenges while preserving image integrity.", "AI": {"tldr": "CAMILA, a context-aware image editing method, validates the coherence between user instructions and images to avoid executing infeasible edits.", "motivation": "Existing image editing models struggle with infeasible or contradictory instructions, leading to nonsensical results.", "method": "CAMILA uses context-aware masking and language alignment to apply only logically valid edits to images.", "result": "CAMILA surpasses state-of-the-art models in performance, semantic alignment, and handling complex instructions.", "conclusion": "CAMILA successfully ensures relevant edits are applied while discarding non-executable ones, preserving core image integrity."}}
{"id": "2509.19506", "pdf": "https://arxiv.org/pdf/2509.19506", "abs": "https://arxiv.org/abs/2509.19506", "authors": ["Mohan Guo", "Cong Liu", "Patrick Forr\u00e9"], "title": "Frame-based Equivariant Diffusion Models for 3D Molecular Generation", "categories": ["cs.LG"], "comment": null, "summary": "Recent methods for molecular generation face a trade-off: they either enforce\nstrict equivariance with costly architectures or relax it to gain scalability\nand flexibility. We propose a frame-based diffusion paradigm that achieves\ndeterministic E(3)-equivariance while decoupling symmetry handling from the\nbackbone. Building on this paradigm, we investigate three variants: Global\nFrame Diffusion (GFD), which assigns a shared molecular frame; Local Frame\nDiffusion (LFD), which constructs node-specific frames and benefits from\nadditional alignment constraints; and Invariant Frame Diffusion (IFD), which\nrelies on pre-canonicalized invariant representations. To enhance expressivity,\nwe further utilize EdgeDiT, a Diffusion Transformer with edge-aware attention.\n  On the QM9 dataset, GFD with EdgeDiT achieves state-of-the-art performance,\nwith a test NLL of -137.97 at standard scale and -141.85 at double scale,\nalongside atom stability of 98.98%, and molecular stability of 90.51%. These\nresults surpass all equivariant baselines while maintaining high validity and\nuniqueness and nearly 2x faster sampling compared to EDM. Altogether, our study\nestablishes frame-based diffusion as a scalable, flexible, and physically\ngrounded paradigm for molecular generation, highlighting the critical role of\nglobal structure preservation.", "AI": {"tldr": "The paper introduces a scalable and physically grounded paradigm for molecular generation, combining deterministic E(3)-equivariance with flexible computational methods.", "motivation": "To overcome the trade-off faced by molecular generation methods between strict equivariance and computational scalability.", "method": "The paper proposes a frame-based diffusion paradigm, exploring three variants (GFD, LFD, IFD) paired with EdgeDiT for enhanced expressivity.", "result": "On the QM9 dataset, GFD with EdgeDiT achieves state-of-the-art performance in test metrics, surpassing all equivariant baselines with faster sampling.", "conclusion": "Frame-based diffusion successfully balances scalability, flexibility, and symmetry enforcement, solidifying its role in molecular generation with superior results."}}
{"id": "2509.19900", "pdf": "https://arxiv.org/pdf/2509.19900", "abs": "https://arxiv.org/abs/2509.19900", "authors": ["Xinjue Wang", "Esa Ollila", "Sergiy A. Vorobyov", "Ammar Mian"], "title": "Generalized Nonnegative Structured Kruskal Tensor Regression", "categories": ["eess.SP", "stat.AP", "stat.ML"], "comment": null, "summary": "This paper introduces Generalized Nonnegative Structured Kruskal Tensor\nRegression (NS-KTR), a novel tensor regression framework that enhances\ninterpretability and performance through mode-specific hybrid regularization\nand nonnegativity constraints. Our approach accommodates both linear and\nlogistic regression formulations for diverse response variables while\naddressing the structural heterogeneity inherent in multidimensional tensor\ndata. We integrate fused LASSO, total variation, and ridge regularizers, each\ntailored to specific tensor modes, and develop an efficient alternating\ndirection method of multipliers (ADMM) based algorithm for parameter\nestimation. Comprehensive experiments on synthetic signals and real\nhyperspectral datasets demonstrate that NS-KTR consistently outperforms\nconventional tensor regression methods. The framework's ability to preserve\ndistinct structural characteristics across tensor dimensions while ensuring\nphysical interpretability makes it especially suitable for applications in\nsignal processing and hyperspectral image analysis.", "AI": {"tldr": "The paper presents a new tensor regression framework, NS-KTR, with hybrid regularization and nonnegativity constraints, achieving better performance and interpretability, especially for hyperspectral image analysis.", "motivation": "The authors aim to improve interpretability and performance in tensor regression models, particularly for applications involving structurally heterogeneous multidimensional data.", "method": "The paper introduces NS-KTR, incorporating mode-specific hybrid regularizers (fused LASSO, total variation, ridge) and nonnegativity constraints, using an ADMM-based algorithm for parameter estimation.", "result": "Experiments show that NS-KTR consistently surpasses conventional tensor regression methods in analyzing synthetic and real hyperspectral datasets.", "conclusion": "NS-KTR effectively preserves tensor structure while maintaining physical interpretability, making it ideal for signal processing and hyperspectral image use cases."}}
{"id": "2509.20138", "pdf": "https://arxiv.org/pdf/2509.20138", "abs": "https://arxiv.org/abs/2509.20138", "authors": ["Wieger Wesselink", "Kees Huizing", "Huub van de Wetering"], "title": "Formal Verification of Minimax Algorithms", "categories": ["cs.AI", "68Q60, 68T20"], "comment": "12 pages", "summary": "Using the Dafny verification system, we formally verify a range of minimax\nsearch algorithms, including variations with alpha-beta pruning and\ntransposition tables. For depth-limited search with transposition tables, we\nintroduce a witness-based correctness criterion and apply it to two\nrepresentative algorithms. All verification artifacts, including proofs and\nPython implementations, are publicly available.", "AI": {"tldr": "The paper utilizes the Dafny verification system to formally verify minimax search algorithms, incorporating alpha-beta pruning and transposition tables.", "motivation": "The research aims to improve the reliability of minimax search algorithms by formalizing their verification and ensuring correctness through a well-defined theoretical framework.", "method": "The authors use the Dafny verification system to rigorously verify a variety of minimax search algorithms. They introduce a witness-based correctness criterion for depth-limited search with transposition tables.", "result": "The verification was successfully applied to a variety of minimax search algorithms, with all artifacts such as proofs and implementations being made publicly available.", "conclusion": "This work advances the reliability of minimax search algorithm implementations by providing a formal verification process and making the results publicly accessible."}}
{"id": "2509.19360", "pdf": "https://arxiv.org/pdf/2509.19360", "abs": "https://arxiv.org/abs/2509.19360", "authors": ["Jiawei Lian", "Jianhong Pan", "Lefan Wang", "Yi Wang", "Shaohui Mei", "Lap-Pui Chau"], "title": "Semantic Representation Attack against Aligned Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) increasingly employ alignment techniques to\nprevent harmful outputs. Despite these safeguards, attackers can circumvent\nthem by crafting prompts that induce LLMs to generate harmful content.\n  Current methods typically target exact affirmative responses, such as ``Sure,\nhere is...'', suffering from limited convergence, unnatural prompts, and high\ncomputational costs.\n  We introduce Semantic Representation Attack, a novel paradigm that\nfundamentally reconceptualizes adversarial objectives against aligned LLMs.\n  Rather than targeting exact textual patterns, our approach exploits the\nsemantic representation space comprising diverse responses with equivalent\nharmful meanings.\n  This innovation resolves the inherent trade-off between attack efficacy and\nprompt naturalness that plagues existing methods.\n  The Semantic Representation Heuristic Search algorithm is proposed to\nefficiently generate semantically coherent and concise adversarial prompts by\nmaintaining interpretability during incremental expansion.\n  We establish rigorous theoretical guarantees for semantic convergence and\ndemonstrate that our method achieves unprecedented attack success rates\n(89.41\\% averaged across 18 LLMs, including 100\\% on 11 models) while\nmaintaining stealthiness and efficiency.\n  Comprehensive experimental results confirm the overall superiority of our\nSemantic Representation Attack.\n  The code will be publicly available.", "AI": {"tldr": "The paper proposes a novel attack method to circumvent alignment techniques in large language models (LLMs), achieving high success rates while being more stealthy and efficient compared to existing methods.", "motivation": "Existing adversarial techniques for aligned LLMs suffer from limitations like unnatural prompts, low success rates, and high computational costs. This paper aims to improve attack efficacy and prompt naturalness when eliciting harmful content from LLMs.", "method": "The authors propose the Semantic Representation Attack paradigm, leveraging the semantic representation space to generate attacks. They introduce the Semantic Representation Heuristic Search algorithm for concise and interpretable prompt generation.", "result": "The method achieves an average attack success rate of 89.41% across 18 LLMs, with perfect success on 11 models. It also maintains stealthiness and computational efficiency.", "conclusion": "Semantic Representation Attack significantly improves adversarial success rates, naturalness, and efficiency, outperforming existing methods. The approach offers theoretical guarantees for semantic convergence and has comprehensive empirical support."}}
{"id": "2509.19658", "pdf": "https://arxiv.org/pdf/2509.19658", "abs": "https://arxiv.org/abs/2509.19658", "authors": ["Youngju Yoo", "Jiaheng Hu", "Yifeng Zhu", "Bo Liu", "Qiang Liu", "Roberto Mart\u00edn-Mart\u00edn", "Peter Stone"], "title": "RoboSSM: Scalable In-context Imitation Learning via State-Space Models", "categories": ["cs.RO", "cs.AI"], "comment": "8 pages, 11 figures", "summary": "In-context imitation learning (ICIL) enables robots to learn tasks from\nprompts consisting of just a handful of demonstrations. By eliminating the need\nfor parameter updates at deployment time, this paradigm supports few-shot\nadaptation to novel tasks. However, recent ICIL methods rely on Transformers,\nwhich have computational limitations and tend to underperform when handling\nlonger prompts than those seen during training. In this work, we introduce\nRoboSSM, a scalable recipe for in-context imitation learning based on\nstate-space models (SSM). Specifically, RoboSSM replaces Transformers with\nLonghorn -- a state-of-the-art SSM that provides linear-time inference and\nstrong extrapolation capabilities, making it well-suited for long-context\nprompts. We evaluate our approach on the LIBERO benchmark and compare it\nagainst strong Transformer-based ICIL baselines. Experiments show that RoboSSM\nextrapolates effectively to varying numbers of in-context demonstrations,\nyields high performance on unseen tasks, and remains robust in long-horizon\nscenarios. These results highlight the potential of SSMs as an efficient and\nscalable backbone for ICIL. Our code is available at\nhttps://github.com/youngjuY/RoboSSM.", "AI": {"tldr": "The paper introduces RoboSSM, a scalable approach for in-context imitation learning using state-space models. It addresses the shortcomings of Transformer-based models in handling long prompts efficiently.", "motivation": "To overcome the computational limitations and performance issues of Transformer-based in-context imitation learning methods when dealing with long input prompts.", "method": "The authors replace Transformers with the Longhorn state-space model (SSM), which provides linear-time inference and robust extrapolation. Experiments are run on the LIBERO benchmark to compare the model with Transformer-based baselines.", "result": "RoboSSM demonstrates effective extrapolation for varying numbers of demonstrations, strong performance on new tasks, and robustness in long tasks. It outperforms Transformer-based models in these scenarios.", "conclusion": "State-space models like RoboSSM provide a scalable and efficient alternative to Transformers for in-context imitation learning, especially in long-context and few-shot adaptation scenarios."}}
{"id": "2509.19733", "pdf": "https://arxiv.org/pdf/2509.19733", "abs": "https://arxiv.org/abs/2509.19733", "authors": ["Hongtao Yang", "Bineng Zhong", "Qihua Liang", "Zhiruo Zhu", "Yaozong Zheng", "Ning Li"], "title": "Robust RGB-T Tracking via Learnable Visual Fourier Prompt Fine-tuning and Modality Fusion Prompt Generation", "categories": ["cs.CV"], "comment": "Accepted by TMM2025", "summary": "Recently, visual prompt tuning is introduced to RGB-Thermal (RGB-T) tracking\nas a parameter-efficient finetuning (PEFT) method. However, these PEFT-based\nRGB-T tracking methods typically rely solely on spatial domain information as\nprompts for feature extraction. As a result, they often fail to achieve optimal\nperformance by overlooking the crucial role of frequency-domain information in\nprompt learning. To address this issue, we propose an efficient Visual Fourier\nPrompt Tracking (named VFPTrack) method to learn modality-related prompts via\nFast Fourier Transform (FFT). Our method consists of symmetric feature\nextraction encoder with shared parameters, visual fourier prompts, and Modality\nFusion Prompt Generator that generates bidirectional interaction prompts\nthrough multi-modal feature fusion. Specifically, we first use a frozen feature\nextraction encoder to extract RGB and thermal infrared (TIR) modality features.\nThen, we combine the visual prompts in the spatial domain with the frequency\ndomain prompts obtained from the FFT, which allows for the full extraction and\nunderstanding of modality features from different domain information. Finally,\nunlike previous fusion methods, the modality fusion prompt generation module we\nuse combines features from different modalities to generate a fused modality\nprompt. This modality prompt is interacted with each individual modality to\nfully enable feature interaction across different modalities. Extensive\nexperiments conducted on three popular RGB-T tracking benchmarks show that our\nmethod demonstrates outstanding performance.", "AI": {"tldr": "The paper introduces VFPTrack, a novel RGB-T tracking method that incorporates frequency-domain information and fusion techniques for effective multi-modal tracking.", "motivation": "Existing RGB-T tracking methods relying solely on the spatial domain overlook frequency-domain information, which limits their performance potential.", "method": "The method combines spatial and frequency-domain information using a frozen encoder for modality feature extraction, Fast Fourier Transform for frequency-domain prompts, and a Modality Fusion Prompt Generator for fused modality interactions.", "result": "Extensive experiments across three prominent RGB-T tracking benchmarks reveal superior performance of the proposed method.", "conclusion": "The paper demonstrates the importance of integrating frequency-domain features and cross-modality interaction for enhancing RGB-T tracking methods."}}
{"id": "2509.19526", "pdf": "https://arxiv.org/pdf/2509.19526", "abs": "https://arxiv.org/abs/2509.19526", "authors": ["Ali Baheri", "Lars Lindemann"], "title": "Metriplectic Conditional Flow Matching for Dissipative Dynamics", "categories": ["cs.LG", "cs.SY", "eess.SY"], "comment": null, "summary": "Metriplectic conditional flow matching (MCFM) learns dissipative dynamics\nwithout violating first principles. Neural surrogates often inject energy and\ndestabilize long-horizon rollouts; MCFM instead builds the\nconservative-dissipative split into both the vector field and a structure\npreserving sampler. MCFM trains via conditional flow matching on short\ntransitions, avoiding long rollout adjoints. In inference, a Strang-prox scheme\nalternates a symplectic update with a proximal metric step, ensuring discrete\nenergy decay; an optional projection enforces strict decay when a trusted\nenergy is available. We provide continuous and discrete time guarantees linking\nthis parameterization and sampler to conservation, monotonic dissipation, and\nstable rollouts. On a controlled mechanical benchmark, MCFM yields phase\nportraits closer to ground truth and markedly fewer energy-increase and\npositive energy rate events than an equally expressive unconstrained neural\nflow, while matching terminal distributional fit.", "AI": {"tldr": "MCFM integrates dissipative dynamics into neural modeling while preserving first principles, optimizing short transitions without destabilizing long rollouts.", "motivation": "The paper addresses the issue of instability and energy injection in neural surrogates for modeling dissipative dynamics, especially over long rollouts.", "method": "MCFM employs conditional flow matching on short transitions for training, alongside an inference method combining symplectic updates and proximal metric steps for energy-preservation.", "result": "MCFM was tested on a controlled mechanical benchmark, showing superior energy stability and accuracy in phase portraits compared to unconstrained neural models.", "conclusion": "MCFM offers a robust solution for modeling dissipative dynamics with guarantees of conservation, monotonic dissipation, and stable rollouts between discrete and continuous time."}}
{"id": "2509.19901", "pdf": "https://arxiv.org/pdf/2509.19901", "abs": "https://arxiv.org/abs/2509.19901", "authors": ["Xinyu Liu", "Chao Qin", "Wei You"], "title": "Pure Exploration via Frank-Wolfe Self-Play", "categories": ["cs.LG", "cs.GT", "math.ST", "stat.ML", "stat.TH"], "comment": null, "summary": "We study pure exploration in structured stochastic multi-armed bandits,\naiming to efficiently identify the correct hypothesis from a finite set of\nalternatives. For a broad class of tasks, asymptotic analyses reduce to a\nmaximin optimization that admits a two-player zero-sum game interpretation\nbetween an experimenter and a skeptic: the experimenter allocates measurements\nto rule out alternatives while the skeptic proposes alternatives. We\nreformulate the game by allowing the skeptic to adopt a mixed strategy,\nyielding a concave-convex saddle-point problem. This viewpoint leads to\nFrank-Wolfe Self-Play (FWSP): a projection-free, regularization-free,\ntuning-free method whose one-hot updates on both sides match the bandit\nsampling paradigm. However, structural constraints introduce sharp pathologies\nthat complicate algorithm design and analysis: our linear-bandit case study\nexhibits nonunique optima, optimal designs with zero mass on the best arm,\nbilinear objectives, and nonsmoothness at the boundary. We address these\nchallenges via a differential-inclusion argument, proving convergence of the\ngame value for best-arm identification in linear bandits. Our analysis proceeds\nthrough a continuous-time limit: a differential inclusion with a Lyapunov\nfunction that decays exponentially, implying a vanishing duality gap and\nconvergence to the optimal value. Although Lyapunov analysis requires\ndifferentiability of the objective, which is not guaranteed on the boundary, we\nshow that along continuous trajectories the algorithm steers away from\npathological nonsmooth points and achieves uniform global convergence to the\noptimal game value. We then embed the discrete-time updates into a perturbed\nflow and show that the discrete game value also converges. Building on FWSP, we\nfurther propose a learning algorithm based on posterior sampling. Numerical\nexperiments demonstrate a vanishing duality gap.", "AI": {"tldr": "The paper addresses structured stochastic multi-armed bandit problems for hypothesis identification, introduces a game-theoretic optimization reformulation, and proposes a method (FWSP) for efficient convergence through both theoretical and numerical support.", "motivation": "The motivation is to efficiently solve the problem of pure exploration in structured stochastic multi-armed bandits, which involves identifying the correct hypothesis from a finite set of alternatives, despite complex structural constraints.", "method": "The authors reformulate the optimization as a two-player zero-sum game with a mixed strategy for the skeptic, leading to the development of a saddle-point problem solved by a method called Frank-Wolfe Self-Play (FWSP). They employ differential-inclusion arguments and Lyapunov functions for convergence analysis.", "result": "The method addresses challenges like nonsmoothness and nonunique optima, proving convergence to the optimal game value for linear-bandit hypothesis identification. Further, it supports theoretical findings with a learning algorithm and numerical experiments showing vanishing duality gaps.", "conclusion": "The FWSP framework successfully navigates structural pathologies in bandit problems, providing a theoretical and practical approach for efficient hypothesis identification with convergence guarantees and empirical validation."}}
{"id": "2509.20175", "pdf": "https://arxiv.org/pdf/2509.20175", "abs": "https://arxiv.org/abs/2509.20175", "authors": ["Lorenzo Giusti", "Ole Anton Werner", "Riccardo Taiello", "Matilde Carvalho Costa", "Emre Tosun", "Andrea Protani", "Marc Molina", "Rodrigo Lopes de Almeida", "Paolo Cacace", "Diogo Reis Santos", "Luigi Serio"], "title": "Federation of Agents: A Semantics-Aware Communication Fabric for Large-Scale Agentic AI", "categories": ["cs.AI", "cs.CL"], "comment": "18 pages, 4 figures", "summary": "We present Federation of Agents (FoA), a distributed orchestration framework\nthat transforms static multi-agent coordination into dynamic, capability-driven\ncollaboration. FoA introduces Versioned Capability Vectors (VCVs):\nmachine-readable profiles that make agent capabilities searchable through\nsemantic embeddings, enabling agents to advertise their capabilities, cost, and\nlimitations. Our aarchitecturecombines three key innovations: (1) semantic\nrouting that matches tasks to agents over sharded HNSW indices while enforcing\noperational constraints through cost-biased optimization, (2) dynamic task\ndecomposition where compatible agents collaboratively break down complex tasks\ninto DAGs of subtasks through consensus-based merging, and (3) smart clustering\nthat groups agents working on similar subtasks into collaborative channels for\nk-round refinement before synthesis. Built on top of MQTT,s publish-subscribe\nsemantics for scalable message passing, FoA achieves sub-linear complexity\nthrough hierarchical capability matching and efficient index maintenance.\nEvaluation on HealthBench shows 13x improvements over single-model baselines,\nwith clustering-enhanced laboration particularly effective for complex\nreasoning tasks requiring multiple perspectives. The system scales horizontally\nwhile maintaining consistent performance, demonstrating that semantic\norchestration with structured collaboration can unlock the collective\nintelligence of heterogeneous federations of AI agents.", "AI": {"tldr": "Federation of Agents (FoA) is a distributed framework enabling collaborative multi-agent task execution using capability profiles and semantic algorithms. It achieves significant efficiency gains on complex reasoning tasks.", "motivation": "The need for dynamic and efficient coordination among heterogeneous multi-agent systems to handle complex tasks and unlock collective intelligence.", "method": "FoA employs Versioned Capability Vectors (VCVs) for agent profiling, semantic routing for task-agent matching, dynamic task decomposition through consensus, and clustering for collaborative refinement of subtasks.", "result": "FoA achieves 13x performance improvements on complex reasoning tasks compared to single-model baselines and scales horizontally without compromising performance.", "conclusion": "FoA demonstrates that semantic orchestration paired with structured collaboration can significantly enhance the capabilities of federated AI systems."}}
{"id": "2509.19364", "pdf": "https://arxiv.org/pdf/2509.19364", "abs": "https://arxiv.org/abs/2509.19364", "authors": ["Angelina Wang", "Daniel E. Ho", "Sanmi Koyejo"], "title": "The Inadequacy of Offline LLM Evaluations: A Need to Account for Personalization in Model Behavior", "categories": ["cs.CL", "cs.AI"], "comment": "forthcoming in Patterns", "summary": "Standard offline evaluations for language models -- a series of independent,\nstate-less inferences made by models -- fail to capture how language models\nactually behave in practice, where personalization fundamentally alters model\nbehavior. For instance, identical benchmark questions to the same language\nmodel can produce markedly different responses when prompted to a state-less\nsystem, in one user's chat session, or in a different user's chat session. In\nthis work, we provide empirical evidence showcasing this phenomenon by\ncomparing offline evaluations to field evaluations conducted by having 800 real\nusers of ChatGPT and Gemini pose benchmark and other provided questions to\ntheir chat interfaces.", "AI": {"tldr": "Standard offline evaluations fail to capture the behavioral variations of language models influenced by personalization.", "motivation": "Language models often change behavior based on user context and personalization, which is not reflected in standard offline evaluations.", "method": "Empirical comparison was conducted between traditional offline evaluations and field evaluations using 800 users interacting with ChatGPT and Gemini.", "result": "Responses to identical benchmark questions varied significantly between state-less systems and personalized user sessions.", "conclusion": "The study demonstrates the inadequacy of offline evaluations and highlights the impact of user personalization in assessing language models."}}
{"id": "2509.19672", "pdf": "https://arxiv.org/pdf/2509.19672", "abs": "https://arxiv.org/abs/2509.19672", "authors": ["Dongzhe Zheng", "Wenjie Mei"], "title": "Memory-Augmented Potential Field Theory: A Framework for Adaptive Control in Non-Convex Domains", "categories": ["cs.RO", "math.DS"], "comment": "Accepted by NeurIPS 2025", "summary": "Stochastic optimal control methods often struggle in complex non-convex\nlandscapes, frequently becoming trapped in local optima due to their inability\nto learn from historical trajectory data. This paper introduces\nMemory-Augmented Potential Field Theory, a unified mathematical framework that\nintegrates historical experience into stochastic optimal control. Our approach\ndynamically constructs memory-based potential fields that identify and encode\nkey topological features of the state space, enabling controllers to\nautomatically learn from past experiences and adapt their optimization\nstrategy. We provide a theoretical analysis showing that memory-augmented\npotential fields possess non-convex escape properties, asymptotic convergence\ncharacteristics, and computational efficiency. We implement this theoretical\nframework in a Memory-Augmented Model Predictive Path Integral (MPPI)\ncontroller that demonstrates significantly improved performance in challenging\nnon-convex environments. The framework represents a generalizable approach to\nexperience-based learning within control systems (especially robotic dynamics),\nenhancing their ability to navigate complex state spaces without requiring\nspecialized domain knowledge or extensive offline training.", "AI": {"tldr": "This paper introduces a framework combining stochastic optimal control with memory to improve navigation in non-convex landscapes.", "motivation": "Stochastic optimal control methods often fail in non-convex scenarios because they cannot utilize historical data to escape local optima.", "method": "The paper proposes Memory-Augmented Potential Field Theory, which constructs dynamic fields encoding historical experience to guide optimization.", "result": "A Memory-Augmented Model Predictive Path Integral controller was developed, showing enhanced performance in challenging non-convex environments.", "conclusion": "The proposed approach generalizes experience-based learning for control systems, particularly robots, without relying on extensive offline training or domain-specific knowledge."}}
{"id": "2509.19743", "pdf": "https://arxiv.org/pdf/2509.19743", "abs": "https://arxiv.org/abs/2509.19743", "authors": ["Xinhao Zhong", "Shuoyang Sun", "Xulin Gu", "Chenyang Zhu", "Bin Chen", "Yaowei Wang"], "title": "Rectified Decoupled Dataset Distillation: A Closer Look for Fair and Comprehensive Evaluation", "categories": ["cs.CV"], "comment": null, "summary": "Dataset distillation aims to generate compact synthetic datasets that enable\nmodels trained on them to achieve performance comparable to those trained on\nfull real datasets, while substantially reducing storage and computational\ncosts. Early bi-level optimization methods (e.g., MTT) have shown promising\nresults on small-scale datasets, but their scalability is limited by high\ncomputational overhead. To address this limitation, recent decoupled dataset\ndistillation methods (e.g., SRe$^2$L) separate the teacher model pre-training\nfrom the synthetic data generation process. These methods also introduce random\ndata augmentation and epoch-wise soft labels during the post-evaluation phase\nto improve performance and generalization. However, existing decoupled\ndistillation methods suffer from inconsistent post-evaluation protocols, which\nhinders progress in the field. In this work, we propose Rectified Decoupled\nDataset Distillation (RD$^3$), and systematically investigate how different\npost-evaluation settings affect test accuracy. We further examine whether the\nreported performance differences across existing methods reflect true\nmethodological advances or stem from discrepancies in evaluation procedures.\nOur analysis reveals that much of the performance variation can be attributed\nto inconsistent evaluation rather than differences in the intrinsic quality of\nthe synthetic data. In addition, we identify general strategies that improve\nthe effectiveness of distilled datasets across settings. By establishing a\nstandardized benchmark and rigorous evaluation protocol, RD$^3$ provides a\nfoundation for fair and reproducible comparisons in future dataset distillation\nresearch.", "AI": {"tldr": "This paper identifies and addresses inconsistencies in dataset distillation evaluations, proposing a new method (RD$^3$) that standardizes evaluation to enable fair comparisons.", "motivation": "Existing dataset distillation methods are hindered by high computational overhead and inconsistent evaluation protocols, which obscure genuine methodological advances.", "method": "The authors propose Rectified Decoupled Dataset Distillation (RD$^3$), systematically analyzing post-evaluation settings and establishing a standardized benchmark with rigorous evaluation.", "result": "The study reveals that performance variations among methods are often due to evaluation inconsistencies, not actual methodological improvements, and identifies strategies for more effective dataset distillation.", "conclusion": "RD$^3$ creates a foundation for replicable and fair comparisons in dataset distillation research, advancing the field with standardized evaluation practices."}}
{"id": "2509.19538", "pdf": "https://arxiv.org/pdf/2509.19538", "abs": "https://arxiv.org/abs/2509.19538", "authors": ["Zongyue Li", "Xiao Han", "Yusong Li", "Niklas Strauss", "Matthias Schubert"], "title": "DAWM: Diffusion Action World Models for Offline Reinforcement Learning via Action-Inferred Transitions", "categories": ["cs.LG", "cs.AI"], "comment": "ICML2025 workshop Building Physically Plausible World Models", "summary": "Diffusion-based world models have demonstrated strong capabilities in\nsynthesizing realistic long-horizon trajectories for offline reinforcement\nlearning (RL). However, many existing methods do not directly generate actions\nalongside states and rewards, limiting their compatibility with standard\nvalue-based offline RL algorithms that rely on one-step temporal difference\n(TD) learning. While prior work has explored joint modeling of states, rewards,\nand actions to address this issue, such formulations often lead to increased\ntraining complexity and reduced performance in practice. We propose\n\\textbf{DAWM}, a diffusion-based world model that generates future state-reward\ntrajectories conditioned on the current state, action, and return-to-go, paired\nwith an inverse dynamics model (IDM) for efficient action inference. This\nmodular design produces complete synthetic transitions suitable for one-step\nTD-based offline RL, enabling effective and computationally efficient training.\nEmpirically, we show that conservative offline RL algorithms such as TD3BC and\nIQL benefit significantly from training on these augmented trajectories,\nconsistently outperforming prior diffusion-based baselines across multiple\ntasks in the D4RL benchmark.", "AI": {"tldr": "DAWM is a diffusion-based world model designed to generate state-reward trajectories for offline RL. It solves limitations of existing models by incorporating an inverse dynamics model (IDM).", "motivation": "To address the limitations of existing diffusion-based models in offline RL, which do not directly generate actions and struggle with one-step TD learning compatibility.", "method": "DAWM generates future state-reward trajectories conditioned on current state, action, and return-to-go. It uses an inverse dynamics model for efficient action inference and modular training.", "result": "DAWM, when combined with offline RL algorithms like TD3BC and IQL, significantly outperforms prior diffusion-based baselines on the D4RL benchmark.", "conclusion": "DAWM offers a computationally efficient and effective way to integrate diffusion-based methods with standard offline RL algorithms, improving performance across tasks."}}
{"id": "2509.19930", "pdf": "https://arxiv.org/pdf/2509.19930", "abs": "https://arxiv.org/abs/2509.19930", "authors": ["Mohammad Tabish", "Benedict Leimkuhler", "Stefan Klus"], "title": "How deep is your network? Deep vs. shallow learning of transfer operators", "categories": ["cs.LG", "math.DS", "stat.ML"], "comment": null, "summary": "We propose a randomized neural network approach called RaNNDy for learning\ntransfer operators and their spectral decompositions from data. The weights of\nthe hidden layers of the neural network are randomly selected and only the\noutput layer is trained. The main advantage is that without a noticeable\nreduction in accuracy, this approach significantly reduces the training time\nand resources while avoiding common problems associated with deep learning such\nas sensitivity to hyperparameters and slow convergence. Additionally, the\nproposed framework allows us to compute a closed-form solution for the output\nlayer which directly represents the eigenfunctions of the operator. Moreover,\nit is possible to estimate uncertainties associated with the computed spectral\nproperties via ensemble learning. We present results for different dynamical\noperators, including Koopman and Perron-Frobenius operators, which have\nimportant applications in analyzing the behavior of complex dynamical systems,\nand the Schr\\\"odinger operator. The numerical examples, which highlight the\nstrengths but also weaknesses of the proposed framework, include several\nstochastic dynamical systems, protein folding processes, and the quantum\nharmonic oscillator.", "AI": {"tldr": "RaNNDy, a randomized neural network approach, learns transfer operators and spectral decompositions efficiently by training only the output layer while maintaining accuracy.", "motivation": "To overcome limitations in deep learning such as sensitivity to hyperparameters and slow convergence, while reducing training time and computational resources for learning transfer operators.", "method": "RaNNDy uses neural networks with randomly initialized hidden layer weights, trains only the output layer, and computes a closed-form solution to directly obtain eigenfunctions of the operator.", "result": "RaNNDy effectively learns spectral properties of dynamical operators, like Koopman and Perron-Frobenius operators, with numerical demonstrations on stochastic systems, protein folding, and quantum oscillators.", "conclusion": "The method achieves a balance between efficiency and accuracy, with ensemble learning for uncertainty estimation, making it well-suited for analyzing complex dynamical systems."}}
{"id": "2509.19365", "pdf": "https://arxiv.org/pdf/2509.19365", "abs": "https://arxiv.org/abs/2509.19365", "authors": ["Wannes Janssens", "Matthias Bogaert", "Dirk Van den Poel"], "title": "LLM-Assisted Topic Reduction for BERTopic on Social Media Data", "categories": ["cs.CL", "cs.LG"], "comment": "13 pages, 8 figures. To be published in the Post-Workshop proceedings\n  of the ECML PKDD 2025 Conference", "summary": "The BERTopic framework leverages transformer embeddings and hierarchical\nclustering to extract latent topics from unstructured text corpora. While\neffective, it often struggles with social media data, which tends to be noisy\nand sparse, resulting in an excessive number of overlapping topics. Recent work\nexplored the use of large language models for end-to-end topic modelling.\nHowever, these approaches typically require significant computational overhead,\nlimiting their scalability in big data contexts. In this work, we propose a\nframework that combines BERTopic for topic generation with large language\nmodels for topic reduction. The method first generates an initial set of topics\nand constructs a representation for each. These representations are then\nprovided as input to the language model, which iteratively identifies and\nmerges semantically similar topics. We evaluate the approach across three\nTwitter/X datasets and four different language models. Our method outperforms\nthe baseline approach in enhancing topic diversity and, in many cases,\ncoherence, with some sensitivity to dataset characteristics and initial\nparameter selection.", "AI": {"tldr": "This paper introduces a two-stage framework for topic modeling by combining BERTopic and large language models, achieving improved topic diversity and coherence on social media datasets.", "motivation": "To address the challenge of noisy and sparse social media data, which BERTopic struggles with, and to overcome scalability limitations in large language model-based approaches.", "method": "The proposed framework combines BERTopic for initial topic extraction and transformer-based language models for iterative topic refinement by merging semantically similar topics.", "result": "Evaluation across three Twitter/X datasets and four language models demonstrates the framework improves topic diversity and often coherence, although results vary by dataset and parameter settings.", "conclusion": "The framework offers a scalable solution that integrates BERTopic with large language models, outperforming baseline methods in tackling social media data for topic modeling."}}
{"id": "2509.19688", "pdf": "https://arxiv.org/pdf/2509.19688", "abs": "https://arxiv.org/abs/2509.19688", "authors": ["Devesh Nath", "Haoran Yin", "Glen Chou"], "title": "Formal Safety Verification and Refinement for Generative Motion Planners via Certified Local Stabilization", "categories": ["cs.RO", "cs.LG", "cs.SY", "eess.SY", "math.OC"], "comment": "10 pages, 12 figures", "summary": "We present a method for formal safety verification of learning-based\ngenerative motion planners. Generative motion planners (GMPs) offer advantages\nover traditional planners, but verifying the safety and dynamic feasibility of\ntheir outputs is difficult since neural network verification (NNV) tools scale\nonly to a few hundred neurons, while GMPs often contain millions. To preserve\nGMP expressiveness while enabling verification, our key insight is to imitate\nthe GMP by stabilizing references sampled from the GMP with a small neural\ntracking controller and then applying NNV to the closed-loop dynamics. This\nyields reachable sets that rigorously certify closed-loop safety, while the\ncontroller enforces dynamic feasibility. Building on this, we construct a\nlibrary of verified GMP references and deploy them online in a way that\nimitates the original GMP distribution whenever it is safe to do so, improving\nsafety without retraining. We evaluate across diverse planners, including\ndiffusion, flow matching, and vision-language models, improving safety in\nsimulation (on ground robots and quadcopters) and on hardware\n(differential-drive robot).", "AI": {"tldr": "The paper introduces a verification method for generative motion planners (GMPs) using neural network tools to ensure safety and dynamic feasibility. They solve scalability issues by using a small tracking controller to certify closed-loop safety.", "motivation": "To address the challenge of verifying the outputs of learning-based generative motion planners, which often involve large neural networks that exceed the capabilities of current verification tools.", "method": "The method involves stabilizing GMP's sampled references with a small neural tracking controller, applying neural network verification techniques to the closed-loop system, and building a library of verified references for safer deployment.", "result": "Simulation and hardware tests show enhanced safety for ground robots, quadcopters, and differential-drive robots across diverse types of motion planners like diffusion and vision-language models.", "conclusion": "The proposed approach effectively improves the safety of generative motion planners without requiring retraining, demonstrating scalability and application across diverse systems."}}
{"id": "2509.19746", "pdf": "https://arxiv.org/pdf/2509.19746", "abs": "https://arxiv.org/abs/2509.19746", "authors": ["Yi Yang"], "title": "nnFilterMatch: A Unified Semi-Supervised Learning Framework with Uncertainty-Aware Pseudo-Label Filtering for Efficient Medical Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Semi-supervised learning (SSL) has emerged as a promising paradigm in medical\nimage segmentation, offering competitive performance while substantially\nreducing the need for extensive manual annotation. When combined with active\nlearning (AL), these strategies further minimize annotation burden by\nselectively incorporating the most informative samples. However, conventional\nSSL_AL hybrid approaches often rely on iterative and loop-based retraining\ncycles after each annotation round, incurring significant computational\noverhead and limiting scalability in clinical applications. In this study, we\npresent a novel, annotation-efficient, and self-adaptive deep segmentation\nframework that integrates SSL with entropy-based pseudo-label filtering\n(FilterMatch), an AL-inspired mechanism, within the single-pass nnU-Net\ntraining segmentation framework (nnFilterMatch). By selectively excluding\nhigh-confidence pseudo-labels during training, our method circumvents the need\nfor retraining loops while preserving the benefits of uncertainty-guided\nlearning. We validate the proposed framework across multiple clinical\nsegmentation benchmarks and demonstrate that it achieves performance comparable\nto or exceeding fully supervised models, even with only 5\\%--20\\% labeled data.\nThis work introduces a scalable, end-to-end learning strategy for reducing\nannotation demands in medical image segmentation without compromising accuracy.\nCode is available here: https://github.com/Ordi117/nnFilterMatch.git.", "AI": {"tldr": "This paper introduces nnFilterMatch, an efficient framework combining semi-supervised learning and active learning to significantly reduce annotation efforts in medical image segmentation.", "motivation": "Medical image segmentation is essential but heavily reliant on annotated data, which is labor-intensive. Combining SSL and AL can reduce this burden, though existing approaches suffer from scalability and computational inefficiencies.", "method": "The paper integrates entropy-based pseudo-label filtering with the nnU-Net framework, selectively excluding high-confidence pseudo-labels during training to avoid iterative retraining cycles.", "result": "The proposed framework delivers segmentation performance comparable to fully supervised models, while requiring only 5-20% labeled data, validated across multiple clinical benchmarks.", "conclusion": "nnFilterMatch offers a scalable and annotation-efficient method for medical image segmentation, retaining high accuracy without the need for extensive data labeling or retraining cycles."}}
{"id": "2509.19554", "pdf": "https://arxiv.org/pdf/2509.19554", "abs": "https://arxiv.org/abs/2509.19554", "authors": ["Yi Ren"], "title": "Learning Dynamics of Deep Learning -- Force Analysis of Deep Neural Networks", "categories": ["cs.LG", "cs.AI"], "comment": "175 pages", "summary": "This thesis explores how deep learning models learn over time, using ideas\ninspired by force analysis. Specifically, we zoom in on the model's training\nprocedure to see how one training example affects another during learning, like\nanalyzing how forces move objects. We break this influence into two parts: how\nsimilar the two examples are, and how strong the updating force is. This\nframework helps us understand a wide range of the model's behaviors in\ndifferent real systems. For example, it explains why certain examples have\nnon-trivial learning paths, why (and why not) some LLM finetuning methods work,\nand why simpler, more structured patterns tend to be learned more easily. We\napply this approach to various learning tasks and uncover new strategies for\nimproving model training. While the method is still developing, it offers a new\nway to interpret models' behaviors systematically.", "AI": {"tldr": "The thesis introduces a novel force-inspired framework to analyze how training examples influence each other during deep learning processes, offering insights into model behavior.", "motivation": "The paper aims to address the need for better interpretability of deep learning models by exploring how training examples interact during the learning process.", "method": "Inspired by force analysis, the study decomposes the influence of one example on another into similarity and updating force, and applies this framework across various real-world systems.", "result": "The approach provides insights into learning paths of examples, the efficacy of LLM fine-tuning methods, and why structured patterns are easier to learn by deep models.", "conclusion": "This method enriches our understanding of deep learning behaviors and offers new strategies to refine training processes, although it is still in the development stage."}}
{"id": "2509.19962", "pdf": "https://arxiv.org/pdf/2509.19962", "abs": "https://arxiv.org/abs/2509.19962", "authors": ["Feiyang Fu", "Tongxian Guo", "Zhaoqiang Liu"], "title": "Learnable Sampler Distillation for Discrete Diffusion Models", "categories": ["cs.LG", "stat.ML"], "comment": "NeurIPS 2025", "summary": "Discrete diffusion models (DDMs) have shown powerful generation ability for\ndiscrete data modalities like text and molecules. However, their practical\napplication is hindered by inefficient sampling, requiring a large number of\nsampling steps. Accelerating DDMs by using larger step sizes typically\nintroduces significant problems in generation quality, as it amplifies the\nimpact of both the compounding decoding error due to factorized predictions and\ndiscretization error from numerical approximations, leading to a significant\ndecrease in sampling quality. To address these challenges, we propose learnable\nsampler distillation (LSD), a novel approach to train fast and high-fidelity\nsamplers for DDMs. LSD employs a distillation approach where a student sampler\nwith a few steps learns to align its intermediate score trajectory with that of\na high-quality teacher sampler with numerous steps. This alignment is achieved\nby optimizing learnable sampler coefficients that adaptively adjust sampling\ndynamics. Additionally, we further propose LSD+, which also learns time\nschedules that allocate steps non-uniformly. Experiments across text\ngeneration, image generation, and synthetic tasks demonstrate that our proposed\napproaches outperform existing samplers for DDMs, achieving substantially\nhigher sampling quality with significantly fewer sampling steps. Our code is\navailable at\n\\href{https://github.com/feiyangfu/LSD}{https://github.com/feiyangfu/LSD}.", "AI": {"tldr": "This paper introduces Learnable Sampler Distillation (LSD) and LSD+ to speed up discrete diffusion models (DDMs) while maintaining high generation quality, reducing sampling steps significantly.", "motivation": "The inefficiency in sampling of discrete diffusion models (DDMs) limits their practical use, as larger step sizes degrade generation quality due to compounded prediction and discretization errors.", "method": "LSD trains a student sampler with fewer steps to align its intermediate score trajectory with a high-quality teacher sampler using adaptive coefficients. LSD+ extends this by learning non-uniform step schedules.", "result": "Experiments on text, image, and synthetic data show LSD and LSD+ outperform existing samplers in both sampling quality and step efficiency.", "conclusion": "LSD and LSD+ represent significant advancements for DDMs, offering accelerated sampling without sacrificing generation fidelity, thus broadening their practical application."}}
{"id": "2509.20270", "pdf": "https://arxiv.org/pdf/2509.20270", "abs": "https://arxiv.org/abs/2509.20270", "authors": ["Xingjian Kang", "Linda Vorberg", "Andreas Maier", "Alexander Katzmann", "Oliver Taubmann"], "title": "Scan-do Attitude: Towards Autonomous CT Protocol Management using a Large Language Model Agent", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Managing scan protocols in Computed Tomography (CT), which includes adjusting\nacquisition parameters or configuring reconstructions, as well as selecting\npostprocessing tools in a patient-specific manner, is time-consuming and\nrequires clinical as well as technical expertise. At the same time, we observe\nan increasing shortage of skilled workforce in radiology. To address this\nissue, a Large Language Model (LLM)-based agent framework is proposed to assist\nwith the interpretation and execution of protocol configuration requests given\nin natural language or a structured, device-independent format, aiming to\nimprove the workflow efficiency and reduce technologists' workload. The agent\ncombines in-context-learning, instruction-following, and structured toolcalling\nabilities to identify relevant protocol elements and apply accurate\nmodifications. In a systematic evaluation, experimental results indicate that\nthe agent can effectively retrieve protocol components, generate device\ncompatible protocol definition files, and faithfully implement user requests.\nDespite demonstrating feasibility in principle, the approach faces limitations\nregarding syntactic and semantic validity due to lack of a unified device API,\nand challenges with ambiguous or complex requests. In summary, the findings\nshow a clear path towards LLM-based agents for supporting scan protocol\nmanagement in CT imaging.", "AI": {"tldr": "The paper introduces a Large Language Model (LLM)-based agent to assist with managing CT scan protocols via natural language or structured requests, aiming to streamline workflows and reduce technologist workload.", "motivation": "CT scan protocol management is time-consuming, requiring expertise, amidst a shortage of skilled radiology professionals.", "method": "The proposed agent uses LLM's capabilities including in-context learning, instruction-following, and structured tool-calling for interpreting, modifying, and implementing protocol requests.", "result": "Experimental evaluation demonstrates the agent's ability to retrieve protocol components, generate device-compatible configurations, and satisfy user requests, though limitations exist concerning ambiguous inputs and lack of unified APIs.", "conclusion": "The approach suggests potential for LLM-based agents to enhance CT scan protocol management, though challenges remain in ensuring semantic accuracy and handling complexity."}}
{"id": "2509.19368", "pdf": "https://arxiv.org/pdf/2509.19368", "abs": "https://arxiv.org/abs/2509.19368", "authors": ["Ruanjun Li", "Ziheng Liu", "Yuanming Shi", "Jiawei Shao", "Chi Zhang", "Xuelong Li"], "title": "Pipeline Parallelism is All You Need for Optimized Early-Exit Based Self-Speculative Decoding", "categories": ["cs.CL", "cs.AI"], "comment": "17 pages, 7 figures", "summary": "Large language models (LLMs) deliver impressive generation quality, but incur\nvery high inference cost because each output token is generated\nauto-regressively through all model layers. Early-exit based self-speculative\ndecoding (EESD) has emerged to mitigate this cost. However, in practice, many\napproaches struggle to achieve the expected acceleration in such\ndraft-then-verify paradigm even with a well-aligned early-exit head and\nselected exit position. Our analysis reveals that EESD only pays off when the\nvast majority of draft tokens are accepted by the LLM. Otherwise, the draft\ncost may overcome the acceleration gain and lead to a negative speedup. To\nmitigate this, we propose Pipeline-Parallel Self-Speculative Decoding (PPSD)\nthat fully pipelines the draft and verification work so that no effort is\nwasted on failed predictions. It has two key innovations. We configure the\nmodel layers as a pipeline in which early-exit (draft) computations and\nremaining-layer (verification) computations overlap. We interleave drafting and\nverification per token. While the LLM is verifying the current token in its\nfinal layers, the early-exit path simultaneously drafts the next token. Such a\nverify-while-draft scheme keeps all units busy and validates tokens on-the-fly\nanalogous to pipelining the speculation and verification stages. Empirical\nresults confirm that PPSD achieves state-of-the-art acceleration in\nself-speculative LLM inference. On diverse benchmarks, PPSD achieves speedup\nratios in the range of 2.01x~3.81x, which gains almost the optimal acceleration\nat the fixed acceptance rate and exit position, showcasing its advancement in\nproviding efficient self-speculation.", "AI": {"tldr": "This paper proposes Pipeline-Parallel Self-Speculative Decoding (PPSD) to accelerate large language model inference using a pipelined draft-verify structure.", "motivation": "The high inference cost of large language models due to auto-regressive token generation motivates the need for faster decoding approaches.", "method": "The authors propose PPSD, which interleaves token drafting and verification by overlapping early-exit and remaining-layer computations in a pipelined fashion.", "result": "Empirical results show PPSD achieves a speedup ratio of 2.01x~3.81x while maintaining performance and optimizing efficiency at fixed acceptance rates.", "conclusion": "PPSD demonstrates state-of-the-art acceleration in self-speculative decoding, proving its effectiveness in enhancing large language model inference efficiency."}}
{"id": "2509.19696", "pdf": "https://arxiv.org/pdf/2509.19696", "abs": "https://arxiv.org/abs/2509.19696", "authors": ["Noah Geiger", "Tamim Asfour", "Neville Hogan", "Johannes Lachner"], "title": "Diffusion-Based Impedance Learning for Contact-Rich Manipulation Tasks", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "15 pages, 12 figures", "summary": "Learning methods excel at motion generation in the information domain but are\nnot primarily designed for physical interaction in the energy domain. Impedance\nControl shapes physical interaction but requires task-aware tuning by selecting\nfeasible impedance parameters. We present Diffusion-Based Impedance Learning, a\nframework that combines both domains. A Transformer-based Diffusion Model with\ncross-attention to external wrenches reconstructs a simulated Zero-Force\nTrajectory (sZFT). This captures both translational and rotational task-space\nbehavior. For rotations, we introduce a novel SLERP-based quaternion noise\nscheduler that ensures geometric consistency. The reconstructed sZFT is then\npassed to an energy-based estimator that updates stiffness and damping\nparameters. A directional rule is applied that reduces impedance along non task\naxes while preserving rigidity along task directions. Training data were\ncollected for a parkour scenario and robotic-assisted therapy tasks using\nteleoperation with Apple Vision Pro. With only tens of thousands of samples,\nthe model achieved sub-millimeter positional accuracy and sub-degree rotational\naccuracy. Its compact model size enabled real-time torque control and\nautonomous stiffness adaptation on a KUKA LBR iiwa robot. The controller\nachieved smooth parkour traversal within force and velocity limits and 30/30\nsuccess rates for cylindrical, square, and star peg insertions without any\npeg-specific demonstrations in the training data set. All code for the\nTransformer-based Diffusion Model, the robot controller, and the Apple Vision\nPro telemanipulation framework is publicly available. These results mark an\nimportant step towards Physical AI, fusing model-based control for physical\ninteraction with learning-based methods for trajectory generation.", "AI": {"tldr": "The paper introduces a framework called Diffusion-Based Impedance Learning, which integrates learning-based motion generation and physical interaction control using a Transformer-based Diffusion Model. Results show high accuracy and adaptability in robotic tasks.", "motivation": "The paper studies the challenge of integrating learning methods optimized for motion generation with physical interaction techniques such as Impedance Control, which demands fine-tuned parameters for effective physical task execution.", "method": "The authors utilize a Transformer-based Diffusion Model with a novel SLERP-based quaternion noise scheduler to reconstruct task trajectories and adapt impedance parameters using an energy-based estimator. The approach incorporates cross-attention to external forces and directional rules for impedance adjustment.", "result": "The framework demonstrated sub-millimeter positional accuracy and sub-degree rotational accuracy in robotic operations, successfully handling parkour traversal and achieving a perfect success rate in peg insertion tasks with minimal training samples.", "conclusion": "The proposed method bridges the gap between learning-based trajectory generation and physical interaction control, offering a scalable and precise solution for robotic systems. The publicly available code encourages further development in Physical AI."}}
{"id": "2509.19749", "pdf": "https://arxiv.org/pdf/2509.19749", "abs": "https://arxiv.org/abs/2509.19749", "authors": ["Shao-Yu Chang", "Jingyi Xu", "Hieu Le", "Dimitris Samaras"], "title": "Talking Head Generation via AU-Guided Landmark Prediction", "categories": ["cs.CV"], "comment": null, "summary": "We propose a two-stage framework for audio-driven talking head generation\nwith fine-grained expression control via facial Action Units (AUs). Unlike\nprior methods relying on emotion labels or implicit AU conditioning, our model\nexplicitly maps AUs to 2D facial landmarks, enabling physically grounded,\nper-frame expression control. In the first stage, a variational motion\ngenerator predicts temporally coherent landmark sequences from audio and AU\nintensities. In the second stage, a diffusion-based synthesizer generates\nrealistic, lip-synced videos conditioned on these landmarks and a reference\nimage. This separation of motion and appearance improves expression accuracy,\ntemporal stability, and visual realism. Experiments on the MEAD dataset show\nthat our method outperforms state-of-the-art baselines across multiple metrics,\ndemonstrating the effectiveness of explicit AU-to-landmark modeling for\nexpressive talking head generation.", "AI": {"tldr": "The paper introduces a two-stage model for creating realistic, audio-driven talking head videos with fine expression control using facial Action Units (AUs), outperforming existing methods.", "motivation": "To improve control, accuracy, and realism in audio-driven talking head generation, particularly focusing on per-frame expression control using facial Action Units (AUs).", "method": "A two-stage approach: Stage 1 employs a variational motion generator for predicting landmark sequences from audio and AU intensities, and Stage 2 uses a diffusion-based synthesizer to generate high-quality videos based on landmarks and a reference image.", "result": "The proposed method demonstrates superior performance in expression accuracy, temporal stability, and visual quality compared to state-of-the-art models, as validated on the MEAD dataset.", "conclusion": "Explicitly modeling Action Units (AUs) to generate facial landmarks substantially enhances the quality and control in expressive talking head generation, illustrating the framework's effectiveness."}}
{"id": "2509.19586", "pdf": "https://arxiv.org/pdf/2509.19586", "abs": "https://arxiv.org/abs/2509.19586", "authors": ["Alexander Ho", "Sukyeong Lee", "Francis T. F. Tsai"], "title": "A Foundation Chemical Language Model for Comprehensive Fragment-Based Drug Discovery", "categories": ["cs.LG", "cs.AI", "q-bio.BM"], "comment": null, "summary": "We introduce FragAtlas-62M, a specialized foundation model trained on the\nlargest fragment dataset to date. Built on the complete ZINC-22 fragment subset\ncomprising over 62 million molecules, it achieves unprecedented coverage of\nfragment chemical space. Our GPT-2 based model (42.7M parameters) generates\n99.90% chemically valid fragments. Validation across 12 descriptors and three\nfingerprint methods shows generated fragments closely match the training\ndistribution (all effect sizes < 0.4). The model retains 53.6% of known ZINC\nfragments while producing 22% novel structures with practical relevance. We\nrelease FragAtlas-62M with training code, preprocessed data, documentation, and\nmodel weights to accelerate adoption.", "AI": {"tldr": "FragAtlas-62M is a GPT-2 based model trained on over 62 million molecules, effectively generating novel and chemically valid fragments while maintaining high accuracy and releasing resources for public use.", "motivation": "To achieve unprecedented coverage of fragment chemical space and provide tools for accelerating research and application in molecular science.", "method": "A GPT-2 based model with 42.7M parameters, trained on the ZINC-22 fragment dataset, validated on descriptors and fingerprint methods.", "result": "The model generated chemically valid fragments with 99.90% validity, closely matching training distributions, retaining 53.6% known fragments, and producing 22% novel structures.", "conclusion": "FragAtlas-62M is a significant advancement for molecular research, offering high validity and novel fragment generation capabilities, complemented with accessible resources for broad adoption."}}
{"id": "2509.20201", "pdf": "https://arxiv.org/pdf/2509.20201", "abs": "https://arxiv.org/abs/2509.20201", "authors": ["Albert Kj\u00f8ller Jacobsen", "Johanna Marie Gegenfurtner", "Georgios Arvanitidis"], "title": "Staying on the Manifold: Geometry-Aware Noise Injection", "categories": ["cs.LG", "math.DG", "stat.ML"], "comment": null, "summary": "It has been shown that perturbing the input during training implicitly\nregularises the gradient of the learnt function, leading to smoother models and\nenhancing generalisation. However, previous research mostly considered the\naddition of ambient noise in the input space, without considering the\nunderlying structure of the data. In this work, we propose several methods of\nadding geometry-aware input noise that accounts for the lower dimensional\nmanifold the input space inhabits. We start by projecting ambient Gaussian\nnoise onto the tangent space of the manifold. In a second step, the noise\nsample is mapped on the manifold via the associated geodesic curve. We also\nconsider Brownian motion noise, which moves in random steps along the manifold.\nWe show that geometry-aware noise leads to improved generalization and\nrobustness to hyperparameter selection on highly curved manifolds, while\nperforming at least as well as training without noise on simpler manifolds. Our\nproposed framework extends to learned data manifolds.", "AI": {"tldr": "The paper explores geometry-aware input noise techniques to improve generalization and robustness in training models, focusing on manifold structures rather than ambient noise.", "motivation": "Enhance regularization and generalization performance in models by considering the inherent manifold structure of input data rather than relying on ambient noise.", "method": "Develop methods that generate input noise aligned with data manifold geometry, including projecting Gaussian noise onto tangent spaces, mapping via geodesic curves, and Brownian motion along manifolds.", "result": "Geometry-aware noise demonstrates better generalization and robustness, particularly on curved manifolds, while maintaining performance on simpler manifolds.", "conclusion": "Geometry-aware noise provides a structured approach to input regularization, resulting in smoother and more generalizable models particularly in complex data landscapes."}}
{"id": "2509.18122", "pdf": "https://arxiv.org/pdf/2509.18122", "abs": "https://arxiv.org/abs/2509.18122", "authors": ["Yue Zhang", "Jiaxin Zhang", "Qiuyu Ren", "Tahsin Saffat", "Xiaoxuan Liu", "Zitong Yang", "Banghua Zhu", "Yi Ma"], "title": "GAUSS: Benchmarking Structured Mathematical Skills for Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "120 pages (including appendix)", "summary": "We introduce \\textbf{GAUSS} (\\textbf{G}eneral \\textbf{A}ssessment of\n\\textbf{U}nderlying \\textbf{S}tructured \\textbf{S}kills in Mathematics), a\nbenchmark that evaluates LLMs' mathematical abilities across twelve core skill\ndimensions, grouped into three domains: knowledge and understanding, problem\nsolving and communication, and meta-skills and creativity. By categorizing\nproblems according to cognitive skills and designing tasks that isolate\nspecific abilities, GAUSS constructs comprehensive, fine-grained, and\ninterpretable profiles of models' mathematical abilities. These profiles\nfaithfully represent their underlying mathematical intelligence. To exemplify\nhow to use the \\textsc{GAUSS} benchmark, we have derived the skill profile of\n\\textsc{GPT-5-thinking}, revealing its strengths and weaknesses as well as its\ndifferences relative to \\textsc{o4-mini-high}, thereby underscoring the value\nof multidimensional, skill-based evaluation.", "AI": {"tldr": "The paper introduces GAUSS, a benchmark for evaluating LLMs' mathematical abilities across twelve skills grouped into three domains.", "motivation": "To provide a detailed and interpretable evaluation framework for understanding the mathematical capabilities of LLMs.", "method": "Tasks are designed to isolate specific abilities, categorized by cognitive skills, creating skill profiles of models such as GPT-5-thinking.", "result": "Skill profiles reveal strengths, weaknesses, and differences between models, demonstrating the utility of multidimensional evaluation.", "conclusion": "GAUSS offers a fine-grained assessment framework that faithfully represents the mathematical intelligence of LLMs, aiding in comprehensive analysis."}}
{"id": "2509.19369", "pdf": "https://arxiv.org/pdf/2509.19369", "abs": "https://arxiv.org/abs/2509.19369", "authors": ["Changhyun Jeon", "Jinhee Park", "Jungwoo Choi", "Keonwoo Kim", "Jisu Kim", "Minji Hong"], "title": "SLM-Based Agentic AI with P-C-G: Optimized for Korean Tool Use", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We propose a small-scale language model (SLM) based agent architecture,\nPlanner-Caller-Generator (P-C-G), optimized for Korean tool use. P-C-G\nseparates planning, calling, and generation by role: the Planner produces an\ninitial batch plan with limited on-demand replanning; the Caller returns a\nnormalized call object after joint schema-value validation; and the Generator\nintegrates tool outputs to produce the final answer. We apply a Korean-first\nvalue policy to reduce execution failures caused by frequent Korean-to-English\ncode switching in Korean settings. Evaluation assumes Korean queries and Korean\ntool/parameter specifications; it covers single-chain, multi-chain,\nmissing-parameters, and missing-functions scenarios, and is conducted via an\nLLM-as-a-Judge protocol averaged over five runs under a unified I/O interface.\nResults show that P-C-G delivers competitive tool-use accuracy and end-to-end\nquality while reducing tokens and maintaining acceptable latency, indicating\nthat role-specialized SLMs are a cost-effective alternative for Korean tool-use\nagents.", "AI": {"tldr": "This paper introduces a Planner-Caller-Generator architecture for small language models optimized for Korean tool use, reducing errors and improving efficiency in Korean queries.", "motivation": "There is a need for efficient and specialized language model architectures to handle Korean-related tasks due to challenges like frequent code-switching between Korean and English.", "method": "The introduced architecture separates tasks into three roles: planning, calling tools with validation, and generating answers. A Korean-first value policy is used to address code-switching issues.", "result": "The proposed model demonstrates competitive accuracy, reduced token usage, and acceptable latency in various scenarios, assessed under a unified interface across multiple evaluation runs.", "conclusion": "Role-specialized small-scale language models, like P-C-G, are effective alternatives for Korean tool-use tasks, combining efficiency and cost-effectiveness."}}
{"id": "2509.19712", "pdf": "https://arxiv.org/pdf/2509.19712", "abs": "https://arxiv.org/abs/2509.19712", "authors": ["Liquan Wang", "Jiangjie Bian", "Eric Heiden", "Animesh Garg"], "title": "TopoCut: Learning Multi-Step Cutting with Spectral Rewards and Discrete Diffusion Policies", "categories": ["cs.RO"], "comment": null, "summary": "Robotic manipulation tasks involving cutting deformable objects remain\nchallenging due to complex topological behaviors, difficulties in perceiving\ndense object states, and the lack of efficient evaluation methods for cutting\noutcomes. In this paper, we introduce TopoCut, a comprehensive benchmark for\nmulti-step robotic cutting tasks that integrates a cutting environment and\ngeneralized policy learning. TopoCut is built upon three core components: (1)\nWe introduce a high-fidelity simulation environment based on a particle-based\nelastoplastic solver with compliant von Mises constitutive models, augmented by\na novel damage-driven topology discovery mechanism that enables accurate\ntracking of multiple cutting pieces. (2) We develop a comprehensive reward\ndesign that integrates the topology discovery with a pose-invariant spectral\nreward model based on Laplace-Beltrami eigenanalysis, facilitating consistent\nand robust assessment of cutting quality. (3) We propose an integrated policy\nlearning pipeline, where a dynamics-informed perception module predicts\ntopological evolution and produces particle-wise, topology-aware embeddings to\nsupport PDDP (Particle-based Score-Entropy Discrete Diffusion Policy) for\ngoal-conditioned policy learning. Extensive experiments demonstrate that\nTopoCut supports trajectory generation, scalable learning, precise evaluation,\nand strong generalization across diverse object geometries, scales, poses, and\ncutting goals.", "AI": {"tldr": "TopoCut introduces a benchmark for robotic cutting tasks, combining a detailed simulation environment, advanced reward modeling, and an integrated learning pipeline.", "motivation": "Robotic cutting of deformable objects is challenging due to complex behaviors, perception difficulties, and a lack of effective evaluation methods.", "method": "TopoCut integrates a high-fidelity simulation, a robust reward design system based on eigenanalysis, and a learning pipeline using topology-aware embeddings for particle-based goal-conditioned policy learning.", "result": "TopoCut enables trajectory generation, scalable learning, precise evaluation, and generalization across diverse object shapes, sizes, and cutting objectives.", "conclusion": "TopoCut provides a comprehensive framework to address challenges in robotic cutting tasks, presenting strong learning and evaluation capabilities while promoting generalization."}}
{"id": "2509.19753", "pdf": "https://arxiv.org/pdf/2509.19753", "abs": "https://arxiv.org/abs/2509.19753", "authors": ["Jinhui Zheng", "Xueyuan Gong"], "title": "ExpFace: Exponential Angular Margin Loss for Deep Face Recognition", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Face recognition is an open-set problem requiring high discriminative power\nto ensure that intra-class distances remain smaller than inter-class distances.\nMargin-based softmax losses, such as SphereFace, CosFace, and ArcFace, have\nbeen widely adopted to enhance intra-class compactness and inter-class\nseparability, yet they overlook the impact of noisy samples. By examining the\ndistribution of samples in the angular space, we observe that clean samples\npredominantly cluster in the center region, whereas noisy samples tend to shift\ntoward the peripheral region. Motivated by this observation, we propose the\nExponential Angular Margin Loss (ExpFace), which introduces an angular\nexponential term as the margin. This design applies a larger penalty in the\ncenter region and a smaller penalty in the peripheral region within the angular\nspace, thereby emphasizing clean samples while suppressing noisy samples. We\npresent a unified analysis of ExpFace and classical margin-based softmax losses\nin terms of margin embedding forms, similarity curves, and gradient curves,\nshowing that ExpFace not only avoids the training instability of SphereFace and\nthe non-monotonicity of ArcFace, but also exhibits a similarity curve that\napplies penalties in the same manner as the decision boundary in the angular\nspace. Extensive experiments demonstrate that ExpFace achieves state-of-the-art\nperformance. To facilitate future research, we have released the source code\nat: https://github.com/dfr-code/ExpFace.", "AI": {"tldr": "The paper introduces ExpFace, an advanced face recognition loss function that better handles noisy samples and achieves superior performance compared to previous methods.", "motivation": "The study aims to address the limitations of existing margin-based softmax losses in face recognition, particularly their inability to effectively deal with noisy samples.", "method": "ExpFace is proposed as a loss function introducing an angular exponential term, which applies larger penalties to clean samples and smaller penalties to noisy samples, improving training stability and performance.", "result": "ExpFace not only solves issues found in SphereFace and ArcFace but also achieves state-of-the-art results in face recognition benchmarks.", "conclusion": "The proposed ExpFace loss enhances the discriminative power of face recognition systems while addressing the challenges posed by noisy data. Source code is made publicly available for further research."}}
{"id": "2509.19601", "pdf": "https://arxiv.org/pdf/2509.19601", "abs": "https://arxiv.org/abs/2509.19601", "authors": ["Jichi Wang", "Eduardo D. Sontag", "Domitilla Del Vecchio"], "title": "Modular Machine Learning with Applications to Genetic Circuit Composition", "categories": ["cs.LG", "cs.SY", "eess.SY"], "comment": null, "summary": "In several applications, including in synthetic biology, one often has\ninput/output data on a system composed of many modules, and although the\nmodules' input/output functions and signals may be unknown, knowledge of the\ncomposition architecture can significantly reduce the amount of training data\nrequired to learn the system's input/output mapping. Learning the modules'\ninput/output functions is also necessary for designing new systems from\ndifferent composition architectures. Here, we propose a modular learning\nframework, which incorporates prior knowledge of the system's compositional\nstructure to (a) identify the composing modules' input/output functions from\nthe system's input/output data and (b) achieve this by using a reduced amount\nof data compared to what would be required without knowledge of the\ncompositional structure. To achieve this, we introduce the notion of modular\nidentifiability, which allows recovery of modules' input/output functions from\na subset of the system's input/output data, and provide theoretical guarantees\non a class of systems motivated by genetic circuits. We demonstrate the theory\non computational studies showing that a neural network (NNET) that accounts for\nthe compositional structure can learn the composing modules' input/output\nfunctions and predict the system's output on inputs outside of the training set\ndistribution. By contrast, a neural network that is agnostic of the structure\nis unable to predict on inputs that fall outside of the training set\ndistribution. By reducing the need for experimental data and allowing module\nidentification, this framework offers the potential to ease the design of\nsynthetic biological circuits and of multi-module systems more generally.", "AI": {"tldr": "This paper presents a modular learning framework that uses prior knowledge of a system's compositional structure to identify module input/output functions with less training data, offering potential improvements for synthetic biology and multi-module systems.", "motivation": "In systems composed of many modules with unknown input/output functions, leveraging knowledge of the composition architecture can reduce training data needs and is critical for designing new systems.", "method": "The paper introduces modular identifiability and applies a neural network framework that incorporates compositional structure to learn module input/output functions with theoretical guarantees.", "result": "Computational studies show that neural networks using compositional structure can predict system outputs even for inputs outside the training data distribution, unlike structure-agnostic neural networks.", "conclusion": "The proposed framework reduces experimental data needs, facilitates module identification, and enables the design of synthetic biological circuits and other complex multi-module systems."}}
{"id": "2509.20323", "pdf": "https://arxiv.org/pdf/2509.20323", "abs": "https://arxiv.org/abs/2509.20323", "authors": ["Sara Fridovich-Keil", "Mert Pilanci"], "title": "A Recovery Guarantee for Sparse Neural Networks", "categories": ["cs.LG", "math.OC", "stat.ML"], "comment": "Code is available at https://github.com/voilalab/MLP-IHT", "summary": "We prove the first guarantees of sparse recovery for ReLU neural networks,\nwhere the sparse network weights constitute the signal to be recovered.\nSpecifically, we study structural properties of the sparse network weights for\ntwo-layer, scalar-output networks under which a simple iterative hard\nthresholding algorithm recovers these weights exactly, using memory that grows\nlinearly in the number of nonzero weights. We validate this theoretical result\nwith simple experiments on recovery of sparse planted MLPs, MNIST\nclassification, and implicit neural representations. Experimentally, we find\nperformance that is competitive with, and often exceeds, a high-performing but\nmemory-inefficient baseline based on iterative magnitude pruning.", "AI": {"tldr": "The paper presents theoretical guarantees and empirical validation for sparse recovery in ReLU-based neural networks using an efficient iterative hard thresholding algorithm.", "motivation": "Sparse recovery in neural networks is critical for memory efficiency and performance optimization, and the study aims to establish theoretical guarantees for this process.", "method": "The authors analyze sparse two-layer scalar-output networks and use iterative hard thresholding algorithms for weight recovery, providing theoretical backing and experimental validation.", "result": "The algorithm recovers sparse network weights using linear memory and delivers competitive experimental performance, often exceeding memory-intensive iterative magnitude pruning.", "conclusion": "Sparse recovery in ReLU neural networks can be effectively achieved with an iterative hard thresholding algorithm, ensuring both theoretical guarantees and practical performance advantages."}}
{"id": "2509.19153", "pdf": "https://arxiv.org/pdf/2509.19153", "abs": "https://arxiv.org/abs/2509.19153", "authors": ["Massimo Bartoletti", "Enrico Lipparini", "Livio Pompianu"], "title": "LLMs as verification oracles for Solidity", "categories": ["cs.CR", "cs.AI", "cs.SE"], "comment": null, "summary": "Ensuring the correctness of smart contracts is critical, as even subtle flaws\ncan lead to severe financial losses. While bug detection tools able to spot\ncommon vulnerability patterns can serve as a first line of defense, most\nreal-world exploits and losses stem from errors in the contract business logic.\nFormal verification tools such as SolCMC and the Certora Prover address this\nchallenge, but their impact remains limited by steep learning curves and\nrestricted specification languages. Recent works have begun to explore the use\nof large language models (LLMs) for security-related tasks such as\nvulnerability detection and test generation. Yet, a fundamental question\nremains open: can LLMs serve as verification oracles, capable of reasoning\nabout arbitrary contract-specific properties? In this paper, we provide the\nfirst systematic evaluation of GPT-5, a state-of-the-art reasoning LLM, in this\nrole. We benchmark its performance on a large dataset of verification tasks,\ncompare its outputs against those of established formal verification tools, and\nassess its practical effectiveness in real-world auditing scenarios. Our study\ncombines quantitative metrics with qualitative analysis, and shows that recent\nreasoning-oriented LLMs can be surprisingly effective as verification oracles,\nsuggesting a new frontier in the convergence of AI and formal methods for\nsecure smart contract development and auditing.", "AI": {"tldr": "This paper evaluates the effectiveness of GPT-5, a reasoning-oriented language model, as a verification oracle for smart contracts.", "motivation": "Many existing tools for ensuring the correctness of smart contracts are limited by their complexity and narrow capabilities, while most real-world exploits result from errors in business logic.", "method": "The authors systematically evaluate GPT-5's performance on a dataset of verification tasks, comparing it to formal verification tools, and analyze its practical effectiveness through a mix of quantitative and qualitative methods.", "result": "The study demonstrates that reasoning-oriented LLMs, like GPT-5, are surprisingly effective at reasoning about contract-specific properties and performing verification tasks.", "conclusion": "GPT-5 shows promise as a new tool for secure smart contract development and auditing, indicating a potential integration of AI with formal methods in this domain."}}
{"id": "2509.19370", "pdf": "https://arxiv.org/pdf/2509.19370", "abs": "https://arxiv.org/abs/2509.19370", "authors": ["Zhaoyu Ma", "Yuan Shan", "Jiahao Zhao", "Nan Xu", "Lei Wang"], "title": "Meow: End-to-End Outline Writing for Automatic Academic Survey", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "As academic paper publication numbers grow exponentially, conducting in-depth\nsurveys with LLMs automatically has become an inevitable trend. Outline\nwriting, which aims to systematically organize related works, is critical for\nautomated survey generation. Yet existing automatic survey methods treat\noutline writing as mere workflow steps in the overall pipeline. Such\ntemplate-based workflows produce outlines that lack in-depth understanding of\nthe survey topic and fine-grained styles. To address these limitations, we\npropose Meow, the first metadata-driven outline writing framework that produces\norganized and faithful outlines efficiently. Specifically, we first formulate\noutline writing as an end-to-end task that generates hierarchical structured\noutlines from paper metadata. We then curate a high-quality dataset of surveys\nfrom arXiv, bioRxiv, and medRxiv, and establish systematic evaluation metrics\nfor outline quality assessment. Finally, we employ a two-stage training\napproach combining supervised fine-tuning and reinforcement learning. Our 8B\nreasoning model demonstrates strong performance with high structural fidelity\nand stylistic coherence.", "AI": {"tldr": "This paper proposes Meow, a metadata-driven framework for generating structured outlines to improve automatic survey writing.", "motivation": "To address limitations in current template-based automatic survey methods, which lack depth and stylistic coherence in outline writing.", "method": "The authors created Meow, a framework that formulates outline writing as an end-to-end task using hierarchical generation from paper metadata, supported by a curated dataset and two-stage training (supervised fine-tuning and reinforcement learning).", "result": "The proposed 8B reasoning model achieved high performance with structural fidelity and stylistic coherence in generated outlines.", "conclusion": "The metadata-driven approach of Meow enables efficient, organized, and faithful outline writing, paving the way for enhanced automated survey generation."}}
{"id": "2509.19725", "pdf": "https://arxiv.org/pdf/2509.19725", "abs": "https://arxiv.org/abs/2509.19725", "authors": ["Naveed D. Riaziat", "Joseph Chen", "Axel Krieger", "Jeremy D. Brown"], "title": "Towards Autonomous Robotic Electrosurgery via Thermal Imaging", "categories": ["cs.RO"], "comment": "Accepted for publication in the proceedings of the 2025 IEEE/RSJ\n  International Conference on Intelligent Robots and Systems (IROS 2025)", "summary": "Electrosurgery is a surgical technique that can improve tissue cutting by\nreducing cutting force and bleeding. However, electrosurgery adds a risk of\nthermal injury to surrounding tissue. Expert surgeons estimate desirable\ncutting velocities based on experience but have no quantifiable reference to\nindicate if a particular velocity is optimal. Furthermore, prior demonstrations\nof autonomous electrosurgery have primarily used constant tool velocity, which\nis not robust to changes in electrosurgical tissue characteristics, power\nsettings, or tool type. Thermal imaging feedback provides information that can\nbe used to reduce thermal injury while balancing cutting force by controlling\ntool velocity. We introduce Thermography for Electrosurgical Rate Modulation\nvia Optimization (ThERMO) to autonomously reduce thermal injury while balancing\ncutting force by intelligently controlling tool velocity. We demonstrate ThERMO\nin tissue phantoms and compare its performance to the constant velocity\napproach. Overall, ThERMO improves cut success rate by a factor of three and\ncan reduce peak cutting force by a factor of two. ThERMO responds to varying\nenvironmental disturbances, reduces damage to tissue, and completes cutting\ntasks that would otherwise result in catastrophic failure for the constant\nvelocity approach.", "AI": {"tldr": "ThERMO, a technique for electrosurgery, uses thermal imaging to optimize tool velocity, reducing tissue damage and improving cutting outcomes.", "motivation": "Current electrosurgery lacks a quantifiable method to optimize cutting velocity, leading to risks of thermal injury and reliance on constant velocity, which fails under varying conditions.", "method": "ThERMO utilizes thermal imaging feedback and optimization algorithms to dynamically adjust the tool velocity, reducing thermal injury and balancing cutting force.", "result": "ThERMO achieved a threefold improvement in cut success rate, halved peak cutting forces, and demonstrated robustness to environmental disturbances compared to constant velocity methods.", "conclusion": "ThERMO presents a promising approach in autonomous electrosurgery, mitigating tissue damage, improving success rates, and adapting to diverse surgical conditions."}}
{"id": "2509.19760", "pdf": "https://arxiv.org/pdf/2509.19760", "abs": "https://arxiv.org/abs/2509.19760", "authors": ["Xiangyang Chen", "Shuzhao Li", "Xiuwen Zhu", "Yongfan Chen", "Fan Yang", "Cheng Fang", "Lin Qu", "Xiaoxiao Xu", "Hu Wei", "Minggang Wu"], "title": "Logics-Parsing Technical Report", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in Large Vision-Language models (LVLM) have spurred\nsignificant progress in document parsing task. Compared to traditional\npipeline-based methods, end-to-end paradigms have shown their excellence in\nconverting PDF images into structured outputs through integrated Optical\nCharacter Recognition (OCR), table recognition, mathematical formula\nrecognition and so on. However, the absence of explicit analytical stages for\ndocument layouts and reading orders limits the LVLM's capability in handling\ncomplex document types such as multi-column newspapers or posters. To address\nthis limitation, we propose in this report Logics-Parsing: an end-to-end\nLVLM-based model augmented with reinforcement learning. Our model incorporates\nmeticulously designed reward mechanisms to optimize complex layout analysis and\nreading order inference. In addition, we expand the model's versatility by\nincorporating diverse data types such as chemical formulas and handwritten\nChinese characters into supervised fine-tuning. Finally, to enable rigorous\nevaluation of our approach, we introduce LogicsParsingBench, a curated set of\n1,078 page-level PDF images spanning nine major categories and over twenty\nsub-categories, which will be released later. Comprehensive experiments\nconducted on LogicsParsingBench have validated the efficacy and\nState-of-the-art (SOTA) performance of our proposed model across diverse\ndocument analysis scenarios. Project Page:\nhttps://github.com/alibaba/Logics-Parsing", "AI": {"tldr": "This paper introduces Logics-Parsing, an LVLM-based model with reinforcement learning to enhance document parsing capabilities for complex layouts and reading order inference.", "motivation": "The paper aims to overcome limitations of existing LVLM paradigms in handling complex document types, which are hindered by missing analytical stages for layouts and reading orders.", "method": "The proposed model, Logics-Parsing, utilizes reinforcement learning with well-crafted reward mechanisms for layout and reading order optimization. It also incorporates diverse data types in supervised fine-tuning.", "result": "Logics-Parsing demonstrates State-of-the-Art (SOTA) performance on diverse document parsing tasks, validated through experiments using a curated benchmark dataset, LogicsParsingBench.", "conclusion": "The study concludes that Logics-Parsing significantly enhances document parsing in various complex scenarios, providing a robust solution supported by extensive evaluation and a new benchmark dataset."}}
{"id": "2509.19604", "pdf": "https://arxiv.org/pdf/2509.19604", "abs": "https://arxiv.org/abs/2509.19604", "authors": ["Jiayi Xin", "Aniruddh Raghu", "Nick Bhattacharya", "Adam Carr", "Melanie Montgomery", "Hunter Elliott"], "title": "Improved Therapeutic Antibody Reformatting through Multimodal Machine Learning", "categories": ["cs.LG"], "comment": "NeurIPS 2025 AI4Science Workshop and NeurIPS 2025 Multi-modal\n  Foundation Models and Large Language Models for Life Sciences Workshop", "summary": "Modern therapeutic antibody design often involves composing multi-part\nassemblages of individual functional domains, each of which may be derived from\na different source or engineered independently. While these complex formats can\nexpand disease applicability and improve safety, they present a significant\nengineering challenge: the function and stability of individual domains are not\nguaranteed in the novel format, and the entire molecule may no longer be\nsynthesizable. To address these challenges, we develop a machine learning\nframework to predict \"reformatting success\" -- whether converting an antibody\nfrom one format to another will succeed or not. Our framework incorporates both\nantibody sequence and structural context, incorporating an evaluation protocol\nthat reflects realistic deployment scenarios. In experiments on a real-world\nantibody reformatting dataset, we find the surprising result that large\npretrained protein language models (PLMs) fail to outperform simple,\ndomain-tailored, multimodal representations. This is particularly evident in\nthe most difficult evaluation setting, where we test model generalization to a\nnew starting antibody. In this challenging \"new antibody, no data\" scenario,\nour best multimodal model achieves high predictive accuracy, enabling\nprioritization of promising candidates and reducing wasted experimental effort.", "AI": {"tldr": "The paper proposes a machine learning framework to predict the success of converting antibodies from one format to another, addressing engineering challenges in therapeutic antibody design.", "motivation": "To address the challenges in therapeutic antibody design, where functional and stability issues arise when converting antibodies into novel formats.", "method": "The study utilizes a machine learning framework integrating antibody sequence and structural context. It also evaluates models under realistic deployment scenarios and compares performance of large pre-trained protein language models to domain-tailored multimodal representations.", "result": "Large pretrained language models fail to outperform simpler, tailored multimodal models, especially in challenging scenarios like generalizing to a new starting antibody. Their multimodal model achieves high predictive accuracy.", "conclusion": "Domain-tailored multimodal representations outperform pretrained models in novel scenarios, enabling effective candidate prioritization and reducing experimental efforts."}}
{"id": "2509.20345", "pdf": "https://arxiv.org/pdf/2509.20345", "abs": "https://arxiv.org/abs/2509.20345", "authors": ["Meshi Bashari", "Yonghoon Lee", "Roy Maor Lotan", "Edgar Dobriban", "Yaniv Romano"], "title": "Statistical Inference Leveraging Synthetic Data with Distribution-Free Guarantees", "categories": ["stat.ME", "cs.LG", "stat.ML"], "comment": null, "summary": "The rapid proliferation of high-quality synthetic data -- generated by\nadvanced AI models or collected as auxiliary data from related tasks --\npresents both opportunities and challenges for statistical inference. This\npaper introduces a GEneral Synthetic-Powered Inference (GESPI) framework that\nwraps around any statistical inference procedure to safely enhance sample\nefficiency by combining synthetic and real data. Our framework leverages\nhigh-quality synthetic data to boost statistical power, yet adaptively defaults\nto the standard inference method using only real data when synthetic data is of\nlow quality. The error of our method remains below a user-specified bound\nwithout any distributional assumptions on the synthetic data, and decreases as\nthe quality of the synthetic data improves. This flexibility enables seamless\nintegration with conformal prediction, risk control, hypothesis testing, and\nmultiple testing procedures, all without modifying the base inference method.\nWe demonstrate the benefits of our method on challenging tasks with limited\nlabeled data, including AlphaFold protein structure prediction, and comparing\nlarge reasoning models on complex math problems.", "AI": {"tldr": "The paper introduces the GESPI framework to improve statistical inference by safely combining real and synthetic data.", "motivation": "To address the challenges of integrating synthetic data with real data while ensuring statistical inference remains reliable and efficient.", "method": "The GESPI wraps around existing inference methods, adaptively using synthetic data when it is high-quality and defaulting to real data when synthetic data is unreliable, without any distributional assumptions.", "result": "GESPI enhances statistical power, adheres to error bounds, and seamlessly integrates with existing statistical procedures, demonstrated through tasks like AlphaFold predictions and reasoning model evaluations.", "conclusion": "The framework offers a robust solution for leveraging synthetic data in statistical inference, boosting efficiency and accuracy without compromising reliability."}}
{"id": "2509.19371", "pdf": "https://arxiv.org/pdf/2509.19371", "abs": "https://arxiv.org/abs/2509.19371", "authors": ["Kangtao Lv", "Haibin Chen", "Yujin Yuan", "Langming Liu", "Shilei Liu", "Yongwei Wang", "Wenbo Su", "Bo Zheng"], "title": "How to inject knowledge efficiently? Knowledge Infusion Scaling Law for Pre-training Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have attracted significant attention due to\ntheir impressive general capabilities across diverse downstream tasks. However,\nwithout domain-specific optimization, they often underperform on specialized\nknowledge benchmarks and even produce hallucination. Recent studies show that\nstrategically infusing domain knowledge during pretraining can substantially\nimprove downstream performance. A critical challenge lies in balancing this\ninfusion trade-off: injecting too little domain-specific data yields\ninsufficient specialization, whereas excessive infusion triggers catastrophic\nforgetting of previously acquired knowledge. In this work, we focus on the\nphenomenon of memory collapse induced by over-infusion. Through systematic\nexperiments, we make two key observations, i.e. 1) Critical collapse point:\neach model exhibits a threshold beyond which its knowledge retention\ncapabilities sharply degrade. 2) Scale correlation: these collapse points scale\nconsistently with the model's size. Building on these insights, we propose a\nknowledge infusion scaling law that predicts the optimal amount of domain\nknowledge to inject into large LLMs by analyzing their smaller counterparts.\nExtensive experiments across different model sizes and pertaining token budgets\nvalidate both the effectiveness and generalizability of our scaling law.", "AI": {"tldr": "This paper investigates the trade-off between domain knowledge infusion and general knowledge retention in large language models (LLMs). It introduces a scaling law to optimize the process and verifies its effectiveness across various settings.", "motivation": "LLMs perform well on general tasks but struggle with domain-specific ones, especially without optimization. Balancing domain-specific knowledge infusion without causing memory collapse is a challenge.", "method": "The authors conducted systematic experiments to identify thresholds of knowledge retention and scaled these observations across model sizes. From these insights, they developed a scaling law to optimize domain knowledge infusion.", "result": "The team observed key patterns in memory collapse and established a scaling law correlating knowledge infusion thresholds to model sizes. Experimental validations demonstrated the law\u2019s effectiveness and its broader applicability.", "conclusion": "The scaling law helps optimize domain knowledge infusion in LLMs while maintaining general knowledge retention capabilities. It works effectively across different model sizes and token budgets."}}
{"id": "2509.19732", "pdf": "https://arxiv.org/pdf/2509.19732", "abs": "https://arxiv.org/abs/2509.19732", "authors": ["Kyo Kutsuzawa", "Mitsuhiro Hayashibe"], "title": "Simultaneous estimation of contact position and tool shape with high-dimensional parameters using force measurements and particle filtering", "categories": ["cs.RO"], "comment": "Accepted to The International Journal of Robotics Research (IJRR)", "summary": "Estimating the contact state between a grasped tool and the environment is\nessential for performing contact tasks such as assembly and object\nmanipulation. Force signals are valuable for estimating the contact state, as\nthey can be utilized even when the contact location is obscured by the tool.\nPrevious studies proposed methods for estimating contact positions using\nforce/torque signals; however, most methods require the geometry of the tool\nsurface to be known. Although several studies have proposed methods that do not\nrequire the tool shape, these methods require considerable time for estimation\nor are limited to tools with low-dimensional shape parameters. Here, we propose\na method for simultaneously estimating the contact position and tool shape,\nwhere the tool shape is represented by a grid, which is high-dimensional (more\nthan 1000 dimensional). The proposed method uses a particle filter in which\neach particle has individual tool shape parameters, thereby to avoid directly\nhandling a high-dimensional parameter space. The proposed method is evaluated\nthrough simulations and experiments using tools with curved shapes on a plane.\nConsequently, the proposed method can estimate the shape of the tool\nsimultaneously with the contact positions, making the contact-position\nestimation more accurate.", "AI": {"tldr": "The paper introduces a method for estimating both the contact position and tool shape during contact tasks, leveraging a particle filter to manage high-dimensional tool parameters.", "motivation": "Traditional methods for contact position estimation often require pre-known tool geometry or are limited to specific tool shapes, leading to inefficiencies in diverse scenarios.", "method": "The authors utilize a particle filter to estimate high-dimensional tool shapes and contact positions simultaneously, avoiding direct handling of complex parameter spaces.", "result": "The proposed method is validated through simulations and experiments, demonstrating improved accuracy in estimating contact positions while also determining tool shapes.", "conclusion": "The technique advances contact state estimation, enabling more accurate and simultaneous determination of contact positions and tool shapes without prior knowledge of tool geometry."}}
{"id": "2509.19778", "pdf": "https://arxiv.org/pdf/2509.19778", "abs": "https://arxiv.org/abs/2509.19778", "authors": ["Hartmut H\u00e4ntze", "Myrthe Buser", "Alessa Hering", "Lisa C. Adams", "Keno K. Bressem"], "title": "Sex-based Bias Inherent in the Dice Similarity Coefficient: A Model Independent Analysis for Multiple Anatomical Structures", "categories": ["cs.CV", "J.3"], "comment": null, "summary": "Overlap-based metrics such as the Dice Similarity Coefficient (DSC) penalize\nsegmentation errors more heavily in smaller structures. As organ size differs\nby sex, this implies that a segmentation error of equal magnitude may result in\nlower DSCs in women due to their smaller average organ volumes compared to men.\nWhile previous work has examined sex-based differences in models or datasets,\nno study has yet investigated the potential bias introduced by the DSC itself.\nThis study quantifies sex-based differences of the DSC and the normalized DSC\nin an idealized setting independent of specific models. We applied\nequally-sized synthetic errors to manual MRI annotations from 50 participants\nto ensure sex-based comparability. Even minimal errors (e.g., a 1 mm boundary\nshift) produced systematic DSC differences between sexes. For small structures,\naverage DSC differences were around 0.03; for medium-sized structures around\n0.01. Only large structures (i.e., lungs and liver) were mostly unaffected,\nwith sex-based DSC differences close to zero. These findings underline that\nfairness studies using the DSC as an evaluation metric should not expect\nidentical scores between men and women, as the metric itself introduces bias. A\nsegmentation model may perform equally well across sexes in terms of error\nmagnitude, even if observed DSC values suggest otherwise. Importantly, our work\nraises awareness of a previously underexplored source of sex-based differences\nin segmentation performance. One that arises not from model behavior, but from\nthe metric itself. Recognizing this factor is essential for more accurate and\nfair evaluations in medical image analysis.", "AI": {"tldr": "The study reveals that the Dice Similarity Coefficient (DSC) introduces sex-based biases in segmentation evaluation, penalizing smaller structures more heavily, resulting in lower scores for women due to smaller organ sizes.", "motivation": "To investigate whether the Dice Similarity Coefficient (DSC), a common segmentation evaluation metric, introduces bias based on sex due to differences in organ sizes.", "method": "Synthetic errors were deliberately applied to manual MRI annotations from 50 participants to analyze segmentation performance differences in DSC scores between sexes under controlled conditions.", "result": "Systematic DSC differences were found between sexes, with notable disparities for smaller structures but negligible differences for larger organs. Small structures showed differences around 0.03, while medium-sized structures exhibited differences around 0.01.", "conclusion": "The DSC metric itself introduces bias, and thus fairness analyses should account for this metric-induced bias in order to achieve more accurate and fair evaluations in medical image segmentation."}}
{"id": "2509.19625", "pdf": "https://arxiv.org/pdf/2509.19625", "abs": "https://arxiv.org/abs/2509.19625", "authors": ["Juan Manuel Perez", "Kevin Garcia", "Brooklyn Berry", "Dongjin Song", "Yifeng Gao"], "title": "Adaptive von Mises-Fisher Likelihood Loss for Supervised Deep Time Series Hashing", "categories": ["cs.LG"], "comment": "6 pages, 6 figures, Conference: ICMLA 2025", "summary": "Indexing time series by creating compact binary representations is a\nfundamental task in time series data mining. Recently, deep learning-based\nhashing methods have proven effective for indexing time series based on\nsemantic meaning rather than just raw similarity. The purpose of deep hashing\nis to map samples with the same semantic meaning to identical binary hash\ncodes, enabling more efficient search and retrieval. Unlike other supervised\nrepresentation learning methods, supervised deep hashing requires a\ndiscretization step to convert real-valued representations into binary codes,\nbut this can induce significant information loss. In this paper, we propose a\nvon Mises-Fisher (vMF) hashing loss. The proposed deep hashing model maps data\nto an M-dimensional hyperspherical space to effectively reduce information loss\nand models each data class as points following distinct vMF distributions. The\ndesigned loss aims to maximize the separation between each modeled vMF\ndistribution to provide a better way to maximize the margin between each\nsemantically different data sample. Experimental results show that our method\noutperforms existing baselines. The implementation is publicly available at\nhttps://github.com/jmpq97/vmf-hashing", "AI": {"tldr": "The paper presents a novel deep hashing method using von Mises-Fisher distributions for enhanced time series indexing. It maps data onto a hyperspherical space to reduce information loss and maximize separation between semantic classes.", "motivation": "Deep learning-based hashing methods aim to improve time series indexing by focusing on semantic meanings rather than raw similarity. Traditional methods face challenges due to the information loss during discretization.", "method": "The paper proposes a von Mises-Fisher (vMF) hashing loss, mapping data to a hyperspherical space while modeling semantic classes as distinct vMF distributions. The objective is to maximize the margin between classes.", "result": "Experimental findings illustrate that the vMF hashing model achieves superior performance compared to existing methods in efficiently indexing time series data.", "conclusion": "The method successfully addresses information loss in traditional deep hashing and demonstrates improved accuracy and efficiency in time series data mining, suggesting its potential for broader applications."}}
{"id": "2509.19306", "pdf": "https://arxiv.org/pdf/2509.19306", "abs": "https://arxiv.org/abs/2509.19306", "authors": ["Jingyi Wang", "Zhongyuan Zhao", "Qingtian Wang", "Zexu Li", "Yue Wang", "Tony Q. S. Quek"], "title": "A Federated Fine-Tuning Paradigm of Foundation Models in Heterogenous Wireless Networks", "categories": ["eess.SP", "cs.AI", "cs.IT", "cs.NI", "math.IT"], "comment": null, "summary": "Edge intelligence has emerged as a promising strategy to deliver low-latency\nand ubiquitous services for mobile devices. Recent advances in fine-tuning\nmechanisms of foundation models have enabled edge intelligence by integrating\nlow-rank adaptation (LoRA) with federated learning. However, in wireless\nnetworks, the device heterogeneity and resource constraints on edge devices\npose great threats to the performance of federated fine-tuning. To tackle these\nissues, we propose to optimize federated fine-tuning in heterogenous wireless\nnetworks via online learning. First, the framework of switching-based federated\nfine-tuning in wireless networks is provided. The edge devices switches to LoRA\nmodules dynamically for federated fine-tuning with base station to jointly\nmitigate the impact of device heterogeneity and transmission unreliability.\nSecond, a tractable upper bound on the inference risk gap is derived based on\ntheoretical analysis. To improve the generalization capability, we formulate a\nnon-convex mixed-integer programming problem with long-term constraints, and\ndecouple it into model switching, transmit power control, and bandwidth\nallocation subproblems. An online optimization algorithm is developed to solve\nthe problems with polynomial computational complexity. Finally, the simulation\nresults on the SST-2 and QNLI data sets demonstrate the performance gains in\ntest accuracy and energy efficiency.", "AI": {"tldr": "The paper addresses the performance issues in federated fine-tuning of foundation models in heterogeneous wireless networks. It proposes an online optimization framework to improve accuracy and energy efficiency using LoRA and federated learning techniques.", "motivation": "Device heterogeneity and resource constraints in wireless networks pose challenges to federated fine-tuning, necessitating strategies to maintain low latency and high performance.", "method": "A switching-based federated fine-tuning framework is introduced, involving LoRA modules, theoretical risk gap analysis, and optimization of model switching, power control, and bandwidth allocation via an online algorithm.", "result": "Simulations on SST-2 and QNLI datasets show improvements in test accuracy and energy efficiency, validating the proposed approach.", "conclusion": "The proposed framework effectively mitigates the challenges of device heterogeneity and resource constraints in wireless networks, enhancing the performance of federated fine-tuning in edge intelligence systems."}}
{"id": "2509.19476", "pdf": "https://arxiv.org/pdf/2509.19476", "abs": "https://arxiv.org/abs/2509.19476", "authors": ["Yutaro Sigris", "Andreas Waldis"], "title": "A Pipeline to Assess Merging Methods via Behavior and Internals", "categories": ["cs.CL"], "comment": "BlackboxNLP", "summary": "Merging methods combine the weights of multiple language models (LMs) to\nleverage their capacities, such as for domain adaptation. While existing\nstudies investigate merged models from a solely behavioral perspective, we\noffer the first comprehensive view by assessing and connecting their behavior\nand internals. We present a novel evaluation pipeline that first merges\nmultiple parent LMs, and then evaluates the merged models in comparison to the\ninitial ones based on their behavior on downstream tasks, like MMLU, and the\ninternal encoded linguistic competence. We showcase this pipeline by assessing\nthe merging of instruction fine-tuned with math- and code-adapted LMs from the\nQwen2.5 family. Our results show that merging methods impacts behavior and\ninternals differently. While the performance of merged models is typically\nbetween that of the two parent models, their encoded information about\nlinguistic phenomena, particularly in morphology and syntax, can surpass the\nparent models. Moreover, we find weak ranking correlation between this behavior\nand internal evaluation. With our pipeline and initial results, we emphasize\nthe need for more comprehensive evaluations of model merging methods to gain a\nfaithful understanding of their capabilities and reliability, beyond potential\nsuperficial behavioral advances.", "AI": {"tldr": "The paper introduces a novel evaluation pipeline to analyze both behavioral and internal aspects of merged language models (LMs), with findings that merging can enhance certain internal competencies beyond those of parent models.", "motivation": "Existing studies of model merging only focus on behavioral output, lacking insights into internal mechanisms and broader evaluations.", "method": "The paper merges language models and evaluates them across two dimensions: performance in downstream tasks and internal linguistic competence.", "result": "Merged models often achieve intermediate performance but can outperform parent models in linguistic competence, with weak correlation between behavior and internal evaluation.", "conclusion": "The findings highlight the necessity for evaluating model merging methods comprehensively to fully understand their capabilities, especially beyond surface-level behavioral improvements."}}
{"id": "2509.19734", "pdf": "https://arxiv.org/pdf/2509.19734", "abs": "https://arxiv.org/abs/2509.19734", "authors": ["Akshay Jaitly", "Jon Arrizabalaga", "Guanrui Li"], "title": "Trajectory Planning Using Safe Ellipsoidal Corridors as Projections of Orthogonal Trust Regions", "categories": ["cs.RO"], "comment": null, "summary": "Planning collision free trajectories in complex environments remains a core\nchallenge in robotics. Existing corridor based planners which rely on\ndecomposition of the free space into collision free subsets scale poorly with\nenvironmental complexity and require explicit allocations of time windows to\ntrajectory segments. We introduce a new trajectory parameterization that\nrepresents trajectories in a nonconvex collision free corridor as being in a\nconvex cartesian product of balls. This parameterization allows us to decouple\nproblem size from geometric complexity of the solution and naturally avoids\nexplicit time allocation by allowing trajectories to evolve continuously inside\nellipsoidal corridors. Building on this representation, we formulate the\nOrthogonal Trust Region Problem (Orth-TRP), a specialized convex program with\nseparable block constraints, and develop a solver that exploits this parallel\nstructure and the unique structure of each parallel subproblem for efficient\noptimization. Experiments on a quadrotor trajectory planning benchmark show\nthat our approach produces smoother trajectories and lower runtimes than\nstate-of-the-art corridor based planners, especially in highly complicated\nenvironments.", "AI": {"tldr": "The paper introduces a new approach to trajectory planning in robotics by representing trajectories in a convex Cartesian product of balls to address scalability and time allocation challenges in complex environments.", "motivation": "Existing planners for collision-free trajectory planning struggle with scaling in complex environments and require explicit time allocation for trajectory segments.", "method": "The paper parameterizes trajectories using convex Cartesian products of balls, avoiding explicit time allocations, and develops a specialized solver for the Orthogonal Trust Region Problem (Orth-TRP) to enable efficient optimization.", "result": "Their approach demonstrated smoother trajectories and lower runtimes compared to state-of-the-art planners, particularly in complex environments.", "conclusion": "The proposed method addresses scalability and time allocation issues effectively, with demonstrated efficiency and smoothness, making it promising for complex robotic environments."}}
{"id": "2509.19779", "pdf": "https://arxiv.org/pdf/2509.19779", "abs": "https://arxiv.org/abs/2509.19779", "authors": ["Yu-Shen Huang", "Tzu-Han Chen", "Cheng-Yen Hsiao", "Shaou-Gang Miaou"], "title": "EfficienT-HDR: An Efficient Transformer-Based Framework via Multi-Exposure Fusion for HDR Reconstruction", "categories": ["cs.CV"], "comment": "10 pages, 9 figures", "summary": "Achieving high-quality High Dynamic Range (HDR) imaging on\nresource-constrained edge devices is a critical challenge in computer vision,\nas its performance directly impacts downstream tasks such as intelligent\nsurveillance and autonomous driving. Multi-Exposure Fusion (MEF) is a\nmainstream technique to achieve this goal; however, existing methods generally\nface the dual bottlenecks of high computational costs and ghosting artifacts,\nhindering their widespread deployment. To this end, this study proposes a\nlight-weight Vision Transformer architecture designed explicitly for HDR\nreconstruction to overcome these limitations. This study is based on the\nContext-Aware Vision Transformer and begins by converting input images to the\nYCbCr color space to separate luminance and chrominance information. It then\nemploys an Intersection-Aware Adaptive Fusion (IAAF) module to suppress\nghosting effectively. To further achieve a light-weight design, we introduce\nInverted Residual Embedding (IRE), Dynamic Tanh (DyT), and propose Enhanced\nMulti-Scale Dilated Convolution (E-MSDC) to reduce computational complexity at\nmultiple levels. Our study ultimately contributes two model versions: a main\nversion for high visual quality and a light-weight version with advantages in\ncomputational efficiency, both of which achieve an excellent balance between\nperformance and image quality. Experimental results demonstrate that, compared\nto the baseline, the main version reduces FLOPS by approximately 67% and\nincreases inference speed by more than fivefold on CPU and 2.5 times on an edge\ndevice. These results confirm that our method provides an efficient and\nghost-free HDR imaging solution for edge devices, demonstrating versatility and\npracticality across various dynamic scenarios.", "AI": {"tldr": "The paper introduces a lightweight Vision Transformer for HDR image reconstruction, reducing computational demand and avoiding artifacts, tailored for resource-constrained edge devices.", "motivation": "To address the challenges of high computational costs and ghosting artifacts in HDR imaging for edge devices, which impact applications like autonomous driving and surveillance.", "method": "The authors propose a Vision Transformer using Context-Aware processing, color-space conversion, and innovative tools like IAAF, IRE, DyT, and E-MSDC to enhance efficiency and image quality.", "result": "The proposed models reduce computational costs significantly\u201467% fewer FLOPS, up to 5x speedup on CPU, and 2.5x improvement on edge devices\u2014while maintaining high-quality HDR imaging.", "conclusion": "The study successfully achieves efficient, ghost-free HDR imaging suitable for various dynamic scenarios on edge devices, proving practical and versatile."}}
{"id": "2509.19312", "pdf": "https://arxiv.org/pdf/2509.19312", "abs": "https://arxiv.org/abs/2509.19312", "authors": ["Minghui Wu", "Zhen Gao"], "title": "E2E Learning Massive MIMO for Multimodal Semantic Non-Orthogonal Transmission and Fusion", "categories": ["eess.SP", "cs.AI", "cs.IT", "cs.LG", "math.IT"], "comment": null, "summary": "Massive multiple-input multiple-output (MIMO) promises high spectral\nefficiency but also leads to high-dimensional downlink channel state\ninformation (CSI), which complicates real-time channel acquisition and\nprecoding. To address this, we propose an end-to-end (E2E) uplink-downlink CSI\nfusion precoding network that jointly models downlink CSI reference signal\n(CSI-RS) design, CSI feedback, and base-station (BS) precoding within a single\nE2E neural architecture. Concretely, a projection network built on the MAXIM\narchitecture takes uplink sounding reference signals (SRS) as input and outputs\nfrequency-, beam-, and port-domain projection matrices for designing downlink\nCSI-RS. User equipment (UE) then compresses/quantizes the resulting CSI-RS\nobservations and feeds back a compact representation. At the base station (BS),\ntwo complementary branches produce candidate precoders: one is a feedback-only\nprecoding network driven by quantized downlink observations, and the other is\nan SRS-only precoding network driven by uplink SRS. These candidate precoders\nare subsequently combined by a fusion precoding network to yield the final\ntransmit precoder. All the modules are trained with a\nspectral-efficiency-oriented loss under a three-stage schedule. Simulation\nresults show that the proposed approach effectively harnesses both SRS-derived\ninformation and UE feedback, achieving markedly better performance than\nconventional baselines.", "AI": {"tldr": "The paper presents an end-to-end uplink-downlink CSI fusion precoding network to enhance efficiency in massive MIMO systems.", "motivation": "Address the challenge of high-dimensional downlink CSI that complicates real-time channel acquisition and precoding in massive MIMO systems.", "method": "The authors propose a neural architecture involving a projection network for CSI-RS design, UE-based compression/quantization, and a fusion precoding network at the BS combining feedback-driven and SRS-driven precoders.", "result": "Simulation results demonstrate that the proposed approach outperforms traditional methods by leveraging SRS-derived data and UE feedback.", "conclusion": "The integrated E2E framework optimizes precoding in massive MIMO, leading to superior spectral efficiency compared to conventional techniques."}}
{"id": "2509.19540", "pdf": "https://arxiv.org/pdf/2509.19540", "abs": "https://arxiv.org/abs/2509.19540", "authors": ["Jayanth Krishna Chundru", "Rudrashis Poddar", "Jie Cao", "Tianyu Jiang"], "title": "Do LLMs Encode Frame Semantics? Evidence from Frame Identification", "categories": ["cs.CL"], "comment": null, "summary": "We investigate whether large language models encode latent knowledge of frame\nsemantics, focusing on frame identification, a core challenge in frame semantic\nparsing that involves selecting the appropriate semantic frame for a target\nword in context. Using the FrameNet lexical resource, we evaluate models under\nprompt-based inference and observe that they can perform frame identification\neffectively even without explicit supervision. To assess the impact of\ntask-specific training, we fine-tune the model on FrameNet data, which\nsubstantially improves in-domain accuracy while generalizing well to\nout-of-domain benchmarks. Further analysis shows that the models can generate\nsemantically coherent frame definitions, highlighting the model's internalized\nunderstanding of frame semantics.", "AI": {"tldr": "The paper examines whether large language models inherently understand frame semantics and shows they can effectively perform frame identification tasks without specific training, with further fine-tuning boosting their performance.", "motivation": "The motivation is to investigate whether large language models have an inherent understanding of frame semantics, an essential aspect of natural language understanding.", "method": "The study uses FrameNet for evaluation through prompt-based inference, and fine-tunes models on FrameNet data to test the effects of task-specific training.", "result": "The models can perform frame identification effectively without explicit supervision, and fine-tuning significantly improves in-domain and out-of-domain performance. Additionally, the models can generate coherent frame definitions.", "conclusion": "Large language models encode latent frame-semantic knowledge that can be harnessed for frame identification tasks, with fine-tuning further enhancing capabilities and demonstrating an understanding of frame semantics."}}
{"id": "2509.19752", "pdf": "https://arxiv.org/pdf/2509.19752", "abs": "https://arxiv.org/abs/2509.19752", "authors": ["Rushuai Yang", "Hangxing Wei", "Ran Zhang", "Zhiyuan Feng", "Xiaoyu Chen", "Tong Li", "Chuheng Zhang", "Li Zhao", "Jiang Bian", "Xiu Su", "Yi Chen"], "title": "Beyond Human Demonstrations: Diffusion-Based Reinforcement Learning to Generate Data for VLA Training", "categories": ["cs.RO"], "comment": null, "summary": "Vision-language-action (VLA) models have shown strong generalization across\ntasks and embodiments; however, their reliance on large-scale human\ndemonstrations limits their scalability owing to the cost and effort of manual\ndata collection. Reinforcement learning (RL) offers a potential alternative to\ngenerate demonstrations autonomously, yet conventional RL algorithms often\nstruggle on long-horizon manipulation tasks with sparse rewards. In this paper,\nwe propose a modified diffusion policy optimization algorithm to generate\nhigh-quality and low-variance trajectories, which contributes to a diffusion\nRL-powered VLA training pipeline. Our algorithm benefits from not only the high\nexpressiveness of diffusion models to explore complex and diverse behaviors but\nalso the implicit regularization of the iterative denoising process to yield\nsmooth and consistent demonstrations. We evaluate our approach on the LIBERO\nbenchmark, which includes 130 long-horizon manipulation tasks, and show that\nthe generated trajectories are smoother and more consistent than both human\ndemonstrations and those from standard Gaussian RL policies. Further, training\na VLA model exclusively on the diffusion RL-generated data achieves an average\nsuccess rate of 81.9%, which outperforms the model trained on human data by\n+5.3% and that on Gaussian RL-generated data by +12.6%. The results highlight\nour diffusion RL as an effective alternative for generating abundant,\nhigh-quality, and low-variance demonstrations for VLA models.", "AI": {"tldr": "The paper introduces a modified diffusion algorithm to enhance reinforcement learning (RL) for generating high-quality trajectories, improving vision-language-action (VLA) model training without expensive human demonstration data.", "motivation": "The high cost and effort of acquiring large-scale human demonstration data for VLA models limit their scalability, necessitating an autonomous alternative for generating demonstrations.", "method": "The authors propose a modified diffusion policy optimization algorithm that leverages the high expressiveness of diffusion models and iterative denoising to generate high-quality, smooth, and diverse trajectories.", "result": "Using the LIBERO benchmark with 130 long-horizon manipulation tasks, the diffusion-based RL-generated trajectories outperformed human and Gaussian RL-generated data in smoothness and quality, achieving an 81.9% success rate in VLA training.", "conclusion": "Diffusion RL offers a scalable and effective alternative to human and traditional RL demonstration data, enabling high-quality training for VLA models with superior results."}}
{"id": "2509.19793", "pdf": "https://arxiv.org/pdf/2509.19793", "abs": "https://arxiv.org/abs/2509.19793", "authors": ["Yixun Zhang", "Feng Zhou", "Jianqin Yin"], "title": "BiTAA: A Bi-Task Adversarial Attack for Object Detection and Depth Estimation via 3D Gaussian Splatting", "categories": ["cs.CV"], "comment": "Intend to submit to RA-L", "summary": "Camera-based perception is critical to autonomous driving yet remains\nvulnerable to task-specific adversarial manipulations in object detection and\nmonocular depth estimation. Most existing 2D/3D attacks are developed in task\nsilos, lack mechanisms to induce controllable depth bias, and offer no\nstandardized protocol to quantify cross-task transfer, leaving the interaction\nbetween detection and depth underexplored. We present BiTAA, a bi-task\nadversarial attack built on 3D Gaussian Splatting that yields a single\nperturbation capable of simultaneously degrading detection and biasing\nmonocular depth. Specifically, we introduce a dual-model attack framework that\nsupports both full-image and patch settings and is compatible with common\ndetectors and depth estimators, with optional expectation-over-transformation\n(EOT) for physical reality. In addition, we design a composite loss that\ncouples detection suppression with a signed, magnitude-controlled log-depth\nbias within regions of interest (ROIs) enabling controllable near or far\nmisperception while maintaining stable optimization across tasks. We also\npropose a unified evaluation protocol with cross-task transfer metrics and\nreal-world evaluations, showing consistent cross-task degradation and a clear\nasymmetry between Det to Depth and from Depth to Det transfer. The results\nhighlight practical risks for multi-task camera-only perception and motivate\ncross-task-aware defenses in autonomous driving scenarios.", "AI": {"tldr": "This paper introduces BiTAA, a dual-task adversarial attack that disrupts object detection and biases monocular depth estimation using 3D Gaussian Splatting.", "motivation": "Current adversarial attacks for autonomous driving are task-specific, lack depth bias control, and fail to explore cross-task interactions between object detection and depth estimation.", "method": "BiTAA employs a dual-model attack framework, a composite loss function, and supports both full-image and patch-based perturbations. It also introduces a unified evaluation protocol for cross-task transfer metrics.", "result": "Experiments demonstrate consistent cross-task degradation, effective depth misperception control, and asymmetry in transfer from detection to depth compared to depth to detection.", "conclusion": "This work identifies practical risks in multi-task camera-based perception systems and underscores the need for cross-task-aware defenses in autonomous driving scenarios."}}
{"id": "2509.19638", "pdf": "https://arxiv.org/pdf/2509.19638", "abs": "https://arxiv.org/abs/2509.19638", "authors": ["MohammadReza EskandariNasab", "Shah Muhammad Hamdi", "Soukaina Filali Boubrahimi"], "title": "TIMED: Adversarial and Autoregressive Refinement of Diffusion-Based Time Series Generation", "categories": ["cs.LG", "cs.CV"], "comment": "Accepted to the IEEE International Conference on Data Mining (ICDM)\n  2025", "summary": "Generating high-quality synthetic time series is a fundamental yet\nchallenging task across domains such as forecasting and anomaly detection,\nwhere real data can be scarce, noisy, or costly to collect. Unlike static data\ngeneration, synthesizing time series requires modeling both the marginal\ndistribution of observations and the conditional temporal dependencies that\ngovern sequential dynamics. We propose TIMED, a unified generative framework\nthat integrates a denoising diffusion probabilistic model (DDPM) to capture\nglobal structure via a forward-reverse diffusion process, a supervisor network\ntrained with teacher forcing to learn autoregressive dependencies through\nnext-step prediction, and a Wasserstein critic that provides adversarial\nfeedback to ensure temporal smoothness and fidelity. To further align the real\nand synthetic distributions in feature space, TIMED incorporates a Maximum Mean\nDiscrepancy (MMD) loss, promoting both diversity and sample quality. All\ncomponents are built using masked attention architectures optimized for\nsequence modeling and are trained jointly to effectively capture both\nunconditional and conditional aspects of time series data. Experimental results\nacross diverse multivariate time series benchmarks demonstrate that TIMED\ngenerates more realistic and temporally coherent sequences than\nstate-of-the-art generative models.", "AI": {"tldr": "TIMED is a generative framework that improves synthetic time series generation using diffusion models, autoregressive supervision, adversarial training, and feature alignment methods.", "motivation": "The scarcity, noise, and cost associated with collecting real time series data make high-quality synthetic generation an essential and challenging task.", "method": "TIMED combines denoising diffusion probabilistic models, autoregressive networks, adversarial critics, and Maximum Mean Discrepancy (MMD) loss to jointly train masked attention architectures for modeling time series.", "result": "Experimental evaluations across various benchmarks show that TIMED surpasses state-of-the-art models in producing more realistic and temporally coherent synthetic time series.", "conclusion": "By leveraging diverse components tailored for both unconditional and conditional dynamics, TIMED effectively addresses challenges in synthetic time series generation."}}
{"id": "2509.19557", "pdf": "https://arxiv.org/pdf/2509.19557", "abs": "https://arxiv.org/abs/2509.19557", "authors": ["Iris Kamsteeg", "Juan Cardenas-Cartagena", "Floris van Beers", "Gineke ten Holt", "Tsegaye Misikir Tashu", "Matias Valdenegro-Toro"], "title": "Confidence Calibration in Large Language Model-Based Entity Matching", "categories": ["cs.CL", "cs.LG"], "comment": "9 pages, 2 figures. UncertaiNLP 2025 Workshop @ EMNLP Camera Ready", "summary": "This research aims to explore the intersection of Large Language Models and\nconfidence calibration in Entity Matching. To this end, we perform an empirical\nstudy to compare baseline RoBERTa confidences for an Entity Matching task\nagainst confidences that are calibrated using Temperature Scaling, Monte Carlo\nDropout and Ensembles. We use the Abt-Buy, DBLP-ACM, iTunes-Amazon and Company\ndatasets. The findings indicate that the proposed modified RoBERTa model\nexhibits a slight overconfidence, with Expected Calibration Error scores\nranging from 0.0043 to 0.0552 across datasets. We find that this overconfidence\ncan be mitigated using Temperature Scaling, reducing Expected Calibration Error\nscores by up to 23.83%.", "AI": {"tldr": "The study investigates confidence calibration methods for RoBERTa in Entity Matching tasks using datasets like Abt-Buy, DBLP-ACM, and others, suggesting Temperature Scaling as an effective mitigation.", "motivation": "Exploring confidence calibration in Large Language Models for accurate Entity Matching predictions.", "method": "Empirical comparison of RoBERTa baseline confidence to adjusted calibration methods: Temperature Scaling, Monte Carlo Dropout, and Ensembles across multiple datasets.", "result": "The modified RoBERTa model showed slight overconfidence, with Expected Calibration Error (ECE) scores between 0.0043 and 0.0552. Temperature Scaling significantly reduced ECE by up to 23.83%.", "conclusion": "Temperature Scaling proves effective at improving confidence calibration in Entity Matching tasks involving RoBERTa, offering a feasible solution to overconfidence issues."}}
{"id": "2509.19804", "pdf": "https://arxiv.org/pdf/2509.19804", "abs": "https://arxiv.org/abs/2509.19804", "authors": ["Sowoo Lee", "Dongyun Kang", "Jaehyun Park", "Hae-Won Park"], "title": "DynaFlow: Dynamics-embedded Flow Matching for Physically Consistent Motion Generation from State-only Demonstrations", "categories": ["cs.RO"], "comment": "8 pages", "summary": "This paper introduces DynaFlow, a novel framework that embeds a\ndifferentiable simulator directly into a flow matching model. By generating\ntrajectories in the action space and mapping them to dynamically feasible state\ntrajectories via the simulator, DynaFlow ensures all outputs are physically\nconsistent by construction. This end-to-end differentiable architecture enables\ntraining on state-only demonstrations, allowing the model to simultaneously\ngenerate physically consistent state trajectories while inferring the\nunderlying action sequences required to produce them. We demonstrate the\neffectiveness of our approach through quantitative evaluations and showcase its\nreal-world applicability by deploying the generated actions onto a physical Go1\nquadruped robot. The robot successfully reproduces diverse gait present in the\ndataset, executes long-horizon motions in open-loop control and translates\ninfeasible kinematic demonstrations into dynamically executable, stylistic\nbehaviors. These hardware experiments validate that DynaFlow produces\ndeployable, highly effective motions on real-world hardware from state-only\ndemonstrations, effectively bridging the gap between kinematic data and\nreal-world execution.", "AI": {"tldr": "DynaFlow is a framework integrating a differentiable simulator with a flow matching model to produce physically consistent state trajectories from state-only demonstrations, with real-world validation on a quadruped robot.", "motivation": "There is a need to bridge the gap between kinematic demonstrations and real-world executable motions, ensuring physical consistency in state trajectories and enabling better deployment of robotic motion learning from state-only datasets.", "method": "DynaFlow embeds a differentiable simulator into a flow matching model, enabling trajectory generation in action space that maps to dynamically feasible state trajectories. The framework learns end-to-end from state-only demonstrations.", "result": "DynaFlow demonstrates effectiveness through quantitative evaluations and physical deployment on a Go1 quadruped robot. The robot reproduces diverse dataset gaits, executes long-horizon motions, and converts infeasible kinematic demonstrations into feasible, stylistic behaviors.", "conclusion": "DynaFlow produces physically consistent, deployable motions directly from state-only demonstrations, providing an effective solution for connecting kinematic data with real-world robotic applications."}}
{"id": "2509.19805", "pdf": "https://arxiv.org/pdf/2509.19805", "abs": "https://arxiv.org/abs/2509.19805", "authors": ["Shantanusinh Parmar"], "title": "StrCGAN: A Generative Framework for Stellar Image Restoration", "categories": ["cs.CV", "astro-ph.IM", "astro-ph.SR"], "comment": null, "summary": "We introduce StrCGAN (Stellar Cyclic GAN), a generative model designed to\nenhance low-resolution astrophotography images. Our goal is to reconstruct\nhigh-fidelity ground truth-like representations of celestial objects, a task\nthat is challenging due to the limited resolution and quality of\nsmall-telescope observations such as the MobilTelesco dataset. Traditional\nmodels such as CycleGAN provide a foundation for image-to-image translation but\nare restricted to 2D mappings and often distort the morphology of stars and\ngalaxies. To overcome these limitations, we extend the CycleGAN framework with\nthree key innovations: 3D convolutional layers to capture volumetric spatial\ncorrelations, multi-spectral fusion to align optical and near-infrared (NIR)\ndomains, and astrophysical regularization modules to preserve stellar\nmorphology. Ground-truth references from multi-mission all-sky surveys spanning\noptical to NIR guide the training process, ensuring that reconstructions remain\nconsistent across spectral bands. Together, these components allow StrCGAN to\ngenerate reconstructions that are not only visually sharper but also physically\nconsistent, outperforming standard GAN models in the task of astrophysical\nimage enhancement.", "AI": {"tldr": "StrCGAN is a generative model improving astrophotography by enhancing low-resolution images using 3D convolutions, spectral fusion, and astrophysical regularization.", "motivation": "Low-resolution astrophotography struggles to represent celestial objects accurately due to small-telescope limitations and existing models like CycleGAN causing distortions.", "method": "StrCGAN innovates CycleGAN with 3D convolutional layers, multi-spectral fusion, and astrophysical regularization. Training uses multi-mission all-sky surveys for spectral consistency.", "result": "StrCGAN produces visually sharper and physically consistent reconstructions, outperforming standard GANs in astrophysical image enhancement.", "conclusion": "StrCGAN effectively addresses the limitations of traditional image-to-image translation for astrophotography, ensuring high-quality and accurate celestial reconstructions."}}
{"id": "2509.19648", "pdf": "https://arxiv.org/pdf/2509.19648", "abs": "https://arxiv.org/abs/2509.19648", "authors": ["Hongyi Chen", "Xiucheng Li", "Xinyang Chen", "Yun Cheng", "Jing Li", "Kehai Chen", "Liqiang Nie"], "title": "Toward Scalable and Structured Global Station Weather Forecasting", "categories": ["cs.LG", "physics.ao-ph"], "comment": null, "summary": "Global Station Weather Forecasting (GSWF) is a key meteorological research\narea, critical to energy, aviation, and agriculture. Existing time series\nforecasting methods often ignore or unidirectionally model spatial correlation\nwhen conducting large-scale global station forecasting. This contradicts the\nintrinsic nature underlying observations of the global weather system, limiting\nforecast performance. To address this, we propose a novel Spatial Structured\nAttention Block in this paper. It partitions the spatial graph into a set of\nsubgraphs and instantiates Intra-subgraph Attention to learn local spatial\ncorrelation within each subgraph, and aggregates nodes into subgraph\nrepresentations for message passing among the subgraphs via Inter-subgraph\nAttention -- considering both spatial proximity and global correlation.\nBuilding on this block, we develop a multiscale spatiotemporal forecasting\nmodel by progressively expanding subgraph scales. The resulting model is both\nscalable and able to produce structured spatial correlation, and meanwhile, it\nis easy to implement. The experimental results show that it can achieve\nperformance improvements up to 16.8% over time series forecasting baselines at\nlow running costs.", "AI": {"tldr": "This paper introduces a new model for weather forecasting that improves accuracy by addressing spatial correlations often ignored in traditional methods.", "motivation": "Existing global weather forecasting methods fail to capture the intrinsic spatial interconnections of the global weather system, which limits prediction accuracy.", "method": "The authors propose a Spatial Structured Attention Block that creates and processes subgraphs to model both local and global spatial correlations, and integrate it into a scalable multiscale spatiotemporal forecasting model.", "result": "The proposed model achieves up to a 16.8% improvement in performance compared to conventional forecasting methods, with low computational costs.", "conclusion": "The approach effectively balances scalability, structured spatial modeling, and implementation simplicity, making it a significant improvement over existing methods."}}
{"id": "2509.19315", "pdf": "https://arxiv.org/pdf/2509.19315", "abs": "https://arxiv.org/abs/2509.19315", "authors": ["Yiqiao Chen", "Zijian Huang", "Zhenghui Feng"], "title": "Advancing Few-Shot Pediatric Arrhythmia Classification with a Novel Contrastive Loss and Multimodal Learning", "categories": ["eess.SP", "cs.AI", "cs.LG"], "comment": "12pages, 10 figures", "summary": "Pediatric arrhythmias are a major risk factor for disability and sudden\ncardiac death, yet their automated classification remains challenging due to\nclass imbalance, few-shot categories, and complex signal characteristics, which\nseverely limit the efficiency and reliability of early screening and clinical\nintervention. To address this problem, we propose a multimodal end-to-end deep\nlearning framework that combines dual-branch convolutional encoders for ECG and\nIEGM, semantic attention for cross-modal feature alignment, and a lightweight\nTransformer encoder for global dependency modeling. In addition, we introduce a\nnew contrastive loss fucntion named Adaptive Global Class-Aware Contrastive\nLoss (AGCACL) to enhance intra-class compactness and inter-class separability\nthrough class prototypes and a global similarity matrix. To the best of our\nknowledge, this is the first systematic study based on the Leipzig Heart Center\npediatric/congenital ECG+IEGM dataset, for which we also provide a complete and\nreproducible preprocessing pipeline. Experimental results demonstrate that the\nproposed method achieves the overall best performance on this dataset,\nincluding 97.76\\% Top-1 Accuracy, 94.08\\% Macro Precision, 91.97\\% Macro\nRecall, 92.97\\% Macro F1, and 92.36\\% Macro F2, with improvements of +13.64,\n+15.96, +19.82, and +19.44 percentage points over the strongest baseline in\nMacro Precision/Recall/F1/F2, respectively. These findings indicate that the\nframework significantly improves the detectability and robustness for minority\narrhythmia classes, offering potential clinical value for rhythm screening,\npre-procedural assessment, and postoperative follow-up in pediatric and\ncongenital heart disease populations.", "AI": {"tldr": "This paper introduces a deep learning framework to improve detection and classification of pediatric arrhythmias using multimodal ECG and IEGM signals, achieving significant performance improvements.", "motivation": "Pediatric arrhythmias pose risks like sudden cardiac death but their automated classification faces challenges such as class imbalance and complex signal properties. Enhancing detection methods is crucial for early screening and effective clinical actions.", "method": "The framework combines dual-branch convolutional encoders for ECG and IEGM signals, semantic attention for cross-modal feature alignment, a lightweight Transformer encoder for global dependency modeling, and a novel Adaptive Global Class-Aware Contrastive Loss (AGCACL) for improved class representation.", "result": "The proposed framework achieved top-notch performance metrics like 97.76% Top-1 Accuracy and significant improvements in macro evaluation measures over the strongest baseline by up to +19.82 percentage points in Macro Recall.", "conclusion": "This framework enhances detectability and robustness for minority arrhythmia classes, offering significant potential for clinical applications in early diagnosis and management of pediatric heart disease."}}
{"id": "2509.19563", "pdf": "https://arxiv.org/pdf/2509.19563", "abs": "https://arxiv.org/abs/2509.19563", "authors": ["Stefania Radu", "Marco Zullich", "Matias Valdenegro-Toro"], "title": "Uncertainty in Semantic Language Modeling with PIXELS", "categories": ["cs.CL", "cs.LG"], "comment": "9 pages, 6 figures, UncertaiNLP 2025 Workshop @ EMNLP Camera Ready", "summary": "Pixel-based language models aim to solve the vocabulary bottleneck problem in\nlanguage modeling, but the challenge of uncertainty quantification remains\nopen. The novelty of this work consists of analysing uncertainty and confidence\nin pixel-based language models across 18 languages and 7 scripts, all part of 3\nsemantically challenging tasks. This is achieved through several methods such\nas Monte Carlo Dropout, Transformer Attention, and Ensemble Learning. The\nresults suggest that pixel-based models underestimate uncertainty when\nreconstructing patches. The uncertainty is also influenced by the script, with\nLatin languages displaying lower uncertainty. The findings on ensemble learning\nshow better performance when applying hyperparameter tuning during the named\nentity recognition and question-answering tasks across 16 languages.", "AI": {"tldr": "The paper analyzes uncertainty and confidence in pixel-based language models across multiple languages and scripts using methods like Monte Carlo Dropout, Transformer Attention, and Ensemble Learning.", "motivation": "The goal is to address the challenge of uncertainty quantification in pixel-based language models, which aim to overcome the vocabulary bottleneck problem in language modeling.", "method": "The study uses approaches such as Monte Carlo Dropout, Transformer Attention, and Ensemble Learning to analyze uncertainty in semantically demanding tasks across 18 languages and 7 scripts.", "result": "Pixel-based models underestimate uncertainty during patch reconstruction, while the script influences uncertainty level\u2014Latin languages show lower uncertainty. Ensemble learning with hyperparameter tuning improves performance in named entity recognition and question-answering tasks across 16 languages.", "conclusion": "The findings underscore the importance of proper uncertainty quantification in pixel-based language models and demonstrate the script-specific variations in uncertainty, paving the way for improved language modeling techniques."}}
{"id": "2509.19851", "pdf": "https://arxiv.org/pdf/2509.19851", "abs": "https://arxiv.org/abs/2509.19851", "authors": ["Benjamin Bogenberger", "Oliver Harrison", "Orrin Dahanaggamaarachchi", "Lukas Brunke", "Jingxing Qian", "Siqi Zhou", "Angela P. Schoellig"], "title": "Where Did I Leave My Glasses? Open-Vocabulary Semantic Exploration in Real-World Semi-Static Environments", "categories": ["cs.RO"], "comment": null, "summary": "Robots deployed in real-world environments, such as homes, must not only\nnavigate safely but also understand their surroundings and adapt to environment\nchanges. To perform tasks efficiently, they must build and maintain a semantic\nmap that accurately reflects the current state of the environment. Existing\nresearch on semantic exploration largely focuses on static scenes without\npersistent object-level instance tracking. A consistent map is, however,\ncrucial for real-world robotic applications where objects in the environment\ncan be removed, reintroduced, or shifted over time. In this work, to close this\ngap, we propose an open-vocabulary, semantic exploration system for semi-static\nenvironments. Our system maintains a consistent map by building a probabilistic\nmodel of object instance stationarity, systematically tracking semi-static\nchanges, and actively exploring areas that have not been visited for a\nprolonged period of time. In addition to active map maintenance, our approach\nleverages the map's semantic richness with LLM-based reasoning for\nopen-vocabulary object-goal navigation. This enables the robot to search more\nefficiently by prioritizing contextually relevant areas. We evaluate our\napproach across multiple real-world semi-static environments. Our system\ndetects 95% of map changes on average, improving efficiency by more than 29% as\ncompared to random and patrol baselines. Overall, our approach achieves a\nmapping precision within 2% of a fully rebuilt map while requiring\nsubstantially less exploration and further completes object goal navigation\ntasks about 14% faster than the next-best tested strategy (coverage\npatrolling). A video of our work can be found at\nhttp://tiny.cc/sem-explor-semi-static .", "AI": {"tldr": "The paper introduces an innovative semantic exploration system for robots in semi-static environments, aiming to adapt to changes and enable efficient object-goal navigation.", "motivation": "To address the lack of persistent object-level instance tracking in existing semantic exploration research, which limits robots\u2019 ability to adapt to semi-static environmental changes.", "method": "The system builds a probabilistic model for object stationarity, actively explores areas with potential changes, and integrates semantic richness with LLM-based reasoning for efficient map maintenance and navigation.", "result": "The system detects 95% of map changes, achieves near-perfect mapping precision (within 2% of a fully rebuilt map), improves exploration efficiency by over 29%, and completes object-goal navigation 14% faster than competing strategies.", "conclusion": "The proposed system significantly enhances robot adaptability in semi-static environments through systematic mapping, active exploration, and semantic reasoning, making it more practical for real-world applications."}}
{"id": "2509.19819", "pdf": "https://arxiv.org/pdf/2509.19819", "abs": "https://arxiv.org/abs/2509.19819", "authors": ["Yuchuan Mao", "Zhi Gao", "Xiaomeng Fan", "Yuwei Wu", "Yunde Jia", "Chenchen Jing"], "title": "Adaptive Model Ensemble for Continual Learning", "categories": ["cs.CV"], "comment": null, "summary": "Model ensemble is an effective strategy in continual learning, which\nalleviates catastrophic forgetting by interpolating model parameters, achieving\nknowledge fusion learned from different tasks. However, existing model ensemble\nmethods usually encounter the knowledge conflict issue at task and layer\nlevels, causing compromised learning performance in both old and new tasks. To\nsolve this issue, we propose meta-weight-ensembler that adaptively fuses\nknowledge of different tasks for continual learning. Concretely, we employ a\nmixing coefficient generator trained via meta-learning to generate appropriate\nmixing coefficients for model ensemble to address the task-level knowledge\nconflict. The mixing coefficient is individually generated for each layer to\naddress the layer-level knowledge conflict. In this way, we learn the prior\nknowledge about adaptively accumulating knowledge of different tasks in a fused\nmodel, achieving efficient learning in both old and new tasks.\nMeta-weight-ensembler can be flexibly combined with existing continual learning\nmethods to boost their ability of alleviating catastrophic forgetting.\nExperiments on multiple continual learning datasets show that\nmeta-weight-ensembler effectively alleviates catastrophic forgetting and\nachieves state-of-the-art performance.", "AI": {"tldr": "The paper presents Meta-weight-ensembler, a meta-learning-based model ensemble approach to address task- and layer-level knowledge conflicts in continual learning, achieving state-of-the-art performance.", "motivation": "To handle catastrophic forgetting in continual learning by resolving task- and layer-level knowledge conflicts during model ensemble.", "method": "Proposes Meta-weight-ensembler, which uses a meta-learning-trained mixing coefficient generator. The generator adapts layer-specific coefficients for model ensemble, enabling efficient integration of knowledge from different tasks.", "result": "Meta-weight-ensembler outperforms existing approaches by effectively alleviating catastrophic forgetting and improving performance on multiple continual learning datasets.", "conclusion": "Meta-weight-ensembler is a versatile approach that can be integrated into existing continual learning frameworks, offering better task performance and knowledge retention."}}
{"id": "2509.19654", "pdf": "https://arxiv.org/pdf/2509.19654", "abs": "https://arxiv.org/abs/2509.19654", "authors": ["Kevin Garcia", "Cassandra Garza", "Brooklyn Berry", "Yifeng Gao"], "title": "Symbol-Temporal Consistency Self-supervised Learning for Robust Time Series Classification", "categories": ["cs.LG", "I.2.6"], "comment": "4 pages, 2 figures, IEEE-EMBS BSN 2025", "summary": "The surge in the significance of time series in digital health domains\nnecessitates advanced methodologies for extracting meaningful patterns and\nrepresentations. Self-supervised contrastive learning has emerged as a\npromising approach for learning directly from raw data. However, time series\ndata in digital health is known to be highly noisy, inherently involves concept\ndrifting, and poses a challenge for training a generalizable deep learning\nmodel. In this paper, we specifically focus on data distribution shift caused\nby different human behaviors and propose a self-supervised learning framework\nthat is aware of the bag-of-symbol representation. The bag-of-symbol\nrepresentation is known for its insensitivity to data warping, location shifts,\nand noise existed in time series data, making it potentially pivotal in guiding\ndeep learning to acquire a representation resistant to such data shifting. We\ndemonstrate that the proposed method can achieve significantly better\nperformance where significant data shifting exists.", "AI": {"tldr": "This paper introduces a self-supervised learning framework leveraging bag-of-symbol representation to enhance pattern recognition in noisy time series data within digital health. Results show its effectiveness under data distribution shifts.", "motivation": "Time series data in digital health is often noisy and subject to concept drift, posing challenges for building generalizable deep learning models.", "method": "The authors propose a self-supervised learning framework that incorporates bag-of-symbol representation, known for its resistance to data warping, location shifts, and noise.", "result": "The framework significantly improves performance in scenarios with considerable data distribution shifts.", "conclusion": "Bag-of-symbol representation aids deep learning in acquiring robust representations, paving the way for more reliable time series analysis in digital health domains."}}
{"id": "2509.19567", "pdf": "https://arxiv.org/pdf/2509.19567", "abs": "https://arxiv.org/abs/2509.19567", "authors": ["Dimitrios Siskos", "Stavros Papadopoulos", "Pablo Peso Parada", "Jisi Zhang", "Karthikeyan Saravanan", "Anastasios Drosou"], "title": "Retrieval Augmented Generation based context discovery for ASR", "categories": ["cs.CL", "eess.AS"], "comment": "Accepted at EMNLP 2025", "summary": "This work investigates retrieval augmented generation as an efficient\nstrategy for automatic context discovery in context-aware Automatic Speech\nRecognition (ASR) system, in order to improve transcription accuracy in the\npresence of rare or out-of-vocabulary terms. However, identifying the right\ncontext automatically remains an open challenge. This work proposes an\nefficient embedding-based retrieval approach for automatic context discovery in\nASR. To contextualize its effectiveness, two alternatives based on large\nlanguage models (LLMs) are also evaluated: (1) large language model (LLM)-based\ncontext generation via prompting, and (2) post-recognition transcript\ncorrection using LLMs. Experiments on the TED-LIUMv3, Earnings21 and SPGISpeech\ndemonstrate that the proposed approach reduces WER by up to 17% (percentage\ndifference) relative to using no-context, while the oracle context results in a\nreduction of up to 24.1%.", "AI": {"tldr": "The paper explores using retrieval augmented generation for automatic context discovery in ASR to address rare/out-of-vocabulary terms, achieving significant WER reductions.", "motivation": "To improve transcription accuracy in ASR systems, particularly in scenarios involving rare or out-of-vocabulary terms, by automating the context identification process.", "method": "The paper proposes an efficient embedding-based retrieval approach for automatic context discovery, comparing it with two LLM-based alternatives: context generation via prompting and post-recognition transcript correction.", "result": "The proposed approach achieved up to a 17% relative reduction in WER, and up to 24.1% when using oracle context, across experiments on datasets like TED-LIUMv3, Earnings21, and SPGISpeech.", "conclusion": "Automatic context discovery via embedding-based retrieval is an effective way to enhance ASR performance, comparing favorably with LLM-based alternatives and demonstrating substantial accuracy improvements."}}
{"id": "2509.19853", "pdf": "https://arxiv.org/pdf/2509.19853", "abs": "https://arxiv.org/abs/2509.19853", "authors": ["BinXu Wu", "TengFei Zhang", "Chen Yang", "JiaHao Wen", "HaoCheng Li", "JingTian Ma", "Zhen Chen", "JingYuan Wang"], "title": "SAGE:State-Aware Guided End-to-End Policy for Multi-Stage Sequential Tasks via Hidden Markov Decision Process", "categories": ["cs.RO"], "comment": null, "summary": "Multi-stage sequential (MSS) robotic manipulation tasks are prevalent and\ncrucial in robotics. They often involve state ambiguity, where visually similar\nobservations correspond to different actions. We present SAGE, a state-aware\nguided imitation learning framework that models tasks as a Hidden Markov\nDecision Process (HMDP) to explicitly capture latent task stages and resolve\nambiguity. We instantiate the HMDP with a state transition network that infers\nhidden states, and a state-aware action policy that conditions on both\nobservations and hidden states to produce actions, thereby enabling\ndisambiguation across task stages. To reduce manual annotation effort, we\npropose a semi-automatic labeling pipeline combining active learning and soft\nlabel interpolation. In real-world experiments across multiple complex MSS\ntasks with state ambiguity, SAGE achieved 100% task success under the standard\nevaluation protocol, markedly surpassing the baselines. Ablation studies\nfurther show that such performance can be maintained with manual labeling for\nonly about 13% of the states, indicating its strong effectiveness.", "AI": {"tldr": "The paper introduces SAGE, an imitation learning framework enhancing robotic manipulation tasks by addressing state ambiguities. It achieves significant success in real-world experiments with minimal manual labeling.", "motivation": "Robotic manipulation tasks frequently encounter state ambiguity, where similar observations can lead to incorrect actions, necessitating solutions for improved task performance.", "method": "The paper models tasks using a Hidden Markov Decision Process (HMDP) and introduces a state transition network for hidden state inference combined with a state-aware action policy to produce accurate actions.", "result": "SAGE achieved 100% success in real-world experiments for complex robotic tasks, outclassing baseline models, and demonstrated significant efficiency requiring minimal manual labeling (13%).", "conclusion": "SAGE effectively resolves state ambiguity in sequential robotic manipulation tasks, demonstrating strong performance and requiring minimal annotation effort."}}
{"id": "2509.19841", "pdf": "https://arxiv.org/pdf/2509.19841", "abs": "https://arxiv.org/abs/2509.19841", "authors": ["Tai-Ming Huang", "Wei-Tung Lin", "Kai-Lung Hua", "Wen-Huang Cheng", "Junichi Yamagishi", "Jun-Cheng Chen"], "title": "ThinkFake: Reasoning in Multimodal Large Language Models for AI-Generated Image Detection", "categories": ["cs.CV"], "comment": null, "summary": "The increasing realism of AI-generated images has raised serious concerns\nabout misinformation and privacy violations, highlighting the urgent need for\naccurate and interpretable detection methods. While existing approaches have\nmade progress, most rely on binary classification without explanations or\ndepend heavily on supervised fine-tuning, resulting in limited generalization.\nIn this paper, we propose ThinkFake, a novel reasoning-based and generalizable\nframework for AI-generated image detection. Our method leverages a Multimodal\nLarge Language Model (MLLM) equipped with a forgery reasoning prompt and is\ntrained using Group Relative Policy Optimization (GRPO) reinforcement learning\nwith carefully designed reward functions. This design enables the model to\nperform step-by-step reasoning and produce interpretable, structured outputs.\nWe further introduce a structured detection pipeline to enhance reasoning\nquality and adaptability. Extensive experiments show that ThinkFake outperforms\nstate-of-the-art methods on the GenImage benchmark and demonstrates strong\nzero-shot generalization on the challenging LOKI benchmark. These results\nvalidate our framework's effectiveness and robustness. Code will be released\nupon acceptance.", "AI": {"tldr": "The paper introduces ThinkFake, a novel method for detecting AI-generated images that emphasizes reasoning and interpretability, outperforming existing methods and offering better generalization.", "motivation": "The realistic nature of AI-generated images has raised concerns about misinformation and privacy, necessitating more accurate, interpretable, and generalizable detection methods.", "method": "ThinkFake employs a Multimodal Large Language Model (MLLM) with a forgery reasoning prompt, trained using Group Relative Policy Optimization (GRPO) reinforcement learning and structured reward functions. It incorporates a structured detection pipeline for enhanced reasoning and adaptability.", "result": "ThinkFake surpasses state-of-the-art methods on the GenImage benchmark and demonstrates robust zero-shot generalization on the LOKI benchmark.", "conclusion": "The proposed ThinkFake method provides a highly effective, interpretable, and generalizable solution to identifying AI-generated images, with plans for open code release."}}
{"id": "2509.19661", "pdf": "https://arxiv.org/pdf/2509.19661", "abs": "https://arxiv.org/abs/2509.19661", "authors": ["Puning Zhao", "Zhikun Zhang", "Bo Sun", "Li Shen", "Liang Zhang", "Shaowei Wang", "Zhe Liu"], "title": "Consistent Estimation of Numerical Distributions under Local Differential Privacy by Wavelet Expansion", "categories": ["cs.LG"], "comment": null, "summary": "Distribution estimation under local differential privacy (LDP) is a\nfundamental and challenging task. Significant progresses have been made on\ncategorical data. However, due to different evaluation metrics, these methods\ndo not work well when transferred to numerical data. In particular, we need to\nprevent the probability mass from being misplaced far away. In this paper, we\npropose a new approach that express the sample distribution using wavelet\nexpansions. The coefficients of wavelet series are estimated under LDP. Our\nmethod prioritizes the estimation of low-order coefficients, in order to ensure\naccurate estimation at macroscopic level. Therefore, the probability mass is\nprevented from being misplaced too far away from its ground truth. We establish\ntheoretical guarantees for our methods. Experiments show that our wavelet\nexpansion method significantly outperforms existing solutions under Wasserstein\nand KS distances.", "AI": {"tldr": "The paper introduces a method to estimate numerical data distributions under Local Differential Privacy (LDP) using wavelet expansions, focusing on improving accuracy at a macroscopic level.", "motivation": "Existing methods for LDP distribution estimation work well for categorical data but perform poorly on numerical data, as they fail to prevent probability mass from being misplaced far from the true values.", "method": "The authors propose estimating the coefficients of wavelet series under LDP, prioritizing the estimation of low-order coefficients to enhance macroscopic accuracy of distribution.", "result": "Theoretical guarantees are established for the proposed method, and experiments indicate that it significantly outperforms existing approaches using Wasserstein and KS distances as evaluation metrics.", "conclusion": "Wavelet expansions provide a more accurate and effective approach for estimating numerical distributions under LDP compared to existing methods, ensuring better alignment with the ground truth."}}
{"id": "2509.19569", "pdf": "https://arxiv.org/pdf/2509.19569", "abs": "https://arxiv.org/abs/2509.19569", "authors": ["Aleksis Datseris", "Sylvia Vassileva", "Ivan Koychev", "Svetla Boytcheva"], "title": "ExPe: Exact Positional Encodings for Generative Transformer Models with Extrapolating Capabilities", "categories": ["cs.CL"], "comment": null, "summary": "This paper introduces a novel approach to position embeddings in transformer\nmodels, named \"Exact Positional Embeddings\" (ExPE). An absolute positional\nembedding method that can extrapolate to sequences of lengths longer than the\nones it was trained on. Traditional transformer models rely on absolute or\nrelative position embeddings to incorporate positional information into token\nembeddings, which often struggle with extrapolation to sequences longer than\nthose seen during training. Our proposed method utilizes a novel embedding\nstrategy that encodes exact positional information by overriding specific\ndimensions of the embedding vectors, thereby enabling a more precise\nrepresentation of token positions. The proposed approach not only maintains the\nintegrity of the original embeddings but also enhances the model's ability to\ngeneralize to more extended sequences. In causal language modeling, our ExPE\nembeddings significantly reduce perplexity compared to rotary and sinusoidal\nembeddings, when tested on sequences longer than those used in training.", "AI": {"tldr": "The paper introduces Exact Positional Embeddings (ExPE), a novel method for handling positional embeddings in transformer models to better extrapolate to longer sequences.", "motivation": "To address the limitations of existing absolute and relative position embeddings in transformer models, which struggle to generalize to sequence lengths beyond what they were trained on.", "method": "A new embedding strategy called ExPE that encodes exact positional information by overriding specific dimensions of the embedding vectors, enabling better position representation and generalization to longer sequences.", "result": "ExPE significantly reduces perplexity in causal language modeling tasks on sequences longer than those seen during training, outperforming rotary and sinusoidal embeddings.", "conclusion": "ExPE improves the ability of transformer models to generalize to longer sequences while maintaining embedding integrity, demonstrating its effectiveness over traditional methods."}}
{"id": "2509.19892", "pdf": "https://arxiv.org/pdf/2509.19892", "abs": "https://arxiv.org/abs/2509.19892", "authors": ["Keyu Wang", "Bingcong Lu", "Zhengxue Cheng", "Hengdi Zhang", "Li Song"], "title": "D3Grasp: Diverse and Deformable Dexterous Grasping for General Objects", "categories": ["cs.RO"], "comment": null, "summary": "Achieving diverse and stable dexterous grasping for general and deformable\nobjects remains a fundamental challenge in robotics, due to high-dimensional\naction spaces and uncertainty in perception. In this paper, we present D3Grasp,\na multimodal perception-guided reinforcement learning framework designed to\nenable Diverse and Deformable Dexterous Grasping. We firstly introduce a\nunified multimodal representation that integrates visual and tactile perception\nto robustly grasp common objects with diverse properties. Second, we propose an\nasymmetric reinforcement learning architecture that exploits privileged\ninformation during training while preserving deployment realism, enhancing both\ngeneralization and sample efficiency. Third, we meticulously design a training\nstrategy to synthesize contact-rich, penetration-free, and kinematically\nfeasible grasps with enhanced adaptability to deformable and contact-sensitive\nobjects. Extensive evaluations confirm that D3Grasp delivers highly robust\nperformance across large-scale and diverse object categories, and substantially\nadvances the state of the art in dexterous grasping for deformable and\ncompliant objects, even under perceptual uncertainty and real-world\ndisturbances. D3Grasp achieves an average success rate of 95.1% in real-world\ntrials,outperforming prior methods on both rigid and deformable objects\nbenchmarks.", "AI": {"tldr": "D3Grasp is a reinforcement learning framework integrating visual and tactile inputs to achieve robust, diverse, and adaptable dexterous grasping for both rigid and deformable objects.", "motivation": "Dexterous grasping, especially for deformable and diverse objects, faces challenges due to high-dimensional action spaces and perceptual uncertainty.", "method": "The paper introduces a unified multimodal representation combining visual and tactile inputs, uses an asymmetric reinforcement learning architecture leveraging privileged information during training, and designs a strategy to ensure kinematically feasible grasps.", "result": "D3Grasp achieves a 95.1% average success rate in real-world tests, surpassing previous methods for both rigid and deformable object handling.", "conclusion": "The framework significantly advances dexterous grasping capabilities, enhancing generalization, adaptability, and real-world applicability under uncertainty."}}
{"id": "2509.19843", "pdf": "https://arxiv.org/pdf/2509.19843", "abs": "https://arxiv.org/abs/2509.19843", "authors": ["Filippo Ziliotto", "Jelin Raphael Akkara", "Alessandro Daniele", "Lamberto Ballan", "Luciano Serafini", "Tommaso Campari"], "title": "PersONAL: Towards a Comprehensive Benchmark for Personalized Embodied Agents", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Recent advances in Embodied AI have enabled agents to perform increasingly\ncomplex tasks and adapt to diverse environments. However, deploying such agents\nin realistic human-centered scenarios, such as domestic households, remains\nchallenging, particularly due to the difficulty of modeling individual human\npreferences and behaviors. In this work, we introduce PersONAL (PERSonalized\nObject Navigation And Localization, a comprehensive benchmark designed to study\npersonalization in Embodied AI. Agents must identify, retrieve, and navigate to\nobjects associated with specific users, responding to natural-language queries\nsuch as \"find Lily's backpack\". PersONAL comprises over 2,000 high-quality\nepisodes across 30+ photorealistic homes from the HM3D dataset. Each episode\nincludes a natural-language scene description with explicit associations\nbetween objects and their owners, requiring agents to reason over user-specific\nsemantics. The benchmark supports two evaluation modes: (1) active navigation\nin unseen environments, and (2) object grounding in previously mapped scenes.\nExperiments with state-of-the-art baselines reveal a substantial gap to human\nperformance, highlighting the need for embodied agents capable of perceiving,\nreasoning, and memorizing over personalized information; paving the way towards\nreal-world assistive robot.", "AI": {"tldr": "The paper introduces PersONAL, a benchmark focusing on personalized object identification and navigation in human-centric environments for Embodied AI.", "motivation": "To address the challenges of deploying Embodied AI agents in realistic, human-centered scenarios where individualized preferences and behaviors must be considered.", "method": "Developed PersONAL, comprising over 2,000 episodes with natural-language object-user associations in photorealistic environments, and evaluated agent capabilities in navigation and object grounding.", "result": "State-of-the-art baselines show significant gaps compared to human performance, demonstrating the difficulty in reasoning and memorizing personalized information.", "conclusion": "PersONAL advances the study of personalized Embodied AI, emphasizing the need for agents capable of interacting effectively in human-centered contexts."}}
{"id": "2509.19671", "pdf": "https://arxiv.org/pdf/2509.19671", "abs": "https://arxiv.org/abs/2509.19671", "authors": ["Andrew Wang", "Jiashuo Zhang", "Michael Oberst"], "title": "Revisiting Performance Claims for Chest X-Ray Models Using Clinical Context", "categories": ["cs.LG"], "comment": null, "summary": "Public healthcare datasets of Chest X-Rays (CXRs) have long been a popular\nbenchmark for developing computer vision models in healthcare. However, strong\naverage-case performance of machine learning (ML) models on these datasets is\ninsufficient to certify their clinical utility. In this paper, we use clinical\ncontext, as captured by prior discharge summaries, to provide a more holistic\nevaluation of current ``state-of-the-art'' models for the task of CXR\ndiagnosis. Using discharge summaries recorded prior to each CXR, we derive a\n``prior'' or ``pre-test'' probability of each CXR label, as a proxy for\nexisting contextual knowledge available to clinicians when interpreting CXRs.\nUsing this measure, we demonstrate two key findings: First, for several\ndiagnostic labels, CXR models tend to perform best on cases where the pre-test\nprobability is very low, and substantially worse on cases where the pre-test\nprobability is higher. Second, we use pre-test probability to assess whether\nstrong average-case performance reflects true diagnostic signal, rather than an\nability to infer the pre-test probability as a shortcut. We find that\nperformance drops sharply on a balanced test set where this shortcut does not\nexist, which may indicate that much of the apparent diagnostic power derives\nfrom inferring this clinical context. We argue that this style of analysis,\nusing context derived from clinical notes, is a promising direction for more\nrigorous and fine-grained evaluation of clinical vision models.", "AI": {"tldr": "The paper evaluates clinical vision models for Chest X-Ray diagnosis by incorporating context from prior discharge summaries and highlights potential shortcomings in their diagnostic performance.", "motivation": "Current benchmarks for Chest X-Ray diagnosis models lack a deeper evaluation involving clinical context, which may hinder their reliability in real-world clinical settings.", "method": "The authors use prior discharge summaries to derive a 'pre-test' probability for Chest X-Ray labels, evaluating state-of-the-art models based on this contextual clinical knowledge.", "result": "Findings show that models perform better on cases with low pre-test probability and worse on high pre-test probability cases. Additionally, models rely heavily on pre-test probability shortcuts, affecting their diagnostic accuracy on balanced datasets.", "conclusion": "Incorporating clinical context via discharge summaries provides critical insights into model evaluation, promoting more nuanced and rigorous assessments for clinical utility."}}
{"id": "2509.19580", "pdf": "https://arxiv.org/pdf/2509.19580", "abs": "https://arxiv.org/abs/2509.19580", "authors": ["Yanfang", "Ye", "Zheyuan Zhang", "Tianyi Ma", "Zehong Wang", "Yiyang Li", "Shifu Hou", "Weixiang Sun", "Kaiwen Shi", "Yijun Ma", "Wei Song", "Ahmed Abbasi", "Ying Cheng", "Jane Cleland-Huang", "Steven Corcelli", "Patricia Culligan", "Robert Goulding", "Ming Hu", "Ting Hua", "John Lalor", "Fang Liu", "Tengfei Luo", "Ed Maginn", "Nuno Moniz", "Jason Rohr", "Brett Savoie", "Daniel Slate", "Tom Stapleford", "Matthew Webber", "Olaf Wiest", "Johnny Zhang", "Nitesh Chawla"], "title": "LLMs4All: A Review on Large Language Models for Research and Applications in Academic Disciplines", "categories": ["cs.CL"], "comment": null, "summary": "Cutting-edge Artificial Intelligence (AI) techniques keep reshaping our view\nof the world. For example, Large Language Models (LLMs) based applications such\nas ChatGPT have shown the capability of generating human-like conversation on\nextensive topics. Due to the impressive performance on a variety of\nlanguage-related tasks (e.g., open-domain question answering, translation, and\ndocument summarization), one can envision the far-reaching impacts that can be\nbrought by the LLMs with broader real-world applications (e.g., customer\nservice, education and accessibility, and scientific discovery). Inspired by\ntheir success, this paper will offer an overview of state-of-the-art LLMs and\ntheir integration into a wide range of academic disciplines, including: (1)\narts, letters, and law (e.g., history, philosophy, political science, arts and\narchitecture, law), (2) economics and business (e.g., finance, economics,\naccounting, marketing), and (3) science and engineering (e.g., mathematics,\nphysics and mechanical engineering, chemistry and chemical engineering, life\nsciences and bioengineering, earth sciences and civil engineering, computer\nscience and electrical engineering). Integrating humanity and technology, in\nthis paper, we will explore how LLMs are shaping research and practice in these\nfields, while also discussing key limitations, open challenges, and future\ndirections in the era of generative AI. The review of how LLMs are engaged\nacross disciplines-along with key observations and insights-can help\nresearchers and practitioners interested in exploiting LLMs to advance their\nworks in diverse real-world applications.", "AI": {"tldr": "This paper reviews the applications of Large Language Models (LLMs) across academic disciplines, their successes, limitations, challenges, and future directions.", "motivation": "To understand how LLMs are influencing diverse academic fields and to help researchers integrate generative AI into real-world applications.", "method": "The paper offers an overview and analysis of how LLMs function across arts, economics, sciences, and engineering, along with examining their limitations and future developments.", "result": "LLMs are found to be impactful across various disciplines, demonstrating transformative potential while having inherent challenges.", "conclusion": "LLMs have broad applicability across multiple fields, promising significant advancements yet requiring further work to overcome limitations and address challenges."}}
{"id": "2509.19916", "pdf": "https://arxiv.org/pdf/2509.19916", "abs": "https://arxiv.org/abs/2509.19916", "authors": ["Zijun Che", "Yinghong Zhang", "Shengyi Liang", "Boyu Zhou", "Jun Ma", "Jinni Zhou"], "title": "GUIDE: A Diffusion-Based Autonomous Robot Exploration Framework Using Global Graph Inference", "categories": ["cs.RO"], "comment": null, "summary": "Autonomous exploration in structured and complex indoor environments remains\na challenging task, as existing methods often struggle to appropriately model\nunobserved space and plan globally efficient paths. To address these\nlimitations, we propose GUIDE, a novel exploration framework that\nsynergistically combines global graph inference with diffusion-based\ndecision-making. We introduce a region-evaluation global graph representation\nthat integrates both observed environmental data and predictions of unexplored\nareas, enhanced by a region-level evaluation mechanism to prioritize reliable\nstructural inferences while discounting uncertain predictions. Building upon\nthis enriched representation, a diffusion policy network generates stable,\nforesighted action sequences with significantly reduced denoising steps.\nExtensive simulations and real-world deployments demonstrate that GUIDE\nconsistently outperforms state-of-the-art methods, achieving up to 18.3% faster\ncoverage completion and a 34.9% reduction in redundant movements.", "AI": {"tldr": "The paper presents GUIDE, a framework for efficient autonomous indoor exploration using a graph-based representation and diffusion policy for better decision-making.", "motivation": "To improve autonomous exploration in structured and complex indoor environments, which is currently hindered by limitations in modeling unobserved spaces and planning efficient paths.", "method": "The proposed GUIDE framework integrates a region-evaluation global graph representation with a diffusion policy network. The graph accounts for observed and predicted unexplored areas, and the diffusion policy generates foresighted actions with reduced denoising.", "result": "GUIDE achieves up to 18.3% faster coverage completion and a 34.9% reduction in redundant movements in both simulations and real-world tests.", "conclusion": "The GUIDE framework demonstrates significant improvements over existing methods in terms of exploration efficiency and path planning, indicating its potential as a transformational tool in autonomous navigation."}}
{"id": "2509.19870", "pdf": "https://arxiv.org/pdf/2509.19870", "abs": "https://arxiv.org/abs/2509.19870", "authors": ["Xin Wang", "Jie Li", "Zejia Weng", "Yixu Wang", "Yifeng Gao", "Tianyu Pang", "Chao Du", "Yan Teng", "Yingchun Wang", "Zuxuan Wu", "Xingjun Ma", "Yu-Gang Jiang"], "title": "FreezeVLA: Action-Freezing Attacks against Vision-Language-Action Models", "categories": ["cs.CV"], "comment": null, "summary": "Vision-Language-Action (VLA) models are driving rapid progress in robotics by\nenabling agents to interpret multimodal inputs and execute complex,\nlong-horizon tasks. However, their safety and robustness against adversarial\nattacks remain largely underexplored. In this work, we identify and formalize a\ncritical adversarial vulnerability in which adversarial images can \"freeze\" VLA\nmodels and cause them to ignore subsequent instructions. This threat\neffectively disconnects the robot's digital mind from its physical actions,\npotentially inducing inaction during critical interventions. To systematically\nstudy this vulnerability, we propose FreezeVLA, a novel attack framework that\ngenerates and evaluates action-freezing attacks via min-max bi-level\noptimization. Experiments on three state-of-the-art VLA models and four robotic\nbenchmarks show that FreezeVLA attains an average attack success rate of 76.2%,\nsignificantly outperforming existing methods. Moreover, adversarial images\ngenerated by FreezeVLA exhibit strong transferability, with a single image\nreliably inducing paralysis across diverse language prompts. Our findings\nexpose a critical safety risk in VLA models and highlight the urgent need for\nrobust defense mechanisms.", "AI": {"tldr": "This paper introduces a critical vulnerability in Vision-Language-Action (VLA) models, where adversarial images can freeze the robot's actions, presenting a significant safety risk.", "motivation": "The authors aim to address the unexplored safety and robustness concerns in VLA models, especially their susceptibility to adversarial attacks that can disconnect robot decision-making from physical actions.", "method": "FreezeVLA, a novel attack framework based on min-max bi-level optimization, is proposed to systematically study and evaluate action-freezing attacks on robots.", "result": "Experimental results across three VLA models and four robotic benchmarks indicate that FreezeVLA achieves a 76.2% attack success rate, outperforming existing attack methods and demonstrating strong transferability of adversarial images.", "conclusion": "The study uncovers a pressing safety vulnerability in VLA models and emphasizes the necessity for developing effective defensive strategies to prevent critical robotic inaction."}}
{"id": "2509.19674", "pdf": "https://arxiv.org/pdf/2509.19674", "abs": "https://arxiv.org/abs/2509.19674", "authors": ["Kunlun Xu", "Yibo Feng", "Jiangmeng Li", "Yongsheng Qi", "Jiahuan Zhou"], "title": "C${}^2$Prompt: Class-aware Client Knowledge Interaction for Federated Continual Learning", "categories": ["cs.LG", "cs.CV"], "comment": "Accepted by NeurIPS 2025", "summary": "Federated continual learning (FCL) tackles scenarios of learning from\ncontinuously emerging task data across distributed clients, where the key\nchallenge lies in addressing both temporal forgetting over time and spatial\nforgetting simultaneously. Recently, prompt-based FCL methods have shown\nadvanced performance through task-wise prompt communication.In this study, we\nunderscore that the existing prompt-based FCL methods are prone to class-wise\nknowledge coherence between prompts across clients. The class-wise knowledge\ncoherence includes two aspects: (1) intra-class distribution gap across\nclients, which degrades the learned semantics across prompts, (2) inter-prompt\nclass-wise relevance, which highlights cross-class knowledge confusion. During\nprompt communication, insufficient class-wise coherence exacerbates knowledge\nconflicts among new prompts and induces interference with old prompts,\nintensifying both spatial and temporal forgetting. To address these issues, we\npropose a novel Class-aware Client Knowledge Interaction (C${}^2$Prompt) method\nthat explicitly enhances class-wise knowledge coherence during prompt\ncommunication. Specifically, a local class distribution compensation mechanism\n(LCDC) is introduced to reduce intra-class distribution disparities across\nclients, thereby reinforcing intra-class knowledge consistency. Additionally, a\nclass-aware prompt aggregation scheme (CPA) is designed to alleviate\ninter-class knowledge confusion by selectively strengthening class-relevant\nknowledge aggregation. Extensive experiments on multiple FCL benchmarks\ndemonstrate that C${}^2$Prompt achieves state-of-the-art performance. Our\nsource code is available at\nhttps://github.com/zhoujiahuan1991/NeurIPS2025-C2Prompt", "AI": {"tldr": "This paper proposes a novel method (C${}^2$Prompt) to address temporal and spatial forgetting in Federated Continual Learning by improving class-wise knowledge coherence.", "motivation": "Address challenges in Federated Continual Learning, specifically temporal and spatial forgetting, due to insufficient class-wise knowledge coherence during prompt communication.", "method": "Introduced Class-aware Client Knowledge Interaction (C${}^2$Prompt), consisting of a Local Class Distribution Compensation (LCDC) mechanism and Class-aware Prompt Aggregation (CPA) to enhance class-wise knowledge coherence.", "result": "The proposed method achieves state-of-the-art performance on multiple Federated Continual Learning benchmarks.", "conclusion": "C${}^2$Prompt effectively addresses the class-wise knowledge coherence problems in Federated Continual Learning, thereby mitigating both temporal and spatial forgetting."}}
{"id": "2509.19593", "pdf": "https://arxiv.org/pdf/2509.19593", "abs": "https://arxiv.org/abs/2509.19593", "authors": ["Dylan Hutson", "Daniel Vennemeyer", "Aneesh Deshmukh", "Justin Zhan", "Tianyu Jiang"], "title": "GuessingGame: Measuring the Informativeness of Open-Ended Questions in Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "EMNLP 2025, 17 pages, 2 figures", "summary": "We introduce GuessingGame, a protocol for evaluating large language models\n(LLMs) as strategic question-askers in open-ended, open-domain settings. A\nGuesser LLM identifies a hidden object by posing free-form questions to an\nOracle without predefined choices or candidate lists. To measure question\nquality, we propose two information gain (IG) metrics: a Bayesian method that\ntracks belief updates over semantic concepts using LLM-scored relevance, and an\nentropy-based method that filters candidates via ConceptNet. Both metrics are\nmodel-agnostic and support post hoc analysis. Across 858 games with multiple\nmodels and prompting strategies, higher IG strongly predicts efficiency: a\none-standard-deviation IG increase reduces expected game length by 43\\%.\nPrompting constraints guided by IG, such as enforcing question diversity,\nenable weaker models to significantly improve performance. These results show\nthat question-asking in LLMs is both measurable and improvable, and crucial for\ninteractive reasoning.", "AI": {"tldr": "The paper introduces GuessingGame, a protocol for assessing large language models (LLMs) in open-ended strategic question-asking. It proposes two information gain metrics and finds that question quality predicts game efficiency, showing that LLMs' question-asking can be measured and improved.", "motivation": "The motivation is to evaluate and improve the strategic question-asking ability of large language models, particularly in open-domain and interactive reasoning scenarios.", "method": "The method involves a GuessingGame protocol where a Guesser LLM identifies hidden objects by asking free-form questions to an Oracle. Two model-agnostic information gain metrics\u2014Bayesian belief tracking and entropy-based filtering\u2014measure question quality.", "result": "In experiments across 858 games using various models and prompting strategies, higher information gain correlates with reduced game length, and guided constraints significantly improve weaker models' performance.", "conclusion": "The study concludes that LLMs' question-asking ability is measurable, improvable, and central to interactive reasoning tasks."}}
{"id": "2509.19954", "pdf": "https://arxiv.org/pdf/2509.19954", "abs": "https://arxiv.org/abs/2509.19954", "authors": ["Pinhao Song", "Yurui Du", "Ophelie Saussus", "Sofie De Schrijver", "Irene Caprara", "Peter Janssen", "Renaud Detry"], "title": "Robot Trajectron V2: A Probabilistic Shared Control Framework for Navigation", "categories": ["cs.RO"], "comment": "26 pages, 20 figures", "summary": "We propose a probabilistic shared-control solution for navigation, called\nRobot Trajectron V2 (RT-V2), that enables accurate intent prediction and safe,\neffective assistance in human-robot interaction. RT-V2 jointly models a user's\nlong-term behavioral patterns and their noisy, low-dimensional control signals\nby combining a prior intent model with a posterior update that accounts for\nreal-time user input and environmental context. The prior captures the\nmultimodal and history-dependent nature of user intent using recurrent neural\nnetworks and conditional variational autoencoders, while the posterior\nintegrates this with uncertain user commands to infer desired actions. We\nconduct extensive experiments to validate RT-V2 across synthetic benchmarks,\nhuman-computer interaction studies with keyboard input, and brain-machine\ninterface experiments with non-human primates. Results show that RT-V2\noutperforms the state of the art in intent estimation, provides safe and\nefficient navigation support, and adequately balances user autonomy with\nassistive intervention. By unifying probabilistic modeling, reinforcement\nlearning, and safe optimization, RT-V2 offers a principled and generalizable\napproach to shared control for diverse assistive technologies.", "AI": {"tldr": "This paper introduces RT-V2, a probabilistic control system for safe and effective human-robot interaction, especially in assistive navigation.", "motivation": "The motivation is to improve intent prediction and enhance safety and efficiency in shared human-robot interaction by addressing the noisy and uncertain nature of human user input.", "method": "RT-V2 combines a prior intent model (using RNNs and CVAEs) with posterior updates to account for real-time user input and context. The approach integrates probabilistic modeling, reinforcement learning, and safe optimization techniques.", "result": "Experiments on synthetic benchmarks, human studies, and brain-machine interfaces demonstrate superior performance of RT-V2 in intent estimation, safety, and navigation efficiency compared to the state of the art.", "conclusion": "RT-V2 is a principled, generalizable shared-control framework that balances user autonomy and assistive intervention, offering significant potential for diverse assistive technologies."}}
{"id": "2509.19875", "pdf": "https://arxiv.org/pdf/2509.19875", "abs": "https://arxiv.org/abs/2509.19875", "authors": ["Yunqing Hu", "Zheming Yang", "Chang Zhao", "Wen Ji"], "title": "Adaptive Guidance Semantically Enhanced via Multimodal LLM for Edge-Cloud Object Detection", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Traditional object detection methods face performance degradation challenges\nin complex scenarios such as low-light conditions and heavy occlusions due to a\nlack of high-level semantic understanding. To address this, this paper proposes\nan adaptive guidance-based semantic enhancement edge-cloud collaborative object\ndetection method leveraging Multimodal Large Language Models (MLLM), achieving\nan effective balance between accuracy and efficiency. Specifically, the method\nfirst employs instruction fine-tuning to enable the MLLM to generate structured\nscene descriptions. It then designs an adaptive mapping mechanism that\ndynamically converts semantic information into parameter adjustment signals for\nedge detectors, achieving real-time semantic enhancement. Within an edge-cloud\ncollaborative inference framework, the system automatically selects between\ninvoking cloud-based semantic guidance or directly outputting edge detection\nresults based on confidence scores. Experiments demonstrate that the proposed\nmethod effectively enhances detection accuracy and efficiency in complex\nscenes. Specifically, it can reduce latency by over 79% and computational cost\nby 70% in low-light and highly occluded scenes while maintaining accuracy.", "AI": {"tldr": "This paper proposes a semantic-enhanced edge-cloud collaborative object detection method using Multimodal Large Language Models to address challenges in complex scenarios like low-light conditions and occlusions.", "motivation": "Traditional object detection struggles in complex scenarios due to limited semantic understanding. This study aims to improve accuracy and efficiency in such conditions.", "method": "The method uses instruction fine-tuned MLLMs for generating structured scene descriptions and an adaptive mapping to translate semantic data into parameter adjustments. It employs an edge-cloud framework to toggle cloud-based or edge-based detections dynamically.", "result": "The proposed method improves both detection accuracy and efficiency, reducing latency by 79% and computational cost by 70% in challenging scenarios.", "conclusion": "An MLLM-based adaptive semantic guidance approach significantly enhances object detection under challenging conditions, effectively balancing performance and resource usage."}}
{"id": "2509.19698", "pdf": "https://arxiv.org/pdf/2509.19698", "abs": "https://arxiv.org/abs/2509.19698", "authors": ["Gunbir Singh Baveja", "Mark Schmidt"], "title": "A Unified Noise-Curvature View of Loss of Trainability", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Loss of trainability (LoT) in continual learning occurs when gradient steps\nno longer yield improvement as tasks evolve, so accuracy stalls or degrades\ndespite adequate capacity and supervision. We analyze LoT incurred with Adam\nthrough an optimization lens and find that single indicators such as Hessian\nrank, sharpness level, weight or gradient norms, gradient-to-parameter ratios,\nand unit-sign entropy are not reliable predictors. Instead we introduce two\ncomplementary criteria: a batch-size-aware gradient-noise bound and a curvature\nvolatility-controlled bound that combine into a per-layer predictive threshold\nthat anticipates trainability behavior. Using this threshold, we build a simple\nper-layer scheduler that keeps each layers effective step below a safe limit,\nstabilizing training and improving accuracy across concatenated ReLU (CReLU),\nWasserstein regularization, and L2 weight decay, with learned learning-rate\ntrajectories that mirror canonical decay.", "AI": {"tldr": "This paper addresses the loss of trainability (LoT) in continual learning by proposing criteria and a scheduling method to improve training stability and accuracy.", "motivation": "The study aims to overcome the loss of trainability in continual learning tasks where accuracy deteriorates despite sufficient model capacity and adequate supervision.", "method": "The authors propose two predictive criteria: a gradient-noise bound (accounting for batch size) and a curvature volatility-bound. These form a per-layer threshold used to guide learning-rate scheduling.", "result": "Results show that the proposed scheduler improves training stability and accuracy with tailored learning rate trajectories across techniques like concatenated ReLU, Wasserstein regularization, and L2 weight decay.", "conclusion": "The findings suggest that a per-layer scheduler based on predictive bounds can effectively mitigate LoT, providing stability and enhancing continual learning performance."}}
{"id": "2509.19595", "pdf": "https://arxiv.org/pdf/2509.19595", "abs": "https://arxiv.org/abs/2509.19595", "authors": ["Mohammad Saim", "Phan Anh Duong", "Cat Luong", "Aniket Bhanderi", "Tianyu Jiang"], "title": "Anatomy of a Feeling: Narrating Embodied Emotions via Large Vision-Language Models", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "The embodiment of emotional reactions from body parts contains rich\ninformation about our affective experiences. We propose a framework that\nutilizes state-of-the-art large vision-language models (LVLMs) to generate\nEmbodied LVLM Emotion Narratives (ELENA). These are well-defined, multi-layered\ntext outputs, primarily comprising descriptions that focus on the salient body\nparts involved in emotional reactions. We also employ attention maps and\nobserve that contemporary models exhibit a persistent bias towards the facial\nregion. Despite this limitation, we observe that our employed framework can\neffectively recognize embodied emotions in face-masked images, outperforming\nbaselines without any fine-tuning. ELENA opens a new trajectory for embodied\nemotion analysis across the modality of vision and enriches modeling in an\naffect-aware setting.", "AI": {"tldr": "The paper introduces ELENA, a framework using advanced vision-language models to analyze embodied emotional reactions by focusing on body parts involved in emotions.", "motivation": "To improve the understanding and modeling of affective experiences by focusing on bodily emotional responses and overcoming existing limitations focused on facial bias.", "method": "Employs large vision-language models (LVLMs) to generate multi-layered emotion narratives, incorporating attention maps to detect emotional expressions, particularly in face-masked images.", "result": "The proposed framework can effectively recognize embodied emotions across images, even outperforming baseline methods, without requiring additional fine-tuning.", "conclusion": "ELENA broadens the scope of embodied emotion analysis, paving the way for better affect-aware systems using vision-language models."}}
{"id": "2509.19958", "pdf": "https://arxiv.org/pdf/2509.19958", "abs": "https://arxiv.org/abs/2509.19958", "authors": ["Alexander Spiridonov", "Jan-Nico Zaech", "Nikolay Nikolov", "Luc Van Gool", "Danda Pani Paudel"], "title": "Generalist Robot Manipulation beyond Action Labeled Data", "categories": ["cs.RO"], "comment": "Accepted at Conference on Robot Learning 2025", "summary": "Recent advances in generalist robot manipulation leverage pre-trained\nVision-Language Models (VLMs) and large-scale robot demonstrations to tackle\ndiverse tasks in a zero-shot manner. A key challenge remains: scaling\nhigh-quality, action-labeled robot demonstration data, which existing methods\nrely on for robustness and generalization. To address this, we propose a method\nthat benefits from videos without action labels - featuring humans and/or\nrobots in action - enhancing open-vocabulary performance and enabling\ndata-efficient learning of new tasks. Our method extracts dense, dynamic 3D\npoint clouds at the hand or gripper location and uses a proposed 3D dynamics\npredictor for self-supervision. This predictor is then tuned to an action\npredictor using a smaller labeled dataset for action alignment. We show that\nour method not only learns from unlabeled human and robot demonstrations -\nimproving downstream generalist robot policies - but also enables robots to\nlearn new tasks without action labels (i.e., out-of-action generalization) in\nboth real-world and simulated settings.", "AI": {"tldr": "The paper introduces a method to enhance robot manipulation by learning efficiently from unlabeled human and robot demonstration videos, using 3D point clouds and a self-supervised dynamics predictor.", "motivation": "The motivation is to address the challenge of scaling high-quality, action-labeled robot demonstration data, which existing robot manipulation methods require but are costly and difficult to obtain.", "method": "The method uses dense 3D point clouds at hand or gripper locations, a self-supervised 3D dynamics predictor, and minimal labeled data to tune the predictor into an action predictor.", "result": "The method improves generalist robot performance, enables out-of-action generalization, and supports learning tasks from unlabeled demonstrations in both real-world and simulated settings.", "conclusion": "The proposed approach allows robots to generalize better and learn new tasks efficiently without requiring extensive labeled demonstration data."}}
{"id": "2509.19895", "pdf": "https://arxiv.org/pdf/2509.19895", "abs": "https://arxiv.org/abs/2509.19895", "authors": ["R\u00e9mi Giraud", "Rodrigo Borba Pinheiro", "Yannick Berthoumieu"], "title": "Generalized Shortest Path-based Superpixels for 3D Spherical Image Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "The growing use of wide angle image capture devices and the need for fast and\naccurate image analysis in computer visions have enforced the need for\ndedicated under-representation approaches. Most recent decomposition methods\nsegment an image into a small number of irregular homogeneous regions, called\nsuperpixels. Nevertheless, these approaches are generally designed to segment\nstandard 2D planar images, i.e., captured with a 90o angle view without\ndistortion. In this work, we introduce a new general superpixel method called\nSphSPS (for Spherical Shortest Path-based Superpixels)1 , dedicated to wide\n360o spherical or omnidirectional images. Our method respects the geometry of\nthe 3D spherical acquisition space and generalizes the notion of shortest path\nbetween a pixel and a superpixel center, to fastly extract relevant clustering\nfeatures. We demonstrate that considering the geometry of the acquisition space\nto compute the shortest path enables to jointly improve the segmentation\naccuracy and the shape regularity of superpixels. To evaluate this regularity\naspect, we also generalize a global regularity metric to the spherical space,\naddressing the limitations of the only existing spherical compactness measure.\nFinally, the proposed SphSPS method is validated on the reference 360o\nspherical panorama segmentation dataset and on synthetic road omnidirectional\nimages. Our method significantly outperforms both planar and spherical\nstate-of-the-art approaches in terms of segmentation accuracy,robustness to\nnoise and regularity, providing a very interesting tool for superpixel-based\napplications on 360o images.", "AI": {"tldr": "SphSPS, a novel superpixel method for 360\u00b0 images, improves segmentation accuracy and regularity by incorporating spherical geometry.", "motivation": "To address the need for fast and accurate superpixel-based segmentation methods tailored specifically for wide 360-degree spherical or omnidirectional images.", "method": "Introduced SphSPS, a spherical superpixel segmentation method that incorporates the geometry of the spherical 3D acquisition space and uses the notion of shortest path-based clustering.", "result": "SphSPS outperformed existing planar and spherical methods in segmentation accuracy, noise robustness, and regularity in tests on 360-degree spherical panorama datasets and synthetic road omnidirectional images.", "conclusion": "Considering the spherical geometry significantly improves segmentation accuracy and superpixel regularity, making SphSPS a promising tool for analyzing 360-degree images."}}
{"id": "2509.19702", "pdf": "https://arxiv.org/pdf/2509.19702", "abs": "https://arxiv.org/abs/2509.19702", "authors": ["Patrick Lutz", "Aditya Gangrade", "Hadi Daneshmand", "Venkatesh Saligrama"], "title": "Linear Transformers Implicitly Discover Unified Numerical Algorithms", "categories": ["cs.LG", "cs.AI"], "comment": "To appear at NeurIPS 2025", "summary": "We train a linear attention transformer on millions of masked-block matrix\ncompletion tasks: each prompt is masked low-rank matrix whose missing block may\nbe (i) a scalar prediction target or (ii) an unseen kernel slice of Nystr\\\"om\nextrapolation. The model sees only input-output pairs and a mean-squared loss;\nit is given no normal equations, no handcrafted iterations, and no hint that\nthe tasks are related. Surprisingly, after training, algebraic unrolling\nreveals the same parameter-free update rule across three distinct computational\nregimes (full visibility, rank-limited updates, and distributed computation).\nWe prove that this rule achieves second-order convergence on full-batch\nproblems, cuts distributed iteration complexity, and remains accurate with\nrank-limited attention. Thus, a transformer trained solely to patch missing\nblocks implicitly discovers a unified, resource-adaptive iterative solver\nspanning prediction, estimation, and Nystr\\\"om extrapolation, highlighting a\npowerful capability of in-context learning.", "AI": {"tldr": "The paper trains a linear attention transformer on masked-block matrix completion tasks and uncovers that the transformer implicitly discovers a unified iterative solver across various computational regimes.", "motivation": "To investigate whether a transformer model can independently learn an iterative solving method for matrix completion tasks without explicit guidance or handcrafted rules.", "method": "The transformer was trained on millions of masked-low rank matrix completion tasks, using input-output pairs and a mean-squared loss, without any explicit mathematical rules or iterations provided.", "result": "The trained transformer was able to implicitly discover a parameter-free update rule that performs well across full visibility, rank-limited updates, and distributed computation, achieving second-order convergence and adaptation in multiple contexts.", "conclusion": "Transformers trained on matrix completion tasks reveal in-context learning capabilities, implicitly discovering adaptive iterative solvers for prediction, estimation, and Nystr\u00f6m extrapolation without external guidance."}}
{"id": "2509.19328", "pdf": "https://arxiv.org/pdf/2509.19328", "abs": "https://arxiv.org/abs/2509.19328", "authors": ["Sina Montazeri", "Waltenegus Dargie", "Yunhe Feng", "Kewei Sha"], "title": "Human Activity Recognition Based on Electrocardiogram Data Only", "categories": ["eess.SP", "cs.AI", "cs.LG"], "comment": "This is a preprint version. Content may change before final\n  publication", "summary": "Human activity recognition is critical for applications such as early\nintervention and health analytics. Traditional activity recognition relies on\ninertial measurement units (IMUs), which are resource intensive and require\ncalibration. Although electrocardiogram (ECG)-based methods have been explored,\nthese have typically served as supplements to IMUs or have been limited to\nbroad categorical classification such as fall detection or active vs. inactive\nin daily activities. In this paper, we advance the field by demonstrating, for\nthe first time, robust recognition of activity only with ECG in six distinct\nactivities, which is beyond the scope of previous work. We design and evaluate\nthree new deep learning models, including a CNN classifier with\nSqueeze-and-Excitation blocks for channel-wise feature recalibration, a ResNet\nclassifier with dilated convolutions for multiscale temporal dependency\ncapture, and a novel CNNTransformer hybrid combining convolutional feature\nextraction with attention mechanisms for long-range temporal relationship\nmodeling. Tested on data from 54 subjects for six activities, all three models\nachieve over 94% accuracy for seen subjects, while CNNTransformer hybrid\nreaching the best accuracy of 72% for unseen subjects, a result that can be\nfurther improved by increasing the training population. This study demonstrates\nthe first successful ECG-only activity classification in multiple physical\nactivities, offering significant potential for developing next-generation\nwearables capable of simultaneous cardiac monitoring and activity recognition\nwithout additional motion sensors.", "AI": {"tldr": "This paper presents a significant advancement in activity recognition by demonstrating the ability to classify six distinct activities using only ECG data, with over 94% accuracy for seen subjects.", "motivation": "To develop a more efficient method for human activity recognition without relying on resource-intensive IMUs, addressing the limitations of existing ECG-based methods which are either supplemental or limited in scope.", "method": "The paper introduces three deep learning models: (1) CNN with Squeeze-and-Excitation blocks, (2) ResNet with dilated convolutions, and (3) a CNNTransformer hybrid model. These are tested on ECG data collected from 54 subjects to classify six physical activities.", "result": "All three models achieved over 94% accuracy for seen subjects. For unseen subjects, the CNNTransformer hybrid model achieved the highest accuracy of 72%, showing potential for further improvement with a larger training dataset.", "conclusion": "This study establishes the feasibility of ECG-only activity classification across multiple physical activities, proposing a path for next-gen wearables combining cardiac monitoring and activity recognition without motion sensors."}}
{"id": "2509.19611", "pdf": "https://arxiv.org/pdf/2509.19611", "abs": "https://arxiv.org/abs/2509.19611", "authors": ["Syeda Jannatus Saba", "Steven Skiena"], "title": "Evaluating Language Translation Models by Playing Telephone", "categories": ["cs.CL"], "comment": "Accepted to EMNLP 2025 Main Conference as a long paper", "summary": "Our ability to efficiently and accurately evaluate the quality of machine\ntranslation systems has been outrun by the effectiveness of current language\nmodels--which limits the potential for further improving these models on more\nchallenging tasks like long-form and literary translation. We propose an\nunsupervised method to generate training data for translation evaluation over\ndifferent document lengths and application domains by repeated rounds of\ntranslation between source and target languages. We evaluate evaluation systems\ntrained on texts mechanically generated using both model rotation and language\ntranslation approaches, demonstrating improved performance over a popular\ntranslation evaluation system (xCOMET) on two different tasks: (i) scoring the\nquality of a given translation against a human reference and (ii) selecting\nwhich of two translations is generationally closer to an original source\ndocument.", "AI": {"tldr": "The paper introduces a new unsupervised method to generate training data for evaluating machine translation systems, demonstrating improvements over existing evaluation methods.", "motivation": "Current methods for evaluating machine translation systems struggle to match the growing effectiveness of modern language models, limiting progress in challenging translation tasks.", "method": "The authors use unsupervised generation of training data through repeated rounds of translation between source and target languages and assess its effectiveness against other approaches.", "result": "The proposed method improves over a popular translation evaluation system (xCOMET) on tasks involving translation quality scoring and translation selection.", "conclusion": "This method enables better and more reliable evaluation of machine translation systems, potentially enhancing development for complex tasks."}}
{"id": "2509.19972", "pdf": "https://arxiv.org/pdf/2509.19972", "abs": "https://arxiv.org/abs/2509.19972", "authors": ["Albina Klepach", "Egor E. Nuzhin", "Alexey A. Tsukanov", "Nikolay V. Brilliantov"], "title": "An effective control of large systems of active particles: An application to evacuation problem", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Manipulation of large systems of active particles is a serious challenge\nacross diverse domains, including crowd management, control of robotic swarms,\nand coordinated material transport. The development of advanced control\nstrategies for complex scenarios is hindered, however, by the lack of\nscalability and robustness of the existing methods, in particular, due to the\nneed of an individual control for each agent. One possible solution involves\ncontrolling a system through a leader or a group of leaders, which other agents\ntend to follow. Using such an approach we develop an effective control strategy\nfor a leader, combining reinforcement learning (RL) with artificial forces\nacting on the system. To describe the guidance of active particles by a leader\nwe introduce the generalized Vicsek model. This novel method is then applied to\nthe problem of the effective evacuation by a robot-rescuer (leader) of large\ngroups of people from hazardous places. We demonstrate, that while a\nstraightforward application of RL yields suboptimal results, even for advanced\narchitectures, our approach provides a robust and efficient evacuation\nstrategy. The source code supporting this study is publicly available at:\nhttps://github.com/cinemere/evacuation.", "AI": {"tldr": "The paper proposes a novel leader-based control strategy using reinforcement learning and artificial forces for guiding active particles, aimed at effective crowd evacuation.", "motivation": "The paper addresses challenges in managing large systems of active particles across multiple domains, focusing on scalability, robustness, and removing the need for individualized agent control.", "method": "A leader-based control strategy is developed that combines reinforcement learning with artificial forces within the generalized Vicsek model.", "result": "The results demonstrate that the proposed approach achieves robust and efficient evacuation strategies, outperforming straightforward reinforcement learning methods.", "conclusion": "The method provides advancements in large-scale particle control, with publicly accessible code for further studies and applications."}}
{"id": "2509.19896", "pdf": "https://arxiv.org/pdf/2509.19896", "abs": "https://arxiv.org/abs/2509.19896", "authors": ["Pin-Jui Huang", "Yu-Hsuan Liao", "SooHeon Kim", "NoSeong Park", "JongBae Park", "DongMyung Shin"], "title": "Efficient Cell Painting Image Representation Learning via Cross-Well Aligned Masked Siamese Network", "categories": ["cs.CV"], "comment": "9 pages, 3 figures, reference 4 pages", "summary": "Computational models that predict cellular phenotypic responses to chemical\nand genetic perturbations can accelerate drug discovery by prioritizing\ntherapeutic hypotheses and reducing costly wet-lab iteration. However,\nextracting biologically meaningful and batch-robust cell painting\nrepresentations remains challenging. Conventional self-supervised and\ncontrastive learning approaches often require a large-scale model and/or a huge\namount of carefully curated data, still struggling with batch effects. We\npresent Cross-Well Aligned Masked Siamese Network (CWA-MSN), a novel\nrepresentation learning framework that aligns embeddings of cells subjected to\nthe same perturbation across different wells, enforcing semantic consistency\ndespite batch effects. Integrated into a masked siamese architecture, this\nalignment yields features that capture fine-grained morphology while remaining\ndata- and parameter-efficient. For instance, in a gene-gene relationship\nretrieval benchmark, CWA-MSN outperforms the state-of-the-art publicly\navailable self-supervised (OpenPhenom) and contrastive learning (CellCLIP)\nmethods, improving the benchmark scores by +29\\% and +9\\%, respectively, while\ntraining on substantially fewer data (e.g., 0.2M images for CWA-MSN vs. 2.2M\nimages for OpenPhenom) or smaller model size (e.g., 22M parameters for CWA-MSN\nvs. 1.48B parameters for CellCLIP). Extensive experiments demonstrate that\nCWA-MSN is a simple and effective way to learn cell image representation,\nenabling efficient phenotype modeling even under limited data and parameter\nbudgets.", "AI": {"tldr": "The paper introduces a new framework, Cross-Well Aligned Masked Siamese Network (CWA-MSN), to improve cellular phenotypic representation learning overcoming batch effects, while being efficient in data and parameters.", "motivation": "Accelerate drug discovery by creating effective computational models for predicting cellular responses to chemical and genetic perturbations that avoid the challenges of batch effects and inefficiency in conventional learning methods.", "method": "Developed CWA-MSN, a representation learning framework that aligns embeddings of similarly treated cells across experimental batches within a masked siamese architecture to ensure semantic consistency while being computationally efficient.", "result": "CWA-MSN outperforms existing approaches (OpenPhenom and CellCLIP) in a gene-gene relationship retrieval task with significantly fewer data (0.2M vs. 2.2M images) and smaller model size (22M vs. 1.48B parameters), achieving improvements of +29% and +9% over the benchmarks, respectively.", "conclusion": "CWA-MSN provides an efficient and robust method to learn cell image representation, overcoming batch effects and reducing data and parameter requirements, making it highly effective for phenotype modeling in drug discovery."}}
{"id": "2509.19705", "pdf": "https://arxiv.org/pdf/2509.19705", "abs": "https://arxiv.org/abs/2509.19705", "authors": ["J. Ben Tamo", "Nishant S. Chouhan", "Micky C. Nnamdi", "Yining Yuan", "Shreya S. Chivilkar", "Wenqi Shi", "Steven W. Hwang", "B. Randall Brenn", "May D. Wang"], "title": "Causal Machine Learning for Surgical Interventions", "categories": ["cs.LG", "cs.AI", "stat.AP", "stat.ME"], "comment": null, "summary": "Surgical decision-making is complex and requires understanding causal\nrelationships between patient characteristics, interventions, and outcomes. In\nhigh-stakes settings like spinal fusion or scoliosis correction, accurate\nestimation of individualized treatment effects (ITEs) remains limited due to\nthe reliance on traditional statistical methods that struggle with complex,\nheterogeneous data. In this study, we develop a multi-task meta-learning\nframework, X-MultiTask, for ITE estimation that models each surgical decision\n(e.g., anterior vs. posterior approach, surgery vs. no surgery) as a distinct\ntask while learning shared representations across tasks. To strengthen causal\nvalidity, we incorporate the inverse probability weighting (IPW) into the\ntraining objective. We evaluate our approach on two datasets: (1) a public\nspinal fusion dataset (1,017 patients) to assess the effect of anterior vs.\nposterior approaches on complication severity; and (2) a private AIS dataset\n(368 patients) to analyze the impact of posterior spinal fusion (PSF) vs.\nnon-surgical management on patient-reported outcomes (PROs). Our model achieves\nthe highest average AUC (0.84) in the anterior group and maintains competitive\nperformance in the posterior group (0.77). It outperforms baselines in\ntreatment effect estimation with the lowest overall $\\epsilon_{\\text{NN-PEHE}}$\n(0.2778) and $\\epsilon_{\\text{ATE}}$ (0.0763). Similarly, when predicting PROs\nin AIS, X-MultiTask consistently shows superior performance across all domains,\nwith $\\epsilon_{\\text{NN-PEHE}}$ = 0.2551 and $\\epsilon_{\\text{ATE}}$ = 0.0902.\nBy providing robust, patient-specific causal estimates, X-MultiTask offers a\npowerful tool to advance personalized surgical care and improve patient\noutcomes. The code is available at https://github.com/Wizaaard/X-MultiTask.", "AI": {"tldr": "The study presents X-MultiTask, a meta-learning framework for estimating individualized treatment effects (ITEs) in surgical decision-making, with high accuracy demonstrated on spinal datasets.", "motivation": "To address the limitations of traditional statistical methods in handling complex and heterogeneous data for accurate ITE estimation in critical surgical settings such as spinal surgeries.", "method": "The paper introduces X-MultiTask, a multi-task meta-learning framework that treats each surgical decision as a distinct task and learns shared representations across tasks. It incorporates inverse probability weighting (IPW) to ensure causal validity.", "result": "On two datasets, X-MultiTask achieves superior performance, with high AUC and low errors ($\\epsilon_{NN-PEHE}$ and $\\epsilon_{ATE}$), outperforming baseline methods in treatment effect estimation.", "conclusion": "X-MultiTask demonstrates its effectiveness in advancing personalized surgical care by providing robust, patient-specific causal estimates. It has significant potential to improve surgical decision-making and patient outcomes."}}
{"id": "2509.19330", "pdf": "https://arxiv.org/pdf/2509.19330", "abs": "https://arxiv.org/abs/2509.19330", "authors": ["Zejun Liu", "Yunshan Chen", "Chengxi Xie", "Huan Liu"], "title": "LibEMER: A novel benchmark and algorithms library for EEG-based Multimodal Emotion Recognition", "categories": ["eess.SP", "cs.AI", "cs.HC", "cs.LG", "cs.MM"], "comment": "5 pages, 2 figures", "summary": "EEG-based multimodal emotion recognition(EMER) has gained significant\nattention and witnessed notable advancements, the inherent complexity of human\nneural systems has motivated substantial efforts toward multimodal approaches.\nHowever, this field currently suffers from three critical limitations: (i) the\nabsence of open-source implementations. (ii) the lack of standardized and\ntransparent benchmarks for fair performance analysis. (iii) in-depth discussion\nregarding main challenges and promising research directions is a notable\nscarcity. To address these challenges, we introduce LibEMER, a unified\nevaluation framework that provides fully reproducible PyTorch implementations\nof curated deep learning methods alongside standardized protocols for data\npreprocessing, model realization, and experimental setups. This framework\nenables unbiased performance assessment on three widely-used public datasets\nacross two learning tasks. The open-source library is publicly accessible at:\nhttps://anonymous.4open.science/r/2025ULUIUBUEUMUEUR485384", "AI": {"tldr": "The paper presents LibEMER, an open-source evaluation framework for EEG-based multimodal emotion recognition, addressing major limitations in the field like lack of implementations and standard benchmarks.", "motivation": "The field of EEG-based multimodal emotion recognition suffers from a lack of open-source implementations, standardized benchmarks, and in-depth discussions on challenges and future directions.", "method": "LibEMER, a unified open-source evaluation framework, provides reproducible PyTorch implementations, standardized preprocessing and modeling protocols, and supports unbiased performance assessments on three public datasets for two learning tasks.", "result": "An accessible and standardized library called LibEMER, which can fairly evaluate multimodal emotion recognition methods, is presented to enhance transparency and comparability.", "conclusion": "LibEMER addresses critical gaps in EEG-based multimodal emotion recognition by fostering reproducibility, transparency, and providing a unified platform for future research exploration."}}
{"id": "2509.19640", "pdf": "https://arxiv.org/pdf/2509.19640", "abs": "https://arxiv.org/abs/2509.19640", "authors": ["Ryan Shea", "Zhou Yu"], "title": "AutoSpec: An Agentic Framework for Automatically Drafting Patent Specification", "categories": ["cs.CL"], "comment": "EMNLP Findings 2025", "summary": "Patents play a critical role in driving technological innovation by granting\ninventors exclusive rights to their inventions. However the process of drafting\na patent application is often expensive and time-consuming, making it a prime\ncandidate for automation. Despite recent advancements in language models,\nseveral challenges hinder the development of robust automated patent drafting\nsystems. First, the information within a patent application is highly\nconfidential, which often prevents the use of closed-source LLMs for automating\nthis task. Second, the process of drafting a patent application is difficult\nfor even the most advanced language models due to their long context, technical\nwriting style, and specialized domain knowledge. To address these challenges,\nwe introduce AutoSpec, a secure, agentic framework for Automatically drafting\npatent Specification. Our approach decomposes the drafting process into a\nsequence of manageable subtasks, each solvable by smaller, open-source language\nmodels enhanced with custom tools tailored for drafting patent specification.\nTo assess our system, we design a novel evaluation protocol in collaboration\nwith experienced patent attorneys. Our automatic and expert evaluations show\nthat AutoSpec outperforms existing baselines on a patent drafting task.", "AI": {"tldr": "AutoSpec is a framework that automates patent drafting by decomposing the task into smaller subtasks, leveraging open-source language models and specialized tools.", "motivation": "Drafting patent applications is an expensive and time-consuming process, motivating the need for automation.", "method": "AutoSpec decomposes the drafting process into subtasks handled by open-source language models enhanced with custom tools specific to patent drafting.", "result": "AutoSpec demonstrates superior performance compared to baselines, validated through expert and automatic evaluations involving patent attorneys.", "conclusion": "The proposed AutoSpec framework addresses confidentiality concerns and technical challenges, offering a robust solution for automated patent drafting."}}
{"id": "2509.20009", "pdf": "https://arxiv.org/pdf/2509.20009", "abs": "https://arxiv.org/abs/2509.20009", "authors": ["Simon Sch\u00e4fer", "Bassam Alrifaee", "Ehsan Hashemi"], "title": "Lidar-based Tracking of Traffic Participants with Sensor Nodes in Existing Urban Infrastructure", "categories": ["cs.RO"], "comment": "21 pages, 9 figures, this work was submitted to Wileys'Advanced\n  Intelligent Systems for review", "summary": "This paper presents a lidar-only state estimation and tracking framework,\nalong with a roadside sensing unit for integration with existing urban\ninfrastructure. Urban deployments demand scalable, real-time tracking\nsolutions, yet traditional remote sensing remains costly and computationally\nintensive, especially under perceptually degraded conditions. Our sensor node\ncouples a single lidar with an edge computing unit and runs a computationally\nefficient, GPU-free observer that simultaneously estimates object state, class,\ndimensions, and existence probability. The pipeline performs: (i) state updates\nvia an extended Kalman filter, (ii) dimension estimation using a 1D\ngrid-map/Bayesian update, (iii) class updates via a lookup table driven by the\nmost probable footprint, and (iv) existence estimation from track age and\nbounding-box consistency. Experiments in dynamic urban-like scenes with diverse\ntraffic participants demonstrate real-time performance and high precision: The\ncomplete end-to-end pipeline finishes within \\SI{100}{\\milli\\second} for\n\\SI{99.88}{\\%} of messages, with an excellent detection rate. Robustness is\nfurther confirmed under simulated wind and sensor vibration. These results\nindicate that reliable, real-time roadside tracking is feasible on CPU-only\nedge hardware, enabling scalable, privacy-friendly deployments within existing\ncity infrastructure. The framework integrates with existing poles, traffic\nlights, and buildings, reducing deployment costs and simplifying large-scale\nurban rollouts and maintenance efforts.", "AI": {"tldr": "This study introduces a scalable, lidar-only roadside tracking framework optimized for urban environments, ensuring real-time processing on CPU-only edge hardware without relying on GPUs.", "motivation": "Traditional remote sensing systems are expensive and resource-intensive, especially in urban areas with unpredictable environmental challenges, creating the need for scalable, efficient, and cost-effective tracking solutions.", "method": "The approach integrates a lidar sensor with an edge computing unit, executing extended Kalman filtering for state updates, Bayesian updates for dimension estimation, a footprint-based lookup table for class updates, and bounding-box consistency for existence estimation.", "result": "The solution achieves real-time performance, processing within 100 milliseconds for 99.88% of messages, with impressive detection rates and robustness against challenges like wind and vibration.", "conclusion": "This lidar-based framework enables scalable, cost-effective, and privacy-respecting urban deployments through integration with existing city infrastructure, minimizing deployment costs."}}
{"id": "2509.19898", "pdf": "https://arxiv.org/pdf/2509.19898", "abs": "https://arxiv.org/abs/2509.19898", "authors": ["Jiangxue Yu", "Hui Wang", "San Jiang", "Xing Zhang", "Dejin Zhang", "Qingquan Li"], "title": "Aerial-Ground Image Feature Matching via 3D Gaussian Splatting-based Intermediate View Rendering", "categories": ["cs.CV"], "comment": null, "summary": "The integration of aerial and ground images has been a promising solution in\n3D modeling of complex scenes, which is seriously restricted by finding\nreliable correspondences. The primary contribution of this study is a feature\nmatching algorithm for aerial and ground images, whose core idea is to generate\nintermediate views to alleviate perspective distortions caused by the extensive\nviewpoint changes. First, by using aerial images only, sparse models are\nreconstructed through an incremental SfM (Structure from Motion) engine due to\ntheir large scene coverage. Second, 3D Gaussian Splatting is then adopted for\nscene rendering by taking as inputs sparse points and oriented images. For\naccurate view rendering, a render viewpoint determination algorithm is designed\nby using the oriented camera poses of aerial images, which is used to generate\nhigh-quality intermediate images that can bridge the gap between aerial and\nground images. Third, with the aid of intermediate images, reliable feature\nmatching is conducted for match pairs from render-aerial and render-ground\nimages, and final matches can be generated by transmitting correspondences\nthrough intermediate views. By using real aerial and ground datasets, the\nvalidation of the proposed solution has been verified in terms of feature\nmatching and scene rendering and compared comprehensively with widely used\nmethods. The experimental results demonstrate that the proposed solution can\nprovide reliable feature matches for aerial and ground images with an obvious\nincrease in the number of initial and refined matches, and it can provide\nenough matches to achieve accurate ISfM reconstruction and complete 3DGS-based\nscene rendering.", "AI": {"tldr": "The paper presents a feature matching algorithm for 3D modeling using aerial and ground images, focusing on generating intermediate views to mitigate perspective distortions from large viewpoint differences.", "motivation": "Aerial and ground images often face challenges in integration for 3D modeling due to perspective distortions from varying viewpoints. Reliable correspondences are essential for overcoming these issues, motivating this study's focus on feature matching.", "method": "The method involves incremental SfM for sparse model reconstruction using aerial images, 3D Gaussian Splatting for scene rendering, and determining render viewpoints to create intermediate views for feature matching. Features are matched by bridging render, aerial, and ground views.", "result": "Experimental results show improved feature matches, an increase in both initial and refined matches, accurate ISfM reconstruction, and high-quality 3D Gaussian Splatting-based scene rendering.", "conclusion": "The proposed algorithm effectively enhances feature matching between aerial and ground images, facilitates accurate 3D modeling, and outperforms existing methods in terms of match quantity and scene rendering quality."}}
{"id": "2509.19750", "pdf": "https://arxiv.org/pdf/2509.19750", "abs": "https://arxiv.org/abs/2509.19750", "authors": ["Kainat"], "title": "Cuffless Blood Pressure Prediction from Speech Sentences using Deep Learning Methods", "categories": ["cs.LG", "cs.AI"], "comment": "MS Thesis", "summary": "This research presents a novel method for noninvasive arterial blood pressure\nABP prediction using speech signals employing a BERT based regression model\nArterial blood pressure is a vital indicator of cardiovascular health and\naccurate monitoring is essential in preventing hypertension related\ncomplications Traditional cuff based methods often yield inconsistent results\ndue to factors like whitecoat and masked hypertension Our approach leverages\nthe acoustic characteristics of speech capturing voice features to establish\ncorrelations with blood pressure levels Utilizing advanced deep learning\ntechniques we analyze speech signals to extract relevant patterns enabling real\ntime monitoring without the discomfort of conventional methods In our study we\nemployed a dataset comprising recordings from 95 participants ensuring diverse\nrepresentation The BERT model was fine tuned on extracted features from speech\nleading to impressive performance metrics achieving a mean absolute error MAE\nof 136 mmHg for systolic blood pressure SBP and 124 mmHg for diastolic blood\npressure DBP with R scores of 099 and 094 respectively These results indicate\nthe models robustness in accurately predicting blood pressure levels\nFurthermore the training and validation loss analysis demonstrates effective\nlearning and minimal overfitting Our findings suggest that integrating deep\nlearning with speech analysis presents a viable alternative for blood pressure\nmonitoring paving the way for improved applications in telemedicine and remote\nhealth monitoring By providing a user friendly and accurate method for blood\npressure assessment this research has significant implications for enhancing\npatient care and proactive management of cardiovascular health", "AI": {"tldr": "A novel BERT-based AI analyzes speech to predict blood pressure noninvasively, showing high accuracy and potential in health monitoring.", "motivation": "Improve noninvasive blood pressure tracking to overcome limitations of traditional cuff-based methods and to enhance cardiovascular health monitoring.", "method": "Speech signals were analyzed using a fine-tuned BERT regression model with features extracted from recordings of 95 participants.", "result": "The model achieved a mean absolute error of 1.36 mmHg for SBP and 1.24 mmHg for DBP with R-scores of 0.99 and 0.94 respectively, demonstrating strong accuracy and minimal overfitting.", "conclusion": "This speech-analysis-based method provides a reliable, discomfort-free solution for blood pressure monitoring, offering significant potential for telemedicine and cardiovascular care."}}
{"id": "2509.19331", "pdf": "https://arxiv.org/pdf/2509.19331", "abs": "https://arxiv.org/abs/2509.19331", "authors": ["Enhao Huang", "Zhiyu Zhang", "Tianxiang Xu", "Chunshu Xia", "Kaichun Hu", "Yuchen Yang", "Tongtong Pan", "Dong Dong", "Zhan Qin"], "title": "Holographic Transformers for Complex-Valued Signal Processing: Integrating Phase Interference into Self-Attention", "categories": ["eess.SP", "cs.AI", "cs.LG"], "comment": null, "summary": "Complex-valued signals encode both amplitude and phase, yet most deep models\ntreat attention as real-valued correlation, overlooking interference effects.\nWe introduce the Holographic Transformer, a physics-inspired architecture that\nincorporates wave interference principles into self-attention. Holographic\nattention modulates interactions by relative phase and coherently superimposes\nvalues, ensuring consistency between amplitude and phase. A dual-headed decoder\nsimultaneously reconstructs the input and predicts task outputs, preventing\nphase collapse when losses prioritize magnitude over phase. We demonstrate that\nholographic attention implements a discrete interference operator and maintains\nphase consistency under linear mixing. Experiments on PolSAR image\nclassification and wireless channel prediction show strong performance,\nachieving high classification accuracy and F1 scores, low regression error, and\nincreased robustness to phase perturbations. These results highlight that\nenforcing physical consistency in attention leads to generalizable improvements\nin complex-valued learning and provides a unified, physics-based framework for\ncoherent signal modeling. The code is available at\nhttps://github.com/EonHao/Holographic-Transformers.", "AI": {"tldr": "The paper introduces the Holographic Transformer, a physics-inspired model that incorporates wave interference principles in self-attention to enhance complex-valued signal processing.", "motivation": "Most deep learning models overlook the phase element in complex-valued signals, leading to inconsistent modeling that undermines effective signal representation.", "method": "The Holographic Transformer integrates wave interference into self-attention, modulates interactions via relative phase, maintains amplitude-phase consistency, and employs a dual-headed decoder to prevent phase collapse.", "result": "Experiments reveal improved classification accuracy, F1 scores, reduced regression error, and increased robustness to phase perturbations in PolSAR image classification and wireless channel prediction.", "conclusion": "Incorporating physical consistency into attention improves complex-valued learning and provides a coherent, unified framework for signal modeling based on physics principles."}}
{"id": "2509.19657", "pdf": "https://arxiv.org/pdf/2509.19657", "abs": "https://arxiv.org/abs/2509.19657", "authors": ["Yicheng Yang", "Zixian Li", "Jean Paul Bizimana", "Niaz Zafri", "Yongfeng Dong", "Tianyi Li"], "title": "Large Language Models for Pedestrian Safety: An Application to Predicting Driver Yielding Behavior at Unsignalized Intersections", "categories": ["cs.CL", "cs.AI", "cs.SI"], "comment": null, "summary": "Pedestrian safety is a critical component of urban mobility and is strongly\ninfluenced by the interactions between pedestrian decision-making and driver\nyielding behavior at crosswalks. Modeling driver--pedestrian interactions at\nintersections requires accurately capturing the complexity of these behaviors.\nTraditional machine learning models often struggle to capture the nuanced and\ncontext-dependent reasoning required for these multifactorial interactions, due\nto their reliance on fixed feature representations and limited\ninterpretability. In contrast, large language models (LLMs) are suited for\nextracting patterns from heterogeneous traffic data, enabling accurate modeling\nof driver-pedestrian interactions. Therefore, this paper leverages multimodal\nLLMs through a novel prompt design that incorporates domain-specific knowledge,\nstructured reasoning, and few-shot prompting, enabling interpretable and\ncontext-aware inference of driver yielding behavior, as an example application\nof modeling pedestrian--driver interaction. We benchmarked state-of-the-art\nLLMs against traditional classifiers, finding that GPT-4o consistently achieves\nthe highest accuracy and recall, while Deepseek-V3 excels in precision. These\nfindings highlight the critical trade-offs between model performance and\ncomputational efficiency, offering practical guidance for deploying LLMs in\nreal-world pedestrian safety systems.", "AI": {"tldr": "This paper explores leveraging large language models (LLMs), specifically GPT-4o, to model driver-yielding behavior at crosswalks, demonstrating their potential superiority over traditional methods.", "motivation": "The paper aims to enhance pedestrian safety by accurately modeling driver-pedestrian interactions, a task challenging for traditional machine learning methods due to the need for nuanced and context-dependent reasoning.", "method": "The authors use multimodal LLMs with a novel prompt design that embeds domain-specific knowledge, structured reasoning, and few-shot prompting to analyze and model driver-yielding behavior.", "result": "The research finds that GPT-4o outperforms traditional classifiers in accuracy and recall, while Deepseek-V3 excels in precision, highlighting trade-offs in performance and computational efficiency.", "conclusion": "LLMs, with the proposed methods, hold significant promise for deploying high-performing, interpretable models in pedestrian safety systems, despite the trade-offs between accuracy, recall, precision, and computational cost."}}
{"id": "2509.20036", "pdf": "https://arxiv.org/pdf/2509.20036", "abs": "https://arxiv.org/abs/2509.20036", "authors": ["Yinzhao Dong", "Ji Ma", "Liu Zhao", "Wanyue Li", "Peng Lu"], "title": "MARG: MAstering Risky Gap Terrains for Legged Robots with Elevation Mapping", "categories": ["cs.RO"], "comment": null, "summary": "Deep Reinforcement Learning (DRL) controllers for quadrupedal locomotion have\ndemonstrated impressive performance on challenging terrains, allowing robots to\nexecute complex skills such as climbing, running, and jumping. However,\nexisting blind locomotion controllers often struggle to ensure safety and\nefficient traversal through risky gap terrains, which are typically highly\ncomplex, requiring robots to perceive terrain information and select\nappropriate footholds during locomotion accurately. Meanwhile, existing\nperception-based controllers still present several practical limitations,\nincluding a complex multi-sensor deployment system and expensive computing\nresource requirements. This paper proposes a DRL controller named MAstering\nRisky Gap Terrains (MARG), which integrates terrain maps and proprioception to\ndynamically adjust the action and enhance the robot's stability in these tasks.\nDuring the training phase, our controller accelerates policy optimization by\nselectively incorporating privileged information (e.g., center of mass,\nfriction coefficients) that are available in simulation but unmeasurable\ndirectly in real-world deployments due to sensor limitations. We also designed\nthree foot-related rewards to encourage the robot to explore safe footholds.\nMore importantly, a terrain map generation (TMG) model is proposed to reduce\nthe drift existing in mapping and provide accurate terrain maps using only one\nLiDAR, providing a foundation for zero-shot transfer of the learned policy. The\nexperimental results indicate that MARG maintains stability in various risky\nterrain tasks.", "AI": {"tldr": "The paper introduces MARG, a DRL controller integrating terrain information and proprioception to enhance locomotion stability in risky terrains. It uses privileged information during training and a terrain map generation model with one LiDAR for zero-shot policy transfer.", "motivation": "Despite advancements in quadrupedal locomotion, current controllers struggle with risky gap terrains due to challenges in perceiving and navigating complex environments. Existing perception-based methods are also resource-intensive.", "method": "The researchers developed MARG, a DRL-based controller using simulated privileged information and a terrain map generation model with one LiDAR. Three foot-related rewards were included to improve safe foothold navigation.", "result": "MARG demonstrated stability across various risky terrain tasks in experimental testing.", "conclusion": "MARG offers a stable and resource-efficient approach to locomotion in complex terrains, leveraging terrain mapping and proprioception for improved robotic performance."}}
{"id": "2509.19936", "pdf": "https://arxiv.org/pdf/2509.19936", "abs": "https://arxiv.org/abs/2509.19936", "authors": ["Miren Samaniego", "Igor Rodriguez", "Elena Lazkano"], "title": "CapStARE: Capsule-based Spatiotemporal Architecture for Robust and Efficient Gaze Estimation", "categories": ["cs.CV"], "comment": null, "summary": "We introduce CapStARE, a capsule-based spatio-temporal architecture for gaze\nestimation that integrates a ConvNeXt backbone, capsule formation with\nattention routing, and dual GRU decoders specialized for slow and rapid gaze\ndynamics. This modular design enables efficient part-whole reasoning and\ndisentangled temporal modeling, achieving state-of-the-art performance on\nETH-XGaze (3.36) and MPIIFaceGaze (2.65) while maintaining real-time inference\n(< 10 ms). The model also generalizes well to unconstrained conditions in\nGaze360 (9.06) and human-robot interaction scenarios in RT-GENE (4.76),\noutperforming or matching existing methods with fewer parameters and greater\ninterpretability. These results demonstrate that CapStARE offers a practical\nand robust solution for real-time gaze estimation in interactive systems. The\nrelated code and results for this article can be found on:\nhttps://github.com/toukapy/capsStare", "AI": {"tldr": "The paper introduces CapStARE, a real-time gaze estimation model combining advanced architecture elements and achieving state-of-the-art performance on multiple datasets.", "motivation": "To address the need for efficient, real-time gaze estimation solutions adaptable to various conditions and interactions.", "method": "CapStARE integrates a ConvNeXt backbone, capsule formation with attention routing, and dual GRU decoders for specialized slow and rapid gaze dynamics modeling.", "result": "CapStARE achieved state-of-the-art results on ETH-XGaze, MPIIFaceGaze, generalized well on Gaze360 and RT-GENE, with fewer parameters and good interpretability.", "conclusion": "The model proves to be a practical and robust solution for real-time gaze estimation in interactive systems, outperforming or matching existing methods."}}
{"id": "2509.19771", "pdf": "https://arxiv.org/pdf/2509.19771", "abs": "https://arxiv.org/abs/2509.19771", "authors": ["Hyunwoo Kim", "Hyo Kyung Lee"], "title": "Frictional Q-Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We draw an analogy between static friction in classical mechanics and\nextrapolation error in off-policy RL, and use it to formulate a constraint that\nprevents the policy from drifting toward unsupported actions. In this study, we\npresent Frictional Q-learning, a deep reinforcement learning algorithm for\ncontinuous control, which extends batch-constrained reinforcement learning. Our\nalgorithm constrains the agent's action space to encourage behavior similar to\nthat in the replay buffer, while maintaining a distance from the manifold of\nthe orthonormal action space. The constraint preserves the simplicity of\nbatch-constrained, and provides an intuitive physical interpretation of\nextrapolation error. Empirically, we further demonstrate that our algorithm is\nrobustly trained and achieves competitive performance across standard\ncontinuous control benchmarks.", "AI": {"tldr": "The paper introduces Frictional Q-learning for off-policy reinforcement learning, taking inspiration from the physics of static friction to manage extrapolation error.", "motivation": "The researchers aimed to address the challenge of extrapolation error in off-policy reinforcement learning by drawing a parallel to static friction in classical mechanics.", "method": "The Frictional Q-learning algorithm constrains an agent\u2019s action space to stick closer to behavior seen in the replay buffer while distancing itself from unsupported actions, inspired by static friction.", "result": "The algorithm demonstrated robust training and competitive performance on standard continuous control benchmarks.", "conclusion": "The approach simplifies batch-constrained reinforcement learning while offering a novel way to understand and control extrapolation error in continuous control tasks."}}
{"id": "2509.19695", "pdf": "https://arxiv.org/pdf/2509.19695", "abs": "https://arxiv.org/abs/2509.19695", "authors": ["Shuyu Zhang", "Yifan Wei", "Jialuo Yuan", "Xinru Wang", "Yanmin Zhu", "Bin Li"], "title": "DyBBT: Dynamic Balance via Bandit inspired Targeting for Dialog Policy with Cognitive Dual-Systems", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Task oriented dialog systems often rely on static exploration strategies that\ndo not adapt to dynamic dialog contexts, leading to inefficient exploration and\nsuboptimal performance. We propose DyBBT, a novel dialog policy learning\nframework that formalizes the exploration challenge through a structured\ncognitive state space capturing dialog progression, user uncertainty, and slot\ndependency. DyBBT proposes a bandit inspired meta-controller that dynamically\nswitches between a fast intuitive inference (System 1) and a slow deliberative\nreasoner (System 2) based on real-time cognitive states and visitation counts.\nExtensive experiments on single- and multi-domain benchmarks show that DyBBT\nachieves state-of-the-art performance in success rate, efficiency, and\ngeneralization, with human evaluations confirming its decisions are well\naligned with expert judgment. Code is available at\nhttps://github.com/carsonz/DyBBT.", "AI": {"tldr": "DyBBT is a framework for task-oriented dialogue systems that uses dynamic exploration strategies based on cognitive states, yielding state-of-the-art performance.", "motivation": "Task-oriented dialogue systems often struggle with static exploration methods that fail to adapt to dynamic contexts, leading to inefficient performance.", "method": "DyBBT utilizes a bandit-inspired meta-controller to switch between an intuitive inference and deliberative reasoning based on a structured cognitive state space.", "result": "Extensive experiments on various benchmarks showed DyBBT significantly improved success rate, efficiency, and generalization in dialogue systems, confirmed by human evaluations.", "conclusion": "DyBBT presents an adaptive, efficient framework that outperforms existing dialogue systems by leveraging cognitive-state-driven exploration strategies."}}
{"id": "2509.20070", "pdf": "https://arxiv.org/pdf/2509.20070", "abs": "https://arxiv.org/abs/2509.20070", "authors": ["Abraham George", "Amir Barati Farimani"], "title": "LLM Trainer: Automated Robotic Data Generating via Demonstration Augmentation using LLMs", "categories": ["cs.RO"], "comment": "9 pages, 5 figures, 4 tables. Submitted to ICRA 2026", "summary": "We present LLM Trainer, a fully automated pipeline that leverages the world\nknowledge of Large Language Models (LLMs) to transform a small number of human\ndemonstrations (as few as one) into a large robot dataset for imitation\nlearning. Our approach decomposes demonstration generation into two steps: (1)\noffline demonstration annotation that extracts keyframes, salient objects, and\npose-object relations; and (2) online keypose retargeting that adapts those\nkeyframes to a new scene, given an initial observation. Using these modified\nkeypoints, our system warps the original demonstration to generate a new\ntrajectory, which is then executed, and the resulting demo, if successful, is\nsaved. Because the annotation is reusable across scenes, we use Thompson\nsampling to optimize the annotation, significantly improving generation success\nrate. We evaluate our method on a range of tasks, and find that our data\nannotation method consistently outperforms expert-engineered baselines. We\nfurther show an ensemble policy that combines the optimized LLM feed-forward\nplan with a learned feedback imitation learning controller. Finally, we\ndemonstrate hardware feasibility on a Franka Emika Panda robot. For additional\nmaterials and demonstration videos, please see the project website:\nhttps://sites.google.com/andrew.cmu.edu/llm-trainer", "AI": {"tldr": "LLM Trainer automates robot dataset generation by leveraging LLMs for transforming a small number of human demonstrations into reusable, adaptable trajectories.", "motivation": "The paper addresses the challenge of creating large robot datasets for imitation learning from limited human demonstrations, aiming to improve efficiency and usability.", "method": "The approach involves offline annotation (extracting keyframes and pose-object relations) and online retargeting (adapting annotations to new scenes). Probabilistic Thompson Sampling further optimizes the annotation.", "result": "The method outperforms expert baselines in data annotation success, enables robot demonstrations across tasks, combines feed-forward plans with feedback controllers, and demonstrates effectiveness on hardware.", "conclusion": "This pipeline augments robotic imitation learning by automating dataset generation, reducing human effort, and validating its application with successful results on hardware systems."}}
{"id": "2509.19937", "pdf": "https://arxiv.org/pdf/2509.19937", "abs": "https://arxiv.org/abs/2509.19937", "authors": ["Guo Chen", "Jiarun Liu", "Sicong Du", "Chenming Wu", "Deqi Li", "Shi-Sheng Huang", "Guofeng Zhang", "Sheng Yang"], "title": "GS-RoadPatching: Inpainting Gaussians via 3D Searching and Placing for Driving Scenes", "categories": ["cs.CV"], "comment": null, "summary": "This paper presents GS-RoadPatching, an inpainting method for driving scene\ncompletion by referring to completely reconstructed regions, which are\nrepresented by 3D Gaussian Splatting (3DGS). Unlike existing 3DGS inpainting\nmethods that perform generative completion relying on 2D perspective-view-based\ndiffusion or GAN models to predict limited appearance or depth cues for missing\nregions, our approach enables substitutional scene inpainting and editing\ndirectly through the 3DGS modality, extricating it from requiring\nspatial-temporal consistency of 2D cross-modals and eliminating the need for\ntime-intensive retraining of Gaussians. Our key insight is that the highly\nrepetitive patterns in driving scenes often share multi-modal similarities\nwithin the implicit 3DGS feature space and are particularly suitable for\nstructural matching to enable effective 3DGS-based substitutional inpainting.\nPractically, we construct feature-embedded 3DGS scenes to incorporate a patch\nmeasurement method for abstracting local context at different scales and,\nsubsequently, propose a structural search method to find candidate patches in\n3D space effectively. Finally, we propose a simple yet effective\nsubstitution-and-fusion optimization for better visual harmony. We conduct\nextensive experiments on multiple publicly available datasets to demonstrate\nthe effectiveness and efficiency of our proposed method in driving scenes, and\nthe results validate that our method achieves state-of-the-art performance\ncompared to the baseline methods in terms of both quality and interoperability.\nAdditional experiments in general scenes also demonstrate the applicability of\nthe proposed 3D inpainting strategy. The project page and code are available\nat: https://shanzhaguoo.github.io/GS-RoadPatching/", "AI": {"tldr": "GS-RoadPatching introduces a novel approach to driving scene completion using 3D Gaussian Splatting rather than relying on 2D view-based diffusion or GAN models, achieving superior visual quality and efficiency.", "motivation": "Existing inpainting methods face limitations when relying on 2D perspective-driven models, including spatial-temporal inconsistency and retraining overhead. Driving scenes often demand repetitive pattern matching that better suits 3DGS techniques.", "method": "The method leverages feature-embedded 3D Gaussian Splatting scenes for abstracting local context through patch measurements and employs structural search for candidate patches. A substitution-and-fusion optimization strategy ensures visual harmony.", "result": "Experiments on driving scene datasets demonstrate state-of-the-art quality and efficiency of the approach. Additional trials in general scenes affirm its applicability beyond driving contexts.", "conclusion": "GS-RoadPatching refines the use of 3DGS for scene inpainting, offering a superior alternative to 2D generative methods by exploiting multi-modal patterns inherent in driving scenes."}}
{"id": "2509.19773", "pdf": "https://arxiv.org/pdf/2509.19773", "abs": "https://arxiv.org/abs/2509.19773", "authors": ["Jong Kwon Oh", "Hanbaek Lyu", "Hwijae Son"], "title": "Sobolev acceleration for neural networks", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Sobolev training, which integrates target derivatives into the loss\nfunctions, has been shown to accelerate convergence and improve generalization\ncompared to conventional $L^2$ training. However, the underlying mechanisms of\nthis training method remain only partially understood. In this work, we present\nthe first rigorous theoretical framework proving that Sobolev training\naccelerates the convergence of Rectified Linear Unit (ReLU) networks. Under a\nstudent-teacher framework with Gaussian inputs and shallow architectures, we\nderive exact formulas for population gradients and Hessians, and quantify the\nimprovements in conditioning of the loss landscape and gradient-flow\nconvergence rates. Extensive numerical experiments validate our theoretical\nfindings and show that the benefits of Sobolev training extend to modern deep\nlearning tasks.", "AI": {"tldr": "The paper explores Sobolev training, showing its advantage in accelerating convergence and improving generalization by providing a theoretical framework proving these benefits for ReLU networks.", "motivation": "To address the lack of deep theoretical understanding of Sobolev training, especially its convergence acceleration mechanisms.", "method": "The authors analyzed Sobolev training under a student-teacher framework with shallow architectures and Gaussian inputs. They derived explicit formulas for gradients and Hessians and assessed improvements in the loss landscape and gradient-flow rates.", "result": "Sobolev training helps improve conditioning of the loss and accelerates gradient-flow convergence. Numerical experiments further demonstrated its effectiveness in modern deep learning tasks.", "conclusion": "This work offers a theoretical basis for Sobolev training's benefits and confirms its utility in both theoretical and practical deep learning applications."}}
{"id": "2509.19727", "pdf": "https://arxiv.org/pdf/2509.19727", "abs": "https://arxiv.org/abs/2509.19727", "authors": ["Seungjong Sun", "Seo Yeon Baek", "Jang Hyun Kim"], "title": "Personality Vector: Modulating Personality of Large Language Models by Model Merging", "categories": ["cs.CL"], "comment": "EMNLP 2025", "summary": "Driven by the demand for personalized AI systems, there is growing interest\nin aligning the behavior of large language models (LLMs) with human traits such\nas personality. Previous attempts to induce personality in LLMs have shown\npromising results, but they struggle to capture the continuous and\nmultidimensional nature of human traits. In this work, we propose a novel\nmethod for personality modulation in LLMs via model merging. Specifically, we\nconstruct personality vectors by subtracting the weights of a pre-trained model\nfrom those of the fine-tuned model on a given personality trait. By merging\npersonality vectors, we enable LLMs to exhibit desired personality traits\nwithout additional training. Extensive experiments show that personality\nvectors enable continuous control over trait intensity and support the\ncomposition of multiple traits. Furthermore, personality vectors transfer\nacross diverse downstream models, suggesting that they encode generalizable\nrepresentations of personality. Our code is available at here.", "AI": {"tldr": "This paper introduces a novel method to fine-tune personality traits in large language models by leveraging a 'model merging' technique, achieving continuous control and multidimensional trait composition.", "motivation": "Personalized AI systems require language models to align with complex human traits such as personality, which existing methods struggle to capture effectively due to the continuous and multidimensional nature of traits.", "method": "The authors propose constructing 'personality vectors' by comparing the weights of pre-trained and fine-tuned models on specific personality traits. These vectors are merged into LLMs to induce desired personalities without further training.", "result": "Experiments demonstrate that personality vectors allow fine control over the intensity of traits, support the combination of traits, and can generalize across various downstream models.", "conclusion": "The proposed model merging method offers an effective approach to modulate and control personality traits in language models, paving the way for more personalized AI systems."}}
{"id": "2509.20077", "pdf": "https://arxiv.org/pdf/2509.20077", "abs": "https://arxiv.org/abs/2509.20077", "authors": ["Xun Li", "Rodrigo Santa Cruz", "Mingze Xi", "Hu Zhang", "Madhawa Perera", "Ziwei Wang", "Ahalya Ravendran", "Brandon J. Matthews", "Feng Xu", "Matt Adcock", "Dadong Wang", "Jiajun Liu"], "title": "Queryable 3D Scene Representation: A Multi-Modal Framework for Semantic Reasoning and Robotic Task Planning", "categories": ["cs.RO", "cs.CV", "cs.HC"], "comment": null, "summary": "To enable robots to comprehend high-level human instructions and perform\ncomplex tasks, a key challenge lies in achieving comprehensive scene\nunderstanding: interpreting and interacting with the 3D environment in a\nmeaningful way. This requires a smart map that fuses accurate geometric\nstructure with rich, human-understandable semantics. To address this, we\nintroduce the 3D Queryable Scene Representation (3D QSR), a novel framework\nbuilt on multimedia data that unifies three complementary 3D representations:\n(1) 3D-consistent novel view rendering and segmentation from panoptic\nreconstruction, (2) precise geometry from 3D point clouds, and (3) structured,\nscalable organization via 3D scene graphs. Built on an object-centric design,\nthe framework integrates with large vision-language models to enable semantic\nqueryability by linking multimodal object embeddings, and supporting\nobject-level retrieval of geometric, visual, and semantic information. The\nretrieved data are then loaded into a robotic task planner for downstream\nexecution. We evaluate our approach through simulated robotic task planning\nscenarios in Unity, guided by abstract language instructions and using the\nindoor public dataset Replica. Furthermore, we apply it in a digital duplicate\nof a real wet lab environment to test QSR-supported robotic task planning for\nemergency response. The results demonstrate the framework's ability to\nfacilitate scene understanding and integrate spatial and semantic reasoning,\neffectively translating high-level human instructions into precise robotic task\nplanning in complex 3D environments.", "AI": {"tldr": "The paper introduces the 3D Queryable Scene Representation (3D QSR) framework, enabling robots to interpret and interact meaningfully with 3D environments for executing high-level human instructions.", "motivation": "Robots must achieve comprehensive scene understanding to interpret high-level instructions and perform tasks in complex 3D environments.", "method": "The proposed framework unifies three 3D representations (novel view rendering from panoptic reconstruction, precise geometry from 3D point clouds, and 3D scene graphs). It incorporates vision-language models to link multimodal object embeddings and enables robotic task planning by combining spatial and semantic reasoning.", "result": "The framework was evaluated using both Unity-simulated scenarios and a real wet lab environment, demonstrating its ability to bridge abstract human instructions with robotic task planning in complex 3D settings.", "conclusion": "The 3D QSR framework enhances scene understanding, enabling robots to precisely execute tasks by integrating spatial and semantic data into task planning."}}
{"id": "2509.19943", "pdf": "https://arxiv.org/pdf/2509.19943", "abs": "https://arxiv.org/abs/2509.19943", "authors": ["Edmund Bu", "Yossi Gandelsman"], "title": "Interpreting ResNet-based CLIP via Neuron-Attention Decomposition", "categories": ["cs.CV", "cs.AI"], "comment": "NeurIPS 2025 Workshop on Mechanistic Interpretability", "summary": "We present a novel technique for interpreting the neurons in CLIP-ResNet by\ndecomposing their contributions to the output into individual computation\npaths. More specifically, we analyze all pairwise combinations of neurons and\nthe following attention heads of CLIP's attention-pooling layer. We find that\nthese neuron-head pairs can be approximated by a single direction in\nCLIP-ResNet's image-text embedding space. Leveraging this insight, we interpret\neach neuron-head pair by associating it with text. Additionally, we find that\nonly a sparse set of the neuron-head pairs have a significant contribution to\nthe output value, and that some neuron-head pairs, while polysemantic,\nrepresent sub-concepts of their corresponding neurons. We use these\nobservations for two applications. First, we employ the pairs for training-free\nsemantic segmentation, outperforming previous methods for CLIP-ResNet. Second,\nwe utilize the contributions of neuron-head pairs to monitor dataset\ndistribution shifts. Our results demonstrate that examining individual\ncomputation paths in neural networks uncovers interpretable units, and that\nsuch units can be utilized for downstream tasks.", "AI": {"tldr": "This paper introduces a technique for interpreting neurons in CLIP-ResNet by analyzing computation paths, enabling improved semantic segmentation and dataset distribution monitoring.", "motivation": "To better understand and interpret the inner workings of CLIP-ResNet neurons, aiming to enhance its applications and usability.", "method": "The authors decompose neuron contributions into computation paths by analyzing neuron-head pairs and associating these pairs with text in the embedding space.", "result": "Neuron-head pairs were found to contribute significantly and sparsely to outputs, enabling semantic segmentation and dataset monitoring tasks to outperform prior methods.", "conclusion": "Examining individual computation paths within neural networks reveals interpretable units that are beneficial for downstream applications."}}
{"id": "2509.19774", "pdf": "https://arxiv.org/pdf/2509.19774", "abs": "https://arxiv.org/abs/2509.19774", "authors": ["Xiaocheng Fang", "Jiarui Jin", "Haoyu Wang", "Che Liu", "Jieyi Cai", "Guangkun Nie", "Jun Li", "Hongyan Li", "Shenda Hong"], "title": "PPGFlowECG: Latent Rectified Flow with Cross-Modal Encoding for PPG-Guided ECG Generation and Cardiovascular Disease Detection", "categories": ["cs.LG", "cs.AI", "eess.SP"], "comment": null, "summary": "In clinical practice, electrocardiography (ECG) remains the gold standard for\ncardiac monitoring, providing crucial insights for diagnosing a wide range of\ncardiovascular diseases (CVDs). However, its reliance on specialized equipment\nand trained personnel limits feasibility for continuous routine monitoring.\nPhotoplethysmography (PPG) offers accessible, continuous monitoring but lacks\ndefinitive electrophysiological information, preventing conclusive diagnosis.\nGenerative models present a promising approach to translate PPG into clinically\nvaluable ECG signals, yet current methods face substantial challenges,\nincluding the misalignment of physiological semantics in generative models and\nthe complexity of modeling in high-dimensional signals. To this end, we propose\nPPGFlowECG, a two-stage framework that aligns PPG and ECG in a shared latent\nspace via the CardioAlign Encoder and employs latent rectified flow to generate\nECGs with high fidelity and interpretability. To the best of our knowledge,\nthis is the first study to experiment on MCMED, a newly released clinical-grade\ndataset comprising over 10 million paired PPG-ECG samples from more than\n118,000 emergency department visits with expert-labeled cardiovascular disease\nannotations. Results demonstrate the effectiveness of our method for PPG-to-ECG\ntranslation and cardiovascular disease detection. Moreover, cardiologist-led\nevaluations confirm that the synthesized ECGs achieve high fidelity and improve\ndiagnostic reliability, underscoring our method's potential for real-world\ncardiovascular screening.", "AI": {"tldr": "The study introduces PPGFlowECG, a framework to translate PPG signals into ECG signals for real-world cardiovascular screening, demonstrating improved fidelity and diagnostic reliability using a large clinical dataset.", "motivation": "The paper seeks to address the limitations of ECG, which requires specialized equipment and personnel, and PPG, which lacks diagnostic definitiveness, by creating a method to bridge the gap between these two cardiac monitoring techniques.", "method": "The authors propose PPGFlowECG, a two-stage framework composed of a CardioAlign Encoder aligning PPG and ECG in a shared latent space and a latent rectified flow to generate high-quality ECGs. The method is tested on the MCMED dataset.", "result": "The approach effectively translates PPG signals into high-fidelity ECG signals and improves the detection of cardiovascular diseases. Cardiologist-led evaluations confirm the reliability of the generated ECGs.", "conclusion": "PPGFlowECG offers a promising solution for non-invasive, continuous cardiovascular monitoring, enabling effective PPG-to-ECG translation and enhancing diagnostic reliability for CVD detection."}}
{"id": "2509.19335", "pdf": "https://arxiv.org/pdf/2509.19335", "abs": "https://arxiv.org/abs/2509.19335", "authors": ["Xudong Zhang", "Jingbo Tan", "Zhizhen Ren", "Jintao Wang", "Yihua Ma", "Jian Song"], "title": "CSIYOLO: An Intelligent CSI-based Scatter Sensing Framework for Integrated Sensing and Communication Systems", "categories": ["eess.SP", "cs.AI"], "comment": "13 pages, 16 figures, 3 tables. This work has been submitted to the\n  IEEE for possible publication", "summary": "ISAC is regarded as a promising technology for next-generation communication\nsystems, enabling simultaneous data transmission and target sensing. Among\nvarious tasks in ISAC, scatter sensing plays a crucial role in exploiting the\nfull potential of ISAC and supporting applications such as autonomous driving\nand low-altitude economy. However, most existing methods rely on either\nwaveform and hardware modifications or traditional signal processing schemes,\nleading to poor compatibility with current communication systems and limited\nsensing accuracy. To address these challenges, we propose CSIYOLO, a framework\nthat performs scatter localization only using estimated CSI from a single base\nstation-user equipment pair. This framework comprises two main components:\nanchor-based scatter parameter detection and CSI-based scatter localization.\nFirst, by formulating scatter parameter extraction as an image detection\nproblem, we propose an anchor-based scatter parameter detection method inspired\nby You Only Look Once architectures. After that, a CSI-based localization\nalgorithm is derived to determine scatter locations with extracted parameters.\nMoreover, to improve localization accuracy and implementation efficiency, we\ndesign an extendable network structure with task-oriented optimizations,\nenabling multi-scale anchor detection and better adaptation to CSI\ncharacteristics. A noise injection training strategy is further designed to\nenhance robustness against channel estimation errors. Since the proposed\nframework operates solely on estimated CSI without modifying waveforms or\nsignal processing pipelines, it can be seamlessly integrated into existing\ncommunication systems as a plugin. Experiments show that our proposed method\ncan significantly outperform existing methods in scatter localization accuracy\nwith relatively low complexities under varying numbers of scatters and\nestimation errors.", "AI": {"tldr": "This paper introduces CSIYOLO, a framework for scatter localization using estimated Channel State Information (CSI) from a single base station, achieving superior localization accuracy without requiring waveform modification.", "motivation": "To address the lack of compatibility and low accuracy limitations in current ISAC methods for scatter sensing, aiming for applications in areas like autonomous driving and low-altitude navigation.", "method": "The framework uses an anchor-based scatter parameter detection inspired by YOLO architecture and a CSI-based localization algorithm. It employs an extendable network structure and a noise injection training strategy for enhanced robustness and efficiency.", "result": "Experiments reveal that CSIYOLO significantly surpasses existing methods in scatter localization accuracy under different numbers of scatters and estimation errors.", "conclusion": "CSIYOLO seamlessly integrates with existing communication systems, offering high-accuracy scatter localization with low implementation complexities."}}
{"id": "2509.19742", "pdf": "https://arxiv.org/pdf/2509.19742", "abs": "https://arxiv.org/abs/2509.19742", "authors": ["Shuyu Zhang", "Yifan Wei", "Xinru Wang", "Yanmin Zhu", "Yangfan He", "Yixuan Weng", "Bin Li"], "title": "HiCoLoRA: Addressing Context-Prompt Misalignment via Hierarchical Collaborative LoRA for Zero-Shot DST", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Zero-shot Dialog State Tracking (zs-DST) is essential for enabling\nTask-Oriented Dialog Systems (TODs) to generalize to new domains without costly\ndata annotation. A central challenge lies in the semantic misalignment between\ndynamic dialog contexts and static prompts, leading to inflexible cross-layer\ncoordination, domain interference, and catastrophic forgetting. To tackle this,\nwe propose Hierarchical Collaborative Low-Rank Adaptation (HiCoLoRA), a\nframework that enhances zero-shot slot inference through robust prompt\nalignment. It features a hierarchical LoRA architecture for dynamic\nlayer-specific processing (combining lower-layer heuristic grouping and\nhigher-layer full interaction), integrates Spectral Joint Domain-Slot\nClustering to identify transferable associations (feeding an Adaptive Linear\nFusion Mechanism), and employs Semantic-Enhanced SVD Initialization\n(SemSVD-Init) to preserve pre-trained knowledge. Experiments on multi-domain\ndatasets MultiWOZ and SGD show that HiCoLoRA outperforms baselines, achieving\nSOTA in zs-DST. Code is available at https://github.com/carsonz/HiCoLoRA.", "AI": {"tldr": "The paper presents HiCoLoRA, a model designed to improve zero-shot dialogue state tracking by addressing key alignment issues. HiCoLoRA achieves state-of-the-art results on multi-domain datasets MultiWOZ and SGD.", "motivation": "Zero-shot dialogue state tracking is crucial for making task-oriented dialogue systems adaptable to new domains without extensive data labeling. Current systems face challenges like semantic misalignment, rigid system coordination, domain interference, and forgetfulness.", "method": "The paper introduces the HiCoLoRA framework, which uses a hierarchical LoRA architecture for layer-specific processing, spectral joint domain-slot clustering for transferable relationship identification, and semantic-enhanced SVD initialization for preserving pre-trained knowledge.", "result": "HiCoLoRA demonstrated superior performance compared to baseline models, setting state-of-the-art benchmarks on the MultiWOZ and SGD datasets.", "conclusion": "HiCoLoRA addresses semantic misalignment and rigidity issues in zero-shot dialogue state tracking, offering a robust method for dynamic prompt alignment and multi-domain generalization."}}
{"id": "2509.20081", "pdf": "https://arxiv.org/pdf/2509.20081", "abs": "https://arxiv.org/abs/2509.20081", "authors": ["Jose E. Maese", "Luis Merino", "Fernando Caballero"], "title": "DB-TSDF: Directional Bitmask-based Truncated Signed Distance Fields for Efficient Volumetric Mapping", "categories": ["cs.RO"], "comment": null, "summary": "This paper presents a high-efficiency, CPU-only volumetric mapping framework\nbased on a Truncated Signed Distance Field (TSDF). The system incrementally\nfuses raw LiDAR point-cloud data into a voxel grid using a directional\nbitmask-based integration scheme, producing dense and consistent TSDF\nrepresentations suitable for real-time 3D reconstruction. A key feature of the\napproach is that the processing time per point-cloud remains constant,\nregardless of the voxel grid resolution, enabling high resolution mapping\nwithout sacrificing runtime performance. In contrast to most recent TSDF/ESDF\nmethods that rely on GPU acceleration, our method operates entirely on CPU,\nachieving competitive results in speed. Experiments on real-world open datasets\ndemonstrate that the generated maps attain accuracy on par with contemporary\nmapping techniques.", "AI": {"tldr": "This paper introduces a CPU-based volumetric mapping framework using TSDF that processes LiDAR data efficiently, maintaining constant runtime regardless of resolution, with accuracy comparable to GPU-accelerated methods.", "motivation": "To develop a high-resolution volumetric mapping framework suitable for efficient real-time 3D reconstruction without relying on GPU acceleration.", "method": "A directional bitmask-based integration scheme is utilized to fuse LiDAR point-cloud data incrementally into voxel grids, ensuring consistent TSDF representation with constant processing time per cloud.", "result": "The proposed system achieves real-time processing performance while matching the accuracy of advanced mapping methods, demonstrated through experiments with real-world datasets.", "conclusion": "The framework successfully provides competitive real-time, high-resolution mapping capabilities using CPU resources, proving to be an efficient alternative to GPU-based approaches."}}
{"id": "2509.19952", "pdf": "https://arxiv.org/pdf/2509.19952", "abs": "https://arxiv.org/abs/2509.19952", "authors": ["Sarmistha Das", "R E Zera Marveen Lyngkhoi", "Kirtan Jain", "Vinayak Goyal", "Sriparna Saha", "Manish Gupta"], "title": "When Words Can't Capture It All: Towards Video-Based User Complaint Text Generation with Multimodal Video Complaint Dataset", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "While there exists a lot of work on explainable complaint mining,\narticulating user concerns through text or video remains a significant\nchallenge, often leaving issues unresolved. Users frequently struggle to\nexpress their complaints clearly in text but can easily upload videos depicting\nproduct defects (e.g., vague text such as `worst product' paired with a\n5-second video depicting a broken headphone with the right earcup). This paper\nformulates a new task in the field of complaint mining to aid the common users'\nneed to write an expressive complaint, which is Complaint Description from\nVideos (CoD-V) (e.g., to help the above user articulate her complaint about the\ndefective right earcup). To this end, we introduce ComVID, a video complaint\ndataset containing 1,175 complaint videos and the corresponding descriptions,\nalso annotated with the emotional state of the complainer. Additionally, we\npresent a new complaint retention (CR) evaluation metric that discriminates the\nproposed (CoD-V) task against standard video summary generation and description\ntasks. To strengthen this initiative, we introduce a multimodal\nRetrieval-Augmented Generation (RAG) embedded VideoLLaMA2-7b model, designed to\ngenerate complaints while accounting for the user's emotional state. We conduct\na comprehensive evaluation of several Video Language Models on several tasks\n(pre-trained and fine-tuned versions) with a range of established evaluation\nmetrics, including METEOR, perplexity, and the Coleman-Liau readability score,\namong others. Our study lays the foundation for a new research direction to\nprovide a platform for users to express complaints through video. Dataset and\nresources are available at: https://github.com/sarmistha-D/CoD-V.", "AI": {"tldr": "The paper introduces the task of generating complaint descriptions from videos (CoD-V), alongside a new dataset (ComVID) and a model for this purpose.", "motivation": "Many users struggle to express their complaints clearly in text, despite being able to provide illustrative videos, leading to unresolved issues. This gap signifies a need for better tools to convert videos into articulate complaints.", "method": "The authors formulated the CoD-V task and introduced the ComVID dataset of complaint videos and descriptions. They also developed a Retrieval-Augmented Generation (RAG) embedded VideoLLaMA2-7b model, considering the complainant's emotional state, and evaluated it with metrics like METEOR and readability scores.", "result": "The study provided the first dataset and benchmark for CoD-V, introduced a new evaluation metric called complaint retention (CR), and demonstrated that multimodal models can generate quality complaint descriptions.", "conclusion": "The research establishes a new direction in complaint mining, enabling users to transform video complaints into articulate textual descriptions. The provided dataset and resources pave the way for future advancements."}}
{"id": "2509.19781", "pdf": "https://arxiv.org/pdf/2509.19781", "abs": "https://arxiv.org/abs/2509.19781", "authors": ["Ziyi Han", "Xutong Liu", "Ruiting Zhou", "Xiangxiang Dai", "John C. S. Lui"], "title": "Faster, Smaller, and Smarter: Task-Aware Expert Merging for Online MoE Inference", "categories": ["cs.LG"], "comment": null, "summary": "Sparse Mixture of Experts (SMoE) has become a preferred architecture for\nscaling Transformer capacity without increasing computational cost, as it\nactivates only a small subset of experts for each input. However, deploying\nsuch an approach for \\textit{online inference} remains challenging due to the\nlarge size of a full SMoE model and the complexity of expert routing,\nespecially in resource-constrained edge networks. Moreover, during the online\ninference, task information is often unavailable, making the task-level routing\nerror-prone. In this work, we propose a novel tree-structured adaptive neural\nbandit router, \\texttt{Tanbr}, to enable efficient and reliable online MoE\ninference. Instead of relying on explicit task tags, \\texttt{Tanbr} estimates\nthe task distribution over time from historical data and uses it to guide\ntask-aware expert merging within a given pre-trained MoE. To handle the large\ncontinuous space of merging weights, \\texttt{Tanbr} employs a binary tree to\nprogressively partition the space and generate finer candidate weights. It then\napplies a neural bandit to learn the non-linear mapping from merging weight to\nmodel performance and decides optimal expert merging. We prove that\n\\texttt{Tanbr} achieves a sublinear regret bound of {\\small\n$\\mathcal{O}(\\sqrt{T} \\log(T))$} over {\\small $T$} rounds, despite operating\nover a continuous decision space, matching regret bounds compared to existing\nmethods. Extensive experiments show that \\texttt{Tanbr} reduces inference\nlatency by at least {\\small $45\\%$} and memory usage by up to {\\small $25\\%$},\nwhile maintaining a high accuracy compared to many state-of-the-art methods.", "AI": {"tldr": "The paper introduces the novel Tanbr algorithm, a tree-structured adaptive neural bandit router to enable efficient online inference in Sparse Mixture of Experts models, optimizing routing without explicit task information.", "motivation": "Sparse Mixture of Experts models pose challenges for online inference due to their large model size and complex expert routing, especially in edge networks where task-specific information is often unavailable.", "method": "The proposed Tanbr algorithm estimates task distribution over time using historical data and employs a binary tree structure to refine merging weights in a continuous space. It further utilizes a neural bandit approach to learn optimal expert merging without relying on explicit task tags.", "result": "Experiments demonstrate substantial improvements: reduced inference latency by 45%, decreased memory usage by 25%, and competitive accuracy levels compared to state-of-the-art alternatives.", "conclusion": "Tanbr proves to be an effective solution for overcoming routing challenges in Sparse Mixture of Experts models during online inference, showing scalability and reliability in resource-constrained settings."}}
{"id": "2509.19745", "pdf": "https://arxiv.org/pdf/2509.19745", "abs": "https://arxiv.org/abs/2509.19745", "authors": ["Pei Zhang", "Andong Chen", "Xi Chen", "Baosong Yang", "Derek F. Wong", "Fei Huang"], "title": "PART: Progressive Alignment Representation Training for Multilingual Speech-To-Text with LLMs", "categories": ["cs.CL", "cs.SD"], "comment": null, "summary": "Large language models (LLMs) have expanded from text to speech, giving rise\nto Speech Large Models (SLMs) that support recognition, translation, and\nsynthesis. A key challenge is aligning speech and text representations, which\nbecomes harder in multilingual settings. Existing methods often freeze LLM\nparameters and train encoders on multilingual data, but this forces\ncross-language convergence and limits performance. We introduce Progressive\nAlignment Representation Training (PART), a multi-stage and multi-task\nframework that separates within-language from cross-language alignment. During\ncross-language training, LLM parameters are dynamically activated, and\ntext-based tasks are later introduced to enhance multilingual understanding.\nExperiments on CommonVoice 15, Fleurs, Wenetspeech, and CoVoST2 show that PART\nsurpasses conventional approaches, with analysis confirming its ability to\nbalance language-specific distinctions and cross-language generalization. These\nresults demonstrate PART's effectiveness and generality for multilingual speech\nmodality alignment.", "AI": {"tldr": "This paper introduces Progressive Alignment Representation Training (PART), a framework to improve multilingual speech and text alignment in speech large models (SLMs), outperforming existing methods.", "motivation": "Aligning speech and text representations in multilingual settings is challenging, and current methods forcing cross-language convergence limit model performance.", "method": "The authors propose PART, a multi-stage framework that separates language-specific and cross-language alignment. PART dynamically activates LLM parameters during cross-language training and incorporates text tasks to enhance multilingual understanding.", "result": "Experiments on datasets like CommonVoice 15 and CoVoST2 show PART surpasses traditional methods, achieving better balance between language-specific details and cross-language generalization.", "conclusion": "PART offers an effective approach to multilingual speech and text modality alignment, demonstrating improved performance and applicability across languages."}}
{"id": "2509.20082", "pdf": "https://arxiv.org/pdf/2509.20082", "abs": "https://arxiv.org/abs/2509.20082", "authors": ["Surov Maksim"], "title": "Orbital Stabilization and Time Synchronization of Unstable Periodic Motions in Underactuated Robots", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "This paper presents a control methodology for achieving orbital stabilization\nwith simultaneous time synchronization of periodic trajectories in\nunderactuated robotic systems. The proposed approach extends the classical\ntransverse linearization framework to explicitly incorporate\ntime-desynchronization dynamics. To stabilize the resulting extended transverse\ndynamics, we employ a combination of time-varying LQR and sliding-mode control.\nThe theoretical results are validated experimentally through the implementation\nof both centralized and decentralized control strategies on a group of six\nButterfly robots.", "AI": {"tldr": "This paper proposes a control methodology combining time-varying LQR and sliding-mode control to achieve orbital stabilization and time synchronization in underactuated robotic systems, validated on Butterfly robots.", "motivation": "Underactuated robotic systems face challenges in achieving synchronization and stabilization of periodic trajectories.", "method": "The paper revises the transverse linearization framework to incorporate time-desynchronization dynamics, using time-varying LQR and sliding-mode control for stabilization.", "result": "The approach is tested experimentally with centralized and decentralized control strategies on a group of six Butterfly robots.", "conclusion": "The methodology successfully achieves orbital stabilization and time synchronization in underactuated robotic systems."}}
{"id": "2509.19965", "pdf": "https://arxiv.org/pdf/2509.19965", "abs": "https://arxiv.org/abs/2509.19965", "authors": ["Phyo Thet Yee", "Dimitrios Kollias", "Sudeepta Mishra", "Abhinav Dhall"], "title": "SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding", "categories": ["cs.CV"], "comment": "Accepted at WACV 2026, project page :\n  https://novicemm.github.io/synchrorama", "summary": "Audio-driven talking face generation has received growing interest,\nparticularly for applications requiring expressive and natural human-avatar\ninteraction. However, most existing emotion-aware methods rely on a single\nmodality (either audio or image) for emotion embedding, limiting their ability\nto capture nuanced affective cues. Additionally, most methods condition on a\nsingle reference image, restricting the model's ability to represent dynamic\nchanges in actions or attributes across time. To address these issues, we\nintroduce SynchroRaMa, a novel framework that integrates a multi-modal emotion\nembedding by combining emotional signals from text (via sentiment analysis) and\naudio (via speech-based emotion recognition and audio-derived valence-arousal\nfeatures), enabling the generation of talking face videos with richer and more\nauthentic emotional expressiveness and fidelity. To ensure natural head motion\nand accurate lip synchronization, SynchroRaMa includes an audio-to-motion (A2M)\nmodule that generates motion frames aligned with the input audio. Finally,\nSynchroRaMa incorporates scene descriptions generated by Large Language Model\n(LLM) as additional textual input, enabling it to capture dynamic actions and\nhigh-level semantic attributes. Conditioning the model on both visual and\ntextual cues enhances temporal consistency and visual realism. Quantitative and\nqualitative experiments on benchmark datasets demonstrate that SynchroRaMa\noutperforms the state-of-the-art, achieving improvements in image quality,\nexpression preservation, and motion realism. A user study further confirms that\nSynchroRaMa achieves higher subjective ratings than competing methods in\noverall naturalness, motion diversity, and video smoothness. Our project page\nis available at <https://novicemm.github.io/synchrorama>.", "AI": {"tldr": "This paper introduces SynchroRaMa, a new system for generating emotionally expressive and realistic talking face videos by leveraging multi-modal emotion embeddings from text and audio, as well as dynamic scene inputs.", "motivation": "Most existing methods for talking face generation rely on single modalities for emotion embedding and use single reference images, limiting their ability to generate expressive and dynamic outputs.", "method": "The proposed SynchroRaMa system combines emotion embeddings from text sentiment analysis and audio features, integrates an audio-to-motion module for synchronization, and utilizes scene descriptions generated by Large Language Models (LLMs) for dynamic attribute control.", "result": "SynchroRaMa outperforms current state-of-the-art methods in terms of image quality, expression accuracy, and motion realism. Both quantitative and qualitative experiments, as well as user studies, validate its superior performance in naturalness, motion diversity, and smoothness.", "conclusion": "This framework advances talking face generation by addressing limitations of single-modality emotion embedding and temporal inconsistency, enabling more expressive and realistic talking face videos."}}
{"id": "2509.19789", "pdf": "https://arxiv.org/pdf/2509.19789", "abs": "https://arxiv.org/abs/2509.19789", "authors": ["Carlo Bosio", "Greg Woelki", "Noureldin Hendy", "Nicholas Roy", "Byungsoo Kim"], "title": "RDAR: Reward-Driven Agent Relevance Estimation for Autonomous Driving", "categories": ["cs.LG", "cs.AI", "cs.RO"], "comment": "10 pages, 6 figures", "summary": "Human drivers focus only on a handful of agents at any one time. On the other\nhand, autonomous driving systems process complex scenes with numerous agents,\nregardless of whether they are pedestrians on a crosswalk or vehicles parked on\nthe side of the road. While attention mechanisms offer an implicit way to\nreduce the input to the elements that affect decisions, existing attention\nmechanisms for capturing agent interactions are quadratic, and generally\ncomputationally expensive. We propose RDAR, a strategy to learn per-agent\nrelevance -- how much each agent influences the behavior of the controlled\nvehicle -- by identifying which agents can be excluded from the input to a\npre-trained behavior model. We formulate the masking procedure as a Markov\nDecision Process where the action consists of a binary mask indicating agent\nselection. We evaluate RDAR on a large-scale driving dataset, and demonstrate\nits ability to learn an accurate numerical measure of relevance by achieving\ncomparable driving performance, in terms of overall progress, safety and\nperformance, while processing significantly fewer agents compared to a state of\nthe art behavior model.", "AI": {"tldr": "This paper introduces RDAR, a method to optimize autonomous driving systems by focusing only on relevant agents in a scene, significantly reducing computational complexity while preserving driving performance.", "motivation": "Human drivers naturally focus on a few relevant agents while driving, but autonomous systems lack this ability, leading to unnecessary computational overhead when processing all agents in a scene.", "method": "The authors propose RDAR, which learns the relevance of each agent in a traffic scene through a Markov Decision Process that applies binary masking to exclude irrelevant agents. The method uses a pre-trained behavior model as the baseline.", "result": "RDAR demonstrates comparable driving performance (progress, safety, and general behavior) while processing far fewer agents than current state-of-the-art behavior models, validated on a large-scale driving dataset.", "conclusion": "RDAR effectively improves efficiency in autonomous driving systems by intelligently selecting relevant agents, showing the potential to maintain performance while reducing computational demands."}}
{"id": "2509.19337", "pdf": "https://arxiv.org/pdf/2509.19337", "abs": "https://arxiv.org/abs/2509.19337", "authors": ["Stefanos Bakirtzis", "Paul Almasan", "Jos\u00e9 Su\u00e1rez-Varela", "Gabriel O. Ferreira", "Michail Kalntis", "Andr\u00e9 Felipe Zanella", "Ian Wassell", "Andra Lutu"], "title": "Radio Propagation Modelling: To Differentiate or To Deep Learn, That Is The Question", "categories": ["cs.NI", "cs.AI"], "comment": null, "summary": "Differentiable ray tracing has recently challenged the status quo in radio\npropagation modelling and digital twinning. Promising unprecedented speed and\nthe ability to learn from real-world data, it offers a real alternative to\nconventional deep learning (DL) models. However, no experimental evaluation on\nproduction-grade networks has yet validated its assumed scalability or\npractical benefits. This leaves mobile network operators (MNOs) and the\nresearch community without clear guidance on its applicability. In this paper,\nwe fill this gap by employing both differentiable ray tracing and DL models to\nemulate radio coverage using extensive real-world data collected from the\nnetwork of a major MNO, covering 13 cities and more than 10,000 antennas. Our\nresults show that, while differentiable ray-tracing simulators have contributed\nto reducing the efficiency-accuracy gap, they struggle to generalize from\nreal-world data at a large scale, and they remain unsuitable for real-time\napplications. In contrast, DL models demonstrate higher accuracy and faster\nadaptation than differentiable ray-tracing simulators across urban, suburban,\nand rural deployments, achieving accuracy gains of up to 3 dB. Our experimental\nresults aim to provide timely insights into a fundamental open question with\ndirect implications on the wireless ecosystem and future research.", "AI": {"tldr": "The paper evaluates differentiable ray tracing against deep learning models for large-scale radio coverage emulation using real-world data. The results favor deep learning in accuracy and adaptability.", "motivation": "To validate the scalability and practical benefits of differentiable ray tracing in production-grade networks and provide guidance for mobile network operators and the research community.", "method": "Comparing differentiable ray tracing and deep learning models using extensive real-world data covering 13 cities and over 10,000 antennas.", "result": "Differentiable ray-tracing simulators have reduced the efficiency-accuracy gap but struggle to generalize at scale, whereas DL models achieve up to 3 dB accuracy gains and adapt faster across various deployments.", "conclusion": "DL models outperform differentiable ray tracing in accuracy and adaptability, making them more suitable for large-scale and real-time applications in radio network emulation."}}
{"id": "2509.19768", "pdf": "https://arxiv.org/pdf/2509.19768", "abs": "https://arxiv.org/abs/2509.19768", "authors": ["Sina J. Semnani", "Han Zhang", "Xinyan He", "Merve Tekg\u00fcrler", "Monica S. Lam"], "title": "CHURRO: Making History Readable with an Open-Weight Large Vision-Language Model for High-Accuracy, Low-Cost Historical Text Recognition", "categories": ["cs.CL", "cs.CV"], "comment": "EMNLP 2025", "summary": "Accurate text recognition for historical documents can greatly advance the\nstudy and preservation of cultural heritage. Existing vision-language models\n(VLMs), however, are designed for modern, standardized texts and are not\nequipped to read the diverse languages and scripts, irregular layouts, and\nfrequent degradation found in historical materials.\n  This paper presents CHURRO, a 3B-parameter open-weight VLM specialized for\nhistorical text recognition. The model is trained on CHURRO-DS, the largest\nhistorical text recognition dataset to date. CHURRO-DS unifies 155 historical\ncorpora comprising 99,491 pages, spanning 22 centuries of textual heritage\nacross 46 language clusters, including historical variants and dead languages.\n  We evaluate several open-weight and closed VLMs and optical character\nrecognition (OCR) systems on CHURRO-DS and find that CHURRO outperforms all\nother VLMs. On the CHURRO-DS test set, CHURRO achieves 82.3% (printed) and\n70.1% (handwritten) normalized Levenshtein similarity, surpassing the\nsecond-best model, Gemini 2.5 Pro, by 1.4% and 6.5%, respectively, while being\n15.5 times more cost-effective.\n  By releasing the model and dataset, we aim to enable community-driven\nresearch to improve the readability of historical texts and accelerate\nscholarship.", "AI": {"tldr": "CHURRO is a specialized vision-language model (VLM) for historical text recognition, trained on the largest dataset for this purpose, achieving superior performance and cost-efficiency compared to existing models.", "motivation": "To improve the readability and study of historical texts, which are often challenging due to their irregular layouts, diverse scripts, and frequent degradation, by addressing the limitations of existing VLMs focused on modern standardized texts.", "method": "The paper introduces CHURRO, a 3-billion-parameter VLM, trained on CHURRO-DS, a dataset comprising 99,491 pages from 155 historical corpora, spanning 46 language clusters across 22 centuries.", "result": "CHURRO achieves top performance in historical text recognition, with 82.3% similarity for printed texts and 70.1% for handwritten texts, outperforming the second-best model in both metrics while being 15.5 times more cost-effective.", "conclusion": "Releasing CHURRO and CHURRO-DS aims to foster community-driven research, improving historical text readability and accelerating the study of cultural heritage."}}
{"id": "2509.20084", "pdf": "https://arxiv.org/pdf/2509.20084", "abs": "https://arxiv.org/abs/2509.20084", "authors": ["Guillermo Gil", "Jose Antonio Cobano", "Luis Merino", "Fernando Caballero"], "title": "C-3TO: Continuous 3D Trajectory Optimization on Neural Euclidean Signed Distance Fields", "categories": ["cs.RO"], "comment": "9 pages, 5 figures, submitted to ICRA 2026", "summary": "This paper introduces a novel framework for continuous 3D trajectory\noptimization in cluttered environments, leveraging online neural Euclidean\nSigned Distance Fields (ESDFs). Unlike prior approaches that rely on\ndiscretized ESDF grids with interpolation, our method directly optimizes smooth\ntrajectories represented by fifth-order polynomials over a continuous neural\nESDF, ensuring precise gradient information throughout the entire trajectory.\nThe framework integrates a two-stage nonlinear optimization pipeline that\nbalances efficiency, safety and smoothness. Experimental results demonstrate\nthat C-3TO produces collision-aware and dynamically feasible trajectories.\nMoreover, its flexibility in defining local window sizes and optimization\nparameters enables straightforward adaptation to diverse user's needs without\ncompromising performance. By combining continuous trajectory parameterization\nwith a continuously updated neural ESDF, C-3TO establishes a robust and\ngeneralizable foundation for safe and efficient local replanning in aerial\nrobotics.", "AI": {"tldr": "The paper introduces a framework (C-3TO) for continuous 3D trajectory optimization in cluttered environments using neural Euclidean Signed Distance Fields (ESDFs).", "motivation": "The research aims to improve the efficiency and accuracy of trajectory optimization in 3D cluttered environments and addresses limitations of prior ESDF-based methods that rely on discretized grids.", "method": "The proposed approach utilizes a neural ESDF for smooth trajectory representation with fifth-order polynomials and employs a two-stage nonlinear optimization pipeline.", "result": "C-3TO generates collision-aware and dynamically feasible 3D trajectories, showing adaptability to various environmental and user-defined constraints.", "conclusion": "The framework establishes a reliable ground for safe and efficient aerial robotic local replanning with generalizability for diverse applications."}}
{"id": "2509.19973", "pdf": "https://arxiv.org/pdf/2509.19973", "abs": "https://arxiv.org/abs/2509.19973", "authors": ["Pei Liu", "Hongliang Lu", "Haichao Liu", "Haipeng Liu", "Xin Liu", "Ruoyu Yao", "Shengbo Eben Li", "Jun Ma"], "title": "OmniScene: Attention-Augmented Multimodal 4D Scene Understanding for Autonomous Driving", "categories": ["cs.CV"], "comment": null, "summary": "Human vision is capable of transforming two-dimensional observations into an\negocentric three-dimensional scene understanding, which underpins the ability\nto translate complex scenes and exhibit adaptive behaviors. This capability,\nhowever, remains lacking in current autonomous driving systems, where\nmainstream approaches primarily rely on depth-based 3D reconstruction rather\nthan true scene understanding. To address this limitation, we propose a novel\nhuman-like framework called OmniScene. First, we introduce the OmniScene\nVision-Language Model (OmniVLM), a vision-language framework that integrates\nmulti-view and temporal perception for holistic 4D scene understanding. Then,\nharnessing a teacher-student OmniVLM architecture and knowledge distillation,\nwe embed textual representations into 3D instance features for semantic\nsupervision, enriching feature learning, and explicitly capturing human-like\nattentional semantics. These feature representations are further aligned with\nhuman driving behaviors, forming a more human-like\nperception-understanding-action architecture. In addition, we propose a\nHierarchical Fusion Strategy (HFS) to address imbalances in modality\ncontributions during multimodal integration. Our approach adaptively calibrates\nthe relative significance of geometric and semantic features at multiple\nabstraction levels, enabling the synergistic use of complementary cues from\nvisual and textual modalities. This learnable dynamic fusion enables a more\nnuanced and effective exploitation of heterogeneous information. We evaluate\nOmniScene comprehensively on the nuScenes dataset, benchmarking it against over\nten state-of-the-art models across various tasks. Our approach consistently\nachieves superior results, establishing new benchmarks in perception,\nprediction, planning, and visual question answering.", "AI": {"tldr": "The paper introduces \"OmniScene,\" a human-like framework for 4D scene understanding in autonomous driving systems, combining multimodal perception and dynamic fusion strategies to outperform state-of-the-art models.", "motivation": "Autonomous driving systems lack true 3D egocentric scene understanding akin to human vision, hindering complex decision-making capabilities.", "method": "The authors developed OmniScene, incorporating OmniVLM (a vision-language model), knowledge distillation, and a hierarchical fusion strategy for integrating geometric and semantic features across modalities.", "result": "OmniScene consistently surpasses ten state-of-the-art models in various tasks such as perception, planning, prediction, and visual question answering on the nuScenes dataset.", "conclusion": "OmniScene introduces an advanced framework that bridges the gap between human-like scene understanding and autonomous driving systems, enhancing their adaptability and decision-making capabilities."}}
{"id": "2509.19803", "pdf": "https://arxiv.org/pdf/2509.19803", "abs": "https://arxiv.org/abs/2509.19803", "authors": ["Guochao Jiang", "Wenfeng Feng", "Guofeng Quan", "Chuzhan Hao", "Yuewei Zhang", "Guohua Liu", "Hao Wang"], "title": "VCRL: Variance-based Curriculum Reinforcement Learning for Large Language Models", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Policy-based reinforcement learning currently plays an important role in\nimproving LLMs on mathematical reasoning tasks. However, existing rollout-based\nreinforcement learning methods (GRPO, DAPO, GSPO, etc.) fail to explicitly\nconsider LLMs' learning ability for samples of different difficulty levels,\nwhich is contrary to the human cognitive process of mathematical reasoning\ntasks from easy to difficult. Intuitively, we find that the variance of the\nrollout group's reward in RLVR partly reflects the difficulty of the current\nsample for LLMs. Samples that are too easy or too difficult have a lower\nvariance, while samples with moderate difficulty have a higher variance. Based\non this, we propose VCRL, a curriculum reinforcement learning framework that\ndynamically controls the difficulty of training samples based on the variance\nof group rewards. Experiments on five mathematical benchmarks and two models\nreveal the advantages of VCRL over the current LLM RL baselines.", "AI": {"tldr": "This paper introduces VCRL, a novel curriculum reinforcement learning framework that dynamically adjusts training sample difficulty based on reward variance, enhancing mathematical reasoning in LLMs.", "motivation": "Existing reinforcement learning methods for LLMs in mathematical reasoning tasks overlook the importance of training with samples of varying difficulty levels, which is contrary to the natural human cognitive process.", "method": "The authors propose VCRL, using the variance of rollout group's rewards to identify sample difficulty. Samples are dynamically selected for training to match their difficulty levels with the learning progress.", "result": "Experiments conducted on five mathematical benchmarks and two models demonstrated that VCRL surpasses existing reinforcement learning baselines for LLMs.", "conclusion": "VCRL effectively aligns LLM training with a human-like learning progression, improving performance on mathematical reasoning tasks."}}
