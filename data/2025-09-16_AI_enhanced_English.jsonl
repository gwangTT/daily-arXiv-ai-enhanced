{"id": "2509.10711", "pdf": "https://arxiv.org/pdf/2509.10711", "abs": "https://arxiv.org/abs/2509.10711", "authors": ["Subhajit Pramanick", "Saswata Jana", "Partha Sarathi Mandal", "Gokarna Sharma"], "title": "Asynchronous Gathering of Opaque Robots with Mobility Faults", "categories": ["cs.DC", "cs.MA", "cs.RO"], "comment": "38 pages, 26 figures, and 1 table", "summary": "We consider the fundamental benchmarking problem of gathering in an\n$(N,f)$-fault system consisting of $N$ robots, of which at most $f$ might fail\nat any execution, under asynchrony. Two seminal results established\nimpossibility of a solution in the oblivious robot (OBLOT) model in a\n$(2,0)$-fault system under semi-synchrony and in a $(3,1)$-Byzantine fault\nsystem under asynchrony. Recently, a breakthrough result circumvented the first\nimpossibility result by giving a deterministic algorithm in a $(2,0)$-fault\nsystem under asynchrony in the luminous robot (LUMI) model using 2-colored\nlights. However, a breakthrough result established impossibility of gathering\nin a $(2,1)$-crash system in the LUMI model under semi-synchrony. In this\npaper, we consider a {\\em mobility fault} model in which a robot crash only\nimpacts it mobility but not the operation of the light.\n  We establish four results under asynchrony in LUMI with the mobility fault\nmodel. We show that it is impossible to solve gathering in a $(2,1)$-mobility\nfault system using 2-colored lights, and then give a solution using 3-colored\nlights, which is optimal w.r.t. the number of colors. We then consider an\n$(N,f)$-mobility fault system, $f<N$, both $N,f$ not known, and give two\ndeterministic algorithms that exhibit a nice time-color trade-off: The first\nwith time $O(N)$ using 7-colored lights and the second with time\n$O(\\max\\{\\ell,f\\})$ using 26-colored lights, where $\\ell< N$ is the number of\ndistinct convex layers of robot positions in the initial configuration.\nInterestingly, for $l, f = O(1)$, our result is optimal. Our algorithms for an\n$(N,f)$-mobility fault system are the first to be analysed time complexity, can\nwithstand obstructed visibility (opaque robot model) and asynchronous\nscheduling.", "AI": {"tldr": "This paper addresses robot gathering under different fault and asynchrony conditions, proposing algorithms with varying light colors and analyzing time complexities.", "motivation": "To overcome previous impossibility results in robot gathering under fault systems using models with limited resources and conditions.", "method": "The authors establish its impossibility and propose algorithms using mobility fault model leveraging color lights and asynchronous scheduling.", "result": "Four key results are presented: impossibility with 2-colored lights under certain conditions, solutions with 3-colored lights, algorithms for unknown (N,f) systems with time-color trade-offs.", "conclusion": "Introduces new deterministic algorithms optimized for time, colors, and system conditions making progress in fault-tolerant robot gathering."}}
{"id": "2509.10712", "pdf": "https://arxiv.org/pdf/2509.10712", "abs": "https://arxiv.org/abs/2509.10712", "authors": ["Rahma Nouaji", "Stella Bitchebe", "Ricardo Macedo", "Oana Balmau"], "title": "MinatoLoader: Accelerating Machine Learning Training Through Efficient Data Preprocessing", "categories": ["cs.DC", "cs.LG"], "comment": "Paper accepted at EuroSys 2026 (will be updated after the\n  camera-ready)", "summary": "Data loaders are used by Machine Learning (ML) frameworks like PyTorch and\nTensorFlow to apply transformations to data before feeding it into the\naccelerator. This operation is called data preprocessing. Data preprocessing\nplays an important role in the ML training workflow because if it is\ninefficiently pipelined with the training, it can yield high GPU idleness,\nresulting in important training delays. Unfortunately, existing data loaders\nturn out to waste GPU resources, with $76\\%$ GPU idleness when using the\nPyTorch data loader, for example. One key source of inefficiency is the\nvariability in preprocessing time across samples within the same dataset.\nExisting data loaders are oblivious to this variability, and they construct\nbatches without any consideration of slow or fast samples. In this case, the\nentire batch is delayed by a single slow sample, stalling the training pipeline\nand resulting in head-of-line blocking.\n  To address these inefficiencies, we present MinatoLoader, a general-purpose\ndata loader for PyTorch that accelerates training and improves GPU utilization.\nMinatoLoader is designed for a single-server setup, containing multiple GPUs.\nIt continuously prepares data in the background and actively constructs batches\nby prioritizing fast-to-preprocess samples, while slower samples are processed\nin parallel.\n  We evaluate MinatoLoader on servers with V100 and A100 GPUs. On a machine\nwith four A100 GPUs, MinatoLoader improves the training time of a wide range of\nworkloads by up to $7.5\\times$ ($3.6\\times$ on average) over PyTorch DataLoader\nand Pecan, and up to $3\\times$ ($2.2\\times$ on average) over DALI. It also\nincreases average GPU utilization from 46.4\\% with PyTorch to 90.45\\%, while\npreserving model accuracy and enabling faster convergence.", "AI": {"tldr": "MinatoLoader improves GPU utilization and ML training speed significantly by addressing inefficiencies in data preprocessing.", "motivation": "Common data loaders in ML frameworks are inefficient, causing high GPU idleness due to variability in preprocessing times, leading to training delays.", "method": "MinatoLoader utilizes background preprocessing and prioritizes fast-to-preprocess data while slow samples are handled parallelly, optimizing batch construction.", "result": "MinatoLoader achieves up to 7.5\u00d7 training speed improvement and increases GPU utilization from 46.4% to 90.45% across diverse workloads on high-performance GPUs.", "conclusion": "MinatoLoader significantly enhances ML training efficiency, achieving faster convergence without compromising model accuracy."}}
{"id": "2509.10719", "pdf": "https://arxiv.org/pdf/2509.10719", "abs": "https://arxiv.org/abs/2509.10719", "authors": ["Mohammed Humaid Siddiqui", "Fernando Guzman", "Yufei Wu", "Ruishu Ann"], "title": "Coordinated Reinforcement Learning Prefetching Architecture for Multicore Systems", "categories": ["cs.DC", "cs.AR", "cs.LG", "cs.PF", "68M20, 68T05", "C.1.2; C.1.4; I.2.6"], "comment": "47 pages, 12 figures, technical report prepared at Fairleigh\n  Dickinson University", "summary": "Hardware prefetching is critical to fill the performance gap between CPU\nspeeds and slower memory accesses. With multicore architectures becoming\ncommonplace, traditional prefetchers are severely challenged. Independent core\noperation creates significant redundancy (up to 20% of prefetch requests are\nduplicates), causing unnecessary memory bus traffic and wasted bandwidth.\nFurthermore, cutting-edge prefetchers such as Pythia suffer from about a 10%\nperformance loss when scaling from a single-core to a four-core system. To\nsolve these problems, we propose CRL-Pythia, a coordinated reinforcement\nlearning based prefetcher specifically designed for multicore systems. In this\nwork, CRL-Pythia addresses these issues by enabling cross-core sharing of\ninformation and cooperative prefetching decisions, which greatly reduces\nredundant prefetch requests and improves learning convergence across cores. Our\nexperiments demonstrate that CRL-Pythia outperforms single Pythia\nconfigurations in all cases, with approximately 12% IPC (instructions per\ncycle) improvement for bandwidth-constrained workloads, while imposing moderate\nhardware overhead. Our sensitivity analyses also verify its robustness and\nscalability, thereby making CRL-Pythia a practical and efficient solution to\ncontemporary multicore systems.", "AI": {"tldr": "CRL-Pythia improves multicore system performance and reduces redundancy in prefetching by leveraging coordinated reinforcement learning.", "motivation": "The increasing redundancy in traditional prefetchers and performance loss of modern prefetchers like Pythia in multicore systems highlight the need for a novel approach.", "method": "The paper introduces CRL-Pythia, a prefetcher based on coordinated reinforcement learning designed to enable cross-core sharing of information and cooperative prefetching.", "result": "CRL-Pythia outperforms traditional Pythia configurations, enhancing IPC by approximately 12% for bandwidth-constrained workloads and showing robustness and scalability.", "conclusion": "CRL-Pythia is an efficient and practical solution for modern multicore systems, addressing prefetching redundancy and boosting performance."}}
{"id": "2509.10803", "pdf": "https://arxiv.org/pdf/2509.10803", "abs": "https://arxiv.org/abs/2509.10803", "authors": ["Nafees Iqbal", "Jed Brown"], "title": "Enhancing Type Safety in MPI with Rust: A Statically Verified Approach for RSMPI", "categories": ["cs.DC"], "comment": null, "summary": "The Message Passing Interface (MPI) is a fundamental tool for building\nhigh-performance computing (HPC) applications, enabling efficient communication\nacross distributed systems. Despite its widespread adoption, MPI's low-level\ninterface and lack of built-in type safety make it prone to runtime errors,\nundefined behavior, and debugging challenges, especially in large-scale\napplications. Rust, a modern systems programming language, offers a compelling\nsolution with its strong type system, which enforces memory and type safety at\ncompile time without compromising performance. This paper introduces a\ntype-safe communication framework for MPI, built on the RSMPI library, to\naddress the limitations of traditional MPI programming. At its core is the\nTypedCommunicator, an abstraction that enforces static type safety in\npoint-to-point communication operations. By leveraging Rust's Equivalence\ntrait, our framework guarantees that only compatible types can participate in\ncommunication, catching mismatches either at compile time or through runtime\nvalidation. The framework supports both single-value and slice-based\ncommunication, providing an intuitive API for diverse data structures. Our\nimplementation demonstrates that this approach eliminates common MPI errors,\nimproves developer productivity, and maintains performance, adhering to Rust's\nprinciple of zero-cost abstractions. This work lays the foundation for\nextending type safety to collective operations, advancing the robustness of\nparallel computing in Rust.", "AI": {"tldr": "The paper introduces a type-safe communication framework for the Message Passing Interface (MPI) using the Rust programming language to eliminate errors and improve robustness in high-performance computing.", "motivation": "MPI, while widely used in HPC, has a low-level interface that is prone to runtime errors and lacks type safety. Rust's strong type system can address these issues without affecting performance.", "method": "The framework is built on the RSMPI library and introduces the TypedCommunicator abstraction, which enforces static type safety in point-to-point communication. It uses Rust\u2019s Equivalence trait to ensure type compatibility, with support for both compile-time and runtime validation.", "result": "The approach eliminates common MPI errors, enhances developer productivity, and maintains performance through Rust\u2019s zero-cost abstractions.", "conclusion": "The framework enhances the reliability of MPI programming by incorporating type safety and aligns with Rust's principles. It also sets the stage for future work on type-safe collective operations in parallel computing."}}
{"id": "2509.10562", "pdf": "https://arxiv.org/pdf/2509.10562", "abs": "https://arxiv.org/abs/2509.10562", "authors": ["I. A. Lopatin", "S. V. Kozyrev", "A. N. Pechen"], "title": "Predator-Prey Model: Driven Hunt for Accelerated Grokking", "categories": ["cs.NE", "nlin.AO"], "comment": "15 pages, 9 figures", "summary": "A machine learning method is proposed using two agents that simulate the\nbiological behavior of a predator and a prey. In this method, the predator and\nthe prey interact with each other - the predator chases the prey while the prey\nruns away from the predator - to perform an optimization on the landscape. This\nmethod allows, for the case of a ravine landscape (i.e., a landscape with\nnarrow ravines and with gentle slopes along the ravines) to avoid getting\noptimization stuck in the ravine. For this, in the optimization over a ravine\nlandscape the predator drives the prey along the ravine. Thus we also call this\napproach, for the case of ravine landscapes, the driven hunt method. For some\nexamples of grokking (i.e., delayed generalization) problems we show that this\nmethod allows for achieving up to a hundred times faster learning compared to\nthe standard learning procedure.", "AI": {"tldr": "A machine learning optimization method mimics predator-prey dynamics to enhance learning efficiency, particularly avoiding pitfalls in ravine-like landscapes.", "motivation": "To address inefficiencies in optimization processes, especially avoiding stagnation in complex landscapes like narrow ravines in delayed generalization problems.", "method": "Introduces two interacting agents: a predator chasing prey and a prey escaping predator. This dynamic guides optimization through difficult landscape terrains, like ravines.", "result": "Demonstrated up to 100x faster learning in delayed generalization problems using this predator-prey-driven optimization approach compared to standard methods.", "conclusion": "The predator-prey interaction method effectively enhances optimization and learning across complex landscapes, proving highly superior in specific grokking problems."}}
{"id": "2509.10541", "pdf": "https://arxiv.org/pdf/2509.10541", "abs": "https://arxiv.org/abs/2509.10541", "authors": ["V. Benes", "M. Svitek", "A. Michalikova", "M. Melicherik"], "title": "Situation Model of the Transport, Transport Emissions and Meteorological Conditions", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Air pollution in cities and the possibilities of reducing this pollution\nrepresents one of the most important factors that today's society has to deal\nwith. This paper focuses on a systemic approach to traffic emissions with their\nrelation to meteorological conditions, analyzing the effect of weather on the\nquantity and dispersion of traffic emissions in a city. Using fuzzy inference\nsystems (FIS) the model for prediction of changes in emissions depending on\nvarious conditions is developed. The proposed model is based on traffic,\nmeteorology and emission data measured in Prague, Czech Republic. The main\nobjective of the work is to provide insight into how urban planners and\npolicymakers can plan and manage urban transport more effectively with\nenvironmental protection in mind.", "AI": {"tldr": "The paper addresses urban air pollution from traffic, linking it to weather effects, and uses a predictive fuzzy inference system (FIS) model based on data from Prague.", "motivation": "Urban air pollution remains a key challenge for modern society, and there is a need to reduce traffic-related emissions while considering environmental factors.", "method": "The paper develops a predictive model using fuzzy inference systems (FIS), incorporating traffic, meteorological, and emission data from Prague.", "result": "The model predicts changes in traffic emissions based on varying meteorological and traffic conditions.", "conclusion": "The work highlights the importance of integrating meteorological factors into urban planning for managing traffic emissions and aiding policymakers in creating environmentally friendly urban transport systems."}}
{"id": "2509.11418", "pdf": "https://arxiv.org/pdf/2509.11418", "abs": "https://arxiv.org/abs/2509.11418", "authors": ["Runming Li", "Yue Yao", "Robert Harper"], "title": "Mechanizing Synthetic Tait Computability in Istari", "categories": ["cs.PL"], "comment": null, "summary": "Categorical gluing is a powerful technique for proving meta-theorems of type\ntheories such as canonicity and normalization. Synthetic Tait Computability\n(STC) provides an abstract treatment of the complex gluing models by\ninternalizing the gluing category into a modal dependent type theory with a\nphase distinction. This work presents a mechanization of STC in the Istari\nproof assistant. Istari is a Martin-L\\\"{o}f-style extensional type theory with\nequality reflection. Equality reflection eliminates the nuisance of transport\nreasoning typically found in intensional proof assistants. This work develops a\nreusable library for synthetic phase distinction, including modalities,\nextension types, and strict glue types, and applies it to two case studies: (1)\na canonicity model for dependent type theory with dependent products and\nbooleans with large elimination, and (2) a Kripke canonicity model for the\ncost-aware logical framework. Our results demonstrate that the core STC\nconstructions can be formalized essentially verbatim in Istari, preserving the\nelegance of the on-paper arguments while ensuring machine-checked correctness.", "AI": {"tldr": "This paper mechanizes Synthetic Tait Computability (STC) in the Istari proof assistant and demonstrates its applicability to formalizing type theory meta-theorems.", "motivation": "To provide a machine-checked formalization of Synthetic Tait Computability (STC) for simplifying the development and validation of gluing models used to prove meta-theorems of type theories.", "method": "The authors developed a reusable library in Istari proof assistant, consisting of synthetic phase distinction constructs like modalities, extension types, and strict glue types, and applied it to canonicity case studies of dependent type theories.", "result": "The formalization successfully replicated the core STC constructions in Istari, maintaining the theoretical elegance of manual arguments while ensuring correctness through automated verification.", "conclusion": "The work confirms that STC can be mechanized in a proof assistant like Istari, enabling reliable and efficient reasoning about type theories and their meta-theorems."}}
{"id": "2509.10463", "pdf": "https://arxiv.org/pdf/2509.10463", "abs": "https://arxiv.org/abs/2509.10463", "authors": ["Qiuyu Chen", "Xin Jin", "Yue Song", "Xihui Liu", "Shuai Yang", "Tao Yang", "Ziqiang Li", "Jianguo Huang", "Yuntao Wei", "Ba'ao Xie", "Nicu Sebe", "Wenjun", "Zeng", "Jooyeol Yun", "Davide Abati", "Mohamed Omran", "Jaegul Choo", "Amir Habibian", "Auke Wiggers", "Masato Kobayashi", "Ning Ding", "Toru Tamaki", "Marzieh Gheisari", "Auguste Genovesio", "Yuheng Chen", "Dingkun Liu", "Xinyao Yang", "Xinping Xu", "Baicheng Chen", "Dongrui Wu", "Junhao Geng", "Lexiang Lv", "Jianxin Lin", "Hanzhe Liang", "Jie Zhou", "Xuanxin Chen", "Jinbao Wang", "Can Gao", "Zhangyi Wang", "Zongze Li", "Bihan Wen", "Yixin Gao", "Xiaohan Pan", "Xin Li", "Zhibo Chen", "Baorui Peng", "Zhongming Chen", "Haoran Jin"], "title": "The 1st International Workshop on Disentangled Representation Learning for Controllable Generation (DRL4Real): Methods and Results", "categories": ["cs.LG", "cs.CV"], "comment": "Workshop summary paper for ICCV 2025, 9 accepted papers, 9 figures,\n  IEEE conference format, covers topics including diffusion models,\n  controllable generation, 3D-aware disentanglement, autonomous driving\n  applications, and EEG analysis", "summary": "This paper reviews the 1st International Workshop on Disentangled\nRepresentation Learning for Controllable Generation (DRL4Real), held in\nconjunction with ICCV 2025. The workshop aimed to bridge the gap between the\ntheoretical promise of Disentangled Representation Learning (DRL) and its\napplication in realistic scenarios, moving beyond synthetic benchmarks.\nDRL4Real focused on evaluating DRL methods in practical applications such as\ncontrollable generation, exploring advancements in model robustness,\ninterpretability, and generalization. The workshop accepted 9 papers covering a\nbroad range of topics, including the integration of novel inductive biases\n(e.g., language), the application of diffusion models to DRL, 3D-aware\ndisentanglement, and the expansion of DRL into specialized domains like\nautonomous driving and EEG analysis. This summary details the workshop's\nobjectives, the themes of the accepted papers, and provides an overview of the\nmethodologies proposed by the authors.", "AI": {"tldr": "The paper summarizes the focus and outcomes of the 1st International Workshop on Disentangled Representation Learning in practical applications, featuring 9 papers exploring advancements and methodologies in DRL.", "motivation": "Bridge the gap between theoretical promises of Disentangled Representation Learning (DRL) and its use in real-world applications beyond synthetic benchmarks.", "method": "Reviewed methodologies integrating novel inductive biases, exploration of diffusion models, and specialized applications like 3D-aware disentanglement and autonomous driving.", "result": "Nine papers were presented, offering insights into practical applications of DRL across diverse domains and examining robustness, interpretability, and generalization.", "conclusion": "The workshop highlighted the progress and challenges in transitioning DRL from theoretical frameworks to impactful real-world applications."}}
{"id": "2509.10570", "pdf": "https://arxiv.org/pdf/2509.10570", "abs": "https://arxiv.org/abs/2509.10570", "authors": ["Wei Dai", "Shengen Wu", "Wei Wu", "Zhenhao Wang", "Sisuo Lyu", "Haicheng Liao", "Limin Yu", "Weiping Ding", "Runwei Guan", "Yutao Yue"], "title": "Large Foundation Models for Trajectory Prediction in Autonomous Driving: A Comprehensive Survey", "categories": ["cs.RO", "cs.AI"], "comment": "22 pages, 6 figures", "summary": "Trajectory prediction serves as a critical functionality in autonomous\ndriving, enabling the anticipation of future motion paths for traffic\nparticipants such as vehicles and pedestrians, which is essential for driving\nsafety. Although conventional deep learning methods have improved accuracy,\nthey remain hindered by inherent limitations, including lack of\ninterpretability, heavy reliance on large-scale annotated data, and weak\ngeneralization in long-tail scenarios. The rise of Large Foundation Models\n(LFMs) is transforming the research paradigm of trajectory prediction. This\nsurvey offers a systematic review of recent advances in LFMs, particularly\nLarge Language Models (LLMs) and Multimodal Large Language Models (MLLMs) for\ntrajectory prediction. By integrating linguistic and scene semantics, LFMs\nfacilitate interpretable contextual reasoning, significantly enhancing\nprediction safety and generalization in complex environments. The article\nhighlights three core methodologies: trajectory-language mapping, multimodal\nfusion, and constraint-based reasoning. It covers prediction tasks for both\nvehicles and pedestrians, evaluation metrics, and dataset analyses. Key\nchallenges such as computational latency, data scarcity, and real-world\nrobustness are discussed, along with future research directions including\nlow-latency inference, causality-aware modeling, and motion foundation models.", "AI": {"tldr": "The paper reviews recent advances in trajectory prediction in autonomous driving using Large Foundation Models (LFMs) like Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs).", "motivation": "Trajectory prediction is crucial for autonomous driving safety, but conventional deep learning methods face issues like lack of interpretability, high dependence on annotated data, and poor generalization in rare scenarios.", "method": "The paper explores three methodologies: trajectory-language mapping, multimodal fusion, and constraint-based reasoning using LFMs to improve prediction safety and contextual reasoning.", "result": "LFMs enable interpretable reasoning by integrating linguistic and scene semantics, improving generalization and prediction safety in complex environments.", "conclusion": "LFMs are promising for trajectory prediction, offering better interpretability and generalization. Challenges like computational delays and data scarcity need addressing, with future focus on low-latency inference and motion foundation models."}}
{"id": "2509.10546", "pdf": "https://arxiv.org/pdf/2509.10546", "abs": "https://arxiv.org/abs/2509.10546", "authors": ["Gang Cheng", "Haibo Jin", "Wenbin Zhang", "Haohan Wang", "Jun Zhuang"], "title": "Uncovering the Vulnerability of Large Language Models in the Financial Domain via Risk Concealment", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Preprint, under review. TL;DR: We propose a multi-turn red-teaming\n  framework, RCA, that reveals critical regulatory vulnerabilities in financial\n  LLMs, achieving over 93% attack success on a proposed new benchmark,\n  FIN-Bench", "summary": "Large Language Models (LLMs) are increasingly integrated into financial\napplications, yet existing red-teaming research primarily targets harmful\ncontent, largely neglecting regulatory risks. In this work, we aim to\ninvestigate the vulnerability of financial LLMs through red-teaming approaches.\nWe introduce Risk-Concealment Attacks (RCA), a novel multi-turn framework that\niteratively conceals regulatory risks to provoke seemingly compliant yet\nregulatory-violating responses from LLMs. To enable systematic evaluation, we\nconstruct FIN-Bench, a domain-specific benchmark for assessing LLM safety in\nfinancial contexts. Extensive experiments on FIN-Bench demonstrate that RCA\neffectively bypasses nine mainstream LLMs, achieving an average attack success\nrate (ASR) of 93.18%, including 98.28% on GPT-4.1 and 97.56% on OpenAI o1.\nThese findings reveal a critical gap in current alignment techniques and\nunderscore the urgent need for stronger moderation mechanisms in financial\ndomains. We hope this work offers practical insights for advancing robust and\ndomain-aware LLM alignment.", "AI": {"tldr": "This paper introduces Risk-Concealment Attacks (RCA) to expose regulatory compliance vulnerabilities in financial LLMs, achieving a high attack success rate.", "motivation": "The paper aims to address the lack of focus on regulatory compliance risks in financial applications of LLMs, which predominantly rely on unsafe or harmful intent studies.", "method": "The authors developed RCA, a multi-turn attack framework that exploits financial LLM vulnerabilities and created FIN-Bench, a benchmark for systematic evaluation of financial LLM safety.", "result": "Experiments reveal RCA achieves a high attack success rate (93.18% on average), successfully bypassing nine LLMs, including 98.28% success on GPT-4.1.", "conclusion": "The study highlights gaps in current LLM alignment methods and stresses the importance of enhanced moderation in financial domains."}}
{"id": "2509.10572", "pdf": "https://arxiv.org/pdf/2509.10572", "abs": "https://arxiv.org/abs/2509.10572", "authors": ["Ashlesha Akella", "Akshar Kaul", "Krishnasuri Narayanam", "Sameep Mehta"], "title": "Quality Assessment of Tabular Data using Large Language Models and Code Generation", "categories": ["cs.SE", "cs.AI", "cs.DB"], "comment": "EMNLP industry track submitted", "summary": "Reliable data quality is crucial for downstream analysis of tabular datasets,\nyet rule-based validation often struggles with inefficiency, human\nintervention, and high computational costs. We present a three-stage framework\nthat combines statistical inliner detection with LLM-driven rule and code\ngeneration. After filtering data samples through traditional clustering, we\niteratively prompt LLMs to produce semantically valid quality rules and\nsynthesize their executable validators through code-generating LLMs. To\ngenerate reliable quality rules, we aid LLMs with retrieval-augmented\ngeneration (RAG) by leveraging external knowledge sources and domain-specific\nfew-shot examples. Robust guardrails ensure the accuracy and consistency of\nboth rules and code snippets. Extensive evaluations on benchmark datasets\nconfirm the effectiveness of our approach.", "AI": {"tldr": "The paper introduces a three-stage framework combining statistical outlier detection and LLM-driven code generation to enhance tabular data quality validation.", "motivation": "Traditional rule-based validation methods for tabular data often suffer from inefficiencies, excessive human involvement, and high computational costs, highlighting the need for automated and scalable approaches.", "method": "The framework combines clustering for initial filtering, LLMs for generating semantic quality rules, and code generation to implement validation. It incorporates retrieval-augmented generation for domain knowledge and employs guardrails for accuracy and consistency.", "result": "The proposed method was validated on benchmark datasets, demonstrating its ability to generate accurate and semantically valid quality rules efficiently.", "conclusion": "The approach enhances automated data quality validation, reduces dependency on manual effort, and offers scalability for large tabular datasets."}}
{"id": "2509.10547", "pdf": "https://arxiv.org/pdf/2509.10547", "abs": "https://arxiv.org/abs/2509.10547", "authors": ["Pascal Helson", "Arvind Kumar"], "title": "Biomarkers of brain diseases", "categories": ["q-bio.NC", "cs.AI", "cs.LG"], "comment": null, "summary": "Despite the diversity of brain data acquired and advanced AI-based algorithms\nto analyze them, brain features are rarely used in clinics for diagnosis and\nprognosis. Here we argue that the field continues to rely on cohort comparisons\nto seek biomarkers, despite the well-established degeneracy of brain features.\nUsing a thought experiment, we show that more data and more powerful algorithms\nwill not be sufficient to identify biomarkers of brain diseases. We argue that\ninstead of comparing patient versus healthy controls using single data type, we\nshould use multimodal (e.g. brain activity, neurotransmitters, neuromodulators,\nbrain imaging) and longitudinal brain data to guide the grouping before\ndefining multidimensional biomarkers for brain diseases.", "AI": {"tldr": "The paper critiques the reliance on cohort comparisons for brain disease biomarkers and suggests multimodal and longitudinal data as an alternative.", "motivation": "The authors aim to address the limitation of current practices in analyzing brain data for clinical use, focusing on the challenge of identifying reliable biomarkers due to degeneracy in brain features.", "method": "The authors use reasoning and a thought experiment to argue against traditional cohort comparisons and advocate for multimodal and longitudinal analyses.", "result": "More data and advanced algorithms alone are insufficient to pinpoint reliable biomarkers for brain diseases.", "conclusion": "Shifting to multimodal and longitudinal brain data analyses is essential for identifying robust biomarkers for brain diseases and improving clinical applications."}}
{"id": "2509.10466", "pdf": "https://arxiv.org/pdf/2509.10466", "abs": "https://arxiv.org/abs/2509.10466", "authors": ["Christian Fane"], "title": "A Real-Time Diminished Reality Approach to Privacy in MR Collaboration", "categories": ["cs.CV", "cs.HC", "H.5.1; H.5.2; I.4.6; I.4.4; I.2.10; I.4.9; K.4.1"], "comment": "50 pages, 12 figures | Demo video: https://youtu.be/udBxj35GEKI?t=499\n  | Code: https://github.com/c1h1r1i1s1 (multiple repositories)", "summary": "Diminished reality (DR) refers to the digital removal of real-world objects\nby compositing background content in their place. This thesis presents a\nreal-time, inpainting-based DR system designed to enable privacy control in\nshared-space mixed reality (MR) meetings. The system allows a primary headset\nuser to selectively remove personal or sensitive items from their environment,\nensuring that those objects are no longer visible to other participants.\nRemoval is achieved through semantic segmentation and precise object selection,\nfollowed by real-time inpainting from the viewpoint of a secondary observer,\nimplemented using a mobile ZED 2i depth camera. The solution is designed to be\nportable and robust, requiring neither a fixed secondary viewpoint nor prior 3D\nscanning of the environment. The system utilises YOLOv11 for object detection\nand a modified Decoupled Spatial-Temporal Transformer (DSTT) model for\nhigh-quality video inpainting. At 720p resolution, the pipeline sustains frame\nrates exceeding 20 fps, demonstrating the feasibility of real-time diminished\nreality for practical privacy-preserving MR applications.", "AI": {"tldr": "This paper presents a real-time diminished reality system for privacy control in mixed reality settings by digitally removing objects using inpainting techniques and semantic segmentation.", "motivation": "The paper aims to address privacy issues in shared-space mixed reality meetings by enabling users to digitally remove sensitive objects from their environment.", "method": "The system employs YOLOv11 for object detection, semantic segmentation for precise object selection, and a modified DSTT model for real-time video inpainting using mobile ZED 2i depth camera.", "result": "The system achieves real-time performance at over 20 fps on 720p resolution, demonstrating its practical feasibility and portability.", "conclusion": "The proposed diminished reality system is effective, portable, and supports privacy-preserving applications in mixed reality settings without requiring prior environmental scanning or fixed viewpoints."}}
{"id": "2509.10627", "pdf": "https://arxiv.org/pdf/2509.10627", "abs": "https://arxiv.org/abs/2509.10627", "authors": ["Yu-Hong Lai", "Chieh-Lin Tsai", "Wen Sheng Lim", "Han-Wen Hu", "Tei-Wei Kuo", "Yuan-Hao Chang"], "title": "ReCross: Efficient Embedding Reduction Scheme for In-Memory Computing using ReRAM-Based Crossbar", "categories": ["cs.AR", "cs.ET"], "comment": null, "summary": "Deep learning-based recommendation models (DLRMs) are widely deployed in\ncommercial applications to enhance user experience. However, the large and\nsparse embedding layers in these models impose substantial memory bandwidth\nbottlenecks due to high memory access costs and irregular access patterns,\nleading to increased inference time and energy consumption. While resistive\nrandom access memory (ReRAM) based crossbars offer a fast and energy-efficient\nsolution through in-memory embedding reduction operations, naively mapping\nembeddings onto crossbar arrays leads to poor crossbar utilization and thus\ndegrades performance. We present ReCross, an efficient ReRAM-based in-memory\ncomputing (IMC) scheme designed to minimize execution time and enhance energy\nefficiency in DLRM embedding reduction. ReCross co-optimizes embedding access\npatterns and ReRAM crossbar characteristics by intelligently grouping and\nmapping co-occurring embeddings, replicating frequently accessed embeddings\nacross crossbars, and dynamically selecting in-memory processing operations\nusing a newly designed dynamic switch ADC circuit that considers runtime energy\ntrade-offs. Experimental results demonstrate that ReCross achieves a 3.97x\nreduction in execution time and a 6.1x improvement in energy efficiency\ncompared to state-of-the-art IMC approaches.", "AI": {"tldr": "Deep learning recommendation models (DLRMs) face memory issues due to large embedding layers. ReCross improves memory operations using ReRAM-based computing, achieving faster execution and better energy efficiency.", "motivation": "DLRMs are hindered by memory bottlenecks caused by high cost and irregularity of access in large embedding layers, leading to inefficiency in terms of energy and speed.", "method": "ReCross optimizes embedding access patterns through strategies like intelligent grouping, replication of frequently accessed embeddings, and dynamic processing using a new ADC circuit.", "result": "ReCross reduces execution time by 3.97x and improves energy efficiency by 6.1x compared to state-of-the-art IMC solutions.", "conclusion": "ReCross successfully overcomes the inefficiencies in DLRM's memory system, providing a much faster and energy-efficient solution leveraging ReRAM-based in-memory computing."}}
{"id": "2509.10853", "pdf": "https://arxiv.org/pdf/2509.10853", "abs": "https://arxiv.org/abs/2509.10853", "authors": ["Tien-En Chang", "Argon Chen"], "title": "Variable Selection Using Relative Importance Rankings", "categories": ["stat.ML", "cs.LG"], "comment": "26 pages, 9 figures", "summary": "Although conceptually related, variable selection and relative importance\n(RI) analysis have been treated quite differently in the literature. While RI\nis typically used for post-hoc model explanation, this paper explores its\npotential for variable ranking and filter-based selection before model\ncreation. Specifically, we anticipate strong performance from the RI measures\nbecause they incorporate both direct and combined effects of predictors,\naddressing a key limitation of marginal correlation that ignores dependencies\namong predictors. We implement and evaluate the RI-based variable selection\nmethods using general dominance (GD), comprehensive relative importance (CRI),\nand a newly proposed, computationally efficient variant termed CRI.Z.\n  We first demonstrate how the RI measures more accurately rank the variables\nthan the marginal correlation, especially when there are suppressed or weak\npredictors. We then show that predictive models built on these rankings are\nhighly competitive, often outperforming state-of-the-art methods such as the\nlasso and relaxed lasso. The proposed RI-based methods are particularly\neffective in challenging cases involving clusters of highly correlated\npredictors, a setting known to cause failures in many benchmark methods.\nAlthough lasso methods have dominated the recent literature on variable\nselection, our study reveals that the RI-based method is a powerful and\ncompetitive alternative. We believe these underutilized tools deserve greater\nattention in statistics and machine learning communities. The code is available\nat: https://github.com/tien-endotchang/RI-variable-selection.", "AI": {"tldr": "This paper proposes using relative importance (RI) measures for variable selection prior to model creation, demonstrating their superior ranking ability and competitive modeling performance compared to methods like lasso.", "motivation": "Current methods for variable selection, like marginal correlation, fail to account for dependencies among predictors, limiting their effectiveness. The authors aim to explore RI measures as a solution.", "method": "The study implements general dominance (GD), comprehensive relative importance (CRI), and CRI.Z to compare RI-based variable selection approaches against widely-used benchmarks like lasso.", "result": "RI measures outperform marginal correlation in ranking predictors and lead to predictive models that rival or exceed the performance of lasso methods, particularly in settings with highly correlated predictors.", "conclusion": "The results highlight RI-based methods as strong and competitive alternatives to conventional variable selection frameworks, warranting increased attention in statistics and machine learning fields."}}
{"id": "2509.11076", "pdf": "https://arxiv.org/pdf/2509.11076", "abs": "https://arxiv.org/abs/2509.11076", "authors": ["Zibo Wang", "Yuhang Zhou", "Zhibin Wang", "Shipeng Li", "Xinjing Huang", "Chendong Cai", "Bingxu Mu", "Yuqing Sun", "Zhiheng Hu", "Bin She", "Shu You", "Guanghuan Fang", "Rong Gu", "Wanchun Dou", "Guihai Chen", "Chen Tian"], "title": "Chameleon: Taming Dynamic Operator Sequences for Memory-Intensive LLM Training", "categories": ["cs.DC"], "comment": null, "summary": "The increasing size of large language models (LLMs) has led to a surge in\nmemory requirements during training, often exceeding the capacity of\nhigh-bandwidth memory (HBM). Swap-based memory optimization incurs neither\naccuracy loss nor additional end-to-end overhead when effectively overlapped,\nthus being an attractive solution. However, existing swap methods assume\nconsistent operator sequences, which is impractical in Eager Mode, where\noperator sequences can vary during change.\n  We propose Chameleon, which redesigns the end-to-end process of swap-based\nmemory optimization and is the first work to consider varying operator\nsequences in Eager Mode. Chameleon (i) introduces a lightweight online profiler\nto enable continuous profiling for monitoring operator sequences, (ii)\ngenerates effective swap policies with limited operator information, and (iii)\noptimizes the policy execution module for accurate policy application and\nbetter performance. Experimental results demonstrate that Chameleon reduces\nprofiling overhead by 84.25%, enables training models up to 4x larger than\nhardware memory while adapting to changes in operator sequences, improves\nperformance by up to 38.94% compared to recomputation or high-degree\nparallelism.", "AI": {"tldr": "This paper introduces Chameleon, a swap-based memory optimization framework for training large language models (LLMs) in Eager Mode, addressing dynamic operator sequences.", "motivation": "The rising memory demands of large language models often exceed hardware capabilities, necessitating efficient memory optimization methods without compromising accuracy or performance, especially in dynamic environments like Eager Mode.", "method": "Chameleon integrates a lightweight online profiler for continuous monitoring, employs strategic swap policy generation with minimal operator data, and enhances execution modules for precise application and efficiency.", "result": "The system reduces profiling overhead by 84.25%, allows training of models up to 4x larger than hardware memory, adapts to operator changes, and achieves a performance boost of up to 38.94% compared to other methods.", "conclusion": "Chameleon successfully addresses the limitations of conventional swap methods, enabling efficient training of larger LLMs while adapting dynamically to operator sequence changes."}}
{"id": "2509.11016", "pdf": "https://arxiv.org/pdf/2509.11016", "abs": "https://arxiv.org/abs/2509.11016", "authors": ["Xu Yang", "Rui Wang", "Kaiwen Li", "Wenhua Li", "Ling Wang"], "title": "Deep Reinforcement Learning-Assisted Component Auto-Configuration of Differential Evolution Algorithm for Constrained Optimization: A Foundation Model", "categories": ["cs.NE"], "comment": null, "summary": "Despite significant efforts to manually design high-performance evolutionary\nalgorithms, their adaptability remains limited due to the dynamic and\never-evolving nature of real-world problems. The \"no free lunch\" theorem\nhighlights that no single algorithm performs optimally across all problems.\nWhile online adaptation methods have been proposed, they often suffer from\ninefficiency, weak convergence, and limited generalization on constrained\noptimization problems (COPs).\n  To address these challenges, we introduce a novel framework for automated\ncomponent configuration in Differential Evolution (DE) algorithm to address\nCOPs, powered by Deep Reinforcement Learning (DRL). Specifically, we propose\nSuperDE, a foundation model that dynamically configures DE's evolutionary\ncomponents based on real-time evolution. Trained offline through meta-learning\nacross a wide variety of COPs, SuperDE is capable of recommending optimal\nper-generation configurations for unseen problems in a zero-shot manner.\nUtilizing a Double Deep Q-Network (DDQN), SuperDE adapts its configuration\nstrategies in response to the evolving population states during optimization.\nExperimental results demonstrate that SuperDE significantly outperforms\nexisting state-of-the-art algorithms on benchmark test suites, achieving\nsuperior generalization and optimization performance.", "AI": {"tldr": "This paper proposes SuperDE, a framework using Deep Reinforcement Learning (DRL) for automated configuration of Differential Evolution (DE) to tackle constrained optimization problems (COPs) effectively.", "motivation": "The adaptability of evolutionary algorithms is limited due to their dependence on manual design and the challenges imposed by dynamic, real-world problems. Existing solutions lack efficiency, generalization, and strong convergence, highlighting the need for a more flexible algorithmic approach.", "method": "The authors propose SuperDE, a model powered by Deep Reinforcement Learning. SuperDE trains offline using meta-learning over various COPs and uses a Double Deep Q-Network to dynamically and adaptively configure DE components in real-time.", "result": "SuperDE achieved significant performance improvements over existing state-of-the-art algorithms on benchmark constrained optimization problems.", "conclusion": "The proposed SuperDE framework demonstrates enhanced adaptability, generalization, and optimization performance, addressing the existing limitations of evolutionary algorithms on constrained optimization problems."}}
{"id": "2509.10660", "pdf": "https://arxiv.org/pdf/2509.10660", "abs": "https://arxiv.org/abs/2509.10660", "authors": ["Nam H. Le", "Patrick Erickson", "Yanbo Zhang", "Michael Levin", "Josh Bongard"], "title": "ZapGPT: Free-form Language Prompting for Simulated Cellular Control", "categories": ["cs.AI", "cs.MA", "q-bio.CB"], "comment": null, "summary": "Human language is one of the most expressive tools for conveying intent, yet\nmost artificial or biological systems lack mechanisms to interpret or respond\nmeaningfully to it. Bridging this gap could enable more natural forms of\ncontrol over complex, decentralized systems. In AI and artificial life, recent\nwork explores how language can specify high-level goals, but most systems still\ndepend on engineered rewards, task-specific supervision, or rigid command sets,\nlimiting generalization to novel instructions. Similar constraints apply in\nsynthetic biology and bioengineering, where the locus of control is often\ngenomic rather than environmental perturbation.\n  A key open question is whether artificial or biological collectives can be\nguided by free-form natural language alone, without task-specific tuning or\ncarefully designed evaluation metrics. We provide one possible answer here by\nshowing, for the first time, that simple agents' collective behavior can be\nguided by free-form language prompts: one AI model transforms an imperative\nprompt into an intervention that is applied to simulated cells; a second AI\nmodel scores how well the prompt describes the resulting cellular dynamics; and\nthe former AI model is evolved to improve the scores generated by the latter.\n  Unlike previous work, our method does not require engineered fitness\nfunctions or domain-specific prompt design. We show that the evolved system\ngeneralizes to unseen prompts without retraining. By treating natural language\nas a control layer, the system suggests a future in which spoken or written\nprompts could direct computational, robotic, or biological systems to desired\nbehaviors. This work provides a concrete step toward this vision of AI-biology\npartnerships, in which language replaces mathematical objective functions,\nfixed rules, and domain-specific programming.", "AI": {"tldr": "The paper demonstrates the ability to guide collective behavior of simulated agents with natural language prompts, removing the need for task-specific engineering and achieving generalization to unseen instructions.", "motivation": "To bridge the gap where artificial and biological systems struggle to interpret and respond meaningfully to language, which could enable natural control over complex systems.", "method": "The proposed approach utilizes two AI models\u2014one transforms language prompts into interventions for cells, and the other evaluates the results. By evolving the first model based on evaluation scores, they remove the need for engineered rewards or task-specific tuning.", "result": "The system successfully generalizes to unseen prompts without needing retraining, showcasing the feasibility of free-form language as a control layer.", "conclusion": "This method represents a significant step towards AI-biology integration, suggesting future possibilities where natural language guides computational and biological systems."}}
{"id": "2509.11901", "pdf": "https://arxiv.org/pdf/2509.11901", "abs": "https://arxiv.org/abs/2509.11901", "authors": ["Kentaro Kobayashi", "Yukiyoshi Kameyama"], "title": "Expressive Power of One-Shot Control Operators and Coroutines", "categories": ["cs.PL", "cs.LO"], "comment": "Full version of the paper accepted at APLAS 2025. Includes appendices\n  with proofs. 59 pages", "summary": "Control operators, such as exceptions and effect handlers, provide a means of\nrepresenting computational effects in programs abstractly and modularly. While\nmost theoretical studies have focused on multi-shot control operators, one-shot\ncontrol operators -- which restrict the use of captured continuations to at\nmost once -- are gaining attention for their balance between expressiveness and\nefficiency. This study aims to fill the gap. We present a mathematically\nrigorous comparison of the expressive power among one-shot control operators,\nincluding effect handlers, delimited continuations, and even asymmetric\ncoroutines. Following previous studies on multi-shot control operators, we\nadopt Felleisen's macro-expressiveness as our measure of expressiveness. We\nverify the folklore that one-shot effect handlers and one-shot\ndelimited-control operators can be macro-expressed by asymmetric coroutines,\nbut not vice versa. We explain why a previous informal argument fails, and how\nto revise it to make a valid macro-translation.", "AI": {"tldr": "This paper conducts a formal mathematical comparison of one-shot control operators' expressive powers, linking them with concepts like effect handlers, delimited continuations, and asymmetric coroutines.", "motivation": "The paper seeks to bridge a gap in the study of one-shot control operators, which are gaining traction for their balance between computational expressiveness and efficiency.", "method": "The authors apply Felleisen's macro-expressiveness framework to measure and compare the expressive powers of various one-shot control operators.", "result": "They validate that asymmetric coroutines can macro-express one-shot effect handlers and delimited-control operators. However, they also establish that the reverse is not true, revising prior informal arguments to ensure a formal basis.", "conclusion": "The study contributes a rigorous formal foundation for understanding the relationships and expressive hierarchies among one-shot control operators, while correcting misconceptions in prior work."}}
{"id": "2509.10495", "pdf": "https://arxiv.org/pdf/2509.10495", "abs": "https://arxiv.org/abs/2509.10495", "authors": ["Fanze Kong", "Chen-Chih Lai", "Yubin Lu"], "title": "Moment Estimates and DeepRitz Methods on Learning Diffusion Systems with Non-gradient Drifts", "categories": ["cs.LG", "physics.comp-ph"], "comment": null, "summary": "Conservative-dissipative dynamics are ubiquitous across a variety of complex\nopen systems. We propose a data-driven two-phase method, the Moment-DeepRitz\nMethod, for learning drift decompositions in generalized diffusion systems\ninvolving conservative-dissipative dynamics. The method is robust to noisy\ndata, adaptable to rough potentials and oscillatory rotations. We demonstrate\nits effectiveness through several numerical experiments.", "AI": {"tldr": "The paper introduces the Moment-DeepRitz Method to learn drift decompositions in complex diffusion systems, emphasizing robustness to noisy data and adaptability.", "motivation": "To address the challenge of modeling conservative-dissipative dynamics in generalized diffusion systems from noisy and complex data.", "method": "The Moment-DeepRitz Method splits learning into two phases, combining moment dynamics modeling with the DeepRitz framework for drift decomposition.", "result": "Through numerical experiments, the method successfully handles rough potentials and oscillatory rotations while being robust to noise.", "conclusion": "The Moment-DeepRitz Method is a reliable and adaptable solution for analyzing drift decompositions in systems with conservative-dissipative dynamics."}}
{"id": "2509.10692", "pdf": "https://arxiv.org/pdf/2509.10692", "abs": "https://arxiv.org/abs/2509.10692", "authors": ["Giuseppe Silano", "Amr Afifi", "Martin Saska", "Antonio Franchi"], "title": "STL-Based Motion Planning and Uncertainty-Aware Risk Analysis for Human-Robot Collaboration with a Multi-Rotor Aerial Vehicle", "categories": ["cs.RO"], "comment": "39 pages, 13 figures", "summary": "This paper presents a novel approach to motion planning and risk analysis for\nenhancing human-robot collaboration using a Multi-Rotor Aerial Vehicle (MRAV).\nThe proposed method uses Signal Temporal Logic (STL) to encode key mission\nobjectives, such as safety, timing, and human preferences, with a strong focus\non ergonomics and comfort. An optimization framework generates dynamically\nfeasible trajectories while considering the MRAV's physical constraints. Given\nthe nonlinear and non-convex nature of the problem, smooth approximations and\ngradient-based techniques assist in handling the problem's computational\ncomplexity. Additionally, an uncertainty-aware risk analysis is incorporated to\nassess potential deviations from the mission specifications, providing insights\ninto the likelihood of mission success under uncertain conditions. Further, an\nevent-triggered replanning strategy is implemented to respond to unforeseen\nevents and external disturbances. The approach is validated through MATLAB and\nGazebo simulations, using an object handover task in a mock-up environment\ninspired by power line maintenance scenarios. The results highlight the\nmethod's effectiveness in achieving safe, efficient, and resilient human-robot\ncollaboration.", "AI": {"tldr": "The paper introduces a method for improving human-robot collaboration using a Multi-Rotor Aerial Vehicle (MRAV) through Signal Temporal Logic and an uncertainty-aware risk analysis.", "motivation": "The study aims to enhance safety, timing, and human preferences in human-robot collaboration tasks, focusing on ergonomics and comfort.", "method": "The authors utilize Signal Temporal Logic for encoding mission objectives, apply gradient-based optimization for trajectory planning, incorporate a risk analysis for uncertainties, and implement an event-triggered replanning strategy.", "result": "Simulations in MATLAB and Gazebo demonstrate the method's success in creating safe, efficient, and adaptive trajectories for an object handover task inspired by power line maintenance scenarios.", "conclusion": "The approach effectively improves human-robot collaboration by generating resilient and dynamic mission plans while addressing safety and comfort."}}
{"id": "2509.10625", "pdf": "https://arxiv.org/pdf/2509.10625", "abs": "https://arxiv.org/abs/2509.10625", "authors": ["Iv\u00e1n Vicente Moreno Cencerrado", "Arnau Padr\u00e9s Masdemont", "Anton Gonzalvez Hawthorne", "David Demitri Africa", "Lorenzo Pacchiardi"], "title": "No Answer Needed: Predicting LLM Answer Accuracy from Question-Only Linear Probes", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Do large language models (LLMs) anticipate when they will answer correctly?\nTo study this, we extract activations after a question is read but before any\ntokens are generated, and train linear probes to predict whether the model's\nforthcoming answer will be correct. Across three open-source model families\nranging from 7 to 70 billion parameters, projections on this \"in-advance\ncorrectness direction\" trained on generic trivia questions predict success in\ndistribution and on diverse out-of-distribution knowledge datasets,\noutperforming black-box baselines and verbalised predicted confidence.\nPredictive power saturates in intermediate layers, suggesting that\nself-assessment emerges mid-computation. Notably, generalisation falters on\nquestions requiring mathematical reasoning. Moreover, for models responding \"I\ndon't know\", doing so strongly correlates with the probe score, indicating that\nthe same direction also captures confidence. By complementing previous results\non truthfulness and other behaviours obtained with probes and sparse\nauto-encoders, our work contributes essential findings to elucidate LLM\ninternals.", "AI": {"tldr": "The paper analyzes large language models (LLMs) to determine if they can predict their correctness in generating answers, using linear probes on activation data before token generation.", "motivation": "The study aims to explore whether LLMs inherently possess mechanisms to self-assess answer correctness before outputting a response, thereby contributing to understanding LLM internals.", "method": "Researchers train linear probes on activation data taken immediately after a question is read but before any token generation, focusing on multiple open-source LLM families and analyzing performance across in-distribution and out-of-distribution datasets.", "result": "Linear probes successfully predict answer correctness, saturate in intermediate layers, and indicate self-assessment emergence mid-computation. However, models struggle with mathematical reasoning and exhibit confidence correlation for 'I don't know' responses.", "conclusion": "The findings advance the understanding of LLM mechanisms for self-assessment, revealing emergent behaviors and offering insights into their predictive confidence and limitations in reasoning."}}
{"id": "2509.10649", "pdf": "https://arxiv.org/pdf/2509.10649", "abs": "https://arxiv.org/abs/2509.10649", "authors": ["Johan Cederbladh", "Loek Cleophas", "Eduard Kamburjan", "Lucas Lima", "Rakshit Mittal", "Hans Vangheluwe"], "title": "Reasonable Experiments in Model-Based Systems Engineering", "categories": ["cs.SE", "cs.SY", "eess.SY"], "comment": null, "summary": "With the current trend in Model-Based Systems Engineering towards Digital\nEngineering and early Validation & Verification, experiments are increasingly\nused to estimate system parameters and explore design decisions. Managing such\nexperimental configuration metadata and results is of utmost importance in\naccelerating overall design effort. In particular, we observe it is important\nto 'intelligent-ly' reuse experiment-related data to save time and effort by\nnot performing potentially superfluous, time-consuming, and resource-intensive\nexperiments. In this work, we present a framework for managing experiments on\ndigital and/or physical assets with a focus on case-based reasoning with domain\nknowledge to reuse experimental data efficiently by deciding whether an\nalready-performed experiment (or associated answer) can be reused to answer a\nnew (potentially different) question from the engineer/user without having to\nset up and perform a new experiment. We provide the general architecture for\nsuch an experiment manager and validate our approach using an industrial\nvehicular energy system-design case study.", "AI": {"tldr": "The paper introduces a framework using case-based reasoning and domain knowledge to efficiently manage and reuse experimental data in system design, validated through a vehicular energy system case study.", "motivation": "To reduce time, effort, and costs in system design by reusing experimental data instead of conducting unnecessary experiments.", "method": "Introduced a framework incorporating case-based reasoning with domain knowledge for intelligent reuse of experimental data, supported by a general architecture design.", "result": "Validated the proposed framework using an industrial vehicular energy system-design case study, confirming its effectiveness.", "conclusion": "The framework accelerates design efforts by enabling intelligent reuse of experimental data, reducing redundancy in experimentation and enhancing efficiency in model-based systems engineering."}}
{"id": "2509.10549", "pdf": "https://arxiv.org/pdf/2509.10549", "abs": "https://arxiv.org/abs/2509.10549", "authors": ["Xing-Chan Lin"], "title": "How Easterners and Westerners perceive ADHD differently", "categories": ["q-bio.NC"], "comment": "7 pages, 1 figure", "summary": "Attention Deficit Hyperactivity Disorder has traditionally been\nconceptualized as a neurodevelopmental condition associated with deficits such\nas inattention, impulsivity, and poor time management. Such perspectives often\nemphasize pathology and functional limitations. More recent scholarship,\nhowever, has begun to reconceptualize ADHD by identifying potential adaptive\ncharacteristics, including heightened energy, periods of hyperfocus, and\nadvanced cognitive flexibility. A growing body of qualitative research also\nexamines self reported experiences of high functioning individuals with ADHD,\nhighlighting positive traits and successful coping strategies. Despite these\ndevelopments, the majority of existing studies remain concentrated in Western\ncontexts, particularly the United Kingdom, Netherlands, and Canada. This review\nseeks to address this imbalance by analyzing literature on ADHD strengths in\nEurope and North America alongside emerging studies from Hong Kong, Malaysia,\nand Singapore.", "AI": {"tldr": "This paper reviews literature on ADHD's positive traits across Western and Asian contexts, seeking to balance the predominantly Western focus in research.", "motivation": "To address the prevalent deficit-centered view of ADHD and explore the potential strengths and adaptive traits associated with the condition, especially emphasizing underrepresented Asian contexts.", "method": "Analyzing existing literature on ADHD strengths from both Western countries (UK, Netherlands, Canada) and Asian countries (Hong Kong, Malaysia, Singapore).", "result": "The review highlights evidence of ADHD's adaptive traits across various cultural settings but identifies a research gap in Asian contexts, compared to Western studies.", "conclusion": "Broadening the scope of ADHD research to include underrepresented regions offers a more holistic understanding of ADHD, including its strengths."}}
{"id": "2509.10555", "pdf": "https://arxiv.org/pdf/2509.10555", "abs": "https://arxiv.org/abs/2509.10555", "authors": ["Alejandra Perez", "Chinedu Nwoye", "Ramtin Raji Kermani", "Omid Mohareri", "Muhammad Abdullah Jamal"], "title": "SurgLaVi: Large-Scale Hierarchical Dataset for Surgical Vision-Language Representation Learning", "categories": ["cs.CV"], "comment": null, "summary": "Vision-language pre-training (VLP) offers unique advantages for surgery by\naligning language with surgical videos, enabling workflow understanding and\ntransfer across tasks without relying on expert-labeled datasets. However,\nprogress in surgical VLP remains constrained by the limited scale, procedural\ndiversity, semantic quality, and hierarchical structure of existing datasets.\nIn this work, we present SurgLaVi, the largest and most diverse surgical\nvision-language dataset to date, comprising nearly 240k clip-caption pairs from\nmore than 200 procedures, and comprising hierarchical levels at phase-, step-,\nand task-level. At the core of SurgLaVi lies a fully automated pipeline that\nsystematically generates fine-grained transcriptions of surgical videos and\nsegments them into coherent procedural units. To ensure high-quality\nannotations, it applies dual-modality filtering to remove irrelevant and noisy\nsamples. Within this framework, the resulting captions are enriched with\ncontextual detail, producing annotations that are both semantically rich and\neasy to interpret. To ensure accessibility, we release SurgLaVi-\\b{eta}, an\nopen-source derivative of 113k clip-caption pairs constructed entirely from\npublic data, which is over four times larger than existing surgical VLP\ndatasets. To demonstrate the value of SurgLaVi datasets, we introduce SurgCLIP,\na CLIP-style video-text contrastive framework with dual encoders, as a\nrepresentative base model. SurgCLIP achieves consistent improvements across\nphase, step, action, and tool recognition, surpassing prior state-of-the-art\nmethods, often by large margins. These results validate that large-scale,\nsemantically rich, and hierarchically structured datasets directly translate\ninto stronger and more generalizable representations, establishing SurgLaVi as\na key resource for developing surgical foundation models.", "AI": {"tldr": "The paper introduces SurgLaVi, the largest and most diverse surgical vision-language dataset, along with a new contrastive learning model SurgCLIP, significantly enhancing surgical video understanding.", "motivation": "To address the limitations of existing surgical vision-language datasets, which suffer from constrained scale, diversity, and semantic quality, thereby impeding progress in surgical VLP.", "method": "The authors create SurgLaVi, a comprehensive dataset generated through an automated pipeline for fine-grained transcription and segmentation of surgical videos, using dual-modality filtering to improve annotation quality. They also introduce SurgCLIP, a model with dual encoders for video-text contrastive learning.", "result": "SurgCLIP, trained on the SurgLaVi dataset, delivers consistent improvements in critical surgical tasks such as phase, step, action, and tool recognition, outperforming state-of-the-art models.", "conclusion": "The SurgLaVi dataset and SurgCLIP model demonstrate how large-scale, semantically rich, and hierarchical datasets can significantly enhance surgical foundational models. SurgLaVi is poised to be a crucial resource in this domain."}}
{"id": "2509.10702", "pdf": "https://arxiv.org/pdf/2509.10702", "abs": "https://arxiv.org/abs/2509.10702", "authors": ["Charles Hong", "Qijing Huang", "Grace Dinh", "Mahesh Subedar", "Yakun Sophia Shao"], "title": "DOSA: Differentiable Model-Based One-Loop Search for DNN Accelerators", "categories": ["cs.AR", "cs.LG"], "comment": "Published at MICRO 2023", "summary": "In the hardware design space exploration process, it is critical to optimize\nboth hardware parameters and algorithm-to-hardware mappings. Previous work has\nlargely approached this simultaneous optimization problem by separately\nexploring the hardware design space and the mapspace - both individually large\nand highly nonconvex spaces - independently. The resulting combinatorial\nexplosion has created significant difficulties for optimizers.\n  In this paper, we introduce DOSA, which consists of differentiable\nperformance models and a gradient descent-based optimization technique to\nsimultaneously explore both spaces and identify high-performing design points.\nExperimental results demonstrate that DOSA outperforms random search and\nBayesian optimization by 2.80x and 12.59x, respectively, in improving DNN model\nenergy-delay product, given a similar number of samples. We also demonstrate\nthe modularity and flexibility of DOSA by augmenting our analytical model with\na learned model, allowing us to optimize buffer sizes and mappings of a real\nDNN accelerator and attain a 1.82x improvement in energy-delay product.", "AI": {"tldr": "Introduces DOSA for combined optimization of hardware parameters and algorithm mappings using differentiable models and gradient-based optimization, achieving notable improvements.", "motivation": "Optimize the interplay of hardware parameters and algorithm-to-hardware mappings due to complexities in their simultaneous exploration.", "method": "Proposes DOSA, blending differentiable performance models and gradient descent techniques for unified exploration and optimization.", "result": "DOSA outperforms random search and Bayesian optimization significantly in enhancing energy-delay product. Demonstrates versatility by optimizing buffer sizes on a real-world DNN accelerator.", "conclusion": "DOSA offers a powerful, flexible solution for integrated hardware-design and mapping optimization, addressing challenges of prior independent approaches."}}
{"id": "2509.11070", "pdf": "https://arxiv.org/pdf/2509.11070", "abs": "https://arxiv.org/abs/2509.11070", "authors": ["Jia-Qi Yang", "Lei Shi"], "title": "Kernel-based Stochastic Approximation Framework for Nonlinear Operator Learning", "categories": ["stat.ML", "cs.LG", "cs.NA", "math.FA", "math.NA", "math.ST", "stat.TH"], "comment": "34 pages, 3 figures", "summary": "We develop a stochastic approximation framework for learning nonlinear\noperators between infinite-dimensional spaces utilizing general Mercer\noperator-valued kernels. Our framework encompasses two key classes: (i) compact\nkernels, which admit discrete spectral decompositions, and (ii) diagonal\nkernels of the form $K(x,x')=k(x,x')T$, where $k$ is a scalar-valued kernel and\n$T$ is a positive operator on the output space. This broad setting induces\nexpressive vector-valued reproducing kernel Hilbert spaces (RKHSs) that\ngeneralize the classical $K=kI$ paradigm, thereby enabling rich structural\nmodeling with rigorous theoretical guarantees. To address target operators\nlying outside the RKHS, we introduce vector-valued interpolation spaces to\nprecisely quantify misspecification error. Within this framework, we establish\ndimension-free polynomial convergence rates, demonstrating that nonlinear\noperator learning can overcome the curse of dimensionality. The use of general\noperator-valued kernels further allows us to derive rates for intrinsically\nnonlinear operator learning, going beyond the linear-type behavior inherent in\ndiagonal constructions of $K=kI$. Importantly, this framework accommodates a\nwide range of operator learning tasks, ranging from integral operators such as\nFredholm operators to architectures based on encoder-decoder representations.\nMoreover, we validate its effectiveness through numerical experiments on the\ntwo-dimensional Navier-Stokes equations.", "AI": {"tldr": "This paper proposes a stochastic approximation framework for learning nonlinear operators in infinite-dimensional spaces using general operator-valued kernels, achieving dimension-free convergence rates and overcoming the curse of dimensionality.", "motivation": "To develop a robust framework for learning nonlinear operators in infinite-dimensional spaces, addressing challenges like dimensionality and operator misspecifications.", "method": "The authors introduce a framework using Mercer operator-valued kernels and vector-valued RKHSs, focusing on compact and diagonal kernel classes. They quantify misspecification error via interpolation spaces and establish convergence rates for nonlinear operator learning.", "result": "The proposed framework achieves dimension-free polynomial convergence rates, allowing for effective learning of nonlinear operators. Numerical experiments on the Navier-Stokes equations validate its practical applicability.", "conclusion": "The framework broadens the scope of operator learning while overcoming dimensionality challenges, offering rigorous guarantees and practical applications in complex nonlinear systems."}}
{"id": "2509.11134", "pdf": "https://arxiv.org/pdf/2509.11134", "abs": "https://arxiv.org/abs/2509.11134", "authors": ["Jiaang Duan", "Shenglin Xu", "Shiyou Qian", "Dingyu Yang", "Kangjin Wang", "Chenzhi Liao", "Yinghao Yu", "Qin Hua", "Hanwen Hu", "Qi Wang", "Wenchao Wu", "Dongqing Bao", "Tianyu Lu", "Jian Cao", "Guangtao Xue", "Guodong Yang", "Liping Zhang", "Gang Chen"], "title": "GFS: A Preemption-aware Scheduling Framework for GPU Clusters with Predictive Spot Instance Management", "categories": ["cs.DC"], "comment": "This paper has been accepted to the 31st ACM International Conference\n  on Architectural Support for Programming Languages and Operating Systems\n  (ASPLOS 2026)", "summary": "The surge in large language models (LLMs) has fundamentally reshaped the\nlandscape of GPU usage patterns, creating an urgent need for more efficient\nmanagement strategies. While cloud providers employ spot instances to reduce\ncosts for low-priority (LP) tasks, existing schedulers still grapple with high\neviction rates and lengthy queuing times. To address these limitations, we\npresent GFS, a novel preemptive scheduling framework that enhances\nservice-level objective (SLO) compliance for high-priority (HP) tasks while\nminimizing preemptions to LP tasks. Firstly, GFS utilizes a lightweight\nforecasting model that predicts GPU demand among different tenants, enabling\nproactive resource management. Secondly, GFS employs a dynamic allocation\nmechanism to adjust the spot quota for LP tasks with guaranteed durations.\nLastly, GFS incorporates a preemptive scheduling policy that prioritizes HP\ntasks while minimizing the impact on LP tasks. We demonstrate the effectiveness\nof GFS through both real-world implementation and simulations. The results show\nthat GFS reduces eviction rates by 33.0\\%, and cuts queuing delays by 44.1\\%\nfor LP tasks. Furthermore, GFS enhances the GPU allocation rate by up to 22.8\\%\nin real production clusters. In a production cluster of more than 10,000 GPUs,\nGFS yields roughly \\$459,715 in monthly benefits.", "AI": {"tldr": "GFS is a scheduling framework aimed at optimizing GPU resource allocation between high-priority and low-priority tasks, reducing eviction rates and queuing delays.", "motivation": "The paper addresses inefficiencies in GPU management, particularly high eviction rates and long queuing times under current scheduling mechanisms for cloud computing tasks.", "method": "GFS employs three core strategies: a lightweight forecasting model for tenant GPU demand, dynamic spot allocation for LP tasks, and a preemptive scheduling policy to prioritize HP tasks while minimizing LP impact.", "result": "GFS decreases eviction rates by 33.0%, queuing delays by 44.1%, and improves GPU allocation rates by 22.8%. In real production, it generates $459,715 in monthly benefits for a 10,000-GPU cluster.", "conclusion": "GFS is an effective solution for enhancing GPU scheduling efficiency, enabling cost savings and improved resource utilization in cloud environments."}}
{"id": "2509.11113", "pdf": "https://arxiv.org/pdf/2509.11113", "abs": "https://arxiv.org/abs/2509.11113", "authors": ["Vedant Sawal", "Hiu Yung Wong"], "title": "Application of Machine Learning for Correcting Defect-induced Neuromorphic Circuit Inference Errors", "categories": ["cs.NE", "cs.AI"], "comment": null, "summary": "This paper presents a machine learning-based approach to correct inference\nerrors caused by stuck-at faults in fully analog ReRAM-based neuromorphic\ncircuits. Using a Design-Technology Co-Optimization (DTCO) simulation\nframework, we model and analyze six spatial defect types-circular,\ncircular-complement, ring, row, column, and checkerboard-across multiple layers\nof a multi-array neuromorphic architecture. We demonstrate that the proposed\ncorrection method, which employs a lightweight neural network trained on the\ncircuit's output voltages, can recover up to 35% (from 55% to 90%) inference\naccuracy loss in defective scenarios. Our results, based on handwritten digit\nrecognition tasks, show that even small corrective networks can significantly\nimprove circuit robustness. This method offers a scalable and energy-efficient\npath toward enhanced yield and reliability for neuromorphic systems in edge and\ninternet-of-things (IoTs) applications. In addition to correcting the specific\ndefect types used during training, our method also demonstrates the ability to\ngeneralize-achieving reasonable accuracy when tested on different types of\ndefects not seen during training. The framework can be readily extended to\nsupport real-time adaptive learning, enabling on-chip correction for dynamic or\naging-induced fault profiles.", "AI": {"tldr": "The paper introduces a lightweight neural network approach to correct inference errors caused by stuck-at faults in fully analog ReRAM-based neuromorphic circuits, recovering up to 35% accuracy loss.", "motivation": "The authors aim to address inference errors and reliability issues in neuromorphic systems caused by stuck-at faults and spatial defects in ReRAM-based architectures, which are critical for scalable and energy-efficient IoT applications.", "method": "A Design-Technology Co-Optimization (DTCO) simulation framework is used to model multiple spatial defects, with a lightweight neural network trained on circuit output voltages employed for corrective measures.", "result": "The corrective method enhances inference accuracy from 55% to 90% on handwritten digit recognition tasks, generalizes to unseen defect types, and improves circuit robustness significantly.", "conclusion": "The proposed approach offers scalable, energy-efficient solutions for improving yield, reliability, and adaptability of neuromorphic systems in edge IoT applications, supporting adaptive learning for dynamic fault profiles."}}
{"id": "2509.10704", "pdf": "https://arxiv.org/pdf/2509.10704", "abs": "https://arxiv.org/abs/2509.10704", "authors": ["Xingchen Wan", "Han Zhou", "Ruoxi Sun", "Hootan Nakhost", "Ke Jiang", "Rajarishi Sinha", "Sercan \u00d6. Ar\u0131k"], "title": "Maestro: Self-Improving Text-to-Image Generation via Agent Orchestration", "categories": ["cs.AI", "cs.CV"], "comment": "15 pages, 7 figures, 2 tables (22 pages, 9 figures and 3 tables\n  including references and appendices)", "summary": "Text-to-image (T2I) models, while offering immense creative potential, are\nhighly reliant on human intervention, posing significant usability challenges\nthat often necessitate manual, iterative prompt engineering over often\nunderspecified prompts. This paper introduces Maestro, a novel self-evolving\nimage generation system that enables T2I models to autonomously self-improve\ngenerated images through iterative evolution of prompts, using only an initial\nprompt. Maestro incorporates two key innovations: 1) self-critique, where\nspecialized multimodal LLM (MLLM) agents act as 'critics' to identify\nweaknesses in generated images, correct for under-specification, and provide\ninterpretable edit signals, which are then integrated by a 'verifier' agent\nwhile preserving user intent; and 2) self-evolution, utilizing MLLM-as-a-judge\nfor head-to-head comparisons between iteratively generated images, eschewing\nproblematic images, and evolving creative prompt candidates that align with\nuser intents. Extensive experiments on complex T2I tasks using black-box models\ndemonstrate that Maestro significantly improves image quality over initial\nprompts and state-of-the-art automated methods, with effectiveness scaling with\nmore advanced MLLM components. This work presents a robust, interpretable, and\neffective pathway towards self-improving T2I generation.", "AI": {"tldr": "The paper introduces Maestro, a self-evolving system improving text-to-image models by iteratively evolving prompts using critiques from multimodal agents.", "motivation": "Text-to-image models require significant human intervention for prompt engineering, posing usability challenges.", "method": "Maestro employs a two-step process: self-critique for identifying image weaknesses through multimodal agents, and self-evolution for prompt iteration guided by human and system validation.", "result": "Maestro enhances image quality from initial prompts and outperforms state-of-the-art automated systems in experiments involving complex tasks.", "conclusion": "The study provides an effective and interpretable solution to improve text-to-image model outputs by enabling iterative self-improvement of prompts."}}
{"id": "2201.06325", "pdf": "https://arxiv.org/pdf/2201.06325", "abs": "https://arxiv.org/abs/2201.06325", "authors": ["Umang Mathur", "Andreas Pavlogiannis", "H\u00fcnkar Can Tun\u00e7", "Mahesh Viswanathan"], "title": "A Tree Clock Data Structure for Causal Orderings in Concurrent Executions", "categories": ["cs.LO", "cs.DC", "cs.DS", "cs.PL", "cs.SE"], "comment": null, "summary": "Dynamic techniques are a scalable and effective way to analyze concurrent\nprograms. Instead of analyzing all behaviors of a program, these techniques\ndetect errors by focusing on a single program execution. Often a crucial step\nin these techniques is to define a causal ordering between events in the\nexecution, which is then computed using vector clocks, a simple data structure\nthat stores logical times of threads. The two basic operations of vector\nclocks, namely join and copy, require $\\Theta(k)$ time, where $k$ is the number\nof threads. Thus they are a computational bottleneck when $k$ is large.\n  In this work, we introduce tree clocks, a new data structure that replaces\nvector clocks for computing causal orderings in program executions. Joining and\ncopying tree clocks takes time that is roughly proportional to the number of\nentries being modified, and hence the two operations do not suffer the a-priori\n$\\Theta(k)$ cost per application. We show that when used to compute the classic\nhappens-before (HB) partial order, tree clocks are optimal, in the sense that\nno other data structure can lead to smaller asymptotic running time. Moreover,\nwe demonstrate that tree clocks can be used to compute other partial orders,\nsuch as schedulable-happens-before (SHB) and the standard Mazurkiewicz (MAZ)\npartial order, and thus are a versatile data structure. Our experiments show\nthat just by replacing vector clocks with tree clocks, the computation becomes\nfrom $2.02 \\times$ faster (MAZ) to $2.66 \\times$ (SHB) and $2.97 \\times$ (HB)\non average per benchmark. These results illustrate that tree clocks have the\npotential to become a standard data structure with wide applications in\nconcurrent analyses.", "AI": {"tldr": "The paper introduces tree clocks as an optimized data structure to compute causal orders in concurrent program analysis. It greatly improves computation efficiency compared to traditional vector clocks, especially for a large number of threads.", "motivation": "Vector clocks, though widely used to compute causal orderings in concurrent program executions, suffer from a computational bottleneck as their basic operations scale linearly with the number of threads.", "method": "The authors propose tree clocks as a replacement for vector clocks. Joining and copying operations in tree clocks depend on the number of modified entries rather than the total thread count, reducing computational overhead.", "result": "Tree clocks achieved significant speed improvements in benchmarks over vector clocks: 2.02x faster for Mazurkiewicz (MAZ), 2.66x faster for schedulable-happens-before (SHB), and 2.97x faster for happens-before (HB) analyses.", "conclusion": "Tree clocks offer optimal, versatile, and efficient computation for causal orderings in concurrent programs, outperforming vector clocks and presenting broad applicability in concurrent program analyses."}}
{"id": "2509.10496", "pdf": "https://arxiv.org/pdf/2509.10496", "abs": "https://arxiv.org/abs/2509.10496", "authors": ["Imen Jarraya", "Safa Ben Atitallah", "Fatimah Alahmeda", "Mohamed Abdelkadera", "Maha Drissa", "Fatma Abdelhadic", "Anis Koubaaa"], "title": "SOH-KLSTM: A Hybrid Kolmogorov-Arnold Network and LSTM Model for Enhanced Lithium-Ion Battery Health Monitoring", "categories": ["cs.LG"], "comment": null, "summary": "Accurate and reliable State Of Health (SOH) estimation for Lithium (Li)\nbatteries is critical to ensure the longevity, safety, and optimal performance\nof applications like electric vehicles, unmanned aerial vehicles, consumer\nelectronics, and renewable energy storage systems. Conventional SOH estimation\ntechniques fail to represent the non-linear and temporal aspects of battery\ndegradation effectively. In this study, we propose a novel SOH prediction\nframework (SOH-KLSTM) using Kolmogorov-Arnold Network (KAN)-Integrated\nCandidate Cell State in LSTM for Li batteries Health Monitoring. This hybrid\napproach combines the ability of LSTM to learn long-term dependencies for\naccurate time series predictions with KAN's non-linear approximation\ncapabilities to effectively capture complex degradation behaviors in Lithium\nbatteries.", "AI": {"tldr": "The paper proposes a hybrid SOH prediction framework (SOH-KLSTM) combining KAN and LSTM to improve Lithium battery health monitoring by addressing non-linear and temporal degradation challenges.", "motivation": "To enhance SOH estimation techniques for Lithium batteries, addressing limitations in conventional methods that fail to effectively represent non-linear and temporal degradation aspects.", "method": "The study introduces SOH-KLSTM, a hybrid framework leveraging Kolmogorov-Arnold Network (KAN) for non-linear approximations and Long Short-Term Memory (LSTM) for time series predictions.", "result": "The proposed approach effectively captures complex degradation behaviors in Lithium batteries.", "conclusion": "SOH-KLSTM demonstrates the potential to improve the accuracy and reliability of Lithium battery health monitoring by integrating KAN and LSTM strengths."}}
{"id": "2509.10730", "pdf": "https://arxiv.org/pdf/2509.10730", "abs": "https://arxiv.org/abs/2509.10730", "authors": ["Yunfan Ren", "Yixi Cai", "Haotian Li", "Nan Chen", "Fangcheng Zhu", "Longji Yin", "Fanze Kong", "Rundong Li", "Fu Zhang"], "title": "A Survey on LiDAR-based Autonomous Aerial Vehicles", "categories": ["cs.RO"], "comment": null, "summary": "This survey offers a comprehensive overview of recent advancements in\nLiDAR-based autonomous Unmanned Aerial Vehicles (UAVs), covering their design,\nperception, planning, and control strategies. Over the past decade, LiDAR\ntechnology has become a crucial enabler for high-speed, agile, and reliable UAV\nnavigation, especially in GPS-denied environments. The paper begins by\nexamining the evolution of LiDAR sensors, emphasizing their unique advantages\nsuch as high accuracy, long-range depth measurements, and robust performance\nunder various lighting conditions, making them particularly well-suited for UAV\napplications. The integration of LiDAR with UAVs has significantly enhanced\ntheir autonomy, enabling complex missions in diverse and challenging\nenvironments. Subsequently, we explore essential software components, including\nperception technologies for state estimation and mapping, as well as trajectory\nplanning and control methodologies, and discuss their adoption in LiDAR-based\nUAVs. Additionally, we analyze various practical applications of the\nLiDAR-based UAVs, ranging from industrial operations to supporting different\naerial platforms and UAV swarm deployments. The survey concludes by discussing\nexisting challenges and proposing future research directions to advance\nLiDAR-based UAVs and enhance multi-UAV collaboration. By synthesizing recent\ndevelopments, this paper aims to provide a valuable resource for researchers\nand practitioners working to push the boundaries of LiDAR-based UAV systems.", "AI": {"tldr": "This paper surveys LiDAR-based UAV advancements in design, perception, and control, emphasizing their application in complex environments and future directions.", "motivation": "LiDAR technology has become vital for UAVs, providing reliable navigation in GPS-denied environments and enabling advanced autonomy.", "method": "The paper examines LiDAR sensor evolution, integrates software components for perception and control, and reviews practical UAV applications, concluding with challenges and future research proposals.", "result": "LiDAR significantly enhances UAV autonomy, enabling diverse practical applications such as industrial use and UAV swarm collaboration in challenging environments.", "conclusion": "The survey highlights LiDAR as a key enabler for advanced UAV systems, suggesting research directions for overcoming challenges and improving multi-UAV collaboration."}}
{"id": "2509.10644", "pdf": "https://arxiv.org/pdf/2509.10644", "abs": "https://arxiv.org/abs/2509.10644", "authors": ["Enora Rice", "Katharina von der Wense", "Alexis Palmer"], "title": "Interdisciplinary Research in Conversation: A Case Study in Computational Morphology for Language Documentation", "categories": ["cs.CL"], "comment": "Accepted to EMNLP 2025", "summary": "Computational morphology has the potential to support language documentation\nthrough tasks like morphological segmentation and the generation of Interlinear\nGlossed Text (IGT). However, our research outputs have seen limited use in\nreal-world language documentation settings. This position paper situates the\ndisconnect between computational morphology and language documentation within a\nbroader misalignment between research and practice in NLP and argues that the\nfield risks becoming decontextualized and ineffectual without systematic\nintegration of User-Centered Design (UCD). To demonstrate how principles from\nUCD can reshape the research agenda, we present a case study of GlossLM, a\nstate-of-the-art multilingual IGT generation model. Through a small-scale user\nstudy with three documentary linguists, we find that despite strong metric\nbased performance, the system fails to meet core usability needs in real\ndocumentation contexts. These insights raise new research questions around\nmodel constraints, label standardization, segmentation, and personalization. We\nargue that centering users not only produces more effective tools, but surfaces\nricher, more relevant research directions", "AI": {"tldr": "This paper highlights the potential of computational morphology in aiding language documentation but identifies a gap between research outputs and real-world usability. It advocates for User-Centered Design to address this issue, presenting a case study and user findings to propose new research directions.", "motivation": "To bridge the gap between computational morphology outputs and their actual usability in language documentation, ensuring tools truly meet practical needs.", "method": "The authors conducted a user study with documentary linguists using GlossLM, a multilingual top-performing model for IGT generation, and evaluated its usability against linguistic documentary needs.", "result": "The study revealed that, despite strong performance metrics, the system fell short in meeting essential user needs like usability within real-world language documentation contexts.", "conclusion": "Integrating User-Centered Design can reshape computational morphology research, making tools more effective and unveiling vital research directions, like standardization and personalization for users."}}
{"id": "2509.10819", "pdf": "https://arxiv.org/pdf/2509.10819", "abs": "https://arxiv.org/abs/2509.10819", "authors": ["Christoph Hochrainer", "Valentin W\u00fcstholz", "Maria Christakis"], "title": "Arguzz: Testing zkVMs for Soundness and Completeness Bugs", "categories": ["cs.SE", "cs.CR", "cs.PL"], "comment": null, "summary": "Zero-knowledge virtual machines (zkVMs) are increasingly deployed in\ndecentralized applications and blockchain rollups since they enable verifiable\noff-chain computation. These VMs execute general-purpose programs, frequently\nwritten in Rust, and produce succinct cryptographic proofs. However, zkVMs are\ncomplex, and bugs in their constraint systems or execution logic can cause\ncritical soundness (accepting invalid executions) or completeness (rejecting\nvalid ones) issues.\n  We present Arguzz, the first automated tool for testing zkVMs for soundness\nand completeness bugs. To detect such bugs, Arguzz combines a novel variant of\nmetamorphic testing with fault injection. In particular, it generates\nsemantically equivalent program pairs, merges them into a single Rust program\nwith a known output, and runs it inside a zkVM. By injecting faults into the\nVM, Arguzz mimics malicious or buggy provers to uncover overly weak\nconstraints.\n  We used Arguzz to test six real-world zkVMs (RISC Zero, Nexus, Jolt, SP1,\nOpenVM, and Pico) and found eleven bugs in three of them. One RISC Zero bug\nresulted in a $50,000 bounty, despite prior audits, demonstrating the critical\nneed for systematic testing of zkVMs.", "AI": {"tldr": "Arguzz is an automated testing tool for identifying soundness and completeness bugs in zero-knowledge virtual machines (zkVMs), validating their cryptographic proofs by detecting faulty executions.", "motivation": "zkVMs are deployed in blockchain applications to enable verifiable off-chain computation, but their complexity can lead to critical bugs affecting soundness and completeness.", "method": "Arguzz uses a novel approach combining metamorphic testing with fault injection to generate equivalent program pairs and test zkVMs for issues by simulating faults.", "result": "The tool tested six zkVMs and uncovered eleven bugs in three systems, including a $50,000 bug in RISC Zero, showcasing Arguzz's effectiveness.", "conclusion": "Systematic testing via tools like Arguzz is essential to ensuring the reliability and security of zkVMs, especially given their significant role in decentralized applications."}}
{"id": "2509.10552", "pdf": "https://arxiv.org/pdf/2509.10552", "abs": "https://arxiv.org/abs/2509.10552", "authors": ["D. A. Blanco-Mora", "A. Dierolf", "J. Gon\u00e7alves", "M. van Der Meulen"], "title": "Trial-Level Time-frequency EEG Desynchronization as a Neural Marker of Pain", "categories": ["q-bio.NC", "cs.LG"], "comment": "7 pages, 3 Figures", "summary": "Pain remains one of the most pressing health challenges, yet its measurement\nstill relies heavily on self-report, limiting monitoring in non-communicative\npatients and hindering translational research. Neural oscillations recorded\nwith electroencephalography (EEG) provide a promising avenue for identifying\nreproducible markers of nociceptive processing. Prior studies have reported\npain-related event-related desynchronization (ERD) in the alpha and beta bands,\nbut most rely on trial-averaging, obscuring variability that may be critical\nfor perception. We analyzed high-density EEG from 59 healthy participants who\nunderwent electrical stimulation under Pain and No-Pain conditions. Per-trial\ntime-frequency decomposition revealed robust beta-band ERD in frontal-central\nelectrodes that differentiated Pain from No-Pain trials. Generalized linear\nmixed models demonstrated that ERD scaled with subjective intensity ratings\n(VAS), and that age and gender moderated this relationship. Reverse models\nfurther showed that ERD predicted VAS ratings across participants, underscoring\nits potential as a nonverbal marker of pain. These findings provide preliminary\nevidence that trial-level EEG oscillations can serve as reliable indicators of\npain and open avenues for individualized, report-free pain monitoring. Future\nwork should validate these results in patient populations and extend analyses\nto multimodal approaches combining EEG, MRI, and attention-based modulation\nstrategies.", "AI": {"tldr": "The study explores EEG oscillations as potential non-verbal markers for pain intensity, finding robust beta-band desynchronization linked to subjective pain ratings.", "motivation": "Current pain measurement relies heavily on self-reports, limiting utility for non-communicative patients and translational research.", "method": "The researchers used high-density EEG to analyze electrical stimulation trials, examining event-related desynchronization (ERD) in alpha and beta bands across Pain and No-Pain conditions.", "result": "Beta-band ERD effectively differentiated Pain from No-Pain and corresponded to subjective pain intensity ratings, with moderators like age and gender affecting the link.", "conclusion": "EEG oscillations offer promise as reliable, individualized, nonverbal indicators for pain, warranting validation in clinical settings and integration with multimodal approaches."}}
{"id": "2509.10620", "pdf": "https://arxiv.org/pdf/2509.10620", "abs": "https://arxiv.org/abs/2509.10620", "authors": ["Emily Kaczmarek", "Justin Szeto", "Brennan Nichyporuk", "Tal Arbel"], "title": "Building a General SimCLR Self-Supervised Foundation Model Across Neurological Diseases to Advance 3D Brain MRI Diagnoses", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted to ICCV 2025 Workshop CVAMD", "summary": "3D structural Magnetic Resonance Imaging (MRI) brain scans are commonly\nacquired in clinical settings to monitor a wide range of neurological\nconditions, including neurodegenerative disorders and stroke. While deep\nlearning models have shown promising results analyzing 3D MRI across a number\nof brain imaging tasks, most are highly tailored for specific tasks with\nlimited labeled data, and are not able to generalize across tasks and/or\npopulations. The development of self-supervised learning (SSL) has enabled the\ncreation of large medical foundation models that leverage diverse, unlabeled\ndatasets ranging from healthy to diseased data, showing significant success in\n2D medical imaging applications. However, even the very few foundation models\nfor 3D brain MRI that have been developed remain limited in resolution, scope,\nor accessibility. In this work, we present a general, high-resolution\nSimCLR-based SSL foundation model for 3D brain structural MRI, pre-trained on\n18,759 patients (44,958 scans) from 11 publicly available datasets spanning\ndiverse neurological diseases. We compare our model to Masked Autoencoders\n(MAE), as well as two supervised baselines, on four diverse downstream\nprediction tasks in both in-distribution and out-of-distribution settings. Our\nfine-tuned SimCLR model outperforms all other models across all tasks. Notably,\nour model still achieves superior performance when fine-tuned using only 20% of\nlabeled training samples for predicting Alzheimer's disease. We use publicly\navailable code and data, and release our trained model at\nhttps://github.com/emilykaczmarek/3D-Neuro-SimCLR, contributing a broadly\napplicable and accessible foundation model for clinical brain MRI analysis.", "AI": {"tldr": "A SimCLR-based self-supervised learning (SSL) foundation model for high-resolution 3D brain MRI analysis is proposed, outperforming existing models in diverse tasks even using limited labeled data.", "motivation": "Current deep learning models processing 3D brain MRI are task-specific, unable to generalize across tasks or populations due to limited labeled data. SSL enables the creation of robust models for 2D imaging, but 3D MRI models have limitations.", "method": "The model is pre-trained using SimCLR SSL on 44,958 scans from 11 datasets, spanning diverse neurological diseases. Comparisons are made against Masked Autoencoders (MAE) and supervised baselines across prediction tasks.", "result": "The SimCLR-based model demonstrates superior performance across all tasks, including Alzheimer's prediction using limited labeled data, outperforming MAE and supervised baselines.", "conclusion": "This research introduces an accessible, broadly applicable foundation model for clinical brain MRI analysis, advancing generalization and task adaptability in 3D brain MRI processing."}}
{"id": "2509.10751", "pdf": "https://arxiv.org/pdf/2509.10751", "abs": "https://arxiv.org/abs/2509.10751", "authors": ["Lucas M. Leipnitz de Fraga", "Cl\u00e1udio Machado Diniz"], "title": "Design and Analysis of Approximate Hardware Accelerators for VVC Intra Angular Prediction", "categories": ["cs.AR"], "comment": "Accepted SBCCI 2025", "summary": "The Versatile Video Coding (VVC) standard significantly improves compression\nefficiency over its predecessor, HEVC, but at the cost of substantially higher\ncomputational complexity, particularly in intra-frame prediction. This stage\nemploys various directional modes, each requiring multiple multiplications\nbetween reference samples and constant coefficients. To optimize these\noperations at hardware accelerators, multiplierless constant multiplication\n(MCM) blocks offer a promising solution. However, VVC's interpolation filters\nhave more than fifty distinct coefficients, making MCM implementations\nresource-intensive. This work proposes an approximation method to reduce the\nnumber of interpolation coefficients by averaging fixed subsets of them,\ntherefore decreasing MCM block size and potentially lowering circuit area and\npower consumption. Six different MCM block architectures for angular intra\nprediction are introduced, in which five use the approximation method\nintroduced in this work, and evaluate the trade-off between coefficient\nreduction and coding efficiency compared with a conventional multiplier\narchitecture. Experimental results in ten videos demonstrate that only two MCM\nimplementations exceed a 4% BD-Rate increase and 2.6% on average in the worst\ncase, while two of the MCM implementations have circuit area reduction of 20%\nand 44%. For three of the architectures, parallel sample prediction modules\nwere synthesized, showing a reduction of 30% gate area compared to single\nsample processing units, and a reduction in energy consumption for two of the\nimplementations.", "AI": {"tldr": "This paper introduces efficient multiplierless constant multiplication (MCM) architectures for angular intra prediction in VVC to reduce complexity and hardware resource usage with minimal coding efficiency loss.", "motivation": "VVC improves video compression but increases computational complexity, particularly in angular intra prediction. Hardware resource limitations necessitate optimization strategies.", "method": "Proposes averaging subsets of interpolation coefficients to reduce complexity in MCM architectures. Introduced six architectures to analyze trade-offs between coefficient reduction and coding efficiency.", "result": "Experimental results show only minor coding efficiency losses (average BD-Rate increase of 2.6%), but significant hardware resource savings (up to 44% reduced circuit area and lower energy consumption).", "conclusion": "The proposed architectures effectively balance coding efficiency and hardware resource optimization, providing substantial benefits for VVC implementations in hardware accelerators."}}
{"id": "2509.11146", "pdf": "https://arxiv.org/pdf/2509.11146", "abs": "https://arxiv.org/abs/2509.11146", "authors": ["Byungchang So"], "title": "Maximum diversity, weighting and invariants of time series", "categories": ["stat.ML", "cs.LG", "eess.SP", "math.MG", "46N40, 51F99, 68T10"], "comment": null, "summary": "Magnitude, obtained as a special case of Euler characteristic of enriched\ncategory, represents a sense of the size of metric spaces and is related to\nclassical notions such as cardinality, dimension, and volume. While the studies\nhave explained the meaning of magnitude from various perspectives, continuity\nalso gives a valuable view of magnitude. Based on established results about\ncontinuity of magnitude and maximum diversity, this article focuses on\ncontinuity of weighting, a distribution whose totality is magnitude, and its\nvariation corresponding to maximum diversity. Meanwhile, recent studies also\nilluminated the connection between magnitude and data analysis by applying\nmagnitude theory to point clouds representing the data or the set of model\nparameters. This article will also provide an application for time series\nanalysis by introducing a new kind of invariants of periodic time series, where\nthe invariance follows directly from the continuity results. As a use-case, a\nsimple machine learning experiment is conducted with real-world data, in which\nthe suggested invariants improved the performance.", "AI": {"tldr": "This paper examines the concept of magnitude, connecting it to continuity, and proposes new invariant tools for periodic time series analysis, supported by a machine learning use case.", "motivation": "To deepen the understanding of magnitude in metric spaces by examining its continuity and extending its application to time series analysis.", "method": "The study investigates the continuity properties of magnitude and weighting in relation to maximum diversity and introduces new invariant measures for time series analysis.", "result": "The invariants derived from the continuity of magnitude improved machine learning performance in a real-world dataset use case.", "conclusion": "The paper demonstrates that magnitude, understood through continuity, has practical applications in data analysis, particularly enhancing periodic time series representation and machine learning outcomes."}}
{"id": "2509.11152", "pdf": "https://arxiv.org/pdf/2509.11152", "abs": "https://arxiv.org/abs/2509.11152", "authors": ["Wajih Boukaram", "David Keyes", "Sherry Li", "Yang Liu", "George Turkiyyah"], "title": "Linear Complexity $\\mathcal{H}^2$ Direct Solver for Fine-Grained Parallel Architectures", "categories": ["cs.DC"], "comment": null, "summary": "We present factorization and solution phases for a new linear complexity\ndirect solver designed for concurrent batch operations on fine-grained parallel\narchitectures, for matrices amenable to hierarchical representation. We focus\non the strong-admissibility-based $\\mathcal{H}^2$ format, where strong\nrecursive skeletonization factorization compresses remote interactions. We\nbuild upon previous implementations of $\\mathcal{H}^2$ matrix construction for\nefficient factorization and solution algorithm design, which are illustrated\ngraphically in stepwise detail. The algorithms are ``blackbox'' in the sense\nthat the only inputs are the matrix and right-hand side, without analytical or\ngeometrical information about the origin of the system. We demonstrate linear\ncomplexity scaling in both time and memory on four representative families of\ndense matrices up to one million in size. Parallel scaling up to 16 threads is\nenabled by a multi-level matrix graph coloring and avoidance of dynamic memory\nallocations thanks to prefix-sum memory management. An experimental backward\nerror analysis is included. We break down the timings of different phases,\nidentify phases that are memory-bandwidth limited, and discuss alternatives for\nphases that may be sensitive to the trend to employ lower precisions for\nperformance.", "AI": {"tldr": "This paper introduces a linear complexity direct solver for matrices in hierarchical formats, optimized for fine-grained parallel architectures. It achieves efficient factorization and solution with compact, parallelizable algorithms.", "motivation": "The need to solve large-scale dense matrices efficiently on modern parallel architectures motivated the development of a scalable and memory-efficient algorithm.", "method": "The solver leverages a strong-admissibility-based hierarchical ($\\mathcal{H}^2$) representation and recursive skeletonization factorization while avoiding dynamic memory allocations through prefix-sum memory management.", "result": "The solver achieves linear scaling in time and memory for matrices with sizes up to one million and demonstrates parallel scaling across 16 threads.", "conclusion": "The proposed methodology is effective for large-scale dense matrices, offering linear complexity and scalable performance while requiring minimal input information about the matrix or its origin."}}
{"id": "2509.11755", "pdf": "https://arxiv.org/pdf/2509.11755", "abs": "https://arxiv.org/abs/2509.11755", "authors": ["Paul Templier", "Hannah Janmohamed", "David Labonte", "Antoine Cully"], "title": "Time to Play: Simulating Early-Life Animal Dynamics Enhances Robotics Locomotion Discovery", "categories": ["cs.NE", "cs.RO"], "comment": null, "summary": "Developmental changes in body morphology profoundly shape locomotion in\nanimals, yet artificial agents and robots are typically trained under static\nphysical parameters. Inspired by ontogenetic scaling of muscle power in\nbiology, we propose Scaling Mechanical Output over Lifetime (SMOL), a novel\ncurriculum that dynamically modulates robot actuator strength to mimic natural\nvariations in power-to-weight ratio during growth and ageing. Integrating SMOL\ninto the MAP-Elites quality-diversity framework, we vary the torque in standard\nrobotics tasks to mimic the evolution of strength in animals as they grow up\nand as their body changes. Through comprehensive empirical evaluation, we show\nthat the SMOL schedule consistently elevates both performance and diversity of\nlocomotion behaviours across varied control scenarios, by allowing agents to\nleverage advantageous physics early on to discover skills that act as stepping\nstones when they reach their final standard body properties. Based on studies\nof the total power output in humans, we also implement the SMOL-Human schedule\nthat models isometric body variations due to non-linear changes like puberty,\nand study its impact on robotics locomotion.", "AI": {"tldr": "The paper introduces SMOL, a novel curriculum for robotic locomotion inspired by developmental changes in animals, enhancing performance and diversity through dynamic modulation of actuator strength.", "motivation": "The motivation is to address the lack of dynamic training in robotics, inspired by how animals adapt their locomotion as their body morphology changes during growth and ageing.", "method": "The SMOL curriculum integrates into the MAP-Elites framework, dynamically adjusting robot actuator strength to simulate the varying power-to-weight ratio seen in animals throughout development.", "result": "Empirical tests demonstrate that SMOL improves the performance and diversity of robotic locomotion behaviors by leveraging early advantageous physics to develop robust skills.", "conclusion": "SMOL successfully mimics natural developmental changes, providing insights into enhancing robotic systems and allowing robots to develop more adaptable and diverse locomotion behaviors."}}
{"id": "2509.10707", "pdf": "https://arxiv.org/pdf/2509.10707", "abs": "https://arxiv.org/abs/2509.10707", "authors": ["Sajjad Abdoli", "Rudi Cilibrasi", "Rima Al-Shikh"], "title": "Understanding AI Evaluation Patterns: How Different GPT Models Assess Vision-Language Descriptions", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "As AI systems increasingly evaluate other AI outputs, understanding their\nassessment behavior becomes crucial for preventing cascading biases. This study\nanalyzes vision-language descriptions generated by NVIDIA's Describe Anything\nModel and evaluated by three GPT variants (GPT-4o, GPT-4o-mini, GPT-5) to\nuncover distinct \"evaluation personalities\" the underlying assessment\nstrategies and biases each model demonstrates. GPT-4o-mini exhibits systematic\nconsistency with minimal variance, GPT-4o excels at error detection, while\nGPT-5 shows extreme conservatism with high variability. Controlled experiments\nusing Gemini 2.5 Pro as an independent question generator validate that these\npersonalities are inherent model properties rather than artifacts. Cross-family\nanalysis through semantic similarity of generated questions reveals significant\ndivergence: GPT models cluster together with high similarity while Gemini\nexhibits markedly different evaluation strategies. All GPT models demonstrate a\nconsistent 2:1 bias favoring negative assessment over positive confirmation,\nthough this pattern appears family-specific rather than universal across AI\narchitectures. These findings suggest that evaluation competence does not scale\nwith general capability and that robust AI assessment requires diverse\narchitectural perspectives.", "AI": {"tldr": "This paper investigates the evaluation behaviors of GPT models in assessing AI-generated outputs, revealing distinct 'evaluation personalities' and biases.", "motivation": "The motivation is to understand the behaviors, strategies, and biases of AI systems like GPT when they evaluate outputs from other AI systems to prevent cascading biases in large-scale AI evaluations.", "method": "The study analyzes descriptions created by NVIDIA's Describe Anything Model and assesses them using three GPT models. Controlled experiments with Gemini 2.5 Pro as an independent generator validate findings, and cross-family analyses highlight differences between GPT and Gemini evaluation tactics.", "result": "GPT-4o-mini showcases consistency, GPT-4o excels at error detection, and GPT-5 is highly variable and conservative. All GPT models demonstrate a 2:1 bias favoring negative over positive assessments, but this pattern is family-specific.", "conclusion": "Robust AI evaluation requires diverse perspectives from different AI architectures, as evaluation competence does not scale directly with general AI capability."}}
{"id": "2509.10694", "pdf": "https://arxiv.org/pdf/2509.10694", "abs": "https://arxiv.org/abs/2509.10694", "authors": ["Kahfi S. Zulkifli", "Wenbo Qian", "Shaowei Zhu", "Yuan Zhou", "Zhen Zhang", "Chang Lou"], "title": "Verifying Computational Graphs in Production-Grade Distributed Machine Learning Frameworks", "categories": ["cs.LG", "cs.PL"], "comment": null, "summary": "Modern machine learning frameworks support very large models by incorporating\nparallelism and optimization techniques. Yet, these very techniques add new\nlayers of complexity, introducing silent errors that severely degrade model\nperformance. Existing solutions are either ad hoc or too costly for production.\n  We present Scalify, a lightweight framework that exposes silent errors by\nverifying semantic equivalence of computational graphs using equality\nsaturation and Datalog-style reasoning. To scale, Scalify partitions graphs\nwith parallel rewriting and layer memoization, reuses rewrite templates, and\naugments equality saturation with relational reasoning and symbolic bijection\ninference. It further localizes discrepancies to precise code sites, turning\nverification results into actionable debugging guidance. Scalify verifies\nmodels as large as Llama-3.1-405B within minutes on a commodity machine and\nexposed five unknown bugs in Amazon production machine learning frameworks.", "AI": {"tldr": "Scalify is a lightweight framework that checks the semantic equivalence of large machine learning models, using advanced techniques to detect silent errors and improve debugging.", "motivation": "Modern machine learning techniques often introduce silent errors in very large models due to parallelism and optimization, and existing solutions to address this are either ineffective or impractical.", "method": "Scalify uses equality saturation and Datalog-style reasoning to verify computational graphs, scales through graph partitioning and parallel rewriting, and converts debugging results into actionable insights.", "result": "The framework successfully verified massive models like Llama-3.1-405B within minutes and identified five unknown bugs in production frameworks at Amazon.", "conclusion": "Scalify provides an effective, scalable solution for debugging and verifying large machine learning models, enabling faster and more reliable production readiness."}}
{"id": "2509.10500", "pdf": "https://arxiv.org/pdf/2509.10500", "abs": "https://arxiv.org/abs/2509.10500", "authors": ["Etienne Russeil", "Fabr\u00edcio Olivetti de Fran\u00e7a", "Konstantin Malanchev", "Guillaume Moinard", "Maxime Cherrey"], "title": "Exploring Multi-view Symbolic Regression methods in physical sciences", "categories": ["cs.LG", "astro-ph.GA", "astro-ph.IM", "physics.data-an"], "comment": "15 pages, 7 figures. Presented at the \"Symbolic regression in the\n  physical sciences\" conference at the Royal Society. Submitted to\n  Philosophical Transactions A", "summary": "Describing the world behavior through mathematical functions help scientists\nto achieve a better understanding of the inner mechanisms of different\nphenomena. Traditionally, this is done by deriving new equations from first\nprinciples and careful observations. A modern alternative is to automate part\nof this process with symbolic regression (SR). The SR algorithms search for a\nfunction that adequately fits the observed data while trying to enforce\nsparsity, in the hopes of generating an interpretable equation. A particularly\ninteresting extension to these algorithms is the Multi-view Symbolic Regression\n(MvSR). It searches for a parametric function capable of describing multiple\ndatasets generated by the same phenomena, which helps to mitigate the common\nproblems of overfitting and data scarcity. Recently, multiple implementations\nadded support to MvSR with small differences between them. In this paper, we\ntest and compare MvSR as supported in Operon, PySR, phy-SO, and eggp, in\ndifferent real-world datasets. We show that they all often achieve good\naccuracy while proposing solutions with only few free parameters. However, we\nfind that certain features enable a more frequent generation of better models.\nWe conclude by providing guidelines for future MvSR developments.", "AI": {"tldr": "This paper evaluates and compares Multi-view Symbolic Regression (MvSR) implementations in Operon, PySR, phy-SO, and eggp across real-world datasets, highlighting accuracy and sparseness while noting features leading to better models.", "motivation": "The paper aims to extend the understanding of MvSR approaches, which mitigate data scarcity and overfitting by finding parametric functions for multiple datasets, analyzing their efficacy for symbolic regression.", "method": "The study tests and compares MvSR implementations across different tools, utilizing real-world datasets to evaluate their accuracy, simplicity (fewer free parameters), and influences of their features.", "result": "All tested implementations (Operon, PySR, phy-SO, and eggp) perform well with good accuracy and sparse (interpretable) solutions. Certain features are identified that generate better models more frequently.", "conclusion": "The paper provides actionable guidelines for improving MvSR developments based on observed behaviors and comparative findings across implementations."}}
{"id": "2509.10735", "pdf": "https://arxiv.org/pdf/2509.10735", "abs": "https://arxiv.org/abs/2509.10735", "authors": ["Mohammad Rafiee Javazm", "Yash Kulkarni", "Jiaqi Xue", "Naruhiko Ikoma", "Farshid Alambeigi"], "title": "Analytical Design and Development of a Modular and Intuitive Framework for Robotizing and Enhancing the Existing Endoscopic Procedures", "categories": ["cs.RO"], "comment": null, "summary": "Despite the widespread adoption of endoscopic devices for several cancer\nscreening procedures, manual control of these devices still remains challenging\nfor clinicians, leading to several critical issues such as increased workload,\nfatigue, and distractions. To address these issues, in this paper, we introduce\nthe design and development of an intuitive, modular, and easily installable\nmechatronic framework. This framework includes (i) a novel nested collet-chuck\ngripping mechanism that can readily be integrated and assembled with the\nexisting endoscopic devices and control their bending degrees-of-freedom\n(DoFs); (ii) a feeder mechanism that can control the insertion/retraction DoF\nof a colonoscope, and (iii) a complementary and intuitive user interface that\nenables simultaneous control of all DoFs during the procedure. To analyze the\ndesign of the proposed mechanisms, we also introduce a mathematical modeling\napproach and a design space for optimal selection of the parameters involved in\nthe design of gripping and feeder mechanisms. Our simulation and experimental\nstudies thoroughly demonstrate the performance of the proposed mathematical\nmodeling and robotic framework.", "AI": {"tldr": "This paper presents a mechatronic framework to ease the operation of endoscopic devices, featuring advanced gripping and feeder mechanisms along with a user-friendly interface.", "motivation": "Manual control of endoscopic devices is challenging for clinicians and leads to issues such as fatigue, distractions, and increased workload.", "method": "The paper introduces a modular framework consisting of a nested collet-chuck mechanism, a feeder mechanism for insertion/retraction, and an intuitive user interface. Mathematical modeling was applied for design analysis.", "result": "The framework's performance and mathematical modeling were validated through simulation and experimental studies, demonstrating effectiveness.", "conclusion": "The mechatronic framework simplifies endoscope control, reduces clinician challenges, and can be readily integrated into existing devices."}}
{"id": "2509.10663", "pdf": "https://arxiv.org/pdf/2509.10663", "abs": "https://arxiv.org/abs/2509.10663", "authors": ["Zineddine Tighidet", "Andrea Mogini", "Hedi Ben-younes", "Jiali Mei", "Patrick Gallinari", "Benjamin Piwowarski"], "title": "Context Copying Modulation: The Role of Entropy Neurons in Managing Parametric and Contextual Knowledge Conflicts", "categories": ["cs.CL"], "comment": "Accepted at EMNLP 2025", "summary": "The behavior of Large Language Models (LLMs) when facing contextual\ninformation that conflicts with their internal parametric knowledge is\ninconsistent, with no generally accepted explanation for the expected outcome\ndistribution. Recent work has identified in autoregressive transformer models a\nclass of neurons -- called entropy neurons -- that produce a significant effect\non the model output entropy while having an overall moderate impact on the\nranking of the predicted tokens. In this paper, we investigate the preliminary\nclaim that these neurons are involved in inhibiting context copying behavior in\ntransformers by looking at their role in resolving conflicts between contextual\nand parametric information. We show that entropy neurons are responsible for\nsuppressing context copying across a range of LLMs, and that ablating them\nleads to a significant change in the generation process. These results enhance\nour understanding of the internal dynamics of LLMs when handling conflicting\ninformation.", "AI": {"tldr": "This paper identifies 'entropy neurons' in autoregressive transformer models that suppress context copying when contextual and parametric information conflict, thereby affecting generation outcomes.", "motivation": "The paper aims to understand why Large Language Models (LLMs) handle conflicting information inconsistently and explore the mechanism behind context suppression.", "method": "The study investigates the role of entropy neurons by analyzing their influence on conflict resolution and generation processes in multiple LLMs, including methods such as controlled ablation.", "result": "Entropy neurons are shown to suppress context copying behavior across multiple LLMs, and their removal significantly alters the model's generation process.", "conclusion": "The findings deepen insights into how LLMs resolve contextual versus parametric conflicts, linking suppression of copying behavior to specific internal mechanisms."}}
{"id": "2509.10920", "pdf": "https://arxiv.org/pdf/2509.10920", "abs": "https://arxiv.org/abs/2509.10920", "authors": ["Guan-Yan Yang", "Farn Wang", "You-Zong Gu", "Ya-Wen Teng", "Kuo-Hui Yeh", "Ping-Hsueh Ho", "Wei-Ling Wen"], "title": "TPSQLi: Test Prioritization for SQL Injection Vulnerability Detection in Web Applications", "categories": ["cs.SE", "cs.CR"], "comment": "20 pages; 8 figures", "summary": "The rapid proliferation of network applications has led to a significant\nincrease in network attacks. According to the OWASP Top 10 Projects report\nreleased in 2021, injection attacks rank among the top three vulnerabilities in\nsoftware projects. This growing threat landscape has increased the complexity\nand workload of software testing, necessitating advanced tools to support agile\ndevelopment cycles. This paper introduces a novel test prioritization method\nfor SQL injection vulnerabilities to enhance testing efficiency. By leveraging\nprevious test outcomes, our method adjusts defense strength vectors for\nsubsequent tests, optimizing the testing workflow and tailoring defense\nmechanisms to specific software needs. This approach aims to improve the\neffectiveness and efficiency of vulnerability detection and mitigation through\na flexible framework that incorporates dynamic adjustments and considers the\ntemporal aspects of vulnerability exposure.", "AI": {"tldr": "The paper addresses injection attacks by proposing a dynamic test prioritization method for SQL injection vulnerabilities, optimizing software testing for better efficiency.", "motivation": "Injection attacks, particularly SQL injection, are a top concern in software security, and there is a need for advanced tools to manage increasing test complexity in agile development.", "method": "The method dynamically adjusts defense strength vectors for SQL injection testing, leveraging results from prior tests to optimize the subsequent test workflows.", "result": "The approach enhances the efficiency of vulnerability detection and mitigation by tailoring testing mechanisms to software-specific needs.", "conclusion": "The dynamic and temporal adjustment framework allows more efficient and effective management of SQL injection risks."}}
{"id": "2509.10557", "pdf": "https://arxiv.org/pdf/2509.10557", "abs": "https://arxiv.org/abs/2509.10557", "authors": ["Atefeh Irani", "Maryam S. Mirian", "Alex Lassooij", "Reshad Hosseini", "Hadi Moradi", "Martin J. McKeown"], "title": "HiLWS: A Human-in-the-Loop Weak Supervision Framework for Curating Clinical and Home Video Data for Neurological Assessment", "categories": ["q-bio.NC", "cs.LG", "I.2.1"], "comment": null, "summary": "Video-based assessment of motor symptoms in conditions such as Parkinson's\ndisease (PD) offers a scalable alternative to in-clinic evaluations, but\nhome-recorded videos introduce significant challenges, including visual\ndegradation, inconsistent task execution, annotation noise, and domain shifts.\nWe present HiLWS, a cascaded human-in-the-loop weak supervision framework for\ncurating and annotating hand motor task videos from both clinical and home\nsettings. Unlike conventional single-stage weak supervision methods, HiLWS\nemploys a novel cascaded approach, first applies weak supervision to aggregate\nexpert-provided annotations into probabilistic labels, which are then used to\ntrain machine learning models. Model predictions, combined with expert input,\nare subsequently refined through a second stage of weak supervision. The\ncomplete pipeline includes quality filtering, optimized pose estimation, and\ntask-specific segment extraction, complemented by context-sensitive evaluation\nmetrics that assess both visual fidelity and clinical relevance by prioritizing\nambiguous cases for expert review. Our findings reveal key failure modes in\nhome recorded data and emphasize the importance of context-sensitive curation\nstrategies for robust medical video analysis.", "AI": {"tldr": "This paper introduces HiLWS, a novel human-in-the-loop weak supervision framework for curating and analyzing home-recorded videos for motor symptom assessment in Parkinson's Disease (PD).", "motivation": "To address the challenges of using home-recorded videos (e.g. visual degradation, inconsistent execution, and annotation noise) for assessing motor symptoms in conditions like Parkinson's Disease, where scalable alternatives to in-clinic evaluations are needed.", "method": "HiLWS applies a two-stage human-in-the-loop weak supervision approach. It first aggregates expert-provided annotations into probabilistic labels for training models. Then, it refines model predictions with expert input in a second round of supervision. The pipeline also incorporates quality filtering, optimized pose estimation, and task segmentation with context-aware evaluation metrics.", "result": "HiLWS identifies failure modes in home-recorded data, shows the importance of context-sensitive curation, and demonstrates the effectiveness of the framework in improving medical video analysis and clinical relevance.", "conclusion": "HiLWS provides a scalable, robust solution to address challenges with home-recorded video data for motor symptom assessment, highlighting the critical role of context-sensitive strategies and expert involvement."}}
{"id": "2509.10651", "pdf": "https://arxiv.org/pdf/2509.10651", "abs": "https://arxiv.org/abs/2509.10651", "authors": ["Xiaoyang Ma", "Yiyang Chai", "Xinran Qu", "Hong Sun"], "title": "USCTNet: A deep unfolding nuclear-norm optimization solver for physically consistent HSI reconstruction", "categories": ["cs.CV"], "comment": null, "summary": "Reconstructing hyperspectral images (HSIs) from a single RGB image is\nill-posed and can become physically inconsistent when the camera spectral\nsensitivity (CSS) and scene illumination are misspecified. We formulate\nRGB-to-HSI reconstruction as a physics-grounded inverse problem regularized by\na nuclear norm in a learnable transform domain, and we explicitly estimate CSS\nand illumination to define the forward operator embedded in each iteration,\nensuring colorimetric consistency. To avoid the cost and instability of full\nsingular-value decompositions (SVDs) required by singular-value thresholding\n(SVT), we introduce a data-adaptive low-rank subspace SVT operator. Building on\nthese components, we develop USCTNet, a deep unfolding solver tailored to HSI\nthat couples a parameter estimation module with learnable proximal updates.\nExtensive experiments on standard benchmarks show consistent improvements over\nstate-of-the-art RGB-based methods in reconstruction accuracy. Code:\nhttps://github.com/psykheXX/USCTNet-Code-Implementation.git", "AI": {"tldr": "The paper introduces USCTNet, a physics-inspired deep learning method to reconstruct hyperspectral images (HSIs) from single RGB images by estimating camera spectral sensitivity (CSS) and illumination for consistency, and optimizing with a low-rank subspace singular-value thresholding operator.", "motivation": "The motivation is to address the ill-posed nature of HSI reconstruction from RGB images, improving reconstruction accuracy while ensuring physical and colorimetric consistency.", "method": "The method formulates HSI reconstruction as a physics-grounded inverse problem, regularized by a nuclear norm in a learnable transform domain. The forward operator is adaptively defined by estimating CSS and illumination, while a low-rank subspace SVT operator replaces traditional computationally expensive full SVDs. USCTNet integrates these ideas into a deep unfolding framework with learnable proximal updates and a parameter estimation module.", "result": "The proposed USCTNet demonstrates consistent improvements in reconstruction accuracy over state-of-the-art RGB-based HSI reconstruction methods in extensive benchmark experiments.", "conclusion": "The approach enhances HSI reconstruction by addressing physical inconsistencies and computational inefficiencies, presenting a significant step forward in RGB-to-HSI methods."}}
{"id": "2509.11503", "pdf": "https://arxiv.org/pdf/2509.11503", "abs": "https://arxiv.org/abs/2509.11503", "authors": ["Rishab Parthasarathy", "Akshay Attaluri", "Gilford Ting"], "title": "always_comm: An FPGA-based Hardware Accelerator for Audio/Video Compression and Transmission", "categories": ["cs.AR"], "comment": "8 pages, 8 figures, 1 table, equal contribution", "summary": "We present a design for an extensible video conferencing stack implemented\nentirely in hardware on a Nexys4 DDR FPGA, which uses the M-JPEG codec to\ncompress video and a UDP networking stack to communicate between the FPGA and\nthe receiving computer. This networking stack accepts real-time updates from\nboth the video codec and the audio controller, which means that video will be\nable to be streamed at 30 FPS from the FPGA to a computer. On the computer\nside, a Python script reads the Ethernet packets and decodes the packets into\nthe video and the audio for real time playback. We evaluate this architecture\nusing both functional, simulation-driven verification in Cocotb and by\nsynthesizing SystemVerilog RTL code using Vivado for deployment on our Nexys4\nDDR FPGA, where we evaluate both end-to-end latency and throughput of video\ntransmission.", "AI": {"tldr": "The paper discusses a hardware-based video conferencing system utilizing an FPGA, supporting real-time video streaming at 30 FPS and end-to-end verification.", "motivation": "To develop an efficient, real-time hardware video conferencing system implemented on FPGA with high throughput and low latency.", "method": "Designed and implemented a stack using M-JPEG codec for compression, a UDP networking stack, and an FPGA for real-time video streaming; evaluated using functional verification and hardware synthesis.", "result": "Demonstrated video streaming at 30 FPS, performed end-to-end latency and throughput experiments with successful results on the FPGA.", "conclusion": "The proposed architecture is feasible for real-time hardware-based video conferencing, showing good performance in terms of latency and throughput."}}
{"id": "2509.11208", "pdf": "https://arxiv.org/pdf/2509.11208", "abs": "https://arxiv.org/abs/2509.11208", "authors": ["Leon Chlon", "Ahmed Karim", "Maggie Chlon"], "title": "Predictable Compression Failures: Why Language Models Actually Hallucinate", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Large language models perform near-Bayesian inference yet violate permutation\ninvariance on exchangeable data. We resolve this by showing transformers\nminimize expected conditional description length (cross-entropy) over\norderings, $\\mathbb{E}_\\pi[\\ell(Y \\mid \\Gamma_\\pi(X))]$, which admits a\nKolmogorov-complexity interpretation up to additive constants, rather than the\npermutation-invariant description length $\\ell(Y \\mid X)$. This makes them\nBayesian in expectation, not in realization. We derive (i) a Quantified\nMartingale Violation bound showing order-induced deviations scale as $O(\\log\nn)$ with constants; (ii) the Expectation-level Decompression Law linking\ninformation budgets to reliability for Bernoulli predicates; and (iii)\ndeployable planners (B2T/RoH/ISR) for answer/abstain decisions. Empirically,\npermutation dispersion follows $a+b\\ln n$ (Qwen2-7B $b \\approx 0.377$,\nLlama-3.1-8B $b \\approx 0.147$); permutation mixtures improve ground-truth\nlikelihood/accuracy; and randomized dose-response shows hallucinations drop by\n$\\sim 0.13$ per additional nat. A pre-specified audit with a fixed ISR=1.0\nachieves near-0\\% hallucinations via calibrated refusal at 24\\% abstention. The\nframework turns hallucinations into predictable compression failures and\nenables principled information budgeting.", "AI": {"tldr": "The paper explores how large language models (LLMs) lack permutation invariance despite Bayesian inference capabilities and proposes a solution using transformers that minimize expected conditional description length.", "motivation": "The study aims to address the issue that LLMs, while performing near-Bayesian inference, fail to maintain permutation invariance on exchangeable data, which is a critical aspect for reliability.", "method": "The authors propose that transformers minimize expected conditional description length over data orderings and provide a new theoretical framework including quantified martingale violation bounds, an Expectation-level Decompression Law, and deployable decision planners.", "result": "Key findings include a $\\log n$ scaling law for order deviations, improved model reliability via permutation mixtures, and significant reduction in hallucinations in pre-specified audits.", "conclusion": "The paper identifies and addresses permutation-related deficiencies in LLMs, turning hallucination issues into predictable compression errors and introducing a principled framework for better information management."}}
{"id": "2509.11156", "pdf": "https://arxiv.org/pdf/2509.11156", "abs": "https://arxiv.org/abs/2509.11156", "authors": ["Suvarthi Sarkar", "Aadarshraj Sah", "Poddutoori Sweeya Reddy", "Aryabartta Sahu"], "title": "Adaptive K-PackCache: Cost-Centric Data Caching in Cloud", "categories": ["cs.DC"], "comment": null, "summary": "Recent advances in data analytics have enabled the accurate prediction of\nuser access patterns, giving rise to the idea of packed caching delivering\nmultiple co accessed data items together as a bundle. This improves caching\nefficiency, as accessing one item often implies the need for others. Prior work\nhas explored only 2 item pairwise packing. In this paper, we extend the concept\nto general K packing, allowing variable size bundles for improved flexibility\nand performance. We formulate the K PackCache problem from a content delivery\nnetwork CDN operator perspective, aiming to minimize total cost comprising two\ncomponents: transfer cost modeled as a base cost plus a linearly increasing\nterm with the number of items packed, and memory rental cost for caching, which\ndepends on how long and how much is stored. Overpacking increases cost due to\nlow utility, underpacking leads to missed sharing opportunities. We propose an\nonline algorithm, Adaptive K PackCache AKPC, which dynamically forms, merges,\nand splits data cliques based on user access patterns and content correlation.\nOur approach supports batch requests, enables approximate clique merging, and\noffers a formal competitive guarantee. Through extensive evaluation on the\nNetflix and Spotify datasets, AKPC reduces total cost by up to 63 and 55\npercentage over online baselines, respectively, and achieves performance within\n15 and 13 percentage of the optimal. This demonstrates its scalability and\neffectiveness for real world caching systems.", "AI": {"tldr": "The paper introduces a general K packing for caching data efficiently in content delivery networks (CDNs), using a proposed online algorithm, Adaptive K PackCache (AKPC).", "motivation": "To address the inefficiencies of pairwise caching in CDNs by enabling variable size data bundles that minimize costs while maximizing access efficiency.", "method": "The problem, termed K PackCache, is modeled to optimize transfer and memory rental costs. AKPC is proposed as an online algorithm that dynamically adjusts data bundling based on access patterns and content correlation. It employs clique merging and supports batch processing.", "result": "The AKPC algorithm achieved cost reductions of up to 63% and 55% on Netflix and Spotify data, respectively, compared to online baselines. Its results were within 15% and 13% of the theoretical optimal performance.", "conclusion": "The AKPC method is scalable and effective for real-world caching systems, providing significant cost savings and competitive performance in managing data access patterns."}}
{"id": "2509.10762", "pdf": "https://arxiv.org/pdf/2509.10762", "abs": "https://arxiv.org/abs/2509.10762", "authors": ["Arlen Kumar", "Leanid Palkhouski"], "title": "AI Answer Engine Citation Behavior An Empirical Analysis of the GEO16 Framework", "categories": ["cs.AI"], "comment": null, "summary": "AI answer engines increasingly mediate access to domain knowledge by\ngenerating responses and citing web sources. We introduce GEO-16, a 16 pillar\nauditing framework that converts on page quality signals into banded pillar\nscores and a normalized GEO score G that ranges from 0 to 1. Using 70 product\nintent prompts, we collected 1,702 citations across three engines (Brave\nSummary, Google AI Overviews, and Perplexity) and audited 1,100 unique URLs. In\nour corpus, the engines differed in the GEO quality of the pages they cited,\nand pillars related to Metadata and Freshness, Semantic HTML, and Structured\nData showed the strongest associations with citation. Logistic models with\ndomain clustered standard errors indicate that overall page quality is a strong\npredictor of citation, and simple operating points (for example, G at least\n0.70 combined with at least 12 pillar hits) align with substantially higher\ncitation rates in our data. We report per engine contrasts, vertical effects,\nthreshold analysis, and diagnostics, then translate findings into a practical\nplaybook for publishers. The study is observational and focuses on English\nlanguage B2B SaaS pages; we discuss limitations, threats to validity, and\nreproducibility considerations.", "AI": {"tldr": "The paper presents GEO-16, a framework to audit citation quality in AI answer engines. It evaluates page features and provides a scoring system to predict citation likelihood.", "motivation": "There is a growing need to understand how AI answer engines select and cite web sources when generating responses for users.", "method": "The authors introduced a 16-pillar auditing framework called GEO-16, collected 1,702 citations from three AI engines using 70 prompts, and assessed quality metrics on 1,100 unique URLs.", "result": "The study found differences among engines in citation page quality. Logistic models showed strong correlations between citation likelihood and high GEO scores, along with specific page quality metrics like metadata.", "conclusion": "Findings suggest ways publishers can optimize for higher citations by focusing on page quality attributes. The study provides a practical playbook and highlights its scope, limitations, and reproducibility."}}
{"id": "2509.10501", "pdf": "https://arxiv.org/pdf/2509.10501", "abs": "https://arxiv.org/abs/2509.10501", "authors": ["Wentao Gao", "Jiuyong Li", "Lin Liu", "Thuc Duy Le", "Xiongren Chen", "Xiaojing Du", "Jixue Liu", "Yanchang Zhao", "Yun Chen"], "title": "From Noise to Precision: A Diffusion-Driven Approach to Zero-Inflated Precipitation Prediction", "categories": ["cs.LG", "cs.AI"], "comment": "ECAI 2025 Accepted", "summary": "Zero-inflated data pose significant challenges in precipitation forecasting\ndue to the predominance of zeros with sparse non-zero events. To address this,\nwe propose the Zero Inflation Diffusion Framework (ZIDF), which integrates\nGaussian perturbation for smoothing zero-inflated distributions,\nTransformer-based prediction for capturing temporal patterns, and\ndiffusion-based denoising to restore the original data structure. In our\nexperiments, we use observational precipitation data collected from South\nAustralia along with synthetically generated zero-inflated data. Results show\nthat ZIDF demonstrates significant performance improvements over multiple\nstate-of-the-art precipitation forecasting models, achieving up to 56.7\\%\nreduction in MSE and 21.1\\% reduction in MAE relative to the baseline\nNon-stationary Transformer. These findings highlight ZIDF's ability to robustly\nhandle sparse time series data and suggest its potential generalizability to\nother domains where zero inflation is a key challenge.", "AI": {"tldr": "The paper introduces the Zero Inflation Diffusion Framework (ZIDF) to tackle zero-inflated data challenges in precipitation forecasting.", "motivation": "The motivation is to address challenges in precipitation forecasting caused by datasets with a high frequency of zero values and sparse non-zero occurrences.", "method": "The proposed Zero Inflation Diffusion Framework (ZIDF) combines Gaussian perturbation for data smoothing, Transformer models for understanding temporal patterns, and diffusion mechanisms for restoring original data structures.", "result": "The ZIDF outperforms existing models, reducing Mean Squared Error (MSE) by up to 56.7% and Mean Absolute Error (MAE) by 21.1% compared to a baseline model.", "conclusion": "ZIDF demonstrates robustness in handling sparse data and shows potential applicability to similar domains facing zero-inflated data challenges."}}
{"id": "2509.10757", "pdf": "https://arxiv.org/pdf/2509.10757", "abs": "https://arxiv.org/abs/2509.10757", "authors": ["Kimia Khabiri", "Parsa Hosseininejad", "Shishir Gopinath", "Karthik Dantu", "Steven Y. Ko"], "title": "FastTrack: GPU-Accelerated Tracking for Visual SLAM", "categories": ["cs.RO", "cs.DC"], "comment": "Accepted for presentation at IROS 2025, preprint", "summary": "The tracking module of a visual-inertial SLAM system processes incoming image\nframes and IMU data to estimate the position of the frame in relation to the\nmap. It is important for the tracking to complete in a timely manner for each\nframe to avoid poor localization or tracking loss. We therefore present a new\napproach which leverages GPU computing power to accelerate time-consuming\ncomponents of tracking in order to improve its performance. These components\ninclude stereo feature matching and local map tracking. We implement our design\ninside the ORB-SLAM3 tracking process using CUDA. Our evaluation demonstrates\nan overall improvement in tracking performance of up to 2.8x on a desktop and\nJetson Xavier NX board in stereo-inertial mode, using the well-known SLAM\ndatasets EuRoC and TUM-VI.", "AI": {"tldr": "This paper introduces a GPU-accelerated approach to enhance the tracking performance in visual-inertial SLAM systems, achieving up to 2.8x improvement.", "motivation": "SLAM systems require timely processing of image and IMU data for accurate frame localization. Slow processing could result in poor localization or tracking loss.", "method": "The researchers leveraged GPU computing, specifically CUDA, to speed up key tracking components like stereo feature matching and local map tracking within the ORB-SLAM3 framework.", "result": "The GPU-based enhancements improved tracking performance by up to 2.8x on both desktop systems and Jetson Xavier NX in stereo-inertial mode, as validated on SLAM datasets EuRoC and TUM-VI.", "conclusion": "GPU acceleration significantly boosts tracking efficiency in visual-inertial SLAM systems, making them more reliable for real-world applications."}}
{"id": "2509.10685", "pdf": "https://arxiv.org/pdf/2509.10685", "abs": "https://arxiv.org/abs/2509.10685", "authors": ["Jiayou Zhong", "Anudeex Shetty", "Chao Jia", "Xuanrui Lin", "Usman Naseem"], "title": "Pluralistic Alignment for Healthcare: A Role-Driven Framework", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted to EMNLP 2025 (Main Proceedings)", "summary": "As large language models are increasingly deployed in sensitive domains such\nas healthcare, ensuring their outputs reflect the diverse values and\nperspectives held across populations is critical. However, existing alignment\napproaches, including pluralistic paradigms like Modular Pluralism, often fall\nshort in the health domain, where personal, cultural, and situational factors\nshape pluralism. Motivated by the aforementioned healthcare challenges, we\npropose a first lightweight, generalizable, pluralistic alignment approach,\nEthosAgents, designed to simulate diverse perspectives and values. We\nempirically show that it advances the pluralistic alignment for all three modes\nacross seven varying-sized open and closed models. Our findings reveal that\nhealth-related pluralism demands adaptable and normatively aware approaches,\noffering insights into how these models can better respect diversity in other\nhigh-stakes domains.", "AI": {"tldr": "The paper proposes EthosAgents, a pluralistic alignment framework for large language models, to better respect diverse values in healthcare and other sensitive domains.", "motivation": "Existing alignment methods fail to address healthcare-specific pluralistic needs shaped by personal, cultural, and situational factors.", "method": "Developing EthosAgents, a lightweight, generalizable, pluralistic alignment approach that simulates diverse perspectives and values.", "result": "Empirical evidence demonstrates that EthosAgents improves pluralistic alignment across diverse health-related scenarios for various models.", "conclusion": "EthosAgents provides a path to improve diversity-respecting outputs in healthcare and high-stakes domains, highlighting the need for adaptable and normatively aware AI systems."}}
{"id": "2509.10946", "pdf": "https://arxiv.org/pdf/2509.10946", "abs": "https://arxiv.org/abs/2509.10946", "authors": ["Roberto Morabito", "Guanghan Wu"], "title": "When the Code Autopilot Breaks: Why LLMs Falter in Embedded Machine Learning", "categories": ["cs.SE", "cs.AI"], "comment": "This paper has been accepted for publication in Computer (IEEE). Upon\n  publication, the copyright will be transferred to IEEE", "summary": "Large Language Models (LLMs) are increasingly used to automate software\ngeneration in embedded machine learning workflows, yet their outputs often fail\nsilently or behave unpredictably. This article presents an empirical\ninvestigation of failure modes in LLM-powered ML pipelines, based on an\nautopilot framework that orchestrates data preprocessing, model conversion, and\non-device inference code generation. We show how prompt format, model behavior,\nand structural assumptions influence both success rates and failure\ncharacteristics, often in ways that standard validation pipelines fail to\ndetect. Our analysis reveals a diverse set of error-prone behaviors, including\nformat-induced misinterpretations and runtime-disruptive code that compiles but\nbreaks downstream. We derive a taxonomy of failure categories and analyze\nerrors across multiple LLMs, highlighting common root causes and systemic\nfragilities. Though grounded in specific devices, our study reveals broader\nchallenges in LLM-based code generation. We conclude by discussing directions\nfor improving reliability and traceability in LLM-powered embedded ML systems.", "AI": {"tldr": "The paper explores failure modes in LLM-powered embedded ML pipelines and proposes a taxonomy of errors, highlighting systemic fragilities and suggesting improvements.", "motivation": "To investigate and understand the diverse and unpredictable failure modes of LLM-generated code in embedded ML workflows, which are not easily caught by standard validation techniques.", "method": "An empirical investigation was conducted using an autopilot framework, analyzing the effects of prompt formats, model behaviors, and structural assumptions on failure modes across multiple LLMs.", "result": "The study identified various error-prone behaviors, created a taxonomy of failure categories, and uncovered common root causes that affect the success and reliability of LLM-based workflows.", "conclusion": "The paper emphasizes the need for improved tools and techniques to enhance reliability and traceability in LLM-powered embedded ML systems."}}
{"id": "2509.10650", "pdf": "https://arxiv.org/pdf/2509.10650", "abs": "https://arxiv.org/abs/2509.10650", "authors": ["Nicol\u00e1s Hinrichs", "Noah Guzm\u00e1n", "Melanie Weber"], "title": "On a Geometry of Interbrain Networks", "categories": ["q-bio.NC", "cs.LG"], "comment": "4 pages, 1 figure, submitted to NeurReps workshop 2025", "summary": "Effective analysis in neuroscience benefits significantly from robust\nconceptual frameworks. Traditional metrics of interbrain synchrony in social\nneuroscience typically depend on fixed, correlation-based approaches,\nrestricting their explanatory capacity to descriptive observations. Inspired by\nthe successful integration of geometric insights in network science, we propose\nleveraging discrete geometry to examine the dynamic reconfigurations in neural\ninteractions during social exchanges. Unlike conventional synchrony approaches,\nour method interprets inter-brain connectivity changes through the evolving\ngeometric structures of neural networks. This geometric framework is realized\nthrough a pipeline that identifies critical transitions in network connectivity\nusing entropy metrics derived from curvature distributions. By doing so, we\nsignificantly enhance the capacity of hyperscanning methodologies to uncover\nunderlying neural mechanisms in interactive social behavior.", "AI": {"tldr": "The paper proposes a novel geometric framework for analyzing inter-brain synchrony in social neuroscience using curvature-based entropy.", "motivation": "Current approaches to studying inter-brain synchrony are limited to correlation-based methods, which lack explanatory depth for dynamic social interactions. The study is motivated by the success of geometric methods in network science.", "method": "A discrete geometry framework is introduced to study neural networks' evolving structures. It applies entropy metrics from curvature distributions to detect key transitions in inter-brain connectivity during social interactions.", "result": "The method enhances hyperscanning methodologies by providing deeper insights into the neural mechanisms governing social interactions.", "conclusion": "The proposed geometric approach offers a robust and innovative avenue for advancing the analysis of neural interactions in social neuroscience."}}
{"id": "2509.10683", "pdf": "https://arxiv.org/pdf/2509.10683", "abs": "https://arxiv.org/abs/2509.10683", "authors": ["Felicia Liu", "Jay J. Yoo", "Farzad Khalvati"], "title": "A Comparison and Evaluation of Fine-tuned Convolutional Neural Networks to Large Language Models for Image Classification and Segmentation of Brain Tumors on MRI", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have shown strong performance in text-based\nhealthcare tasks. However, their utility in image-based applications remains\nunexplored. We investigate the effectiveness of LLMs for medical imaging tasks,\nspecifically glioma classification and segmentation, and compare their\nperformance to that of traditional convolutional neural networks (CNNs). Using\nthe BraTS 2020 dataset of multi-modal brain MRIs, we evaluated a\ngeneral-purpose vision-language LLM (LLaMA 3.2 Instruct) both before and after\nfine-tuning, and benchmarked its performance against custom 3D CNNs. For glioma\nclassification (Low-Grade vs. High-Grade), the CNN achieved 80% accuracy and\nbalanced precision and recall. The general LLM reached 76% accuracy but\nsuffered from a specificity of only 18%, often misclassifying Low-Grade tumors.\nFine-tuning improved specificity to 55%, but overall performance declined\n(e.g., accuracy dropped to 72%). For segmentation, three methods - center\npoint, bounding box, and polygon extraction, were implemented. CNNs accurately\nlocalized gliomas, though small tumors were sometimes missed. In contrast, LLMs\nconsistently clustered predictions near the image center, with no distinction\nof glioma size, location, or placement. Fine-tuning improved output formatting\nbut failed to meaningfully enhance spatial accuracy. The bounding polygon\nmethod yielded random, unstructured outputs. Overall, CNNs outperformed LLMs in\nboth tasks. LLMs showed limited spatial understanding and minimal improvement\nfrom fine-tuning, indicating that, in their current form, they are not\nwell-suited for image-based tasks. More rigorous fine-tuning or alternative\ntraining strategies may be needed for LLMs to achieve better performance,\nrobustness, and utility in the medical space.", "AI": {"tldr": "The paper assesses the capability of Large Language Models (LLMs) for glioma classification and segmentation using medical imaging, comparing them with CNNs. CNNs outperformed LLMs on both tasks, indicating LLMs' current inefficacy in image-based applications.", "motivation": "To explore the unexplored potential and effectiveness of LLMs for medical imaging tasks, as they have shown strong text-based healthcare performance.", "method": "Tested a vision-language LLM (LLaMA 3.2 Instruct) and custom 3D CNNs on the BraTS 2020 brain MRI dataset for glioma classification and segmentation. Evaluated the models' accuracy, precision, recall, and spatial understanding before and after fine-tuning.", "result": "CNNs achieved 80% accuracy in classification, while LLMs reached 76% but struggled with low specificity. Fine-tuning mildly improved LLM performance but reduced overall accuracy. For segmentation, CNNs localized gliomas effectively, but LLMs showed severe spatial limitations, even after fine-tuning.", "conclusion": "Current LLMs lack the spatial comprehension and robustness required for medical imaging tasks like glioma classification and segmentation. Rigorous fine-tuning or different training strategies are required for their improvement in image-based applications."}}
{"id": "2509.11529", "pdf": "https://arxiv.org/pdf/2509.11529", "abs": "https://arxiv.org/abs/2509.11529", "authors": ["Rishab Parthasarathy"], "title": "SuperUROP: An FPGA-Based Spatial Accelerator for Sparse Matrix Operations", "categories": ["cs.AR"], "comment": "7 pages, 6 figures, work done as the Citadel Undergraduate Research\n  Scholar in the MIT SuperUROP program", "summary": "Solving sparse systems of linear equations is a fundamental problem in the\nfield of numerical methods, with applications spanning from circuit design to\nurban planning. These problems can have millions of constraints, such as when\nlaying out transistors on a circuit, or trying to optimize traffic light\ntimings, making fast sparse solvers extremely important. However, existing\nstate-of-the-art software-level solutions for solving sparse linear systems,\ntermed iterative solvers, are extremely inefficient on current hardware. This\ninefficiency can be attributed to two key reasons: (1) poor short-term data\nreuse, which causes frequent, irregular memory accesses, and (2) complex data\ndependencies, which limit parallelism. Hence, in this paper, we present an FPGA\nimplementation of the existing Azul accelerator, an SRAM-only hardware\naccelerator that achieves both high memory bandwidth utilization and arithmetic\nintensity. Azul features a grid of tiles, each of which is composed of a\nprocessing element (PE) and a small independent SRAM memory, which are all\nconnected over a network on chip (NoC). We implement Azul on FPGA using simple\nRISC-V CPU cores connected to a memory hierarchy of different FPGA memory\nmodules. We utilize custom RISC-V ISA augmentations to implement a task-based\nprogramming model for the various PEs, allowing communication over the NoC.\nFinally, we design simple distributed test cases so that we can functionally\nverify the FPGA implementation, verifying equivalent performance to an\narchitectural simulation of the Azul framework.", "AI": {"tldr": "The paper introduces an FPGA implementation of the Azul accelerator to efficiently solve sparse linear systems with improved memory utilization and arithmetic intensity.", "motivation": "Solving sparse linear systems is crucial due to their vast applications, but current iterative solvers are inefficient on modern hardware due to poor data reuse and parallelism.", "method": "The authors implemented Azul on FPGA by integrating RISC-V CPU cores with a memory hierarchy, enabling a task-based programming model over an NoC, and tested performance equivalency via distributed test cases.", "result": "The FPGA implementation was successfully verified to match the performance of Azul's architectural simulation.", "conclusion": "The implemented Azul architecture on FPGA demonstrates efficient and functional performance for solving sparse systems on hardware accelerators."}}
{"id": "2509.11316", "pdf": "https://arxiv.org/pdf/2509.11316", "abs": "https://arxiv.org/abs/2509.11316", "authors": ["Zihan Dong", "Xin Zhou", "Ryumei Nakada", "Lexin Li", "Linjun Zhang"], "title": "Contrastive Network Representation Learning", "categories": ["stat.ML", "cs.LG", "stat.ME"], "comment": null, "summary": "Network representation learning seeks to embed networks into a\nlow-dimensional space while preserving the structural and semantic properties,\nthereby facilitating downstream tasks such as classification, trait prediction,\nedge identification, and community detection. Motivated by challenges in brain\nconnectivity data analysis that is characterized by subject-specific,\nhigh-dimensional, and sparse networks that lack node or edge covariates, we\npropose a novel contrastive learning-based statistical approach for network\nedge embedding, which we name as Adaptive Contrastive Edge Representation\nLearning (ACERL). It builds on two key components: contrastive learning of\naugmented network pairs, and a data-driven adaptive random masking mechanism.\nWe establish the non-asymptotic error bounds, and show that our method achieves\nthe minimax optimal convergence rate for edge representation learning. We\nfurther demonstrate the applicability of the learned representation in multiple\ndownstream tasks, including network classification, important edge detection,\nand community detection, and establish the corresponding theoretical\nguarantees. We validate our method through both synthetic data and real brain\nconnectivities studies, and show its competitive performance compared to the\nbaseline method of sparse principal components analysis.", "AI": {"tldr": "The paper introduces ACERL, a novel contrastive learning-based method for network edge embedding, designed for subject-specific brain connectivity data, and demonstrates its effectiveness theoretically and empirically.", "motivation": "To address challenges in analyzing brain connectivity data, characterized by high-dimensionality, sparsity, and lack of covariates.", "method": "Adaptive Contrastive Edge Representation Learning (ACERL) combines contrastive learning of augmented network pairs with an adaptive random masking mechanism and establishes theoretical error bounds.", "result": "ACERL achieves minimax optimal convergence rates and shows robust performance in network classification, edge detection, and community detection across synthetic and brain connectivity data.", "conclusion": "ACERL is both theoretically grounded and empirically validated, achieving competitive or superior results compared to traditional methods like sparse PCA."}}
{"id": "2509.11162", "pdf": "https://arxiv.org/pdf/2509.11162", "abs": "https://arxiv.org/abs/2509.11162", "authors": ["Chuanchao Gao", "Arvind Easwaran"], "title": "Energy-Efficient Joint Offloading and Resource Allocation for Deadline-Constrained Tasks in Multi-Access Edge Computing", "categories": ["cs.DC"], "comment": null, "summary": "This paper addresses the deadline-constrained task offloading and resource\nallocation problem in multi-access edge computing. We aim to determine where\neach task is offloaded and processed, as well as corresponding communication\nand computation resource allocations, to maximize the total saved energy for\nIoT devices, while considering task deadline and system resource constraints.\nEspecially, our system allows each task to be offloaded to one of its\naccessible access points (APs) and processed on a server that is not co-located\nwith its offloading AP. We formulate this problem as an Integer Nonlinear\nProgramming problem and show it is NP-Hard. To address this problem, we propose\na Graph-Matching-based Approximation Algorithm ($\\mathtt{GMA}$), the first\napproximation algorithm of its kind. $\\mathtt{GMA}$ leverages linear\nrelaxation, tripartite graph construction, and a Linear Programming rounding\ntechnique. We prove that $\\mathtt{GMA}$ is a\n$\\frac{1-\\alpha}{2+\\epsilon}$-approximation algorithm, where $\\epsilon$ is a\nsmall positive value, and $\\alpha$ ($0$$\\le$$\\alpha$$<$$1$) is a system\nparameter that ensures the resource allocated to any task by an AP or a server\ncannot exceed $\\alpha$ times its resource capacity. Experiments show that, in\npractice, $\\mathtt{GMA}$'s energy saving achieves $97\\%$ of the optimal value\non average.", "AI": {"tldr": "This paper addresses energy-efficient task offloading and resource allocation in multi-access edge computing, proposing an algorithm that achieves near-optimal performance.", "motivation": "Optimize energy saving for IoT devices while considering task deadlines and resource constraints.", "method": "Formulated the problem as NP-Hard Integer Nonlinear Programming and introduced a Graph-Matching-based Approximation Algorithm (GMA).", "result": "The GMA algorithm achieves near-optimal energy saving, matching 97% of the optimal value on average.", "conclusion": "The GMA algorithm provides an effective approximation solution for energy-efficient task offloading in challenging computational environments."}}
{"id": "2509.10769", "pdf": "https://arxiv.org/pdf/2509.10769", "abs": "https://arxiv.org/abs/2509.10769", "authors": ["Tara Bogavelli", "Roshnee Sharma", "Hari Subramani"], "title": "AgentArch: A Comprehensive Benchmark to Evaluate Agent Architectures in Enterprise", "categories": ["cs.AI", "cs.CL", "cs.MA"], "comment": null, "summary": "While individual components of agentic architectures have been studied in\nisolation, there remains limited empirical understanding of how different\ndesign dimensions interact within complex multi-agent systems. This study aims\nto address these gaps by providing a comprehensive enterprise-specific\nbenchmark evaluating 18 distinct agentic configurations across state-of-the-art\nlarge language models. We examine four critical agentic system dimensions:\norchestration strategy, agent prompt implementation (ReAct versus function\ncalling), memory architecture, and thinking tool integration. Our benchmark\nreveals significant model-specific architectural preferences that challenge the\nprevalent one-size-fits-all paradigm in agentic AI systems. It also reveals\nsignificant weaknesses in overall agentic performance on enterprise tasks with\nthe highest scoring models achieving a maximum of only 35.3\\% success on the\nmore complex task and 70.8\\% on the simpler task. We hope these findings inform\nthe design of future agentic systems by enabling more empirically backed\ndecisions regarding architectural components and model selection.", "AI": {"tldr": "This paper evaluates 18 agentic configurations of large language models across four system dimensions to identify optimal designs for enterprise tasks.", "motivation": "The study aims to address gaps in understanding of how design dimensions interact in multi-agent systems, especially within enterprise applications.", "method": "The authors benchmark and evaluate 18 distinct agentic configurations, examining orchestration strategy, prompt implementation, memory architecture, and tool integration.", "result": "They find model-specific architectural preferences and highlight weaknesses, with success rates ranging only from 35.3% to 70.8% on enterprise tasks.", "conclusion": "The findings challenge the one-size-fits-all approach in agentic AI design and aim to inform future system architectures for improved performance."}}
{"id": "2509.11065", "pdf": "https://arxiv.org/pdf/2509.11065", "abs": "https://arxiv.org/abs/2509.11065", "authors": ["Yuan Si", "Daming Li", "Hanyuan Shi", "Jialu Zhang"], "title": "ViScratch: Using Large Language Models and Gameplay Videos for Automated Feedback in Scratch", "categories": ["cs.SE", "cs.PL"], "comment": null, "summary": "Block-based programming environments such as Scratch are increasingly popular\nin programming education, in particular for young learners. While the use of\nblocks helps prevent syntax errors, semantic bugs remain common and difficult\nto debug. Existing tools for Scratch debugging rely heavily on predefined rules\nor user manual inputs, and crucially, they ignore the platform's inherently\nvisual nature.\n  We introduce ViScratch, the first multimodal feedback generation system for\nScratch that leverages both the project's block code and its generated gameplay\nvideo to diagnose and repair bugs. ViScratch uses a two-stage pipeline: a\nvision-language model first aligns visual symptoms with code structure to\nidentify a single critical issue, then proposes minimal, abstract syntax tree\nlevel repairs that are verified via execution in the Scratch virtual machine.\n  We evaluate ViScratch on a set of real-world Scratch projects against\nstate-of-the-art LLM-based tools and human testers. Results show that gameplay\nvideo is a crucial debugging signal: ViScratch substantially outperforms prior\ntools in both bug identification and repair quality, even without access to\nproject descriptions or goals. This work demonstrates that video can serve as a\nfirst-class specification in visual programming environments, opening new\ndirections for LLM-based debugging beyond symbolic code alone.", "AI": {"tldr": "The paper introduces ViScratch, a debugging tool for Scratch that uses gameplay videos and block code to better identify and fix bugs, showing superior performance over existing tools.", "motivation": "Debugging in block-based environments like Scratch is challenging because current tools ignore the visual nature of these platforms, focusing only on textual or manual inputs.", "method": "ViScratch uses a two-stage pipeline leveraging a vision-language model to align gameplay video symptoms with code structure, identifying bugs and proposing repairs verified in the Scratch virtual machine.", "result": "The system significantly outperforms state-of-the-art tools and human testers in bug identification and repair, validating the importance of integrating video for debugging.", "conclusion": "ViScratch demonstrates that gameplay video can be a crucial debugging asset in visual programming, paving the way for future LLM-based debugging innovations that include multimodal inputs."}}
{"id": "2509.10503", "pdf": "https://arxiv.org/pdf/2509.10503", "abs": "https://arxiv.org/abs/2509.10503", "authors": ["Haolin Yuan", "Jingtao Li", "Weiming Zhuang", "Chen Chen", "Lingjuan Lyu"], "title": "FEDEXCHANGE: Bridging the Domain Gap in Federated Object Detection for Free", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Federated Object Detection (FOD) enables clients to collaboratively train a\nglobal object detection model without accessing their local data from diverse\ndomains. However, significant variations in environment, weather, and other\ndomain specific factors hinder performance, making cross domain generalization\na key challenge. Existing FOD methods often overlook the hardware constraints\nof edge devices and introduce local training regularizations that incur high\ncomputational costs, limiting real-world applicability. In this paper, we\npropose FEDEXCHANGE, a novel FOD framework that bridges domain gaps without\nintroducing additional local computational overhead. FEDEXCHANGE employs a\nserver side dynamic model exchange strategy that enables each client to gain\ninsights from other clients' domain data without direct data sharing.\nSpecifically, FEDEXCHANGE allows the server to alternate between model\naggregation and model exchange. During aggregation rounds, the server\naggregates all local models as usual. In exchange rounds, FEDEXCHANGE clusters\nand exchanges local models based on distance measures, allowing local models to\nlearn from a variety of domains. As all operations are performed on the server\nside, clients can achieve improved cross domain utility without any additional\ncomputational overhead. Extensive evaluations demonstrate that FEDEXCHANGE\nenhances FOD performance, achieving 1.6X better mean average precision in\nchallenging domains, such as rainy conditions, while requiring only 0.8X the\ncomputational resources compared to baseline methods.", "AI": {"tldr": "FEDEXCHANGE is a new framework for Federated Object Detection (FOD) that improves cross-domain performance using server-side model exchange, addressing domain gaps without extra computational cost for clients.", "motivation": "The paper addresses low performance in Federated Object Detection caused by domain variations while ensuring compatibility with hardware limitations of edge devices.", "method": "FEDEXCHANGE introduces a server-side dynamic model exchange strategy alternating between model aggregation and exchange. Local models cluster and learn from diverse domains without direct data sharing.", "result": "FEDEXCHANGE achieves 1.6X better mean average precision in challenging conditions (e.g., rain) while reducing computational resource use to 0.8X compared to baseline methods.", "conclusion": "The proposed framework improves cross-domain generalization in FOD without additional client-side computational burdens, demonstrating better efficiency and performance."}}
{"id": "2509.10771", "pdf": "https://arxiv.org/pdf/2509.10771", "abs": "https://arxiv.org/abs/2509.10771", "authors": ["Clemens Schwarke", "Mayank Mittal", "Nikita Rudin", "David Hoeller", "Marco Hutter"], "title": "RSL-RL: A Learning Library for Robotics Research", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "RSL-RL is an open-source Reinforcement Learning library tailored to the\nspecific needs of the robotics community. Unlike broad general-purpose\nframeworks, its design philosophy prioritizes a compact and easily modifiable\ncodebase, allowing researchers to adapt and extend algorithms with minimal\noverhead. The library focuses on algorithms most widely adopted in robotics,\ntogether with auxiliary techniques that address robotics-specific challenges.\nOptimized for GPU-only training, RSL-RL achieves high-throughput performance in\nlarge-scale simulation environments. Its effectiveness has been validated in\nboth simulation benchmarks and in real-world robotic experiments, demonstrating\nits utility as a lightweight, extensible, and practical framework to develop\nlearning-based robotic controllers. The library is open-sourced at:\nhttps://github.com/leggedrobotics/rsl_rl.", "AI": {"tldr": "RSL-RL is a streamlined, open-source reinforcement learning library for robotics with GPU-optimized performance and ease of customization.", "motivation": "To provide a lightweight, robotics-focused reinforcement learning library that facilitates easy algorithm adaptation and high-performance simulation.", "method": "The library is tailored for GPU-only training and includes widely used RL algorithms and robotics-specific techniques.", "result": "Validated in both simulation benchmarks and real-world robotic experiments, showcasing its practicality and efficiency.", "conclusion": "RSL-RL serves as a versatile and effective tool for developing robotic controllers, optimized for use in robotics research."}}
{"id": "2509.10696", "pdf": "https://arxiv.org/pdf/2509.10696", "abs": "https://arxiv.org/abs/2509.10696", "authors": ["Shuaiqi Wang", "Vikas Raunak", "Arturs Backurs", "Victor Reis", "Pei Zhou", "Sihao Chen", "Longqi Yang", "Zinan Lin", "Sergey Yekhanin", "Giulia Fanti"], "title": "Struct-Bench: A Benchmark for Differentially Private Structured Text Generation", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Differentially private (DP) synthetic data generation is a promising\ntechnique for utilizing private datasets that otherwise cannot be exposed for\nmodel training or other analytics. While much research literature has focused\non generating private unstructured text and image data, in enterprise settings,\nstructured data (e.g., tabular) is more common, often including natural\nlanguage fields or components. Existing synthetic data evaluation techniques\n(e.g., FID) struggle to capture the structural properties and correlations of\nsuch datasets. In this work, we propose Struct-Bench, a framework and benchmark\nfor evaluating synthetic datasets derived from structured datasets that contain\nnatural language data. The Struct-Bench framework requires users to provide a\nrepresentation of their dataset structure as a Context-Free Grammar (CFG). Our\nbenchmark comprises 5 real-world and 2 synthetically generated datasets, each\nannotated with CFGs. We show that these datasets demonstrably present a great\nchallenge even for state-of-the-art DP synthetic data generation methods.\nStruct-Bench also includes reference implementations of different metrics and a\nleaderboard, thereby providing researchers a standardized evaluation platform\nto benchmark and investigate privacy-preserving synthetic data generation\nmethods. Further, we also present a case study showing how to use Struct-Bench\nto improve the synthetic data quality of Private Evolution (PE) on structured\ndata. The benchmark and the leaderboard have been publicly made available at\nhttps://struct-bench.github.io.", "AI": {"tldr": "The paper introduces Struct-Bench, a framework for evaluating differentially private (DP) synthetic datasets derived from structured datasets, especially those with natural language components.", "motivation": "There is a need to evaluate the quality of differentially private synthetic data generated for structured datasets, particularly as existing techniques like FID are inadequate for capturing structural and correlational properties.", "method": "The authors propose Struct-Bench, which uses Context-Free Grammar (CFG) to represent dataset structure, and includes 5 real-world and 2 synthetic datasets annotated with CFGs. It also provides metrics, a leaderboard, and aims to standardize evaluations.", "result": "The paper demonstrates the difficulty of current DP synthetic generation methods on the benchmark datasets, showcasing Struct-Bench's utility. A case study also illustrates how Struct-Bench improves Private Evolution on structured data.", "conclusion": "Struct-Bench offers a valuable, standardized platform for advancing and evaluating DP synthetic data generation methods, especially for structured datasets with natural language. Researchers now have tools to measure and improve synthesis quality."}}
{"id": "2509.11000", "pdf": "https://arxiv.org/pdf/2509.11000", "abs": "https://arxiv.org/abs/2509.11000", "authors": ["Omid Gheibi", "Christian K\u00e4stner", "Pooyan Jamshidi"], "title": "Hardness, Structural Knowledge, and Opportunity: An Analytical Framework for Modular Performance Modeling", "categories": ["cs.SE", "cs.AI", "cs.LG"], "comment": null, "summary": "Performance-influence models are beneficial for understanding how\nconfigurations affect system performance, but their creation is challenging due\nto the exponential growth of configuration spaces. While gray-box approaches\nleverage selective \"structural knowledge\" (like the module execution graph of\nthe system) to improve modeling, the relationship between this knowledge, a\nsystem's characteristics (we call them \"structural aspects\"), and potential\nmodel improvements is not well understood. This paper addresses this gap by\nformally investigating how variations in structural aspects (e.g., the number\nof modules and options per module) and the level of structural knowledge impact\nthe creation of \"opportunities\" for improved \"modular performance modeling\". We\nintroduce and quantify the concept of modeling \"hardness\", defined as the\ninherent difficulty of performance modeling. Through controlled experiments\nwith synthetic system models, we establish an \"analytical matrix\" to measure\nthese concepts. Our findings show that modeling hardness is primarily driven by\nthe number of modules and configuration options per module. More importantly,\nwe demonstrate that both higher levels of structural knowledge and increased\nmodeling hardness significantly enhance the opportunity for improvement. The\nimpact of these factors varies by performance metric; for ranking accuracy\n(e.g., in debugging task), structural knowledge is more dominant, while for\nprediction accuracy (e.g., in resource management task), hardness plays a\nstronger role. These results provide actionable insights for system designers,\nguiding them to strategically allocate time and select appropriate modeling\napproaches based on a system's characteristics and a given task's objectives.", "AI": {"tldr": "The paper explores how structural knowledge and system characteristics impact performance modeling and introduces the concept of modeling \"hardness.\" Key findings link modular complexity and structural knowledge to modeling opportunities for task-specific improvements.", "motivation": "To address the challenge of modeling how configurations impact system performance in the face of exponentially growing configuration spaces and establish a formal understanding of the impact of structural aspects and knowledge on performance modeling.", "method": "The paper conducts controlled experiments with synthetic system models and establishes an analytical matrix to investigate the relationship between structural aspects, structural knowledge, and performance modeling improvements.", "result": "The study finds that modeling hardness is primarily influenced by the number of modules and options per module. The level of structural knowledge and modeling hardness both significantly enhance modeling opportunities, with their impact varying by performance metric.", "conclusion": "System designers can strategically use insights about structural characteristics and modeling hardness to select appropriate modeling approaches for specific performance tasks, optimizing time and resources."}}
{"id": "2509.10891", "pdf": "https://arxiv.org/pdf/2509.10891", "abs": "https://arxiv.org/abs/2509.10891", "authors": ["Zhipeng Wang", "Yingqi Rong", "Kaiwei Liu", "Mingzhe Yang", "Jiang Zhang", "Jing He"], "title": "Causal Emergence of Consciousness through Learned Multiscale Neural Dynamics in Mice", "categories": ["q-bio.NC"], "comment": null, "summary": "Consciousness spans macroscopic experience and microscopic neuronal activity,\nyet linking these scales remains challenging. Prevailing theories, such as\nIntegrated Information Theory, focus on a single scale, overlooking how causal\npower and its dynamics unfold across scales. Progress is constrained by scarce\ncross-scale data and difficulties in quantifying multiscale causality and\ndynamics. Here, we present a machine learning framework that infers multiscale\ncausal variables and their dynamics from near-cellular-resolution calcium\nimaging in the mouse dorsal cortex. At lower levels, variables primarily\naggregate input-driven information, whereas at higher levels they realize\ncausality through metastable or saddle-point dynamics during wakefulness,\ncollapsing into localized, stochastic dynamics under anesthesia. A\none-dimensional top-level conscious variable captures the majority of causal\npower, yet variables across other scales also contribute substantially, giving\nrise to high emergent complexity in the conscious state. Together, these\nfindings provide a multiscale causal framework that links neural activity to\nconscious states.", "AI": {"tldr": "The paper introduces a machine learning framework that identifies multiscale causal variables from calcium imaging data in the mouse brain to explore the connection between neural activity and consciousness.", "motivation": "To address the challenge of bridging the gap between macroscopic experiences and microscopic neuronal activity in the study of consciousness, and the limitations of existing theories that focus on single scales.", "method": "The authors developed a machine learning framework to infer multiscale causal variables and their dynamics from high-resolution calcium imaging in the dorsal cortex of mice.", "result": "The study found that lower-level variables aggregate input-driven information, while higher-level variables exhibit causal dynamics through metastable states in wakefulness, transitioning to localized stochastic dynamics during anesthesia. A dominant top-level variable captures most causal power but other scales also contribute, indicating high emergent complexity.", "conclusion": "The findings establish a multiscale causal framework demonstrating the relationship between neural activity and conscious states, accounting for dynamics at various scales."}}
{"id": "2509.10687", "pdf": "https://arxiv.org/pdf/2509.10687", "abs": "https://arxiv.org/abs/2509.10687", "authors": ["Hao Zhang", "Chun-Han Yao", "Simon Donn\u00e9", "Narendra Ahuja", "Varun Jampani"], "title": "Stable Part Diffusion 4D: Multi-View RGB and Kinematic Parts Video Generation", "categories": ["cs.CV"], "comment": "Page: https://stablepartdiffusion4d.github.io/", "summary": "We present Stable Part Diffusion 4D (SP4D), a framework for generating paired\nRGB and kinematic part videos from monocular inputs. Unlike conventional part\nsegmentation methods that rely on appearance-based semantic cues, SP4D learns\nto produce kinematic parts - structural components aligned with object\narticulation and consistent across views and time. SP4D adopts a dual-branch\ndiffusion model that jointly synthesizes RGB frames and corresponding part\nsegmentation maps. To simplify the architecture and flexibly enable different\npart counts, we introduce a spatial color encoding scheme that maps part masks\nto continuous RGB-like images. This encoding allows the segmentation branch to\nshare the latent VAE from the RGB branch, while enabling part segmentation to\nbe recovered via straightforward post-processing. A Bidirectional Diffusion\nFusion (BiDiFuse) module enhances cross-branch consistency, supported by a\ncontrastive part consistency loss to promote spatial and temporal alignment of\npart predictions. We demonstrate that the generated 2D part maps can be lifted\nto 3D to derive skeletal structures and harmonic skinning weights with few\nmanual adjustments. To train and evaluate SP4D, we construct KinematicParts20K,\na curated dataset of over 20K rigged objects selected and processed from\nObjaverse XL (Deitke et al., 2023), each paired with multi-view RGB and part\nvideo sequences. Experiments show that SP4D generalizes strongly to diverse\nscenarios, including real-world videos, novel generated objects, and rare\narticulated poses, producing kinematic-aware outputs suitable for downstream\nanimation and motion-related tasks.", "AI": {"tldr": "Stable Part Diffusion 4D (SP4D) generates RGB and kinematic part videos from monocular inputs by leveraging a dual-branch diffusion model and innovative spatial encoding for flexible part segmentation.", "motivation": "To overcome the limitations of conventional appearance-based part segmentation methods by introducing kinematic-aware structural components consistent across views and time.", "method": "SP4D uses a dual-branch diffusion architecture for RGB and part map synthesis paired with spatial color encoding and Bidirectional Diffusion Fusion (BiDiFuse) for cross-branch consistency.", "result": "SP4D successfully generates 2D part maps that can be lifted to 3D skeletal structures and skinning weights, and performs strongly in diverse scenarios, including novel and articulated poses.", "conclusion": "SP4D offers an effective solution for generating kinematic-aware video outputs, supporting animation and motion-related tasks while showcasing strong generalization capabilities."}}
{"id": "2509.12053", "pdf": "https://arxiv.org/pdf/2509.12053", "abs": "https://arxiv.org/abs/2509.12053", "authors": ["Yujun Lin", "Zhekai Zhang", "Song Han"], "title": "LEGO: Spatial Accelerator Generation and Optimization for Tensor Applications", "categories": ["cs.AR", "cs.AI", "cs.LG"], "comment": "The first two authors have equal contributions; Published as a\n  conference paper in HPCA 2025; 13 pages, 14 figures", "summary": "Modern tensor applications, especially foundation models and generative AI\napplications require multiple input modalities (both vision and language),\nwhich increases the demand for flexible accelerator architecture. Existing\nframeworks suffer from the trade-off between design flexibility and\nproductivity of RTL generation: either limited to very few hand-written\ntemplates or cannot automatically generate the RTL. To address this challenge,\nwe propose the LEGO framework, which targets tensor applications and\nautomatically generates spatial architecture design and outputs synthesizable\nRTL code without handwritten RTL design templates. Leveraging the\naffine-transformation-based architecture representation, LEGO front end finds\ninterconnections between function units, synthesizes the memory system, and\nfuses different spatial dataflow designs based on data reuse analysis. LEGO\nback end then translates the hardware in a primitive-level graph to perform\nlower-level optimizations, and applies a set of linear-programming algorithms\nto optimally insert pipeline registers and reduce the overhead of unused logic\nwhen switching spatial dataflows. Our evaluation demonstrates that LEGO can\nachieve 3.2x speedup and 2.4x energy efficiency compared to previous work\nGemmini, and can generate one architecture for diverse modern foundation models\nin generative AI applications.", "AI": {"tldr": "LEGO is a framework that automates RTL generation for tensor applications, delivering 3.2x speedup and 2.4x energy efficiency compared to existing solutions.", "motivation": "Current tensor applications demand flexible accelerator architectures for handling multimodal data, which traditional RTL generation methods struggle to address.", "method": "LEGO employs affine-transformation-based representation to synthesize interconnections, memory systems, and spatial dataflows, followed by lower-level optimizations using linear programming.", "result": "LEGO achieves significant performance improvements, with 3.2x faster execution and 2.4x better energy efficiency than Gemmini.", "conclusion": "LEGO simplifies spatial architecture design for tensor applications, enhancing both productivity and design flexibility while delivering superior performance and energy efficiency."}}
{"id": "2509.11338", "pdf": "https://arxiv.org/pdf/2509.11338", "abs": "https://arxiv.org/abs/2509.11338", "authors": ["Rok Cestnik", "Erik A. Martens"], "title": "Next-Generation Reservoir Computing for Dynamical Inference", "categories": ["stat.ML", "cs.LG"], "comment": "10 pages, 10 figures", "summary": "We present a simple and scalable implementation of next-generation reservoir\ncomputing for modeling dynamical systems from time series data. Our approach\nuses a pseudorandom nonlinear projection of time-delay embedded input, allowing\nan arbitrary dimension of the feature space, thus providing a flexible\nalternative to the polynomial-based projections used in previous\nnext-generation reservoir computing variants. We apply the method to benchmark\ntasks -- including attractor reconstruction and bifurcation diagram estimation\n-- using only partial and noisy observations. We also include an exploratory\nexample of estimating asymptotic oscillation phases. The models remain stable\nover long rollouts and generalize beyond training data. This framework enables\nthe precise control of system state and is well suited for surrogate modeling\nand digital twin applications.", "AI": {"tldr": "The paper introduces a new implementation of next-generation reservoir computing for modeling dynamic systems, offering scalable, flexible, and stable frameworks applicable to various tasks like attractor reconstruction and bifurcation estimation.", "motivation": "The need to improve reservoir computing methods for better scalability, flexibility, and performance in modeling dynamical systems from time series data.", "method": "The paper proposes a pseudorandom nonlinear projection of time-delay embedded input, avoiding the limitations of traditional polynomial-based projections. Benchmark tasks are used to validate the approach.", "result": "The proposed models perform well in attractor reconstruction and bifurcation diagram estimation, even with partial and noisy data. They also generalize beyond training data and maintain long-term stability.", "conclusion": "The framework is highly adaptable for precise control of system states, making it suitable for surrogate modeling and digital twin applications."}}
{"id": "2509.11396", "pdf": "https://arxiv.org/pdf/2509.11396", "abs": "https://arxiv.org/abs/2509.11396", "authors": ["Adam Janiak", "Damian Kowalczyk", "Maciej Lichtenstein"], "title": "Parallel/Distributed Tabu Search for Scheduling Microprocessor Tasks in Hybrid Flowshop", "categories": ["cs.DC", "cs.SY", "eess.SY", "C.2.4"], "comment": "authors listed in alphabetical order", "summary": "The paper deals with the makespan minimization in the hybrid flow shop\nscheduling problem with multiprocessor tasks. The hybrid flow shop (HFS)\ngeneralizes the classical flow shop processor configuration by replacing each\nprocessor (processing stage) by some number of identical parallel processors.\nSimilarly, the multiprocessor tasks generalize the classical assumption, by\nallowing a task to require more than one processor simultaneously for its\nprocessing. In this work we present the algorithm for solving the problem based\non the tabu search technique. The proposed algorithm uses parallel and\ndistributed mechanisms for neighborhood evaluation and well balances\nheterogeneous network environment.", "AI": {"tldr": "This paper addresses minimizing makespan in a hybrid flow shop scheduling problem with multiprocessor tasks using a tabu search algorithm.", "motivation": "To improve scheduling efficiency in hybrid flow shops with multiprocessor tasks, which are more complex than traditional flow shops.", "method": "The authors propose a tabu search algorithm that employs parallel and distributed mechanisms to evaluate neighborhoods while optimizing for heterogeneous network environments.", "result": "Development of an advanced scheduling algorithm aimed at achieving better balance and efficiency in hybrid flow shop scenarios.", "conclusion": "The proposed algorithm effectively tackles the challenge of makespan minimization in complex scheduling environments utilizing parallel and distributed approaches."}}
{"id": "2509.10818", "pdf": "https://arxiv.org/pdf/2509.10818", "abs": "https://arxiv.org/abs/2509.10818", "authors": ["Boris Kovalerchuk", "Brent D. Fegley"], "title": "LLM Enhancement with Domain Expert Mental Model to Reduce LLM Hallucination with Causal Prompt Engineering", "categories": ["cs.AI", "cs.HC"], "comment": "25 pages,4 figures, 2 tables", "summary": "Difficult decision-making problems abound in various disciplines and domains.\nThe proliferation of generative techniques, especially large language models\n(LLMs), has excited interest in using them for decision support. However, LLMs\ncannot yet resolve missingness in their training data, leading to\nhallucinations. Retrieval-Augmented Generation (RAG) enhances LLMs by\nincorporating external information retrieval, reducing hallucinations and\nimproving accuracy. Yet, RAG and related methods are only partial solutions, as\nthey may lack access to all necessary sources or key missing information. Even\neveryday issues often challenge LLMs' abilities. Submitting longer prompts with\ncontext and examples is one approach to address knowledge gaps, but designing\neffective prompts is non-trivial and may not capture complex mental models of\ndomain experts. For tasks with missing critical information, LLMs are\ninsufficient, as are many existing systems poorly represented in available\ndocuments. This paper explores how LLMs can make decision-making more\nefficient, using a running example of evaluating whether to respond to a call\nfor proposals. We propose a technology based on optimized human-machine\ndialogue and monotone Boolean and k-valued functions to discover a\ncomputationally tractable personal expert mental model (EMM) of\ndecision-making. Our EMM algorithm for LLM prompt engineering has four steps:\n(1) factor identification, (2) hierarchical structuring of factors, (3)\ngenerating a generalized expert mental model specification, and (4) generating\na detailed generalized expert mental model from that specification.", "AI": {"tldr": "The paper discusses enhancing decision-making using LLMs by addressing their hallucination issues and introducing a computationally tractable expert mental model (EMM) for improved prompt engineering.", "motivation": "The paper aims to tackle the challenges faced by LLMs in decision-making tasks due to hallucinations and gaps in knowledge, while advancing techniques to better integrate expert mental models into computational processes.", "method": "It introduces a four-step EMM algorithm involving factor identification, hierarchical structuring, specification generation, and detailed model creation to improve LLM prompt engineering.", "result": "The proposed methodology outlines how human-machine dialogues and Boolean functions can help create the expert mental model, addressing missing critical information gaps.", "conclusion": "The structured EMM technique can make LLM-based decision-making more accurate and efficient, accommodating complex domain expertise for challenging tasks like deciding on proposal responses."}}
{"id": "2509.11559", "pdf": "https://arxiv.org/pdf/2509.11559", "abs": "https://arxiv.org/abs/2509.11559", "authors": ["Tarakaram Gollamudi", "Anitha Gollamudi", "Joshua Gancher"], "title": "ILA: Correctness via Type Checking for Fully Homomorphic Encryption", "categories": ["cs.CR", "cs.PL"], "comment": null, "summary": "RLWE-based Fully Homomorphic Encryption (FHE) schemes add some small\n\\emph{noise} to the message during encryption. The noise accumulates with each\nhomomorphic operation. When the noise exceeds a critical value, the FHE circuit\nproduces an incorrect output. This makes developing FHE applications quite\nsubtle, as one must closely track the noise to ensure correctness. However,\nexisting libraries and compilers offer limited support to statically track the\nnoise. Additionally, FHE circuits are also plagued by wraparound errors that\nare common in finite modulus arithmetic. These two limitations of existing\ncompilers and libraries make FHE applications too difficult to develop with\nconfidence.\n  In this work, we present a \\emph{correctness-oriented} IR, Intermediate\nLanguage for Arithmetic circuits, for type-checking circuits intended for\nhomomorphic evaluation. Our IR is backed by a type system that tracks low-level\nquantitative bounds (e.g., ciphertext noise) without using the secret key.\nUsing our type system, we identify and prove a strong \\emph{functional\ncorrectness} criterion for \\ila circuits. Additionally, we have designed \\ila\nto be maximally general: our core type system does not directly assume a\nparticular FHE scheme, but instead axiomatizes a \\emph{model} of FHE. We\ninstantiate this model with the exact FHE schemes (BGV, BFV and TFHE), and\nobtain functional correctness for free.", "AI": {"tldr": "The paper introduces a type system and intermediate language (IR) to statically track noise and handle errors in RLWE-based Fully Homomorphic Encryption (FHE) circuits.", "motivation": "FHE circuits are prone to noise accumulation and wraparound errors, which are difficult to track and ensure correctness with existing libraries or compilers.", "method": "This work proposes a correctness-oriented intermediate representation (IR) for arithmetic circuits, backed by a type system that tracks quantitative bounds such as ciphertext noise without requiring the secret key.", "result": "The type system ensures strong functional correctness for FHE circuits, and the IR is general, working with multiple FHE schemes (BGV, BFV, TFHE) through an axiomatized FHE model.", "conclusion": "The proposed method simplifies the development of FHE applications by ensuring correctness and compatibility across different FHE schemes."}}
{"id": "2509.10504", "pdf": "https://arxiv.org/pdf/2509.10504", "abs": "https://arxiv.org/abs/2509.10504", "authors": ["Mianchu Wang", "Giovanni Montana"], "title": "Retrosynthesis Planning via Worst-path Policy Optimisation in Tree-structured MDPs", "categories": ["cs.LG", "cs.AI", "cs.CE"], "comment": null, "summary": "Retrosynthesis planning aims to decompose target molecules into available\nbuilding blocks, forming a synthesis tree where each internal node represents\nan intermediate compound and each leaf ideally corresponds to a purchasable\nreactant. However, this tree becomes invalid if any leaf node is not a valid\nbuilding block, making the planning process vulnerable to the \"weakest link\" in\nthe synthetic route. Existing methods often optimise for average performance\nacross branches, failing to account for this worst-case sensitivity. In this\npaper, we reframe retrosynthesis as a worst-path optimisation problem within\ntree-structured Markov Decision Processes (MDPs). We prove that this\nformulation admits a unique optimal solution and offers monotonic improvement\nguarantees. Building on this insight, we introduce Interactive Retrosynthesis\nPlanning (InterRetro), a method that interacts with the tree MDP, learns a\nvalue function for worst-path outcomes, and improves its policy through\nself-imitation, preferentially reinforcing past decisions with high estimated\nadvantage. Empirically, InterRetro achieves state-of-the-art results, solving\n100% of targets on the Retro*-190 benchmark, shortening synthetic routes by\n4.9%, and achieving promising performance using only 10% of the training data -\nrepresenting a significant advance in computational retrosynthesis planning.", "AI": {"tldr": "The paper presents Interactive Retrosynthesis Planning (InterRetro), a new method designed to optimize retrosynthesis planning by focusing on the worst-case sensitivity of synthetic routes.", "motivation": "Existing retrosynthesis methods fail to account for the vulnerability of synthesis trees to weak links, as they optimize for average performance instead of focusing on the worst-case scenarios.", "method": "The authors reformulate retrosynthesis planning as a worst-path optimization problem using tree-structured Markov Decision Processes (MDPs), introducing InterRetro to enhance outcomes through self-imitation and interaction with the tree MDP.", "result": "InterRetro achieves state-of-the-art results by solving 100% of targets on the Retro*-190 benchmark, reducing synthetic route length by 4.9%, and achieving strong performance with minimal training data.", "conclusion": "This work represents a significant advancement in computational retrosynthesis planning, making synthetic routes more robust and efficient by addressing worst-path sensitivities."}}
{"id": "2509.10796", "pdf": "https://arxiv.org/pdf/2509.10796", "abs": "https://arxiv.org/abs/2509.10796", "authors": ["Hanjing Ye", "Weixi Situ", "Jianwei Peng", "Yu Zhan", "Bingyi Xia", "Kuanqi Cai", "Hong Zhang"], "title": "Follow-Bench: A Unified Motion Planning Benchmark for Socially-Aware Robot Person Following", "categories": ["cs.RO"], "comment": "TBD. All code, data, and deployment scripts are publicly available at\n  https://follow-bench.github.io/", "summary": "Robot person following (RPF) -- mobile robots that follow and assist a\nspecific person -- has emerging applications in personal assistance, security\npatrols, eldercare, and logistics. To be effective, such robots must follow the\ntarget while ensuring safety and comfort for both the target and surrounding\npeople. In this work, we present the first end-to-end study of RPF, which (i)\nsurveys representative scenarios, motion-planning methods, and evaluation\nmetrics with a focus on safety and comfort; (ii) introduces Follow-Bench, a\nunified benchmark simulating diverse scenarios, including various target\ntrajectory patterns, dynamic-crowd flows, and environmental layouts; and (iii)\nre-implements six popular RPF planners, ensuring that both safety and comfort\nare systematically considered. Moreover, we evaluate the two highest-performing\nplanners from our benchmark on a differential-drive robot to provide insights\ninto real-world deployment. Extensive simulation and real-world experiments\nprovide quantitative insights into the safety-comfort trade-offs of existing\nplanners, while revealing open challenges and future research directions.", "AI": {"tldr": "This paper delves into robot person following (RPF) applications, creating a unified benchmark (Follow-Bench) and evaluating planner methods in simulations and real-world settings, focusing on safety and comfort.", "motivation": "Robot person following has significant relevance in assisting individuals, ensuring safety, and addressing dynamic environments like eldercare and logistics.", "method": "The study creates Follow-Bench, simulates diverse RPF scenarios, re-implements six planners, and evaluates top-performing planners on real robots.", "result": "The analysis yielded valuable insights into safety-comfort trade-offs in RPF systems and demonstrated how existing planners operate in real-world environments.", "conclusion": "This research highlights the need for refined RPF systems, uncovers existing challenges, and suggests promising directions for future explorations."}}
{"id": "2509.10697", "pdf": "https://arxiv.org/pdf/2509.10697", "abs": "https://arxiv.org/abs/2509.10697", "authors": ["Pengcheng Jiang", "Siru Ouyang", "Yizhu Jiao", "Ming Zhong", "Runchu Tian", "Jiawei Han"], "title": "A Survey on Retrieval And Structuring Augmented Generation with Large Language Models", "categories": ["cs.CL"], "comment": "KDD'25 survey track", "summary": "Large Language Models (LLMs) have revolutionized natural language processing\nwith their remarkable capabilities in text generation and reasoning. However,\nthese models face critical challenges when deployed in real-world applications,\nincluding hallucination generation, outdated knowledge, and limited domain\nexpertise. Retrieval And Structuring (RAS) Augmented Generation addresses these\nlimitations by integrating dynamic information retrieval with structured\nknowledge representations. This survey (1) examines retrieval mechanisms\nincluding sparse, dense, and hybrid approaches for accessing external\nknowledge; (2) explore text structuring techniques such as taxonomy\nconstruction, hierarchical classification, and information extraction that\ntransform unstructured text into organized representations; and (3) investigate\nhow these structured representations integrate with LLMs through prompt-based\nmethods, reasoning frameworks, and knowledge embedding techniques. It also\nidentifies technical challenges in retrieval efficiency, structure quality, and\nknowledge integration, while highlighting research opportunities in multimodal\nretrieval, cross-lingual structures, and interactive systems. This\ncomprehensive overview provides researchers and practitioners with insights\ninto RAS methods, applications, and future directions.", "AI": {"tldr": "This paper surveys RAS (Retrieval and Structuring) methods that enhance LLMs by integrating efficient information retrieval and structured knowledge, addressing challenges like hallucination and outdated information.", "motivation": "To address critical challenges faced by LLMs, such as hallucination generation, outdated information, and limited domain expertise, thereby improving their deployment in real-world applications.", "method": "The survey examines retrieval methods (sparse, dense, hybrid), text structuring techniques (taxonomy, classification, extraction), and mechanisms for integrating structured information through prompts, reasoning, and embeddings.", "result": "The paper identifies challenges in retrieval efficiency, structure quality, and knowledge integration, and highlights future opportunities in areas like multimodal retrieval, cross-lingual structures, and interactive systems.", "conclusion": "This work provides useful insights into RAS techniques and their potential to enhance LLMs, guiding future research and applications."}}
{"id": "2509.11530", "pdf": "https://arxiv.org/pdf/2509.11530", "abs": "https://arxiv.org/abs/2509.11530", "authors": ["Junchi Feng", "Fernanda Garcia-Pina", "Mahya Beheshti", "Todd E Hudson", "William Seiple", "John-Ross Rizzo"], "title": "Residual Gaze Behavior During Navigation in Blindness and Low Vision", "categories": ["q-bio.NC"], "comment": null, "summary": "Background: Outdoor navigation poses significant challenges for people with\nblindness or low vision, yet the role of gaze behavior in supporting mobility\nremains underexplored. Fully sighted individuals typically adopt consistent\nscanning strategies, whereas those with visual impairments rely on\nheterogeneous adaptations shaped by residual vision and experience.\n  Methods: We conducted a comparative eye-tracking study of fully sighted, low\nvision, blind, and fully blind participants navigating outdoor routes. Using a\nwearable eye tracker, we quantified fixation counts, fixation rate, fixation\narea, direction, peak fixation location, and walking speed.\n  Results: Walking speed declined systematically with worsening vision.\nFixation count increased with greater impairment, reflecting slower travel\ntimes and more frequent sampling. Fixation rate rose with worsening vision,\nthough between-group differences were generally not significant between most\ngroups. Fixation spatial coverage decreased along the continuum of vision loss.\nFixation patterns were most consistent in the fully sighted group. Peak\nfixation locations were centered in fully sighted participants but shifted\noutward and became more variable with impairment.\n  Conclusion: Gaze strategies during navigation form a graded continuum across\nvision groups, with fully sighted and fully blind participants at opposite\npoles and low vision and blind groups spanning the middle. Visual acuity alone\ndoes not predict functional gaze use, as rehabilitation experience and adaptive\nstrategies strongly shape behavior. These findings highlight the need for\npersonalized rehabilitation and assistive technologies, with residual gaze\npatterns offering insight into mobility capacity and training opportunities for\nsafer navigation.", "AI": {"tldr": "The study examines gaze strategies in outdoor navigation across vision impairment levels, finding systematic changes with worsening vision and emphasizing the importance of personalized rehabilitation and assistive technologies.", "motivation": "The study aims to examine how gaze behaviors vary during outdoor navigation among people with blindness, low vision, and full sight, addressing a gap in understanding of how visual impairment shapes mobility strategies.", "method": "A comparative eye-tracking study was conducted involving fully sighted, low vision, blind, and fully blind participants. Metrics such as fixation counts, rate, area, direction, peak locations, and walking speed were quantified using wearable eye trackers.", "result": "Walking speed decreased with worsening vision. Fixation count and rate increased, while fixation spatial coverage decreased as vision worsened. Fully sighted participants showed the most consistent fixation patterns. Peak fixation varied more with increasing impairment.", "conclusion": "Gaze strategies vary along a continuum from fully sighted to fully blind individuals. Adaptive techniques and rehabilitation experiences strongly influence behavior, underscoring the need for custom rehabilitation and assistive tech tailored to residual gaze patterns."}}
{"id": "2509.10710", "pdf": "https://arxiv.org/pdf/2509.10710", "abs": "https://arxiv.org/abs/2509.10710", "authors": ["Sven Schreiber", "Noha Sarhan", "Simone Frintrop", "Christian Wilms"], "title": "SegSLR: Promptable Video Segmentation for Isolated Sign Language Recognition", "categories": ["cs.CV"], "comment": "Accepted at GCPR 2025", "summary": "Isolated Sign Language Recognition (ISLR) approaches primarily rely on RGB\ndata or signer pose information. However, combining these modalities often\nresults in the loss of crucial details, such as hand shape and orientation, due\nto imprecise representations like bounding boxes. Therefore, we propose the\nISLR system SegSLR, which combines RGB and pose information through promptable\nzero-shot video segmentation. Given the rough localization of the hands and the\nsigner's body from pose information, we segment the respective parts through\nthe video to maintain all relevant shape information. Subsequently, the\nsegmentations focus the processing of the RGB data on the most relevant body\nparts for ISLR. This effectively combines RGB and pose information. Our\nevaluation on the complex ChaLearn249 IsoGD dataset shows that SegSLR\noutperforms state-of-the-art methods. Furthermore, ablation studies indicate\nthat SegSLR strongly benefits from focusing on the signer's body and hands,\njustifying our design choices.", "AI": {"tldr": "SegSLR is a new ISLR system that combines RGB data and pose information using video segmentation to maintain hand and body detail. It outperforms state-of-the-art methods.", "motivation": "Current ISLR methods lose critical hand shape and orientation details when combining RGB and pose information. This gap motivated the development of a hybrid approach to retain these details.", "method": "SegSLR uses zero-shot video segmentation to combine pose data for rough hand/body localization with RGB information, focusing on relevant signer's body parts to maintain detail.", "result": "SegSLR surpasses state-of-the-art performance on the ChaLearn249 IsoGD dataset and demonstrates its effectiveness through various ablation studies.", "conclusion": "Focusing on signer's hands and body parts using segmented data improves ISLR systems, validating the design decisions of SegSLR."}}
{"id": "2509.10703", "pdf": "https://arxiv.org/pdf/2509.10703", "abs": "https://arxiv.org/abs/2509.10703", "authors": ["Seonghun Son", "Chandrika Mukherjee", "Reham Mohamed Aburas", "Berk Gulmezoglu", "Z. Berkay Celik"], "title": "Side-channel Inference of User Activities in AR/VR Using GPU Profiling", "categories": ["cs.CR", "cs.AR"], "comment": "Accepted to the 2026 Network and Distributed System Security (NDSS)\n  Symposium", "summary": "Over the past decade, AR/VR devices have drastically changed how we interact\nwith the digital world. Users often share sensitive information, such as their\nlocation, browsing history, and even financial data, within third-party apps\ninstalled on these devices, assuming a secure environment protected from\nmalicious actors. Recent research has revealed that malicious apps can exploit\nsuch capabilities and monitor benign apps to track user activities, leveraging\nfine-grained profiling tools, such as performance counter APIs. However,\napp-to-app monitoring is not feasible on all AR/VR devices (e.g., Meta Quest),\nas a concurrent standalone app execution is disabled. In this paper, we present\nOVRWatcher, a novel side-channel primitive for AR/VR devices that infers user\nactivities by monitoring low-resolution (1Hz) GPU usage via a background\nscript, unlike prior work that relies on high-resolution profiling. OVRWatcher\ncaptures correlations between GPU metrics and 3D object interactions under\nvarying speeds, distances, and rendering scenarios, without requiring\nconcurrent app execution, access to application data, or additional SDK\ninstallations. We demonstrate the efficacy of OVRWatcher in fingerprinting both\nstandalone AR/VR and WebXR applications. OVRWatcher also distinguishes virtual\nobjects, such as products in immersive shopping apps selected by real users and\nthe number of participants in virtual meetings, thereby revealing users'\nproduct preferences and potentially exposing confidential information from\nthose meetings. OVRWatcher achieves over 99% accuracy in app fingerprinting and\nover 98% accuracy in object-level inference.", "AI": {"tldr": "OVRWatcher introduces a side-channel attack exploiting low-resolution GPU metrics to infer user activities and preferences on AR/VR devices without concurrent app execution.", "motivation": "To address security concerns on AR/VR devices where sensitive user information can be exposed through malicious monitoring techniques.", "method": "OVRWatcher monitors low-resolution (1Hz) GPU usage via background scripts to infer user activities and interactions without requiring direct app data access or additional SDKs.", "result": "OVRWatcher achieves over 99% accuracy in app fingerprinting and over 98% accuracy in object-level inference of user activities and preferences.", "conclusion": "OVRWatcher underscores the need for enhanced security measures in AR/VR devices to mitigate the risks posed by side-channel attacks leveraging low-resolution GPU metrics."}}
{"id": "2509.11379", "pdf": "https://arxiv.org/pdf/2509.11379", "abs": "https://arxiv.org/abs/2509.11379", "authors": ["Chen Cheng", "John Duchi"], "title": "Some Robustness Properties of Label Cleaning", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.TH"], "comment": "39 pages", "summary": "We demonstrate that learning procedures that rely on aggregated labels, e.g.,\nlabel information distilled from noisy responses, enjoy robustness properties\nimpossible without data cleaning. This robustness appears in several ways. In\nthe context of risk consistency -- when one takes the standard approach in\nmachine learning of minimizing a surrogate (typically convex) loss in place of\na desired task loss (such as the zero-one mis-classification error) --\nprocedures using label aggregation obtain stronger consistency guarantees than\nthose even possible using raw labels. And while classical statistical scenarios\nof fitting perfectly-specified models suggest that incorporating all possible\ninformation -- modeling uncertainty in labels -- is statistically efficient,\nconsistency fails for ``standard'' approaches as soon as a loss to be minimized\nis even slightly mis-specified. Yet procedures leveraging aggregated\ninformation still converge to optimal classifiers, highlighting how\nincorporating a fuller view of the data analysis pipeline, from collection to\nmodel-fitting to prediction time, can yield a more robust methodology by\nrefining noisy signals.", "AI": {"tldr": "The paper shows that machine learning methods relying on aggregated labels (e.g., labels derived from noisy responses) exhibit superior robustness and consistency compared to using raw labels. This is especially true in settings where task losses are mis-specified.", "motivation": "The paper aims to address the limitations of standard machine learning methods that fail when task losses are slightly mis-specified, offering a methodology that incorporates data aggregation to improve outcomes.", "method": "The approach involves leveraging aggregated label information instead of raw labels to refine noisy signals, ensuring better robustness and consistency in machine learning models.", "result": "The study demonstrates that using aggregated labels leads to stronger consistency guarantees and assures convergence to optimal classifiers, even under loss mis-specification.", "conclusion": "The findings underline the importance of considering the entire data analysis pipeline, from data collection to prediction, highlighting the benefits of incorporating aggregated information for more robust methodologies."}}
{"id": "2509.11512", "pdf": "https://arxiv.org/pdf/2509.11512", "abs": "https://arxiv.org/abs/2509.11512", "authors": ["Tasnuva Chowdhury", "Tadashi Maeno", "Fatih Furkan Akman", "Joseph Boudreau", "Sankha Dutta", "Shengyu Feng", "Adolfy Hoisie", "Kuan-Chieh Hsu", "Raees Khan", "Jaehyung Kim", "Ozgur O. Kilic", "Scott Klasky", "Alexei Klimentov", "Tatiana Korchuganova", "Verena Ingrid Martinez Outschoorn", "Paul Nilsson", "David K. Park", "Norbert Podhorszki", "Yihui Ren", "John Rembrandt Steele", "Fr\u00e9d\u00e9ric Suter", "Sairam Sri Vatsavai", "Torre Wenaus", "Wei Yang", "Yiming Yang", "Shinjae Yoo"], "title": "Machine Learning-Driven Predictive Resource Management in Complex Science Workflows", "categories": ["cs.DC", "cs.AI", "cs.LG", "68T05, 68M14, 68W10"], "comment": null, "summary": "The collaborative efforts of large communities in science experiments, often\ncomprising thousands of global members, reflect a monumental commitment to\nexploration and discovery. Recently, advanced and complex data processing has\ngained increasing importance in science experiments. Data processing workflows\ntypically consist of multiple intricate steps, and the precise specification of\nresource requirements is crucial for each step to allocate optimal resources\nfor effective processing. Estimating resource requirements in advance is\nchallenging due to a wide range of analysis scenarios, varying skill levels\namong community members, and the continuously increasing spectrum of computing\noptions. One practical approach to mitigate these challenges involves initially\nprocessing a subset of each step to measure precise resource utilization from\nactual processing profiles before completing the entire step. While this\ntwo-staged approach enables processing on optimal resources for most of the\nworkflow, it has drawbacks such as initial inaccuracies leading to potential\nfailures and suboptimal resource usage, along with overhead from waiting for\ninitial processing completion, which is critical for fast-turnaround analyses.\nIn this context, our study introduces a novel pipeline of machine learning\nmodels within a comprehensive workflow management system, the Production and\nDistributed Analysis (PanDA) system. These models employ advanced machine\nlearning techniques to predict key resource requirements, overcoming challenges\nposed by limited upfront knowledge of characteristics at each step. Accurate\nforecasts of resource requirements enable informed and proactive\ndecision-making in workflow management, enhancing the efficiency of handling\ndiverse, complex workflows across heterogeneous resources.", "AI": {"tldr": "This paper introduces machine learning models within a workflow management system to predict resource requirements in scientific data processing, improving efficiency and decision-making.", "motivation": "As scientific experiments grow in scale and complexity, managing data processing workflows requires accurate resource allocation, which remains challenging due to variable scenarios, skill levels, and computing options.", "method": "The study presents a novel pipeline of machine learning models integrated into the PanDA workflow management system to predict resource requirements based on limited upfront data.", "result": "The machine learning models accurately forecast resource needs, enabling informed workflow decisions and improving the handling of large, diverse workloads.", "conclusion": "Integrating advanced machine learning models into workflow management systems can overcome challenges of resource allocation, enhancing the efficiency and scalability of scientific data processing workflows."}}
{"id": "2509.10837", "pdf": "https://arxiv.org/pdf/2509.10837", "abs": "https://arxiv.org/abs/2509.10837", "authors": ["Yuyin Lu", "Hegang Chen", "Yanghui Rao"], "title": "From Grounding to Skolemization: A Logic-Constrained Vector Symbolic Architecture for Complex Query Answering", "categories": ["cs.AI"], "comment": null, "summary": "Complex Query Answering (CQA) over incomplete Knowledge Graphs (KGs),\ntypically formalized as reasoning with Existential First-Order predicate logic\nwith one free variable (EFO$_1$), faces a fundamental trade-off between logical\nsoundness and computational efficiency. This work establishes the\nGrounding-Skolemization dichotomy for systematically analyzing CQA methods\nthrough the lens of formal logic. While Grounding-based methods inherently\nsuffer from combinatorial explosion, most Skolemization-based methods neglect\nto explicitly model Skolem functions and compromise logical consistency. To\naddress these limitations, we propose the Logic-constrained Vector Symbolic\nArchitecture (LVSA), a neuro-symbolic framework that unifies a differentiable\nSkolemization module and a neural negator, as well as a logical\nconstraint-driven optimization protocol to harmonize geometric and logical\nrequirements. Theoretically, LVSA guarantees universality for all EFO$_1$\nqueries. Empirically, it outperforms state-of-the-art Skolemization-based\nmethods and reduces inference costs by orders of magnitude compared to\nGrounding-based baselines.", "AI": {"tldr": "The paper introduces LVSA, a neuro-symbolic system, to address the trade-off in Complex Query Answering over Knowledge Graphs, achieving both logical soundness and computational efficiency.", "motivation": "The study aims to tackle the inherent trade-off between logical soundness and computational efficiency in answering complex queries on incomplete Knowledge Graphs.", "method": "It establishes the Grounding-Skolemization dichotomy as a framework, and introduces LVSA, combining a differentiable Skolemization module, a neural negator, and a logical optimization protocol.", "result": "LVSA ensures universality for all EFO$_1$ queries and demonstrates superior performance over Skolemization methods while reducing costs significantly compared to Grounding approaches.", "conclusion": "LVSA harmonizes logic and computation, offering a robust solution for Complex Query Answering across Knowledge Graphs."}}
{"id": "2509.11877", "pdf": "https://arxiv.org/pdf/2509.11877", "abs": "https://arxiv.org/abs/2509.11877", "authors": ["Andrei Arusoaie", "Hora\u0163iu Cheval", "Radu Iosif"], "title": "Proceedings 9th edition of Working Formal Methods Symposium", "categories": ["cs.LO", "cs.PL", "cs.SE"], "comment": null, "summary": "This volume contains the proceedings of the 9th Working Formal Methods\nSymposium, which was held at the Alexandru Ioan Cuza University, Ia\\c{s}i,\nRomania on September 17-19, 2025.", "AI": {"tldr": "This document reports on the 9th Working Formal Methods Symposium held in Romania in 2025.", "motivation": "To document and share findings, discussions, and advancements presented during the symposium.", "method": "Compilation of proceedings from the symposium event held at Alexandru Ioan Cuza University.", "result": "The 9th Working Formal Methods Symposium proceedings were successfully compiled and made available.", "conclusion": "The symposium provided valuable contributions to the field of formal methods, demonstrating ongoing scholarly engagement."}}
{"id": "2509.10506", "pdf": "https://arxiv.org/pdf/2509.10506", "abs": "https://arxiv.org/abs/2509.10506", "authors": ["Muxin Ge", "Hanyu Ma", "Yiyang Wu", "Xiaoli Ma", "Yadi Liu", "Ye Aung Moe", "Weizheng Xie"], "title": "AttnBoost: Retail Supply Chain Sales Insights via Gradient Boosting Perspective", "categories": ["cs.LG", "cs.CE"], "comment": null, "summary": "Forecasting product demand in retail supply chains presents a complex\nchallenge due to noisy, heterogeneous features and rapidly shifting consumer\nbehavior. While traditional gradient boosting decision trees (GBDT) offer\nstrong predictive performance on structured data, they often lack adaptive\nmechanisms to identify and emphasize the most relevant features under changing\nconditions. In this work, we propose AttnBoost, an interpretable learning\nframework that integrates feature-level attention into the boosting process to\nenhance both predictive accuracy and explainability. Specifically, the model\ndynamically adjusts feature importance during each boosting round via a\nlightweight attention mechanism, allowing it to focus on high-impact variables\nsuch as promotions, pricing, and seasonal trends. We evaluate AttnBoost on a\nlarge-scale retail sales dataset and demonstrate that it outperforms standard\nmachine learning and deep tabular models, while also providing actionable\ninsights for supply chain managers. An ablation study confirms the utility of\nthe attention module in mitigating overfitting and improving interpretability.\nOur results suggest that attention-guided boosting represents a promising\ndirection for interpretable and scalable AI in real-world forecasting\napplications.", "AI": {"tldr": "The paper introduces AttnBoost, a gradient boosting model enhanced with attention mechanisms to tackle product demand forecasting challenges in retail.", "motivation": "Forecasting product demand in retail is difficult due to noisy data, rapidly changing consumer behavior, and the need for adaptive mechanisms in predictive models.", "method": "The authors enhance gradient boosting decision trees (GBDT) by incorporating feature-level attention, enabling dynamic adjustment of feature importance during the boosting rounds.", "result": "AttnBoost outperforms standard machine learning and deep learning models on retail sales data by improving predictive accuracy and interpretability. Ablation studies highlight its ability to reduce overfitting and provide actionable insights.", "conclusion": "Attention-guided boosting, as demonstrated by AttnBoost, offers a scalable and interpretable solution for complex forecasting challenges in retail supply chains."}}
{"id": "2509.10862", "pdf": "https://arxiv.org/pdf/2509.10862", "abs": "https://arxiv.org/abs/2509.10862", "authors": ["Temma Suzuki", "Kento Kawaharazuka", "Kei Okada"], "title": "A Universal Wire Testing Machine for Enhancing the Performance of Wire-Driven Robots", "categories": ["cs.RO"], "comment": "Accepted at Humanoids2025, website -\n  https://tenrobo18.github.io/wiretester-humanoids2025-webpage/", "summary": "Compared with gears and linkages, wires constitute a lightweight,\nlow-friction transmission mechanism. However, because wires are flexible\nmaterials, they tend to introduce large modeling errors, and their adoption in\nindustrial and research robots remains limited.In this study, we built a\nUniversal Wire Testing Machine that enables measurement and adjustment of wire\ncharacteristics to improve the performance of wire-driven mechanisms. Using\nthis testing machine, we carried out removal of initial wire stretch,\nmeasurement of tension transmission efficiency for eight different diameters of\npassive pulleys, and measurement of the dynamic behavior of variable-length\nwires. Finally, we applied the data obtained from this testing machine to the\nforce control of an actual wire-driven robot, reducing the end-effector force\nerror.", "AI": {"tldr": "The paper introduces a Universal Wire Testing Machine to analyze and improve the performance of wire-driven robotic mechanisms through measurements and adjustments of wire characteristics.", "motivation": "To address the issue of large modeling errors in wire-driven mechanisms, which limits their adoption in industrial and research robotics.", "method": "Development of a Universal Wire Testing Machine used to measure key wire characteristics such as initial wire stretch, tension transmission efficiency with passive pulleys, and dynamic behavior of variable-length wires.", "result": "Data measured using the testing machine improved force control in a wire-driven robot, significantly reducing the end-effector force error.", "conclusion": "Utilizing precise wire characteristic measurements enhances the performance of wire-driven robotic systems."}}
{"id": "2509.10708", "pdf": "https://arxiv.org/pdf/2509.10708", "abs": "https://arxiv.org/abs/2509.10708", "authors": ["Iman Barati", "Mostafa Amiri", "Heshaam Faili"], "title": "SearchInstruct: Enhancing Domain Adaptation via Retrieval-Based Instruction Dataset Creation", "categories": ["cs.CL"], "comment": null, "summary": "Supervised Fine-Tuning (SFT) is essential for training large language models\n(LLMs), significantly enhancing critical capabilities such as instruction\nfollowing and in-context learning. Nevertheless, creating suitable training\ndatasets tailored for specific domains remains challenging due to unique domain\nconstraints and data scarcity. In this paper, we propose SearchInstruct, an\ninnovative method explicitly designed to construct high quality instruction\ndatasets for SFT. Our approach begins with a limited set of domain specific,\nhuman generated questions, which are systematically expanded using a large\nlanguage model. Subsequently, domain relevant resources are dynamically\nretrieved to generate accurate and contextually appropriate answers for each\naugmented question. Experimental evaluation demonstrates that SearchInstruct\nenhances both the diversity and quality of SFT datasets, leading to measurable\nimprovements in LLM performance within specialized domains. Additionally, we\nshow that beyond dataset generation, the proposed method can also effectively\nfacilitate tasks such as model editing, enabling efficient updates to existing\nmodels. To facilitate reproducibility and community adoption, we provide full\nimplementation details, the complete set of generated instruction response\npairs, and the source code in a publicly accessible Git repository:\n[https://github.com/mostafaamiri/SearchInstruct](https://github.com/mostafaamiri/SearchInstruct)", "AI": {"tldr": "This paper introduces SearchInstruct, a method for creating high-quality instruction datasets for supervised fine-tuning (SFT) of language models, targeting challenges in domain-specific data creation.", "motivation": "The work addresses the difficulty of producing tailored datasets for specific domains in supervised fine-tuning, especially due to challenges like domain constraints and data scarcity.", "method": "The authors propose to start with limited domain-specific human-generated questions, expand them using a large language model, and dynamically retrieve relevant resources to generate accurate answers.", "result": "SearchInstruct improves the diversity and quality of SFT datasets, leading to measurable gains in language model performance in specialized domains.", "conclusion": "The method not only enhances training datasets but also supports model tasks like editing, and its full implementation details and resources are made publicly available for community use."}}
{"id": "2509.11132", "pdf": "https://arxiv.org/pdf/2509.11132", "abs": "https://arxiv.org/abs/2509.11132", "authors": ["Xiaoyu Zhang", "Weipeng Jiang", "Juan Zhai", "Shiqing Ma", "Qingshuang Bao", "Chenhao Lin", "Chao Shen", "Tianlin Li", "Yang Liu"], "title": "Rethinking Technology Stack Selection with AI Coding Proficiency", "categories": ["cs.SE"], "comment": "23 pages", "summary": "Large language models (LLMs) are now an integral part of software development\nworkflows and are reshaping the whole process. Traditional technology stack\nselection has not caught up. Most of the existing selection methods focus\nsolely on the inherent attributes of the technology, overlooking whether the\nLLM can effectively leverage the chosen technology. For example, when\ngenerating code snippets using popular libraries like Selenium (one of the most\nwidely used test automation tools with over 33k GitHub stars), existing LLMs\nfrequently generate low-quality code snippets (e.g., using deprecated APIs and\nmethods, or containing syntax errors). As such, teams using LLM assistants risk\nchoosing technologies that cannot be used effectively by LLMs, yielding high\ndebugging effort and mounting technical debt. We foresee a practical question\nin the LLM era, is a technology ready for AI-assisted development? In this\npaper, we first propose the concept, AI coding proficiency, the degree to which\nLLMs can utilize a given technology to generate high-quality code snippets. We\nconduct the first comprehensive empirical study examining AI proficiency across\n170 third-party libraries and 61 task scenarios, evaluating six widely used\nLLMs. Our findings reveal that libraries with similar functionalities can\nexhibit up to 84% differences in the quality score of LLM-generated code, while\ndifferent models also exhibit quality gaps among their generation results using\nthe same library. These gaps translate into real engineering costs and can\nsteer developer choices toward a narrow set of libraries with high AI coding\nproficiency, threatening technological diversity in the ecosystem. We call on\nthe community to integrate AI proficiency assessments into technology selection\nframeworks and develop mitigation strategies, preserving competitive balance in\nAI-driven development.", "AI": {"tldr": "The paper introduces 'AI coding proficiency' as a metric to evaluate whether LLMs can effectively generate high-quality code using third-party libraries, potentially influencing technology selection in software development.", "motivation": "Traditional methods for selecting software technologies often overlook whether LLMs can effectively use them to create high-quality code. Poor integration between LLMs and certain technologies results in technical inefficiencies.", "method": "The authors conducted an empirical study, analyzing 170 third-party libraries and 61 coding tasks to measure AI coding proficiency across six LLMs.", "result": "The study revealed significant performance disparities among libraries and LLMs, with some libraries showing up to 84% differences in code quality scores. These result in additional engineering costs and potential narrowing of technology choices.", "conclusion": "The paper advocates for incorporating AI coding proficiency into technology selection practices to safeguard ecosystem diversity and maintain balanced AI-driven software development."}}
{"id": "2509.11545", "pdf": "https://arxiv.org/pdf/2509.11545", "abs": "https://arxiv.org/abs/2509.11545", "authors": ["Zhuda Yang", "Junhao Liang", "Wing Ho Yung", "Changsong Zhou"], "title": "Representational drift under spontaneous activity -- self-organized criticality enhances representational reliability", "categories": ["q-bio.NC"], "comment": null, "summary": "Neural systems face the challenge of maintaining reliable representations\namid variations from plasticity and spontaneous activity. In particular, the\nspontaneous dynamics in neuronal circuit is known to operate near a highly\nvariable critical state, which intuitively contrasts with the requirement of\nreliable representation. It is intriguing to understand how reliable\nrepresentation could be maintained or even enhanced by critical spontaneous\nstates. We firstly examined the co-existence of the scale-free avalanche in the\nspontaneous activity of mouse visual cortex with restricted representational\ngeometry manifesting representational reliability amid the representational\ndrift with respect to the visual stimulus. To explore how critical spontaneous\nstate influences the neural representation, we built an excitation-inhibition\nnetwork with homeostatic plasticity, which self-organizes to the critical\nspontaneous state. This model successfully reproduced both representational\ndrift and restricted representational geometry observed experimentally, in\ncontrast with randomly shuffled plasticity which causes accumulated drift of\nrepresentational geometry. We further showed that the self-organized critical\nstate enhances the cross-session low-dimensional representation, comparing to\nthe non-critical state, by restricting the synapse weight into a low variation\nspace. Our findings suggest that spontaneous self-organized criticality serves\nnot only as a ubiquitous property of neural systems but also as a functional\nmechanism for maintaining reliable information representation under\ncontinuously changing networks, providing a potential explanation how the brain\nmaintains consistent perception and behavior despite ongoing synaptic rewiring.", "AI": {"tldr": "Examines how critical spontaneous states in neural systems enhance reliable information representation amid changes caused by plasticity and spontaneous activity.", "motivation": "To address how neural systems maintain reliable representations despite the variability introduced by spontaneous critical states and synaptic changes.", "method": "Used experimental observations from mouse visual cortex combined with a computational excitation-inhibition network model with homeostatic plasticity to investigate representational reliability.", "result": "The model reproduced experimental findings showing representational drift coupled with restricted representational geometry and demonstrated enhanced cross-session representation in the critical state compared to non-critical states due to low synapse weight variations.", "conclusion": "Critical spontaneous states serve as a functional mechanism enabling reliable information representation in dynamically changing neural networks, which explains stable perception and behavior despite ongoing synaptic rewiring."}}
{"id": "2509.10748", "pdf": "https://arxiv.org/pdf/2509.10748", "abs": "https://arxiv.org/abs/2509.10748", "authors": ["Jecia Z. Y. Mao", "Francis X Creighton", "Russell H Taylor", "Manish Sahu"], "title": "SCOPE: Speech-guided COllaborative PErception Framework for Surgical Scene Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Accurate segmentation and tracking of relevant elements of the surgical scene\nis crucial to enable context-aware intraoperative assistance and decision\nmaking. Current solutions remain tethered to domain-specific, supervised models\nthat rely on labeled data and required domain-specific data to adapt to new\nsurgical scenarios and beyond predefined label categories. Recent advances in\nprompt-driven vision foundation models (VFM) have enabled open-set, zero-shot\nsegmentation across heterogeneous medical images. However, dependence of these\nmodels on manual visual or textual cues restricts their deployment in\nintroperative surgical settings. We introduce a speech-guided collaborative\nperception (SCOPE) framework that integrates reasoning capabilities of large\nlanguage model (LLM) with perception capabilities of open-set VFMs to support\non-the-fly segmentation, labeling and tracking of surgical instruments and\nanatomy in intraoperative video streams. A key component of this framework is a\ncollaborative perception agent, which generates top candidates of VFM-generated\nsegmentation and incorporates intuitive speech feedback from clinicians to\nguide the segmentation of surgical instruments in a natural human-machine\ncollaboration paradigm. Afterwards, instruments themselves serve as interactive\npointers to label additional elements of the surgical scene. We evaluated our\nproposed framework on a subset of publicly available Cataract1k dataset and an\nin-house ex-vivo skull-base dataset to demonstrate its potential to generate\non-the-fly segmentation and tracking of surgical scene. Furthermore, we\ndemonstrate its dynamic capabilities through a live mock ex-vivo experiment.\nThis human-AI collaboration paradigm showcase the potential of developing\nadaptable, hands-free, surgeon-centric tools for dynamic operating-room\nenvironments.", "AI": {"tldr": "The paper introduces SCOPE, a speech-guided framework that combines large language models with vision foundation models for adaptable segmentation and tracking of surgical scenes.", "motivation": "To overcome the limitations of domain-specific models that rely on predefined labels and labeled data, and enable dynamic and adaptable segmentation and tracking of surgical scenes in real-time.", "method": "The SCOPE framework uses large language models for reasoning and vision foundation models for perception, incorporating speech-guided intuitive feedback for collaborative segmentation and tracking during surgery.", "result": "The framework was successfully evaluated on publicly available and in-house datasets, showcasing its ability to provide on-the-fly segmentation and tracking. Dynamic capabilities were further demonstrated through a live mock experiment.", "conclusion": "SCOPE demonstrates the potential for developing hands-free, surgeon-centric tools for adaptable and dynamic surgical environments through human-AI collaboration."}}
{"id": "2509.11435", "pdf": "https://arxiv.org/pdf/2509.11435", "abs": "https://arxiv.org/abs/2509.11435", "authors": ["Kisung You"], "title": "A Particle-Flow Algorithm for Free-Support Wasserstein Barycenters", "categories": ["stat.ML", "cs.LG", "stat.CO"], "comment": null, "summary": "The Wasserstein barycenter extends the Euclidean mean to the space of\nprobability measures by minimizing the weighted sum of squared 2-Wasserstein\ndistances. We develop a free-support algorithm for computing Wasserstein\nbarycenters that avoids entropic regularization and instead follows the formal\nRiemannian geometry of Wasserstein space. In our approach, barycenter atoms\nevolve as particles advected by averaged optimal-transport displacements, with\nbarycentric projections of optimal transport plans used in place of Monge maps\nwhen the latter do not exist. This yields a geometry-aware particle-flow update\nthat preserves sharp features of the Wasserstein barycenter while remaining\ncomputationally tractable. We establish theoretical guarantees, including\nconsistency of barycentric projections, monotone descent and convergence to\nstationary points, stability with respect to perturbations of the inputs, and\nresolution consistency as the number of atoms increases. Empirical studies on\naveraging probability distributions, Bayesian posterior aggregation, image\nprototypes and classification, and large-scale clustering demonstrate accuracy\nand scalability of the proposed particle-flow approach, positioning it as a\nprincipled alternative to both linear programming and regularized solvers.", "AI": {"tldr": "The paper introduces a new free-support algorithm for computing Wasserstein barycenters using geometry-aware particle-flow updates, avoiding entropic regularization, and ensuring theoretical guarantees.", "motivation": "To address limitations of existing methods for computing Wasserstein barycenters, which often rely on entropic regularization, leading to loss of sharp features.", "method": "A particle-flow algorithm based on the Riemannian geometry of Wasserstein space, using averaged optimal-transport displacements and barycentric projections for updates.", "result": "The proposed method is theoretically grounded with guarantees such as consistency, monotone descent, and stability. It demonstrates scalability and accuracy in various applications, including probability distribution averaging, image classification, and clustering.", "conclusion": "The algorithm offers a computationally efficient, scalable, and principled approach to computing Wasserstein barycenters, providing an alternative to traditional solvers."}}
{"id": "2509.11697", "pdf": "https://arxiv.org/pdf/2509.11697", "abs": "https://arxiv.org/abs/2509.11697", "authors": ["Cheng Zhang", "Wan-Lei Zhao", "Shihai Xiao", "Jiajie Yao", "Xuecang Zhang"], "title": "Towards the Distributed Large-scale k-NN Graph Construction by Graph Merge", "categories": ["cs.DC"], "comment": "14 pages, 14 figures", "summary": "In order to support the real-time interaction with LLMs and the instant\nsearch or the instant recommendation on social media, it becomes an imminent\nproblem to build k-NN graph or indexing graph for the massive number of\nvectorized multimedia data. In such scenarios, the scale of the data or the\nscale of the graph may exceed the processing capacity of a single machine. This\npaper aims to address the graph construction problem of such scale via\nefficient graph merge. For the graph construction on a single node, two generic\nand highly parallelizable algorithms, namely Two-way Merge and Multi-way Merge\nare proposed to merge subgraphs into one. For the graph construction across\nmultiple nodes, a multi-node procedure based on Two-way Merge is presented. The\nprocedure makes it feasible to construct a large-scale k-NN graph/indexing\ngraph on either a single node or multiple nodes when the data size exceeds the\nmemory capacity of one node. Extensive experiments are conducted on both\nlarge-scale k-NN graph and indexing graph construction. For the k-NN graph\nconstruction, the large-scale and high-quality k-NN graphs are constructed by\ngraph merge in parallel. Typically, a billion-scale k-NN graph can be built in\napproximately 17h when only three nodes are employed. For the indexing graph\nconstruction, similar NN search performance as the original indexing graph is\nachieved with the merged indexing graphs while requiring much less time of\nconstruction.", "AI": {"tldr": "This paper introduces scalable graph construction methods, Two-way Merge and Multi-way Merge, for building k-NN graphs and indexing graphs in a distributed manner.", "motivation": "To solve the challenge of constructing k-NN or indexing graphs for massive multimedia data that exceeds the capacity of a single machine, ensuring support for real-time interactions and instant search/recommendation.", "method": "The paper proposes two parallelizable algorithms, Two-way Merge and Multi-way Merge, for merging subgraphs on a single node. It further extends these methods for distributed graph construction across multiple nodes.", "result": "Experiments demonstrate billion-scale k-NN graph construction in approximately 17 hours using three nodes, and indexing graph construction achieving comparable NN search performance while requiring significantly less time.", "conclusion": "The presented approach enables efficient, scalable large-scale graph construction, supporting real-time applications and overcoming memory capacity limitations."}}
{"id": "2509.10875", "pdf": "https://arxiv.org/pdf/2509.10875", "abs": "https://arxiv.org/abs/2509.10875", "authors": ["Jesse Gardner", "Vladimir A. Baulin"], "title": "Is the `Agent' Paradigm a Limiting Framework for Next-Generation Intelligent Systems?", "categories": ["cs.AI", "cond-mat.soft"], "comment": null, "summary": "The concept of the 'agent' has profoundly shaped Artificial Intelligence (AI)\nresearch, guiding development from foundational theories to contemporary\napplications like Large Language Model (LLM)-based systems. This paper\ncritically re-evaluates the necessity and optimality of this agent-centric\nparadigm. We argue that its persistent conceptual ambiguities and inherent\nanthropocentric biases may represent a limiting framework. We distinguish\nbetween agentic systems (AI inspired by agency, often semi-autonomous, e.g.,\nLLM-based agents), agential systems (fully autonomous, self-producing systems,\ncurrently only biological), and non-agentic systems (tools without the\nimpression of agency). Our analysis, based on a systematic review of relevant\nliterature, deconstructs the agent paradigm across various AI frameworks,\nhighlighting challenges in defining and measuring properties like autonomy and\ngoal-directedness. We argue that the 'agentic' framing of many AI systems,\nwhile heuristically useful, can be misleading and may obscure the underlying\ncomputational mechanisms, particularly in Large Language Models (LLMs). As an\nalternative, we propose a shift in focus towards frameworks grounded in\nsystem-level dynamics, world modeling, and material intelligence. We conclude\nthat investigating non-agentic and systemic frameworks, inspired by complex\nsystems, biology, and unconventional computing, is essential for advancing\ntowards robust, scalable, and potentially non-anthropomorphic forms of general\nintelligence. This requires not only new architectures but also a fundamental\nreconsideration of our understanding of intelligence itself, moving beyond the\nagent metaphor.", "AI": {"tldr": "The paper critiques the agent-centric paradigm in AI, highlighting its ambiguities and biases, and proposes non-agentic and systemic frameworks as alternatives for robust general intelligence.", "motivation": "The paper aims to challenge the prevailing agent-centric approach in AI research, which is deemed limiting due to its anthropocentric biases and conceptual ambiguities.", "method": "The researchers conducted a systematic review of literature to contrast agentic, agential, and non-agentic systems, analyzing the challenges in defining properties such as autonomy and goal-directedness.", "result": "Key findings indicate that the agentic framing of AI systems, especially LLMs, can obscure computational mechanisms and mislead conceptual understanding.", "conclusion": "Shifting focus from agent-centric frameworks to system-level dynamics and material intelligence inspired by complex systems and biology is necessary for advancing scalable, non-anthropomorphic general intelligence."}}
{"id": "2509.10509", "pdf": "https://arxiv.org/pdf/2509.10509", "abs": "https://arxiv.org/abs/2509.10509", "authors": ["Sai Teja Reddy Adapala"], "title": "The Anti-Ouroboros Effect: Emergent Resilience in Large Language Models from Recursive Selective Feedback", "categories": ["cs.LG", "cs.AI", "I.2.6; I.2.7"], "comment": "5 pages, 3 figures, 2 tables. Code is available at:\n  https://github.com/imsaitejareddy/ouroboros-effect-experiment", "summary": "The stability of recursively trained large language models (LLMs) is a\nfoundational problem for AI safety. Prevailing theory predicts model collapse,\na progressive degradation when models are trained on their own output. We\nchallenge this narrative by introducing a selective feedback mechanism.\nContrary to expectation, instead of merely slowing decay, our experiments\nprovide strong evidence that this pressure reverses it, inducing a\nstatistically significant performance improvement in a Gemma 2B model on a\ncomplex summarization task. We name this phenomenon the Anti-Ouroboros Effect.\nWe contrast this with a foundational experiment using a simple classifier,\nwhere the theoretical degenerative loop was validated, highlighting the unique\ndynamics of high-dimensional models. Our findings establish that systemic\nresilience can be an emergent property of LLMs under simple selection pressure,\nsuggesting a powerful and scalable principle for developing safer and more\nrobust AI systems. Across five generations, a quality-filtered condition\nimproved by 6.6% in ROUGE-L F1 score, whereas an unfiltered control degraded by\n3.5% and a random-filter control degraded by 4.2%", "AI": {"tldr": "The study explores model collapse in large language models and introduces the \"Anti-Ouroboros Effect,\" showing that selective feedback can improve model performance instead of degrading it.", "motivation": "The paper investigates model collapse, where recursively trained LLMs degrade, and aims to find a mechanism to prevent or reverse this issue for AI safety.", "method": "The researchers applied a selective feedback mechanism during recursive training of a Gemma 2B model on summarization tasks, contrasting results with experiments on simpler models.", "result": "The Gemma 2B model showed a performance improvement of 6.6% in ROUGE-L F1 under quality-filtered conditions, whereas controls experienced performance degradation.", "conclusion": "Selective feedback can induce systemic resilience in LLMs, suggesting a scalable strategy for developing safer and more robust AI systems."}}
{"id": "2509.10884", "pdf": "https://arxiv.org/pdf/2509.10884", "abs": "https://arxiv.org/abs/2509.10884", "authors": ["Qingxiang Liu", "Ting Huang", "Zeyu Zhang", "Hao Tang"], "title": "Nav-R1: Reasoning and Navigation in Embodied Scenes", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Embodied navigation requires agents to integrate perception, reasoning, and\naction for robust interaction in complex 3D environments. Existing approaches\noften suffer from incoherent and unstable reasoning traces that hinder\ngeneralization across diverse environments, and difficulty balancing\nlong-horizon semantic reasoning with low-latency control for real-time\nnavigation. To address these challenges, we propose Nav-R1, an embodied\nfoundation model that unifies reasoning in embodied environments. We first\nconstruct Nav-CoT-110K, a large-scale dataset of step-by-step Chains-of-Thought\n(CoT) for embodied tasks, which enables cold-start initialization with\nstructured reasoning. Building on this foundation, we design a GRPO-based\nreinforcement learning framework with three complementary rewards: format,\nunderstanding, and navigation, to improve structural adherence, semantic\ngrounding, and path fidelity. Furthermore, we introduce a Fast-in-Slow\nreasoning paradigm, decoupling deliberate semantic reasoning from low-latency\nreactive control for efficient yet coherent navigation. Extensive evaluations\non embodied AI benchmarks demonstrate that Nav-R1 consistently outperforms\nstrong baselines, with over 8% average improvement in reasoning and navigation\nperformance. Real-world deployment on a mobile robot further validates its\nrobustness under limited onboard resources. Code:\nhttps://github.com/AIGeeksGroup/Nav-R1. Website:\nhttps://aigeeksgroup.github.io/Nav-R1.", "AI": {"tldr": "This paper introduces Nav-R1, an embodied foundation model designed for effective 3D environment navigation by integrating structured reasoning and efficient control mechanisms.", "motivation": "Embodied navigation systems face challenges of incoherent reasoning and balancing long-horizon semantic reasoning with reactive control for real-time navigation.", "method": "The authors create Nav-CoT-110K, a step-by-step reasoning dataset, and a GRPO-based reinforcement learning framework with three rewards for structured reasoning and robust navigation. They also adopt a Fast-in-Slow paradigm to separate semantic reasoning from reactive navigation.", "result": "Nav-R1 outperform strong baselines in embodied AI benchmarks, achieving over 8% improvement in reasoning and navigation. It also demonstrates robustness during real-world mobile robot deployment.", "conclusion": "Nav-R1 bridges reasoning and navigation in embodied AI tasks with a novel dataset, learning framework, and reasoning paradigm, showing strong results both in benchmarks and practical applications."}}
{"id": "2509.10737", "pdf": "https://arxiv.org/pdf/2509.10737", "abs": "https://arxiv.org/abs/2509.10737", "authors": ["Zaur Gouliev", "Jennifer Waters", "Chengqian Wang"], "title": "PolyTruth: Multilingual Disinformation Detection using Transformer-Based Language Models", "categories": ["cs.CL", "cs.LG", "68T50, 68T07", "I.2.7; H.3.3"], "comment": "11 pages, 5 figures, 4 tables. Submitted to arXiv in Computation and\n  Language", "summary": "Disinformation spreads rapidly across linguistic boundaries, yet most AI\nmodels are still benchmarked only on English. We address this gap with a\nsystematic comparison of five multilingual transformer models: mBERT, XLM,\nXLM-RoBERTa, RemBERT, and mT5 on a common fake-vs-true machine learning\nclassification task. While transformer-based language models have demonstrated\nnotable success in detecting disinformation in English, their effectiveness in\nmultilingual contexts still remains up for debate. To facilitate evaluation, we\nintroduce PolyTruth Disinfo Corpus, a novel corpus of 60,486 statement pairs\n(false claim vs. factual correction) spanning over twenty five languages that\ncollectively cover five language families and a broad topical range from\npolitics, health, climate, finance, and conspiracy, half of which are\nfact-checked disinformation claims verified by an augmented MindBugs Discovery\ndataset. Our experiments revealed performance variations. Models such as\nRemBERT achieved better overall accuracy, particularly excelling in\nlow-resource languages, whereas models like mBERT and XLM exhibit considerable\nlimitations when training data is scarce. We provide a discussion of these\nperformance patterns and implications for real-world deployment. The dataset is\npublicly available on our GitHub repository to encourage further\nexperimentation and advancement. Our findings illuminate both the potential and\nthe current limitations of AI systems for multilingual disinformation\ndetection.", "AI": {"tldr": "The paper evaluates multilingual transformer models' effectiveness in detecting disinformation across 25 languages using a novel corpus. Models like RemBERT excel in low-resource languages, whereas mBERT and XLM struggle.", "motivation": "The spread of disinformation across languages highlights the need for multilingual AI models, as existing solutions mainly benchmark results in English.", "method": "Researchers introduced the PolyTruth Disinfo Corpus containing statement pairs in 25 languages to compare five multilingual transformer models on a fake-vs-true classification task.", "result": "RemBERT outperformed others in overall accuracy and low-resource languages, while mBERT and XLM showed limitations in handling scarce training data.", "conclusion": "The study emphasizes the promise and limitations of current multilingual transformer models for combating disinformation and calls for further exploration using the publicly available dataset."}}
{"id": "2509.11238", "pdf": "https://arxiv.org/pdf/2509.11238", "abs": "https://arxiv.org/abs/2509.11238", "authors": ["Dongming Jin", "Zhi Jin", "Yiran Zhang", "Zheng Fang", "Linyu Li", "Yuanpeng He", "Xiaohong Chen", "Weisong Sun"], "title": "UserTrace: User-Level Requirements Generation and Traceability Recovery from Software Project Repositories", "categories": ["cs.SE"], "comment": "21page, 5 figures", "summary": "Software maintainability critically depends on high-quality requirements\ndescriptions and explicit traceability between requirements and code. Although\nautomated code summarization (ACS) and requirements traceability (RT)\ntechniques have been widely studied, existing ACS methods mainly generate\nimplementation-level (i.e., developer-oriented) requirements (IRs) for\nfine-grained units (e.g., methods), while RT techniques often overlook the\nimpact of project evolution. As a result, user-level (i.e., end user-oriented)\nrequirements (URs) and live trace links remain underexplored, despite their\nimportance for supporting user understanding and for validating whether\nAI-generated software aligns with user intent. To address this gap, we propose\nUserTrace, a multi-agent system that automatically generates URs and recovers\nlive trace links (from URs to IRs to code) from software repositories.\nUserTrace coordinates four specialized agents (i.e., Code Reviewer, Searcher,\nWriter, and Verifier) through a three-phase process: structuring repository\ndependencies, deriving IRs for code units, and synthesizing URs with\ndomain-specific context. Our comparative evaluation shows that UserTrace\nproduces URs with higher completeness, correctness, and helpfulness than an\nestablished baseline, and achieves superior precision in trace link recovery\ncompared to five state-of-the-art RT approaches. A user study further\ndemonstrates that UserTrace helps end users validate whether the AI-generated\nrepositories align with their intent.", "AI": {"tldr": "UserTrace, a multi-agent system, addresses gaps in generating user-level requirements and ensures live traceability between requirements, code, and their evolution.", "motivation": "Existing methods focus on implementation-level requirements and overlook user-level requirements and evolution traceability, affecting validation of AI-generated software.", "method": "UserTrace employs four specialized agents (Code Reviewer, Searcher, Writer, Verifier) in a three-phase process: structuring repository dependencies, deriving implementation-level requirements, and synthesizing user-level requirements.", "result": "Evaluation shows UserTrace improves completeness, correctness, and helpfulness of user-level requirements and achieves better trace link precision than RT benchmarks.", "conclusion": "UserTrace enhances end-user validation of AI-generated software and strengthens traceability, addressing critical deficiencies in current techniques."}}
{"id": "2509.12162", "pdf": "https://arxiv.org/pdf/2509.12162", "abs": "https://arxiv.org/abs/2509.12162", "authors": ["Aymen Balti", "Assane Wade", "Abdelatif Oujbara", "M. A.", "Aziz-Alaoui", "Hicham Bellarabi", "Frederic Dutertre", "Benjamin Ambrosio"], "title": "Quantifying Mental States in Work Environment: Mathematical Perspectives", "categories": ["q-bio.NC"], "comment": null, "summary": "This article presents a study involving 87 participants exposed to a\nstressful scenario in a virtual reality (VR) environment. An algorithm was\ndeveloped to assign a positive or negative valence based on questionnaire\nresponses. EEG signals were recorded, and a k-nearest neighbors (KNN) algorithm\nwas trained to infer emotional valence from these signals. Our objective is to\nfurther develop mathematical models capable of describing the dynamic evolution\nof emotional and mental states.", "AI": {"tldr": "This study uses VR and EEG signals analyzed by a KNN algorithm to infer emotional valence during stress scenarios.", "motivation": "The study aims to improve understanding and modeling of emotional and mental state dynamics under stress.", "method": "87 participants were exposed to a VR stress scenario. Emotional valence was derived from questionnaires, and EEG data was analyzed using a KNN algorithm.", "result": "The KNN algorithm was successfully trained to infer emotional valence from EEG data.", "conclusion": "This research supports the development of mathematical models for dynamic emotional and mental states, leveraging technology like EEG and VR."}}
{"id": "2509.10759", "pdf": "https://arxiv.org/pdf/2509.10759", "abs": "https://arxiv.org/abs/2509.10759", "authors": ["Yi-Ruei Liu", "You-Zhe Xie", "Yu-Hsiang Hsu", "I-Sheng Fang", "Yu-Lun Liu", "Jun-Cheng Chen"], "title": "Every Camera Effect, Every Time, All at Once: 4D Gaussian Ray Tracing for Physics-based Camera Effect Data Generation", "categories": ["cs.CV"], "comment": null, "summary": "Common computer vision systems typically assume ideal pinhole cameras but\nfail when facing real-world camera effects such as fisheye distortion and\nrolling shutter, mainly due to the lack of learning from training data with\ncamera effects. Existing data generation approaches suffer from either high\ncosts, sim-to-real gaps or fail to accurately model camera effects. To address\nthis bottleneck, we propose 4D Gaussian Ray Tracing (4D-GRT), a novel two-stage\npipeline that combines 4D Gaussian Splatting with physically-based ray tracing\nfor camera effect simulation. Given multi-view videos, 4D-GRT first\nreconstructs dynamic scenes, then applies ray tracing to generate videos with\ncontrollable, physically accurate camera effects. 4D-GRT achieves the fastest\nrendering speed while performing better or comparable rendering quality\ncompared to existing baselines. Additionally, we construct eight synthetic\ndynamic scenes in indoor environments across four camera effects as a benchmark\nto evaluate generated videos with camera effects.", "AI": {"tldr": "The paper introduces 4D-GRT, a method combining Gaussian Splatting with ray tracing to simulate camera effects. It outperforms existing methods in speed and quality for real-world camera effect rendering.", "motivation": "Common vision systems fail with real-world camera effects due to insufficient training data or ineffective modeling approaches.", "method": "4D-GRT utilizes a two-stage pipeline: reconstructing dynamic scenes using 4D Gaussian Splatting, then generating videos with physically accurate camera effects via ray tracing.", "result": "4D-GRT demonstrates superior or equal rendering quality while achieving the fastest rendering speed compared to baseline methods.", "conclusion": "4D-GRT effectively models real-world camera effects, providing a benchmark for evaluating dynamic scenes under different camera conditions."}}
{"id": "2509.10934", "pdf": "https://arxiv.org/pdf/2509.10934", "abs": "https://arxiv.org/abs/2509.10934", "authors": ["Tiancheng Xu", "Alan L. Cox", "Scott Rixner"], "title": "Design and accuracy trade-offs in Computational Statistics", "categories": ["math.NA", "cs.AR", "cs.NA"], "comment": "Quantitative analysis of using posits versus binary64 in log-space in\n  the context of statistical bioinformatics. Published at 2025 IEEE IISWC\n  (https://iiswc.org/iiswc2025/)", "summary": "Statistical computations are becoming increasingly important. These\ncomputations often need to be performed in log-space because probabilities\nbecome extremely small due to repeated multiplications. While using logarithms\neffectively prevents numerical underflow, this paper shows that its cost is\nhigh in performance, resource utilization, and, notably, numerical accuracy.\nThis paper then argues that using posit, a recently proposed floating-point\nformat, is a better strategy for statistical computations operating on\nextremely small numbers because of its unique encoding mechanism. To that end,\nthis paper performs a comprehensive analysis comparing posit, binary64, and\nlogarithm representations, examining both individual arithmetic operations,\nstatistical bioinformatics applications, and their accelerators. FPGA\nimplementation results highlight that posit-based accelerators can achieve up\nto two orders of magnitude higher accuracy, up to 60\\% lower resource\nutilization, and up to $1.3\\times$ speedup, compared to log-space accelerators.\nSuch improvement translates to $2\\times$ performance per unit resource on the\nFPGA.", "AI": {"tldr": "This paper explores the use of the posit floating-point format for statistical computations in log-space, presenting significant improvements in accuracy, resource efficiency, and performance.", "motivation": "Statistical computations in log-space can mitigate numerical underflow but are shown to have high costs in accuracy, performance, and resource utilization.", "method": "Proposes using the posit floating-point format and performs comparative analysis with binary64 and logarithm representations in terms of operations, statistical applications, and FPGA implementations.", "result": "Posit-based accelerators demonstrated up to two orders of magnitude higher accuracy, 60% resource utilization reduction, and 1.3x speedup over log-space accelerators on FPGA.", "conclusion": "The posit format offers superior accuracy, resource efficiency, and performance for statistical computations and FPGA accelerators compared to traditional log-space methods."}}
{"id": "2509.11511", "pdf": "https://arxiv.org/pdf/2509.11511", "abs": "https://arxiv.org/abs/2509.11511", "authors": ["Suman Cha", "Hyunjoong Kim"], "title": "Learning Majority-to-Minority Transformations with MMD and Triplet Loss for Imbalanced Classification", "categories": ["stat.ML", "cs.LG"], "comment": ".19 pages, 6 figures", "summary": "Class imbalance in supervised classification often degrades model performance\nby biasing predictions toward the majority class, particularly in critical\napplications such as medical diagnosis and fraud detection. Traditional\noversampling techniques, including SMOTE and its variants, generate synthetic\nminority samples via local interpolation but fail to capture global data\ndistributions in high-dimensional spaces. Deep generative models based on GANs\noffer richer distribution modeling yet suffer from training instability and\nmode collapse under severe imbalance. To overcome these limitations, we\nintroduce an oversampling framework that learns a parametric transformation to\nmap majority samples into the minority distribution. Our approach minimizes the\nmaximum mean discrepancy (MMD) between transformed and true minority samples\nfor global alignment, and incorporates a triplet loss regularizer to enforce\nboundary awareness by guiding synthesized samples toward challenging borderline\nregions. We evaluate our method on 29 synthetic and real-world datasets,\ndemonstrating consistent improvements over classical and generative baselines\nin AUROC, G-mean, F1-score, and MCC. These results confirm the robustness,\ncomputational efficiency, and practical utility of the proposed framework for\nimbalanced classification tasks.", "AI": {"tldr": "The paper proposes a novel oversampling method using parametric transformations and loss regularizers to better address class imbalance issues in supervised classification.", "motivation": "The motivation is to address the challenges of class imbalance in supervised learning, which can degrade model performance, especially in critical fields like healthcare and fraud detection.", "method": "The authors propose learning a parametric transformation to map majority samples into the minority distribution, minimizing the maximum mean discrepancy (MMD) for global distribution alignment and using a triplet loss regularizer to improve boundary awareness.", "result": "The proposed method shows consistent improvement across 29 datasets (synthetic and real-world) in evaluation metrics like AUROC, G-mean, F1-score, and MCC, outperforming traditional oversampling techniques and generative baselines.", "conclusion": "The method is robust, computationally efficient, and demonstrates practical utility for class imbalance issues in real-world datasets."}}
{"id": "2509.11754", "pdf": "https://arxiv.org/pdf/2509.11754", "abs": "https://arxiv.org/abs/2509.11754", "authors": ["Zhiyuan Ren", "Mingxuan Lu", "Wenchi Cheng"], "title": "A Uniqueness Theorem for Distributed Computation under Physical Constraint", "categories": ["cs.DC", "cs.NI"], "comment": null, "summary": "Foundational models of computation often abstract away physical hardware\nlimitations. However, in extreme environments like In-Network Computing (INC),\nthese limitations become inviolable laws, creating an acute trilemma among\ncommunication efficiency, bounded memory, and robust scalability. Prevailing\ndistributed paradigms, while powerful in their intended domains, were not\ndesigned for this stringent regime and thus face fundamental challenges. This\npaper demonstrates that resolving this trilemma requires a shift in perspective\n- from seeking engineering trade-offs to deriving solutions from logical\nnecessity. We establish a rigorous axiomatic system that formalizes these\nphysical constraints and prove that for the broad class of computations\nadmitting an idempotent merge operator, there exists a unique, optimal\nparadigm. Any system satisfying these axioms must converge to a single normal\nform: Self-Describing Parallel Flows (SDPF), a purely data-centric model where\nstateless executors process flows that carry their own control logic. We\nfurther prove this unique paradigm is convergent, Turing-complete, and minimal.\nIn the same way that the CAP theorem established a boundary for what is\nimpossible in distributed state management, our work provides a constructive\ndual: a uniqueness theorem that reveals what is \\textit{inevitable} for\ndistributed computation flows under physical law.", "AI": {"tldr": "This paper introduces a rigorous framework to solve the challenges in In-Network Computing (INC), specifically addressing the trilemma between communication efficiency, bounded memory, and robust scalability. It establishes a unique, optimal computational paradigm and provides proofs regarding its fundamental properties.", "motivation": "To address the acute trilemma in INC environments\u2014communication efficiency, bounded memory, and robust scalability\u2014which current distributed paradigms struggle to handle given their physical hardware constraints.", "method": "The authors establish an axiomatic system based on physical constraints and prove the existence of a unique computational paradigm known as Self-Describing Parallel Flows (SDPF), which adheres to these axioms.", "result": "The paper demonstrates that SDPF is the only paradigm that satisfies the predefined axioms. It is proven to be convergent, Turing-complete, and minimal, providing a foundational solution analogous to the CAP theorem in distributed computing.", "conclusion": "The work reveals that SDPF is an inevitable paradigm for distributed computation flows in high-constraint environments, offering a logical and optimal solution to problems stemming from physical hardware limitations."}}
{"id": "2509.10931", "pdf": "https://arxiv.org/pdf/2509.10931", "abs": "https://arxiv.org/abs/2509.10931", "authors": ["Seongho Joo", "Hyukhun Koh", "Kyomin Jung"], "title": "Harmful Prompt Laundering: Jailbreaking LLMs with Abductive Styles and Symbolic Encoding", "categories": ["cs.AI", "cs.CL"], "comment": "EMNLP 2025", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\ndiverse tasks, but their potential misuse for harmful purposes remains a\nsignificant concern. To strengthen defenses against such vulnerabilities, it is\nessential to investigate universal jailbreak attacks that exploit intrinsic\nweaknesses in the architecture and learning paradigms of LLMs. In response, we\npropose \\textbf{H}armful \\textbf{P}rompt \\textbf{La}undering (HaPLa), a novel\nand broadly applicable jailbreaking technique that requires only black-box\naccess to target models. HaPLa incorporates two primary strategies: 1)\n\\textit{abductive framing}, which instructs LLMs to infer plausible\nintermediate steps toward harmful activities, rather than directly responding\nto explicit harmful queries; and 2) \\textit{symbolic encoding}, a lightweight\nand flexible approach designed to obfuscate harmful content, given that current\nLLMs remain sensitive primarily to explicit harmful keywords. Experimental\nresults show that HaPLa achieves over 95% attack success rate on GPT-series\nmodels and 70% across all targets. Further analysis with diverse symbolic\nencoding rules also reveals a fundamental challenge: it remains difficult to\nsafely tune LLMs without significantly diminishing their helpfulness in\nresponding to benign queries.", "AI": {"tldr": "The paper introduces HaPLa, a method for universal jailbreaking of LLMs, achieving over 95% success on GPT models by exploiting architectural weaknesses.", "motivation": "The rise of misuse in LLMs for harmful purposes highlights the urgent need to understand and defend against potential vulnerabilities.", "method": "The HaPLa approach uses abductive framing for indirect harmful query inference and symbolic encoding to obfuscate harmful content from LLMs.", "result": "HaPLa demonstrates a 95% attack success rate on GPT models and 70% success across other LLMs using black-box attack strategies.", "conclusion": "Defensively tuning LLMs remains challenging as making them safer often compromises their utility in benign tasks."}}
{"id": "2509.10511", "pdf": "https://arxiv.org/pdf/2509.10511", "abs": "https://arxiv.org/abs/2509.10511", "authors": ["Umberto Gon\u00e7alves de Sousa"], "title": "LogGuardQ: A Cognitive-Enhanced Reinforcement Learning Framework for Cybersecurity Anomaly Detection in Security Logs", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": "17 pages, 6 figures", "summary": "Reinforcement learning (RL) has transformed sequential decision-making, but\ntraditional algorithms like Deep Q-Networks (DQNs) and Proximal Policy\nOptimization (PPO) often struggle with efficient exploration, stability, and\nadaptability in dynamic environments. This study presents LogGuardQ (Adaptive\nLog Guard with Cognitive enhancement), a novel framework that integrates a\ndual-memory system inspired by human cognition and adaptive exploration\nstrategies driven by temperature decay and curiosity. Evaluated on a dataset of\n1,000,000 simulated access logs with 47.9% anomalies over 20,000 episodes,\nLogGuardQ achieves a 96.0% detection rate (versus 93.0% for DQN and 47.1% for\nPPO), with precision of 0.4776, recall of 0.9996, and an F1-score of 0.6450.\nThe mean reward is 20.34 \\pm 44.63 across all episodes (versus 18.80 \\pm 43.98\nfor DQN and -0.17 \\pm 23.79 for PPO), with an average of 5.0 steps per episode\n(constant across models). Graphical analyses, including learning curves\nsmoothed with a Savgol filter (window=501, polynomial=2), variance trends,\naction distributions, and cumulative detections, demonstrate LogGuardQ's\nsuperior stability and efficiency. Statistical tests (Mann-Whitney U) confirm\nsignificant performance advantages (e.g., p = 0.0002 vs. DQN with negligible\neffect size, p < 0.0001 vs. PPO with medium effect size, and p < 0.0001 for DQN\nvs. PPO with small effect size). By bridging cognitive science and RL,\nLogGuardQ offers a scalable approach to adaptive learning in uncertain\nenvironments, with potential applications in cybersecurity, intrusion\ndetection, and decision-making under uncertainty.", "AI": {"tldr": "LogGuardQ integrates human-inspired cognitive mechanics and adaptive exploration into RL, significantly outperforming DQN and PPO in anomaly detection tasks.", "motivation": "Reinforcement learning struggles with exploration, stability, and adaptability in dynamic settings, hindering efficient decision-making in real-world applications.", "method": "Introduces LogGuardQ, combining a dual-memory cognitive framework, adaptive exploration via temperature decay, and curiosity-driven strategies, tested on simulated access logs.", "result": "LogGuardQ achieves a 96.0% anomaly detection rate (versus DQN's 93.0% and PPO's 47.1%), higher precision and recall, improved rewards, and better stability confirmed by statistical tests.", "conclusion": "LogGuardQ effectively bridges cognitive science and RL, providing a scalable and adaptive system with strong potential for cybersecurity and uncertainty-based decision-making."}}
{"id": "2509.10888", "pdf": "https://arxiv.org/pdf/2509.10888", "abs": "https://arxiv.org/abs/2509.10888", "authors": ["Weijie Liu", "Ziyi Qiu", "Shihang Wang", "Deqing Mei", "Yancheng Wang"], "title": "Design of scalable orthogonal digital encoding architecture for large-area flexible tactile sensing in robotics", "categories": ["cs.RO"], "comment": "6 pages, 9 figures(Accepted to IEEE/RSJ International Conference on\n  Intelligent Robots and Systems, 2025)", "summary": "Human-like embodied tactile perception is crucial for the next-generation\nintelligent robotics. Achieving large-area, full-body soft coverage with high\nsensitivity and rapid response, akin to human skin, remains a formidable\nchallenge due to critical bottlenecks in encoding efficiency and wiring\ncomplexity in existing flexible tactile sensors, thus significantly hinder the\nscalability and real-time performance required for human skin-level tactile\nperception. Herein, we present a new architecture employing code division\nmultiple access-inspired orthogonal digital encoding to overcome these\nchallenges. Our decentralized encoding strategy transforms conventional serial\nsignal transmission by enabling parallel superposition of energy-orthogonal\nbase codes from distributed sensing nodes, drastically reducing wiring\nrequirements and increasing data throughput. We implemented and validated this\nstrategy with off-the-shelf 16-node sensing array to reconstruct the pressure\ndistribution, achieving a temporal resolution of 12.8 ms using only a single\ntransmission wire. Crucially, the architecture can maintain sub-20ms latency\nacross orders-of-magnitude variations in node number (to thousands of nodes).\nBy fundamentally redefining signal encoding paradigms in soft electronics, this\nwork opens new frontiers in developing scalable embodied intelligent systems\nwith human-like sensory capabilities.", "AI": {"tldr": "The paper proposes a novel decentralized encoding strategy inspired by code division multiple access to improve tactile sensors, achieving human-like scalability and responsiveness.", "motivation": "Existing tactile sensors face challenges in mimicking human skin due to issues like wiring complexity and scalability.", "method": "Introduced a decentralized encoding architecture enabling parallel signal transmission using orthogonal digital base codes, validated with a 16-node sensing array.", "result": "Achieved high-speed pressure distribution reconstruction in 12.8ms with only a single transmission wire, scalable to thousands of nodes while maintaining sub-20ms latency.", "conclusion": "The approach redefines signal encoding paradigms in soft electronics, enabling scalable intelligent systems with human-like tactile perception."}}
{"id": "2509.10739", "pdf": "https://arxiv.org/pdf/2509.10739", "abs": "https://arxiv.org/abs/2509.10739", "authors": ["Mobina Pournemat", "Keivan Rezaei", "Gaurang Sriramanan", "Arman Zarei", "Jiaxiang Fu", "Yang Wang", "Hamid Eghbalzadeh", "Soheil Feizi"], "title": "Reasoning Under Uncertainty: Exploring Probabilistic Reasoning Capabilities of LLMs", "categories": ["cs.CL"], "comment": "25 pages, 4 figures, 6 tables", "summary": "Despite widespread success in language understanding and generation, large\nlanguage models (LLMs) exhibit unclear and often inconsistent behavior when\nfaced with tasks that require probabilistic reasoning. In this work, we present\nthe first comprehensive study of the reasoning capabilities of LLMs over\nexplicit discrete probability distributions. Given observations from a\nprobability distribution, we evaluate models on three carefully designed tasks,\nmode identification, maximum likelihood estimation, and sample generation, by\nprompting them to provide responses to queries about either the joint\ndistribution or its conditionals. These tasks thus probe a range of\nprobabilistic skills, including frequency analysis, marginalization, and\ngenerative behavior. Through comprehensive empirical evaluations, we\ndemonstrate that there exists a clear performance gap between smaller and\nlarger models, with the latter demonstrating stronger inference and surprising\ncapabilities in sample generation. Furthermore, our investigations reveal\nnotable limitations, including sensitivity to variations in the notation\nutilized to represent probabilistic outcomes and performance degradation of\nover 60% as context length increases. Together, our results provide a detailed\nunderstanding of the probabilistic reasoning abilities of LLMs and identify key\ndirections for future improvement.", "AI": {"tldr": "This paper investigates the probabilistic reasoning abilities of large language models (LLMs) and finds both strengths and notable limitations.", "motivation": "To understand and evaluate the probabilistic reasoning capabilities of LLMs over explicit discrete probability distributions.", "method": "The authors designed three tasks\u2014mode identification, maximum likelihood estimation, and sample generation\u2014to evaluate LLMs' reasoning using queries related to joint distributions and conditionals.", "result": "Large LLMs demonstrated stronger reasoning abilities and unexpected sample generation skills compared to smaller models, but displayed significant sensitivity to notation changes and performance degradation with longer context lengths.", "conclusion": "The work highlights both strengths and limitations in LLMs' probabilistic reasoning abilities, providing insights for future model improvements."}}
{"id": "2509.11252", "pdf": "https://arxiv.org/pdf/2509.11252", "abs": "https://arxiv.org/abs/2509.11252", "authors": ["Chengze li", "Yitong Zhang", "Jia Li", "Liyi Cai", "Ge Li"], "title": "Beyond Autoregression: An Empirical Study of Diffusion Large Language Models for Code Generation", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "LLMs have become the mainstream approaches to code generation. Existing LLMs\nmainly employ autoregressive generation, i.e. generating code token-by-token\nfrom left to right. However, the underlying autoregressive generation has two\nlimitations in code generation. First, autoregressive LLMs only generate a\ntoken at each step, showing low efficiency in practice. Second, programming is\na non-sequential process involving back-and-forth editing, while autoregressive\nLLMs only employ the left-to-right generation order. These two intrinsic\nlimitations hinder the further development of LLMs in code generation.\nRecently, diffusion LLMs have emerged as a promising alternative. Diffusion\nLLMs address the above limitations with two advances, including multi-token\nprediction (i.e. generating multiple tokens at each step) and flexible\ngeneration order (i.e. flexibly determining which positions to generate\ntokens). However, there is no systematic study exploring diffusion LLMs in code\ngeneration. To bridge the knowledge gap, we present the first empirical study\nof diffusion LLMs for code generation. Our study involves 9 representative\ndiffusion LLMs and conduct experiments on 4 widely used benchmarks. Based on\nthe results, we summarize the following findings. (1) Existing diffusion LLMs\nare competitive with autoregressive LLMs with similar sizes. (2) Diffusion LLMs\nhave a stronger length extrapolation ability than autoregressive LLMs and\nperform better in long code understanding. (3) We explore factors impacting the\neffectiveness and efficiency of diffusion LLMs, and provide practical guidance.\n(4) We discuss several promising further directions to improve diffusion LLMs\non code generation. We open-source all source code, data, and results to\nfacilitate the following research. The code is publicly available at\nhttps://github.com/zhangyitonggg/dllm4code.", "AI": {"tldr": "This paper examines diffusion LLMs as an alternative to autoregressive models for code generation, analyzing their advantages and limitations based on a systematic study.", "motivation": "To address the inefficiencies and rigidity in autoregressive LLMs used for code generation by evaluating diffusion LLMs, which promise multi-token prediction and flexible generation order.", "method": "Conduct an empirical study on 9 diffusion LLMs across 4 common benchmarks, comparing their performance to autoregressive LLMs.", "result": "Findings reveal that diffusion LLMs are competitive with autoregressive models, excel in long code understanding, and demonstrate strong length extrapolation capabilities.", "conclusion": "Diffusion LLMs show promise for advancing code generation, with room for optimization. The study identifies impactful factors and proposes directions for improvement, contributing to further research with open-source resources."}}
{"id": "2509.10900", "pdf": "https://arxiv.org/pdf/2509.10900", "abs": "https://arxiv.org/abs/2509.10900", "authors": ["Yangyang Du"], "title": "Geometric Phase of Stochastic Oscillators", "categories": ["math-ph", "cond-mat.stat-mech", "math.MP", "nlin.CD", "q-bio.NC"], "comment": null, "summary": "Several definitions of phase have been proposed for stochastic oscillators,\namong which the mean-return-time phase and the stochastic asymptotic phase have\ndrawn particular attention. Quantitative comparisons between these two\ndefinitions have been done in previous studies, but physical interpretations of\nsuch a relation are still missing. In this work, we illustrate this relation\nusing the geometric phase, which is an essential concept in both classical and\nquantum mechanics. We use properties of probability currents and the\ngeneralized Doob's h-transform to explain how the geometric phase arises in\nstochastic oscillators. Such an analogy is also reminiscent of the\nnoise-induced phase shift in oscillatory systems with deterministic\nperturbation, allowing us to compare the phase responses in deterministic and\nstochastic oscillators. The resulting framework unifies these distinct phase\ndefinitions and reveals that their difference is governed by a geometric drift\nterm analogous to curvature. This interpretation bridges spectral theory,\nstochastic dynamics, and geometric phase, and provides new insight into how\nnoise reshapes oscillatory behavior. Our results suggest broader applications\nof geometric-phase concepts to coupled stochastic oscillators and neural\nmodels.", "AI": {"tldr": "The study explores the relationship between two phase definitions in stochastic oscillators using the geometric phase framework, providing a unified interpretation and insights into noise-induced oscillatory behavior.", "motivation": "To provide a physical interpretation and understanding of the relationship between the mean-return-time phase and stochastic asymptotic phase in stochastic oscillators.", "method": "The paper utilizes concepts from geometric phase, probability currents, and generalized Doob's h-transform to analyze and unify different phase definitions in stochastic oscillators.", "result": "The findings show that the difference between the two phase definitions is governed by a geometric drift term analogous to curvature, bridging concepts in spectral theory, stochastic dynamics, and geometric phase.", "conclusion": "The study establishes a unified framework for distinct phase definitions, offering insights into noise effects on oscillators and suggesting broader applicability to coupled stochastic and neural models."}}
{"id": "2509.10761", "pdf": "https://arxiv.org/pdf/2509.10761", "abs": "https://arxiv.org/abs/2509.10761", "authors": ["Marcelo Sandoval-Castaneda", "Bryan Russell", "Josef Sivic", "Gregory Shakhnarovich", "Fabian Caba Heilbron"], "title": "EditDuet: A Multi-Agent System for Video Non-Linear Editing", "categories": ["cs.CV"], "comment": "SIGGRAPH 2025", "summary": "Automated tools for video editing and assembly have applications ranging from\nfilmmaking and advertisement to content creation for social media. Previous\nvideo editing work has mainly focused on either retrieval or user interfaces,\nleaving actual editing to the user. In contrast, we propose to automate the\ncore task of video editing, formulating it as sequential decision making\nprocess. Ours is a multi-agent approach. We design an Editor agent and a Critic\nagent. The Editor takes as input a collection of video clips together with\nnatural language instructions and uses tools commonly found in video editing\nsoftware to produce an edited sequence. On the other hand, the Critic gives\nnatural language feedback to the editor based on the produced sequence or\nrenders it if it is satisfactory. We introduce a learning-based approach for\nenabling effective communication across specialized agents to address the\nlanguage-driven video editing task. Finally, we explore an LLM-as-a-judge\nmetric for evaluating the quality of video editing system and compare it with\ngeneral human preference. We evaluate our system's output video sequences\nqualitatively and quantitatively through a user study and find that our system\nvastly outperforms existing approaches in terms of coverage, time constraint\nsatisfaction, and human preference.", "AI": {"tldr": "This paper proposes automating video editing using a multi-agent system with an Editor agent and a Critic agent, where the Editor performs edits based on natural language instructions and the Critic provides feedback.", "motivation": "To simplify the video editing process by automating intricate tasks that traditionally required user engagement.", "method": "The paper employs a multi-agent framework consisting of an Editor agent that performs video edits using standard editing tools based on language instructions, and a Critic agent that evaluates and provides feedback on the edits.", "result": "The proposed system was tested through qualitative and quantitative user studies, demonstrating superior performance over existing methods in coverage, time constraint satisfaction, and user preference.", "conclusion": "Automating video editing with specialized agents enhances editing quality and efficiency, making it a practical tool for users across various applications."}}
{"id": "2509.11767", "pdf": "https://arxiv.org/pdf/2509.11767", "abs": "https://arxiv.org/abs/2509.11767", "authors": ["Jakub Dobosz", "Maximilian Engelhardt", "Diego Dupleich", "Maciej Stapor", "Pawel Kulakowski"], "title": "Vital Signs Monitoring with mmWave OFDM JCAS System", "categories": ["cs.ET", "cs.AR"], "comment": null, "summary": "Wireless techniques for monitoring human vital signs, such as heart and\nbreathing rates, offer a promising solution in the context of joint\ncommunication and sensing (JCAS) with applications in medicine, sports, safety,\nsecurity, and even the military. This paper reports experimental results\nobtained at the Fraunhofer Institute for Integrated Circuits in Ilmenau,\ndemonstrating the effectiveness of an indoor orthogonal frequency-division\nmultiplexing (OFDM) JCAS system for detecting human heart and breathing rates.\nThe system operated in a bistatic configuration at an FR2 frequency of 26.5 GHz\nwith a variable bandwidth of up to 1 GHz. Measurements were taken under various\nscenarios, including a subject lying down, sitting, or walking, in both\nline-of-sight and non-line-of-sight conditions, and with one or two subjects\npresent simultaneously. The results indicate that while vital sign detection is\ngenerally feasible, its effectiveness is influenced by several factors, such as\nthe subjects clothing, activity, as well as the distance and angle relative to\nthe sensing system. In addition, no significant influence of bandwidth was\ndetected since the vital signs information is encoded in the phase of the\nsignal.", "AI": {"tldr": "This paper showcases the use of a 26.5 GHz bistatic OFDM wireless system to monitor human heart and breathing rates in different conditions, identifying factors impacting signal detection.", "motivation": "To explore the potential of wireless methods for monitoring human vital signs within joint communication and sensing (JCAS) applications.", "method": "Experimental evaluation of an indoor bistatic OFDM JCAS system operating at 26.5 GHz frequency and up to 1 GHz bandwidth under various scenarios involving human subjects in line-of-sight and non-line-of-sight configurations.", "result": "The system is generally effective in detecting vital signs but performance is influenced by subject clothing, activities, distance, and angle to the system. Bandwidth showed no significant impact, as vital information is phase-encoded.", "conclusion": "Wireless detection of vital signs is feasible but performance is scenario-dependent, highlighting both the potential and limitations of the technology for real-world applications."}}
{"id": "2509.11532", "pdf": "https://arxiv.org/pdf/2509.11532", "abs": "https://arxiv.org/abs/2509.11532", "authors": ["Davide La Vecchia", "Hang Liu"], "title": "E-ROBOT: a dimension-free method for robust statistics and machine learning via Schr\u00f6dinger bridge", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "We propose the Entropic-regularized Robust Optimal Transport (E-ROBOT)\nframework, a novel method that combines the robustness of ROBOT with the\ncomputational and statistical benefits of entropic regularization. We show\nthat, rooted in the Schr\\\"{o}dinger bridge problem theory, E-ROBOT defines the\nrobust Sinkhorn divergence $\\overline{W}_{\\varepsilon,\\lambda}$, where the\nparameter $\\lambda$ controls robustness and $\\varepsilon$ governs the\nregularization strength. Letting $n\\in \\mathbb{N}$ denote the sample size, a\ncentral theoretical contribution is establishing that the sample complexity of\n$\\overline{W}_{\\varepsilon,\\lambda}$ is $\\mathcal{O}(n^{-1/2})$, thereby\navoiding the curse of dimensionality that plagues standard ROBOT. This\ndimension-free property unlocks the use of $\\overline{W}_{\\varepsilon,\\lambda}$\nas a loss function in large-dimensional statistical and machine learning tasks.\nWith this regard, we demonstrate its utility through four applications:\ngoodness-of-fit testing; computation of barycenters for corrupted 2D and 3D\nshapes; definition of gradient flows; and image colour transfer. From the\ncomputation standpoint, a perk of our novel method is that it can be easily\nimplemented by modifying existing (\\texttt{Python}) routines. From the\ntheoretical standpoint, our work opens the door to many research directions in\nstatistics and machine learning: we discuss some of them.", "AI": {"tldr": "The E-ROBOT framework combines the robustness of ROBOT with the benefits of entropic regularization, defining a robust Sinkhorn divergence with favorable sample complexity.", "motivation": "The paper addresses the limitations of standard ROBOT methods, such as the curse of dimensionality, and aims to improve computational and statistical performance in high-dimensional tasks.", "method": "The authors propose E-ROBOT, which integrates robustness parameters (\u03bb) and regularization strength (\u03b5) in defining a robust Sinkhorn divergence based on the Schr\u00f6dinger bridge problem.", "result": "The robust Sinkhorn divergence demonstrates a sample complexity of O(n^-1/2), allowing its usage in high-dimensional machine learning applications such as goodness-of-fit testing, barycenters for corrupted images, gradient flows, and color transfer.", "conclusion": "E-ROBOT offers both theoretical advancements and practical ease of integration, opening up new pathways for research and applications in statistics and machine learning."}}
{"id": "2509.11904", "pdf": "https://arxiv.org/pdf/2509.11904", "abs": "https://arxiv.org/abs/2509.11904", "authors": ["Julien Dallot", "Caio Caldeira", "Arash Pourdamghani", "Olga Goussevskaia", "Stefan Schmid"], "title": "LASLiN: A Learning-Augmented Peer-to-Peer Network", "categories": ["cs.DC"], "comment": null, "summary": "We introduce a learning-augmented peer-to-peer (P2P) network design that\nleverages the predictions of traffic patterns to optimize the network's\ntopology. While keeping formal guarantees on the standard P2P metrics (routing\npath length, maximum degree), we optimize the network in a demand-aware manner\nand minimize the path lengths weighted by the peer-to-peer communication\ndemands. Our protocol is learning-augmented, meaning that each node receives an\nindividual, possibly inaccurate prediction about the future traffic patterns,\nwith the goal of improving the network's performances. We strike a trade-off\nbetween significantly improved performances when the predictions are correct\n(consistency) and polylogarithmic performances when the predictions are\narbitrary (robustness).\n  We have two main contributions. First, we consider the centralized setting\nand show that the problem of constructing an optimum static skip list network\n(SLN) is solvable in polynomial time and can be computed via dynamic\nprogramming. This problem is the natural demand-aware extension of the optimal\nskip list problem.\n  Second, we introduce the Uniform P2P protocol which generalizes skip list\nnetworks (SLN) by relaxing the node's heights from discrete to continuous. We\nshow that Uniform achieves state-of-the-art performances: logarithmic routing\nand maximum degree, both with high probability. We then use Uniform to build a\nlearning-augmented P2P protocol in order to incorporate demand-awareness,\nleading to our main contribution, LASLiN. We prove that the performances of\nLASLiN are consistent with those of an optimum static SLN with correct\npredictions (given via our dynamic programming approach), and are at most a\nlogarithmic factor off the state-of-the-art P2P protocols if the predictions\nare arbitrary wrong. For the special case of highly sparse demands, we show\nthat LASLiN achieves improved performances.", "AI": {"tldr": "The paper proposes a learning-augmented peer-to-peer (P2P) network protocol that adapts its topology based on traffic predictions, optimizing communication paths and maintaining robust performance even when predictions are inaccurate.", "motivation": "To design a P2P network that optimally adjusts its topology based on traffic demands while retaining formal guarantees on routing efficiency and robustness, even in the presence of inaccurate predictions.", "method": "The authors use dynamic programming to solve demand-aware skip list network (SLN) problems in a centralized setting and also propose the Uniform P2P protocol, a generalization of SLN that incorporates learning-augmented traffic predictions to develop LASLiN.", "result": "LASLiN achieves significantly better performance with accurate traffic predictions and maintains polylogarithmic performance guarantees in the worst-case scenario of inaccurate predictions. It also provides enhanced performance for highly sparse demand patterns.", "conclusion": "By effectively combining traffic predictions with robust P2P network design, the proposed LASLiN protocol strikes a balance between optimal efficiency and resilience, setting a new standard for demand-aware P2P networks."}}
{"id": "2509.10932", "pdf": "https://arxiv.org/pdf/2509.10932", "abs": "https://arxiv.org/abs/2509.10932", "authors": ["Seongho Joo", "Hyukhun Koh", "Kyomin Jung"], "title": "Public Data Assisted Differentially Private In-Context Learning", "categories": ["cs.AI", "cs.CL"], "comment": "EMNLP 2025 Findings", "summary": "In-context learning (ICL) in Large Language Models (LLMs) has shown\nremarkable performance across various tasks without requiring fine-tuning.\nHowever, recent studies have highlighted the risk of private data leakage\nthrough the prompt in ICL, especially when LLMs are exposed to malicious\nattacks. While differential privacy (DP) provides strong privacy guarantees, it\noften significantly reduces the utility of in-context learning (ICL). To\naddress this challenge, we incorporate task-related public data into the ICL\nframework while maintaining the DP guarantee. Based on this approach, we\npropose a private in-context learning algorithm that effectively balances\nprivacy protection and model utility. Through experiments, we demonstrate that\nour approach significantly improves the utility of private ICL with the\nassistance of public data. Additionally, we show that our method is robust\nagainst membership inference attacks, demonstrating empirical privacy\nprotection.", "AI": {"tldr": "The paper introduces a private in-context learning (ICL) algorithm for Large Language Models (LLMs) that combines differential privacy (DP) with task-related public data to improve utility while maintaining privacy.", "motivation": "The study addresses the risk of private data leakage during in-context learning (ICL) with Large Language Models, particularly under malicious attacks, and the utility loss caused by implementing differential privacy.", "method": "A new private in-context learning algorithm that integrates public task-related data alongside differential privacy (DP), aiming to achieve a trade-off between privacy protection and model utility.", "result": "The approach successfully enhances the utility of private ICL and exhibits resilience against membership inference attacks, showing improved empirical privacy protection.", "conclusion": "The proposed method demonstrates a practical balance between privacy and utility in in-context learning by leveraging public data under a differential privacy framework."}}
{"id": "2509.10512", "pdf": "https://arxiv.org/pdf/2509.10512", "abs": "https://arxiv.org/abs/2509.10512", "authors": ["Jiaxing Cao", "Yuzhou Gao", "Jiwei Huang"], "title": "A Service-Oriented Adaptive Hierarchical Incentive Mechanism for Federated Learning", "categories": ["cs.LG", "cs.GT", "cs.SY", "eess.SY"], "comment": "Accepted at CollaborateCom 2025", "summary": "Recently, federated learning (FL) has emerged as a novel framework for\ndistributed model training. In FL, the task publisher (TP) releases tasks, and\nlocal model owners (LMOs) use their local data to train models. Sometimes, FL\nsuffers from the lack of training data, and thus workers are recruited for\ngathering data. To this end, this paper proposes an adaptive incentive\nmechanism from a service-oriented perspective, with the objective of maximizing\nthe utilities of TP, LMOs and workers. Specifically, a Stackelberg game is\ntheoretically established between the LMOs and TP, positioning TP as the leader\nand the LMOs as followers. An analytical Nash equilibrium solution is derived\nto maximize their utilities. The interaction between LMOs and workers is\nformulated by a multi-agent Markov decision process (MAMDP), with the optimal\nstrategy identified via deep reinforcement learning (DRL). Additionally, an\nAdaptively Searching the Optimal Strategy Algorithm (ASOSA) is designed to\nstabilize the strategies of each participant and solve the coupling problems.\nExtensive numerical experiments are conducted to validate the efficacy of the\nproposed method.", "AI": {"tldr": "The paper proposes an adaptive incentive mechanism for federated learning to balance utilities among task publishers (TP), local model owners (LMOs), and workers by employing game theory and reinforcement learning.", "motivation": "FL often lacks sufficient training data, necessitating a mechanism that motivates participation and ensures fairness among all stakeholders in the training process.", "method": "The authors propose a Stackelberg game model between TPs and LMOs to maximize utilities, while interactions between LMOs and workers are modeled as a multi-agent Markov decision process. Deep reinforcement learning helps identify optimal strategies, and a new algorithm ASOSA stabilizes the participants' strategies.", "result": "The proposed method demonstrates efficacy through extensive numerical experiments, showcasing its ability to balance and optimize utilities among participants.", "conclusion": "The incentive mechanism successfully aligns the objectives of FL's stakeholders, providing a structured approach to optimize and stabilize their interactions in a data-restrained environment."}}
{"id": "2509.10948", "pdf": "https://arxiv.org/pdf/2509.10948", "abs": "https://arxiv.org/abs/2509.10948", "authors": ["Navid Aftabi", "Philip Samaha", "Jin Ma", "Long Cheng", "Ramy Harik", "Dan Li"], "title": "ViSTR-GP: Online Cyberattack Detection via Vision-to-State Tensor Regression and Gaussian Processes in Automated Robotic Operations", "categories": ["cs.RO", "cs.AI", "cs.CR", "cs.SY", "eess.SY", "math.OC"], "comment": null, "summary": "Industrial robotic systems are central to automating smart manufacturing\noperations. Connected and automated factories face growing cybersecurity risks\nthat can potentially cause interruptions and damages to physical operations.\nAmong these attacks, data-integrity attacks often involve sophisticated\nexploitation of vulnerabilities that enable an attacker to access and\nmanipulate the operational data and are hence difficult to detect with only\nexisting intrusion detection or model-based detection. This paper addresses the\nchallenges in utilizing existing side-channels to detect data-integrity attacks\nin robotic manufacturing processes by developing an online detection framework,\nViSTR-GP, that cross-checks encoder-reported measurements against a\nvision-based estimate from an overhead camera outside the controller's\nauthority. In this framework, a one-time interactive segmentation initializes\nSAM-Track to generate per-frame masks. A low-rank tensor-regression surrogate\nmaps each mask to measurements, while a matrix-variate Gaussian process models\nnominal residuals, capturing temporal structure and cross-joint correlations. A\nframe-wise test statistic derived from the predictive distribution provides an\nonline detector with interpretable thresholds. We validate the framework on a\nreal-world robotic testbed with synchronized video frame and encoder data,\ncollecting multiple nominal cycles and constructing replay attack scenarios\nwith graded end-effector deviations. Results on the testbed indicate that the\nproposed framework recovers joint angles accurately and detects data-integrity\nattacks earlier with more frequent alarms than all baselines. These\nimprovements are most evident in the most subtle attacks. These results show\nthat plants can detect data-integrity attacks by adding an independent physical\nchannel, bypassing the controller's authority, without needing complex\ninstrumentation.", "AI": {"tldr": "A framework, ViSTR-GP, uses an independent vision-based channel to detect sophisticated data-integrity attacks in robotic manufacturing systems early and reliably.", "motivation": "The paper aims to address the increasingly critical cybersecurity risks in industrial robotic systems, particularly detecting data-integrity attacks, which exploit vulnerabilities to manipulate operational data and are challenging to detect using current methods.", "method": "ViSTR-GP utilizes an overhead camera outside the controller\u2019s authority to cross-check encoder data via a vision-based surrogate paired with a matrix-variate Gaussian process and interpretable test statistics. It employs SAM-Track for segmentation, low-rank tensor regression, and monitors residuals in real-time.", "result": "On a real-world robotic testbed with synchronized video and encoder data, ViSTR-GP accurately recovered joint angles and detected data-integrity attacks earlier and with higher frequency than existing baselines, outperforming especially in subtle attack scenarios.", "conclusion": "The framework demonstrates that adding an independent physical channel enables early and frequent detection of attacks without complex modifications to manufacturing systems, improving cybersecurity robustness in automated factories."}}
{"id": "2509.10744", "pdf": "https://arxiv.org/pdf/2509.10744", "abs": "https://arxiv.org/abs/2509.10744", "authors": ["Ozan Gokdemir", "Neil Getty", "Robert Underwood", "Sandeep Madireddy", "Franck Cappello", "Arvind Ramanathan", "Ian T. Foster", "Rick L. Stevens"], "title": "Automated MCQA Benchmarking at Scale: Evaluating Reasoning Traces as Retrieval Sources for Domain Adaptation of Small Language Models", "categories": ["cs.CL", "cs.AI", "I.2.7; I.2.11"], "comment": "This manuscript has been accepted for publication at the\n  Supercomputing 25 (SC '25) Conference (Frontiers in Generative AI for HPC\n  Science and Engineering: Foundations, Challenges, and Opportunities Workshop)\n  in St. Louis, MO, USA on November 16th, 2025. It will appear in the SC25\n  Workshop Proceedings after that date", "summary": "As scientific knowledge grows at an unprecedented pace, evaluation benchmarks\nmust evolve to reflect new discoveries and ensure language models are tested on\ncurrent, diverse literature. We propose a scalable, modular framework for\ngenerating multiple-choice question-answering (MCQA) benchmarks directly from\nlarge corpora of scientific papers. Our pipeline automates every stage of MCQA\ncreation, including PDF parsing, semantic chunking, question generation, and\nmodel evaluation. As a case study, we generate more than 16,000 MCQs from\n22,000 open-access articles in radiation and cancer biology. We then evaluate a\nsuite of small language models (1.1B-14B parameters) on these questions,\ncomparing baseline accuracy with retrieval-augmented generation (RAG) from\npaper-derived semantic chunks and from reasoning traces distilled from GPT-4.1.\nWe find that reasoning-trace retrieval consistently improves performance on\nboth synthetic and expert-annotated benchmarks, enabling several small models\nto surpass GPT-4 on the 2023 Astro Radiation and Cancer Biology exam.", "AI": {"tldr": "The paper introduces a framework to create large-scale MCQA benchmarks directly from scientific papers, showcasing its efficiency in radiation and cancer biology.", "motivation": "Current evaluation benchmarks struggle to keep up with the rapid growth of scientific knowledge. The paper aims to address this gap by ensuring that language models are tested on up-to-date, diverse literature.", "method": "A modular pipeline for MCQA creation was developed, incorporating processes like PDF parsing, semantic chunking, automated question generation, and model evaluation. The framework was applied to scientific papers in radiation and cancer biology.", "result": "The framework generated over 16,000 MCQs from 22,000 scientific papers. Small-scale language models were evaluated, and reasoning-trace retrieval methods improved their performance significantly, surpassing GPT-4 in specific benchmark tests.", "conclusion": "Reasoning-trace retrieval is effective for enhancing the capabilities of smaller models on scientific benchmarks, showcasing a scalable solution for creating dynamic, domain-specific evaluation frameworks."}}
{"id": "2509.11258", "pdf": "https://arxiv.org/pdf/2509.11258", "abs": "https://arxiv.org/abs/2509.11258", "authors": ["Regan Meloche", "Durga Sivakumar", "Amal A. Anda", "Sofana Alfuhaid", "Daniel Amyot", "Luigi Logrippo", "John Mylopoulos"], "title": "A Web-Based Environment for the Specification and Generation of Smart Legal Contracts", "categories": ["cs.SE"], "comment": "12 pages, 5 figures, 2 tables, conference", "summary": "Monitoring the compliance of contract performance against legal obligations\nis important in order to detect violations, ideally, as soon as they occur.\nSuch monitoring can nowadays be achieved through the use of smart contracts,\nwhich provide protection against tampering as well as some level of automation\nin handling violations. However, there exists a large gap between natural\nlanguage contracts and smart contract implementations. This paper introduces a\nWeb-based environment that partly fills that gap by supporting the\nuser-assisted refinement of Symboleo specifications corresponding to legal\ncontract templates, followed by the automated generation of monitoring smart\ncontracts deployable on the Hyperledger Fabric platform. This environment,\nillustrated using a sample contract from the transactive energy domain, shows\nmuch potential in accelerating the development of smart contracts in a legal\ncompliance context.", "AI": {"tldr": "The paper addresses automating monitoring of contract compliance through a web-based environment that bridges legal agreements and smart contracts.", "motivation": "To overcome the gap between natural language legal contracts and smart contract implementations, enabling more efficient compliance monitoring.", "method": "A web-based tool is developed to assist in refining Symboleo specifications derived from legal templates, and it automates the generation of smart contracts for deployment on Hyperledger Fabric.", "result": "The introduced environment successfully facilitates the conversion of legal contracts into deployable smart contracts, with a case study in the transactive energy domain demonstrating its utility.", "conclusion": "The proposed solution shows promise for accelerating legal compliance workflows by integrating smart contract automation into the process."}}
{"id": "2509.11582", "pdf": "https://arxiv.org/pdf/2509.11582", "abs": "https://arxiv.org/abs/2509.11582", "authors": ["Hinnerk Schulz-Hildebrandt"], "title": "When Purple Perceived Only at Fixation: A Fixation and Distance-Dependent Color Illusion", "categories": ["physics.optics", "q-bio.NC", "78-04", "I.4.8; H.1.2"], "comment": "5 pages, 6 figures, 3 visualizations (Figshare)", "summary": "In this paper a novel optical illusion is described in which purple\nstructures are perceived as purple at the point of fixation, while the\nsurrounding structures of the same purple color are perceived toward a blue\nhue. As the viewing distance increases, a greater number of purple structures\nrevert to a purple appearance.", "AI": {"tldr": "This paper introduces a new optical illusion where purple structures near the fixation point appear purple, but those farther away shift to a blue hue.", "motivation": "To investigate and describe an optical illusion that deviates from typical color perception and depends on the distance of viewing.", "method": "Observation of color perception changes for purple structures at varying viewing distances.", "result": "The study finds that fixation point structures are perceived as purple, while remote structures exhibit a blue hue that reverts to purple as viewing distance increases.", "conclusion": "Distance-dependent color perception phenomena are highlighted, revealing how spatial factors affect visual processing of identical colors."}}
{"id": "2509.10767", "pdf": "https://arxiv.org/pdf/2509.10767", "abs": "https://arxiv.org/abs/2509.10767", "authors": ["Sajad Amiri", "Shahram Taeb", "Sara Gharibi", "Setareh Dehghanfard", "Somayeh Sadat Mehrnia", "Mehrdad Oveisi", "Ilker Hacihaliloglu", "Arman Rahmim", "Mohammad R. Salmanpour"], "title": "Enhancement Without Contrast: Stability-Aware Multicenter Machine Learning for Glioma MRI Imaging", "categories": ["cs.CV", "F.2.2; I.2.7"], "comment": "14 Pages, 1 Figure, and 6 Tables", "summary": "Gadolinium-based contrast agents (GBCAs) are central to glioma imaging but\nraise safety, cost, and accessibility concerns. Predicting contrast enhancement\nfrom non-contrast MRI using machine learning (ML) offers a safer alternative,\nas enhancement reflects tumor aggressiveness and informs treatment planning.\nYet scanner and cohort variability hinder robust model selection. We propose a\nstability-aware framework to identify reproducible ML pipelines for multicenter\nprediction of glioma MRI contrast enhancement. We analyzed 1,446 glioma cases\nfrom four TCIA datasets (UCSF-PDGM, UPENN-GB, BRATS-Africa, BRATS-TCGA-LGG).\nNon-contrast T1WI served as input, with enhancement derived from paired\npost-contrast T1WI. Using PyRadiomics under IBSI standards, 108 features were\nextracted and combined with 48 dimensionality reduction methods and 25\nclassifiers, yielding 1,200 pipelines. Rotational validation was trained on\nthree datasets and tested on the fourth. Cross-validation prediction accuracies\nranged from 0.91 to 0.96, with external testing achieving 0.87 (UCSF-PDGM),\n0.98 (UPENN-GB), and 0.95 (BRATS-Africa), with an average of 0.93. F1,\nprecision, and recall were stable (0.87 to 0.96), while ROC-AUC varied more\nwidely (0.50 to 0.82), reflecting cohort heterogeneity. The MI linked with ETr\npipeline consistently ranked highest, balancing accuracy and stability. This\nframework demonstrates that stability-aware model selection enables reliable\nprediction of contrast enhancement from non-contrast glioma MRI, reducing\nreliance on GBCAs and improving generalizability across centers. It provides a\nscalable template for reproducible ML in neuro-oncology and beyond.", "AI": {"tldr": "The paper introduces a machine learning framework to reliably predict glioma MRI contrast enhancement using non-contrast images, offering an alternative to gadolinium-based contrast agents.", "motivation": "Safety, cost, and accessibility concerns associated with gadolinium-based contrast agents necessitate alternative methods for glioma imaging.", "method": "A stability-aware machine learning framework tested across multiple datasets and configurations using rotational validation to identify reproducible pipelines for contrast enhancement prediction.", "result": "The proposed framework achieved high cross-validation accuracy (0.91\u20130.96) and external testing accuracy (0.87\u20130.98), demonstrating strong performance despite cohort heterogeneity.", "conclusion": "Stability-aware model selection supports robust and generalizable ML pipelines for non-contrast MRI prediction, reducing reliance on contrast agents and advancing neuro-oncology imaging methods."}}
{"id": "2509.11675", "pdf": "https://arxiv.org/pdf/2509.11675", "abs": "https://arxiv.org/abs/2509.11675", "authors": ["Rodrigue Govan", "Romane Scherrer", "Philippe Fournier-Viger", "Nazha Selmaoui-Folcher"], "title": "SpaPool: Soft Partition Assignment Pooling for__Graph Neural Networks", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "This paper introduces SpaPool, a novel pooling method that combines the\nstrengths of both dense and sparse techniques for a graph neural network.\nSpaPool groups vertices into an adaptive number of clusters, leveraging the\nbenefits of both dense and sparse approaches. It aims to maintain the\nstructural integrity of the graph while reducing its size efficiently.\nExperimental results on several datasets demonstrate that SpaPool achieves\ncompetitive performance compared to existing pooling techniques and excels\nparticularly on small-scale graphs. This makes SpaPool a promising method for\napplications requiring efficient and effective graph processing.", "AI": {"tldr": "The paper introduces SpaPool, a new pooling method for graph neural networks combining dense and sparse techniques. It clusters vertices adaptively to maintain graph structure while reducing size effectively.", "motivation": "To address the limitations of existing graph pooling methods that may compromise structural integrity or efficiency.", "method": "SpaPool utilizes an adaptive clustering mechanism that leverages both dense and sparse pooling techniques to preserve graph structure while reducing graph size.", "result": "Experimental results show that SpaPool performs competitively with existing pooling methods, excelling on small-scale graphs.", "conclusion": "SpaPool is a promising method for graph-related tasks, particularly in applications requiring efficient graph processing with maintained structural quality."}}
{"id": "2509.12136", "pdf": "https://arxiv.org/pdf/2509.12136", "abs": "https://arxiv.org/abs/2509.12136", "authors": ["Tomer Bitan", "Tal Kadosh", "Erel Kaplan", "Shira Meiri", "Le Chen", "Peter Morales", "Niranjan Hasabnis", "Gal Oren"], "title": "UniPar: A Unified LLM-Based Framework for Parallel and Accelerated Code Translation in HPC", "categories": ["cs.DC"], "comment": "Accepted to IEEE HPEC conference 2025. 9 pages, incl references", "summary": "Translating programs between various parallel programming languages is an\nimportant problem in the high-performance computing (HPC) community. Existing\ntools for this problem are either too narrow in scope and/or outdated. Recent\nexplosive growth in the popularity of large language models (LLMs) and their\nability to generate and translate code offers a potential alternative approach.\nToward that end, we first need to systematically evaluate the ability of LLMs\nto translate between parallel languages.\n  In this work, we introduce UniPar, a systematic evaluation framework for\nLLM-based parallel code translation. Specifically, in this work, we target\ntranslations between serial code, CUDA, and OpenMP. Our goal is to assess how\nwell current instruction-tuned LLMs -- specifically GPT-4o-mini and\nLLaMA-3.3-70B-Instruct -- can be used out of the box or enhanced through known\nstrategies. We evaluated four major usage modes: hyperparameter optimization\nfor decoding, zero- and few-shot prompting, supervised fine-tuning, and\niterative feedback through compiler-based repair. As a part of the evaluation,\nwe construct a new dataset called PARATRANS, covering both serial-to-parallel\ntranslation and cross-paradigm transformations.\n  Our findings reveal that while off-the-shelf models struggle under the\ndefault settings (e.g., GPT-4o-mini achieves only 46% compilation and 15%\nfunctional correctness), our UniPar methodology -- combining fine-tuning,\nhyperparameter tuning, and compiler-guided repair -- improves performance by up\nto 2X (69% compilation and 33% correctness). We believe that our findings will\nprovide useful insights for researchers to further improve LLMs for the\nparallel language translation problem.\n  UniPar source code and PARATRANS dataset are available at our GitHub\nrepository https://github.com/Scientific-Computing-Lab/UniPar_AI.", "AI": {"tldr": "This paper introduces UniPar, a framework to evaluate the use of large language models (LLMs) for translating parallel programming code between CUDA, OpenMP, and serial code, showing that enhanced methods can significantly improve performance.", "motivation": "The paper addresses the problem of translating code between different parallel programming languages, an essential challenge in high-performance computing. Existing tools are outdated and limited, while the rise of LLMs provides a new potential avenue for effective translations.", "method": "The authors developed UniPar, evaluating LLMs like GPT-4o-mini and LLaMA-3.3-70B-Instruct using strategies such as hyperparameter optimization, prompting techniques, supervised fine-tuning, and compiler-based iterative feedback. They also introduced a dataset, PARATRANS, for testing cross-paradigm and serial-to-parallel translations.", "result": "The study found that off-the-shelf LLMs perform poorly in translating parallel code, achieving low functional correctness (15%). However, the UniPar methodology improved these outcomes significantly, doubling compilation rates (69%) and correctness (33%).", "conclusion": "UniPar demonstrates that combining fine-tuning, hyperparameter optimization, and compiler-guided repair can notably enhance LLM performance in parallel code translation, offering a promising path for future improvements in this domain."}}
{"id": "2509.10972", "pdf": "https://arxiv.org/pdf/2509.10972", "abs": "https://arxiv.org/abs/2509.10972", "authors": ["Ron Sun"], "title": "Enhancing Computational Cognitive Architectures with LLMs: A Case Study", "categories": ["cs.AI"], "comment": null, "summary": "Computational cognitive architectures are broadly scoped models of the human\nmind that combine different psychological functionalities (as well as often\ndifferent computational methods for these different functionalities) into one\nunified framework. They structure them in a psychologically plausible and\nvalidated way. However, such models thus far have only limited computational\ncapabilities, mostly limited by the computational tools and techniques that\nwere adopted. More recently, LLMs have proved to be more capable\ncomputationally than any other tools. Thus, in order to deal with both\nreal-world complexity and psychological realism at the same time, incorporating\nLLMs into cognitive architectures naturally becomes an important task. In the\npresent article, a synergistic combination of the Clarion cognitive\narchitecture and LLMs is discussed as a case study. The implicit-explicit\ndichotomy that is fundamental to Clarion is leveraged for a seamless\nintegration of Clarion and LLMs. As a result, computational power of LLMs is\ncombined with psychological nicety of Clarion.", "AI": {"tldr": "The paper explores integrating Large Language Models (LLMs) into Clarion, a cognitive architecture, to enhance computational capabilities while maintaining psychological realism.", "motivation": "Traditional cognitive architectures, while striving for psychological realism, are limited in computational capabilities. With the rise of more computationally advanced LLMs, combining these with cognitive architectures could help balance real-world complexity with psychological plausibility.", "method": "The authors propose integrating LLMs with the Clarion cognitive architecture by leveraging Clarion's implicit-explicit dichotomy to facilitate a seamless synergy.", "result": "The integration combines the computational power of LLMs with the psychological strengths of the Clarion cognitive architecture, creating a framework that is both computationally robust and psychologically informed.", "conclusion": "Incorporating LLMs into cognitive architectures like Clarion provides a powerful means to address both computational demands and psychological realism, demonstrating potential for further exploration in this synergistic approach."}}
{"id": "2509.10513", "pdf": "https://arxiv.org/pdf/2509.10513", "abs": "https://arxiv.org/abs/2509.10513", "authors": ["Sugyeong Eo", "Jungjun Lee", "Chanjun Park", "Heuiseok Lim"], "title": "Mixture-of-Clustered-Experts: Advancing Expert Specialization and Generalization in Instruction Tuning", "categories": ["cs.LG"], "comment": null, "summary": "A sparse Mixture-of-Experts (MoE) architecture has emerged as a highly\nscalable solution by conditionally activating sub-modules without a\nproportional increase in computational costs. However, improving expert\nspecialization to enhance performance and generalization remains a challenge\nfor MoE, especially in instruction tuning scenarios characterized by\nsignificant input heterogeneity. In this work, we propose the\nMixture-of-Clustered-Experts (MoCE) to address this limitation through a\ndual-stage routing mechanism. The first stage in the mechanism performs expert\ngroup routing based on sequence-level features, while the second stage\nactivates the top-$k$ experts within the group at the token level. This\napproach enables the effective partitioning of heterogeneous inputs based on\ntheir knowledge requirements, encouraging expert group specialization while\nmaintaining the advantages of token-level routing. We evaluate MoCE across a\ncomprehensive set of benchmarks, demonstrating its consistent superiority over\nstrong baselines and its enhanced generalization capabilities. Detailed\nanalysis further highlights the robustness and effectiveness of MoCE.", "AI": {"tldr": "The paper introduces Mixture-of-Clustered-Experts (MoCE), a dual-stage routing mechanism that improves over traditional Mixture-of-Experts by enhancing expert specialization for instruction tuning scenarios.", "motivation": "To address the challenge of improving expert specialization and generalization in Mixture-of-Experts (MoE), especially in instruction-tuning scenarios with heterogeneous inputs.", "method": "Proposed a dual-stage routing mechanism in MoCE where sequence-level features are used for expert group routing, followed by token-level activation of top-k experts within the group.", "result": "Demonstrates superior performance and generalization capabilities of MoCE over strong baselines through comprehensive benchmarks.", "conclusion": "MoCE effectively partitions heterogeneous inputs and achieves better expert group specialization while maintaining scalability and robustness of traditional MoE models."}}
{"id": "2509.10952", "pdf": "https://arxiv.org/pdf/2509.10952", "abs": "https://arxiv.org/abs/2509.10952", "authors": ["Yangcen Liu", "Woo Chul Shin", "Yunhai Han", "Zhenyang Chen", "Harish Ravichandar", "Danfei Xu"], "title": "ImMimic: Cross-Domain Imitation from Human Videos via Mapping and Interpolation", "categories": ["cs.RO"], "comment": "Conference of Robot Learning", "summary": "Learning robot manipulation from abundant human videos offers a scalable\nalternative to costly robot-specific data collection. However, domain gaps\nacross visual, morphological, and physical aspects hinder direct imitation. To\neffectively bridge the domain gap, we propose ImMimic, an embodiment-agnostic\nco-training framework that leverages both human videos and a small amount of\nteleoperated robot demonstrations. ImMimic uses Dynamic Time Warping (DTW) with\neither action- or visual-based mapping to map retargeted human hand poses to\nrobot joints, followed by MixUp interpolation between paired human and robot\ntrajectories. Our key insights are (1) retargeted human hand trajectories\nprovide informative action labels, and (2) interpolation over the mapped data\ncreates intermediate domains that facilitate smooth domain adaptation during\nco-training. Evaluations on four real-world manipulation tasks (Pick and Place,\nPush, Hammer, Flip) across four robotic embodiments (Robotiq, Fin Ray, Allegro,\nAbility) show that ImMimic improves task success rates and execution\nsmoothness, highlighting its efficacy to bridge the domain gap for robust robot\nmanipulation. The project website can be found at\nhttps://sites.google.com/view/immimic.", "AI": {"tldr": "The paper proposes ImMimic, a framework to teach robots manipulation tasks using human video data and a small set of robot demonstrations, addressing domain gaps via retargeting and interpolation methods.", "motivation": "Teaching robots through direct human video data is scalable but challenged by domain differences, such as visual and physical discrepancies, preventing direct imitation.", "method": "The approach, ImMimic, utilizes Dynamic Time Warping to map human hand movements to robot joints and applies MixUp interpolation to create intermediate domains for co-training.", "result": "ImMimic shows improved task success rates and smoother executions across various manipulation tasks and robot embodiments, demonstrating its effectiveness.", "conclusion": "ImMimic successfully bridges the gap between human and robotic domains, making robot learning from human videos feasible and effective for real-world applications."}}
{"id": "2509.10746", "pdf": "https://arxiv.org/pdf/2509.10746", "abs": "https://arxiv.org/abs/2509.10746", "authors": ["Adarsh Srinivasan", "Jacob Dineen", "Muhammad Umar Afzal", "Muhammad Uzair Sarfraz", "Irbaz B. Riaz", "Ben Zhou"], "title": "RECAP: Transparent Inference-Time Emotion Alignment for Medical Dialogue Systems", "categories": ["cs.CL"], "comment": null, "summary": "Large language models in healthcare often miss critical emotional cues,\ndelivering medically sound but emotionally flat advice. This is especially\nproblematic in clinical contexts where patients are distressed and vulnerable,\nand require empathic communication to support safety, adherence, and trust. We\npresent RECAP (Reflect-Extract-Calibrate-Align-Produce), an inference-time\nframework that adds structured emotional reasoning without retraining. By\ndecomposing empathy into transparent appraisal-theoretic stages and exposing\nper-dimension Likert signals, RECAP produces nuanced, auditable responses.\nAcross EmoBench, SECEU, and EQ-Bench, RECAP improves emotional reasoning by\n22-28% on 8B models and 10-13% on larger models over zero-shot baselines.\nClinician evaluations further confirm superior empathetic communication. RECAP\nshows that modular, theory-grounded prompting can systematically enhance\nemotional intelligence in medical AI while preserving the accountability\nrequired for deployment.", "AI": {"tldr": "This paper proposes RECAP, a framework for enhancing emotional reasoning in healthcare language models without retraining.", "motivation": "Address the frequent lack of emotional understanding in healthcare language models, which impacts patient trust and adherence.", "method": "Introduce RECAP, a framework that adds structured emotional reasoning using appraisal-theoretic stages and Likert signals during inference.", "result": "RECAP improves emotional reasoning by 22-28% in smaller models and 10-13% in larger models across multiple benchmarks, validated by clinicians.", "conclusion": "Theory-grounded, modular frameworks like RECAP can enhance medical AI's emotional intelligence while maintaining accountability for deployment."}}
{"id": "2509.11312", "pdf": "https://arxiv.org/pdf/2509.11312", "abs": "https://arxiv.org/abs/2509.11312", "authors": ["Wenchao Gu", "Yupan Chen", "Yanlin Wang", "Hongyu Zhang", "Cuiyun Gao", "Michael R. Lyu"], "title": "Weakly Supervised Vulnerability Localization via Multiple Instance Learning", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Software vulnerability detection has emerged as a significant concern in the\nfield of software security recently, capturing the attention of numerous\nresearchers and developers. Most previous approaches focus on coarse-grained\nvulnerability detection, such as at the function or file level. However, the\ndevelopers would still encounter the challenge of manually inspecting a large\nvolume of code inside the vulnerable function to identify the specific\nvulnerable statements for modification, indicating the importance of\nvulnerability localization. Training the model for vulnerability localization\nusually requires ground-truth labels at the statement-level, and labeling\nvulnerable statements demands expert knowledge, which incurs high costs. Hence,\nthe demand for an approach that eliminates the need for additional labeling at\nthe statement-level is on the rise. To tackle this problem, we propose a novel\napproach called WAVES for WeAkly supervised Vulnerability Localization via\nmultiplE inStance learning, which does not need the additional statement-level\nlabels during the training. WAVES has the capability to determine whether a\nfunction is vulnerable (i.e., vulnerability detection) and pinpoint the\nvulnerable statements (i.e., vulnerability localization). Specifically,\ninspired by the concept of multiple instance learning, WAVES converts the\nground-truth label at the function-level into pseudo labels for individual\nstatements, eliminating the need for additional statement-level labeling. These\npseudo labels are utilized to train the classifiers for the function-level\nrepresentation vectors. Extensive experimentation on three popular benchmark\ndatasets demonstrates that, in comparison to previous baselines, our approach\nachieves comparable performance in vulnerability detection and state-of-the-art\nperformance in statement-level vulnerability localization.", "AI": {"tldr": "The paper introduces WAVES, an approach for vulnerability localization that uses function-level labels to predict statement-level vulnerabilities without requiring additional labeling efforts.", "motivation": "Existing methods in software vulnerability detection are coarse-grained at the function or file level, leaving developers with extensive manual work to localize specific vulnerable statements.", "method": "WAVES uses a technique called weakly supervised learning combined with multiple instance learning to derive pseudo labels from function-level ground-truth labels. These pseudo labels serve to train models for identifying vulnerable code statements.", "result": "Experiments on three benchmark datasets show WAVES offers comparable vulnerability detection performance and outstanding statement-level vulnerability localization compared to prior methods.", "conclusion": "WAVES addresses the high costs of manual labeling at the statement-level, providing an efficient and effective tool for vulnerability detection and localization."}}
{"id": "2509.10779", "pdf": "https://arxiv.org/pdf/2509.10779", "abs": "https://arxiv.org/abs/2509.10779", "authors": ["Yilun Xiao"], "title": "Group Evidence Matters: Tiling-based Semantic Gating for Dense Object Detection", "categories": ["cs.CV"], "comment": "8 pages, 7 figures", "summary": "Dense small objects in UAV imagery are often missed due to long-range\nviewpoints, occlusion, and clutter[cite: 5]. This paper presents a\ndetector-agnostic post-processing framework that converts overlap-induced\nredundancy into group evidence[cite: 6]. Overlapping tiling first recovers\nlow-confidence candidates[cite: 7]. A Spatial Gate (DBSCAN on box centroids)\nand a Semantic Gate (DBSCAN on ResNet-18 embeddings) then validates group\nevidence[cite: 7]. Validated groups receive controlled confidence reweighting\nbefore class-aware NMS fusion[cite: 8]. Experiments on VisDrone show a recall\nincrease from 0.685 to 0.778 (+0.093) and a precision adjustment from 0.801 to\n0.595, yielding F1=0.669[cite: 9]. Post-processing latency averages 0.095 s per\nimage[cite: 10]. These results indicate recall-first, precision-trade-off\nbehavior that benefits recall-sensitive applications such as far-field counting\nand monitoring[cite: 10]. Ablation confirms that tiling exposes missed objects,\nspatial clustering stabilizes geometry, semantic clustering enforces appearance\ncoherence, and reweighting provides calibrated integration with the\nbaseline[cite: 11]. The framework requires no retraining and integrates with\nmodern detectors[cite: 12]. Future work will reduce semantic gating cost and\nextend the approach with temporal cues[cite: 13].", "AI": {"tldr": "This paper introduces a post-processing framework to boost recall in dense small object detection from UAV imagery by leveraging overlapping tiling and clustering steps without retraining detectors.", "motivation": "Dense small objects in UAV imagery are often missed due to challenges like long-range viewpoints, occlusion, and clutter, necessitating methods to recover these missed detections.", "method": "The proposed framework utilizes overlapping tiling to expose low-confidence candidates, Spatial and Semantic Gates (based on DBSCAN clustering) for geometry and appearance validation, and controlled confidence reweighting before non-maximum suppression fusion.", "result": "Experiments on the VisDrone dataset report a recall improvement from 0.685 to 0.778 (+0.093), with a reduction in precision but yielding an F1 score of 0.669. The average post-processing latency is 0.095 seconds per image.", "conclusion": "The proposed approach enhances recall for applications requiring high recall, such as monitoring and counting in far-field UAV imagery, and is detector-agnostic, requiring no retraining. Future work aims to reduce semantic clustering costs and incorporate temporal information."}}
{"id": "2509.11962", "pdf": "https://arxiv.org/pdf/2509.11962", "abs": "https://arxiv.org/abs/2509.11962", "authors": ["Mika Sipil\u00e4", "Klaus Nordhausen", "Sara Taskinen"], "title": "Identifiable Autoregressive Variational Autoencoders for Nonlinear and Nonstationary Spatio-Temporal Blind Source Separation", "categories": ["stat.ML", "cs.LG", "stat.ME"], "comment": null, "summary": "The modeling and prediction of multivariate spatio-temporal data involve\nnumerous challenges. Dimension reduction methods can significantly simplify\nthis process, provided that they account for the complex dependencies between\nvariables and across time and space. Nonlinear blind source separation has\nemerged as a promising approach, particularly following recent advances in\nidentifiability results. Building on these developments, we introduce the\nidentifiable autoregressive variational autoencoder, which ensures the\nidentifiability of latent components consisting of nonstationary autoregressive\nprocesses. The blind source separation efficacy of the proposed method is\nshowcased through a simulation study, where it is compared against\nstate-of-the-art methods, and the spatio-temporal prediction performance is\nevaluated against several competitors on air pollution and weather datasets.", "AI": {"tldr": "This paper introduces an identifiable autoregressive variational autoencoder for modeling multivariate spatio-temporal data, emphasizing blind source separation and spatio-temporal prediction performance.", "motivation": "The need to address challenges in modeling complex spatio-temporal relationships in multivariate data and leveraging recent advances in nonlinear blind source separation.", "method": "An identifiable autoregressive variational autoencoder is proposed, designed to handle nonstationary autoregressive processes while ensuring identifiability of latent components.", "result": "Simulation studies and evaluations on air pollution and weather datasets show improved blind source separation and prediction performance compared to state-of-the-art methods.", "conclusion": "The proposed method proves effective in handling complex spatio-temporal dependencies and demonstrates its potential as a robust tool in multivariate spatio-temporal modeling and prediction."}}
{"id": "2509.12138", "pdf": "https://arxiv.org/pdf/2509.12138", "abs": "https://arxiv.org/abs/2509.12138", "authors": ["Mengjiao Han", "Andres Sewell", "Joseph Insley", "Janet Knowles", "Victor A. Mateevitsi", "Michael E. Papka", "Steve Petruzza", "Silvio Rizzi"], "title": "Distributed 3D Gaussian Splatting for High-Resolution Isosurface Visualization", "categories": ["cs.DC"], "comment": null, "summary": "3D Gaussian Splatting (3D-GS) has recently emerged as a powerful technique\nfor real-time, photorealistic rendering by optimizing anisotropic Gaussian\nprimitives from view-dependent images. While 3D-GS has been extended to\nscientific visualization, prior work remains limited to single-GPU settings,\nrestricting scalability for large datasets on high-performance computing (HPC)\nsystems. We present a distributed 3D-GS pipeline tailored for HPC. Our approach\npartitions data across nodes, trains Gaussian splats in parallel using\nmulti-nodes and multi-GPUs, and merges splats for global rendering. To\neliminate artifacts, we add ghost cells at partition boundaries and apply\nbackground masks to remove irrelevant pixels. Benchmarks on the\nRichtmyer-Meshkov datasets (about 106.7M Gaussians) show up to 3X speedup\nacross 8 nodes on Polaris while preserving image quality. These results\ndemonstrate that distributed 3D-GS enables scalable visualization of\nlarge-scale scientific data and provide a foundation for future in situ\napplications.", "AI": {"tldr": "The paper proposes a distributed 3D Gaussian Splatting (3D-GS) pipeline for scalable scientific visualization on high-performance computing (HPC) systems, achieving significant speedup while maintaining image quality.", "motivation": "The motivation is to overcome the limitations of existing 3D Gaussian Splatting approaches for scientific visualization, which are restricted to single-GPU systems and cannot efficiently handle large datasets on HPC systems.", "method": "The authors designed a distributed 3D-GS pipeline that partitions data across multiple nodes, enables parallel training of Gaussian splats on multi-nodes and multi-GPUs, and combines the results for global rendering. They also introduce ghost cells to handle partition boundaries and background masks to filter irrelevant pixels.", "result": "The proposed approach achieved up to a 3X speedup using 8 nodes on an HPC system (Polaris) with the Richtmyer-Meshkov dataset, which contains approximately 106.7 million Gaussians, while preserving image quality.", "conclusion": "Distributed 3D-GS facilitates scalable visualization of large-scale scientific datasets, demonstrating its potential for future in situ applications."}}
{"id": "2509.11026", "pdf": "https://arxiv.org/pdf/2509.11026", "abs": "https://arxiv.org/abs/2509.11026", "authors": ["Ziang Li", "Manasi Ganti", "Zixian Ma", "Helena Vasconcelos", "Qijia He", "Ranjay Krishna"], "title": "Rethinking Human Preference Evaluation of LLM Rationales", "categories": ["cs.AI", "cs.CL"], "comment": "Published in the XLLM-Reason-Plan Workshop on the Application of LLM\n  Explainability to Reasoning and Planning at COLM 2025", "summary": "Large language models (LLMs) often generate natural language rationales --\nfree-form explanations that help improve performance on complex reasoning tasks\nand enhance interpretability for human users. However, evaluating these\nrationales remains challenging. While recent work has relied on binary\npreference judgments from humans or LLM judges, such evaluations are often\nopaque and coarse-grained, offering limited insight into what makes one\nrationale better than another. In this work, we rethink preference evaluation\nfor LLM-generated rationales by asking: (1) What attributes define good\nrationales? (2) Can human preferences be explained by these attributes? (3) Can\nattribute-based evaluation overcome the limitations of binary comparisons? We\nidentify a set of key rationale attributes from prior literature and assess\nthem using automatic metrics, LLM judgments, and human annotations. We then\nanalyze two standard human preference datasets MT Bench and Chatbot Arena using\nSHAP to identify which attributes best explain human preference outcomes.\nFinally, we re-evaluate model-generated rationales using attribute-specific ELO\nscores, revealing more nuanced model comparisons and insights. Our findings\nsuggest that fine-grained attribute evaluations can better characterize\nrationale quality and guide future research toward more interpretable and\nreliable evaluation practices.", "AI": {"tldr": "The paper investigates how to improve the evaluation of natural language rationales generated by large language models (LLMs) by examining fine-grained attributes rather than relying solely on binary preference judgments.", "motivation": "Evaluating natural language rationales from LLMs is challenging because existing binary judgments are opaque and coarse-grained, offering little detailed insight into rationale quality.", "method": "The authors identify key attributes defining good rationales from prior literature, assess these attributes using automatic metrics, LLM judgments, and human annotations, and analyze datasets using SHAP to determine which attributes influence human preferences. They also re-evaluate rationales using attribute-specific ELO scores.", "result": "The study demonstrates that fine-grained evaluations of rationale attributes provide deeper insights into rationale quality and better explain human preference outcomes.", "conclusion": "Fine-grained attribute-based evaluation is a more nuanced and effective approach to assessing rationale quality, advancing interpretability and reliability in LLM-generated explanations."}}
{"id": "2509.10514", "pdf": "https://arxiv.org/pdf/2509.10514", "abs": "https://arxiv.org/abs/2509.10514", "authors": ["Shaoxin Tian", "Hongkai Liu", "Yuying Yang", "Jiali Yu", "Zizheng Miao", "Xuming Huang", "Zhishuai Liu", "Zhang Yi"], "title": "A Differential Manifold Perspective and Universality Analysis of Continuous Attractors in Artificial Neural Networks", "categories": ["cs.LG"], "comment": null, "summary": "Continuous attractors are critical for information processing in both\nbiological and artificial neural systems, with implications for spatial\nnavigation, memory, and deep learning optimization. However, existing research\nlacks a unified framework to analyze their properties across diverse dynamical\nsystems, limiting cross-architectural generalizability. This study establishes\na novel framework from the perspective of differential manifolds to investigate\ncontinuous attractors in artificial neural networks. It verifies compatibility\nwith prior conclusions, elucidates links between continuous attractor phenomena\nand eigenvalues of the local Jacobian matrix, and demonstrates the universality\nof singular value stratification in common classification models and datasets.\nThese findings suggest continuous attractors may be ubiquitous in general\nneural networks, highlighting the need for a general theory, with the proposed\nframework offering a promising foundation given the close mathematical\nconnection between eigenvalues and singular values.", "AI": {"tldr": "The paper introduces a novel framework using differential manifolds to study continuous attractors in neural systems, aiming for cross-architectural generalizability.", "motivation": "Existing studies lack a unified framework for analyzing continuous attractors across diverse dynamical systems, limiting applicability to biological and artificial neural systems.", "method": "The paper applies differential manifold theory to analyze continuous attractors, linking them to local Jacobian matrix eigenvalues and exploring singular value stratification in classification models.", "result": "Compatibility with prior work is verified, along with universal applicability of singular value stratification in neural networks and datasets.", "conclusion": "Continuous attractors likely occur ubiquitously in neural networks, necessitating a general theory to unify observations; the proposed framework serves as a strong mathematical foundation."}}
{"id": "2509.10968", "pdf": "https://arxiv.org/pdf/2509.10968", "abs": "https://arxiv.org/abs/2509.10968", "authors": ["Leo Cazenille", "Loona Macabre", "Nicolas Bredeche"], "title": "Pogosim -- a Simulator for Pogobot robots", "categories": ["cs.RO"], "comment": "18 pages, 1 table, 7 figures", "summary": "Pogobots are a new type of open-source/open-hardware robots specifically\ndesigned for swarm robotics research. Their cost-effective and modular design,\ncomplemented by vibration-based and wheel-based locomotion, fast infrared\ncommunication and extensive software architecture facilitate the implementation\nof swarm intelligence algorithms. However, testing even simple distributed\nalgorithms directly on robots is particularly labor-intensive. Scaling to more\ncomplex problems or calibrate user code parameters will have a prohibitively\nhigh strain on available resources. In this article we present Pogosim, a fast\nand scalable simulator for Pogobots, designed to reduce as much as possible\nalgorithm development costs. The exact same code will be used in both\nsimulation and to experimentally drive real robots. This article details the\nsoftware architecture of Pogosim, explain how to write configuration files and\nuser programs and how simulations approximate or differ from experiments. We\ndescribe how a large set of simulations can be launched in parallel, how to\nretrieve and analyze the simulation results, and how to optimize user code\nparameters using optimization algorithms.", "AI": {"tldr": "The paper introduces Pogosim, a simulator for Pogobots, aiding in swarm robotic algorithm development efficiently by minimizing manual testing and allowing seamless integration with real-world robots.", "motivation": "Developing swarm robotics algorithms on physical robots is labor-intensive and resource-draining, especially as complexity scales.", "method": "The authors designed Pogosim, a simulation tool for Pogobots that shares the same codebase with the real robots, thereby ensuring streamlined transitions and enabling parallel simulation execution.", "result": "Pogosim accelerates the development process, enables efficient parameter optimization for user code, and simplifies the transition between simulation and real-world application on Pogobots.", "conclusion": "Pogosim is a cost-effective tool for swarm robotics, enabling scalable, efficient testing and algorithm optimization while maintaining fidelity to real-world implementations."}}
{"id": "2509.10798", "pdf": "https://arxiv.org/pdf/2509.10798", "abs": "https://arxiv.org/abs/2509.10798", "authors": ["Yijun Liu", "Yixuan Wang", "Yuzhuang Xu", "Shiyu Ji", "Yang Xu", "Qingfu Zhu", "Wanxiang Che"], "title": "Judge Q: Trainable Queries for Optimized Information Retention in KV Cache Eviction", "categories": ["cs.CL", "cs.AI"], "comment": "preprint", "summary": "Large language models (LLMs) utilize key-value (KV) cache to store historical\ninformation during sequence processing. The size of KV cache grows linearly as\nthe length of the sequence extends, which seriously affects memory usage and\ndecoding efficiency. Current methods for KV cache eviction typically utilize\nthe last window from the pre-filling phase as queries to compute the KV\nimportance scores for eviction. Although this scheme is simple to implement, it\ntends to overly focus on local information, potentially leading to the neglect\nor omission of crucial global information. To mitigate this issue, we propose\nJudge Q, a novel training method which incorporates a soft token list. This\nmethod only tunes the model's embedding layer at a low training cost. By\nconcatenating the soft token list at the end of the input sequence, we train\nthese tokens' attention map to the original input sequence to align with that\nof the actual decoded tokens. In this way, the queries corresponding to the\nsoft tokens can effectively capture global information and better evaluate the\nimportance of the keys and values within the KV cache, thus maintaining\ndecoding quality when KV cache is evicted. Under the same eviction budget, our\nmethod exhibits less performance degradation compared to existing eviction\napproaches. We validate our approach through experiments conducted on models\nsuch as Llama-3.1-8B-Instruct and Mistral-7B-Instruct-v0.3, using benchmarks\nincluding LongBench, RULER, and Needle-in-a-Haystack. Results indicate an\nimprovement of approximately 1 point on the LongBench and over 3 points on\nRULER. This proposed methodology can be seamlessly integrated into existing\nopen-source models with minimal training overhead, thereby enhancing\nperformance in KV cache eviction scenarios.", "AI": {"tldr": "The paper proposes \"Judge Q,\" a training method to enhance KV cache eviction in LLMs by integrating global information through a soft token list. Experiments show improved performance on benchmarks like LongBench and RULER.", "motivation": "Managing memory and decoding efficiency in LLMs is challenging due to the linear growth of KV cache size as sequences expand. Current eviction methods often neglect global information, which hinders overall performance.", "method": "The authors introduce \"Judge Q,\" a method using soft token lists appended to input sequences. These tokens train attention maps to align with original decoded tokens, facilitating better evaluation of KV cache importance.", "result": "Under a similar eviction budget, \"Judge Q\" reduces performance degradation in KV cache eviction compared to existing approaches. Experimental validation on Llama-3.1-8B and Mistral-7B models shows improvements of ~1 point on LongBench and ~3 points on RULER.", "conclusion": "\"Judge Q\" is an effective, lightweight method for improving KV cache eviction in LLMs, enabling better decoding quality and seamless integration into open-source models."}}
{"id": "2509.11446", "pdf": "https://arxiv.org/pdf/2509.11446", "abs": "https://arxiv.org/abs/2509.11446", "authors": ["Mohammad Amin Zadenoori", "Jacek D\u0105browski", "Waad Alhoshan", "Liping Zhao", "Alessio Ferrari"], "title": "Large Language Models (LLMs) for Requirements Engineering (RE): A Systematic Literature Review", "categories": ["cs.SE", "D.2.0; D.2.1; I.2.7"], "comment": null, "summary": "Large Language Models (LLMs) are finding applications in numerous domains,\nand Requirements Engineering (RE) is increasingly benefiting from their\ncapabilities to assist with complex, language-intensive tasks. This paper\npresents a systematic literature review of 74 primary studies published between\n2023 and 2024, examining how LLMs are being applied in RE. The study\ncategorizes the literature according to several dimensions, including\npublication trends, RE activities, prompting strategies, and evaluation\nmethods. Our findings indicate notable patterns, among which we observe\nsubstantial differences compared to previous works leveraging standard Natural\nLanguage Processing (NLP) techniques. Most of the studies focus on using LLMs\nfor requirements elicitation and validation, rather than defect detection and\nclassification, which were dominant in the past. Researchers have also\nbroadened their focus and addressed novel tasks, e.g., test generation,\nexploring the integration of RE with other software engineering (SE)\ndisciplines. Although requirements specifications remain the primary focus,\nother artifacts are increasingly considered, including issues from issue\ntracking systems, regulations, and technical manuals. The studies mostly rely\non GPT-based models, and often use Zero-shot or Few-shot prompting. They are\nusually evaluated in controlled environments, with limited use in industry\nsettings and limited integration in complex workflows. Our study outlines\nimportant future directions, such as leveraging the potential to expand the\ninfluence of RE in SE, exploring less-studied tasks, improving prompting\nmethods, and testing in real-world environments. Our contribution also helps\nresearchers and practitioners use LLMs more effectively in RE, by providing a\nlist of identified tools leveraging LLMs for RE, as well as datasets.", "AI": {"tldr": "The paper reviews 74 studies from 2023-2024 on using Large Language Models (LLMs) in Requirements Engineering (RE), exploring publication trends, activities, strategies, and methods. It highlights shifts in focus, underlining challenges and opportunities.", "motivation": "LLMs have potential to enhance complex language-intensive processes in RE, prompting a need to systematically evaluate their applications for advancing the field.", "method": "A systematic literature review categorizing 74 studies by dimensions such as publication trends, RE activities, prompting approaches, and evaluation strategies.", "result": "LLMs are primarily leveraged for requirements elicitation and validation, showing differences from prior NLP approaches. Notable gaps include limited industrial application and integration.", "conclusion": "Future research should focus on real-world testing, improved prompting methods, and new applications to maximize LLMs' relevance to RE and integration with broader software engineering workflows."}}
{"id": "2509.10813", "pdf": "https://arxiv.org/pdf/2509.10813", "abs": "https://arxiv.org/abs/2509.10813", "authors": ["Weipeng Zhong", "Peizhou Cao", "Yichen Jin", "Li Luo", "Wenzhe Cai", "Jingli Lin", "Hanqing Wang", "Zhaoyang Lyu", "Tai Wang", "Bo Dai", "Xudong Xu", "Jiangmiao Pang"], "title": "InternScenes: A Large-scale Simulatable Indoor Scene Dataset with Realistic Layouts", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "The advancement of Embodied AI heavily relies on large-scale, simulatable 3D\nscene datasets characterized by scene diversity and realistic layouts. However,\nexisting datasets typically suffer from limitations in data scale or diversity,\nsanitized layouts lacking small items, and severe object collisions. To address\nthese shortcomings, we introduce \\textbf{InternScenes}, a novel large-scale\nsimulatable indoor scene dataset comprising approximately 40,000 diverse scenes\nby integrating three disparate scene sources, real-world scans, procedurally\ngenerated scenes, and designer-created scenes, including 1.96M 3D objects and\ncovering 15 common scene types and 288 object classes. We particularly preserve\nmassive small items in the scenes, resulting in realistic and complex layouts\nwith an average of 41.5 objects per region. Our comprehensive data processing\npipeline ensures simulatability by creating real-to-sim replicas for real-world\nscans, enhances interactivity by incorporating interactive objects into these\nscenes, and resolves object collisions by physical simulations. We demonstrate\nthe value of InternScenes with two benchmark applications: scene layout\ngeneration and point-goal navigation. Both show the new challenges posed by the\ncomplex and realistic layouts. More importantly, InternScenes paves the way for\nscaling up the model training for both tasks, making the generation and\nnavigation in such complex scenes possible. We commit to open-sourcing the\ndata, models, and benchmarks to benefit the whole community.", "AI": {"tldr": "InternScenes is a large-scale, diverse, and realistic 3D indoor scene dataset containing approximately 40,000 scenes, designed for Embodied AI applications.", "motivation": "The paper aims to address the limitations of existing 3D scene datasets, which often lack scale, diversity, realistic small-item layouts, and suffer from object collisions.", "method": "The authors introduce InternScenes by integrating scenes from real-world scans, procedural generation, and designer-created layouts. They utilize a comprehensive processing pipeline to enhance simulatability, add interactive objects, and resolve object collisions.", "result": "InternScenes comprises 40,000 scenes with 1.96 million objects across 288 object classes, featuring realistic layouts with an average of 41.5 objects per region. Benchmarks show the dataset enables training and poses new challenges for scene layout generation and point-goal navigation.", "conclusion": "InternScenes advances the field of Embodied AI by providing a comprehensive and scalable dataset, facilitating both model training and navigation in complex indoor environments. The open-sourcing of data, models, and benchmarks benefits the research community."}}
{"id": "2509.12166", "pdf": "https://arxiv.org/pdf/2509.12166", "abs": "https://arxiv.org/abs/2509.12166", "authors": ["Francesco Amato", "Julien Jacques"], "title": "MMM: Clustering Multivariate Longitudinal Mixed-type Data", "categories": ["stat.ML", "cs.LG", "stat.ME"], "comment": null, "summary": "Multivariate longitudinal data of mixed-type are increasingly collected in\nmany science domains. However, algorithms to cluster this kind of data remain\nscarce, due to the challenge to simultaneously model the within- and\nbetween-time dependence structures for multivariate data of mixed kind. We\nintroduce the Mixture of Mixed-Matrices (MMM) model: reorganizing the data in a\nthree-way structure and assuming that the non-continuous variables are\nobservations of underlying latent continuous variables, the model relies on a\nmixture of matrix-variate normal distributions to perform clustering in the\nlatent dimension. The MMM model is thus able to handle continuous, ordinal,\nbinary, nominal and count data and to concurrently model the heterogeneity, the\nassociation among the responses and the temporal dependence structure in a\nparsimonious way and without assuming conditional independence. The inference\nis carried out through an MCMC-EM algorithm, which is detailed. An evaluation\nof the model through synthetic data shows its inference abilities. A real-world\napplication on financial data is presented.", "AI": {"tldr": "The paper introduces the Mixture of Mixed-Matrices (MMM) model to cluster multivariate longitudinal mixed-type data by modeling dependencies and heterogeneity. An MCMC-EM algorithm is used for inference, with synthetic and real-world data evaluations presented.", "motivation": "To address the scarcity of clustering algorithms capable of handling multivariate longitudinal data of mixed types due to the challenges in modeling complex data dependencies and types.", "method": "The MMM model reorganizes data into a three-way structure, assumes non-continuous variables are latent continuous variables, and performs clustering via a mixture of matrix-variate normal distributions. Inference is achieved through an MCMC-EM algorithm.", "result": "The proposed MMM model demonstrates inference abilities on synthetic data and is successfully applied to a real-world financial dataset, showcasing its utility and robustness.", "conclusion": "The MMM model effectively clusters multivariate longitudinal mixed-type data, modeling heterogeneity, response associations, and temporal dependencies without conditional independence assumptions."}}
{"id": "2509.12141", "pdf": "https://arxiv.org/pdf/2509.12141", "abs": "https://arxiv.org/abs/2509.12141", "authors": ["Weihao Zhu", "Long Shi", "Kang Wei", "Zhen Mei", "Zhe Wang", "Jiaheng Wang", "Jun Li"], "title": "When MoE Meets Blockchain: A Trustworthy Distributed Framework of Large Models", "categories": ["cs.DC"], "comment": null, "summary": "As an enabling architecture of Large Models (LMs), Mixture of Experts (MoE)\nhas become prevalent thanks to its sparsely-gated mechanism, which lowers\ncomputational overhead while maintaining learning performance comparable to\ndense LMs. The essence of MoE lies in utilizing a group of neural networks\n(called experts) with each specializing in different types of tasks, along with\na trainable gating network that selectively activates a subset of these experts\nto handle specific tasks. Traditional cloud-based MoE encounters challenges\nsuch as prolonged response latency, high bandwidth consumption, and data\nprivacy leakage. To address these issues, researchers have proposed to deploy\nMoE over distributed edge networks. However, a key concern of distributed MoE\nframeworks is the lack of trust in data interactions among distributed experts\nwithout the surveillance of any trusted authority, and thereby prone to\npotential attacks such as data manipulation. In response to the security issues\nof traditional distributed MoE, we propose a blockchain-aided trustworthy MoE\n(B-MoE) framework that consists of three layers: the edge layer, the blockchain\nlayer, and the storage layer. In this framework, the edge layer employs the\nactivated experts downloaded from the storage layer to process the learning\ntasks, while the blockchain layer functions as a decentralized trustworthy\nnetwork to trace, verify, and record the computational results of the experts\nfrom the edge layer. The experimental results demonstrate that B-MoE is more\nrobust to data manipulation attacks than traditional distributed MoE during\nboth the training and inference processes.", "AI": {"tldr": "The paper introduces a blockchain-aided Mixture of Experts framework (B-MoE) to improve trustworthiness and security of distributed MoE architectures on edge networks.", "motivation": "Traditional cloud-based Mixture of Experts (MoE) architectures face issues like high latency, bandwidth usage, and data privacy concerns when deployed on edge networks.", "method": "The B-MoE framework integrates three layers: edge, blockchain, and storage. The edge layer processes tasks using distributed experts, the blockchain layer ensures decentralized trust and verification, and the storage layer holds the necessary models.", "result": "Experimental outcomes showed that the B-MoE framework is more resistant to data manipulation attacks during both training and inference compared to conventional distributed MoE setups.", "conclusion": "Using blockchain, the B-MoE framework enhances the security and robustness of distributed MoE architectures, addressing critical trust issues in edge network scenarios."}}
{"id": "2509.11035", "pdf": "https://arxiv.org/pdf/2509.11035", "abs": "https://arxiv.org/abs/2509.11035", "authors": ["Yu Cui", "Hang Fu", "Haibin Zhang", "Licheng Wang", "Cong Zuo"], "title": "Free-MAD: Consensus-Free Multi-Agent Debate", "categories": ["cs.AI", "cs.CR"], "comment": null, "summary": "Multi-agent debate (MAD) is an emerging approach to improving the reasoning\ncapabilities of large language models (LLMs). Existing MAD methods rely on\nmultiple rounds of interaction among agents to reach consensus, and the final\noutput is selected by majority voting in the last round. However, this\nconsensus-based design faces several limitations. First, multiple rounds of\ncommunication increases token overhead and limits scalability. Second, due to\nthe inherent conformity of LLMs, agents that initially produce correct\nresponses may be influenced by incorrect ones during the debate process,\ncausing error propagation. Third, majority voting introduces randomness and\nunfairness in the decision-making phase, and can degrade the reasoning\nperformance.\n  To address these issues, we propose \\textsc{Free-MAD}, a novel MAD framework\nthat eliminates the need for consensus among agents. \\textsc{Free-MAD}\nintroduces a novel score-based decision mechanism that evaluates the entire\ndebate trajectory rather than relying on the last round only. This mechanism\ntracks how each agent's reasoning evolves, enabling more accurate and fair\noutcomes. In addition, \\textsc{Free-MAD} reconstructs the debate phase by\nintroducing anti-conformity, a mechanism that enables agents to mitigate\nexcessive influence from the majority. Experiments on eight benchmark datasets\ndemonstrate that \\textsc{Free-MAD} significantly improves reasoning performance\nwhile requiring only a single-round debate and thus reducing token costs. We\nalso show that compared to existing MAD approaches, \\textsc{Free-MAD} exhibits\nimproved robustness in real-world attack scenarios.", "AI": {"tldr": "The paper introduces Free-MAD, a novel multi-agent debate framework improving reasoning in large language models by avoiding consensus and introducing score-based evaluations and anti-conformity mechanisms.", "motivation": "Current multi-agent debate methods suffer from scalability issues, error propagation due to conformity, and randomness/unfairness during decision-making, prompting a need for better frameworks.", "method": "Free-MAD avoids consensus and introduces a score-based mechanism to evaluate the whole debate trajectory. Anti-conformity is introduced to counteract influence from the majority, and only single-round debates are utilized.", "result": "Free-MAD improves reasoning performance across eight benchmarks, reduces token costs through single-round debates, and shows enhanced robustness against real-world attack scenarios compared to existing methods.", "conclusion": "Free-MAD provides a scalable and more robust alternative to existing multi-agent debate frameworks, addressing key limitations and improving overall performance."}}
{"id": "2509.10515", "pdf": "https://arxiv.org/pdf/2509.10515", "abs": "https://arxiv.org/abs/2509.10515", "authors": ["Xiaobo Wang", "Zixia Jia", "Jiaqi Li", "Qi Liu", "Zilong Zheng"], "title": "Adaptive Preference Optimization with Uncertainty-aware Utility Anchor", "categories": ["cs.LG"], "comment": "Accepted by EMNLP 2025 Findings", "summary": "Offline preference optimization methods are efficient for large language\nmodels (LLMs) alignment. Direct Preference optimization (DPO)-like learning,\none of the most popular approaches, stands out for its efficiency in reward\nmodeling. However, these methods typically follow the convention to use\nBradley-Terry (BT) reward modeling that faces several critical assumptions,\nincluding the requirement for pairwise training data, model distribution\nshifting, human rationality assumption, etc. To address these limitations, we\npropose a general framework for offline preference optimization methods,\nAdaptive Preference Optimization with Utility Anchor (UAPO), which introduces\nan anchoring function to estimate the uncertainties brought from preference\ndata annotation. Our method enables training even in scenarios where the data\nis unpaired, significantly enhancing data utilization efficiency. Moreover, the\nanchor design makes UAPO more robust in the training process. Experimental\nresults demonstrate that UAPO achieves competitive outcomes without the strict\ndependency on data pairing, paving the way for more flexible and effective\npreference optimization methods.", "AI": {"tldr": "This paper introduces UAPO, a framework for offline preference optimization in LLMs, overcoming limitations of traditional Bradley-Terry-based methods and enabling robust training with unpaired data.", "motivation": "Address limitations of existing Bradley-Terry reward modeling in LLM alignment, such as reliance on paired data and assumptions about human rationality.", "method": "Introduce UAPO, a framework using an anchoring function to estimate uncertainty from preference data annotations, allowing training with unpaired data and enhancing robustness.", "result": "Experimental results show UAPO achieves competitive outcomes, utilizing data more efficiently without strict pairing constraints.", "conclusion": "UAPO provides a flexible, efficient, and robust approach to offline preference optimization in aligning LLMs."}}
{"id": "2509.10979", "pdf": "https://arxiv.org/pdf/2509.10979", "abs": "https://arxiv.org/abs/2509.10979", "authors": ["Dimitri Jacquemont", "Carlo Bosio", "Teaya Yang", "Ruiqi Zhang", "Ozgur Orun", "Shuai Li", "Reza Alam", "Thomas M. Schutzius", "Simo A. Makiharju", "Mark W. Mueller"], "title": "Autonomous Close-Proximity Photovoltaic Panel Coating Using a Quadcopter", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": "7 pages, 10 figures. Submitted to IEEE RA-L", "summary": "Photovoltaic (PV) panels are becoming increasingly widespread in the domain\nof renewable energy, and thus, small efficiency gains can have massive effects.\nAnti-reflective and self-cleaning coatings enhance panel performance but\ndegrade over time, requiring periodic reapplication. Uncrewed Aerial Vehicles\n(UAVs) offer a flexible and autonomous way to apply protective coatings more\noften and at lower cost compared to traditional manual coating methods. In this\nletter, we propose a quadcopter-based system, equipped with a liquid dispersion\nmechanism, designed to automate such tasks. The localization stack only uses\nonboard sensors, relying on visual-inertial odometry and the relative position\nof the PV panel detected with respect to the quadcopter. The control relies on\na model-based controller that accounts for the ground effect and the mass\ndecrease of the quadcopter during liquid dispersion. We validate the autonomy\ncapabilities of our system through extensive indoor and outdoor experiments.", "AI": {"tldr": "This study introduces a quadcopter-based system for automated application of coatings on photovoltaic (PV) panels, enhancing panel efficiency through periodic and low-cost maintenance.", "motivation": "The efficiency of photovoltaic panels is critical for renewable energy applications, but coatings that improve their performance degrade over time. Current reapplication methods are costly and labor-intensive, motivating the development of an autonomous, efficient solution.", "method": "The authors designed a quadcopter equipped with a liquid dispersion mechanism and an onboard sensor-based localization stack. This includes visual-inertial odometry and a model-based control system tailored to account for the quadcopter's mass changes and ground effects during operation.", "result": "The proposed system demonstrated robust autonomous capabilities in both indoor and outdoor environments using the onboard navigation and control systems.", "conclusion": "Using UAVs for maintaining photovoltaic panels is a feasible and effective solution, providing flexibility and cost savings compared to traditional methods."}}
{"id": "2509.10833", "pdf": "https://arxiv.org/pdf/2509.10833", "abs": "https://arxiv.org/abs/2509.10833", "authors": ["Dominic Petrak", "Thy Thy Tran", "Iryna Gurevych"], "title": "Towards Automated Error Discovery: A Study in Conversational AI", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "comment": "Accepted to EMNLP 2025 main conference", "summary": "Although LLM-based conversational agents demonstrate strong fluency and\ncoherence, they still produce undesirable behaviors (errors) that are\nchallenging to prevent from reaching users during deployment. Recent research\nleverages large language models (LLMs) to detect errors and guide\nresponse-generation models toward improvement. However, current LLMs struggle\nto identify errors not explicitly specified in their instructions, such as\nthose arising from updates to the response-generation model or shifts in user\nbehavior. In this work, we introduce Automated Error Discovery, a framework for\ndetecting and defining errors in conversational AI, and propose SEEED (Soft\nClustering Extended Encoder-Based Error Detection), as an encoder-based\napproach to its implementation. We enhance the Soft Nearest Neighbor Loss by\namplifying distance weighting for negative samples and introduce Label-Based\nSample Ranking to select highly contrastive examples for better representation\nlearning. SEEED outperforms adapted baselines -- including GPT-4o and Phi-4 --\nacross multiple error-annotated dialogue datasets, improving the accuracy for\ndetecting unknown errors by up to 8 points and demonstrating strong\ngeneralization to unknown intent detection.", "AI": {"tldr": "This paper proposes Automated Error Discovery and SEEED, a novel approach to detect and define errors in conversational AI, improving error detection and generalization.", "motivation": "Address challenges in current LLMs that struggle to identify unspecified or emerging errors in conversational AI, ensuring reliable performance.", "method": "Proposes a framework called Automated Error Discovery and introduces SEEED, which enhances Soft Nearest Neighbor Loss for representation learning and uses Label-Based Sample Ranking to improve error detection.", "result": "SEEED outperforms benchmarks like GPT-4o and Phi-4 in error detection accuracy, showing an improvement of up to 8 points and strong generalization capabilities.", "conclusion": "SEEED represents a significant advancement in conversational AI error detection, demonstrating improved performance and adaptability to unknown scenarios."}}
